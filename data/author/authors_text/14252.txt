Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 71?75,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
English to Indian Languages Machine Transliteration System at 
NEWS 2010 
Amitava Das1, Tanik Saikh2, Tapabrata Mondal3, Asif Ekbal4, Sivaji Bandyopadhyay5 
Department of Computer Science and Engineering1,2,3,5 
Jadavpur University,  
Kolkata-700032, India  
amitava.santu@gmail.com1, tanik4u@gmail.com2, tapabratamon-
dal@gmail.com3, sivaji_cse_ju@yahoo.com5  
Department of Computational Linguistics4 
University of Heidelberg 
Im Neuenheimer Feld 325 
69120 Heidelberg, Germany 
ekbal@cl.uni-heidelberg.de 
 
Abstract 
 
This paper reports about our work in the 
NEWS 2010 Shared Task on Transliteration 
Generation held as part of ACL 2010. One 
standard run and two non-standard runs were 
submitted for English to Hindi and Bengali 
transliteration while one standard and one non-
standard run were submitted for Kannada and 
Tamil. The transliteration systems are based 
on Orthographic rules and Phoneme based 
technology. The system has been trained on 
the NEWS 2010 Shared Task on Translitera-
tion Generation datasets. For the standard run, 
the system demonstrated mean F-Score values 
of 0.818 for Bengali, 0.714 for Hindi, 0.663 
for Kannada and 0.563 for Tamil. The reported 
mean F-Score values of non-standard runs are 
0.845 and 0.875 for Bengali non-standard run-
1 and 2, 0.752 and 0.739 for Hindi non-
standard run-1 and 2, 0.662 for Kannada non-
standard run-1 and 0.760 for Tamil non-
standard run-1. Non-Standard Run-2 for Ben-
gali has achieved the highest score among all 
the submitted runs. Hindi Non-Standard Run-1 
and Run-2 runs are ranked as the 5th and 6th 
among all submitted Runs. 
1 Introduction 
Transliteration is the method of translating one 
source language word into another target lan-
guage by expressing and preserving the original 
pronunciation in their source language. Thus, the 
central problem in transliteration is predicting the 
pronunciation of the original word. Translitera-
tion between two languages that use the same set 
of alphabets is trivial: the word is left as it is. 
However, for languages those use different al-
phabet sets the names must be transliterated or 
rendered in the target language alphabets. Trans-
literation of words is necessary in many applica-
tions, such as machine translation, corpus align-
ment, cross-language Information Retrieval, in-
formation extraction and automatic lexicon ac-
quisition. In the literature, a number of translite-
ration algorithms are available involving English 
(Li et al, 2004; Vigra and Khudanpur, 2003; Go-
to et al, 2003), European languages (Marino et 
al., 2005) and some of the Asian languages, 
namely Chinese (Li et al, 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al, 2003; 
Knight and Graehl, 1998), Korean (Jung et al, 
2000) and Arabic (Al-Onaizan and Knight, 
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving 
Indian languages (Ekbal et al, 2006; Ekbal et al, 
2007; Surana and Singh, 2008). The detailed re-
port of our participation in NEWS 2009 could be 
found in (Das et al, 2009).  
One standard run for Bengali (Bengali 
Standard Run: BSR), Hindi (Hindi Standard 
Run: HSR), Kannada (Kannada Standard Run: 
KSR) and Tamil (Tamil Standard Run: TSR) 
were submitted. Two non-standard runs for Eng-
lish to Hindi (Hindi Non-Standard Run 1 & 2: 
HNSR1 & HNSR2) and Bengali (Bengali Non-
Standard Run 1 & 2: BNSR1 & BNSR1) transli-
teration were submitted. Only one non-standard 
run were submitted for Kannada (Kannada Non-
Standard Run-1: KNSR1) and Tamil (Tamil 
Non-Standard Run-1: TNSR1). 
71
2 Machine Transliteration Systems  
Five different transliteration models have been 
proposed in the present report that can generate 
the transliteration in Indian language from an 
English word. The transliteration models are 
named as Trigram Model (Tri), Joint Source-
Channel Model (JSC), Modified Joint Source-
Channel Model (MJSC), Improved Modified 
Joint Source-Channel Model (IMJSC) and Inter-
national Phonetic Alphabet Based Model (IPA). 
Among all the models the first four are catego-
rized as orthographic model and the last one i.e. 
IPA based model is categorized as phoneme 
based model. 
An English word is divided into Translitera-
tion Units (TUs) with patterns C*V*, where C 
represents a consonant and V represents a vowel. 
The targeted words in Indian languages are di-
vided into TUs with patterns C+M?, where C 
represents a consonant or a vowel or a conjunct 
and M represents the vowel modifier or matra. 
The TUs are the basic lexical units for machine 
transliteration. The system considers the English 
and Indian languages contextual information in 
the form of collocated TUs simultaneously to 
calculate the plausibility of transliteration from 
each English TU to various Indian languages 
candidate TUs and chooses the one with maxi-
mum probability. The system learns the map-
pings automatically from the bilingual NEWS 
2010 training set being guided by linguistic fea-
tures/knowledge. The output of the mapping 
process is a decision-list classifier with collo-
cated TUs in the source language and their 
equivalent TUs in collocation in the target lan-
guage along with the probability of each decision 
obtained from the training set. A Direct example 
base has been maintained that contains the bilin-
gual training examples that do not result in the 
equal number of TUs in both the source and tar-
get sides during alignment. The Direct example 
base is checked first during machine translitera-
tion of the input English word. If no match is 
obtained, the system uses direct orthographic 
mapping by identifying the equivalent TU in In-
dian languages for each English TU in the input 
and then placing the target language TUs in or-
der. The IPA based model has been used for 
English dictionary words. Words which are not 
present in the dictionary are handled by other 
orthographic models as Trigram, JSC, MJSC and 
IMJSC. 
The transliteration models are described below 
in which S and T denotes the source and the tar-
get words respectively: 
3 Orthographic Transliteration models 
The orthographic models work on the idea of 
TUs from both source and target languages. The 
orthographic models used in the present system 
are described below. For transliteration, P(T), 
i.e., the probability of transliteration in the target 
language, is calculated from a English-Indian 
languages bilingual database If, T is not found in 
the dictionary, then a very small value is 
assigned to P(T). These models have been 
desribed in details in Ekbal et al (2007). 
3.1 Trigram 
This is basically the Trigram model where the 
previous and the next source TUs are considered 
as the context.  
( | ) ( , | )1, 11
K
P S T P s t s sk k kk
= < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.2  Joint Source-Channel Model (JSC) 
This is essentially the Joint Source-Channel 
model (Hazhiou et al, 2004) where the 
previous TUs with reference to the current TUs 
in both the source (s) and the target sides (t) are 
considered as the context.  
( | ) ( , | , )11
K
P S T P s t s tk kk
= < > < >?
?
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.3 Modified Joint Source-Channel Model 
(MJSC) 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the Modified 
Joint Source-Channel model. 
( | ) ( , | , )1, 11
K
P S T P s t s t sk k kk
= < > < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.4 Improved Modified Joint Source-
Channel Model (IMJSC) 
In this model, the previous two and the next TUs 
in the source and the previous target TU are 
considered as the context. This is the  Improved 
Modified Joint Source-Channel model. 
72
( | ) ( , | , )1 1, 11
K
P S T P s t s s t sk k k kk
= < > < >? + ? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
4 International Phonetic Alphabet 
(IPA) Model 
The NEWS 2010 Shared Task on Transliteration 
Generation challenge addresses general domain 
transliteration problem rather than named entity 
transliteration. Due to large number of dictionary 
words as reported in Table 1 in NEWS 2010 data 
set a phoneme based transliteration algorithm  
has been devised.  
 Train Dev Test 
Bengali 7.77% 5.14% 6.46% 
Hindi 27.82% 15.80% 3.7% 
Kannada 27.60% 14.63% 4.4% 
Tamil 27.87% 17.31% 3.0% 
Table 1: Statistics of Dictionary Words 
The International Phonetic Alphabet (IPA) is a 
system of representing phonetic notations based 
primarily on the Latin alphabet and devised by 
the International Phonetic Association as a 
standardized representation of the sounds of 
spoken language. The machine-readable 
Carnegie Mellon Pronouncing Dictionary 1  has 
been used as an external resource to capture 
source language IPA structure. The dictionary 
contains over 125,000 words and their 
transcriptions with mappings from words to their 
pronunciations in the given phoneme set. The 
current phoneme set contains 39 distinct 
phonemes. As there is no such parallel IPA 
dictionary available for Indian languages, 
English IPA structures have been mapped to TUs 
in Indian languages during training. An example 
of such mapping between phonemes and TUs are 
shown in Table 3, for which the vowels may 
carry lexical stress as reported in Table 2. This 
phone set is based on the ARPAbet2 symbol set 
developed for speech recognition uses.  
Representation Stress level 
0 No 
1 Primary 
2 Secondary 
Table 2: Stress Level on Vowel 
A pre-processing module checks whether a 
targeted source English word is a valid 
dictionary word or not. The dictionary words are 
then handled by phoneme based transliteration 
module. 
                                                 
1
 www.speech.cs.cmu.edu/cgi-bin/cmudict 
2
 http://en.wikipedia.org/wiki/Arpabet 
Phoneme Example Translation TUs 
AA odd AA0-D - 
AH hut HH0-AH-T - 
D dee D-IY1 -?	 
Table 3: Phoneme Map Patterns of English 
Words and TUs 
In the target side we use our TU segregation 
logic to get phoneme wise transliteration pattern. 
We present this problem as a sequence labelling 
problem, because transliteration pattern changes 
depending upon the contextual phonemes in 
source side and TUs in the target side. We use a 
standard machine learning based sequence 
labeller Conditional Random Field (CRF)3 here. 
IPA based model increased the performance 
for Bengali, Hindi and Tamil languages as 
reported in Section 6. The performance has 
decreased for Kannada. 
5 Ranking 
The ranking among the transliterated outputs 
follow the order reported in Table 4: The ranking 
decision is based on the experiments as described 
in (Ekbal et al, 2006) and additionally based on 
the experiments on NEWS 2010 development 
dataset. 
Word Type  Ranking Order 1 2 3 4 5 
Dictionary IPA IMJSC MJSC JSC Tri 
Non-
Dictionary IMJSC MJSC JSC Tri - 
Table 4: Phoneme Patterns of English Words 
In BSR, HSR, KSR and TSR the orthographic 
TU based models such as: IMJSC, MJSC, JSC 
and Tri have been used only trained by NEWS 
2010 dataset. In BNSR1 and HNSR1 all the or-
thographic models have been trained with addi-
tional census dataset as described in Section 6. In 
case of BNSR2, HNSR2, KNSR1 and TNSR1 
the output of the IPA based model has been add-
ed with highest priority. As no census data is 
available for Kannada and Tamil therefore there 
is only one Non-Standard Run was submitted for 
these two languages only with the output of IPA 
based model along with the output of Standard 
Run. 
6 Experimental Results  
We have trained our transliteration models using 
the NEWS 2010 datasets obtained from the 
NEWS 2010 Machine Transliteration Shared 
Task (Li et al, 2010). A brief statistics of the 
                                                 
3
 http://crfpp.sourceforge.net 
73
datasets are presented in Table 5. During train-
ing, we have split multi-words into collections of 
single word transliterations. It was observed that 
the number of tokens in the source and target 
sides mismatched in various multi-words and 
these cases were not considered further. Follow-
ing are some examples:  
Paris Charles de Gaulle  ???? 
???? ??	?
 ? ?????  
Suven Life Scie  ??? ??
	??
 
Delta Air Lines  ???? 
???	
 
In the training set, some multi-words were 
partly translated and not transliterated. Such ex-
amples were dropped from the training set. In the 
following example the English word ?National? 
is being translated in the target as ??????. 
Australian National Univer-
sity  ????? ???? 
?JU_CSE_GREC10: Named Entity Generation at GREC 2010 
Amitava Das1, Tanik Saikh2, Tapabrata Mondal3, Sivaji Bandyopadhyay4 
Department of Computer Science and Engineering 
Jadavpur University,  
Kolkata-700032, India  
amitava.santu@gmail.com1, tanik4u@gmail.com2, tapabratamon-
dal@gmail.com3, sivaji_cse_ju@yahoo.com4  
 
Abstract 
 
This paper presents the experiments carried 
out at Jadavpur University as part of the par-
ticipation in the GREC Named Entity Genera-
tion Challenge 2010. The Baseline system is 
based on the SEMCAT, SYNCAT and SYN-
FUNC features of REF and REG08-TYPE and 
CASE features of REFEX elements. The dis-
course level system is based on the additional 
positional features: paragraph number, sen-
tence number, word position in the sentence 
and mention number of a particular named ent-
ity in the document. The inclusion of discourse 
level features has improved the performance 
of the system. 
1 Baseline System 
The baseline system is based on the following 
linguistic features of REF elements: SEMCAT 
(Semantic Category), SYNCAT (Syntactic Cate-
gory) and SYNFUNC (Syntactic Function) (Anja 
Belz, 2010) and the following linguistic features 
of REFEX elements: REG08-TYPE (Entity type) 
and CASE (Case marker). The baseline system 
has been separately trained on the training set 
data for the three domains: chefs, composers and 
inventors. The system has been tested on each 
development set by identifying the most probable 
REFEX element among the possible alternatives 
based on the REF element feature combination. 
The probability assigned to a REFEX element 
corresponding to a certain feature combination of 
REF element is calculated as follows:  
( )
i
i
D
REFEX
v D
REF
Np R
N
=  
where ( )vp R is the probability of the targeted 
REFEX element to be assigned, iDREFN is the total 
number of occurrences of REF element feature 
combinations, iD denotes the domain i.e., Chefs, 
Composers and Inventors and iDREFN denotes the 
total number of occurrences of the REFEX ele-
ment corresponding to the REF feature combina-
tion. 
It has been observed that many times the most 
probable REFEX element as identified from the 
training set is not present among the alternative 
REFEX elements. In these cases the system as-
signs the next highest probable REFEX element 
learnt from the training set that matches with one 
of the REFEX elements among the alternatives. 
In some cases more than one REFEX element get 
same probability in the training set. In these cas-
es, the REFEX element that occurs earlier in the 
alternative set is assigned. The experimental re-
sult of Baseline system is reported in Table 1. 
 Chefs Composers Inventors 
Precision 0.63 0.68 0.70 
Recall 0.69 0.60 0.64 
F-Measure 0.66 0.64 0.68 
Table 1: Result of Baseline System 
2 Discourse Level System 
The discourse level features like paragraph num-
ber, sentence number and position of a particular 
word in a sentence have been added with the fea-
tures considered in the baseline system. As men-
tioned in Section 1, more than one REFEX ele-
ment can have the same probability value. This 
happens as REFEX elements are identified by 
two features only REG08-TYPE and CASE.  
 Nam
e 
Pro-
noun 
Com-
mon 
Emp-
ty 
Chefs 2317 3071 55 646 
Composers 2616 4037 92 858 
Inventors 1959 2826 75 621 
Table 2: Distribution of REFEX Types among 
three domains. 
The above problem occurs mainly for Name 
type. Pronouns are very frequent in all the three 
domains but they have small number of varia-
tions as: he, her, him, himself, his, she, who, 
whom and whose. Common type REFEX ele-
ments are too infrequent in the training set and 
they are very hard to generalize. Empty type has 
only one REFEX value as: ?_?.  The distribution 
of the various REFEX types among the three 
domains in the training set is shown in Table 2. 
2.1 Analysis of Name type entities 
Table 2 shows that name types are very frequent 
in all the three domains. Name type entities are 
further differentiated by adding more features 
derived from the analysis of the name type ele-
ment.  
Firstly, the full name of each named entity has 
been identified by Entity identification number 
(id), maximum length among all occurrences of 
that named entity and case marker as plain. For 
example, in Figure 1, the REFEX element of id 3 
has been chosen as a full name of entity ?0? as it 
has the longest string with case ?plain?. 
After identification of full name of each RE-
FEX entity, the following features are identified 
for each occurrence of an entity:: Complete 
Name Genitive (CNG), Complete Name (CN), 
First Name Genitive (FNG), First Name (FN), 
Last Name Genitive (LNG), Last Name (LN), 
Middle Name Genitive (MNG) and Middle 
Name (MN). These features are binary in nature 
and for each occurrence of an entity only one of 
the above features will be true. 
Pronouns are kept as the REFEX element fea-
ture with its surface level pattern as they have 
only 9 variations. Common types are considered 
with tag level ?common? as they hard to general-
ize. Empty types are tagged as ?empty? as they 
have only one tag value ?_?.  
1 
<REFEX ENTITY="0" REG08-
TYPE="name" CASE="genitive">Alain 
Senderens's</REFEX> 
CNG 
2 
<REFEX ENTITY="0" REG08-
TYPE="name" 
CASE="genitive">Senderens's</REFEX> 
LNG 
3 
<REFEX ENTITY="0" REG08-
TYPE="name" CASE="plain">Alain 
Senderens</REFEX> 
CN 
4 
<REFEX ENTITY="0" REG08-
TYPE="name" 
CASE="plain">Senderens</REFEX> 
LN 
Figure 1: Example of Full Name Identification 
3 Experimental Results 
The experimental results of the discourse level 
system on the development set are reported in the 
Table 3 and Table 4 respectively. Table 3 reports 
the results when the system has been trained sep-
arately with domain specific training set and Ta-
ble 4 reports the results when the training has 
been carried out on the complete training set.  
The comparison of the results of the baseline 
and the discourse level system shows an overall 
improvement. But there are some interesting ob-
servations when comparing the results in Table 3 
and Table 4. Currently detailed analyses of the 
results are being carried out.
 Chefs Composers Inventors 
P R F P R F P R F 
Name 0.69 0.74 0.71 0.78 0.61 0.69 0.77 0.67 0.71 
Pronoun 0.81 0.76 0.79 0.70 0.84 0.76 0.76 0.87 0.81 
Common 0.76 0.87 0.81 0.37 0.44 0.40 0.44 0.65 0.68 
Empty 0.92 0.88 0.90 0.86 0.92 0.89 0.72 0.65 0.68 
 
Table 3: Experimental Results of Discourse Level System on the Development Set (Training with 
Domain Specific Training Set) 
 
 
Reg08 Type 
   String  
Accuracy 
BLEU 
NIST 
String Edit 
Distance 
Precision Recall Mean Mean Normalized 1 2 3 4 
Chefs 0.66 0.70 0.57 0.68 0.70 0.76 0.81 3.70 0.77 0.38 
Composers 0.63 0.67 0.56 0.61 0.57 0.54 0.50 3.34 1.07 0.40 
Inventors 0.60 0.62 0.50 0.55 0.54 0.52 0.49 2.90 1.25 0.47 
Total 0.63 0.66 0.54 0.61 0.58 0.57 0.55 3.83 1.03 0.42 
 
Table 4: Table 4: Experimental Results of Discourse Level System on the Development Set (Training 
with Complete Training Set) 
References 
Anja Belz. 2010. GREC Named Entity Generation Challenge 2010: Participants? Pack. 
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 38?42,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Shared task system description: Measuring the Compositionality of 
Bigrams using Statistical Methodologies 
Tanmoy Chakraborty, Santanu Pal, Tapabrata Mondal, Tanik Saikh, 
 Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University 
its_tanmoy@yahoo.co.in, santanu.pal.ju@gmail.com, 
tapabratamondal@gmail.com, tanik4u@gmail.com, 
sivaji_cse_ju@yahoo.com 
 
Abstract 
The measurement of relative 
compositionality of bigrams is crucial to 
identify Multi-word Expressions 
(MWEs) in Natural Language 
Processing (NLP) tasks. The article 
presents the experiments carried out as 
part of the participation in the shared 
task ?Distributional Semantics and 
Compositionality (DiSCo)? organized as 
part of the DiSCo workshop in ACL-
HLT 2011. The experiments deal with 
various collocation based statistical 
approaches to compute the relative 
compositionality of three types of 
bigram phrases (Adjective-Noun, Verb-
subject and Verb-object combinations). 
The experimental results in terms of both 
fine-grained and coarse-grained 
compositionality scores have been 
evaluated with the human annotated gold 
standard data. Reasonable results have 
been obtained in terms of average point 
difference and coarse precision.  
1 Introduction 
The present work examines the relative 
compositionality of Adjective-Noun (ADJ-NN; 
e.g., blue chip), Verb-subject (V-SUBJ; where 
noun acting as a subject of a verb, e.g., name 
imply) and Verb-object (V-OBJ; where noun 
acting as an object of a verb, e.g., beg question) 
combinations using collocation based statistical 
approaches. Measuring the relative 
compositionality is useful in applications such as 
machine translation where the highly non-
compositional collocations can be handled in a 
special way (Hwang and Sasaki, 2005). 
Multi-word expressions (MWEs) are 
sequences of words that tend to co-occur more 
frequently than chance and are either 
idiosyncratic or decomposable into multiple 
simple words (Baldwin, 2006). Deciding 
idiomaticity of MWEs is highly important for 
machine translation, information retrieval, 
question answering, lexical acquisition, parsing 
and language generation. Compositionality 
refers to the degree to which the meaning of a 
MWE can be predicted by combining the 
meanings of its components. Unlike syntactic 
compositionality (e.g. by and large), semantic 
compositionality is continuous (Baldwin, 2006).   
Several studies have been carried out for 
detecting compositionality of noun-noun MWEs 
using WordNet hypothesis (Baldwin et al, 
2003), verb-particle constructions using 
statistical similarities (Bannard et al, 2003; 
McCarthy et al, 2003) and verb-noun pairs 
using Latent Semantic Analysis (Katz and 
Giesbrecht, 2006).  
Our contributions are two-fold: firstly, we 
experimentally show that collocation based 
statistical compositionality measurement can 
assist in identifying the continuum of 
compositionality of MWEs. Secondly, we show 
that supervised weighted parameter tuning 
results in accuracy that is comparable to the best 
manually selected combination of parameters.  
38
2 Proposed Methodologies 
The present task was to identify the numerical 
judgment of compositionality of individual 
phrase. The statistical co-occurrence features 
used in this experiment are described.     
Frequency:  If two words occur together 
quite frequently, the lexical meaning of the 
composition may be different from the 
combination of their individual meanings. The 
frequency of an individual phrase is directly 
used in the following methods. 
Point-wise Information (PMI): An 
information-theoretic motivated measure for 
discovering interesting collocations is point-wise 
mutual information (Church and Hanks, 1990). 
It is originally defined as the mutual information 
between particular events X and Y and in our 
case the occurrence of particular words, as 
follows: 
   = log ,. ? log ,.   1  
PMI represents the amount of information 
provided by the occurrence of the event 
represented by X about the occurrence of the 
event represented by Y. 
T-test:  T-test has been widely used for 
collocation discovery. This statistical test tells us 
the probability of a certain constellation 
(Nugues, 2006). It looks at the mean and 
variance of a sample of measurements. The null 
hypothesis is that the sample is drawn from a 
distribution with mean. T-score is computed 
using the equation (2): 
,  = 