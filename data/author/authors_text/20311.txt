Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1159?1168,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Nonlinear Evidence Fusion and Propagation 
for Hyponymy Relation Mining 
 
Fan Zhang2*     Shuming Shi1     Jing Liu2     Shuqi Sun3*     Chin-Yew Lin1 
1Microsoft Research Asia 
2Nankai University, China 
3Harbin Institute of Technology, China 
{shumings, cyl}@microsoft.com 
 
 
 
Abstract 
This paper focuses on mining the hypon-
ymy (or is-a) relation from large-scale, 
open-domain web documents. A nonlinear 
probabilistic model is exploited to model 
the correlation between sentences in the 
aggregation of pattern matching results. 
Based on the model, we design a set of ev-
idence combination and propagation algo-
rithms. These significantly improve the 
result quality of existing approaches.  Ex-
perimental results conducted on 500 mil-
lion web pages and hypernym labels for 
300 terms show over 20% performance 
improvement in terms of P@5, MAP and 
R-Precision. 
1 Introduction1 
An important task in text mining is the automatic 
extraction of entities and their lexical relations; this 
has wide applications in natural language pro-
cessing and web search. This paper focuses on 
mining the hyponymy (or is-a) relation from large-
scale, open-domain web documents. From the 
viewpoint of entity classification, the problem is to 
automatically assign fine-grained class labels to 
terms. 
There have been a number of approaches 
(Hearst 1992; Pantel & Ravichandran 2004; Snow 
et al, 2005; Durme & Pasca, 2008; Talukdar et al, 
2008) to address the problem. These methods typi-
cally exploited manually-designed or automatical-
                                                          
* This work was performed when Fan Zhang and Shuqi Sun 
were interns at Microsoft Research Asia 
ly-learned patterns (e.g., ?NP such as NP?, ?NP 
like NP?, ?NP is a NP?). Although some degree of 
success has been achieved with these efforts, the 
results are still far from perfect, in terms of both 
recall and precision. As will be demonstrated in 
this paper, even by processing a large corpus of 
500 million web pages with the most popular pat-
terns, we are not able to extract correct labels for 
many (especially rare) entities. Even for popular 
terms, incorrect results often appear in their label 
lists. 
The basic philosophy in existing hyponymy ex-
traction approaches (and also many other text-
mining methods) is counting: count the number of 
supporting sentences. Here a supporting sentence 
of a term-label pair is a sentence from which the 
pair can be extracted via an extraction pattern. We 
demonstrate that the specific way of counting has a 
great impact on result quality, and that the state-of-
the-art counting methods are not optimal. Specifi-
cally, we examine the problem from the viewpoint 
of probabilistic evidence combination and find that 
the probabilistic assumption behind simple count-
ing is the statistical independence between the ob-
servations of supporting sentences. By assuming a 
positive correlation between supporting sentence 
observations and adopting properly designed non-
linear combination functions, the results precision 
can be improved. 
It is hard to extract correct labels for rare terms 
from a web corpus due to the data sparseness prob-
lem. To address this issue, we propose an evidence 
propagation algorithm motivated by the observa-
tion that similar terms tend to share common hy-
pernyms. For example, if we already know that 1) 
Helsinki and Tampere are cities, and 2) Porvoo is 
similar to Helsinki and Tampere, then Porvoo is 
1159
very likely also a city. This intuition, however, 
does not mean that the labels of a term can always 
be transferred to its similar terms. For example, 
Mount Vesuvius and Kilimanjaro are volcanoes 
and Lhotse is similar to them, but Lhotse is not a 
volcano. Therefore we should be very conservative 
and careful in hypernym propagation. In our prop-
agation algorithm, we first construct some pseudo 
supporting sentences for a term from the support-
ing sentences of its similar terms. Then we calcu-
late label scores for terms by performing nonlinear 
evidence combination based on the (pseudo and 
real) supporting sentences. Such a nonlinear prop-
agation algorithm is demonstrated to perform bet-
ter than linear propagation. 
Experimental results on a publicly available col-
lection of 500 million web pages with hypernym 
labels annotated for 300 terms show that our non-
linear evidence fusion and propagation significant-
ly improve the precision and coverage of the 
extracted hyponymy data. This is one of the tech-
nologies adopted in our semantic search and min-
ing system NeedleSeek2. 
In the next section, we discuss major related ef-
forts and how they differ from our work. Section 3 
is a brief description of the baseline approach. The 
probabilistic evidence combination model that we 
exploited is introduced in Section 4. Our main ap-
proach is illustrated in Section 5. Section 6 shows 
our experimental settings and results. Finally, Sec-
tion 7 concludes this paper. 
2 Related Work 
Existing efforts for hyponymy relation extraction 
have been conducted upon various types of data 
sources, including plain-text corpora (Hearst 1992; 
Pantel & Ravichandran, 2004; Snow et al, 2005; 
Snow et al, 2006; Banko, et al, 2007; Durme & 
Pasca, 2008; Talukdar et al, 2008), semi-
structured web pages (Cafarella  et al, 2008; Shin-
zato & Torisawa, 2004), web search results (Geraci 
et al, 2006; Kozareva et al, 2008; Wang & Cohen, 
2009), and query logs (Pasca 2010). Our target for 
optimization in this paper is the approaches that 
use lexico-syntactic patterns to extract hyponymy 
relations from plain-text corpora. Our future work 
will study the application of the proposed algo-
rithms on other types of approaches. 
                                                          
2 http://research.microsoft.com/en-us/projects/needleseek/ or 
http://needleseek.msra.cn/  
The probabilistic evidence combination model 
that we exploit here was first proposed in (Shi et 
al., 2009), for combining the page in-link evidence 
in building a nonlinear static-rank computation 
algorithm. We applied it to the hyponymy extrac-
tion problem because the model takes the depend-
ency between supporting sentences into 
consideration and the resultant evidence fusion 
formulas are quite simple. In (Snow et al, 2006), a 
probabilistic model was adopted to combine evi-
dence from heterogeneous relationships to jointly 
optimize the relationships. The independence of 
evidence was assumed in their model. In compari-
son, we show that better results will be obtained if 
the evidence correlation is modeled appropriately. 
Our evidence propagation is basically about us-
ing term similarity information to help instance 
labeling. There have been several approaches 
which improve hyponymy extraction with instance 
clusters built by distributional similarity. In (Pantel 
& Ravichandran, 2004), labels were assigned to 
the committee (i.e., representative members) of a 
semantic class and used as the hypernyms of the 
whole class. Labels generated by their approach 
tend to be rather coarse-grained, excluding the pos-
sibility of a term having its private labels (consid-
ering the case that one meaning of a term is not 
covered by the input semantic classes). In contrast 
to their method, our label scoring and ranking ap-
proach is applied to every single term rather than a 
semantic class. In addition, we also compute label 
scores in a nonlinear way, which improves results 
quality. In Snow et al (2005), a supervised ap-
proach was proposed to improve hypernym classi-
fication using coordinate terms. In comparison, our 
approach is unsupervised. Durme & Pasca (2008) 
cleaned the set of instance-label pairs with a 
TF*IDF like method, by exploiting clusters of se-
mantically related phrases. The core idea is to keep 
a term-label pair (T, L) only if the number of terms 
having the label L in the term T?s cluster is above a 
threshold and if L is not the label of too many clus-
ters (otherwise the pair will be discarded). In con-
trast, we are able to add new (high-quality) labels 
for a term with our evidence propagation method. 
On the other hand, low quality labels get smaller 
score gains via propagation and are ranked lower. 
Label propagation is performed in (Talukdar et 
al., 2008; Talukdar & Pereira, 2010) based on mul-
tiple instance-label graphs. Term similarity infor-
mation was not used in their approach. 
1160
Most existing work tends to utilize small-scale 
or private corpora, whereas the corpus that we used 
is publicly available and much larger than most of 
the existing work. We published our term sets (re-
fer to Section 6.1) and their corresponding user 
judgments so researchers working on similar topics 
can reproduce our results. 
 
Type Pattern 
Hearst-I NPL {,} (such as) {NP,}
* {and|or} NP  
Hearst-II 
NPL {,} (include(s) | including) {NP,}
* 
{and|or} NP 
Hearst-III NPL {,} (e.g.|e.g) {NP,}
* {and|or} NP 
IsA-I NP (is|are|was|were|being) (a|an) NPL 
IsA-II NP (is|are|was|were|being) {the, those} NPL 
IsA-III NP (is|are|was|were|being) {another, any} NPL 
Table 1. Patterns adopted in this paper (NP: named 
phrase representing an entity; NPL: label) 
3 Preliminaries 
The problem addressed in this paper is corpus-
based is-a relation mining: extracting hypernyms 
(as labels) for entities from a large-scale, open-
domain document corpus. The desired output is a 
mapping from terms to their corresponding hyper-
nyms, which can naturally be represented as a 
weighted bipartite graph (term-label graph). Typi-
cally we are only interested in top labels of a term 
in the graph. 
Following existing efforts, we adopt pattern-
matching as a basic way of extracting hyper-
nymy/hyponymy relations. Two types of patterns 
(refer to Table 1) are employed, including the pop-
ular ?Hearst patterns? (Hearst, 1992) and the IsA 
patterns which are exploited less frequently in ex-
isting hyponym mining efforts. One or more term-
label pairs can be extracted if a pattern matches a 
sentence. In the baseline approach, the weight of 
an edge T?L (from term T to hypernym label L) in 
the term-label graph is computed as, 
 w(T?L)      ( )       
   
    ( )
 (3.1) 
where m is the number of times the pair (T, L) is 
extracted from the corpus, DF(L) is the number of 
in-links of L in the graph, N is total number of 
terms in the graph, and IDF means the ?inverse 
document frequency?. 
A term can only keep its top-k neighbors (ac-
cording to the edge weight) in the graph as its final 
labels. 
Our pattern matching algorithm implemented in 
this paper uses part-of-speech (POS) tagging in-
formation, without adopting a parser or a chunker. 
The noun phrase boundaries (for terms and labels) 
are determined by a manually designed POS tag 
list. 
4 Probabilistic Label-Scoring Model 
Here we model the hyponymy extraction problem 
from the probability theory point of view, aiming 
at estimating the score of a term-label pair (i.e., the 
score of a label w.r.t. a term) with probabilistic 
evidence combination. The model was studied in 
(Shi et al, 2009) to combine the page in-link evi-
dence in building a nonlinear static-rank computa-
tion algorithm. 
We represent the score of a term-label pair by 
the probability of the label being a correct hyper-
nym of the term, and define the following events, 
AT,L: Label L is a hypernym of term T (the ab-
breviated form A is used in this paper unless it is 
ambiguous). 
Ei: The observation that (T, L) is extracted from 
a sentence Si via pattern matching (i.e., Si is a sup-
porting sentence of the pair). 
Assuming that we already know m supporting 
sentences (S1~Sm), our problem is to compute 
P(A|E1,E2,..,Em), the posterior probability that L is 
a hypernym of term T, given evidence E1~Em. 
Formally, we need to find a function f to satisfy, 
 P(A|E1,?,Em) = f(P(A), P(A|E1)?, P(A|Em) ) (4.1) 
For simplicity, we first consider the case of 
m=2. The case of m>2 is quite similar. 
We start from the simple case of independent 
supporting sentences. That is, 
  (     )   (  )   (  ) (4.2) 
  (       )   (    )   (    ) (4.3) 
By applying Bayes rule, we get, 
 
 (       )  
 (       )   ( )
 (     )
 
          
 (    )   ( )
 (  )
 
 (    )   ( )
 (  )
 
 
 ( )
 
          
 (    )   (    )
 ( )
 
(4.4) 
Then define 
 (   )     
 (   )
 ( )
     ( (   ))     ( ( )) 
1161
Here G(A|E) represents the log-probability-gain 
of A given E, with the meaning of the gain in the 
log-probability value of A after the evidence E is 
observed (or known). It is a measure of the impact 
of evidence E to the probability of event A. With 
the definition of G(A|E), Formula 4.4 can be trans-
formed to, 
  (       )   (    )   (    ) (4.5) 
Therefore, if E1 and E2 are independent, the log-
probability-gain of A given both pieces of evidence 
will exactly be the sum of the gains of A given eve-
ry single piece of evidence respectively. It is easy 
to prove (by following a similar procedure) that the 
above Formula holds for the case of m>2, as long 
as the pieces of evidence are mutually independent. 
Therefore for a term-label pair with m mutually 
independent supporting sentences, if we set every 
gain G(A|Ei) to be a constant value g, the posterior 
gain score of the pair will be ?         . If the 
value g is the IDF of label L, the posterior gain will 
be, 
 G(AT,L|E1?,Em) ?    ( )
 
         ( ) (4.6) 
This is exactly the Formula 3.1. By this way, we 
provide a probabilistic explanation of scoring the 
candidate labels for a term via simple counting. 
 
 Hearst-I IsA-I 
E1: Hearst-I 
E2: IsA-I 
RA: 
 (      )
 (    ) (    )
  66.87 17.30 24.38 
R: 
 (    )
 (  ) (  )
  5997 1711 802.7 
RA/R 0.011 0.010 0.030 
Table 2. Evidence dependency estimation for intra-
pattern and inter-pattern supporting sentences 
In the above analysis, we assume the statistical 
independence of the supporting sentence observa-
tions, which may not hold in reality. Intuitively, if 
we already know one supporting sentence S1 for a 
term-label pair (T, L), then we have more chance to 
find another supporting sentence than if we do not 
know S1. The reason is that, before we find S1, we 
have to estimate the probability with the chance of 
discovering a supporting sentence for a random 
term-label pair. The probability is quite low be-
cause most term-label pairs do not have hyponymy 
relations. Once we have observed S1, however, the 
chance of (T, L) having a hyponymy relation in-
creases. Therefore the chance of observing another 
supporting sentence becomes larger than before. 
Table 2 shows the rough estimation of 
 (      )
 (    ) (    )
 (denoted as RA), 
 (    )
 (  ) (  )
 (denoted 
as R), and their ratios. The statistics are obtained 
by performing maximal likelihood estimation 
(MLE) upon our corpus and a random selection of 
term-label pairs from our term sets (see Section 
6.1) together with their top labels3. The data veri-
fies our analysis about the correlation between E1 
and E2 (note that R=1 means independent). In addi-
tion, it can be seen that the conditional independ-
ence assumption of Formula 4.3 does not hold 
(because RA>1). It is hence necessary to consider 
the correlation between supporting sentences in the 
model. The estimation of Table 2 also indicates 
that, 
 
 (     )
 (  ) (  )
 
 (       )
 (    ) (    )
 (4.7) 
By following a similar procedure as above, with 
Formulas 4.2 and 4.3 replaced by 4.7, we have, 
  (       )   (    )   (    ) (4.8) 
This formula indicates that when the supporting 
sentences are positively correlated, the posterior 
score of label L w.r.t. term T (given both the sen-
tences) is smaller than the sum of the gains caused 
by one sentence only. In the extreme case that sen-
tence S2 fully depends on E1 (i.e. P(E2|E1)=1), it is 
easy to prove that 
  (       )   (    )  
It is reasonable, since event E2 does not bring in 
more information than E1. 
Formula 4.8 cannot be used directly for compu-
ting the posterior gain. What we really need is a 
function h satisfying 
  (         )   ( (    )    (    )) (4.9) 
and 
  (      )  ?   
 
     (4.10) 
Shi et al (2009) discussed other constraints to h 
and suggested the following nonlinear functions, 
   (      )    (  ? ( 
    )    )  (4.11) 
                                                          
3 RA is estimated from the labels judged as ?Good?; whereas 
the estimation of R is from all judged labels. 
1162
   (      )  ??   
  
   
 
           (p>1) (4.12) 
In the next section, we use the above two h func-
tions as basic building blocks to compute label 
scores for terms. 
5 Our Approach 
Multiple types of patterns (Table 1) can be adopted 
to extract term-label pairs. For two supporting sen-
tences the correlation between them may depend 
on whether they correspond to the same pattern. In 
Section 5.1, our nonlinear evidence fusion formu-
las are constructed by making specific assumptions 
about the correlation between intra-pattern sup-
porting sentences and inter-pattern ones. 
Then in Section 5.2, we introduce our evidence 
propagation technique in which the evidence of a 
(T, L) pair is propagated to the terms similar to T. 
5.1 Nonlinear evidence fusion 
For a term-label pair (T, L), assuming K patterns 
are used for hyponymy extraction and the support-
ing sentences discovered with pattern i are, 
                  (5.1) 
where mi is the number of supporting sentences 
corresponding to pattern i. Also assume the gain 
score of Si,j is xi,j, i.e., xi,j=G(A|Si,j). 
Generally speaking, supporting sentences corre-
sponding to the same pattern typically have a high-
er correlation than the sentences corresponding to 
different patterns. This can be verified by the data 
in Table-2. By ignoring the inter-pattern correla-
tions, we make the following simplified assump-
tion: 
Assumption: Supporting sentences correspond-
ing to the same pattern are correlated, while those 
of different patterns are independent. 
According to this assumption, our label-scoring 
function is, 
      (   )  ? (               )
 
   
 (5.2) 
In the simple case that         ( ) , if the h 
function of Formula 4.12 is adopted, then, 
      (   )  (? ?  
 
 
   
)     ( ) (5.3) 
We use an example to illustrate the above for-
mula. 
Example: For term T and label L1, assume the 
numbers of the supporting sentences corresponding 
to the six pattern types in Table 1 are (4, 4, 4, 4, 4, 
4), which means the number of supporting sen-
tences discovered by each pattern type is 4. Also 
assume the supporting-sentence-count vector of 
label L2 is (25, 0, 0, 0, 0, 0). If we use Formula 5.3 
to compute the scores of L1 and L2, we can have 
the following (ignoring IDF for simplicity), 
Score(L1)   ?    ; Score(L2) ?     
One the other hand, if we simply count the total 
number of supporting sentences, the score of L2 
will be larger. 
The rationale implied in the formula is: For a 
given term T, the labels supported by multiple 
types of patterns tend to be more reliable than 
those supported by a single pattern type, if they 
have the same number of supporting sentences. 
5.2 Evidence propagation 
According to the evidence fusion algorithm de-
scribed above, in order to extract term labels relia-
bly, it is desirable to have many supporting 
sentences of different types. This is a big challenge 
for rare terms, due to their low frequency in sen-
tences (and even lower frequency in supporting 
sentences because not all occurrences can be cov-
ered by patterns). With evidence propagation, we 
aim at discovering more supporting sentences for 
terms (especially rare terms). Evidence propaga-
tion is motivated by the following two observa-
tions: 
(I) Similar entities or coordinate terms tend to 
share some common hypernyms. 
(II) Large term similarity graphs are able to be 
built efficiently with state-of-the-art techniques 
(Agirre et al, 2009; Pantel et al, 2009; Shi et al, 
2010). With the graphs, we can obtain the similari-
ty between two terms without their hypernyms be-
ing available. 
The first observation motivates us to ?borrow? 
the supporting sentences from other terms as auxil-
iary evidence of the term. The second observation 
means that new information is brought with the 
state-of-the-art term similarity graphs (in addition 
to the term-label information discovered with the 
patterns of Table 1). 
1163
Our evidence propagation algorithm contains 
two phases. In phase I, some pseudo supporting 
sentences are constructed for a term from the sup-
porting sentences of its neighbors in the similarity 
graph. Then we calculate the label scores for terms 
based on their (pseudo and real) supporting sen-
tences. 
Phase I: For every supporting sentence S and 
every similar term T1 of the term T, add a pseudo 
supporting sentence S1 for T1, with the gain score, 
  (         )       (    )   (      ) (5.5) 
where         is the propagation factor, and 
   (   ) is the term similarity function taking val-
ues in [0, 1]. The formula reasonably assumes that 
the gain score of the pseudo supporting sentence 
depends on the gain score of the original real sup-
porting sentence, the similarity between the two 
terms, and the propagation factor. 
Phase II: The nonlinear evidence combination 
formulas in the previous subsection are adopted to 
combine the evidence of pseudo supporting sen-
tences. 
Term similarity graphs can be obtained by dis-
tributional similarity or patterns (Agirre et al, 
2009; Pantel et al, 2009; Shi et al, 2010). We call 
the first type of graph DS and the second type PB. 
DS approaches are based on the distributional hy-
pothesis (Harris, 1985), which says that terms ap-
pearing in analogous contexts tend to be similar. In 
a DS approach, a term is represented by a feature 
vector, with each feature corresponding to a con-
text in which the term appears. The similarity be-
tween two terms is computed as the similarity 
between their corresponding feature vectors. In PB 
approaches, a list of carefully-designed (or auto-
matically learned) patterns is exploited and applied 
to a text collection, with the hypothesis that the 
terms extracted by applying each of the patterns to 
a specific piece of text tend to be similar. Two cat-
egories of patterns have been studied in the litera-
ture (Heast 1992; Pasca 2004; Kozareva et al, 
2008; Zhang et al, 2009): sentence lexical patterns, 
and HTML tag patterns. An example of sentence 
lexical patterns is ?T {, T}*{,} (and|or) T?. HTML 
tag patterns include HTML tables, drop-down lists, 
and other tag repeat patterns. In this paper, we 
generate the DS and PB graphs by adopting the 
best-performed methods studied in (Shi et al, 
2010). We will compare, by experiments, the prop-
agation performance of utilizing the two categories 
of graphs, and also investigate the performance of 
utilizing both graphs for evidence propagation. 
6 Experiments 
6.1 Experimental setup 
Corpus We adopt a publicly available dataset in 
our experiments: ClueWeb094. This is a very large 
dataset collected by Carnegie Mellon University in 
early 2009 and has been used by several tracks of 
the Text Retrieval Conference (TREC)5. The whole 
dataset consists of 1.04 billion web pages in ten 
languages while only those in English, about 500 
million pages, are used in our experiments. The 
reason for selecting such a dataset is twofold: First, 
it is a corpus large enough for conducting web-
scale experiments and getting meaningful results. 
Second, since it is publicly available, it is possible 
for other researchers to reproduce the experiments 
in this paper. 
Term sets Approaches are evaluated by using 
two sets of selected terms: Wiki200, and Ext100. 
For every term in the term sets, each approach 
generates a list of hypernym labels, which are 
manually judged by human annotators. Wiki200 is 
constructed by first randomly selecting 400 Wik-
ipedia6 titles as our candidate terms, with the prob-
ability of a title T being selected to be     (  
 ( )), where F(T) is the frequency of T in our data 
corpus. The reason of adopting such a probability 
formula is to balance popular terms and rare ones 
in our term set. Then 200 terms are manually se-
lected from the 400 candidate terms, with the prin-
ciple of maximizing the diversity of terms in terms 
of length (i.e., number of words) and type (person, 
location, organization, software, movie, song, ani-
mal, plant, etc.). Wiki200 is further divided into 
two subsets: Wiki100H and Wiki100L, containing 
respectively the 100 high-frequency and low-
frequency terms. Ext100 is built by first selecting 
200 non-Wikipedia-title terms at random from the 
term-label graph generated by the baseline ap-
proach (Formula 3.1), then manually selecting 100 
terms. 
Some sample terms in the term sets are listed in 
Table 3. 
 
                                                          
4 http://boston.lti.cs.cmu.edu/Data/clueweb09/  
5 http://trec.nist.gov/  
6 http://www.wikipedia.org/  
1164
Term 
Set 
Sample Terms 
Wiki200 
Canon EOS 400D, Disease management, El Sal-
vador, Excellus Blue Cross Blue Shield, F33, 
Glasstron, Indium, Khandala, Kung Fu, Lake 
Greenwood, Le Gris, Liriope, Lionel Barrymore, 
Milk, Mount Alto, Northern Wei, Pink Lady, 
Shawshank, The Dog Island, White flight, World 
War II? 
Ext100 
A2B, Antique gold, GPTEngine, Jinjiang Inn, 
Moyea SWF to Apple TV Converter, Nanny ser-
vice, Outdoor living, Plasmid DNA, Popon, Spam 
detection, Taylor Ho Bynum, Villa Michelle? 
Table 3. Sample terms in our term sets 
 
Annotation For each term in the term set, the 
top-5 results (i.e., hypernym labels) of various 
methods are mixed and judged by human annota-
tors. Each annotator assigns each result item a 
judgment of ?Good?, ?Fair? or ?Bad?. The annota-
tors do not know the method by which a result item 
is generated. Six annotators participated in the la-
beling with a rough speed of 15 minutes per term. 
We also encourage the annotators to add new good 
results which are not discovered by any method. 
The term sets and their corresponding user anno-
tations are available for download at the following 
links (dataset ID=data.queryset.semcat01): 
http://research.microsoft.com/en-us/projects/needleseek/ 
http://needleseek.msra.cn/datasets/ 
Evaluation We adopt the following metrics to 
evaluate the hypernym list of a term generated by 
each method. The evaluation score on a term set is 
the average over all the terms. 
Precision@k: The percentage of relevant (good 
or fair) labels in the top-k results (labels judged as 
?Fair? are counted as 0.5) 
Recall@k: The ratio of relevant labels in the top-
k results to the total number of relevant labels 
R-Precision: Precision@R where R is the total 
number of labels judged as ?Good? 
Mean average precision (MAP): The average of 
precision values at the positions of all good or fair 
results 
Before annotation and evaluation, the hypernym 
list generated by each method for each term is pre-
processed to remove duplicate items. Two hyper-
nyms are called duplicate items if they share the 
same head word (e.g., ?military conflict? and ?con-
flict?). For duplicate hypernyms, only the first (i.e., 
the highest ranked one) in the list is kept. The goal 
with such a preprocessing step is to partially con-
sider results diversity in evaluation and to make a 
more meaningful comparison among different 
methods. Consider two hypernym lists for ?sub-
way?: 
List-1: restaurant; chain restaurant; worldwide chain 
restaurant; franchise; restaurant franchise? 
List-2: restaurant; franchise; transportation; company; 
fast food? 
There are more detailed hypernyms in the first 
list about ?subway? as a restaurant or a franchise; 
while the second list covers a broader range of 
meanings for the term. It is hard to say which is 
better (without considering the upper-layer appli-
cations). With this preprocessing step, we keep our 
focus on short hypernyms rather than detailed ones. 
 
Term Set Method MAP R-Prec P@1 P@5 
Wiki200 
Linear 0.357 0.376 0.783 0.547 
Log 
0.371 
 3.92% 
0.384 
 2.13% 
0.803 
 2.55% 
0.561 
 2.56% 
PNorm 
0.372 
 4.20% 
0.384 
 2.13% 
0.800 
 2.17% 
0.562 
 2.74% 
Wiki100H 
Linear 0.363 0.382 0.805 0.627 
Log 
0.393 
 8.26% 
0.402 
 5.24% 
0.845 
 4.97% 
0.660 
 5.26% 
PNorm 
0.395 
 8.82% 
0.403 
 5.50% 
0.840 
 4.35% 
0.662 
 5.28% 
Table 4. Performance comparison among various 
evidence fusion methods (Term sets: Wiki200 and 
Wiki100H; p=2 for PNorm) 
6.2 Experimental results 
We first compare the evaluation results of different 
evidence fusion methods mentioned in Section 4.1. 
In Table 4, Linear means that Formula 3.1 is used 
to calculate label scores, whereas Log and PNorm 
represent our nonlinear approach with Formulas 
4.11 and 4.12 being utilized. The performance im-
provement numbers shown in the table are based 
on the linear version; and the upward pointing ar-
rows indicate relative percentage improvement 
over the baseline. From the table, we can see that 
the nonlinear methods outperform the linear ones 
on the Wiki200 term set. It is interesting to note 
that the performance improvement is more signifi-
cant on Wiki100H, the set of high frequency terms. 
By examining the labels and supporting sentences 
for the terms in each term set, we find that for 
many low-frequency terms (in Wiki100L), there 
are only a few supporting sentences (corresponding 
1165
to one or two patterns). So the scores computed by 
various fusion algorithms tend to be similar. In 
contrast, more supporting sentences can be discov-
ered for high-frequency terms. Much information 
is contained in the sentences about the hypernyms 
of the high-frequency terms, but the linear function 
of Formula 3.1 fails to make effective use of it. 
The two nonlinear methods achieve better perfor-
mance by appropriately modeling the dependency 
between supporting sentences and computing the 
log-probability gain in a better way. 
The comparison of the linear and nonlinear 
methods on the Ext100 term set is shown in Table 
5. Please note that the terms in Ext100 do not ap-
pear in Wikipedia titles. Thanks to the scale of the 
data corpus we are using, even the baseline ap-
proach achieves reasonably good performance. 
Please note that the terms (refer to Table 3) we are 
using are ?harder? than those adopted for evalua-
tion in many existing papers. Again, the results 
quality is improved with the nonlinear methods, 
although the performance improvement is not big 
due to the reason that most terms in Ext100 are 
rare. Please note that the recall (R@1, R@5) in this 
paper is pseudo-recall, i.e., we treat the number of 
known relevant (Good or Fair) results as the total 
number of relevant ones. 
 
Method MAP R-Prec P@1 P@5 R@1 R@5 
Linear 0.384 0.429 0.665 0.472 0.116 0.385 
Log 
0.395 0.429 0.715 0.472 0.125 0.385 
 2.86%  0%  7.52%  0%  7.76%  0% 
PNorm 
0.390 0.429 0.700 0.472 0.120 0.385 
 1.56%  0%   5.26%  0%  3.45%  0% 
Table 5. Performance comparison among various 
evidence fusion methods (Term set: Ext100; p=2 
for PNorm) 
The parameter p in the PNorm method is related 
to the degree of correlations among supporting 
sentences. The linear method of Formula 3.1 corre-
sponds to the special case of p=1; while p=  rep-
resents the case that other supporting sentences are 
fully correlated to the supporting sentence with the 
maximal log-probability gain. Figure 1 shows that, 
for most of the term sets, the best performance is 
obtained for   [2.0, 4.0]. The reason may be that 
the sentence correlations are better estimated with 
p values in this range. 
 
 
Figure 1. Performance curves of PNorm with dif-
ferent parameter values (Measure: MAP) 
The experimental results of evidence propaga-
tion are shown in Table 6. The methods for com-
parison are, 
Base: The linear function without propagation. 
NL: Nonlinear evidence fusion (PNorm with 
p=2) without propagation. 
LP: Linear propagation, i.e., the linear function 
is used to combine the evidence of pseudo support-
ing sentences. 
NLP: Nonlinear propagation where PNorm 
(p=2) is used to combine the pseudo supporting 
sentences. 
NL+NLP: The nonlinear method is used to 
combine both supporting sentences and pseudo 
supporting sentences. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.357 0.376 0.783 0.547 0.317 
NL 
0.372 0.384 0.800 0.562 0.325 
 4.20%  2.13%  2.17%  2.74%  2.52% 
LP 
0.357 0.376 0.783 0.547 0.317 
 0%  0%  0%  0%  0% 
NLP 
0.396 0.418 0.785 0.605 0.357 
 10.9%  11.2%  0.26%  10.6%  12.6% 
NL+NLP 
0.447 0.461 0.840 0.667 0.404 
 25.2%  22.6%  7.28%  21.9%  27.4% 
Table 6. Evidence propagation results (Term set: 
Wiki200; Similarity graph: PB; Nonlinear formula: 
PNorm) 
In this paper, we generate the DS (distributional 
similarity) and PB (pattern-based) graphs by adopt-
ing the best-performed methods studied in (Shi et 
al., 2010). The performance improvement numbers 
(indicated by the upward pointing arrows) shown 
in tables 6~9 are relative percentage improvement 
1166
over the base approach (i.e., linear function with-
out propagation). The values of parameter   are set 
to maximize the MAP values. 
Several observations can be made from Table 6. 
First, no performance improvement can be ob-
tained with the linear propagation method (LP), 
while the nonlinear propagation algorithm (NLP) 
works quite well in improving both precision and 
recall. The results demonstrate the high correlation 
between pseudo supporting sentences and the great 
potential of using term similarity to improve hy-
pernymy extraction. The second observation is that 
the NL+NLP approach achieves a much larger per-
formance improvement than NL and NLP. Similar 
results (omitted due to space limitation) can be 
observed on the Ext100 term set. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.357 0.376 0.783 0.547 0.317 
NL+NLP 
(PB) 
0.415 0.439 0.830 0.633 0.379 
 16.2%  16.8%  6.00%  15.7%  19.6% 
NL+NLP 
(DS) 
0.456 0.469 0.843 0.673 0.406 
 27.7%  24.7%  7.66%  23.0%  28.1% 
NL+NLP
(PB+DS) 
0.473 0.487 0.860 0.700 0.434 
 32.5%  29.5%  9.83%  28.0%  36.9% 
Table 7. Combination of PB and DS graphs for 
evidence propagation (Term set: Wiki200; Nonlin-
ear formula: Log) 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.351 0.370 0.760 0.467 0.317 
NL+NLP 
(PB) 
0.411 0.448 0.770 0.564 0.401 
?17.1% ?21.1% ?1.32% ?20.8% ?26.5% 
NL+NLP 
(DS) 
0.469 0.490 0.815 0.622 0.438 
 33.6%  32.4%  7.24%  33.2%  38.2% 
NL+NLP
(PB+DS) 
0.491 0.513 0.860 0.654 0.479 
 39.9%  38.6%  13.2%  40.0%  51.1% 
Table 8. Combination of PB and DS graphs for 
evidence propagation (Term set: Wiki100L) 
Now let us study whether it is possible to com-
bine the PB and DS graphs to obtain better results. 
As shown in Tables 7, 8, and 9 (for term sets 
Wiki200, Wiki100L, and Ext100 respectively, us-
ing the Log formula for fusion and propagation), 
utilizing both graphs really yields additional per-
formance gains. We explain this by the fact that the 
information in the two term similarity graphs tends 
to be complimentary. The performance improve-
ment over Wiki100L is especially remarkable. This 
is reasonable because rare terms do not have ade-
quate information in their supporting sentences due 
to data sparseness. As a result, they benefit the 
most from the pseudo supporting sentences propa-
gated with the similarity graphs. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.384 0.429 0.665 0.472 0.385 
NL+NLP 
(PB) 
0.454 0.479 0.745 0.550 0.456 
 18.3%  11.7%  12.0%  16.5%  18.4% 
NL+NLP 
(DS) 
0.404 0.441 0.720 0.486 0.402 
 5.18%  2.66%  8.27%  2.97%  4.37% 
NL+NLP(P
B+DS) 
0.483 0.518 0.760 0.586 0.492 
 26.0%  20.6%  14.3%  24.2%  27.6% 
Table 9. Combination of PB and DS graphs for 
evidence propagation (Term set: Ext100) 
7 Conclusion 
We demonstrated that the way of aggregating sup-
porting sentences has considerable impact on re-
sults quality of the hyponym extraction task using 
lexico-syntactic patterns, and the widely-used 
counting method is not optimal. We applied a se-
ries of nonlinear evidence fusion formulas to the 
problem and saw noticeable performance im-
provement. The data quality is improved further 
with the combination of nonlinear evidence fusion 
and evidence propagation. We also introduced a 
new evaluation corpus with annotated hypernym 
labels for 300 terms, which were shared with the 
research community. 
Acknowledgments 
We would like to thank Matt Callcut for reading 
through the paper. Thanks to the annotators for 
their efforts in judging the hypernym labels. 
Thanks to Yueguo Chen, Siyu Lei, and the anony-
mous reviewers for their helpful comments and 
suggestions. The first author is partially supported 
by the NSF of China (60903028,61070014), and 
Key Projects in the Tianjin Science and Technolo-
gy Pillar Program. 
 
 
 
 
1167
References  
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas-
ca, and A. Soroa. 2009. A Study on Similarity and 
Relatedness Using Distributional and WordNet-based 
Approaches. In Proc. of NAACL-HLT?2009. 
M. Banko, M.J. Cafarella, S. Soderland, M. Broadhead, 
and O. Etzioni. 2007. Open Information Extraction 
from the Web. In Proc. of IJCAI?2007. 
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. 
Zhang. 2008. WebTables: Exploring the Power of 
Tables on the Web. In Proceedings of the 34th Con-
ference on Very Large Data Bases (VLDB?2008), 
pages 538?549, Auckland, New Zealand. 
B. Van Durme and M. Pasca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of 
labeled instances for open-domain information ex-
traction. Twenty-Third AAAI Conference on Artifi-
cial Intelligence. 
F. Geraci, M. Pellegrini, M. Maggini, and F. Sebastiani. 
2006. Cluster Generation and Cluster Labelling for 
Web Snippets: A Fast and Accurate Hierarchical So-
lution. In Proceedings of the 13th Conference on 
String Processing and Information Retrieval 
(SPIRE?2006), pages 25?36, Glasgow, Scotland. 
Z. S. Harris. 1985. Distributional Structure. The Philos-
ophy of Linguistics. New York: Oxford University 
Press. 
M. Hearst. 1992. Automatic Acquisition of Hyponyms 
from Large Text Corpora. In Fourteenth International 
Conference on Computational Linguistics, Nantes, 
France. 
Z. Kozareva, E. Riloff, E.H. Hovy. 2008. Semantic 
Class Learning from the Web with Hyponym Pattern 
Linkage Graphs. In Proc. of ACL'2008. 
P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu and 
V. Vyas. 2009. Web-Scale Distributional Similarity 
and Entity Set Expansion. EMNLP?2009. Singapore. 
P. Pantel and D. Ravichandran. 2004. Automatically 
Labeling Semantic Classes. In Proc. of the 2004 Hu-
man Language Technology Conference (HLT-
NAACL?2004), 321?328. 
M. Pasca. 2004. Acquisition of Categorized Named 
Entities for Web Search. In Proc. of CIKM?2004. 
M. Pasca. 2010. The Role of Queries in Ranking La-
beled Instances Extracted from Text. In Proc. of 
COLING?2010, Beijing, China. 
S. Shi, B. Lu, Y. Ma, and J.-R. Wen. 2009. Nonlinear 
Static-Rank Computation. In Proc. of CIKM?2009, 
Kong Kong. 
S. Shi, H. Zhang, X. Yuan, J.-R. Wen. 2010. Corpus-
based Semantic Class Mining: Distributional vs. Pat-
tern-Based Approaches. In Proc. of COLING?2010, 
Beijing, China. 
K. Shinzato and K. Torisawa. 2004. Acquiring Hypon-
ymy Relations from Web Documents. In Proc. of the 
2004 Human Language Technology Conference 
(HLT-NAACL?2004). 
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Discov-
ery. In Proceedings of the 19th Conference on Neural 
Information Processing Systems. 
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic 
Taxonomy Induction from Heterogenous Evidence. 
In Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics 
(COLING-ACL-06), 801?808. 
P. P. Talukdar and F. Pereira. 2010. Experiments in 
Graph-based Semi-Supervised Learning Methods for 
Class-Instance Acquisition. In 48th Annual Meeting 
of the Association for Computational Linguistics 
(ACL?2010). 
P. P. Talukdar, J. Reisinger, M. Pasca, D. Ravichandran, 
R. Bhagat, and F. Pereira. 2008. Weakly-Supervised 
Acquisition of Labeled Class Instances using Graph 
Random Walks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language 
Processing (EMNLP?2008), pages 581?589. 
R.C. Wang. W.W. Cohen. Automatic Set Instance Ex-
traction using the Web. In Proc. of the 47th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-IJCNLP?2009), pages 441?449, Sin-
gapore. 
H. Zhang, M. Zhu, S. Shi, and J.-R. Wen. 2009. Em-
ploying Topic Models for Pattern-based Semantic 
Class Discovery. In Proc. of the 47th Annual Meet-
ing of the Association for Computational Linguistics 
(ACL-IJCNLP?2009), pages 441?449, Singapore. 
 
1168
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 459?464,
Dublin, Ireland, August 23-24, 2014.
Peking: Profiling Syntactic Tree Parsing Techniques for Semantic Graph
Parsing
Yantao Du, Fan Zhang, Weiwei Sun and Xiaojun Wan
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{duyantao,ws,wanxiaojun}@pku.edu.cn, zhangf717@gmail.com
Abstract
Using the SemEval-2014 Task 8 data, we
profile the syntactic tree parsing tech-
niques for semantic graph parsing. In par-
ticular, we implement different transition-
based and graph-based models, as well as
a parser ensembler, and evaluate their ef-
fectiveness for semantic dependency pars-
ing. Evaluation gauges how successful
data-driven dependency graph parsing can
be by applying existing techniques.
1 Introduction
Bi-lexical dependency representation is quite pow-
erful and popular to encode syntactic or semantic
information, and parsing techniques under the de-
pendency formalism have been well studied and
advanced in the last decade. The major focus is
limited to tree structures, which fortunately corre-
spond to many computationally good properties.
On the other hand, some leading linguistic theo-
ries argue that more general graphs are needed to
encode a wide variety of deep syntactic and se-
mantic phenomena, e.g. topicalization, relative
clauses, etc. However, algorithms for statistical
graph spanning have not been well explored be-
fore, and therefore it is not very clear how good
data-driven parsing techniques developed for tree
parsing can be for graph generating.
Following several well-established syntactic
theories, SemEval-2014 task 8 (Oepen et al.,
2014) proposes using graphs to represent seman-
tics. Considering that semantic dependency pars-
ing is a quite new topic and there is little previ-
ous work, we think it worth appropriately profil-
ing successful tree parsing techniques for graph
parsing. To this end, we build a hybrid system
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
that combines several important data-driven pars-
ing techniques and evaluate their impact with the
given data. In particular, we implement different
transition-based and graph-based models, as well
as a parser ensembler.
Our experiments highlight the following facts:
? Graph-based models are more effective than
transition-based models.
? Parser ensemble is very useful to boost the
parsing accuracy.
2 Architecture
We explore two kinds of basic models: One is
transition-based, and the other is tree approxima-
tion. Transition-based models are widely used for
dependency tree parsing, and they can be adapted
to graph parsing (Sagae and Tsujii, 2008; Titov
et al., 2009). Here we implement 5 transition-
based models for dependency graph parsing, each
of which is based on different transition system.
The motivation of developing tree approxima-
tion models is to apply existing graph-based tree
parsers to generate graphs. At the training time,
we convert the dependency graphs from the train-
ing data into dependency trees, and train second-
order arc-factored models
1
. At the test phase, we
parse sentences using this tree parser, and convert
the output trees back into semantic graphs. We
think tree approximation can appropriately evalu-
ate the possible effectiveness of graph-based mod-
els for graph spanning.
Finally, we integrate the outputs of different
models with a simple voter to boost the perfor-
mance. The motivation of using system combi-
nation and the choice of voting is mainly due to
the experiments presented by (Surdeanu and Man-
ning, 2010). When we obtain all the outputs of
1
The mate parser (code.google.com/p/
mate-tools/) is used.
459
these models, we combine them into a final result,
which is better than any of them. For combination,
we explore various systems for this task, since em-
pirically we know that variety leads to better per-
formance.
3 Transition-Based Models
Transition-based models are usually used for de-
pendency tree parsing. For this task, we exploit it
for dependency graph parsing.
A transition system S contains a set C of con-
figurations and a set T of transitions. A configu-
ration c ? C generally contains a stack ? of nodes,
a buffer ? of nodes, and a set A of arcs. The ele-
ments in A is in the form (x, l, y), which denotes
a arc from x to y labeled l. A transition t ? T can
be applied to a configuration and turn it into a new
one by adding new arcs or manipulating elements
of the stack or the buffer. A statistical transition-
based parser leverages a classifier to approximate
an oracle that is able to generate target graphs by
transforming the initial configuration c
s
(x) into a
terminal configuration c
t
? C
t
.
An oracle of a given graph on sentence x is a
sequence of transitions which transform the initial
configuration to the terminal configuration the arc
set A
c
t
of which is the set of the arcs of the graph.
3.1 Our Transition Systems
We implemented 5 different transition systems for
graph parsing. Here we describe two of them
in detail, one is the Titov system proposed in
(Titov et al., 2009), and the other is our Naive
system. The configurations of the two systems
each contain a stack ?, a buffer ?, and a set A of
arcs, denoted by ??, ?,A?. The initial configura-
tion of a sentence x = w
1
w
2
? ? ?w
n
is c
s
(x) =
?[0], [1, 2, ? ? ? , n], {}?, and the terminal configu-
ration set C
t
is the set of all configurations with
empty buffer. These two transition systems are
shown in 1.
The transitions of the Titov system are:
? LEFT-ARC
l
adds an arc from the front of the
buffer to the top of the stack, labeled l, into
A.
? RIGHT-ARC
l
adds an arc from the top of the
stack to the front of the buffer, labeled l, into
A.
? SHIFT removes the front of the buffer and
push it onto the stack;
? POP pops the top of the stack.
? SWAP swaps the top two elements of the
stack.
This system uses a transition SWAP to change the
node order in the stack, thus allowing some cross-
ing arcs to be built.
The transitions of the Naive system are similar
to the Titov system?s, except that we can directly
manipulate all the nodes in the stack instead of just
the top two. In this case, the transition SWAP is not
needed.
The Titov system can cover a great proportion,
though not all, of graphs in this task. For more
discussion, see (Titov et al., 2009). The Naive
system, by comparison, covers all graphs. That
is to say, with this system, we can find an oracle
for any dependency graph on a sentence x. Other
transition systems we build are also designed for
dependency graph parsing, and they can cover de-
pendency graphs without self loop as well.
3.2 Statistical Disambiguation
First of all, we derive oracle transition sequences
for every sentence, and train Passive-Aggressive
models (Crammer et al., 2006) to predict next tran-
sition given a configuration. When it comes to
parsing, we start with the initial configuration, pre-
dicting next transition and updating the configura-
tion with the transition iteratively. And finally we
will get a terminal configuration, we then stop and
output the arcs of the graph contained in the final
configuration.
We extracted rich feature for we utilize a set
of rich features for disambiguation, referencing to
Zhang and Nivre (2011). We examine the several
tops of the stack and the one or more fronts of the
buffer, and combine the lemmas and POS tags of
them in many ways as the features. Additionally,
we also derive features from partial parses such as
heads and dependents of these nodes.
3.3 Sentence Reversal
Reversing the order the words of a given sentence
is a simple way to yield heterogeneous parsing
models, thus improving parsing accuracy of the
model ensemble (Sagae, 2007). In our experi-
ments, one transition system produces two mod-
els, one trained on the normal corpus, and the other
on the corpus of reversed sentences. Therefore we
can get 10 parse of a sentence based on 5 transition
systems.
460
LEFT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(j, l, i)})
RIGHT-ARC
l
(?|i, j|?,A)? (?|i, j|?,A ? {(i, l, j)})
SHIFT (?, j|?,A)? (?|j, ?,A)
POP (?|i, ?, A)? (?, ?,A)
SWAP (?|i|j, ?,A)? (?|j|i, ?, A)
Titov System
LEFT-ARC
k
l
(?|i
k
| . . . |i
2
|i
1
, j|?,A)? (?|i
k
| . . . |i
2
|i
1
, j|?,A ? {(j, l, i
k
)})
RIGHT-ARC
k
l
(?|i
k
| . . . |i
2
|i
1
, j|?,A)? (?|i
k
| . . . |i
2
|i
1
, j|?,A ? {(i
k
, l, j)})
SHIFT (?, j|?,A)? (?|j, ?,A)
POP
k
(?|i
k
|i
k?1
| . . . |i
2
|i
1
, ?, A)? (?|i
k?1
| . . . |i
2
|i
1
, ?, A)
Naive System
Figure 1: Two of our transition systems.
4 Tree Approximation Models
Parsing based on graph spanning is quite challeng-
ing since computational properties of the seman-
tic graphs given by the shared task are less ex-
plored and thus still unknown. On the other hand,
finding the best higher-order spanning for general
graph is NP complete, and therefore it is not easy,
if not impossible, to implement arc-factored mod-
els with exact inference. In our work, we use a
practical idea to indirectly profile the graph-based
parsing techniques for dependency graph parsing.
Inspired by the PCFG approximation idea (Fowler
and Penn, 2010; Zhang and Krieger, 2011) for
deep parsing, we study tree approximation ap-
proaches for graph spanning.
This tree approximation technique can be ap-
plied to both transition-based and graph-based
parsers. However, since transition systems that
can directly handle build graphs have been devel-
oped, we only use this technique to evaluate the
possible effectiveness of graph-based models for
semantic parsing.
4.1 Graph-to-Tree Transformation
In particular, we develop different methods to con-
vert a semantic graph into a tree, and use edge
labels to encode dependency relations as well as
structural information which helps to transform a
converted tree back to its original graph. By the
graph-to-tree transformation, we can train a tree
parser with a graph-annotated corpus, and utilize
the corresponding tree-to-graph transformation to
generate target graphs from the outputs of the tree
parser. Given that the tree-to-graph transformation
is quite trivial, we only describe the graph-to-tree
transformation approach.
We use graph traversal algorithms to convert a
directed graph to a directed tree. The transforma-
tion implies that we may lose, add or modify some
dependency relations in order to make the graph a
tree.
4.2 Auxiliary Labels
In the transformed trees, we use auxiliary labels to
carry out information of the original graphs. To
encode multiple edges to one, we keep the origi-
nal label on the directed edge but may add other
edges? information. On the other hand, through-
out most transformations, some edges must be re-
versed to make a tree, so we need a symbol to in-
dicate a edge on the tree is reversed during trans-
formation. The auxiliary labels are listed below:
? Label with following ?R: The symbol ?R
means this directed edge is reversed from the
original directed graph.
? Separator: Semicolon separates two encoded
original edges.
? [N ] followed by label: The symbol [N ] (N
is an integer) represents the head of the edge.
The dependent is the current one, but the head
is the dependent?s N -th ancestor where 1st
ancestor is its father and 2nd ancestor is its
father?s father.
See Figure 2 for example.
4.3 Traversal Strategies
Given directed graph (V,E), the task is to traverse
all edges on the graph and decide how to change
the labels or not contain the edge on the output.
We use 3 strategies for traversal. Here we use
x ?
g
y to denote the edge on graph, and x ?
t
y
the edge on tree.
461
Mrs Ward was relieved
noun ARG1 verb ARG1 verb ARG2
adj ARG1
root
Mrs Ward was relieved
noun ARG1?R verb ARG1 verb ARG2
root
Mrs Ward was relieved
noun ARG1?R verb ARG2
adj ARG1;[2]verb ARG1
root
Figure 2: One dependency graph and two possible
dependency trees after converting.
Depth-first-search We try graph traversal by
depth-first-search starting from the root on the di-
rected graph ignoring the direction of edges. Dur-
ing the traversal, we add edges to the directed tree
with (perhaps new) labels. We traverse the graph
recursively. Suppose the depth-first-search is run-
ning at the node x and the nodes set A which have
been searched. And suppose we find node y is
linked to x on the graph (x ?
g
y or y ?
g
x).
If y /? A, we add the directed edge x ?
t
y to the
tree immediately. In the case of y ?
g
x, we add
?R to the edge label. If y ? A, then y must be one
of the ancestors of x. In this case, we add this in-
formation to the label of the existing edge z ?
t
x.
Since the distance between two nodes x and y is
sufficient to indicate the node y, we use the dis-
tance to represent the head or dependent of this
directed edge and add the label and the distance to
the label of z ?
t
x. It is clear that the auxiliary
label [N ] can be used for multiple edge encoding.
Under this strategy, all edges can be encoded on
the tree.
Breadth-first-search An alternative traversal
strategy is based on breadth-first-search starting
from the root. This search ignores the direction
of edge too. We regard the search tree as the de-
pendency tree. During the breadth-first-search, if
(x, l, y) exists but node y has been searched, we
just ignore the edge. Under this strategy, we may
lose some edges.
Iterative expanding This strategy is based on
depth-first-search but slightly different. The strat-
egy only searches through the forward edges on
the directed graph at first. When there is no for-
ward edge to expend, a traversed node linked to
some nodes that are not traversed must be the de-
pendent of them. Then we choose an edge and add
it (reversed) to the tree and continue to expand the
tree. Also, we ignore the edges that does not sat-
isfy the tree constraint. We call this strategy iter-
ative expanding. When we need to expand output
tree, we need to design a strategy to decide which
edge to be add. The measure to decide which node
should be expanded first is its possible location on
the tree and the number of nodes it can search dur-
ing depth-first-search. Intuitively, we want the re-
versed edges to be as few as possible. For this
purpose, this strategy is practical but not necessar-
ily the best. Like the Breadth-first-search strategy,
this strategy may also cause edge loss.
4.4 Forest-to-Tree
After a primary searching process, if there is still
edge x ?
g
y that has not been searched yet, we
start a new search procedure from x or y. Even-
tually, we obtain a forest rather than a tree. To
combine disconnected trees in this forest to the fi-
nal dependency tree, we use edges with label None
to link them. Let the node setW be the set of roots
of the trees in the forest, which are not connected
to original graph root. The mission is to assign a
node v /? W for each w ? W . If we assign v
i
for
w
i
, we add the edge v
i
? w
i
labeled by None to
the final dependency tree. We try 3 strategies in
this step:
? For each w ? W we look for the first node
v /?W on the left of w.
? For each w ? W we look for the first node
v /?W on the right of w.
? By defining the distance between two nodes
as how many words are there between the two
words, we can select the nearest node. If the
distances of more than one node are equal,
we choose v randomly.
We also tried to link all of the nodes in W di-
rectly to the root, but it does not work well.
5 Model Ensemble
We have 19 heterogeneous basic models (10
transition-based models, 9 tree approximation
models), and use a simple voter to combine their
outputs.
462
Algorithm DM PAS PCEDT
DFS 0 0 0
BFS 0.0117 0.0320 0.0328
FEF 0.0127 0.0380 0.0328
Table 1: Edge loss of transformation algorithms.
For each pair of words of a sentence, we count
the number of the models that give positive pre-
dictions. If the number is greater than a threshold,
we put this arc to the final graph, and label the arc
with the most common label of what the models
give.
Furthermore, we find that the performance of
the tree approximation models is better than the
transition based models, and therefore we take
weights of individual models too. Instead of just
counting, we sum the weights of the models that
give positive predictions. The tree approximation
models are assigned higher weights.
6 Experiments
There are 3 subtasks in the task, namely DM, PAS,
and PCEDT. For subtask DM, we finally obtained
19 models, just as stated in previous sections.
For subtask PAS and PCEDT, only 17 models are
trained due to the tight schedule.
The tree approximation algorithms may cause
some edge loss, and the statistics are shown in Ta-
ble 1. We can see that DFS does not cause edge
loss, but edge losses of other two algorithm are
not negligible. This may result in a lower recall
and higher precision, but we can tune the final re-
sults during model ensemble. Edge loss in subtask
DM is less than those in subtask PAS and PCEDT.
We present the performance of several repre-
sentative models in Table 2. We can see that the
tree approximation models performs better than
the transition-based models, which highlights the
effective of arc-factored models for semantic de-
pendency parsing. For model ensemble, besides
the accuracy of each single model, it is also im-
portant that the models to be ensembled are very
different. As shown in Table 2, the evaluation be-
tween some of our models indicates that our mod-
els do vary a lot.
Following the suggestion of the task organizers,
we use section 20 of the train data as the devel-
opment set. With the help of development set,
we tune the parameters of the models and ensem-
Models DM PAS PCEDT
Titov 0.8468 0.8754 0.6978
Titov
r
0.8535 0.8928 0.7063
Naive 0.8481 - -
DFS
n
0.8692 0.9034 0.7370
DFS
l
0.8692 0.9015 0.7246
BFS
n
0.8686 0.8818 0.7247
Titov vs. Titov
r
0.8607 0.8831 0.7613
Titov vs. Naive 0.9245 - -
Titov vs. DFS
n
0.8590 0.8865 0.7650
DFS
n
vs. DFS
l
0.9273 0.9579 0.8688
DFS
n
vs. BFS
n
0.9226 0.9169 0.8367
Table 2: Evaluation between some of our models.
Labeled f-score on test set is shown. Titov
r
stands
for reversed Titov, DFS
n
for DFS+nearest, DFS
l
for DFS+left, and BFS
n
for BFS+nearest. The up-
per part gives the performance, and the lower part
gives the agreement between systems.
Format LP LR LF LM
DM 0.9027 0.8854 0.8940 0.2982
PAS 0.9344 0.9069 0.9204 0.3872
PCEDT 0.7875 0.7396 0.7628 0.1120
Table 3: Final results of the ensembled model.
bling. We set the weight of each transition-based
model 1, and tree approximation model 2 in run
1, 3 in run 2. The threshold is set to a half of the
total weight. The final results given by the orga-
nizers are shown in Table 3. Compared to Table 2
demonstrates the effectiveness of parser ensemble.
7 Conclusion
Data-driven dependency parsing techniques have
been greatly advanced during the parst decade.
Two dominant approaches, i.e. transition-based
and graph-based methods, have been well stud-
ied. In addition, parser ensemble has been shown
very effective to take advantages to combine the
strengthes of heterogeneous base parsers. In this
work, we propose different models to profile the
three techniques for semantic dependency pars-
ing. The experimental results suggest several di-
rections for future study.
Acknowledgement
The work was supported by NSFC (61300064,
61170166 and 61331011) and National High-Tech
R&D Program (2012AA011101).
463
References
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. JOURNAL OF MA-
CHINE LEARNING RESEARCH, 7:551?585.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with combinatory cate-
gorial grammar. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, Uppsala, Sweden, July.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Haji?c, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 Task
8: Broad-coverage semantic dependency parsing. In
Proceedings of the 8th International Workshop on
Semantic Evaluation, Dublin, Ireland.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 753?760, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Kenji Sagae. 2007. Dependency parsing and domain
adaptation with lr models and parser ensembles. In
In Proceedings of the Eleventh Conference on Com-
putational Natural Language Learning.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, Los Angeles, California, June.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarisa-
tion for synchronous parsing of semantic and syn-
tactic dependencies. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1562?1567, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Yi Zhang and Hans-Ulrich Krieger. 2011. Large-scale
corpus-driven PCFG approximation of an hpsg. In
Proceedings of the 12th International Conference on
Parsing Technologies, Dublin, Ireland, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Portland, Oregon, USA, June.
464
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 149?154,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Sentence-level Rewriting Detection
Fan Zhang
University of Pittsburgh
Pittsburgh, PA, 15260
zhangfan@cs.pitt.edu
Diane Litman
University of Pittsburgh
Pittsburgh, PA, 15260
litman@cs.pitt.edu
Abstract
Writers usually need iterations of revisions
and edits during their writings. To bet-
ter understand the process of rewriting,
we need to know what has changed be-
tween the revisions. Prior work mainly fo-
cuses on detecting corrections within sen-
tences, which is at the level of words
or phrases. This paper proposes to de-
tect revision changes at the sentence level.
Looking at revisions at a higher level al-
lows us to have a different understanding
of the revision process. This paper also
proposes an approach to automatically de-
tect sentence revision changes. The pro-
posed approach shows high accuracy in an
evaluation using first and final draft essays
from an undergraduate writing class.
1 Introduction
Rewriting is considered to be an important process
during writing. However, conducting successful
rewriting is not an easy task, especially for novice
writers. Instructors work hard on providing sug-
gestions for rewriting (Wells et al., 2013), but usu-
ally such advice is quite general. We need to un-
derstand the changes between revisions better to
provide more specific and helpful advice.
There has already been work on detecting cor-
rections in sentence revisions (Xue and Hwa,
2014; Swanson and Yamangil, 2012; Heilman
and Smith, 2010; Rozovskaya and Roth, 2010).
However, these works mainly focus on detecting
changes at the level of words or phrases. Ac-
cording to Faigley?s definition of revision change
(Faigley and Witte, 1981), these works could help
the identification of Surface Changes (changes
that do not add or remove information to the orig-
inal text). However, Text Changes (changes that
add or remove information) will be more difficult
to identify if we only look at revisions within sen-
tences. According to Hashemi and Schunn (2014),
when instructors were presented a comparison of
differences between papers derived from words,
they felt the information regarding changes be-
tween revisions was overwhelming.
This paper proposes to look at the changes be-
tween revisions at the level of sentences. Com-
paring to detecting changes at the word level, de-
tecting changes at the sentence level contains less
information, but still keeps enough information
to understand the authors? intention behind their
modifications to the text. The sentence level edits
could then be grouped and classified into differ-
ent types of changes. The long-term goal of this
project is to allow us to be able to identify both
Text Changes and Surface Changes automatically.
Students, teachers, and researchers could then per-
form analysis on the different types of changes and
have a better understanding of the rewriting pro-
cess. As a preliminary work, this paper explores
steps toward this goal: First, automatically gener-
ate the description of changes based on four prim-
itives: Add, Delete, Modify, Keep; Second, merge
the primitives that come from the same purpose.
2 Related work
Hashemi and Schunn (2014) presented a tool
to help professors summarize students? changes
across papers before and after peer review. They
first split the original documents into sentences
and then built on the output of Compare Suite
(CompareSuite, 2014) to count and highlight
changes in different colors. Figure 1 shows a
screenshot of their work. As we can see, the mod-
ifications to the text are misinterpreted. Line 66
in the final draft should correspond to line 55 and
line 56 in the first draft, while line 67 and line 68
should be a split of line 57 in the first draft. How-
ever, line 67 is aligned to line 56 wrongly in their
work. This wrong alignment caused many mis-
149
recognized modifications. According to Hashemi,
the instructors who use the system think that the
overwhelming information of changes make the
system less useful. We hypothesize that since their
work is based on analysis at the word level, al-
though their approach might work for identifying
differences within one sentence, it makes mistakes
when sentence analysis is the primary concern.
Our work avoids the above problem by detect-
ing differences at the sentence level. Sentence
alignment is the first step of our method; fur-
ther inferences about revision changes are then
based on the alignments generated. We borrow
ideas from the research on sentence alignment for
monolingual corpora. Existing research usually
focuses on the alignment from the text to its sum-
marization or its simplification (Jing, 2002; Barzi-
lay and Elhadad, 2003; Bott and Saggion, 2011).
Barzilay and Elhadad (2003) treat sentence align-
ment as a classification task. The paragraphs are
clustered into groups, and a binary classifier is
trained to decide whether two sentences should be
aligned or not. Nelken (2006) further improves
the performance by using TF*IDF score instead of
word overlap and also utilizing global optimiza-
tion to take sentence order information into con-
sideration. We argue that summarization could
be considered as a special form of revision and
adapted Nelken?s approach to our approach.
Edit sequences are then inferred based on the
results of sentence alignment. Fragments of ed-
its that come from the same purpose will then be
merged. Related work to our method is sentence
clustering (Shen et al., 2011; Wang et al., 2009).
While sentence clustering is trying to find and
cluster sentences similar to each other, our work
is to find a cluster of sentences in one document
that is similar to one sentence in the other docu-
ment after merging.
3 Sentence-level changes across revisions
3.1 Primitives for sentence-level changes
Previous work in educational revision analysis
(Faigley and Witte, 1981; Connor and Asenav-
age, 1994) categorized revision changes to be ei-
ther surface changes or text-based changes. With
both categories, six kinds of changes were defined
as shown in Table 1.
Different from Faigley?s definition, we define
only 4 primitives for our first step of edit sequence
generation: Add, Delete, Modify and Keep. This
Code Explanation
Addition Adding a word or phrase
Deletion Omitting a word or phrase
Substitutions exchange words with synonyms
Permutation rearrange of words or phrases
Distribution one segment divided into two
Consolidation combine two segments into one
Table 1: Code Definition by L.Faigley and S.Witte
definition is similar to Bronner?s work (Bronner
and Monz, 2012). We choose this definition be-
cause these 4 primitives only correspond to one
sentence at a time. Add, Delete, Modify indicates
that the writer has added/deleted/modified a sen-
tence. Keep means the original sentence is not
modified. We believe Permutation, Distribution
and Consolidation as defined by Faigley could be
described with these four primitives, which could
be recognized in the later merge step.
3.2 Data and annotation
The corpus we choose consists of paired first and
final drafts of short papers written by undergradu-
ates in a course ?Social Implications of Comput-
ing Technology?. Students are required to write
papers on one topic and then revise their own pa-
pers. The revisions are guided by other students?
feedback based on a grading rubric, using a web-
based peer review system. Students first submitted
their original paper into the system, and then were
randomly assigned to review and comment others?
work according to the writing rubric. The authors
would receive the others? anonymous comments,
and then could choose to revise their work based
on others? comments as well as their own insights
obtained by reviewing other papers.
The papers in the corpus contain two topics.
In the first topic, the students discussed the role
that Big Data played in Obama?s presidential cam-
paign. This topic contains 11 pairs of first and final
drafts of short papers. We name this C1. The other
topic, named C2, talks about intellectual property
and contains 10 pairs of paper drafts. The students
involved in these two topics are from the same
class. Students make more modifications to their
papers in C2. More details can be seen in Table 2.
Our revision change detection approach con-
tains three steps: sentence alignment, edit se-
quence generation and merge of edit sequences.
Thus we annotated for these three steps.
150
(a) first draft (b) final draft
(c) Revision detection using Hashemi?s approach
Figure 1: Fragments of a paper in corpus C2 discussing intellectual property, (c) is Hashemi?s work,
green for recognized modifications, blue for insertions and red for deletion
For sentence alignment, each sentence in the fi-
nal draft is assigned the index of its aligned sen-
tence in the original draft. If a sentence is newly
added, it will be annotated as ADD. Sentence
alignment is not necessarily one-to-one. It can
also be one-to-many (Consolidation) and many-
to-one (Distribution). Table 3 shows a fragment
of the annotation for the text shown in Figure 1.
For edit sequences, the annotators do the anno-
tation based on the initial draft. For the same frag-
ment in Table 3, the annotated sequence is: Keep,
Modify, Delete, Modify, Add
1
.
For edit sequence merging, we further annotate
Consolidation and Distribution based on the edit
sequences. In our example, 66 consolidates 55 and
56, while 57 distributes to 67 and 68.
pairs #D1 #D2 Avg1 Avg2
C1 11 761 791 22.5 22.7
C2 10 645 733 24.7 24.5
Table 2: Detailed information of corpora. #D1 and
#D2 are the number of sentences in the first and
final draft, Avg1 and Avg2 are the average number
of words in one sentence in the first and final draft
As a preliminary work, we only have one anno-
tator doing all the annotations. But for the anno-
tation of sentence alignments, we have two anno-
1
66 consolidates 55, 56; while 57 distributes to 67, 68.
Notice that Consolidation is illustrated as Modify, Delete and
Distribution is illustrated as Modify, Add. As the annotators
annotate based on the first draft, Modify always appears be-
fore Add or Delete
tators annotating on one pair of papers. The paper
contains 76 sentences, and the annotators only dis-
agree in one sentence. The kappa is 0.794
2
, which
suggests that the annotation is reliable based on
our annotation scheme.
4 Automatic detection of revision
changes
The detection of revision changes contains three
parts: sentence alignment, edit sequence genera-
tion and edit sequence merging. The first two parts
generate edit sequences detected at the sentence
level, while the third part groups edit sequences
and classifies them into different types of changes.
Currently the third step only covers the identifica-
tion of Consolidation and Distribution.
Sentence Index (Final) 65 66 67 68
Sentence Index (First) 54 55,56 57 57
Table 3: An example of alignment annotation
Sentence alignment We adapted Nelken?s ap-
proach to our problem.
Alignment based on sentence similarity
The alignment task goes through three stages.
1. Data preparation: for each sentence in the an-
notated final draft, if it is not a new sentence, cre-
ate a sentence pair with its aligned sentence in the
2
We calculate the Kappa value following Macken?s idea
(Macken, 2010), where the aligned sentences are categorized
as direct-link, while new added sentences are categorized as
null-link (ADD).
151
first draft. The pair is considered to be an aligned
pair. Also, randomly select another sentence from
the first draft to make a negative sentence pair.
Thus we ensure there are nearly equal numbers of
positive and negative cases in the training data.
2. Training: according to the similarity met-
ric defined, calculate the similarity of the sentence
pairs. A logistic regression classifier predicting
whether a sentence pair is aligned or not is trained
with the similarity score as the feature. In addi-
tion to classification, the classifier is also used to
provide a similarity score for global alignment.
3. Alignment: for each pair of paper drafts, con-
struct sentence pairs using the Cartesian product
of sentences in the first draft and sentences in the
final. Logistic regression classifier is used to deter-
mine whether the sentence pair is aligned or not.
We added Levenshtein distance (LD) (Leven-
shtein, 1966) as another similarity metric in ad-
dition to Nelken?s metrics. Together three similar-
ity metrics were compared: Levenshtein Distance,
Word Overlap(WO), and TF*IDF.
Global alignment
Sentences are likely to preserve the same or-
der between rewritings. Thus, sentence or-
dering should be an important feature in sen-
tence alignment. Nelken?s work modifies the
Needleman-Wunsch alignment (Needleman and
Wunsch, 1970) to find the sentence alignments and
goes in the following steps.
Step1: The logistic regression classifier previ-
ously trained assigns a probability value from 0 to
1 for each sentence pair s(i, j). Use this value as
the similarity score of sentence pair: sim(i, j).
Step2: Starting from the first pair of sen-
tences, find the best path to maximize the likeli-
hood between sentences according to the formula
s(i, j) = max{s(i ? 1, j ? 1) + sim(i, j), s(i ?
1, j) + sim(i, j) , s(i, j ? 1) + sim(i, j)}
Step3: Infer the sentence alignments by back
tracing the matrix s(i, j).
We found out that changing bolded parts in the
formula to s(i, j) = max{s(i ? 1, j ? 1) +
sim(i, j), s(i ? 1, j) + insertcost , s(i, j ? 1) +
deletecost} shows better performance in our prob-
lem. According to our experiment with C1, insert-
cost and deletecost are both set to 0.1 as they are
found to be the most effective during practice.
Edit sequence generation This step is an inter-
mediate step, which tries to generate the edit se-
quence based on the sentence alignment results
from the previous step. The edit sequences gen-
erated would later be grouped together and clas-
sified into different types. In our current work, a
rule-based method is proposed for this step.
Step1: The index of original document i and the
index of the modified document j both start from
0. If sentence i in the original document is aligned
to sentence j in the modified one, go to step 2, if
not go to step 3.
Step2: If the two sentences are exactly the same,
add Keep to the edit sequence, if not, add Modify.
Increase i and j by 1, go to step 1.
Step3: Check the predicted alignment index of
sentence j, if the predicted index is larger than sen-
tence i in the original document, add Delete and
increase i by 1, otherwise, mark as Add and in-
crease j by 1, go to step 1.
Edit sequence merging Distribution means
splitting one sentence into two or more sentences,
while Consolidation means merging two or more
sentences into one sentence. These two operations
can be derived with primitives Modify, Add and
Delete. They follow the following patterns:
Consolidation: Modify-Delete-Delete-...
Distribution: Modify-Add-Add-...
These sequences both start with Modify fol-
lowed with a repetitive number of Delete or Add.
A group of edit sequences can be merged if they
can be merged to a sentence close to the sentence
in the other draft. We applied a rule-based ap-
proach based on our observations.
We first scan through the sequence generated
above. Sequences with Modify-Add-... or Mod-
ify-Delete-... are extracted. For each sequence ex-
tracted, if there are n consecutive Add or Delete
following Modify, create n groups, Group
i
(i ?
n) contains sentences from the modified sentence
to the next consecutive i sentences. For each
group, merge all the sentences, and use the clas-
sifier trained above to get the similarity score
Sim
group
i
between the merged sentence and the
original one. If there are multiple groups classi-
fied as aligned, choose group i that has the largest
Sim
group
i
, merge the basic edit operations into
Consolidation or Distribution. If none of the
groups are classified as aligned, do not merge.
5 Evaluation
Sentence alignment We use accuracy as the
evaluation metric. For each pair of drafts, we
count the number of sentences in the final draft
152
N1
. For each sentence in the final draft, we count
the number of sentences that get the correct align-
ment as N
2
. The accuracy of the sentence align-
ment is
N
2
N
1
.
3
We use Hashemi?s approach as the baseline.
Compare Suite colors the differences out, as
shown in Figure 1. We treat the green sentences
as Modify and aligned to the original sentence.
For our method, we tried four groups of set-
tings. Group 1 and group 2 perform leave-one-out
cross validation on C1 and C2 (test on one pair of
paper drafts and train on the others). Group 3 and
group 4 train on one corpus and test on the other.
Group LD WO TF*IDF Baseline
1 0.9811 0.9863 0.9931 0.9427
2 0.9649 0.9593 0.9667 0.9011
3 0.9727 0.9700 0.9727 0.9045
4 0.9860 0.9886 0.9798 0.9589
Table 4: Accuracy of our approach vs. baseline
Table 4 shows that all our methods beat the
baseline
4
. Among the three similarity metrics,
TF*IDF is the most predictive.
Edit sequence generation We use WER (Word
Error Rate) from speech recognition for evaluat-
ing the generated sequence by comparing the gen-
erated sequence to the gold standard.
WER is calculated based on edit distances be-
tween sequences. The ratio is calculated as:
WER =
S+D+I
N
, where S means the number of
modifications, D means the number of deletes, I
means the number of inserts.
We apply our method on the gold standard of
sentence alignment. The generated edit sequence
is then compared with the gold standard edit se-
quence to calculate WER. Hashemi?s approach is
chosen as the baseline. The WER of our method is
0.035 on C1 and 0.017 on C2, comparing to 0.091
on C1 and 0.153 on C2 for the baseline, which
shows that our rule-based method has promise.
3
Notice that we have the case that one sentence is aligned
to two sentences (i.e. Consolidation, as sentence 66 in Table
3). In our evaluation, an alignment is considered to be correct
only if the alignment covers all the sentences that should be
covered. For example, if Sentence 66 in Table 3 is aligned to
Sentence 55 in the first draft, it is counted as an error.
4
For Groups 1 and 2, we calculate the accuracy of
Hashemi?s approach under a leave-one-out setting, each time
remove one pair of document and calculate the accuracy. A
significance test is also conducted, the worst metric LD in
Group 1 and WO in Group 2 both beat the baseline signifi-
cantly ( p
1
= 0.025,p
2
= 0.017) in two-tailed T-test.
Applying our method on the predicted alignment
on the first step gets 0.067 on C1 and 0.025 on C2,
which although degraded still beats the baseline.
Edit sequence merging There are only a limited
number of Consolidation and Distribution exam-
ples in our corpus. Together there are 9 Consolida-
tion and 5 Distribution operations. In our current
data, the number of sentences involved in these
operations is always 2. Our rule-based method
achieved 100% accuracy in the identification of
these operations. It needs further work to see if
this method would perform equally well in more
complicated corpora.
6 Conclusion
This paper presents a preliminary work in the ef-
fort of describing changes across revisions at a
higher level than words, motivated by a long term
goal to build educational applications to support
revision analysis for writing. Comparing to revi-
sion analysis based on words or phrases, our ap-
proach is able to capture higher level revision op-
erations. We also propose algorithms to detect re-
vision changes automatically. Experiments show
that our method has a reliable performance.
Currently we are investigating applying se-
quence merging on the automatic generated edit
sequences based on edit distances directly. Our
next plan is to develop a tool for comparing drafts,
and conduct user studies to have extrinsic evalua-
tions on whether our method would provide more
useful information to the user. We are also plan-
ning to do further analysis based on the revisions
detected, and ultimately be able to distinguish be-
tween surface changes and text-based changes.
Acknowledgments
We would like to thank W. Wang, W. Luo, H. Xue,
and the ITSPOKE group for their helpful feedback
and all the anonymous reviewers for their sugges-
tions.
This research is supported by the Institute of
Education Sciences, U.S. Department of Educa-
tion, through Grant R305A120370 to the Univer-
sity of Pittsburgh. The opinions expressed are
those of the authors and do not necessarily repre-
sent the views of the Institute or the U.S. Depart-
ment of Education.
153
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25?
32. Association for Computational Linguistics.
Stefan Bott and Horacio Saggion. 2011. An un-
supervised alignment algorithm for text simplifica-
tion corpus construction. In Proceedings of the
Workshop on Monolingual Text-To-Text Generation,
pages 20?26. Association for Computational Lin-
guistics.
Amit Bronner and Christof Monz. 2012. User edits
classification using document revision histories. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 356?366. Association for Computa-
tional Linguistics.
CompareSuite. 2014. Compare suite, feature-rich
file and folder compare tool. http://www.
comparesuite.com.
Ulla Connor and Karen Asenavage. 1994. Peer re-
sponse groups in esl writing classes: How much im-
pact on revision? Journal of Second Language Writ-
ing, 3(3):257?276.
Lester Faigley and Stephen Witte. 1981. Analyzing
revision. College composition and communication,
pages 400?414.
Homa B. Hashemi and Christian D. Schunn. 2014.
A tool for summarizing students? shanges across
drafts. In International Conference on Intelligent
Tutoring Systems(ITS).
Michael Heilman and Noah A Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1011?1019.
Association for Computational Linguistics.
Hongyan Jing. 2002. Using hidden markov modeling
to decompose human-written summaries. Computa-
tional linguistics, 28(4):527?543.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Lieve Macken. 2010. An annotation scheme and
gold standard for dutch-english word alignment.
In 7th conference on International Language Re-
sources and Evaluation (LREC 2010), pages 3369?
3374. European Language Resources Association
(ELRA).
Saul B Needleman and Christian D Wunsch. 1970.
A general method applicable to the search for simi-
larities in the amino acid sequence of two proteins.
Journal of molecular biology, 48(3):443?453.
Rani Nelken and Stuart M Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In EACL.
Alla Rozovskaya and Dan Roth. 2010. Annotating
esl errors: Challenges and rewards. In Proceedings
of the NAACL HLT 2010 fifth workshop on innova-
tive use of NLP for building educational applica-
tions, pages 28?36. Association for Computational
Linguistics.
Chao Shen, Tao Li, and Chris HQ Ding. 2011. Inte-
grating clustering and multi-document summariza-
tion by bi-mixture probabilistic latent semantic anal-
ysis (plsa) with sentence bases. In AAAI.
Ben Swanson and Elif Yamangil. 2012. Correction
detection and error type selection as an esl educa-
tional aid. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 357?361. Association for Computa-
tional Linguistics.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 297?300. Association for Computational Lin-
guistics.
Jaclyn M. Wells, Morgan Sousa, Mia Martini, and
Allen Brizee. 2013. Steps for revising your pa-
per. http://owl.english.purdue.edu/
owl/resource/561/05.
Huichao Xue and Rebecca Hwa. 2014. Improved cor-
rection detection in revised esl sentences. In Pro-
ceedings of The 52nd Annual Meeting of the Associ-
ation for Computational Linguistics(ACL).
154
