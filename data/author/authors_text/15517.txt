Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183?192,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training a Parser for Machine Translation Reordering
Jason Katz-Brown Slav Petrov Ryan McDonald Franz Och
David Talbot Hiroshi Ichikawa Masakazu Seno Hideto Kazawa
Google
{jasonkb|slav|ryanmcd|och|talbot|ichikawa|seno|kazawa}@google.com
Abstract
We propose a simple training regime that can
improve the extrinsic performance of a parser,
given only a corpus of sentences and a way
to automatically evaluate the extrinsic quality
of a candidate parse. We apply our method
to train parsers that excel when used as part
of a reordering component in a statistical ma-
chine translation system. We use a corpus of
weakly-labeled reference reorderings to guide
parser training. Our best parsers contribute
significant improvements in subjective trans-
lation quality while their intrinsic attachment
scores typically regress.
1 Introduction
The field of syntactic parsing has received a great
deal of attention and progress since the creation of
the Penn Treebank (Marcus et al, 1993; Collins,
1997; Charniak, 2000; McDonald et al, 2005;
Petrov et al, 2006; Nivre, 2008). A common?
and valid?criticism, however, is that parsers typi-
cally get evaluated only on Section 23 of the Wall
Street Journal portion of the Penn Treebank. This
is problematic for many reasons. As previously ob-
served, this test set comes from a very narrow do-
main that does not necessarily reflect parser perfor-
mance on text coming from more varied domains
(Gildea, 2001), especially web text (Foster, 2010).
There is also evidence that after so much repeated
testing, parsers are indirectly over-fitting to this set
(Petrov and Klein, 2007). Furthermore, parsing was
never meant as a stand-alone task, but is rather a
means to an end, towards the goal of building sys-
tems that can process natural language input.
This is not to say that parsers are not used in larger
systems. All to the contrary, as parsing technology
has become more mature, parsers have become ef-
ficient and accurate enough to be useful in many
natural language processing systems, most notably
in machine translation (Yamada and Knight, 2001;
Galley et al, 2004; Xu et al, 2009). While it has
been repeatedly shown that using a parser can bring
net gains on downstream application quality, it is of-
ten unclear how much intrinsic parsing accuracy ac-
tually matters.
In this paper we try to shed some light on this is-
sue by comparing different parsers in the context of
machine translation (MT). We present experiments
on translation from English to three Subject-Object-
Verb (SOV) languages,1 because those require ex-
tensive syntactic reordering to produce grammatical
translations. We evaluate parse quality on a num-
ber of extrinsic metrics, including word reordering
accuracy, BLEU score and a human evaluation of fi-
nal translation quality. We show that while there is
a good correlation between those extrinsic metrics,
parsing quality as measured on the Penn Treebank
is not a good indicator of the final downstream ap-
plication quality. Since the word reordering metric
can be computed efficiently offline (i.e. without the
use of the final MT system), we then propose to tune
parsers specifically for that metric, with the goal of
improving the performance of the overall system.
To this end we propose a simple training regime
1We experiment with Japanese, Korean and Turkish, but
there is nothing language specific in our approach.
183
which we refer to as targeted self-training (Sec-
tion 2). Similar to self-training, a baseline model
is used to produce predictions on an unlabeled data
set. However, rather than directly training on the
output of the baseline model, we generate a list of
hypotheses and use an external signal to select the
best candidate. The selected parse trees are added
to the training data and the model is then retrained.
The experiments in Section 5 show that this simple
procedure noticeably improves our parsers for the
task at hand, resulting in significant improvements
in downstream translation quality, as measured in a
human evaluation on web text.
This idea is similar in vein to McClosky. et al
(2006) and Petrov et al (2010), except that we use an
extrinsic quality metric instead of a second parsing
model for making the selection. It is also similar to
Burkett and Klein (2008) and Burkett et al (2010),
but again avoiding the added complexity introduced
by the use of additional (bilingual) models for can-
didate selection.
It should be noted that our extrinsic metric is com-
puted from data that has been manually annotated
with reference word reorderings. Details of the re-
ordering metric and the annotated data we used are
given in Sections 3 and 4. While this annotation re-
quires some effort, such annotations are much easier
to obtain than full parse trees. In our experiments
in Section 6 we show that we can obtain similar
improvements on downstream translation quality by
targeted self-training with weakly labeled data (in
form of word reorderings), as with training on the
fully labeled data (with full syntactic parse trees).
2 Targeted Self-Training
Our technique for retraining a baseline parser is an
extension of self-training. In standard parser self-
training, one uses the baseline parsing model to
parse a corpus of sentences, and then adds the 1-best
output of the baseline parser to the training data. To
target the self-training, we introduce an additional
step, given as Algorithm 1. Instead of taking the 1-
best parse, we produce a ranked n-best list of predic-
tions and select the parser which gives the best score
according to an external evaluation function. That
is, instead of relying on the intrinsic model score,
we use an extrinsic score to select the parse towards
Algorithm 1 Select parse that maximizes an extrin-
sic metric.
Input: baseline parser B
Input: sentence S
Input: function COMPUTEEXTRINSIC(parse P )
Output: a parse for the input sentence
Pn = {P1, . . . , Pn} ? n-best parses of S by B
maxScore = 0
bestParse = ?
for k = 1 to n do
extrinsicScore = COMPUTEEXTRINSIC(Pk)
if extrinsicScore > maxScore then
maxScore = extrinsicScore
bestParse = Pk
end if
end for
return bestParse
which to update. In the case of a tie, we prefer the
parse ranked most highly in the n-best list.
The motivation of this selection step is that good
performance on the downstream external task, mea-
sured by the extrinsic metric, should be predictive
of an intrinsically good parse. At the very least,
even if the selected parse is not syntactically cor-
rect, or even if it goes against the original treebank-
ing guidelines, it results in a higher extrinsic score
and should therefore be preferred.
One could imagine extending this framework by
repeatedly running self-training on successively im-
proving parsers in an EM-style algorithm. A recent
work by Hall et al (2011) on training a parser with
multiple objective functions investigates a similar
idea in the context of online learning.
In this paper we focus our attention on machine
translation as the final application, but one could en-
vision applying our techniques to other applications
such as information extraction or question answer-
ing. In particular, we explore one application of
targeted self-training, where computing the extrin-
sic metric involves plugging the parse into an MT
system?s reordering component and computing the
accuracy of the reordering compared to a reference
word order. We now direct our attention to the de-
tails of this application.
184
3 The MT Reordering Task
Determining appropriate target language word or-
der for a translation is a fundamental problem in
MT. When translating between languages with sig-
nificantly different word order such as English and
Japanese, it has been shown that metrics which ex-
plicitly account for word-order are much better cor-
related with human judgments of translation qual-
ity than those that give more weight to word choice,
like BLEU (Lavie and Denkowski, 2009; Isozaki et
al., 2010a; Birch and Osborne, 2010). This demon-
strates the importance of getting reordering right.
3.1 Reordering as a separately evaluable
component
One way to break down the problem of translat-
ing between languages with different word order
is to handle reordering and translation separately:
first reorder source-language sentences into target-
language word order in a preprocessing step, and
then translate the reordered sentences. It has been
shown that good results can be achieved by reorder-
ing each input sentence using a series of tree trans-
formations on its parse tree. The rules for tree
transformation can be manually written (Collins et
al., 2005; Wang, 2007; Xu et al, 2009) or auto-
matically learned (Xia and McCord, 2004; Habash,
2007; Genzel, 2010).
Doing reordering as a preprocessing step, sepa-
rately from translation, makes it easy to evaluate re-
ordering performance independently from the MT
system. Accordingly, Talbot et al (2011) present a
framework for evaluating the quality of reordering
separately from the lexical choice involved in trans-
lation. They propose a simple reordering metric
based on METEOR?s reordering penalty (Lavie and
Denkowski, 2009). This metric is computed solely
on the source language side. To compute it, one
takes the candidate reordering of the input sentence
and partitions it into a set C of contiguous spans
whose content appears contiguously in the same or-
der in the reference. The reordering score is then
computed as
?(esys, eref) = 1?
|C| ? 1
|e| ? 1 .
This metric assigns a score between 0 and 1 where 1
indicates that the candidate reordering is identical to
the reference and 0 indicates that no two words that
are contiguous in the candidate reordering are con-
tiguous in the reference. For example, if a reference
reordering is A B C D E, candidate reordering A
B E C D would get score 1?(3?1)/(5?1) = 0.5.
Talbot et al (2011) show that this reordering score
is strongly correlated with human judgment of trans-
lation quality. Furthermore, they propose to evalu-
ate the reordering quality of an MT system by com-
puting its reordering score on a test set consisting
of source language sentences and their reference re-
orderings. In this paper, we take the same approach
for evaluation, and in addition, we use corpora of
source language sentences and their reference re-
orderings for training the system, not just testing
it. We describe in more detail how the reference re-
ordering data was prepared in Section 4.1.
3.2 Reordering quality as predictor of parse
quality
Figure 1 gives concrete examples of good and bad
reorderings of an English sentence into Japanese
word order. It shows that a bad parse leads to a bad
reordering (lacking inversion of verb ?wear? and ob-
ject ?sunscreen?) and a low reordering score. Could
we flip this causality around, and perhaps try to iden-
tify a good parse tree based on its reordering score?
With the experiments in this paper, we show that in-
deed a high reordering score is predictive of the un-
derlying parse tree that was used to generate the re-
ordering being a good parse (or, at least, being good
enough for our purpose).
In the case of translating English to Japanese or
another SOV language, there is a large amount of
reordering required, but with a relatively small num-
ber of reordering rules one can cover a large pro-
portion of reordering phenomena. Isozaki et al
(2010b), for instance, were able to get impressive
English?Japanese results with only a single re-
ordering rule, given a suitable definition of a head.
Hence, the reordering task depends crucially on a
correct syntactic analysis and is extremely sensitive
to parser errors.
185
4 Experimental Setup
4.1 Treebank data
In our experiments the baseline training corpus is
the Wall Street Journal (WSJ) section of the Penn
Treebank (Marcus et al, 1993) using standard train-
ing/development/testing splits. We converted the
treebank to match the tokenization expected by our
MT system. In particular, we split tokens containing
hyphens into multiple tokens and, somewhat sim-
plistically, gave the original token?s part-of-speech
tag to all newly created tokens. In Section 6 we
make also use of the Question Treebank (QTB)
(Judge et al, 2006), as a source of syntactically an-
notated out-of-domain data. Though we experiment
with both dependency parsers and phrase structure
parsers, our MT system assumes dependency parses
as input. We use the Stanford converter (de Marneffe
et al, 2006) to convert phrase structure parse trees to
dependency parse trees (for both treebank trees and
predicted trees).
4.2 Reference reordering data
We aim to build an MT system that can accurately
translate typical English text that one finds on the
Internet to SOV langauges. To this end, we ran-
domly sampled 13595 English sentences from the
web and created Japanese-word-order reference re-
orderings for them. We split the sentences arbitrarily
into a 6268-sentence Web-Train corpus and a 7327-
sentence Web-Test corpus.
To make the reference alignments we used the
technique suggested by Talbot et al (2011): ask
annotators to translate each English sentence to
Japanese extremely literally and annotate which En-
glish words align to which Japanese words. Golden
reference reorderings can be made programmati-
cally from these annotations. Creating a large set
of reference reorderings is straightforward because
annotators need little special background or train-
ing, as long as they can speak both the source and
target languages. We chose Japanese as the target
language through which to create the English refer-
ence reorderings because we had access to bilingual
annotators fluent in English and Japanese.
Good parse
Reordered:
15 or greater of an SPF has that sunscreen Wear
Reordering score: 1.0 (matches reference)
Bad parse
Reordered:
15 or greater of an SPF has that Wear sunscreen
Reordering score: 0.78 (?Wear? is out of place)
Figure 1: Examples of good and bad parses and cor-
responding reorderings for translation from English to
Japanese. The good parse correctly identifies ?Wear? as
the main verb and moves it to the end of the sentence; the
bad parse analyses ?Wear sunscreen? as a noun phrase
and does not reorder it. This example was one of the
wins in the human evaluation of Section 5.2.
4.3 Parsers
The core dependency parser we use is an implemen-
tation of a transition-based dependency parser using
an arc-eager transition strategy (Nivre, 2008). The
parser is trained using the averaged perceptron algo-
rithm with an early update strategy as described in
Zhang and Clark (2008). The parser uses the fol-
lowing features: word identity of the first two words
on the buffer, the top word on the stack and the head
of the top word on the stack (if available); part-of-
speech identities of the first four words on the buffer
and top two words on the stack; dependency arc la-
bel identities for the top word on the stack, the left
and rightmost modifier of the top word on the stack,
and the leftmost modifier of the first word in the
buffer. We also include conjunctions over all non-
lexical features.
We also give results for the latent variable parser
(a.k.a. BerkeleyParser) of Petrov et al (2006). We
convert the constituency trees output by the Berke-
leyParser to labeled dependency trees using the same
procedure that is applied to the treebanks.
While the BerkeleyParser views part-of-speech
(POS) tagging as an integral part of parsing, our
dependency parser requires the input to be tagged
186
with a separate POS tagger. We use the TnT tag-
ger (Brants, 2000) in our experiments, because of
its efficiency and ease of use. Tagger and parser are
always trained on the same data.
For all parsers, we lowercase the input at train and
test time. We found that this improves performance
in parsing web text. In addition to general upper-
case/lowercase noisiness of the web text negatively
impacting scores, we found that the baseline case-
sensitive parsers are especially bad at parsing imper-
ative sentences, as discussed in Section 5.3.2.
4.4 Reordering rules
In this paper we focus on English to Japanese, Ko-
rean, and Turkish translation. We use a superset of
the reordering rules proposed by Xu et al (2009),
which flatten a dependency tree into SOV word or-
der that is suitable for all three languages. The rules
define a precedence order for the dependents of each
part of speech. For example, a slightly simplified
version of the precedence order of child labels for
a verbal head HEADVERB is: advcl, nsubj, prep,
[other children], dobj, prt, aux, neg, HEADVERB,
mark, ref, compl.
Alternatively, we could have used an automatic
reordering-rule learning framework like that of Gen-
zel (2010). Because the reordering accuracy met-
ric can be computed for any source/target language
pair, this would have made our approach language
completely independent and applicable to any lan-
guage pair. We chose to use manually written rules
to eliminate the variance induced by the automatic
reordering-rule learning framework.
4.5 MT system
We carried out all our translation experiments on a
state-of-the-art phrase-based statistical MT system.
During both training and testing, the system reorders
source-language sentences in a preprocessing step
using the above-mentioned rules. During decoding,
we used an allowed jump width of 4 words. In ad-
dition to the regular distance distortion model, we
incorporate a maximum entropy based lexicalized
phrase reordering model (Zens and Ney, 2006) as
a feature used in decoding.
Overall for decoding, we use between 20 to
30 features, whose weights are optimized using
MERT (Och, 2003). All experiments for a given lan-
guage pair use the same set of MERT weights tuned
on a system using a separate parser (that is neither
the baseline nor the experiment parser). This po-
tentially underestimates the improvements that can
be obtained, but also eliminates MERT as a pos-
sible source of improvement, allowing us to trace
back improvements in translation quality directly to
parser changes.2
For parallel training data, we use a custom collec-
tion of parallel documents. They come from vari-
ous sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. For all language pairs, we
trained on approximately 300 million source words
each.
5 Experiments Reordering Web Text
We experimented with parsers trained in three dif-
ferent ways:
1. Baseline: trained only on WSJ-Train.
2. Standard self-training: trained on WSJ-Train
and 1-best parse of the Web-Train set by base-
line parser.
3. Targeted self-training: trained on WSJ-Train
and, for each sentence in Web-Train, the parse
from the baseline parser?s 512-best list that
when reordered gives the highest reordering
score.3
5.1 Standard self-training vs targeted
self-training
Table 1 shows that targeted self-training on Web-
Train significantly improves Web-Test reordering
score more than standard self-training for both the
shift-reduce parser and for the BerkeleyParser. The
reordering score is generally divorced from the at-
tachment scores measured on the WSJ-Test tree-
bank: for the shift-reduce parser, Web-Test reorder-
ing score and WSJ-Test labeled attachment score
2We also ran MERT on all systems and the pattern of im-
provement is consistent, but sometimes the improvement is big-
ger or smaller after MERT. For instance, the BLEU delta for
Japanese is +0.0030 with MERT on both sides as opposed to
+0.0025 with no MERT.
3We saw consistent but diminishing improvements as we in-
creased the size of the n-best list.
187
Parser Web-Test reordering WSJ-Test LAS
Shift-reduce WSJ baseline 0.757 85.31%
+ self-training 1x 0.760 85.26%
+ self-training 10x 0.756 84.14%
+ targeted self-training 1x 0.770 85.19%
+ targeted self-training 10x 0.777 84.48%
Berkeley WSJ baseline 0.780 88.66%
+ self-training 1x 0.785 89.21%
+ targeted self-training 1x 0.790 89.32%
Table 1: English?Japanese reordering scores on Web-Test for standard self-training and targeted self-training on
Web-Train. Label ?10x? indicates that the self-training data was weighted 10x relative to the WSJ training data.
Bolded reordering scores are different from WSJ-only baseline with 95% confidence but are not significantly different
from each other within the same group.
English to BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.1777 0.1802 2.56 2.69 yes (at 95% level)
Korean 0.3229 0.3259 2.61 2.70 yes (at 90% level)
Turkish 0.1344 0.1370 2.10 2.20 yes (at 95% level)
Table 2: BLEU scores and human evaluation results for translation between three language pairs, varying only the
parser between systems. ?WSJ-only? corresponds to the baseline WSJ-only shift-reduce parser; ?Targeted? corre-
sponds to the Web-Train targeted self-training 10x shift-reduce parser.
(LAS) are anti-correlated, but for BerkeleyParser
they are correlated. Interestingly, weighting the self-
training data more seems to have a negative effect on
both metrics.4
One explanation for the drops in LAS is that some
parts of the parse tree are important for downstream
reordering quality while others are not (or only to
a lesser extent). Some distinctions between labels
become less important; for example, arcs labeled
?amod? and ?advmod? are transformed identically
by the reordering rules. Some semantic distinctions
also become less important; for example, any sane
interpretation of ?red hot car? would be reordered
the same, that is, not at all.
5.2 Translation quality improvement
To put the improvement of the MT system in terms
of BLEU score (Papineni et al, 2002), a widely used
metric for automatic MT evaluation, we took 5000
sentences from Web-Test and had humans gener-
ate reference translations into Japanese, Korean, and
4We did not attempt this experiment for the BerkeleyParser
since training was too slow.
Turkish. We then trained MT systems varying only
the parser used for reordering in training and decod-
ing. Table 2 shows that targeted self-training data
increases BLEU score for translation into all three
languages.
In addition to BLEU increase, a side-by-side hu-
man evaluation on 500 sentences (sampled from
the 5000 used to compute BLEU scores) showed
a statistically significant improvement for all three
languages (see again Table 2). For each sen-
tence, we asked annotators to simultaneously score
both translations from 0 to 6, with guidelines
that 6=?Perfect?, 4=?Most Meaning/Grammar?,
2=?Some Meaning/Grammar?, 0=?Nonsense?. We
computed confidence intervals for the average score
difference using bootstrap resampling; a difference
is significant if the two-sided confidence interval
does not include 0.
5.3 Analysis
As the divergence between the labeled attachment
score on the WSJ-Test data and the reordering score
on the WSJ-Test data indicates, parsing web text
188
Parser Click as N Click as V Imperative rate
case-sensitive shift-reduce WSJ-only 74 0 6.3%
case-sensitive shift-reduce + Web-Train targeted self-training 75 0 10.5%
case-insensitive shift-reduce WSJ-only 75 0 10.3%
case-insensitive shift-reduce + Web-Train targeted self-training 75 0 11.6%
Berkeley WSJ-only 35 35 11.9%
Berkeley + Web-Train targeted self-training 13 58 12.5%
(WSJ-Train) 1 0 0.7%
Table 3: Counts on Web-Test of ?click? tagged as a noun and verb and percentage of sentences parsed imperatively.
poses very different challenges compared to parsing
newswire. We show how our method improves pars-
ing performance and reordering performance on two
examples: the trendy word ?click? and imperative
sentences.
5.3.1 Click
The word ?click? appears only once in the train-
ing portion of the WSJ (as a noun), but appears many
times in our Web test data. Table 3 shows the distri-
bution of part-of-speech tags that different parsers
assign to ?click?. The WSJ-only parsers tag ?click?
as a noun far too frequently. The WSJ-only shift-
reduce parser refuses to tag ?click? as a verb even
with targeted self-training, but BerkeleyParser does
learn to tag ?click? more often as a verb.
It turns out that the shift-reduce parser?s stub-
bornness is not due to a fundamental problem of
the parser, but due to an artifact in TnT. To in-
crease speed, TnT restricts the choices of tags for
known words to previously-seen tags. This causes
the parser?s n-best lists to never hypothesize ?click?
as a verb, and self-training doesn?t click no matter
how targeted it is. This shows that the targeted self-
training approach heavily relies on the diversity of
the baseline parser?s n-best lists.
It should be noted here that it would be easy to
combine our approach with the uptraining approach
of Petrov et al (2010). The idea would be to use the
BerkeleyParser to generate the n-best lists; perhaps
we could call this targeted uptraining. This way, the
shift-reduce parser could benefit both from the gen-
erally higher quality of the parse trees produced by
the BerkeleyParser, as well as from the information
provided by the extrinsic scoring function.
5.3.2 Imperatives
As Table 3 shows, the WSJ training set contains
only 0.7% imperative sentences.5 In contrast, our
test sentences from the web contain approximately
10% imperatives. As a result, parsers trained exclu-
sively on the WSJ underproduce imperative parses,
especially a case-sensitive version of the baseline.
Targeted self-training helps the parsers to predict im-
perative parses more often.
Targeted self-training works well for generating
training data with correctly-annotated imperative
constructions because the reordering of main sub-
jects and verbs in an SOV language like Japanese
is very distinct: main subjects stay at the begin-
ning of the sentence, and main verbs are reordered
to the end of the sentence. It is thus especially easy
to know whether an imperative parse is correct or
not by looking at the reference reordering. Figure 1
gives an example: the bad (WSJ-only) parse doesn?t
catch on to the imperativeness and gets a low re-
ordering score.
6 Targeted Self-Training vs Training on
Treebanks for Domain Adaptation
If task-specific annotation is cheap, then it is rea-
sonable to consider whether we could use targeted
self-training to adapt a parser to a new domain as
a cheaper alternative to making new treebanks. For
example, if we want to build a parser that can reorder
question sentences better than our baseline WSJ-
only parser, we have these two options:
1. Manually construct PTB-style trees for 2000
5As an approximation, we count every parse that begins with
a root verb as an imperative.
189
questions and train on the resulting treebank.
2. Create reference reorderings for 2000 questions
and then do targeted self-training.
To compare these approaches, we created reference
reordering data for our train (2000 sentences) and
test (1000 sentences) splits of the Question Tree-
bank (Judge et al, 2006). Table 4 shows that both
ways of training on QTB-Train sentences give sim-
ilarly large improvements in reordering score on
QTB-Test. Table 5 confirms that this corresponds
to very large increases in English?Japanese BLEU
score and subjective translation quality. In the hu-
man side-by-side comparison, the baseline transla-
tions achieved an average score of 2.12, while the
targeted self-training translations received a score of
2.94, where a score of 2 corresponds to ?some mean-
ing/grammar? and ?4? corresponds to ?most mean-
ing/grammar?.
But which of the two approaches is better? In
the shift-reduce parser, targeted self-training gives
higher reordering scores than training on the tree-
bank, and in BerkeleyParser, the opposite is true.
Thus both approaches produce similarly good re-
sults. From a practical perspective, the advantage of
targeted self-training depends on whether the extrin-
sic metric is cheaper to calculate than treebanking.
For MT reordering, making reference reorderings is
cheap, so targeted self-training is relatively advanta-
geous.
As before, we can examine whether labeled at-
tachment score measured on the test set of the
QTB is predictive of reordering quality. Table 4
shows that targeted self-training raises LAS from
64.78?69.17%. But adding the treebank leads
to much larger increases, resulting in an LAS of
84.75%, without giving higher reordering score. We
can conclude that high LAS is not necessary to
achieve top reordering scores.
Perhaps our reordering rules are somehow defi-
cient when it comes to reordering correctly-parsed
questions, and as a result the targeted self-training
process steers the parser towards producing patho-
logical trees with little intrinsic meaning. To explore
this possibility, we computed reordering scores after
reordering the QTB-Test treebank trees directly. Ta-
ble 4 shows that this gives reordering scores similar
to those of our best parsers. Therefore it is at least
possible that the targeted self-training process could
have resulted in a parser that achieves high reorder-
ing score by producing parses that look like those in
the QuestionBank.
7 Related Work
Our approach to training parsers for reordering is
closely related to self/up-training (McClosky. et al,
2006; Petrov et al, 2010). However, unlike uptrain-
ing, our method does not use only the 1-best output
of the first-stage parser, but has access to the n-best
list. This makes it similar to the work of McClosky.
et al (2006), except that we use an extrinsic metric
(MT reordering score) to select a high quality parse
tree, rather than a second, reranking model that has
access to additional features.
Targeted self-training is also similar to the re-
training of Burkett et al (2010) in which they
jointly parse unannotated bilingual text using a mul-
tiview learning objective, then retrain the monolin-
gual parser models to include each side of the jointly
parsed bitext as monolingual training data. Our ap-
proach is different in that it doesn?t use a second
parser and bitext to guide the creation of new train-
ing data, and instead relies on n-best lists and an
extrinsic metric.
Our method can be considered an instance of
weakly or distantly supervised structured prediction
(Chang et al, 2007; Chang et al, 2010; Clarke et al,
2010; Ganchev et al, 2010). Those methods attempt
to learn structure models from related external sig-
nals or aggregate data statistics. This work differs
in two respects. First, we use the external signals
not as explicit constraints, but to compute an ora-
cle score used to re-rank a set of parses. As such,
there are no requirements that it factor by the struc-
ture of the parse tree and can in fact be any arbitrary
metric. Second, our final objective is different. In
weakly/distantly supervised learning, the objective
is to use external knowledge to build better struc-
tured predictors. In our case this would mean using
the reordering metric as a means to train better de-
pendency parsers. Our objective, on the other hand,
is to use the extrinsic metric to train parsers that are
specifically better at the reordering task, and, as a re-
sult, better suited for MT. This makes our work more
in the spirit of Liang et al (2006), who train a per-
190
Parser QTB-Test reordering QTB-Test LAS
Shift-reduce WSJ baseline 0.663 64.78%
+ treebank 1x 0.704 77.12%
+ treebank 10x 0.768 84.75%
+ targeted self-training 1x 0.746 67.84%
+ targeted self-training 10x 0.779 69.17%
Berkeley WSJ baseline 0.733 76.50%
+ treebank 1x 0.800 87.79%
+ targeted self-training 1x 0.775 80.64%
(using treebank trees directly) 0.788 100%
Table 4: Reordering and labeled attachment scores on QTB-Test for treebank training and targeted self-training on
QTB-Train.
English to QTB-Test BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.2379 0.2615 2.12 2.94 yes (at 95% level)
Table 5: BLEU scores and human evaluation results for English?Japanese translation of the QTB-Test corpus, varying
only the parser between systems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training
10x shift-reduce parser.
ceptron model for an end-to-end MT system where
the alignment parameters are updated based on se-
lecting an alignment from a n-best list that leads to
highest BLEU score. As mentioned earlier, this also
makes our work similar to Hall et al (2011) who
train a perceptron algorithm on multiple objective
functions with the goal of producing parsers that are
optimized for extrinsic metrics.
It has previously been observed that parsers of-
ten perform differently for downstream applications.
Miyao et al (2008) compared parser quality in the
biomedical domain using a protein-protein interac-
tion (PPI) identification accuracy metric. This al-
lowed them to compare the utility of extant depen-
dency parsers, phrase structure parsers, and deep
structure parsers for the PPI identification task. One
could apply the targeted self-training technique we
describe to optimize any of these parsers for the PPI
task, similar to how we have optimized our parser
for the MT reordering task.
8 Conclusion
We introduced a variant of self-training that targets
parser training towards an extrinsic evaluation met-
ric. We use this targeted self-training approach to
train parsers that improve the accuracy of the word
reordering component of a machine translation sys-
tem. This significantly improves the subjective qual-
ity of the system?s translations from English into
three SOV languages. While the new parsers give
improvements in these external evaluations, their in-
trinsic attachment scores go down overall compared
to baseline parsers trained only on treebanks. We
conclude that when using a parser as a component
of a larger external system, it can be advantageous
to incorporate an extrinsic metric into parser train-
ing and evaluation, and that targeted self-training is
an effective technique for incorporating an extrinsic
metric into parser training.
References
A. Birch and M. Osborne. 2010. LRscore for evaluating
lexical and reordering quality in MT. In ACL-2010
WMT.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP ?00.
D. Burkett and D. Klein. 2008. Two languages are better
than one (for syntactic parsing). In EMNLP ?08.
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
Learning better monolingual models with unannotated
bilingual text. In CoNLL ?10.
191
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL
?07.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In ICML ?10.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In CoNLL ?10.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In ACL
?05.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL ?97.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC ?06.
J. Foster. 2010. ?cba to check the spelling?: Investigat-
ing parser performance on discussion forum posts. In
NAACL ?10.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In HLT-NAACL ?04.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
COLING ?10.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP ?01.
N. Habash. 2007. Syntactic preprocessing for statistical
machine translation. In MTS ?07.
K. Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard.
2011. Training dependency parsers by jointly optimiz-
ing multiple objectives. In EMNLP ?11.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010a. Automatic evaluation of translation quality for
distant language pairs. In EMNLP ?10.
H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010b.
Head finalization: A simple reordering rule for SOV
languages. In ACL-2010 WMT.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
Bank: creating a corpus of parse-annotated questions.
In ACL ?06.
A. Lavie and M. Denkowski. 2009. The Meteor metric
for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL ?06.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
D. McClosky., E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL ?06.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
?05.
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and J. Tsu-
jii. 2008. Task-oriented evaluation of syntactic parsers
and their representations. In ACL ?08.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL ?02.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL ?07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
S. Petrov, P. Chang, and M. Ringgaard H. Alshawi. 2010.
Uptraining for accurate deterministic question parsing.
In EMNLP ?10.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalua-
tion framework for machine translation reordering. In
EMNLP-2011 WMT.
C. Wang. 2007. Chinese syntactic reordering for statisti-
cal machine translation. In EMNLP ?07.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. In
Coling ?04.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In NAACL-HLT ?09.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL ?01.
R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In NAACL-
06 WMT.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In EMNLP ?08.
192
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1489?1499,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training dependency parsers by jointly optimizing multiple objectives
Keith Hall Ryan McDonald Jason Katz-Brown Michael Ringgaard
Google Research
{kbhall|ryanmcd|jasonkb|ringgaard}@google.com
Abstract
We present an online learning algorithm for
training parsers which allows for the inclusion
of multiple objective functions. The primary
example is the extension of a standard su-
pervised parsing objective function with addi-
tional loss-functions, either based on intrinsic
parsing quality or task-specific extrinsic mea-
sures of quality. Our empirical results show
how this approach performs for two depen-
dency parsing algorithms (graph-based and
transition-based parsing) and how it achieves
increased performance on multiple target tasks
including reordering for machine translation
and parser adaptation.
1 Introduction
The accuracy and speed of state-of-the-art depen-
dency parsers has motivated a resumed interest in
utilizing the output of parsing as an input to many
downstream natural language processing tasks. This
includes work on question answering (Wang et al,
2007), sentiment analysis (Nakagawa et al, 2010),
MT reordering (Xu et al, 2009), and many other
tasks. In most cases, the accuracy of parsers de-
grades when run on out-of-domain data (Gildea,
2001; McClosky et al, 2006; Blitzer et al, 2006;
Petrov et al, 2010). But these accuracies are mea-
sured with respect to gold-standard out-of-domain
parse trees. There are few tasks that actually depend
on the complete parse tree. Furthermore, when eval-
uated on a downstream task, often the optimal parse
output has a model score lower than the best parse
as predicted by the parsing model. While this means
that we are not properly modeling the downstream
task in the parsers, it also means that there is some
information from small task or domain-specific data
sets which could help direct our search for optimal
parameters during parser training. The goal being
not necessarily to obtain better parse performance,
but to exploit the structure induced from human la-
beled treebank data while targeting specific extrinsic
metrics of quality, which can include task specific
metrics or external weak constraints on the parse
structure.
One obvious approach to this problem is to em-
ploy parser reranking (Collins, 2000). In such a
setting, an auxiliary reranker is added in a pipeline
following the parser. The standard setting involves
training the base parser and applying it to a devel-
opment set (this is often done in a cross-validated
jack-knife training framework). The reranker can
then be trained to optimize for the downstream or
extrinsic objective. While this will bias the reranker
towards the target task, it is limited by the oracle
performance of the original base parser.
In this paper, we propose a training algorithm for
statistical dependency parsers (Ku?bler et al, 2009)
in which a single model is jointly optimized for a
regular supervised training objective over the tree-
bank data as well as a task-specific objective ? or
more generally an extrinsic objective ? on an ad-
ditional data set. The case where there are both
gold-standard trees and a task-specific objective for
the entire training set is a specific instance of the
larger problem that we address here. Specifically,
the algorithm takes the form of an online learner
where a training instance is selected and the param-
1489
eters are optimized based on the objective function
associated with the instance (either intrinsic or ex-
trinsic), thus jointly optimizing multiple objectives.
An update schedule trades-off the relative impor-
tance of each objective function. We call our algo-
rithm augmented-loss training as it optimizes mul-
tiple losses to augment the traditional supervised
parser loss.
There have been a number of efforts to exploit
weak or external signals of quality to train better pre-
diction models. This includes work on generalized
expectation (Mann and McCallum, 2010), posterior
regularization (Ganchev et al, 2010) and constraint
driven learning (Chang et al, 2007; Chang et al,
2010). The work of Chang et al (2007) on constraint
driven learning is perhaps the closest to our frame-
work and we draw connections to it in Section 5.
In these studies the typical goal is to use the weak
signal to improve the structured prediction models
on the intrinsic evaluation metrics. For our setting
this would mean using weak application specific sig-
nals to improve dependency parsing. Though we
explore such ideas in our experiments, in particular
for semi-supervised domain adaptation, we are pri-
marily interested in the case where the weak signal
is precisely what we wish to optimize, but also de-
sire the benefit from using both data with annotated
parse structures and data specific to the task at hand
to guide parser training.
In Section 2 we outline the augmented-loss algo-
rithm and provide a convergence analysis. In Sec-
tion 3 and 4 we present a set of experiments defin-
ing diffent augmented losses covering a task-specific
extrinsic loss (MT reordering), a domain adapta-
tion loss, and an alternate intrinsic parser loss. In
all cases we show the augmented-loss framework
can lead to significant gains in performance. In
Section 5 we tie our augmented-loss algorithm to
other frameworks for encoding auxiliary informa-
tion and/or joint objective optimization.
2 Methodology
We present the augmented-loss algorithm in the con-
text of the structured perceptron. The structured
perceptron (Algorithm 1) is an on-line learning al-
gorithm which takes as input: 1) a set of training
examples di = (xi, yi) consisting of an input sen-
Algorithm 1 Structured Perceptron
{Input data sets: D = {d1 = (x1, y1) . . . dN = (xN , yN )}}
{Input 0/1 loss: L(F?(x), y) = [F?(x) 6= y ? 1 : 0]}
{Let: F?(x) = arg maxy?Y ? ? ?(y)}
{Initialize model parameters: ? = ~0}
repeat
for i = 1 . . . N do
{Compute structured loss}
y?i = F?(xi)
if L(y?i, yi) > 0 then
{Update model Parameters}
? = ? + ?(yi)? ?(y?i)
end if
end for
until converged
{Return model ?}
tence xi and an output yi; and 2) a loss-function,
L(y?, y), that measures the cost of predicting out-
put y? relative to the gold standard y and is usu-
ally the 0/1 loss (Collins, 2002). For dependency
parser training, this set-up consists of input sen-
tences x and the corresponding gold dependency
tree y ? Yx, where Yx is the space of possible
parse trees for sentence x. In the perceptron setting,
F?(x) = arg maxy?Yx ? ??(y) where ? is mappingfrom a parse tree y for sentence x to a high dimen-
sional feature space. Learning proceeds by predict-
ing a structured output given the current model, and
if that structure is incorrect, updating the model: re-
warding features that fire in the gold-standard ?(yi),
and discounting features that fire in the predicted
output, ?(y?i).
The structured perceptron, as given in Algo-
rithm 1, only updates when there is a positive loss,
meaning that there was a prediction mistake. For
the moment we will abstract away from details such
as the precise definition of F (x) and ?(y). We
will show in the next section that our augmented-
loss method is general and can be applied to any de-
pendency parsing framework that can be trained by
the perceptron algorithm, such as transition-based
parsers (Nivre, 2008; Zhang and Clark, 2008) and
graph-based parsers (McDonald et al, 2005).
2.1 Augmented-Loss Training
The augmented-loss training algorithm that we pro-
pose is based on the structured perceptron; however,
the augmented-loss training framework is a general
1490
mechanism to incorporate multiple loss functions in
online learner training. Algorithm 2 is the pseudo-
code for the augmented-loss structured perceptron
algorithm. The algorithm is an extension to Algo-
rithm 1 where there are 1) multiple loss functions
being evaluated L1, . . . , LM ; 2) there are multiple
datasets associated with each of these loss functions
D1, . . . ,DM ; and 3) there is a schedule for pro-
cessing examples from each of these datasets, where
Sched(j, i) is true if the jth loss function should be
updated on the ith iteration of training. Note that
for data point dji = (x, y), which is the ith training
instance of the jth data set, that y does not neces-
sarily have to be a dependency tree. It can either
be a task-specific output of interest, a partial tree, or
even null, in the case where learning will be guided
strictly by the loss Lj . The training algorithm is ef-
fectively the same as the perceptron, the primary dif-
ference is that if Lj is an extrinsic loss, we cannot
compute the standard updates since we do not nec-
essarily know the correct parse (the line indicated by
?). Section 2.2 shows one method for updating the
parser parameters for extrinsic losses.
In the experiments in this paper, we only consider
the case where there are two loss functions: a super-
vised dependency parsing labeled-attachment loss;
and an additional loss, examples of which are pre-
sented in Section 3.
2.2 Inline Ranker Training
In order to make Algorithm 2 more concrete, we
need a way of defining the loss and resulting pa-
rameter updates for the case when Lj is not a stan-
dard supervised parsing loss (? from Algorithm 2).
Assume that we have a cost function C(xi, y?, yi)
which, given a training example (xi, yi) will give a
score for a parse y? ? Yxi relative to some output
yi. While we can compute the score for any parse,
we are unable to determine the features associated
with the optimal parse, as yi need not be a parse
tree. For example, consider a machine translation re-
ordering system which uses the parse y? to reorder the
words of xi, the optimal reordering being yi. Then
C(xi, y?, yi) is a reordering cost which is large if the
predicted parse induces a poor reordering of xi.
We propose a general purpose loss function which
is based on parser k-best lists. The inline reranker
uses the currently trained parser model ? to parse
Algorithm 2 Augmented-Loss Perceptron
{Input data sets}:
D1 = {d11 = (x11, y11) . . . d1N1 = (x1N1 , y1N1)},
. . .
DM = {dM1 = (xM1 , yM1 ) . . . dMNM = (xMNM , yMNM )}
{Input loss functions: L1 . . . LM}
{Initialize indexes: c1 . . . cM = ~0}
{Initialize model parameters: ? = ~0}
i = 0
repeat
for j = 1 . . .M do
{Check whether to update Lj on iteration i}
if Sched(j, i) then
{Compute index of instance ? reset if cj ? N j}
cj = [(cj ? N j) ? 0 : cj + 1]
{Compute structured loss for instance}
if Lj is intrinsic loss then
y? = F?(xjcj )
if Lj(y?, yjcj ) > 0 then
? = ? + ?(yjcj )? ?(y?) {yjcj is a tree}end if
else if Lj is an extrinsic loss then
{See Section 2.2}?
end if
end if
end for
i = i+ 1
until converged
{Return model ?}
the external input, producing a k-best set of parses:
Fk-best? (xi) = {y?1, . . . , y?k}. We can compute the
cost function C(xi, y?, yi) for all y? ? Fk-best? (xi). If
the 1-best parse, y?1, has the lowest cost, then there is
no lower cost parse in this k-best list. Otherwise, the
lowest-cost parse in Fk-best? (xi) is taken to be the
correct output structure yi, and the 1-best parse is
taken to be an incorrect prediction. We can achieve
this by substituting the following into Algorithm 2
at line ?.
Algorithm 3 Reranker Loss
{y?1, . . . , y?k} = Fk-best? (xi)
? = min? C(xjcj , y?? , yjcj ) {? is min const index}
Lj(y?1, yjcj ) = C(xjcj , y?1, yjcj )? C(xjcj , y?? , yjcj )
if Lj(y?1, yjcj ) > 0 then
? = ? + ?(y?? )? ?(y?1)
end if
Again the algorithm only updates when there is
an error ? when the 1-best output has a higher cost
than any other output in the k-best list ? resulting
1491
in positive Lj . The intuition behind this method is
that in the presence of only a cost function and a
k-best list, the parameters will be updated towards
the parse structure that has the lowest cost, which
over time will move the parameters of the model to
a place with low extrinsic loss.
We exploit this formulation of the general-
purpose augmented-loss function as it allows one to
include any extrinsic cost function which is depen-
dent of parses. The scoring function used does not
need to be factored, requiring no internal knowledge
of the function itself. Furthermore, we can apply this
to any parsing algorithm which can generate k-best
lists. For each parse, we must retain the features
associated with the parse (e.g., for transition-based
parsing, the features associated with the transition
sequence resulting in the parse).
There are two significant differences from the in-
line reranker loss function and standard reranker
training. First, we are performing this decision per
example as each data item is processed (this is done
in the inner loop of the Algorithm 2). Second, the
feedback function for selecting a parse is based on
an external objective function. The second point is
actually true for many minimum-error-rate training
scenarios, but in those settings the model is updated
as a post-processing stage (after the base-model is
trained).
2.3 Convergence of Inline Ranker Training
A training setD is loss-separable with margin ? > 0
if there exists a vector u with ?u? = 1 such that
for all y?, y?? ? Yx and (x, y) ? D, if L(y?, y) <
L(y??, y), then u??(y?)?u??(y??) ? ?. Furthermore,
let R ? ||?(y)? ?(y?)||, for all y, y?.
Assumption 1. Assume training set D is loss-
separable with margin ?.
Theorem 1. Given Assumption 1. Letm be the num-
ber of mistakes made when training the perceptron
(Algorithm 2) with inline ranker loss (Algorithm 3)
on D, where a mistake occurs for (x, y) ? D with
parameter vector ? when ?y?j ? F k-best? (x) where
y?j 6= y?1 and L(y?j , y) < L(y?1, y). If training is run
indefinitely, then m ? R2?2 .
Proof. Identical to the standard perceptron proof,
e.g., Collins (2002), by inserting in loss-separability
for normal separability.
Like the original perceptron theorem, this implies
that the algorithm will converge. However, unlike
the original theorem, it does not imply that it will
converge to a parameter vector ? such that for all
(x, y) ? D, if y? = arg maxy? ? ??(y?) then L(y?, y) =
0. Even if we assume for every x there exists an out-
put with zero loss, Theorem 1 still makes no guar-
antees. Consider a training set with one instance
(x, y). Now, set k = 2 for the k-best output list and
let y?1, y?2, and y?3 be the top-3 scoring outputs and
let L(y?1, y) = 1, L(y?2, y) = 2 and L(y?3, y) = 0.
In this case, no updates will ever be made and y?1
will remain unchanged even though it doesn?t have
minimal loss. Consider the following assumption:
Assumption 2. For any parameter vector ? that ex-
ists during training, either 1) for all (x, y) ? D,
L(y?1, y) = 0 (or some optimal minimum loss),
or 2) there exists at least one (x, y) ? D where
?y?j ? F k-best? (x) such that L(y?j , y) < L(y?1, y).
Assumption 2 states that for any ? that exists
during training, but before convergence, there is at
least one example in the training data where k is
large enough to include one output with a lower loss
when y?1 does not have the optimal minimal loss. If
k = ?, then this is the standard perceptron as it
guarantees the optimal loss output to be in the k-best
list. But we are assuming something much weaker
here, i.e., not that the k-best list will include the min-
imal loss output, only a single output with a lower
loss than the current best guess. However, it is strong
enough to show the following:
Theorem 2. Given Assumption 1 and Assumption 2.
Training the perceptron (Algorithm 2) with inline
ranker loss (Algorithm 3) on D 1) converges in fi-
nite time, and 2) produces parameters ? such that
for all (x, y) ? D, if y? = arg maxy? ? ? ?(y?) then
L(y?, y) = 0 (or equivalent minimal loss).
Proof. It must be the case for all (x, y) ? D that
L(y?1, y) = 0 (and y?1 is the argmax) after a finite
amount of time. Otherwise, by Assumption 2, there
exists some x, such that when it is next processed,
there would exist an output in the k-best list that
had a lower loss, which will result in an additional
mistake. Theorem 1 guarantees that this can not
continue indefinitely as the number of mistakes is
bounded.
1492
Thus, the perceptron algorithm will converge to
optimal minimal loss under the assumption that k
is large enough so that the model can keep improv-
ing. Note that this does not mean k must be large
enough to include a zero or minimum loss output,
just large enough to include a better output than
the current best hypothesis. Theorem 2, when cou-
pled with Theorem 1, implies that augmented-loss
learning will make at most R2/?2 mistakes at train-
ing, but does not guarantee the rate at which these
mistakes will be made, only that convergence is fi-
nite, providing that the scheduling time (defined by
Sched()) between seeing the same instance is always
finite, which is always true in our experiments.
This analysis does not assume anything about the
loss L. Every instance (x, y) can use a different loss.
It is only required that the loss for a specific input-
output pair is fixed throughout training. Thus, the
above analysis covers the case where some training
instances use an extrinsic loss and others an intrin-
sic parsing loss. This also suggests more efficient
training methods when extracting the k-best list is
prohibitive. One can parse with k = 2, 4, 8, 16, . . .
until an k is reached that includes a lower loss parse.
It may be the case that for most instances a small
k is required, but the algorithm is doing more work
unnecessarily if k is large.
3 Experimental Set-up
3.1 Dependency Parsers
The augmented-loss framework we present is gen-
eral in the sense that it can be combined with any
loss function and any parser, provided the parser can
be parameterized as a linear classifier, trained with
the perceptron and is capable of producing a k-best
list of trees. For our experiments we focus on two
dependency parsers.
? Transition-based: An implementation of the
transition-based dependency parsing frame-
work (Nivre, 2008) using an arc-eager transi-
tion strategy and are trained using the percep-
tron algorithm as in Zhang and Clark (2008)
with a beam size of 8. Beams with varying
sizes can be used to produce k-best lists. The
features used by all models are: the part-of-
speech tags of the first four words on the buffer
and of the top two words on the stack; the word
identities of the first two words on the buffer
and of the top word on the stack; the word iden-
tity of the syntactic head of the top word on the
stack (if available); dependency arc label iden-
tities for the top word on the stack, the left and
rightmost modifier of the top word on the stack,
and the left most modifier of the first word in
the buffer (if available). All feature conjunc-
tions are included.
? Graph-based: An implementation of graph-
based parsing algorithms with an arc-factored
parameterization (McDonald et al, 2005). We
use the non-projective k-best MST algorithm to
generate k-best lists (Hall, 2007), where k = 8
for the experiments in this paper. The graph-
based parser features used in the experiments
in this paper are defined over a word, wi at po-
sition i; the head of this word w?(i) where ?(i)
provides the index of the head word; and part-
of-speech tags of these words ti. We use the
following set of features similar to McDonald
et al (2005):
isolated features: wi, ti, w?(i), t?(i)
word-tag pairs: (wi, ti); (w?(i), t?(i))
word-head pairs: (wi, w?(i)), (ti, t?(i))
word-head-tag triples: (t?(i), wi, ti)
(w?(i), wi, ti)
(w?(i), t?(i), ti)
(w?(i), t?(i), wi)
tag-neighbourhood: (t?(i), t?(i)+1, ti?1, ti)
(t?(i), t?(i)+1, ti+1, ti)
(t?(i), t?(i)?1, ti?1, ti)
(t?(i), t?(i)?1, ti+1, ti)
between features: ?j i < j < ?(i) || ?(i) < j < i
(t?(i), tj , ti)
arc-direction/length : (i? ?(i) > 0, |i? ?(i)|)
3.2 Data and Tasks
In the next section, we present a set of scoring func-
tions that can be used in the inline reranker loss
framework, resulting in a new augmented-loss for
each one. Augmented-loss learning is then applied
to target a downstream task using the loss functions
to measure gains. We show empirical results for two
extrinsic loss-functions (optimizing for the down-
stream task): machine translation and domain adap-
tation; and for one intrinsic loss-function: an arc-
length parsing score. For some experiments we also
1493
measure the standard intrinsic parser metrics unla-
beled attachment score (UAS) and labeled attach-
ment score (LAS) (Buchholz and Marsi, 2006).
In terms of treebank data, the primary training
corpus is the Penn Wall Street Journal Treebank
(PTB) (Marcus et al, 1993). We also make use
of the Brown corpus, and the Question Treebank
(QTB) (Judge et al, 2006). For PTB and Brown
we use standard training/development/testing splits
of the data. For the QTB we split the data into
three sections: 2000 training, 1000 development,
and 1000 test. All treebanks are converted to de-
pendency format using the Stanford converter v1.6
(de Marneffe et al, 2006).
4 Experiments
4.1 Machine Translation Reordering Score
As alluded to in Section 2.2, we use a reordering-
based loss function to improve word order in a ma-
chine translation system. In particular, we use a sys-
tem of source-side reordering rules which, given a
parse of the source sentence, will reorder the sen-
tence into a target-side order (Collins et al, 2005).
In our experiments we work with a set of English-
Japanese reordering rules1 and gold reorderings
based on human generated correct reordering of an
aligned target sentences. We use a reordering score
based on the reordering penalty from the METEOR
scoring metric. Though we could have used a fur-
ther downstream measure like BLEU, METEOR has
also been shown to directly correlate with translation
quality (Banerjee and Lavie, 2005) and is simpler to
measure.
reorder-score = 1? # chunks? 1# unigrams matched? 1
reorder-cost = 1? reorder-score
All reordering augmented-loss experiments are
run with the same treebank data as the baseline
(the training portions of PTB, Brown, and QTB).
The extrinsic reordering training data consists of
10930 examples of English sentences and their cor-
rect Japanese word-order. We evaluate our results on
an evaluation set of 6338 examples of similarly cre-
ated reordering data. The reordering cost, evaluation
1Our rules are similar to those from Xu et al (2009).
Exact Reorder
trans?PTB + Brown + QTB 35.29 76.49
trans?0.5?aug.-loss 38.71 78.19
trans?1.0?aug.-loss 39.02 78.39
trans?2.0?aug.-loss 39.58 78.67
graph?PTB + Brown + QTB 25.71 69.84
graph?0.5? aug.-loss 28.99 72.23
graph?1.0?aug.-loss 29.99 72.88
graph?2.0?aug.-loss 30.03 73.15
Table 1: Reordering scores for parser-based reordering
(English-to-Japanese). Exact is the number of correctly
reordered sentences. All models use the same treebank-
data (PTB, QTB, and the Brown corpus). Results for
three augmented-loss schedules are shown: 0.5 where for
every two treebank updates we make one augmented-loss
update, 1 is a 1-to-1 mix, and 2 is where we make twice
as many augmented-loss updates as treebank updates.
criteria and data used in our experiments are based
on the work of Talbot et al (2011).
Table 1 shows the results of using the reordering
cost as an augmented-loss to the standard treebank
objective function. Results are presented as mea-
sured by the reordering score as well as a coarse
exact-match score (the number of sentences which
would have correct word-order given the parse and
the fixed reordering rules). We see continued im-
provements as we adjust the schedule to process the
extrinsic loss more frequently, the best result being
when we make two augmented-loss updates for ev-
ery one treebank-based loss update.
4.2 Semi-supervised domain adaptation
Another application of the augmented-loss frame-
work is to improve parser domain portability in the
presence of partially labeled data. Consider, for ex-
ample, the case of questions. Petrov et al (2010)
observed that dependency parsers tend to do quite
poorly when parsing questions due to their lim-
ited exposure to them in the news corpora from
the PennTreebank. Table 2 shows the accuracy
of two parsers (LAS, UAS and the F1 of the root
dependency attachment) on the QuestionBank test
data. The first is a parser trained on the standard
training sections of the PennTreebank (PTB) and
the second is a parser trained on the training por-
tion of the QuestionBank (QTB). Results for both
1494
LAS UAS Root-F1
trans?PTB 67.97 73.52 47.60
trans?QTB 84.59 89.59 91.06
trans?aug.-loss 76.27 86.42 83.41
graph?PTB 65.27 72.72 43.10
graph?QTB 82.73 87.44 91.58
graph?aug.-loss 72.82 80.68 86.26
Table 2: Domain adaptation results. Table shows (for
both transition and graph-based parsers) the labeled ac-
curacy score (LAS), unlabeled accuracy score (UAS)
and Root-F1 for parsers trained on the PTB and QTB
and tested on the QTB. The augmented-loss parsers are
trained on the PTB but with a partial tree loss on QTB
that considers only root dependencies.
transition-based parsers and graph-based parsers are
given. Clearly there is significant drop in accu-
racy for a parser trained on the PTB. For example,
the transition-based PTB parser achieves a LAS of
67.97% relative to 84.59% for the parser trained on
the QTB.
We consider the situation where it is possible to
ask annotators a single question about the target do-
main that is relatively easy to answer. The question
should be posed so that the resulting answer pro-
duces a partially labeled dependency tree. Root-F1
scores from Table 2 suggest that one simple ques-
tion is ?what is the main verb of this sentence?? for
sentences that are questions. In most cases this task
is straight-forward and will result in a single depen-
dency, that from the root to the main verb of the sen-
tence. We feel this is a realistic partial labeled train-
ing setting where it would be possible to quickly col-
lect a significant amount of data.
To test whether such weak information can signif-
icantly improve the parsing of questions, we trained
an augmented-loss parser using the training set of
the QTB stripped of all dependencies except the de-
pendency from the root to the main verb of the sen-
tence. In other words, for each sentence, the parser
may only observe a single dependency at training
from the QTB ? the dependency to the main verb.
Our augmented-loss function in this case is a simple
binary function: 0 if a parse has the correct root de-
pendency and 1 if it does not. Thus, the algorithm
will select the first parse in the k-best list that has the
correct root as the proxy to a gold standard parse.2
The last row in each section of Table 2 shows the
results for this augmented-loss system when weight-
ing both losses equally during training. By simply
having the main verb annotated in each sentence ?
the sentences from the training portion of the QTB
? the parser can eliminate half of the errors of the
original parser. This is reflected by both the Root-
F1 as well as LAS/UAS. It is important to point out
that these improvements are not limited to simply
better root predictions. Due to the fact that parsing
algorithms make many parsing decisions jointly at
test time, all such decisions influence each other and
improvements are seen across the board. For exam-
ple, the transition-based PTB parser has an F1 score
of 41.22% for verb subjects (nsubj), whereas the
augmented-loss parser has an F1 of 73.52%. Clearly
improving just a single (and simple to annotate) de-
pendency leads to general parser improvements.
4.3 Average Arc Length Score
The augmented-loss framework can be used to in-
corporate multiple treebank-based loss functions as
well. Labeled attachment score is used as our base
model loss function. In this set of experiments we
consider adding an additional loss function which
weights the lengths of correct and incorrect arcs, the
average (labeled) arc-length score:
ALS =
?
i ?(??i, ?i)(i? ?i)?
i(i? ?i)
For each word of the sentence we compute the dis-
tance between the word?s position i and the posi-
tion of the words head ?i. The arc-length score is
the summed length of all those with correct head as-
signments (?(??i, ?i) is 1 if the predicted head and
the correct head match, 0 otherwise). The score is
normalized by the summed arc lengths for the sen-
tence. The labeled version of this score requires that
the labels of the arc are also correct. Optimizing
for dependency arc length is particularly important
as parsers tend to do worse on longer dependencies
(McDonald and Nivre, 2007) and these dependen-
cies are typically the most meaningful for down-
stream tasks, e.g., main verb dependencies for tasks
2For the graph-based parser one can also find the higest scor-
ing tree with correct root by setting the score of all competing
arcs to ??.
1495
LAS UAS ALS
trans?PTB 88.64 91.64 82.96
trans?unlabeled aug.-loss 88.74 91.91 83.65
trans?labeled aug.-loss 88.84 91.91 83.46
graph?PTB 85.75 88.70 73.88
graph?unlabeled aug.-loss 85.80 88.81 74.26
graph?labeled aug.-loss 85.85 88.93 74.40
Table 3: Results for both parsers on the development set
of the PTB. When training with ALS (labeled and unla-
beled), we see an improvement in UAS, LAS, and ALS.
Furthermore, if we use a labeled-ALS as the metric for
augmented-loss training, we also see a considerable in-
crease in LAS.
like information extraction (Yates and Etzioni, 2009)
and textual entailment (Berant et al, 2010).
In Table 3 we show results for parsing with the
ALS augmented-loss objective. For each parser, we
consider two different ALS objective functions; one
based on unlabeled-ALS and the other on labeled-
ALS. The arc-length score penalizes incorrect long-
distance dependencies more than local dependen-
cies; long-distance dependencies are often more de-
structive in preserving sentence meaning and can be
more difficult to predict correctly due to the larger
context on which they depend. Combining this with
the standard attachment scores biases training to fo-
cus on the difficult head dependencies.
For both experiments we see that by adding the
ALS augmented-loss we achieve an improvement in
LAS and UAS in addition to ALS. The augmented-
loss not only helps us improve on the longer depen-
dencies (as reflected in the increased ALS), but also
in the main parser objective function of LAS and
UAS. Using the labeled loss function provides better
reinforcement as can be seen in the improvements
over the unlabeled loss-function. As with all experi-
ments in this paper, the graph-based parser baselines
are much lower than the transition-based parser due
to the use of arc-factored features. In these experi-
ments we used an inline-ranker loss with 8 parses.
We experimented with larger sizes (16 and 64) and
found very similar improvements: for example, the
transition parser?s LAS for the labeled loss is 88.68
and 88.84, respectively).
We note that ALS can be decomposed locally and
could be used as the primary objective function for
parsing. A parse with perfect scores under ALS
and LAS will match the gold-standard training tree.
However, if we were to order incorrect parses of a
sentence, ALS and LAS will suggest different order-
ings. Our results show that by optimizing for losses
based on a combination of these metrics we train a
more robust parsing model.
5 Related Work
A recent study by Katz-Brown et al (2011) also in-
vestigates the task of training parsers to improve MT
reordering. In that work, a parser is used to first
parse a set of manually reordered sentences to pro-
duce k-best lists. The parse with the best reordering
score is then fixed and added back to the training set
and a new parser is trained on resulting data. The
method is called targeted self-training as it is simi-
lar in vein to self-training (McClosky et al, 2006),
with the exception that the new parse data is targeted
to produce accurate word reorderings. Our method
differs as it does not statically fix a new parse, but
dynamically updates the parameters and parse selec-
tion by incorporating the additional loss in the inner
loop of online learning. This allows us to give guar-
antees of convergence. Furthermore, we also evalu-
ate the method on alternate extrinsic loss functions.
Liang et al (2006) presented a perceptron-based
algorithm for learning the phrase-translation param-
eters in a statistical machine translation system.
Similar to the inline-ranker loss function presented
here, they use a k-best lists of hypotheses in order to
identify parameters which can improve a global ob-
jective function: BLEU score. In their work, they
are interested in learning a parameterization over
translation phrases (including the underlying word-
alignment) which optimizes the BLEU score. Their
goal is considerably different; they want to incor-
porate additional features into their model and de-
fine an objective function which allows them to do
so; whereas, we are interested in allowing for mul-
tiple objective functions in order to adapt the parser
model parameters to downstream tasks or alternative
intrinsic (parsing) objectives.
The work that is most similar to ours is that
of Chang et al (2007), who introduced the Con-
straint Driven Learning algorithm (CODL). Their al-
gorithm specifically optimizes a loss function with
1496
the addition of constraints based on unlabeled data
(what we call extrinsic datasets). For each unla-
beled example, they use the current model along
with their set of constraints to select a set of k au-
tomatically labeled examples which best meet the
constraints. These induced examples are then added
to their training set and, after processing each unla-
beled dataset, they perform full model optimization
with the concatenation of training data and newly
generated training items. The augmented-loss al-
gorithm can be viewed as an online version of this
algorithm which performs model updates based on
the augmented-loss functions directly (rather than
adding a set of examples to the training set). Un-
like the CODL approach, we do not perform com-
plete optimization on each iteration over the unla-
beled dataset; rather, we incorporate the updates in
our online learning algorithm. As mentioned earlier,
CODL is one example of learning algorithms that
use weak supervision, others include Mann and Mc-
Callum (2010) and Ganchev et al (2010). Again,
these works are typically interested in using the ex-
trinsic metric ? or, in general, extrinsic information
? to optimize the intrinsic metric in the absence of
any labeled intrinsic data. Our goal is to optimize
both simultaneously.
The idea of jointly training parsers to optimize
multiple objectives is related to joint learning and in-
ference for tasks like information extraction (Finkel
and Manning, 2009) and machine translation (Bur-
kett et al, 2010). In such works, a large search space
that covers both the space of parse structures and
the space of task-specific structures is defined and
parameterized so that standard learning and infer-
ence algorithms can be applied. What sets our work
apart is that there is still just a single parameter set
that is being optimized ? the parser parameters. Our
method only uses feedback from task specific objec-
tives in order to update the parser parameters, guid-
ing it towards better downstream performance. This
is advantageous for two reasons. First, it decouples
the tasks, making inference and learning more effi-
cient. Second, it does not force arbitrary paraemter
factorizations in order to define a joint search space
that can be searched efficiently.
Finally, augmented-loss training can be viewed
as multi-task learning (Caruana, 1997) as the model
optimizes multiple objectives over multiple data sets
with a shared underlying parameter space.
6 Discussion
The empirical results show that incorporating an
augmented-loss using the inline-ranker loss frame-
work achieves better performance under metrics as-
sociated with the external loss function. For the in-
trinsic loss, we see that the augmented-loss frame-
work can also result in an improvement in parsing
performance; however, in the case of ALS, this is
due to the fact that the loss function is very closely
related to the standard evaluation metrics of UAS
and LAS.
Although our analysis suggests that this algorithm
is guaranteed to converge only for the separable
case, it makes a further assumption that if there is
a better parse under the augmented-loss, then there
must be a lower cost parse in the k-best list. The em-
pirical evaluation presented here is based on a very
conservative approximation by choosing lists with
at most 8 parses. However, in our experiments, we
found that increasing the size of the lists did not sig-
nificantly increase our accuracy under the external
metrics. If we do have at least one improvement
in our k-best lists, the analysis suggests that this is
enough to move in the correct direction for updating
the model. The assumption that there will always
be an improvement in the k-best list if there is some
better parse breaks down as training continues. We
suspect that an increasing k, as suggested in Sec-
tion 2.3, will allow for continued improvements.
Dependency parsing, as presented in this pa-
per, is performed over (k-best) part-of-speech tags
and is therefore dependent on the quality of the
tagger. The experiments presented in this paper
made use of a tagger trained on the source treebank
data which severely limits the variation in parses.
The augmented-loss perceptron algorithm presented
here can be applied to any online learning prob-
lem, including part-of-speech tagger training. To
build a dependency parser which is better adapted
to a downstream task, one would want to perform
augmented-loss training on the tagger as well.
7 Conclusion
We introduced the augmented-loss training algo-
rithm and show that the algorithm can incorporate
1497
additional loss functions to adapt the model towards
extrinsic evaluation metrics. Analytical results are
presented that show that the algorithm can opti-
mize multiple objective functions simultaneously.
We present an empirical analysis for training depen-
dency parsers for multiple parsing algorithms and
multiple loss functions.
The augmented-loss framework supports both in-
trinsic and extrinsic losses, allowing for both com-
binations of objectives as well as multiple sources
of data for which the results of a parser can be eval-
uated. This flexibility makes it possible to tune a
model for a downstream task. The only requirement
is a metric which can be defined over parses of the
downstream data. Our dependency parsing results
show that we are not limited to increasing parser
performance via more data or external domain adap-
tation techniques, but that we can incorporate the
downstream task into parser training.
Acknowledgements: We would like to thank Kuz-
man Ganchev for feedback on an earlier draft of this
paper as well as Slav Petrov for frequent discussions
on this topic.
References
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion.
J. Berant, I. Dagan, and J. Goldberger. 2010. Global
learning of focused entailment graphs. In Proc. of
ACL.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of EMNLP.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
D. Burkett, J. Blitzer, and D. Klein. 2010. Joint parsing
and alignment with weakly synchronized grammars.
In Proc. of NAACL.
R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41?75.
M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
Proc. of ACL.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In Proc. of ICML.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of ACL.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. of ICML.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.
M.C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC, Genoa,
Italy.
J.R. Finkel and C.D. Manning. 2009. Joint parsing and
named entity recognition. In Proc. of NAACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of EMNLP.
K. Hall. 2007. k-best spanning tree parsing. In Proc. of
ACL, June.
J. Judge, A. Cahill, and J. Van Genabith. 2006. Question-
bank: Creating a corpus of parse-annotated questions.
In Proc. of ACL, pages 497?504.
J. Katz-Brown, S. Petrov, R. McDonald, D. Talbot,
F. Och, H. Ichikawa, M. Seno, and H. Kazawa. 2011.
Training a parser for machine translation reordering.
In Proc. of EMNLP.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publishers.
P. Liang, A. Bouchard-Ct, D. Klein, and B. Taskar. 2006.
An end-to-end discriminative approach to machine
translation. In Proc. of COLING/ACL.
G.S. Mann and A. McCallum. 2010. Generalized Ex-
pectation Criteria for Semi-Supervised Learning with
Weakly Labeled Data. The Journal of Machine Learn-
ing Research, 11:955?984.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19:313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of EMNLP-CoNLL.
1498
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. De-
pendency tree-based sentiment classification using crfs
with hidden variables. In Proc. of NAACL.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
S. Petrov, P.C. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In Proc. of EMNLP, pages 705?713.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalu-
ation framework for machine translation reordering.
In Proc. of the Sixth Workshop on Statistical Machine
Translation.
M. Wang, N.A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proc. of EMNLP-CoNLL.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Us-
ing a dependency parser to improve SMT for Subject-
Object-Verb languages. In Proc. of NAACL.
A. Yates and O. Etzioni. 2009. Unsupervised meth-
ods for determining object and relation synonyms on
the web. Journal of Artificial Intelligence Research,
34(1):255?296.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP, pages 562?571.
1499
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 12?21,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
A Lightweight Evaluation Framework for Machine Translation Reordering
David Talbot1 and Hideto Kazawa2 and Hiroshi Ichikawa2
Jason Katz-Brown2 and Masakazu Seno2 and Franz J. Och1
1 Google Inc. 2 Google Japan
1600 Amphitheatre Parkway Roppongi Hills Mori Tower
Mountain View, CA 94043 6-10-1 Roppongi, Tokyo 106-6126
{talbot, och}@google.com {kazawa, ichikawa}@google.com
{jasonkb, seno}@google.com
Abstract
Reordering is a major challenge for machine
translation between distant languages. Recent
work has shown that evaluation metrics that
explicitly account for target language word or-
der correlate better with human judgments of
translation quality. Here we present a simple
framework for evaluating word order indepen-
dently of lexical choice by comparing the sys-
tem?s reordering of a source sentence to ref-
erence reordering data generated from manu-
ally word-aligned translations. When used to
evaluate a system that performs reordering as
a preprocessing step our framework allows the
parser and reordering rules to be evaluated ex-
tremely quickly without time-consuming end-
to-end machine translation experiments. A
novelty of our approach is that the translations
used to generate the reordering reference data
are generated in an alignment-oriented fash-
ion. We show that how the alignments are
generated can significantly effect the robust-
ness of the evaluation. We also outline some
ways in which this framework has allowed our
group to analyze reordering errors for English
to Japanese machine translation.
1 Introduction
Statistical machine translation systems can perform
poorly on distant language pairs such as English
and Japanese. Reordering errors are a major source
of poor or misleading translations in such systems
(Isozaki et al, 2010). Unfortunately the stan-
dard evaluation metrics used by the statistical ma-
chine translation community are relatively insensi-
tive to the long-distance reordering phenomena en-
countered when translating between such languages
(Birch et al, 2010).
The ability to rapidly evaluate the impact of
changes on a system can significantly accelerate the
experimental cycle. In a large statistical machine
translation system, we should ideally be able to ex-
periment with separate components without retrain-
ing the complete system. Measures such as per-
plexity have been successfully used to evaluate lan-
guage models independently in speech recognition
eliminating some of the need for end-to-end speech
recognition experiments. In machine translation,
alignment error rate has been used with some mixed
success to evaluate word-alignment algorithms but
no standard evaluation frameworks exist for other
components of a machine translation system (Fraser
and Marcu, 2007).
Unfortunately, BLEU (Papineni et al, 2001) and
other metrics that work with the final output of a ma-
chine translation system are both insensitive to re-
ordering phenomena and relatively time-consuming
to compute: changes to the system may require the
realignment of the parallel training data, extraction
of phrasal statistics and translation of a test set. As
training sets grow in size, the cost of end-to-end ex-
perimentation can become significant. However, it is
not clear that measurements made on any single part
of the system will correlate well with human judg-
ments of the translation quality of the whole system.
Following Collins et al (2005a) and Wang (2007),
Xu et al (2009) showed that when translating from
English to Japanese (and to other SOV languages
such as Korean and Turkish) applying reordering as
12
a preprocessing step that manipulates a source sen-
tence parse tree can significantly outperform state-
of-the-art phrase-based and hierarchical machine
translation systems. This result is corroborated by
Birch et al (2009) whose results suggest that both
phrase-based and hierarchical translation systems
fail to capture long-distance reordering phenomena.
In this paper we describe a lightweight framework
for measuring the quality of the reordering compo-
nents in a machine translation system. While our
framework can be applied to any translation sys-
tem in which it is possible to derive a token-level
alignment from the input source tokens to the out-
put target tokens, it is of particular practical interest
when applied to a system that performs reordering
as a preprocessing step (Xia and McCord, 2004). In
this case, as we show, it allows for extremely rapid
and sensitive analysis of changes to parser, reorder-
ing rules and other reordering components.
In our framework we evaluate the reordering pro-
posed by a system separately from its choice of tar-
get words by comparing it to a reference reordering
of the sentence generated from a manually word-
aligned translation. Unlike previous work (Isozaki
et al, 2010), our approach does not rely on the sys-
tem?s output matching the reference translation lexi-
cally. This makes the evaluation more robust as there
may be many ways to render a source phrase in the
target language and we would not wish to penalize
one that simply happens not to match the reference.
In the next section we review related work on
reordering for translation between distant language
pairs and automatic approaches to evaluating re-
ordering in machine translation. We then describe
our evaluation framework including certain impor-
tant details of how our reference reorderings were
created. We evaluate the framework by analyz-
ing how robustly it is able to predict improvements
in subjective translation quality for an English to
Japanese machine translation system. Finally, we
describe ways in which the framework has facili-
tated development of the reordering components in
our system.
2 Related Work
2.1 Evaluating Reordering
The ability to automatically evaluate machine trans-
lation output has driven progress in statistical ma-
chine translation; however, shortcomings of the
dominant metric, BLEU (Papineni et al, 2001) , par-
ticularly with respect to reordering, have long been
recognized (Callison-burch and Osborne, 2006).
Reordering has also been identified as a major fac-
tor in determining the difficulty of statistical ma-
chine translation between two languages (Birch et
al., 2008) hence BLEU scores may be most unreli-
able precisely for those language pairs for which sta-
tistical machine translation is most difficult (Isozaki
et al, 2010).
There have been many results showing that met-
rics that account for reordering are better correlated
with human judgements of translation quality (Lavie
and Denkowski, 2009; Birch and Osborne, 2010;
Isozaki et al, 2010). Examples given in Isozaki et
al. (2010) where object and subject arguments are
reversed in a Japanese to English statistical machine
translation system demonstrate how damaging re-
ordering errors can be and it should therefore not
come as a surprise that word order is a strong pre-
dictor of translation quality; however, there are other
advantages to be gained by focusing on this specific
aspect of the translation process in isolation.
One problem for all automatic evaluation metrics
is that multiple equally good translations can be con-
structed for most input sentences and typically our
reference data will contain only a small fraction of
these. Equally good translations for a sentence may
differ both in terms of lexical choice and word or-
der. One of the potential advantages of designing a
metric that looks only at word order, is that it may,
to some extent, factor out variability along the di-
mension of the lexical choice. Previous work on au-
tomatic evaluation metrics that focus on reordering,
however, has not fully exploited this.
The evaluation metrics proposed in Isozaki et al
(2010) compute a reordering score by comparing
the ordering of unigrams and bigrams that appear
in both the system?s translation and the reference.
These scores are therefore liable to overestimate
the reordering quality of sentences that were poorly
translated. While Isozaki et al (2010) does propose
13
a work-around to this problem which combines the
reordering score with a lexical precision term, this
clearly introduces a bias in the metric whereby poor
translations are evaluated primarily on their lexical
choice and good translations are evaluated more on
the basis of their word order. In our experience
word order is particularly poor in those sentences
that have the lowest lexical overlap with reference
translations; hence we would like to be able to com-
pute the quality of reordering in all sentences inde-
pendently of the quality of their lexical choice.
Birch and Osborne (2010) are closer to our ap-
proach in that they use word alignments to induce a
permutation over the source sentence. They com-
pare a source-side permutation generated from a
word alignment of the reference translation with one
generated from the system?s using various permuta-
tion distances. However, Birch and Osborne (2010)
only demonstrate that these metrics are correlated
with human judgements of translation quality when
combined with BLEU score and hence take lexical
choice into account.
Birch et al (2010) present the only results we
are aware of that compute the correlation be-
tween human judgments of translation quality and
a reordering-only metric independently of lexical
choice. Unfortunately, the experimental set-up there
is somewhat flawed. The authors ?undo? reorderings
in their reference translations by permuting the ref-
erence tokens and presenting the permuted transla-
tions to human raters. While many machine trans-
lation systems (including our own) assume that re-
ordering and translation can be factored into sepa-
rate models, e.g. (Xia and McCord, 2004), and per-
form these two operations in separate steps, the lat-
ter conditioned on the former, Birch et al (2010) are
making a much stronger assumption when they per-
form these simulations: they are assuming that lexi-
cal choice and word order are entirely independent.
It is easy to find cases where this assumption does
not hold and we would in general be very surprised
if a similar change in the reordering component in
our system did not also result in a change in the lex-
ical choice of the system; an effect which their ex-
periments are unable to model.
Another minor difference between our evaluation
framework and (Birch et al, 2010) is that we use
a reordering score that is based on the minimum
number of chunks into which the candidate and ref-
erence permutations can be concatenated similar to
the reordering component of METEOR (Lavie and
Denkowski, 2009). As we show, this is better cor-
related with human judgments of translation quality
than Kendall?s ? . This may be due to the fact that
it counts the number of ?jumps? a human reader has
to make in order to parse the system?s order if they
wish to read the tokens in the reference word order.
Kendall?s ? on the other hand penalizes every pair
of words that are in the wrong order and hence has
a quadratic (all-pairs) flavor which in turn might ex-
plain why Birch et al (2010) found that the square-
root of this quantity was a better predictor of trans-
lation quality.
2.2 Evaluation Reference Data
To create the word-aligned translations from which
we generate our reference reordering data, we used
a novel alignment-oriented translation method. The
method (described in more detail below) seeks
to generate reference reorderings that a machine
translation system might reasonably be expected to
achieve. Fox (2002) has analyzed the extent to
which translations seen in a parallel corpus can be
broken down into clean phrasal units: they found
that most sentence pairs contain examples of re-
ordering that violate phrasal cohesion, i.e. the cor-
responding words in the target language are not
completely contiguous or solely aligned to the cor-
responding source phrase. These reordering phe-
nomena are difficult for current statistical transla-
tion models to learn directly. We therefore deliber-
ately chose to create reference data that avoids these
phenomena as much as possible by having a single
annotator generate both the translation and its word
alignment. Our word-aligned translations are cre-
ated with a bias towards simple phrasal reordering.
Our analysis of the correlation between reorder-
ing scores computed on reference data created from
such alignment-oriented translations with scores
computed on references generated from standard
professional translations of the same sentences sug-
gests that the alignment-oriented translations are
more useful for evaluating a current state-of-the-art
system. We note also that while prior work has con-
jectured that automatically generated alignments are
a suitable replacement for manual alignments in the
14
context of reordering evaluation (Birch et al, 2008),
our results suggest that this is not the case at least for
the language pair we consider, English-Japanese.
3 A Lightweight Reordering Evaluation
We now present our lightweight reordering evalu-
ation framework; this consists of (1) a method for
generating reference reordering data from manual
word-alignments; and (2) a reordering metric for
scoring a sytem?s proposed reordering against this
reference data; and (3) a stand-alone evaluation tool.
3.1 Generating Reference Reordering Data
We follow Birch and Osborne (2010) in using ref-
erence reordering data that consists of permuations
of source sentences in a test set. We generate these
from word alignments of the source sentences to
reference translations. Unlike previous work, how-
ever, we have the same annotator generate both the
reference translation and the word alignment. We
also explicitly encourage the translators to generate
translations that are easy to align even if this does
result in occasionally unnatural translations. For in-
stance in English to Japanese translation we require
that all personal pronouns are translated; these are
often omitted in natural Japanese. We insist that
all but an extremely small set of words (articles and
punctuation for English to Japanese) be aligned. We
also disprefer non-contiguous alignments of a sin-
gle source word and require that all target words be
aligned to at least one source token. In Japanese
this requires deciding how to align particles that
mark syntactic roles; we choose to align these to-
gether with the content word (jiritsu-go) of the cor-
responding constituent (bunsetsu). Asking annota-
tors to translate and perform word alignment on the
same sentence in a single session does not necessar-
ily increase the annotation burden over stand-alone
word alignment since it encourages the creation of
alignment-friendly translations which can be aligned
more rapidly. Annotators need little special back-
ground or training for this task, as long as they can
speak both the source and target languages.
To generate a permutation from word alignments
we rank the source tokens by the position of the first
target token to which they are aligned. If multiple
source tokens are aligned to a single target word
or span we ignore the ordering within these source
spans; this is indicated by braces in Table 2. We
place unaligned source words immediately before
the next aligned source word or at the end of the
sentence if there is none. Table 2 shows the ref-
erence reordering derived from various translations
and word alignments.
3.2 Fuzzy Reordering Score
To evaluate the quality of a system?s reordering
against this reference data we use a simple reorder-
ing metric related to METEOR?s reordering compo-
nent (Lavie and Denkowski, 2009) . Given the refer-
ence permutation of the source sentence ?ref and the
system?s reordering of the source sentence ?sys ei-
ther generated directly by a reordering component or
inferred from the alignment between source and tar-
get phrases used in the decoder, we align each word
in ?sys to an instance of itself in ?ref taking the first
unmatched instance of the word if there is more than
one. We then define C to be the number chunks of
contiguously aligned words. If M is the number of
words in the source sentence then the fuzzy reorder-
ing score is computed as,
FRS(?sys, ?ref) = 1?
C ? 1
M ? 1
. (1)
This metric assigns a score between 0 and 1 where
1 indicates that the system?s reordering is identical
to the reference. C has an intuitive interpretation as
the number of times a reader would need to jump in
order to read the system?s reordering of the sentence
in the order proposed by the reference.
3.3 Evaluation Tool
While the framework we propose can be applied to
any machine translation system in which a reorder-
ing of the source sentence can be inferred from the
translation process, it has proven particularly use-
ful applied to a system that performs reordering as
a separate preprocessing step. Such pre-ordering
approaches (Xia and McCord, 2004; Collins et al,
2005b) can be criticized for greedily committing to
a single reordering early in the pipeline but in prac-
tice they have been shown to perform extremely well
on language pairs that require long distance reorder-
ing and have been successfully combined with other
more integrated reordering models (Xu et al, 2009).
15
The performance of a parser-based pre-ordering
component is a function of the reordering rules and
parser; it is therefore desirable that these can be eval-
uated efficiently. Both parser and reordering rules
may be evaluated using end-to-end automatic met-
rics such as BLEU score or in human evaluations.
Parsers may also be evaluated using intrinsic tree-
bank metrics such as labeled accuracy. Unfortu-
nately these metrics are either expensive to compute
or, as we show, unpredictive of improvements in hu-
man perceptions of translation quality.
Having found that the fuzzy reordering score pro-
posed here is well-correlated with changes in human
judgements of translation quality, we established a
stand-alone evaluation tool that takes a set of re-
ordering rules and a parser and computes the re-
ordering scores on a set of reference reorderings.
This has become the most frequently used method
for evaluating changes to the reordering component
in our system and has allowed teams working on
parsing, for instance, to contribute significant im-
provements quite independently.
4 Experimental Set-up
We wish to determine whether our evaluation frame-
work can predict which changes to reordering com-
ponents will result in statistically significant im-
provements in subjective translation quality of the
end-to-end system. To that end we created a num-
ber of systems that differ only in terms of reorder-
ing components (parser and/or reordering rules). We
then analyzed the corpus- and sentence-level corre-
lation of our evaluation metric with judgements of
human translation quality.
Previous work has compared either quite separate
systems, e.g. (Isozaki et al, 2010), or systems that
are artificially different from each other (Birch et al,
2010). There has also been a tendency to measure
corpus-level correlation. We are more interested in
comparing systems that differ in a realistic manner
from one another as would typically be required in
development. We also believe sentence-level cor-
relation is more important than corpus-level corre-
lation since good sentence-level correlation implies
that a metric can be used for detailed analysis of a
system and potentially to optimize it.
4.1 Systems
We carried out all our experiments using a state-of-
the-art phrase-based statistical English-to-Japanese
machine translation system (Och, 2003). Dur-
ing both training and testing, the system reorders
source-language sentences in a preprocessing step
using a set of rules written in the framework pro-
posed by (Xu et al, 2009) that reorder an English
dependency tree into target word order. During de-
coding, we set the reordering window to 4 words.
In addition to the regular distance distortion model,
we incorporate a maximum entropy based lexical-
ized phrase reordering model (Zens and Ney, 2006).
For parallel training data, we use an in-house collec-
tion of parallel documents. These come from vari-
ous sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. We trained our system on
about 300 million source words.
The reordering rules applied to the English de-
pendency tree define a precedence order for the chil-
dren of each head category (a coarse-grained part of
speech). For example, a simplified version of the
precedence order for child labels of a verbal head
HEADVERB is: advcl, nsubj, prep, [other children],
dobj, prt, aux, neg, HEADVERB, mark, ref, compl.
The dependency parser we use is an implementa-
tion of a transition-based dependency parser (Nivre,
2008). The parser is trained using the averaged per-
ceptron algorithm with an early update strategy as
described in Zhang and Clark (2008).
We created five systems using different parsers;
here targeted self-training refers to a training pro-
cedure proposed by Katz-Brown et al (2011) that
uses our reordering metric and separate reference re-
ordering data to pick parses for self-training: an n-
best list of parses is generated for each English sen-
tence for which we have reference reordering data
and the parse tree that results in the highest fuzzy
reordering score is added to our parser?s training set.
Parsers P3, P4 and P5 differ in how that framework
is applied and how much data is used.
? P1 Penn Treebank, perceptron, greedy search
? P2 Penn Treebank, perceptron, beam search
? P3 Penn Treebank, perceptron, beam search,
targeted self-training on web data
16
? P4 Penn Treebank, perceptron, beam search,
targeted self-training on web data
? P5 Penn Treebank, perceptron, beam search,
targeted self-training on web data, case insen-
sitive
We also created five systems using the fifth parser
(P5) but with different sets of reordering rules:
? R1 No reordering
? R2 Reverse reordering
? R3 Head final reordering with reverse reorder-
ing for words before the head
? R4 Head final reordering with reverse reorder-
ing for words after the head
? R5 Superset of rules from (Xu et al, 2009)
Reverse reordering places words in the reverse of the
English order. Head final reordering moves the head
of each dependency after all its children. Rules in R3
and R4 overlap significantly with the rules for noun
and verb subtrees respectively in R5. Otherwise all
systems were identical. The rules in R5 have been
extensively hand-tuned while R1 and R2 are rather
naive. System P5R5 was our best performing system
at the time these experiments were conducted.
We refer to systems by a combination of parser
and reordering rules set identifiers, for instance, sys-
tem P2R5, uses parser P2 with reordering rules R5.
We conducted two subjective evaluations in which
bilingual human raters were asked to judge trans-
lations on a scale from 0 to 6 where 0 indicates
nonsense and 6 is perfect. The first experiment
(Parsers) contrasted systems with different parsers
and the second (Rules) varied the reordering rules.
In each case three bilingual evaluators were shown
the source sentence and the translations produced by
all five systems.
4.2 Meta-analysis
We perform a meta-analysis of the following metrics
and the framework by computing correlations with
the results of these subjective evaluations of transla-
tion quality:
1. Evaluation metrics: BLEU score on final trans-
lations, Kendall?s ? and fuzzy reordering score
on reference reordering data
2. Evaluation data: both manually-generated and
automatically-generated word alignments on
both standard professional and alignment-
oriented translations of the test sentences
The automatic word alignments were generated us-
ing IBM Model 1 in order to avoid directional biases
that higher-order models such as HMMs have.
Results presented in square parentheses are 95
percent confidence intervals estimated by bootstrap
resampling on the test corpus (Koehn, 2004).
Our test set contains 500 sentences randomly
sampled from the web. We have both professional
and alignment-friendly translations for these sen-
tences. We created reference reorderings for this
data using the method described in Section 3.1.
The lack of a broad domain and publically avail-
able Japanese test corpus makes the use of this non-
standard test set unfortunately unavoidable.
The human raters were presented with the source
sentence, the human reference translation and the
translations of the various systems simultaneously,
blind and in a random order. Each rater was allowed
to rate no more than 3 percent of the sentences and
three ratings were elicited for each sentence. Rat-
ings were a single number between 0 and 6 where 0
indicates nonsense and 6 indicates a perfectly gram-
matical translation of the source sentence.
5 Results
Table 2 shows four reference reorderings generated
from various translations and word alignments. The
automatic alignments are significantly sparser than
the manual ones but in these examples the refer-
ence reorderings still seem reasonable. Note how the
alignment-oriented translation includes a pronoun
(translation for ?I?) that is dropped in the slightly
more natural standard translation to Japanese.
Table 1 shows the human judgements of transla-
tion quality for the 10 systems (note that P5R5 ap-
pears in both experiments but was scored differently
as human judgments are affected by which other
translations are present in an experiment). There is a
clear ordering of the systems in each experiment and
17
1. Parsers Subjective Score (0-6) 2. Rules Subjective Score (0-6)
P1R5 2.173 [2.086, 2.260] P5R1 1.258 [1.191, 1.325]
P2R5 2.320 [2.233, 2.407] P5R2 1.825 [1.746, 1.905]
P3R5 2.410 [2.321, 2.499] P5R3 1.849 [1.767, 1.931]
P4R5 2.453 [2.366, 2.541] P5R4 2.205 [2.118, 2.293]
P5R5 2.501 [2.413, 2.587] P5R5 2.529 [2.441, 2.619]
Table 1: Human judgements of translation quality for 1. Parsers and 2. Rules.
Metric Sentence-level correlation
r ?
Fuzzy reordering 0.435 0.448
Kendall?s ? 0.371 0.450
BLEU 0.279 0.302
Table 6: Pearson?s correlation (r) and Spearman?s rank
correlation (?) with subjective translation quality at
sentence-level.
we see that both the choice of parser and reordering
rules clearly effects subjective translation quality.
We performed pairwise significance tests using
bootstrap resampling for each pair of ?improved?
systems in each experiment. Tables 3, 4 and 5
shows which pairs were judged to be statistically
significant improvements at either 95 or 90 percent
level under the different metrics. These tests were
computed on the same 500 sentences. All pairs
but one are judged to be statistically significant im-
provements in subjective translation quality. Sig-
nificance tests performed using the fuzzy reorder-
ing metric are identical to the subjective scores for
the Parsers experiment but differ on one pairwise
comparison for the Rules experiment. According to
BLEU score, however, none of the parser changes
are significant at the 95 percent level and only one
pairwise comparison (between the two most differ-
ent systems) was significant at the 90 percent level.
BLEU score appears more sensitive to the larger
changes in the Rules experiment but is still in dis-
agreement with the results of the human evaluation
on four pairwise comparisons.
Table 6 shows the sentence-level correlation of
different metrics with human judgments of transla-
tion quality. Here both the fuzzy reordering score
and Kendall?s ? are computed on the reference
reordering data generated as described in Section
3.1. Both metrics are computed by running our
Translation Alignment Sentence-level
r ?
Alignment-oriented Manual 0.435 0.448
Alignment-oriented Automatic 0.234 0.252
Standard Manual 0.271 0.257
Standard Automatic 0.177 0.159
Table 7: Pearson?s correlation (r) and Spearman?s rank
correlation (?) with subjective translation quality at the
sentence-level for different types of reordering reference
data: (i) alignment-oriented translation vs. standard, (ii)
manual vs. automatic alignment.
lightweight evaluation tool and involve no transla-
tion whatsoever. These lightweight metrics are also
more correlated with subjective quality than BLEU
score at the sentence level.
Table 7 shows how the correlation between fuzzy
reordering score and subjective translation quality
degrades as we move from manual to automatic
alignments and from alignment-oriented translations
to standard ones. The automatically aligned refer-
ences, in particular, are less correlated with subjec-
tive translation scores then BLEU; we believe this
may be due to the poor quality of word alignments
for languages such as English and Japanese due to
the long-distance reordering between them.
Finally we present some intrinsic evaluation met-
rics for the parsers used in the first of our experi-
ments. Table 8 demonstrates that certain changes
may not be best captured by standard parser bench-
marks. While the first four parser models improve
on the WSJ benchmarks as they improve subjective
translation quality the best parser according to sub-
jective translation qualtiy (P5) is actually the worst
under both metrics on the treebank data. We con-
jecture that this is due to the fact that P5 (unlike the
other parsers) is case insensitive. While this helps us
significantly on our test set drawn from the web, it
18
Standard / ManualSource              How Can I Qualify For A Mortgage Tax Deduction ?Reordering        A Mortgage {{ Tax Deduction }} For I Qualify How Can ?Translation                        ?? ??? ?? ? ?? ? ?? ? ?? ? ? ?? ?? ? ?? ?? ? ?Alignment          6,6,7_8,4,3,3,3,3,3,0,0,0,0,0,1,1,9,9
Alignment-oriented / ManualSource              How Can I Qualify For A Mortgage Tax Deduction ?Reordering        I How A Mortgage {{ Tax Deduction }} For Qualify Can ?Translation                         ? ? ?? ? ?? ?? ??? ? ?? ? ?? ? ??? ?? ? ?? ?? ? ?Alignment         2,2,0,0,0,6,6,6,7_8,4,3,3,3,1,1,1,1,1,9
Standard / AutomaticSource              We do not claim to cure , prevent or treat any disease .Reordering        any disease cure , prevent or treat claim to We do not .Translation           ???? ?? ? ?? ,  ?? ,            ??? ?? ? ?? ?? ?? ? ? ?? ?? ? .Alignment         10,11,,5,6,7,,8,9,,,4,,,,2,2,2,12
Alignment-oriented / AutomaticSource            We do not claim to cure , prevent or treat any disease .Reordering        We any disease cure , prevent or treat claim to do not .Translation              ? ? ? ???? ?? ? ?? ,           ?? ???? ?? ? ?? ? ?? ? ?? ? .Alignment          0,0,,10,11,,5,6,7,8,9,,,,3,4,2,2,12
Table 2: Reference reordering data generated via various methods: (i) alignment-oriented vs. standard translation, (ii)
manual vs. automatic word alignment
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 +** +** +** +** P5R1 +** +** +** +**
P2R5 +** +** +** P5R2 0 +** +**
P3R5 +** P5R3 +** +**
P4R5 0 P5R4 +**
Table 3: Pairwise significance in subjective evaluation (0 = not significant, * = 90 percent, ** = 95 percent).
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 +** +** +** +** P5R1 0 +** +** +**
P2R5 +** +** +** P5R2 +** +** +**
P3R5 +** +** P5R3 +** +**
P4R5 0 P5R4 +**
Table 4: Pairwise significance in fuzzy reordering score (0 = not significant, * = 90 percent, ** = 95 percent).
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 0 0 +* +* P5R1 +** +** +** +**
P2R5 0 0 0 P5R2 0 +** +**
P3R5 0 0 P5R3 0 +*
P4R5 0 P5R4 0
Table 5: Pairwise significance in BLEU score (0 = not significant, * = 90 percent, ** = 95 percent).
19
Parser Labeled attachment POS accuracy
P1 0.807 0.954
P2 0.822 0.954
P3 0.827 0.955
P4 0.830 0.955
P5 0.822 0.944
Table 8: Intrinsic parser metrics on WSJ dev set.
Figure 1: P1 and P5?s parse trees and automatic reorder-
ing (using R5 ruleset) and fuzzy score.
hurts parsing performance on cleaner newswire.
6 Discussion
We have found that in practice this evaluation frame-
work is sufficiently correlated with human judg-
ments of translation quality to be rather useful for
performing detailed error analysis of our English-to-
Japanese system. We have used it in the following
ways in simple error analysis sessions:
? To identify which words are most frequently re-
ordered incorrectly
? To identify systematic parser and/or POS errors
? To identify the worst reordered sentences
? To evaluate individual reordering rules
Figures 1 and 2 show pairs of parse trees together
with their resulting reorderings and scores against
Figure 2: P1 and P5?s parse trees and automatic reorder-
ing (using R5 ruleset) and fuzzy score.
the reference. These are typical of the parser er-
rors that impact reordering and which are correctly
identified by our framework. In related joint work
(Katz-Brown et al, 2011) and (Hall et al, 2011), it
is shown that the framework can be used to optimize
reordering components automatically.
7 Conclusions
We have presented a lightweight framework for eval-
uating reordering in machine translation and demon-
strated that this is able to accurately distinguish sig-
nificant changes in translation quality due to changes
in preprocessing components such as the parser or
reordering rules used by the system. The sentence-
level correlation of our metric with judgements of
human translation quality was shown to be higher
than other standard evaluation metrics while our
evaluation has the significant practical advantage of
not requiring an end-to-end machine translation ex-
periment when used to evaluate a separate reorder-
ing component. Our analysis has also highlighted
the benefits of creating focused evaluation data that
attempts to factor out some of the phenomena found
in real human translation. While previous work has
provided meta-analysis of reordering metrics across
quite independent systems, ours is we believe the
first to provide a detailed comparison of systems
20
that differ only in small but realistic aspects such as
parser quality. In future work we plan to use the
framework to provide a more comprehensive analy-
sis of the reordering capabilities of a broad range of
machine translation systems.
References
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 327?332,
Uppsala, Sweden, July.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting success in machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 745?
754, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A quantitative analysis of reordering phenom-
ena. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation, pages 197?205, Athens,
Greece, March.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for mt evaluation: evaluating reorder-
ing. Machine Translation, 24:15?26, March.
Chris Callison-burch and Miles Osborne. 2006. Re-
evaluating the role of bleu in machine translation re-
search. In In EACL, pages 249?256.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005a. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 531?540, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005b. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 531?540, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 304?3111, July.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33:293?303, September.
Keith Hall, Ryan McDonald, and Jason Katz-Brown.
2011. Training dependency parsers by jointly optimiz-
ing multiple objective functions. In Proc. of EMNLP
2011.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic evalu-
ation of translation quality for distant language pairs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 944?
952, Cambridge, MA, October. Association for Com-
putational Linguistics.
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz
Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno,
and Hideto Kazawa. 2011. Training a Parser for Ma-
chine Translation Reordering. In Proc. of EMNLP
2011.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23(2-3):105?115.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Chao Wang. 2007. Chinese syntactic reordering for
statistical machine translation. In In Proceedings of
EMNLP, pages 737?745.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the 20th international con-
ference on Computational Linguistics, COLING ?04,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
245?253, Boulder, Colorado, June.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation, pages 55?63.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
21
