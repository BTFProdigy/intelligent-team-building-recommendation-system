Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 963?970, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Query Expansion with the Minimum User Feedback
by Transductive Learning
Masayuki OKABE
Information and Media Center
Toyohashi University of Technology
Aichi, 441-8580, Japan
okabe@imc.tut.ac.jp
Kyoji UMEMURA
Information and Computer Sciences
Toyohashi University of Technology
Aichi, 441-8580, Japan
umemura@tutics.tut.ac.jp
Seiji YAMADA
National Institute of Informatics
Tokyo,101-8430, Japan
seiji@nii.ac.jp
Abstract
Query expansion techniques generally se-
lect new query terms from a set of top
ranked documents. Although a user?s
manual judgment of those documents
would much help to select good expansion
terms, it is difficult to get enough feedback
from users in practical situations. In this
paper we propose a query expansion tech-
nique which performs well even if a user
notifies just a relevant document and a
non-relevant document. In order to tackle
this specific condition, we introduce two
refinements to a well-known query expan-
sion technique. One is application of a
transductive learning technique in order to
increase relevant documents. The other is
a modified parameter estimation method
which laps the predictions by multiple
learning trials and try to differentiate the
importance of candidate terms for expan-
sion in relevant documents. Experimen-
tal results show that our technique outper-
forms some traditional query expansion
methods in several evaluation measures.
1 Introduction
Query expansion is a simple but very useful tech-
nique to improve search performance by adding
some terms to an initial query. While many query
expansion techniques have been proposed so far, a
standard method of performing is to use relevance
information from a user (Ruthven, 2003). If we
can use more relevant documents in query expan-
sion, the likelihood of selecting query terms achiev-
ing high search improvement increases. However it
is impractical to expect enough relevance informa-
tion. Some researchers said that a user usually noti-
fies few relevance feedback or nothing (Dumais and
et al, 2003).
In this paper we investigate the potential perfor-
mance of query expansion under the condition that
we can utilize little relevance information, espe-
cially we only know a relevant document and a non-
relevant document. To overcome the lack of rele-
vance information, we tentatively increase the num-
ber of relevant documents by a machine learning
technique called Transductive Learning. Compared
with ordinal inductive learning approach, this learn-
ing technique works even if there is few training ex-
amples. In our case, we can use many documents
in a hit-list, however we know the relevancy of few
documents. When applying query expansion, we use
those increased documents as if they were true rel-
evant ones. When applying the learning, there oc-
curs some difficult problems of parameter settings.
We also try to provide a reasonable resolution for
the problems and show the effectiveness of our pro-
posed method in experiments.
The point of our query expansion method is that
we focus on the availability of relevance information
in practical situations. There are several researches
which deal with this problem. Pseudo relevance
feedback which assumes top n documents as rele-
vant ones is one example. This method is simple and
relatively effective if a search engine returns a hit-
963
list which contains a certain number of relative doc-
uments in the upper part. However, unless this as-
sumption holds, it usually gives a worse ranking than
the initial search. Thus several researchers propose
some specific procedure to make pseudo feedback
be effective (Yu and et al 2003; Lam-Adesina and
Jones, 2001). In another way, Onoda (Onoda et al,
2004) tried to apply one-class SVM (Support Vec-
tor Machine) to relevance feedback. Their purpose
is to improve search performance by using only non-
relevant documents. Though their motivation is sim-
ilar to ours in terms of applying a machine learning
method to complement the lack of relevance infor-
mation, the assumption is somewhat different. Our
assumption is to utilizes manual but the minimum
relevance judgment.
Transductive leaning has already been applied in
the field of image retrieval (He and et al, 2004). In
this research, they proposed a transductive method
called the manifold-ranking algorithm and showed
its effectiveness by comparing with active learn-
ing based Support Vector Machine. However, their
setting of relevance judgment is not different from
many other traditional researches. They fix the total
number of images that are marked by a user to 20.
As we have already claimed, this setting is not prac-
tical because most users feel that 20 is too much for
judgment. We think none of research has not yet an-
swered the question. For relevance judgment, most
of the researches have adopted either of the follow-
ing settings. One is the setting of ?Enough relevant
documents are available?, and the other is ?No rele-
vant document is available?. In contrast to them, we
adopt the setting of ?Only one relevant document is
available?. Our aim is to achieve performance im-
provement with the minimum effort of judging rele-
vancy of documents.
The reminder of this paper is structured as fol-
lows. Section 2 describes two fundamental tech-
niques for our query expansion method. Section 3
explains a technique to complement the smallness
of manual relevance judgment. Section 4 introduces
a whole procedure of our query expansion method
step by step. Section 5 shows empirical evidence
of the effectiveness of our method compared with
two traditional query expansion methods. Section 6
investigates the experimental results more in detail.
Finally, Section 7 summarizes our findings.
2 Basic Methods
2.1 Query Expansion
So far, many query expansion techniques have been
proposed. While some techniques focus on the
domain specific search which prepares expansion
terms in advance using some domain specific train-
ing documents (Flake and et al 2002; Oyama and et
al, 2001), most of techniques are based on relevance
feedback which is given automatically or manually.
In this technique, expansion terms are selected
from relevant documents by a scoring function. The
Robertson?s wpq method (Ruthven, 2003) is often
used as such a scoring function in many researches
(Yu and et al 2003; Lam-Adesina and Jones, 2001).
We also use it as our basic scoring function. It cal-
culates the score of each term by the following for-
mula.
wpqt =
(rt
R ?
nt ? rt
N ? R
)
?log rt/(R ? rt)(nt ? rt)/(N ? nt ? R + rt)
(1)
where rt is the number of seen relevant documents
containing term t. nt is the number of documents
containing t. R is the number of seen relevant doc-
uments for a query. N is the number of documents
in the collection. The second term of this formula
is called the Robertson/Spark Jones weight (Robert-
son, 1990) which is the core of the term weighting
function in the Okapi system (Robertson, 1997).
This formula is originated in the following for-
mula.
wpqt = (pt ? qt) log
pt(1? qt)
qt(1? pt)
(2)
where pt is the probability that a term t appears in
relevant documents. qt is the probability that a term
t appears in non-relevant documents. We can easily
notice that it is very important how the two prob-
ability of pt and qt should be estimated. The first
formula estimates pt with rtR and qt with
Nt?Rt
N?R . For
the good estimation of pt and qt, plenty of relevant
document is necessary. Although pseudo feedback
which automatically assumes top n documents as
relevant is one method and is often used, its perfor-
mance heavily depends on the quality of an initial
search. As we show later, pseudo feedback has lim-
ited performance.
We here consider a query expansion technique
which uses manual feedback. It is no wonder
964
manual feedback shows excellent and stable perfor-
mance if enough relevant documents are available,
hence the challenge is how it keeps high perfor-
mance with less amount of manual relevance judg-
ment. In particular, we restrict the manual judgment
to the minimum amount, namely only a relevant
document and a non-relevant document. In this
assumption, the problem is how to find more rele-
vant documents based on a relevant document and a
non-relevant document. We use transductive learn-
ing technique which is suitable for the learning prob-
lem where there is small training examples.
2.2 Transductive Learning
Transductive learning is a machine learning tech-
nique based on the transduction which directly de-
rives the classification labels of test data without
making any approximating function from training
data (Vapnik, 1998). Because it does not need to
make approximating function, it works well even if
the number of training data is small.
The learning task is defined on a data set X
of n points. X consists of training data set
L = (x?1, x?2, ..., x?l) and test data set U =
(x?l+1, x?l+2, ..., x?l+u); typically l ? u. The purpose
of the learning is to assign a label to each data point
in U under the condition that the label of each data
point in L are given.
Recently, transductive learning or semi-
supervised learning is becoming an attractive
subject in the machine learning field. Several
algorithms have been proposed so far (Joachims,
1999; Zhu and et al, 2003; Blum and et al, 2004)
and they show the advantage of this approach in
various learning tasks. In order to apply transductive
learning to our query expansion, we select an algo-
rithm called ?Spectral Graph Transducer (SGT)?
(Joachims, 2003), which is one of the state of the art
and the best transductive learning algorithms. SGT
formalizes the problem of assigning labels to U with
an optimization problem of the constrained ratiocut.
By solving the relaxed problem, it produces an
approximation to the original solution.
When applying SGT to query expansion, X cor-
responds to a set of top n ranked documents in a
hit-list. X does not corresponds to a whole docu-
ment collection because the number of documents
in a collection is too huge1 for any learning sys-
tem to process. L corresponds to two documents
with manual judgments, a relevant document and
a non-relevant document. Furthermore, U corre-
sponds to the documents of X ? L whose rele-
vancy is unknown. SGT is used to produce the rel-
evancy of documents in U . SGT actually assigns
values around ?+ ? ? for documents possibly be-
ing relevant and ?? ? ? for documents possibly be-
ing non-relevant. ?+ = +
?
1?fp
fp , ?? = ?
?
fp
1?fp ,
? = 12(?+ + ??), and fp is the fraction of relevant
documents in X . We cannot know the true value of
fp in advance, thus we have to estimate its approxi-
mation value before applying SGT.
According to Joachims, parameter k (the number
of k-nearest points of a data x?) and d (the number
of eigen values to ...) give large influence to SGT?s
learning performance. Of course those two parame-
ters should be set carefully. However, besides them,
fp is much more important for our task because it
controls the learning performance. Since extremely
small L (actually |L| = 2 is our setting) give no
information to estimate the true value of fp, we do
not strain to estimate its single approximation value
but propose a new method to utilize the results of
learning with some promising fp. We describe the
method in the next section.
3 Parameter Estimations based on
Multiple SGT Predictions
3.1 Sampling for Fraction of Positive Examples
SGT prepares 2 estimation methods to set fp au-
tomatically. One is to estimate from the fraction
of positive examples in training examples. This
method is not suitable for our task because fp is
always fixed to 0.5 by this method if the number
of training examples changes despite the number of
relevant documents is small in many practical situa-
tions. The other is to estimate with a heuristic that
the difference between a setting of fp and the frac-
tion of positive examples actually assigned by SGT
should be as small as possible. The procedure pro-
vided by SGT starts from fp = 0.5 and the next fp is
set to the fraction of documents assigned as relevant
in the previous SGT trial. It repeats until fp changes
1Normally it is more than ten thousand.
965
Input
Ntr // the number of training examples
Output
S // a set of sampling points
piv = ln(Ntr); // sampling interval
nsp = 0; // the number of sampling points
for(i = piv; i ? Ntr ? 1; i+ = piv){
add i to ;
nsp++;
if(nsp == 10){ exit; }
}
Figure 1: Pseudo code of sampling procedure for fp
five times or the difference converges less than 0.01.
This method is neither works well because the con-
vergence is not guaranteed at all.
Presetting of fp is primarily very difficult problem
and consequently we take another approach which
laps the predictions of multiple SGT trials with some
sampled fp instead of setting a single fp. This ap-
proach leads to represent a relevant document by not
a binary value but a real value between 0 and 1. The
sampling procedure for fp is illustrated in Figure 1.
In this procedure, sampling interval changes accord-
ing to the number of training examples. In our pre-
liminary test, the number of sampling points should
be around 10. However this number is adhoc one,
thus we may need another value for another corpus.
3.2 Modified estimations for pt and qt
Once we get a set of sampling points S = {f ip :
i = 1 ? 10}, we run SGT with each f ip and laps
each resultant of prediction to calculate pt and qt as
follows.
pt =
?
i rit
?
i Ri
(3)
qt =
?
i nt ? rit
?
i N ?Ri
(4)
Here, Ri is the number of documents which SGT
predicts as relevant with ith value of f ip, and rit is
the number of documents in Ri where a term t ap-
pears. In each trial, SGT predicts the relevancy of
documents by binary value of 1 (for relevant) and 0
(for non-relevant), yet by lapping multiple resultant
of predictions, the binary prediction value changes
to a real value which can represents the relevancy of
documents in more detail. The main merit of this
approach in comparison with fixing fp to a single
value, it can differentiate a value of pt if Ntr is small.
4 Expansion Procedures
We here explain a whole procedure of our query ex-
pansion method step by step.
1. Initial Search: A retrieval starts by inputting a
query for a topic to an IR system.
2. Relevance Judgment for Documents in a
Hit-List: The IR system returns a hit-list for
the initial query. Then the hit-list is scanned
to check whether each document is relevant or
non-relevant in descending order of the rank-
ing. In our assumption, this reviewing pro-
cess terminates when a relevant document and
a non-relevant one are found.
3. Finding more relevant documents by trans-
ductive learning: Because only two judged
documents are too few to estimate pt and qt
correctly, our query expansion tries to increase
the number of relevant documents for the wpq
formula using the SGT transductive learning al-
gorithm. As shown in Figure2, SGT assigns a
value of the possibility to be relevant for the
topic to each document with no relevance judg-
ment (documents under the dashed line in the
Fig) based on two judged documents (docu-
ments above the dashed line in the Figure).
1. Document     1
2. Document     0
3. Document     ?
4. Document     ?
              :
i.  Document     ?
              :
Manually
assigned
Assigned by
Transductive
 Learning
Labels
Hit list
?1? means a positive label
?0? means a negative label
??? means an unknown label
Figure 2: A method to find tentative relevant docu-
ments
966
4. Selecting terms to expand the initial query:
Our query expansion method calculates the
score of each term appearing in relevant docu-
ments (including documents judged as relevant
by SGT) using wpq formula, and then selects
a certain number of expansion terms according
to the ranking of the score. Selected terms are
added to the initial query. Thus an expanded
query consists of the initial terms and added
terms.
5. The Next Search with an expanded query:
The expanded query is inputted to the IR sys-
tem and a new hit-list will be returned. One
cycle of query expansion finishes at this step.
In the above procedures, we naturally intro-
duced transductive learning into query expan-
sion as the effective way in order to automati-
cally find some relevant documents. Thus we
do not need to modify a basic query expan-
sion procedure and can fully utilize the poten-
tial power of the basic query expansion.
The computational cost of transductive learn-
ing is not so much. Actually transductive learn-
ing takes a few seconds to label 100 unla-
beled documents and query expansion with all
the labeled documents also takes a few sec-
onds. Thus our system can expand queries suf-
ficiently quick in practical applications.
5 Experiments
This section provides empirical evidence on how
our query expansion method can improve the per-
formance of information retrieval. We compare our
method with other traditional methods.
5.1 Environmental Settings
5.1.1 Data set
We use the TREC-8 data set (Voorhees and Har-
man, 1999) for our experiment. The document cor-
pus contains about 520,000 news articles. Each doc-
ument is preprocessed by removing stopwords and
stemming. We also use fifty topics (No.401-450)
and relevance judgments which are prepared for ad-
hoc task in the TREC-8. Queries for an initial search
are nouns extracted from the title tag in each topic.
5.1.2 Retrieval Models
We use two representative retrieval models which
are bases of the Okapi (Robertson, 1997) and
SMART systems. They showed highest perfor-
mance in the TREC-8 competition.
Okapi : The weight function in Okapi is BM25. It
calculates each document?s score by the follow-
ing formula.
score(d) =
?
T?Q
w(1) ? (k1 + 1)tf(k3 + 1)qtf(K + tf)(k3 + qtf)
(5)
w(1) = log (rt + 0.5)/(R ? rt + 0.5)(nt ? rt + 0.5)/(N ? nt ? R + rt + 0.5)
(6)
K = k1
(
(1 ? b) + b dlavdl
)
(7)
where Q is a query containing terms T , tf
is the term?s frequency in a document, qtf is
the term?s frequency in a text from which Q
was derived. rt and nt are described in sec-
tion 2. K is calculated by (7), where dl and
avdl denote the document length and the av-
erage document length. In our experiments,
we set k1 = 1.2, k3 = 1000, b = 0.75, and
avdl = 135.6. Terms for query expansion are
ranked in decreasing order of rt ? w(1) for the
following Okapi?s retrieval tests without SGT
(Okapi manual and Okapi pseudo) to make
conditions the same as of TREC-8.
SMART : The SMART?s weighting function is as
follows2.
score(d) =
?
T?Q
{1 + ln(1 + ln(tf))} ? log(N + 1df ) ? pivot (8)
pivot = 1
0.8 + 0.2 ? dlavdl
(9)
df is the term?s document frequency. tf , dl and
avdl are the same as Okapi. When doing rele-
vance feedback, a query vector is modified by
the following Rocchio?s method (with parame-
ters ? = 3, ? = 2, ? = 2).
Q?new = ?Q?old+
?
|Drel|
?
Drel
d?? ?|Dnrel|
?
Dnrel
d? (10)
2In this paper, we use AT&T?s method (Singhal et al, 1999)
applied in TREC-8
967
Table 1: Results of Initial Search
P10 P30 RPREC MAP R05P
Okapi ini 0.466 0.345 0.286 0.239 0.195
SMART ini 0.460 0.336 0.271 0.229 0.187
Drel and Dnrel are sets of seen relevant and
non-relevant documents respectively. Terms
for query expansion are ranked in decreasing
order of the above Rocchio?s formula.
Table 1 shows their initial search results of Okapi
(Okapi ini) and SMART (SMART ini). We adopt
five evaluation measures. Their meanings are as fol-
lows (Voorhees and Harman, 1999).
P10 : The precision after the first 10 documents are
retrieved.
P30 : The precision after the first 30 documents are
retrieved.
R-Prec : The precision after the first R documents
are retrieved, where R is the number of relevant
documents for the current topic.
MAP : Mean average precision (MAP) is the aver-
age precision for a single topic is the mean of
the precision obtained after each relevant doc-
ument is retrieved (using zero as the precision
for relevant documents that are not retrieved).
R05P : Recall at the rank where precision first dips
below 0.5 (after at least 10 documents have
been retrieved).
The performance of query expansion or relevance
feedback is usually evaluated on a residual collec-
tion where seen documents are removed. However
we compare our method with pseudo feedback based
ones, thus we do not use residual collection in the
following experiments.
5.1.3 Settings of Manual Feedback
For manual feedback, we set an assumption that
a user tries to find relevant and non-relevant doc-
uments within only top 10 documents in the result
of an initial search. If a topic has no relevant doc-
ument or no non-relevant document in the top 10
documents, we do not apply manual feedback, in-
stead we consider the result of the initial search for
Table 2: Results of Okapi sgt (5 terms expanded)
P10 P30 RPREC MAP R05P
20 0.516 0.381 0.308 0.277 0.233
50 0.494 0.380 0.286 0.265 0.207
100 0.436 0.345 0.283 0.253 0.177
Table 3: Results of Okapi sgt (10 terms expanded)
P10 P30 RPREC MAP R05P
20 0.508 0.383 0.301 0.271 0.216
50 0.520 0.387 0.294 0.273 0.208
100 0.494 0.365 0.283 0.261 0.190
Table 4: Results of Okapi sgt (15 terms expanded)
P10 P30 RPREC MAP R05P
20 0.538 0.381 0.298 0.274 0.223
50 0.528 0.387 0.298 0.283 0.222
100 0.498 0.363 0.280 0.259 0.197
Table 5: Results of Okapi sgt (20 terms expanded)
P10 P30 RPREC MAP R05P
20 0.546 0.387 0.307 0.289 0.235
50 0.520 0.385 0.299 0.282 0.228
100 0.498 0.369 0.272 0.255 0.188
such topics. There are 8 topics 3 which we do not
apply manual feedback methods.
5.2 Basic Performance
Firstly, we evaluate the basic performance of our
query expansion method by changing the number
of training examples. Since our method is based on
Okapi model, we represent it as Okapi sgt (with pa-
rameters k = 0.5?Ntr, d = 0.8?Ntr. k is the num-
ber of nearest neighbors, d is the number of eigen
values to use and Ntr is the number of training ex-
amples).
Table 2-5 shows five evaluation measures of
Okapi sgt when the number of expansion terms
changes. We test 20, 50 and 100 as the number of
training examples and 5, 10 15 and 20 for the num-
ber of expansion terms. As for the number of train-
ing examples, performance of 20 and 50 does not
differ so much in all the number of expansion terms.
However performance of 100 is clearly worse than
of 20 and 50. The number of expansion terms does
not effect so much in every evaluation measures. In
the following experiments, we compare the results
of Okapi sgt when the number of training examples
is 50 with other query expansion methods.
3Topic numbers are 409, 410, 424, 425, 431, 432, 437 and
450
968
Table 6: Results of Manual Feedback Methods
(MAP)
5 10 15 20
Okapi sgt 0.265 0.273 0.274 0.282
Okapi man 0.210 0.189 0.172 0.169
SMART man 0.209 0.222 0.220 0.219
Table 7: Results of Manual Feedback Methods (10
terms expanded)
P10 P30 RPREC MAP R05P
Okapi sgt 0.520 0.387 0.294 0.273 0.208
Okapi man 0.420 0.285 0.212 0.189 0.132
SMART man 0.434 0.309 0.250 0.222 0.174
5.3 Comparison with other Manual Feedback
Methods
We next compare our query expansion method with
the following manual feedback methods.
Okapi man : This method simply uses only one
relevant document judged by hand. This is
called incremental relevance feedback (Aal-
bersberg, 1992; Allan, 1996; Iwayama, 2000).
SMART man : This method is SMART?s manual
relevance feedback (with parameters ? = 3,
? = 2, ? = 0). ? is set to 0 because the perfor-
mance is terrible if ? is set to 2.
Table 6 shows the mean average precision of
three methods when the number of expansion terms
changes. Since the number of feedback docu-
ments is extremely small, two methods except for
Okapi sgt get worse than their initial searches.
Okapi man slightly decreases as the number of ex-
pansion terms increases. Contrary, SMART man
do not change so much as the number of expansion
terms increases. Table 7 shows another evaluation
measures with 10 terms expanded. It is clear that
Okapi sgt outperforms the other two methods.
5.4 Comparison with Pseudo Feedback
Methods
We finally compare our query expansion method
with the following pseudo feedback methods.
Okapi pse : This is a pseudo version of Okapi
which assumes top 10 documents in the initial
search as relevant ones as well as TREC-8 set-
tings.
Table 8: Results of Pseudo Feedback Methods
(MAP)
5 10 15 20
Okapi sgt 0.265 0.273 0.274 0.282
Okapi pse 0.253 0.249 0.247 0.246
SMART pse 0.236 0.243 0.242 0.242
Table 9: Results of Pseudo Feedback Methods (10
terms expanded)
P10 P30 RPREC MAP R05P
Okapi sgt 0.520 0.387 0.294 0.273 0.208
Okapi pse 0.478 0.369 0.279 0.249 0.206
SMART pse 0.466 0.359 0.272 0.243 0.187
SMART pse : This is a pseudo version of SMART.
It also assumes top 10 documents as relevant
ones. In addition, it assumes top 500-1000 doc-
uments as non-relevant ones.
In TREC-8, above two methods uses TREC1-5 disks
for query expansion and a phase extraction tech-
nique. However we do not adopt these methods in
our experiments4. Since these methods showed the
highest performance in the TREC-8 adhoc task, it
is reasonable to compare our method with them as
competitors.
Table 8 shows the mean average precision of
three methods when the number of expansion terms
changes. Performance does not differ so much if the
number of expansion terms changes. Okapi sgt out-
performs at any number of expansion. Table 9 shows
the results in other evaluation measures. Okapi sgt
also outperforms except for R05P. In particular, per-
formance in P10 is quite well. It is preferable behav-
ior for the use in practical situations.
6 Discussion
In the experiments, the feedback documents for
Okapi sgt is top ranked ones. However some users
do not select such documents. They may choose
another relevant and non-relevant documents which
rank in top 10. Thus we test an another experiment
where relevant and non-relevant documents are se-
lected randomly from top 10 rank. Table 10 shows
the result. Compared with table 2, the performance
seems to become slightly worse. This shows that a
4Thus the performance in our experiments is a bit worse than
the result of TREC-8
969
Table 10: Results of Okapi sgt with random feed-
back (5 terms expanded)
P10 P30 RPREC MAP R05P
20 0.498 0.372 0.288 0.265 0.222
50 0.456 0.359 0.294 0.268 0.200
100 0.452 0.335 0.270 0.246 0.186
user should select higher ranked documents for rel-
evance feedback.
7 Conclusion
In this paper we proposed a novel query expansion
method which only use the minimum manual judg-
ment. To complement the lack of relevant docu-
ments, this method utilizes the SGT transductive
learning algorithm to predict the relevancy of un-
judged documents. Since the performance of SGT
much depends on an estimation of the fraction of
relevant documents, we propose a method to sam-
ple some good fraction values. We also propose a
method to laps the predictions of multiple SGT tri-
als with above sampled fraction values and try to
differentiate the importance of candidate terms for
expansion in relevant documents. The experimental
results showed our method outperforms other query
expansion methods in the evaluations of several cri-
teria.
References
I. J. Aalbersberg. 1992. Incremental relevance feedback.
In Proceedings of SIGIR ?92, pages 11?22.
J. Allan. 1996. Incremental relevance feedback for infor-
mation filtering. In Proceedings of SIGIR ?96, pages
270?278.
A. Blum and et al 2004. Semi-supervised learning using
randomized mincuts. In Proceedings of ICML 2004.
S. Dumais and et al 2003. Sigir 2003 workshop report:
Implicit measures of user interests and preferences. In
SIGIR Forum.
G. W. Flake and et al 2002. Extracting query modifi-
cation from nonlinear svms. In Proceedings of WWW
2002.
J. He and et al 2004. Manifold-ranking based image
retrieval. In Proceedings of Multimedia 2004, pages
9?13. ACM.
M. Iwayama. 2000. Relevance feedback with a small
number of relevance judgements: Incremental rele-
vance feedback vs. document clustering. In Proceed-
ings of SIGIR 2000, pages 10?16.
T. Joachims. 1999. Transductive inference for text clas-
sification using support vector machines. In Proceed-
ings of ICML ?99.
T. Joachims. 2003. Transductive learning via spectral
graph partitioning. In Proceedings of ICML 2003,
pages 143?151.
A. M. Lam-Adesina and G. J. F. Jones. 2001. Applying
summarization techniques for term selection in rele-
vance feedback. In Proceedings of SIGIR 2001, pages
1?9.
T. Onoda, H. Murata, and S. Yamada. 2004. Non-
relevance feedback document retrieva. In Proceedings
of CIS 2004. IEEE.
S. Oyama and et al 2001. keysword spices: A new
method for building domain-specific web search en-
gines. In Proceedings of IJCAI 2001.
S. E. Robertson. 1990. On term selection for query ex-
pansion. Journal of Documentation, 46(4):359?364.
S. E. Robertson. 1997. Overview of the okapi projects.
Journal of the American Society for Information Sci-
ence, 53(1):3?7.
I. Ruthven. 2003. Re-examining the potential effective-
ness of interactive query expansion. In Proceedings of
SIGIR 2003, pages 213?220.
A. Singhal, S. Abney, B. Bacchiani, M. Collins, D. Hin-
dle, and F. Pereira. 1999. At&t at trec-8.
V Vapnik. 1998. Statistical learning theory. Wiley.
E. Voorhees and D. Harman. 1999. Overview of the
eighth text retrieval conference.
S. Yu and et al 2003. Improving pseud-relevance feed-
back in web information retrieval using web page seg-
mentation. In Proceedings of WWW 2003.
X Zhu and et al 2003. Semi-supervised learning using
gaussian fields and harmonic functions. In Proceed-
ings of ICML 2003, pages 912?914.
970
Empirical Term Weighting and Expansion Frequency 
Kyoji Umemura 
Toyohashi University of Technology 
Toyohashi Aichi 441-8580 Japan 
umemura@tut  i cs .  ru t .  ac .  jp  
Kenneth W. Church 
AT&T Labs-Research 
180 Park Ave., Florham Park, NJ. 
kwc~research ,  art. com 
Abstract 
We propose an empirical method for estimating 
term weights directly from relevance judgements, 
avoiding various standard but potentially trouble- 
some assumptions. It is common to assume, for ex- 
ample, that weights vary with term frequency (t f )  
and inverse document frequency (idf) in a particu- 
lar way, e.g., t f .  idf, but the fact that there are so 
many variants of this formula in the literature sug- 
gests that there remains considerable uncertainty 
about these assumptions. Our method is similar to 
the Berkeley regression method where labeled rel- 
evance judgements are fit as a linear combination 
of (transforms of) t f, idf, etc. Training meth- 
ods not only improve performance, but also ex- 
tend naturally to include additional factors such 
as burstiness and query expansion. The proposed 
histogram-based training method provides a sim- 
ple way to model complicated interactions among 
factors such as t f ,  idf, burstiness and expansion 
frequency (a generalization of query expansion). 
The correct handling of expanded term is realized 
based on statistical information. Expansion fre- 
quency dramatically improves performance from 
a level comparable to BKJJBIDS, Berkeley's en- 
try in the Japanese NACSIS NTCIR-1 evaluation 
for short queries, to the level of JCB1, the top 
system in the evaluation. JCB1 uses sophisti- 
cated (and proprietary) natural anguage process- 
ing techniques developed by Just System, a leader 
in the Japanese word-processing industry. We are 
encouraged that the proposed method, which is 
simple to understand and replicate, can reach this 
level of performance. 
1 In t roduct ion  
An empirical method for estimating term weights 
directly from relevance judgements is proposed. 
The method is designed to make as few assump- 
tions as possible. It is similar to Berkeley's use 
of regression (Cooper et al, 1994) (Chen et al, 
1999) where labeled relevance judgements are fit 
as a linear combination of (transforms of) t f ,  idf, 
etc., but avoids potentially troublesome assump- 
tions by introducing histogram methods. Terms 
are grouped into bins. Weights are computed 
based on the number of relevant and irrelevant 
documents associated with each bin. The result- 
? t: a term 
? d: a document 
? t f ( t ,  d): term freq = # of instances of t in d 
? df(t): doc freq = # of docs d with t f(t ,  d) > 1 
? N: # of documents in collection 
? idf(t): inverse document freq: -log2 d~t) 
? df(t ,  tel, t f0): # of relevant documents d with 
t f(t ,  d) = tfo 
? df(t, rel, tfo): # of irrelevant documents d
with tf(t ,  d) = tfo 
? el(t): expansion frequency = # docs d in 
query expansion with t f ( t ,  d) > 1 
? TF(t): standard notion of frequency in 
corpus-based NLP: TF(t)  = ~d tf(t ,  d) 
? B(t): burstiness: B(t) = 1 iff ~ is large. df(t) 
Table 1: Notation 
ing weights usually lie between 0 and idf, which 
is a surprise; standard formulas like t f .  idf would 
assign values well outside this range. 
The method extends naturally to include ad- 
ditional factors such as query expansion. Terms 
mentioned explicitly in the query receive much 
larger weights than terms brought in via query 
expansion. In addition, whether or not a term 
t is mentioned explicitly in the query, if t ap- 
pears in documents brought in by query expan- 
sion (el(t) > 1) then t will receive a much larger 
weight than it would have otherwise (ef(t) = 0). 
The interactions among these factors, however, are 
complicated and collection dependent. It is safer 
to use histogram methods than to impose unnec- 
essary and potentially troublesome assumptions 
such as normality and independence. 
Under the vector space model, the score for a 
document d and a query q is computed by sum- 
ming a contribution for each term t over an ap- 
propriate set of terms, T. T is often limited to 
terms shared by both the document and the query 
(minus stop words), though not always (e.g, query 
expansion). 
117 
i# 
12.89 
10.87 
9.79 
8.96 
7.75 
6.82 
5.78 
4.74 
3.85 
2.85 
1.78 
0.88 
t /=O t f= l  t / :=2 t f=3 t f>4 
-0.37 9.73 11.69 12.45 13.59 
-0.49 8.00 9.95 11.47 12.06 
-0.86 7.36 9.38 10.63 10.88 
-0.60 6.26 7.99 8.99 9.41 
-0.34 4.62 5.82 6.62 7.98 
-1.26 3.94 6.05 7.59 8.98 
-0.83 3.16 5.17 5.77 7.00 
-0.84 2.46 3.91 4.54 5.58 
-0.60 1.58 2:.76 3.57 4.55 
-1.02 1.00 1.72 2.55 3.96 
-1.33 -0.06 1.05 2.46 4.50 
-0.16 0.17 0.19 -0.10 -0.37 
Table 2: Empirical estimates of A as a function of 
t f  and idf. Terms are assi._~ed to bins based on 
idf. The column labeled idf is the mean idf for 
the terms in each bin. A is estimated separately for 
each bin and each t f  value, based on the labeled 
relevance judgements. 
score~(d, q) = E t/(t,  d) . idf(t) 
tET 
Under the probabilistic retrieval model, docu- 
ments are scored by summing a similar contribu- 
tion for each term t. 
= ~ l P(tJrel) 
In this work, we use A to refer to term weights. 
q) = d, q) 
tET  
This paper will start by showing how to estimate A 
from relevance judgements. Three parameteriza- 
tions will be considered: (1) fit-G, (2) fit-B, which 
introduces burstiness, and (3) fit-E, which intro- 
duces expansion frequency. The evaluation section 
shows that each model improves on the previous 
one. But in addition to performance, we are also 
interested in the interpretations of the parameters. 
2 Superv ised  Tra in ing  
The statistical task is to compute A, our best esti- 
mate of A, based on a training set. This paper will 
use supervised methods where the training mate- 
rials not only include a large number of documents 
but also a few queries labeled with relevance judge- 
ments. 
To make the training task more manageable, it 
is common practice to map the space of all terms 
into a lower dimensional feature space. In other 
words, instead of estimating a different A for each 
term in the vocabulary, we can model A as a func- 
tion of tf and idf and various other features of 
Train 
/ ~4 .~ / 1 ~ 
0 2 4 6 8 10 12 
IDF 
Test 
4 ~ s~- 
. 
~ 2  11  
0 2 4 6 8 10 12 
IDF 
Figure 1: Empirical weights, A. Top panel shows 
values in previous table. Most points fall between 
the dashed lines (lower limit of A = 0 and upper 
limit of A = idf). The plotting character denotes 
t f .  Note that the line with t f  = 4 is above the 
line with t f  = 3, which is above the line with 
t f  = 2, and so on. The higher lines have larger 
intercepts and larger slopes than the lower lines. 
That is, when we fit A ,~, a(tf) + b(tf) ,  idf, with 
separate regression coefficients, a(tf) and b(tf), 
for each value of t f ,  we find that both a(tf) and 
b(tf) increase with t\]. 
terms. In this way, all of the terms in a bin are 
assigned the weight, A. The common practice, 
for example, of assigning t f  ? idf weights can be 
interpreted as grouping all terms with the same 
idf into a bin and assigning them all the same 
weight, namely t f .  idf. Cooper and his colleagues 
at Berkeley (Cooper et al, 1994) (Chen et al, 
1999) have been using regression methods to fit 
as a linear combination of idf , log(t f )  and var- 
ious other features. This method is also grouping 
terms into bins based on their features and assign- 
ing similar weights to terms with similar features. 
In general, term weighting methods that are fit 
to data are more flexible than weighting methods 
that are not fit to data. We believe this additional 
flexibility improves precision and recall (table 8). 
Instead of multiple regression, though, we 
choose a more empirical approach. Parametric as- 
118 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
20 
21 
Description (function of term t) 
df(t, rel,O) _-- # tel does d with t f ( t ,d)  = 0 
dr(t, tel, 1) _= # rel does d with tf(t ,  d) = 1 
dr(t, rel, 2) _= # rel does d with t f(t ,  d) = 2 
df(t, rel,3) ~ # rel does d with t f ( t ,d)  = 3 
df(t, rel,4+) ~ # tel does d with t f ( t ,d)  _> 
dr(t, tel, O) ~ # tel does d with t f(t ,  d) = 0 
dr(t, rel, 1) ~_ # tel does d with t f(t ,  d) = 1 
dr(t, tel, 2) ~ # rel does d with t f(t ,  d) = 2 
where dr(bin, rel, t f )  is 
1 
dr(bin, tel, t f )  ~ Ib/=l ~ df(t, re l , t f )  
tEbin 
Similarly, the denominator can be approximated 
as :  
dr(bin, tel, t \]) P(bin, tfl~) ~ log2 
df ( t ,~ ,3)  -= #reml does d with t / ( t ,d)  = 3 
df(t, rel,4+) ~ # tel does d with t f ( t ,d)  _> 
# tel does d 
# tel does d 
freq of term in corpus: TF(t)  = ~a tf (t ,  d) 
# does d in collection = N 
dff = # does d with t f(t ,  d) _> 1 
where dr(bin, tel, t f )  is 
1 
dff(bin, tel, t / )  ~ Ib/nl ~ dff(t, ~ ,  t f)  
tEbin 
ef  = # does d in query exp. with t f(t ,  d) > 1 ~re t is an estimate of the total number of relevant 
where: D (description), E (query expansion) documents. Since some queries have more rele- 
25 burstiness: B
Table 3: Training file schema: a record of 25 fields 
is computed for each term (ngram) in each query 
in training set. 
sumptions, when appropriate, can be very pow- 
erful (better estimates from less training data), 
but errors resulting from inappropriate assump- 
tions can outweigh the benefits. In this empirical 
investigation of term weighting we decided to use 
conservative non-parametric histogram methods 
to hedge against the risk of inappropriate para- 
metric assumptions. 
Terms are assigned to bins based on features 
such as idf, as illustrated in table 2. (Later we 
will also use B and/or ef  in the binning process.) 
is computed separately for each bin, based on the 
use of terms in relevant and irrelevant documents, 
according to the labeled training material. 
The estimation method starts with a training 
file which indicates, among other things, the num- 
ber of relevant and irrelevant documents for each 
term t in each training query, q. That is, for 
each t and q, we are are given dr(t, rel, tfo) and 
dr(t, tel, tfo), where dr(t, tel, tfo) is the number 
of relevant documents d with tf(t ,  d) = tfo, and 
df(t, rel, tfo) is the number of irrelevant docu- 
ments d with tf(t ,  d) = tfo. The schema for the 
training file is described in table 3. From these 
training observations we wish to obtain a mapping 
from bins to As that can be applied to unseen test 
material. We interpret )~ as a log likelihood ratio: 
, P(bin, tflrel) ~(bin, t / )  = ~og2-z-::-- 
~'\[bin, t / IN )  
where the numerator can be approximated as: 
,.~ _ dr(bin, rel, t f )  P(bin, triter) ~ togs 
Nrel 
vant documents than others, N~t is computed by 
averaging: 
1 
tEbin 
To ensure that Nr~l + ~"~/= N, where N is the 
number of documents in the collection, we define 
This estimation procedure is implemented with 
the simple awk program in figure 2. The awk pro- 
gram reads each line of the training file, which con- 
tains a line for each term in each training query. 
As described in table 3, each training line contains 
25 fields. The first five fields contain dr(t, tel, t f)  
for five values of t f ,  and the next five fields con- 
tain df(t, rel, t f )  for the same five values of t f .  
The next two fields contain N ,a  and N;-~. As the 
awk program reads each of these lines from the 
training file, it assigns each term in each train- 
ing query to a bin (based on \[log2(df)\], except 
when df < 100), and maintains running sums of 
the first dozen fields which are used for comput- 
ing dr(bin, rel, t f),  df(bin, re'---l, tf) ,  l~rret and I~--~ 
for five values of t f .  Finally, after reading all the 
training material, the program outputs the table 
of ks shown in table 2. The table contains a col- 
umn for each of the five t f  values and a row for 
each of the dozen idf bins. Later, we will consider 
more interesting binning rules that make use of 
additional statistics uch as burstiness and query 
expansion. 
2.1 Interpolating Between Bins 
Recall that the task is to apply the ks to new un- 
seen test data. One could simply use the ks in 
table 2 as is. That is, when we see a new term 
in the test material, we find the closest bin in ta- 
ble 2 and report the corresponding ~ value. But 
since the idf of a term in the test set could easily 
fall between two bins, it seems preferable to find 
the two closest bins and interpolate between them. 
119 
awk ' funct ion  log2(x)  { 
re turn  log(x ) / log(2)  } 
$21 - / 'D /  { N = $14; df=$15;  
# binning ru le  
if(df < I00) {bin = O} 
else {bin=int (log2 (dr)) } ; 
docfreq\[bin\] += df; 
Nbin \[bin\] ++; 
# average df(t,rel,tf), df(t,irrel,tf) 
for(i=l;i<=12;i++) n\[i,bin\]+=$i } 
END {for(bin in Nbin) { 
nbin = Nbin\[bin\] 
Nrel = n\[l l ,bin\]/nbin 
Nirrel = N-Nrel 
idf = -log2 ( (docfreq \[bin\]/nbin)/N) 
printf("Y.6.2f ", idf) 
for (i=l ; i<=5 ; i++) { 
if(Nrel==O) prel = 0 
else prel = (n\[i,bin\]/nbin)/Nrel 
if(Nirrel == O) pirrel = 0 
else pirrel = (n\[i+5,bin\]/nbin)/Nirrel 
if(prel <= 0 \]} pirrel <= O) { 
printf "Y.6s ", "NA" } 
else { 
printf "Y.6.2f ", log2(prel/pirrel)} } 
print ""}}' 
Figure 2: awk program for computing ks. 
We use linear regression to interpolate along the 
idf dimension, as illustrated in table 4. Table 4 is 
a smoothed version of table 2 where A ~ a + b.idf. 
There are five pairs of coefficients, a and b, one for 
each value of t f .  
Note that interpolation is generally not neces- 
sary on the t f  dimension because t f  is highly 
quantized. As long as t f  < 4, which it usually 
is, the closest bin is an exact match. Even when 
tff > 4, there is very little room for adjustments if 
we accept he upper limit of A < idf. 
Although we interpolate along the idf dimen- 
sion, interpolation is not all that important along 
that dimension either. Figure 1 shows that the 
differences between the test data and the train- 
ing data dominate the issues that interpolation is
attempting to deal with. The main advantage of 
regression is computational convenience; it is eas- 
ier to compute a + b. idf than to perform a binary 
search to find the closest bin. 
Previous work (Cooper et al, 1994) used mul- 
tiple regression techniques. Although our perfor- 
mance is similar (until we include query expan- 
sion) we believe that it is safer and easier to treat 
each value of t f  as a separate regression for rea- 
sons discussed in table 5. In so doing, we are ba- 
sically restricting the regression analysis to such 
an extent hat it is unlikely to do much harm (or 
much good). Imposing the limits of 0 < A _< idf 
also serves the purpose of preventing the regres- 
sion from wandering too far astray. 
tf a b 
0 -0.95 0.05 
1 -0.98 0.69 
2 -0.15 0.78 
3 0.53 0.81 
4+ 1.32 0.77 
Table 4: Regression coefficients for method fit-G. 
This table approximates the data in table 1 with 
~ a(t f )  + b(t f ) ,  idf. Note that both the inter- 
cepts, a(tf) ,  and the slopes, b(tf), increase with 
t f  (with a minor exception for b(4+)). 
tf 
0 
1 
2 
3 
4 
5 
a(tf) bit/) 
-0.95 0.05 
-0.98 0.69 
-0.15 0.78 
0.53 0.81 
1.32 0.77 
1.32 0.77 
a2 + c2. log(1 + t f )  b2 
-4.1 0.66 
-1.4 0.66 
0.18 0.66 
1.3 0.66 
2.2 0.66 
2.9 0.66 
Table 5: A comparison of the regression coeffi- 
cients for method fit-G with comparable coeffi- 
cients from the multiple regression: A = a2 + b2 ? 
idf + c2 ? log(1 + t f )  where a2 ---- -4.1,  b2 = 0.66 
and c2 = 3.9. The differences in the two fits are 
particularly large when t f  = 0; note that b(0) is 
negligible (0.05) and b2 is quite large (0.66). Re- 
ducing the number of parameters from 10 to 3 in 
this way increases the sum of square errors, which 
may or may not result in a large degradation in 
precision and recall. Why take the chance? 
3 Burs t iness  
Table 6 is like tables 4 but the binning rule not 
only uses idf, but also burstiness (B). Burstiness 
(Church and Gale, 1995)(Katz, 1996)(Church, 
2000) is intended to account for the fact that some 
very good keywords uch as "Kennedy" tend to 
be mentioned quite a few times in a document 
or not at all, whereas less good keywords uch as 
"except" tend to be mentioned about the same 
number of times no matter what the document 
tf  
0 
1 
2 
3 
4+ 
B=0 
a b 
-0.05 -0.00 -0.61 
-1.23 0.63 -0.80 
-0.76 0.71 -0.05 
0.00 0.69 0.23 
0.68 0.71 0.75 
B=i  
a b 
0.02 
0.79 
0.79 
0.82 
0.83 
Table 6: Regression coefficients for method fit-B. 
Note that the slopes and intercepts are larger when 
B = 1 than when B = 0 (except when t f  = 0). 
Even though A usually lies between-0 and idf, we 
restrict A to 0 < A < idf, just to make sure. 
120 
tf ef 
1 0 
2 0 
3 0 
4+ 0 
1 
2 
3 
4+ 
1 
2 
3 
4+ 
2 
2 
2 
2 
1 3 
2 3 
3 3 
4+ 3 
where=D 
a b 
-1.57 0.37 
-3.41 0.82 
-1.30 0.11 
0.40 0.06 
-1.84 0.87 
-2.12 1.10 
-0.66 0.95 
0.84 0.98 
-1.87 0.92 
-1.77 1.12 
-1.72 1.10 
-3.06 1.71" 
-2.52 0.95 
-1.81 1.02 
0.45 0.85 
0.38 1.22 
where=E 
a b 
-2.64 
-2.70 
-2.98 
-3.35 
-3.00 
-2.78 
-3.07 
-3.25 
0.68 
0.71 
0.74 
0.78 
0.86 
0.85 
0.93 
0.79 
-2.71 0.91 
-2.28 0.88 
-2.63 0.97 
-3.66 1.14 
Table 7: Many of the regression coefficients for 
method fit-E. (The coefficients marked with an 
asterisk are worrisome because the bins are too 
small and/or the slopes fall well outside the nor- 
mal range of 0 to 1.) The slopes rarely exceeded .8 
is previous models (fit-G and fit-B), whereas fit-E 
has more slopes closer to 1. The larger slopes are 
associated with robust conditions, e.g., terms ap- 
pearing in the query (where = D), the document 
(t f  > 1) and the expansion (el > 1). If a term 
appears in several documents brought in by query 
? expansion (el > 2), then the slope can be large 
even if the term is not explicitly mentioned in the 
query (where = E). The interactions among t f  , 
idf, ef and where are complicated and not easily 
captured with a straightforward multiple regres- 
sion. 
is about. Since "Kennedy" and "except" have 
similar idf values, they would normally receive 
similar term weights, which doesn't seem right. 
Kwok (1996) suggested average term frequency, 
avtf = TF(t)/df(t),  be used as a tie-breaker for 
cases like this, where TF(t) = ~a if(t ,  d) is the 
standard notion of frequency in the corpus-based 
NLP. Table 6 shows how Kwok's suggestion can 
be reformulated in our empirical framework. The 
table shows the slopes and intercepts for ten re- 
gressions, one for each combination of t f  and B 
(B = 1 iff avtf is large. That is, B = 1 iff 
TF(t)/df(t) > 1.83 - 0.048-idf). 
4 Query  Expans ion  
We applied query expansion (Buckley et al, 1995) 
to generate an expanded part of the query. The 
original query is referred to as the description (D) 
and the new part is referred to as the expansion 
(E). (Queries also contain a narrative (N) part that 
is not used in the experiments below so that our 
results could be compared to previously published 
results.) 
The expansion is formed by applying a base- 
line query engine (fit-B model) to the description 
part of the query. Terms that appear in the top 
k = 10 retrieved ocuments are assigned to the E 
portion of the query (where(t) = E), unless they 
were previously assigned to some other portion of 
the query (e.g., where(t) = D). All terms, t, no 
matter where they appear in the query, also re- 
ceive an expansion frequency el, an integer from 
0 to k = 10 indicating how many of the top k 
documents contain t. 
The fit-E model is: A = a(tf, where, ef) + 
b( t f , where, el) ? i df , where the regression coeffi- 
cients, a and b, not only depend on t f  as in fit-G, 
but also depend on where the term appears in the 
query and expansion frequency el.  We consider 5 
values of t f ,  2 values of where (D and E) and 6 
values of ef  (0, 1, 2, 3, 4 or more). 32 of these 
60 pairs of coefficients are shown in table 7. As 
before, most of the slopes are between 0 and 1. 
is usually between 0 and idf, but we restrict A to 
0 < A < idf, just to make sure. 
In tables 4-7, the slopes usually lie between 0 
and 1. In the previous models, fit-B and fit-G, 
the largest slopes were about 0.8, whereas in fit- 
E, the slope can be much closer to 1. The larger 
slopes are associated with very robust conditions, 
e.g., terms mentioned explicitly in all three areas of 
interest: (1) the query (where = D), (2) the doc- 
ument (t f  > 1) and (3) the expansion (el > 1). 
Under such robust conditions, we would expect o 
find very little shrinking (downweighting to com- 
pensate for uncertainty). 
On the other hand, when the term is not men- 
tioned in one of these areas, there can be quite 
a bit of shrinking. Table 7 shows that the slopes 
are generally much smaller when the term is not 
in the query (where = E) or when the term is 
not in the expansion (el = 0). However, there are 
some exceptions. The bottom right corner of ta- 
ble 7 contains ome large slopes even though these 
terms are not mentioned explicitly in the query 
(where = E). The mitigating factor in this case 
is the large el. If a term is mentioned in several 
documents in the expansion (el _> 2), then it is 
not as essential that it be mentioned explicitly in 
the query. 
With this model, as with fit-G and fit-B, ~ tends 
to increase monotonically with t f  and idf, though 
there are some interesting exceptions. When the 
term appears in the query (where = D) but not 
in the expansion (el = 0), the slopes are quite 
small (e.g., b(3,D,0) = 0.11), and the slopes actu- 
ally decrease as t f  increases (b(2, D, 0) = 0.83 > 
b(3,D,0) = 0.11). We normally expect to see 
slopes of .7 or more when t.f > 3, but in this case 
(b(3, D, 0) = 0.11), there is a considerable shrink- 
ing because we very much expected to see the term 
in the expansion and we d idn' t .  ... 
As we have seen, the interactions among t f, idf, 
e f  and where are complicated and probably de- 
121 
filter trained on sys. 
NA ? JCB1 
2+, El tf, where,ef fit-E 
2 B,tf fit-B 
2, K tf + ... BKJJBIDS 
2, K B,tf fit-B 
2, K tf  fit-G 
2, K none log(1 + t f ) .  idf 
2, K none t f .  idf 
11 
.360 
.354 
.283 
.272 
.264 
.257 
.249 
.112 
Table 8: Training helps: methods above the line 
use training (with the possible xception of JCB1); 
methods below the line do not. 
pend on many factors uch as language, collection, 
typical query patterns and so on. To cope with 
such complications, we believe that it is safer to 
use histogram methods than to try to account for 
all of these interactions at once in a single multiple 
regression. The next section will show that fit-E 
has very encouraging performance. 
5 Experiments 
Two measures of performance are reported: (1) 11 
point average precision and (2) R, precision after 
retrieving Nrd documents, where Nrd is the num- 
ber of relevant documents. We used the "short 
query" condition of the NACSIS NTCIR-1 Test 
Collection (Kando et al, 1999) which consists of 
about 300,000 documents in Japanese, plus about 
30 queries with labeled relevance judgement for 
training and 53 queries with relevance judgements 
for testing. The result of "short query" is shown in 
page 25 of(Kando et al, 1999), which shows that 
"short query" is hard for statistical methods. 
Two previously published systems are included 
in the tables below: JCB1 and BKJJBIDS. JCB1, 
submitted by Just System, a company with a com- 
mercially successful product for Japanese word- 
processing, produced the best results using sophis- 
ticated (and proprietary) natural language pro- 
cessing techniques.(Fujita, 1999) BKJJBIDS used 
Berkeley's logistic regression methods (with about 
half a dozen variables) to fit term weights to the 
labeled training material. 
Table 8 shows that training often helps. The 
methods above the line (with the possible excep- 
tion of JCB1) use training; the methods below the 
line do not. Fit-E has very respectable perfor- 
mance, nearly up to the level of JCB1, not bad for 
a purely statistical method. 
The performance of fit-B is close to that of 
BKJJBIDS. For comparison sake, fit-B is shown 
both with and without the K filter. The K filter 
restricts terms to sequences of Katakana nd Kanji 
characters. BKJJBIDS uses a similar heuristic to 
eliminate Japanese function words. Although the 
K filter does not change performance very much, 
the use of this filter changes the relative order of 
fit-B and BKJJBIDS. These results suggest hat 
R ? 2: restrict terms to bigrams explicitly men- 
.351 tioned in query (where ~- D) 
.363 ? 2+: restrict terms to bigrams, but include 
.293 where = E as well as where = D 
.282 
.282 * W: restrict terms to words, as identified by 
.267 Chasen (Matsumoto et al, 1997) 
.262 ? K: restrict terms to sequences of Katakana 
.138 and/or Kanji characters 
? B: restrict erms to bursty (B -- 1) terms 
? Ek: require terms to appear in more than k 
docs brought in by query expansion (el(t) > 
k). 
Table 9: Filters: results vary somewhat depending 
on these choices, though not too much, which is 
fortunate, since since we don't understand stop 
lists very well. 
filter trained on sys. 
2+, E1 tf, where,ef fit-E 
2+, E2 tf, where,ef fit-E 
2+, E4 tf, where,ef fit-E 
2+ tf, where,ef fit-E 
NA NA JCB1 
11 R 
.354 .363 
.350 .359 
.333 .341 
.332 .366 
.360 .351 
Table 10: The best filters (Ek) improve the per- 
formance of the best method (fit-E) to nearly the 
level of JCB1. 
the K filter is slightly unhelpful. 
A number of filters have been considered (ta- 
ble 9). Results vary somewhat depending on these 
choices, though not too much, which is fortunate, 
since since we don't understand stop lists very 
well. To the extent hat there is a pattern, we sus- 
pect that words axe slightly better than bigrams, 
and that the E filter is slightly better than the B 
filter which is slightly better than the K filter. Ta- 
ble 10 shows that the best filters (Ek) improve the 
performance of the best method (fit-E) to nearly 
the level of JCB1. 
filter sys. UL 
2 fit-B + 
2 fit-B + 
2 fit-B - 
2 fit-B - 
2 fit-G + 
2 fit-G - 
2 fit-G + 
2 fit-G - 
LL I I  
+ .283 
- .280 
+ .280 
- .275 
+ .266 
? .251 
- .248 
- .232 
R 
.293 
.296 
.296 
.288 
.279 
.268 
.259 
.249 
Table 11: Limits do no harm: two limits are 
slightly better than one, and one is  slightly bet- 
ter than none. (UL  = upper limit of ~ < idf; LL 
= lower limit of 0 _< ~) 
122 
The final experiment (table 11) shows that re- 
stricting ~ to 0 < ~ < id\] improves performance 
slightly. The combination of both the upper limit 
and the lower limit is slightly better than just one 
limit which is better than none. We view limits as 
a robustness device. Hopefully, they won't have 
to do much but every once in a while they prevent 
the system from wandering far astray. 
6 Conclusions 
This paper introduced an empirical histogram- 
based supervised learning method for estimating 
term weights, ~. Terms are assigned to bins based 
on features uch as inverse document frequency, 
burstiness and expansion frequency. A different 
is estimated for each bin and each t f  by counting 
the number of relevant and irrelevant documents 
associated with the bin and tff value. Regression 
techniques are used to interpolate between bins, 
but care is taken so that the regression cannot do 
too much harm (or too much good). Three varia- 
tions were considered: fit-G, fit-B and fit-E. The 
performance of query expansion (fit-E) is particu- 
larly encouraging. Using simple purely statistical 
methods, fit-E is nearly comparable to JCB1, a 
sophisticated natural language processing system 
developed by Just System, a leader in the Japanese 
word processing industry. 
.-: In addition to performance, we are also inter- 
ested in the interpretation of the weights. Empiri- 
cal weights tend to lie between 0 and idf. We find 
these limits to be a surprise given that standard 
term weighting formulas uch as t f .  idf generally 
do not conform to these limits. In addition, we 
find that ~ generally grows linearly with idf, and 
that the slope is between 0 and 1. We interpret the 
slope as a statistical shrink. The larger slopes are 
associated with very robust conditions, e.g., terms 
mentioned explicitly in all three areas of interest: 
(1) the query (where = D), (2) the document 
( t f  _> 1) and (3) the expansion (ef > 1). There 
is generally more shrinking for terms brought in 
by query expansion (where = E), but if a term 
is mentioned in several documents in the expan- 
sion (el > 2), then it is not as essential that the 
term be mentioned explicitly in the query. The 
interactions among t f, id\], where, B, el, etc., are 
complicated, and therefore, we have found it safer 
and easier to use histogram methods than to try 
to account for  all of the interactions at once in a 
single multiple regression. 
Acknowdedgement 
Authors thank Prof. Mitchell P. Marcus of Uni- 
versity of Pennsylvania for the valuable discussion 
about noise reduction in context of information 
retrieval. This reseach is supported by Sumitomo 
Electric. 
Re ferences  
Chris Buckley, Gerard Salton, James Allan, and Amit 
Singhal. 1995. Automatic query expansion us- 
ing smart: Trec 3. In The Third Text REtrieval 
Conference(TREC-3), pages 69-80. 
Aitao Chen, Fredric C. Gey, Kazuaki Kishida, Hailing 
Jiang, and Qun Liang. 1999. Comparing multiple 
methods for japanese and japanese-english text re- 
trieval. In NTCIR Workshop 1, pages 49-58, Tokyo 
Japan, Sep. 
Kenneth W. Church and William A. Gale. 1995. 
Poisson mixture. Natural Language Engineering, 
1(2):163-190. 
Kenneth W. Church. 2000. Empirical estimates of 
adaptation: The chance of two noriegas is closer 
to p/2 than p2. In Coling-2000, pages 180-186. 
William S. Cooper, Aitao Chen, and Fredric C. Gey. 
1994. Full text retrieval based on probabilistic equa- 
tion with coefficients fitted by logistic regressions. 
In The Second Text REtrieval Conference(TREU- 
2), pages 57-66. 
Sumio Fujita. 1999. Notes on phrasal index- 
ing: Jscb evaluation experiments at ntcir ad 
hoc'. In NTCIR Workshop 1, pages 101-108, 
http://www.rd.nacsis.ac.jp/ -ntcadm/, Sep. 
Noriko Kando, Kazuko Kuriyama, Toshihiko Nozue, 
Koji Eguchi, and Hiroyuki Katoand Souichiro Hi- 
daka. 1999. Overview of ir tasks at the first nt- 
cir workshop. In NTCIR Workshop 1, pages 11-44, 
http://www.rd.nacsis.ac.jp/ "ntcadm/, Sep. 
Slava M. Katz. 1996. Distribution of content words 
and phrases in text and language modelling. Natural 
Language Engineering, 2(1):15-59. 
K. L. Kwok. 1996. A new method of weighting query 
terms for ad-hoc retrieval. In SIGIR96, pages 187- 
195, Zurich, Switzerland. 
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, 
Yoshitaka Hirano, Osamu Imaichi, and Tomoaki 
Imamura. 1997. Japanese morphological nalysis 
system chasen manual. Technical Report NAIST- 
IS-TR97007, NAIST, Nara, Japan, Feb. 
123 
Selecting the Most Highly Correlated Pairs within a Large Vocabulary
Kyoji Umemura
Department of Computer Science
Toyoahshi University of Technology
umemura@tutics.tut.ac.jp
Abstract
Occurence patterns of words in documents
can be expressed as binary vectors. When
two vectors are similar, the two words cor-
responding to the vectors may have some
implicit relationship with each other. We
call these two words a correlated pair.
This report describes a method for obtain-
ing the most highly correlated pairs of a
given size. In practice, the method re-
quires
 
	
computation time,
and
 
memory space, where

is the
number of documents or records. Since
this does not depend on the size of the
vocabulary under analysis, it is possible
to compute correlations between all the
words in a corpus.
1 Introduction
In order to find relationships between words in a
large corpus or between labels in a large database,
we may use a distance measure between the binary
vectors of

dimensions, where

is the number of
documents or records, and the  th element is 1 if the
 th document/record contains the word or the label,
or 0 otherwise.
There are several distance measures suitable
for this purpose, such as the mutual informa-
tion(Church and Hanks, 1990), the dice coeffi-
cient(Manning and Schueutze 8.5, 1999), the phi
coefficient(Manning and Schuetze 5.3.3, 1999), the
cosine measure(Manning and Schueutze 8.5, 1999)
and the confidence(Arrawal and Srikant, 1995).
There are also special functions for certain applica-
tions, such as then complimentary similarity mea-
sure (CSM)(Hagita and Sawaki, 1995) which is
known as to be suitable for cases with a noisy pat-
tern.
All of these five measures can be obtained from a
simple contingency table. This table has four num-
bers for each word/label  and word/label  . The
first number is the number of documents/records
that have both  and  . We define this number as


ffVery Low-Dimensional Latent Semantic Indexing for Local Query Regions
Yinghui Xu Kyoji Umemura
Toyohashi Unversity of Technology Dept. of Information and Computer Sciences
1-1, Hibarigaoka, Toyohashi, Aichi,Japan
xyh@ss.ics.tut.ac.jp umemura@tutics.tut.ac.jp
Abstract
In this paper, we focus on performing
LSI on very low SVD dimensions. The
results show that there is a nearly linear
surface in the local query region. Using
low-dimensional LSI on local query re-
gion we can capture such a linear surface,
obtain much better performance than
VSM and come comparably to global
LSI. The surprisingly small requirements
of the SVD dimension resolve the com-
putation restrictions. Moreover, on the
condition that several relevant sample
documents are available, application of
low-dimensional LSI to these documents
yielded comparable IR performance to
local RF but in a different manner.
1 Introduction
The increasing size of searchable text collection
poses a great challenge to performing the informa-
tion retrieval (IR) task. Latent Semantic Index-
ing (LSI) is an enhancement of the familiar Vector
Model of IR. It satisfies the IR task through discov-
ering corpus-wide word relationship based on co-
occurrence analysis of a whole collection. LSI has
been successfully applied to various document col-
lections and has achieved favorable results, some-
times outperforming VSM (Dumais, 1996). How-
ever, the principal challenges to applying LSI to
large data collections are the cost of computing and
storing SVD.
Local analysis of the information in a set of top-
ranked documents for the query is one promising
way to solve the computationally demanding IR task
for a large collection. To solve the computational
complexity of LSI, David Hull introduced one inter-
esting method, local LSI, for routing problems(Hull,
1994). The basic idea is: apply the SVD to a set of
documents known to be relevant to the query; then
all the documents in the collection can be folded into
the reduced space of those relevant documents. By
concentrating on the local space around the query re-
sults, we may be able to compute using flexible and
efficient LSI algorithms.
In this paper we put much emphasis on local
dimensionality analysis of the local query regions
filled with relevant documents. In ideal experimen-
tal cases, local LSI involves only the documents
known to be relevant to the query. To our surprise,
in most of our experiments, local LSI obtains its best
IR performance using just one or two SVD dimen-
sions. These interesting results moved us to try per-
forming local LSI with one or two SVD dimensions
on the top return sets of VSM in ad-hoc IR experi-
ments. We found that this worked surprisingly well.
In a practical setting, local LSI may be regarded as a
variation of pseudo relevance feedback (RF). There-
fore, the comparative results with local RF are pro-
vided in this paper as well. The experiments show
that local LSI with one or two SVD dimensions can
contribute to expanding the query information in a
manner different from traditional local RF.
This paper is organized as follows: Section 2 re-
views existing related techniques. Section 3 de-
scribes the implementation architecture of the ex-
periments and gives the experiment results. Section
4 explains the result and points out characteristic of
the local LSI. Section 5 draws the conclusions.
2 Related works
2.1 Latent Semantic Indexing
Latent semantic indexing (Berry et al, 1999) is one
kind of vector-based query-expansion methods that
use neither terms nor documents as the orthogo-
nal basis of a semantic space. Instead, it computes
the most significant orthogonal dimensions in the
term-document matrix of the corpus, via SVD, and
projects documents into the low rank subspace thus
found. LSI then computes semantic similarity based
on the proximity among projected vectors.
LSI uses SVD to factor the term-document
training matrix A into three factors: A =
U?V T = Udiag(?1, ?2, ? ? ? , ?n)V T Where
U = (u1, u2, ? ? ? , um) ? <m?m and V =
(v1, v2, ? ? ? , vn) ? <n?n are unitary matrices (i.e.
UTU = I, V TV = I), whose columns are the
left and the right singular vectors of A respectively,
? ? <m?n is a diagonal matrix whose diagonal ele-
ments are non-negative and arranged in descending
order (?1 ? ?2 ? ? ? ? ? ?k), and p = min(m,n).
The values ?1, ?2, ? ? ? , ?p are known as the singular
values of A, and are the square roots of the eigen-
values of AAT and ATA.Suppose the rank of A is
r, then r ? p and only ?1 ? ?2 ? ? ? ? ? ?r are
positive, while the remaining (p-r), if r<p, singular
values are zero. In LSI retrieval, researchers are only
concerned with the first r singular values of A. LSI
uses the structure from SVD to obtain the reduced-
dimension form of the training matrix A as its ?la-
tent semantic space.? Notation for k ? r, defines the
reduced-dimension form of A to be A = U?V T =
Udiag(?1, ?2, ? ? ? , ?k, 0, ? ? ? , 0)V T . That is, Ak is
obtained by discarding the r-k least significant sin-
gular values and the corresponding left and right
singular vectors of A (since they are now mul-
tiplied by zeros). Then, the first k columns of
U that correspond to the k largest singular values
of A together constitute the projection matrix for
LSI: Sim(~d, ~q) = (ATk ~d) ? (ATk ~q). Analogous to
VSM, the vector representation of a document is
the weighted sum of the vector representation of
its constituent terms. For document vector di and
query vector qi, ATk ~d and ATK~q are now the LSI vec-
tor representations of that document and query, re-
spectively, in the reduced-dimension vector space.
This process is known as ?folding in? documents (or
queries) into the training space. Actually, LSI as-
sumes that the semantic associations among terms
can be found through this one-step analysis of their
statistical usage in the collection, and they are im-
plicitly stored in the singular vectors computed by
SVD.
2.2 Relevance Feedback
A feedback query creation algorithm developed by
Rocchio (Rocchio, 1971) in the mid-1960s has, over
the years, proven to be one of the most successful
profile learning algorithms. The algorithm is based
upon the fact that if the relevance for a query is
known, an optimal query vector will maximize the
average query-document similarity for the relevant
documents, and will simultaneously minimize the
average query-document similarity for non-relevant
documents. Rocchio shows that an optimal query
vector is the difference vector among the centroid
vectors for the relevant and non-relevant documents.
~Qo = 1R
?
D?Rel. ~D ? 1N?R
?
D/?Rel. ~D where R is
the number of relevant documents, and N is the to-
tal number of documents in the collection. Also, all
negative components of the resulting optimal query
are assigned a zero weight. To maintain focus of the
query, researchers have found that it is useful to in-
clude the original user-query in the feedback query
creation process. Also, coefficients have been intro-
duced in Rocchio?s formulation, which control the
contribution of the original query, the relevant docu-
ments, and the non-relevant documents to the feed-
back query. These modifications yield the follow-
ing query reformulation function: ~Qn = ? ? ~Qo +
? ? 1R
?
D?rel
~D ? ? ? 1N?R
?
D/?rel
~D In this paper,
the experiment results based on the local RF were
performed for comparing with the results of Local
LSI. The terms in the query are reweighted using
the Rocchio formula with ? : ? : ? = 1 : 1 : 0.
As for the local information relevant to the query,
they were obtained by extracting several top-ranked
documents through the VSM retrieving process in
the experiments. Jiang has ever used the similar
experiments (VSM+LSI) for Local LSI in his pa-
per ?Approximate Dimension Reduction at NTCIR?
(Fan and Littmen, 2000).
document set (corpus)topic set (query)
    term (m) X doc (n) matrix
1. <docid - document vector>
2. <termid -term vector>   Qids (          )
query vector(   )
preprocess
1. stopword removal.
2. porter's stemming.
3. smart "ltc" tw
doci, docj, ... ,dock
doc1   vector
:       :
docn  vector
creating local query region
through the identified documents
SVD
reduced feature space (          )
organized by singular vectors
LQSXW
GRFYHFWRU
LQSXW
TXHU\YHFWRU\HV
query vector set
Qid1  (t1, t2, ... tr)
:
Qidk  (t1, t2, ... tr)
output the score-list for Qids
and continuefor next query
score table
Qid  Docid  score
QR
relevant sets of Qids
local LSI
local
KA
nkji ?? ,,,1 "
ks ??1
sq
G
Ks ?
( ) ( ) ( )local localT Ts k kscore qid q A d A= GG <
Figure 1: Implementation architecture
3 Experiment Set-up and Results
3.1 Implementation Architecture
Figure 1 shows overall the architecture of the exper-
iment. The procedure is described as follows:
1. Indexing the document collection and query
sets
2. Given a query, retrieving some document by
the relevant sets. In some cases the relevant sets
are derived from the known relevant documents
and in other cases we regarded the top returned
documents as the relevant sets.
3. performing the singular value decomposition
on documents identified in 2.
4. Only a few dimensions for the LSI are retained.
5. Projecting the document vectors and the query
vector into the user-cared feature space, and
then using the standard Cosine measure to get
the final score for this query.
6. Back to the step 2 and continue the analysis on
the next query in the same way.
Step 1 is pre-processing procedure for IR system.
Only the tag removal, upper case characters trans-
verse, stoplist removal and Porter?s stemming were
adopted in this phase (Frakes and Baeza-Yates,
1992). Next, the smart ?ltc? term weighting scheme
(Salton and McGill, 1983) was used to compute the
entries of the term document matrix for the collec-
tion and entries of the query vector. The second step
can be regarded as filter container. In this paper,
the three kinds of routine schemes were performed.
In the first case, the local space for each query was
represented simply by all document vectors, which
have already been judged to be relevant (appearing
in the relevant judgment file). We note that although
it is an ideal case, it may form a useful upper bound
on performance. In the second case, we assume the
condition that the user provides a reasonable num-
ber of relevant documents. In the third case, the lo-
cal space for each query was built on the top return
sets of VSM. The use of the top returned items from
VSM is similar to blind feedback or pseudo RF.
3.2 Characteristic of test collection
There are three test collections in our experiments.
Two of them, Cranfield and Medlars, are small. The
third one is a large-scale test collection, NACSIS.
The Cranfield corpus consists of 1,400 documents
on aerodynamics and 225 queries, while Medlars
consists of 1,033 medical abstracts and 30 queries.
Although these two collections are very small, they
were used extensively in the past by IR researchers.
As for the NACSIS test collection for the IR 1 & 2
(NTCIR 1&NTCIR 2) (Kando, 2001), these docu-
ments are abstracts of academic papers presented at
meetings hosted by 65 Japanese scientists and lin-
guists. In our experiments, the English Monolingual
IR was performed. This collection consists of ap-
proximately 320,000 English documents in NTCIR-
1 and NTCIR-2.
3.3 Local Routine Experiments (Ideal Case)
We first present the experimental results on the ideal
condition. The document vectors already judged to
be relevant to the query were used. SVD calcula-
tion are performed on the local region organized by
Table 1: Results on the Cran., Med. and NTCIR are shown in terms of ave. precision, precision at document
cutoff of 10. Results of the local LSI experiment based on three different SVD dimensions were provided.
Cranfield Medlars NTCIR (E-E) (D run)
K Avr. P-R R-p K Avr. P-R R-p K Avr. P-R R-p
VSM - 0.4148 0.3885 - 0.5306 0.5359 - 0.212 0.2277
+0% +0% +0% +0% +0% +0%
G. 200 0.4543 0.4180 80 0.6680 0.6648 - - -
LSI +9% +0% +8% +0% +26% +0% +25% +0% - - -
1 0.8833 0.8243 1 0.8946 0.8139 1 0.6997 0.6508
+113% +95% +112% +97% +69% +34% +52% +22% +230% +186%
L. 2 0.8607 0.8185 2 0.8769 0.8035 2 0.7062 0.6314
LSI +108% +90% +108% +96% +67% +32% +52% +20% +233% +177%
3 0.8585 0.8102 3 0.8726 0.8019 3 0.6934 0.6293
+107% +90% +108% +96% +68% +30% +51% +20% +228% +176%
these relevant documents with respect to its query.
The IR performance of VSM and global LSI were
regarded as the baseline for comparison. As for the
NTCIR collection, English-English Monolingual IR
was performed and we only extracted the ?D? (De-
scription) field of the topic as the query. Due to
its large size, only the result of VSM is the base-
line. Additionally, to observe the influences of SVD
factors on the IR performance for local LSI exper-
iments, results based on LSI dimension from 1 to
3 were also provided for comparison. As we ex-
pected, the majority of experimental studies are di-
rected towards obtaining better solutions for the lo-
cal routine LSI method. In table 1, K represents
the SVD dimension for LSI analysis. As for the k
value of global LSI, it is the parameter by which
LSI yields the best IR performance. The improve-
ment in the average precision of local routine LSI
is 113, 69 and 233 percent better than that of VSM
on Cranfield, Medlars and NTCIR test collections
respectively. The improvement in average precision
of local routine LSI is 95 and 34 percent better than
that of global LSI on Cranfield with 200 SVD di-
mensions and Medlars with 80 SVD dimensions re-
spectively. Moreover, in the case of SVD factors
equal to 1, we obtain the best IR performance among
all cases on the Cranfield and Medlars. While the
NTCIR collection obtained its best IR performance
with 2 SVD dimensions, there is only a slight dif-
ference between the case with 1 singular vector and
the case with 2. Such small numbers caught our at-
tention, since they indicate that there is a nearly lin-
ear surface in the local region and that the dominant
SVD dimensions can capture such surface and yield
a good IR performance for local LSI analysis.
To clarify how the local LSI space influences IR
performance, we projected the document vectors
onto the extracted local routine LSI space and fig-
ured out the distribution in figure 2. The data of
plots are based on one query from the Medlars col-
lection. Only the largest singular vector was used
for the left plot, and the two largest were used for the
right. Based on the plots, we find that these dimen-
sions do not vary significantly for the non-relevant
documents, Thus, they tend to cluster around the
origin. On the other hand, the relevant document
space illustrates that local SVD factors are designed
to capture their structure.
Since the pre-judged set of documents is generally
not available for the ad-hoc query, In this paper, to
investigate the efficiency of local LSI using very low
dimensions, we continue to do some experiments us-
ing different numbers of relevant documents, which
were selected from the relevant judgment file. The
comparative results based on four cases in which the
SVD factors equal 1, 2 and 3, respectively, were
shown in Table 2. The second column is the condi-
tion, which means that the number of relevant doc-
uments belonging to the analyzing object (query)
should exceed the value in table. Column 3 ?#qry?
0 200 400 600 800 1000 1200
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Doc. ID
in
ne
r p
ro
du
ct
 v
al
ue
 
the distribution of inner product of document vector 
with the largest singular vector
non-relevant document
query
relevant document
0.0 0.1 0.2 0.3 0.4 0.5 0.6
-0.6
-0.4
-0.2
0.0
0.2
0.4
se
co
nd
 fa
ct
or
largest factor
 non-relevant document
 query
 relevant document
Figure 2: Medlars: document collection distribution after represented by the Local region singular vectors.
For the left figure the X-axis is doc.ID and Y are the inner products of the doc vector with the largest singular
vectors. X and Y coordinates on the right are the inner product of the document vectors with the first and
second largest singular vector, respectively.
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Medlars
Av
e.
 p
re
ci
si
on
-re
ca
ll
local LSI (s=20,k=1)
global LSI (k=80)
VSM
0.0 0.2 0.4 0.6 0.8 1.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Cranfield
Av
e.
 p
re
ci
si
on
-re
ca
ll
local LSI (s=3,k=2)
global LSI (k=200)
VSM
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
NTCIR
Av
e.
 p
re
ci
si
on
-re
ca
ll
Local LSI (s=3,k=2)
VSM
Figure 3: the Ave. precision-recall comparison plots between the best run of local LSI with the baseline
VSM and global LSI.
indicates the numbers of queries in the test collec-
tion which satisfy the condition appearing in the sec-
ond column. The fourth column gives the parameter
indicating the number of relevant documents to be
used for creating the local space of the correspon-
dent query. As we expected, local LSI using one or
two SVD dimensions built from the first two singu-
lar vectors resulted in the best IR performance in the
partially ideal experiments. The comparison of the
results was shown in the Table 2.
We know that the most important step in LSI is
the phase of SVD. It requires O(k ? nz2) to find
the k leading eigenvectors. The parameter nz is the
non-zero entries of the term-by-document matrix.
These requirements are unacceptably high for doc-
ument data sets as the non-zero entries number tens
of thousands. According to the LSI analyzing proce-
dure, it includes the SVD phase and the subsequent
projecting treatment. For global LSI, the compu-
tation complexity can be evaluated by: O(nz2k +
#qry ? k2 ? nz2 ? qnz2) k = (100 ? 300)
While our approach can be estimated by: O(#qry?
[(nz2lockloc) + (k2loc ? nz2 ? qnz2)]) k = 1 or 2
In the above equation, ?nzloc? represents the non-
zero entries of the local query region. ?qnz? are
non-zero entities in the query vector. The value of
?nzloc? varies with the number of known relevant
documents. Note that the difference between these
two equations shows clearly that local LSI on small
SVD dimensions is much easier to compute than
global LSI. According to our observation, it is par-
ticularly fast when computing only the largest sin-
gular value.
Based on the above experiments, the interesting
results and the power of the two largest singular vec-
tors prompted us to try putting the local LSI with one
or two singular dimensions into the practical experi-
ments. In this paper, we used the simplest and most
efficient VSM method as the initial retrieval step for
extracting the relevant information around the query.
We assume that the top-ranked documents obtained
by VSM are relevant documents. The details are in-
troduced in section 3.4.
3.4 Ad-hoc local LSI experiment
In this experiment, we note that using the top re-
turned items from VSM is sometimes called blind
feedback or pseudo RF. Hence, we borrow the idea
of local RF. The expanded query representation was
obtained by combining the original query vector
with its projecting result on the local SVD dimen-
sions. The equation for expanding the scheme is as
follows: ~qnew = ~qori+Alock (Alock )T ~qori In the equa-
tion:
Alock = U lock ?lock (V lock )T
Sim(~d, ~qnew) = ~d?(~qori + U lock (?lock )2(U lock )T ~qori)
As for the parameter k, representing the SVD dimen-
sionality of the local region, we set its value equal to
1 or 2 in this experiment. At first, to show that local
LSI on small dimensions works well is a practical
case clearly, we gave the comparable plots between
local LSI with the baseline VSM and global LSI.
The 11ppt. average precision recall plots of local
LSI were figured out for the three test collections in
figure 3. The symbol s in the figure represents the
sample size and k represents the SVD dimension.
To our satisfactions, local LSI based query expan-
sion method does much better than VSM and more
closely approaches the global LSI.
Next, to investigate the effectiveness of low di-
mensional LSI on local query region in restructur-
ing the user cared information space, local RF with
Rocchio?s weights ? : ? : ? = 1 : 1 : 0, as in
Xu and Croft (Xu and Croft, 2000), was used for
comparison. Both of them were used on the same
sample documents. The difference between them is
a twofold one. First, the standard RF formula shown
in section 2.2 make use of weighting parameters for
query expansion, while our approach does not. Sec-
ondly, different combination object was used. The
local RF experiment performed in this paper makes
use of the centroid of the top s returned document
vectors. In our approach, we combine the original
query vector with its projecting results on the low
local SVD space.
Table 3 shows these results in terms of varying
feedback size with one or two SVD dimensions. The
first column ?sample size? in the table is the value of
s according to which we would select the top rank
documents . We see that local LSI outperforms lo-
cal RF for most combinations of sample size and
one or two SVD dimensions in the experiment on
Medlars. The best run on Medlars using local LSI
is 8.4% better than the best in local RF. As for the
best run on Cranfield and on NTCIR, local LSI got
comparable results with the local RF. In the exper-
iments, we note that with the increasing of sample
size, the precision of local LSI decreased more than
that of local RF. Based on our analysis, there are two
reasons for this. First, In the VSM based local LSI
experiments, we assume that the top s documents
from the initial retrieval by VSM are relevant, al-
though that assumption does not always hold. In the
case where the dominant components of the top s
return sets are non-relevant, the maintained SVD di-
mensions would deviate from the orientation that we
preferred. This will influence the following projec-
tion procedure greatly. The average precision-recall
results of VSM on Cranfield and NTCIR is 0.38 and
0.21, respectively. Neither one is ideal. The second
factor is the characteristic of the test collection. The
number of relevant documents for query sets ranges
from 2 to 40 and from 3 to 170 for the Cranfield
and NTCIR, respectively. With such wide range of
query sets, some queries don?t have enough relevant
documents for this strategy to be feasible. There-
fore, from the experiment results, it is still reason-
able for us to believe that if several relevant sample
documents of a query are available, low-dimensional
local LSI will be able to achieve comparable perfor-
mance to local RF.
4 Analysis and discussion
4.1 Local dimensions
One important variable for LSI retrieval is the num-
ber of dimensions in the reduced space. In this pa-
per, we found that one or two SVD dimensions are
able to represent the structure of the local region
that corresponds to the user?s interests. The first two
largest singular vectors will represent the two major
Table 2: Ave. precision-recall comparing results
based on different SVD factors.
Coll. Cond. #qry #sel. SVD Ave.
Rel. fact. P-R
1 0.6857
10 2 0.6667
Cran. >15 27 3 0.6654
(#rel) 1 0.5749
5 2 0.5692
3 0.5641
1 0.7945
10 2 0.8007
Med. >15 25 3 0.7952
(#rel) 1 0.7160
5 2 0.7142
3 0.7137
1 0.3899
10 2 0.3987
NTCIR >15 23 3 0.3967
(#rel) 1 0.2917
5 2 0.2913
3 0.2883
interests. The local SVD dimensions built on them
have the ability to absorb the interests of a query and
have no interest in the non-relevant information. It
indicates that there is near linear surface in the local
query region. That is why local LSI works well on
small dimensions, especially on the condition that
there is only one dominant interests in the query. Of
course, in cases where there is much noisy informa-
tion in the local region, the SVD dimension may fail
to satisfy the true needs of the user. Finally, based
on the experiments in this paper, we would like to
point out that for performing SVD on a particular
local query region, the requirement of the SVD di-
mension should not be demanding. In our opinion,
2 or less is sufficient to obtain ideal IR performance.
4.2 Size of local region
The size of a local region is also one important pa-
rameter for local LSI. We did not do much analysis
on how to determine the best size of the local region
for local LSI. In the absence of any clear guidelines
now, we merely offer some suggestions and an anal-
ysis. The local region should be large enough so that
it will contain more relevant information. However,
there are also several reasons why the local region
should not be too large. Adding a large number of
non-relevant documents of marginal value will only
increase the number of LSI factors needed to de-
scribe the local region without improving their qual-
ity, and this will only degrade the IR performance.
Therefore, as for the size of the local region, it is a
tradeoff. According to the experimental results and
the analysis in section 3.4, since the local LSI does
well on one or two SVD dimensions, so as to avoid
influences of non-relevant information brought by
more involved documents, it is better to restrict the
size of a local region below 30. In the experiments
on Medlars, local LSI produced its best run at 20 top
return documents. Of course, the threshold for the
size of local region should be collection-dependent
and experiment-determined. It may also be possible
to set the threshold by the performance of the initial
retrieval method, but we have not yet analyzed this.
4.3 Advantages
Finally, we would like to point out the advantages
of low-dimensional LSI analysis for local query re-
gion. Our results compared with VSM and global
LSI show clearly that local LSI with low dimensions
performs much better than VSM under some sample
sets and achieves the comparable IR performance to
global LSI. Additionally, because the largest singu-
lar vectors are essential for retrieval performance on
the local query regions, local LSI approaches the
computational complexity of global LSI by using
such small SVD dimensions. Despite the fact that
local LSI has increased the cost of separate SVD
computation for each query, the relative modest re-
quirements of SVD dimension make it feasible for
large scale IR task.
Compared with the local RF method, both the lo-
cal LSI and local RF achieve better results by pro-
viding high-centralized relevant information in the
local region. Provided that relevant sample docu-
ments are used with the same number, local RF is
able to make use of the combination of document
vectors and a heuristic procedure to improve IR per-
formance, while local LSI makes use of SVD to
extract the useful information from the information
space. In some sense, this SVD method is more
comprehensive than local RF.
Table 3: comparative results of Local LSI and Local
relevance feedback on the local region organized by
the return sets of VSM on Med., Cran. and NTCIR,
respectively. The SVD dimension value for the local
LSI is the one from which the best IR performance
was obtained at the specific sample size.
#ss. svd 11 ppt. Ave. P-R R-p
(s) fac. LLSI LRF LLSI LRF
3 2 0.5858 0.5977 0.5760 0.5816
5 1 0.6417 0.6243 0.6300 0.6198
10 1 0.6577 0.6152 0.6431 0.6093
20 1 0.6764 0.6044 0.6393 0.5845
30 1 0.6598 0.5854 0.6246 0.5699
40 2 0.6514 0.5776 0.6157 0.5722
#ss. svd 11 ppt. Ave. P-R R-p
(s) fac. LLSI LRF LLSI LRF
3 2 0.4524 0.4528 0.4206 0.4186
5 2 0.4443 0.4403 0.4203 0.4145
10 2 0.4357 0.4327 0.3981 0.3988
20 2 0.3993 0.4269 0.3571 0.3870
30 2 0.3782 0.4252 0.3345 0.3915
40 2 0.3464 0.4232 0.3106 0.3873
#ss. svd 11 ppt. Ave. P-R R-p
(s) fac. LLSI LRF LLSI LRF
3 2 0.2367 0.2346 0.2380 0.2297
5 2 0.2292 0.2302 0.2341 0.2347
10 2 0.2119 0.2249 0.2205 0.2404
20 2 0.1728 0.2110 0.1800 0.2203
30 2 0.1458 0.2026 0.1575 0.2208
40 2 0.1404 0.1978 0.1470 0.2171
5 Conclusion and future work
In this paper, the results show that very low-
dimensional LSI on the local query region performs
IR task well. Such small dimensional requirements
of local LSI make it more attractive, enabling us to
better address the computation complexity. We can
perform the low-dimensional LSI on several known
relevant document spaces to obtain significant im-
provements in retrieval performance. Moreover,
provided that several relevant sample documents are
available, local LSI using small dimensions obtains
results comparable to the local RF although in a dif-
ferent manner. Our future work will:
1. Continue to study the optimal size of local re-
gion for local LSI so as to automatically deter-
mine it.
2. Find a more efficient initial retrieval method
for obtaining high quality sample sets of each
query.
Acknowledgement
This work was supported in The 21st Century COE
Program ?Intelligent Human Sensing?, from the
ministry of Education, Culture,Sports,Science and
Technology.
References
M. W. Berry, Zlatko Drmax, and Elizabeth R. Jessup.
1999. Matrix, vector space, and information retrieval
(technical report). SIAM Review, 41:335?362.
S. T. Dumais. 1996. Using for information filtering:
Trec-3 experiments. In In Donna K. Harman, editor,
The 3rd Text Retrieval Conference (TREC-3), pages
282?291. Department of Commerce, National Institute
of Standards and Technology.
J. Fan and M. L. Littmen. 2000. Approximate dimension
equalization in vector based information retrieval. In
Proceedings of the Seventeenth International Confer-
ence on Machine Learning. Morgan-Kauffman.
W. B. Frakes and R. Baeza-Yates. 1992. Information
retrieval - Data Structure Algorithms. Prentice Hall,
Englewood Cliffs, New Jersey 07632.
D. Hull. 1994. Improving text retrieval for the routing
problem using latent semantic indexing. In In pro-
ceedings of the 17th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 282?291. Association for com-
puting Machnery.
N. Kando. 2001. Clir syetem evaluation at ntcir work-
shop. National Information (NII) Japan.
J. Rocchio. 1971. Relevance feedback in information
retrieval. In The Smart Retrieval System-Experiments
in Automatic Document Processing, pages 313?323.
Englewood Cliffs, NJ, 1971, Prentice-Hall, Inc.
G. Salton and M. J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill, New York,
NY.
J. Xu and W. B. Croft. 2000. Improving the effec-
tiveness of informational retrieval with local context
analysis. ACM Transactions on Information Systems
(TOIS), 18(1).
Dynamic Programming Matching for Large Scale Information Retrieval
Eiko Yamamoto
Communications Research Laboratory, Kyoto Japan
eiko@crl.go.jp
Masahiro Kishida Yoshinori Takenami
Sumitomo Electric Information Systems Co., Ltd., Osaka Japan
{kishida-masahiro, takenami-yoshinori}@sei.co.jp
Yoshiyuki Takeda Kyoji Umemura
Toyohashi University of Technology, Aichi Japan
{take@ss.ics, umemura@tutics}.tut.ac.jp
Abstract
Though dynamic programming matching
can carry out approximate string matching
when there may be deletions or insertions
in a document, its effectiveness and
efficiency are usually too poor to use it for
large-scale information retrieval. In this
paper, we propose a method of dynamic
programming matching for information
retrieval. This method is as effective as a
conventional information retrieval system,
even though it is capable of approximate
matching. It is also as efficient as a
conventional system.
Keywords: Dynamic programming,
Corpus-based, Japanese.
1 Introduction
The dynamic programming method is well-known
for its ability to calculate the edit distance between
strings. The method can also be applied to informa-
tion retrieval. Dynamic programming matching can
measure the similarity between documents, even if
there are partial deletions or insertions. However,
there are two problems in applying this method to
information retrieval. One problem is search effec-
tiveness. It is poor because dynamic programming
matching lacks an adequate weighting schema. The
second problem is computational efficiency. Also,
lack of an adequate indexing schema means that dy-
namic programming matching usually has to process
the entire document.
Yamamoto et al proposed a method of dynamic
programming matching with acceptable search ef-
fectiveness (Yamamoto et al, 2000; Yamamoto,
Takeda, and Umemura, 2003). They report that
the effectiveness of dynamic programming match-
ing improves by introducing an IDF (Inverse Doc-
ument Frequency) weighting schema for all strings
that contribute similarity. They calculate matching
weights not only for words but also for all strings.
Although they report that effectiveness is improved,
the speed of their method is slower than that of
conventional dynamic programming matching, and
much slower than that of a typical information re-
trieval system.
In this paper, we aim to improve the retrieval ef-
ficiency of the dynamic programming method while
keeping its search effectiveness. From a mathemat-
ical point of view, we have only changed the defini-
tion of the weighting. The mathematical structure of
similarity remains the same as that of the dynamic
programming method proposed by (Yamamoto et
al., 2000; Yamamoto, Takeda, and Umemura, 2003).
Although it has the same definition, the new weight-
ing method makes it possible to build a more effi-
cient information retrieval system by creating the in-
dex in advance. To our surprise, we have observed
that our proposed method is not only more efficient
but also more effective.
2 Similarities Based on Dynamic
Programming Matching
In this section, we introduce several similarities
proposed by (Yamamoto et al, 2000; Yamamoto,
Takeda, and Umemura, 2003). All of them are a
form of dynamic programming matching. These
similarities include translation of the edit distance.
This distance has been described by several authors.
We have adopted Korfhage?s definition: ?the edit
distance is the minimum number of edit operations,
such as insertion and deletion, which are required to
map one string into the other? (Korfhage, 1997).
There are three related similarities. The first is dy-
namic programming matching, which is simply con-
version of the edit distance. The second similarity
is an extension of the first similarity, introducing a
character weighting for each contributing character.
The third and proposed similarity is an extension of
the second one, using string weight instead of char-
acter weight.
2.1 Dynamic Programming Matching
As stated above, dynamic programming (DP)
matching is a conversion of edit distance. We call
this similarity SIM1. While the edit distance (ED) is
a measure of difference, counting different charac-
ters between two strings , SIM1 is a measure of sim-
ilarity, counting matching characters between two
strings. ED and SIM1 are defined as follows:
Definition 2.1 Edit Distance (Korfhage, 1997)
Let ? and ? be strings, x and y be a character, and
?? be empty string.
? If both strings are empty then
ED(??, ??) = 0.0
? If x 6= y then
ED(x, y) = 1.0
? If their first characters are the same then
ED(x?, x?) =
MIN(ED(?, x?), ED(x?, ?),
ED(?, ?) + 1.0)
? Otherwise
ED(x?, y?) =
MIN(ED(?, y?), ED(x?, ?),
ED(?, ?))
Definition 2.2 SIM1
Let ? and ? be strings, x and y be a character, and
?? be empty string.
? If both strings are empty then
SIM1(??, ??) = 0.0
? If x 6= y then
SIM1(x, y) = 0.0
? If their first characters are the same then
SIM1(x?, x?) =
MAX(SIM1(?, x?), SIM1(x?, ?),
SIM1(?, ?) + 1.0)
? Otherwise
SIM1(x?, y?) =
MAX(SIM1(?, y?), SIM1(x?, ?),
SIM1(?, ?))
2.2 Character Weight DP Similarity
SIM1 adds 1.0 to the similarity between two strings
for every matching character, and this value is con-
stant for all the time. Our assumption for the new
function is that different characters make different
contributions. For example, in Japanese informa-
tion retrieval, Hiragana characters are usually used
for functional words and make a different contribu-
tion than Kanji characters, which are usually used
for content words. Thus, it is natural to assign a dif-
ferent similarity weight according to the nature of
the character. The below method of defining Charac-
ter Weight DP Similarity adds not 1.0 but a specific
weight depending on the matching character. We
call this similarity SIM2. It resembles Ukkonen?s
Enhanced Dynamic Programming ASM (Approxi-
mate String Matching) (Berghel and Roach, 1996).
The weight is expressed by a function called Score.
SIM2 is defined as follows:
Definition 2.3 SIM2
Let ? and ? be strings, x and y be a character, and
?? be empty string.
? If both strings are empty then
SIM2(??, ??) = 0.0
? If x 6= y then
SIM2(x, y) = 0.0
? If their first characters are the same then
SIM2(x?, x?) =
MAX(SIM2(?, x?), SIM2(x?, ?),
SIM2(?, ?) + Score(x))
? Otherwise
SIM2(x?, y?) =
MAX(SIM2(?, y?), SIM2(x?, ?),
SIM2(?, ?))
2.3 String Weight DP Similarity
DP procedure usually considers just a single char-
acter at a time, but since some long substrings can
receive good scores, it is natural to consider all pre-
fixes of the longest common prefix, not just the next
character.
While SIM2 uses a character weight whenever a
character matches between strings, a single char-
acter may not be enough. In some cases, even
when each character has a low weight, the string
as a whole may be a good clue for information re-
trieval. For example, ?chirimenjyako? is a Japanese
word that could be a retrieval key word. This word,
which means ?boiled and dried baby sardines,? con-
sists only of Hiragana characters ?chi-ri-me-n-jya-
ko? but each character would make a small contri-
bution in SIM2.
The proposed similarity is called String Weight
DP Similarity, which is a generalization of SIM2.
We call this similarity SIM3. It considers the weight
of all matching strings and is defined as follows:
Definition 2.4 SIM3
Let ? and ? be strings, x and y be a character, and
?? be empty string.
? If both strings are empty then
SIM3(??, ??) = Score(??) = 0.0
? Otherwise
SIM3(?, ?) =
MAX(SIM3s(?, ?), SIM3g(?, ?))
? SIM3s(??, ??) =
MAX(Score(?) + SIM3(??, ??))
where ?(= ??) is the maximum length
string matching from the first character.
? SIM3g(x?, y?) =
MAX(SIM3(?, y?), SIM3(x?, ?),
SIM3(?, ?))
2.4 Weighting Function
Yamamoto et al have used IDF (Inverse Document
Frequency) as a weight for each string. The weight
is computed using a Score function as follows:
Definition 2.5 Yamamoto et al?s Score function
Let ? be string, df(?) the frequency of documents
including ? in the document set for retrieval, and N
be the number of documents in the set.
Score(?) = IDF (?) = ?log(df(?)/N)
The standard one-character-at-a-time DP method
assumes that long matches cannot receive exception-
ally good scores. In other words, it regards Score(?)
as 0 if the length of ? is greater than one. If the
Score function obeys the inequality, Score(??) <
Score(?) + Score(?) for all substrings ? and ?,
the best path would consist of a sequence of sin-
gle characters, and we would not need to consider
long phrases. However, we are proposing a different
Score function. It sometimes assigns good scores to
long phrases, and therefore SIM2 has to be extended
into SIM3 to establish a DP procedure that considers
more than just one character at a time.
3 Proposed Weighting Function
Although SIM3, as shown in Section 2.3, has rea-
sonable effectiveness, its computation is harder than
that of the edit distance, and much harder than that
of the similarity used in a conventional information
retrieval system. In this paper, we have modified
the weighting function so that it keeps its effective-
ness while improving efficiency. To achieve this im-
provement, we use the SIM3 with the same defini-
tion but with a different score function.
3.1 Proposed String Weighting
We reduce the computational cost by limiting strings
that have positive scores. First, we select bigrams as
such strings. In other words, we assign a score of
zero if the length of the string does not equal to 2.
Several language systems use Kanji characters (e.g.
Chinese and Japanese), and bigram is an effective
indexing unit for information retrieval for these lan-
guage systems (Ogawa and Matsuda, 1997). In addi-
tion, we may assume that the contribution of a longer
string is approximated by the total bigram weight-
ing. We have also restricted our attention to infre-
quent bigrams. Thus, we have restricted the weight-
ing function Score as follows, where K is the num-
ber decided by the given query.
? If string length is 2 and cf(?) < K then
Score(?) = ?log(df(?)/N)
? Otherwise Score(?) = 0.0
3.2 Using a Suffix Array for Indexing
Since we have restricted the number of match-
ing strings, and all the matching strings appear in
a query, we can collect all the positions of such
strings. To make it possible, we need some index-
ing in advance. We have used a suffix array for this
index. Below we summarize our proposed algorithm
using a suffix array:
I. Make a suffix array of the document set.
II. For each query,
A. Make a set of substrings consisting of two
characters (bigram).
B. For a given number n, extract the total n of
less frequent bigrams, calculating corpus
frequency.
C. For each bigram from step B,
i. Record all positions in which the bi-
gram appears in the query and docu-
ment set,
ii. Record all documents that contain the
bigram.
D. For each document recorded,
i. Compute the similarity between the
query and the document with SIM3,
using the recorded position of the cor-
responding bigram.
ii. Assign the similarity to the document.
E. Extract the most similar 1000 documents
from the recorded documents as a retrieval
result for the query.
We call the retrieval method described above Fast
Dynamic Programming (FDP). In general, retrieval
systems use indexes to find documents. FDP also
uses an index as a usual method. However, unlike
conventional methods, FDP requires information not
only on the document identification but also on the
position of bigrams.
Manber and Myers proposed a data structure
called ?suffix array.? (Manber and Myers, 1993)
Figure 1 shows an example of suffix array. Each
suffix is expressed by one integer corresponding to
its position. We use this suffix array to find out the
position of selected bigrams. A suffix array can be
created in O(N log(N)) time because we need to
sort all suffixes in alphabetical order. We can get
the position of any string in O(log(N)) time by a
binary search of suffixes and by then obtaining its
corresponding position.
4 Experiment
In the experiment, we compared the proposed FDP
method with SIM1, SIM2, and SIM3, which were
described in Section 2. We measured three values:
Figure 1: Suffix Array
search effectiveness, memory usage, and execution
time.
We used the NTCIR1 collection (NTCIR Project,
1999). This collection consists of 83 retrieval topics
and roughly 330,000 documents of Japanese tech-
nical abstracts. The 83 topics include 30 training
topics (topic01-30); the rest are for testing (topic31-
83). The testing topics were more difficult than the
training topics. Each topic contains five parts, ?TI-
TLE?, ?DESCRIPTION?, ?NARRATIVE?, ?CON-
CEPT?, and ?FIELD.? We retrieved using ?DE-
SCRIPTION,? which is retrieval query and a short
sentence.
All the experiments reported in this section were
conducted using a dual AMD Athlon MP 1900+
with 3GB of physical memory, running TurboLinux
7.0.
4.1 Search Effectiveness
The proposed FDP method restricts the number of
bigrams that can contribute to string matching. That
is, only a small number of strings are considered. It
was not clear whether FDP maintains its effective-
ness like SIM3. To verify it, we compared the effec-
tiveness of FDP with that of SIM1, SIM2, and SIM3.
We also needed to know how the effectiveness might
vary by the number of bigrams. We set number n
at 5, 10, 15, 20, 30, 50, and 500. They were named
FDP5, FDP10, FDP15, FDP20, FDP30, FDP50, and
FDP500, respectively.
Table 1: Statistical Significant Test for difference of MAP (? = 0.005, ? = 83? 1)
SIM2 SIM3 FDP5 FDP10 FDP15 FDP20 FDP30 FDP50 FDP500
SIM1 << << << << << << << << <<
SIM2 << = < << << << << <<
SIM3 = = < << << << <<
FDP5 = << << << << <<
FDP10 = << < < <
FDP15 < = = =
FDP20 = = =
FDP30 = =
FDP50 =
Table 2: Search Effectiveness for Topic01-30
Method 11 pt. average R-precision
SIM1 0.1349 0.1790
SIM2 0.1948 0.2296
SIM3 0.2691 0.3024
FDP5 0.2547 0.2649
FDP10 0.2948 0.3089
FDP15 0.3109 0.3446
FDP20 0.3207 0.3574
FDP30 0.3176 0.3421
FDP50 0.3131 0.3377
FDP500 0.3172 0.3419
The NTCIR1 collection also contains a relevance
judgment. We obtained the 11-point average pre-
cision and R-precision using standard tools called
TRECEVAL. And we tested about statistical signif-
icance for difference of MAP (Mean Average Preci-
sion) (Kishida et al, 2002).
Tables 2 and 3 show the search effectiveness for
all methods. We found that FDP20 is the most ef-
fective. Table 1 shows the results of one-sided t-test
for difference of MAP x?i ? y?i, where x?i and y?i are
MAP of i-th method in the first row and MAP of
i-th method in the first column, respectively. The
level of significance ? is 0.005 and the degree of
freedom ? is 83 ? 1. The Symbols <<,<,= rep-
resent ?much less than ??, ?less than ?, and ?not
less than ??, respectively. We found that except for
FDP5 and FDP10, the other FDPs are significantly
more effective than SIM3 at a level of significance
0.005. In additional, this shows that FDP30, FDP50,
and FDP500 are not significantly more effective than
FDP20. These have demonstrated our proposed FDP
Table 3: Search Effectiveness for Topic31-83
Method 11 pt. average R-precision
SIM1 0.0545 0.0845
SIM2 0.1245 0.1596
SIM3 0.1807 0.2083
FDP5 0.1277 0.1505
FDP10 0.1766 0.2013
FDP15 0.2144 0.2280
FDP20 0.2398 0.2621
FDP30 0.2353 0.2485
FDP50 0.2354 0.2488
FDP500 0.2350 0.2477
method maintains its effectiveness, even though the
strings that contribute similarity are restricted to a
small number of bigrams. Also, it is interesting that
the FDP with 20 bigrams is significantly more effec-
tive than the one with many more bigrams.
4.2 Memory Usage
The proposed method needs to record all the posi-
tions considered bigrams. A memory area is there-
fore required to hold position information; in the
worst case, the memory size required is the prod-
uct of the number of documents and the number of
substrings in a query. This means the memory re-
quirement could be very large. However, using FDP,
we have found that the amount of memory requested
is of a reasonable size.
In other words, the size of the memory area is the
total sum of collection frequency for all strings that
contribute similarity. We examined the amount of
memory used by comparison for the total sum of col-
lection frequency.
020000000
40000000
60000000
80000000
100000000
1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81
Query
Tota
l Co
llect
ion F
requ
ency
AllNgram20BigramAllBigram
Figure 2: Memory Usage (Total Number of Collection Frequency for Each String)
 
0
500000
1000000
1500000
2000000
2500000
3000000
3500000
4000000
4500000
1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81
Query
Tota
l Co
llect
ion 
Freq
uenc
y
20BigramAllBigram
Figure 3: Memory Usage for Different Number of Restricted Bigrams
Figure 2 shows the total sum of collection fre-
quency for three kinds of string sets. In the fig-
ure, AllNgram is for sets of all substrings consid-
ered by SIM3, AllBigram is for sets of all bigrams,
and 20Bigram is for sets of 20 bigrams considered
by FDP20. The field surrounded by the plot line
and the horizontal axis represents the total sum of
collection frequency. As the figure shows, AllBi-
gram and 20Bigram occupy a much smaller field
than AllNgram. This means the memory require-
ment of FDP is much smaller than that of SIM3.
This result shows that FDP is possible to efficiently
perform large-scale information retrieval on a com-
puter with a reasonable amount of memory.
Figure 3 shows enlarged graphs of AllBigram and
20Bigram from Figure 2. The figure shows that
20Bigram equals AllBigram for most queries, but
not always. However, as shown in Table 2 and Ta-
ble 3, FDP20 actually has the highest precision in all
FDPs. This means that considering more bigrams is
not necessarily an advantage. Probably, by choosing
substrings with a high contribution, we manage to
get rid of noisy strings.
4.3 Execution Time
We measured execution time under the same con-
ditions as described in Section 4.1. Notice we im-
plemented SIM1, SIM2, and SIM3 in C language.
On the other hand, FDP is implemented in Java
(JDK1.3.1.04). When we noted the time required
to make a suffix array, we found that FDP took 1.5
times as long as SIM in Figure 4. Thus, for the same
algorithm, the execution speed of Java is generally
slower than that of C language.
Figures 5 and 6 show the time taken to retrieve
for each topic01-30 and topic31-83. In the figures,
the vertical axis is the number of documents, and the
horizontal axis is the execution time. We found that
all SIMs took much longer than FDPs. This demon-
strates that our algorithm in Section 3 sharply im-
proves execution speed. Moreover, we found that
execution time did not increase exponentially even
if the candidate documents for retrieval increased;
instead, the retrieval collection becomes larger and
larger. This suggests that FDP is an effective DP
technique for large-scale information retrieval.
5 Related Work
Our proposed technique is a type of DP matching.
The most typical application of DP matching is gene
information research, because DP is effective for
gene information matching. However, this system
has a very slow processing speed.
In recent years, advances in this field of re-
search have meant that high-speed systems have
been required for gene information retrieval. A
high-speed gene information retrieval system called
BLAST was developed (Setubal and Meidanis,
2001). BLAST has achieved higher processing
speed by using heuristics that specify characteristic
gene arrangements, rather than using DP matching.
In contrast, we have managed to achieve fast match-
ing using the DP technique.
Moreover, in music information retrieval, an error
in copying a tune corresponds to a deficit (deletion)
and insertion of data. For this reason, a music search
engine has been built based on the DP technique (Hu
and Dannenberg, 2002). Since there is a great deal
of music information available these days, scalabil-
ity is also an important problem for music informa-
tion retrieval systems. Our proposed DP method is
scalable and can cope with deficits. It therefore has
potential applications in music information retrieval.
6 Conclusion
In this study, we proposed a DP matching method
for large-scale information retrieval. To improve
its efficiency, this method selects the strings that
contribute more to retrieval. This selection process
reduces the memory requirement and frequency of
memory access. We conclude that our method is
suitable for large-scale information retrieval where
approximate matching is required.
Acknowledgement
This work was supported in The 21st Century COE
Program ?Intelligent Human Sensing,? from the
Ministry of Education, Culture, Sports, Science, and
Technology.
References
Hal Berghel and David Roach. 1996. An extension of
Ukkonen?s enhanced dynamic programming ASM al-
gorithm. Journal of ACM TOIS, 4(1):94?106.
Kazuaki Kishida, Makoto Iwayama, and Koji Eguchi.
2002. Methodology and Pragmatics of Retrieval Ex-
periments at NTCIR Workshop. Pre-meeting Lecture
at the NTCIR-3 Workshop.
Ning Hu and Roger B. Dannenberg. 2002. Comparison
of Melodic Database Retrieval Techniques Using Sung
Queries. Proceedings of JCDL 2002, 301?307.
Robert R. Korfhage. 1997. Information Storage and
Retrieval. WILEY COMPUTER PUBLISHING, 291?
303.
Udi Manber and Gene Myers. 1993. Suffix arrays: a
new method for on-line string searches. SIAM Journal
of Computing, 22(5):935?948.
NTCIR Project. http://research.nii.ac.jp/ntcir/.
Yasushi Ogawa and Toru Matsuda. 1997. Overlapping
statistical word indexing: A new indexing method for
Japanese text. Proceedings of SIGIR97, 226?234.
Joao Carlos Setubal and Joao Meidanis. 2001.
Introduction to Computational Molecular Biology.
BrooksCole Publishing Company.
Eiko Yamamoto, Mikio Yamamoto, Kyoji Umemura, and
Kenneth W. Church. 2000. Dynamic Prgramming:
A Method for Taking Advantage of Technical Ter-
minology in Japanese Documents. Proceedings of
IRAL2000, 125?131.
Eiko Yamamoto, Yoshiyuki Takeda, and Kyoji Umemura.
2003. An IR Similarity Measure which is Tolerant
for Morphological Variation. Journal of Natural Lan-
guage Processing, 10(1):63?80. (in Japanese).
 7.93
17.31 23.37
57.63 108.17
185.42 263.57
391.11 541.42 698.15
856.31 1512.88
10.31
23.24
50.68 81.12
151.05 272.29
354.78 561.57
788.54 1031.59 1279.31 1530.43
1
10
100
1000
10000
5000 10000 20000 30000 50000 80000 100000 150000 200000 250000 300000 allThe Number of Documents
Mak
ing T
ime 
[sec]
SIMFDP
Figure 4: Suffix Array Generation Time
1
10
100
1000
10000
5000 10000 20000 30000 50000 80000 100000 150000 200000 250000 300000 all
The Number of Documents
Exe
cusi
on T
ime 
[sec
]
SIM1 SIM2 SIM3 FDP5 FDP10FDP15 FDP20 FDP30 FDP50
Figure 5: Execution Time for Topic01-30
1
10
100
1000
10000
5000 10000 20000 30000 50000 80000 100000 150000 200000 250000 300000 all
The Number of Documents
Exe
cus
ion 
Tim
e [s
ec]
SIM1 SIM2 SIM3 FDP5 FDP10FDP15 FDP20 FDP30 FDP50
Figure 6: Execution Time for Topic31-83
