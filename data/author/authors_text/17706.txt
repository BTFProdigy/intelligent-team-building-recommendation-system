Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 797?803,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Recognizing Identical Events with Graph Kernels
Goran Glavas? and Jan S?najder
University of Zagreb
Faculty of Electrical Engineering and Computing
Text Analysis and Knowledge Engineering Lab
Unska 3, 10000 Zagreb, Croatia
{goran.glavas,jan.snajder}@fer.hr
Abstract
Identifying news stories that discuss the
same real-world events is important for
news tracking and retrieval. Most exist-
ing approaches rely on the traditional vec-
tor space model. We propose an approach
for recognizing identical real-world events
based on a structured, event-oriented doc-
ument representation. We structure docu-
ments as graphs of event mentions and use
graph kernels to measure the similarity be-
tween document pairs. Our experiments
indicate that the proposed graph-based ap-
proach can outperform the traditional vec-
tor space model, and is especially suitable
for distinguishing between topically simi-
lar, yet non-identical events.
1 Introduction
News stories typically describe real-world events.
Topic detection and tracking (TDT) aims to de-
tect stories that discuss identical or directly related
events, and track these stories as they evolve over
time (Allan, 2002). Being able to identify the sto-
ries that describe the same real-world event is es-
sential for TDT, and event-based information re-
trieval in general.
In TDT, an event is defined as something hap-
pening in a certain place at a certain time (Yang
et al, 1999), while a topic is defined as a set of
news stories related by some seminal real-world
event (Allan, 2002). To identify news stories on
the same topic, most TDT approaches rely on tra-
ditional vector space models (Salton et al, 1975),
as more sophisticated natural language processing
techniques have not yet proven to be useful for
this task. On the other hand, significant advances
in sentence-level event extraction have been made
over the last decade, in particular as the result of
standardization efforts such as TimeML (Puste-
jovsky et al, 2003a) and TimeBank (Pustejovsky
et al, 2003b), as well as dedicated evaluation tasks
(ACE, 2005; Verhagen et al, 2007; Verhagen et
al., 2010). However, these two lines of research
have largely remained isolated from one another.
In this paper we bridge this gap and address
the task of recognizing stories discussing identical
events by considering structured representations
from sentence-level events. More concretely, we
structure news stories into event graphs built from
individual event mentions extracted from text. To
measure event-based similarity of news stories, we
compare their event graphs using graph kernels
(Borgwardt, 2007). We conduct preliminary ex-
periments on two event-oriented tasks and show
that the proposed approach can outperform tradi-
tional vector space model in recognizing identical
real-world events. Moreover, we demonstrate that
our approach is especially suitable for distinguish-
ing between topically similar, yet non-identical
real-world events.
2 Related Work
The traditional vector space model (VSM) (Salton
et al, 1975) computes the cosine between bag-of-
words representations of documents. The VSM is
at the core of most approaches that identify same-
topic news stories (Hatzivassiloglou et al, 2000;
Brants et al, 2003; Kumaran and Allan, 2005;
Atkinson and Van der Goot, 2009). However, it
has been observed that some word classes (e.g.,
named entities, noun phrases, collocations) have
more significance than the others. Among them,
named entities have been considered as particu-
larly important, as they often identify the partici-
pants of an event. In view of this, Hatzivassiloglou
et al (2000) restrict the set of words to be used
for document representation to words constituting
noun phrases and named entities. Makkonen et
797
al. (2004) divide document terms into four seman-
tic categories (locations, temporal expressions,
proper names, and general terms) and construct
separate vector for each of them. Kumaran and
Allan (2004) represent news stories with three dif-
ferent vectors, modeling all words, named-entity
words, and all non-named-entity words occurring
in documents. When available, recognition of
identical events can rely on meta-information as-
sociated with news stories, such as document cre-
ation time (DCT). Atkinson and Van der Goot
(2009) combine DCT with VSM, assuming that
temporally distant news stories are unlikely to de-
scribe the same event.
In research on event extraction, the task of rec-
ognizing identical events is known as event coref-
erence resolution (Bejan and Harabagiu, 2010;
Lee et al, 2012). There, however, the aim is to
identify sentence-level event mentions referring to
the same real-world events, and not stories that
discuss identical events.
3 Kernels on Event Graphs
To identify the news describing the same real-
world event, we (1) structure event-oriented in-
formation from text into event graphs and (2) use
graph kernels to measure the similarity between a
pair of event graphs.
3.1 Event graphs
An event graph is a vertex- and edge-labeled
mixed graph in which vertices represent individ-
ual event mentions and edges represent temporal
relations between event mentions. We adopt a
generic representation of event mentions, as pro-
posed by Glavas? and S?najder (2013): each men-
tion consists of an anchor (a word that conveys
the core meaning) and four types of arguments
(agent, target, time, location). Furthermore, we
consider four types of temporal relations between
event mentions: before, after, overlap, and equal
(Allen, 1983). As relations overlap and equal are
symmetric, whereas before and after are not, an
event graph may contain both directed and undi-
rected edges.
Formally, an event graph G is represented as a
tuple G = (V,E,A,m, r), where V is the set of
vertices, E is the set of undirected edges, A is the
set of directed edges (arcs), m : V ? M is a
bijection mapping the vertices to event mentions,
and r : E ? R is the edge-labeling function, as-
signing temporal relations to edges (cf. Fig. 1).
The construction of an event graph from a news
story involves the extraction of event mentions
(anchors and arguments) and the extraction of
temporal relations between mentions. We use a
supervised model (with 80% F1 extraction perfor-
mance) based on a rich set of features similar to
those proposed by Bethard (2008) to extract event
anchors. We then employ a robust, rule-based ap-
proach proposed by Glavas? and S?najder (2013) to
extract generic event arguments. Finally, we em-
ploy a supervised model (60% micro-averaged F1
classification performance) with a rich set of fea-
tures, similar to those proposed by Bethard (2008),
to extract temporal relations between event men-
tions. A detailed description of the graph con-
struction steps is outside the scope of this paper.
To compute event graph kernels (cf. Section
3.2), we need to determine whether two event
mentions co-refer. To resolve cross-document
event coreference, we use the model proposed
by Glavas? and S?najder (2013). The model de-
termines coreference by comparing factual event
anchors and arguments of four coarse-grained se-
mantic types (agent, target, location, and time),
and achieves an F-score of 67% (79% precision
and 57% recall) on the cross-document mention
pairs from the EventCorefBank dataset (Bejan and
Harabagiu, 2008). In what follows, cf (m1,m2)
denotes whether event mentions m1 and m2 co-
refer (equals 1 if mentions co-refer, 0 otherwise).
3.2 Graph kernels
Graph kernels are fast polynomial alternatives
to traditional graph comparison techniques (e.g.,
subgraph isomorphism), which provide an expres-
sive measure of similarity between graphs (Borg-
wardt, 2007). We employ two different graph ker-
nels: product graph kernel and weighted decom-
position kernel. We chose these kernels because
their general forms have intuitive interpretations
for event matching. These particular kernels have
shown to perform well on a number of tasks from
chemoinformatics (Mahe? et al, 2005; Menchetti
et al, 2005).
Product graph kernel. A product graph kernel
(PGK) counts the common walks between two in-
put graphs (Ga?rtner et al, 2003). The graph prod-
uct of two labeled graphs, G and G? , denoted
GP = G?G?, is a graph with the vertex set
VP =
{
(v, v?) | v ? VG, v? ? VG? , ?(v, v?)
}
798
where ?(v, v?) is a predicate that holds when
vertices v and v? are identically labeled (Ham-
mack et al, 2011). Given event graphs G =
(V,E,A,m, r) and G? = (V ?, E?, A?,m?, r?), we
consider the vertices to be identically labeled if
the corresponding event mentions co-refer, i.e.,
?(v, v?) .= cf (m(v),m?(v?)). The edge set of the
graph product depends on the type of the product.
We experiment with two different products: ten-
sor product and conormal product. In the tensor
product, an edge is introduced iff the correspond-
ing edges exist in both input graphs and the labels
of those edges match (i.e., both edges represent the
same temporal relation). In the conormal product,
an edge is introduced iff the corresponding edge
exists in at least one input graph. Thus, a conor-
mal product may compensate for omitted temporal
relations in the input graphs.
Let AP be the adjacency matrix of the graph
productGP built from input graphsG andG?. The
product graph kernel that counts common walks in
G and G? can be computed efficiently as:
KPG(G,G?) =
|VP |?
i,j=1
[(I ? ?AP )?1]ij (1)
when ? < 1/t , where t is the maximum degree of
a vertex in the graph product GP . In our experi-
ments, we set ? to 1/(t+ 1) .
Weighted decomposition kernel. A weighted
decomposition kernel (WDK) compares small
graph parts, called selectors, being matched ac-
cording to an equality predicate. The importance
of the match is weighted by the similarity of the
contexts in which the matched selectors occur.
For a description of a general form of WDK, see
Menchetti et al (2005).
Let S(G) be the set of all pairs (s, z), where s is
the selector (subgraph of interest) and z is the con-
text of s. We decompose event graphs into individ-
ual vertices, i.e., we define selectors to be the indi-
vidual vertices. In this case, similarly as above, the
equality predicate ?(v, v?) for two vertices v ? G
and v? ? G? holds if and only if the correspond-
ing event mentions m(v) and m?(v?) co-refer. Us-
ing selectors that consist of more than one vertex
would require a more complex and perhaps a less
intuitive definition of the equality predicate ?. The
selector context Zv of vertex v is a subgraph of G
that contains v and all its immediate neighbors. In
other words, we consider as context all event men-
tions that are in a direct temporal relation with the
selected mention. WDK between event graphs G
and G? is computed as:
KWD(G,G?) =
?
v?VG,v??VG?
cf (m(v),m?(v?)) ?(Zv, Z ?v?)
(2)
where ?(Zv, Z ?v?) is the context kernel measuring
the similarity between the context Zv of selector
v ? G and the context Z ?v? of selector v? ? G?.
We compute the context kernel ? as the number of
coreferent mention pairs found between the con-
texts, normalized by the context size:
?(Zv, Z ?v?) =
?
w?VZv ,w??VZ?v?
cf(m(w),m?(w?))
max(|VZv |, |VZ?v? |)
The intuition behind this is that a pair of corefer-
ent mentions m(v) and m?(v?) should contribute
to the overall event similarity according to the
number of pairs of coreferent mentions,m(w) and
m?(w?), that are in temporal relation with v and v?,
respectively.
Graph kernels example. As an example, con-
sider the following two story snippets describing
the same sets of real-world events:
Story 1: A Cezanne masterpiece worth at least $131
million that was the yanked from the wall of a Zurich
art gallery in 2008 has been recovered, Serbian po-
lice said today. Four arrests were made overnight
in connection with the theft, which was one of the
biggest art heists in recent history.
Story 2: Serbian police have recovered a painting
by French impressionist Paul Cezanne worth an esti-
mated 100 million euros (131.7 million U.S. dollars),
media reported on Thursday. The painting ?A boy in
a red vest? was stolen in 2008 from a Zurich museum
by masked perpetrators. Four members of an interna-
tional crime ring were arrested Wednesday.
The corresponding event graphs G and G? are
shown in Fig. 1a and 1b, respectively, while their
product is shown in Fig. 1c. There are three pairs
of coreferent event mentions between G and G?:
(yanked, stolen), (recovered, recovered), and (ar-
rests, arrested). Accordingly, the product graph
P has three nodes. The dashed edge between ver-
tices (yanked, stolen) and (arrests, arrested) exists
only in the conormal product graph. By substi-
tuting into (1) the adjacency matrix and maximum
vertex degree of tensor product graph P , we obtain
799
(a) Event graph G (Story 1) (b) Event graph G? (Story 2) (c) Product graph P
Figure 1: Example event graphs and their product
the tensor PGK score as:
KPG =
3?
i,j=1
?
?
(
I ? 13
(
0 0 1
0 0 1
1 1 0
))?1?
?
i,j
? 5.6
Similarly, for the conormal product graph P we
obtain the conormal PGK score of KPG = 9. By
substitutingG andG? into (2), we obtain the WDK
score as:
KWD =
?
(v,v?)?VP
?(Zv, Z ?v?) =
2
3 +
3
4 +
2
4 ? 1.9
where VP contains pairs of coreferent event men-
tions: (yanked, stolen), (recovered, recovered),
and (arrests, arrested).
4 Experiments
We conducted two preliminary experiments to in-
vestigate whether kernels on event graphs can be
used to recognize identical events.
4.1 Task 1: Recognizing identical events
Dataset. In the first experiment, we classify
pairs of news stories as either describing identical
real-world events or not. For this we need a collec-
tion of stories in which pairs of stories on identi-
cal events have been annotated as such. TDT cor-
pora (Wayne, 2000) is not directly usable because
it has no such annotations. We therefore decided
to build a small annotated dataset.1 To this end,
we use the news clusters of the EMM NewsBrief
service (Steinberger et al, 2009). EMM clusters
news stories from different sources using a docu-
ment similarity score. We acquired 10 randomly
chosen news clusters, manually inspected each of
them, and retained in each cluster only the doc-
uments that describe the same real-world events.
Additionally, we ensured that no documents from
1Datasets for both experiments are available at:
http://takelab.fer.hr/evkernels
Model P R F
Tensor PGK 89.7 82.3 85.8
Conormal PGK 89.3 77.8 83.2
WDK 88.6 73.7 80.5
SVM Graph 91.1 87.6 89.3
SVM Graph + VSM 93.8 96.2 95.0
VSM baseline 90.9 82.9 86.7
Table 1: Results for recognition of identical events
different clusters discuss the same event. To ob-
tain the gold standard dataset, we build all pairs
of documents. The final dataset consists of 64
documents in 10 clusters, with 195 news pairs
from the same clusters (positive pairs) and 1821
news pairs from different clusters (negative pairs).
We divide the dataset into a train and a test set
(7:3 split ratio). Note that, although our dataset
has ground-truth annotations, it is incomplete in
the sense that some pairs of documents describ-
ing the same events, which were not recognized
as such by the EMM, are not included. Further-
more, because EMM similarity score uses VSM
cosine similarity as one of the features, VSM co-
sine similarity constitutes a competitive baseline
on this dataset.
Results. For each graph kernel and the VSM
baseline, we determine the optimal threshold on
the train set and evaluate the classification per-
formance on the test set. The results are given
in Table 1. The precision is consistently higher
than recall for all kernels and the baseline. High
precision is expected, as clusters represent topi-
cally dissimilar events. PGK models (both ten-
sor and conormal) outperform the WDK model,
indicating that common walks correlate better to
event-based document similarity than common
subgraphs. Individually, none of the graph kernels
outperforms the baseline. To investigate whether
the two kernels complement each other, we fed the
800
Original
?Taliban militants have attacked a prison in north-west
Pakistan, freeing at least 380 prisoners. . . ?
Event-preserving paraphrase
?Taliban militants in northwest Pakistan attacked the
prison, liberated at least 380 prisoners . . . ?
Event-shifting paraphrase
?Taliban militants have been arrested in north-west Pak-
istan. At least 380 militants have been arrested. . . ?
Table 2: Event paraphrasing example
individual kernel scores to an SVM model (with
RBF kernel), along with additional graph-based
features such as the number of nodes and the num-
ber of edges (SVM graph model). Finally, we com-
bined the graph-based features with the VSM co-
sine similarity (SVM graph + VSM model). SVM
graph model significantly (at p < 0.05, student?s
2-tailed t-test) outperforms the individual kernel
models and the baseline. The combined model
(SVM graph + VSM) significantly (at p < 0.01)
outperforms the baseline and all kernel models.
4.2 Task 2: Event-based similarity ranking
Dataset. In the second experiment we focus
on the task of distinguishing between news sto-
ries that describe topically very similar, yet dis-
tinct events. For this purpose, we use a small
set of event paraphrases, constructed as fol-
lows. We manually selected 10 news stories from
EMM NewsBrief and altered each of them to
obtain two meaning-preserving (event-preserving)
and two meaning-changing (event-shifting) para-
phrases. To obtain the meaning-preserving para-
phrases, we use Google translate and round-trip
translation via two pairs of arbitrarily chosen lan-
guages (Danish/Finnish and Croatian/Hungarian).
Annotators manually corrected lexical and syn-
tactic errors introduced by the round-trip transla-
tion. To obtain meaning-changing paraphrases, we
asked human annotators to alter each story so that
it topically resembles the original, but describes a
different real-world event. The extent of the al-
teration was left to the annotators, i.e., no specific
transformations were proposed. Paraphrase exam-
ples are given in Table 2. The final dataset consists
of 60 news pairs: 30 positive and 30 negative.
Results. For each method we ranked the pairs
based on the assigned similarity scores. An ideal
method would rank all positive pairs above all neg-
ative pairs. We evaluated the performance using
Model R-prec. Avg. prec.
Tensor PGK 86.7 96.8
Conormal PGK 93.3 97.5
WDK 86.7 95.7
VSM baseline 80.0 77.1
Table 3: Results for event-based similarity ranking
two different rank evaluation metrics: R-precision
(precision at rank 30, as there are 30 positive pairs)
and average precision. The performance of graph
kernel models and the VSM baseline is given in
Table 3. We tested the significance of differences
using stratified shuffling (Yeh, 2000). When con-
sidering average precision, all kernel models sig-
nificantly (at p < 0.01) outperform the baseline.
However, when considering R-precision, only the
conormal PGK model significantly (at p < 0.05)
outperforms the baseline. There is no statistical
significance in performance differences between
the considered kernel methods. Inspection of the
rankings reveals that graph kernels assign very low
scores to negative pairs, i.e., they distinguish well
between textual representations of topically simi-
lar, but different real-world events.
5 Conclusion
We proposed a novel approach for recognizing
identical events that relies on structured, graph-
based representations of events described in a
document. We use graph kernels as an expres-
sive framework for modeling the similarity be-
tween structured events. Preliminary results on
two event-similarity tasks are encouraging, indi-
cating that our approach can outperform tradi-
tional vector-space model, and is suitable for dis-
tinguishing between topically very similar events.
Further improvements could be obtained by in-
creasing the accuracy of event coreference resolu-
tion, which has a direct influence on graph kernels.
The research opens up many interesting direc-
tions for further research. Besides a systematic
evaluation on larger datasets, we intend to inves-
tigate the applications in event tracking and event-
oriented information retrieval.
Acknowledgments
This work has been supported by the Ministry of
Science, Education and Sports, Republic of Croa-
tia under the Grant 036-1300646-1986. We thank
the reviewers for their constructive comments.
801
References
ACE. 2005. Evaluation of the detection and recogni-
tion of ACE: Entities, values, temporal expressions,
relations, and events.
James Allan. 2002. Topic Detection and Tracking:
Event-based Information Organization, volume 12.
Kluwer Academic Pub.
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Martin Atkinson and Erik Van der Goot. 2009. Near
real time information mining in multilingual news.
In Proceedings of the 18th International Conference
on World Wide Web, pages 1153?1154. ACM.
Cosmin Adrian Bejan and Sanda Harabagiu. 2008. A
linguistic resource for discovering event structures
and resolving event coreference. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation (LREC 2008).
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1412?1422. Association for Com-
putational Linguistics.
Steven Bethard. 2008. Finding Event, Temporal and
Causal Structure in Text: A Machine Learning Ap-
proach. Ph.D. thesis, University of Colorado at
Boulder.
Karsten Michael Borgwardt. 2007. Graph Ker-
nels. Ph.D. thesis, Ludwig-Maximilians-Universita?t
Mu?nchen.
Thorsten Brants, Francine Chen, and Ayman Farahat.
2003. A system for new event detection. In Pro-
ceedings of the 26th Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, pages 330?337. ACM.
Thomas Ga?rtner, Peter Flach, and Stefan Wrobel.
2003. On graph kernels: Hardness results and ef-
ficient alternatives. In Learning Theory and Kernel
Machines, pages 129?143. Springer.
Goran Glavas? and Jan S?najder. 2013. Exploring coref-
erence uncertainty of generically extracted event
mentions. In Proceedings of 14th International
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 408?422. Springer.
Richard Hammack, Wilfried Imrich, and Sandi
Klavz?ar. 2011. Handbook of Product Graphs. Dis-
crete Mathematics and Its Applications. CRC Press.
Vasileios Hatzivassiloglou, Luis Gravano, and Anki-
needu Maganti. 2000. An investigation of linguistic
features and clustering algorithms for topical doc-
ument clustering. In Proceedings of the 23rd An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 224?231. ACM.
Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detec-
tion. In Proceedings of the 27th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 297?304.
ACM.
Giridhar Kumaran and James Allan. 2005. Using
names and topics for new event detection. In Pro-
ceedings of the Conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 121?128. Association for
Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489?500. Association for Computational Linguis-
tics.
Pierre Mahe?, Nobuhisa Ueda, Tatsuya Akutsu, Jean-
Luc Perret, and Jean-Philippe Vert. 2005. Graph
kernels for molecular structure-activity relationship
analysis with support vector machines. Journal
of Chemical Information and Modeling, 45(4):939?
951.
Juha Makkonen, Helena Ahonen-Myka, and Marko
Salmenkivi. 2004. Simple semantics in topic detec-
tion and tracking. Information Retrieval, 7(3):347?
368.
Sauro Menchetti, Fabrizio Costa, and Paolo Frasconi.
2005. Weighted decomposition kernels. In Pro-
ceedings of the 22nd International Conference on
Machine Learning, pages 585?592. ACM.
James Pustejovsky, Jose? Castano, Robert Ingria, Roser
Sauri, Robert Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir Radev. 2003a. Timeml: Robust
specification of event and temporal expressions in
text. New Directions in Question Answering, 3:28?
34.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003b. The TimeBank corpus. In Cor-
pus Linguistics, volume 2003, page 40.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
802
Ralf Steinberger, Bruno Pouliquen, and Erik Van
Der Goot. 2009. An introduction to the euro-
pean media monitor family of applications. In Pro-
ceedings of the Information Access in a Multilin-
gual World-Proceedings of the SIGIR 2009 Work-
shop, pages 1?8.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 Task 15: TempEval tempo-
ral relation identification. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 75?80. Association for Computational Lin-
guistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 Task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62. Association for Computational Linguistics.
Charles Wayne. 2000. Multilingual topic detection
and tracking: Successful research enabled by cor-
pora and evaluation. In Proceedings of the Second
International Conference on Language Resources
and Evaluation Conference (LREC 2000), volume
2000, pages 1487?1494.
Yiming Yang, Jaime G Carbonell, Ralf D Brown,
Thomas Pierce, Brian T Archibald, and Xin Liu.
1999. Learning approaches for detecting and track-
ing news events. Intelligent Systems and their Ap-
plications, IEEE, 14(4):32?43.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th Conference on Computational
linguistics, pages 947?953. Association for Compu-
tational Linguistics.
803
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 18?23,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Aspect-Oriented Opinion Mining from User Reviews in Croatian
Goran Glava?? Damir Korenc?ic?? Jan ?najder?
?University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
?Rud?er Bo?kovic? Institute, Department of Electronics
Bijenic?ka cesta 54, 10000 Zagreb, Croatia
{goran.glavas,jan.snajder}@fer.hr damir.korencic@irb.hr
Abstract
Aspect-oriented opinion mining aims to
identify product aspects (features of prod-
ucts) about which opinion has been ex-
pressed in the text. We present an approach
for aspect-oriented opinion mining from
user reviews in Croatian. We propose meth-
ods for acquiring a domain-specific opinion
lexicon, linking opinion clues to product
aspects, and predicting polarity and rating
of reviews. We show that a supervised ap-
proach to linking opinion clues to aspects
is feasible, and that the extracted clues and
aspects improve polarity and rating predic-
tions.
1 Introduction
For companies, knowing what customers think of
their products and services is essential. Opinion
mining is being increasingly used to automatically
recognize opinions about products in natural lan-
guage texts. Numerous approaches to opinion min-
ing have been proposed, ranging from domain-
specific (Fahrni and Klenner, 2008; Qiu et al, 2009;
Choi et al, 2009) to cross-domain approaches (Wil-
son et al, 2009; Taboada et al, 2011), and from
lexicon-based methods (Popescu and Etzioni, 2007;
Jijkoun et al, 2010; Taboada et al, 2011) to ma-
chine learning approaches (Boiy and Moens, 2009;
Go et al, 2009).
While early attempts focused on classifying
overall document opinion (Turney, 2002; Pang et
al., 2002), more recent approaches identify opin-
ions expressed about individual product aspects
(Popescu and Etzioni, 2007; Fahrni and Klenner,
2008; Mukherjee and Liu, 2012). Identifying opin-
ionated aspects allows for aspect-based comparison
across reviews and enables opinion summarization
for individual aspects. Furthermore, opinionated
aspects may be useful for predicting overall review
polarity and rating.
While many opinion mining systems and re-
sources have been developed for major languages,
there has been considerably less development for
less prevalent languages, such as Croatian. In this
paper we present a method for domain-specific,
aspect-oriented opinion mining from user reviews
in Croatian. We address two tasks: (1) identifica-
tion of opinion expressed about individual product
aspects and (2) predicting the overall opinion ex-
pressed by a review. We assume that solving the
first task successfully will help improve the perfor-
mance on the second task. We propose a simple
semi-automated approach for acquiring domain-
specific lexicon of opinion clues and prominent
product aspects. We use supervised machine learn-
ing to detect the links between opinion clues (e.g.,
excellent, horrible) and product aspects (e.g., pizza,
delivery). We conduct preliminary experiments on
restaurant reviews and show that our method can
successfully pair opinion clues with the targeted
aspects. Furthermore, we show that the extracted
clues and opinionated aspects help classify review
polarity and predict user-assigned ratings.
2 Related Work
Aspect-based opinion mining typically consists
of three subtasks: sentiment lexicon acquisition,
aspect-clue pair identification, and overall review
opinion prediction. Most approaches to domain-
specific sentiment lexicon acquisition start from a
manually compiled set of aspects and opinion clues
and then expand it with words satisfying certain
co-occurrence or syntactic criteria in a domain-
specific corpus (Kanayama and Nasukawa, 2006;
Popescu and Etzioni, 2007; Fahrni and Klenner,
2008; Mukherjee and Liu, 2012). Kobayashi et
18
al. (2007) extract aspect-clue pairs from weblog
posts using a supervised model with parts of de-
pendency trees as features. Kelly et al (2012)
use a semi-supervised SVM model with syntactic
features to classify the relations between entity-
property pairs. Opinion classification of reviews
has been approached using supervised text cate-
gorization techniques (Pang et al, 2002; Funk et
al., 2008) and semi-supervised methods based on
the similarity between unlabeled documents and a
small set of manually labeled documents or clues
(Turney, 2002; Goldberg and Zhu, 2006).
Sentiment analysis and opinion mining ap-
proaches have been proposed for several Slavic
languages (Chetviorkin et al, 2012; Buczynski and
Wawer, 2008; Smr?, 2006; Smailovic? et al, 2012).
Methods that rely on translation, using resources
developed for major languages, have also been pro-
posed (Smr?, 2006; Steinberger et al, 2012). Thus
far, there has been little work on opinion mining
for Croatian. Glava? et al (2012) use graph-based
algorithms to acquire a sentiment lexicon from a
newspaper corpus. Agic? et al (2010) describe a
rule-based method for detecting polarity phrases
in financial domain. To the best of our knowledge,
our work is the first that deals with aspect-oriented
opinion mining for Croatian.
3 Aspect-Oriented Opinion Mining
Our approach consists of three steps: (1) acquisi-
tion of an opinion lexicon of domain-specific opin-
ion clues and product aspects, (2) recognition of
aspects targeted by opinion clues, and (3) predic-
tion of overall review polarity and opinion rating.
The linguistic preprocessing includes sentence
segmentation, tokenization, lemmatization, POS-
tagging, and dependency parsing. We use the in-
flectional lexicon from ?najder et al (2008) for
lemmatization, POS tagger from Agic? et al (2008),
and dependency parser from Agic? (2012). As we
are dealing with noisy user-generated text, prior to
any of these steps, we use GNU Aspell tool1 for
spelling correction.
Step 1: Acquisition of the opinion lexicon. We
use a simple semi-automatic method to acquire
opinion clues and aspects. We identify candidates
for positive clues as lemmas that appear much more
frequently in positive than in negative reviews (we
determine review polarity based on user-assigned
1http://aspell.net/
rating). Analogously, we consider as negative
clue candidates lemmas that occur much more fre-
quently in negative than in positive reviews. As-
suming that opinion clues target product aspects,
we extract as aspect candidates all lemmas that
frequently co-occur with opinion clues. We then
manually filter out the false positives from the lists
of candidate clues and aspects.
Unlike some approaches (Popescu and Etzioni,
2007; Kobayashi et al, 2007), we do not require
that clues or aspects belong to certain word cate-
gories or to a predefined taxonomy. Our approach
is pragmatic ? clues are words that express opin-
ions about aspects, while aspects are words that
opinion clues target. For example, we treat words
like stic?i (to arrive) and sve (everything) as aspects,
because they can be targets of opinion clues, as in
?pizza je stigla kasno" (?pizza arrived late") and
?sve super!" (?everything?s great!").
Step 2: Identifying opinionated aspects. We
aim to pair in each sentence the aspects with the
opinion clues that target them. For example, in
?dobra pizza, ali lazanje su u?asne" (?good pizza,
but lasagna was terrible"), the clue dobra (good)
should be paired with the aspect pizza, and u?asne
(terrible) should be paired with lazanje (lasagne).
In principle, the polarity of an opinion is deter-
mined by both the opinion clue and the aspect. At
an extreme, an aspect can invert the prior polarity
of an opinion clue (e.g., ?cold pizza" has a negative,
whereas ?cold ice-cream" has a positive polarity).
However, given that no such cases occurred in our
dataset, we chose not to consider this particular
type of inversion. On the other hand, the polarity
of an opinion may be inverted explicitly by the use
of negations. To account for this, we use a very
simple rule to recognize negations: we consider an
aspect-clue pair to be negated if there is a negation
word within a?3 token window of the opinion clue
(e.g., ?pizza im nikad nije hladna" ? ?their pizza is
never cold").
To identify the aspect-clue pairs, we train a super-
vised model that classifies all possible pairs within
a sentence as either paired or not paired. We use
four sets of features:
(1) Basic features: the distance between the as-
pect and the clue (in number of tokens); the number
of aspects and clues in the sentence; the sentence
length (in number of tokens); punctuation, other
aspects, and other clues in between the aspect and
the clue; the order of the aspect and the clue (i.e.,
19
which one comes before);
(2) Lexical features: the aspect and clue lemmas;
bag-of-words in between the aspect and the clue; a
feature indicating whether the aspect is conjoined
with another aspect (e.g., ?pizza i sendvic? su bili
izvrsni" ? ?pizza and sandwich were amazing");
a feature indicating whether the clue is conjoined
with another clue (e.g., ?velika i slasna pizza" ?
?large and delicious pizza");
(3) Part-of-speech features: POS tags of the as-
pect and the clue word; set of POS tags in between
the aspect and the clue; set of POS tags preced-
ing the aspect/clue; set of POS tags following the
aspect/clue; an agreement of gender and number
between the aspect and the clue;
(4) Syntactic dependency features: dependency
relation labels along the path from the aspect to the
clue in the dependency tree (two features: a con-
catenation of these labels and a set of these labels);
a feature indicating whether the given aspect is syn-
tactically the closest to the given clue; a feature
indicating whether the given clue is syntactically
the closest to given aspect.
Step 3: Predicting overall review opinion. We
use extracted aspects, clues, and aspect-clue pairs
to predict the overall review opinion. We consider
two separate tasks: (1) prediction of review po-
larity (positive or negative) and (2) prediction of
user-assigned rating that accompanies a review. We
frame the first task as a binary classification prob-
lem, and the second task as a regression problem.
We use the following features for both tasks:
(1) Bag-of-word (BoW): the standard tf-idf
weighted BoW representation of the review;
(2) Review length: the number of tokens in the
review (longer reviews are more likely to contain
more opinion clues and aspects);
(3) Emoticons: the number of positive (e.g.,
?:)?) and negative emoticons (e.g., ?:(?);
(4) Opinion clue features: the number and the
lemmas of positive and negative opinion clues;
(5) Opinionated aspect features: the number and
the lemmas of positively and negatively opinion-
ated aspects.
4 Evaluation
For experimental evaluation, we acquired a
domain-specific dataset of restaurant reviews2 from
2Available under CC BY-NC-SA license from
http://takelab.fer.hr/cropinion
(HR) Zaista za svaku pohvalu! Jelo su nam dostavili
15 minuta ranije. Naruc?ili smo pizzu koja je bila
prepuna dodataka, dobro pec?ena, i vrlo ukusna.
(EN) Really laudable! Food was delivered 15 minutes
early. We ordered pizza which was filled with ex-
tras, well-baked, and very tasteful.
Rating: 6/6
Table 1: Example of a review (text and rating)
Pauza.hr,3 Croatia?s largest food ordering website.
The dataset contains 3310 reviews, totaling about
100K tokens. Each review is accompanied by an
opinion rating on a scale from 0.5 (worst) to 6
(best). The average user rating is 4.5, with 74%
of comments rated above 4. We use these user-
assigned ratings as gold-standard labels for super-
vised learning. Table 1 shows an example of a
review (clues are bolded and aspects are under-
lined). We split the dataset into a development and
a test set (7:3 ratio) and use the former for lexicon
acquisition and model training.
Experiment 1: Opinionated aspects. To build
a set on which we can train the aspect-clue pair-
ing model, we sampled 200 reviews from the de-
velopment set and extracted from each sentence
all possible aspect-clue pairs. We obtained 1406
aspect-clue instances, which we then manually la-
beled as either paired or not paired. Similarly for
the test set, we annotated 308 aspect-clue instances
extracted from a sample of 70 reviews. Among
the extracted clues, 77% are paired with at least
one aspect and 23% are unpaired (the aspect is
implicit).
We trained a support vector machine (SVM) with
radial basis kernel and features described in Section
3. We optimized the model using 10-fold cross-
validation on the training set. The baseline assigns
to each aspect the closest opinion clue within the
same sentence. We use stratified shuffling test (Yeh,
2000) to determine statistical significance of per-
formance differences.
Results are shown in Table 2. All of our
supervised models significantly outperform the
closest clue baseline (p < 0.01). The Ba-
sic+Lex+POS+Synt model outperforms Basic
model (F-score difference is statistically significant
at p < 0.01), while the F-score differences between
Basic and both Basic+Lex and Basic+Lex+POS
are pairwise significant at p < 0.05. The F-score
3http://pauza.hr/
20
Model Precision Recall F1
Baseline 31.8 71.0 43.9
Basic 77.2 76.1 76.6
Basic+Lex 78.1 82.6 80.3
Basic+Lex+POS 80.9 79.7 80.3
Basic+Lex+POS+Synt 84.1 80.4 82.2
Table 2: Aspect-clue pairing performance
Review polarity Review rating
Model Pos Neg Avg r MAE
BoW 94.1 79.1 86.6 0.74 0.94
BoW+E 94.4 80.3 87.4 0.75 0.91
BoW+E+A 95.7 85.2 90.5 0.80 0.82
BoW+E+C 95.7 85.6 90.7 0.81 0.79
BoW+E+A+C 96.0 86.2 91.1 0.83 0.76
E ? emoticons; A ? opinionated aspects; C ? opinion clues
Table 3: Review polarity and rating performance
differences between Basic+Lex, Basic+Lex+POS,
and Basic+Lex+POS+Synt are pairwise not statis-
tically significant (p < 0.05). This implies that
linguistic features increase the classification per-
formance, but there are no significant differences
between models employing different linguistic fea-
ture sets. We also note that improvements over the
Basic model are not as large as we expected; we
attribute this to the noisy user-generated text and
the limited size of the training set.
Experiment 2: Overall review opinion. We
considered two models: a classification model for
predicting review polarity and a regression model
for predicting user-assigned rating. We trained the
models on the full development set (2276 reviews)
and evaluated on the full test set (1034 reviews).
For the classification task, we consider reviews
rated lower than 2.5 as negative and those rated
higher than 4 as positive. Ratings between 2.5 and
4 are mostly inconsistent (assigned to both positive
and negative reviews), thus we did not consider
reviews with these ratings. For classification, we
used SVM with radial basis kernel, while for re-
gression we used support vector regression (SVR)
model. We optimized both models using 10-fold
cross-validation on the training set.
Table 3 shows performance of models with dif-
ferent feature sets. The model with bag-of-words
features (BoW) is the baseline. For polarity classi-
fication, we report F1-scores for positive and nega-
tive class. For rating prediction, we report Pearson
correlation (r) and mean average error (MAE).
The models that use opinion clue features
(BoW+E+C) or opinionated aspect features
(BoW+E+A and BoW+E+A+C) outperform the
baseline model (difference in classification and re-
gression performance is significant at p < 0.05
and p < 0.01, respectively; tested using stratified
shuffling test). This confirms our assumption that
opinion clues and opinionated aspects improve the
prediction of overall review opinion. Performance
on negative reviews is consistently lower than for
positive reviews; this can be ascribed to the fact
that the dataset is biased toward positive reviews.
Models BoW+E+A and BoW+E+C perform simi-
larly (the difference is not statistically significant at
p < 0.05), suggesting that opinion clues improve
the performance just as much as opinionated as-
pects. We believe this is due to (1) the existence of
a considerable number (23%) of unpaired opinion
clues (e.g., u?asno (terrible) in ?Bilo je u?asno!"
(?It was terrible!")) and (2) the fact that most opin-
ionated aspects inherit the prior polarity of the clue
that targets them (also supported by the fact the
BoW+E+A+C model does not significantly outper-
form the BoW+E+C nor the BoW+E+A models).
Moreover, note that, in general, user-assigned rat-
ings may deviate from the opinions expressed in
text (e.g., because some users chose to comment
only on some aspects). However, the issue of an-
notation quality is out of scope and we leave it for
future work.
5 Conclusion
We presented a method for aspect-oriented opinion
mining from user reviews in Croatian. We proposed
a simple, semi-automated approach for acquiring
product aspects and domain-specific opinion clues.
We showed that a supervised model with linguistic
features can effectively assign opinions to the in-
dividual product aspects. Furthermore, we demon-
strated that opinion clues and opinionated aspects
improve prediction of overall review polarity and
user-assigned opinion rating.
For future work we intend to evaluate our
method on other datasets and domains, varying
in level of language complexity and correctness.
Of particular interest are the domains with aspect-
focused ratings and reviews (e.g., electronic prod-
uct reviews). Aspect-based opinion summarization
is another direction for future work.
21
Acknowledgments
This work has been supported by the Ministry of
Science, Education and Sports, Republic of Croatia
under the Grant 036-1300646-1986 and Grant 098-
0982560-2566.
References
?eljko Agic?, Marko Tadic?, and Zdravko Dovedan.
2008. Improving part-of-speech tagging accuracy
for Croatian by morphological analysis. Informat-
ica, 32(4):445?451.
?eljko Agic?, Nikola Ljube?ic?, and Marko Tadic?. 2010.
Towards sentiment analysis of financial texts in
Croatian. In Nicoletta Calzolari, editor, Proceed-
ings of the Seventh conference on International Lan-
guage Resources and Evaluation (LREC?10), Val-
letta, Malta. European Language Resources Associ-
ation (ELRA).
?eljko Agic?. 2012. K-best spanning tree dependency
parsing with verb valency lexicon reranking. In Pro-
ceedings of 24th international Conference on Com-
putational Linguistics (COLING 2012): Posters,
pages 1?12.
Erik Boiy and Marie-Francine Moens. 2009. A
machine learning approach to sentiment analysis
in multilingual web texts. Information retrieval,
12(5):526?558.
Aleksander Buczynski and Aleksander Wawer. 2008.
Shallow parsing in sentiment analysis of product re-
views. In Proceedings of the Partial Parsing work-
shop at LREC, pages 14?18.
Ilia Chetviorkin, Pavel Braslavskiy, and Natalia
Loukachevich. 2012. Sentiment analysis track at
romip 2011. Dialog.
Yoonjung Choi, Youngho Kim, and Sung-Hyon
Myaeng. 2009. Domain-specific sentiment analysis
using contextual feature generation. In Proceedings
of the 1st international CIKM workshop on Topic-
sentiment analysis for mass opinion, pages 37?44.
ACM.
Angela Fahrni and Manfred Klenner. 2008. Old wine
or warm beer: Target-specific sentiment analysis of
adjectives. In Proc. of the Symposium on Affective
Language in Human and Machine, AISB, pages 60?
63.
Adam Funk, Yaoyong Li, Horacio Saggion, Kalina
Bontcheva, and Christian Leibold. 2008. Opin-
ion analysis for business intelligence applications.
In Proceedings of the first international workshop
on Ontology-supported business intelligence, page 3.
ACM.
Goran Glava?, Jan ?najder, and Bojana Dalbelo Ba?ic?.
2012. Semi-supervised acquisition of Croatian sen-
timent lexicon. In Text, Speech and Dialogue, pages
166?173. Springer.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
Andrew B Goldberg and Xiaojin Zhu. 2006. See-
ing stars when there aren?t many stars: Graph-based
semi-supervised learning for sentiment categoriza-
tion. In Proceedings of the First Workshop on Graph
Based Methods for Natural Language Processing,
pages 45?52. Association for Computational Lin-
guistics.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating focused topic-
specific sentiment lexicons. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 585?594. Association for
Computational Linguistics.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?06, pages 355?363,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Colin Kelly, Barry Devereux, and Anna Korhonen.
2012. Semi-supervised learning for automatic con-
ceptual property extraction. In Proceedings of the
3rd Workshop on Cognitive Modeling and Com-
putational Linguistics, CMCL ?12, pages 11?20,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
1065?1074.
Arjun Mukherjee and Bing Liu. 2012. Aspect ex-
traction through semi-supervised modeling. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Long Papers-
Volume 1, pages 339?348. Association for Computa-
tional Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79?86. Asso-
ciation for Computational Linguistics.
Ana-Maria Popescu and Oren Etzioni. 2007. Extract-
ing product features and opinions from reviews. In
Natural language processing and text mining, pages
9?28. Springer.
22
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through dou-
ble propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
pages 1199?1204.
Jasmina Smailovic?, Miha Grc?ar, and Martin ?nidar?ic?.
2012. Sentiment analysis on tweets in a financial do-
main. In Proceedings of the 4th Jozef Stefan Inter-
national Postgraduate School Students Conference,
pages 169?175.
Pavel Smr?. 2006. Using WordNet for opinion mining.
In Proceedings of the Third International WordNet
Conference, pages 333?335. Masaryk University.
Jan ?najder, Bojana Dalbelo Ba?ic?, and Marko Tadic?.
2008. Automatic acquisition of inflectional lexica
for morphological normalisation. Information Pro-
cessing & Management, 44(5):1720?1731.
Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova,
Ralf Steinberger, Hristo Tanev, Silvia V?zquez, and
Vanni Zavarella. 2012. Creating sentiment dictio-
naries via triangulation. Decision Support Systems.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Peter D Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th an-
nual meeting on association for computational lin-
guistics, pages 417?424. Association for Computa-
tional Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational linguistics, 35(3):399?433.
Alexander Yeh. 2000. More accurate tests for the
statistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics-Volume 2, pages 947?953. Association
for Computational Linguistics.
23
Proceedings of the TextGraphs-8 Workshop, pages 1?5,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Event-Centered Information Retrieval Using Kernels on Event Graphs
Goran Glavas? and Jan S?najder
University of Zagreb
Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
{goran.glavas,jan.snajder}@fer.hr
Abstract
Traditional information retrieval models as-
sume keyword-based queries and use unstruc-
tured document representations. There is
an abundance of event-centered texts (e.g.,
breaking news) and event-oriented informa-
tion needs that often involve structure that
cannot be expressed using keywords. We
present a novel retrieval model that uses a struc-
tured event-based representation. We struc-
ture queries and documents as graphs of event
mentions and employ graph kernels to measure
the query-document similarity. Experimental
results on two event-oriented test collections
show significant improvements over state-of-
the-art keyword-based models.
1 Introduction
The purpose of an information retrieval (IR) system is
to retrieve the documents relevant to user?s informa-
tion need expressed in the form of a query. Many in-
formation needs are event-oriented, while at the same
time there exists an abundance of event-centered texts
(e.g., breaking news, police reports) that could satisfy
these needs. Furthermore, event-oriented information
needs often involve structure that cannot easily be
expressed with keyword-based queries (e.g., ?What
are the countries that President Bush has visited and
in which has his visit triggered protests??). Tradi-
tional IR models (Salton et al, 1975; Robertson and
Jones, 1976; Ponte and Croft, 1998) rely on shal-
low unstructured representations of documents and
queries, making no use of syntactic, semantic, or
discourse level information. On the other hand, mod-
els utilizing structured event-based representations
have not yet proven useful in IR. However, signifi-
cant advances in event extraction have been achieved
in the last decade as the result of standardization ef-
forts (Pustejovsky et al, 2003) and shared evaluation
tasks (Verhagen et al, 2010), renewing the interest
in structured event-based text representations.
In this paper we present a novel retrieval model
that relies on structured event-based representation
of text and addresses event-centered queries. We
define an event-oriented query as a query referring
to one or more real-world events, possibly includ-
ing their participants, the circumstances under which
the events occurred, and the temporal relations be-
tween the events. We account for such queries by
structuring both documents and queries into event
graphs (Glavas? and S?najder, 2013b). The event
graphs are built from individual event mentions ex-
tracted from text, capturing their protagonists, times,
locations, and temporal relations. To measure the
query-document similarity, we compare the corre-
sponding event graphs using graph kernels (Borg-
wardt, 2007). Experimental results on two news story
collections show significant improvements over state-
of-the-art keyword-based models. We also show that
our models are especially suitable for retrieval from
collections containing topically similar documents.
2 Related Work
Most IR systems are a variant of the vector space
model (Salton et al, 1975), probabilistic model
(Robertson and Jones, 1976), or language model
(Ponte and Croft, 1998), which do not account for
associations between query terms. Recent models in-
troduce co-occurrence-based (Park et al, 2011) and
syntactic (Shinzato et al, 2012) dependencies. How-
ever, these dependencies alone in most cases cannot
capture in sufficient detail the semantics of events.
A more comprehensive set of dependencies can be
modeled with graph-based representations. Graph-
1
based IR approaches come in two flavors: (1) the
entire document collection is represented as a sin-
gle graph in which queries are inserted as additional
vertices (Mihalcea and Tarau, 2004); (2) each query
and each document are represented as graphs of con-
cepts, and the relevance of a document for a query is
determined by comparing the corresponding graphs
(Montes-y Go?mez et al, 2000). Our approach fits
into the latter group but we represent documents as
graphs of events rather than graphs of concepts. In
NLP, graph kernels have been used for question type
classification (Suzuki, 2005), cross-lingual retrieval
(Noh et al, 2009), and recognizing news stories on
the same event (Glavas? and S?najder, 2013b).
Event-based IR is addressed explicitly by Lin et
al. (2007), who compare predicate-argument struc-
tures extracted from queries to those extracted from
documents. However, queries have to be manually
decomposed into semantic roles and can contain only
a single predicate. Kawahara et al (2013) propose a
similar approach and demonstrate that ranking based
on semantic roles outperforms ranking based on syn-
tactic dependencies. Both these approaches target the
problem of syntactic alternation but do not consider
the queries made of multiple predicates, such as those
expressing temporal relations between events.
3 Kernels on Event Graphs
Our approach consists of two steps. First, we con-
struct event graphs from both the document and the
query. We then use a graph kernel to measure the
query-document similarity and rank the documents.
3.1 Event Graphs
An event graph is a mixed graph in which vertices rep-
resent the individual event mentions and edges repre-
sent temporal relations between them. More formally,
an event graph is a tuple G = (V,E,A,m, r), where
V is the set of vertices, E is the set of undirected
edges, A is the set of directed edges, m : V ? M
maps the vertices to event mentions, and r : E ? R
assigns temporal relations to edges.
We use a generic representation of a factual event
mention, which consists of an event anchor and event
arguments of four coarse types (agent, target, time,
and location) (Glavas? and S?najder, 2013a; Glavas?
and S?najder, 2013b). We adopt the set of temporal
relations used in TempEval-2 (Verhagen et al, 2010)
(before, after, and overlap), with additional temporal
equivalence relation (equal).
To build an event graph, we first extract the event
mentions and then extract the temporal relations be-
tween them. To extract the event anchors, we use
a supervised model based on a rich feature set pro-
posed by Glavas? and S?najder (2013b), performing
at 80% F1-score. We then use a robust rule-based
approach from Glavas? and S?najder (2013a) to extract
event arguments. Finally, we extract the temporal
relations using a supervised model with a rich fea-
ture set proposed by Glavas? and S?najder (2013b).
Relation classification performs at 60% F1-score.
To compute the product graph kernels, we must
identify event mentions from the query that corefer
with mentions from the document. To this end, we
employ the model from Glavas? and S?najder (2013a),
which compares the anchors and four types of argu-
ments between a pair of event mentions. The model
performs at 67% F-score on the EventCorefBank
dataset (Bejan and Harabagiu, 2008).
3.2 Product Graph Kernels
Graph kernels provide an expressive measure of sim-
ilarity between graphs (Borgwardt, 2007). In this
work, we use product graph kernel (PGK), a type of
random walk graph kernel that counts the common
walks between two graphs (Ga?rtner et al, 2003).
Product graph. The graph product of two labeled
graphs, G and G
?
, denoted GP = G?G?, is a graph
with the vertex set
VP =
{
(v, v?) | v ? VG, v
? ? VG? , ?(v, v
?)
}
where predicate ?(v, v?) holds iff vertices v and v? are
identically labeled (Hammack et al, 2011). Vertices
of event graphs have the same label if the event men-
tions they denote corefer. The edge set of the product
is conditioned on the type of the graph product. In the
tensor product, an edge exists in the product iff the
corresponding edges exist in both input graphs and
have the same label, i.e., denote the same temporal
relation. In the conormal product, an edge is intro-
duced iff the corresponding edge exists in at least one
input graph. A conormal product may compensate
for omitted temporal relations in the input graphs but
may introduce spurious edges that do not represent
2
(a) Query graph (b) Document graph (c) Tensor product (d) Conormal product
Figure 1: Examples of event graphs and their products
true overlap between queries and documents. Fig. 1
shows an example of input graphs and their products.
PGK computation. The PGK for input graphs G
and G? is computed as
kPG(G,G
?) =
|VP |?
i,j=1
[(I ? ?AP )
?1]ij
provided ? < 1/d , where d is the maximum vertex
degree in the product graph GP with the adjacency
matrix AP . In experiments, we set ? to 1/(d+ 1) .
PGK suffers from tottering (Mahe? et al, 2005), a phe-
nomenon due to the repetition of edges in a random
walk. A walk that totters between neighboring ver-
tices produces an unrealistically high similarity score.
To prevent tottering between neighboring vertices,
Mahe? et al (2005) transform the input graphs before
computing the kernel score on their product: each
edge (vi, vj) is converted into a vertex ve; the edge it-
self gets replaced with edges (ve, vi) and (ve, vj). We
experiment with Mahe? extension for PGK, account-
ing for the increased probability of one-edge-cycle
tottering due the small size of query graphs.
4 Experiments
Test Collections and Queries. To the best of our
knowledge, there is no standard test collection avail-
able for event-centered IR that we could use to evalu-
ate our models. Thus, we decided to build two such
test collections, with 50 queries each: (1) a general
collection of topically diverse news stories and (2) a
topic-specific collection of news on Syria crisis. The
first collection contains 25,948 news stories obtained
from EMM News Brief, an online news clustering
service.1 For the topic-specific collection, we se-
lected from the general collection 1387 documents
that contain the word ?Syria? or its derivations.
1http://emm.newsbrief.eu
General collection (news stories)
q1: An ICT giant purchased the phone maker after the
government approved the acquisition
q2: The warship tried to detain Chinese fishermen but
was obstructed by the Chinese vessels
Topic-specific collection (Syria crisis)
q3: Syrian forces killed civilians, torched houses, and
ransacked stores, overrunning a farmer village
q4: Rebels murdered many Syrian soldiers and the gov-
ernment troops blasted the town in central Syria
Table 1: Example queries from the test collection
For each collection we asked an annotator to com-
pile 50 queries. She was instructed to select at ran-
dom a document from the collection, read the docu-
ment carefully, and compile at least one query con-
sisting of at least two event mentions, in such a way
that the selected document is relevant for the query.
Example queries are shown in Table 1. For instance,
query q1 (whose corresponding event graph is shown
in Fig. 1a) was created based on the following docu-
ment (whose event graph is shown in Fig. 1b):
Google Inc. won approval from Chinese regula-
tors for its $12.5 billion purchase of Motorola
Mobility Holdings Inc., clearing a final hurdle
for a deal that boosts its patents portfolio. . .
Relevance judgments. To create relevance judg-
ments, we use the standard IR pooling method with
two baseline retrieval models ? a TF-IDF weighted
vector space model (VSM) and a language model.
Our graph-based model was not used for pooling be-
cause of time limitations (note that this favors the
baseline models because pool-based evaluation is
biased against models not contributing to the pool
(Bu?ttcher et al, 2007)). Given that EMM News Brief
builds clusters of related news and that most EMM
3
Collection
Model General Specific
Baselines TF-IDF VSM 0.335 0.199
Hiemstra LM 0.300 0.175
In expC2 0.341 0.188
DFR BM25 0.332 0.192
Graph-based Tensor 0.502 0.407
Conormal 0.434 0.359
Mahe? Tensor 0.497 0.412
Mahe? Conormal 0.428 0.362
Table 2: Retrieval performance (MAP)
clusters contain less than 50 news stories, we esti-
mate that there are at most 50 relevant documents per
query. To get an even better estimate of recall, for
each query we pooled the union of top 75 documents
retrieved by each of the two baseline models.
One annotator made the relevance judgments for
all queries. We asked another annotator to provide
judgments for two randomly chosen queries and ob-
tained perfect agreement, which confirmed our intu-
ition that determining relevance for complex event-
centered queries is not difficult. The average number
of relevant documents per query in the general and
topic-specific collection is 12 and 8, respectively.2
Results. Table 2 shows the mean average preci-
sion (MAP) on both test collections for four graph
kernel-based models (tensor/conormal product and
with/without Mahe? extension). We compare our
models to baselines from the three traditional IR
paradigms: a TF-IDF-weighted cosine VSM, the
language model of Hiemstra (2001), and the best-
performing models from the probabilistic Divergence
from Randomness (DFR) framework (In expC2 and
DFR BM25) (Amati, 2003; Ounis et al, 2006). We
evaluate these models using the Terrier IR platform.3
Overall, all models perform worse on the topic-
specific collection, in which all documents are topi-
cally related. Our graph kernel models outperform
all baseline models (p<0.01 for tensor models and
p<0.05 for conormal models; paired student?s t-test)
on both collections, with a wider margin on topic-
specific than on the general collection. This result
2Available at http://takelab.fer.hr/data
3http://terrier.org
[?1;?0.1](?0.1; 0] (0; 0.1] (0.1; 0.3] (0.3; 1]
0
5
10
15
Figure 2: Histogram of AP differences
suggests that the graph-based models are especially
suitable for retrieval over topic-specific collections.
There is no significant difference between the ten-
sor product and conormal product models, indicating
that the conormal product introduces spurious edges
more often than it remedies for incorrect extraction
of temporal relations. The performance differences
due to Mahe? extension are not significant, providing
no conclusive evidence on the effect of tottering.
To gain more insights into the performance of our
event graph-based model, we analyzed per query
differences in average precision between our best-
performing model (Tensor) and the best-performing
baseline (In expC2) on queries from the general col-
lection. Fig. 2 shows the histogram of differences.
Our graph kernel-based model outperforms the base-
line on 42 out of 50 queries. A closer inspection
of the eight queries on which our model performs
worse than the baseline reveals that this is due to (1)
an important event mention not being extracted from
the query (2 cases) or a (2) failure in coreference
resolution between an event mention from the query
and a mention from the document (6 cases).
5 Conclusion and Perspectives
We presented a graph-based model for event-centered
information retrieval. The model represents queries
and documents as event graphs and ranks the docu-
ments based on graph kernel similarity. The experi-
ments demonstrate that for event-based queries our
graph-based model significantly outperforms state-of-
the-art keyword-based retrieval models. Our models
are especially suitable for topic-specific collections,
on which traditional IR models perform poorly.
An interesting topic for further research is the ex-
tension of the model with other types of dependen-
cies between events, such as entailment, causality,
4
and structural relations. Another direction concerns
the effective integration of event graph-based and
keyword-based models. We will also consider ap-
plications of event graphs on other natural language
processing tasks such as text summarization.
Acknowledgments. This work has been supported
by the Ministry of Science, Education and Sports,
Republic of Croatia under the Grant 036-1300646-
1986. We thank the reviewers for their comments.
References
Giambattista Amati. 2003. Probability models for infor-
mation retrieval based on divergence from randomness.
Ph.D. thesis, University of Glasgow.
Cosmin Adrian Bejan and Sanda Harabagiu. 2008. A
linguistic resource for discovering event structures and
resolving event coreference. In Proc. of the LREC
2008.
Karsten Michael Borgwardt. 2007. Graph Kernels. Ph.D.
thesis, Ludwig-Maximilians-Universita?t Mu?nchen.
Stefan Bu?ttcher, Charles LA Clarke, Peter CK Yeung, and
Ian Soboroff. 2007. Reliable information retrieval
evaluation with incomplete and biased judgements. In
Proc. of the ACM SIGIR, pages 63?70. ACM.
Thomas Ga?rtner, Peter Flach, and Stefan Wrobel. 2003.
On graph kernels: Hardness results and efficient alterna-
tives. In Learning Theory and Kernel Machines, pages
129?143. Springer.
Goran Glavas? and Jan S?najder. 2013a. Exploring coref-
erence uncertainty of generically extracted event men-
tions. In Proc. of the CICLing 2013, pages 408?422.
Springer.
Goran Glavas? and Jan S?najder. 2013b. Recognizing iden-
tical events with graph kernels. In Proc. of the ACL
2013, pages 797?803.
Richard Hammack, Wilfried Imrich, and Sandi Klavz?ar.
2011. Handbook of Product Graphs. Discrete Mathe-
matics and Its Applications. CRC Press.
Djoerd Hiemstra. 2001. Using language models for infor-
mation retrieval. Taaluitgeverij Neslia Paniculata.
Daisuke Kawahara, Keiji Shinzato, Tomohide Shibata, and
Sadao Kurohashi. 2013. Precise information retrieval
exploiting predicate-argument structures. In Proc. of
the IJCNLP 2013. In press.
Chia-Hung Lin, Chia-Wei Yen, Jen-Shin Hong, Samuel
Cruz-Lara, et al 2007. Event-based textual document
retrieval by using semantic role labeling and corefer-
ence resolution. In IADIS International Conference
WWW/Internet 2007.
Pierre Mahe?, Nobuhisa Ueda, Tatsuya Akutsu, Jean-Luc
Perret, and Jean-Philippe Vert. 2005. Graph kernels
for molecular structure-activity relationship analysis
with support vector machines. Journal of Chemical
Information and Modeling, 45(4):939?951.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing order into texts. In Proc. of the EMNLP 2004,
volume 4. Barcelona, Spain.
Manuel Montes-y Go?mez, Aurelio Lo?pez-Lo?pez, and
Alexander Gelbukh. 2000. Information retrieval with
conceptual graph matching. In Database and Expert
Systems Applications, pages 312?321. Springer.
Tae-Gil Noh, Seong-Bae Park, Hee-Geun Yoon, Sang-Jo
Lee, and Se-Young Park. 2009. An automatic transla-
tion of tags for multimedia contents using folksonomy
networks. In Proc. of the ACM SIGIR 2009, pages
492?499. ACM.
Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He,
Craig Macdonald, and Christina Lioma. 2006. Terrier:
A high performance and scalable information retrieval
platform. In Proceedings of the OSIR Workshop, pages
18?25.
Jae Hyun Park, W Bruce Croft, and David A Smith. 2011.
A quasi-synchronous dependence model for informa-
tion retrieval. In Proc. of the 20th ACM International
Conference on Information and Knowledge Manage-
ment, pages 17?26. ACM.
Jay Ponte and Bruce Croft. 1998. A language modeling
approach to information retrieval. In Proc. of the ACM
SIGIR, pages 275?281. ACM.
James Pustejovsky, Jose? Castano, Robert Ingria, Roser
Sauri, Robert Gaizauskas, Andrea Setzer, Graham Katz,
and Dragomir Radev. 2003. TimeML: Robust specifi-
cation of event and temporal expressions in text. New
Directions in Question Answering, 3:28?34.
Stephen E Robertson and K Sparck Jones. 1976. Rele-
vance weighting of search terms. Journal of the Ameri-
can Society for Information science, 27(3):129?146.
Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975.
A vector space model for automatic indexing. Commu-
nications of the ACM, 18(11):613?620.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
and Sadao Kurohashi. 2012. Tsubaki: An open search
engine infrastructure for developing information ac-
cess methodology. Journal of Information Processing,
20(1):216?227.
Jun Suzuki. 2005. Kernels for structured data in natural
language processing. Doctor Thesis, Nara Institute of
Science and Technology.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 Task 13:
TempEval-2. In Proc. of the SemEval 2010, pages
57?62.
5
