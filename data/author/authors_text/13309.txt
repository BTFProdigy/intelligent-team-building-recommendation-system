Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 897?902,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Is Twitter A Better Corpus for Measuring Sentiment Similarity?
Shi Feng1, Le Zhang1, Binyang Li2,3, Daling Wang1, Ge Yu1, Kam-Fai Wong3
1Northeastern University, Shenyang, China
2University of International Relations, Beijing, China
3The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
{fengshi,wangdaling,yuge}@ise.neu.edu.cn, zhang777le@gmail.com
{byli,kfwong}@se.cuhk.edu.hk
Abstract
Extensive experiments have validated the ef-
fectiveness of the corpus-based method for
classifying the word?s sentiment polarity.
However, no work is done for comparing d-
ifferent corpora in the polarity classification
task. Nowadays, Twitter has aggregated huge
amount of data that are full of people?s senti-
ments. In this paper, we empirically evaluate
the performance of different corpora in sen-
timent similarity measurement, which is the
fundamental task for word polarity classifica-
tion. Experiment results show that the Twitter
data can achieve a much better performance
than the Google, Web1T and Wikipedia based
methods.
1 Introduction
Measuring semantic similarity for words and short
texts has long been a fundamental problem for many
applications such as word sense disambiguation,
query expansion, search advertising and so on.
Determining the word?s polarity plays a critical
role in opinion mining and sentiment analysis task.
Usually we can detect the word?s polarity by mea-
suring it?s semantic similarity with a positive seed
word sep and a negative seed word sen respectively,
as shown in Formula (1):
SO(w) = sim(w, sep)? sim(w, sen) (1)
where sim(wi, wj) is the semantic similarity mea-
surement method for the given word wi and wj . A
lot of papers have been published for designing ap-
propriate similarity measurements. One direction is
to learn similarity from the knowledge base or con-
cept taxonomy (Lin, 1998; Resnik, 1999). Anoth-
er direction is to learn semantic similarity with the
help of large corpus such as Web or Wikipedia da-
ta (Sahami and Heilman, 2006; Yih and Meek, 2007;
Bollegala et al, 2011; Gabrilovich and Markovitch,
2007). The basic assumption of this kind of methods
is that the word with similar semantic meanings of-
ten co-occur in the given corpus. Extensive experi-
ments have validated the effectiveness of the corpus-
based method in polarity classification task (Turney,
2002; Kaji and Kitsuregawa, 2007; Velikovich et al,
2010). For example, PMI is a well-known similari-
ty measurement (Turney, 2002), which makes use of
the whole Web as the corpus, and utilizes the search
engine hits number to estimate the co-occurrence
probability of the give word pairs. The PMI based
method has achieved promising results. However,
according to Kanayama?s investigation, only 60%
co-occurrences in the same window in Web pages
reflect the same sentiment orientation (Kanayama
and Nasukawa, 2006). Therefore, we may ask the
question whether the choosing of corpus can change
the performance of sim and is there any better cor-
pus than the Web page data for measuring the senti-
ment similarity?
Everyday, enormous numbers of tweets that con-
tain people?s rich sentiments are published in Twit-
ter. The Twitter may be a good source for measuring
the sentiment similarity. Compared with the Web
page data, the tweets have a higher rate of subjective
text posts. The length limitation can guarantee the
polarity consistency of each tweet. Moreover, the
tweets contain graphical emoticons, which can be
897
considered as natural sentiment labels for the corre-
sponding tweets in Twitter. In this paper, we attempt
to empirically evaluate the performance of differen-
t corpora in sentiment similarity measurement task.
As far as we know, no work is done on this topic.
2 The Characteristics of Twitter Data
As the world?s second largest SNS website, at the
end of 2012 Twitter had aggregated more than 500
million registered users, among which 200 million
were active users . More than 400 million tweets are
posted every day.
Several examples of typical posts from Twitter are
shown below.
(1) She had a headache and feeling light headed
with no energy. :(
(2) @username Nice work! Looks like you had a
fun day. I?m headed there Sat or Sun. :)
(3) I seen the movie on Direc Tv. I ordered it and
I really liked it. I can?t wait to get it for blu ray!
Excellent work Rob!
We observe that comparing with the other corpus,
the Twitter data has several advantages in measuring
the sentiment similarity.
Large. Users like to record their personal feelings
and talk about the trend topics in Twitter (Java et al,
2007; Kwak et al, 2010). So there are huge amount
of subjective texts with various topics generated in
the millions of tweets everyday. Further more, the
flexible Twitter API makes these data easy to access
and collect.
Length Limitation. Twitter has a length limita-
tion of 140 characters. Users have limited space to
express their feelings. So the sentiments in tweet-
s are usually concise, straightforward and polarity
consistent.
Emoticons. Users tend to utilize emoticons to
emphasize their sentiment feelings. According to
the statistics, about 8.1% tweets contain at least one
emoticon (Yang and Leskovec, 2011). Since the
tweets have the length limitation, the sentiments ex-
pressed in these short texts are usually consistent
with the embedded emoticons, such as the word fun
and headache in above examples.
In addition to the above advantages, there are al-
so some disadvantages for measuring sentiment sim-
ilarity using Twitter data. The spam tweets that
caused by advertisements may add noise and bias
during the similarity measurement. The short length
may also bring in lower co-occurrence probability
of words. Some words may not co-occur with each
other when the corpus is small. These disadvantages
set obstacles for measuring sentiment similarity by
using Twitter data as corpus. In the experiment sec-
tion, we will see if we can overcome these draw-
backs and get benefit from the advantages of Twitter
data.
3 The Corpus-based Sentiment Similarity
Measurements
The intuition behind the corpus-based semantic sim-
ilarity measuring method is that the words with sim-
ilar meanings tend to co-occur in the corpus. Given
the word wi, wj , we use the notation P (wi) to de-
note the occurrence counts of word wi in the corpus
C. P (wi, wj) denotes the co-occurrence counts of
word wi and wj in C. In this paper we employ the
corpus-based version of the three well-known simi-
larity measurements: Jaccard, Dice and PMI.
CorpusJaccard(wi, wj)
= P (wi,wj)P (wi)+P (wj)?P (wi,wj)
(2)
CorpusDice(wi, wj) =
2? P (wi, wj)
P (wi) + P (wj)
(3)
CorpusPMI(wi, wj) = log2(
P (wi,wj)
N
P (wi)
N
P (wj)
N
) (4)
In Formula (4), N is the number of documents in
the corpus C. The above similarity measurements
may have their own strengths and weaknesses. In
this paper, we utilize these classical measurements
to evaluate the quality of the corpus in polarity clas-
sification task.
Google is the world?s largest search engine, which
has indexed a huge number of Web pages. Us-
ing the extreme large indexed Web pages as cor-
pus, Cilibrasi and Vitanyi (2007) presented a method
for measuring similarity between words and phras-
es based on information distance and Kolmogorov
complexity. The search result page counts of Google
were utilized to estimate the occurrence frequencies
of the words in the corpus. Suppose wi, wj rep-
resent the candidate words, the Normalized Google
898
Distance is defined as:
NGD(wi, wj) =
max{logP (wi),logP (wj)}?logP (wi,wj)
logN?min{logP (wi),logP (wj)}
(5)
where P (wi) denotes page counts returned by
Google using wi as keyword; P (wi, wj) denotes the
page counts by using wi and wj as joint keywords;
N is the number of Web pages indexed by Google.
Cilibrasi and Vitanyi have validated the effective-
ness of Google distance in measuring the semantic
similarity between concept words.
Based on the above formulas, we compare the
Twitter data with the Web and Wikipedia data as the
similarity measurement corpus. Given a candidate
word w, we firstly measure its sentiment similar-
ity with a positive seed word and a negative seed
word respectively in Formula (1), and the difference
of sim is used to further detect the polarity of w.
The above four similarity measurements serve as
sim with Web, Wikipedia and Twitter data as cor-
pus. Turney (2002) chose excellent and poor as
seed words. However, using isolated seed word-
s may cause the bias problem. Therefore, we fur-
ther select two groups of seed words that are lack
of sensitivity to context and form a positive seed set
PS and a negative seed set NS (Turney, 2003). The
Formula (1) can be rewritten as:
SO(w) =
?
sep?PS
sim(w, sep)?
?
sen?NS
sim(w, sen) (6)
Based on the Formula(6) and the sentiment seed
words, we can measure the sentiment polarity of the
given candidate words.
4 Experiment
4.1 Experiment Setup
Corpus Preparing. The Twitter corpus corre-
sponds to the 476 million Twitter tweets (Yang and
Leskovec, 2011), which includes over 476 million
Twitter posts from 20 million users, covering a 7
month period from June 1, 2009 to December 31,
2009. We filter out the non-English tweets and the
spam tweets that have only few words with URLs.
The tweets that contain three or more trending topics
are also removed. Finally, we construct the Twitter
corpus that consists of 266.8 million English tweets.
For calculating page counts in Web data, the candi-
date words were launched to Google from February
2013 to April 2013. We also conduct the experi-
ments on the Google Web 1T data that consists of
Google n-gram counts (frequency of occurrence of
each n-gram) for 1 ? n ? 5 (Brants and Franz,
2006). The Web 1T data provides a nice approxi-
mation to the word co-occurrence statistics in Web
pages in a predefined window size (1 ? n ? 5).
For example, the 5 gram Web1T data means the co-
occurrence window size is 5. The English Wikipedia
dump 1 we used was extracted at the end of March
2013, which contained more than 13 million articles.
We extracted the plain texts of the Wikipedia data as
the training corpus for the Formula (6).
EvaluationMethod. Two well-know sentiment lex-
icons are utilized as gold standard for polarity clas-
sification task. The statistics of Liu?s sentiment lex-
icon (Liu et al, 2005) and MPQA subjectivity lexi-
con (Wilson et al, 2005) are shown in Table 1. For
each word w in the lexicons, we employ the Formu-
la (6) to calculate the word?s polarity using different
corpora. If SO(w) > 0, the word w is classified in-
to the positive category. Otherwise if SO(w) < 0, it
is classified into the negative category. The accura-
cy of the classification result is used to measure the
quality of the corpus.
Positive# Negative#
Liu 2,006 4,783
MPQA 2,304 4,153
Table 1: Lexicon size
4.2 Experiment Results
Firstly, we chose the seed words excellent and poor
as Turney?s (2002) settings. The polarity classifica-
tion accuracies are shown in Table 2.
In Table 2, Google, Web1T, Wikipedia, Twitter
represent the corpora that used in the experiment;
CJ, CD, CP, GD represent the Formula (2) to For-
mula (5) respectively. We can see from the Table 2
that the Twitter based method can achieve the best
performance. The rich sentiment information and
1http://en.wikipedia.org/
899
Lexicon Corpus CJ CD CP GD
Liu
Google 0.5116 0.5117 0.5064 0.5076
Web1T-5gram 0.3903 0.3903 0.3897 0.3864
Web1T-4gram 0.3771 0.3771 0.3772 0.3227
Wikipedia 0.5280 0.5280 0.5350 0.5412
Twitter 0.5567 0.5567 0.5635 0.5635
MPQA
Google 0.4897 0.4890 0.4891 0.4864
Web1T-5gram 0.3843 0.3843 0.3837 0.3783
Web1T-4gram 0.3729 0.3729 0.3714 0.3225
Wikipedia 0.5181 0.5181 0.5380 0.5344
Twitter 0.5421 0.5421 0.5493 0.5494
Table 2: Polarity classification accuracies using excellent
and poor as seed words
natural window size (140 characters) have a posi-
tive impact on determining the word?s polarity. The
Google based method gets a lower accuracy, this
may be due to the length of Web documents which
can not usually guarantee the semantic consistency
in the returned data. Even though two words appear
in one page (returned by Google), they might not be
semantically related. Furthermore, the Google based
method is time-consuming, because we have to peri-
odically send queries in order to avoid being blocked
by Google. The Web1T based method gets a much
worse accuracy. After detailed analysis, we find that
although the small window size (4 or 5) can guar-
antee the semantic consistency, the short length also
brings in lower co-occurrence probability. Statistics
show that about 38% SO values are zero when using
Web1T corpus. Due to the short length, the Twitter
data also suffers from the low co-occurrence prob-
lem.
To tackle the low co-occurrence problem, the seed
word sets are selected as Turney?s (2003) settings.
The positive word set PS={good, nice, excellent,
positive, fortunate, correct, superior} and negative
word set NS = {bad, nasty, poor, negative, unfortu-
nate, wrong, inferior} for the Formula (6). These
seed words have been verified to be effective in Tur-
ney?s paper for polarity classification. The experi-
ment results are shown in Table 3.
Table 3 shows that the performance of Twitter cor-
pus is much improved since the multiple seed words
alleviate the problem of low co-occurrence probabil-
ity in tweets. Generally, when using the seed word
groups the Twitter can achieve a much better per-
formance than all the other corpora. The improve-
ments are statistically significant (p-value < 0.05).
Lexicon Corpus CJ CD CP GD
Liu
Google 0.4859 0.4936 0.4884 0.5060
Web1T-5gram 0.5785 0.5785 0.3963 0.5782
Web1T-4gram 0.5766 0.5766 0.3872 0.5775
Wikipedia 0.6226 0.6225 0.5957 0.6145
Twitter 0.6678 0.6678 0.6917 0.6457
Twitter+ 0.6921 0.6921 0.7273 0.6599
MPQA
Google 0.5108 0.5225 0.5735 0.5763
Web1T-5gram 0.5737 0.5737 0.4225 0.5718
Web1T-4gram 0.5749 0.5749 0.3329 0.4797
Wikipedia 0.6086 0.6085 0.5773 0.5985
Twitter 0.6431 0.6431 0.6671 0.6253
Twitter+ 0.6665 0.6665 0.7001 0.6383
Table 3: Polarity classification accuracies using the seed
word groups
We further add the emoticons ?:)? and ?:(? into the
seed word groups, denoted by Twitter+ in Table 3.
The emoticons are natural sentiment labels. We can
see that the performances are further improved by
considering emoticons as seed words. The above
experiment results have validated the effectiveness
of Twitter data as a better corpus for measuring the
sentiment similarity. The results also reveal the po-
tential usefulness of Twitter corpus in semantic sim-
ilarity measurement.
5 Related Work
Detecting the polarity of words is the fundamental
problem for most of sentiment analysis tasks (Hatzi-
vassiloglou and McKeown, 1997; Pang and Lee,
2007; Feldman, 2013).
Many methods have been proposed to measure
the words? or short texts similarity based on large
corpus (Sahami and Heilman, 2006; Yih and Meek,
2007; Gabrilovich and Markovitch, 2007). Bolle-
gala et al (2011) submitted the word to the search
engine, and the related result pages were employed
to represent the meaning of the original word. Mi-
halcea et al (2006) proposed a method to measure
the semantic similarity of words or short texts, con-
sidering both corpus-based and knowledge-based in-
formation. Although the previous algorithms have
achieved promising results, there are no work done
on evaluating the quality of different corpora.
Mohtarami et al (2012; 2013a; 2013b) intro-
duced the concept of sentiment similarity, which
was considered as different from the traditional se-
mantic similarity, and more focused on revealing the
underlying sentiment relations between words. Mo-
900
htarami et al (2013b) proposed a hidden emotion-
al model to calculating the sentiment similarity of
word pairs. However, the impact of the different cor-
pora is not considered for this task.
Mohammad et al (2013) generated word-
sentiment association lexicons from Tweets with the
help of hashtags and emoticons. Pak and Paroubek
(2010) collected tweets with happy and sad emoti-
cons as training corpus, and built sentiment classi-
fier based on traditional machine learning methods.
Brody and Diakopoulos (2011) showed that length-
ening was strongly associated with subjectivity and
sentiment in tweets. Davidov et al (2010) treated 50
Twitter tags and 15 smileys as sentiment labels and
a supervised sentiment classification framework was
proposed to classify the tweets. The previous litera-
tures have showed that the emoticons can be treated
as natural sentiment labels of the tweets.
6 Conclusion and Future Work
The quality of corpus may affect the performance
of sentiment similarity measurement. In this pa-
per, we compare the Twitter data with the Google,
Web1T and Wikipedia data in polarity classification
task. The experiment results validate that when us-
ing the seed word groups the Twitter can achieve a
much better performance than the other corpora and
adding emoticons as seed words can further improve
the performance. It is observed that the twitter cor-
pus is a potential good source for measuring senti-
ment similarity between words. In future work, we
intend to design new similarity measurements that
can make best of the advantages of Twitter data.
Acknowledgments
This research is partially supported by Gener-
al Research Fund of Hong Kong (No. 417112)
and Shenzhen Fundamental Research Program (J-
CYJ20130401172046450). This research is al-
so supported by the State Key Development Pro-
gram for Basic Research of China (Grant No.
2011CB302200-G), State Key Program of Nation-
al Natural Science of China (Grant No. 61033007),
National Natural Science Foundation of China
(Grant No. 61370074, 61100026), and the Funda-
mental Research Funds for the Central Universities
(N120404007).
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using
Word Lengthening to Detect Sentiment in Microblogs.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
562?570, Edinburgh, UK, ACL.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizu-
ka. 2011. A Web Search Engine-Based Approach to
Measure Semantic Similarity between Words. IEEE
Transactions on Knowledge and Data Engineering,
23(7): 977?990.
Rudi Cilibrasi and Paul Vitanyi. 2007. The Google Simi-
larity Distance. IEEE Transactions on Knowledge and
Data Engineering, 19(3): 370?383.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced Sentiment Learning Using Twitter Hashtags
and Smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages 241?
249, Beijing, China, ACL.
Ronen Feldman. 2013. Techniques and Applications for
Sentiment Analysis. Communications of the ACM,
56(4):82?89.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606?1611, Hyderabad, India.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics, pages
174?181, Madrid, Spain, ACL.
Akshay Java, Xiaodan Song, Tim Finin, and Belle L. T-
seng. 2007. Why We Twitter: An Analysis of a Mi-
croblogging Community. In Proceedings of the 9th
International Workshop on Knowledge Discovery on
the Web and 1st International Workshop on Social Net-
works Analysis, pages 118?138, San Jose, CA, USA,
Springer.
Nobuhiro Kaji and Masaru Kitsuregawa. Building Lex-
icon for Sentiment Analysis from Massive Collec-
tion of HTML Documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pp. 1075?1083, Prague, Czech
Republic, ACL.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Ful-
ly Automatic Lexicon Expansion for Domain-oriented
901
Sentiment Analysis. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 355?363, Sydney, Australia, ACL.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
B. Moon. 2010. What is Twitter, a Social Network or a
News Media? In Proceedings of the 19th Internation-
al Conference on World Wide Web, pages 591?600,
Raleigh, North Carolina, USA, ACM.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics,
pages 768?774, Montreal, Quebec, Canada, ACL.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: Analyzing and Comparing Opin-
ions on the Web. In Proceedings of the 14th interna-
tional conference on World Wide Web, pages 342?351,
Chiba, Japan, ACM.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence and the
18th Innovative Applications of Artificial Intelligence
Conference, pages 775?780, Boston, Massachusetts,
USA, AAAI Press.
Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh Phu
Tran, and Chew Lim Tan. 2012. Sense Sentimen-
t Similarity: An Analysis. In Proceedings of the
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence, pages 1706?1712, Toronto, Ontario, Canada,
AAAI Press.
Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013a.
From Semantic to Emotional Space in Probabilistic
Sense Sentiment Analysis. In Proceedings of the
Twenty-Seventh AAAI Conference on Artificial Intel-
ligence, pages 711?717, Bellevue, Washington, USA,
AAAI Press.
Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013b.
Probabilistic Sense Sentiment Similarity through Hid-
den Emotions. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics, pages 983?992, Sofia, Bulgaria, ACL.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-the-
Art in Sentiment Analysis of Tweets. In Proceedings
of the seventh international workshop on Semantic E-
valuation Exercises, Atlanta, Georgia, USA, ACL.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
Corpus for Sentiment Analysis and Opinion Mining.
In Proceedings of the 2010 International Conference
on Language Resources and Evaluation, pages 1320?
1326, Valletta, Malta, ELRA.
Philip Resnik. 1999. Semantic Similarity in a Taxonomy:
An Information based Measure and Its Application to
Problems of Ambiguity in Natural Language. Journal
of Artificial Intelligence Research, 11:95?130.
Mehran Sahami and Timothy D. Heilman. 2006. A Web-
based Kernel Function for Measuring the Similarity of
Short Text Snippets. In Proceedings of the 15th inter-
national conference on World Wide Web, pages 377?
386, Edinburgh, Scotland, UK, ACM.
Bo Pang and Lillian Lee. 2007. Opinion Mining and Sen-
timent Analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
Peter D. Turney. 2002. Thumbs Up or Thumbs Down?
Semantic Orientation Applied to Unsupervised Classi-
fication of Reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 417?424, Philadelphia, PA, USA, ACL.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transaction Information Sys-
tem, 21(4): 315?346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan T. McDonald. The Viability of Web-
derived Polarity Lexicons. In Proceedings of the 2010
North American Chapter of the Association of Compu-
tational Linguistics, pp. 777?785, Los Angeles, Cali-
fornia, USA, ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman-
n. 2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of the 2005
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 347?354, Vancouver, British Columbi-
a, Canada, ACL.
Jaewon Yang and Jure Leskovec. 2011. Patterns of Tem-
poral Variation in Online Media. In Proceedings of
the Forth International Conference on Web Search and
Web Data Mining, pages 177?186, Hong Kong, Chi-
na, ACM.
Wen-tau Yih and Christopher Meek. 2007. Improving
Similarity Measures for Short Segments of Text. In
Proceedings of the 22nd AAAI Conference on Artificial
Intelligence, pages 1489?1494, Vancouver, British
Columbia, Canada, AAAI Press.
902
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1367?1375,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Unified Graph Model for Sentence-based Opinion Retrieval 
 
 
Binyang Li, Lanjun Zhou, Shi Feng, Kam-Fai Wong 
Department of Systems Engineering and Engineering Management 
The Chinese University of Hong Kong 
{byli, ljzhou, sfeng, kfwong}@se.cuhk.edu.hk 
 
 
Abstract 
There is a growing research interest in opinion 
retrieval as on-line users? opinions are becom-
ing more and more popular in business, social 
networks, etc. Practically speaking, the goal of 
opinion retrieval is to retrieve documents, 
which entail opinions or comments, relevant to 
a target subject specified by the user?s query. A 
fundamental challenge in opinion retrieval is 
information representation. Existing research 
focuses on document-based approaches and 
documents are represented by bag-of-word. 
However, due to loss of contextual information, 
this representation fails to capture the associa-
tive information between an opinion and its 
corresponding target. It cannot distinguish dif-
ferent degrees of a sentiment word when asso-
ciated with different targets. This in turn se-
riously affects opinion retrieval performance. 
In this paper, we propose a sentence-based ap-
proach based on a new information representa-
tion, namely topic-sentiment word pair, to cap-
ture intra-sentence contextual information be-
tween an opinion and its target. Additionally, 
we consider inter-sentence information to cap-
ture the relationships among the opinions on 
the same topic. Finally, the two types of infor-
mation are combined in a unified graph-based 
model, which can effectively rank the docu-
ments. Compared with existing approaches, 
experimental results on the COAE08 dataset 
showed that our graph-based model achieved 
significant improvement. 
1 Introduction 
In recent years, there is a growing interest in 
sharing personal opinions on the Web, such as 
product reviews, economic analysis, political 
polls, etc. These opinions cannot only help inde-
pendent users make decisions, but also obtain 
valuable feedbacks (Pang et al, 2008). Opinion 
oriented research, including sentiment classifica-
tion, opinion extraction, opinion question ans-
wering, and opinion summarization, etc. are re-
ceiving growing attention (Wilson, et al, 2005; 
Liu et al, 2005; Oard et al, 2006). However, 
most existing works concentrate on analyzing 
opinions expressed in the documents, and none 
on how to represent the information needs re-
quired to retrieve opinionated documents. In this 
paper, we focus on opinion retrieval, whose goal 
is to find a set of documents containing not only 
the query keyword(s) but also the relevant opi-
nions. This requirement brings about the chal-
lenge on how to represent information needs for 
effective opinion retrieval. 
In order to solve the above problem, previous 
work adopts a 2-stage approach. In the first stage, 
relevant documents are determined and ranked 
by a score, i.e. tf-idf value. In the second stage, 
an opinion score is generated for each relevant 
document (Macdonald and Ounis, 2007; Oard et 
al., 2006). The opinion score can be acquired by 
either machine learning-based sentiment classifi-
ers, such as SVM (Zhang and Yu, 2007), or a 
sentiment lexicons with weighted scores from 
training documents (Amati et al, 2007; Hannah 
et al, 2007; Na et al, 2009). Finally, an overall 
score combining the two is computed by using a 
score function, e.g. linear combination, to re-rank 
the retrieved documents. 
Retrieval in the 2-stage approach is based on 
document and document is represented by 
bag-of-word. This representation, however, can 
only ensure that there is at least one opinion in 
each relevant document, but it cannot determine 
the relevance pairing of individual opinion to its 
target. In general, by simply representing a 
document in bag-of-word, contextual informa-
tion i.e. the corresponding target of an opinion, is 
neglected. This may result in possible mismatch 
between an opinion and a target and in turn af-
fects opinion retrieval performance. By the same 
token, the effect to documents consisting of mul-
1367
tiple topics, which is common in blogs and 
on-line reviews, is also significant. In this setting, 
even if a document is regarded opinionated, it 
cannot ensure that all opinions in the document 
are indeed relevant to the target concerned. 
Therefore, we argue that existing information 
representation i.e. bag-of-word, cannot satisfy 
the information needs for opinion retrieval. 
In this paper, we propose to handle opinion re-
trieval in the granularity of sentence. It is ob-
served that a complete opinion is always ex-
pressed in one sentence, and the relevant target 
of the opinion is mostly the one found in it. 
Therefore, it is crucial to maintain the associative 
information between an opinion and its target 
within a sentence. We define the notion of a top-
ic-sentiment word pair, which is composed of a 
topic term (i.e. the target) and a sentiment word 
(i.e. opinion) of a sentence. Word pairs can 
maintain intra-sentence contextual information to 
express the potential relevant opinions. In addi-
tion, inter-sentence contextual information is also 
captured by word pairs to represent the relation-
ship among opinions on the same topic. In prac-
tice, the inter-sentence information reflects the 
degree of a word pair. Finally, we combine both 
intra-sentence and inter-sentence contextual in-
formation to construct a unified undirected graph 
to achieve effective opinion retrieval. 
The rest of the paper is organized as follows. 
In Section 2, we describe the motivation of our 
approach. Section 3 presents a novel unified 
graph-based model for opinion retrieval. We 
evaluated our model and the results are presented 
in Section 4. We review related works on opi-
nion retrieval in Section 5. Finally, in Section 6, 
the paper is concluded and future work is sug-
gested.  
2 Motivation 
In this section, we start from briefly describing 
the objective of opinion retrieval. We then illu-
strate the limitations of current opinion retrieval 
approaches, and analyze the motivation of our 
method.  
2.1 Formal Description of Problem 
Opinion retrieval was first presented in the 
TREC 2006 Blog track, and the objective is to 
retrieve documents that express an opinion about 
a given target. The opinion target can be a ?tradi-
tional? named entity (e.g. a name of person, lo-
cation, or organization, etc.), a concept (e.g. a 
type of technology), or an event (e.g. presidential 
election). The topic of the document is not re-
quired to be the same as the target, but an opi-
nion about the target has to be presented in the 
document or one of the comments to the docu-
ment (Macdonald and Ounis, 2006). Therefore, 
in this paper we regard the information needs for 
opinion retrieval as relevant opinion. 
2.2 Motivation of Our Approach 
In traditional information retrieval (IR) 
bag-of-word representation is the most common 
way to express information needs. However, in 
opinion retrieval, information need target at re-
levant opinion, and this renders bag-of-word re-
presentation ineffective. 
Consider the example in Figure 1. There are 
three sentences A, B, and C in a document di. 
Now given an opinion-oriented query Q related 
to ?Avatar?. According to the conventional 
2-stage opinion retrieval approach, di is 
represented by a bag-of-word. Among the words, 
there is a topic term Avatar (t1) occurring twice, 
i.e. Avatar in A and Avatar in C, and two senti-
ment words comfortable (o1) and favorite (o2) 
(refer to Figure 2 (a)). In order to rank this doc-
ument, an overall score of the document di is 
computed by a simple combination of the rele-
vant score ( ???????? ) and the opinion score 
(???????), e.g. equal weighted linear combination, 
as follows. 
???????? ? ???????? ?     ??????? 
For simplicity, we let ???????? ? ? ?? ? ?? ?? , and 
???????  be computed by using lexicon-based 
method: ??????? ? ????????????????? ? ??????????????. 
 
 
 
 
 
Figure 1: A retrieved document di on the target 
?Avatar?. 
Although bag-of-word representation achieves 
good performance in retrieving relevant docu-
ments, our study shows that it cannot satisfy the 
information needs for retrieval of relevant opi-
nion. It suffers from the following limitations: 
(1) It cannot maintain contextual information; 
thus, an opinion may not be related to the target 
of the retrieved document is neglected. In this 
example, only the opinion favorite (o2) on Avatar 
in C is the relevant opinion. But due to loss of 
contextual information between the opinion and 
its corresponding target, Avatar in A and com-
A. ???????????? 
Tomorrow, Avatar will be shown in China. 
B. ????? IMAX??????????
I?ve reserved a comfortable seat in IMAX. 
C. ??????????? 3D??? 
Avatar is my favorite 3D movie.  
1368
fortable (o1) are also regarded as relevant opi-
nion mistakenly, creating a false positive. In re-
ality comfortable (o1) describes ?the seats in 
IMAX?, which is an irrelevant opinion, and sen-
tence A is a factual statement rather than an opi-
nion statement. 
    
     
  (a)                (b)        
Figure 2: Two kinds of information representa-
tion of opinion retrieval. (t1=?Avatar? o1= ?com-
fortable?, o2=?favorite?) 
(1) Current approaches cannot capture the re-
lationship among opinions about the same topic. 
Suppose there is another document including 
sentence C which expresses the same opinion on 
Avatar. Existing information representation 
simply does not cater for the two identical opi-
nions from different documents. In addition, if 
many documents contain opinions on Avatar, the 
relationship among them is not clearly 
represented by existing approaches.  
In this paper, we process opinion retrieval in 
the granularity of sentence as we observe that a 
complete opinion always exists within a sentence 
(refer to Figure 2 (b)). To represent a relevant 
opinion, we define the notion of topic-sentiment 
word pair, which consists of a topic term and a 
sentiment word. A word pair maintains the asso-
ciative information between the two words, and 
enables systems to draw up the relationship 
among all the sentences with the same opinion 
on an identical target. This relationship informa-
tion can identify all documents with sentences 
including the sentiment words and to determine 
the contributions of such words to the target 
(topic term). Furthermore, based on word pairs, 
we designed a unified graph-based method for 
opinion retrieval (see later in Section 3). 
3 Graph-based model 
3.1 Basic Idea 
Different from existing approaches which simply 
make use of document relevance to reflect the 
relevance of opinions embedded in them, our 
approach concerns more on identifying the re-
levance of individual opinions. Intuitively, we 
believed that the more relevant opinions appear 
in a document, the more relevant is that docu-
ment for subsequent opinion analysis operations. 
Further, since the lexical scope of an opinion 
does not usually go beyond a sentence, we pro-
pose to handle opinion retrieval in the granularity 
of sentence. 
Without loss of generality, we assume that 
there is a document set ? ? ???, ??, ??,? , ???, and 
a specific query  ? ? ???, ??, ??,? , ??? , where 
??, ??, ??,? , ?? are query keywords. Opinion re-
trieval aims at retrieving documents from ? 
with relevant opinion about the query ?. In ad-
dition, we construct a sentiment word lexicon ?? 
and a topic term lexicon ?? (see Section 4). To 
maintain the associative information between the 
target and the opinion, we consider the document 
set as a bag of sentences, and define a sentence 
set as ? ? ???, ??, ??,? , ???. For each sentence, we 
capture the intra-sentence information through 
the topic-sentiment word pair.  
Definition 1. topic-sentiment word pair ???  con-
sists of two elements, one is from ??, and the 
other one is from ??.  
??? ? ?? ??, ?? ? |?? ? ?? , ?? ? ????. 
The topic term from ?? determines relevance 
by the query term matching, and the sentiment 
word from ?? is used to express an opinion. We 
use the word pair to maintain the associative in-
formation between the topic term and the opinion 
word (also referred to as sentiment word). The 
word pair is used to identify a relevant opinion in 
a sentence. In Figure 2 (b), t1, i.e. Avatar in C, is 
a topic term relevant to the query, and o2 (?favo-
rite?) is supposed to be an opinion; and the word 
pair < t1, o2> indicates sentence C contains a re-
levant opinion. Similarly, we map each sentence 
in word pairs by the following rule, and express 
the intra-sentence information using word pairs. 
For each sentiment word of a sentence, we 
choose the topic term with minimum distance as 
the other element of the word pair: 
?? ? ?? ??, ?? ? |?? ? min???????, ??? for each ??? 
According to the mapping rule, although a 
sentence may give rise to a number of word pairs, 
only the pair with the minimum word distance is 
selected. We do not take into consideration of the 
other words in a sentence as relevant opinions 
are generally formed in close proximity. A sen-
tence is regarded non-opinionated unless it con-
tains at least one word pair. 
In practice, not all word pairs carry equal 
weights to express a relevant opinion as the con-
tribution of an opinion word differs from differ-
ent target topics, and vice versa. For example, 
the word pair < t1, o2> should be more probable 
as a relevant opinion than < t1, o1>. To consider 
1369
that, inter-sentence contextual information is ex-
plored. This is achieved by assigning a weight to 
each word pair to measure their associative de-
grees to different queries. We believe that the 
more a word pair appears the higher should be 
the weight between the opinion and the target in 
the context. 
We will describe how to utilize intra-sentence 
contextual information to express relevant opi-
nion, and inter-sentence information to measure 
the degree of each word pair through a 
graph-based model in the following section. 
3.2 HITS Model 
We propose an opinion retrieval model based on 
HITS, a popular graph ranking algorithm 
(Kleinberg, 1999). By considering both in-
tra-sentence information and inter-sentence in-
formation, we can determine the weight of a 
word pair and rank the documents.  
HITS algorithm distinguishes hubs and au-
thorities in objects. A hub object has links to 
many authorities. An authority object, which has 
high-quality content, would have many hubs 
linking to it. The hub scores and authority scores 
are computed in an iterative way. Our proposed 
opinion retrieval model contains two layers. The 
upper level contains all the topic-sentiment word 
pairs ??? ? ?? ??, ?? ? |?? ? ??, ?? ? ???? . The lower 
level contains all the documents to be retrieved. 
Figure 3 gives the bipartite graph representation 
of the HITS model.  
 
Figure 3: Bipartite link graph. 
For our purpose, the word pairs layer is consi-
dered as hubs and the documents layer authori-
ties. If a word pair occurs in one sentence of a 
document, there will be an edge between them. 
In Figure 3, we can see that the word pair that 
has links to many documents can be assigned a 
high weight to denote a strong associative degree 
between the topic term and a sentiment word, 
and it likely expresses a relevant opinion. On the 
other hand, if a document has links to many word 
pairs, the document is with many relevant opi-
nions, and it will result in high ranking. 
Formally, the representation for the bipartite 
graph is denoted as ? ?? ??, ??, ??? ? , where 
?? ? ????? is the set of all pairs of topic words 
and sentiment words, which appear in one sen-
tence. ?? ? ??? ?  is the set of documents. 
??? ? ????
? |??? ? ??, ?? ? ???  corresponds to the 
connection between documents and top-
ic-sentiment word pairs. Each edge ????  is asso-
ciated with a weight ???? ? ?0,1?  denoting the 
contribution of ???  to the document ?? . The 
weight ????  is computed by the contribution of 
word pair ??? in all sentences of ?? as follows: 
???
? ? ?
|??|
? ?? ? ??????, ??? ? ?1 ? ????????, ?????????????  ?1? 
? |??| is the number of sentences in ??; 
? ? is introduced as the trade-off parameter to 
balance the ??????, ??? and ??????, ???; 
? ??????, ??? is computed to judge the relevance 
of ??  in ?? which belongs to ??; 
??????, ??? ? ? ???,?? ? ?? ???              (2) 
where ? ???,??  is the number of ??  appears in ?? , 
and 
?? ????log?
???
?.??????
?                   (3) 
where ? ??? is the number of sentences that the 
word ?? appears in. 
? ??????, ???  is the contribution of ??  in ?? 
which belongs to ??. 
??????, ??? ?
??,????
??,????
?0.5??1.5?
???????
??? ?
      (4) 
where ??? is the average number of sentences in 
??; ? ???,?? is the number of ?? appears in ?? (Al-
lan et al, 2003; Otterbacher et al, 2005). 
It is found that the contribution of a sentiment 
word ??  will not decrease even if it appears in 
all the sentences. Therefore in Equation 4, we 
just use the length of a sentence instead of ?? ??? 
to normalize long sentences which would likely 
contain more sentiment words. 
The authority score ??????????????????  of 
document ?? and a hub score ?????????????????? 
of ???  at the ?? ? 1???  iteration are computed 
based on the hub scores and authority scores in 
the ??? iteration as follows. 
?????????????????? ? ? ???
? ? ????????????????????  (5) 
?????????????????? ? ? ???
? ? ???????????????????  (6) 
We let ? ? ???,??|??|?|??| denote the adjacency 
matrix.  
???????? ???????                 (7) 
????????? ?? ?????                (8) 
where ????? ? ????????????????|??|??  is the vector 
of authority scores for documents at the ??? ite-
ration and ?????? ? ????????????????|??|??  is the 
vector of hub scores for the word pairs at ??? 
iteration. In order to ensure convergence of the 
iterative form, ?? and ??? are normalized in each 
iteration cycle.  
1370
For computation of the final scores, the initial 
scores of all documents are set to 
?
??, and top-
ic-sentiment word pairs are set to 
?
???? . The 
above iterative steps are then used to compute 
the new scores until convergence. Usually the 
convergence of the iteration algorithm is 
achieved when the difference between the scores 
computed at two successive iterations for any 
nodes falls below a given threshold (Wan et al, 
2008; Li et al, 2009; Erkan and Radev, 2004). In 
our model, we use the hub scores to denote the 
associative degree of each word pair and the au-
thority scores as the total scores. The documents 
are then ranked based on the total scores. 
4 Experiment 
We performed the experiments on the Chinese 
benchmark dataset to verify our proposed ap-
proach for opinion retrieval. We first tested the 
effect of the parameter ?  of our model. To 
demonstrate the effectiveness of our opinion re-
trieval model, we compared its performance with 
the same of other approaches. In addition, we 
studied each individual query to investigate the 
influence of query to our model. Furthermore, 
we showed the top-5 highest weight word pairs 
of 5 queries to further demonstrate the effect of 
word pair. 
4.1 Experiment Setup  
4.1.1 Benchmark Datasets 
Our experiments are based on the Chinese 
benchmark dataset, COAE08 (Zhao et al, 2008). 
COAE dataset is the benchmark data set for the 
opinion retrieval track in the Chinese Opinion 
Analysis Evaluation (COAE) workshop, consist-
ing of blogs and reviews. 20 queries are provided 
in COAE08. In our experiment, we created re-
levance judgments through pooling method, 
where documents are ranked at different levels: 
irrelevant, relevant but without opinion, and re-
levant with opinion. Since polarity is not consi-
dered, all relevant documents with opinion are 
classified into the same level. 
4.1.2 Sentiment Lexicon  
In our experiment, the sentiment lexicon is 
composed by the following resources (Xu et al, 
2007):  
(1) The Lexicon of Chinese Positive Words, 
which consists of 5,054 positive words and 
the Lexicon of Chinese Negative Words, 
which consists of 3,493 negative words; 
(2) The opinion word lexicon provided by Na-
tional Taiwan University which consists of 
2,812 positive words and 8,276 negative 
words; 
(3) Sentiment word lexicon and comment word 
lexicon from Hownet. It contains 1836 posi-
tive sentiment words, 3,730 positive com-
ments, 1,254 negative sentiment words and 
3,116 negative comment words. 
The different graphemes corresponding to 
Traditional Chinese and Simplified Chinese are 
both considered so that the sentiment lexicons 
from different sources are applicable to process 
Simplified Chinese text. The lexicon was ma-
nually verified.  
4.1.3 Topic Term Collection 
In order to acquire the collection of topic terms, 
we adopt two expansion methods, dictio-
nary-based method and pseudo relevance feed-
back method.  
The dictionary-based method utilizes Wikipe-
dia (Popescu and Etzioni, 2005) to find an entry 
page for a phrase or a single term in a query. If 
such an entry exists, all titles of the entry page 
are extracted as synonyms of the query concept. 
For example, if we search ???? (Green Tsu-
nami, a firewall) in Wikipedia, it is re-directed to 
an entry page titled ?????? (Youth Escort). 
This term is then added as a synonym of ???? 
(Green Tsunami) in the query. Synonyms are 
treated the same as the original query terms in a 
retrieval process. The content words in the entry 
page are ranked by their frequencies in the page. 
The top-k terms are returned as potential ex-
panded topic terms. 
The second query expansion method is a 
web-based method. It is similar to the pseudo 
relevance feedback expansion but using web 
documents as the document collection. The 
query is submitted to a web search engine, such 
as Google, which returns a ranked list of docu-
ments. In the top-n documents, the top-m topic 
terms which are highly correlated to the query 
terms are returned. 
4.2 Performance Evaluation 
4.2.1 Parameter Tuning 
We first studied how the parameter ? (see Equ-
ation 1) influenced the mean average precision 
(MAP) in our model. The result is given in Fig-
ure 4. 
1371
 
Figure 4: Performance of MAP with varying ?. 
Best MAP performance was achieved in 
COAE08 evaluation, when ? was set between 
0.4 and 0.6. Therefore, in the following experi-
ments, we set ? ? 0.4. 
4.2.2 Opinion Retrieval Model Comparison 
To demonstrate the effectiveness of our proposed 
model, we compared it with the following mod-
els using different evaluation metrics: 
(1) IR: We adopted a classical information re-
trieval model, and further assumed that all re-
trieved documents contained relevant opinions. 
(2) Doc: The 2-stage document-based opinion 
retrieval model was adopted. The model used 
sentiment lexicon-based method for opinion 
identification and a conventional information 
retrieval method for relevance detection.  
(3) ROSC: This was the model which achieved 
the best run in TREC Blog 07. It employed ma-
chine learning method to identify opinions for 
each sentence, and to determine the target topic 
by a NEAR operator. 
(4) ROCC: This model was similar to ROSC, 
but it considered the factor of sentence and re-
garded the count of relevant opinionated sen-
tence to be the opinion score (Zhang and Yu, 
2007). In our experiment, we treated this model 
as the evaluation baseline. 
(5) GORM: our proposed graph-based opinion 
retrieval model. 
Approach COAE08 Evaluation metrics 
Run id MAP R-pre bPref P@10
IR 0.2797 0.3545 0.2474 0.4868
Doc 0.3316 0.3690 0.3030 0.6696
ROSC 0.3762 0.4321 0.4162 0.7089
Baseline 0.3774 0.4411 0.4198 0.6931
GORM 0.3978 0.4835 0.4265 0.7309
Table 1: Comparison of different approaches on 
COAE08 dataset, and the best is highlighted. 
Most of the above models were originally de-
signed for opinion retrieval in English, and 
re-designed them to handle Chinese opinionated 
documents. We incorporated our own Chinese 
sentiment lexicon for this purpose. In our expe-
riments, in addition to MAP, other metrics such 
as R-precision (R-prec), binary Preference (bPref) 
and Precision at 10 documents (P@10) were also 
used. The evaluation results based on these me-
trics are shown in Table 1. 
Table 1 summarized the results obtained. We 
found that GORM achieved the best performance 
in all the evaluation metrics. Our baseline, ROSC 
and GORM which were sentence-based ap-
proaches achieved better performance than the 
document-based approaches by 20% in average. 
Moreover, our GORM approach did not use ma-
chine learning techniques, but it could still 
achieve outstanding performance. 
To study GORM influenced by different que-
ries, the MAP from median average precision on 
individual topic was shown in Figure 5. 
Figure 5: Difference of MAP from Median on 
COAE08 dataset. (MAP of Median is 0.3724) 
As shown in Figure 5, the MAP performance 
was very low on topic 8 and topic 11. Topic 8, i.e. 
???? (Jackie Chan), it was influenced by topic 
7, i.e. ????? (Jet Lee) as there were a number 
of similar relevant targets for the two topics, and 
therefore many word pairs ended up the same. 
As a result, documents belonging to topic 7 and 
topic 8 could not be differentiated, and they both 
performed badly. In order to solve this problem, 
we extracted the topic term with highest relevant 
weight in the sentence to form word pairs so that 
it reduce the impact on the topic terms in com-
mon. 24% and 30% improvement were achieved, 
respectively. 
As to topic 11, i.e. ????? (Lord of King), 
there were only 8 relevant documents without 
any opinion and 14 documents with relevant 
opinions. As a result, the graph constructed by 
insufficient documents worked ineffectively.  
Except for the above queries, GORM per-
formed well in most of the others. To further in-
vestigate the effect of word pair, we summarized 
the top-5 word pairs with highest weight of 5 
queries in Table 2. 
 
0.2
0.25
0.3
0.35
0.4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M
A
P
?
COAE08
?0.4
?0.3
?0.2
?0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
1 2 3 4 5 6 7 8 9 1011121314151617181920
D
if
fe
re
nc
e
Topic
Difference from Median Average Precision per 
Topic
1372
Table 2: Top-5 highest weight word pairs for 5 queries in COAE08 dataset. 
Table 2 showed that most word pairs could 
represent the relevant opinions about the corres-
ponding queries. This showed that inter-sentence 
information was very helpful to identify the as-
sociative degree of a word pair. Furthermore, 
since word pairs can indicate relevant opinions 
effectively, it is worth further study on how they 
could be applied to other opinion oriented appli-
cations, e.g. opinion summarization, opinion 
prediction, etc. 
5 Related Work 
Our research focuses on relevant opinion rather 
than on relevant document retrieval. We, there-
fore, review related works in opinion identifica-
tion research. Furthermore, we do not support the 
conventional 2-stage opinion retrieval approach. 
We conducted literature review on unified opi-
nion retrieval models and related work in this 
area is presented in the section. 
5.1 Lexicon-based Opinion Identification 
Different from traditional IR, opinion retrieval 
focuses on the opinion nature of documents. 
During the last three years, NTICR and TREC 
evaluations have shown that sentiment lex-
icon-based methods led to good performance in 
opinion identification.  
A lightweight lexicon-based statistical ap-
proach was proposed by Hannah et al (2007). In 
this method, the distribution of terms in relevant 
opinionated documents was compared to their 
distribution in relevant fact-based documents to 
calculate an opinion weight. These weights were 
used to compute opinion scores for each re-
trieved document. A weighted dictionary was 
generated from previous TREC relevance data 
(Amati et al, 2007). This dictionary was submit-
ted as a query to a search engine to get an initial 
query-independent opinion score of all retrieved 
documents. Similarly, a pseudo opinionated 
word composed of all opinion words was first 
created, and then used to estimate the opinion 
score of a document (Na et al, 2009). This me-
thod was shown to be very effective in TREC 
evaluations (Lee et al, 2008). More recently, 
Huang and Croft (2009) proposed an effective 
relevance model, which integrated both 
query-independent and query-dependent senti-
ment words into a mixture model. 
In our approach, we also adopt sentiment lex-
icon-based method for opinion identification. 
Unlike the above methods, we generate a weight 
to a sentiment word for each target (associated 
topic term) rather than assign a unified weight or 
an equal weight to the sentiment word for the 
whole topics. Besides, in our model no training 
data is required. We just utilize the structure of 
our graph to generate a weight to reflect the as-
sociative degree between the two elements of a 
word pair in different context. 
5.2 Unified Opinion Retrieval Model 
In addition to conventional 2-stage approach, 
there has been some research on unified opinion 
retrieval models.  
Eguchi and Lavrenko proposed an opinion re-
trieval model in the framework of generative 
language modeling (Eguchi and Lavrenko, 2006). 
They modeled a collection of natural language 
documents or statements, each of which con-
sisted of some topic-bearing and some senti-
ment-bearing words. The sentiment was either 
represented by a group of predefined seed words, 
or extracted from a training sentiment corpus. 
This model was shown to be effective on the 
MPQA corpus.  
Mei et al tried to build a fine-grained opinion 
retrieval system for consumer products (Mei et 
al., 2007). The opinion score for a product was a 
mixture of several facets. Due to the difficulty in 
Top-5 MAP 
??? 
Chen Kaige 
??? 
Six States 
???? 
Macro-regulation 
??? 
Stephen Chow 
Vista 
Vista 
<??? ??> 
Chen Kaige Support 
<??? ??> 
Chen Kaige Best 
<???? ?> 
Limitless Revile 
<?? ??> 
Movie Excellent 
<?? ???> 
Cast Strong 
<?? ??> 
Room rate Rise 
<?? ??> 
Regulate Strengthen
<?? ??> 
CCP Strengthen 
<?? ??> 
Room rate Steady 
<?? ??> 
Housing Security 
<?? ??> 
Economics Steady 
<?? ??> 
Price Rise 
<?? ??> 
Development Steady
<?? ??> 
Consume Rise 
<?? ??> 
Social Security 
<?? ??> 
Movie Like 
<??? ??> 
Stephen Chow Like 
<?? ??> 
Protagonist Best 
<?? ?> 
Comedy Good 
<?? ??> 
Works Splendid 
<?? ?> 
Price Expensive 
<?? ??> 
Microsoft Like 
 <Vista ??> 
Vista Recommend
<?? ??> 
Problem Vital 
<?? ?> 
Performance No 
1373
associating sentiment with products and facets, 
the system was only tested using small scale text 
collections.  
Zhang and Ye proposed a generative model to 
unify topic relevance and opinion generation 
(Zhang and Ye, 2008). This model led to satis-
factory performance, but an intensive computa-
tion load was inevitable during retrieval, since 
for each possible candidate document, an opinion 
score was summed up from the generative prob-
ability of thousands of sentiment words. 
Huang and Croft proposed a unified opinion 
retrieval model according to the Kullback-Leib- 
ler divergence between the two probability dis-
tributions of opinion relevance model and docu-
ment model (Huang and Croft, 2009). They di-
vided the sentiment words into query-dependent 
and query-independent by utilizing several sen-
timent expansion techniques, and integrated them 
into a mixed model. However, in this model, the 
contribution of a sentiment word was its corres-
ponding incremental mean average precision 
value. This method required that large amount of 
training data and manual labeling. 
Different from the above opinion retrieval ap-
proaches, our proposed graph-based model 
processes opinion retrieval in the granularity of 
sentence. Instead of bag-of-word, the sentence is 
split into word pairs which can maintain the 
contextual information. On the one hand, word 
pair can identify the relevant opinion according 
to intra-sentence contextual information. On the 
other hand, it can measure the degree of a rele-
vant opinion by considering the inter-sentence 
contextual information. 
6 Conclusion and Future Work 
In this work we focus on the problem of opinion 
retrieval. Different from existing approaches, 
which regard document relevance as the key in-
dicator of opinion relevance, we propose to ex-
plore the relevance of individual opinion. To do 
that, opinion retrieval is performed in the granu-
larity of sentence. We define the notion of word 
pair, which can not only maintain the association 
between the opinion and the corresponding target 
in the sentence, but it can also build up the rela-
tionship among sentences through the same word 
pair. Furthermore, we convert the relationships 
between word pairs and sentences into a unified 
graph, and use the HITS algorithm to achieve 
document ranking for opinion retrieval. Finally, 
we compare our approach with existing methods. 
Experimental results show that our proposed 
model performs well on COAE08 dataset.  
The novelty of our work lies in using word 
pairs to represent the information needs for opi-
nion retrieval. On the one hand, word pairs can 
identify the relevant opinion according to in-
tra-sentence contextual information. On the other 
hand, word pairs can measure the degree of a 
relevant opinion by taking inter-sentence con-
textual information into consideration. With the 
help of word pairs, the information needs for 
opinion retrieval can be represented appropriate-
ly. 
In the future, more research is required in the 
following directions: 
(1) Since word pairs can indicate relevant opi-
nions effectively, it is worth further study on 
how they could be applied to other opinion 
oriented applications, e.g. opinion summa-
rization, opinion prediction, etc. 
(2) The characteristics of blogs will be taken 
into consideration, i.e., the post time, which 
could be helpful to create a more time sensi-
tivity graph to filter out fake opinions. 
(3) Opinion holder is another important role of 
an opinion, and the identification of opinion 
holder is a main task in NTCIR. It would be 
interesting to study opinion holders, e.g. its 
seniority, for opinion retrieval. 
Acknowledgements: This work is partially 
supported by the Innovation and Technology 
Fund of Hong Kong SAR (No. ITS/182/08) and 
National 863 program (No. 2009AA01Z150). 
Special thanks to Xu Hongbo for providing the 
Chinese sentiment resources. We also thank Bo 
Chen, Wei Gao, Xu Han and anonymous re-
viewers for their helpful comments. 
References 
James Allan, Courtney Wade, and Alvaro Bolivar. 
2003. Retrieval and novelty detection at the sen-
tence level. In SIGIR ?03: Proceedings of the 26th 
annual international ACM SIGIR conference on 
Research and development in information retrieval, 
pages 314-321. ACM. 
Giambattista Amati, Edgardo Ambrosi, Marco Bianc-
hi, Carlo Gaibisso, and Giorgio Gambosi. 2007. 
FUB, IASI-CNR and University of Tor Vergata at 
TREC 2007 Blog Track. In Proceedings of the 15th 
Text Retrieval Conference. 
Koji Eguchi and Victor Lavrenko. Sentiment retrieval 
using generative models. 2006. In EMNLP ?06, 
Proceedings of 2006 Conference on Empirical Me-
thods in Natural Language Processing, page 
345-354. 
1374
Gunes Erkan and Dragomir R. Radev. 2004. Lexpa-
gerank: Prestige in multi-document text summariza-
tion. In EMNLP ?04, Proceedings of 2004 Confe-
rence on Empirical Methods in Natural Language 
Processing. 
David Hannah, Craig Macdonald, Jie Peng, Ben He, 
and Iadh Ounis. 2007. University of Glasgow at 
TREC 2007: Experiments in Blog and Enterprise 
Tracks with Terrier. In Proceedings of the 15th Text 
Retrieval Conference. 
Xuanjing Huang, William Bruce Croft. 2009. A Uni-
fied Relevance Model for Opinion Retrieval. In 
Proceedings of CIKM. 
Jon M. Kleinberg. 1999. Authoritative sources in a 
hyperlinked environment. J. ACM, 46(5): 604-632. 
Yeha Lee, Seung-Hoon Na, Jungi Kim, Sang-Hyob 
Nam, Hun-young Jung, Jong-Hyeok Lee. 2008. 
KLE at TREC 2008 Blog Track: Blog Post and Feed 
Retrieval. In Proceedings of the 15th Text Retrieval 
Conference. 
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan 
Zhu. 2009. Answering Opinion Questions with 
Random Walks on Graphs. In ACL ?09, Proceedings 
of the 48th Annual Meeting of the Association for 
Computational Linguistics. 
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. 
Opinion observer: Analyzing and comparing opi-
nion s on the web. In WWW ?05: Proceedings of the 
14th International Conference on World Wide Web. 
Craig Macdonald and Iadh Ounis. 2007. Overview of 
the TREC-2007 Blog Track. In Proceedings of the 
15th Text Retrieval Conference. 
Craig Macdonald and Iadh Ounis. 2006. Overview of 
the TREC-2006 Blog Track. In Proceedings of the 
14th Text Retrieval Conference. 
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, 
and Chengxiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In 
WWW ?07: Proceedings of the 16 International 
Conference on World Wide Web. 
Seung-Hoon Na, Yeha Lee, Sang-Hyob Nam, and 
Jong-Hyeok Lee. 2009. Improving opinion retrieval 
based on query-specific sentiment lexicon. In 
ECIR ?09: Proceedings of the 31st annual European 
Conference on Information Retrieval, pages 
734-738. 
Douglas Oard, Tamer Elsayed, Jianqiang Wang, Ye-
jun Wu, Pengyi Zhang, Eileen Abels, Jimmy Lin, 
and Dagbert Soergel. 2006. TREC-2006 at Mary-
land: Blog, Enterprise, Legal and QA Tracks. In 
Proceedings of the 15th Text Retrieval Conference. 
Jahna Otterbacher, Gunes Erkan, and Dragomir R. 
Radev. 2005. Using random walks for ques-
tion-focused sentence retrieval. In EMNLP ?05, 
Proceedings of 2005 Conference on Empirical Me-
thods in Natural Language Processing. 
Larry Page, Sergey Brin, Rajeev Motwani, and Terry 
Winograd. 1998. The pagerank citation ranking: 
Bringing order to the web. Technical report, Stan-
ford University. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2): 1-135.  
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinion s from reviews. In 
EMNLP ?05, Proceedings of 2005 Conference on 
Empirical Methods in Natural Language 
Processing. 
Xiaojun Wan and Jianwu Yang. 2008. Mul-
ti-document summarization using cluster-based link 
analysis. In SIGIR ?08: Proceedings of the 31th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval, 
pages 299-306. ACM. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
2005. Recognizing contextual polarity in 
phrase-level sentiment analysis. In EMNLP ?05, 
Proceedings of 2005 Conference on Empirical Me-
thods in Natural Language Processing.  
Ruifeng Xu, Kam-Fai Wong and Yunqing Xia. 2007. 
Opinmine - Opinion Analysis System by CUHK for 
NTCIR-6 Pilot Task. In Proceedings of NTCIR-6.  
Min Zhang and Xingyao Ye. 2008. A generation 
model to unify topic relevance and lexicon-based 
sentiment for opinion retrieval. In SIGIR ?08: Pro-
ceedings of the 31st Annual International ACM SI-
GIR conference on Research and Development in 
Information Retrieval, pages 411-418. ACM. 
Wei Zhang and Clement Yu. 2007. UIC at TREC 
2007 Blog Track. In Proceedings of the 15th Text 
Retrieval Conference. 
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan, 
Kang Liu, and Qi Zhang. 2008. Overview of Chi-
nese Opinion Analysis Evaluation 2008. In Pro-
ceedings of the First Chinese Opinion Analysis 
Evaluation. 
 
1375
