Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1066?1075,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Discriminative Substring Decoding for Transliteration
Colin Cherry and Hisami Suzuki
Microsoft Research
One Microsoft Way
Redmond, WA, 98052
{colinc,hisamis}@microsoft.com
Abstract
We present a discriminative substring de-
coder for transliteration. This decoder
extends recent approaches for discrimi-
native character transduction by allow-
ing for a list of known target-language
words, an important resource for translit-
eration. Our approach improves upon
Sherif and Kondrak?s (2007b) state-of-the-
art decoder, creating a 28.5% relative im-
provement in transliteration accuracy on
a Japanese katakana-to-English task. We
also conduct a controlled comparison of
two feature paradigms for discriminative
training: indicators and hybrid generative
features. Surprisingly, the generative hy-
brid outperforms its purely discriminative
counterpart, despite losing access to rich
source-context features. Finally, we show
that machine transliterations have a posi-
tive impact on machine translation quality,
improving human judgments by 0.5 on a
4-point scale.
1 Introduction
Transliteration occurs when a word is borrowed
into a language with a different character set.
The word is transcribed into the new character
set in such a way as to maintain rough phonetic
correspondence; for example, the English word
hip-hop becomes 2#7;#7 [hippuhoppu],
when transliterated into Japanese. A task fre-
quently of interest to the NLP community is back-
transliteration, where one seeks the original word,
given the borrowed form.
We investigate machine transliteration as a
method to handle out-of-vocabulary items in a
Japanese-to-English translation system. More
often than not, this will correspond to back-
transliteration. Our goal is to prevent the copy-
ing or deletion of Japanese words when they are
missing from our statistical machine translation
(SMT) system?s translation tables. This can have
a substantial impact on the quality of SMT output,
transforming translations of questionable useful-
ness, such as:
Avoid using a5JAK account.1
into the far more informative:
Avoid using a Freemail account.
Though the techniques we present here are
language-independent, we focus this study on
the task of Japanese katakana-to-English back-
transliteration. Katakana is one of the four char-
acter types used in the Japanese writing system
(along with hiragana, kanji and Roman alpha-
bet), consisting of about 50 syllabic characters.
It is used primarily to spell foreign loanwords
(e.g., !GL( [chokoreeto] ? chocolate),
and names (e.g., JS(S [kurinton] ? Clin-
ton). Therefore, katakana is a strong indicator
that a Japanese word can be back-transliterated.
However, katakana can also be used to spell sci-
entific names of animals and plants (e.g., B
[kamo] ? duck), onomatopoeic expressions (e.g.,
0C0C [bashabasha] ? splash) and for-
eign origin words that are not transliterations (e.g.,
;! [hochikisu] ? stapler). These un-
transliterable cases constitute about 10% of the
katakana words in our data.
We employ a discriminative substring decoder
for machine transliteration. Following Sherif and
Kondrak (2007b), the decoder operates on short
source substrings, with each operation producing
one or more target characters, as shown in Fig-
ure 1. However, where previous approaches em-
ploy generative modeling, we use structured per-
ceptron training to discriminatively tune parame-
ters according to 0-1 transliteration accuracy. This
15JAK is romanized as [furiimeeru]
1066
? ? ??
tho m son
Figure 1: Example substring derivation
allows us to test novel methods for the use of tar-
get lexicons in discriminative character transduc-
tion, allowing our decoder to benefit from a list of
known target words. Perhaps more significantly,
our framework allows us to test two competing
styles of features:
? sparse indicators, designed to capture the
same channel and language modeling data
collected by previous generative models, and
? components of existing generative models,
used as real-valued features in a discrimina-
tively weighted, generative hybrid.
Note that generative hybrids are the norm in
SMT, where translation scores are provided by
a discriminative combination of generative mod-
els (Och, 2003). Substring-based transliteration
with a generative hybrid model is very similar to
existing solutions for phrasal SMT (Koehn et al,
2003), operating on characters rather than words.
Unlike out-of-the-box phrasal SMT solutions, our
generative hybrid benefits from a target a lexicon.
As we will show, this is the difference between a
weak baseline and a strong competitor.
We demonstrate that despite recent successes in
discriminative character transduction using indi-
cator features (Jiampojamarn et al, 2008; Dreyer
et al, 2008), our generative hybrid performs sur-
prisingly well, producing our highest translitera-
tion accuracies. Researchers frequently compare
against a phrasal SMT baseline when evaluating a
new transduction technique (Freitag and Khadivi,
2007; Dreyer et al, 2008); however, we are careful
to vary only the features in our comparison. Con-
founding variables, such as alignment, decoder
and training method, are held constant.
We also include a human evaluation of
transliteration-augmented SMT output. Though
human evaluations are too expensive to allow a
comparison between transliteration systems, we
are able to show that adding our transliterations
to a production-level SMT engine results in a sub-
stantial improvement in translation quality.
2 Background
This work draws inspiration from previous work
in transliteration, which we divide into similarity
and transduction-based approaches. We also dis-
cuss recent successes in discriminative character
transduction that have influenced this work.
2.1 Similarity-based transliteration
In similarity-based transliteration, a character-
based, cross-lingual similarity metric is calculated
(or bootstrapped) from known transliteration pairs.
Given a source word s, its transliteration is the tar-
get word t most similar to s, where t is drawn from
some pool of candidates. This approach may also
be referred to as transliteration discovery.
Brill et al (2001) describe a katakana-to-
English approach with an EM-learned edit dis-
tance, which bootstraps from a small number of
examples to learn transliteration pairs from query
logs. Bilac and Tanaka (2005) harvest translitera-
tion candidates from comparable bilingual corpora
(conference abstracts in English and Japanese),
and use distributional as well as phonetic simi-
larity to choose among them. Sherif and Kon-
drak (2007a) also bootstrap a learned edit dis-
tance for Arabic named entities, with candidate
pairs drawn from sentence or document-aligned
parallel text. Klementiev and Roth (2006) boot-
strap an SVM classifier trained to detect true
transliteration-pairs. They draw candidates from
comparable news text, using date information to
provide further clues as to aligned named entities.
Bergsma and Kondrak (2007) extend the classifi-
cation approach with features derived from a char-
acter alignment. They train from bilingual dic-
tionaries and word-aligned parallel text, selecting
negative examples to target false-friends.
The work of Hermjakob et al (2008) is par-
ticularly relevant to this paper, as they incorpo-
rate a similarity-based transliteration system into
an Arabic-to-English SMT engine. They employ
a hand-crafted cross-lingual similarity metric, and
use capitalized n-grams from the Google n-gram
corpus as candidates. With such a huge candidate
list, a cross-lingual indexing scheme is designed
for fast candidate look-up. Their work also ad-
dresses the question of when to transliterate (as
opposed to translate), a realistic concern when de-
ploying a transliteration component in SMT. This,
however, is not of so much concern for katakana,
as it is used primarily for loanwords.
1067
2.2 Transduction-based transliteration
The approach presented in this paper is an instance
of transduction-based transliteration, where the
source word is transformed into a target word us-
ing a sequence of character-level operations. The
parameters of the transduction process are learned
from a collection of transliteration pairs. These
systems do not require a list of candidates, but
many incorporate a target lexicon, favoring target
words that occur in the lexicon. This approach is
also known as transliteration generation.
The majority of transliteration generation ap-
proaches are based on the noisy channel model,
where a target t is generated according to
P (t|s) ? P (s|t)P (t). This approach is typi-
fied by finite-state transliteration, where the var-
ious stages of the channel model are represented
by finite state transducers and automata. Early
systems employed a complex channel, passing
through multiple phonetic representations (Knight
and Graehl, 1998; Bilac and Tanaka, 2004), but
later versions replaced characters directly (Al-
Onaizan and Knight, 2002). Sherif and Kondrak
(2007b) extend this approach with substring oper-
ations in the style of phrasal SMT, and show that
doing so improves both accuracy as well as space
and time efficiency. Note that it is possible to in-
corporate a target lexicon by making P (t) a word
unigram model with a character-based back-off.
Li et al (2004) present an alternative to the
noisy channel with their joint n-gram model,
which calculates P (s, t). This formulation allows
operations to be conditioned on both source and
target context. However, the inclusion of a candi-
date list is more difficult in this setting, as P (t) is
not given its own model.
Zelenko and Aone (2006) investigate a purely
discriminative, alignment-free approach to
transliteration generation. The target word is
constructed one character at a time, with each
new character triggering a suite of features,
including indicators for near-by source and target
characters, as well a generative target language
model. Freitag and Khadivi (2007) propose a dis-
criminative, latent edit distance for transliteration.
In this case, training data need not be aligned in
advance, but a latent alignment is produced during
decoding. Again, the target word is constructed
one character at a time, using edit operations
that are scored according to source and target
context features. Both approaches train using a
structured perceptron, as we do here. However,
these models represent a dramatic departure from
the existing literature, while ours has clear analogs
to the well-known noisy-channel paradigm, which
allows for useful comparisons and insights into
the advantages of discriminative training.
2.3 Discriminative character transduction
While our chosen application is transliteration,
our decoder is influenced by recent successes in
general-purpose discriminative transduction. Ji-
ampojamarn et al (2008) describe a discrimina-
tive letter-to-phoneme substring transducer, while
Dreyer et al (2008) describe a discriminative char-
acter transducer with a latent derivation structure
for morphological transformations. Both models
are extremely effective, but both rely exclusively
on indicator features; they do not explore the use
of knowledge-rich generative models. Our indica-
tor system uses an extended version of the Jiampo-
jamarn et al (2008) feature set.
3 Methods
We adopt a discriminative substring decoder for
our transliteration task. A structured percep-
tron (Collins, 2002) learns weights for our translit-
eration features, which are drawn from two broad
classes: indicator and hybrid generative features.
3.1 Structured perceptron
The decoder?s discriminative parameters are
learned with structured perceptron training. Let
a derivation d describe a substring operation se-
quence that transliterates a source word into a tar-
get word. Given an input training corpus of such
derivations D = d
1
. . . d
n
, a vector feature func-
tion on derivations
~
F (d), and an initial weight vec-
tor ~w, the perceptron performs two steps for each
training example d
i
? D:
? Decode:
?
d = argmax
d?D(src(d
i
))
(
~w ?
~
F (d)
)
? Update: ~w = ~w +
~
F (d
i
)?
~
F (
?
d)
where D(src(d)) enumerates all possible deriva-
tions with the same source side as d. To improve
generalization, the final feature vector is the aver-
age of all vectors found during learning (Collins,
2002). Accuracy on the development set is used
to select the number of times we pass through all
d
i
? D.
Given the above framework, we require training
derivations D, feature vectors
~
F , and a decoder to
1068
carry out the argmax over all d reachable from a
particular source word. We describe each of these
components in turn below.
3.2 Training derivations
Note that the above framework describes a max-
derivation decoder trained on a corpus of gold-
standard derivations, as opposed to a max-
transliteration decoder trained directly on source-
target pairs. By building the entire system on the
derivation level, we side-step issues that can oc-
cur when perceptron training with hidden deriva-
tions (Liang et al, 2006), but we also introduce the
need to transform our training source-target pairs
into training derivations.
Training derivations can be learned unsu-
pervised from source-target pairs using char-
acter alignment techniques. Previously, this
has been done using an EM-learned edit dis-
tance (Ristad and Yianilos, 1998), or generaliza-
tions thereof (Brill and Moore, 2000; Jiampoja-
marn et al, 2007). We opt for an alternative align-
ment technique, similar to the word-aligner de-
scribed by Zhang et al (2008). This approach
employs variational EM with sparse priors, along
with hard length limits, to reduce the length of
substrings operated upon. By doing so, we hope to
learn only non-compositional transliteration units.
Our aligner produces only monotonic align-
ments, and does not allow either the source or tar-
get side of an operation to be empty. The same
restrictions are imposed during decoding. In this
way, each alignment found by variational EM is
also an unambiguous derivation. We align our
training corpus with a maximum substring length
of three characters. The same derivations are used
to train all of the transliteration systems tested in
this paper.
3.3 Features
We employ two main types of features: indicators
and hybrid generative models. Indicators detect
binary events in a derivation, such as the presence
of a particular operation. Hybrid generative fea-
tures assign a real-valued probability to a deriva-
tion, based on statistics collected from training
derivations. There are few generative features and
each carries a substantial amount of information,
while indicators are sparse and knowledge-poor.
We treat these two classes of features as distinct.
We do so because researchers often use either one
approach or the other.
2
Furthermore, it is not
clear how to optimally employ training derivations
when combining generative models and sparse in-
dicators: generative models need large amounts of
data to collect statistics and relatively little for per-
ceptron training,
3
while sparse indicators require
only a large perceptron training set.
We can further divide feature space according
to the information required to calculate each fea-
ture. Both feature sets can be partitioned into the
following subtypes:
? Emission: How accurate are the operations
used by this derivation?
? Transition: Does the target string produced
by this derivation look like a well-formed tar-
get character sequence?
? Lexicon: Does the target string contain
known words from a target lexicon?
Indicator Features
Previous approaches to discriminative character
transduction tend to employ only sparse indica-
tors (Jiampojamarn et al, 2008; Dreyer et al,
2008). This is because sparsity is not a major con-
cern in character-based domains, and sparse indi-
cators are extremely flexible.
Our emission and transition indicator features
follow Jiampojamarn et al (2008). Emission indi-
cators are centered around an operation, such as
[( ? tho]. Minimally, an indicator exists for
each operation. Many more source context fea-
tures can be generated by conjoining an operation
with source n-grams found within a fixed win-
dow of C characters to either side of the operation.
These source context features have minimal com-
putational cost, and they allow each operator to ac-
count for large, overlapping portions of the source,
even when the substrings being operated upon are
small. Meanwhile, transition indicators stand in
for a character-based target language model. Indi-
cators are built for each possible target n-gram, for
n = 1 . . .K, allowing the perceptron to construct
a discriminative back-off model. Development ex-
periments lead us to select C = 3 and K = 5.
2
Generative hybrids are often accompanied by a small
number of unsparse indicators, such as operation count.
3
Perceptron training on the same data used for model
construction can lead to overconfidence in model quality.
One can address this problem by using a large number of
modeling-training folds (Collins et al, 2005), but we do not
do so here.
1069
Indicator lexicon features are novel to this work.
Given access to a target lexicon with type fre-
quencies, we opt to create features that indicate
the frequencies of generated target words accord-
ing to coarse bins. Experiments on our develop-
ment set lead to the selection of 5 frequency bins:
[< 2,000], [< 200], [< 20], [< 2], [< 1]. To keep
the model linear, these features are cumulative;
thus, generating a word with frequency 126 will
result in both the [< 2, 000] and [< 200] features
firing. Note that a single transliteration can po-
tentially generate multiple target words, and doing
so can have a major impact on how often the lex-
icon features fire. Thus, we employ another fea-
ture that indicates the introduction of a new word.
We expect these frequency indicators to be supe-
rior to a word-level unigram model, as they allow
the designer to select notable frequencies. In par-
ticular, the bins we have selected do not give any
advantage to extremely common words, as these
are generally less likely to be transliterated.
Hybrid Generative Features
We begin with the three components of the gener-
ative noisy channel employed by Sherif and Kon-
drak (2007b). Their transliteration probability is:
P (t|s) ? P
E
(s|t) ?max [P
T
(t), P
L
(t)] (1)
Inspired by the linear models used in SMT (Och,
2003), we can discriminatively weight the compo-
nents of this generative model, producing:
w
E
logP
E
(s|t) + w
T
logP
T
(t) + w
L
logP
L
(t)
with weights w learned by perceptron training.
These three models conveniently align with our
three feature subtypes. Emission information is
provided by P
E
(s|t), which is estimated by maxi-
mum likelihood on the operations observed in our
training derivations. Including source context is
difficult in such a model. To compensate for this,
all systems using P
E
(s|t) also use composed op-
erations, which are constructed from operation se-
quences observed in the training set. This removes
the length limit on substring operations.
4
P
T
(t)
provides transition information through a charac-
ter language model, estimated on the target side
4
Derivations built by our character aligner use opera-
tions on substrings of maximum length 3. To enable per-
ceptron training with composed operations, once P
E
(s|t)
has been estimated by counting composed operations in the
initial alignments, we re-align our training examples with
those composed operations to maximize P
E
(s|t), creating
new training derivations.
of the training derivations. In our implementation,
we employ a KN-smoothed 7-gram model (Kneser
and Ney, 1995). Finally, P
L
(t) is a unigram tar-
get word model, estimated from the same type fre-
quencies used to build our lexicon indicators.
Since we have adopted a linear model, we are
no longer constrained by the original generative
story. Therefore, we are free to incorporate other
SMT-inspired features: P
E
?
(t|s), target character
count, and operation count.
5
Feature summary
The indicator and hybrid-generative feature sets
each provide a discriminative version of the noisy
channel model. In the case of transition and lexi-
con features, both systems have access to the ex-
act same information, but encode that information
differently. The lexicon encoding is the most dra-
matic difference, with the indicators using a small
number of frequency bins, and the generative uni-
gram model providing a single, real-valued feature
that is proportional to frequency.
In the case of their emission features, the
two systems actually encode different information.
Both have access to the same training derivations,
but the indicator system provides source context
through n-gram indicators, while the generative
system does so using composed operations.
3.4 Decoder
Our decoder builds upon machine translation?s
monotone phrasal decoding (Zens and Ney, 2004),
or equivalently, the sequence tagging algorithm
used in semi-Markov CRFs (Sarawagi and Co-
hen, 2004). This dynamic programming (DP) de-
coder extends the Viterbi algorithm for HMMs
by operating on one or more source characters (a
substring) at each step. A DP block stores the
best scoring solution for a particular prefix. Each
block is subdivided into cells, which maintain the
context necessary to calculate target-side features.
We employ a beam, keeping only the 40 highest-
scoring cells for each block, which speeds up in-
ference at the expense of optimality. We found
that the beam had no major effect on perceptron
training, nor on the system?s final accuracy.
Previously, target lexicons have been used
primarily in finite-state transliteration, as they
are easily encoded as finite-state-acceptors (Al-
Onaizan and Knight, 2002; Sherif and Kondrak,
5
Character and operation counts also fit in the indicator
system, but did not improve performance in development.
1070
2007b). It is possible to extend the DP decoder to
also use a target lexicon. By encoding the lexicon
as a trie, and adding the trie index to the context
tracked by the DP cells, we can provide access to
frequency estimates for words and word prefixes.
This has the side-effect of creating a new cell for
each target prefix; however, in the character do-
main, this remains computationally tractable.
4 Data
4.1 Wikipedia training and test data
Our katakana-to-English training data is de-
rived from bilingually-linked Wikipedia titles.
Any Japanese Wikipedia article with an entirely
katakana title and a linked English article results
in training pair. This results in 60K transliteration
pairs; we removed 2K pairs for development, and
2K for held-out testing.
The remaining 56K training pairs are quite
noisy. As mentioned earlier, roughly 10% of our
examples are simply not transliterable, but ap-
proximate Wikipedia title translations are an even
more substantial source of noise. For example,
S4E@ [konpyuutageemu] ? com-
puter game is aligned with the English article
Computer and video games. We found it ben-
eficial, in terms of both speed and accuracy, to
do some coarse alignment-based pruning. After
alignment, the operations used by all derivations
are counted. Any operation that is used fewer than
three times is eliminated, along with any deriva-
tion using that operation. The goal is to eliminate
loose transliteration pairs from our data, where a
word or initial is included in one language but
not the other. This results in 40K training pairs.
Despite the noise in the Wikipedia data, there are
clear advantages in using it for training transliter-
ation models: it is available for any language pair,
it reflects recent trends and events, and the amount
of data increases daily. As we will see below, the
model trained on this data performs well on a test
set from a very different domain.
All systems use development set accuracy to
select their meta-parameters, such as the number
of perceptron iterations, the size of the source-
context window, and the n-gram length used in
character language modeling. The hybrid gener-
ative system further splits the training set, using
38K derivations for the calculation of its emission
and transition models, and 2K derivations for per-
ceptron training its model weights.
4.2 Machine translation test data
In order to see how effective our transliterator
is on out-of-domain test data, we also created
test data from a log of translation requests to
a web-based, Japanese-to-English translation ser-
vice.
6
Out of 5,000 randomly selected transla-
tion requests, there are 312 cases where katakana
source words are out-of-vocabulary for the MT
system, and therefore remain untranslated. We
created a reference translation (not necessarily a
transliteration) for these katakana words by man-
ually selecting the corresponding English word(s)
in the sentence-level reference translation, which
was produced independently from this experiment.
This test set is quite divergent from the Wikipedia
titles: only 17 (5.5%) of its katakana words are
found in the Wikipedia training data, and six of
these did not agree on the English translation.
4.3 English lexicon
Our English lexicon is derived from two over-
lapping data sources: the English gigaword cor-
pus (LDC2003T05; GW) and the language model
training data for our SMT system, which contains
selections from Europarl, gigaword, and web-
harvested text. Both are lowercased. We com-
bine the unigram frequency counts from the two
sources by taking the max when they overlap. The
resulting lexicon has 5M types, 2.5M of which
have frequency 1.
5 Experiments
In this section, we summarize development exper-
iments, and then conduct a comparison on our two
transliteration test sets. We report 0-1 accuracy: a
transliteration is only correct if it exactly matches
the reference. For the comparison experiments,
we also report 10-best accuracy, where a system
is correct if it includes the correct transliteration
somewhere in its 10-best list.
5.1 Baselines
We compare our systems against a re-
implementation of Sherif and Kondrak?s (2007b)
noisy-channel substring decoder. This uses the
same P
E
, P
T
and P
L
models as our hybrid gen-
erative system, but employs a two-pass decoding
scheme to find the max transliteration according
to Equation 1. It represents a purely generative
solution using otherwise identical architecture.
6
http://www.microsofttranslator.com
1071
Since our hybrid generative system implements
a model that is very similar to those used in phrasal
SMT, we also compare against a state-of-the-art
phrasal SMT system (Moore and Quirk, 2007).
This system is trained by applying the standard
SMT pipeline to our Wikipedia title pairs, treat-
ing characters as words, using a 7-gram character-
level language model, and disabling re-ordering.
Unfortunately, the decoder?s architecture does not
allow the use of a word-level unigram model, re-
ducing the usefulness of this baseline. Instead, we
include the target lexicon as a second character-
level language model. This baseline indicates the
level of performance one can expect by applying
phrasal SMT straight out of the box.
Comparing the two baselines qualitatively, both
use a combination of generative models inspired
by the noisy channel. Sherif and Kondrak em-
ploy a word-level unigram model without discrim-
inatively weighting the models, while the Phrasal
SMT approach uses weights derived from max-
BLEU training without word-level unigrams. The
obvious question of what happens when one does
both will be answered by our hybrid generative
system.
5.2 Development experiments
Table 1 shows development set accuracy for a
number of systems and feature types, along with
the model size of the corresponding systems,
where size is measured in terms of the number of
non-zero discriminatively-trained parameters. The
accuracy of the Sherif and Kondrak baseline is
shown as SK07. Despite its lack of discrimina-
tive training, word-level unigrams allow the SK07
baseline to outperform Phrasal SMT . In future ex-
periments, we compare only against SK07.
The indicator system was tested using only op-
eration indicators, with source context, transition
and lexicon indicators added incrementally. All
feature types have a substantial impact, with the
lexicon providing the boost needed to surpass the
baseline. Note that the inclusion of the five fre-
quency bins is sufficient to decrease the overall
feature count of the system by 600K, as much
fewer mistakes are made during training.
Development of the hybrid generative system
used the SK07 baseline as a starting point. The re-
sult of combining its three components into a flat
linear model, with all weights set to 1, is shown
in Table 1 as Linear SK07. This violation of
Table 1: Development accuracy and model size
System Acc. Size
Baseline Phrasal SMT 30.7 8
SK07 33.5 ?
Indicator Operations only 3.6 6.8K
+ source context 23.9 2.8M
+ transition 28.6 3.1M
+ lexicon 44.2 2.5M
+ gen. lexicon 44.1 3.0M
Generative Linear SK07 31.7 ?
+ perceptron 42.4 3
+ SMT features 44.1 6
+ ind. lexicon 44.3 12
conditional independence assumptions results in a
drop in accuracy. However, the + perceptron line
shows that setting the three weights with percep-
tron training results in a huge boost in accuracy,
nearly matching our indicator system. Adding fea-
tures inspired by SMT, such as P
E
?
(t|s), elimi-
nates the gap between the two.
5.3 Development discussion
Considering their differences, the two systems?
proximity in score is quite surprising. Given the
character domain?s lack of sparsity, and the large
amount of available training data, we had expected
the hybrid generative system to behave only as
a strong baseline; instead, it matched the perfor-
mance of the indicator system. However, this
is not unprecedented: discriminatively weighted
generative models have been shown to outperform
purely discriminative competitors in various NLP
classification tasks (Raina et al, 2004; Toutanova,
2006), and remain the standard approach in statis-
tical translation modeling (Och, 2003).
Examining the development results on an
example-by-example basis, we see that the two
systems make mostly the same mistakes: for 87%
of examples, either both systems are right, or both
are wrong. The remainder represents a (relatively
small) opportunity to improve through system or
feature combination: an oracle that perfectly se-
lects between the two scores 50.6.
One opportunity for straight-forward combina-
tion is the target lexicon. Because lexicon frequen-
cies are drawn from an independent word list, and
not the transliteration training derivations, there is
no reason why both systems cannot use both lex-
icon representations. Unfortunately, doing so has
1072
Table 2: Test set comparisons
Wikipedia MT
System Acc. Top 10 Acc. Top 10
SK07 33.5 57.9 38.8 57.0
Generative 43.0 65.6 42.9 58.3
Indicator 42.5 63.5 43.6 57.7
little impact, as is shown in each system?s final row
in Table 1. Adding the word unigram model to
the indicator system results in slightly lower per-
formance, and a much larger model. Adding the
frequency bins to the generative system does im-
prove performance slightly, but attempts to com-
pletely replace the generative system?s word uni-
gram model with frequency bins resulted in a sub-
stantial drop in accuracy.
7
5.4 Test set comparisons
Table 2 shows the accuracies of the systems se-
lected during development on our testing data. On
the held-out Wikipedia examples, the trends ob-
served during development remain the same, with
the generative system expanding its lead. Mov-
ing to 10-best accuracies changes little, except for
slightly narrowing the gap between SK07 and the
discriminative systems.
The second column of Table 2 compares the
systems on our MT test set. As discussed ear-
lier, this data is quite different from the Wikipedia
training set, and as a result, the systems? differ-
ences are less pronounced. 1-best accuracy still
shows the discriminative systems having a definite
advantage, but at the 10-best level, those distinc-
tions are muted.
Compared with the previous work on katakana-
to-English transliteration, these accuracies do not
look particularly high: both Knight and Graehl
(1998) and Bilac and Tanaka (2004) report accu-
racies above 60% for 1-best transliteration. We
should emphasize that this is due to the difficulty
of our test data, and that we have tested against a
baseline that has been shown to outperform Knight
and Graehl (1998). The test data was not filtered
for noise, leaving untransliterable cases and loose
translations intact. The accuracies reported above
are under-estimates of real performance: many
transliterations not matching the reference may
still be useful to a human reader, such as differ-
7
Lexicon replacement experiment is not shown in Table 1.
ences in inflection (e.g.,L!.) [rechinoido]
? retinoids, transliterated as retinoid), and spac-
ing (e.g. IL
- [shierareone]? Sierra
Leone, transliterated as sierraleone).
6 Integration with machine translation
We used the transliterations from our indicator
system to augment a Japanese-to-English MT sys-
tem.
8
This treelet-based SMT system (Quirk et
al., 2005) is trained on about 4.6M parallel sen-
tence pairs from diverse sources including bilin-
gual books, dictionaries and web publications.
Our goal is to measure the impact of machine
transliterations on end-to-end translation quality.
6.1 Evaluation method
We use the MT-log translation pairs described
in Section 4.2 as a sentence-level translation test
set. For each katakana word left untranslated by
the baseline SMT engine, we generated 10-best
transliteration candidates and added the katakana-
English pairs to the SMT system?s translation ta-
ble. Perceptron scores were exponentiated, then
normalized, to create probabilities, which were
given to the SMT system as P (source|target);
9
all other translation features were set to log 1.
We translated the test set with and without the
augmented translation table. 120 sentences were
randomly selected from the cases where the trans-
lations output by the two SMT systems differed,
and were submitted for two types of human evalu-
ation. In the absolute evaluation, each SMT out-
put was assigned a score between 1 and 4 (1 =
completely useless; 4 = perfect translation); in the
relative evaluation, the evaluators were presented
with a pair of SMT outputs, with and without the
transliteration table, and were asked to judge if
they preferred one translation over the other. In
both evaluation settings, the machine-translated
sentences were evaluated by two native speakers
of English who have no knowledge of Japanese,
with access to a reference translation.
6.2 Results
The evaluation results show that our translitera-
tor does improve the quality of SMT. The BLEU
8
The human evaluation was carried out before we discov-
ered the effectiveness of the hybrid generative system, but
recall that the performance of the two is similar.
9
The perceptron scores are more naturally interpreted as
P (target |source), but the opposite direction is generally the
highest-weighted feature in the SMT system?s linear model.
1073
Table 3: Relative translation evaluation
evaluator 1 preference
eval2pref +translit equal baseline sum
+translit 95 0 2 97
equal 19 1 2 22
baseline 1 0 0 1
sum 115 1 4 120
score on the entire test set improved only slightly,
from 21.8 to 22.0. However, in the absolute hu-
man evaluation, the transliteration table increased
the average human judgement from 1.5 to 2 out of
a maximum score of 4. Table 3 shows the results
of the relative evaluation along with the judges?
sentence-level agreement. In 95 out of 120 cases,
both annotators agreed that the augmented table
produced a better translation than the baseline.
One might expect that any replacement of
katakana would improve the perception of MT
quality. This is not necessarily the case: it
can be more confusing to have a drastically
incorrect transliteration, such as transliterating
#7M [appurooda] ? uploader incor-
rectly as applaud. Fortunately, Table 3 shows that
we make very few of these sorts of mistakes: the
baseline is preferred only rarely. Also note that,
according the MT 10-best accuracies in Table 2,
we would have expected to improve at most 60%
of cases, however, the human judgements indicate
that our actual rate of improvement is closer to
80%, which demonstrates that even an imperfect
transliteration is often useful.
7 Conclusion
We have presented a discriminative substring de-
coder for transliteration. Our decoder is based
on recent approaches for discriminative charac-
ter transduction, extended to provide access to a
target lexicon. We have presented a comparison
of indicator and hybrid generative features in a
controlled setting, demonstrating that generative
models perform surprisingly well when discrim-
inatively weighted. We have also shown our dis-
criminative models to be superior to a state-of-the-
art generative system. Finally, we have demon-
strated that machine transliteration is immediately
useful to end-to-end SMT.
As mentioned earlier, by focusing on katakana,
we bypass the problem of deciding when to
transliterate rather than translate; next, we plan to
combine our models with a classifier that makes
such a decision, allowing us to integrate transliter-
ation into SMT for other language pairs.
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in Arabic text. In ACL
Workshop on Comp. Approaches to Semitic Lan-
guages.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
ACL, pages 656?663, Prague, Czech Republic, June.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid
back-transliteration system for Japanese. In COL-
ING, pages 597?603, Geneva, Switzerland.
Slaven Bilac and Hozumi Tanaka. 2005. Extracting
transliteration pairs from comparable corpora. In
Proceedings of the Annual Meeting of the Natural
Language Processing Society, Japan.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
ACL, pages 286?293, Morristown, NJ.
Eric Brill, Gary Kacmarcik, and Chris Brockett.
2001. Automatically harvesting katakana-english
term pairs from search engine query logs. In Asia
Federation of Natural Language Processing.
Michael Collins, Brian Roark, and Murat Sarac?lar.
2005. Discriminative syntactic language modeling
for speech recognition. In ACL, pages 507?514,
Ann Arbor, USA, June.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In EMNLP, pages 1080?
1089, Honolulu, Hawaii, October.
Dayne Freitag and Shahram Khadivi. 2007. A se-
quence alignment model based on the averaged per-
ceptron. In EMNLP, pages 238?247, Prague, Czech
Republic, June.
Ulf Hermjakob, Kevin Knight, and Hal Daum?e III.
2008. Name translation in statistical machine trans-
lation - learning when to transliterate. In ACL, pages
389?397, Columbus, Ohio, June.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many align-
ments and hidden markov models to letter-to-
phoneme conversion. In HLT-NAACL, pages 372?
379, Rochester, New York, April.
1074
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In ACL,
pages 905?913, Columbus, Ohio, June.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In HLT-NAACL, pages
82?88, New York City, USA, June.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In-
ternational Conference on Acoustics, Speech, and
Signal Processing (ICASSP-95), pages 181?184.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In ACL, pages 159?166, Barcelona, Spain, July.
Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In COLING-
ACL, pages 761?768, Sydney, Australia, July.
Robert Moore and Chris Quirk. 2007. Faster
beam-search decoding for phrasal statistical ma-
chine translation. In MT Summit XI.
Franz J. Och. 2003. Minimum error rate training for
statistical machine translation. In ACL, pages 160?
167.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In ACL, pages 271?279, Ann
Arbor, USA, June.
Rajat Raina, Yirong Shen, Andrew Y. Ng, and Andrew
McCallum. 2004. Classification with hybrid gener-
ative/discriminative models. In Advances in Neural
Information Processing Systems 16.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20(5):522?
532.
Sunita Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In ICML.
Tarek Sherif and Grzegorz Kondrak. 2007a. Boot-
strapping a stochastic transducer for Arabic-English
transliteration extraction. In ACL, pages 864?871,
Prague, Czech Republic, June.
Tarek Sherif and Grzegorz Kondrak. 2007b.
Substring-based transliteration. In ACL, pages 944?
951, Prague, Czech Republic, June.
Kristina Toutanova. 2006. Competitive generative
models with structure learning for nlp classification
tasks. In EMNLP, pages 576?584, Sydney, Aus-
tralia, July.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In EMNLP, pages
612?617, Sydney, Australia, July.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
HLT-NAACL, pages 257?264, Boston, USA, May.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL, pages 97?105, Columbus, Ohio, June.
1075
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 209?217,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Unsupervised Morphological Segmentation with Log-Linear Models
Hoifung Poon?
Dept. of Computer Sci. & Eng.
University of Washington
Seattle, WA 98195
hoifung@cs.washington.edu
Colin Cherry
Microsoft Research
Redmond, WA 98052
colinc@microsoft.com
Kristina Toutanova
Microsoft Research
Redmond, WA 98052
kristout@microsoft.com
Abstract
Morphological segmentation breaks words
into morphemes (the basic semantic units). It
is a key component for natural language pro-
cessing systems. Unsupervised morphologi-
cal segmentation is attractive, because in ev-
ery language there are virtually unlimited sup-
plies of text, but very few labeled resources.
However, most existing model-based systems
for unsupervised morphological segmentation
use directed generative models, making it dif-
ficult to leverage arbitrary overlapping fea-
tures that are potentially helpful to learning.
In this paper, we present the first log-linear
model for unsupervised morphological seg-
mentation. Our model uses overlapping fea-
tures such as morphemes and their contexts,
and incorporates exponential priors inspired
by the minimum description length (MDL)
principle. We present efficient algorithms
for learning and inference by combining con-
trastive estimation with sampling. Our sys-
tem, based on monolingual features only, out-
performs a state-of-the-art system by a large
margin, even when the latter uses bilingual in-
formation such as phrasal alignment and pho-
netic correspondence. On the Arabic Penn
Treebank, our system reduces F1 error by 11%
compared to Morfessor.
1 Introduction
The goal of morphological segmentation is to seg-
ment words into morphemes, the basic syntac-
tic/semantic units. This is a key subtask in many
? This research was conducted during the author?s intern-
ship at Microsoft Research.
NLP applications, including machine translation,
speech recognition and question answering. Past
approaches include rule-based morphological an-
alyzers (Buckwalter, 2004) and supervised learn-
ing (Habash and Rambow, 2005). While successful,
these require deep language expertise and a long and
laborious process in system building or labeling.
Unsupervised approaches are attractive due to the
the availability of large quantities of unlabeled text,
and unsupervised morphological segmentation has
been extensively studied for a number of languages
(Brent et al, 1995; Goldsmith, 2001; Dasgupta and
Ng, 2007; Creutz and Lagus, 2007). The lack
of supervised labels makes it even more important
to leverage rich features and global dependencies.
However, existing systems use directed generative
models (Creutz and Lagus, 2007; Snyder and Barzi-
lay, 2008b), making it difficult to extend them with
arbitrary overlapping dependencies that are poten-
tially helpful to segmentation.
In this paper, we present the first log-linear model
for unsupervised morphological segmentation. Our
model incorporates simple priors inspired by the
minimum description length (MDL) principle, as
well as overlapping features such as morphemes and
their contexts (e.g., in Arabic, the string Al is likely
a morpheme, as is any string between Al and a word
boundary). We develop efficient learning and infer-
ence algorithms using a novel combination of two
ideas from previous work on unsupervised learn-
ing with log-linear models: contrastive estimation
(Smith and Eisner, 2005) and sampling (Poon and
Domingos, 2008).
We focus on inflectional morphology and test our
209
approach on datasets in Arabic and Hebrew. Our
system, using monolingual features only, outper-
forms Snyder & Barzilay (2008b) by a large mar-
gin, even when their system uses bilingual informa-
tion such as phrasal alignment and phonetic corre-
spondence. On the Arabic Penn Treebank, our sys-
tem reduces F1 error by 11% compared to Mor-
fessor Categories-MAP (Creutz and Lagus, 2007).
Our system can be readily applied to supervised
and semi-supervised learning. Using a fraction of
the labeled data, it already outperforms Snyder &
Barzilay?s supervised results (2008a), which further
demonstrates the benefit of using a log-linear model.
2 Related Work
There is a large body of work on the unsupervised
learning of morphology. In addition to morpholog-
ical segmentation, there has been work on unsuper-
vised morpheme analysis, where one needs to deter-
mine features of word forms (Kurimo et al, 2007)
or identify words with the same lemma by model-
ing stem changes (Schone and Jurafsky, 2001; Gold-
smith, 2001). However, we focus our review specif-
ically on morphological segmentation.
In the absence of labels, unsupervised learning
must incorporate a strong learning bias that reflects
prior knowledge about the task. In morphological
segmentation, an often-used bias is the minimum
description length (MDL) principle, which favors
compact representations of the lexicon and corpus
(Brent et al, 1995; Goldsmith, 2001; Creutz and La-
gus, 2007). Other approaches use statistics on mor-
pheme context, such as conditional entropy between
adjacent n-grams, to identify morpheme candidates
(Harris, 1955; Keshava and Pitler, 2006). In this pa-
per, we incorporate both intuitions into a simple yet
powerful model, and show that each contributes sig-
nificantly to performance.
Unsupervised morphological segmentation sys-
tems also differ from the engineering perspective.
Some adopt a pipeline approach (Schone and Ju-
rafsky, 2001; Dasgupta and Ng, 2007; Demberg,
2007), which works by first extracting candidate
affixes and stems, and then segmenting the words
based on the candidates. Others model segmenta-
tion using a joint probabilistic distribution (Goldwa-
ter et al, 2006; Creutz and Lagus, 2007; Snyder and
Barzilay, 2008b); they learn the model parameters
from unlabeled data and produce the most proba-
ble segmentation as the final output. The latter ap-
proach is arguably more appealing from the mod-
eling standpoint and avoids error propagation along
the pipeline. However, most existing systems use
directed generative models; Creutz & Lagus (2007)
used an HMM, while Goldwater et al (2006) and
Snyder & Barzilay (2008b) used Bayesian models
based on Pitman-Yor or Dirichlet processes. These
models are difficult to extend with arbitrary overlap-
ping features that can help improve accuracy.
In this work we incorporate novel overlapping
contextual features and show that they greatly im-
prove performance. Non-overlapping contextual
features previously have been used in directed gen-
erative models (in the form of Markov models) for
unsupervised morphological segmentation (Creutz
and Lagus, 2007) or word segmentation (Goldwater
et al, 2007). In terms of feature sets, our model is
most closely related to the constituent-context model
proposed by Klein and Manning (2001) for grammar
induction. If we exclude the priors, our model can
also be seen as a semi-Markov conditional random
field (CRF) model (Sarawagi and Cohen, 2004).
Semi-Markov CRFs previously have been used for
supervised word segmentation (Andrew, 2006), but
not for unsupervised morphological segmentation.
Unsupervised learning with log-linear models has
received little attention in the past. Two notable ex-
ceptions are Smith & Eisner (2005) for POS tagging,
and Poon & Domingos (2008) for coreference res-
olution. Learning with log-linear models requires
computing the normalization constant (a.k.a. the
partition function) Z . This is already challenging in
supervised learning. In unsupervised learning, the
difficulty is further compounded by the absence of
supervised labels. Smith & Eisner (2005) proposed
contrastive estimation, which uses a small neighbor-
hood to compute Z . The neighborhood is carefully
designed so that it not only makes computation eas-
ier but also offers sufficient contrastive information
to aid unsupervised learning. Poon & Domingos
(2008), on the other hand, used sampling to approx-
imate Z .1 In this work, we benefit from both tech-
niques: contrastive estimation creates a manageable,
1Rosenfeld (1997) also did this for language modeling.
210
wvlAvwn
(##__##)
w
(##__vl)
vlAv
(#w__wn)
wn
(Av__##)
Figure 1: The morpheme and context (in parentheses)
features for the segmented word w-vlAv-wn.
informative Z , while sampling enables the use of
powerful global features.
3 Log-Linear Model for Unsupervised
Morphological Segmentation
Central to our approach is a log-linear model that
defines the joint probability distribution for a cor-
pus (i.e., the words) and a segmentation on the cor-
pus. The core of this model is a morpheme-context
model, with one feature for each morpheme,2 and
one feature for each morpheme context. We rep-
resent contexts using the n-grams before and after
the morpheme, for some constant n. To illustrate
this, a segmented Arabic corpus is shown below
along with its features, assuming we are tracking bi-
gram contexts. The segmentation is indicated with
hyphens, while the hash symbol (#) represents the
word boundary.
Segmented Corpus hnAk w-vlAv-wn bn-w
Al-ywm Al-jmAEp
Morpheme Feature:Value hnAk:1 w:2 vlAv:1
wn:1 bn:1 Al:2 ywm:1 jmAEp:1
hnAk:1 wvlAvwn:1 bnw:1 Alywm:1 Alj-
mAEp:1
Bigram Context Feature:Value ## vl:1
#w wn:1 Av ##:1 ## w#:1 bn ##:1
## yw:1 Al ##:2 ## jm:1 ## ##:5
Furthermore, the corresponding features for the seg-
mented word w-vlAv-wn are shown in Figure 1.
Each feature is associated with a weight, which
correlates with the likelihood that the correspond-
ing morpheme or context marks a valid morpholog-
ical segment. Such overlapping features allow us to
capture rich segmentation regularities. For example,
given the Arabic word Alywm, to derive its correct
segmentation Al-ywm, it helps to know that Al and
ywm are likely morphemes whereas Aly or lyw are
2The word as a whole is also treated as a morpheme in itself.
not; it also helps to know that Al ## or ## yw are
likely morpheme contexts whereas ly ## or ## wm
are not. Ablation tests verify the importance of these
overlapping features (see Section 7.2).
Our morpheme-context model is inspired by
the constituent-context model (CCM) proposed by
Klein and Manning (2001) for grammar induction.
The morphological segmentation of a word can be
viewed as a flat tree, where the root node corre-
sponds to the word and the leaves correspond to
morphemes (see Figure 1). The CCM uses uni-
grams for context features. For this task, however,
we found that bigrams and trigrams lead to much
better accuracy. We use trigrams in our full model.
For learning, one can either view the corpus as
a collection of word types (unique words) or tokens
(word occurrences). Some systems (e.g., Morfessor)
use token frequency for parameter estimation. Our
system, however, performs much better using word
types. This has also been observed for other mor-
phological learners (Goldwater et al, 2006). Thus
we use types in learning and inference, and effec-
tively enforce the constraint that words can have
only one segmentation per type. Evaluation is still
based on tokens to reflect the performance in real
applications.
In addition to the features of the morpheme-
context model, we incorporate two priors which cap-
ture additional intuitions about morphological seg-
mentations. First, we observe that the number of
distinct morphemes used to segment a corpus should
be small. This is achieved when the same mor-
phemes are re-used across many different words.
Our model incorporates this intuition by imposing
a lexicon prior: an exponential prior with nega-
tive weight on the length of the morpheme lexi-
con. We define the lexicon to be the set of unique
morphemes identified by a complete segmentation
of the corpus, and the lexicon length to be the to-
tal number of characters in the lexicon. In this
way, we can simultaneously emphasize that a lexi-
con should contain few unique morphemes, and that
those morphemes should be short. However, the lex-
icon prior alone incorrectly favors the trivial seg-
mentation that shatters each word into characters,
which results in the smallest lexicon possible (sin-
gle characters). Therefore, we also impose a corpus
prior: an exponential prior on the number of mor-
211
phemes used to segment each word in the corpus,
which penalizes over-segmentation. We notice that
longer words tend to have more morphemes. There-
fore, each word?s contribution to this prior is nor-
malized by the word?s length in characters (e.g., the
segmented word w-vlAv-wn contributes 3/7 to the to-
tal corpus size). Notice that it is straightforward to
incorporate such a prior in a log-linear model, but
much more challenging to do so in a directed gen-
erative model. These two priors are inspired by the
minimum description length (MDL) length princi-
ple; the lexicon prior favors fewer morpheme types,
whereas the corpus prior favors fewer morpheme to-
kens. They are vital to the success of our model,
providing it with the initial inductive bias.
We also notice that often a word is decomposed
into a stem and some prefixes and suffixes. This is
particularly true for languages with predominantly
inflectional morphology, such as Arabic, Hebrew,
and English. Thus our model uses separate lexicons
for prefixes, stems, and suffixes. This results in a
small but non-negligible accuracy gain in our exper-
iments. We require that a stem contain at least two
characters and no fewer characters than any affixes
in the same word.3 In a given word, when a mor-
pheme is identified as the stem, any preceding mor-
pheme is identified as a prefix, whereas any follow-
ing morpheme as a suffix. The sample segmented
corpus mentioned earlier induces the following lex-
icons:
Prefix w Al
Stem hnAk vlAv bn ywm jmAEp
Suffix wn w
Before presenting our formal model, we first in-
troduce some notation. Let W be a corpus (i.e., a set
of words), and S be a segmentation that breaks each
word in W into prefixes, a stem, and suffixes. Let ?
be a string (character sequence). Each occurrence of
? will be in the form of ?1??2, where ?1, ?2 are the
adjacent character n-grams, and c = (?1, ?2) is the
context of ? in this occurrence. Thus a segmentation
can be viewed as a set of morpheme strings and their
contexts. For a string x, L(x) denotes the number of
characters in x; for a word w, MS(w) denotes the
3In a segmentation where several morphemes have the max-
imum length, any of them can be identified as the stem, each
resulting in a distinct segmentation.
number of morphemes in w given the segmentation
S; Pref(W,S), Stem(W,S), Suff(W,S) denote
the lexicons of prefixes, stems, and suffixes induced
by S for W . Then, our model defines a joint proba-
bility distribution over a restricted set of W and S:
P?(W,S) = 1Z ? u?(W,S)
where
u?(W,S) = exp(
?
?
??f?(S) +
?
c
?cfc(S)
+ ? ? ?
??Pref(W,S)
L(?)
+ ? ? ?
??Stem(W,S)
L(?)
+ ? ? ?
??Suff(W,S)
L(?)
+ ? ? ?
w?W
MS(w)/L(w) )
Here, f?(S) and fc(S) are respectively the occur-
rence counts of morphemes and contexts under S,
and ? = (??, ?c : ?, c) are their feature weights.
?, ? are the weights for the priors. Z is the nor-
malization constant, which sums over a set of cor-
pora and segmentations. In the next section, we will
define this set for our model and show how to effi-
ciently perform learning and inference.
4 Unsupervised Learning
As mentioned in Smith & Eisner (2005), learning
with probabilistic models can be viewed as moving
probability mass to the observed data. The question
is from where to take this mass. For log-linear mod-
els, the answer amounts to defining the set that Z
sums over. We use contrastive estimation and define
the set to be a neighborhood of the observed data.
The instances in the neighborhood can be viewed
as pseudo-negative examples, and learning seeks to
discriminate them from the observed instances.
Formally, let W ? be the observed corpus, and let
N(?) be a function that maps a string to a set of
strings; let N(W ?) denote the set of all corpora that
can be derived from W ? by replacing every word
w ?W ? with one in N(w). Then,
Z = ?
W?N(W ?)
?
S
u(W,S).
212
Unsupervised learning maximizes the log-likelihood
of observing W ?
L?(W ?) = log
?
S
P (W ?, S)
We use gradient descent for this optimization; the
partial derivatives for feature weights are
?
??i
L?(W ?) = ES|W ?[fi]? ES,W [fi]
where i is either a string ? or a context c. The first
expected count ranges over all possible segmenta-
tions while the words are fixed to those observed in
W ?. For the second expected count, the words also
range over the neighborhood.
Smith & Eisner (2005) considered various neigh-
borhoods for unsupervised POS tagging, and
showed that the best neighborhoods are TRANS1
(transposing any pair of adjacent words) and
DELORTRANS1 (deleting any word or transposing
any pair of adjacent words). We can obtain their
counterparts for morphological segmentation by
simply replacing ?words? with ?characters?. As
mentioned earlier, the instances in the neighbor-
hood serve as pseudo-negative examples from which
probability mass can be taken away. In this regard,
DELORTRANS1 is suitable for POS tagging since
deleting a word often results in an ungrammatical
sentence. However, in morphology, a word less a
character is often a legitimate word too. For exam-
ple, deleting l from the Hebrew word lyhwh (to the
lord) results in yhwh (the lord). Thus DELORTRANS1
forces legal words to compete against each other for
probability mass, which seems like a misguided ob-
jective. Therefore, in our model we use TRANS1. It
is suited for our task because transposing a pair of
adjacent characters usually results in a non-word.
To combat overfitting in learning, we impose a
Gaussian prior (L2 regularization) on all weights.
5 Supervised Learning
Our learning algorithm can be readily applied to su-
pervised or semi-supervised learning. Suppose that
gold segmentation is available for some words, de-
noted as S?. If S? contains gold segmentations
for all words in W , we are doing supervised learn-
ing; otherwise, learning is semi-supervised. Train-
ing now maximizes L?(W ?, S?); the partial deriva-
tives become
?
??i
L?(W ?, S?) = ES|W ?,S?[fi] ? ES,W [fi]
The only difference in comparison with unsuper-
vised learning is that we fix the known segmenta-
tion when computing the first expected counts. In
Section 7.3, we show that when labels are available,
our model also learns much more effectively than a
directed graphical model.
6 Inference
In Smith & Eisner (2005), the objects (sentences) are
independent from each other, and exact inference is
tractable. In our model, however, the lexicon prior
renders all objects (words) interdependent in terms
of segmentation decisions. Consider the simple cor-
pus with just two words: Alrb, lAlrb. If lAlrb is seg-
mented into l-Al-rb, Alrb can be segmented into Al-
rb without paying the penalty imposed by the lexi-
con prior. If, however, lAlrb remains a single mor-
pheme, and we still segment Alrb into Al-rb, then
we introduce two new morphemes into the lexicons,
and we will be penalized by the lexicon prior ac-
cordingly. As a result, we must segment the whole
corpus jointly, making exact inference intractable.
Therefore, we resort to approximate inference. To
compute ES|W ?[fi], we use Gibbs sampling. To de-
rive a sample, the procedure goes through each word
and samples the next segmentation conditioned on
the segmentation of all other words. With m sam-
ples S1, ? ? ? , Sm, the expected count can be approx-
imated as
ES|W ?[fi] ? 1m
?
j
fi(Sj)
There are 2n?1 ways to segment a word of n char-
acters. To sample a new segmentation for a partic-
ular word, we need to compute conditional proba-
bility for each of these segmentations. We currently
do this by explicit enumeration.4 When n is large,
4These segmentations could be enumerated implicitly us-
ing the dynamic programming framework employed by semi-
Markov CRFs (Sarawagi and Cohen, 2004). However, in such a
setting, our lexicon prior would likely need to be approximated.
We intend to investigate this in future work.
213
this is very expensive. However, we observe that
the maximum number of morphemes that a word
contains is usually a small constant for many lan-
guages; in the Arabic Penn Treebank, the longest
word contains 14 characters, but the maximum num-
ber of morphemes in a word is only 5. Therefore,
we impose the constraint that a word can be seg-
mented into no more than k morphemes, where k
is a language-specific constant. We can determine
k from prior knowledge or use a development set.
This constraint substantially reduces the number of
segmentation candidates to consider; with k = 5, it
reduces the number of segmentations to consider by
almost 90% for a word of 14 characters.
ES,W [fi] can be computed by Gibbs sampling in
the same way, except that in each step we also sam-
ple the next word from the neighborhood, in addition
to the next segmentation.
To compute the most probable segmentation, we
use deterministic annealing. It works just like a sam-
pling algorithm except that the weights are divided
by a temperature, which starts with a large value and
gradually drops to a value close to zero. To make
burn-in faster, when computing the expected counts,
we initialize the sampler with the most probable seg-
mentation output by annealing.
7 Experiments
We evaluated our system on two datasets. Our main
evaluation is on a multi-lingual dataset constructed
by Snyder & Barzilay (2008a; 2008b). It consists of
6192 short parallel phrases in Hebrew, Arabic, Ara-
maic (a dialect of Arabic), and English. The paral-
lel phrases were extracted from the Hebrew Bible
and its translations via word alignment and post-
processing. For Arabic, the gold segmentation was
obtained using a highly accurate Arabic morpholog-
ical analyzer (Habash and Rambow, 2005); for He-
brew, from a Bible edition distributed by Westmin-
ster Hebrew Institute (Groves and Lowery, 2006).
There is no gold segmentation for English and Ara-
maic. Like Snyder & Barzilay, we evaluate on the
Arabic and Hebrew portions only; unlike their ap-
proach, our system does not use any bilingual in-
formation. We refer to this dataset as S&B . We
also report our results on the Arabic Penn Treebank
(ATB), which provides gold segmentations for an
Arabic corpus with about 120,000 Arabic words.
As in previous work, we report recall, precision,
and F1 over segmentation points. We used 500
phrases from the S&B dataset for feature develop-
ment, and also tuned our model hyperparameters
there. The weights for the lexicon and corpus pri-
ors were set to ? = ?1, ? = ?20. The feature
weights were initialized to zero and were penalized
by a Gaussian prior with ?2 = 100. The learning
rate was set to 0.02 for all experiments, except the
full Arabic Penn Treebank, for which it was set to
0.005.5 We used 30 iterations for learning. In each
iteration, 200 samples were collected to compute
each of the two expected counts. The sampler was
initialized by running annealing for 2000 samples,
with the temperature dropping from 10 to 0.1 at 0.1
decrements. The most probable segmentation was
obtained by running annealing for 10000 samples,
using the same temperature schedule. We restricted
the segmentation candidates to those with no greater
than five segments in all experiments.
7.1 Unsupervised Segmentation on S&B
We followed the experimental set-up of Snyder &
Barzilay (2008b) to enable a direct comparison. The
dataset is split into a training set with 4/5 of the
phrases, and a test set with the remaining 1/5. First,
we carried out unsupervised learning on the training
data, and computed the most probable segmentation
for it. Then we fixed the learned weights and the seg-
mentation for training, and computed the most prob-
able segmentation for the test set, on which we eval-
uated.6 Snyder & Barzilay (2008b) compared sev-
eral versions of their systems, differing in how much
bilingual information was used. Using monolingual
information only, their system (S&B-MONO) trails
the state-of-the-art system Morfessor; however, their
best system (S&B-BEST), which uses bilingual in-
formation that includes phrasal alignment and pho-
netic correspondence between Arabic and Hebrew,
outperforms Morfessor and achieves the state-of-
the-art results on this dataset.
5The ATB set is more than an order of magnitude larger and
requires a smaller rate.
6With unsupervised learning, we can use the entire dataset
for training since no labels are provided. However, this set-
up is necessary for S&B?s system because they used bilingual
information in training, which is not available at test time.
214
ARABIC Prec. Rec. F1
S&B-MONO 53.0 78.5 63.2
S&B-BEST 67.8 77.3 72.2
FULL 76.0 80.2 78.1
HEBREW Prec. Rec. F1
S&B-MONO 55.8 64.4 59.8
S&B-BEST 64.9 62.9 63.9
FULL 67.6 66.1 66.9
Table 1: Comparison of segmentation results on the S&B
dataset.
Table 1 compares our system with theirs. Our sys-
tem outperforms both S&B-MONO and S&B-BEST
by a large margin. For example, on Arabic, our sys-
tem reduces F1 error by 21% compared to S&B-
BEST, and by 40% compared to S&B-MONO. This
suggests that the use of monolingual morpheme con-
text, enabled by our log-linear model, is more help-
ful than their bilingual cues.
7.2 Ablation Tests
To evaluate the contributions of the major compo-
nents in our model, we conducted seven ablation
tests on the S&B dataset, each using a model that
differed from our full model in one aspect. The first
three tests evaluate the effect of priors, whereas the
next three test the effect of context features. The
last evaluates the impact of using separate lexicons
for affixes and stems.
NO-PRIOR The priors are not used.
NO-COR-PR The corpus prior is not used.
NO-LEX-PR The lexicon prior is not used.
NO-CONTEXT Context features are not used.
UNIGRAM Unigrams are used in context.
BIGRAM Bigrams are used in context.
SG-LEXICON A single lexicon is used, rather than
three distinct ones for the affixes and stems.
Table 2 presents the ablation results in compari-
son with the results of the full model. When some or
all priors are excluded, the F1 score drops substan-
tially (over 10 points in all cases, and over 40 points
in some). In particular, excluding the corpus prior,
as in NO-PRIOR and NO-COR-PR, results in over-
segmentation, as is evident from the high recalls and
low precisions. When the corpus prior is enacted
but not the lexicon priors (NO-LEX-PR), precision
ARABIC Prec. Rec. F1
FULL 76.0 80.2 78.1
NO-PRIOR 24.6 89.3 38.6
NO-COR-PR 23.7 87.4 37.2
NO-LEX-PR 79.1 51.3 62.3
NO-CONTEXT 71.2 62.1 66.3
UNIGRAM 71.3 76.5 73.8
BIGRAM 73.1 78.4 75.7
SG-LEXICON 72.8 82.0 77.1
HEBREW Prec. Rec. F1
FULL 67.6 66.1 66.9
NO-PRIOR 34.0 89.9 49.4
NO-COR-PR 35.6 90.6 51.1
NO-LEX-PR 65.9 49.2 56.4
NO-CONTEXT 63.0 47.6 54.3
UNIGRAM 63.0 63.7 63.3
BIGRAM 69.5 66.1 67.8
SG-LEXICON 67.4 65.7 66.6
Table 2: Ablation test results on the S&B dataset.
is much higher, but recall is low; the system now errs
on under-segmentation because recurring strings are
often not identified as morphemes.
A large accuracy drop (over 10 points in F1
score) also occurs when the context features are
excluded (NO-CONTEXT), which underscores the
importance of these overlapping features. We also
notice that the NO-CONTEXT model is compara-
ble to the S&B-MONO model; they use the same
feature types, but different priors. The accuracies of
the two systems are comparable, which suggests that
we did not sacrifice accuracy by trading the more
complex and restrictive Dirichlet process prior for
exponential priors. A priori, it is unclear whether us-
ing contexts larger than unigrams would help. While
potentially beneficial, they also risk aggravating the
data sparsity and making our model more prone to
overfitting. For this problem, however, enlarging the
context (using higher n-grams up to trigrams) helps
substantially. For Arabic, the highest accuracy is at-
tained by using trigrams, which reduces F1 error by
16% compared to unigrams; for Hebrew, by using
bigrams, which reduces F1 error by 17%. Finally, it
helps to use separate lexicons for affixes and stems,
although the difference is small.
215
ARABIC %Lbl. Prec. Rec. F1
S&B-MONO-S 100 73.2 92.4 81.7
S&B-BEST-S 200 77.8 92.3 84.4
FULL-S 25 84.9 85.5 85.2
50 88.2 86.8 87.5
75 89.6 86.4 87.9
100 91.7 88.5 90.0
HEBREW %Lbl. Prec. Rec. F1
S&B-MONO-S 100 71.4 79.1 75.1
S&B-BEST-S 200 76.8 79.2 78.0
FULL-S 25 78.7 73.3 75.9
50 82.8 74.6 78.4
75 83.1 77.3 80.1
100 83.0 78.9 80.9
Table 3: Comparison of segmentation results with super-
vised and semi-supervised learning on the S&B dataset.
7.3 Supervised and Semi-Supervised Learning
To evaluate our system in the supervised and semi-
supervised learning settings, we report the perfor-
mance when various amounts of labeled data are
made available during learning, and compare them
to the results of Snyder & Barzilay (2008a). They
reported results for supervised learning using mono-
lingual features only (S&B-MONO-S), and for su-
pervised bilingual learning with labels for both lan-
guages (S&B-BEST-S). On both languages, our sys-
tem substantially outperforms both S&B-MONO-S
and S&B-BEST-S. E.g., on Arabic, our system re-
duces F1 errors by 46% compared to S&B-MONO-
S, and by 36% compared to S&B-BEST-S. More-
over, with only one-fourth of the labeled data, our
system already outperforms S&B-MONO-S. This
demonstrates that our log-linear model is better
suited to take advantage of supervised labels.
7.4 Arabic Penn Treebank
We also evaluated our system on the Arabic Penn
Treebank (ATB). As is common in unsupervised
learning, we trained and evaluated on the entire set.
We compare our system with Morfessor (Creutz and
Lagus, 2007).7 In addition, we compare with Mor-
fessor Categories-MAP, which builds on Morfessor
and conducts an additional greedy search specifi-
cally tailored to segmentation. We found that it per-
7We cannot compare with Snyder & Barzilay?s system as its
strongest results require bilingual data, which is not available.
ATB-7000 Prec. Rec. F1
MORFESSOR-1.0 70.6 34.3 46.1
MORFESSOR-MAP 86.9 46.4 60.5
FULL 83.4 77.3 80.2
ATB Prec. Rec. F1
MORFESSOR-1.0 80.7 20.4 32.6
MORFESSOR-MAP 77.4 72.6 74.9
FULL 88.5 69.2 77.7
Table 4: Comparison of segmentation results on the Ara-
bic Penn Treebank.
forms much better than Morfessor on Arabic but
worse on Hebrew. To test each system in a low-
data setting, we also ran experiments on the set con-
taining the first 7,000 words in ATB with at least
two characters (ATB-7000). Table 4 shows the re-
sults. Morfessor performs rather poorly on ATB-
7000. Morfessor Categories-MAP does much bet-
ter, but its performance is dwarfed by our system,
which further cuts F1 error by half. On the full ATB
dataset, Morfessor performs even worse, whereas
Morfessor Categories-MAP benefits from the larger
dataset and achieves an F1 of 74.9. Still, our system
substantially outperforms it, further reducing F1 er-
ror by 11%.8
8 Conclusion
This paper introduces the first log-linear model for
unsupervised morphological segmentation. It lever-
ages overlapping features such as morphemes and
their contexts, and enables easy extension to incor-
porate additional features and linguistic knowledge.
For Arabic and Hebrew, it outperforms the state-
of-the-art systems by a large margin. It can also
be readily applied to supervised or semi-supervised
learning when labeled data is available. Future di-
rections include applying our model to other in-
flectional and agglutinative languages, modeling in-
ternal variations of morphemes, leveraging parallel
data in multiple languages, and combining morpho-
logical segmentation with other NLP tasks, such as
machine translation.
8Note that the ATB and ATB-7000 experiments each mea-
sure accuracy on their entire training set. This difference in
testing conditions explains why some full ATB results are lower
than ATB-7000.
216
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
In Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Michael R. Brent, Sreerama K. Murthy, and Andrew
Lundberg. 1995. Discovering morphemic suffixes: A
case study in minimum description length induction.
In Proceedings of the 15th Annual Conference of the
Cognitive Science Society.
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer version 2.0.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological
segmentation. In Proceedings of Human Language
Technology (NAACL).
Vera Demberg. 2007. A language-independent unsuper-
vised model for morphological segmentation. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics, Prague, Czech Repub-
lic.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens by
estimating power-law generators. In Advances in Neu-
ral Information Processing Systems 18.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word segmenta-
tion: Context is important. In Proceedings of the 31st
Boston University Conference on Language Develop-
ment.
Alan Groves and Kirk Lowery, editors. 2006. The West-
minster Hebrew Bible Morphology Database. West-
minster Hebrew Institute, Philadelphia, PA, USA.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics.
Zellig S. Harris. 1955. From phoneme to morpheme.
Language, 31(2):190?222.
Samarth Keshava and Emily Pitler. 2006. A simple, intu-
itive approach to morpheme induction. In Proceedings
of 2nd Pascal Challenges Workshop, Venice, Italy.
Dan Klein and Christopher D. Manning. 2001. Natu-
ral language grammar induction using a constituent-
context model. In Advances in Neural Information
Processing Systems 14.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of Morpho Challenge in CLEF 2007.
In Working Notes of the CLEF 2007 Workshop.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with markov logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 649?
658, Honolulu, HI. ACL.
Ronald Rosenfeld. 1997. A whole sentence maximum
entropy language model. In IEEE workshop on Auto-
matic Speech Recognition and Understanding.
Sunita Sarawagi and William Cohen. 2004. Semimarkov
conditional random fields for information extraction.
In Proceedings of the Twenty First International Con-
ference on Machine Learning.
Patrick Schone and Daniel Jurafsky. 2001. Knowlege-
free induction of inflectional morphologies. In Pro-
ceedings of Human Language Technology (NAACL).
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008a. Cross-
lingual propagation for morphological analysis. In
Proceedings of the Twenty Third National Conference
on Artificial Intelligence.
Benjamin Snyder and Regina Barzilay. 2008b. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics.
217
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 308?316,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
On the Syllabification of Phonemes
Susan Bartlett? and Grzegorz Kondrak? and Colin Cherry?
?Department of Computing Science ?Microsoft Research
University of Alberta One Microsoft Way
Edmonton, AB, T6G 2E8, Canada Redmond, WA, 98052
{susan,kondrak}@cs.ualberta.ca colinc@microsoft.com
Abstract
Syllables play an important role in speech
synthesis and recognition. We present sev-
eral different approaches to the syllabifica-
tion of phonemes. We investigate approaches
based on linguistic theories of syllabification,
as well as a discriminative learning technique
that combines Support Vector Machine and
Hidden Markov Model technologies. Our
experiments on English, Dutch and German
demonstrate that our transparent implemen-
tation of the sonority sequencing principle
is more accurate than previous implemen-
tations, and that our language-independent
SVM-based approach advances the current
state-of-the-art, achieving word accuracy of
over 98% in English and 99% in German and
Dutch.
1 Introduction
Syllabification is the process of dividing a word
into its constituent syllables. Although some work
has been done on syllabifying orthographic forms
(Mu?ller et al, 2000; Bouma, 2002; Marchand and
Damper, 2007; Bartlett et al, 2008), syllables are,
technically speaking, phonological entities that can
only be composed of strings of phonemes. Most
linguists view syllables as an important unit of
prosody because many phonological rules and con-
straints apply within syllables or at syllable bound-
aries (Blevins, 1995).
Apart from their purely linguistic significance,
syllables play an important role in speech synthesis
and recognition (Kiraz and Mo?bius, 1998; Pearson
et al, 2000). The pronunciation of a given phoneme
tends to vary depending on its location within a syl-
lable. While actual implementations vary, text-to-
speech (TTS) systems must have, at minimum, three
components (Damper, 2001): a letter-to-phoneme
(L2P) module, a prosody module, and a synthesis
module. Syllabification can play a role in all three
modules.
Because of the productive nature of language, a
dictionary look-up process for syllabification is in-
adequate. No dictionary can ever contain all possi-
ble words in a language. For this reason, it is neces-
sary to develop systems that can automatically syl-
labify out-of-dictionary words.
In this paper, we advance the state-of-the-art
in both categorical (non-statistical) and supervised
syllabification. We outline three categorical ap-
proaches based on common linguistic theories of
syllabification. We demonstrate that when imple-
mented carefully, such approaches can be very ef-
fective, approaching supervised performance. We
also present a data-driven, discriminative solution:
a Support Vector Machine Hidden Markov Model
(SVM-HMM), which tags each phoneme with its
syllabic role. Given enough data, the SVM-HMM
achieves impressive accuracy thanks to its ability
to capture context-dependent generalizations, while
also memorizing inevitable exceptions. Our ex-
periments on English, Dutch and German demon-
strate that our SVM-HMM approach substantially
outperforms the existing state-of-the-art learning ap-
proaches. Although direct comparisons are difficult,
our system achieves over 99% word accuracy on
German and Dutch, and the highest reported accu-
racy on English.
The paper is organized as follows. We outline
common linguistic theories of syllabification in Sec-
tion 2, and we survey previous computational sys-
308
tems in Section 3. Our linguistically-motivated ap-
proaches are described in Section 4. In Section 5,
we describe our system based on the SVM-HMM.
The experimental results are presented in Section 6.
2 Theories of Syllabification
There is some debate as to the exact structure of
a syllable. However, phonologists are in gen-
eral agreement that a syllable consists of a nucleus
(vowel sound), preceded by an optional onset and
followed by an optional coda. In many languages,
both the onset and the coda can be complex, i.e.,
composed of more than one consonant. For exam-
ple, the word breakfast [brEk-fst] contains two syl-
lables, of which the first has a complex onset [br],
and the second a complex coda [st]. Languages dif-
fer with respect to various typological parameters,
such as optionality of onsets, admissibility of co-
das, and the allowed complexity of the syllable con-
stituents. For example, onsets are required in Ger-
man, while Spanish prohibits complex codas.
There are a number of theories of syllabification;
we present three of the most prevalent. The Legal-
ity Principle constrains the segments that can be-
gin and end syllables to those that appear at the be-
ginning and end of words. In other words, a sylla-
ble is not allowed to begin with a consonant clus-
ter that is not found at the beginning of some word,
or end with a cluster that is not found at the end of
some word (Goslin and Frauenfelder, 2001). Thus,
a word like admit [dmIt] must be syllabified [d-
mIt] because [dm] never appears word-initially or
word-finally in English. A shortcoming of the le-
gality principle is that it does not always imply a
unique syllabification. For example, in a word like
askew [skju], the principle does not rule out any of
[-skju], [s-kju], or [sk-ju], as all three employ le-
gal onsets and codas.
The Sonority Sequencing Principle (SSP) pro-
vides a stricter definition of legality. The sonor-
ity of a sound is its inherent loudness, holding fac-
tors like pitch and duration constant (Crystal, 2003).
Low vowels like [a], the most sonorous sounds, are
high on the sonority scale, while plosive consonants
like [t] are at the bottom. When syllabifying a
word, SSP states that sonority should increase from
the first phoneme of the onset to the syllable?s nu-
cleus, and then fall off to the coda (Selkirk, 1984).
Consequently, in a word like vintage [vIntI], we
can rule out a syllabification like [vI-ntI] because
[n] is more sonorant than [t]. However, SSP does
not tell us whether to prefer [vIn-tI] or [vInt-I].
Moreover, when syllabifying a word like vintner
[vIntnr], the theory allows both [vIn-tnr] and [vInt-
nr], even though [tn] is an illegal onset in English.
Both the Legality Principle and SSP tell us which
onsets and codas are permitted in legal syllables, and
which are not. However, neither theory gives us any
guidance when deciding between legal onsets. The
Maximal Onset Principle addresses this by stating
we should extend a syllable?s onset at the expense
of the preceding syllable?s coda whenever it is legal
to do so (Kahn, 1976). For example, the principle
gives preference to [-skju] and [vIn-tI] over their
alternatives.
3 Previous Computational Approaches
Unlike tasks such as part of speech tagging or syn-
tactic parsing, syllabification does not involve struc-
tural ambiguity. It is generally believed that syllable
structure is usually predictable in a language pro-
vided that the rules have access to all conditioning
factors: stress, morphological boundaries, part of
speech, etymology, etc. (Blevins, 1995). However,
in speech applications, the phonemic transcription of
a word is often the only linguistic information avail-
able to the system. This is the common assumption
underlying a number of computational approaches
that have been proposed for the syllabification of
phonemes.
Daelemans and van den Bosch (1992) present one
of the earliest systems on automatic syllabification:
a neural network-based implementation for Dutch.
Daelemans et al (1997) also explore the application
of exemplar-based generalization (EBG), sometimes
called instance-based learning. EBG generally per-
forms a simple database look-up to syllabify a test
pattern, choosing the most common syllabification.
In cases where the test pattern is not found in the
database, the most similar pattern is used to syllab-
ify the test pattern.
Hidden Markov Models (HMMs) are another
popular approach to syllabification. Krenn (1997)
introduces the idea of treating syllabification as a
309
tagging task. Working from a list of syllabified
phoneme strings, she automatically generates tags
for each phone. She uses a second-order HMM to
predict sequences of tags; syllable boundaries can be
trivially recovered from the tags. Demberg (2006)
applies a fourth-order HMM to the syllabification
task, as a component of a larger German text-to-
speech system. Schmid et al (2007) improve on
Demberg?s results by applying a fifth-order HMM
that conditions on both the previous tags and their
corresponding phonemes.
Kiraz and Mo?bius (1998) present a weighted
finite-state-based approach to syllabification. Their
language-independent method builds an automaton
for each of onsets, nuclei, and codas, by count-
ing occurrences in training data. These automatons
are then composed into a transducer accepting se-
quences of one or more syllables. They do not report
quantitative results for their method.
Pearson et al (2000) compare two rule-based sys-
tems (they do not elaborate on the rules employed)
with a CART decision tree-based approach and a
?global statistics? algorithm. The global statistics
method is based on counts of consonant clusters
in contexts such as word boundaries, short vow-
els, or long vowels. Each test word has syllable
boundaries placed according to the most likely lo-
cation given a cluster and its context. In experi-
ments performed with their in-house dataset, their
statistics-based method outperforms the decision-
tree approach and the two rule-based methods.
Mu?ller (2001) presents a hybrid of a categori-
cal and data-driven approach. First, she manually
constructs a context-free grammar of possible sylla-
bles. This grammar is then made probabilistic using
counts obtained from training data. Mu?ller (2006)
attempts to make her method language-independent.
Rather than hand-crafting her context-free grammar,
she automatically generates all possible onsets, nu-
clei, and codas, based on the phonemes existing in
the language. The results are somewhat lower than
in (Mu?ller, 2001), but the approach can be more eas-
ily ported across languages.
Goldwater and Johnson (2005) also explore us-
ing EM to learn the structure of English and Ger-
man phonemes in an unsupervised setting, following
Mu?ller in modeling syllable structure with PCFGs.
They initialize their parameters using a deterministic
parser implementing the sonority principle and esti-
mate the parameters for their maximum likelihood
approach using EM.
Marchand et al (2007) apply their Syllabification
by Analogy (SbA) technique, originally developed
for orthographic forms, to the pronunciation do-
main. For each input word, SbA finds the most sim-
ilar substrings in a lexicon of syllabified phoneme
strings and then applies the dictionary syllabifica-
tions to the input word. Their survey paper also in-
cludes comparisons with a method broadly based on
the legality principle. The authors find their legality-
based implementation fares significantly worse than
SbA.
4 Categorical Approaches
Categorical approaches to syllabification are appeal-
ing because they are efficient and linguistically intu-
itive. In addition, they require little or no syllable-
annotated data. We present three categorical al-
gorithms that implement the linguistic insights out-
lined in Section 2. All three can be viewed as vari-
ations on the basic pseudo-code shown in Figure 1.
Every vowel is labeled as a nucleus, and every con-
sonant is labeled as either an onset or a coda. The
algorithm labels all consonants as onsets unless it is
illegal to do so. Given the labels, it is straightfor-
ward to syllabify a word. The three methods differ
in how they determine a ?legal? onset.
As a rough baseline, the MAXONSET implemen-
tation considers all combinations of consonants to be
legal onsets. Only word-final consonants are labeled
as codas.
LEGALITY combines the Legality Principle with
onset maximization. In our implementation, we col-
lect all word-initial consonant clusters from the cor-
pus and deem them to be legal onsets. With this
method, no syllable can have an onset that does not
appear word-initially in the training data. We do not
test for the legality of codas. The performance of
LEGALITY depends on the number of phonetic tran-
scriptions that are available, but the transcriptions
need not be annotated with syllable breaks.
SONORITY combines the Sonority Sequencing
Principle with onset maximization. In this approach,
an onset is considered legal if every member of the
onset ranks lower on the sonority scale than ensuing
310
until current phoneme is a vowel
label current phoneme as an onset
end loop
until all phonemes have been labeled
label current phoneme as a nucleus
if there are no more vowels in the word
label all remaining consonants as codas
else
onset := all consonants before next vowel
coda := empty
until onset is legal
coda := coda plus first phoneme of onset
onset := onset less first phoneme
end loop
end if
end loop
Insert syllable boundaries before onsets
Figure 1: Pseudo-code for syllabifying a string of
phonemes.
consonants. SONORITY requires no training data be-
cause it implements a sound linguistic theory. How-
ever, an existing development set for a given lan-
guage can help with defining and validating addi-
tional language-specific constraints.
Several sonority scales of varying complexity
have been proposed. For example, Selkirk (1984)
specifies a hierarchy of eleven distinct levels. We
adopt a minimalistic scale shown in Figure 2. which
avoids most of the disputed sonority contrasts (Jany
et al, 2007). We set the sonority distance parame-
ter to 2, which ensures that adjacent consonants in
the onset differ by at least two levels of the scale.
For example, [pr] is an acceptable onset because it
is composed of an obstruent and a liquid, but [pn] is
not, because nasals directly follow obstruents on our
sonority scale.
In addition, we incorporate several English-
specific constraints listed by Kenstowicz (1994,
pages 257?258). The constraints, or filters, prohibit
complex onsets containing:
(i) two labials (e.g., [pw], [bw], [fw], [vw]),
(ii) a non-strident coronal followed by a lateral
(e.g., [tl], [dl], [Tl])
(iii) a voiced fricative (e.g., [vr], [zw], except [vj]),
(iv) a palatal consonant (e.g., [Sl], [?r], except [Sr]).
Sound Examples Level
Vowels u, , . . . 4
Glides w, j, . . . 3
Liquids l, r, . . . 2
Nasals m, N, . . . 1
Obstruents g, T, . . . 0
Figure 2: The sonority scale employed by SONORITY.
A special provision allows for prepending the
phoneme [s] to onsets beginning with a voiceless
plosive. This reflects the special status of [s] in En-
glish, where onsets like [sk] and [sp] are legal even
though the sonority is not strictly increasing.
5 Supervised Approach: SVM-HMM
If annotated data is available, a classifier can be
trained to predict the syllable breaks. A Support
Vector Machine (SVM) is a discriminative super-
vised learning technique that allows for a rich fea-
ture representation of the input space. In principle,
we could use a multi-class SVM to classify each
phoneme according to its position in a syllable on
the basis of a set of features. However, a traditional
SVM would treat each phoneme in a word as an in-
dependent instance, preventing us from considering
interactions between labels. In order to overcome
this shortcoming, we employ an SVM-HMM1 (Al-
tun et al, 2003), an instance of the Structured SVM
formalism (Tsochantaridis et al, 2004) that has been
specialized for sequence tagging.
When training a structured SVM, each training
instance xi is paired with its label yi, drawn from
the set of possible labels, Yi. In our case, the train-
ing instances xi are words, represented as sequences
of phonemes, and their labels yi are syllabifications,
represented as sequences of onset/nucleus/coda tags.
For each training example, a feature vector ?(x, y)
represents the relationship between the example and
a candidate tag sequence. The SVM finds a weight
vector w, such that w ??(x, y) separates correct tag-
gings from incorrect taggings by as large a margin
as possible. Hamming distance DH is used to cap-
ture how close a wrong sequence y is to yi, which
1http://svmlight.joachims.org/svm struct.html
311
in turn impacts the required margin. Tag sequences
that share fewer tags in common with the correct se-
quence are separated by a larger margin.
Mathematically, a (simplified) statement of the
SVM learning objective is:
?i?y?Yi,y 6=yi :[?(xi, yi) ? w > ?(xi, y) ? w +DH(y, yi)] (1)
This objective is only satisfied when w tags all train-
ing examples correctly. In practice, slack variables
are introduced, which allow us to trade off training
accuracy and the complexity of w via a cost parame-
ter. We tune this parameter on our development set.
The SVM-HMM training procedure repeatedly
uses the Viterbi algorithm to find, for the current
w and each (xi, yi) training pair, the sequence y
that most drastically violates the inequality shown in
Equation 1. These incorrect tag sequences are added
to a growing set, which constrains the quadratic op-
timization procedure used to find the next w. The
process iterates until no new violating sequences are
found, producing an approximation to the inequality
over all y ? Yi. A complete explanation is given by
Tsochantaridis et al (2004).
Given a weight vector w, a structured SVM tags
new instances x according to:
argmaxy?Y [?(x, y) ? w] (2)
The SVM-HMM gets the HMM portion of its name
from its use of the HMM Viterbi algorithm to solve
this argmax.
5.1 Features
We investigated several tagging schemes, described
in detail by Bartlett (2007). During development,
we found that tagging each phoneme with its syl-
labic role (Krenn, 1997) works better than the simple
binary distinction between syllable-final and other
phonemes (van den Bosch, 1997). We also dis-
covered that accuracy can be improved by number-
ing the tags. Therefore, in our tagging scheme, the
single-syllable word strengths [strENTs] would be la-
beled with the sequence {O1 O2 O3 N1 C1 C2 C3}.
Through the use of the Viterbi algorithm, our fea-
ture vector ?(x, y) is naturally divided into emis-
sion and transition features. Emission features link
an aspect of the input word x with a single tag in the
Method English
MAXONSET 61.38
LEGALITY 93.16
SONORITY 95.00
SVM-HMM 98.86
tsylb 93.72
Table 1: Word accuracy on the CELEX dataset.
sequence y. Unlike a generative HMM, these emis-
sion features do not require any conditional indepen-
dence assumptions. Transition features link tags to
tags. Our only transition features are counts of adja-
cent tag pairs occurring in y.
For the emission features, we use the current
phoneme and a fixed-size context window of sur-
rounding phonemes. Thus, the features for the
phoneme [k] in hockey [hAki] might include the [A]
preceding it, and the [i] following it. In experiments
on our development set, we found that the optimal
window size is nine: four phonemes on either side
of the focus phoneme. Because the SVM-HMM is a
linear classifier, we need to explicitly state any im-
portant conjunctions of features. This allows us to
capture more complex patterns in the language that
unigrams alone cannot describe. For example, the
bigram [ps] is illegal as an onset in English, but per-
fectly reasonable as a coda. Experiments on the de-
velopment set showed that performance peaked us-
ing all unigrams, bigrams, trigrams, and four-grams
found within our context window.
6 Syllabification Experiments
We developed our approach using the English por-
tion of the CELEX lexical database (Baayen et al,
1995). CELEX provides the phonemes of a word
and its correct syllabification. It does not designate
the phonemes as onsets, nuclei, or codas, which is
the labeling we want to predict. Fortunately, extract-
ing the labels from a syllabified word is straightfor-
ward. All vowel phones are assigned to be nuclei;
consonants preceding the nucleus in a syllable are
assigned to be onsets, while consonants following
the nucleus in a syllable are assigned to be codas.
The results in Table 1 were obtained on a test set
of 5K randomly selected words. For training the
SVM-HMM, we randomly selected 30K words not
312
appearing in the test set, while 6K training examples
were held out for development testing. We report
the performance in terms of word accuracy (entire
words syllabified correctly). Among the categori-
cal approaches, SONORITY clearly outperforms not
only LEGALITY, but also tsylb (Fisher, 1996), an
implementation of the complex algorithm of Kahn
(1976), which makes use of lists of legal English
onsets. Overall, our SVM-based approach is a clear
winner.
The results of our discriminative method com-
pares favorably with the results of competing ap-
proaches on English CELEX. Since there are no
standard train-test splits for syllabification, the
comparison is necessarily indirect, but note that
our training set is substantially smaller. For
her language-independent PCFG-based approach,
Mu?ller (2006) reports 92.64% word accuracy on the
set of 64K examples from CELEX using 10-fold
cross-validation. The Learned EBG approach of
van den Bosch (1997) achieves 97.78% word accu-
racy when training on approximately 60K examples.
Therefore, our results represent a nearly 50% reduc-
tion of the error rate.
Figure 3: Word accuracy on English CELEX as a func-
tion of the number of thousands of training examples.
Though the SVM-HMM?s training data require-
ments are lower than previous supervised syllabi-
fication approaches, they are still substantial. Fig-
ure 3 shows a learning curve over varying amounts
of training data. Performance does not reach accept-
able levels until 5K training examples are provided.
6.1 Error Analysis
There is a fair amount of overlap in the errors made
by the SVM-HMM and the SONORITY. Table 4
shows a few characteristic examples. The CELEX
syllabifications of tooth-ache and pass-ports fol-
low the morphological boundaries of the compound
words. Morphological factors are a source of er-
rors for both approaches, but significantly more so
for SONORITY. The performance difference comes
mainly from the SVM?s ability to handle many of
these morphological exceptions. The SVM gener-
ates the correct syllabification of northeast [norT-
ist], even though an onset of [T] is perfectly legal.
On the other hand, the SVM sometimes overgener-
alizes, as in the last example in Table 4.
SVM-HMM SONORITY
tu-Tek tu-Tek toothache
pae-sports pae-sports passports
norT-ist nor-Tist northeast
dIs-plizd dI-splizd displeased
dIs-koz dI-skoz discos
Figure 4: Examples of syllabification errors. (Correct
syllabifications are shown in bold.)
6.2 The NETtalk Dataset
Marchand et al (2007) report a disappointing word
accuracy of 54.14% for their legality-based imple-
mentation, which does not accord with the results
of our categorical approaches on English CELEX.
Consequently, we also apply our methods to the
dataset they used for their experiments: the NETtalk
dictionary. NETtalk contains 20K English words; in
the experiments reported here, we use 13K training
examples and 7K test words.
As is apparent from Table 2, our performance
degrades significantly when switching to NETtalk.
The steep decline found in the categorical meth-
ods is particularly notable, and indicates significant
divergence between the syllabifications employed
in the two datasets. Phonologists do not always
agree on the correct syllable breaks for a word,
but the NETtalk syllabifications are often at odds
with linguistic intuitions. We randomly selected 50
words and compared their syllabifications against
those found in Merriam-Webster Online. We found
that CELEX syllabifications agree with Merriam-
Webster in 84% of cases, while NETtalk only agrees
52% of the time.
Figure 5 shows several words from the NETtalk
313
Method English
MAXONSET 33.64
SONORITY 52.80
LEGALITY 53.08
SVM-HMM 92.99
Table 2: Word accuracy on the NETtalk dataset.
and CELEX datasets. We see that CELEX fol-
lows the maximal onset principle consistently, while
NETtalk does in some instances but not others. We
also note that there are a number of NETtalk syllab-
ifications that are clearly wrong, such as the last two
examples in Figure 5. The variability of NETtalk
is much more difficult to capture with any kind of
principled approach. Thus, we argue that low per-
formance on NETtalk indicate inconsistent syllabi-
fications within that dataset, rather than any actual
deficiency of the methods.
NETtalk CELEX
?aes-taIz ?ae-staIz chastise
rEz-Id-ns rE-zI-dns residence
dI-strOI dI-strOI destroy
fo-tAn fo-tAn photon
Ar-pE-io Ar-pE-i-o arpeggio
Der--baU-t DE-r-baUt thereabout
Figure 5: Examples of CELEX and NETtalk syllabifica-
tions.
NETtalk?s variable syllabification practices
notwithstanding, the SVM-HMM approach still
outperforms the previous benchmark on the dataset.
Marchand et al (2007) report 88.53% word accu-
racy for their SbA technique using leave-one-out
testing on the entire NETtalk set (20K words). With
fewer training examples, we reduce the error rate by
almost 40%.
6.3 Other Languages
We performed experiments on German and Dutch,
the two other languages available in the CELEX lex-
ical database. The German and Dutch lexicons of
CELEX are larger than the English lexicon. For both
languages, we selected a 25K test set, and two dif-
ferent training sets, one containing 50K words and
the other containing 250K words. The results are
Method German Dutch
MAXONSET 19.51 23.44
SONORITY 76.32 77.51
LEGALITY 79.55 64.31
SVM-HMM (50K words) 99.26 97.79
SVM-HMM (250K words) 99.87 99.16
Table 3: Word accuracy on the CELEX dataset.
presented in Table 3.
While our SVM-HMM approach is entirely lan-
guage independent, the same cannot be said about
other methods. The maximal onset principle appears
to hold much more strongly for English than for Ger-
man and Dutch (e.g., patron: [pe-trn] vs. [pat-ron]).
LEGALITY and SONORITY also appear to be less
effective, possibly because of greater tendency for
syllabifications to match morphological boundaries
(e.g., English exclusive: [Ik-sklu-sIv] vs. Dutch ex-
clusief [Eks-kly-zif]). SONORITY is further affected
by our decision to employ the constraints of Ken-
stowicz (1994), although they clearly pertain to En-
glish. We expect that adapting them to specific lan-
guages would bring the results closer to the level of
the English experiments.
Although our SVM system is tuned using an
English development set, the results on both Ger-
man and Dutch are excellent. We could not find
any quantitative data for comparisons on Dutch,
but the comparison with the previously reported re-
sults on German CELEX demonstrates the qual-
ity of our approach. The numbers that follow re-
fer to 10-fold cross-validation on the entire lex-
icon (over 320K entries) unless noted otherwise.
Krenn (1997) obtains tag accuracy of 98.34%, com-
pared to our system?s tag accuracy of 99.97% when
trained on 250K words. With a hand-crafted gram-
mar, Mu?ller (2002) achieves 96.88% word accuracy
on CELEX-derived syllabifications, with a training
corpus of two million tokens. Without a hand-
crafted grammar, she reports 90.45% word accu-
racy (Mu?ller, 2006). Applying a standard smoothing
algorithm and fourth-order HMM, Demberg (2006)
scores 98.47% word accuracy. A fifth-order joint
N -gram model of Schmid et al (2007) achieves
99.85% word accuracy with about 278K training
points. However, unlike generative approaches, our
314
Method English German
SONORITY 97.0 94.2
SVM-HMM 99.9 99.4
Categorical Parser 94.9 92.7
Maximum Likelihood 98.1 97.4
Table 4: Word accuracy on the datasets of Goldwater and
Johnson (2005).
SVM-HMM can condition each emission on large
portions of the input using only a first-order Markov
model, which implies much faster syllabification
performance.
6.4 Direct Comparison with an MLE approach
The results of the competitive approaches that have
been quoted so far (with the exception of tsylb)
are not directly comparable, because neither the re-
spective implementations, nor the actual train-test
splits are publicly available. However, we managed
to obtain the English and German data sets used
by Goldwater and Johnson (2005) in their study,
which focused primarily on unsupervised syllabi-
fication. Their experimental framework is similar
to (Mu?ller, 2001). They collect words from running
text and create a training set of 20K tokens and a
test set of 10K tokens. The running text was taken
from the Penn WSJ and ECI corpora, and the syl-
labified phonemic transcriptions were obtained from
CELEX. Table 4 compares our experimental results
with their reported results obtained with: (a) su-
pervised Maximum Likelihood training procedures,
and (b) a Categorical Syllable Parser implementing
the principles of sonority sequencing and onset max-
imization without Kenstowicz?s (1994) onset con-
straints.
The accuracy figures in Table 4 are noticeably
higher than in Table 1. This stems from fundamen-
tal differences in the experimental set-up; Goldwater
and Johnson (2005) test on tokens as found in text,
therefore many frequent short words are duplicated.
Furthermore, some words occur during both training
and testing, to the benefit of the supervised systems
(SVM-HMM and Maximum Likelihood). Neverthe-
less, the results confirm the level of improvement
obtained by both our categorical and supervised ap-
proaches.
7 Conclusion
We have presented several different approaches to
the syllabification of phonemes. The results of our
linguistically-motivated algorithms, show that it is
possible to achieve adequate syllabification word
accuracy in English with no little or no syllable-
annotated training data. We have demonstrated that
the poor performance of categorical methods on En-
glish NETtalk actually points to problems with the
NETtalk annotations, rather than with the methods
themselves.
We have also shown that SVM-HMMs can be
used to great effect when syllabifying phonemes.
In addition to being both efficient and language-
independent, they establish a new state-of-the-art for
English and Dutch syllabification. However, they
do require thousands of labeled training examples to
achieve this level of accuracy. In the future, we plan
to explore a hybrid approach, which would benefit
from both the generality of linguistic principles and
the smooth exception-handling of supervised tech-
niques, in order to make best use of whatever data is
available.
Acknowledgements
We are grateful to Sharon Goldwater for providing
the experimental data sets for comparison. This re-
search was supported by the Natural Sciences and
Engineering Research Council of Canada and the
Alberta Informatics Circle of Research Excellence.
References
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden markov support vector ma-
chines. Proceedings of the 20Th International Confer-
ence on Machine Learning (ICML).
R. Baayen, R. Piepenbrock, and L. Gulikers. 1995. The
CELEX lexical database (CD-ROM).
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2008. Automatic syllabification with structured SVMs
for letter-to-phoneme conversion. In Proceedings of
ACL-08: HLT, pages 568?576, Columbus, Ohio.
Susan Bartlett. 2007. Discriminative approach to auto-
matic syllabification. Master?s thesis, Department of
Computing Science, University of Alberta.
Juliette Blevins. 1995. The syllable in phonological
theory. In John Goldsmith, editor, The handbook of
phonological theory, pages 206?244. Blackwell.
315
Gosse Bouma. 2002. Finite state methods for hyphen-
ation. Natural Language Engineering, 1:1?16.
David Crystal. 2003. A dictionary of linguistics and pho-
netics. Blackwell.
Walter Daelemans and Antal van den Bosch. 1992. Gen-
eralization performance of backpropagaion learning
on a syllabification task. In Proceedings of the 3rd
Twente Workshop on Language Technology, pages 27?
38.
Walter Daelemans, Antal van den Bosch, and Ton Wei-
jters. 1997. IGTree: Using trees for compression and
classification in lazy learning algorithms. Artificial In-
telligence Review, pages 407?423.
Robert Damper. 2001. Learning about speech from
data: Beyond NETtalk. In Data-Driven Techniques in
Speech Synthesis, pages 1?25. Kluwer Academic Pub-
lishers.
Vera Demberg. 2006. Letter-to-phoneme conversion for
a German text-to-speech system. Master?s thesis, Uni-
versity of Stuttgart.
William Fisher. 1996. Tsylb syllabification package.
ftp://jaguar.ncsl.nist.gov/pub/tsylb2-1.1.tar.Z. Last ac-
cessed 31 March 2008.
Sharon Goldwater and Mark Johnson. 2005. Represen-
tational bias in usupervised learning of syllable struc-
ture. In Prcoeedings of the 9th Conference on Compu-
tational Natural Language Learning (CoNLL), pages
112?119.
Jeremy Goslin and Ulrich Frauenfelder. 2001. A com-
parison of theoretical and human syllabification. Lan-
guage and Speech, 44:409?436.
Carmen Jany, Matthew Gordon, Carlos M Nash, and
Nobutaka Takara. 2007. How universal is the sonor-
ity hierarchy? A cross-linguistic study. In 16th Inter-
national Congress of Phonetic Sciences, pages 1401?
1404.
Daniel Kahn. 1976. Syllable-based generalizations in
English Phonology. Ph.D. thesis, Indiana University.
Michael Kenstowicz. 1994. Phonology in Generative
Grammar. Blackwell.
George Kiraz and Bernd Mo?bius. 1998. Multilingual
syllabification using weighted finite-state transducers.
In Proceedings of the 3rd Workshop on Speech Synthe-
sis.
Brigitte Krenn. 1997. Tagging syllables. In Proceedings
of Eurospeech, pages 991?994.
Yannick Marchand and Robert Damper. 2007. Can syl-
labification improve pronunciation by analogy of En-
glish? Natural Language Engineering, 13(1):1?24.
Yannick Marchand, Connie Adsett, and Robert Damper.
2007. Automatic syllabification in English: A com-
parison of different algorithms. Language and Speech.
To appear.
Karin Mu?ller, Bernd Mo?bius, and Detlef Prescher. 2000.
Inducing probabilistic syllable classes using multivari-
ate clustering. In Prcoeedings of the 38th meeting of
the ACL.
Karin Mu?ller. 2001. Automatic detection of syllable
boundaries combining the advantages of treebank and
bracketed corpora training. Proceedings on the 39Th
Meeting of the ACL.
Karin Mu?ller. 2002. Probabilistic context-free grammars
for phonology. Proceedings of the 6th Workshop of the
ACL Special Interest Group in Computational Phonol-
ogy (SIGPHON), pages 80?90.
Karin Mu?ller. 2006. Improving syllabification mod-
els with phonotactic knowledge. Proceedings of the
Eighth Meeting of the ACL Special Interest Group on
Computational Phonology At HLT-NAACL.
Steve Pearson, Roland Kuhn, Steven Fincke, and Nick
Kibre. 2000. Automatic methods for lexical stress as-
signment and syllabification. In Proceedings of the 6th
International Conference on Spoken Language Pro-
cessing (ICSLP).
Helmut Schmid, Bernd Mo?bius, and Julia Weidenkaff.
2007. Tagging syllable boundaries with joint N-gram
models. In Proceedings of Interspeech.
Elisabeth Selkirk. 1984. On the major class features and
syllable theory. In Language Sound Structure. MIT
Press.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. Proceedings of the 21st International
Conference on Machine Learning (ICML).
Antal van den Bosch. 1997. Learning to pronounce
written words: a study in inductive language learning.
Ph.D. thesis, Universiteit Maastricht.
316
Proceedings of NAACL HLT 2009: Short Papers, pages 1?4,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Cohesive Constraints in A Beam Search Phrase-based Decoder
Nguyen Bach and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, stephan.vogel}@cs.cmu.edu
Colin Cherry
Microsoft Research
One Microsoft Way
Redmond, WA, 98052, USA
collinc@microsoft.com
Abstract
Cohesive constraints allow the phrase-based decoder
to employ arbitrary, non-syntactic phrases, and en-
courage it to translate those phrases in an order that
respects the source dependency tree structure. We
present extensions of the cohesive constraints, such
as exhaustive interruption count and rich interrup-
tion check. We show that the cohesion-enhanced de-
coder significantly outperforms the standard phrase-
based decoder on English?Spanish. Improvements
between 0.5 and 1.2 BLEU point are obtained on
English?Iraqi system.
1 Introduction
Phrase-based machine translation is driven by a phrasal
translation model, which relates phrases (contiguous seg-
ments of words) in the source to phrases in the tar-
get. This translation model can be derived from a word-
aligned bitext. Translation candidates are scored accord-
ing to a linear model combining several informative fea-
ture functions. Crucially, this model incorporates trans-
lation model scores and n-gram language model scores.
The component features are weighted to minimize a
translation error criterion on a development set (Och,
2003). Decoding the source sentence takes the form of
a beam search through the translation space, with inter-
mediate states corresponding to partial translations. The
decoding process advances by extending a state with the
translation of a source phrase, until each source word has
been translated exactly once. Re-ordering occurs when
the source phrase to be translated does not immediately
follow the previously translated phrase. This is penalized
with a discriminatively-trained distortion penalty. In or-
der to calculate the current translation score, each state
can be represented by a triple:
? A coverage vector HC indicates which source words
have already been translated.
? A span f? indicates the last source phrase translated
to create this state.
? A target word sequence stores context needed by the
target language model.
As cohesion concerns only movement in the source, we
can completely ignore the language model context, mak-
ing state effectively an (f? ,HC ) tuple.
To enforce cohesion during the state expansion pro-
cess, cohesive phrasal decoding has been proposed in
(Cherry, 2008; Yamamoto et al, 2008). The cohesion-
enhanced decoder enforces the following constraint: once
the decoder begins translating any part of a source sub-
tree, it must cover all the words under that subtree before
it can translate anything outside of it. This notion can be
applied to any projective tree structure, but we use de-
pendency trees, which have been shown to demonstrate
greater cross-lingual cohesion than other structures (Fox,
2002). We use a tree data structure to store the depen-
dency tree. Each node in the tree contains surface word
form, word position, parent position, dependency type
and POS tag. We use T to stand for our dependency tree,
and T (n) to stand for the subtree rooted at node n. Each
subtree T (n) covers a span of contiguous source words;
for subspan f? covered by T (n), we say f? ? T (n).
Cohesion is checked as we extend a state (f?h,HC h)
with the translation of f?h+1, creating a new state
(f?h+1,HC h+1). Algorithm 1 presents the cohesion
check described by Cherry (2008). Line 2 selects focal
points, based on the last translated phrase. Line 4 climbs
from each focal point to find the largest subtree that needs
to be completed before the translation process can move
elsewhere in the tree. Line 5 checks each such subtree
for completion. Since there are a constant number of fo-
cal points (always 2) and the tree climb and completion
checks are both linear in the size of the source, the entire
check can be shown to take linear time.
The selection of only two focal points is motivated by
a ?violation free? assumption. If one assumes that the
1
Algorithm 1 Interruption Check (Coh1) (Cherry, 2008)
Input: Source tree T , previous phrase f?h, current
phrase f?h+1, coverage vector HC
1: Interruption ? False
2: F ? the left and right-most tokens of f?h
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists and T (n) is not covered in HCh+1
then
6: Interruption ? True
7: end if
8: end for
9: Return Interruption
Figure 1: A candidate translation where Coh1 does not fire
translation represented by (f?h,HC h) contains no cohe-
sion violations, then checking only the end-points of f?h
is sufficient to maintain cohesion. However, once a soft
cohesion constraint has been implemented, this assump-
tion no longer holds.
2 Extensions of Cohesive Constraints
2.1 Exhaustive Interruption Check (Coh2)
Because of the ?violation free? assumption, Algorithm 1
implements the design decision to only suffer a violation
penalty once, when cohesion is initially broken. How-
ever, this is not necessarily the best approach, as the de-
coder does not receive any further incentive to return to
the partially translated subtree and complete it.
For example, Figure 1 illustrates a translation candi-
date of the English sentence ?the presidential election
of the united states begins tomorrow? into French. We
consider f?4 = ?begins?, f?5 = ?tomorrow?. The decoder
already translated ?the presidential election? making the
coverage vector HC 5 = ?1 1 1 0 0 0 0 1 1?. Algorithm 1
tells the decoder that no violation has been made by trans-
lating ?tomorrow? while the decoder should be informed
that there exists an outstanding violation. Algorithm 1
found the violation when the decoder previously jumped
from ?presidential? to ?begins?, and will not find another
violation when it jumps from ?begins? to ?tomorrow?.
Algorithm 2 is a modification of Algorithm 1, chang-
ing only line 2. The resulting system checks all previ-
Algorithm 2 Exhaustive Interruption Check (Coh2)
Input: Source tree T , previous phrase fh, current
phrase fh+1, coverage vector HC
1: Interruption ? False
2: F ? {f |HCh(f) = 1}
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists and T (n) is not covered in HC h+1
then
6: Interruption ? True
7: end if
8: end for
9: Return Interruption
Algorithm 3 Interruption Count (Coh3)
Input: Source tree T , previous phrase f?h, current
phrase f?h+1, coverage vector HC
1: ICount ? 0
2: F ? the left and right-most tokens of f?h
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists then
6: for each of e ? T (n) and HCh+1(e) = 0 do
7: ICount = ICount+ 1
8: end for
9: end if
10: end for
11: Return ICount
ously covered tokens, instead of only the left and right-
most tokens of f?h+1, and therefore makes no violation-
free assumption. For the example above, Algorithm 2
will inform the decoder that translating ?tomorrow? also
incurs a violation. Because |F | is no longer constant,
the time complexity of Coh2 is worse than Coh1. How-
ever, we can speed up the interruption check algorithm
by hashing cohesion checks, so we only need to run Al-
gorithm 2 once per (f?h+1,HC h+1) .
2.2 Interruption Count (Coh3) and Exhaustive
Interruption Count (Coh4)
Algorithm 1 and 2 described above interpret an inter-
ruption as a binary event. As it is possible to leave several
words untranslated with a single jump, some interrup-
tions may be worse than others. To implement this obser-
vation, an interruption count is used to assign a penalty
to cohesion violations, based on the number of words left
uncovered in the interrupted subtree. We initialize the in-
terruption count with zero. At any search state when the
cohesion violation is detected the count is incremented by
2
Algorithm 4 Exhaustive Interruption Count (Coh4)
Input: Source tree T , previous phrase fh, current
phrase fh+1, coverage vector HC
1: ICount ? 0
2: F ? {f |HCh(f) = 1}
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists then
6: for each of e ? T (n) and HCh+1(e) = 0 do
7: ICount = ICount+ 1
8: end for
9: end if
10: end for
11: Return ICount
one. The modification of Algorithm 1 and 2 lead to Inter-
ruption Count (Coh3) and Exhaustive Interruption Count
(Coh4) algorithms, respectively. The changes only hap-
pen in lines 1, 5 and 6. We use an additional bit vector
to make sure that if a node has been reached once during
an interruption check, it should not be counted again. For
the example in Section 2.1, Algorithm 4 will return 4 for
ICount (?of?; ?the?; ?united?; ?states?).
2.3 Rich Interruption Constraints (Coh5)
The cohesion constraints in Sections 2.1 and 2.2 do not
leverage node information in the dependency tree struc-
tures. We propose the rich interruption constraints (Coh5)
algorithm to combine four constraints which are Interrup-
tion, Interruption Count, Verb Count and Noun Count.
The first two constraints are identical to what was de-
scribed above. Verb and Noun count constraints are en-
forcing the following rule: a cohesion violation will be
penalized more in terms of the number of verb and noun
words that have not been covered. For example, we want
to translate the English sentence ?the presidential elec-
tion of the united states begins tomorrow? to French with
the dependency structure as in Figure 1. We consider f?h
= ?the united states?, f?h+1 = ?begins?. The coverage bit
vector HC h+1 is ?0 0 0 0 1 1 1 1 0?. Algorithm 5 will re-
turn true for Interruption, 4 for ICount (?the?; ?pres-
idential?; ?election?; ?of?), 0 for V erbCount and 1 for
NounCount (?election?).
3 Experiments
We built baseline systems using GIZA++ (Och and Ney,
2003), Moses? phrase extraction with grow-diag-final-
end heuristic (Koehn et al, 2007), a standard phrase-
based decoder (Vogel, 2003), the SRI LM toolkit (Stol-
cke, 2002), the suffix-array language model (Zhang and
Vogel, 2005), a distance-based word reordering model
Algorithm 5 Rich Interruption Constraints (Coh5)
Input: Source tree T , previous phrase f?h, current
phrase f?h+1, coverage vector HC
1: Interruption ? False
2: ICount, V erbCount,NounCount ? 0
3: F ? the left and right-most tokens of f?h
4: for each of f ? F do
5: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
6: if n exists then
7: for each of e ? T (n) and HCh+1(e) = 0 do
8: Interruption ? True
9: ICount = ICount+ 1
10: if POS of e is ?VB? then
11: V erbCount ? V erbCount+ 1
12: else if POS of e is ?NN? then
13: NounCount ? NounCount+ 1
14: end if
15: end for
16: end if
17: end for
18: Return Interruption, ICount, V erbCount,
NounCount
with a window of 3, and the maximum number of target
phrases restricted to 10. Results are reported using low-
ercase BLEU (Papineni et al, 2002). All model weights
were trained on development sets via minimum-error rate
training (MERT) (Och, 2003) with 200 unique n-best lists
and optimizing toward BLEU. We used the MALT parser
(Nivre et al, 2006) to obtain source English dependency
trees and the Stanford parser for Arabic (Marneffe et al,
2006). In order to decide whether the translation output
of one MT engine is significantly better than another one,
we used the bootstrap method (Zhang et al, 2004) with
1000 samples (p < 0.05). We perform experiments on
English?Iraqi and English?Spanish. Detailed corpus
statistics are shown in Table 1. Table 2 shows results in
lowercase BLEU and bold type is used to indicate high-
est scores. An italic text indicates the score is statistically
significant better than the baseline.
English?Iraqi English?Spanish
English Iraqi English Spanish
sentence pairs 654,556 1,310,127
unique sent. pairs 510,314 1,287,016
avg. sentence length 8.4 5.9 27.4 28.6
# words 5.5 M 3.8 M 35.8 M 37.4 M
vocabulary 34 K 109 K 117 K 173 K
Table 1: Corpus statistics
Our English-Iraqi data come from the DARPA
TransTac program. We used TransTac T2T July 2007
3
English?Iraqi English?Spanish
july07 june08 ncd07 nct07
Baseline 31.58 23.58 33.18 32.04
+Coh1 32.63 24.45 33.49 32.72
+Coh2 32.51 24.73 33.52 32.81
+Coh3 32.43 24.19 33.37 32.87
+Coh4 32.32 24.66 33.47 33.20
+Coh5 31.98 24.42 33.54 33.27
Table 2: Scores of baseline and cohesion-enhanced systems on
English?Iraqi and English?Spanish systems
(july07) as the development set and TransTac T2T June
2008 (june08) as the held-out evaluation set. Each test set
has 4 reference translation. We applied the suffix-array
LM up to 6-gram with Good-Turing smoothing. Our co-
hesion constraints produced improvements ranging be-
tween 0.5 and 1.2 BLEU point on the held-out evaluation
set.
We used the Europarl and News-Commentary parallel
corpora for English?Spanish as provided in the ACL-
WMT 2008 shared task evaluation. The baseline sys-
tem used the parallel corpus restricting sentence length
to 100 words for word alignment and a 4-gram SRI
LM with modified Kneyser-Ney smoothing. We used
nc-devtest2007(ncd07) as the development set and nc-
test2007(nct07) as the held-out evaluation set. Each test
set has 1 translation reference. We obtained improve-
ments ranging between 0.7 and 1.2 BLEU. All cohesion
constraints perform statistically significant better than the
baseline on the held-out evaluation set.
4 Conclusions
In this paper, we explored cohesive phrasal decoding, fo-
cusing on variants of cohesive constraints. We proposed
four novel cohesive constraints namely exhaustive inter-
ruption check (Coh2), interruption count (Coh3), exhaus-
tive interruption count (Coh4) and rich interruption con-
straints (Coh5). Our experimental results show that with
cohesive constraints the system generates better transla-
tions in comparison with strong baselines. To ensure the
robustness and effectiveness of the proposed approaches,
we conducted experiments on 2 different language pairs,
namely English?Iraqi and English?Spanish. These ex-
periments also covered a wide range of training corpus
sizes, ranging from 600K sentence pairs up to 1.3 mil-
lion sentence pairs. All five proposed approaches give
positive results. The improvements on English?Spanish
are statistically significant at the 95% level. We observe
a consistent pattern indicating that the improvements are
stable in both language pairs.
Acknowledgments
This work is in part supported by the US DARPA TransTac pro-
grams. Any opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the views of DARPA. We would like
to thank Qin Gao and Matthias Eck for helpful conversations,
Johan Hall and Joakim Nirve for helpful suggestions on train-
ing and using the English dependency model. We also thanks
the anonymous reviewers for helpful comments.
References
Colin Cherry. 2008. Cohesive phrase-based decoding for statis-
tical machine translation. In Proceedings of ACL-08: HLT,
pages 72?80, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of EMNLP?02, pages 304?311,
Philadelphia, PA, July 6-7.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL?07, pages 177?180, Prague,
Czech Republic, June.
Marie-Catherine Marneffe, Bill MacCartney, and Christopher
Manning. 2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC?06, Genoa,
Italy.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. MaltParser:
A data-driven parser-generator for dependency parsing. In
Proceedings of LREC?06, Genoa, Italy.
Franz J. Och and Hermann Ney. 2003. A systematic compar-
ison of various statistical alignment models. In Computa-
tional Linguistics, volume 1:29, pages 19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of ACL?03, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: A method for automatic evaluation of
machine translation. In Proceedings of ACL?02, pages 311?
318, Philadelphia, PA, July.
Andreas Stolcke. 2002. SRILM ? An extensible language mod-
eling toolkit. In Proc. Intl. Conf. on Spoken Language Pro-
cessing, volume 2, pages 901?904, Denver.
Stephan Vogel. 2003. SMT decoder dissected: Word reorder-
ing. In Proceedings of NLP-KE?03, pages 561?566, Bejing,
China, Oct.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro Sumita.
2008. Imposing constraints from the source tree on ITG
constraints for SMT. In Proceedings of the ACL-08: HLT,
SSST-2, pages 1?9, Columbus, Ohio, June. Association for
Computational Linguistics.
Ying Zhang and Stephan Vogel. 2005. An efficient phrase-to-
phrase alignment model for arbitrarily long phrase and large
corpora. In Proceedings of EAMT?05, Budapest, Hungary,
May. The European Association for Machine Translation.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do we
need to have a better system? In Proceedings of LREC?04,
pages 2051?2054.
4
Proceedings of ACL-08: HLT, pages 72?80,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Cohesive Phrase-based Decoding for Statistical Machine Translation
Colin Cherry?
Microsoft Research
One Microsoft Way
Redmond, WA, 98052
colinc@microsoft.com
Abstract
Phrase-based decoding produces state-of-the-
art translations with no regard for syntax. We
add syntax to this process with a cohesion
constraint based on a dependency tree for
the source sentence. The constraint allows
the decoder to employ arbitrary, non-syntactic
phrases, but ensures that those phrases are
translated in an order that respects the source
tree?s structure. In this way, we target the
phrasal decoder?s weakness in order model-
ing, without affecting its strengths. To fur-
ther increase flexibility, we incorporate cohe-
sion as a decoder feature, creating a soft con-
straint. The resulting cohesive, phrase-based
decoder is shown to produce translations that
are preferred over non-cohesive output in both
automatic and human evaluations.
1 Introduction
Statistical machine translation (SMT) is complicated
by the fact that words can move during translation.
If one assumes arbitrary movement is possible, that
alone is sufficient to show the problem to be NP-
complete (Knight, 1999). Syntactic cohesion1 is
the notion that all movement occurring during trans-
lation can be explained by permuting children in a
parse tree (Fox, 2002). Equivalently, one can say
that phrases in the source, defined by subtrees in
its parse, remain contiguous after translation. Early
?Work conducted while at the University of Alberta.
1We use the term ?syntactic cohesion? throughout this paper
to mean what has previously been referred to as ?phrasal cohe-
sion?, because the non-linguistic sense of ?phrase? has become
so common in machine translation literature.
methods for syntactic SMT held to this assump-
tion in its entirety (Wu, 1997; Yamada and Knight,
2001). These approaches were eventually super-
seded by tree transducers and tree substitution gram-
mars, which allow translation events to span sub-
tree units, providing several advantages, including
the ability to selectively produce uncohesive transla-
tions (Eisner, 2003; Graehl and Knight, 2004; Quirk
et al, 2005). What may have been forgotten during
this transition is that there is a reason it was once be-
lieved that a cohesive translation model would work:
for some language pairs, cohesion explains nearly
all translation movement. Fox (2002) showed that
cohesion is held in the vast majority of cases for
English-French, while Cherry and Lin (2006) have
shown it to be a strong feature for word alignment.
We attempt to use this strong, but imperfect, char-
acterization of movement to assist a non-syntactic
translation method: phrase-based SMT.
Phrase-based decoding (Koehn et al, 2003) is a
dominant formalism in statistical machine transla-
tion. Contiguous segments of the source are trans-
lated and placed in the target, which is constructed
from left to right. The process iterates within a beam
search until each word from the source has been
covered by exactly one phrasal translation. Candi-
date translations are scored by a linear combination
of models, weighted according to Minimum Error
Rate Training or MERT (Och, 2003). Phrasal SMT
draws strength from being able to memorize non-
compositional and context-specific translations, as
well as local reorderings. Its primary weakness is
in movement modeling; its default distortion model
applies a flat penalty to any deviation from source
72
order, forcing the decoder to rely heavily on its lan-
guage model. Recently, a number of data-driven dis-
tortion models, based on lexical features and relative
distance, have been proposed to compensate for this
weakness (Tillman, 2004; Koehn et al, 2005; Al-
Onaizan and Papineni, 2006; Kuhn et al, 2006).
There have been a number of proposals to in-
corporate syntactic information into phrasal decod-
ing. Early experiments with syntactically-informed
phrases (Koehn et al, 2003), and syntactic re-
ranking of K-best lists (Och et al, 2004) produced
mostly negative results. The most successful at-
tempts at syntax-enhanced phrasal SMT have di-
rectly targeted movement modeling: Zens et al
(2004) modified a phrasal decoder with ITG con-
straints, while a number of researchers have em-
ployed syntax-driven source reordering before de-
coding begins (Xia and McCord, 2004; Collins et
al., 2005; Wang et al, 2007).2 We attempt some-
thing between these two approaches: our constraint
is derived from a linguistic parse tree, but it is used
inside the decoder, not as a preprocessing step.
We begin in Section 2 by defining syntactic cohe-
sion so it can be applied to phrasal decoder output.
Section 3 describes how to add both hard and soft
cohesion constraints to a phrasal decoder. Section 4
provides our results from both automatic and human
evaluations. Sections 5 and 6 provide a qualitative
discussion of cohesive output and conclude.
2 Cohesive Phrasal Output
Previous approaches to measuring the cohesion of
a sentence pair have worked with a word align-
ment (Fox, 2002; Lin and Cherry, 2003). This align-
ment is used to project the spans of subtrees from
the source tree onto the target sentence. If a modifier
and its head, or two modifiers of the same head, have
overlapping spans in the projection, then this indi-
cates a cohesion violation. To check phrasal trans-
lations for cohesion violations, we need a way to
project the source tree onto the decoder?s output.
Fortunately, each phrase used to create the target
sentence can be tracked back to its original source
phrase, providing an alignment between source and
2While certainly both syntactic and successful, we consider
Hiero (Chiang, 2007) to be a distinct approach, and not an ex-
tension to phrasal decoding?s left-to-right beam search.
target phrases. Since each source token is used ex-
actly once during translation, we can transform this
phrasal alignment into a word-to-phrase alignment,
where each source token is linked to a target phrase.
We can then project the source subtree spans onto
the target phrase sequence. Note that we never con-
sider individual tokens on the target side, as their
connection to the source tree is obscured by the
phrasal abstraction that occurred during translation.
Let em1 be the input source sentence, and f?
p
1 be the
output target phrase sequence. Our word-to-phrase
alignment ai ? [1, p], 1 ? i ? m, maps a source
token position i to a target phrase position ai. Next,
we introduce our source dependency tree T . Each
source token ei is also a node in T . We define T (ei)
to be the subtree of T rooted at ei. We define a local
tree to be a head node and its immediate modifiers.
With this notation in place, we can define our pro-
jected spans. Following Lin and Cherry (2003), we
define a head span to be the projection of a single
token ei onto the target phrase sequence:
spanH (ei, T, a
m
1 ) = [ai, ai]
and the subtree span to be the projection of the sub-
tree rooted at ei:
spanS (ei, T, a
m
1 ) =
[
min
{j|ej?T (ei)}
aj , max
{k|ek?T (ei)}
ak
]
Consider the simple phrasal translation shown in
Figure 1 along with a dependency tree for the En-
glish source. If we examine the local tree rooted at
likes , we get the following projected spans:
spanS (nobody , T, a) = [1, 1]
spanH (likes, T, a) = [1, 1]
spanS (pay , T, a) = [1, 2]
For any local tree, we consider only the head span of
the head, and the subtree spans of any modifiers.
Typically, cohesion would be determined by
checking these projected spans for intersection.
However, at this level of resolution, avoiding inter-
section becomes highly restrictive. The monotone
translation in Figure 1 would become non-cohesive:
nobody intersects with both its sibling pay and with
its head likes at phrase index 1. This complica-
tion stems from the use of multi-word phrases that
73
nobody likes to pay taxes
personne n ' aime payer des imp?ts
(nobody likes) (paying taxes)
1 2
Figure 1: An English source tree with translated French
output. Segments are indicated with underlined spans.
do not correspond to syntactic constituents. Re-
stricting phrases to syntactic constituents has been
shown to harm performance (Koehn et al, 2003), so
we tighten our definition of a violation to disregard
cases where the only point of overlap is obscured by
our phrasal resolution. To do so, we replace span
intersection with a new notion of span innersection.
Assume we have two spans [u, v] and [x, y] that
have been sorted so that [u, v] ? [x, y] lexicograph-
ically. We say that the two spans innersect if and
only if x < v. So, [1, 3] and [2, 4] innersect, while
[1, 3] and [3, 4] do not. One can think of innersection
as intersection, minus the cases where the two spans
share only a single boundary point, where x = v.
When two projected spans innersect, it indicates that
the second syntactic constituent must begin before
the first ends. If the two spans in question corre-
spond to nodes in the same local tree, innersection
indicates an unambiguous cohesion violation. Un-
der this definition, the translation in Figure 1 is co-
hesive, as none of its spans innersect.
Our hope is that syntactic cohesion will help the
decoder make smarter distortion decisions. An ex-
ample with distortion is shown in Figure 2. In this
case, we present two candidate French translations
of an English sentence, assuming there is no entry
in the phrase table for ?voting session.? Because the
proper French construction is ?session of voting?,
the decoder has to move voting after session using a
distortion operation. Figure 2 shows two methods to
do so, each using an equal numbers of phrases. The
projected spans for the local tree rooted at begins
in each candidate are shown in Table 1. Note the
innersection between the head begins and its modi-
fier session in (b). Thus, a cohesion-aware system
would receive extra guidance to select (a), which
maintains the original meaning much better than (b).
Span (a) (b)
spanS (session, T, a) [1,3] [1,3]*
spanH (begins, T, a) [4,4] [2,2]*
spanS (tomorrow , T, a) [4,4] [4,4]
Table 1: Spans of the local trees rooted at begins from
Figures 2 (a) and (b). Innersection is marked with a ?*?.
2.1 K-best List Filtering
A first attempt at using cohesion to improve SMT
output would be to apply our definition as a filter on
K-best lists. That is, we could have a phrasal de-
coder output a 1000-best list, and return the highest-
ranked cohesive translation to the user. We tested
this approach on our English-French development
set, and saw no improvement in BLEU score. Er-
ror analysis revealed that only one third of the un-
cohesive translations had a cohesive alternative in
their 1000-best lists. In order to reach the remain-
ing two thirds, we need to constrain the decoder?s
search space to explore only cohesive translations.
3 Cohesive Decoding
This section describes a modification to standard
phrase-based decoding, so that the system is con-
strained to produce only cohesive output. This will
take the form of a check performed each time a hy-
pothesis is extended, similar to the ITG constraint
for phrasal SMT (Zens et al, 2004). To create a
such a check, we need to detect a cohesion viola-
tion inside a partial translation hypothesis. We can-
not directly apply our span-based cohesion defini-
tion, because our word-to-phrase alignment is not
yet complete. However, we can still detect viola-
tions, and we can do so before the spans involved
are completely translated.
Recall that when two projected spans a and b
(a < b) innersect, it indicates that b begins before a
ends. We can say that the translation of b interrupts
the translation of a. We can enforce cohesion by en-
suring that these interruptions never happen. Be-
cause the decoder builds its translations from left to
right, eliminating interruptions amounts to enforcing
the following rule: once the decoder begins translat-
ing any part of a source subtree, it must cover all
74
the voting session begins tomorrow
la session de  vote d?bute  demain
2 3 4
1
(the) (session) (of voting) (begins tomorrow)
(a) (b)
1 2
the voting session begins tomorrow
3
4
la session  commence ? voter demain
(the) (session begins) (to vote) (tomorrow)
Figure 2: Two candidate translations for the same parsed source. (a) is cohesive, while (b) is not.
the words under that subtree before it can translate
anything outside of it.
For example, in Figure 2b, the decoder translates
the , which is part of T (session) in f?1. In f?2, it trans-
lates begins , which is outside T (session). Since we
have yet to cover voting , we know that the projected
span of T (session) will end at some index v > 2,
creating an innersection. This eliminates the hypoth-
esis after having proposed only the first two phrases.
3.1 Algorithm
In this section, we formally define an interruption,
and present an algorithm to detect one during de-
coding. During both discussions, we represent each
target phrase as a set that contains the English tokens
used in its translation: f?j = {ei|ai = j}. Formally,
an interruption occurs whenever the decoder would
add a phrase f?h+1 to the hypothesis f?h1 , and:
?r ? T such that:
?e ? T (r) s.t. e ? f?h1 (a. Started)
?e? /? T (r) s.t. e? ? f?h+1 (b. Interrupted)
?e?? ? T (r) s.t. e?? /? f?h+11 (c. Unfinished)
(1)
The key to checking for interruptions quickly is
knowing which subtrees T (r) to check for qualities
(1:a,b,c). A na??ve approach would check every sub-
tree that has begun translation in f?h1 . Figure 3a high-
lights the roots of all such subtrees for a hypothetical
T and f?h1 . Fortunately, with a little analysis that ac-
counts for f?h+1, we can show that at most two sub-
trees need to be checked.
For a given interruption-free f?h1 , we call subtrees
that have begun translation, but are not yet complete,
open subtrees. Only open subtrees can lead to inter-
ruptions. We can focus our interruption check on
f?h, the last phrase in f?h1 , as any open subtree T (r)
must contain at least one e ? f?h. If this were not the
Algorithm 1 Interruption check.
? Get the left and right-most tokens used to create
f?h, call them eL and eR
? For each of e ? {eL, eR}:
i. r? ? e, r ? null
While ?e? ? f?h+1 such that e? /? T (r?):
r ? r?, r? ? parent(r)
ii. If r 6= null and ?e?? ? T (r) such that
e?? /? f?h+11 , then f?h+1 interrupts T (r).
case, then the open T (r)must have began translation
somewhere in f?h?11 , and T (r) would be interrupted
by the placement of f?h. Since our hypothesis f?h1
is interruption-free, this is impossible. This leaves
the subtrees highlighted in Figure 3b to be checked.
Furthermore, we need only consider subtrees that
contain the left and right-most source tokens eL and
eR translated by f?h. Since f?h was created from a
contiguous string of source tokens, any distinct sub-
tree between these two endpoints will be completed
within f?h. Finally, for each of these focus points
eL and eR, only the highest containing subtree T (r)
that does not completely contain f?h+1 needs to be
considered. Anything higher would contain all of
f?h+1, and would not satisfy requirement (1:b) of our
interruption definition. Any lower subtree would be
a descendant of r, and therefore the check for the
lower subtree is subsumed by the check for T (r).
This leaves only two subtrees, highlighted in our
running example in Figure 3c.
With this analysis in place, an extension f?h+1 of
the hypothesis f?h1 can be checked for interruptions
with Algorithm 1. Step (i) in this algorithm finds
an ancestor r? such that T (r?) completely contains
75
f h
f h+1
f  
h
1
f h
f h+1
f  
h
1
f h
f h+1
f  
h
1
a)
b) c)
Figure 3: Narrowing down the source subtrees to be checked for completeness.
f?h+1, and then returns r, the highest node that does
not contain f?h+1. We know this r satisfies require-
ments (1:a,b). If there is no T (r) that does not con-
tain f?h+1, then e and its ancestors cannot lead to an
interruption. Step (ii) then checks the coverage vec-
tor of the hypothesis3 to make sure that T (r) is cov-
ered in f?h+11 . If T (r) is not complete in f?
h+1
1 , then
that satisfies requirement (1:c), which means an in-
terruption has occurred.
For example, in Figure 2b, our first interruption
occurs as we add f?h+1 = f?2 to f?h1 = f?
1
1 . The de-
tection algorithm would first get the left and right
boundaries of f?1; in this case, the is both eL and
eR. Then, it would climb up the tree from the until
it reached r? = begins and r = session . It would
then check T (session) for coverage in f?21 . Since
voting ? T (session) is not covered in f?21 , it would
detect an interruption.
Walking up the tree takes at most linear time,
and each check to see if T (r) contains all of f?h+1
can be performed in constant time, provided the
source spans of each subtree have been precom-
puted. Checking to see if all of T (r) has been cov-
ered in Step (ii) takes at most linear time. This
makes the entire process linear in the size of the
source sentence.
3.2 Soft Constraint
Syntactic cohesion is not a perfect constraint for
translation. Parse errors and systematic violations
can create cases where cohesion works against the
decoder. Fox (2002) demonstrated and counted
cases where cohesion was not maintained in hand-
aligned sentence-pairs, while Cherry and Lin (2006)
3This coverage vector is maintained by all phrasal decoders
to track how much of the source sentence has been covered by
the current partial translation, and to ensure that the same token
is not translated twice.
showed that a soft cohesion constraint is superior to
a hard constraint for word alignment. Therefore, we
propose a soft version of our cohesion constraint.
We perform our interruption check, but we do not
invalidate any hypotheses. Instead, each hypothe-
sis maintains a count of the number of extensions
that have caused interruptions during its construc-
tion. This count becomes a feature in the decoder?s
log-linear model, the weight of which is trained with
MERT. After the first interruption, the exact mean-
ing of further interruptions becomes difficult to in-
terpret; but the interruption count does provide a
useful estimate of the extent to which the translation
is faithful to the source tree structure.
Initially, we were not certain to what extent this
feature would be used by the MERT module, as
BLEU is not always sensitive to syntactic improve-
ments. However, trained with our French-English
tuning set, the interruption count received the largest
absolute feature weight, indicating, at the very least,
that the feature is worth scaling to impact decoder.
3.3 Implementation
We modify the Moses decoder (Koehn et al, 2007)
to translate head-annotated sentences. The decoder
stores the flat sentence in the original sentence data
structure, and the head-encoded dependency tree in
an attached tree data structure. The tree structure
caches the source spans corresponding to each of
its subtrees. We then implement both a hard check
for interruptions to be used before hypotheses are
placed on the stack,4 and a soft check that is used to
calculate an interruption count feature.
4A hard cohesion constraint used in conjunction with a tra-
ditional distortion limit also requires a second linear-time check
to ensure that all subtrees currently in progress can be finished
under the constraints induced by the distortion limit.
76
Set Cohesive Uncohesive
Dev-Test 1170 330
Test 1563 437
Table 2: Number of sentences that receive cohesive trans-
lations from the baseline decoder. This property also de-
fines our evaluation subsets.
4 Experiments
We have adapted the notion of syntactic cohesion so
that it is applicable to phrase-based decoding. This
results in a translation process that respects source-
side syntactic boundaries when distorting phrases.
In this section we will test the impact of such infor-
mation on an English to French translation task.
4.1 Experimental Details
We test our cohesion-enhanced Moses decoder
trained using 688K sentence pairs of Europarl
French-English data, provided by the SMT 2006
Shared Task (Koehn and Monz, 2006). Word align-
ments are provided by GIZA++ (Och and Ney,
2003) with grow-diag-final combination, with in-
frastructure for alignment combination and phrase
extraction provided by the shared task. We decode
withMoses, using a stack size of 100, a beam thresh-
old of 0.03 and a distortion limit of 4. Weights for
the log-linear model are set using MERT, as imple-
mented by Venugopal and Vogel (2005). Our tuning
set is the first 500 sentences of the SMT06 develop-
ment data. We hold out the remaining 1500 develop-
ment sentences for development testing (dev-test),
and the entirety of the provided 2000-sentence test
set for blind testing (test). Since we require source
dependency trees, all experiments test English to
French translation. English dependency trees are
provided by Minipar (Lin, 1994).
Our cohesion constraint directly targets sentences
for which an unmodified phrasal decoder produces
uncohesive output according to the definition in Sec-
tion 2. Therefore, we present our results not only on
each test set in its entirety, but also on the subsets
defined by whether or not the baseline naturally pro-
duces a cohesive translation. The sizes of the result-
ing evaluation sets are given in Table 2.
Our development tests indicated that the soft and
hard cohesion constraints performed somewhat sim-
ilarly, with the soft constraint providing more sta-
ble, and generally better results. We confirmed these
trends on our test set, but to conserve space, we pro-
vide detailed results for only the soft constraint.
4.2 Automatic Evaluation
We first present our soft cohesion constraint?s ef-
fect on BLEU score (Papineni et al, 2002) for both
our dev-test and test sets. We compare against an
unmodified baseline decoder, as well as a decoder
enhanced with a lexical reordering model (Tillman,
2004; Koehn et al, 2005). For each phrase pair in
our translation table, the lexical reordering model
tracks statistics on its reordering behavior as ob-
served in our word-aligned training text. The lex-
ical reordering model provides a good comparison
point as a non-syntactic, and potentially orthogonal,
improvement to phrase-based movement modeling.
We use the implementation provided in Moses, with
probabilities conditioned on bilingual phrases and
predicting three orientation bins: straight, inverted
and disjoint. Since adding features to the decoder?s
log-linear model is straight-forward, we also experi-
ment with a combined system that uses both the co-
hesion constraint and a lexical reordering model.
The results of our experiments are shown in Ta-
ble 3, and reveal some interesting phenomena. First
of all, looking across columns, we can see that there
is a definite divide in BLEU score between our two
evaluation subsets. Sentences with cohesive base-
line translations receive much higher BLEU scores
than those with uncohesive baseline translations.
This indicates that the cohesive subset is easier to
translate with a phrase-based system. Our definition
of cohesive phrasal output appears to provide a use-
ful feature for estimating translation confidence.
Comparing the baseline with and without the soft
cohesion constraint, we see that cohesion has only a
modest effect on BLEU, when measured on all sen-
tence pairs, with improvements ranging between 0.2
and 0.5 absolute points. Recall that the majority of
baseline translations are naturally cohesive. The co-
hesion constraint?s effect is much more pronounced
on the more difficult uncohesive subsets, showing
absolute improvements between 0.5 and 1.1 points.
Considering the lexical reordering model, we see
that its effect is very similar to that of syntactic co-
hesion. Its BLEU scores are very similar, with lex-
77
Dev-Test Test
System All Cohesive Uncohesive All Cohesive Uncohesive
base 32.04 33.80 27.46 32.35 33.78 28.73
lex 32.19 33.91 27.86 32.71 33.89 29.66
coh 32.22 33.82 28.04 32.88 34.03 29.86
lex+coh 32.45 34.12 28.09 32.90 34.04 29.83
Table 3: BLEU scores with an integrated soft cohesion constraint (coh) or a lexical reordering model (lex). Any system
significantly better than base has been highlighted, as tested by bootstrap re-sampling with a 95% confidence interval.
ical reordering also affecting primarily the uncohe-
sive subset. This similarity in behavior is interesting,
as its data-driven, bilingual reordering probabilities
are quite different from our cohesion flag, which is
driven by monolingual syntax.
Examining the system that employs both move-
ment models, we see that the combination (lex+coh)
receives the highest score on the dev-test set. A large
portion of the combined system?s gain is on the co-
hesive subset, indicating that the cohesion constraint
may be enabling better use of the lexical reordering
model on otherwise cohesive translations. Unfor-
tunately, these same gains are not born out on the
test set, where the lexical reordering model appears
unable to improve upon the already strong perfor-
mance of the cohesion constraint.
4.3 Human Evaluation
We also present a human evaluation designed to de-
termine whether bilingual speakers prefer cohesive
decoder output. Our comparison systems are the
baseline decoder (base) and our soft cohesion con-
straint (coh). We evaluate on our dev-test set,5 as it
has our smallest observed BLEU-score gap, and we
wish to determine if it is actually improving. Our ex-
perimental set-up is modeled after the human evalu-
ation presented in (Collins et al, 2005). We provide
two human annotators6 a set of 75 English source
sentences, along with a reference translation and a
pair of translation candidates, one from each sys-
tem. The annotators are asked to indicate which of
the two system translations they prefer, or if they
5The cohesion constraint has no free parameters to optimize
during development, so this does not create an advantage.
6Annotators were both native English speakers who speak
French as a second language. Each has a strong comprehension
of written French.
Annotator #2
Annotator #1 base coh equal sum (#1)
base 6 7 1 14
coh 8 35 4 47
equal 7 4 3 14
sum (#2) 21 46 8
Table 4: Confusion matrix from human evaluation.
consider them to be equal. To avoid bias, the com-
peting systems were presented anonymously and in
random order. Following (Collins et al, 2005), we
provide the annotators with only short sentences:
those with source sentences between 10 and 25 to-
kens long. Following (Callison-Burch et al, 2006),
we conduct a targeted evaluation; we only draw our
evaluation pairs from the uncohesive subset targeted
by our constraint. All 75 sentences that meet these
two criteria are included in the evaluation.
The aggregate results of our human evaluation are
shown in the bottom row and right-most column of
Table 4. Each annotator prefers coh in over 60% of
the test sentences, and each prefers base in less than
30% of the test sentences. This presents strong evi-
dence that we are having a consistent, positive effect
on formerly non-cohesive translations. A complete
confusion matrix indicating agreement between the
two annotators is also given in Table 4. There are a
few more off-diagonal points than one might expect,
but it is clear that the two annotators are in agree-
ment with respect to coh?s improvements. A com-
bination annotator, which selects base or coh only
when both human annotators agree and equal oth-
erwise, finds base is preferred in only 8% of cases,
compared to 47% for coh.
78
(1+) creating structures that do not currently exist and reducing . . .
base de cre?er des structures qui existent actuellement et ne pas re?duire . . .
to create structures that actually exist and do not reduce . . .
coh de cre?er des structures qui n ? existent pas encore et re?duire . . .
to create structures that do not yet exist and reduce . . .
(2?) . . . repealed the 1998 directive banning advertising
base . . . abroge?e l?interdiction de la directive de 1998 de publicite?
. . . repealed the ban from the 1998 directive on advertising
coh . . . abroge?e la directive de 1998 l?interdiction de publicite?
. . . repealed the 1998 directive the ban on advertising
Table 5: A comparison of baseline and cohesion-constrained English-to-French translations, with English glosses.
5 Discussion
Examining the French translations produced by our
cohesion constrained phrasal decoder, we can draw
some qualitative generalizations. The constraint is
used primarily to prevent distortion: it provides an
intelligent estimate as to when source order must be
respected. The resulting translations tend to be more
literal than unconstrained translations. So long as
the vocabulary present in our phrase table and lan-
guage model supports a literal translation, cohesion
tends to produce an improvement. Consider the first
translation example shown in Table 5. In the base-
line translation, the language model encourages the
system to move the negation away from ?exist? and
toward ?reduce.? The result is a tragic reversal of
meaning in the translation. Our cohesion constraint
removes this option, forcing the decoder to assem-
ble the correct French construction for ?does not yet
exist.? The second example shows a case where our
resources do not support a literal translation. In this
case, we do not have a strong translation mapping to
produce a French modifier equivalent to the English
?banning.? Stuck with a noun form (?the ban?), the
baseline is able to distort the sentence into some-
thing that is almost correct (the above gloss is quite
generous). The cohesive system, even with a soft
constraint, cannot reproduce the same movement,
and returns a less grammatical translation.
We also examined cases where the decoder over-
rides the soft cohesion constraint and produces an
uncohesive translation. We found this was done very
rarely, and primarily to overcome parse errors. Only
one correct syntactic construct repeatedly forced the
decoder to override cohesion: Minipar?s conjunction
representation, which connects conjuncts in parent-
child relationships, is at times too restrictive. A sib-
ling representation, which would allow conjuncts to
be permuted arbitrarily, may work better.
6 Conclusion
We have presented a definition of syntactic cohesion
that is applicable to phrase-based SMT. We have
used this definition to develop a linear-time algo-
rithm to detect cohesion violations in partial decoder
hypotheses. This algorithm was used to implement
a soft cohesion constraint for the Moses decoder,
based on a source-side dependency tree.
Our experiments have shown that roughly 1/5 of
our baseline English-French translations contain co-
hesion violations, and these translations tend to re-
ceive lower BLEU scores. This suggests that co-
hesion could be a strong feature in estimating the
confidence of phrase-based translations. Our soft
constraint produced improvements ranging between
0.5 and 1.1 BLEU points on sentences for which the
baseline produces uncohesive translations. A human
evaluation showed that translations created using a
soft cohesion constraint are preferred over uncohe-
sive translations in the majority of cases.
Acknowledgments Special thanks to Dekang Lin,
Shane Bergsma, and Jess Enright for their useful
insights and discussions, and to the anonymous re-
viewers for their comments. The author was funded
by Alberta Ingenuity and iCORE studentships.
79
References
Y. Al-Onaizan and K. Papineni. 2006. Distortion models
for statistical machine translation. In COLING-ACL,
pages 529?536, Sydney, Australia.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. In EACL, pages 249?256.
C. Cherry and D. Lin. 2006. Soft syntactic constraints
for word alignment through discriminative training. In
COLING-ACL, Sydney, Australia, July. Poster.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228, June.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In ACL,
pages 531?540.
J. Eisner. 2003. Learning non-ismorphic tree mappings
for machine translation. In ACL, Sapporo, Japan.
Short paper.
H. J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In EMNLP, pages 304?311.
J. Graehl and K. Knight. 2004. Training tree transducers.
In HLT-NAACL, pages 105?112, Boston, USA, May.
K. Knight. 1999. Squibs and discussions: Decod-
ing complexity in word-replacement translation mod-
els. Computational Linguistics, 25(4):607?615, De-
cember.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation. In HLT-NACCL
Workshop on Statistical Machine Translation, pages
102?121.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In HLT-NAACL, pages 127?
133.
P. Koehn, A. Axelrod, A. Birch Mayne, C. Callison-
Burch, M. Osborne, and David Talbot. 2005. Edin-
burgh system description for the 2005 IWSLT speech
translation evaluation. In International Workshop on
Spoken Language Translation.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL. Demonstra-
tion.
R. Kuhn, D. Yuen, M. Simard, P. Paul, G. Foster, E. Joa-
nis, and H. Johnson. 2006. Segment choice models:
Feature-rich models for global distortion in statistical
machine translation. In HLT-NAACL, pages 25?32,
New York, NY.
D. Lin and C. Cherry. 2003. Word alignment with co-
hesion constraint. In HLT-NAACL, pages 49?51, Ed-
monton, Canada, May. Short paper.
D. Lin. 1994. Principar - an efficient, broad-coverage,
principle-based parser. In COLING, pages 42?48, Ky-
oto, Japan.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?52.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In HLT-
NAACL 2004: Main Proceedings, pages 161?168.
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL, pages 311?318.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In ACL, pages 271?279, Ann Arbor,
USA, June.
C. Tillman. 2004. A unigram orientation model for sta-
tistical machine translation. In HLT-NAACL, pages
101?104. Short paper.
A. Venugopal and S. Vogel. 2005. Considerations in
maximum mutual information and minimum classifi-
cation error training for statistical machine translation.
In EAMT.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP, pages 737?745.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403.
F. Xia and M. McCord. 2004. Improving a statistical mt
system with automatically learned rewrite patterns. In
Proceedings of Coling 2004, pages 508?514.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL, pages 523?530.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004.
Reordering constraints for phrase-based statistical ma-
chine translation. In COLING, pages 205?211,
Geneva, Switzerland, August.
80
Proceedings of ACL-08: HLT, pages 568?576,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Automatic Syllabification with Structured SVMs
for Letter-To-Phoneme Conversion
Susan Bartlett? Grzegorz Kondrak? Colin Cherry?
?Department of Computing Science ?Microsoft Research
University of Alberta One Microsoft Way
Edmonton, AB, T6G 2E8, Canada Redmond, WA, 98052
{susan,kondrak}@cs.ualberta.ca colinc@microsoft.com
Abstract
We present the first English syllabification
system to improve the accuracy of letter-to-
phoneme conversion. We propose a novel dis-
criminative approach to automatic syllabifica-
tion based on structured SVMs. In comparison
with a state-of-the-art syllabification system,
we reduce the syllabification word error rate
for English by 33%. Our approach also per-
forms well on other languages, comparing fa-
vorably with published results on German and
Dutch.
1 Introduction
Pronouncing an unfamiliar word is a task that is of-
ten accomplished by breaking the word down into
smaller components. Even small children learn-
ing to read are taught to pronounce a word by
?sounding out? its parts. Thus, it is not surprising
that Letter-to-Phoneme (L2P) systems, which con-
vert orthographic forms of words into sequences of
phonemes, can benefit from subdividing the input
word into smaller parts, such as syllables or mor-
phemes. Marchand and Damper (2007) report that
incorporating oracle syllable boundary information
improves the accuracy of their L2P system, but they
fail to emulate that result with any of their automatic
syllabification methods. Demberg et al (2007), on
the other hand, find that morphological segmenta-
tion boosts L2P performance in German, but not in
English. To our knowledge, no previous English
orthographic syllabification system has been able
to actually improve performance on the larger L2P
problem.
In this paper, we focus on the task of automatic
orthographic syllabification, with the explicit goal
of improving L2P accuracy. A syllable is a subdi-
vision of a word, typically consisting of a vowel,
called the nucleus, and the consonants preceding and
following the vowel, called the onset and the coda,
respectively. Although in the strict linguistic sense
syllables are phonological rather than orthographic
entities, our L2P objective constrains the input to or-
thographic forms. Syllabification of phonemic rep-
resentation is in fact an easier task, which we plan to
address in a separate publication.
Orthographic syllabification is sometimes re-
ferred to as hyphenation. Many dictionaries pro-
vide hyphenation information for orthographic word
forms. These hyphenation schemes are related to,
and influenced by, phonemic syllabification. They
serve two purposes: to indicate where words may
be broken for end-of-line divisions, and to assist the
dictionary reader with correct pronunciation (Gove,
1993). Although these purposes are not always con-
sistent with our objective, we show that we can im-
prove L2P conversion by taking advantage of the
available hyphenation data. In addition, automatic
hyphenation is a legitimate task by itself, which
could be utilized in word editors or in synthesizing
new trade names from several concepts.
We present a discriminative approach to ortho-
graphic syllabification. We formulate syllabifica-
tion as a tagging problem, and learn a discriminative
tagger from labeled data using a structured support
vector machine (SVM) (Tsochantaridis et al, 2004).
With this approach, we reduce the error rate for En-
glish by 33%, relative to the best existing system.
Moreover, we are also able to improve a state-of-the-
art L2P system by incorporating our syllabification
models. Our method is not language specific; when
applied to German and Dutch, our performance is
568
comparable with the best existing systems in those
languages, even though our system has been devel-
oped and tuned on English only.
The paper is structured as follows. After dis-
cussing previous computational approaches to the
problem (Section 2), we introduce structured SVMs
(Section 3), and outline how we apply them to ortho-
graphic syllabification (Section 4). We present our
experiments and results for the syllabification task
in Section 5. In Section 6, we apply our syllabifica-
tion models to the L2P task. Section 7 concludes.
2 Related Work
Automatic preprocessing of words is desirable be-
cause the productive nature of language ensures that
no finite lexicon will contain all words. Marchand
et al (2007) show that rule-based methods are rela-
tively ineffective for orthographic syllabification in
English. On the other hand, few data-driven syllabi-
fication systems currently exist.
Demberg (2006) uses a fourth-order Hidden
Markov Model to tackle orthographic syllabification
in German. When added to her L2P system, Dem-
berg?s orthographic syllabification model effects a
one percent absolute improvement in L2P word ac-
curacy.
Bouma (2002) explores syllabification in Dutch.
He begins with finite state transducers, which es-
sentially implement a general preference for onsets.
Subsequently, he uses transformation-based learning
to automatically extract rules that improve his sys-
tem. Bouma?s best system, trained on some 250K
examples, achieves 98.17% word accuracy. Daele-
mans and van den Bosch (1992) implement a back-
propagation network for Dutch orthography, but find
it is outperformed by less complex look-up table ap-
proaches.
Marchand and Damper (2007) investigate the im-
pact of syllabification on the L2P problem in En-
glish. Their Syllabification by Analogy (SbA) algo-
rithm is a data-driven, lazy learning approach. For
each input word, SbA finds the most similar sub-
strings in a lexicon of syllabified words and then
applies these dictionary syllabifications to the input
word. Marchand and Damper report 78.1% word ac-
curacy on the NETtalk dataset, which is not good
enough to improve their L2P system.
Chen (2003) uses an n-gram model and Viterbi
decoder as a syllabifier, and then applies it as a pre-
processing step in his maximum-entropy-based En-
glish L2P system. He finds that the syllabification
pre-processing produces no gains over his baseline
system.
Marchand et al (2007) conduct a more systematic
study of existing syllabification approaches. They
examine syllabification in both the pronunciation
and orthographic domains, comparing their own
SbA algorithm with several instance-based learning
approaches (Daelemans et al, 1997; van den Bosch,
1997) and rule-based implementations. They find
that SbA universally outperforms these other ap-
proaches by quite a wide margin.
Syllabification of phonemes, rather than letters,
has also been investigated (Mu?ller, 2001; Pearson
et al, 2000; Schmid et al, 2007). In this paper, our
focus is on orthographic forms. However, as with
our approach, some previous work in the phonetic
domain has formulated syllabification as a tagging
problem.
3 Structured SVMs
A structured support vector machine (SVM) is a
large-margin training method that can learn to pre-
dict structured outputs, such as tag sequences or
parse trees, instead of performing binary classifi-
cation (Tsochantaridis et al, 2004). We employ a
structured SVM that predicts tag sequences, called
an SVM Hidden Markov Model, or SVM-HMM.
This approach can be considered an HMM because
the Viterbi algorithm is used to find the highest scor-
ing tag sequence for a given observation sequence.
The scoring model employs a Markov assumption:
each tag?s score is modified only by the tag that came
before it. This approach can be considered an SVM
because the model parameters are trained discrimi-
natively to separate correct tag sequences from in-
correct ones by as large a margin as possible. In
contrast to generative HMMs, the learning process
requires labeled training data.
There are a number of good reasons to apply the
structured SVM formalism to this problem. We get
the benefit of discriminative training, not available
in a generative HMM. Furthermore, we can use an
arbitrary feature representation that does not require
569
any conditional independence assumptions. Unlike
a traditional SVM, the structured SVM considers
complete tag sequences during training, instead of
breaking each sequence into a number of training
instances.
Training a structured SVM can be viewed as a
multi-class classification problem. Each training in-
stance xi is labeled with a correct tag sequence yidrawn from a set of possible tag sequences Yi. Asis typical of discriminative approaches, we create a
feature vector ?(x, y) to represent a candidate y and
its relationship to the input x. The learner?s task is
to weight the features using a vector w so that the
correct tag sequence receives more weight than the
competing, incorrect sequences:
?i?y?Yi,y 6=yi [?(xi, yi) ? w > ?(xi, y) ? w] (1)
Given a trained weight vector w, the SVM tags new
instances xi according to:
argmaxy?Yi [?(xi, y) ? w] (2)
A structured SVM finds a w that satisfies Equation 1,
and separates the correct taggings by as large a mar-
gin as possible. The argmax in Equation 2 is con-
ducted using the Viterbi algorithm.
Equation 1 is a simplification. In practice, a struc-
tured distance term is added to the inequality in
Equation 1 so that the required margin is larger for
tag sequences that diverge further from the correct
sequence. Also, slack variables are employed to al-
low a trade-off between training accuracy and the
complexity of w, via a tunable cost parameter.
For most structured problems, the set of negative
sequences in Yi is exponential in the length of xi,and the constraints in Equation 1 cannot be explicitly
enumerated. The structured SVM solves this prob-
lem with an iterative online approach:
1. Collect the most damaging incorrect sequence
y according to the current w.
2. Add y to a growing set Y?i of incorrect se-quences.
3. Find a w that satisfies Equation 1, using the par-
tial Y?i sets in place of Yi.
4. Go to next training example, loop to step 1.
This iterative process is explained in far more detail
in (Tsochantaridis et al, 2004).
4 Syllabification with Structured SVMs
In this paper we apply structured SVMs to the syl-
labification problem. Specifically, we formulate
syllabification as a tagging problem and apply the
SVM-HMM software package1 (Altun et al, 2003).
We use a linear kernel, and tune the SVM?s cost pa-
rameter on a development set. The feature represen-
tation ? consists of emission features, which pair
an aspect of x with a single tag from y, and transi-
tion features, which count tag pairs occurring in y.
With SVM-HMM, the crux of the task is to create
a tag scheme and feature set that produce good re-
sults. In this section, we discuss several different
approaches to tagging for the syllabification task.
Subsequently, we outline our emission feature rep-
resentation. While developing our tagging schemes
and feature representation, we used a development
set of 5K words held out from our CELEX training
data. All results reported in this section are on that
set.
4.1 Annotation Methods
We have employed two different approaches to tag-
ging in this research. Positional tags capture where
a letter occurs within a syllable; Structural tags ex-
press the role each letter is playing within the sylla-
ble.
Positional Tags
The NB tag scheme simply labels every letter
as either being at a syllable boundary (B), or not
(N). Thus, the word im-mor-al-ly is tagged ?N B N
N B N B N N?, indicating a syllable boundary af-
ter each B tag. This binary classification approach
to tagging is implicit in several previous imple-
mentations (Daelemans and van den Bosch, 1992;
Bouma, 2002), and has been done explicitly in both
the orthographic (Demberg, 2006) and phoneme do-
mains (van den Bosch, 1997).
A weakness of NB tags is that they encode no
knowledge about the length of a syllable. Intuitively,
we expect the length of a syllable to be valuable in-
formation ? most syllables in English contain fewer
than four characters. We introduce a tagging scheme
that sequentially numbers the N tags to impart infor-
mation about syllable length. Under the Numbered
1http://svmlight.joachims.org/svm struct.html
570
NB tag scheme, im-mor-al-ly is annotated as ?N1 B
N1 N2 B N1 B N1 N2?. With this tag set, we have
effectively introduced a bias in favor of shorter syl-
lables: tags like N6, N7. . . are comparatively rare, so
the learner will postulate them only when the evi-
dence is particularly compelling.
Structural Tags
Numbered NB tags are more informative than
standard NB tags. However, neither annotation sys-
tem can represent the internal structure of the sylla-
ble. This has advantages: tags can be automatically
generated from a list of syllabified words without
even a passing familiarity with the language. How-
ever, a more informative annotation, tied to phono-
tactics, ought to improve accuracy. Krenn (1997)
proposes the ONC tag scheme, in which phonemes
of a syllable are tagged as an onset, nucleus, or coda.
Given these ONC tags, syllable boundaries can eas-
ily be generated by applying simple regular expres-
sions.
Unfortunately, it is not as straightforward to gen-
erate ONC-tagged training data in the orthographic
domain, even with syllabified training data. Silent
letters are problematic, and some letters can behave
differently depending on their context (in English,
consonants such as m, y, and l can act as vowels in
certain situations). Thus, it is difficult to generate
ONC tags for orthographic forms without at least a
cursory knowledge of the language and its princi-
ples.
For English, tagging the syllabified training set
with ONC tags is performed by the following sim-
ple algorithm. In the first stage, all letters from the
set {a, e, i, o, u} are marked as vowels, while the re-
maining letters are marked as consonants. Next, we
examine all the instances of the letter y. If a y is both
preceded and followed by a consonant, we mark that
instance as a vowel rather than a consonant. In the
second stage, the first group of consecutive vowels
in each syllable is tagged as nucleus. All letters pre-
ceding the nucleus are then tagged as onset, while
all letters following the nucleus are tagged as coda.
Our development set experiments suggested that
numbering ONC tags increases their performance.
Under the Numbered ONC tag scheme, the single-
syllable word stealth is labeled ?O1 O2 N1 N2 C1
C2 C3?.
A disadvantage of Numbered ONC tags is that,
unlike positional tags, they do not represent sylla-
ble breaks explicitly. Within the ONC framework,
we need the conjunction of two tags (such as an N1
tag followed by an O1 tag) to represent the division
between syllables. This drawback can be overcome
by combining ONC tags and NB tags in a hybrid
Break ONC tag scheme. Using Break ONC tags,
the word lev-i-ty is annotated as ?O N CB NB O N?.
The ?NB? tag indicates a letter is both part of the
nucleus and before a syllable break, while the ?N?
tag represents a letter that is part of a nucleus but
in the middle of a syllable. In this way, we get the
best of both worlds: tags that encapsulate informa-
tion about syllable structure, while also representing
syllable breaks explicitly with a single tag.
4.2 Emission Features
SVM-HMM predicts a tag for each letter in a word,
so emission features use aspects of the input to help
predict the correct tag for a specific letter. Consider
the tag for the letter o in the word immorally. With
a traditional HMM, we consider only that it is an
o being emitted, and assess potential tags based on
that single letter. The SVM framework is less re-
strictive: we can include o as an emission feature,
but we can also include features indicating that the
preceding and following letters are m and r respec-
tively. In fact, there is no reason to confine ourselves
to only one character on either side of the focus let-
ter.
After experimenting with the development set, we
decided to include in our feature set a window of
eleven characters around the focus character, five
on either side. Figure 1 shows that performance
gains level off at this point. Special beginning- and
end-of-word characters are appended to words so
that every letter has five characters before and af-
ter. We also experimented with asymmetric context
windows, representing more characters after the fo-
cus letter than before, but we found that symmetric
context windows perform better.
Because our learner is effectively a linear classi-
fier, we need to explicitly represent any important
conjunctions of features. For example, the bigram
bl frequently occurs within a single English sylla-
ble, while the bigram lb generally straddles two syl-
lables. Similarly, a fourgram like tion very often
571
Figure 1: Word accuracy as a function of the window size
around the focus character, using unigram features on the
development set.
forms a syllable in and of itself. Thus, in addition
to the single-letter features outlined above, we also
include in our representation any bigrams, trigrams,
four-grams, and five-grams that fit inside our con-
text window. As is apparent from Figure 2, we see
a substantial improvement by adding bigrams to our
feature set. Higher-order n-grams produce increas-
ingly smaller gains.
Figure 2: Word accuracy as a function of maximum n-
gram size on the development set.
In addition to these primary n-gram features,
we experimented with linguistically-derived fea-
tures. Intuitively, basic linguistic knowledge, such
as whether a letter is a consonant or a vowel, should
be helpful in determining syllabification. However,
our experiments suggested that including features
like these has no significant effect on performance.
We believe that this is caused by the ability of the
SVM to learn such generalizations from the n-gram
features alone.
5 Syllabification Experiments
In this section, we will discuss the results of our best
emission feature set (five-gram features with a con-
text window of eleven letters) on held-out unseen
test sets. We explore several different languages and
datasets, and perform a brief error analysis.
5.1 Datasets
Datasets are especially important in syllabification
tasks. Dictionaries sometimes disagree on the syl-
labification of certain words, which makes a gold
standard difficult to obtain. Thus, any reported ac-
curacy is only with respect to a given set of data.
In this paper, we report the results of experi-
ments on two datasets: CELEX and NETtalk. We
focus mainly on CELEX, which has been devel-
oped over a period of years by linguists in the
Netherlands. CELEX contains English, German,
and Dutch words, and their orthographic syllabifi-
cations. We removed all duplicates and multiple-
word entries for our experiments. The NETtalk dic-
tionary was originally developed with the L2P task
in mind. The syllabification data in NETtalk was
created manually in the phoneme domain, and then
mapped directly to the letter domain.
NETtalk and CELEX do not provide the same
syllabification for every word. There are numer-
ous instances where the two datasets differ in a per-
fectly reasonable manner (e.g. for-ging in NETtalk
vs. forg-ing in CELEX). However, we argue that
NETtalk is a vastly inferior dataset. On a sample of
50 words, NETtalk agrees with Merriam-Webster?s
syllabifications in only 54% of instances, while
CELEX agrees in 94% of cases. Moreover, NETtalk
is riddled with truly bizarre syllabifications, such as
be-aver, dis-hcloth and som-ething. These syllabifi-
cations make generalization very hard, and are likely
to complicate the L2P task we ultimately want to
accomplish. Because previous work in English pri-
marily used NETtalk, we report our results on both
datasets. Nevertheless, we believe NETtalk is un-
suitable for building a syllabification model, and that
results on CELEX are much more indicative of the
efficacy of our (or any other) approach.
At 20K words, NETtalk is much smaller than
CELEX. For NETtalk, we randomly divide the data
into 13K training examples and 7K test words. We
572
randomly select a comparably-sized training set for
our CELEX experiments (14K), but test on a much
larger, 25K set. Recall that 5K training examples
were held out as a development set.
5.2 Results
We report the results using two metrics. Word ac-
curacy (WA) measures how many words match the
gold standard. Syllable break error rate (SBER) cap-
tures the incorrect tags that cause an error in syl-
labification. Word accuracy is the more demand-
ing metric. We compare our system to Syllabifica-
tion by Analogy (SbA), the best existing system for
English (Marchand and Damper, 2007). For both
CELEX and NETtalk, SbA was trained and tested
with the same data as our structured SVM approach.
Data Set Method WA SBER
CELEX
NB tags 86.66 2.69
Numbered NB 89.45 2.51
Numbered ONC 89.86 2.50
Break ONC 89.99 2.42
SbA 84.97 3.96
NETtalk Numbered NB 81.75 5.01SbA 75.56 7.73
Table 1: Syllabification performance in terms of word ac-
curacy and syllable break error percentage.
Table 1 presents the word accuracy and syllable
break error rate achieved by each of our tag sets on
both the CELEX and NETtalk datasets. Of our four
tag sets, NB tags perform noticeably worse. This is
an important result because it demonstrates that it is
not sufficient to simply model a syllable?s bound-
aries; we must also model a syllable?s length or
structure to achieve the best results. Given the simi-
larity in word accuracy scores, it is difficult to draw
definitive conclusions about the remaining three tags
sets, but it does appear that there is an advantage to
modeling syllable structure, as both ONC tag sets
score better than the best NB set.
All variations of our system outperform SbA on
both datasets. Overall, our best tag set lowers the er-
ror rate by one-third, relative to SbA?s performance.
Note that we employ only numbered NB tags for
the NETtalk test; we could not apply structural tag
schemes to the NETtalk training data because of its
bizarre syllabification choices.
Our higher level of accuracy is also achieved more
efficiently. Once a model is learned, our system
can syllabify 25K words in about a minute, while
SbA requires several hours (Marchand, 2007). SVM
training times vary depending on the tag set and
dataset used, and the number of training examples.
On 14K CELEX examples with the ONC tag set,
our model trained in about an hour, on a single-
processor P4 3.4GHz processor. Training time is,
of course, a one-time cost. This makes our approach
much more attractive for inclusion in an actual L2P
system.
Figure 3 shows our method?s learning curve. Even
small amounts of data produce adequate perfor-
mance ? with only 2K training examples, word ac-
curacy is already over 75%. Using a 60K training
set and testing on a held-out 5K set, we see word
accuracies climb to 95.65%.
Figure 3: Word accuracy as function of the size of the
training data.
5.3 Error Analysis
We believe that the reason for the relatively low per-
formance of unnumbered NB tags is the weakness of
the signal coming from NB emission features. With
the exception of q and x, every letter can take on
either an N tag or a B tag with almost equal proba-
bility. This is not the case with Numbered NB tags.
Vowels are much more likely to have N2 or N3 tags
(because they so often appear in the middle of a
syllable), while consonants take on N1 labels with
greater probability.
The numbered NB and ONC systems make many
of the same errors, on words that we might expect to
573
cause difficulty. In particular, both suffer from be-
ing unaware of compound nouns and morphological
phenomena. All three systems, for example, incor-
rectly syllabify hold-o-ver as hol-dov-er. This kind
of error is caused by a lack of knowledge of the com-
ponent words. The three systems also display trou-
ble handling consecutive vowels, as when co-ad-ju-
tors is syllabified incorrectly as coad-ju-tors. Vowel
pairs such as oa are not handled consistently in En-
glish, and the SVM has trouble predicting the excep-
tions.
5.4 Other Languages
We take advantage of the language-independence of
Numbered NB tags to apply our method to other lan-
guages. Without even a cursory knowledge of Ger-
man or Dutch, we have applied our approach to these
two languages.
# Data Points Dutch German
? 50K 98.20 98.81
? 250K 99.45 99.78
Table 2: Syllabification performance in terms of word ac-
curacy percentage.
We have randomly selected two training sets from
the German and Dutch portions of CELEX. Our
smaller model is trained on ? 50K words, while our
larger model is trained on ? 250K. Table 2 shows
our performance on a 30K test set held out from both
training sets. Results from both the small and large
models are very good indeed.
Our performance on these language sets is clearly
better than our best score for English (compare at
95% with a comparable amount of training data).
Syllabification is a more regular process in German
and Dutch than it is in English, which allows our
system to score higher on those languages.
Our method?s word accuracy compares favor-
ably with other methods. Bouma?s finite state ap-
proach for Dutch achieves 96.49% word accuracy
using 50K training points, while we achieve 98.20%.
With a larger model, trained on about 250K words,
Bouma achieves 98.17% word accuracy, against our
99.45%. Demberg (2006) reports that her HMM
approach for German scores 97.87% word accu-
racy, using a 90/10 training/test split on the CELEX
dataset. On the same set, Demberg et al (2007) ob-
tain 99.28% word accuracy by applying the system
of Schmid et al (2007). Our score using a similar
split is 99.78%.
Note that none of these scores are directly com-
parable, because we did not use the same train-test
splits as our competitors, just similar amounts of
training and test data. Furthermore, when assem-
bling random train-test splits, it is quite possible
that words sharing the same lemma will appear in
both the training and test sets. This makes the prob-
lem much easier with large training sets, where the
chance of this sort of overlap becomes high. There-
fore, any large data results may be slightly inflated
as a prediction of actual out-of-dictionary perfor-
mance.
6 L2P Performance
As we stated from the outset, one of our primary mo-
tivations for exploring orthographic syllabification is
the improvements it can produce in L2P systems.
To explore this, we tested our model in conjunc-
tion with a recent L2P system that has been shown
to predict phonemes with state-of-the-art word ac-
curacy (Jiampojamarn et al, 2007). Using a model
derived from training data, this L2P system first di-
vides a word into letter chunks, each containing one
or two letters. A local classifier then predicts a num-
ber of likely phonemes for each chunk, with confi-
dence values. A phoneme-sequence Markov model
is then used to select the most likely sequence from
the phonemes proposed by the local classifier.
Syllabification English Dutch German
None 84.67 91.56 90.18
Numbered NB 85.55 92.60 90.59
Break ONC 85.59 N/A N/A
Dictionary 86.29 93.03 90.57
Table 3: Word accuracy percentage on the letter-to-
phoneme task with and without the syllabification infor-
mation.
To measure the improvement syllabification can
effect on the L2P task, the L2P system was trained
with syllabified, rather than unsyllabified words.
Otherwise, the execution of the L2P system remains
unchanged. Data for this experiment is again drawn
574
from the CELEX dictionary. In Table 3, we re-
port the average word accuracy achieved by the L2P
system using 10-fold cross-validation. We report
L2P performance without any syllabification infor-
mation, with perfect dictionary syllabification, and
with our small learned models of syllabification.
L2P performance with dictionary syllabification rep-
resents an approximate upper bound on the contribu-
tions of our system.
Our syllabification model improves L2P perfor-
mance. In English, perfect syllabification produces
a relative error reduction of 10.6%, and our model
captures over half of the possible improvement, re-
ducing the error rate by 6.0%. To our knowledge,
this is the first time a syllabification model has im-
proved L2P performance in English. Previous work
includes Marchand and Damper (2007)?s experi-
ments with SbA and the L2P problem on NETtalk.
Although perfect syllabification reduces their L2P
relative error rate by 18%, they find that their learned
model actually increases the error rate. Chen (2003)
achieved word accuracy of 91.7% for his L2P sys-
tem, testing on a different dictionary (Pronlex) with
a much larger training set. He does not report word
accuracy for his syllabification model. However, his
baseline L2P system is not improved by adding a
syllabification model.
For Dutch, perfect syllabification reduces the rela-
tive L2P error rate by 17.5%; we realize over 70% of
the available improvement with our syllabification
model, reducing the relative error rate by 12.4%.
In German, perfect syllabification produces only
a small reduction of 3.9% in the relative error rate.
Experiments show that our learned model actually
produces a slightly higher reduction in the relative
error rate. This anomaly may be due to errors or
inconsistencies in the dictionary syllabifications that
are not replicated in the model output. Previously,
Demberg (2006) generated statistically significant
L2P improvements in German by adding syllabifi-
cation pre-processing. However, our improvements
are coming at a much higher baseline level of word
accuracy ? 90% versus only 75%.
Our results also provide some evidence that syl-
labification preprocessing may be more beneficial
to L2P than morphological preprocessing. Dem-
berg et al (2007) report that oracle morphological
annotation produces a relative error rate reduction
of 3.6%. We achieve a larger decrease at a higher
level of accuracy, using an automatic pre-processing
technique. This may be because orthographic syl-
labifications already capture important facts about a
word?s morphology.
7 Conclusion
We have applied structured SVMs to the syllabifi-
cation problem, clearly outperforming existing sys-
tems. In English, we have demonstrated a 33% rela-
tive reduction in error rate with respect to the state of
the art. We used this improved syllabification to in-
crease the letter-to-phoneme accuracy of an existing
L2P system, producing a system with 85.5% word
accuracy, and recovering more than half of the po-
tential improvement available from perfect syllab-
ification. This is the first time automatic syllabi-
fication has been shown to improve English L2P.
Furthermore, we have demonstrated the language-
independence of our system by producing compet-
itive orthographic syllabification solutions for both
Dutch and German, achieving word syllabification
accuracies of 98% and 99% respectively. These
learned syllabification models also improve accu-
racy for German and Dutch letter-to-phoneme con-
version.
In future work on this task, we plan to explore
adding morphological features to the SVM, in an ef-
fort to overcome errors in compound words and in-
flectional forms. We would like to experiment with
performing L2P and syllabification jointly, rather
than using syllabification as a pre-processing step
for L2P. We are also working on applying our
method to phonetic syllabification.
Acknowledgements
Many thanks to Sittichai Jiampojamarn for his help
with the L2P experiments, and to Yannick Marchand
for providing the SbA results.
This research was supported by the Natural Sci-
ences and Engineering Research Council of Canada
and the Alberta Informatics Circle of Research Ex-
cellence.
References
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden Markov support vector ma-
575
chines. Proceedings of the 20th International Confer-
ence on Machine Learning (ICML), pages 3?10.
Susan Bartlett. 2007. Discriminative approach to auto-
matic syllabification. Master?s thesis, Department of
Computing Science, University of Alberta.
Gosse Bouma. 2002. Finite state methods for hyphen-
ation. Natural Language Engineering, 1:1?16.
Stanley Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. Proceedings of the
8th European Conference on Speech Communication
and Technology (Eurospeech).
Walter Daelemans and Antal van den Bosch. 1992.
Generalization performance of backpropagation learn-
ing on a syllabification task. Proceedings of the 3rd
Twente Workshop on Language Technology, pages 27?
38.
Walter Daelemans, Antal van den Bosch, and Ton Wei-
jters. 1997. IGTree: Using trees for compression and
classification in lazy learning algorithms. Artificial In-
telligence Review, pages 407?423.
Vera Demberg, Helmust Schmid, and Gregor Mo?hler.
2007. Phonological constraints and morphological
preprocessing for grapheme-to-phoneme conversion.
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL).
Vera Demberg. 2006. Letter-to-phoneme conversion for
a German text-to-speech system. Master?s thesis, Uni-
versity of Stuttgart.
Philip Babcock Gove, editor. 1993. Webster?s Third New
International Dictionary of the English Language,
Unabridged. Merriam-Webster Inc.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden Markov models to letter-to-phoneme con-
version. Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association of Computational Linguistics HLT-
NAACL, pages 372?379.
Brigitte Krenn. 1997. Tagging syllables. Proceedings of
Eurospeech, pages 991?994.
Yannick Marchand and Robert Damper. 2007. Can syl-
labification improve pronunciation by analogy of En-
glish? Natural Language Engineering, 13(1):1?24.
Yannick Marchand, Connie Adsett, and Robert Damper.
2007. Evaluation of automatic syllabification algo-
rithms for English. In Proceedings of the 6th Inter-
national Speech Communication Association (ISCA)
Workshop on Speech Synthesis.
Yannick Marchand. 2007. Personal correspondence.
Karin Mu?ller. 2001. Automatic detection of syllable
boundaries combining the advantages of treebank and
bracketed corpora training. Proceedings on the 39th
Meeting of the Association for Computational Linguis-
tics (ACL), pages 410?417.
Steve Pearson, Roland Kuhn, Steven Fincke, and Nick
Kibre. 2000. Automatic methods for lexical stress as-
signment and syllabification. In Proceedings of the 6th
International Conference on Spoken Language Pro-
cessing (ICSLP), pages 423?426.
Helmut Schmid, Bernd Mo?bius, and Julia Weidenkaff.
2007. Tagging syllable boundaries with joint N-gram
models. Proceedings of Interspeech.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. Proceedings of the 21st International
Conference on Machine Learning (ICML), pages 823?
830.
Antal van den Bosch. 1997. Learning to pronounce
written words: a study in inductive language learning.
Ph.D. thesis, Universiteit Maastricht.
576
Proceedings of ACL-08: HLT, pages 905?913,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Joint Processing and Discriminative Training for
Letter-to-Phoneme Conversion
Sittichai Jiampojamarn? Colin Cherry? Grzegorz Kondrak?
?Department of Computing Science ?Microsoft Research
University of Alberta One Microsoft Way
Edmonton, AB, T6G 2E8, Canada Redmond, WA, 98052
{sj,kondrak}@cs.ualberta.ca colinc@microsoft.com
Abstract
We present a discriminative structure-
prediction model for the letter-to-phoneme
task, a crucial step in text-to-speech process-
ing. Our method encompasses three tasks
that have been previously handled separately:
input segmentation, phoneme prediction,
and sequence modeling. The key idea is
online discriminative training, which updates
parameters according to a comparison of the
current system output to the desired output,
allowing us to train all of our components
together. By folding the three steps of a
pipeline approach into a unified dynamic
programming framework, we are able to
achieve substantial performance gains. Our
results surpass the current state-of-the-art on
six publicly available data sets representing
four different languages.
1 Introduction
Letter-to-phoneme (L2P) conversion is the task
of predicting the pronunciation of a word, repre-
sented as a sequence of phonemes, from its or-
thographic form, represented as a sequence of let-
ters. The L2P task plays a crucial role in speech
synthesis systems (Schroeter et al, 2002), and is
an important part of other applications, including
spelling correction (Toutanova and Moore, 2001)
and speech-to-speech machine translation (Engel-
brecht and Schultz, 2005).
Converting a word into its phoneme represen-
tation is not a trivial task. Dictionary-based ap-
proaches cannot achieve this goal reliably, due to
unseen words and proper names. Furthermore, the
construction of even a modestly-sized pronunciation
dictionary requires substantial human effort for each
new language. Effective rule-based approaches can
be designed for some languages such as Spanish.
However, Kominek and Black (2006) show that in
languages with a less transparent relationship be-
tween spelling and pronunciation, such as English,
Dutch, or German, the number of letter-to-sound
rules grows almost linearly with the lexicon size.
Therefore, most recent work in this area has focused
on machine-learning approaches.
In this paper, we present a joint framework for
letter-to-phoneme conversion, powered by online
discriminative training. By updating our model pa-
rameters online, considering only the current system
output and its feature representation, we are able to
not only incorporate overlapping features, but also to
use the same learning framework with increasingly
complex search techniques. We investigate two on-
line updates: averaged perceptron and Margin In-
fused Relaxed Algorithm (MIRA). We evaluate our
system on L2P data sets covering English, French,
Dutch and German. In all cases, our system outper-
forms the current state of the art, reducing the best
observed error rate by as much as 46%.
2 Previous work
Letter-to-phoneme conversion is a complex task, for
which a number of diverse solutions have been pro-
posed. It is a structure prediction task; both the input
and output are structured, consisting of sequences of
letters and phonemes, respectively. This makes L2P
a poor fit for many machine-learning techniques that
are formulated for binary classification.
905
The L2P task is also characterized by the exis-
tence of a hidden structure connecting input to out-
put. The training data consists of letter strings paired
with phoneme strings, without explicit links con-
necting individual letters to phonemes. The subtask
of inserting these links, called letter-to-phoneme
alignment, is not always straightforward. For ex-
ample, consider the word ?phoenix? and its corre-
sponding phoneme sequence [f i n I k s], where
we encounter cases of two letters generating a sin-
gle phoneme (ph?f), and a single letter generat-
ing two phonemes (x?k s). Fortunately, align-
ments between letters and phonemes can be discov-
ered reliably with unsupervised generative models.
Originally, L2P systems assumed one-to-one align-
ment (Black et al, 1998; Damper et al, 2005), but
recently many-to-many alignment has been shown
to perform better (Bisani and Ney, 2002; Jiampoja-
marn et al, 2007). Given such an alignment, L2P
can be viewed either as a sequence of classification
problems, or as a sequence modeling problem.
In the classification approach, each phoneme is
predicted independently using a multi-class classi-
fier such as decision trees (Daelemans and Bosch,
1997; Black et al, 1998) or instance-based learn-
ing (Bosch and Daelemans, 1998). These systems
predict a phoneme for each input letter, using the
letter and its context as features. They leverage the
structure of the input but ignore any structure in the
output.
L2P can also be viewed as a sequence model-
ing, or tagging problem. These approaches model
the structure of the output, allowing previously pre-
dicted phonemes to inform future decisions. The
supervised Hidden Markov Model (HMM) applied
by Taylor (2005) achieved poor results, mostly be-
cause its maximum-likelihood emission probabili-
ties cannot be informed by the emitted letter?s con-
text. Other approaches, such as those of Bisani and
Ney (2002) and Marchand and Damper (2000), have
shown that better performance can be achieved by
pairing letter substrings with phoneme substrings,
allowing context to be captured implicitly by these
groupings.
Recently, two hybrid methods have attempted
to capture the flexible context handling of
classification-based methods, while also mod-
eling the sequential nature of the output. The
constraint satisfaction inference (CSInf) ap-
proach (Bosch and Canisius, 2006) improves the
performance of instance-based classification (Bosch
and Daelemans, 1998) by predicting for each letter
a trigram of phonemes consisting of the previous,
current and next phonemes in the sequence. The
final output sequence is the sequence of predicted
phonemes that satisfies the most unigram, bigram
and trigram agreement constraints. The second
hybrid approach (Jiampojamarn et al, 2007) also
extends instance-based classification. It employs a
many-to-many letter-to-phoneme alignment model,
allowing substrings of letters to be classified into
substrings of phonemes, and introducing an input
segmentation step before prediction begins. The
method accounts for sequence information with
post-processing: the numerical scores of possible
outputs from an instance-based phoneme predictor
are combined with phoneme transition probabili-
ties in order to identify the most likely phoneme
sequence.
3 A joint approach
By observing the strengths and weaknesses of previ-
ous approaches, we can create the following priori-
tized desiderata for any L2P system:
1. The phoneme predicted for a letter should be
informed by the letter?s context in the input
word.
2. In addition to single letters, letter substrings
should also be able to generate phonemes.
3. Phoneme sequence information should be in-
cluded in the model.
Each of the previous approaches focuses on one
or more of these items. Classification-based ap-
proaches such as the decision tree system (Black
et al, 1998) and instance-based learning sys-
tem (Bosch and Daelemans, 1998) take into ac-
count the letter?s context (#1). By pairing letter sub-
strings with phoneme substrings, the joint n-gram
approach (Bisani and Ney, 2002) accounts for all
three desiderata, but each operation is informed only
by a limited amount of left context. The many-
to-many classifier of Jiampojamarn et al (2007)
also attempts to account for all three, but it adheres
906
 






	







	





	




 



	





 






	





	

 



	




Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 486?494,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A global model for joint lemmatization and part-of-speech prediction
Kristina Toutanova
Microsoft Research
Redmond, WA 98052
kristout@microsoft.com
Colin Cherry
Microsoft Research
Redmond, WA 98052
colinc@microsoft.com
Abstract
We present a global joint model for
lemmatization and part-of-speech predic-
tion. Using only morphological lexicons
and unlabeled data, we learn a partially-
supervised part-of-speech tagger and a
lemmatizer which are combined using fea-
tures on a dynamically linked dependency
structure of words. We evaluate our
model on English, Bulgarian, Czech, and
Slovene, and demonstrate substantial im-
provements over both a direct transduction
approach to lemmatization and a pipelined
approach, which predicts part-of-speech
tags before lemmatization.
1 Introduction
The traditional problem of morphological analysis
is, given a word form, to predict the set of all of
its possible morphological analyses. A morpho-
logical analysis consists of a part-of-speech tag
(POS), possibly other morphological features, and
a lemma (basic form) corresponding to this tag and
features combination (see Table 1 for examples).
We address this problem in the setting where we
are given a morphological dictionary for training,
and can additionally make use of un-annotated text
in the language. We present a new machine learn-
ing model for this task setting.
In addition to the morphological analysis task
we are interested in performance on two subtasks:
tag-set prediction (predicting the set of possible
tags of words) and lemmatization (predicting the
set of possible lemmas). The result of these sub-
tasks is directly useful for some applications.1 If
we are interested in the results of each of these two
1Tag sets are useful, for example, as a basis of sparsity-
reducing features for text labeling tasks; lemmatization is
useful for information retrieval and machine translation from
a morphologically rich to a morphologically poor language,
where full analysis may not be important.
subtasks in isolation, we might build independent
solutions which ignore the other subtask.
In this paper, we show that there are strong de-
pendencies between the two subtasks and we can
improve performance on both by sharing infor-
mation between them. We present a joint model
for these two subtasks: it is joint not only in that
it performs both tasks simultaneously, sharing in-
formation, but also in that it reasons about multi-
ple words jointly. It uses component tag-set and
lemmatization models and combines their predic-
tions while incorporating joint features in a log-
linear model, defined on a dynamically linked de-
pendency structure of words.
The model is formalized in Section 5 and eval-
uated in Section 6. We report results on English,
Bulgarian, Slovene, and Czech and show that joint
modeling reduces the lemmatization error by up to
19%, the tag-prediction error by up to 26% and the
error on the complete morphological analysis task
by up to 22.6%.
2 Task formalization
The main task that we would like to solve is
as follows: given a lexicon L which contains
all morphological analyses for a set of words
{w1, . . . , wn}, learn to predict all morphological
analyses for other words which are outside of L.
In addition to the lexicon, we are allowed to make
use of unannotated text T in the language. We will
predict morphological analyses for words which
occur in T. Note that the task is defined on word
types and not on words in context.
A morphological analysis of a word w consists
of a (possibly structured) POS tag t, together with
one or several lemmas, which are the possible ba-
sic forms of w when it has tag t. As an exam-
ple, Table 1 illustrates the morphological analy-
ses of several words taken from the CELEX lexi-
cal database of English (Baayen et al, 1995) and
the Multext-East lexicon of Bulgarian (Erjavec,
2004). The Bulgarian words are transcribed in
486
Word Forms Morphological Analyses Tags Lemmas
tell verb base (VB), tell VB tell
told verb past tense (VBD), tell VBD,VBN tell
verb past participle (VBN), tell
tells verb present 3rd person sing (VBZ), tell VBZ tell
telling verb present continuous (VBG), tell VBG,JJ tell
adjective (JJ), telling telling
izpravena adjective fem sing indef (A?FS-N), izpraven A?FS-N izpraven
verb main part past sing fem pass indef (VMPS-SFP-N), izpravia VMPS-SFP-N izpravia
izpraviha verb main indicative 3rd person plural (VMIA3P), izpravia VMIA3P izpravia
Table 1: Examples of morphological analyses of words in English and Bulgarian.
Latin characters. Here by ?POS tags? we mean
both simple main pos-tags such as noun or verb,
and detailed tags which include grammatical fea-
tures, such as VBZ for English indicating present
tense third person singular verb and A?FS-N for
Bulgarian indicating a feminine singular adjective
in indefinite form. In this work we predict only
main POS tags for the Multext-East languages, as
detailed tags were less useful for lemmatization.
Since the predicted elements are sets, we use
precision, recall, and F-measure (F1) to evaluate
performance. The two subtasks, tag-set prediction
and lemmatization are also evaluated in this way.
Table 1 shows the correct tag-sets and lemmas for
each of the example words in separate columns.
Our task setting differs from most work on lemma-
tization which uses either no or a complete rootlist
(Wicentowski, 2002; Dreyer et al, 2008).2 We can
use all forms occurring in the unlabeled text T but
there are no guarantees about the coverage of the
target lemmas or the number of noise words which
may occur in T (see Table 2 for data statistics).
Our setting is thus more realistic since it is what
one would have in a real application scenario.
3 Related work
In work on morphological analysis using machine
learning, the task is rarely addressed in the form
described above. Some exceptions are the work
(Bosch and Daelemans, 1999) which presents a
model for segmenting, stemming, and tagging
words in Dutch, and requires the prediction of
all possible analyses, and (Antal van den Bosch
and Soudi, 2007) which similarly requires the pre-
diction of all morpho-syntactically annotated seg-
mentations of words for Arabic. As opposed to
2These settings refer to the availability of a set of word
forms which are possible lemmas; in the no rootlist setting,
no other word forms in the language are given in addition to
the forms in the training set; in the complete rootlist setting,
a set of word forms which consists of exactly all correct lem-
mas for the words in the test set is given.
our work, these approaches do not make use of un-
labeled data and make predictions for each word
type in isolation.
In machine learning work on lemmatization for
highly inflective languages, it is most often as-
sumed that a word form and a POS tag are given,
and the task is to predict the set of corresponding
lemma(s) (Mooney and Califf, 1995; Clark, 2002;
Wicentowski, 2002; Erjavec and Dz?eroski, 2004;
Dreyer et al, 2008). In our task setting, we do
not assume the availability of gold-standard POS
tags. As a component model, we use a lemmatiz-
ing string transducer which is related to these ap-
proaches and draws on previous work in this and
related string transduction areas. Our transducer is
described in detail in Section 4.1.
Another related line of work approaches the dis-
ambiguation problem directly, where the task is
to predict the correct analysis of word-forms in
context (in sentences), and not all possible anal-
yses. In such work it is often assumed that the cor-
rect POS tags can be predicted with high accuracy
using labeled POS-disambiguated sentences (Er-
javec and Dz?eroski, 2004; Habash and Rambow,
2005). A notable exception is the work of (Adler
et al, 2008), which uses unlabeled data and a
morphological analyzer to learn a semi-supervised
HMM model for disambiguation in context, and
also guesses analyses for unknown words using a
guesser of likely POS-tags. It is most closely re-
lated to our work, but does not attempt to predict
all possible analyses, and does not have to tackle
a complex string transduction problem for lemma-
tization since segmentation is mostly sufficient for
the focus language of that study (Hebrew).
The idea of solving two related tasks jointly to
improve performance on both has been success-
ful for other pairs of tasks (e.g., (Andrew et al,
2004)). Doing joint inference instead of taking a
pipeline approach has also been shown useful for
other problems (e.g., (Finkel et al, 2006; Cohen
and Smith, 2007)).
487
4 Component models
We use two component models as the basis of
addressing the task: one is a partially-supervised
POS tagger which is trained using L and the unla-
beled text T; the other is a lemmatizing transducer
which is trained from L and can use T. The trans-
ducer can optionally be given input POS tags in
training and testing, which can inform the lemma-
tization. The tagger is described in Section 4.2 and
the transducer is described in Section 4.1.
In a pipeline approach to combining the tagging
and lemmatization components, we first predict a
set of tags for each word using the tagger, and then
ask the lemmatizer to predict one lemma for each
of the possible tags. In a direct transduction ap-
proach to the lemmatization subtask, we train the
lemmatizer without access to tags and ask it to
predict a single lemma for each word in testing.
Our joint model, described in Section 5, is defined
in a re-ranking framework, and can choose from
among k-best predictions of tag-sets and lemmas
generated from the component tagger and lemma-
tizer models.
4.1 Morphological analyser
We employ a discriminative character transducer
as a component morphological analyzer. The input
to the transducer is an inflected word (the source)
and possibly an estimated part-of-speech; the out-
put is the lemma of the word (the target). The
transducer is similar to the one described by Ji-
ampojamarn et al (2008) for letter-to-phoneme
conversion, but extended to allow for whole-word
features on both the input and the output. The core
of our engine is the dynamic programming algo-
rithm for monotone phrasal decoding (Zens and
Ney, 2004). The main feature of this algorithm is
its capability to transduce many consecutive char-
acters with a single operation; the same algorithm
is employed to tag subsequences in semi-Markov
CRFs (Sarawagi and Cohen, 2004).
We employ three main categories of features:
context, transition, and vocabulary (rootlist) fea-
tures. The first two are described in detail by Ji-
ampojamarn et al (2008), while the final is novel
to this work. Context features are centered around
a transduction operation such as es ? e , as em-
ployed in gives ? give. Context features include
an indicator for the operation itself, conjoined with
indicators for all n-grams of source context within
a fixed window of the operation. We also employ a
copy feature that indicates if the operation simply
copies the source character, such as e ? e. Tran-
sition features are our Markov, or n-gram features
on transduction operations. Vocabulary features
are defined on complete target words, according
to the frequency of said word in a provided unla-
beled text T. We have chosen to bin frequencies;
experiments on a development set suggested that
two indicators are sufficient: the first fires for any
word that occurred fewer than five times, while a
second also fires for those words that did not oc-
cur at all. By encoding our vocabulary in a trie and
adding the trie index to the target context tracked
by our dynamic programming chart, we can ef-
ficiently track these frequencies during transduc-
tion.
We incorporate the source part-of-speech tag by
appending it to each feature, thus the context fea-
ture es ? e may become es ? e, VBZ. To en-
able communication between the various parts-of-
speech, a universal set of unannotated features also
fires, regardless of the part-of-speech, acting as a
back-off model of how words in general behave
during stemming.
Linear weights are assigned to each of the trans-
ducer?s features using an averaged perceptron for
structure prediction (Collins, 2002). Note that
our features are defined in terms of the operations
employed during transduction, therefore to cre-
ate gold-standard feature vectors, we require not
only target outputs, but also derivations to pro-
duce those outputs. We employ a deterministic
heuristic to create these derivations; given a gold-
standard source-target pair, we construct a deriva-
tion that uses only trivial copy operations until
the first character mismatch. The remainder of
the transduction is performed with a single multi-
character replacement. For example, the deriva-
tion for living ? live would be l ? l , i ? i ,
v ? v , ing ? e. For languages with morpholo-
gies affecting more than just the suffix, one can
either develop a more complex heuristic, or deter-
mine the derivations using a separate aligner such
as that of Ristad and Yianilos (1998).
4.2 Tag-set prediction model
The tag-set model uses a training lexicon L and
unlabeled text T to learn to predict sets of tags
for words. It is based on the semi-supervised tag-
ging model of (Toutanova and Johnson, 2008). It
has two sub-models: one is an ambiguity class
488
or a tag-set model, which can assign probabili-
ties for possible sets of tags of words PTSM (ts|w)
and the other is a word context model, which can
assign probabilities PCM (contextsw|w, ts) to all
contexts of occurrence of word w in an unlabeled
text T. The word-context model is Bayesian and
utilizes a sparse Dirichlet prior on the distributions
of tags given words. In addition, it uses informa-
tion on a four word context of occurrences of w in
the unlabeled text.
Note that the (Toutanova and Johnson, 2008)
model is a tagger that assigns tags to occurrences
of words in the text, whereas we only need to pre-
dict sets of possible tags for word types, such as
the set {VBD, VBN} for the word told. Their com-
ponent sub-model PTSM predicts sets of tags and
it is possible to use it on its own, but by also us-
ing the context model we can take into account
information from the context of occurrence of
words and compute probabilities of tag-sets given
the observed occurrences in T. The two are com-
bined to make a prediction for a tag-set of a test
word w, given unlabeled text T, using Bayes rule:
p(ts|w) ? PTSM (ts|w)PCM (contextsw|w, ts).
We use a direct re-implementation of the word-
context model, using variational inference follow-
ing (Toutanova and Johnson, 2008). For the tag-
set sub-model, we employ a more sophisticated
approach. First, we learn a log-linear classifier in-
stead of a Naive Bayes model, and second, we use
features derived from related words appearing in
T. The possible classes predicted by the classifier
are as many as the observed tag-sets in L. The
sparsity is relieved by adding features for individ-
ual tags t which get shared across tag-sets contain-
ing t.
There are two types of features in the model:
(i) word-internal features: word suffixes, capital-
ization, existence of hyphen, and word prefixes
(such features were also used in (Toutanova and
Johnson, 2008)), and (ii) features based on re-
lated words. These latter features are inspired by
(Cucerzan and Yarowsky, 2000) and are defined as
follows: for a word w such as telling, there is an
indicator feature for every combination of two suf-
fixes ? and ?, such that there is a prefix p where
telling= p? and p? exists in T. For example, if the
word tells is found in T, there would be a feature
for the suffixes ?=ing,?=s that fires. The suffixes
are defined as all character suffixes up to length
three which occur with at least 100 words.
b o u n c e d
VBD     VBN JJ  VBD  VBN
b o u n c e r
JJR NN
bounce
bouncer bounce
?
bounc
bouncer
boucer
fbounce bounce
bounced bounced b o u n c e
VB     NN VB
bounce bounce
? ?
?
Figure 1: A small subset of the graphical model. The
tag-sets and lemmas active in the illustrated assignment are
shown in bold. The extent of joint features firing for the
lemma bounce is shown as a factor indicated by the blue cir-
cle and connected to the assignments of the three words.
5 A global joint model for morphological
analysis
The idea of this model is to jointly predict the set
of possible tags and lemmas of words. In addi-
tion to modeling dependencies between the tags
and lemmas of a single word, we incorporate de-
pendencies between the predictions for multiple
words. The dependencies among words are deter-
mined dynamically. Intuitively, if two words have
the same lemma, their tag-sets are dependent. For
example, imagine that we need to determine the
tag-set and lemmas of the word bouncer. The tag-
set model may guess that the word is an adjective
in comparative form, because of its suffix, and be-
cause its occurrences in T might not strongly in-
dicate that it is a noun. The lemmatizer can then
lemmatize the word like an adjective and come up
with bounce as a lemma. If the tag-set model is
fairly certain that bounce is not an adjective, but
is a verb or a noun, a joint model which looks si-
multaneously at the tags and lemmas of bouncer
and bounce will detect a problem with this assign-
ments and will be able to correct the tagging and
lemmatization error for bouncer.
The main source of information our joint model
uses is information about the assignments of all
words that have the same lemma l. If the tag-set
model is better able to predict the tags of some of
these words, the information can propagate to the
other words. If some of them are lemmatized cor-
rectly, the model can be pushed to lemmatize the
others correctly as well. Since the lemmas of test
words are not given, the dependencies between as-
489
signments of words are determined dynamically
by the currently chosen set of lemmas.
As an example, Figure 1 shows three sample
English words and their possible tag-sets and lem-
mas determined by the component models. It also
illustrates the dependencies between the variables
induced by the features of our model active for the
current (incorrect) assignment.
5.1 Formal model description
Given a set of test words w1, . . . wn and additional
word forms occurring in unlabeled data T, we de-
rive an extended set of words w1, . . . , wm which
contains the original test words and additional re-
lated words, which can provide useful information
about the test words. For example, if bouncer is a
test word and bounce and bounced occur in T these
two words can be added to the set of test words
because they can contribute to the classification
of bouncer. The algorithm for selecting related
words is simple: we add any word for which the
pipelined model predicts a lemma which is also
predicted as one of the top k lemmas for a word
from the test set.
We define a joint model over tag-sets and lem-
mas for all words in the extended set, using fea-
tures defined on a dynamically linked structure
of words and their assigned analyses. It is a re-
ranking model because the tag-sets and possible
lemmas are limited to the top k options provided
by the pipelined model.3 Our model is defined
on a very large set of variables, each of which
can take a large set of values. For example, for
a test set of size about 4,000 words for Slovene an
additional about 9,000 words from T were added
to the extended set. Each of these words has a
corresponding variable which indicates its tag-set
and lemma assignment. The possible assignments
range over all combinations available from the tag-
ging and lemmatizer component models; using the
top three tag-sets per word and top three lemmas
per tag gives an average of around 11.2 possible
assignments per word. This is because the tag-
sets have about 1.2 tags on average and we need
to choose a lemma for each. While it is not the
case that all variables are connected to each other
by features, the connectivity structure can be com-
plex.
More formally, let tsji denote possible tag-sets
3We used top three tag-sets and top three lemmas for each
tag for training.
for word wi, for j = 1 . . . k. Also, let li(t)j de-
note the top lemmas for word wi given tag t. An
assignment of a tag-set and lemmas to a word wi
consists of a choice of a tag-set, tsi (one of the
possible k tag-sets for the word) and, for each tag
t in the chosen tag-set, a choice of a lemma out
of the possible lemmas for that tag and word. For
brevity, we denote such joint assignment by tli.
As a concrete example, in Figure 1, we can see the
current assignments for three words: the assigned
tag-sets are shown underlined and in bolded boxes
(e.g., for bounced, the tag-set {VBD,VBN} is cho-
sen; for both tags, the lemma bounce is assigned).
Other possible tag-sets and other possible lemmas
for each chosen tag are shown in greyed boxes.
Our joint model defines a distribution over as-
signments to all words w1, . . . , wm. The form of
the model is as follows:
P (tl1, . . . , tlm) = eF (tl1,...,tlm)
???
tl?1,...,tl
?m
eF (tl?1,...,tl?m)??
Here F denotes the vector of features defined
over an assignment for all words in the set and ?
is a vector of parameters for the features. Next we
detail the types of features used.
Word-local features. The aim of such features is
to look at the set of all tags assigned to a word to-
gether with all lemmas and capture coarse-grained
dependencies at this level. These features intro-
duce joint dependencies between the tags and lem-
mas of a word, but they are still local to the as-
signment of single words. One such feature is the
number of distinct lemmas assigned across the dif-
ferent tags in the assigned tag-set. Another such
feature is the above joined with the identity of
the tag-set. For example, if a word?s tag-set is
{VBD,VBN}, it will likely have the same lemma
for both tags and the number of distinct lemmas
will be one (e.g., the word bounced), whereas if it
has the tags VBG, JJ the lemmas will be distinct for
the two tags (e.g. telling). In this class of features
are also the log-probabilities from the tag-set and
lemmatizer models.
Non-local features. Our non-local features look,
for every lemma l, at all words which have that
lemma as the lemma for at least one of their as-
signed tags, and derive several predicates on the
joint assignment to these words. For example,
using our word graph in the figure, the lemma
bounce is assigned to bounced for tags VBD and
VBN, to bounce for tags VB and NN, and to
bouncer for tag JJR. One feature looks at the
combination of tags corresponding to the differ-
490
ent forms of the lemma. In this case this would
be [JJR,NN+VB-lem,VBD+VBN]. The feature also
indicates any word which is exactly equal to the
lemma with lem as shown for the NN and VB tags
corresponding to bounce. Our model learns a neg-
ative weight for this feature, because the lemma
of a word with tag JJR is most often a word with
at least one tag equal to JJ. A variant of this
feature also appends the final character of each
word, like this: [JJR+r,NN+VB+e-lem,VBD+VBN-
d]. This variant was helpful for the Slavic lan-
guages because when using only main POS tags,
the granularity of the feature is too coarse. An-
other feature simply counts the number of distinct
words having the same lemma, encouraging re-
using the same lemma for different words. An ad-
ditional feature fires for every distinct lemma, in
effect counting the number of assigned lemmas.
5.2 Training and inference
Since the model is defined to re-rank candidates
from other component models, we need two differ-
ent training sets: one for training the component
models, and another for training the joint model
features. This is because otherwise the accuracy
of the component models would be overestimated
by the joint model. Therefore, we train the com-
ponent models on the training lexicons LTrain and
select their hyperparameters on the LDev lexicons.
We then train the joint model on the LDev lexicons
and evaluate it on the LTest lexicons. When apply-
ing models to the LTest set, the component mod-
els are first retrained on the union of LTrain and
LDev so that all models can use the same amount
of training data, without giving unfair advantage
to the joint model. Such set-up is also used for
other re-ranking models (Collins, 2000).
For training the joint model, we maximize the
log-likelihood of the correct assignment to the
words in LDev, marginalizing over the assign-
ments of other related words added to the graph-
ical model. We compute the gradient approx-
imately by computing expectations of features
given the observed assignments and marginal ex-
pectations of features. For computing these ex-
pectations we use Gibbs sampling to sample com-
plete assignments to all words in the graph.4 We
4We start the Gibbs sampler by the assignments found by
the pipeline method and then use an annealing schedule to
find a neighborhood of high-likelihood assignments, before
taking about 10 complete samples from the graph to compute
expectations.
use gradient descent with a small learning rate, se-
lected to optimize the accuracy on the LDev set.
For finding a most likely assignment at test time,
we use the sampling procedure, this time using a
slower annealing schedule before taking a single
sample to output as a guessed answer.
For the Gibbs sampler, we need to sample an
assignment for each word in turn, given the current
assignments of all other words. Let us denote the
current assignment to all words except wi as tl?i.
The conditional probability of an assignment tli
for word wi is given by:
P (tli|tl?i) = eF (tli,tl
?i)???
tl?i
eF (tl?i,tl?i)??
The summation in the denominator is over all
possible assignments for word wi. To compute
these quantities we need to consider only the fea-
tures involving the current word. Because of the
nature of the features in our model, it is possible
to isolate separate connected components which
do not share features for any assignment. If two
words do not share lemmas for any of their possi-
ble assignments, they will be in separate compo-
nents. Block sampling within a component could
be used if the component is relatively small; how-
ever, for the common case where there are five or
more words in a fully connected component ap-
proximate inference is necessary.
6 Experiments
6.1 Data
We use datasets for four languages: English, Bul-
garian, Slovene, and Czech. For each of the lan-
guages, we need a lexicon with morphological
analyses L and unlabeled text.
For English we derive the lexicon from CELEX
(Baayen et al, 1995), and for the other lan-
guages we use the Multext-East resources (Er-
javec, 2004). For English we use only open-class
words (nouns, verbs, adjectives, and adverbs), and
for the other languages we use words of all classes.
The unlabeled data for English we use is the union
of the Penn Treebank tagged WSJ data (Marcus et
al., 1993) and the BLLIP corpus.5 For the rest of
the languages we use only the text of George Or-
well?s novel 1984, which is provided in morpho-
logically disambiguated form as part of Multext-
East (but we don?t use the annotations). Table 2
5The BLLIP corpus contains approximately 30 million
words of automatically parsed WSJ data. We used these cor-
pora as plain text, without the annotations.
491
Lang LTrain LDev LTest Text
ws tl nf ws tl nf ws tl nf
Eng 5.2 1.5 0.3 7.4 1.4 0.8 7.4 1.4 0.8 320
Bgr 6.9 1.2 40.8 3.8 1.1 53.6 3.8 1.1 52.8 16.3
Slv 7.5 1.2 38.3 4.2 1.2 49.1 4.2 1.2 49.8 17.8
Cz 7.9 1.1 32.8 4.5 1.1 43.2 4.5 1.1 43.0 19.1
Table 2: Data sets used in experiments. The number of
word types (ws) is shown approximately in thousands. Also
shown are average number of complete analyses (tl) and per-
cent target lemmas not found in the unlabeled text (nf).
details statistics about the data set sizes for differ-
ent languages.
We use three different lexicons for each lan-
guage: one for training (LTrain), one for devel-
opment (LDev), and one for testing (LTest). The
global model weights are trained on the develop-
ment set as described in section 5.2. The lex-
icons are derived such that very frequent words
are likely to be in the training lexicon and less
frequent words in the dev and test lexicons, to
simulate a natural process of lexicon construction.
The English lexicons were constructed as follows:
starting with the full CELEX dictionary and the
text of the Penn Treebank corpus, take all word
forms appearing in the first 2000 sentences (and
are found in CELEX) to form the training lexi-
con, and then take all other words occurring in
the corpus and split them equally between the de-
velopment and test lexicons (every second word
is placed in the test set, in the order of first oc-
currence in the corpus). For the rest of the lan-
guages, the same procedure is applied, starting
with the full Multext-East lexicons and the text of
the novel 1984. Note that while it is not possi-
ble for training words to be included in the other
lexicons, it is possible for different forms of the
same lemma to be in different lexicons. The size
of the training lexicons is relatively small and we
believe this is a realistic scenario for application of
such models. In Table 2 we can see the number of
words in each lexicon and the unlabeled corpora
(by type), the average number of tag-lemma com-
binations per word,6 as well as the percentage of
word lemmas which do not occur in the unlabeled
text. For English, the large majority of target lem-
mas are available in T (with only 0.8% missing),
whereas for the Multext-East languages around 40
to 50% of the target lemmas are not found in T;
this partly explains the lower performance on these
languages.
6The tags are main tags for the Multext-East languages
and detailed tags for English.
Language Tag Model Tag Lem T+L
English none ? 94.0 ?
full 89.9 95.3 88.9
no unlab data 80.0 94.1 78.3
Bulgarian none ? 73.2 ?
full 87.9 79.9 75.3
no unlab data 80.2 76.3 70.4
Table 3: Development set results using different tag-set
models and pipelined prediction.
6.2 Evaluation of direct and pipelined models
for lemmatization
As a first experiment which motivates our joint
modeling approach, we present a comparison on
lemmatization performance in two settings: (i)
when no tags are used in training or testing by the
transducer, and (ii) when correct tags are used in
training and tags predicted by the tagging model
are used in testing. In this section, we report per-
formance on English and Bulgarian only. Compa-
rable performance on the other Multext-East lan-
guages is shown in Section 6.
Results are presented in Table 3. The experi-
ments are performed using LTrain for training and
LDev for testing. We evaluate the models on tag-
set F-measure (Tag), lemma-set F-measure(Lem)
and complete analysis F-measure (T+L). We show
the performance on lemmatization when tags are
not predicted (Tag Model is none), and when tags
are predicted by the tag-set model. We can see that
on both languages lemmatization is significantly
improved when a latent tag-set variable is used as
a basis for prediction: the relative error reduction
in Lem F-measure is 21.7% for English and 25%
for Bulgarian. For Bulgarian and the other Slavic
languages we predicted only main POS tags, be-
cause this resulted in better lemmatization perfor-
mance.
It is also interesting to evaluate the contribution
of the unlabeled data T to the performance of the
tag-set model. This can be achieved by remov-
ing the word-context sub-model of the tagger and
also removing related word features. The results
achieved in this setting for English and Bulgarian
are shown in the rows labeled ?no unlab data?. We
can see that the tag-set F-measure of such models
is reduced by 8 to 9 points and the lemmatization
F-measure is similarly reduced. Thus a large por-
tion of the positive impact tagging has on lemma-
tization is due to the ability of tagging models to
exploit unlabeled data.
The results of this experiment show there are
strong dependencies between the tagging and
492
lemmatization subtasks, which a joint model could
exploit.
6.3 Evaluation of joint models
Since our joint model re-ranks candidates pro-
duced by the component tagger and lemmatizer,
there is an upper bound on the achievable perfor-
mance. We report these upper bounds for the four
languages in Table 4, at the rows which list m-best
oracle under Model. The oracle is computed using
five-best tag-set predictions and three-best lemma
predictions per tag. We can see that the oracle per-
formance on tag F-measure is quite high for all
languages, but the performance on lemmatization
and the complete task is close to only 90 percent
for the Slavic languages. As a second oracle we
also report the perfect tag oracle, which selects
the lemmas determined by the transducer using the
correct part-of-speech tags. This shows how well
we could do if we made the tagging model perfect
without changing the lemmatizer. For the Slavic
languages this is quite a bit lower than the m-best
oracles, showing that the majority of errors of the
pipelined approach cannot be fixed by simply im-
proving the tagging model. Our global model has
the potential to improve lemma assignments even
given correct tags, by sharing information among
multiple words.
The actual achieved performance for three dif-
ferent models is also shown. For comparison,
the lemmatization performance of the direct trans-
duction approach which makes no use of tags is
also shown. The pipelined models select one-
best tag-set predictions from the tagging model,
and the 1-best lemmas for each tag, like the mod-
els used in Section 6.2. The model name lo-
cal FS denotes a joint log-linear model which
has only word-internal features. Even with only
word-internal features, performance is improved
for most languages. The the highest improvement
is for Slovene and represents a 7.8% relative re-
duction in F-measure error on the complete task.
When features looking at the joint assignments
of multiple words are added, the model achieves
much larger improvements (models joint FS in the
Table) across all languages.7 The highest overall
improvement compared to the pipelined approach
is again for Slovene and represents 22.6% reduc-
tion in error for the full task; the reduction is 40%
7Since the optimization is stochastic, the results are av-
eraged over four runs. The standard deviations are between
0.02 and 0.11.
Language Model Tag Lem T+L
English tag oracle 100 98.9 98.7
English m-best oracle 97.9 99.0 97.5
English no tags ? 94.3 ?
English pipelined 90.9 95.9 90.0
English local FS 90.8 95.9 90.0
English joint FS 91.7 96.1 91.0
Bulgarian tag oracle 100 84.3 84.3
Bulgarian m-best oracle 98.4 90.7 89.9
Bulgarian no tags ? 73.2 ?
Bulgarian pipelined 87.9 78.5 74.6
Bulgarian local FS 88.9 79.2 75.8
Bulgarian joint FS 89.5 81.0 77.8
Slovene tag oracle 100 85.9 85.9
Slovene m-best oracle 98.7 91.2 90.5
Slovene no tags ? 78.4 ?
Slovene pipelined 89.7 82.1 78.3
Slovene local FS 90.8 82.7 80.0
Slovene joint FS 92.4 85.5 83.2
Czech tag oracle 100 83.2 83.2
Czech m-best oracle 98.1 88.7 87.4
Czech no tags ? 78.7 ?
Czech pipelined 92.3 80.7 77.5
Czech local FS 92.3 80.9 78.0
Czech joint FS 93.7 83.0 80.5
Table 4: Results on the test set achieved by joint and
pipelined models and oracles. The numbers represent tag-set
prediction F-measure (Tag), lemma-set prediction F-measure
(Lem) and F-measure on predicting complete tag, lemma
analysis sets (T+L).
relative to the upper bound achieved by the m-best
oracle. The smallest overall improvement is for
English, representing a 10% error reduction over-
all, which is still respectable. The larger improve-
ment for Slavic languages might be due to the fact
that there are many more forms of a single lemma
and joint reasoning allows us to pool information
across the forms.
7 Conclusion
In this paper we concentrated on the task of mor-
phological analysis, given a lexicon and unanno-
tated data. We showed that the tasks of tag pre-
diction and lemmatization are strongly dependent
and that by building state-of-the art models for
the two subtasks and performing joint inference
we can improve performance on both tasks. The
main contribution of our work was that we intro-
duced a joint model for the two subtasks which in-
corporates dependencies between predictions for
multiple word types. We described a set of fea-
tures and an approximate inference procedure for a
global log-linear model capturing such dependen-
cies, and demonstrated its effectiveness on English
and three Slavic languages.
Acknowledgements
We would like to thank Galen Andrew and Lucy Vander-
wende for useful discussion relating to this work.
493
References
Meni Adler, Yoav Goldberg, and Michael Elhadad. 2008.
Unsupervised lexicon-based resolution of unknown words
for full morpholological analysis. In Proceedings of ACL-
08: HLT.
Galen Andrew, Trond Grenager, and Christopher Manning.
2004. Verb sense and subcategorization: Using joint in-
ference to improve performance on complementary tasks.
In EMNLP.
Erwin Marsi Antal van den Bosch and Abdelhadi Soudi.
2007. Memory-based morphological analysis and part-
of-speech tagging of arabic. In Abdelhadi Soudi, An-
tal van den Bosch, and Gunter Neumann, editors, Arabic
Computational Morphology Knowledge-based and Em-
pirical Methods. Springer.
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995. The
CELEX lexical database.
Antal Van Den Bosch and Walter Daelemans. 1999.
Memory-based morphological analysis. In Proceedings
of the 37th Annual Meeting of the Association for Compu-
tational Linguistics.
Alexander Clark. 2002. Memory-based learning of mor-
phology with stochastic transducers. In Proceedings of
the 40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 513?520.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpholog-
ical and syntactic disambiguation. In EMNLP.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In ICML.
M. Collins. 2002. Discriminative training methods for hid-
den markov models: Theory and experiments with percep-
tron algorithms. In EMNLP.
S. Cucerzan and D. Yarowsky. 2000. Language independent
minimally supervised induction of lexical probabilities. In
Proceedings of ACL 2000.
Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing
(EMNLP), pages 1080?1089, Honolulu, October.
Tomaz? Erjavec and Saa?o Dz?eroski. 2004. Machine learn-
ing of morphosyntactic structure: lemmatizing unknown
Slovene words. Applied Artificial Intelligence, 18:17?
41.
Tomaz? Erjavec. 2004. Multext-east version 3: Multilingual
morphosyntactic specifications, lexicons and corpora. In
Proceedings of LREC-04.
Jenny Rose Finkel, Christopher D. Manning, and Andrew Y.
Ng. 2006. Solving the problem of cascading errors:
Approximate bayesian inference for linguistic annotation
pipelines. In EMNLP.
Nizar Habash and Owen Rambow. 2005. Arabic tokeniza-
tion, part-of-speech tagging and morphological disam-
biguation in one fell swoop. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kon-
drak. 2008. Joint processing and discriminative training
for letter-to-phoneme conversion. In Proceedings of ACL-
08: HLT, pages 905?913, Columbus, Ohio, June.
M. Marcus, B. Santorini, and Marcinkiewicz. 1993. Build-
ing a large annotated coprus of english: the penn treebank.
Computational Linguistics, 19.
Raymond J. Mooney and Mary Elaine Califf. 1995. Induc-
tion of first-order decision lists: Results on learning the
past tense of english verbs. Journal of Artificial Intelli-
gence Research, 3:1?24.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning
string-edit distance. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 20(5):522?532.
Sunita Sarawagi and William Cohen. 2004. Semimarkov
conditional random fields for information extraction. In
ICML.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
LDA-based model for semi-supervised part-of-speech tag-
ging. In nips08.
Richard Wicentowski. 2002. Modeling and Learning Mul-
tilingual Inflectional Morphology in a Minimally Super-
vised Framework. Ph.D. thesis, Johns-Hopkins Univer-
sity.
R. Zens and H. Ney. 2004. Improvements in phrase-based
statistical machine translation. In HLT-NAACL, pages
257?264, Boston, USA, May.
494
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 69?71,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
NEWS 2009 Machine Transliteration Shared Task System Description:
Transliteration with Letter-to-Phoneme Technology
Colin Cherry and Hisami Suzuki
Microsoft Research
One Microsoft Way
Redmond, WA, 98052
{colinc,hisamis}@microsoft.com
Abstract
We interpret the problem of transliterat-
ing English named entities into Hindi or
Japanese Katakana as a variant of the
letter-to-phoneme (L2P) subtask of text-
to-speech processing. Therefore, we apply
a re-implementation of a state-of-the-art,
discriminative L2P system (Jiampojamarn
et al, 2008) to the problem, without fur-
ther modification. In doing so, we hope
to provide a baseline for the NEWS 2009
Machine Transliteration Shared Task (Li
et al, 2009), indicating how much can be
achieved without transliteration-specific
technology. This paper briefly sum-
marizes the original work and our re-
implementation. We also describe a bug
in our submitted implementation, and pro-
vide updated results on the development
and test sets.
1 Introduction
Transliteration occurs when a word is borrowed
into a language with a different character set from
its language of origin. The word is transcribed into
the new character set in a manner that maintains
phonetic correspondence.
When attempting to automate machine translit-
eration, modeling the channel that transforms
source language characters into transliterated tar-
get language characters is a key component to
good performance. Since the primary signal fol-
lowed by human transliterators is phonetic corre-
spondence, it makes sense that a letter-to-phoneme
(L2P) transcription engine would perform well at
this task. Of course, transliteration is often framed
within the larger problems of translation and bilin-
gual named entity co-reference, making available
a number of other interesting features, such as tar-
get lexicons (Knight and Graehl, 1998), distribu-
tional similarity (Bilac and Tanaka, 2005), or the
dates of an entity?s mentions in the news (Kle-
mentiev and Roth, 2006). However, this task?s fo-
cus on generation has isolated the character-level
component, which makes L2P technology a near-
ideal match. For our submission, we re-implement
the L2P approach described by Jiampojamarn et
al. (2008) as faithfully as possible, and apply it
unmodified to the transliteration shared task for
the English-to-Hindi (Kumaran and Kellner, 2007)
and English-to-Japanese Katakana1 tests.
2 Approach
2.1 Summary of L2P approach
The core of the L2P transduction engine is the
dynamic programming algorithm for monotone
phrasal decoding (Zens and Ney, 2004). The main
feature of this algorithm is its capability to trans-
duce many consecutive characters with a single
operation. This algorithm is used to conduct a
search for a max-weight derivation according to
a linear model with indicator features. A sample
derivation is shown in Figure 1.
There are two main categories of features: con-
text and transition features, which follow the first
two feature templates described by Jiampojamarn
et al (2008). Context features are centered around
a transduction operation. These features include
an indicator for the operation itself, which is then
conjoined with indicators for all n-grams of source
context within a fixed window of the operation.
Transition features are Markov or n-gram features.
They ensure that the produced target string makes
sense as a character sequence, and are represented
as indicators on the presence of target n-grams.
The feature templates have two main parameters,
the size S of the character window from which
source context features are drawn, and the max-
imum length T of target n-gram indicators. We
fit these parameters using grid search over 1-best
1Provided by http://www.cjk.org
69
ame ?A , ri ?J , can ?S
Figure 1: Example derivation transforming
?American? into ?AJS?.
accuracy on the provided development sets.
The engine?s features are trained using the
structured perceptron (Collins, 2002). Jiampo-
jamarn et al (2008) show strong improvements
in the L2P domain using MIRA in place of the
perceptron update; unfortunately, we did not im-
plement a k-best MIRA update due to time con-
straints. In our implementation, no special con-
sideration was given to the availability of multi-
ple correct answers in the training data; we always
pick the first reference transliteration and treat it
as the only correct answer. Investigating the use
of all correct answers would be an obvious next
step to improve the system.
2.2 Major differences in implementation
Our system made two alternate design decisions
(we do not claim improvements) over those made
by (Jiampojamarn et al, 2008), mostly based on
the availability of software. First, we employed a
beam of 40 candidates in our decoder, to enable ef-
ficient use of large language model contexts. This
is put to good use in the Hindi task, where we
found n-gram indicators of length up to n = 6
provided optimal development performance.
Second, we employed an alternate character
aligner to create our training derivations. This
aligner is similar to recent non-compositional
phrasal word-alignment models (Zhang et al,
2008), limited so it can only produce monotone
character alignments. The aligner creates sub-
string alignments, without insertion or deletion
operators. As such, an aligned transliteration pair
also serves as a transliteration derivation. We em-
ployed a maximum substring length of 3.
The training data was heuristically cleaned af-
ter alignment. Any derivation found by the aligner
that uses an operation occurring fewer than 3 times
throughout the entire training set was eliminated.
This reduced training set sizes to 8,511 pairs
for English-Hindi and 20,306 pairs for English-
Katakana.
Table 1: Development and test 1-best accuracies,
as reported by the official evaluation tool
System / Test set With Bug Fixed
Hindi Dev 36.7 39.6
Hindi Test 41.8 46.6
Katakana Dev 46.0 47.1
Katakana Test 46.6 46.9
3 The Bug
The submitted version of our system had a bug
in its transition features: instead of generating an
indicator for every possible n-gram in the gener-
ated target sequence, it generated n-grams over
target substrings, defined by the operations used
during transduction. Consider, for example, the
derivation shown in Figure 1, which generates
?AJS?. With buggy trigram transition
features, the final operation would produce the
single indicator [AJ|S], instead of the two
character-level trigrams [AJ|] and [J|S].
This leads to problems with data sparsity, which
we had not noticed on unrelated experiments with
larger training data. We report results both with
the bug and with fixed transition features. We do
so to emphasize the importance of a fine-grained
language discriminative language model, as op-
posed to one which operates on a substring level.
4 Development
Development consisted of performing a parameter
grid search over S and T for each language pair?s
development set. All combinations of S = 0 . . . 4
and T = 0 . . . 7 were tested for each language
pair. Based on these experiments, we selected (for
the fixed version), values of S = 2, T = 6 for
English-Hindi, and S = 4, T = 3 for English-
Katakana.
5 Results
The results of our internal experiments with the
official evaluation tool are shown in Table 1. We
report 1-best accuracy on both development and
test sets, with both the buggy and fixed versions of
our system. As one can see, the bug makes less of
an impact in the English-Katakana setting, where
more training data is available.
70
6 Conclusion
We have demonstrated that an automatic letter-
to-phoneme transducer performs fairly well on
this transliteration shared task, with no language-
specific or transliteration-specific modifications.
Instead, we simply considered Hindi or Katakana
to be an alternate encoding for English phonemes.
In the future, we would like to investigate proper
use of multiple reference answers during percep-
tron training.
Acknowledgments
We would like to thank the NEWS 2009 Machine
Transliteration Shared Task organizers for creating
this venue for comparing transliteration methods.
We would also like to thank Chris Quirk for pro-
viding us with his alignment software.
References
Slaven Bilac and Hozumi Tanaka. 2005. Extracting
transliteration pairs from comparable corpora. In
Proceedings of the Annual Meeting of the Natural
Language Processing Society, Japan.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In ACL,
pages 905?913, Columbus, Ohio, June.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In HLT-NAACL, pages
82?88, New York City, USA, June.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proc. of
the 30th SIGIR.
Haizhou Li, A. Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report on NEWS 2009 machine
transliteration shared task. In Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009), Singapore.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
HLT-NAACL, pages 257?264, Boston, USA, May.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL, pages 97?105, Columbus, Ohio, June.
71
A Comparison of Syntactically Motivated Word Alignment Spaces
Colin Cherry
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
colinc@cs.ualberta.ca
Dekang Lin
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA, USA, 94043
lindek@google.com
Abstract
This work is concerned with the space of
alignments searched by word alignment
systems. We focus on situations where
word re-ordering is limited by syntax. We
present two new alignment spaces that
limit an ITG according to a given depen-
dency parse. We provide D-ITG grammars
to search these spaces completely and
without redundancy. We conduct a care-
ful comparison of five alignment spaces,
and show that limiting search with an ITG
reduces error rate by 10%, while a D-ITG
produces a 31% reduction.
1 Introduction
Bilingual word alignment finds word-level corre-
spondences between parallel sentences. The task
originally emerged as an intermediate result of
training the IBM translation models (Brown et
al., 1993). These models use minimal linguistic
intuitions; they essentially treat sentences as flat
strings. They remain the dominant method for
word alignment (Och and Ney, 2003). There have
been several proposals to introduce syntax into
word alignment. Some work within the framework
of synchronous grammars (Wu, 1997; Melamed,
2003), while others create a generative story that
includes a parse tree provided for one of the sen-
tences (Yamada and Knight, 2001).
There are three primary reasons to add syntax to
word alignment. First, one can incorporate syntac-
tic features, such as grammar productions, into the
models that guide the alignment search. Second,
movement can be modeled more naturally; when a
three-word noun phrase moves during translation,
it can be modeled as one movement operation in-
stead of three. Finally, one can restrict the type of
movement that is considered, shrinking the num-
ber of alignments that are attempted. We investi-
gate this last advantage of syntactic alignment. We
fix an alignment scoring model that works equally
well on flat strings as on parse trees, but we vary
the space of alignments evaluated with that model.
These spaces become smaller as more linguistic
guidance is added. We measure the benefits and
detriments of these constrained searches.
Several of the spaces we investigate draw guid-
ance from a dependency tree for one of the
sentences. We will refer to the parsed lan-
guage as English and the other as Foreign. Lin
and Cherry (2003) have shown that adding a
dependency-based cohesion constraint to an align-
ment search can improve alignment quality. Un-
fortunately, the usefulness of their beam search
solution is limited: potential alignments are con-
structed explicitly, which prevents a perfect search
of alignment space and the use of algorithms like
EM. However, the cohesion constraint is based
on a tree, which should make it amenable to dy-
namic programming solutions. To enable such
techniques, we bring the cohesion constraint in-
side the ITG framework (Wu, 1997).
Zhang and Gildea (2004) compared Yamada
and Knight?s (2001) tree-to-string alignment
model to ITGs. They concluded that methods like
ITGs, which create a tree during alignment, per-
form better than methods with a fixed tree estab-
lished before alignment begins. However, the use
of a fixed tree is not the only difference between
(Yamada and Knight, 2001) and ITGs; the proba-
bility models are also very different. By using a
fixed dependency tree inside an ITG, we can re-
visit the question of whether using a fixed tree is
harmful, but in a controlled environment.
2 Alignment Spaces
Let an alignment be the entire structure that con-
nects a sentence pair, and let a link be the in-
dividual word-to-word connections that make up
an alignment. An alignment space determines
the set of all possible alignments that can ex-
145
ist for a given sentence pair. Alignment spaces
can emerge from generative stories (Brown et al,
1993), from syntactic notions (Wu, 1997), or they
can be imposed to create competition between
links (Melamed, 2000). They can generally be de-
scribed in terms of how links interact.
For the sake of describing the size of alignment
spaces, we will assume that both sentences have n
tokens. The largest alignment space for a sentence
pair has 2n2 possible alignments. This describes
the case where each of the n2 potential links can
be either on or off with no restrictions.
2.1 Permutation Space
A straight-forward way to limit the space of pos-
sible alignments is to enforce a one-to-one con-
straint (Melamed, 2000). Under such a constraint,
each token in the sentence pair can participate in
at most one link. Each token in the English sen-
tence picks a token from the Foreign sentence to
link to, which is then removed from competition.
This allows for n! possible alignments1, a substan-
tial reduction from 2n2 .
Note that n! is also the number of possi-
ble permutations of the n tokens in either one
of the two sentences. Permutation space en-
forces the one-to-one constraint, but allows any re-
ordering of tokens as they are translated. Permu-
tation space methods include weighted maximum
matching (Taskar et al, 2005), and approxima-
tions to maximum matching like competitive link-
ing (Melamed, 2000). The IBM models (Brown
et al, 1993) search a version of permutation space
with a one-to-many constraint.
2.2 ITG Space
Inversion Transduction Grammars, or ITGs (Wu,
1997) provide an efficient formalism to syn-
chronously parse bitext. This produces a parse tree
that decomposes both sentences and also implies
a word alignment. ITGs are transduction gram-
mars because their terminal symbols can produce
tokens in both the English and Foreign sentences.
Inversions occur when the order of constituents is
reversed in one of the two sentences.
In this paper, we consider the alignment space
induced by parsing with a binary bracketing ITG,
such as:
A ? [AA] | ?AA? | e/f (1)
1This is a simplification that ignores null links. The actual
number of possible alignments lies between n! and (n+1)n.
The terminal symbol e/f represents tokens output
to the English and Foreign sentences respectively.
Square brackets indicate a straight combination of
non-terminals, while angle brackets indicate an in-
verted combination: ?A1A2?means that A1A2 ap-
pears in the English sentence, whileA2A1 appears
in the Foreign sentence.
Used as a word aligner, an ITG parser searches
a subspace of permutation space: the ITG requires
that any movement that occurs during translation
be explained by a binary tree with inversions.
Alignments that allow no phrases to be formed in
bitext are not attempted. This results in two for-
bidden alignment structures, shown in Figure 1,
called ?inside-out? transpositions in (Wu, 1997).
Note that no pair of contiguous tokens in the top
    
       
    
Figure 1: Forbidden alignments in ITG
sentence remain contiguous when projected onto
the bottom sentence. Zens and Ney (2003) explore
the re-orderings allowed by ITGs, and provide a
formulation for the number of structures that can
be built for a sentence pair of size n. ITGs explore
almost all of permutation space when n is small,
but their coverage of permutation space falls off
quickly for n > 5 (Wu, 1997).
2.3 Dependency Space
Dependency space defines the set of all align-
ments that maintain phrasal cohesion with respect
to a dependency tree provided for the English sen-
tence. The space is constrained so that the phrases
in the dependency tree always move together.
Fox (2002) introduced the notion of head-
modifier and modifier-modifier crossings. These
occur when a phrase?s image in the Foreign sen-
tence overlaps with the image of its head, or one of
its siblings. An alignment with no crossings main-
tains phrasal cohesion. Figure 2 shows a head-
modifier crossing: the image c of a head 2 overlaps
with the image (b, d) of 2?s modifier, (3, 4). Lin
 	 
 
   
Figure 2: A phrasal cohesion violation.
and Cherry (2003) used the notion of phrasal cohe-
146
sion to constrain a beam search aligner, conduct-
ing a heuristic search of the dependency space.
The number of alignments in dependency space
depends largely on the provided dependency tree.
Because all permutations of a head and its modi-
fiers are possible, a tree that has a single head with
n ? 1 modifiers provides no guidance; the align-
ment space is the same as permutation space. If
the tree is a chain (where every head has exactly
one modifier), alignment space has only 2n per-
mutations, which is by far the smallest space we
have seen. In general, there are
?
? [(m? + 1)!]
permutations for a given tree, where ? stands for a
head node in the tree, andm? counts ??s modifiers.
Dependency space is not a subspace of ITG space,
as it can create both the forbidden alignments in
Figure 1 when given a single-headed tree.
3 Dependency constrained ITG
In this section, we introduce a new alignment
space defined by a dependency constrained ITG,
or D-ITG. The set of possible alignments in this
space is the intersection of the dependency space
for a given dependency tree and ITG space. Our
goal is an alignment search that respects the
phrases specified by the dependency tree, but at-
tempts all ITG orderings of those phrases, rather
than all permutations. The intuition is that most
ordering decisions involve only a small number
of phrases, so the search should still cover a large
portion of dependency space.
This new space has several attractive computa-
tional properties. Since it is a subspace of ITG
space, we will be able to search the space com-
pletely using a polynomial time ITG parser. This
places an upper bound on the search complexity
equal to ITG complexity. This upper bound is
very loose, as the ITG will often be drastically
constrained by the phrasal structure of the depen-
dency tree. Also, by working in the ITG frame-
work, we will be able to take advantage of ad-
vances in ITG parsing, and we will have access
to the forward-backward algorithm to implicitly
count events over all alignments.
3.1 A simple solution
Wu (1997) suggests that in order to have an ITG
take advantage of a known partial structure, one
can simply stop the parser from using any spans
that would violate the structure. In a chart parsing
framework, this can be accomplished by assigning
the invalid spans a value of ?? before parsing
begins. Our English dependency tree qualifies as a
partial structure, as it does not specify a complete
binary decomposition of the English sentence. In
this case, any ITG span that would contain part,
but not all, of two adjacent dependency phrases
can be invalidated. The sentence pair can then be
parsed normally, automatically respecting phrases
specified by the dependency tree.
For example, Figure 3a shows an alignment for
the sentence pair, ?His house in Canada, Sa mai-
son au Canada? and the dependency tree provided
for the English sentence. The spans disallowed by
the tree are shown using underlines. Note that the
illegal spans are those that would break up the ?in
Canada? subtree. After invalidating these spans in
the chart, parsing the sentence pair with the brack-
eting ITG in (1) will produce the two structures
shown in Figure 3b, both of which correspond to
the correct alignment.
This solution is sufficient to create a D-ITG that
obeys the phrase structure specified by a depen-
dency tree. This allows us to conduct a complete
search of a well-defined subspace of the depen-
dency space described in Section 2.3.
3.2 Avoiding redundant derivations with a
recursive ITG
The above solution can derive two structures for
the same alignment. It is often desirable to
eliminate redundant structures when working with
ITGs. Having a single, canonical tree structure for
each possible alignment can help when flattening
binary trees, as it indicates arbitrary binarization
decisions (Wu, 1997). Canonical structures also
eliminate double counting when performing tasks
like EM (Zhang and Gildea, 2004). The nature of
null link handling in ITGs makes eliminating all
redundancies difficult, but we can at least elimi-
nate them in the absence of nulls.
Normally, one would eliminate the redundant
structures produced by the grammar in (1) by re-
placing it with the canonical form grammar (Wu,
1997), which has the following form:
S ? A | B | C
A ? [AB] | [BB] | [CB] |
[AC] | [BC] | [CC]
B ? ?AA? | ?BA? | ?CA? |
?AC? | ?BC? | ?CC?
C ? e/f
(2)
By design, this grammar allows only one struc-
147
                	 
  
            	 
   
                	                  	 
                	 


Figure 3: An example of how dependency trees interact with ITGs. (a) shows the input, dependency
tree, and alignment. Invalidated spans are underlined. (b) shows valid binary structures. (c) shows the
canonical ITG structure for this alignment.
         
       
  Word Alignment with Cohesion Constraint
Dekang Lin and Colin Cherry
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{lindek,colinc}@cs.ualberta.ca
Abstract
We present a syntax-based constraint for word
alignment, known as the cohesion constraint. It
requires disjoint English phrases to be mapped
to non-overlapping intervals in the French sen-
tence. We evaluate the utility of this constraint
in two different algorithms. The results show
that it can provide a significant improvement in
alignment quality.
1 Introduction
The IBM statistical machine translation (SMT) models
have been extremely influential in computational linguis-
tics in the past decade. The (arguably) most striking char-
acteristic of the IBM-style SMT models is their total lack
of linguistic knowledge. The IBM models demonstrated
how much one can do with pure statistical techniques,
which have inspired a whole new generation of NLP re-
search and systems.
More recently, there have been many proposals to
introduce syntactic knowledge into SMT models (Wu,
1997; Alshawi et al, 2000; Yamada and Knight, 2001;
Lopez et al, 2002). A common theme among these
approaches is the assumption that the syntactic struc-
tures of a pair of source-target sentences are isomor-
phic (or nearly isomorphic). This assumption seems too
strong. Human translators often use non-literal transla-
tions, which result in differences in syntactic structures.
According to a study in (Dorr et al, 2002), such transla-
tional divergences are quite common, involving 11-31%
of the sentences.
We introduce a constraint that uses the dependency tree
of the English sentence to maintain phrasal cohesion in
the French sentence. In other words, if two phrases are
disjoint in the English sentence, the alignment must not
map them to overlapping intervals in the French sentence.
For example, in Figure 1, the cohesion constraint will rule
out the possibility of aligning to with a`. The phrases the
reboot and the host to discover all the devices are dis-
joint, but the partial alignment in Figure 1 maps them to
overlapping intervals. This constraint is weaker than iso-
morphism. However, we will show that it can produce a
significant increase in alignment quality.
The reboot causes the host to discover all the devices
det subj det subj aux
pre
det
objmod
  ?    laSuite r?initialisation  ,  l'  h?te rep?re tous les p?riph?riques
after to the reboot the host locate all the peripherals
1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10 11
Figure 1: A cohesion constraint violation
2 Cohesion Constraint
Given an English sentence E = e1e2 . . . el and a French
sentence F = f1f2 . . . fm, an alignment is a set of links
between the words in E and F . An alignment can be
represented as a binary relation A in [1, l] ? [1,m]. A
pair (i, j) is in A if ei and fj are a translation (or part
of a translation) of each other. We call such pairs links.
In Figure 2, the links in the alignment are represented by
dashed lines.
The reboot causes the host to discover all the devices
det subj det subjaux
pre
det
objcomp
  ?    laSuite r?initialisation  ,  l'  h?te rep?re tous les p?riph?riques
1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10
11
after to the reboot the host locate all the peripherals
Figure 2: An example pair of aligned sentence
The cohesion constraint (Fox, 2002) uses the depen-
dency tree TE (Mel?c?uk, 1987) of the English sentence
to restrict possible link combinations. Let TE(ei) be
the subtree of TE rooted at ei. The phrase span of ei,
spanP (ei, TE , A), is the image of the English phrase
headed by ei in F given a (partial) alignment A. More
precisely, spanP (ei, TE , A) = [k1, k2], where
k1 = min{j|(u, j) ? A, eu ? TE(ei)}
k2 = max{j|(u, j) ? A, eu ? TE(ei)}
The head span is the image of ei itself. We define
spanH(ei, TE , A) = [k1, k2], where
k1 = min{j|(i, j) ? A}
k2 = max{j|(i, j) ? A}
In Figure 2, the phrase span of the node discover is
[6, 11] and the head span is [8, 8]; the phrase span of the
node reboot is [3, 4] and the head span is [4, 4]. The word
cause has a phrase span of [3,11] and its head span is the
empty set ?.
With these definitions of phrase and head spans, we de-
fine two notions of overlap, originally introduced in (Fox,
2002) as crossings. Given a head node eh and its modi-
fier em, a head-modifier overlap occurs when:
spanH(eh, TE , A) ? spanP (em, TE , A) 6= ?
Given two nodes em1 and em2 which both modify the
same head node, a modifier-modifier overlap occurs
when:
spanP (em1 , TE , A) ? spanP (em2 , TE , A) 6= ?
Following (Fox, 2002), we say an alignment is cohe-
sive with respect to TE if it does not introduce any head-
modifier or modifier-modifier overlaps. For example, the
alignment A in Figure 1 is not cohesive because there
is an overlap between spanP (reboot, TE , A)=[4, 4] and
spanP (discover, TE , A)=[2, 11].
If an alignmentA? violates the cohesion constraint, any
alignment A that is a superset of A? will also violate the
cohesion constraint. This is because any pair of nodes
that have overlapping spans in A? will still have overlap-
ping spans in A.
Cohesion Checking Algorithm:
We now present an algorithm that checks whether an
individual link (ei, fj) causes a cohesion constraint vi-
olation when it is added to a partial alignment. Let
ep0 , ep1 , ep2 , . . . be a sequence of nodes in TE such that
ep0=ei and epk=parentOf (epk?1) (k = 1, 2, . . .)
1. For all k ? 0, update the spanP and the spanH of
epk to include j.
2. For each epk (k > 0), check for a modifier-modifier
overlap between the updated the phrase span of
epk?1 and the the phrase span of each of the other
children of epk .
3. For each epk (k > 0), check for a head-modifier
overlap between the updated phrase span of epk?1
and the head span of epk .
4. If an overlap is found, return true (the constraint is
violated). Otherwise, return false.
3 Evaluation
To determine the utility of the cohesion constraint, we
incorporated it into two alignment algorithms. The algo-
rithms take as input an English-French sentence pair and
the dependency tree of the English sentence. Both algo-
rithms build an alignment by adding one link at a time.
We implement two versions of each algorithm: one with
the cohesion constraint and one without. We will describe
the versions without cohesion constraint below. For the
versions with cohesion constraint, it is understood that
each new link must also pass the test described in Sec-
tion 2.
The first algorithm is similar to Competitive Linking
(Melamed, 1997). We use a sentence-aligned corpus
to compute the ?2 correlation metric (Gale and Church,
1991) between all English-French word pairs. For a given
sentence pair, we begin with an empty alignment. We
then add links in the order of their ?2 scores so that each
word participates in at most one link. We will refer to this
as the ?2 method.
The second algorithm uses a best-first search (with
fixed beam width and agenda size) to find an alignment
that maximizes P (A|E,F ). A state in this search space
is a partial alignment. A transition is defined as the ad-
dition of a single link to the current state. The algorithm
computes P (A|E,F ) based on statistics obtained from a
word-aligned corpus. We construct the initial corpus with
a system that is similar to the ?2 method. The algorithm
then re-aligns the corpus and trains again for three iter-
ations. We will refer to this as the P (A|E,F ) method.
The details of this algorithm are described in (Cherry and
Lin, 2003).
We trained our alignment programs with the same 50K
pairs of sentences as (Och and Ney, 2000) and tested it on
the same 500 manually aligned sentences. Both the train-
ing and testing sentences are from the Hansard corpus.
We parsed the training and testing corpora with Minipar.1
We adopted the evaluation methodology in (Och and Ney,
2000), which defines three metrics: precision, recall and
alignment error rate (AER).
Table 1 shows the results of our experiments. The first
four rows correspond to the methods described above. As
a reference point, we also provide the results reported in
(Och and Ney, 2000). They implemented IBM Model 4
by bootstrapping from an HMM model. The rows F?E
1available at http://www.cs.ualberta.ca/? lindek/minipar.htm
Table 1: Evaluation Results
Method Prec Rec AER
?2 w/o cohesion 82.7 84.6 16.5
w/ cohesion 89.2 82.7 13.8
P (A|E,F ) w/o cohesion 87.3 85.3 13.6
w/ cohesion 95.7 86.4 8.7
F?E 80.5 91.2 15.6
Och&Ney E?F 80.0 90.8 16.0
Refined 85.9 92.3 11.7
and E?F are the results obtained by this model when
treating French as the source and English as the target
or vice versa. The row Refined shows results obtained
by taking the intersection of E?F and F?E and then
refining this intersection to increase recall.
From Table 1, we can see that the addition of the cohe-
sion constraint leads to significant improvements in per-
formance with both algorithms. The relative reduction in
error rate is 16% with the ?2 method and 36% with the
P (A|E,F ) method. The improvement comes primarily
from increased precision. With the P (A|E,F ) method,
this increase in precision does not come at the expense of
recall.
4 Related Work
There has been a growing trend in the SMT community
to attempt to leverage syntactic data in word alignment.
Methods such as (Wu, 1997), (Alshawi et al, 2000) and
(Lopez et al, 2002) employ a synchronous parsing proce-
dure to constrain a statistical alignment. The work done
in (Yamada and Knight, 2001) measures statistics on op-
erations that transform a parse tree from one language
into another.
The syntactic knowledge that is leveraged in these
methods is tightly coupled with the alignment method it-
self. We have presented a modular constraint that can be
plugged into different alignment algorithms. This has al-
lowed us to test the contribution of the constraint directly.
(Fox, 2002) studied the extent to which the cohesion
constraint holds in a parallel corpus and the reasons for
the violations, but did not apply the constraint to an align-
ment algorithm.
5 Conclusion
We have presented a syntax-based constraint for word
alignment, known as the cohesion constraint. It requires
disjoint English phrases to be mapped to non-overlapping
intervals in the French sentence. Our experiments have
shown that the use of this constraint can provide a rela-
tive reduction in alignment error rate of 36%.
Acknowledgments
We wish to thank Franz Och for providing us with manu-
ally aligned evaluation data. This project is funded by and
jointly undertaken with Sun Microsystems, Inc. We wish
to thank Finola Brady, Bob Kuhns and Michael McHugh
for their help.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. Submitted.
Bonnie J. Dorr, Lisa Pearl, Rebecca Hwa, and Nizar
Habash. 2002. Duster: A method for unraveling
cross-language divergences for statistical word-level
alignment. In Stephen D. Richardson, editor, Proceed-
ings of AMTA-02, pages 31?43, Tiburon, CA, October.
Springer.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP-02, pages
304?311.
W.A. Gale and K.W. Church. 1991. Identifying word
correspondences in parallel texts. In Proceedings of
the 4th Speech and Natural Language Workshop, pages
152?157. DARPA, Morgan Kaufmann.
Adam Lopez, Michael Nossal, Rebecca Hwa, and Philip
Resnik. 2002. Word-level alignment for multilingual
resource acquisition. In Proceedings of the Workshop
on Linguistic Knowledge Acquisition and Representa-
tion: Bootstrapping Annotated Language Data.
I. Dan Melamed. 1997. A word-to-word model of trans-
lational equivalence. In Proceedings of the ACL-97,
pages 490?497. Association for Computational Lin-
guistics.
Igor A. Mel?c?uk. 1987. Dependency syntax: theory and
practice. State University of New York Press, Albany.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447, Hong Kong, China, Octo-
ber.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):374?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Meeting of the Associ-
ation for Computational Linguistics, pages 523?530.
A Probability Model to Improve Word Alignment
Colin Cherry and Dekang Lin
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{colinc,lindek}@cs.ualberta.ca
Abstract
Word alignment plays a crucial role in sta-
tistical machine translation. Word-aligned
corpora have been found to be an excellent
source of translation-related knowledge.
We present a statistical model for comput-
ing the probability of an alignment given a
sentence pair. This model allows easy in-
tegration of context-specific features. Our
experiments show that this model can be
an effective tool for improving an existing
word alignment.
1 Introduction
Word alignments were first introduced as an in-
termediate result of statistical machine translation
systems (Brown et al, 1993). Since their intro-
duction, many researchers have become interested
in word alignments as a knowledge source. For
example, alignments can be used to learn transla-
tion lexicons (Melamed, 1996), transfer rules (Car-
bonell et al, 2002; Menezes and Richardson, 2001),
and classifiers to find safe sentence segmentation
points (Berger et al, 1996).
In addition to the IBM models, researchers have
proposed a number of alternative alignment meth-
ods. These methods often involve using a statistic
such as ?2 (Gale and Church, 1991) or the log likeli-
hood ratio (Dunning, 1993) to create a score to mea-
sure the strength of correlation between source and
target words. Such measures can then be used to
guide a constrained search to produce word align-
ments (Melamed, 2000).
It has been shown that once a baseline alignment
has been created, one can improve results by using
a refined scoring metric that is based on the align-
ment. For example Melamed uses competitive link-
ing along with an explicit noise model in (Melamed,
2000) to produce a new scoring metric, which in turn
creates better alignments.
In this paper, we present a simple, flexible, sta-
tistical model that is designed to capture the infor-
mation present in a baseline alignment. This model
allows us to compute the probability of an align-
ment for a given sentence pair. It also allows for
the easy incorporation of context-specific knowl-
edge into alignment probabilities.
A critical reader may pose the question, ?Why in-
vent a new statistical model for this purpose, when
existing, proven models are available to train on a
given word alignment?? We will demonstrate exper-
imentally that, for the purposes of refinement, our
model achieves better results than a comparable ex-
isting alternative.
We will first present this model in its most general
form. Next, we describe an alignment algorithm that
integrates this model with linguistic constraints in
order to produce high quality word alignments. We
will follow with our experimental results and dis-
cussion. We will close with a look at how our work
relates to other similar systems and a discussion of
possible future directions.
2 Probability Model
In this section we describe our probability model.
To do so, we will first introduce some necessary no-
tation. Let E be an English sentence e1, e2, . . . , em
and let F be a French sentence f1, f2, . . . , fn. We
define a link l(ei, fj) to exist if ei and fj are a trans-
lation (or part of a translation) of one another. We
define the null link l(ei, f0) to exist if ei does not
correspond to a translation for any French word in
F . The null link l(e0, fj) is defined similarly. An
alignment A for two sentences E and F is a set of
links such that every word in E and F participates in
at least one link, and a word linked to e0 or f0 partic-
ipates in no other links. If e occurs in E x times and
f occurs in F y times, we say that e and f co-occur
xy times in this sentence pair.
We define the alignment problem as finding the
alignment A that maximizes P (A|E,F ). This cor-
responds to finding the Viterbi alignment in the
IBM translation systems. Those systems model
P (F,A|E), which when maximized is equivalent to
maximizing P (A|E,F ). We propose here a system
which models P (A|E,F ) directly, using a different
decomposition of terms.
In the IBM models of translation, alignments exist
as artifacts of which English words generated which
French words. Our model does not state that one
sentence generates the other. Instead it takes both
sentences as given, and uses the sentences to deter-
mine an alignment. An alignment A consists of t
links {l1, l2, . . . , lt}, where each lk = l(eik , fjk) for
some ik and jk. We will refer to consecutive subsets
of A as lji = {li, li+1, . . . , lj}. Given this notation,
P (A|E,F ) can be decomposed as follows:
P (A|E,F ) = P (lt1|E,F ) =
t?
k=1
P (lk|E,F, l
k?1
1 )
At this point, we must factor P (lk|E,F, lk?11 ) to
make computation feasible. Let Ck = {E,F, lk?11 }
represent the context of lk. Note that both the con-
text Ck and the link lk imply the occurrence of eik
and fjk . We can rewrite P (lk|Ck) as:
P (lk|Ck) =
P (lk, Ck)
P (Ck)
=
P (Ck|lk)P (lk)
P (Ck, eik , fjk)
=
P (Ck|lk)
P (Ck|eik , fjk)
?
P (lk, eik , fjk)
P (eik , fjk)
= P (lk|eik , fjk)?
P (Ck|lk)
P (Ck|eik , fjk)
Here P (lk|eik , fjk) is link probability given a co-
occurrence of the two words, which is similar in
spirit to Melamed?s explicit noise model (Melamed,
2000). This term depends only on the words in-
volved directly in the link. The ratio P (Ck|lk)P (Ck|eik ,fjk )
modifies the link probability, providing context-
sensitive information.
Up until this point, we have made no simplify-
ing assumptions in our derivation. Unfortunately,
Ck = {E,F, l
k?1
1 } is too complex to estimate con-
text probabilities directly. Suppose FTk is a set
of context-related features such that P (lk|Ck) can
be approximated by P (lk|eik , fjk , FTk). Let C ?k =
{eik , fjk}?FTk. P (lk|C
?
k) can then be decomposed
using the same derivation as above.
P (lk|C
?
k) = P (lk|eik , fjk)?
P (C ?k|lk)
P (C ?k|eik , fjk)
= P (lk|eik , fjk)?
P (FTk|lk)
P (FTk|eik , fjk)
In the second line of this derivation, we can drop
eik and fjk from C ?k, leaving only FTk, because they
are implied by the events which the probabilities are
conditionalized on. Now, we are left with the task
of approximating P (FTk|lk) and P (FTk|eik , fjk).
To do so, we will assume that for all ft ? FTk,
ft is conditionally independent given either lk or
(eik , fjk). This allows us to approximate alignment
probability P (A|E,F ) as follows:
t?
k=1
?
?P (lk|eik , fjk)?
?
ft?FTk
P (ft|lk)
P (ft|eik , fjk)
?
?
In any context, only a few features will be ac-
tive. The inner product is understood to be only over
those features ft that are present in the current con-
text. This approximation will cause P (A|E,F ) to
no longer be a well-behaved probability distribution,
though as in Naive Bayes, it can be an excellent es-
timator for the purpose of ranking alignments.
If we have an aligned training corpus, the prob-
abilities needed for the above equation are quite
easy to obtain. Link probabilities can be deter-
mined directly from |lk| (link counts) and |eik , fj,k|
(co-occurrence counts). For any co-occurring pair
of words (eik , fjk), we check whether it has the
feature ft. If it does, we increment the count of
|ft, eik , fjk |. If this pair is also linked, then we in-
crement the count of |ft, lk|. Note that our definition
of FTk allows for features that depend on previous
links. For this reason, when determining whether or
not a feature is present in a given context, one must
impose an ordering on the links. This ordering can
be arbitrary as long as the same ordering is used in
training1 and probability evaluation. A simple solu-
tion would be to order links according their French
words. We choose to order links according to the
link probability P (lk|eik , fjk) as it has an intuitive
appeal of allowing more certain links to provide con-
text for others.
We store probabilities in two tables. The first ta-
ble stores link probabilities P (lk|eik , fjk). It has an
entry for every word pair that was linked at least
once in the training corpus. Its size is the same as
the translation table in the IBM models. The sec-
ond table stores feature probabilities, P (ft|lk) and
P (ft|eik , fjk). For every linked word pair, this table
has two entries for each active feature. In the worst
case this table will be of size 2?|FT |?|E|?|F |. In
practice, it is much smaller as most contexts activate
only a small number of features.
In the next subsection we will walk through a sim-
ple example of this probability model in action. We
will describe the features used in our implementa-
tion of this model in Section 3.2.
2.1 An Illustrative Example
Figure 1 shows an aligned corpus consisting of
one sentence pair. Suppose that we are concerned
with only one feature ft that is active2 for eik
and fjk if an adjacent pair is an alignment, i.e.,
l(eik?1, fjk?1) ? l
k?1
1 or l(eik+1, fjk+1) ? l
k?1
1 .
This example would produce the probability tables
shown in Table 1.
Note how ft is active for the (a, v) link, and is
not active for the (b, u) link. This is due to our se-
lected ordering. Table 1 allows us to calculate the
probability of this alignment as:
1In our experiments, the ordering is not necessary during
training to achieve good performance.
2Throughout this paper we will assume that null alignments
are special cases, and do not activate or participate in features
unless otherwise stated in the feature description.
a b a
u v v
e
f0
0
Figure 1: An Example Aligned Corpus
Table 1: Example Probability Tables
(a) Link Counts and Probabilities
eik fjk |lk| |eik , fjk | P (lk|eik , fjk)
b u 1 1 1
a f0 1 2 12
e0 v 1 2 12
a v 1 4 14
(b) Feature Counts
eik fjk |ft, lk| |ft, eik , fjk |
a v 1 1
(c) Feature Probabilities
eik fjk P (ft|lk) P (ft|eik , fjk)
a v 1 14
P (A|E,F ) = P (l(b, u)|b, u)?
P (l(a, f0)|a, f0)?
P (l(e0, v)|e0, v)?
P (l(a, v)|a, v)P (ft|l(a,v))P (ft|a,v)
= 1? 12 ?
1
2 ?
1
4 ?
1
1
4
= 14
3 Word-Alignment Algorithm
In this section, we describe a world-alignment al-
gorithm guided by the alignment probability model
derived above. In designing this algorithm we have
selected constraints, features and a search method
in order to achieve high performance. The model,
however, is general, and could be used with any in-
stantiation of the above three factors. This section
will describe and motivate the selection of our con-
straints, features and search method.
The input to our word-alignment algorithm con-
sists of a pair of sentences E and F , and the depen-
dency tree TE for E. TE allows us to make use of
features and constraints that are based on linguistic
intuitions.
3.1 Constraints
The reader will note that our alignment model as de-
scribed above has very few factors to prevent unde-
sirable alignments, such as having all French words
align to the same English word. To guide the model
to correct alignments, we employ two constraints to
limit our search for the most probable alignment.
The first constraint is the one-to-one constraint
(Melamed, 2000): every word (except the null words
e0 and f0) participates in exactly one link.
The second constraint, known as the cohesion
constraint (Fox, 2002), uses the dependency tree
(Mel?c?uk, 1987) of the English sentence to restrict
possible link combinations. Given the dependency
tree TE , the alignment can induce a dependency tree
for F (Hwa et al, 2002). The cohesion constraint
requires that this induced dependency tree does not
have any crossing dependencies. The details about
how the cohesion constraint is implemented are out-
side the scope of this paper.3 Here we will use a sim-
ple example to illustrate the effect of the constraint.
Consider the partial alignment in Figure 2. When
the system attempts to link of and de, the new link
will induce the dotted dependency, which crosses a
previously induced dependency between service and
donne?es. Therefore, of and de will not be linked.
 the status of the data service
l' ?tat du service de donn?es
nn
det
pcomp
moddet
Figure 2: An Example of Cohesion Constraint
3.2 Features
In this section we introduce two types of features
that we use in our implementation of the probabil-
ity model described in Section 2. The first feature
3The algorithm for checking the cohesion constraint is pre-
sented in a separate paper which is currently under review.
the host discovers all the devices
det
subj pre
det
obj
 l'  h?te rep?re tous les p?riph?riques
1 2 3 4 5
1 2 3 4 5 6
6
the host locate all the peripherals
Figure 3: Feature Extraction Example
type fta concerns surrounding links. It has been ob-
served that words close to each other in the source
language tend to remain close to each other in the
translation (Vogel et al, 1996; Ker and Change,
1997). To capture this notion, for any word pair
(ei, fj), if a link l(ei? , fj?) exists where i? 2 ? i? ?
i + 2 and j ? 2 ? j? ? j + 2, then we say that the
feature fta(i?i?, j?j?, ei?) is active for this context.
We refer to these as adjacency features.
The second feature type ftd uses the English
parse tree to capture regularities among grammati-
cal relations between languages. For example, when
dealing with French and English, the location of
the determiner with respect to its governor4 is never
swapped during translation, while the location of ad-
jectives is swapped frequently. For any word pair
(ei, fj), let ei? be the governor of ei, and let rel be
the relationship between them. If a link l(ei? , fj?)
exists, then we say that the feature ftd(j?j?, rel) is
active for this context. We refer to these as depen-
dency features.
Take for example Figure 3 which shows a par-
tial alignment with all links completed except for
those involving ?the?. Given this sentence pair and
English parse tree, we can extract features of both
types to assist in the alignment of the1. The word
pair (the1, l?) will have an active adjacency feature
fta(+1,+1, host) as well as a dependency feature
ftd(?1, det). These two features will work together
to increase the probability of this correct link. In
contrast, the incorrect link (the1, les) will have only
ftd(+3, det), which will work to lower the link
probability, since most determiners are located be-
4The parent node in the dependency tree.
fore their governors.
3.3 Search
Due to our use of constraints, when seeking the
highest probability alignment, we cannot rely on a
method such as dynamic programming to (implic-
itly) search the entire alignment space. Instead, we
use a best-first search algorithm (with constant beam
and agenda size) to search our constrained space of
possible alignments. A state in this space is a par-
tial alignment. A transition is defined as the addi-
tion of a single link to the current state. Any link
which would create a state that does not violate any
constraint is considered to be a valid transition. Our
start state is the empty alignment, where all words in
E and F are linked to null. A terminal state is a state
in which no more links can be added without violat-
ing a constraint. Our goal is to find the terminal state
with highest probability.
For the purposes of our best-first search, non-
terminal states are evaluated according to a greedy
completion of the partial alignment. We build this
completion by adding valid links in the order of
their unmodified link probabilities P (l|e, f) until no
more links can be added. The score the state receives
is the probability of its greedy completion. These
completions are saved for later use (see Section 4.2).
4 Training
As was stated in Section 2, our probability model
needs an initial alignment in order to create its prob-
ability tables. Furthermore, to avoid having our
model learn mistakes and noise, it helps to train on a
set of possible alignments for each sentence, rather
than one Viterbi alignment. In the following sub-
sections we describe the creation of the initial align-
ments used for our experiments, as well as our sam-
pling method used in training.
4.1 Initial Alignment
We produce an initial alignment using the same al-
gorithm described in Section 3, except we maximize
summed ?2 link scores (Gale and Church, 1991),
rather than alignment probability. This produces a
reasonable one-to-one word alignment that we can
refine using our probability model.
4.2 Alignment Sampling
Our use of the one-to-one constraint and the cohe-
sion constraint precludes sampling directly from all
possible alignments. These constraints tie words in
such a way that the space of alignments cannot be
enumerated as in IBM models 1 and 2 (Brown et
al., 1993). Taking our lead from IBM models 3, 4
and 5, we will sample from the space of those high-
probability alignments that do not violate our con-
straints, and then redistribute our probability mass
among our sample.
At each search state in our alignment algorithm,
we consider a number of potential links, and select
between them using a heuristic completion of the re-
sulting state. Our sample S of possible alignments
will be the most probable alignment, plus the greedy
completions of the states visited during search. It
is important to note that any sampling method that
concentrates on complete, valid and high probabil-
ity alignments will accomplish the same task.
When collecting the statistics needed to calcu-
late P (A|E,F ) from our initial ?2 alignment, we
give each s ? S a uniform weight. This is rea-
sonable, as we have no probability estimates at this
point. When training from the alignments pro-
duced by our model, we normalize P (s|E,F ) so
that
?
s?S P (s|E,F ) = 1. We then count links and
features in S according to these normalized proba-
bilities.
5 Experimental Results
We adopted the same evaluation methodology as in
(Och and Ney, 2000), which compared alignment
outputs with manually aligned sentences. Och and
Ney classify manual alignments into two categories:
Sure (S) and Possible (P ) (S?P ). They defined the
following metrics to evaluate an alignment A:
recall = |A?S||S| precision =
|A?P |
|P |
alignment error rate (AER) = |A?S|+|A?P ||S|+|P |
We trained our alignment program with the same
50K pairs of sentences as (Och and Ney, 2000) and
tested it on the same 500 manually aligned sen-
tences. Both the training and testing sentences are
from the Hansard corpus. We parsed the training
Table 2: Comparison with (Och and Ney, 2000)
Method Prec Rec AER
Ours 95.7 86.4 8.7
IBM-4 F?E 80.5 91.2 15.6
IBM-4 E?F 80.0 90.8 16.0
IBM-4 Intersect 95.7 85.6 9.0
IBM-4 Refined 85.9 92.3 11.7
and testing corpora with Minipar.5 We then ran the
training procedure in Section 4 for three iterations.
We conducted three experiments using this
methodology. The goal of the first experiment is to
compare the algorithm in Section 3 to a state-of-the-
art alignment system. The second will determine
the contributions of the features. The third experi-
ment aims to keep all factors constant except for the
model, in an attempt to determine its performance
when compared to an obvious alternative.
5.1 Comparison to state-of-the-art
Table 2 compares the results of our algorithm with
the results in (Och and Ney, 2000), where an HMM
model is used to bootstrap IBM Model 4. The rows
IBM-4 F?E and IBM-4 E?F are the results ob-
tained by IBM Model 4 when treating French as the
source and English as the target or vice versa. The
row IBM-4 Intersect shows the results obtained by
taking the intersection of the alignments produced
by IBM-4 E?F and IBM-4 F?E. The row IBM-4
Refined shows results obtained by refining the inter-
section of alignments in order to increase recall.
Our algorithm achieved over 44% relative error
reduction when compared with IBM-4 used in ei-
ther direction and a 25% relative error rate reduc-
tion when compared with IBM-4 Refined. It also
achieved a slight relative error reduction when com-
pared with IBM-4 Intersect. This demonstrates that
we are competitive with the methods described in
(Och and Ney, 2000). In Table 2, one can see that
our algorithm is high precision, low recall. This was
expected as our algorithm uses the one-to-one con-
straint, which rules out many of the possible align-
ments present in the evaluation data.
5available at http://www.cs.ualberta.ca/? lindek/minipar.htm
Table 3: Evaluation of Features
Algorithm Prec Rec AER
initial (?2) 88.9 84.6 13.1
without features 93.7 84.8 10.5
with ftd only 95.6 85.4 9.3
with fta only 95.9 85.8 9.0
with fta and ftd 95.7 86.4 8.7
5.2 Contributions of Features
Table 3 shows the contributions of features to our al-
gorithm?s performance. The initial (?2) row is the
score for the algorithm (described in Section 4.1)
that generates our initial alignment. The without fea-
tures row shows the score after 3 iterations of refine-
ment with an empty feature set. Here we can see that
our model in its simplest form is capable of produc-
ing a significant improvement in alignment quality.
The rows with ftd only and with fta only describe
the scores after 3 iterations of training using only de-
pendency and adjacency features respectively. The
two features provide significant contributions, with
the adjacency feature being slightly more important.
The final row shows that both features can work to-
gether to create a greater improvement, despite the
independence assumptions made in Section 2.
5.3 Model Evaluation
Even though we have compared our algorithm to
alignments created using IBM statistical models, it
is not clear if our model is essential to our perfor-
mance. This experiment aims to determine if we
could have achieved similar results using the same
initial alignment and search algorithm with an alter-
native model.
Without using any features, our model is similar
to IBM?s Model 1, in that they both take into account
only the word types that participate in a given link.
IBM Model 1 uses P (f |e), the probability of f be-
ing generated by e, while our model uses P (l|e, f),
the probability of a link existing between e and f .
In this experiment, we set Model 1 translation prob-
abilities according to our initial ?2 alignment, sam-
pling as we described in Section 4.2. We then use the
?n
j=1 P (fj |eaj ) to evaluate candidate alignments in
a search that is otherwise identical to our algorithm.
We ran Model 1 refinement for three iterations and
Table 4: P (l|e, f) vs. P (f |e)
Algorithm Prec Rec AER
initial (?2) 88.9 84.6 13.1
P (l|e, f) model 93.7 84.8 10.5
P (f |e) model 89.2 83.0 13.7
recorded the best results that it achieved.
It is clear from Table 4 that refining our initial ?2
alignment using IBM?s Model 1 is less effective than
using our model in the same manner. In fact, the
Model 1 refinement receives a lower score than our
initial alignment.
6 Related Work
6.1 Probability models
When viewed with no features, our proba-
bility model is most similar to the explicit
noise model defined in (Melamed, 2000). In
fact, Melamed defines a probability distribution
P (links(u, v)|cooc(u, v), ?+, ??) which appears to
make our work redundant. However, this distribu-
tion refers to the probability that two word types u
and v are linked links(u, v) times in the entire cor-
pus. Our distribution P (l|e, f) refers to the proba-
bility of linking a specific co-occurrence of the word
tokens e and f . In Melamed?s work, these probabil-
ities are used to compute a score based on a prob-
ability ratio. In our work, we use the probabilities
directly.
By far the most prominent probability models in
machine translation are the IBM models and their
extensions. When trying to determine whether two
words are aligned, the IBM models ask, ?What is
the probability that this English word generated this
French word?? Our model asks instead, ?If we are
given this English word and this French word, what
is the probability that they are linked?? The dis-
tinction is subtle, yet important, introducing many
differences. For example, in our model, E and F
are symmetrical. Furthermore, we model P (l|e, f ?)
and P (l|e, f ??) as unrelated values, whereas the IBM
model would associate them in the translation prob-
abilities t(f ?|e) and t(f ??|e) through the constraint
?
f t(f |e) = 1. Unfortunately, by conditionalizing
on both words, we eliminate a large inductive bias.
This prevents us from starting with uniform proba-
bilities and estimating parameters with EM. This is
why we must supply the model with a noisy initial
alignment, while IBM can start from an unaligned
corpus.
In the IBM framework, when one needs the model
to take new information into account, one must cre-
ate an extended model which can base its parame-
ters on the previous model. In our model, new in-
formation can be incorporated modularly by adding
features. This makes our work similar to maximum
entropy-based machine translation methods, which
also employ modular features. Maximum entropy
can be used to improve IBM-style translation prob-
abilities by using features, such as improvements to
P (f |e) in (Berger et al, 1996). By the same token
we can use maximum entropy to improve our esti-
mates of P (lk|eik , fjk , Ck). We are currently inves-
tigating maximum entropy as an alternative to our
current feature model which assumes conditional in-
dependence among features.
6.2 Grammatical Constraints
There have been many recent proposals to leverage
syntactic data in word alignment. Methods such as
(Wu, 1997), (Alshawi et al, 2000) and (Lopez et al,
2002) employ a synchronous parsing procedure to
constrain a statistical alignment. The work done in
(Yamada and Knight, 2001) measures statistics on
operations that transform a parse tree from one lan-
guage into another.
7 Future Work
The alignment algorithm described here is incapable
of creating alignments that are not one-to-one. The
model we describe, however is not limited in the
same manner. The model is currently capable of
creating many-to-one alignments so long as the null
probabilities of the words added on the ?many? side
are less than the probabilities of the links that would
be created. Under the current implementation, the
training corpus is one-to-one, which gives our model
no opportunity to learn many-to-one alignments.
We are pursuing methods to create an extended
algorithm that can handle many-to-one alignments.
This would involve training from an initial align-
ment that allows for many-to-one links, such as one
of the IBM models. Features that are related to
multiple links should be added to our set of feature
types, to guide intelligent placement of such links.
8 Conclusion
We have presented a simple, flexible, statistical
model for computing the probability of an alignment
given a sentence pair. This model allows easy in-
tegration of context-specific features. Our experi-
ments show that this model can be an effective tool
for improving an existing word alignment.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
P. F. Brown, V. S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?312.
Jaime Carbonell, Katharina Probst, Erik Peterson, Chris-
tian Monson, Alon Lavie, Ralf Brown, and Lori Levin.
2002. Automatic rule learning for resource-limited mt.
In Proceedings of AMTA-02, pages 1?10.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74, March.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In Proceedings of EMNLP-02,
pages 304?311.
W.A. Gale and K.W. Church. 1991. Identifying word
correspondences in parallel texts. In Proceedings
of the 4th Speech and Natural Language Workshop,
pages 152?157. DARPA, Morgan Kaufmann.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating translational correspondence
using annotation projection. In Proceeding of ACL-02,
pages 392?399.
Sue J. Ker and Jason S. Change. 1997. Aligning more
words with high precision for small bilingual cor-
pora. Computational Linguistics and Chinese Lan-
guage Processing, 2(2):63?96, August.
Adam Lopez, Michael Nossal, Rebecca Hwa, and Philip
Resnik. 2002. Word-level alignment for multilingual
resource acquisition. In Proceedings of the Workshop
on Linguistic Knowledge Acquisition and Representa-
tion: Bootstrapping Annotated Language Data.
I. Dan Melamed. 1996. Automatic construction of clean
broad-coverage translation lexicons. In Proceedings
of the 2nd Conference of the Association for Machine
Translation in the Americas, pages 125?134, Mon-
treal.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249, June.
Igor A. Mel?c?uk. 1987. Dependency syntax: theory and
practice. State University of New York Press, Albany.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the Workshop on Data-Driven Machine Trans-
lation.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447, Hong Kong, China, Octo-
ber.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-based
word alignment in statistical translation. In Proceed-
ings of COLING-96, pages 836?841, Copenhagen,
Denmark, August.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):374?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Meeting of the Associ-
ation for Computational Linguistics, pages 523?530.
Proceedings of the 43rd Annual Meeting of the ACL, pages 271?279,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Dependency Treelet Translation: Syntactically Informed Phrasal SMT  
Chris Quirk, Arul Menezes Colin Cherry 
Microsoft Research University of Alberta 
One Microsoft Way Edmonton, Alberta 
Redmond, WA 98052 Canada T6G 2E1 
{chrisq,arulm}@microsoft.com colinc@cs.ualberta.ca 
 
Abstract 
We describe a novel approach to 
statistical machine translation that 
combines syntactic information in the 
source language with recent advances in 
phrasal translation. This method requires a 
source-language dependency parser, target 
language word segmentation and an 
unsupervised word alignment component. 
We align a parallel corpus, project the 
source dependency parse onto the target 
sentence, extract dependency treelet 
translation pairs, and train a tree-based 
ordering model. We describe an efficient 
decoder and show that using these tree-
based models in combination with 
conventional SMT models provides a 
promising approach that incorporates the 
power of phrasal SMT with the linguistic 
generality available in a parser.  
1. Introduction 
Over the past decade, we have witnessed a 
revolution in the field of machine translation 
(MT) toward statistical or corpus-based methods. 
Yet despite this success, statistical machine 
translation (SMT) has many hurdles to overcome. 
While it excels at translating domain-specific 
terminology and fixed phrases, grammatical 
generalizations are poorly captured and often 
mangled during translation (Thurmair, 04).  
1.1. Limitations of string-based phrasal SMT 
State-of-the-art phrasal SMT systems such as 
(Koehn et al, 03) and (Vogel et al, 03) model 
translations of phrases (here, strings of adjacent 
words, not syntactic constituents) rather than 
individual words. Arbitrary reordering of words is 
allowed within memorized phrases, but typically 
only a small amount of phrase reordering is 
allowed, modeled in terms of offset positions at 
the string level. This reordering model is very 
limited in terms of linguistic generalizations. For 
instance, when translating English to Japanese, an 
ideal system would automatically learn large-
scale typological differences: English SVO 
clauses generally become Japanese SOV clauses, 
English post-modifying prepositional phrases 
become Japanese pre-modifying postpositional 
phrases, etc. A phrasal SMT system may learn the 
internal reordering of specific common phrases, 
but it cannot generalize to unseen phrases that 
share the same linguistic structure. 
In addition, these systems are limited to 
phrases contiguous in both source and target, and 
thus cannot learn the generalization that English 
not may translate as French ne?pas except in the 
context of specific intervening words.  
1.2. Previous work on syntactic SMT1 
The hope in the SMT community has been that 
the incorporation of syntax would address these 
issues, but that promise has yet to be realized. 
One simple means of incorporating syntax into 
SMT is by re-ranking the n-best list of a baseline 
SMT system using various syntactic models, but 
Och et al (04) found very little positive impact 
with this approach. However, an n-best list of 
even 16,000 translations captures only a tiny 
fraction of the ordering possibilities of a 20 word 
sentence; re-ranking provides the syntactic model 
no opportunity to boost or prune large sections of 
that search space.  
Inversion Transduction Grammars (Wu, 97), or 
ITGs, treat translation as a process of parallel 
parsing of the source and target language via a 
synchronized grammar. To make this process 
                                                        
1
 Note that since this paper does not address the word alignment problem 
directly, we do not discuss the large body of work on incorporating syntactic 
information into the word alignment process. 
 
271
computationally efficient, however, some severe 
simplifying assumptions are made, such as using 
a single non-terminal label. This results in the 
model simply learning a very high level 
preference regarding how often nodes should 
switch order without any contextual information. 
Also these translation models are intrinsically 
word-based; phrasal combinations are not 
modeled directly, and results have not been 
competitive with the top phrasal SMT systems.  
Along similar lines, Alshawi et al (2000) treat 
translation as a process of simultaneous induction 
of source and target dependency trees using head-
transduction; again, no separate parser is used. 
Yamada and Knight (01) employ a parser in the 
target language to train probabilities on a set of 
operations that convert a target language tree to a 
source language string. This improves fluency 
slightly (Charniak et al, 03), but fails to 
significantly impact overall translation quality. 
This may be because the parser is applied to MT 
output, which is notoriously unlike native 
language, and no additional insight is gained via 
source language analysis.  
Lin (04) translates dependency trees using 
paths. This is the first attempt to incorporate large 
phrasal SMT-style memorized patterns together 
with a separate source dependency parser and 
SMT models. However the phrases are limited to 
linear paths in the tree, the only SMT model used 
is a maximum likelihood channel model and there 
is no ordering model. Reported BLEU scores are 
far below the leading phrasal SMT systems. 
MSR-MT (Menezes & Richardson, 01) parses 
both source and target languages to obtain a 
logical form (LF), and translates source LFs using 
memorized aligned LF patterns to produce a 
target LF. It utilizes a separate sentence 
realization component (Ringger et al, 04) to turn 
this into a target sentence. As such, it does not use 
a target language model during decoding, relying 
instead on MLE channel probabilities and 
heuristics such as pattern size. Recently Aue et al 
(04) incorporated an LF-based language model 
(LM) into the system for a small quality boost. A 
key disadvantage of this approach and related 
work (Ding & Palmer, 02) is that it requires a 
parser in both languages, which severely limits 
the language pairs that can be addressed. 
2. Dependency Treelet Translation 
In this paper we propose a novel dependency tree-
based approach to phrasal SMT which uses tree-
based ?phrases? and a tree-based ordering model 
in combination with conventional SMT models to 
produce state-of-the-art translations.  
Our system employs a source-language 
dependency parser, a target language word 
segmentation component, and an unsupervised 
word alignment component to learn treelet 
translations from a parallel sentence-aligned 
corpus. We begin by parsing the source text to 
obtain dependency trees and word-segmenting the 
target side, then applying an off-the-shelf word 
alignment component to the bitext.  
The word alignments are used to project the 
source dependency parses onto the target 
sentences. From this aligned parallel dependency 
corpus we extract a treelet translation model 
incorporating source and target treelet pairs, 
where a treelet is defined to be an arbitrary 
connected subgraph of the dependency tree. A 
unique feature is that we allow treelets with a 
wildcard root, effectively allowing mappings for 
siblings in the dependency tree. This allows us to 
model important phenomena, such as not ?   
ne?pas. We also train a variety of statistical 
models on this aligned dependency tree corpus, 
including a channel model and an order model.  
To translate an input sentence, we parse the 
sentence, producing a dependency tree for that 
sentence. We then employ a decoder to find a 
combination and ordering of treelet translation 
pairs that cover the source tree and are optimal 
according to a set of models that are combined in 
a log-linear framework as in (Och, 03).  
This approach offers the following advantages 
over string-based SMT systems: Instead of 
limiting learned phrases to contiguous word 
sequences, we allow translation by all possible 
phrases that form connected subgraphs (treelets) 
in the source and target dependency trees. This is 
a powerful extension: the vast majority of 
surface-contiguous phrases are also treelets of the 
tree; in addition, we gain discontiguous phrases, 
including combinations such as verb-object, 
article-noun, adjective-noun etc. regardless of the 
number of intervening words. 
272
Another major advantage is the ability to 
employ more powerful models for reordering 
source language constituents. These models can 
incorporate information from the source analysis. 
For example, we may model directly the 
probability that the translation of an object of a 
preposition in English should precede the 
corresponding postposition in Japanese, or the 
probability that a pre-modifying adjective in 
English translates into a post-modifier in French. 
2.1. Parsing and alignment 
We require a source language dependency parser 
that produces unlabeled, ordered dependency 
trees and annotates each source word with a part-
of-speech (POS). An example dependency tree is 
shown in Figure 1. The arrows indicate the head 
annotation, and the POS for each candidate is 
listed underneath. For the target language we only 
require word segmentation.  
To obtain word alignments we currently use 
GIZA++ (Och & Ney, 03). We follow the 
common practice of deriving many-to-many 
alignments by running the IBM models in both 
directions and combining the results heuristically. 
Our heuristics differ in that they constrain many-
to-one alignments to be contiguous in the source 
dependency tree. A detailed description of these 
heuristics can be found in Quirk et al (04).  
2.2. Projecting dependency trees 
Given a word aligned sentence pair and a source 
dependency tree, we use the alignment to project 
the source structure onto the target sentence. One-
to-one alignments project directly to create a 
target tree isomorphic to the source. Many-to-one 
alignments project similarly; since the ?many? 
source nodes are connected in the tree, they act as 
if condensed into a single node. In the case of 
one-to-many alignments we project the source 
node to the rightmost2 of the ?many? target words, 
and make the rest of the target words dependent 
on it. 
                                                        
2
 If the target language is Japanese, leftmost may be more appropriate. 
Unaligned target words3 are attached into the 
dependency structure as follows: assume there is 
an unaligned word tj in position j. Let i < j and k 
> j be the target positions closest to j such that ti 
depends on tk or vice versa: attach tj to the lower 
of ti or tk. If all the nodes to the left (or right) of 
position j are unaligned, attach tj to the left-most 
(or right-most) word that is aligned. 
The target dependency tree created in this 
process may not read off in the same order as the 
target string, since our alignments do not enforce 
phrasal cohesion. For instance, consider the 
projection of the parse in Figure 1 using the word 
alignment in Figure 2a. Our algorithm produces 
the dependency tree in Figure 2b. If we read off 
the leaves in a left-to-right in-order traversal, we 
do not get the original input string: de d?marrage 
appears in the wrong place. 
A second reattachment pass corrects this 
situation. For each node in the wrong order, we 
reattach it to the lowest of its ancestors such that 
it is in the correct place relative to its siblings and 
parent. In Figure 2c, reattaching d?marrage to et 
suffices to produce the correct order.  
                                                        
3
 Source unaligned nodes do not present a problem, with the exception that if 
the root is unaligned, the projection process produces a forest of target trees 
anchored by a dummy root.  
startup properties and options
Noun Noun Conj Noun
 
Figure 1. An example dependency tree. 
startup properties and options
propri?t?s et options de d?marrage
 
(a) Word alignment. 
 
 
startup properties and options
propri?t?s de d?marrage et options
 
 
 (b) Dependencies after initial projection. 
 
 
startup properties and options
propri?t?s et options de d?marrage
 
(c) Dependencies after reattachment step. 
 
Figure 2. Projection of dependencies. 
273
2.3. Extracting treelet translation pairs 
From the aligned pairs of dependency trees we 
extract all pairs of aligned source and target 
treelets along with word-level alignment linkages, 
up to a configurable maximum size. We also keep 
treelet counts for maximum likelihood estimation.  
2.4. Order model 
Phrasal SMT systems often use a model to score 
the ordering of a set of phrases. One approach is 
to penalize any deviation from monotone 
decoding; another is to estimate the probability 
that a source phrase in position i translates to a 
target phrase in position j (Koehn et al, 03). 
We attempt to improve on these approaches by 
incorporating syntactic information. Our model 
assigns a probability to the order of a target tree 
given a source tree. Under the assumption that 
constituents generally move as a whole, we 
predict the probability of each given ordering of 
modifiers independently. That is, we make the 
following simplifying assumption (where c is a 
function returning the set of nodes modifying t): 
?
?
=
Tt
TStcorderTSTorder ),|))((P(),|)(P(
 
Furthermore, we assume that the position of each 
child can be modeled independently in terms of a 
head-relative position: 
),|),(P(),|))((P(
)(
TStmposTStcorder
tcm
?
?
=  
Figure 3a demonstrates an aligned dependency 
tree pair annotated with head-relative positions; 
Figure 3b presents the same information in an 
alternate tree-like representation. 
We currently use a small set of features 
reflecting very local information in the 
dependency tree to model P(pos(m,t) | S, T): 
? The lexical items of the head and modifier. 
? The lexical items of the source nodes aligned 
to the head and modifier. 
? The part-of-speech ("cat") of the source nodes 
aligned to the head and modifier. 
? The head-relative position of the source node 
aligned to the source modifier. 4 
As an example, consider the children of 
propri?t? in Figure 3. The head-relative positions 
                                                        
4
 One can also include features of siblings to produce a Markov ordering 
model. However, we found that this had little impact in practice. 
of its modifiers la and Cancel are -1 and +1, 
respectively. Thus we try to predict as follows: 
P(pos(m1) = -1 | 
lex(m1)="la", lex(h)="propri?t?", 
lex(src(m1))="the", lex(src(h)="property", 
cat(src(m1))=Determiner, cat(src(h))=Noun, 
position(src(m1))=-2) ? 
P(pos(m2) = +1 | 
lex(m2)="Cancel", lex(h)="propri?t?", 
lex(src(m2))="Cancel", lex(src(h))="property", 
cat(src(m2))=Noun, cat(src(h))=Noun, 
position(src(m2))=-1) 
The training corpus acts as a supervised training 
set: we extract a training feature vector from each 
of the target language nodes in the aligned 
dependency tree pairs. Together these feature 
vectors are used to train a decision tree 
(Chickering, 02). The distribution at each leaf of 
the DT can be used to assign a probability to each 
possible target language position. A more detailed 
description is available in (Quirk et al, 04). 
2.5. Other models 
Channel Models: We incorporate two distinct 
channel models, a maximum likelihood estimate 
(MLE) model and a model computed using 
Model-1 word-to-word alignment probabilities as 
in (Vogel et al, 03). The MLE model effectively 
captures non-literal phrasal translations such as 
idioms, but suffers from data sparsity. The word-
the-2 Cancel-1 property-1 uses these-1 settings+1
la-1 propri?t?-1 Cancel+1 utilise ces-1 param?tres+1
 
(a) Head annotation representation 
 
uses
property-1              settings+1
the-2 Cancel-1                 these-1
la-1             Cancel+1         ces-1
propri?t?-1                        param?tres+1
utilise
 
(b) Branching structure representation. 
 
Figure 3.  Aligned dependency tree pair, annotated with 
head-relative positions 
274
to-word model does not typically suffer from data 
sparsity, but prefers more literal translations.  
Given a set of treelet translation pairs that 
cover a given input dependency tree and produce 
a target dependency tree, we model the 
probability of source given target as the product 
of the individual treelet translation probabilities: 
we assume a uniform probability distribution over 
the decompositions of a tree into treelets.  
Target Model: Given an ordered target language 
dependency tree, it is trivial to read off the surface 
string. We evaluate this string using a trigram 
model with modified Kneser-Ney smoothing.  
Miscellaneous Feature Functions: The log-linear 
framework allows us to incorporate other feature 
functions as ?models? in the translation process. 
For instance, using fewer, larger treelet translation 
pairs often provides better translations, since they 
capture more context and allow fewer possibilities 
for search and model error. Therefore we add a 
feature function that counts the number of phrases 
used. We also add a feature that counts the 
number of target words; this acts as an 
insertion/deletion bonus/penalty.  
3. Decoding 
The challenge of tree-based decoding is that the 
traditional left-to-right decoding approach of 
string-based systems is inapplicable. Additional 
challenges are posed by the need to handle 
treelets?perhaps discontiguous or overlapping?
and a combinatorially explosive ordering space.  
Our decoding approach is influenced by ITG 
(Wu, 97) with several important extensions. First, 
we employ treelet translation pairs instead of 
single word translations. Second, instead of 
modeling rearrangements as either preserving 
source order or swapping source order, we allow 
the dependents of a node to be ordered in any 
arbitrary manner and use the order model 
described in section 2.4 to estimate probabilities. 
Finally, we use a log-linear framework for model 
combination that allows any amount of other 
information to be modeled.  
We will initially approach the decoding 
problem as a bottom up, exhaustive search. We 
define the set of all possible treelet translation 
pairs of the subtree rooted at each input node in 
the following manner: A treelet translation pair x 
is said to match the input dependency tree S iff 
there is some connected subgraph S? that is 
identical to the source side of x. We say that x 
covers all the nodes in S? and is rooted at source 
node s, where s is the root of matched subgraph 
S?.  
We first find all treelet translation pairs that 
match the input dependency tree. Each matched 
pair is placed on a list associated with the input 
node where the match is rooted. Moving bottom-
up through the input dependency tree, we 
compute a list of candidate translations for the 
input subtree rooted at each node s, as follows:  
Consider in turn each treelet translation pair x 
rooted at s. The treelet pair x may cover only a 
portion of the input subtree rooted at s. Find all 
descendents s' of s that are not covered by x, but 
whose parent s'' is covered by x. At each such 
node s'' look at all interleavings of the children of 
s'' specified by x, if any, with each translation t' 
from the candidate translation list5 of each child 
s'. Each such interleaving is scored using the 
models previously described and added to the 
candidate translation list for that input node. The 
resultant translation is the best scoring candidate 
for the root input node. 
As an example, see the example dependency 
tree in Figure 4a and treelet translation pair in 4b. 
This treelet translation pair covers all the nodes in 
4a except the subtrees rooted at software and is. 
                                                        
5
 Computed by the previous application of this procedure to s' during the 
bottom-up traversal. 
installed
software is on
the computer
your
 
 (a) Example input dependency tree. 
installed
on
computer
your
votre
ordinateur
sur
install?s
 
(b) Example treelet translation pair. 
 
Figure 4.  Example decoder structures. 
275
We first compute (and cache) the candidate 
translation lists for the subtrees rooted at software 
and is, then construct full translation candidates 
by attaching those subtree translations to install?s 
in all possible ways. The order of sur relative to 
install?s is fixed; it remains to place the translated 
subtrees for the software and is. Note that if c is 
the count of children specified in the mapping and 
r is the count of subtrees translated via recursive 
calls, then there are (c+r+1)!/(c+1)! orderings. 
Thus (1+2+1)!/(1+1)! = 12 candidate translations 
are produced for each combination of translations 
of the software and is. 
3.1. Optimality-preserving optimizations 
Dynamic Programming 
Converting this exhaustive search to dynamic 
programming relies on the observation that 
scoring a translation candidate at a node depends 
on the following information from its 
descendents: the order model requires features 
from the root of a translated subtree, and the 
target language model is affected by the first and 
last two words in each subtree. Therefore, we 
need to keep the best scoring translation candidate 
for a given subtree for each combination of (head, 
leading bigram, trailing bigram), which is, in the 
worst case, O(V5), where V is the vocabulary size. 
The dynamic programming approach therefore 
does not allow for great savings in practice 
because a trigram target language model forces 
consideration of context external to each subtree.  
Duplicate elimination 
To eliminate unnecessary ordering operations, we 
first check that a given set of words has not been 
previously ordered by the decoder. We use an 
order-independent hash table where two trees are 
considered equal if they have the same tree 
structure and lexical choices after sorting each 
child list into a canonical order. A simpler 
alternate approach would be to compare bags-of-
words. However since our possible orderings are 
bound by the induced tree structure, we might 
overzealously prune a candidate with a different 
tree structure that allows a better target order.  
3.2. Lossy optimizations 
The following optimizations do not preserve 
optimality, but work well in practice. 
N-best lists 
Instead of keeping the full list of translation 
candidates for a given input node, we keep a top-
scoring subset of the candidates. While the 
decoder is no longer guaranteed to find the 
optimal translation, in practice the quality impact 
is minimal with a list size ? 10 (see Table 5.6).  
Variable-sized n-best lists: A further speedup 
can be obtained by noting that the number of 
translations using a given treelet pair is 
exponential in the number of subtrees of the input 
not covered by that pair. To limit this explosion 
we vary the size of the n-best list on any recursive 
call in inverse proportion to the number of 
subtrees uncovered by the current treelet. This has 
the intuitive appeal of allowing a more thorough 
exploration of large treelet translation pairs (that 
are likely to result in better translations) than of 
smaller, less promising pairs.  
Pruning treelet translation pairs 
Channel model scores and treelet size are 
powerful predictors of translation quality. 
Heuristically pruning low scoring treelet 
translation pairs before the search starts allows 
the decoder to focus on combinations and 
orderings of high quality treelet pairs.  
? Only keep those treelet translation pairs with 
an MLE probability above a threshold t. 
? Given a set of treelet translation pairs with 
identical sources, keep those with an MLE 
probability within a ratio r of the best pair.  
? At each input node, keep only the top k treelet 
translation pairs rooted at that node, as ranked 
first by size, then by MLE channel model 
score, then by Model 1 score. The impact of 
this optimization is explored in Table 5.6.  
Greedy ordering 
The complexity of the ordering step at each node 
grows with the factorial of the number of children 
to be ordered. This can be tamed by noting that 
given a fixed pre- and post-modifier count, our 
order model is capable of evaluating a single 
ordering decision independently from other 
ordering decisions. 
One version of the decoder takes advantage of 
this to severely limit the number of ordering 
possibilities considered. Instead of considering all 
interleavings, it considers each potential modifier 
position in turn, greedily picking the most 
276
probable child for that slot, moving on to the next 
slot, picking the most probable among the 
remaining children for that slot and so on. 
The complexity of greedy ordering is linear, 
but at the cost of a noticeable drop in BLEU score 
(see Table 5.4). Under default settings our system 
tries to decode a sentence with exhaustive 
ordering until a specified timeout, at which point 
it falls back to greedy ordering. 
4. Experiments 
We evaluated the translation quality of the system 
using the BLEU metric (Papineni et al, 02) under 
a variety of configurations. We compared against 
two radically different types of systems to 
demonstrate the competitiveness of this approach:  
? Pharaoh: A leading phrasal SMT decoder 
(Koehn et al, 03). 
? The MSR-MT system described in Section 1, 
an EBMT/hybrid MT system.  
4.1. Data 
We used a parallel English-French corpus 
containing 1.5 million sentences of Microsoft 
technical data (e.g., support articles, product 
documentation). We selected a cleaner subset of 
this data by eliminating sentences with XML or 
HTML tags as well as very long (>160 characters) 
and very short (<40 characters) sentences. We 
held out 2,000 sentences for development testing 
and parameter tuning, 10,000 sentences for 
testing, and 250 sentences for lambda training. 
We ran experiments on subsets of the training 
data ranging from 1,000 to 300,000 sentences. 
Table 4.1 presents details about this dataset. 
4.2. Training 
We parsed the source (English) side of the corpus 
using NLPWIN, a broad-coverage rule-based 
parser developed at Microsoft Research able to 
produce syntactic analyses at varying levels of 
depth (Heidorn, 02). For the purposes of these 
experiments we used a dependency tree output 
with part-of-speech tags and unstemmed surface 
words.  
For word alignment, we used GIZA++, 
following a standard training regimen of five 
iterations of Model 1, five iterations of the HMM 
Model, and five iterations of Model 4, in both 
directions.  
We then projected the dependency trees and 
used the aligned dependency tree pairs to extract 
treelet translation pairs and train the order model 
as described above. The target language model 
was trained using only the French side of the 
corpus; additional data may improve its 
performance. Finally we trained lambdas via 
Maximum BLEU (Och, 03) on 250 held-out 
sentences with a single reference translation, and 
tuned the decoder optimization parameters (n-best 
list size, timeouts etc) on the development test set. 
Pharaoh 
The same GIZA++ alignments as above were 
used in the Pharaoh decoder. We used the 
heuristic combination described in (Och & Ney, 
03) and extracted phrasal translation pairs from 
this combined alignment as described in (Koehn 
et al, 03). Except for the order model (Pharaoh 
uses its own ordering approach), the same models 
were used: MLE channel model, Model 1 channel 
model, target language model, phrase count, and 
word count. Lambdas were trained in the same 
manner (Och, 03). 
MSR-MT 
MSR-MT used its own word alignment approach 
as described in (Menezes & Richardson, 01) on 
the same training data. MSR-MT does not use 
lambdas or a target language model. 
5. Results 
We present BLEU scores on an unseen 10,000 
sentence test set using a single reference 
translation for each sentence. Speed numbers are 
the end-to-end translation speed in sentences per 
minute. All results are based on a training set size 
of 100,000 sentences and a phrase size of 4, 
except Table 5.2 which varies the phrase size and 
Table 5.3 which varies the training set size. 
  English French 
Training Sentences 570,562 
 Words 7,327,251 8,415,882 
 Vocabulary 72,440 80,758 
 Singletons 38,037 39,496 
Test Sentences 10,000 
 Words 133,402 153,701 
Table 4.1 Data characteristics 
277
Results for our system and the comparison 
systems are presented in Table 5.1. Pharaoh 
monotone refers to Pharaoh with phrase 
reordering disabled. The difference between 
Pharaoh and the Treelet system is significant at 
the 99% confidence level under a two-tailed 
paired t-test. 
 BLEU Score Sents/min 
Pharaoh monotone 37.06 4286 
Pharaoh 38.83 162 
MSR-MT 35.26 453 
Treelet 40.66 10.1 
Table 5.1 System comparisons  
Table 5.2 compares Pharaoh and the Treelet 
system at different phrase sizes. While all the 
differences are statistically significant at the 99% 
confidence level, the wide gap at smaller phrase 
sizes is particularly striking. We infer that 
whereas Pharaoh depends heavily on long phrases 
to encapsulate reordering, our dependency tree-
based ordering model enables credible 
performance even with single-word ?phrases?. We 
conjecture that in a language pair with large-scale 
ordering differences, such as English-Japanese, 
even long phrases are unlikely to capture the 
necessary reorderings, whereas our tree-based 
ordering model may prove more robust. 
Max. size Treelet BLEU Pharaoh BLEU 
1  37.50 23.18  
2 39.84 32.07  
3 40.36 37.09  
4 (default) 40.66 38.83  
5 40.71 39.41  
6 40.74 39.72  
Table 5.2 Effect of maximum treelet/phrase size 
Table 5.3 compares the same systems at different 
training corpus sizes. All of the differences are 
statistically significant at the 99% confidence 
level. Noting that the gap widens at smaller 
corpus sizes, we suggest that our tree-based 
approach is more suitable than string-based 
phrasal SMT when translating from English into 
languages or domains with limited parallel data. 
We also ran experiments varying different 
system parameters. Table 5.4 explores different 
ordering strategies, Table 5.5 looks at the impact 
of discontiguous phrases and Table 5.6 looks at 
the impact of decoder optimizations such as 
treelet pruning and n-best list size. 
Ordering strategy BLEU  Sents/min  
No order model (monotone) 35.35 39.7 
Greedy ordering 38.85 13.1 
Exhaustive (default) 40.66 10.1 
Table 5.4 Effect of ordering strategies 
 BLEU Score  Sents/min 
Contiguous only 40.08  11.0 
Allow discontiguous 40.66 10.1 
Table 5.5 Effect of allowing treelets that correspond to 
discontiguous phrases 
 BLEU Score  Sents/min  
Pruning treelets   
  Keep top 1 28.58  144.9 
  ? top 3 39.10 21.2 
  ? top 5 40.29 14.6 
  ? top 10 (default) 40.66 10.1 
  ? top 20 40.70 3.5 
  Keep all 40.29 3.2 
N-best list size    
  1-best 37.28 175.4 
  5-best 39.96 79.4 
  10-best 40.42 23.3 
  20-best (default) 40.66 10.1 
  50-best 39.39 3.7 
Table 5.6 Effect of optimizations  
6. Discussion  
We presented a novel approach to syntactically-
informed statistical machine translation that 
leverages a parsed dependency tree representation 
of the source language via a tree-based ordering 
model and treelet phrase extraction. We showed 
that it significantly outperforms a leading phrasal 
SMT system over a wide range of training set 
sizes and phrase sizes. 
Constituents vs. dependencies: Most attempts at 
 1k 3k 10k 30k 100k 300k 
Pharaoh 17.20  22.51  27.70  33.73  38.83  42.75  
Treelet 18.70 25.39 30.96 35.81 40.66 44.32 
Table 5.3 Effect of training set size on treelet translation and comparison system  
 
278
syntactic SMT have relied on a constituency 
analysis rather than dependency analysis. While 
this is a natural starting point due to its well-
understood nature and commonly available tools, 
we feel that this is not the most effective 
representation for syntax in MT. Dependency 
analysis, in contrast to constituency analysis, 
tends to bring semantically related elements 
together (e.g., verbs become adjacent to all their 
arguments) and is better suited to lexicalized 
models, such as the ones presented in this paper.  
7. Future work 
The most important contribution of our system is 
a linguistically motivated ordering approach 
based on the source dependency tree, yet this 
paper only explores one possible model. Different 
model structures, machine learning techniques, 
and target feature representations all have the 
potential for significant improvements.  
Currently we only consider the top parse of an 
input sentence. One means of considering 
alternate possibilities is to build a packed forest of 
dependency trees and use this in decoding 
translations of each input sentence. 
As noted above, our approach shows particular 
promise for language pairs such as English-
Japanese that exhibit large-scale reordering and 
have proven difficult for string-based approaches. 
Further experimentation with such language pairs 
is necessary to confirm this. Our experience has 
been that the quality of GIZA++ alignments for 
such language pairs is inadequate. Following up 
on ideas introduced by (Cherry & Lin, 03) we 
plan to explore ways to leverage the dependency 
tree to improve alignment quality.  
References 
Alshawi, Hiyan, Srinivas Bangalore, and Shona 
Douglas. Learning dependency translation models 
as collections of finite-state head transducers. 
Computational Linguistics, 26(1):45?60, 2000. 
Aue, Anthony, Arul Menezes, Robert C. Moore, Chris 
Quirk, and Eric Ringger. Statistical machine 
translation using labeled semantic dependency 
graphs. TMI 2004. 
Charniak, Eugene, Kevin Knight, and Kenji Yamada. 
Syntax-based language models for statistical 
machine translation. MT Summit 2003. 
Cherry, Colin and Dekang Lin. A probability model to 
improve word alignment. ACL 2003. 
Chickering, David Maxwell. The WinMine Toolkit. 
Microsoft Research Technical Report: MSR-TR-
2002-103. 
Ding, Yuan and Martha Palmer. Automatic learning of 
parallel dependency treelet pairs. IJCNLP 2004. 
Heidorn, George. (2000). ?Intelligent writing 
assistance?. In Dale et al Handbook of Natural 
Language Processing, Marcel Dekker. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
Statistical phrase based translation. NAACL 2003. 
Lin, Dekang. A path-based transfer model for machine 
translation. COLING 2004. 
 Menezes, Arul and Stephen D. Richardson. A best-
first alignment algorithm for automatic extraction of 
transfer mappings from bilingual corpora. DDMT 
Workshop, ACL 2001. 
Och, Franz Josef and Hermann Ney. A systematic 
comparison of various statistical alignment models, 
Computational Linguistics, 29(1):19-51, 2003.  
Och, Franz Josef. Minimum error rate training in 
statistical machine translation. ACL 2003. 
Och, Franz Josef, et al A smorgasbord of features for 
statistical machine translation. HLT/NAACL 2004. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. BLEU: a method for automatic 
evaluation of machine translation. ACL 2002. 
Quirk, Chris, Arul Menezes, and Colin Cherry. 
Dependency Tree Translation. Microsoft Research 
Technical Report: MSR-TR-2004-113. 
Ringger, Eric, et al Linguistically informed statistical 
models of constituent structure for ordering in 
sentence realization. COLING 2004. 
 Thurmair, Gregor. Comparing rule-based and 
statistical MT output. Workshop on the amazing 
utility of parallel and comparable corpora, LREC, 
2004. 
 Vogel, Stephan, Ying Zhang, Fei Huang, Alicia 
Tribble, Ashish Venugopal, Bing Zhao, and Alex 
Waibel. The CMU statistical machine translation 
system. MT Summit 2003. 
Wu, Dekai. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):377?403, 1997. 
Yamada, Kenji and Kevin Knight. A syntax-based 
statistical translation model. ACL, 2001. 
279
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 105?112,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Soft Syntactic Constraints for Word Alignment
through Discriminative Training
Colin Cherry
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
colinc@cs.ualberta.ca
Dekang Lin
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA, USA, 94043
lindek@google.com
Abstract
Word alignment methods can gain valu-
able guidance by ensuring that their align-
ments maintain cohesion with respect to
the phrases specified by a monolingual de-
pendency tree. However, this hard con-
straint can also rule out correct alignments,
and its utility decreases as alignment mod-
els become more complex. We use a pub-
licly available structured output SVM to
create a max-margin syntactic aligner with
a soft cohesion constraint. The resulting
aligner is the first, to our knowledge, to use
a discriminative learning method to train
an ITG bitext parser.
1 Introduction
Given a parallel sentence pair, or bitext, bilin-
gual word alignment finds word-to-word connec-
tions across languages. Originally introduced as a
byproduct of training statistical translation models
in (Brown et al, 1993), word alignment has be-
come the first step in training most statistical trans-
lation systems, and alignments are useful to a host
of other tasks. The dominant IBM alignment mod-
els (Och and Ney, 2003) use minimal linguistic in-
tuitions: sentences are treated as flat strings. These
carefully designed generative models are difficult
to extend, and have resisted the incorporation of
intuitively useful features, such as morphology.
There have been many attempts to incorporate
syntax into alignment; we will not present a com-
plete list here. Some methods parse two flat strings
at once using a bitext grammar (Wu, 1997). Others
parse one of the two strings before alignment be-
gins, and align the resulting tree to the remaining
string (Yamada and Knight, 2001). The statisti-
cal models associated with syntactic aligners tend
to be very different from their IBM counterparts.
They model operations that are meaningful at a
syntax level, like re-ordering children, but ignore
features that have proven useful in IBM models,
such as the preference to align words with simi-
lar positions, and the HMM preference for links to
appear near one another (Vogel et al, 1996).
Recently, discriminative learning technology
for structured output spaces has enabled several
discriminative word alignment solutions (Liu et
al., 2005; Moore, 2005; Taskar et al, 2005). Dis-
criminative learning allows easy incorporation of
any feature one might have access to during the
alignment search. Because the features are han-
dled so easily, discriminative methods use features
that are not tied directly to the search: the search
and the model become decoupled.
In this work, we view synchronous parsing only
as a vehicle to expose syntactic features to a dis-
criminative model. This allows us to include the
constraints that would usually be imposed by a
tree-to-string alignment method as a feature in our
model, creating a powerful soft constraint. We
add our syntactic features to an already strong
flat-string discriminative solution, and we show
that they provide new information resulting in im-
proved alignments.
2 Constrained Alignment
Let an alignment be the complete structure that
connects two parallel sentences, and a link be
one of the word-to-word connections that make
up an alignment. All word alignment methods
benefit from some set of constraints. These limit
the alignment search space and encourage com-
petition between potential links. The IBM mod-
els (Brown et al, 1993) benefit from a one-to-
many constraint, where each target word has ex-
105
the tax causes unrest
l' imp?t cause le malaise
Figure 1: A cohesion constraint violation.
actly one generator in the source. Methods like
competitive linking (Melamed, 2000) and maxi-
mum matching (Taskar et al, 2005) use a one-to-
one constraint, where words in either sentence can
participate in at most one link. Throughout this pa-
per we assume a one-to-one constraint in addition
to any syntax constraints.
2.1 Cohesion Constraint
Suppose we are given a parse tree for one of the
two sentences in our sentence pair. We will re-
fer to the parsed language as English, and the
unparsed language as Foreign. Given this infor-
mation, a reasonable expectation is that English
phrases will move together when projected onto
Foreign. When this occurs, the alignment is said
to maintain phrasal cohesion.
Fox (2002) measured phrasal cohesion in gold
standard alignments by counting crossings. Cross-
ings occur when the projections of two disjoint
phrases overlap. For example, Figure 1 shows a
head-modifier crossing: the projection of the the
tax subtree, impo?t . . . le, is interrupted by the pro-
jection of its head, cause. Alignments with no
crossings maintain phrasal cohesion. Fox?s exper-
iments show that cohesion is generally maintained
for French-English, and that dependency trees pro-
duce the highest degree of cohesion among the
tested structures.
Cherry and Lin (2003) use the phrasal cohesion
of a dependency tree as a constraint on a beam
search aligner. This constraint produces a sig-
nificant reduction in alignment error rate. How-
ever, as Fox (2002) showed, even in a language
pair as close as French-English, there are situa-
tions where phrasal cohesion should not be main-
tained. These include incorrect parses, systematic
violations such as not ? ne . . . pas, paraphrases,
and linguistic exceptions.
We aim to create an alignment system that
obeys cohesion constraints most of the time, but
can violate them when necessary. Unfortunately,
Cherry and Lin?s beam search solution does not
lend itself to a soft cohesion constraint. The im-
perfect beam search may not be able to find the
optimal alignment under a soft constraint. Further-
more, it is not clear what penalty to assign to cross-
ings, or how to learn such a penalty from an iter-
ative training process. The remainder of this pa-
per will develop a complete alignment search that
is aware of cohesion violations, and use discrimi-
native learning technology to assign a meaningful
penalty to those violations.
3 Syntax-aware Alignment Search
We require an alignment search that can find the
globally best alignment under its current objective
function, and can account for phrasal cohesion in
this objective. IBM Models 1 and 2, HMM (Vo-
gel et al, 1996), and weighted maximum matching
alignment all conduct complete searches, but they
would not be amenable to monitoring the syntac-
tic interactions of links. The tree-to-string models
of (Yamada and Knight, 2001) naturally consider
syntax, but special modeling considerations are
needed to allow any deviations from the provided
tree (Gildea, 2003). The Inversion Transduction
Grammar or ITG formalism, described in (Wu,
1997), is well suited for our purposes. ITGs per-
form string-to-string alignment, but do so through
a parsing algorithm that will allow us to inform the
objective function of our dependency tree.
3.1 Inversion Transduction Grammar
An ITG aligns bitext through synchronous pars-
ing. Both sentences are decomposed into con-
stituent phrases simultaneously, producing a word
alignment as a byproduct. Viewed generatively, an
ITG writes to two streams at once. Terminal pro-
ductions produce a token in each stream, or a token
in one stream with the null symbol ? in the other.
We will use standard ITG notation: A ? e/f in-
dicates that the token e is produced on the English
stream, while f is produced on the Foreign stream.
To allow for some degree of movement during
translation, non-terminal productions are allowed
to be either straight or inverted. Straight pro-
ductions, with their non-terminals inside square
brackets [. . .], produce their symbols in the same
order on both streams. Inverted productions, in-
dicated by angled brackets ?. . .?, have their non-
terminals produced in the given order on the En-
glish stream, but this order is reversed in the For-
eign stream.
106
the Canadian agriculture industry
l' industrie agricole Canadienne
Figure 2: An example of an ITG alignment. A
horizontal bar across an arc indicates an inversion.
An ITG chart parser provides a polynomial-
time algorithm to conduct a complete enumeration
of all alignments that are possible according to its
grammar. We will use a binary bracketing ITG, the
simplest interesting grammar in this formalism:
A? [AA] | ?AA? | e/f
This grammar enforces its own weak cohesion
constraint: for every possible alignment, a corre-
sponding binary constituency tree must exist for
which the alignment maintains phrasal cohesion.
Figure 2 shows a word alignment and the corre-
sponding tree found by an ITG parser. Wu (1997)
provides anecdotal evidence that only incorrect
alignments are eliminated by ITG constraints. In
our French-English data set, an ITG rules out
only 0.3% of necessary links beyond those already
eliminated by the one-to-one constraint (Cherry
and Lin, 2006).
3.2 Dependency-augmented ITG
An ITG will search all alignments that conform
to a possible binary constituency tree. We wish
to confine that search to a specific n-array depen-
dency tree. Fortunately, Wu (1997) provides a
method to have an ITG respect a known partial
structure. One can seed the ITG parse chart so that
spans that do not agree with the provided structure
are assigned a value of?? before parsing begins.
The result is that no constituent is ever constructed
with any of these invalid spans.
In the case of phrasal cohesion, the invalid spans
correspond to spans of the English sentence that
interrupt the phrases established by the provided
dependency tree. To put this notion formally, we
first define some terms: given a subtree T[i,k],
where i is the left index of the leftmost leaf in T[i,k]
and k is the right index of its rightmost leaf, we say
any index j ? (i, k) is internal to T[i,k]. Similarly,
any index x /? [i, k] is external to T[i,k]. An in-
valid span is any span for which our provided tree
T[i,k]
x1 i j k x2j'
T
Figure 3: Illustration of invalid spans. [j?, j] and
[j, k] are legal, while [x1, j] and [j, x2] are not.
the tax causes unrest
Figure 4: The invalid spans induced by a depen-
dency tree.
has a subtree T[i,k] such that one endpoint of the
span is internal to T[i,k] while the other is external
to it. Figure 3 illustrates this definition, while Fig-
ure 4 shows the invalid spans induced by a simple
dependency tree.
With these invalid spans in place, the ITG can
no longer merge part of a dependency subtree with
anything other than another part of the same sub-
tree. Since all ITG movement can be explained
by inversions, this constrained ITG cannot in-
terrupt one dependency phrase with part of an-
other. Therefore, the phrasal cohesion of the in-
put dependency tree is maintained. Note that this
will not search the exact same alignment space
as a cohesion-constrained beam search; instead it
uses the union of the cohesion constraint and the
weaker ITG constraints (Cherry and Lin, 2006).
Transforming this form of the cohesion con-
straint into a soft constraint is straight-forward.
Instead of overriding the parser so it cannot use
invalid English spans, we will note the invalid
spans and assign the parser a penalty should it
use them. The value of this penalty will be de-
termined through discriminative training, as de-
scribed in Section 4. Since the penalty is avail-
able within the dynamic programming algorithm,
the parser will be able to incorporate it to find a
globally optimal alignment.
4 Discriminative Training
To discriminatively train our alignment systems,
we adopt the Support Vector Machine (SVM) for
107
Structured Output (Tsochantaridis et al, 2004).
We have selected this system for its high degree of
modularity, and because it has an API freely avail-
able1. We will summarize the learning mechanism
briefly in this section, but readers should refer to
(Tsochantaridis et al, 2004) for more details.
SVM learning is most easily expressed as a con-
strained numerical optimization problem. All con-
straints mentioned in this section are constraints
on this optimizer, and have nothing to do with the
cohesion constraint from Section 2.
4.1 SVM for Structured Output
Traditional SVMs attempt to find a linear sepa-
rator that creates the largest possible margin be-
tween two classes of vectors. Structured output
SVMs attempt to separate the correct structure
from all incorrect structures by the largest possible
margin, for all training instances. This may sound
like a much more difficult problem, but with a few
assumptions in place, the task begins to look very
similar to a traditional SVM.
As in most discriminative training methods, we
begin by assuming that a candidate structure y,
built for an input instance x, can be adequately de-
scribed using a feature vector ?(x, y). We also as-
sume that our ?(x, y) decomposes in such a way
that the features can guide a search to recover the
structure y from x. That is:
struct(x; ~w) = argmaxy?Y ?~w,?(x, y)? (1)
is computable, where Y is the set of all possible
structures, and ~w is a vector that assigns weights
to each component of ?(x, y). ~w is the parameter
vector we will learn using our SVM.
Now the learning task begins to look straight-
forward: we are working with vectors, and the
task of building a structure y has been recast as
an argmax operator. Our learning goal is to find a
~w so that the correct structure is found:
?i, ?y ? Y \ yi : ?~w,?i(yi)? > ?~w,?i(y)? (2)
where xi is the ith training example, yi is its
correct structure, and ?i(y) is short-hand for
?(xi, y). As several ~w will fulfill (2) in a linearly
separable training set, the unique max-margin ob-
jective is defined to be the ~w that maximizes the
minimum distance between yi and the incorrect
structures in Y .
1At http://svmlight.joachims.org/svm struct.html
This learning framework also incorporates a no-
tion of structured loss. In a standard vector clas-
sification problem, there is 0-1 loss: a vector is
either classified correctly or it is not. In the struc-
tured case, some incorrect structures can be bet-
ter than others. For example, having the argmax
select an alignment missing only one link is bet-
ter than selecting one with no correct links and a
dozen wrong ones. A loss function ?(yi, y) quan-
tifies just how incorrect a particular structure y is.
Though Tsochantaridis et al (2004) provide sev-
eral ways to incorporate loss into the SVM ob-
jective, we will use margin re-scaling, as it corre-
sponds to loss usage in another max-margin align-
ment approach (Taskar et al, 2005). In margin
re-scaling, high loss structures must be separated
from the correct structure by a larger margin than
low loss structures.
To allow some misclassifications during train-
ing, a soft-margin requirement replaces our max-
margin objective. A slack variable ?i is introduced
for each training example xi, to allow the learner
to violate the margin at a penalty. The magnitude
of this penalty to determined by a hand-tuned pa-
rameter C. After a few transformations (Tsochan-
taridis et al, 2004), the soft-margin learning ob-
jective can be formulated as a quadratic program:
min~w,?
1
2
||~w||2 +
C
n
n?
i=1
?i, s.t. ?i?i ? 0 (3)
?i, ?y ? Y \ yi : (4)
?~w,?i(yi)??i(y)? ? ?(yi, y)? ?i
Note how the slack variables ?i allow some in-
correct structures to be built. Also note that the
loss ?(yi, y) determines the size of the margin be-
tween structures.
Unfortunately, (4) provides one constraint for
every possible structure for every training exam-
ple. Enumerating these constraints explicitly is in-
feasible, but in reality, only a subset of these con-
straints are necessary to achieve the same objec-
tive. Re-organizing (4) produces:
?i,?y ? Y \ yi :
?i ? ?(yi, y)? ?~w,?i(yi)??i(y)?
(5)
which is equivalent to:
?i : ?i ? max
y?Y\yi
costi(y; ~w) (6)
where costi is defined as:
costi(y; ~w) = ?(yi, y)? ?~w,?i(yi)??i(y)?
108
Provided that the max cost structure can be found
in polynomial time, we have all the components
needed for a constraint generation approach to this
optimization problem.
Constraint generation places an outer loop
around an optimizer that minimizes (3) repeatedly
for a growing set of constraints. It begins by min-
imizing (3) with an empty constraint set in place
of (4). This provides values for ~w and ~?. The max
cost structure
y? = argmaxy?Y\yicosti(y; ~w)
is found for i = 1 with the current ~w. If the re-
sulting costi(y?; ~w) is greater than the current value
of ?i, then this represents a violated constraint2 in
our complete objective, and a new constraint of
the form ?i ? costi(y?; ~w) is added to the con-
straint set. The algorithm then iterates: the opti-
mizer minimizes (3) again with the new constraint
set, and solves the max cost problem for i = i+ 1
with the new ~w, growing the constraint set if nec-
essary. Note that the constraints on ? change with
~w, as cost is a function of ~w. Once the end of
the training set is reached, the learner loops back
to the beginning. Learning ends when the entire
training set can be processed without needing to
add any constraints. It can be shown that this
will occur within a polynomial number of itera-
tions (Tsochantaridis et al, 2004).
With this framework in place, one need only fill
in the details to create an SVM for a new struc-
tured output space:
1. A ?(x, y) function to transform instance-
structure pairs into feature vectors
2. A search to find the best structure given a
weight vector: argmaxy ?~w,?(x, y)?. This
has no role in training, but it is necessary to
use the learned weights.
3. A structured loss function ?(y, y?)
4. A search to find the max cost structure:
argmaxycosti(y;w)
4.2 SVMs for Alignment
Using the Structured SVM API, we have created
two SVM word aligners: a baseline that uses
weighted maximum matching for its argmax op-
erator, and a dependency-augmented ITG that will
2Generally the test to see if ?i > costi(y?; ~w) is approxi-
mated as ?i > costi(y?; ~w) + ? for a small constant ?.
satisfy our requirements for an aligner with a soft
cohesion constraint. Our x becomes a bilingual
sentence-pair, while our y becomes an alignment,
represented by a set of links.
4.2.1 Weighed Maximum Matching
Given a bipartite graph with edge values, the
weighted maximum matching algorithm (West,
2001) will find the matching with maximum
summed edge values. To create a matching align-
ment solution, we reproduce the approach of
(Taskar et al, 2005) within the framework de-
scribed in Section 4.1:
1. We define a feature vector ? for each poten-
tial link l in x, and ? in terms of y?s compo-
nent links: ?(x, y) =
?
l?y ?(l).
2. Our structure search is the matching algo-
rithm. The input bipartite graph has an edge
for each l. Each edge is given the value
v(l)? ?~w, ?(l)?.
3. We adopt the weighted Hamming loss in de-
scribed (Taskar et al, 2005):
?(y, y?) = co|y ? y?|+ cc|y? ? y|
where co is an omission penalty and cc is a
commission penalty.
4. Our max cost search corresponds to their
loss-augmented matching problem. The in-
put graph is modified to prefer costly links:
?l /? y : v(l)? ?~w, ?(l)?+ cc
?l ? y : v(l)? ?~w, ?(l)? ? co
Note that our max cost search could not have been
implemented as loss-augmented matching had we
selected one of the other loss objectives presented
in (Tsochantaridis et al, 2004) in place of margin
rescaling.
We use the same feature representation ?(l) as
(Taskar et al, 2005), with some small exceptions.
Let l = (Ej , Fk) be a potential link between the
jth word of English sentence E and the kth word
of Foreign sentence F . To measure correlation be-
tween Ej and Fk we use conditional link proba-
bility (Cherry and Lin, 2003) in place of the Dice
coefficient:
cor(Ej , Fk) =
#links(Ej , Fk)? d
#cooccurrences(Ej , Fk)
where the link counts are determined by word-
aligning 50K sentence pairs with another match-
ing SVM that uses the ?2 measure (Gale and
109
Church, 1991) in place of Dice. The ?2 measure
requires only co-occurrence counts. d is an abso-
lute discount parameter as in (Moore, 2005). Also,
we omit the IBM Model 4 Prediction features, as
we wish to know how well we can do without re-
sorting to traditional word alignment techniques.
Otherwise, the features remain the same,
including distance features that measure
abs
(
j
|E| ?
k
|F |
)
; orthographic features; word
frequencies; common-word features; a bias term
set alays to 1; and an HMM approximation
cor(Ej+1, Fk+1).
4.2.2 Soft Dependency-augmented ITG
Because of the modularity of the structured out-
put SVM, our SVM ITG re-uses a large amount
infrastructure from the matching solution. We
essentially plug an ITG parser in the place of
the matching algorithm, and add features to take
advantage of information made available by the
parser. x remains a sentence pair, and y becomes
an ITG parse tree that decomposes x and speci-
fies an alignment. Our required components are as
follows:
1. We define a feature vector ?T on instances
of production rules, r. ? is a function of
the decomposition specified by y: ?(x, y) =
?
r?y ?T (r).
2. The structure search is a weighted ITG parser
that maximizes summed production scores.
Each instance of a production rule r is as-
signed a score of ?~w, ?T (r)?
3. Loss is unchanged, defined in terms of the
alignment induced by y.
4. A loss-augmented ITG is used to find the max
cost. Productions of the form A ? e/f
that correspond to links have their scores aug-
mented as in the matching system.
The ?T vector has two new features in addition to
those present in the matching system?s ?. These
features can be active only for non-terminal pro-
ductions, which have the formA? [AA] | ?AA?.
One feature indicates an inverted production A?
?AA?, while the other indicates the use of an in-
valid span according to a provided English depen-
dency tree, as described in Section 3.2. These
are the only features that can be active for non-
terminal productions.
A terminal production rl that corresponds to a
link l is given that link?s features from the match-
ing system: ?T (rl) = ?(l). Terminal productions
r? corresponding to unaligned tokens are given
blank feature vectors: ?T (r?) = ~0.
The SVM requires complete ? vectors for the
correct training structures. Unfortunately, our
training set contains gold standard alignments, not
ITG parse trees. The gold standard is divided into
sure and possible link sets S and P (Och and Ney,
2003). Links in S must be included in a correct
alignment, while P links are optional. We create
ITG trees from the gold standard using the follow-
ing sorted priorities during tree construction:
? maximize the number of links from S
? minimize the number of English dependency
span violations
? maximize the number of links from P
? minimize the number of inversions
This creates trees that represent high scoring align-
ments, using a minimal number of invalid spans.
Only the span and inversion counts of these trees
will be used in training, so we need not achieve a
perfect tree structure. We still evaluate all methods
with the original alignment gold standard.
5 Experiments and Results
We conduct two experiments. The first tests
the dependency-augmented ITG described in Sec-
tion 3.2 as an aligner with hard cohesion con-
straints. The second tests our discriminative ITG
with soft cohesion constraints against two strong
baselines.
5.1 Experimental setup
We conduct our experiments using French-English
Hansard data. Our ?2 scores, link probabilities
and word frequency counts are determined using a
sentence-aligned bitext consisting of 50K sentence
pairs. Our training set for the discriminative align-
ers is the first 100 sentence pairs from the French-
English gold standard provided for the 2003 WPT
workshop (Mihalcea and Pedersen, 2003). For
evaluation we compare to the remaining 347 gold
standard pairs using the alignment evaluation met-
rics: precision, recall and alignment error rate or
AER (Och and Ney, 2003). SVM learning param-
eters are tuned using the 37-pair development set
provided with this data. English dependency trees
are provided by Minipar (Lin, 1994).
110
Table 1: The effect of hard cohesion constraints on
a simple unsupervised link score.
Search Prec Rec AER
Matching 0.723 0.845 0.231
ITG 0.764 0.860 0.200
D-ITG 0.830 0.873 0.153
5.2 Hard Constraint Performance
The goal of this experiment is to empirically con-
firm that the English spans marked invalid by
Section 3.2?s dependency-augmented ITG provide
useful guidance to an aligner. To do so, we
compare an ITG with hard cohesion constraints,
an unconstrained ITG, and a weighted maximum
matching aligner. All aligners use the same sim-
ple objective function. They maximize summed
link values v(l), where v(l) is defined as follows
for an l = (Ej , Fk):
v(l) = ?2(Ej , Fk)? 10
?5abs
(
j
|E|
?
k
|F |
)
All three aligners link based on ?2 correlation
scores, breaking ties in favor of closer pairs. This
allows us to evaluate the hard constraints outside
the context of supervised learning.
Table 1 shows the results of this experiment.
We can see that switching the search method
from weighted maximum matching to a cohesion-
constrained ITG (D-ITG) has produced a 34% rel-
ative reduction in alignment error rate. The bulk
of this improvement results from a substantial in-
crease in precision, though recall has also gone up.
This indicates that these cohesion constraints are a
strong alignment feature. The ITG row shows that
the weaker ITG constraints are also valuable, but
the cohesion constraint still improves on them.
5.3 Soft Constraint Performance
We now test the performance of our SVM ITG
with soft cohesion constraint, or SD-ITG, which
is described in Section 4.2.2. We will test against
two strong baselines. The first baseline, matching
is the matching SVM described in Section 4.2.1,
which is a re-implementation of the state-of-the-
art work in (Taskar et al, 2005)3. The second
baseline, D-ITG is an ITG aligner with hard co-
hesion constraints, but which uses the weights
3Though it is arguably lacking one of its strongest fea-
tures: the output of GIZA++ (Och and Ney, 2003)
Table 2: The performance of SVM-trained align-
ers with various degrees of cohesion constraint.
Method Prec Rec AER
Matching 0.916 0.860 0.110
D-ITG 0.940 0.854 0.100
SD-ITG 0.944 0.878 0.086
trained by the matching SVM to assign link val-
ues. This is the most straight-forward way to com-
bine discriminative training with the hard syntactic
constraints.
The results are shown in Table 2. The first thing
to note is that our Matching baseline is achieving
scores in line with (Taskar et al, 2005), which re-
ports an AER of 0.107 using similar features and
the same training and test sets.
The effect of the hard cohesion constraint has
been greatly diminished after discriminative train-
ing. Matching and D-ITG correspond to the the
entries of the same name in Table 1, only with a
much stronger, learned value function v(l). How-
ever, in place of a 34% relative error reduction, the
hard constraints in the D-ITG produce only a 9%
reduction from 0.110 to 0.100. Also note that this
time the hard constraints result in a reduction in
recall. This indicates that the hard cohesion con-
straint is providing little guidance not provided by
other features, and that it is actually eliminating
more sure links than it is helping to find.
The soft-constrained SD-ITG, which has access
to the D-ITG?s invalid spans as a feature during
SVM training, is fairing substantially better. Its
AER of 0.086 represents a 22% relative error re-
duction compared to the matching system. The
improved error rate is caused by gains in both pre-
cision and recall. This indicates that the invalid
span feature is doing more than just ruling out
links; perhaps it is de-emphasizing another, less
accurate feature?s role. The SD-ITG overrides the
cohesion constraint in only 41 of the 347 test sen-
tences, so we can see that it is indeed a soft con-
straint: it is obeyed nearly all the time, but it can be
broken when necessary. The SD-ITG achieves by
far the strongest ITG alignment result reported on
this French-English set; surpassing the 0.16 AER
reported in (Zhang and Gildea, 2004).
Training times for this system are quite low; un-
supervised statistics can be collected quickly over
a large set, while only the 100-sentence training
111
set needs to be iteratively aligned. Our match-
ing SVM trains in minutes on a single-processor
machine, while the SD-ITG trains in roughly one
hour. The ITG is the bottleneck, so training time
could be improved by optimizing the parser.
6 Related Work
Several other aligners have used discriminative
training. Our work borrows heavily from (Taskar
et al, 2005), which uses a max-margin approach
with a weighted maximum matching aligner.
(Moore, 2005) uses an averaged perceptron for
training with a customized beam search. (Liu et
al., 2005) uses a log-linear model with a greedy
search. To our knowledge, ours is the first align-
ment approach to use this highly modular struc-
tured SVM, and the first discriminative method to
use an ITG for the base aligner.
(Gildea, 2003) presents another aligner with a
soft syntactic constraint. This work adds a cloning
operation to the tree-to-string generative model in
(Yamada and Knight, 2001). This allows subtrees
to move during translation. As the model is gen-
erative, it is much more difficult to incorporate a
wide variety of features as we do here. In (Zhang
and Gildea, 2004), this model was tested on the
same annotated French-English sentence pairs that
we divided into training and test sets for our exper-
iments; it achieved an AER of 0.15.
7 Conclusion
We have presented a discriminative, syntactic
word alignment method. Discriminative training
is conducted using a highly modular SVM for
structured output, which allows code reuse be-
tween the syntactic aligner and a maximum match-
ing baseline. An ITG parser is used for the align-
ment search, exposing two syntactic features: the
use of inverted productions, and the use of spans
that would not be available in a tree-to-string sys-
tem. This second feature creates a soft phrasal co-
hesion constraint. Discriminative training allows
us to maintain all of the features that are useful to
the maximum matching baseline in addition to the
new syntactic features. We have shown that these
features produce a 22% relative reduction in error
rate with respect to a strong flat-string model.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?312.
C. Cherry and D. Lin. 2003. A probability model to improve
word alignment. In Meeting of the Association for Com-
putational Linguistics, pages 88?95, Sapporo, Japan, July.
C. Cherry and D. Lin. 2006. A comparison of syntacti-
cally motivated word alignment spaces. In Proceedings
of EACL, pages 145?152, Trento, Italy, April.
H. J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of EMNLP, pages 304?311.
W. A. Gale and K. W. Church. 1991. Identifying word cor-
respondences in parallel texts. In 4th Speech and Natural
Language Workshop, pages 152?157. DARPA.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In Meeting of the Association for Computa-
tional Linguistics, pages 80?87, Sapporo, Japan.
D. Lin. 1994. Principar - an efficient, broad-coverage,
principle-based parser. In Proceedings of COLING, pages
42?48, Kyoto, Japan.
Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models for word
alignment. In Meeting of the Association for Computa-
tional Linguistics, pages 459?466, Ann Arbor, USA.
I. D. Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221?
249.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In HLT-NAACL Workshop on
Building and Using Parallel Texts, pages 1?10, Edmon-
ton, Canada.
R. Moore. 2005. A discriminative framework for bilingual
word alignment. In Proceedings of HLT-EMNLP, pages
81?88, Vancouver, Canada, October.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational Lin-
guistics, 29(1):19?52, March.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discrimi-
native matching approach to word alignment. In Proceed-
ings of HLT-EMNLP, pages 73?80, Vancouver, Canada.
I. Tsochantaridis, T. Hofman, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdependent
and structured output spaces. In Proceedings of ICML,
pages 823?830.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceedings
of COLING, pages 836?841, Copenhagen, Denmark.
D. West. 2001. Introduction to Graph Theory. Prentice Hall,
2nd edition.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
K. Yamada and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Meeting of the Association for
Computational Linguistics, pages 523?530.
H. Zhang and D. Gildea. 2004. Syntax-based alignment:
Supervised or unsupervised? In Proceedings of COLING,
Geneva, Switzerland, August.
112
ProAlign: Shared Task System Description
Dekang Lin and Colin Cherry
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{lindek,colinc}@cs.ualberta.ca
Abstract
ProAlign combines several different ap-
proaches in order to produce high quality word
word alignments. Like competitive linking,
ProAlign uses a constrained search to find high
scoring alignments. Like EM-based methods,
a probability model is used to rank possible
alignments. The goal of this paper is to give
a bird?s eye view of the ProAlign system to
encourage discussion and comparison.
1 Alignment Algorithm at a Glance
We have submitted the ProAlign alignment system to
the WPT?03 shared task. It received a 5.71% AER on
the English-French task and 29.36% on the Romanian-
English task. These results are with the no-null data; our
output was not formatted to work with explicit nulls.
ProAlign works by iteratively improving an align-
ment. The algorithm creates an initial alignment us-
ing search, constraints, and summed ?2 correlation-based
scores (Gale and Church, 1991). This is similar to the
competitive linking process (Melamed, 2000). It then
learns a probability model from the current alignment,
and conducts a constrained search again, this time scor-
ing alignments according to the probability model. The
process continues until results on a validation set begin to
indicate over-fitting.
For the purposes of our algorithm, we view an align-
ment as a set of links between the words in a sen-
tence pair. Before describing the algorithm, we will de-
fine the following notation. Let E be an English sen-
tence e1, e2, . . . , em and let F be a French sentence
f1, f2, . . . , fn. We define a link l(ei, fj) to exist if ei and
fj are a translation (or part of a translation) of one an-
other. We define the null link l(ei, f0) to exist if ei does
not correspond to a translation for any French word in F .
The null link l(e0, fj) is defined similarly. An alignment
A for two sentences E and F is a set of links such that ev-
ery word in E and F participates in at least one link, and
a word linked to e0 or f0 participates in no other links. If
e occurs in E x times and f occurs in F y times, we say
that e and f co-occur xy times in this sentence pair.
ProAlign conducts a best-first search (with constant
beam and agenda size) to search a constrained space of
possible alignments. A state in this space is a partial
alignment, and a transition is defined as the addition of
a single link to the current state. Any link which would
create a state that does not violate any constraint is con-
sidered to be a valid transition. Our start state is the empty
alignment, where all words in E and F are implicitly
linked to null. A terminal state is a state in which no more
links can be added without violating a constraint. Our
goal is to find the terminal state with the highest proba-
bility.
To complete this algorithm, one requires a set of con-
straints and a method for determining which alignment is
most likely. These are presented in the next two sections.
The algorithm takes as input a set of English-French sen-
tence pairs, along with dependency trees for the English
sentences. The presence of the English dependency tree
allows us to incorporate linguistic features into our model
and linguistic intuitions into our constraints.
2 Constraints
The model used for scoring alignments has no mecha-
nism to prevent certain types of undesirable alignments,
such as having all French words align to the same En-
glish word. To guide the search to correct alignments, we
employ two constraints to limit our search for the most
probable alignment. The first constraint is the one-to-one
constraint (Melamed, 2000): every word (except the null
words e0 and f0) participates in exactly one link.
The second constraint, known as the cohesion con-
straint (Fox, 2002), uses the dependency tree (Mel?c?uk,
1987) of the English sentence to restrict possible link
combinations. Given the dependency tree TE and a (par-
tial) alignment A, the cohesion constraint requires that
phrasal cohesion is maintained in the French sentence. If
two phrases are disjoint in the English sentence, the align-
ment must not map them to overlapping intervals in the
French sentence. This notion of phrasal constraints on
alignments need not be restricted to phrases determined
from a dependency structure. However, the experiments
conducted in (Fox, 2002) indicate that dependency trees
demonstrate a higher degree of phrasal cohesion during
translation than other structures.
Consider the partial alignment in Figure 1. The most
probable lexical match for the English word to is the
French word a`. When the system attempts to link to and
a`, the distinct English phrases [the reboot] and [the host
to discover all the devices] will be mapped to intervals
in the French sentence, creating the induced phrasal in-
tervals [a` . . . [re?initialisation] . . . pe?riphe?riques]. Re-
gardless of what these French phrases will be after the
alignment is completed, we know now that their intervals
will overlap. Therefore, this link will not be added to the
partial alignment.
The?reboot?causes?the?host?to?discover?all?the?devices?
det? subj? det? subj?aux?
pre?
det?
obj?mod?
  ?    la?Suite? r?initialisation  ,? l'  h?te?rep?re?tous?les?p?riph?riques?
after?to? the? reboot? the?host? locate? all? the? peripherals?
1? 2? 3? 4? 5?6? 7? 8? 9? 10?
1? 2? 3? 4? 5?6?7? 8? 9?10? 11?
Figure 1: An Example of Cohesion Constraint
To define this notion more formally, let TE(ei) be
the subtree of TE rooted at ei. The phrase span of
ei, spanP(ei, TE , A), is the image of the English phrase
headed by ei in F given a (partial) alignment A. More
precisely, spanP(ei, TE , A) = [k1, k2], where
k1 = min{j|l(u, j) ? A, eu ? TE(ei)}
k2 = max{j|l(u, j) ? A, eu ? TE(ei)}
The head span is the image of ei itself. We define
spanH(ei, TE , A) = [k1, k2], where
k1 = min{j|l(i, j) ? A}
k2 = max{j|l(i, j) ? A}
In Figure 1, for the node reboot, the phrase span is
[4,4] and the head span is also [4,4]; for the node discover
(with the link between to and a` in place), the phrase span
is [2,11] and the head span is the empty set ?.
With these definitions of phrase and head spans, we de-
fine two notions of overlap, originally introduced in (Fox,
2002) as crossings. Given a head node eh and its modi-
fier em, a head-modifier overlap occurs when:
spanH(eh, TE , A) ? spanP(em, TE , A) 6= ?
Given two nodes em1 and em2 which both modify the
same head node, a modifier-modifier overlap occurs
when:
spanP(em1 , TE , A) ? spanP(em2 , TE , A) 6= ?
Following (Fox, 2002), we say an alignment is co-
hesive with respect to TE if it does not introduce
any head-modifier or modifier-modifier overlaps. For
example, the alignment A in Figure 1 is not cohe-
sive because spanP (reboot, TE , A) = [4, 4] intersects
spanP (discover, TE , A) = [2, 11]. Since both reboot
and discover modify causes, this creates a modifier-
modifier overlap. One can check for constraint viola-
tions inexpensively by incrementally updating the vari-
ous spans as new links are added to the partial alignment,
and checking for overlap after each modification. More
details on the cohesion constraint can be found in (Lin
and Cherry, 2003).
3 Probability Model
We define the word alignment problem as finding the
alignment A that maximizes P (A|E,F ). ProAlign mod-
els P (A|E,F ) directly, using a different decomposition
of terms than the model used by IBM (Brown et al,
1993). In the IBM models of translation, alignments ex-
ist as artifacts of a stochastic process, where the words
in the English sentence generate the words in the French
sentence. Our model does not assume that one sentence
generates the other. Instead it takes both sentences as
given, and uses the sentences to determine an alignment.
An alignment A consists of t links {l1, l2, . . . , lt}, where
each lk = l(eik , fjk) for some ik and jk. We will refer to
consecutive subsets of A as lji = {li, li+1, . . . , lj}. Given
this notation, P (A|E,F ) can be decomposed as follows:
P (A|E,F ) = P (lt1|E,F ) =
t?
k=1
P (lk|E,F, l
k?1
1 )
At this point, we factor P (lk|E,F, lk?11 ) to make com-
putation feasible. Let Ck = {E,F, lk?11 } represent the
context of lk. Note that both the context Ck and the link
lk imply the occurrence of eik and fjk . We can rewrite
P (lk|Ck) as:
P (lk|Ck) =
P (lk, Ck)
P (Ck)
=
P (Ck|lk)P (lk)
P (Ck, eik , fjk)
= P (lk|eik , fjk)?
P (Ck|lk)
P (Ck|eik , fjk)
Here P (lk|eik , fjk) is link probability given a co-
occurrence of the two words, which is similar in spirit to
Melamed?s explicit noise model (Melamed, 2000). This
term depends only on the words involved directly in the
link. The ratio P (Ck|lk)P (Ck|eik ,fjk ) modifies the link probability,
providing context-sensitive information.
Ck remains too broad to deal with in practical sys-
tems. We will consider only a subset FT k of relevant
features of Ck. We will make the Na??ve Bayes-style as-
sumption that these features ft ? FT k are conditionally
independent given either lk or (eik , fjk). This produces a
tractable formulation for P (A|E,F ):
t?
k=1
?
?P (lk|eik , fjk)?
?
ft?FTk
P (ft |lk)
P (ft |eik , fjk)
?
?
More details on the probability model used by ProAlign
are available in (Cherry and Lin, 2003).
3.1 Features used in the shared task
For the purposes of the shared task, we use two feature
types. Each type could have any number of instantiations
for any number of contexts. Note that each feature type
is described in terms of the context surrounding a word
pair.
The first feature type fta concerns surrounding links.
It has been observed that words close to each other in
the source language tend to remain close to each other in
the translation (S. Vogel and Tillmann, 1996). To capture
this notion, for any word pair (ei, fj), if a link l(ei? , fj?)
exists within a window of two words (where i?2 ? i? ?
i+2 and j?2 ? j? ? j+2), then we say that the feature
fta(i? i
?, j ? j?, ei?) is active for this context. We refer
to these as adjacency features.
The second feature type ftd uses the English parse tree
to capture regularities among grammatical relations be-
tween languages. For example, when dealing with French
and English, the location of the determiner with respect
to its governor is never swapped during translation, while
the location of adjectives is swapped frequently. For any
word pair (ei, fj), let ei? be the governor of ei, and let
rel be the relationship between them. If a link l(ei? , fj?)
exists, then we say that the feature ftd(j ? j?, rel) is ac-
tive for this context. We refer to these as dependency
features.
Take for example Figure 2 which shows a partial align-
ment with all links completed except for those involving
the. Given this sentence pair and English parse tree, we
can extract features of both types to assist in the align-
ment of the1. The word pair (the1, l?) will have an active
adjacency feature fta(+1,+1, host) as well as a depen-
dency feature ftd(?1, det). These two features will work
together to increase the probability of this correct link.
the host discovers all the devices
det
subj pre
det
obj
 l'  h?te rep?re tous les p?riph?riques
1 2 3 4 5
1 2 3 4 5 6
6
the host locate all the peripherals
Figure 2: Feature Extraction Example
In contrast, the incorrect link (the1, les) will have only
ftd(+3, det), which will work to lower the link probabil-
ity, since most determiners are located before their gov-
ernors.
3.2 Training the model
Since we always work from a current alignment, training
the model is a simple matter of counting events in the
current alignment. Link probability is the number of time
two words are linked, divided by the number of times
they co-occur. The various feature probabilities can be
calculated by also counting the number of times a feature
occurs in the context of a linked pair of words, and the
number of times the feature is active for co-occurrences
of the same word pair.
Considering only a single, potentially noisy alignment
for a given sentence pair can result in reinforcing errors
present in the current alignment during training. To avoid
this problem, we sample from a space of probable align-
ments, as is done in IBM models 3 and above (Brown
et al, 1993), and weight counts based on the likelihood
of each alignment sampled under the current probability
model. To further reduce the impact of rare, and poten-
tially incorrect events, we also smooth our probabilities
using m-estimate smoothing (Mitchell, 1997).
4 Multiple Alignments
The result of the constrained alignment search is a high-
precision, word-to-word alignment. We then relax the
word-to-word constraint, and use statistics regarding col-
locations with unaligned words in order to make many-to-
one alignments. We also employ a further relaxed link-
ing process to catch some cases where the cohesion con-
straint ruled out otherwise good alignments. These auxil-
iary methods are currently not integrated into our search
or our probability model, although that is certainly a di-
rection for future work.
5 Conclusions
We have presented a brief overview of the major ideas
behind our entry to the WPT?03 Shared Task. Primary
among these ideas are the use of a cohesion constraint in
search, and our novel probability model.
Acknowledgments
This project is funded by and jointly undertaken with Sun
Microsystems, Inc. We wish to thank Finola Brady, Bob
Kuhns and Michael McHugh for their help. We also wish
to thank the WPT?03 reviewers for their helpful com-
ments.
References
P. F. Brown, V. S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?312.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. Submitted.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In 2002 Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP
2002), pages 304?311.
W.A. Gale and K.W. Church. 1991. Identifying word
correspondences in parallel texts. In 4th Speech
and Natural Language Workshop, pages 152?157.
DARPA, Morgan Kaufmann.
Dekang Lin and Colin Cherry. 2003. Word alignment
with cohesion constraint. Submitted.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249, June.
Igor A. Mel?c?uk. 1987. Dependency syntax: theory and
practice. State University of New York Press, Albany.
Tom Mitchell. 1997. Machine Learning. McGraw Hill.
H. Ney S. Vogel and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In 16th In-
ternational Conference on Computational Linguistics,
pages 836?841, Copenhagen, Denmark, August.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 88?95, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
An Expectation Maximization Approach to Pronoun Resolution
Colin Cherry and Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{colinc,bergsma}@cs.ualberta.ca
Abstract
We propose an unsupervised Expectation
Maximization approach to pronoun reso-
lution. The system learns from a fixed
list of potential antecedents for each pro-
noun. We show that unsupervised learn-
ing is possible in this context, as the per-
formance of our system is comparable to
supervised methods. Our results indicate
that a probabilistic gender/number model,
determined automatically from unlabeled
text, is a powerful feature for this task.
1 Introduction
Coreference resolution is the process of determin-
ing which expressions in text refer to the same real-
world entity. Pronoun resolution is the important yet
challenging subset of coreference resolution where a
system attempts to establish coreference between a
pronominal anaphor, such as a third-person pronoun
like he, she, it, or they, and a preceding noun phrase,
called an antecedent. In the following example, a
pronoun resolution system must determine the cor-
rect antecedent for the pronouns ?his? and ?he.?
(1) When the president entered the arena with his
family, he was serenaded by a mariachi band.
Pronoun resolution has applications across many
areas of Natural Language Processing, particularly
in the field of information extraction. Resolving a
pronoun to a noun phrase can provide a new inter-
pretation of a given sentence, giving a Question An-
swering system, for example, more data to consider.
Our approach is a synthesis of linguistic and sta-
tistical methods. For each pronoun, a list of an-
tecedent candidates derived from the parsed corpus
is presented to the Expectation Maximization (EM)
learner. Special cases, such as pleonastic, reflex-
ive and cataphoric pronouns are dealt with linguisti-
cally during list construction. This allows us to train
on and resolve all third-person pronouns in a large
Question Answering corpus. We learn lexicalized
gender/number, language, and antecedent probabil-
ity models. These models, tied to individual words,
can not be learned with sufficient coverage from la-
beled data. Pronouns are resolved by choosing the
most likely antecedent in the candidate list accord-
ing to these distributions. The resulting resolution
accuracy is comparable to supervised methods.
We gain further performance improvement by ini-
tializing EM with a gender/number model derived
from special cases in the training data. This model
is shown to perform reliably on its own. We also
demonstrate how the models learned through our un-
supervised method can be used as features in a su-
pervised pronoun resolution system.
2 Related Work
Pronoun resolution typically employs some com-
bination of constraints and preferences to select
the antecedent from preceding noun phrase candi-
dates. Constraints filter the candidate list of improb-
able antecedents, while preferences encourage se-
lection of antecedents that are more recent, frequent,
etc. Implementation of constraints and preferences
can be based on empirical insight (Lappin and Le-
ass, 1994), or machine learning from a reference-
88
annotated corpus (Ge et al, 1998). The majority
of pronoun resolution approaches have thus far re-
lied on manual intervention in the resolution pro-
cess, such as using a manually-parsed corpus, or
manually removing difficult non-anaphoric cases;
we follow Mitkov et al?s approach (2002) with a
fully-automatic pronoun resolution method. Pars-
ing, noun-phrase identification, and non-anaphoric
pronoun removal are all done automatically.
Machine-learned, fully-automatic systems are
more common in noun phrase coreference resolu-
tion, where the method of choice has been deci-
sion trees (Soon et al, 2001; Ng and Cardie, 2002).
These systems generally handle pronouns as a subset
of all noun phrases, but with limited features com-
pared to systems devoted solely to pronouns. Kehler
used Maximum Entropy to assign a probability dis-
tribution over possible noun phrase coreference re-
lationships (1997). Like his approach, our system
does not make hard coreference decisions, but re-
turns a distribution over candidates.
The above learning approaches require anno-
tated training data for supervised learning. Cardie
and Wagstaff developed an unsupervised approach
that partitions noun phrases into coreferent groups
through clustering (1999). However, the partitions
they generate for a particular document are not use-
ful for processing new documents, while our ap-
proach learns distributions that can be used on un-
seen data. There are also approaches to anaphora
resolution using unsupervised methods to extract
useful information, such as gender and number (Ge
et al, 1998), or contextual role-knowledge (Bean
and Riloff, 2004). Co-training can also leverage
unlabeled data through weakly-supervised reference
resolution learning (Mu?ller et al, 2002). As an alter-
native to co-training, Ng and Cardie (2003) use EM
to augment a supervised coreference system with
unlabeled data. Their feature set is quite different, as
it is designed to generalize from the data in a labeled
set, while our system models individual words. We
suspect that the two approaches can be combined.
Our approach is inspired by the use of EM in
bilingual word alignment, which finds word-to-word
correspondences between a sentence and its transla-
tion. The prominent statistical methods in this field
are unsupervised. Our methods are most influenced
by IBM?s Model 1 (Brown et al, 1993).
3 Methods
3.1 Problem formulation
We will consider our training set to consist of
(p, k, C) triples: one for each pronoun, where p is
the pronoun to be resolved, k is the pronoun?s con-
text, and C is a candidate list containing the nouns p
could potentially be resolved to. Initially, we take k
to be the parsed sentence that p appears in.
C consists of all nouns and pronouns that precede
p, looking back through the current sentence and the
sentence immediately preceding it. This small win-
dow may seem limiting, but we found that a cor-
rect candidate appeared in 97% of such lists in a
labeled development text. Mitkov et al also limit
candidate consideration to the same window (2002).
Each triple is processed with non-anaphoric pronoun
handlers (Section 3.3) and linguistic filters (Sec-
tion 3.4), which produce the final candidate lists.
Before we pass the (p, k, C) triples to EM, we
modify them to better suit our EM formulation.
There are four possibilities for the gender and num-
ber of third-person pronouns in English: masculine,
feminine, neutral and plural (e.g., he, she, it, they).
We assume a noun is equally likely to corefer with
any member of a given gender/number category, and
reduce each p to a category label accordingly. For
example, he, his, him and himself are all labeled as
masc for masculine pronoun. Plural, feminine and
neutral pronouns are handled similarly. We reduce
the context term k to p?s immediate syntactic con-
text, including only p?s syntactic parent, the parent?s
part of speech, and p?s relationship to the parent, as
determined by a dependency parser. Incorporating
context only through the governing constituent was
also done in (Ge et al, 1998). Finally, each candi-
date in C is augmented with ordering information,
so we know how many nouns to ?step over? before
arriving at a given candidate. We will refer to this or-
dering information as a candidate?s j term, for jump.
Our example sentence in Section 1 would create the
two triples shown in Figure 1, assuming the sentence
began the document it was found in.
3.2 Probability model
Expectation Maximization (Dempster et al, 1977) is
a process for filling in unobserved data probabilisti-
cally. To use EM to do unsupervised pronoun reso-
89
his: p = masc k = p?s family
C = arena (0), president (1)
he: p = masc k = serenade p
C = family (0), masc (1), arena (2),
president (3)
Figure 1: EM input for our example sentence.
j-values follow each lexical candidate.
lution, we phrase the resolution task in terms of hid-
den variables of an observed process. We assume
that in each case, one candidate from the candidate
list is selected as the antecedent before p and k are
generated. EM?s role is to induce a probability dis-
tribution over candidates to maximize the likelihood
of the (p, k) pairs observed in our training set:
Pr(Dataset) =
?
(p,k)?Dataset
Pr(p, k) (1)
We can rewrite Pr(p, k) so that it uses a hidden can-
didate (or antecedent) variable c that influences the
observed p and k:
Pr(p, k) =
?
c?C
Pr(p, k, c) (2)
Pr(p, k, c) = Pr(p, k|c)Pr(c) (3)
To improve our ability to generalize to future cases,
we use a na??ve Bayes assumption to state that the
choices of pronoun and context are conditionally in-
dependent, given an antecedent. That is, once we
select the word the pronoun represents, the pronoun
and its context are no longer coupled:
Pr(p, k|c) = Pr(p|c)Pr(k|c) (4)
We can split each candidate c into its lexical com-
ponent l and its jump value j. That is, c = (l, j).
If we assume that l and j are independent, and that
p and k each depend only on the l component of c,
we can combine Equations 3 and 4 to get our final
formulation for the joint probability distribution:
Pr(p, k, c) = Pr(p|l)Pr(k|l)Pr(l)Pr(j) (5)
The jump term j, though important when resolving
pronouns, is not likely to be correlated with any lex-
ical choices in the training set.
Table 1: Examples of learned pronoun probabilities.
Word (l) masc fem neut plur
company 0.03 0.01 0.95 0.01
president 0.94 0.01 0.03 0.02
teacher 0.19 0.71 0.09 0.01
This results in four models that work together to
determine the likelihood of a given candidate. The
Pr(p|l) distribution measures the likelihood of a pro-
noun given an antecedent. Since we have collapsed
the observed pronouns into groups, this models a
word?s affinity for each of the four relevant gen-
der/number categories. We will refer to this as our
pronoun model. Pr(k|l) measures the probability of
the syntactic relationship between a pronoun and its
parent, given a prospective antecedent for the pro-
noun. This is effectively a language model, grading
lexical choice by context. Pr(l) measures the prob-
ability that the word l will be found to be an an-
tecedent. This is useful, as some entities, such as
?president? in newspaper text, are inherently more
likely to be referenced with a pronoun. Finally,
Pr(j) measures the likelihood of jumping a given
number of noun phrases backward to find the cor-
rect candidate. We represent these models with ta-
ble look-up. Table 1 shows selected l-value entries
in the Pr(p|l) table from our best performing EM
model. Note that the probabilities reflect biases in-
herent in our news domain training set.
Given models for the four distributions above,
we can assign a probability to each candidate in
C according to the observations p and k; that is,
Pr(c|p, k) can be obtained by dividing Equation 5
by Equation 2. Remember that c = (l, j).
Pr(c|p, k) =
Pr(p|l)Pr(k|l)Pr(l)Pr(j)
?
c??C Pr(p|l?)Pr(k|l?)Pr(l?)Pr(j?)(6)
Pr(c|p, k) allows us to get fractional counts of
(p, k, c) triples in our training set, as if we had actu-
ally observed c co-occurring with (p, k) in the pro-
portions specified by Equation 6. This estimation
process is effectively the E-step in EM.
The M-step is conducted by redefining our mod-
els according to these fractional counts. For exam-
ple, after assigning fractional counts to candidates
90
according to Pr(c|p, k), we re-estimate Pr(p|l) with
the following equation for a specific (p, l) pair:
Pr(p|l) =
N(p, l)
N(l)
(7)
where N() counts the number of times we see a
given event or joint event throughout the training set.
Given trained models, we resolve pronouns by
finding the candidate c? that is most likely for the
current pronoun, that is c? = argmaxc?CPr(c|p, k).
Because Pr(p, k) is constant with respect to c,
c? = argmaxc?CPr(p, k, c).
3.3 Non-anaphoric Pronouns
Not every pronoun in text refers anaphorically to a
preceding noun phrase. There are a frequent num-
ber of difficult cases that require special attention,
including pronouns that are:
? Pleonastic: pronouns that have a grammatical
function but do not reference an entity. E.g. ?It
is important to observe it is raining.?
? Cataphora: pronouns that reference a future
noun phrase. E.g. ?In his speech, the president
praised the workers.?
? Non-noun referential: pronouns that refer to a
verb phrase, sentence, or implicit concept. E.g.
?John told Mary they should buy a car.?
If we construct them na??vely, the candidate lists
for these pronouns will be invalid, introducing noise
in our training set. Manual handling or removal
of these cases is infeasible in an unsupervised ap-
proach, where the input is thousands of documents.
Instead, pleonastics are identified syntactically us-
ing an extension of the detector developed by Lap-
pin and Leass (1994). Roughly 7% of all pronouns
in our labeled test data are pleonastic. We detect
cataphora using a pattern-based method on parsed
sentences, described in (Bergsma, 2005b). Future
nouns are only included when cataphora are iden-
tified. This approach is quite different from Lap-
pin and Leass (1994), who always include all fu-
ture nouns from the current sentence as candidates,
with a constant penalty added to possible cataphoric
resolutions. The cataphora module identifies 1.4%
of test data pronouns to be cataphoric; in each in-
stance this identification is correct. Finally, we know
of no approach that handles pronouns referring to
verb phrases or implicit entities. The unavoidable
errors for these pronouns, occurring roughly 4% of
the time, are included in our final results.
3.4 Candidate list modifications
It would be possible for C to include every noun
phrase in the current and previous sentence, but per-
formance can be improved by automatically remov-
ing improbable antecedents. We use a standard set of
constraints to filter candidates. If a candidate?s gen-
der or number is known, and does not match the pro-
noun?s, the candidate is excluded. Candidates with
known gender include other pronouns, and names
with gendered designators (such as ?Mr.? or ?Mrs.?).
Our parser also identifies plurals and some gendered
first names. We remove from C all times, dates, ad-
dresses, monetary amounts, units of measurement,
and pronouns identified as pleonastic.
We use the syntactic constraints from Binding
Theory to eliminate candidates (Haegeman, 1994).
For the reflexives himself, herself, itself and them-
selves, this allows immediate syntactic identification
of the antecedent. These cases become unambigu-
ous; only the indicated antecedent is included in C.
We improve the quality of our training set by re-
moving known noisy cases before passing the set
to EM. For example, we anticipate that sentences
with quotation marks will be problematic, as other
researchers have observed that quoted text requires
special handling for pronoun resolution (Kennedy
and Boguraev, 1996). Thus we remove pronouns
occurring in the same sentences as quotes from the
learning process. Also, we exclude triples where
the constraints removed all possible antecedents, or
where the pronoun was deemed to be pleonastic.
Performing these exclusions is justified for training,
but in testing we state results for all pronouns.
3.5 EM initialization
Early in the development of this system, we were
impressed with the quality of the pronoun model
Pr(p|l) learned by EM. However, we found we could
construct an even more precise pronoun model for
common words by examining unambiguous cases in
our training data. Unambiguous cases are pronouns
having only one word in their candidate list C. This
could be a result of the preprocessors described in
91
Sections 3.3 and 3.4, or the pronoun?s position in
the document. A PrU (p|l) model constructed from
only unambiguous examples covers far fewer words
than a learned model, but it rarely makes poor gen-
der/number choices. Furthermore, it can be obtained
without EM. Training on unambiguous cases is sim-
ilar in spirit to (Hindle and Rooth, 1993). We found
in our development and test sets that, after applying
filters, roughly 9% of pronouns occur with unam-
biguous antecedents.
When optimizing a probability function that is not
concave, the EM algorithm is only guaranteed to
find a local maximum; therefore, it can be helpful
to start the process near the desired end-point in pa-
rameter space. The unambiguous pronoun model
described above can provide such a starting point.
When using this initializer, we perform our ini-
tial E-step by weighting candidates according to
PrU (p|l), instead of weighting them uniformly. This
biases the initial E-step probabilities so that a strong
indication of the gender/number of a candidate from
unambiguous cases will either boost the candidate?s
chances or remove it from competition, depending
on whether or not the predicted category matches
that of the pronoun being resolved.
To deal with the sparseness of the PrU (p|l) dis-
tribution, we use add-1 smoothing (Jeffreys, 1961).
The resulting effect is that words with few unam-
biguous occurrences receive a near-uniform gen-
der/number distribution, while those observed fre-
quently will closely match the observed distribution.
During development, we also tried clever initializers
for the other three models, including an extensive
language model initializer, but none were able to im-
prove over PrU (p|l) alone.
3.6 Supervised extension
Even though we have justified Equation 5 with rea-
sonable independence assumptions, our four mod-
els may not be combined optimally for our pronoun
resolution task, as the models are only approxima-
tions of the true distributions they are intended to
represent. Following the approach in (Och and Ney,
2002), we can view the right-hand-side of Equa-
tion 5 as a special case of:
exp
(
?1 log Pr(p|l) + ?2 log Pr(k|l)+
?3 log Pr(l) + ?4 log Pr(j)
)
(8)
where ?i : ?i = 1. Effectively, the log proba-
bilities of our models become feature functions in
a log-linear model. When labeled training data is
available, we can use the Maximum Entropy princi-
ple (Berger et al, 1996) to optimize the ? weights.
This provides us with an optional supervised ex-
tension to the unsupervised system. Given a small
set of data that has the correct candidates indicated,
such as the set we used while developing our unsu-
pervised system, we can re-weight the final models
provided by EM to maximize the probability of ob-
serving the indicated candidates. To this end, we
follow the approach of (Och and Ney, 2002) very
closely, including their handling of multiple correct
answers. We use the limited memory variable met-
ric method as implemented in Malouf?s maximum
entropy package (2002) to set our weights.
4 Experimental Design
4.1 Data sets
We used two training sets in our experiments, both
drawn from the AQUAINT Question Answering
corpus (Vorhees, 2002). For each training set, we
manually labeled pronoun antecedents in a corre-
sponding key containing a subset of the pronouns
in the set. These keys are drawn from a collection
of complete documents. For each document, all pro-
nouns are included. With the exception of the super-
vised extension, the keys are used only to validate
the resolution decisions made by a trained system.
Further details are available in (Bergsma, 2005b).
The development set consists of 333,000 pro-
nouns drawn from 31,000 documents. The devel-
opment key consists of 644 labeled pronouns drawn
from 58 documents; 417 are drawn from sentences
without quotation marks. The development set and
its key were used to guide us while designing the
probability model, and to fine-tune EM and smooth-
ing parameters. We also use the development key as
labeled training data for our supervised extension.
The test set consists of 890,000 pronouns drawn
from 50,000 documents. The test key consists of
1209 labeled pronouns drawn from 118 documents;
892 are drawn from sentences without quotation
marks. All of the results reported in Section 5 are
determined using the test key.
92
4.2 Implementation Details
To get the context values and implement the syntac-
tic filters, we parsed our corpora with Minipar (Lin,
1994). Experiments on the development set indi-
cated that EM generally began to overfit after 2 it-
erations, so we stop EM after the second iteration,
using the models from the second M-step for test-
ing. During testing, ties in likelihood are broken by
taking the candidate closest to the pronoun.
The EM-produced models need to be smoothed,
as there will be unseen words and unobserved (p, l)
or (k, l) pairs in the test set. This is because prob-
lematic cases are omitted from the training set, while
all pronouns are included in the key. We han-
dle out-of-vocabulary events by replacing words or
context-values that occur only once during training
with a special unknown symbol. Out-of-vocabulary
events encountered during testing are also treated
as unknown. We handle unseen pairs with additive
smoothing. Instead of adding 1 as in Section 3.5, we
add ?p = 0.00001 for (k, l) pairs, and ?w = 0.001
for (p, l) pairs. These ? values were determined ex-
perimentally with the development key.
4.3 Evaluation scheme
We evaluate our work in the context of a fully auto-
matic system, as was done in (Mitkov et al, 2002).
Our evaluation criteria is similar to their resolution
etiquette. We define accuracy as the proportion of
pronouns correctly resolved, either to any coreferent
noun phrase in the candidate list, or to the pleonas-
tic category, which precludes resolution. Systems
that handle and state performance for all pronouns
in unrestricted text report much lower accuracy than
most approaches in the literature. Furthermore, au-
tomatically parsing and pre-processing texts causes
consistent degradation in performance, regardless of
the accuracy of the pronoun resolution algorithm. To
have a point of comparison to other fully-automatic
approaches, note the resolution etiquette score re-
ported in (Mitkov et al, 2002) is 0.582.
5 Results
5.1 Validation of unsupervised method
The key concern of our work is whether enough
useful information is present in the pronoun?s cat-
egory, context, and candidate list for unsupervised
learning of antecedents to occur. To that end, our
first set of experiments compare the pronoun resolu-
tion accuracy of our EM-based solutions to that of a
previous-noun baseline on our test key. The results
are shown in Table 2. The columns split the results
into three cases: all pronouns with no exceptions;
all cases where the pronoun was found in a sentence
containing no quotation marks (and therefore resem-
bling the training data provided to EM); and finally
all pronouns excluded by the second case. We com-
pare the following methods:
1. Previous noun: Pick the candidate from the fil-
tered list with the lowest j value.
2. EM, no initializer: The EM algorithm trained
on the test set, starting from a uniform E-step.
3. Initializer, no EM: A model that ranks candi-
dates using only a pronoun model built from
unambiguous cases (Section 3.5).
4. EM w/ initializer: As in (2), but using the ini-
tializer in (3) for the first E-step.
5. Maxent extension: The models produced by
(4) are used as features in a log-linear model
trained on the development key (Section 3.6).
6. Upper bound: The percentage of cases with a
correct answer in the filtered candidate list.
For a reference point, picking the previous noun be-
fore applying any of our candidate filters receives an
accuracy score of 0.281 on the ?All? task.
Looking at the ?All? column in Table 2, we see
EM can indeed learn in this situation. Starting from
uniform parameters it climbs from a 40% baseline
to a 60% accurate model. However, the initializer
can do slightly better with precise but sparse gen-
der/number information alone. As we hoped, com-
bining the initializer and EM results in a statistically
significant1 improvement over EM with a uniform
starting point, but it is not significantly better than
the initializer alone. The advantage of the EM pro-
cess is that it produces multiple models, which can
be re-weighted with maximum entropy to reach our
highest accuracy, roughly 67%. The ? weights that
achieve this score are shown in Table 3.
Maximum entropy leaves the pronoun model
Pr(p|l) nearly untouched and drastically reduces the
1Significance is determined throughout Section 5 using Mc-
Nemar?s test with a significance level ? = 0.05.
93
Table 2: Accuracy for all cases, all excluding sen-
tences with quotes, and only sentences with quotes.
Method All No? ? Only? ?
1 Previous noun 0.397 0.399 0.391
2 EM, no initializer 0.610 0.632 0.549
3 Initializer, no EM 0.628 0.642 0.587
4 EM w/ initializer 0.632 0.663 0.546
5 Maxent extension 0.669 0.696 0.593
6 Upper bound 0.838 0.868 0.754
influence of all other models (Table 3). This, com-
bined with the success of the initializer alone, leads
us to believe that a strong notion of gender/number
is very important in this task. Therefore, we im-
plemented EM with several models that used only
pronoun category, but none were able to surpass the
initializer in accuracy on the test key. One factor
that might help explain the initializer?s success is
that despite using only a PrU (p|l) model, the ini-
tializer also has an implicit factor resembling a Pr(l)
model: when two candidates agree with the category
of the pronoun, add-1 smoothing ensures the more
frequent candidate receives a higher probability.
As was stated in Section 3.4, sentences with quo-
tations were excluded from the learning process be-
cause the presence of a correct antecedent in the can-
didate list was less frequent in these cases. This is
validated by the low upper bound of 0.754 in the
only-quote portion of the test key. We can see that
all methods except for the previous noun heuris-
tic score noticeably better when ignoring those sen-
tences that contain quotation marks. In particular,
the difference between our three unsupervised solu-
tions ((2), (3) and (4)) are more pronounced. Much
of the performance improvements that correspond
to our model refinements are masked in the overall
task because adding the initializer to EM does not
improve EM?s performance on quotes at all. Devel-
oping a method to construct more robust candidate
lists for quotations could improve our performance
on these cases, and greatly increase the percentage
of pronouns we are training on for a given corpus.
Table 3: Weights set by maximum entropy.
Model Pr(p|l) Pr(k|l) Pr(l) Pr(j)
Lambda 0.931 0.056 0.070 0.167
Table 4: Comparison to SVM.
Method Accuracy
Previous noun 0.398
EM w/ initializer 0.664
Maxent extension 0.708
SVM 0.714
5.2 Comparison to supervised system
We put our results in context by comparing our
methods to a recent supervised system. The compar-
ison system is an SVM that uses 52 linguistically-
motivated features, including probabilistic gen-
der/number information obtained through web
queries (Bergsma, 2005a). The SVM is trained
with 1398 separate labeled pronouns, the same train-
ing set used in (Bergsma, 2005a). This data is
also drawn from the news domain. Note the su-
pervised system was not constructed to handle all
pronoun cases, so non-anaphoric pronouns were re-
moved from the test key and from the candidate lists
in the test key to ensure a fair comparison. As ex-
pected, this removal of difficult cases increases the
performance of our system on the test key (Table 4).
Also note there is no significant difference in per-
formance between our supervised extension and the
SVM. The completely unsupervised EM system per-
forms worse, but with only a 7% relative reduction
in performace compared to the SVM; the previous
noun heuristic shows a 44% reduction.
5.3 Analysis of upper bound
If one accounts for the upper bound in Table 2, our
methods do very well on those cases where a cor-
rect answer actually appears in the candidate list: the
best EM solution scores 0.754, and the supervised
extension scores 0.800. A variety of factors result in
the 196 candidate lists that do not contain a true an-
tecedent. 21% of these errors arise from our limited
candidate window (Section 3.1). Incorrect pleonas-
tic detection accounts for another 31% while non-
94
noun referential pronouns cause 25% (Section 3.3).
Linguistic filters (Section 3.4) account for most of
the remainder. An improvement in any of these com-
ponents would result in not only higher final scores,
but cleaner EM training data.
6 Conclusion
We have demonstrated that unsupervised learning is
possible for pronoun resolution. We achieve accu-
racy of 63% on an all-pronoun task, or 75% when
a true antecedent is available to EM. There is now
motivation to develop cleaner candidate lists and
stronger probability models, with the hope of sur-
passing supervised techniques. For example, incor-
porating antecedent context, either at the sentence
or document level, may boost performance. Further-
more, the lexicalized models learned in our system,
especially the pronoun model, are potentially pow-
erful features for any supervised pronoun resolution
system.
References
David L. Bean and Ellen Riloff. 2004. Unsupervised learning
of contextual role knowledge for coreference resolution. In
HLT-NAACL, pages 297?304.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della
Pietra. 1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 22(1):39?71.
Shane Bergsma. 2005a. Automatic acquisition of gender infor-
mation for anaphora resolution. In Proceedings of the 18th
Conference of the Canadian Society for Computational Intel-
ligence (Canadian AI 2005), pages 342?353, Victoria, BC.
Shane Bergsma. 2005b. Corpus-based learning for pronom-
inal anaphora resolution. Master?s thesis, Department
of Computing Science, University of Alberta, Edmonton.
http://www.cs.ualberta.ca/?bergsma/Pubs/thesis.pdf.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19(2):263?312.
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase corefer-
ence as clustering. In Proceedings of the 1999 Joint SIGDAT
Conference on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora, pages 82?89.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, 39(1):1?38.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical
approach to anaphora resolution. In Proceedings of the Sixth
Workshop on Very Large Corpora, pages 161?171.
L. Haegeman. 1994. Introduction to Government & Binding
theory: Second Edition. Basil Blackwell, Cambridge, UK.
Donald Hindle and Mats Rooth. 1993. Structural ambiguity
and lexical relations. Computational Linguistics, 19(1):103?
120.
Harold Jeffreys, 1961. Theory of Probability, chapter 3.23. Ox-
ford: Clarendon Press, 3rd edition.
Andrew Kehler. 1997. Probabilistic coreference in informa-
tion extraction. In Proceedings of the Second Conference on
Empirical Methods in Natural Language Processing, pages
163?173.
Christopher Kennedy and Branimir Boguraev. 1996. Anaphora
for everyone: Pronominal anaphora resolution without a
parser. In Proceedings of the 16th Conference on Compu-
tational Linguistics, pages 113?118.
Shalom Lappin and Herbert J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational Linguis-
tics, 20(4):535?561.
Dekang Lin. 1994. Principar - an efficient, broad-coverage,
principle-based parser. In Proceedings of COLING-94,
pages 42?48, Kyoto, Japan.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings of the
Sixth Conference on Natural Language Learning (CoNLL-
2002), pages 49?55.
Ruslan Mitkov, Richard Evans, and Constantin Orasan. 2002.
A new, fully automatic version of Mitkov?s knowledge-poor
pronoun resolution method. In Proceedings of the Third
International Conference on Computational Linguistics and
Intelligent Text Processing, pages 168?186.
Christoph Mu?ller, Stefan Rapp, and Michael Strube. 2002. Ap-
plying co-training to reference resolution. In Proceedings
of the 40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 352?359.
Vincent Ng and Claire Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 104?111.
Vincent Ng and Claire Cardie. 2003. Weakly supervised nat-
ural language learning without redundant views. In HLT-
NAACL 2003: Proceedings of the Main Conference, pages
173?180.
Franz J. Och and Hermann Ney. 2002. Discriminative training
and maximum entropy models for statistical machine trans-
lation. In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics, pages 295?302,
Philadelphia, PA, July.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim.
2001. A machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics, 27(4):521?
544.
Ellen Vorhees. 2002. Overview of the TREC 2002 question an-
swering track. In Proceedings of the Eleventh Text REtrieval
Conference (TREC).
95
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 21?28, New York City, June 2006. c?2006 Association for Computational Linguistics
Improved Large Margin Dependency Parsing
via Local Constraints and Laplacian Regularization
Qin Iris Wang Colin Cherry Dan Lizotte Dale Schuurmans
Department of Computing Science
University of Alberta
 
wqin,colinc,dlizotte,dale  @cs.ualberta.ca
Abstract
We present an improved approach for
learning dependency parsers from tree-
bank data. Our technique is based on two
ideas for improving large margin train-
ing in the context of dependency parsing.
First, we incorporate local constraints that
enforce the correctness of each individ-
ual link, rather than just scoring the global
parse tree. Second, to cope with sparse
data, we smooth the lexical parameters ac-
cording to their underlying word similar-
ities using Laplacian Regularization. To
demonstrate the benefits of our approach,
we consider the problem of parsing Chi-
nese treebank data using only lexical fea-
tures, that is, without part-of-speech tags
or grammatical categories. We achieve
state of the art performance, improving
upon current large margin approaches.
1 Introduction
Over the past decade, there has been tremendous
progress on learning parsing models from treebank
data (Collins, 1997; Charniak, 2000; Wang et al,
2005; McDonald et al, 2005). Most of the early
work in this area was based on postulating gener-
ative probability models of language that included
parse structure (Collins, 1997). Learning in this con-
text consisted of estimating the parameters of the
model with simple likelihood based techniques, but
incorporating various smoothing and back-off esti-
mation tricks to cope with the sparse data problems
(Collins, 1997; Bikel, 2004). Subsequent research
began to focus more on conditional models of parse
structure given the input sentence, which allowed
discriminative training techniques such as maximum
conditional likelihood (i.e. ?maximum entropy?)
to be applied (Ratnaparkhi, 1999; Charniak, 2000).
In fact, recently, effective conditional parsing mod-
els have been learned using relatively straightfor-
ward ?plug-in? estimates, augmented with similar-
ity based smoothing (Wang et al, 2005). Currently,
the work on conditional parsing models appears to
have culminated in large margin training (Taskar
et al, 2003; Taskar et al, 2004; Tsochantaridis et
al., 2004; McDonald et al, 2005), which currently
demonstrates the state of the art performance in En-
glish dependency parsing (McDonald et al, 2005).
Despite the realization that maximum margin
training is closely related to maximum conditional
likelihood for conditional models (McDonald et
al., 2005), a sufficiently unified view has not yet
been achieved that permits the easy exchange of
improvements between the probabilistic and non-
probabilistic approaches. For example, smoothing
methods have played a central role in probabilistic
approaches (Collins, 1997; Wang et al, 2005), and
yet they are not being used in current large margin
training algorithms. However, as we demonstrate,
not only can smoothing be applied in a large mar-
gin training framework, it leads to generalization im-
provements in much the same way as probabilistic
approaches. The second key observation we make is
somewhat more subtle. It turns out that probabilistic
approaches pay closer attention to the individual er-
rors made by each component of a parse, whereas
the training error minimized in the large margin
approach?the ?structured margin loss? (Taskar et
al., 2003; Tsochantaridis et al, 2004; McDonald et
al., 2005)?is a coarse measure that only assesses
the total error of an entire parse rather than focusing
on the error of any particular component.
21
 funds?Investors?continue? to? pour?cash?into?money?
Figure 1: A dependency tree
In this paper, we make two contributions to the
large margin approach to learning parsers from su-
pervised data. First, we show that smoothing based
on lexical similarity is not only possible in the large
margin framework, but more importantly, allows
better generalization to new words not encountered
during training. Second, we show that the large mar-
gin training objective can be significantly refined to
assess the error of each component of a given parse,
rather than just assess a global score. We show that
these two extensions together lead to greater train-
ing accuracy and better generalization to novel input
sentences than current large margin methods.
To demonstrate the benefit of combining useful
learning principles from both the probabilistic and
large margin frameworks, we consider the prob-
lem of learning a dependency parser for Chinese.
This is an interesting test domain because Chinese
does not have clearly defined parts-of-speech, which
makes lexical smoothing one of the most natural ap-
proaches to achieving reasonable results (Wang et
al., 2005).
2 Lexicalized Dependency Parsing
A dependency tree specifies which words in a sen-
tence are directly related. That is, the dependency
structure of a sentence is a directed tree where the
nodes are the words in the sentence and links rep-
resent the direct dependency relationships between
the words; see Figure 1. There has been a grow-
ing interest in dependency parsing in recent years.
(Fox, 2002) found that the dependency structures
of a pair of translated sentences have a greater de-
gree of cohesion than phrase structures. (Cherry and
Lin, 2003) exploited such cohesion between the de-
pendency structures to improve the quality of word
alignment of parallel sentences. Dependency rela-
tions have also been found to be useful in informa-
tion extraction (Culotta and Sorensen, 2004; Yan-
garber et al, 2000).
A key aspect of a dependency tree is that it does
not necessarily report parts-of-speech or phrase la-
bels. Not requiring parts-of-speech is especially
beneficial for languages such as Chinese, where
parts-of-speech are not as clearly defined as En-
glish. In Chinese, clear indicators of a word?s part-
of-speech such as suffixes ?-ment?, ?-ous? or func-
tion words such as ?the?, are largely absent. One
of our motivating goals is to develop an approach to
learning dependency parsers that is strictly lexical.
Hence the parser can be trained with a treebank that
only contains the dependency relationships, making
annotation much easier.
Of course, training a parser with bare word-to-
word relationships presents a serious challenge due
to data sparseness. It was found in (Bikel, 2004) that
Collins? parser made use of bi-lexical statistics only
1.49% of the time. The parser has to compute back-
off probability using parts-of-speech in vast majority
of the cases. In fact, it was found in (Gildea, 2001)
that the removal of bi-lexical statistics from a state
of the art PCFG parser resulted in very little change
in the output. (Klein and Manning, 2003) presented
an unlexicalized parser that eliminated all lexical-
ized parameters. Its performance was close to the
state of the art lexicalized parsers.
Nevertheless, in this paper we follow the re-
cent work of (Wang et al, 2005) and consider a
completely lexicalized parser that uses no parts-of-
speech or grammatical categories of any kind. Even
though a part-of-speech lexicon has always been
considered to be necessary in any natural language
parser, (Wang et al, 2005) showed that distributional
word similarities from a large unannotated corpus
can be used to supplant part-of-speech smoothing
with word similarity smoothing, to still achieve state
of the art dependency parsing accuracy for Chinese.
Before discussing our modifications to large mar-
gin training for parsing in detail, we first present the
dependency parsing model we use. We then give
a brief overview of large margin training, and then
present our two modifications. Subsequently, we
present our experimental results on fully lexical de-
pendency parsing for Chinese.
3 Dependency Parsing Model
Given a sentence   
		 we are in-
terested in computing a directed dependency tree,
22
 , over  . In particular, we assume that a di-
rected dependency tree  consists of ordered pairs
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 114?115,
New York City, June 2006. c?2006 Association for Computational Linguistics
Biomedical Term Recognition
With the Perceptron HMM Algorithm
Sittichai Jiampojamarn and Grzegorz Kondrak and Colin Cherry
Department of Computing Science,
University of Alberta,
Edmonton, AB, T6G 2E8, Canada
{sj,kondrak,colinc}@cs.ualberta.ca
Abstract
We propose a novel approach to the iden-
tification of biomedical terms in research
publications using the Perceptron HMM
algorithm. Each important term is iden-
tified and classified into a biomedical con-
cept class. Our proposed system achieves
a 68.6% F-measure based on 2,000 train-
ing Medline abstracts and 404 unseen
testing Medline abstracts. The system
achieves performance that is close to the
state-of-the-art using only a small feature
set. The Perceptron HMM algorithm pro-
vides an easy way to incorporate many po-
tentially interdependent features.
1 Introduction
Every day, new scientific articles in the biomedi-
cal field are published and made available on-line.
The articles contain many new terms and names
involving proteins, DNA, RNA, and a wide vari-
ety of other substances. Given the large volume of
the new research articles, it is important to develop
systems capable of extracting meaningful relation-
ships between substances from these articles. Such
systems need to recognize and identify biomedical
terms in unstructured texts. Biomedical term recog-
nition is thus a step towards information extraction
from biomedical texts.
The term recognition task aims at locating
biomedical terminology in unstructured texts. The
texts are unannotated biomedical research publica-
tions written in English. Meaningful terms, which
may comprise several words, are identified in order
to facilitate further text mining tasks. The recogni-
tion task we consider here also involves term clas-
sification, that is, classifying the identified terms
into biomedical concepts: proteins, DNA, RNA, cell
types, and cell lines.
Our biomedical term recognition task is defined
as follows: given a set of documents, in each docu-
ment, find and mark each occurrence of a biomedi-
cal term. A term is considered to be annotated cor-
rectly only if all its composite words are annotated
correctly. Precision, recall and F-measure are deter-
mined by comparing the identified terms against the
terms annotated in the gold standard.
We believe that the biomedical term recogni-
tion task can only be adequately addressed with
machine-learning methods. A straightforward dic-
tionary look-up method is bound to fail because
of the term variations in the text, especially when
the task focuses on locating exact term boundaries.
Rule-based systems can achieve good performance
on small data sets, but the rules must be defined
manually by domain experts, and are difficult to
adapt to other data sets. Systems based on machine-
learning employ statistical techniques, and can be
easily re-trained on different data. The machine-
learning techniques used for this task can be divided
into two main approaches: the word-based methods,
which annotate each word without taking previous
assigned tags into account, and the sequence based
methods, which take other annotation decisions into
account in order to decide on the tag for the current
word.
We propose a biomedical term identification
114
system based on the Perceptron HMM algo-
rithm (Collins, 2004), a novel algorithm for HMM
training. It uses the Viterbi and perceptron algo-
rithms to replace a traditional HMM?s conditional
probabilities with discriminatively trained parame-
ters. The method has been successfully applied to
various tasks, including noun phrase chunking and
part-of-speech tagging. The perceptron makes it
possible to incorporate discriminative training into
the traditional HMM approach, and to augment it
with additional features, which are helpful in rec-
ognizing biomedical terms, as was demonstrated in
the ABTA system (Jiampojamarn et al, 2005). A
discriminative method allows us to incorporate these
features without concern for feature interdependen-
cies. The Perceptron HMM provides an easy and
effective learning algorithm for this purpose.
The features used in our system include the part-
of-speech tag information, orthographic patterns,
word prefix and suffix character strings. The ad-
ditional features are the word, IOB and class fea-
tures. The orthographic features encode the spelling
characteristics of a word, such as uppercase letters,
lowercase letters, digits, and symbols. The IOB and
class features encode the IOB tags associated with
biomedical class concept markers.
2 Results and discussion
We evaluated our system on the JNLPBA Bio-Entity
recognition task. The training data set contains
2,000 Medline abstracts labeled with biomedical
classes in the IOB style. The IOB annotation method
utilizes three types of tags: <B> for the beginning
word of a term, <I> for the remaining words of a
term, and <O> for non-term words. For the purpose
of term classification, the IOB tags are augmented
with the names of the biomedical classes; for ex-
ample, <B-protein> indicates the first word of
a protein term. The held-out set was constructed
by randomly selecting 10% of the sentences from
the available training set. The number of iterations
for training was determined by observing the point
where the performance on the held-out set starts to
level off. The test set is composed of new 404 Med-
line abstracts.
Table 1 shows the results of our system on all five
classes. In terms of F-measure, our system achieves
Class Recall Precision F-measure
protein 76.73 % 65.56 % 70.71 %
DNA 63.07 % 64.47 % 63.76 %
RNA 64.41 % 59.84 % 62.04 %
cell type 64.71 % 76.35 % 70.05 %
cell line 54.20 % 52.02 % 53.09 %
ALL 70.93 % 66.50 % 68.64 %
Table 1: The performance of our system on the test
set with respect to each biomedical concept class.
the average of 68.6%, which a substantial improve-
ment over the baseline system (based on longest
string matching against a lists of terms from train-
ing data) with the average of 47.7%, and over the
basic HMM system, with the average of 53.9%. In
comparison with the results of eight participants at
the JNLPBA shared tasks (Kim et al, 2004), our
system ranks fourth. The performance gap between
our system and the best systems at JNLPBA, which
achieved the average up to 72.6%, can be attributed
to the use of richer and more complete features such
as dictionaries and Gene ontology.
3 Conclusion
We have proposed a new approach to the biomedical
term recognition task using the Perceptron HMM al-
gorithm. Our proposed system achieves a 68.6% F-
measure with a relatively small number of features
as compared to the systems of the JNLPBA partici-
pants. The Perceptron HMM algorithm is much eas-
ier to implement than the SVM-HMMs, CRF, and
the Maximum Entropy Markov Models, while the
performance is comparable to those approaches. In
the future, we plan to experiment with incorporat-
ing external resources, such as dictionaries and gene
ontologies, into our feature set.
References
M. Collins. 2004. Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms. In Proceedings of EMNLP.
S. Jiampojamarn, N. Cercone, and V. Keselj. 2005. Bi-ological named entity recognition using n-grams andclassification methods. In Proceedings of PACLING.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier.
2004. Introduction to the bio-entity recognition task atJNLPBA. In Proceedings of JNLPBA.
115
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 17?24,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Inversion Transduction Grammar for Joint Phrasal Translation Modeling
Colin Cherry
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
colinc@cs.ualberta.ca
Dekang Lin
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA, USA, 9403
lindek@google.com
Abstract
We present a phrasal inversion trans-
duction grammar as an alternative to
joint phrasal translation models. This
syntactic model is similar to its flat-
string phrasal predecessors, but admits
polynomial-time algorithms for Viterbi
alignment and EM training. We demon-
strate that the consistency constraints that
allow flat phrasal models to scale also help
ITG algorithms, producing an 80-times
faster inside-outside algorithm. We also
show that the phrasal translation tables
produced by the ITG are superior to those
of the flat joint phrasal model, producing
up to a 2.5 point improvement in BLEU
score. Finally, we explore, for the first
time, the utility of a joint phrasal transla-
tion model as a word alignment method.
1 Introduction
Statistical machine translation benefits greatly from
considering more than one word at a time. One
can put forward any number of non-compositional
translations to support this point, such as the col-
loquial Canadian French-English pair, (Wo les mo-
teurs, Hold your horses), where no clear word-to-
word connection can be drawn. Nearly all cur-
rent decoding methods have shifted to phrasal rep-
resentations, gaining the ability to handle non-
compositional translations, but also allowing the de-
coder to memorize phenomena such as monolingual
agreement and short-range movement, taking pres-
sure off of language and distortion models.
Despite the success of phrasal decoders, knowl-
edge acquisition for translation generally begins
with a word-level analysis of the training text, tak-
ing the form of a word alignment. Attempts to apply
the same statistical analysis used at the word level
in a phrasal setting have met with limited success,
held back by the sheer size of phrasal alignment
space. Hybrid methods that combine well-founded
statistical analysis with high-confidence word-level
alignments have made some headway (Birch et al,
2006), but suffer from the daunting task of heuris-
tically exploring a still very large alignment space.
In the meantime, synchronous parsing methods effi-
ciently process the same bitext phrases while build-
ing their bilingual constituents, but continue to be
employed primarily for word-to-word analysis (Wu,
1997). In this paper we unify the probability models
for phrasal translation with the algorithms for syn-
chronous parsing, harnessing the benefits of both
to create a statistically and algorithmically well-
founded method for phrasal analysis of bitext.
Section 2 begins by outlining the phrase extrac-
tion system we intend to replace and the two meth-
ods we combine to do so: the joint phrasal transla-
tion model (JPTM) and inversion transduction gram-
mar (ITG). Section 3 describes our proposed solu-
tion, a phrasal ITG. Section 4 describes how to ap-
ply our phrasal ITG, both as a translation model and
as a phrasal word-aligner. Section 5 tests our system
in both these capacities, while Section 6 concludes.
2 Background
2.1 Phrase Table Extraction
Phrasal decoders require a phrase table (Koehn et
al., 2003), which contains bilingual phrase pairs and
17
scores indicating their utility. The surface heuris-
tic is the most popular method for phrase-table con-
struction. It extracts all consistent phrase pairs from
word-aligned bitext (Koehn et al, 2003). The word
alignment provides bilingual links, indicating trans-
lation relationships between words. Consistency is
defined so that alignment links are never broken by
phrase boundaries. For each token w in a consistent
phrase pair p?, all tokens linked tow by the alignment
must also be included in p?. Each consistent phrase
pair is counted as occurring once per sentence pair.
The scores for the extracted phrase pairs are pro-
vided by normalizing these flat counts according to
common English or Foreign components, producing
the conditional distributions p(f? |e?) and p(e?|f?).
The surface heuristic can define consistency ac-
cording to any word alignment; but most often, the
alignment is provided by GIZA++ (Och and Ney,
2003). This alignment system is powered by the
IBM translation models (Brown et al, 1993), in
which one sentence generates the other. These mod-
els produce only one-to-many alignments: each gen-
erated token can participate in at most one link.
Many-to-many alignments can be created by com-
bining two GIZA++ alignments, one where English
generates Foreign and another with those roles re-
versed (Och and Ney, 2003). Combination ap-
proaches begin with the intersection of the two
alignments, and add links from the union heuris-
tically. The grow-diag-final (GDF) combination
heuristic (Koehn et al, 2003) adds links so that each
new link connects a previously unlinked token.
2.2 Joint phrasal translation model
The IBM models that power GIZA++ are trained
with Expectation Maximization (Dempster et al,
1977), or EM, on sentence-aligned bitext. A transla-
tion model assigns probabilities to alignments; these
alignment distributions are used to count translation
events, which are then used to estimate new parame-
ters for the translation model. Sampling is employed
when the alignment distributions cannot be calcu-
lated efficiently. This statistically-motivated process
is much more appealing than the flat counting de-
scribed in Section 2.1, but it does not directly in-
clude phrases.
The joint phrasal translation model (Marcu and
Wong, 2002), or JPTM, applies the same statistical
techniques from the IBMmodels in a phrasal setting.
The JPTM is designed according to a generative pro-
cess where both languages are generated simultane-
ously. First, a bag of concepts, or cepts, C is gener-
ated. Each ci ? C corresponds to a bilingual phrase
pair, ci = (e?i, f?i). These contiguous phrases are
permuted in each language to create two sequences
of phrases. Initially, Marcu and Wong assume that
the number of cepts, as well as the phrase orderings,
are drawn from uniform distributions. That leaves
a joint translation distribution p(e?i, f?i) to determine
which phrase pairs are selected. Given a lexicon of
possible cepts and a predicate L(E,F,C) that de-
termines if a bag of cepts C can be bilingually per-
muted to create the sentence pair (E, F ), the proba-
bility of a sentence pair is:
p(E,F ) ?
?
{C|L(E,F,C)}
?
?
?
ci?C
p(e?i, f?i)
?
? (1)
If left unconstrained, (1) will consider every phrasal
segmentation of E and F , and every alignment be-
tween those phrases. Later, a distortion model based
on absolute token positions is added to (1).
The JPTM faces several problems when scaling
up to large training sets:
1. The alignment space enumerated by the sum
in (1) is huge, far larger than the one-to-many
space explored by GIZA++.
2. The translation distribution p(e?, f?) will cover
all co-occurring phrases observed in the bitext.
This is far too large to fit in main memory, and
can be unwieldly for storage on disk.
3. Given a non-uniform p(e?, f?), there is no effi-
cient algorithm to compute the expectation of
phrase pair counts required for EM, or to find
the most likely phrasal alignment.
Marcu and Wong (2002) address point 2 with a lexi-
con constraint; monolingual phrases that are above
a length threshold or below a frequency threshold
are excluded from the lexicon. Point 3 is handled
by hill-climbing to a likely phrasal alignment and
sampling around it. However, point 1 remains unad-
dressed, which prevents the model from scaling to
large data sets.
Birch et al (2006) handle point 1 directly by re-
ducing the size of the alignment space. This is
18
accomplished by constraining the JPTM to only
use phrase pairs that are consistent with a high-
confidence word alignment, which is provided by
GIZA++ intersection. We refer to this constrained
JPTM as a C-JPTM. This strikes an interesting
middle ground between the surface heuristic de-
scribed in Section 2.1 and the JPTM. Like the sur-
face heuristic, a word alignment is used to limit the
phrase pairs considered, but the C-JPTM reasons
about distributions over phrasal alignments, instead
of taking flat counts. The consistency constraint al-
lows them to scale their C-JPTM up to 700,000 sen-
tence pairs. With this constraint in place, the use of
hill-climbing and sampling during EM training be-
comes one of the largest remaining weaknesses of
the C-JPTM.
2.3 Inversion Transduction Grammar
Like the JPTM, stochastic synchronous grammars
provide a generative process to produce a sentence
and its translation simultaneously. Inversion trans-
duction grammar (Wu, 1997), or ITG, is a well-
studied synchronous grammar formalism. Terminal
productions of the form A ? e/f produce a to-
ken in each stream, or a token in one stream with
the null symbol ? in the other. To allow for move-
ment during translation, non-terminal productions
can be either straight or inverted. Straight produc-
tions, with their non-terminals inside square brack-
ets [. . .], produce their symbols in the given order in
both streams. Inverted productions, indicated by an-
gled brackets ?. . .?, are output in reverse order in the
Foreign stream only.
The work described here uses the binary bracket-
ing ITG, which has a single non-terminal:
A ? [AA] | ?AA? | e/f (2)
This grammar admits an efficient bitext parsing al-
gorithm, and holds no language-specific biases.
(2) cannot represent all possible permutations of
concepts that may occur during translation, because
some permutations will require discontinuous con-
stituents (Melamed, 2003). This ITG constraint is
characterized by the two forbidden structures shown
in Figure 1 (Wu, 1997). Empirical studies suggest
that only a small percentage of human translations
violate these constraints (Cherry and Lin, 2006).
e1
e2
e3
e4
f1 f2 f3 f4 f1 f2 f3 f4
e1
e2
e3
e4
Figure 1: The two ITG forbidden structures.
calmez vous
c
a
l
m
d
o
w
n
calmez vous
c
a
l
m
d
o
w
n
calmez vous
c
a
l
m
d
o
w
n
a) A?[AA] b) A?<AA>
c) A?e/f
Figure 2: Three ways in which a phrasal ITG can
analyze a multi-word span or phrase.
Stochastic ITGs are parameterized like their
PCFG counterparts (Wu, 1997); productions
A ? X are assigned probability Pr(X|A). These
parameters can be learned from sentence-aligned bi-
text using the EM algorithm. The expectation task
of counting productions weighted by their probabil-
ity is handled with dynamic programming, using the
inside-outside algorithm extended to bitext (Zhang
and Gildea, 2004).
3 ITG as a Phrasal Translation Model
This paper introduces a phrasal ITG; in doing so,
we combine ITG with the JPTM. ITG parsing al-
gorithms consider every possible two-dimensional
span of bitext, each corresponding to a bilingual
phrase pair. Each multi-token span is analyzed in
terms of how it could be built from smaller spans us-
ing a straight or inverted production, as is illustrated
in Figures 2 (a) and (b). To extend ITG to a phrasal
setting, we add a third option for span analysis: that
the span under consideration might have been drawn
directly from the lexicon. This option can be added
to our grammar by altering the definition of a termi-
nal production to include phrases: A ? e?/f? . This
third option is shown in Figure 2 (c). The model
implied by this extended grammar is trained using
inside-outside and EM.
Our approach differs from previous attempts to
use ITGs for phrasal bitext analysis. Wu (1997)
used a binary bracketing ITG to segment a sen-
19
tence while simultaneously word-aligning it to its
translation, but the model was trained heuristically
with a fixed segmentation. Vilar and Vidal (2005)
used ITG-like dynamic programming to drive both
training and alignment for their recursive translation
model, but they employed a conditional model that
did not maintain a phrasal lexicon. Instead, they
scored phrase pairs using IBM Model 1.
Our phrasal ITG is quite similar to the JPTM.
Both models are trained with EM, and both em-
ploy generative stories that create a sentence and its
translation simultaneously. The similarities become
more apparent when we consider the canonical-form
binary-bracketing ITG (Wu, 1997) shown here:
S ? A | B | C
A ? [AB] | [BB] | [CB] |
[AC] | [BC] | [CC]
B ? ?AA? | ?BA? | ?CA? |
?AC? | ?BC? | ?CC?
C ? e?/f?
(3)
(3) is employed in place of (2) to reduce redundant
alignments and clean up EM expectations.1 More
importantly for our purposes, it introduces a preter-
minal C, which generates all phrase pairs or cepts.
When (3) is parameterized as a stochastic ITG, the
conditional distribution p(e?/f? |C) is equivalent to
the JPTM?s p(e?, f?); both are joint distributions over
all possible phrase pairs. The distributions condi-
tioned on the remaining three non-terminals assign
probability to concept movement by tracking inver-
sions. Like the JPTM?s distortion model, these pa-
rameters grade each movement decision indepen-
dently. With terminal productions producing cepts,
and inversions measuring distortion, our phrasal ITG
is essentially a variation on the JPTM with an alter-
nate distortion model.
Our phrasal ITG has two main advantages over
the JPTM. Most significantly, we gain polynomial-
time algorithms for both Viterbi alignment and EM
expectation, through the use of ITG parsing and
inside-outside algorithms. These phrasal ITG algo-
rithms are no more expensive asymptotically than
their word-to-word counterparts, since each poten-
tial phrase needs to be analyzed anyway during
1If the null symbol ? is included among the terminals, then
redundant parses will still occur, but far less frequently.
constituent construction. We hypothesize that us-
ing these methods in place of heuristic search and
sampling will improve the phrasal translation model
learned by EM. Also, we can easily incorporate links
to ? by including the symbol among our terminals.
To minimize redundancy, we allow only single to-
kens, not phrases, to align to ?. The JPTM does not
allow links to ?.
The phrasal ITG also introduces two new compli-
cations. ITG Viterbi and inside-outside algorithms
have polynomial complexity, but that polynomial is
O(n6), where n is the length of the longer sentence
in the pair. This is too slow to train on large data
sets without massive parallelization. Also, ITG al-
gorithms explore their alignment space perfectly, but
that space has been reduced by the ITG constraint
described in Section 2.3. We will address each of
these issues in the following two subsections.
3.1 Pruning Spans
First, we address the problem of scaling ITG to large
data. ITG dynamic programming algorithms work
by analyzing each bitext span only once, storing its
value in a table for future use. There are O(n4) of
these spans, and each analysis takes O(n2) time. An
effective approach to speeding up ITG algorithms
is to eliminate unlikely spans as a preprocessing
step, assigning them 0 probability and saving the
time spent processing them. Past approaches have
pruned spans using IBM Model 1 probability esti-
mates (Zhang and Gildea, 2005) or using agreement
with an existing parse tree (Cherry and Lin, 2006).
The former is referred to as tic-tac-toe pruning be-
cause it uses both inside and outside estimates.
We propose a new ITG pruning method that lever-
ages high-confidence links by pruning all spans that
are inconsistent with a provided alignment. This
is similar to the constraint used in the C-JPTM,
but we do not just eliminate those spans as poten-
tial phrase-to-phrase links: we never consider any
ITG parse that builds a non-terminal over a pruned
span.2 This fixed-link pruning will speed up both
Viterbi alignment and EM training by reducing the
number of analyzed spans, and so long as we trust
2Birch et al (2006) re-introduce inconsistent phrase-pairs in
cases where the sentence pair could not be aligned otherwise.
We allow links to ? to handle these situations, completely elim-
inating the pruned spans from our alignment space.
20
our high-confidence links, it will do so harmlessly.
We demonstrate the effectiveness of this pruning
method experimentally in Section 5.1.
3.2 Handling the ITG Constraint
Our remaining concern is the ITG constraint. There
are some alignments that we just cannot build, and
sentence pairs requiring those alignments will occur.
These could potentially pollute our training data; if
the system is unable to build the right alignment, the
counts it will collect from that pair must be wrong.
Furthermore, if our high-confidence links are not
ITG-compatible, our fixed-link pruning will prevent
the aligner from forming any alignments at all.
However, these two potential problems cancel
each other out. Sentence pairs containing non-ITG
translations will tend to have high-confidence links
that are also not ITG-compatible. Our EM learner
will simply skip these sentence pairs during train-
ing, avoiding pollution of our training data. We can
use a linear-time algorithm (Zhang et al, 2006) to
detect non-ITG movement in our high-confidence
links, and remove the offending sentence pairs from
our training corpus. This results in only a minor re-
duction in training data; in our French-English train-
ing set, we lose less than 1%. In the experiments de-
scribed in Section 5, all systems that do not use ITG
will take advantage of the complete training set.
4 Applying the model
Any phrasal translation model can be used for two
tasks: translation modeling and phrasal word align-
ment. Previous work on JPTM has focused on only
the first task. We are interested in phrasal alignment
because it may be better suited to heuristic phrase-
extraction than word-based models. This section de-
scribes how to use our phrasal ITG first as a transla-
tion model, and then as a phrasal aligner.
4.1 Translation Modeling
We can test our model?s utility for translation by
transforming its parameters into a phrase table for
the phrasal decoder Pharaoh (Koehn et al, 2003).
Any joint model can produce the necessary condi-
tional probabilities by conditionalizing the joint ta-
ble in both directions. We use our p(e?/f? |C) dis-
tribution from our stochastic grammar to produce
p(e?|f?) and p(f? |e?) values for its phrasal lexicon.
Pharaoh also includes lexical weighting param-
eters that are derived from the alignments used to
induce its phrase pairs (Koehn et al, 2003). Us-
ing the phrasal ITG as a direct translation model,
we do not produce alignments for individual sen-
tence pairs. Instead, we provide a lexical preference
with an IBM Model 1 feature pM1 that penalizes un-
matched words (Vogel et al, 2003). We include both
pM1(e?|f?) and pM1(f? |e?).
4.2 Phrasal Word Alignment
We can produce a translation model using inside-
outside, without ever creating a Viterbi parse. How-
ever, we can also examine the maximum likelihood
phrasal alignments predicted by the trained model.
Despite its strengths derived from using phrases
throughout training, the alignments predicted by our
phrasal ITG are usually unsatisfying. For exam-
ple, the fragment pair (order of business, ordre des
travaux) is aligned as a phrase pair by our system,
linking every English word to every French word.
This is frustrating, since there is a clear compo-
sitional relationship between the fragment?s com-
ponent words. This happens because the system
seeks only to maximize the likelihood of its train-
ing corpus, and phrases are far more efficient than
word-to-word connections. When aligning text, an-
notators are told to resort to many-to-many links
only when no clear compositional relationship ex-
ists (Melamed, 1998). If we could tell our phrasal
aligner the same thing, we could greatly improve the
intuitive appeal of our alignments. Again, we can
leverage high-confidence links for help.
In the high-confidence alignments provided by
GIZA++ intersection, each token participates in at
most one link. Links only appear when two word-
based IBM translation models can agree. Therefore,
they occur at points of high compositionality: the
two words clearly account for one another. We adopt
an alignment-driven definition of compositional-
ity: any phrase pair containing two or more high-
confidence links is compositional, and can be sep-
arated into at least two non-compositional phrases.
By removing any phrase pairs that are compositional
by this definition from our terminal productions,
we can ensure that our aligner never creates such
phrases during training or alignment. Doing so pro-
duces far more intuitive alignments. Aligned with
21
a model trained using this non-compositional con-
straint (NCC), our example now forms three word-
to-word connections, rather than a single phrasal
one. The phrases produced with this constraint are
very small, and include only non-compositional con-
text. Therefore, we use the constraint only to train
models intended for Viterbi alignment, and not when
generating phrase tables directly as in Section 4.1.
5 Experiments and Results
In this section, we first verify the effectiveness of
fixed-link pruning, and then test our phrasal ITG,
both as an aligner and as a translation model. We
train all translation models with a French-English
Europarl corpus obtained by applying a 25 to-
ken sentence-length limit to the training set pro-
vided for the HLT-NAACL SMT Workshop Shared
Task (Koehn and Monz, 2006). The resulting cor-
pus has 393,132 sentence pairs. 3,376 of these
are omitted for ITG methods because their high-
confidence alignments have ITG-incompatible con-
structions. Like our predecessors (Marcu and Wong,
2002; Birch et al, 2006), we apply a lexicon con-
straint: no monolingual phrase can be used by any
phrasal model unless it occurs at least five times.
High-confidence alignments are provided by inter-
secting GIZA++ alignments trained in each direc-
tion with 5 iterations each of Model 1, HMM, and
Model 4. All GIZA++ alignments are trained with
no sentence-length limit, using the full 688K corpus.
5.1 Pruning Speed Experiments
To measure the speed-up provided by fixed-link
pruning, we timed our phrasal inside-outside algo-
rithm on the first 100 sentence pairs in our training
set, with and without pruning. The results are shown
in Table 1. Tic-tac-toe pruning is included for com-
parison. With fixed-link pruning, on average 95%
of the possible spans are pruned, reducing running
time by two orders of magnitude. This improvement
makes ITG training feasible, even with large bitexts.
5.2 Alignment Experiments
The goal of this experiment is to compare the Viterbi
alignments from the phrasal ITG to gold standard
human alignments. We do this to validate our non-
compositional constraint and to select good align-
ments for use with the surface heuristic.
Table 1: Inside-outside run-time comparison.
Method Seconds Avg. Spans Pruned
No Prune 415 -
Tic-tac-toe 37 68%
Fixed link 5 95%
Table 2: Alignment Comparison.
Method Prec Rec F-measure
GIZA++ Intersect 96.7 53.0 68.5
GIZA++ Union 82.5 69.0 75.1
GIZA++ GDF 84.0 68.2 75.2
Phrasal ITG 50.7 80.3 62.2
Phrasal ITG + NCC 75.4 78.0 76.7
Following the lead of (Fraser and Marcu, 2006),
we hand-aligned the first 100 sentence pairs of
our training set according to the Blinker annota-
tion guidelines (Melamed, 1998). We did not dif-
ferentiate between sure and possible links. We re-
port precision, recall and balanced F-measure (Och
and Ney, 2003). For comparison purposes, we in-
clude the results of three types of GIZA++ combina-
tion, including the grow-diag-final heuristic (GDF).
We tested our phrasal ITG with fixed link prun-
ing, and then added the non-compositional con-
straint (NCC). During development we determined
that performance levels off for both of the ITG mod-
els after 3 EM iterations. The results are shown in
Table 2.
The first thing to note is that GIZA++ Intersection
is indeed very high precision. Our confidence in it
as a constraint is not misplaced. We also see that
both phrasal models have significantly higher recall
than any of the GIZA++ alignments, even higher
than the permissive GIZA++ union. One factor con-
tributing to this is the phrasal model?s use of cepts:
it completely interconnects any phrase pair, while
GIZA++ union and GDF may not. Its global view
of phrases also helps in this regard: evidence for a
phrase can be built up over multiple sentences. Fi-
nally, we note that in terms of alignment quality,
the non-compositional constraint is an unqualified
success for the phrasal ITG. It produces a 25 point
improvement in precision, at the cost of 2 points
22
of recall. This produces the highest balanced F-
measure observed on our test set, but the utility of
its alignments will depend largely on one?s desired
precision-recall trade-off.
5.3 Translation Experiments
In this section, we compare a number of different
methods for phrase table generation in a French to
English translation task. We are interested in an-
swering three questions:
1. Does the phrasal ITG improve on the C-JPTM?
2. Can phrasal translation models outperform the
surface heuristic?
3. Do Viterbi phrasal alignments provide better
input for the surface heuristic?
With this in mind, we test five phrase tables. Two
are conditionalized phrasal models, each EM trained
until performance degrades:
? C-JPTM3 as described in (Birch et al, 2006)
? Phrasal ITG as described in Section 4.1
Three provide alignments for the surface heuristic:
? GIZA++ with grow-diag-final (GDF)
? Viterbi Phrasal ITG with and without the non-
compositional constraint
We use the Pharaoh decoder (Koehn et al, 2003)
with the SMT Shared Task baseline system (Koehn
and Monz, 2006). Weights for the log-linear model
are set using the 500-sentence tuning set provided
for the shared task with minimum error rate train-
ing (Och, 2003) as implemented by Venugopal
and Vogel (2005). Results on the provided 2000-
sentence development set are reported using the
BLEU metric (Papineni et al, 2002). For all meth-
ods, we report performance with and without IBM
Model 1 features (M1), along with the size of the re-
sulting tables in millions of phrase pairs. The results
of all experiments are shown in Table 3.
We see that the Phrasal ITG surpasses the C-
JPTM by more than 2.5 BLEU points. A large com-
ponent of this improvement is due to the ITG?s use
of inside-outside for expectation calculation, though
3Supplied by personal communication. Run with default pa-
rameters, but with maximum phrase length increased to 5.
Table 3: Translation Comparison.
Method BLEU +M1 Size
Conditionalized Phrasal Model
C-JPTM 26.27 28.98 1.3M
Phrasal ITG 28.85 30.24 2.2M
Alignment with Surface Heuristic
GIZA++ GDF 30.46 30.61 9.8M
Phrasal ITG 30.31 30.39 5.8M
Phrasal ITG + NCC 30.66 30.80 9.0M
there are other differences between the two sys-
tems.4 This improvement over search and sampling
is demonstrated by the ITG?s larger table size; by ex-
ploring more thoroughly, it is extracting more phrase
pairs from the same amount of data. Both systems
improve drastically with the addition of IBM Model
1 features for lexical preference. These features also
narrow the gap between the two systems. To help
calibrate the contribution of these features, we pa-
rameterized the ITG?s phrase table using only Model
1 features, which scores 27.17.
Although ITG+M1 comes close, neither phrasal
model matches the performance of the surface
heuristic. Whatever the surface heuristic lacks in
sophistication, it makes up for in sheer coverage,
as demonstrated by its huge table sizes. Even the
Phrasal ITG Viterbi alignments, which over-commit
wildly and have horrible precision, score slightly
higher than the best phrasal model. The surface
heuristic benefits from capturing as much context
as possible, while still covering smaller translation
events with its flat counts. It is not held back by
any lexicon constraints. When GIZA++ GDF+M1
is forced to conform to a lexicon constraint by drop-
ping any phrase with a frequency lower than 5 from
its table, it scores only 29.26, for a reduction of 1.35
BLEU points.
Phrases extracted from our non-compositional
Viterbi alignments receive the highest BLEU score,
but they are not significantly better than GIZA++
GDF. The two methods also produce similarly-sized
tables, despite the ITG?s higher recall.
4Unlike our system, the Birch implementation does table
smoothing and internal lexical weighting, both of which should
help improve their results. The systems also differ in distortion
modeling and ? handling, as described in Section 3.
23
6 Conclusion
We have presented a phrasal ITG as an alternative
to the joint phrasal translation model. This syntactic
solution to phrase modeling admits polynomial-time
training and alignment algorithms. We demonstrate
that the same consistency constraints that allow joint
phrasal models to scale also dramatically speed up
ITGs, producing an 80-times faster inside-outside
algorithm. We show that when used to learn phrase
tables for the Pharaoh decoder, the phrasal ITG is
superior to the constrained joint phrasal model, pro-
ducing tables that result in a 2.5 point improve-
ment in BLEU when used alone, and a 1 point im-
provement when used with IBM Model 1 features.
This suggests that ITG?s perfect expectation count-
ing does matter; other phrasal models could benefit
from either adopting the ITG formalism, or improv-
ing their sampling heuristics.
We have explored, for the first time, the utility of a
joint phrasal model as a word alignment method. We
present a non-compositional constraint that turns the
phrasal ITG into a high-recall phrasal aligner with
an F-measure that is comparable to GIZA++.
With search and sampling no longer a concern,
the remaining weaknesses of the system seem to lie
with the model itself. Phrases are just too efficient
probabilistically: were we to remove all lexicon con-
straints, EM would always align entire sentences to
entire sentences. This pressure to always build the
longest phrase possible may be overwhelming oth-
erwise strong correlations in our training data. A
promising next step would be to develop a prior over
lexicon size or phrase size, allowing EM to intro-
duce large phrases at a penalty, and removing the
need for artificial constraints on the lexicon.
Acknowledgments Special thanks to Alexandra
Birch for the use of her code, and to our reviewers
for their comments. The first author is funded by
Alberta Ingenuity and iCORE studentships.
References
A. Birch, C. Callison-Burch, M. Osborne, and P. Koehn. 2006.
Constraining the phrase-based, joint probability statistical
translation model. In HLT-NAACL Workshop on Statistical
Machine Translation, pages 154?157.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263?312.
C. Cherry and D. Lin. 2006. A comparison of syntactically
motivated word alignment spaces. In EACL, pages 145?152.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, 39(1):1?38.
A. Fraser and D. Marcu. 2006. Semi-supervised training for
statistical word alignment. In ACL, pages 769?776.
P. Koehn and C. Monz. 2006. Manual and automatic evalu-
ation of machine translation. In HLT-NACCL Workshop on
Statistical Machine Translation, pages 102?121.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In HLT-NAACL, pages 127?133.
D. Marcu and W. Wong. 2002. A phrase-based, joint probabil-
ity model for statistic machine translation. In EMNLP, pages
133?139.
I. D. Melamed. 1998. Manual annotation of translational
equivalence: The blinker project. Technical Report 98-07,
Institute for Research in Cognitive Science.
I. D. Melamed. 2003. Multitext grammars and synchronous
parsers. In HLT-NAACL, pages 158?165.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19?52.
F. J. Och. 2003. Minimum error rate training for statistical
machine translation. In ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU:
a method for automatic evaluation of machine translation. In
ACL, pages 311?318.
A. Venugopal and S. Vogel. 2005. Considerations in maximum
mutual information and minimum classification error train-
ing for statistical machine translation. In EAMT.
J. M. Vilar and E. Vidal. 2005. A recursive statistical transla-
tion model. In Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts, pages 199?207.
S. Vogel, Y. Zhang, F. Huang, A. Tribble, A. Venugopal,
B. Zhang, and A. Waibel. 2003. The CMU statistical ma-
chine translation system. In MT Summmit.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?403.
H. Zhang and D. Gildea. 2004. Syntax-based alignment: Su-
pervised or unsupervised? In COLING, pages 418?424.
H. Zhang and D. Gildea. 2005. Stochastic lexicalized inversion
transduction grammar for alignment. In ACL, pages 475?
482.
H. Zhang, L. Huang, D. Gildea, and K. Knight. 2006. Syn-
chronous binarization for machine translation. In HLT-
NAACL, pages 256?263.
24
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 53?61,
Beijing, August 2010
Fast and Accurate Arc Filtering for Dependency Parsing
Shane Bergsma
Department of Computing Science
University of Alberta
sbergsma@ualberta.ca
Colin Cherry
Institute for Information Technology
National Research Council Canada
colin.cherry@nrc-cnrc.gc.ca
Abstract
We propose a series of learned arc fil-
ters to speed up graph-based dependency
parsing. A cascade of filters identify im-
plausible head-modifier pairs, with time
complexity that is first linear, and then
quadratic in the length of the sentence.
The linear filters reliably predict, in con-
text, words that are roots or leaves of de-
pendency trees, and words that are likely
to have heads on their left or right. We
use this information to quickly prune arcs
from the dependency graph. More than
78% of total arcs are pruned while retain-
ing 99.5% of the true dependencies. These
filters improve the speed of two state-of-
the-art dependency parsers, with low over-
head and negligible loss in accuracy.
1 Introduction
Dependency parsing finds direct syntactic rela-
tionships between words by connecting head-
modifier pairs into a tree structure. Depen-
dency information is useful for a wealth of nat-
ural language processing tasks, including ques-
tion answering (Wang et al, 2007), semantic pars-
ing (Poon and Domingos, 2009), and machine
translation (Galley and Manning, 2009).
We propose and test a series of arc filters for
graph-based dependency parsers, which rule out
potential head-modifier pairs before parsing be-
gins. In doing so, we hope to eliminate im-
plausible links early, saving the costs associated
with them, and speeding up parsing. In addi-
tion to the scaling benefits that come with faster
processing, we hope to enable richer features
for parsing by constraining the set of arcs that
need to be considered. This could allow ex-
tremely large feature sets (Koo et al, 2008), or the
look-up of expensive corpus-based features such
as word-pair mutual information (Wang et al,
2006). These filters could also facilitate expen-
sive learning algorithms, such as semi-supervised
approaches (Wang et al, 2008).
We propose three levels of filtering, which are
applied in a sequence of increasing complexity:
Rules: A simple set of machine-learned rules
based only on parts-of-speech. They prune over
25% of potential arcs with almost no loss in cover-
age. Rules save on the wasted effort for assessing
implausible arcs such as DT? DT.
Linear: A series of classifiers that tag words ac-
cording to their possible roles in the dependency
tree. By treating each word independently and en-
suring constant-time feature extraction, they oper-
ate in linear time. We view these as a dependency-
parsing analogue to the span-pruning proposed by
Roark and Hollingshead (2008). Our fast linear
filters prune 54.2% of potential arcs while recov-
ering 99.7% of true pairs.
Quadratic: A final stage that looks at pairs of
words to prune unlikely arcs from the dependency
tree. By employing a light-weight feature set, this
high-precision filter can enable more expensive
processing on the remaining plausible dependen-
cies.
Collectively, we show that more than 78% of
total arcs can be pruned while retaining 99.5% of
the true dependencies. We test the impact of these
filters at both train and test time, using two state-
of-the-art discriminative parsers, demonstrating
speed-ups of between 1.9 and 5.6, with little im-
pact on parsing accuracy.
53
Investors continue to pour cash into money funds
Figure 1: An example dependency parse.
2 Dependency Parsing
A dependency tree represents the syntactic struc-
ture of a sentence as a directed graph (Figure 1),
with a node for each word, and arcs indicat-
ing head-modifier pairs (Me?lc?uk, 1987). Though
dependencies can be extracted from many for-
malisms, there is a growing interest in predict-
ing dependency trees directly. To that end, there
are two dominant approaches: graph-based meth-
ods, characterized by arc features in an exhaus-
tive search, and transition-based methods, char-
acterized by operational features in a greedy
search (McDonald and Nivre, 2007). We focus on
graph-based parsing, as its exhaustive search has
the most to gain from our filters.
Graph-based dependency parsing finds the
highest-scoring tree according to a scoring func-
tion that decomposes under an exhaustive search
(McDonald et al, 2005). The most natural de-
composition scores individual arcs, represented as
head-modifier pairs [h,m]. This enables search
by either minimum spanning tree (West, 2001) or
by Eisner?s (1996) projective parser. This paper
focuses on the projective case, though our tech-
niques transfer to spanning tree parsing. With a
linear scoring function, the parser solves:
parse(s) = argmaxt?s
?
[h,m]?t
w? ? f?(h,m, s)
The weights w? are typically learned using an
online method, such as an averaged percep-
tron (Collins, 2002) or MIRA (Crammer and
Singer, 2003). 2nd-order searches, which consider
two siblings at a time, are available with no in-
crease in asymptotic complexity (McDonald and
Pereira, 2006; Carreras, 2007).
The complexity of graph-based parsing is
bounded by two processes: parsing (carrying out
the argmax) and arc scoring (calculating w? ?
f?(h,m, s)). For a sentence with n words, pro-
jective parsing takes O(n3) time, while the span-
ning tree algorithm is O(n2). Both parsers require
scores for arcs connecting each possible [h,m]
pair in s; therefore, the cost of arc scoring is also
O(n2), and may become O(n3) if the features in-
clude words in s between h and m (Galley and
Manning, 2009). Arc scoring also has a signif-
icant constant term: the number of features ex-
tracted for an [h,m] pair. Our in-house graph-
based parser collects on average 62 features for
each potential arc, a number larger than the length
of most sentences. With the cluster-based features
suggested by Koo et al (2008), this could easily
grow by a factor of 3 or 4.
The high cost of arc scoring, coupled with
the parsing stage?s low grammar constant, means
that graph-based parsers spend much of their time
scoring potential arcs. Johnson (2007) reports that
when arc scores have been precomputed, the dy-
namic programming component of his 1st-order
parser can process an amazing 3,580 sentences per
second.1 Beyond reducing the number of features,
the easiest way to reduce the computational bur-
den of arc scoring is to score only plausible arcs.
3 Related Work
3.1 Vine Parsing
Filtering dependency arcs has been explored pri-
marily in the form of vine parsing (Eisner and
Smith, 2005; Dreyer et al, 2006). Vine pars-
ing establishes that, since most dependencies are
short, one can parse quickly by placing a hard
constraint on arc length. As this coarse fil-
ter quickly degrades the best achievable perfor-
mance, Eisner and Smith (2005) also consider
conditioning the constraint on the part-of-speech
(PoS) tags being linked and the direction of the
arc, resulting in a separate threshold for each
[tag(h), tag(m),dir(h,m)] triple. They sketch
an algorithm where the thresholded length for
each triple starts at the highest value seen in the
training data. Thresholds are then decreased in
a greedy fashion, with each step producing the
smallest possible reduction in reachable training
arcs. We employ this algorithm as a baseline in
our experiments. To our knowledge, vine parsing
1To calibrate this speed, consider that the publicly avail-
able 1st-order MST parser processes 16 sentences per second
on modern hardware. This includes I/O costs in addition to
the costs of arc scoring and parsing.
54
has not previously been tested with a state-of-the-
art, discriminative dependency parser.
3.2 CFG Cell Classification
Roark and Hollingshead (2008) speed up another
exhaustive parsing algorithm, the CKY parser for
CFGs, by classifying each word in the sentence
according to whether it can open (or close) a
multi-word constituent. With a high-precision
tagger that errs on the side of permitting con-
stituents, they show a significant improvement in
speed with no reduction in accuracy.
It is difficult to port their idea directly to depen-
dency parsing without committing to a particular
search algorithm,2 and thereby sacrificing some
of the graph-based formalism?s modularity. How-
ever, some of our linear filters (see Section 4.3)
were inspired by their constraints.
3.3 Coarse-to-fine Parsing
Another common method employed to speed up
exhaustive parsers is a coarse-to-fine approach,
where a cheap, coarse model prunes the search
space for later, more expensive models (Charniak
et al, 2006; Petrov and Klein, 2007). This ap-
proach assumes a common forest or chart repre-
sentation, shared by all granularities, where one
can efficiently track the pruning decisions of the
coarse models. One could imagine applying such
a solution to dependency parsing, but the exact
implementation of the coarse pass would vary ac-
cording to the choice in search algorithm. Our fil-
ters are much more modular: they apply to both
1st-order spanning tree parsing and 2nd-order pro-
jective parsing, with no modification.
Carreras et al (2008) use coarse-to-fine pruning
with dependency parsing, but in that case, a graph-
based dependency parser provides the coarse pass,
with the fine pass being a far-more-expensive tree-
adjoining grammar. Our filters could become a
0th pass, further increasing the efficiency of their
approach.
4 Arc Filters
We propose arc filtering as a preprocessing step
for dependency parsing. An arc filter removes im-
2Johnson?s (2007) split-head CFG could implement this
idea directly with little effort.
plausible head-modifier arcs from the complete
dependency graph (which initially includes all
head-modifier arcs). We use three stages of filters
that operate in sequence on progressively sparser
graphs: 1) rule-based, 2) linear: a single pass
through the n nodes in a sentence (O(n) complex-
ity), and 3) quadratic: a scoring of all remaining
arcs (O(n2)). The less intensive filters are used
first, saving time by leaving fewer arcs to be pro-
cessed by the more intensive systems.
Implementations of our rule-based, linear, and
quadratic filters are publicly available at:
http://code.google.com/p/arcfilter/
4.1 Filter Framework
Our filters assume the input sentences have been
PoS-tagged. We also add an artificial root node
to each sentence to be the head of the tree?s root.
Initially, this node is a potential head for all words
in the sentence.
Each filter is a supervised classifier. For exam-
ple, the quadratic filter directly classifies whether
a proposed head-modifier pair is not a link in the
dependency tree. Training data is created from an-
notated trees. All possible arcs are extracted for
each training sentence, and those that are present
in the annotated tree are labeled as class?1, while
those not present are +1. A similar process gener-
ates training examples for the other filters. Since
our goal is to only filter very implausible arcs, we
bias the classifier to high precision, increasing the
cost for misclassifying a true arc during learning.3
Class-specific costs are command-line parame-
ters for many learning packages. One can inter-
pret the learning objective as minimizing regular-
ized, weighted loss:
min
w?
1
2 ||w?||
2 + C1
?
i:yi=1
l(w?, yi, x?i)
+C2
?
i:yi=?1
l(w?, yi, x?i) (1)
where l() is the learning method?s loss function,
x?i and yi are the features and label for the ith
3Learning with a cost model is generally preferable to
first optimizing error rate and then thresholding the predic-
tion values to select a high-confidence subset (Joachims,
2005), but the latter approach was used successfully for cell
classification in Roark and Hollingshead (2008).
55
not a h ? ? , . ; | CC PRP$ PRP EX
-RRB- -LRB-
no ? ? m EX LS POS PRP$
no m? ? . RP
not a root , DT
no h?m DT?{DT,JJ,NN,NNP,NNS,.}
CD?CD NN?{DT,NNP}
NNP?{DT,NN,NNS}
no m?h {DT,IN,JJ,NN,NNP}?DT
NNP?IN IN?JJ
Table 1: Learned rules for filtering dependency
arcs using PoS tags. The rules filter 25% of pos-
sible arcs while recovering 99.9% of true links.
training example, w? is the learned weight vector,
and C1 and C2 are the class-specific costs. High
precision is obtained when C2 >> C1. For an
SVM, l(w?, yi, x?i) is the standard hinge loss.
We solve the SVM objective using LIBLIN-
EAR (Fan et al, 2008). In our experiments, each
filter is a linear SVM with the typical L1 loss and
L2 regularization.4 We search for the best com-
bination of C1 and C2 using a grid search on de-
velopment data. At test time, an arc is filtered if
w? ? x? > 0.
4.2 Rule-Based Filtering
Our rule-based filters seek to instantly remove
those arcs that are trivially implausible on the ba-
sis of their head and modifier PoS tags. We first
extract labeled examples from gold-standard trees
for whenever a) a word is not a head, b) a word
does not have a head on the left (resp. right), and
c) a pair of words is not linked. We then trained
high-precision SVM classifiers. The only features
in x? are the PoS tag(s) of the head and/or modi-
fier. The learned feature weights identify the tags
and tag-pairs to be filtered. For example, if a tag
has a positive weight in the not-a-head classifier,
all arcs having that node as head are filtered.
The classier selects a small number of high-
4We also tried L1-regularized filters. L1 encourages most
features to have zero weight, leading to more compact and
hence faster models. We found the L1 filters to prune fewer
arcs at a given coverage level, providing less speed-up at
parsing time. Both L1 and L2 models are available in our
publicly available implementation.
precision rules, shown in Table 1. Note that the
rules tend to use common tags with well-defined
roles. By focusing on weighted loss as opposed
to arc frequency, the classifier discovers struc-
tural zeros (Mohri and Roark, 2006), events which
could have been observed, but were not. We
consider this an improvement over the frequency-
based length thresholds employed previously in
tag-specific vine parsing.
4.3 Linear-Time Filtering
In the linear filtering stage, we filter arcs on the
basis of single nodes and their contexts, passing
through the sentences in linear time. For each
node, eight separate classifiers decide whether:
1. It is not a head (i.e., it is a leaf of the tree).
2. Its head is on the left/right.
3. Its head is within 5 nodes on the left/right.
4. Its head is immediately on the left/right.
5. It is the root.
For each of these decisions, we again train high-
precision SVMs with C2 >> C1, and filter di-
rectly based on the classifier output.
If a word is not a head, all arcs with the given
word as head can be pruned. If a word is deemed
to have a head within a certain range on the left
or right, then all arcs that do not obey this con-
straint can be pruned. If a root is found, no other
words should link to the artificial root node. Fur-
thermore, in a projective dependency tree, no arc
will cross the root, i.e., there will be no arcs where
a head and a modifier lie on either side of the root.
We can therefore also filter arcs that violate this
constraint when parsing projectively.
S?gaard and Kuhn (2009) previously proposed
a tagger to further constrain a vine parser. Their
tags are a subset of our decisions (items 4 and 5
above), and have not yet been tested in a state-of-
the-art system.
Development experiments show that if we
could perfectly make decisions 1-5 for each word,
we could remove 91.7% of the total arcs or 95%
of negative arcs, close to the upper bound.
Features
Unlike rule-based filtering, linear filtering uses
a rich set of features (Table 2). Each feature is a
56
PoS-tag features Other features
tagi wordi
tagi, tagi?1 wordi+1
tagi, tagi+1 wordi?1
tagi?1, tagi+1 shapei
tagi?2, tagi?1 prefixi
tagi+1, tagi+2 suffixi
tagj , Left, j=i?5...i?1 i
tagj , Right, j=i+1...i+5 i, n
tagj , (i-j), j=i?5...i?1 n - i
tagj , (i-j), j=i+1...i+5
Table 2: Linear filter features for a node at po-
sition i in a sentence of length n. Each feature
is also conjoined (unless redundant) with wordi,
tagi, shapei, prefixi, and suffixi (both 4 letters).
The shape is the word normalized using the regu-
lar expressions [A-Z]+? A and [a-z]+? a.
binary indicator feature. To increase the speed of
applying eight classifiers, we use the same feature
vector for each of the decisions; learning gives
eight different weight vectors, one corresponding
to each decision function. Feature extraction is
constrained to be O(1) for each node, so that over-
all feature extraction and classification remain a
fast O(n) complexity. Feature extraction would
be O(n2) if, for example, we had a feature for ev-
ery tag on the left or right of a node.
Combining linear decisions
We originally optimized the C1 and C2 param-
eter separately for each linear decision function.
However, we found we could substantially im-
prove the collective performance of the linear fil-
ters by searching for the optimal combination of
the component decisions, testing different levels
of precision for each component. We selected a
few of the best settings for each decision when op-
timized separately, and then searched for the best
combination of these candidates on development
data (testing 12960 combinations in all).
4.4 Quadratic-Time Filtering
In the quadratic filtering stage, a single classifier
decides whether each head-modifier pair should
be filtered. It is trained and applied as described
in Section 4.1.
Binary features
sign(h-m) tagshm
tagm?1, tagshm tagm+1, tagshm
tagh?1, tagshm tagh+1, tagshm
sign(h-m), tagh, wordm
sign(h-m), wordh, tagm
Real features? values
sign(h-m)? h-m
tagh, tagm ? h-m
tagk , tagshm ? Count(tagk ? tagsh...m)
wordk , tagshm ? Count(wordk ? wordsh...m)
Table 3: Quadratic filter features for a head at po-
sition h and a modifier at position m in a sentence
of length n. Here tagshm = (sign(h-m), tagh,
tagm), while tagsh...m and wordsh...m are all the
tags (resp. words) between h and m, but within
?5 positions of h or m.
While theoretically of the same complexity as
the parser?s arc-scoring function (O(n2)), this
process can nevertheless save time by employing
a compact feature set. We view quadratic filter-
ing as a light preprocessing step, using only a por-
tion of the resources that might be used in the final
scoring function.
Features
Quadratic filtering uses both binary and real-
valued features (Table 3). Real-valued features
promote a smaller feature space. For example,
one value can encode distance rather than separate
features for different distances. We also general-
ize the ?between-tag features? used in McDonald
et al (2005) to be the count of each tag between
the head and modifier. The count may be more in-
formative than tag presence alone, particularly for
high-precision filters. We follow Galley and Man-
ning (2009) in using only between-tags within a
fixed range of the head or modifier, so that the ex-
traction for each pair is O(1) and the overall fea-
ture extraction is O(n2).
Using only a subset of the between-tags as fea-
tures has been shown to improve speed but im-
pair parser performance (Galley and Manning,
2009). By filtering quickly first, then scoring all
remaining arcs with a cubic scoring function in the
parser, we hope to get the best of both worlds.
57
5 Filter Experiments
Data
We extract dependency structures from the
Penn Treebank using the Penn2Malt extraction
tool,5 which implements the head rules of Yamada
and Matsumoto (2003). Following convention, we
divide the Treebank into train (sections 2?21), de-
velopment (22) and test sets (23). The develop-
ment and test sets are re-tagged using the Stanford
tagger (Toutanova et al, 2003).
Evaluation Metrics
To measure intrinsic filter quality, we define
Reduction as the proportion of total arcs re-
moved, and Coverage as the proportion of true
head-modifier arcs retained. Our evaluation asks,
for each filter, what Reduction can be obtained at
a given Coverage level? We also give Time: how
long it takes to apply the filters to the test set (ex-
cluding initialization).
We compute an Upper Bound for Reduction on
development data. There are 1.2 million poten-
tial dependency links in those sentences, 96.5%
of which are not present in a gold standard depen-
dency tree. Therefore, the maximum achievable
Reduction is 96.5%.
Systems
We evaluate the following systems:
? Rules: the rule-based filter (Section 4.2)
? Lin.: the linear-time filters (Section 4.3)
? Quad.: the quadratic filter (Section 4.4)
The latter two approaches run on the output of the
previous stage. We compare to the two vine pars-
ing approaches described in Section 3.1:
? Len-Vine uses a hard limit on arc length.
? Tag-Vine (later, Vine) learns a maxi-
mum length for dependency arcs for every
head/modifier tag-combination and order.
5.1 Results
We set each filter?s parameters by selecting
a Coverage-Reduction tradeoff on development
5http://w3.msi.vxu.se/?nivre/research/Penn2Malt.
html
 20
 30
 40
 50
 60
 70
 80
 90
 100
 99.3 99.4 99.5 99.6 99.7 99.8 99.9
R
ed
uc
tio
n 
(%
)
Coverage (%)
Upper Bd
Lin-Orac.
Quad
Lin
Tag-Vine
Len-Vine
Figure 2: Filtering performance for different fil-
ters and cost parameters on development data.
Lin-Orac indicates the percentage filtered using
perfect decisions by the linear components.
Filter Coverage Reduct. Time (s)
Vine 99.62 44.0 2.9s
Rules 99.86 25.8 1.3s
Lin. 99.73 54.2 7.3s
Quad. 99.50 78.4 16.1s
Table 4: Performance (%) of filters on test data.
data (Figure 2). The Lin curve is obtained by vary-
ing both the C1/C2 cost parameters and the combi-
nation of components (plotting the best Reduction
at each Coverage level). We chose the linear fil-
ters with 99.8% Coverage at a 54.2% Reduction.
We apply Quad on this output, varying the cost
parameters to produce its curve. Aside from Len-
Vine, all filters remove a large number of arcs with
little drop in Coverage.
After selecting a desired trade-off for each clas-
sifier, we move to final filtering experiments on
unseen test data (Table 4). The linear filter re-
moves well over half the links but retains an as-
tounding 99.7% of correct arcs. Quad removes
78.4% of arcs at 99.5% Coverage. It thus reduces
the number of links to be scored by a dependency
parser by a factor of five.
The time for filtering the 2416 test sentences
varies from almost instantaneous for Vine and
Rules to around 16 seconds for Quad. Speed num-
bers are highly machine, design, and implemen-
58
Decision Precision Recall
No-Head 99.9 44.8
Right-? 99.9 28.7
Left-? 99.9 39.0
Right-5 99.8 31.5
Left-5 99.9 19.7
Right-1 99.7 6.2
Left-1 99.7 27.3
Root 98.6 25.5
Table 5: Linear Filters: Test-set performance (%)
on decisions for components of the combined 54.2
Reduct./99.73 Coverage linear filter.
Type Coverage Reduct. Oracle
All 99.73 54.2 91.8
All\No-Head 99.76 46.4 87.2
All\Left-? 99.74 53.2 91.4
All\Right-? 99.75 53.6 90.7
All\Left-5 99.74 53.2 89.7
All\Right-5 99.74 51.6 90.4
All\Left-1 99.75 53.5 90.8
All\Right-1 99.73 53.9 90.6
All\Root 99.76 50.2 90.0
Table 6: Contribution of different linear filters to
test set performance (%). Oracle indicates the per-
centage filtered by perfect decisions.
tation dependent, and thus we have stressed the
asymptotic complexity of the filters. However, the
timing numbers show that arc filtering can be done
quite quickly. Section 6 confirms that these are
very reasonable costs in light of the speed-up in
overall parsing.
5.2 Linear Filtering Analysis
It is instructive to further analyze the components
of the linear filter. Table 5 gives the performance
of each classifier on its specific decision. Preci-
sion is the proportion of positive classifications
that are correct. Recall is the proportion of pos-
itive instances that are classified positively (e.g.
the proportion of actual roots that were classified
as roots). The decisions correspond to items 1-5 in
Section 4.3. For example, Right-? is the decision
that a word has no head on the right.
Most notably, the optimum Root decision has
much lower Precision than the others, but this has
little effect on its overall accuracy as a filter (Ta-
ble 6). This is perhaps because the few cases of
false positives are still likely to be main verbs or
auxiliaries, and thus still still likely to have few
links crossing them. Thus many of the filtered
links are still correct.
Table 6 provides the performance of the classi-
fier combination when each linear decision is ex-
cluded. No-Head is the most important compo-
nent in the oracle and the actual combination.
6 Parsing Experiments
6.1 Set-up
In this section, we investigate the impact of our fil-
ters on graph-based dependency parsers. We train
each parser unfiltered, and then measure its speed
and accuracy once filters have been applied. We
use the same training, development and test sets
described in Section 5. We evaluate unlabeled de-
pendency parsing using head accuracy: the per-
centage of words (ignoring punctuation) that are
assigned the correct head.
The filters bypass feature extraction for each fil-
tered arc, and replace its score with an extremely
low negative value. Note that 2nd-order features
consider O(n3) [h,m1,m2] triples. These triples
are filtered if at least one component arc ([h,m1]
or [h,m2]) is filtered.
In an optimal implementation, we might also
have the parser re-use features extracted during
filtering when scoring the remaining arcs. We did
not do this. Instead, filtering was treated as a pre-
processing step, which maximizes the portability
of the filters across parsers. We test on two state-
of-the art parsers:
MST We modified the publicly-available MST
parser (McDonald et al, 2005)6 to employ our fil-
ters before carrying out feature extraction. MST
is trained with 5-best MIRA.
DepPercep We also test an in-house depen-
dency parser, which conducts projective first and
2nd-order searches using the split-head CFG de-
scribed by Johnson (2007), with a weight vec-
tor trained using an averaged perceptron (Collins,
6http://sourceforge.net/projects/mstparser/
59
DepPercep-1 DepPercep-2 MST-1 MST-2
Filter Cost Acc. Time Acc. Time Acc. Time Acc. Time
None +0 91.8 348 92.5 832 91.2 153 91.9 200
Vine +3 91.7 192 92.3 407 91.2 99 91.8 139
Rules +1 91.7 264 92.4 609 91.2 125 91.9 167
Linear +7 91.7 168 92.4 334 91.2 88 91.8 121
Quad. +16 91.7 79 92.3 125 91.2 58 91.8 80
Table 7: The effect of filtering on the speed and accuracy on 1st and 2nd-order dependency parsing.
2002). Its features are a mixture of those de-
scribed by McDonald et al (2005), and those used
in the Koo et al (2008) baseline system; we do not
use word-cluster features.
DepPercep makes some small improvements to
MST?s 1st-order feature set. We carefully de-
termined which feature types should have dis-
tance appended in addition to direction. Also, in-
spired by the reported utility of mixing PoS tags
and word-clusters (Koo et al, 2008), we created
versions of all of the ?Between? and ?Surround-
ing Word? features described by McDonald et al
(2005) where we mix tags and words.7
DepPercep was developed with quadratic filters
in place, which enabled a fast development cycle
for feature engineering. As a result, it does not
implement many of the optimizations in place in
MST, and is relatively slow unfiltered.
6.2 Results
The parsing results are shown in Table 7, where
times are given in seconds, and Cost indicates the
additional cost of filtering. Note that the impact
of all filters on accuracy is negligible, with a de-
crease of at most 0.2%. In general, parsing speed-
ups mirror the amount of arc reduction measured
in our filter analysis (Section 5.1).
Accounting for filter costs, the benefits of
quadratic filtering depend on the parser. The extra
benefit of quadratic over linear is substantial for
DepPercep, but less so for 1st-order MST.
MST shows more modest speed-ups than Dep-
Percep, but MST is already among the fastest
publicly-available data-driven parsers. Under
quadratic filtering, MST-2 goes from processing
7This was enabled by using word features only when the
word is among the 800 most frequent in the training set.
12 sentences per second to 23 sentences.8
DepPercep-2 starts slow, but benefits greatly
from filtering. This is because, unlike MST-2,
it does not optimize feature extraction by fac-
toring its ten 2nd-order features into two triple
([h,m1,m2]) and eight sibling ([m1,m2]) fea-
tures. This suggests that filtering could have a dra-
matic effect on a parser that uses more than a few
triple features, such as Koo et al (2008).
7 Conclusion
We have presented a series of arc filters that speed
up graph-based dependency parsing. By treat-
ing filtering as weighted classification, we learn a
cascade of increasingly complex filters from tree-
annotated data. Linear-time filters prune 54%
of total arcs, while quadratic-time filters prune
78%. Both retain at least 99.5% of true dependen-
cies. By testing two state-of-the-art dependency
parsers, we have shown that our filters produce
substantial speed improvements in even carefully-
optimized parsers, with negligible losses in ac-
curacy. In the future we hope to leverage this
reduced search space to explore features derived
from large corpora.
References
Carreras, Xavier, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In CoNLL.
Carreras, Xavier. 2007. Experiments with a higher-
order projective dependency parser. In EMNLP-
CoNLL.
8This speed accounts for 25 total seconds to apply the
rules, linear, and quadratic filters.
60
Charniak, Eugene, Mark Johnson, Micha Elsner,
Joseph Austerweil, David Ellis, Isaac Haxton,
Catherine Hill, R. Shrivaths, Jeremy Moore,
Michael Pozar, and Theresa Vu. 2006. Multilevel
coarse-to-fine PCFG parsing. In HLT-NAACL.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
JMLR, 3:951?991.
Dreyer, Markus, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In CoNLL.
Eisner, Jason and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
IWPT.
Eisner, Jason. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COL-
ING.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. JMLR,
9:1871?1874.
Galley, Michel and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In ACL-IJCNLP.
Joachims, Thorsten. 2005. A support vector method
for multivariate performance measures. In ICML.
Johnson, Mark. 2007. Transforming projective bilex-
ical dependency grammars into efficiently-parsable
CFGs with unfold-fold. In ACL.
Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL-08: HLT.
McDonald, Ryan and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In EMNLP-CoNLL.
McDonald, Ryan and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In EACL.
McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL.
Me?lc?uk, Igor A. 1987. Dependency syntax: theory
and practice. State University of New York Press.
Mohri, Mehryar and Brian Roark. 2006. Probabilistic
context-free grammar induction based on structural
zeros. In HLT-NAACL.
Petrov, Slav and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Poon, Hoifung and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
Roark, Brian and Kristy Hollingshead. 2008. Classi-
fying chart cells for quadratic complexity context-
free inference. In COLING.
S?gaard, Anders and Jonas Kuhn. 2009. Using a max-
imum entropy-based tagger to improve a very fast
vine parser. In IWPT.
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL.
Wang, Qin Iris, Colin Cherry, Dan Lizotte, and Dale
Schuurmans. 2006. Improved large margin depen-
dency parsing via local constraints and Laplacian
regularization. In CoNLL.
Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A quasi-
synchronous grammar for QA. In EMNLP-CoNLL.
Wang, Qin Iris, Dale Schuurmans, and Dekang Lin.
2008. Semi-supervised convex training for depen-
dency parsing. In ACL-08: HLT.
West, D. 2001. Introduction to Graph Theory. Pren-
tice Hall, 2nd edition.
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In IWPT.
61
Coling 2010: Poster Volume, pages 1550?1557,
Beijing, August 2010
Imposing Hierarchical Browsing Structures onto Spoken Documents
Xiaodan Zhu & Colin Cherry
Institute for Information Technology
National Research Council Canada
{Xiaodan.Zhu,Colin.Cherry}@nrc-cnrc.gc.ca
Gerald Penn
Department of Computer Science
University of Toronto
gpenn@cs.toronto.edu
Abstract
This paper studies the problem of im-
posing a known hierarchical structure
onto an unstructured spoken document,
aiming to help browse such archives.
We formulate our solutions within a
dynamic-programming-based alignment
framework and use minimum error-
rate training to combine a number of
global and hierarchical constraints. This
pragmatic approach is computationally
efficient. Results show that it outperforms
a baseline that ignores the hierarchical
and global features and the improvement
is consistent on transcripts with different
WERs. Directly imposing such hierar-
chical structures onto raw speech without
using transcripts yields competitive
results.
1 Introduction
Though speech has long served as a basic method
of human communication, revisiting and brows-
ing speech content had never been a possibility
before human can record their own voice. Re-
cent technological advances in recording, com-
pressing, and distributing such archives have led
to the consistently increasing availability of spo-
ken content.
Along with this availability comes a demand for
better ways to browse such archives, which is in-
herently more difficult than browsing text. In re-
lying on human beings? ability to browse text, a
solution is therefore to reduce the speech brows-
ing problem to a text browsing task through tech-
nologies that can automatically convert speech to
text, i.e., the automatic speech recognition (ASR).
Research along this line has implicitly changed
the traditional speaking-for-hearing and writing-
for-reading construals: now speech can be read
through its transcripts, though it was not originally
intended for this purpose, which in turn raises a
new set of problems.
The efficiency and convenience of reading spo-
ken documents are affected by at least two facts.
First, the quality of transcripts can impair brows-
ing efficiency, e.g., as shown in (Stark et al, 2000;
Munteanu et al, 2006), though if the goal is only
to browse salient excerpts, recognition errors on
the extracts can be reduced by considering the
confidence scores assigned by ASR (Zechner and
Waibel, 2000; Hori and Furui, 2003).
Even if transcription quality is not a problem,
browsing transcripts is not straightforward. When
intended to be read, written documents are al-
most always presented as more than uninterrupted
strings of text. Consider that for many writ-
ten documents, e.g., books, indicative structures
such as section/subsection headings and tables-of-
contents are standard constituents created manu-
ally to help readers. Structures of this kind, how-
ever, are rarely aligned with spoken documents.
In this paper, we are interested in addressing
the second issue: adding hierarchical browsable
structures to speech transcripts. We define a hi-
erarchical browsable structure as a set of nested
labelled bracketing which, when placed in text,
partition the document into labeled segments. Ex-
amples include the sequence of numbered sec-
tion headings in this paper, or the hierarchical
slide/bullet structure in the slides of a presenta-
tion.
1550
An ideal solution to this task would directly in-
fer both the hierarchical structure and the labels
from unstructured spoken documents. However,
this is a very complex task, involving the analysis
of not only local but also high-level discourse over
large spans of transcribed speech. Specifically for
spoken documents, spoken-language characteris-
tics as well as the lack of formality and thematic
boundaries in transcripts violate many conditions
that a reliable algorithm (Marcu, 2000) relies on
and therefore make the task even harder.
In this paper, we aim at a less ambitious but
naturally occurring problem: imposing a known
hierarchical structure, e.g., presentation slides,
onto the corresponding document, e.g., presenta-
tion transcripts. Given an ordered, nested set of
topic labels, we must place the labels so as to
correctly segment the document into appropriate
units. Such an alignment would provide a useful
tool for presentation browsing, where a user could
easily navigate through a presentation by clicking
on bullets in the presentation slides. The solution
to this task should also provide insights and tech-
niques that will be useful in the harder structure-
inference task, where hierarchies and labels are
not given.
We present a dynamic-programming-based
alignment framework that considers global docu-
ment features and local hierarchical features. This
pragmatic approach is computationally efficient
and outperforms a baseline alignment that ignores
the hierarchical structure of bullets within slides.
We also explore the impact of speech recognition
errors on this task. Furthermore, we study the
feasibility of directly aligning a structure to raw
speech, as opposed to a transcript.
2 Related work
Topic/slide boundary detection The previous
work most directly related to ours is research that
attempts to find flat structures of spoken docu-
ments, such as topic and slide boundaries. For
example, the work of (Chen and Heng, 2003;
Ruddarraju, 2006; Zhu et al, 2008) aims to find
slide boundaries in the corresponding lecture tran-
scripts. Malioutov et al (2007) developed an ap-
proach to detecting topic boundaries of lecture
recordings by finding repeated acoustic patterns.
None of this work, however, has involved hierar-
chical structures that exist at different levels of a
document.
In addition, researchers have also analyzed
other multimedia channels, e.g., video (Liu et al,
2002; Wang et al, 2003; Fan et al, 2006), to de-
tect slide transitions. Such approaches, however,
are unlikely to find semantic structures that are
more detailed than slide transitions, e.g., the bullet
hierarchical structures that we are interested in.
Building tables-of-contents on written text A
notable effort going further than topic segmenta-
tion is the work by Branavan et al (2007), which
aims at the ultimate goal of building tables-of-
contents for written texts. However, the authors
assumed the availability of the hierarchical struc-
tures and the corresponding text spans. Therefore,
their problem was restricted to generating titles for
each span. Our work here can be thought of as the
inverse problem, in which the title of each section
is known, but the corresponding segments in the
spoken documents are unknown. Once the corre-
spondence is found, an existing hierarchical struc-
ture along with its indicative titles is automatically
imposed on the speech recordings. Moreover, this
paper studies spoken documents instead of writ-
ten text. We believe it is more attractive not only
because of the necessity of browsing spoken con-
tent in a more efficient way but also the general
absence of helpful browsing structures that are of-
ten available in written text, as we have already
discussed above.
Rhetoric analysis In general, analyzing dis-
course structures can provide thematic skeletons
(often represented as trees) of a document as well
as relationship between the nodes in the trees. Ex-
amples include the widely known discourse pars-
ing work by Marcu (2000). However, when the
task involves the understanding of high-level dis-
course, it becomes more challenging than just
finding local discourse conveyed on small spans of
text; e.g., the latter is more likely to benefit from
the presence of discourse markers. Specifically
for spoken documents, spoken-language charac-
teristics as well as the absence of formality and
thematic boundaries in transcripts pose additional
1551
difficulty. For example, the boundaries of sen-
tences, paragraphs, and larger text blocks like sec-
tions are often missing. Together with speech
recognition errors as well as other speech charac-
teristics such as speech disfluences, they will im-
pair the conditions on which an effective and reli-
able algorithm of discourse analysis is often built.
3 Problem formulation
We are given a speech sequence U =
u1, u2, ..., um, where ui is an utterance. De-
pending on the application, ui can either stand
for the audio or transcript of the utterance. We
are also given a corresponding hierarchical struc-
ture. In our work, this is a sequence of lecture
slides containing a set of slide titles and bullets,
B = {b1, b2, ..., bn}, organized in a tree structure
T (?,?,?), where ? is the root of the tree that
concatenates all slides of a lecture; i.e., each slide
is a child of the root ? and each slide?s bullets
form a subtree. In the rest of this paper, the word
bullet means both the title of a slide (if any) and
any bullet in it. ? is the set of nodes of the tree
(both terminal and non-terminals, excluding the
root ?), each corresponding to a bullet bi in the
slides. ? is the edge set. With the definitions, our
task is herein to find the triple (bi, uk, ul), denot-
ing that a bullet bi starts from the kth utterance
uk and ends at the lth. Constrained by the tree
structure, the text span corresponding to an an-
cestor bullet contains those corresponding to its
descendants; i.e., if a bullet bi is the ancestor of
another bullet bj in the tree, the acquired bound-
ary triples (bi, uk1, ul1) and (bj , uk2, ul2) should
satisfy uk1 ? uk2 and ul1 ? ul2. In implemen-
tation, we only need to find the starting point of a
bullet, i.e., a pair (bi, uk), since we know the tree
structure in advance and therefore we know that
the starting position of the next sibling bullet is
the ending boundary for the current bullet.
4 Our approaches
Our task is to find the correspondence between
slide bullets and a speech sequence or its tran-
scripts. Research on finding correspondences be-
tween parallel texts pervades natural language
processing. For example, aligning bilingual sen-
tence pairs is an essential step in training ma-
chine translation models. In text summarization,
the correspondence between human-written sum-
maries and their original texts has been identified
(Jing, 2002), too. In speech recognition, forced
alignment is applied to align speech and tran-
scripts. In this paper, we keep the general frame-
work of alignment in solving our problem.
Our solution, however, should be flexible to
consider multiple constraints such as those con-
veyed in hierarchical bullet structures and global
word distribution. Accordingly, the model pro-
posed in this paper depends on two orthogonal
strategies to ensure efficiency and richness of the
model. First of all, we formulate all our solutions
within a classic dynamic programming framework
to enforce computational efficiency (section 4.1).
On the other hand, we explore the approach to in-
corporating hierarchical and global features into
the alignment framework (Section 4.2). The as-
sociated parameters are then optimized with Pow-
ell?s algorithm (Section 4.3).
4.1 A pre-order walk of bullet trees
We formulate our solutions within the classic
dynamic-programming-based alignment frame-
work, dynamic time warping (DTW). To this end,
we need to sequentialize the given hierarchies,
i.e., bullet trees. We propose to do so through a
pre-order walk of a bullet tree; i.e., at any step
of a recursive traversal of the tree, the alignment
model always visits the root first, followed by its
children in a left-to-right order. This sequential-
ization actually corresponds to a reasonable as-
sumption: words appearing earlier on a given slide
are spoken earlier by the speaker. The pre-order
walk is also used by (Branavan et al, 2007) to
reduce the search space of their discriminative
table-of-contents generation. Our sequentializa-
tion strategy can be intuitively thought of as re-
moving indentations that lead each bullet. As
shown in Figure 1, the right panel is a bullet array
resulting from a pre-walk of the slide in the left
panel. In our baseline model, the resulted bullet
array is directly aligned with lecture utterances.
Other orders of bullet traversal could also be
considered, e.g., when speech does not strictly fol-
low bullet orders. In general, one can regard our
1552
task here as a tagging problem to allow further
flexibility on bullet-utterance correspondence, in
which bullets are thought of as tags. However,
considering the fact that bullets are created to or-
ganize speech and in most cases they correspond
to the development of speech content monotoni-
cally, this paper focuses on addressing the prob-
lem in the alignment framework.
Figure 1: A pre-order walk of a bullet tree.
4.2 Incorporating hierarchical and global
features
Our models should be flexible enough to consider
constraints that could be helpful, e.g., the hierar-
chical bullet structures and global word distribu-
tion. We propose to consider all these constraints
in the phase of estimating similarity matrices. To
this end, we use two levels of similarity matrices
to capture local tree constraints and global word
distributions, respectively.
First of all, information conveyed in the hierar-
chies of bullet trees should be considered, such as
the potentially discriminative nature between two
sibling bullets (Branavan et al, 2007) and the re-
lationships between ancestor and descendant bul-
lets. We incorporate them in the bullet-utterance
similarity matrices. Specifically, when estimating
the similarity between a bullet bi and an utterance
uj , we consider local tree constraints based on
where the node bi is located on the slide. We do
so by accounting for first and second-order tree
features. Given a bullet, bi, we first represent it
as multiple vectors, one for each of the following:
its own words, the words appearing in its parent
bullet, grandparent, children, grandchildren, and
the bullets immediately adjacent to bi. That is, bi
is now represented as 6 vectors of words (we do
not discriminate between its left and right siblings
and put these words in the same vector). Simi-
larity between the bullet bi and an utterance uj is
calculated by taking a weighted average over the
similarities between each of the 6 vectors and the
utterance uj . A linear combination is used and the
weights are optimized on a development set.
Global property of word distributions could be
helpful, too. A general term often has less dis-
criminative power in the alignment framework
than a word that is localized to a subsection of
the document and is related to specific subtopics.
For example, in a lecture that teaches introductory
computer science topics, aligning a general term
?computer? should receive a smaller weight than
aligning some topic-specific terms such as ?au-
tomaton.? The latter word is more likely to appear
in a more narrow text span. It is not straightfor-
ward to directly calculate idf scores unless a lec-
ture is split into smaller segments in some way.
Instead, in our models, the distribution property
of a word is considered in word-level similarity
matrices with the following formula.
sim(wi, wj) =
{
0 : i 6= j
1? ? var(wi)maxk(var(wk)) : i = j
Aligning different words receives no bonus,
while matching the same word between bullets
and utterances receives a score of 1 minus a dis-
tribution penalty, as shown in the formula above.
The function var(wi) calculates the standard vari-
ance of the positions where the word wi appears.
Divided by the maximal standard variance of word
positions in the same lecture, the score is normal-
ized to [0,1]. This distribution penalty is weighted
by ?, which is tuned in a development set. Again,
a general term is expected to have a larger posi-
tional variance.
Once a word-level matrix is acquired, it is com-
bined with the bullet-utterance level matrix dis-
cussed above. Specifically, when measuring the
similarity between a word vector (one of the 6
vectors) and the transcripts of an utterance, we
sum up the word-level similarity scores of all
matching words between them, normalize the re-
sulted score by the length of the vector and ut-
terance, and then renormalize it to the range
1553
[0, 1] within the same spoken document. The
final bullet-utterance similarity matrix is incor-
porated into the pre-order-walk suquentialization
discussed above, when alignment is conducted.
4.3 Parameter optimization
Powell?s algorithm (Press et al, 2007) is used to
find the optimal weights for the constraints we in-
corporated above, to directly minimize the objec-
tive function, i.e., the Pk and WindowDiff scores
that we will discuss later. As a summary, we have
7 weights to tune: a weight for each of the fol-
lowing: parent bullet, grandparent, adjacent sib-
lings, children, grandchildren, and the current bul-
let, plus the word distribution penalty ?. The val-
ues of these weights are determined on a develop-
ment set.
Note that the model we propose here does not
exclude the use of further features; instead, many
other features, such as smoothed word similarity
scores, can be easily added to this model. We
are conservative on our model complexity here,
in terms of number of weights need to be tuned,
for the consideration of the size of data that we
can used to estimate these weights. Finally, with
all the 7 weights being determined, we apply the
standard dynamic time warping (DTW).
5 Experimental set-up
5.1 Data
We use a corpus of lectures recorded at a large
research university. The correspondence between
bullets and speech utterances are manually an-
notated in a subset of this lecture corpus, which
contains approximately 30,000 word tokens in
its manual transcripts. Intuitively, this roughly
equals a 120-page double-spaced essay in length.
The lecturer?s voice was recorded with a head-
mounted microphone with a 16kHz sampling rate
and 16-bit samples. Students? comments and
questions were not recorded. The speech is split
into utterances by pauses longer than 200ms, re-
sulting in around 4000 utterances. There are 119
slides that are composed of 921 bullets. A sub-
set containing around 25% consecutive slides and
their corresponding speech/transcripts are used as
our development set to tune the parameters dis-
cussed earlier; the rest data are used as our test
set.
5.2 Evaluation metric
We evaluate our systems according to how well
the segmentation implied by the inferred bullet
alignment matches that of the manually anno-
tated gold-standard bullet algnment. Though one
may consider that different bullets may be of dif-
ferent importance, in this paper we do not use
any heuristics to judge this and we treat all bul-
lets equally in our evaluation. We evaluate our
systems with the Pk and WindowDiff metrics
(Malioutov et al, 2007; Beeferman et al, 1999;
Pevsner and Hearst, 2002). Note that for both
metrics, the lower a score is, the better the per-
formance of a system is. The Pk score computes
the probability of a randomly chosen pair of words
being inconsistently separated. The WindowDiff
is a variant of Pk; it penalizes false positives and
near misses equally.
6 Experimental results
6.1 Alignment performance
Table 1 presents the results on automatic tran-
scripts with a 39% WER, a typical WER in realis-
tic and uncontrolled lecture conditions (Leeuwis
et al, 2003; Hsu and Glass, 2006). The tran-
scripts were generated with the SONIC toolkit
(Pellom, 2001). The acoustic model was trained
on the Wall Street Journal dictation corpus. The
language model was trained on corpora obtained
from the Web through searching the words ap-
pearing on slides as suggested by (Munteanu et
al., 2007).
Pk WindowDiff
UNI 0.481 0.545
TT 0.469 0.534
B-ALN 0.283 0.376
HG-ALN 0.266 0.359
Table 1: The Pk and WindowDiff scores of uni-
form segmentation (UNI), TextTiling (TT), base-
line alignment (B-ALN), and alignment with hier-
archical and global information (HG-ALN).
From Table 1, we can see that the model that
1554
utilizes the hierarchical structures of slides and
global distribution of words, i.e., the HG-ALN
model, reduces both Pk and WindowDiff scores
over the baseline model, B-ALN. As discussed
earlier, the baseline is a re-implementation of
standard dynamic time warping based only on a
pre-order walk of the slides, while the HG-ALN
model incorporates also hierarchical bullet con-
straints and global word distribution.
Table 1 also presents the performance of a
typical topic segmentation algorithm, TextTiling
(Hearst, 1997). Note that similar to (Malioutov et
al., 2007), we force the number of predicted topic
segments to be the target number, i.e., in our task,
the number of bullets. The results show that both
the Pk and WindowDiff scores of TextTiling are
significantly higher than those of the alignment al-
gorithms. Our manual analysis suggests that many
segments are as short as several utterances and the
difference between two consecutive segments is
too subtle to be captured by a lexical cohesion-
based method such as TextTiling. For compari-
son, We also present the results of uniform seg-
mentation (UNI), which simply splits the tran-
script of each lecture evenly into segments with
same numbers of words.
6.2 Performance under different WERs
Speech recognition errors within reasonable
ranges often have very small impact on many spo-
ken language processing tasks such as spoken lan-
guage retrieval (Garofolo et al, 2000) and speech
summarization (Christensen et al, 2004; Maskey,
2008; Murray, 2008; Zhu, 2010). To study the
impact of speech recognition errors on our task
here, we experimented with the alignment mod-
els on manual transcripts as well as on automatic
transcripts with different WERs, including a 39%
and a 46% WER produced by two real recogni-
tion systems. To increase the spectrum of our ob-
servation, we also overfit our ASR models to ob-
tain smaller WERs at the levels of 11%, 19%, and
30%.
From Figure 2, we can see that at all levels of
these different WERs, the HG-ALN model con-
sistently outperforms the B-ALN system (the AU-
DIO model will be discussed below). The Pk
and WindowDiff curves also show that the align-
0 0.1 0.2 0.3 0.4
0.24
0.26
0.28
0.3
0.32
Pk under different WERs
Pk
Word error rate
 
 
B?ALN
HG?ALN
AUDIO
0 0.1 0.2 0.3 0.4
0.34
0.36
0.38
0.4
WindowDiff under different WERs
W
in
do
wD
iff
Word error rate
 
 
B?ALN
HG?ALN
AUDIO
Figure 2: The impact of different WERs on the
alignment models. The performance of an audio-
based model (AUDIO) is also presented.
ment performance is sensitive to recognition er-
rors, particularly when the WER is in the range of
30%?45%, suggesting that the problem we study
here can benefit from the improvement of current
ASR systems in this range, e.g., the recent ad-
vance achieved in (Glass et al, 2007).
6.3 Imposing hierarchical structures onto
raw speech
We can actually impose hierarchical structures di-
rectly onto raw speech, through estimating the
similarity between bullets and speech. This en-
ables navigation through the raw speech by using
slides; e.g., one can hear different parts of speech
by clicking a bullet. We apply keyword spotting to
solve this problem, which detects the occurrences
of each bullet word in the corresponding lecture
audio.
1555
In this paper, we use a token-passing based al-
gorithm provided in the ASR toolkit SONIC (Pel-
lom, 2001). Since the slides are given in advance,
we manually add into the pronunciation dictio-
nary the words that appear in slides but not in
the pronunciation dictionary. To estimate sim-
ilarity between a word vector (discussed earlier
in Section 4.2) and an utterance, we sum up all
keyword-spotting confidence scores assigned be-
tween them, normalize the resulted score by the
length of the vector and the duration of the utter-
ance, and then renormalize it to the range [0, 1]
within the same spoken lecture.
We present the performance of our bullet-audio
alignment model (AUDIO) in Figure 2 so that one
can compare its effectiveness with the transcrip-
tion based methods. The figure shows that the
performance of the AUDIO model is comparable
to the baseline transcription-based model, i.e., B-
ALN, when the WERs of the transcripts are in the
range of 37%?39%. The performance is compara-
ble to the HG-ALN model when WERs are in the
range of 42%?44%. Also, this suggests that incor-
porating hierarchical and global features compen-
sates for the performance degradation of speech
recognition in this range when the WER is 4%-
6% higher.
Note that we did not observe that the perfor-
mance is different when incorporating hierarchi-
cal information and global word distributions into
the AUDIO model, so the AUDIO results in Fig-
ure 2 are the performance of both types of meth-
ods. The current keyword spotting component
yields a high false-positive rate; e.g., it incorrectly
reports many words that are acoustically similar to
parts of other words that really appear in an utter-
ance. This happened even when a high threshold
is set. The noise impairs the benefit of hierarchical
and distribution features.
7 Conclusions and discussions
This paper investigates the problem of imposing
a known hierarchical structure onto an unstruc-
tured spoken document. Results show that incor-
porating local hierarchical constraints and global
word distributions in the efficient dynamic pro-
gramming framework yields a better performance
over the baseline. Further experiments on a wide
range of WERs confirm that the improvement is
consistent, and show that both types of models
are sensitive to speech recognition errors, partic-
ularly when WER increases to 30% and above.
Moreover, directly imposing hierarchical struc-
tures onto raw speech through keyword spotting
achieves competitive performance.
References
Beeferman, D., A. Berger, and J. Lafferty. 1999.
Statistical models for text segmentation. Machine
Learning, 34(1-3):177?210.
Branavan, S., Deshpande P., and Barzilay R. 2007.
Generating a table-of-contents: A hierarchical dis-
criminative approach. In Proc. of Annual Meeting
of the Association for Computational Linguistics.
Chen, Y. and W. J. Heng. 2003. Automatic synchro-
nization of speech transcript and slides in presenta-
tion. In Proc. International Symposium on Circuits
and Systems.
Christensen, H., B. Kolluru, Y. Gotoh, and S. Re-
nals. 2004. From text summarisation to style-
specific summarisation for broadcast news. In Proc.
of the 26th European Conference on Information
Retrieval, pages 223?237.
Fan, Q., K. Barnard, A. Amir, A. Efrat, and M. Lin.
2006. Matching slides to presentation videos using
sift and scene background. In Proc. of ACM Inter-
national Workshop on Multimedia Information Re-
trieval, pages 239?248.
Garofolo, J., G. Auzanne, and E. Voorhees. 2000.
The trec spoken document retrieval track: A success
story. In Proc. of Text Retrieval Conference, pages
16?19.
Glass, J., T. Hazen, S. Cyphers, I. Malioutov,
D. Huynh, and R. Barzilay. 2007. Recent progress
in the mit spoken lecture processing project. Proc.
of Annual Conference of the International Speech
Communication Association, pages 2553?2556.
Hearst, M. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Hori, C. and S. Furui. 2003. A new approach to au-
tomatic speech summarization. IEEE Transactions
on Multimedia, 5(3):368?378.
Hsu, B. and J. Glass. 2006. Style and topic language
model adaptation using hmm-lda. In Proc. of Con-
ference on Empirical Methods in Natural Language
Processing.
1556
Jing, H. 2002. Using hidden markov modeling to
decompose human-written summaries. Computa-
tional Linguistics, 28(4):527?543.
Leeuwis, E., M. Federico, and M. Cettolo. 2003. Lan-
guage modeling and transcription of the ted corpus
lectures. In Proc. of IEEE International Conference
on Acoustics, Speech and Signal Processing.
Liu, T., R. Hjelsvold, and J. R. Kender. 2002. Analysis
and enhancement of videos of electronic slide pre-
sentations. In Proc. IEEE International Conference
on Multimedia and Expo.
Malioutov, I., A. Park, B. Barzilay, and J. Glass. 2007.
Making sense of sound: Unsupervised topic seg-
mentation over acoustic input. In Proc. of Annual
Meeting of the Association for Computational Lin-
guistics, pages 504?511.
Marcu, D. 2000. The theory and practice of discourse
parsing and summarization. The MIT Press.
Maskey, S. 2008. Automatic Broadcast News Speech
Summarization. Ph.D. thesis, Columbia University.
Munteanu, C., R. Baecker, G. Penn, E. Toms, and
E. James. 2006. Effect of speech recognition accu-
racy rates on the usefulness and usability of webcast
archives. In Proc. of ACM Conference on Human
Factors in Computing Systems, pages 493?502.
Munteanu, C., G. Penn, and R. Baecker. 2007.
Web-based language modelling for automatic lec-
ture transcription. In Proc. of Annual Conference
of the International Speech Communication Associ-
ation.
Murray, G. 2008. Using Speech-Specific Character-
istics for Automatic Speech Summarization. Ph.D.
thesis, University of Edinburgh.
Pellom, B. L. 2001. Sonic: The university of col-
orado continuous speech recognizer. Tech. Rep. TR-
CSLR-2001-01, University of Colorado.
Pevsner, L. and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36.
Press, W.H., S.A. Teukolsky, W.T. Vetterling, and B.P.
Flannery. 2007. Numerical recipes: The art of sci-
ence computing. Cambridge University Press.
Ruddarraju, R. 2006. Indexing Presentations Using
Multiple Media Streams. Ph.D. thesis, Georgia In-
stitute of Technology. M.S. Thesis.
Stark, L., S. Whittaker, and J. Hirschberg. 2000. Find-
ing information in audio: A new paradigm for au-
dio browsing and retrieval. In Proc. of International
Conference on Spoken Language Processing.
Wang, F., C. W. Ngo, and T. C. Pong. 2003. Synchro-
nization of lecture videos and electronic slides by
video text analysis. In Proc. of ACM International
Conference on Multimedia.
Zechner, K. and A. Waibel. 2000. Minimizing word
error rate in textual summaries of spoken language.
In Proc. of Applied Natural Language Processing
Conference and Meeting of the North American
Chapter of the Association for Computational Lin-
guistics, pages 186?193.
Zhu, X., X. He, C. Munteanu, and G. Penn. 2008. Us-
ing latent dirichlet alocation to incorporate domain
knowledge for topic transition detection. In Proc.
of Annual Conference of the International Speech
Communication Association.
Zhu, X. 2010. Summarizing Spoken Documents
Through Utterance Selection. Ph.D. thesis, Univer-
sity of Toronto.
1557
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583?593,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Data-Driven Response Generation in Social Media
Alan Ritter
Computer Sci. & Eng.
University of Washington
Seattle, WA 98195
aritter@cs.washington.edu
Colin Cherry
National Research Council Canada
Ottawa, Ontario, K1A 0R6
Colin.Cherry@nrc-cnrc.gc.ca
William B. Dolan
Microsoft Research
Redmond, WA 98052
billdol@microsoft.com
Abstract
We present a data-driven approach to generat-
ing responses to Twitter status posts, based on
phrase-based Statistical Machine Translation.
We find that mapping conversational stimuli
onto responses is more difficult than translat-
ing between languages, due to the wider range
of possible responses, the larger fraction of
unaligned words/phrases, and the presence of
large phrase pairs whose alignment cannot be
further decomposed. After addressing these
challenges, we compare approaches based on
SMT and Information Retrieval in a human
evaluation. We show that SMT outperforms
IR on this task, and its output is preferred over
actual human responses in 15% of cases. As
far as we are aware, this is the first work to
investigate the use of phrase-based SMT to di-
rectly translate a linguistic stimulus into an ap-
propriate response.
1 Introduction
Recently there has been an explosion in the number
of people having informal, public conversations on
social media websites such as Facebook and Twit-
ter. This presents a unique opportunity to build
collections of naturally occurring conversations that
are orders of magnitude larger than those previously
available. These corpora, in turn, present new op-
portunities to apply data-driven techniques to con-
versational tasks.
We investigate the problem of response genera-
tion: given a conversational stimulus, generate an
appropriate response. Specifically, we employ a
large corpus of status-response pairs found on Twit-
ter to create a system that responds to Twitter status
posts. Note that we make no mention of context, in-
tent or dialogue state; our goal is to generate any re-
sponse that fits the provided stimulus; however, we
do so without employing rules or templates, with the
hope of creating a system that is both flexible and
extensible when operating in an open domain.
Success in open domain response generation
could be immediately useful to social media plat-
forms, providing a list of suggested responses to a
target status, or providing conversation-aware auto-
complete for responses in progress. These features
are especially important on hand-held devices (Has-
selgren et al, 2003). Response generation should
also be beneficial in building ?chatterbots? (Weizen-
baum, 1966) for entertainment purposes or compan-
ionship (Wilks, 2006). However, we are most ex-
cited by the future potential of data-driven response
generation when used inside larger dialogue sys-
tems, where direct consideration of the user?s utter-
ance could be combined with dialogue state (Wong
and Mooney, 2007; Langner et al, 2010) to generate
locally coherent, purposeful dialogue.
In this work, we investigate statistical machine
translation as an approach for response generation.
We are motivated by the following observation: In
naturally occurring discourse, there is often a strong
structural relationship between adjacent utterances
(Hobbs, 1985). For example, consider the stimulus-
response pair from the data:
Stimulus: I?m slowly making this soup
...... and it smells gorgeous!
583
Response: I?ll bet it looks delicious too!
Haha
Here ?it? in the response refers to ?this soup? in
the status by co-reference; however, there is also a
more subtle relationship between the ?smells? and
?looks?, as well as ?gorgeous? and ?delicious?. Par-
allelisms such as these are frequent in naturally oc-
curring conversations, leading us to ask whether it
might be possible to translate a stimulus into an ap-
propriate response. We apply SMT to this problem,
treating Twitter as our parallel corpus, with status
posts as our source language and their responses as
our target language. However, the established SMT
pipeline cannot simply be applied out of the box.
We identify two key challenges in adapting SMT
to the response generation task. First, unlike bilin-
gual text, stimulus-response pairs are not semanti-
cally equivalent, leading to a wider range of possible
responses for a given stimulus phrase. Furthermore,
both sides of our parallel text are written in the same
language. Thus, the most strongly associated word
or phrase pairs found by off-the-shelf word align-
ment and phrase extraction tools are identical pairs.
We address this issue with constraints and features to
limit lexical overlap. Secondly, in stimulus-response
pairs, there are far more unaligned words than in
bilingual pairs; it is often the case that large portions
of the stimulus are not referenced in the response
and vice versa. Also, there are more large phrase-
pairs that cannot be easily decomposed (for example
see figure 2). These difficult cases confuse the IBM
word alignment models. Instead of relying on these
alignments to extract phrase-pairs, we consider all
possible phrase-pairs in our parallel text, and apply
an association-based filter.
We compare our approach to response genera-
tion against two Information Retrieval or nearest
neighbour approaches, which use the input stimu-
lus to select a response directly from the training
data. We analyze the advantages and disadvantages
of each approach, and perform an evaluation using
human annotators from Amazon?s Mechanical Turk.
Along the way, we investigate the utility of SMT?s
BLEU evaluation metric when applied to this do-
main. We show that SMT-based solutions outper-
form IR-based solutions, and are chosen over actual
human responses in our data in 15% of cases. As far
as we are aware, this is the first work to investigate
the feasibility of SMT?s application to generating re-
sponses to open-domain linguistic stimuli.
2 Related Work
There has been a long history of ?chatterbots?
(Weizenbaum, 1966; Isbell et al, 2000; Shaikh et
al., 2010), which attempt to engage users, typically
leading the topic of conversation. They usually limit
interactions to a specific scenario (e.g. a Rogerian
psychotherapist), and use a set of template rules for
generating responses. In contrast, we focus on the
simpler task of generating an appropriate response
to a single utterance. We leverage large amounts of
conversational training data to scale to our Social
Media domain, where conversations can be on just
about any topic.
Additionally, there has been work on generat-
ing more natural utterances in goal-directed dia-
logue systems (Ratnaparkhi, 2000; Rambow et al,
2001). Currently, most dialogue systems rely on ei-
ther canned responses or templates for generation,
which can result in utterances which sound very
unnatural in context (Chambers and Allen, 2004).
Recent work has investigated the use of SMT in
translating internal dialogue state into natural lan-
guage (Langner et al, 2010). In addition to dialogue
state, we believe it may be beneficial to consider
the user?s utterance when generating responses in or-
der to generate locally coherent discourse (Barzilay
and Lapata, 2005). Data-driven generation based on
users? utterances might also be a useful way to fill in
knowledge gaps in the system (Galley et al, 2001;
Knight and Hatzivassiloglou, 1995).
Statistical machine translation has been applied to
a smo?rga?sbord of NLP problems, including question
answering (Echihabi and Marcu, 2003), semantic
parsing and generation (Wong and Mooney, 2006;
Wong and Mooney, 2007), summarization (Daume?
III and Marcu, 2009), generating bid-phrases in on-
line advertising (Ravi et al, 2010), spelling correc-
tion (Sun et al, 2010), paraphrase (Dolan et al,
2004; Quirk et al, 2004) and query expansion (Rie-
zler et al, 2007). Most relevant to our efforts is the
work by Soricut and Marcu (2006), who applied the
IBM word alignment models to a discourse order-
ing task, exploiting the same intuition investigated
584
in this paper: certain words (or phrases) tend to trig-
ger the usage of other words in subsequent discourse
units. As far as we are aware, ours is the first work
to explore the use of phrase-based translation in gen-
erating responses to open-domain linguistic stimuli,
although the analogy between translation and dia-
logue has been drawn (Leuski and Traum, 2010).
3 Data
For learning response-generation models, we use
a corpus of roughly 1.3 million conversations
scraped from Twitter (Ritter et al, 2010; Danescu-
Niculescu-Mizil et al, 2011). Twitter conversations
don?t occur in real-time as in IRC; rather as in email,
users typically take turns responding to each other.
Twitter?s 140 character limit, however, keeps con-
versations chat-like. In addition, the Twitter API
maintains a reference from each reply to the post
it responds to, so unlike IRC, there is no need for
conversation disentanglement (Elsner and Charniak,
2008; Wang and Oard, 2009). The first message of a
conversation is typically unique, not directed at any
particular user but instead broadcast to the author?s
followers (a status message). For the purposes of
this paper, we limit the data set to only the first two
utterances from each conversation. As a result of
this constraint, any system trained with this data will
be specialized for responding to Twitter status posts.
4 Response Generation as Translation
When applied to conversations, SMT models the
probability of a response r given the input status-
post s using a log-linear combination of feature
functions. Most prominent among these features
are the conditional phrase-translation probabilities
in both directions, P (s|r) and P (r|s), which ensure
r is an appropriate response to s, and the language
model P (r), which ensures r is a well-formed re-
sponse. As in translation, the response models are
estimated from counts of phrase pairs observed in
the training bitext, and the language model is built
using n-gram statistics from a large set of observed
responses. To find the best response to a given input
status-post, we employ the Moses phrase-based de-
coder (Koehn et al, 2007), which conducts a beam
search for the best response given the input, accord-
ing to the log-linear model.
what . . .  
time . . .  
u  . . . .
get .  . . .
out . .  . .
? . . . . .
i ge
t
off at 5
Figure 1: Example from the data where word alignment
is easy. There is a clear correspondence between words
in the status and the response.
4.1 Challenge: Lexical Repetition
When applied directly to conversation data, off-the-
shelf MT systems simply learn to parrot back the
input, sometimes with slight modification. For ex-
ample, directly applying Moses with default settings
to the conversation data produces a system which
yields the following (typical) output on the above
example:
Stimulus: I?m slowly making this soup
...... and it smells gorgeous!
Response: i?m slowly making this soup
...... and you smell gorgeous!
This ?paraphrasing? phenomenon occurs because
identical word pairs are frequently observed together
in the training data. Because there is a wide range
of acceptable responses to any status, these identical
pairs have the strongest associations in the data, and
therefore dominate the phrase table. In order to dis-
courage lexically similar translations, we filter out
all phrase-pairs where one phrase is a substring of
the other, and introduce a novel feature to penalize
lexical similarity:
?lex(s, t) = J(s, t)
Where J(s, t) is the Jaccard similarity between the
set of words in s and t.
4.2 Challenge: Word Alignment
Alignment is more difficult in conversational data
than bilingual data (Brown et al, 1990), or textual
entailment data (Brockett, 2006; MacCartney et al,
2008). In conversational data, there are some cases
in which there is a decomposable alignment between
585
if . . . .
anyones . . . .
still . . . .
awake . . . .
lets . . . .
play . . . .
a . . . .
game. . . . .
name    .
3    .
kevin    .
costner    .
movies    .
that    .
dont    .
suck    .
. . . . 
eas
ier
qu
est
ion
ple
ase
.
Figure 2: Example from the data where word alignment
is difficult (requires alignment between large phrases in
the status and response).
words, as seen in figure 1, and some difficult cases
where alignment between large phrases is required,
for example figure 2. These difficult sentence pairs
confuse the IBM word alignment models which have
no way to distinguish between the easy and hard
cases.
We aligned words in our parallel data using the
widely used tool GIZA++ (Och and Ney, 2003);
however, the standard growing heuristic resulted in
very noisy alignments. Precision could be improved
considerably by using the intersection of GIZA++
trained in two directions (s? r, and r ? s), but the
alignment also became extremely sparse. The aver-
age number of alignments-per status/response pair
in our data was only 1.7, as compared to a dataset
of aligned French-English sentence pairs (the WMT
08 news commentary data) where the average num-
ber of intersection alignments is 14.
Direct Phrase Pair Extraction
Because word alignment in status/response pairs is
a difficult problem, instead of relying on local align-
ments for extracting phrase pairs, we exploit infor-
mation from all occurrences of the pair in determin-
C(s, t) C(s,?t) C(s)
C(?s, t) C(?s,?t) N ? C(s)
C(t) N ? C(t) N
Figure 3: Contingency table for phrase pair (s,t). Fisher?s
Exact Test estimates the probability of seeing this event,
or one more extreme assuming s and t are independent.
ing whether its phrases form a valid mapping.
We consider all possible phrase-pairs in the train-
ing data,1 then use Fisher?s Exact Test to filter out
pairs with low correlation (Johnson et al, 2007).
Given a source and target phrase s and t, we consider
the contingency table illustrated in figure 3, which
includes co-occurrence counts for s and t, the num-
ber of sentence-pairs containing s, but not t and vice
versa, in addition to the number of pairs containing
neither s nor t. Fisher?s Exact Test provides us with
an estimate of the probability of observing this table,
or one more extreme, assuming s and t are indepen-
dent; in other words it gives us a measure of how
strongly associated they are. In contrast to statistical
tests such as ?2, or the G2 Log Likelihood Ratio,
Fisher?s Exact Test produces accurate p-values even
when the expected counts are small (as is extremely
common in our case).
In Fisher?s Exact Test, the hypergeometric proba-
bility distribution is used to compute the exact prob-
ability of a particular joint frequency assuming a
model of independence:
C(s)!C(?s)!C(t)!C(?t)!
N !C(s, t)!C(?s, t)!C(s,?t)!C(?s,?t)!
The statistic is computed by summing the prob-
ability for the joint frequency in Table 3, and ev-
ery more extreme joint frequency consistent with the
marginal frequencies. Moore (2004) illustrates sev-
eral tricks which make this computation feasible in
practice.
We found that this approach generates phrase-
table entries which appear quite reasonable upon
manual inspection. The top 20 phrase-pairs (after fil-
tering out identical source/target phrases, substrings,
1We define a possible phrase-pair as any pair of phrases
found in a sentence-pair from our training corpus, where both
phrases consist of 4 tokens or fewer. The total number of phrase
pairs in a sentence pair (s, r) is O(|s| ? |r|).
586
Source Target
rt [retweet] thanks for the
potter harry
ice cream
how are you you ?
good morning
chuck norris
watching movie
i miss miss you too
are you i ?m
my birthday happy birthday
wish me luck good luck
how was it was
miss you i miss
swine flu
i love you love you too
how are are you ?
did you i did
jackson michael
how are you i ?m good
michael mj
Table 1: Top 20 Phrase Pairs ranked by the Fisher Exact
Test statistic. Slight variations (substrings or symmetric
pairs) were removed to show more variety. See the sup-
plementary materials for the top 10k (unfiltered) pairs.
and symmetric pairs) are listed in Table 1.2 Our ex-
periments in ?6 show that using the phrase table pro-
duced by Fisher?s Exact Test outperforms one gen-
erated based on the poor quality IBM word align-
ments.
4.3 System Details
For the phrase-table used in the experiments (?6) we
used the 5M phrases with highest association ac-
cording the Fisher Exact Test statistic.3 To build
the language model, we used all of the 1.3M re-
sponses from the training data, along with roughly
1M replies collected using Twitter?s streaming API.
2See the supplementary materials for the top 10k (unfiltered)
phrase pairs.
3Note that this includes an arbitrary subset of the (1,1,1)
pairs (phrase pairs where both phrases were only observed once
in the data). Excluding these (1,1,1) pairs yields a rather small
phrase table, 201K phrase-pairs after filtering, while including
all of them led to a table which was too large for the memory of
the machine used to conduct the experiments.
We do not use any form of SMT reordering
model, as the position of the phrase in the response
does not seem to be very correlated with the corre-
sponding position in the status. Instead we let the
language model drive reordering.
We used the default feature weights provided by
Moses.4 Because automatic evaluation of response
generation is an open problem, we avoided the use of
discriminative training algorithms such as Minimum
Error-Rate Training (Och, 2003).
5 Information Retrieval
One straightforward data-driven approach to re-
sponse generation is nearest neighbour, or informa-
tion retrieval. This general approach has been ap-
plied previously by several authors (Isbell et al,
2000; Swanson and Gordon, 2008; Jafarpour and
Burges, 2010), and is used as a point of compari-
son in our experiments. Given a novel status s and a
training corpus of status/response pairs, two retrieval
strategies can be used to return a best response r?:
IR-STATUS [rargmaxi sim(s,si)] Retrieve the re-sponse ri whose associated status message si
is most similar to the user?s input s.
IR-RESPONSE [rargmaxi sim(s,ri)] Retrieve the re-sponse ri which has highest similarity when di-
rectly compared to s.
At first glance, IR-STATUS may appear to be the
most promising option; intuitively, if an input status
is very similar to a training status, we might expect
the corresponding training response to pair well with
the input. However, as we describe in ?6, it turns
out that directly retrieving the most similar response
(IR-RESPONSE) tends to return acceptable replies
more reliably, as judged by human annotators. To
implement our two IR response generators, we rely
on the default similarity measure implemented in the
Lucene5 Information Retrieval Library, which is an
IDF-weighted Vector-Space similarity.
6 Experiments
In order to compare various approaches to auto-
mated response generation, we used human evalu-
4The language model weight was set to 0.5, the translation
model weights in both directions were both set to 0.2, the lexical
similarity weight was set to -0.2.
5http://lucene.apache.org/
587
ators from Amazon?s Mechanical Turk (Snow et al,
2008). Human evaluation also provides us with data
for a preliminary investigation into the feasibility
of automatic evaluation metrics. While automated
evaluation has been investigated in the area of spo-
ken dialogue systems (Jung et al, 2009), it is unclear
how well it will correlate with human judgment in
open-domain conversations where the range of pos-
sible responses is very large.
6.1 Experimental Conditions
We performed pairwise comparisons of several
response-generation systems. Similar work on eval-
uating MT output (Bloodgood and Callison-Burch,
2010) has asked Turkers to rank more than two
choices, but in order to keep our evaluation as
straightforward as possible, we limited our experi-
ments to pairwise comparisons.
For each experiment comparing 2 systems (a and
b), we built a test set by selecting a random sam-
ple of 200 tweets which had received responses,
and which had a length between 4 and 20 words.
These tweets were selected from conversations col-
lected from a later, non-overlapping time-period
from those used in training. Each experiment used
a different random sample of 200 tweets. For each
of the 200 statuses, we generated a response using
method a and b, then showed the status and both re-
sponses to the Turkers, asking them to choose the
best response. The order of the systems used to
generate a response was randomized, and each of
the 200 HITs was submitted to 3 different Turkers.
Turkers were paid 1? per judgment.
The Turkers were instructed that an appropriate
response should be on the same topic as the sta-
tus, and should also ?make sense? in response to it.
While this is an inherently subjective task, from in-
specting the results, we found Turkers to be quite
competent in judging between two responses.
The systems used in these pairwise comparisons
are summarized in table 2, and example output gen-
erated by each system is presented in Table 3.
6.2 Results
The results of the experiments are summarized in
Table 4. For each experiment we show the fraction
of HITs where the majority of annotators agreed sys-
tem a was better. We also show the p-value from an
System Name Description
RND-BASELINE Picks randomly from the set of
responses which are observed at
least twice in the training data.
The assumption is these are
likely very general responses
IR-STATUS rargmaxi sim(s,si) as describedin ?5
IR-RESPONSE rargmaxi sim(s,ri) as describedin ?5
MT-CHAT Phrase-based translation system
as described in ?4
MT-BASELINE Exactly the same as MT-CHAT,
except using a phrase table ex-
tracted based on word align-
ments from GIZA++
HUMAN Actual responses from the test
data.
Table 2: Summary of systems compared experimentally
exact Binomial significance test; note that all dif-
ferences are significant with above 95% confidence.
Table 4 also shows the S coefficient (Bennett et al,
1954) as a measure of agreement between annotators
for each experiment. S is equivalent the commonly
used Kappa, except it assumes the uniform distribu-
tion when computing expected or chance agreement
(Artstein and Poesio, 2008). The uniform distribu-
tion is appropriate in our setup, since annotators are
not told which system generated each output, and the
order of choices is randomized. If annotators were
indeed choosing randomly, they would be expected
to agree in 50% of cases. In most comparisons the
value of S is in the range of 0.2 to 0.4, which can
be considered ?Fair agreement? (Landis and Koch,
1977).
Note that agreement between annotators is lower
than typically reported in corpus annotation tasks.
When annotating which of two automatically gen-
erated outputs is better, there is not always a clear
answer; both responses might be good or bad. We
can expect strong agreement only in cases where
one response is clearly better. Strong agreement is
not required, however, as we are using many anno-
tations to compare each pair of systems, and the hu-
man judgments are not intended to be used as train-
ing data. Similar agreement was reported in an eval-
uation of automatically generated MT output as part
588
Status MT-CHAT MT-BASELINE IR-STATUS HUMAN RND-BASELINE IR-RESPONSE
riding to the
arc on my bike.
Hopefully I won?t
melt into the
pavement.
good luck on your
bike ride arc
i ca n?t wait for it
! i ?m arc is a year
.
grab some re-
freshing drinks
... and a towel ...
lol ... then eat ice
cream and suika
later ...
make sure to ride
in the middle of
the street
lmfaoooo ! u sure the cup wo
n?t melt ?
I really hate
watching movies
in the living
room.
me too , what
movies are you
doing in room ?
me too ! i love
those ? ) in my
mouth .
:( cheer up ! you
need to invest in
one of those turbo
fans
Why? no comfy
spots or just too
open to public
tas fazendo oq ? someone was
watching in the
living room kasi e
. :) i was too lazy
to go up to my
room . :) )
Hello Did I miss
David Getting
his teen choice
award? Or did
they not show it :(
hello miss ? *
he was getting his
will probably win
.
it ?s just like what
you just did the
exact date hello ?
it did n?t show yet
-ha- -lol-
nothing yet...he
presented though
he is so darn cute
you wish . are they gon na
show teen choice
awards again ? i
missed it
I can think the im-
possible is possi-
ble at any given
moment.
i think you can
do it , anything
is given impossi-
ble dreams really
specs
i ca n?t wait to
see if there is no
point in the end
but if you do a
few hours ...
i hope you use
those powers for
evil . i mean good
! good . that ?s
what i meant .
for sure Eric, and
that makes it not
?impossible? any
longer.
no -user- ? nobody with
whom i might
apply the blue
cobalt ? rabbit ?
at the moment .
given the econ-
omy , perhaps
-url- ( any of em )
I want to enter my
parents house into
a home makeover
contest. Any help
out there?
me too ! thanks
for the win , par-
ents are so smart
, just got home
makeover
oh man , i do
n?t want to come
back to my mom
is getting that
i put troy on the
potty everyday at
lunch and b4 u
know it he was
going on his own
... only took a
week .
check TLC I?m
pretty sure it was
them who were
recently posting
about looking
for houses to be
nominated!
good job you want to do
laundry with me
at my parents
house after i get
off ? maybe get a
free meal out of it
? !
Table 3: Example responses from each system. We tried to pick examples where most (or all) systems generate
reasonable responses for illustration purposes.
System A System B Fraction A p-value Agreement
(S)
System A
BLEU
System B
BLEU
MT-CHAT? IR-STATUS 0.645 5.0e-05 0.347 1.15 0.57
MT-CHAT? IR-RESPONSE 0.593 1.0e-02 0.333 0.84 1.53
IR-STATUS IR-RESPONSE? 0.422 3.3e-02 0.330 0.40 1.59
MT-CHAT? MT-BASELINE 0.577 3.8e-02 0.160 1.23 1.14
MT-CHAT HUMAN? 0.145 2.2e-16 0.433 N/A N/A
MT-CHAT? RND-BASELINE 0.880 2.2e-16 0.383 1.17 0.10
Table 4: Results of pairwise comparisons between various response-generation methods. Each row presents a com-
parison between systems a and b on 200 randomly selected tweets. The column Fraction A lists the fraction of HITs
where the majority of annotators agreed System A?s response was better. The winning system is indicated with an
asterisk?. All differences are significant.
589
of the WMT09 shared tasks (Callison-Burch et al,
2009).6
The results of the paired evaluations provide a
clear ordering on the automatic systems: IR-STATUS
is outperformed by IR-RESPONSE, which is in turn
outperformed by MT-CHAT. These results are
somewhat surprising. We had expected that match-
ing status to status would create a more natural and
effective IR system, but in practice, it appears that
the additional level of indirection employed by IR-
STATUS created only more opportunity for confu-
sion and error. Also, we did not necessarily expect
MT-CHAT?s output to be preferred by human anno-
tators: the SMT system is the only one that generates
a completely novel response, and is therefore the
system most likely to make fluency errors. We had
expected human annotators to pick up on these flu-
ency errors, giving the the advantage to the IR sys-
tems. However, it appears that MT-CHAT?s ability
to tailor its response to the status on a fine-grained
scale overcame the disadvantage of occasionally in-
troducing fluency errors.7
Given MT-CHAT?s success over the IR systems,
we conducted further experiments to validate its out-
put. In order to test how close MT-CHAT?s responses
come to human-level abilities, we compared its out-
put to actual human responses from our dataset. In
some cases the human responses change the topic of
conversation, and completely ignore the initial sta-
tus. For instance, one frequent type of response we
noticed in the data was a greeting: ?How have you
been? I haven?t talked to you in a while.? For the
purposes of this evaluation, we manually filtered out
cases where the human response was completely off-
topic from the status, selecting 200 pairs at random
that met our criteria and using the actual responses
as the HUMAN output.
When compared to the actual human-generated
response, MT-CHAT loses. However, its output is
preferred over the human responses 15% of the time,
a fact that is particularly surprising given the very
small ? by MT standards ? amount of data used to
train the model. A few examples where MT-CHAT?s
output were selected over the human response are
6See inter annotator agreement in table 4.
7Also, as one can see from the example exchanges in Ta-
ble 3, fluency errors are rampant across all systems, including
the gold-standard human responses.
listed in Table 5.
We also evaluated the effect of filtering all possi-
ble phrase pairs using Fisher?s Exact Test, which we
did instead of conducting phrase extraction accord-
ing to the very noisy word alignments. We altered
our MT-CHAT system to use the standard Moses
phrase-extraction pipeline, creating the system de-
noted as MT-BASELINE. We compared this to the
complete MT-CHAT system. Note that both systems
account for lexical repetition as described in ?4.1.
MT-CHAT?s output is preferred 58% of the time over
MT-BASELINE, indicating that direct phrase extrac-
tion is useful in this conversational setting.
Finally, as an additional baseline, we compared
MT-CHAT?s output to random responses selected
from those observed 2 or more times in the train-
ing data. One might argue that short, common re-
sponses are very general, and that a reply like ?lol?
could be considered a good response to almost any
status. However, the human evaluation shows a clear
preference for MT-CHAT?s output: raters favour re-
sponses that are tailored to the stimulus.
6.3 Automatic Evaluation
The field of SMT has benefited greatly from
the existence of an automatic evaluation metric,
BLEU (Papineni et al, 2002), which grades an out-
put candidate according to n-gram matches to one or
more reference outputs. To evaluate whether BLEU
is an appropriate automatic evaluation measure for
response generation, we attempted to measure its
agreement with the human judgments. We calcu-
late BLEU using a single reference derived from our
parallel corpus. We show the smoothed BLEU 1-4
scores for each system on each dataset evaluated in
Table 4. Although these scores are extremely low,
the overall BLEU scores agree with overall annota-
tor judgments in all cases except when comparing
MT-CHAT and IR-RESPONSE. It would seem that
BLEU has some agreement with human judgments
on this task, but perhaps not enough to be immedi-
ately useful.
7 Conclusions
We have presented several data-driven approaches,
based on information retrial and phrase-based SMT,
to the task of automatically responding to open-
590
Status MT-CHAT HUMAN
i want to go to a bonfire right about
now.
that does sound fun bonfire i wan na
go !
did my tweet bring that upon you ?
hehe
I?m the invisible man. man , you could never be invisible i can still see your face surrounded
in blue !
put ya number in my phone ..and
ima call you up !
call me when u do that in your phone
call :) for what ?s up ?
o well hw kum u dnt eva call or txt
me :) ? ! ? ! lolz jpn hw r ya tho :)
? ! ? !
Gray Maynard with hair? I did not
approve this.
maynard with grey hair ! i take it back, the hair isn?t working
for maynard.
Table 5: Examples where MT-CHAT output was preferred over HUMAN response by Turker annotators
domain linguistic stimuli.
Our experiments show that SMT techniques are
better-suited than IR approaches on the task of re-
sponse generation. Our system, MT-CHAT, pro-
duced responses which were preferred by human an-
notators over actual human responses 15% of the
time. Although this is still far from human-level
performance, we believe there is much room for
improvement: from designing appropriate word-
alignment and decoding algorithms that account for
the selective nature of response in dialogue, to sim-
ply adding more training data.
We described the many challenges posed by
adapting phrase-based SMT to dialogue, and pre-
sented initial solutions to several, including direct
phrasal alignment, and phrase-table scores discour-
aging responses that are lexically similar to the sta-
tus. Finally, we have provided results from an initial
experiment to evaluate the BLEU metric when ap-
plied to response generation, showing that though
the metric as is does not work well, there is suffi-
cient correlation to suggest that a similar, dialogue-
focused approach may be feasible.
By generating responses to Tweets out of context,
we have demonstrated that the models underlying
phrase-based SMT are capable of guiding the con-
struction of appropriate responses. In the future, we
are excited about the role these models could po-
tentially play in guiding response construction for
conversationally-aware chat input schemes, as well
as goal-directed dialogue systems.
Acknowledgments
We would like to thank Oren Etzioni, Michael
Gamon, Jerry Hobbs, Dirk Hovy, Yun-Cheng Ju,
Kristina Toutanova, Saif Mohammad, Patrick Pan-
tel, and Luke Zettlemoyer, in addition to the anony-
mous reviewers for helpful discussions and com-
ments on a previous draft. The first author is sup-
pored by a National Defense Science and Engineer-
ing Graduate (NDSEG) Fellowship 32 CFR 168a.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput. Lin-
guist., 34:555?596, December.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: an entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05.
E. M. Bennett, R. Alpert, and A. C. Goldstein. 1954.
Communications through limited-response question-
ing. Public Opinion Quarterly, 18(3):303?308.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using mechanical turk to build machine translation
evaluation sets. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, CSLDAMT
?10, pages 208?211, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Chris Brockett. 2006. Aligning the rte 2006 corpus. In
Microsoft Research Techincal report MSR-TR-2007-
77.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Comput.
Linguist., 16:79?85, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation, StatMT ?09.
591
Nathanael Chambers and James Allen. 2004. Stochas-
tic language generation in a dialogue system: Toward
a domain independent generator. In Michael Strube
and Candy Sidner, editors, Proceedings of the 5th SIG-
dial Workshop on Discourse and Dialogue, pages 9?
18, Cambridge, Massachusetts, USA, April 30 - May
1. Association for Computational Linguistics.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark my words! Linguistic
style accommodation in social media. In Proceedings
of WWW.
Hal Daume? III and Daniel Marcu. 2009. Induction of
word and phrase alignments for automatic document
summarization. CoRR, abs/0907.0804.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics, COLING ?04, Morristown, NJ,
USA. Association for Computational Linguistics.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics - Volume 1, ACL
?03, pages 16?23, Morristown, NJ, USA. Association
for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2008. You talking
to me? a corpus and algorithm for conversation disen-
tanglement. In Proceedings of ACL-08: HLT, June.
Michel Galley, Eric Fosler-Lussier, and Alexandros
Potamianos. 2001. Hybrid natural language gener-
ation for spoken dialogue systems. In Proceedings
of the 7th European Conference on Speech Commu-
nication and Technology (EUROSPEECH?01), pages
1735?1738, Aalborg, Denmark, September.
Jon Hasselgren, Erik Montnemery, Pierre Nugues, and
Markus Svensson. 2003. Hms: a predictive text entry
method using bigrams. In Proceedings of the 2003
EACL Workshop on Language Modeling for Text Entry
Methods, TextEntry ?03.
Jerry R. Hobbs. 1985. On the coherence and structure of
discourse.
Charles Lee Isbell, Jr., Michael J. Kearns, Dave Ko-
rmann, Satinder P. Singh, and Peter Stone. 2000.
Cobot in lambdamoo: A social statistics agent. In Pro-
ceedings of the Seventeenth National Conference on
Artificial Intelligence and Twelfth Conference on In-
novative Applications of Artificial Intelligence, pages
36?41. AAAI Press.
Sina Jafarpour and Christopher J. C. Burges. 2010. Fil-
ter, rank, and transfer the knowledge: Learning to chat.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Comput. Speech Lang.,
23:479?509, October.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings of
the 33rd annual meeting on Association for Computa-
tional Linguistics, ACL ?95.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL. The
Association for Computer Linguistics.
J R Landis and G G Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics.
Brian Langner, Stephan Vogel, and Alan W. Black. 2010.
Evaluating a dialog language generation system: com-
paring the mountain system to other nlg approaches.
In INTERSPEECH.
Anton Leuski and David R. Traum. 2010. Practical
language processing for virtual humans. In Twenty-
Second Annual Conference on Innovative Applications
of Artificial Intelligence (IAAI-10).
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 802?811, Morristown,
NJ, USA. Association for Computational Linguistics.
Robert C. Moore. 2004. On log-likelihood-ratios and the
significance of rare events. In EMNLP.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL, pages 311?318.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 142?149.
592
Owen Rambow, Srinivas Bangalore, and Marilyn Walker.
2001. Natural language generation in dialog systems.
In Proceedings of the first international conference on
Human language technology research, HLT ?01, pages
1?4, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference.
Sujith Ravi, Andrei Broder, Evgeniy Gabrilovich, Vanja
Josifovski, Sandeep Pandey, and Bo Pang. 2010. Au-
tomatic generation of bid phrases for online advertis-
ing. In Proceedings of the third ACM international
conference on Web search and data mining, WSDM
?10.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
464?471, Prague, Czech Republic, June. Association
for Computational Linguistics.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 172?180,
Morristown, NJ, USA. Association for Computational
Linguistics.
Samira Shaikh, Tomek Strzalkowski, Sarah Taylor, and
Nick Webb. 2010. Vca: an experiment with a mul-
tiparty virtual chat agent. In Proceedings of the 2010
Workshop on Companionable Dialogue Systems.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, COLING-ACL ?06.
Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk.
2010. Learning phrase-based spelling error models
from clickthrough data. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 266?274, Morristown,
NJ, USA. Association for Computational Linguistics.
Reid Swanson and Andrew S. Gordon. 2008. Say any-
thing: A massively collaborative open domain story
writing companion. In Proceedings of the 1st Joint
International Conference on Interactive Digital Story-
telling: Interactive Storytelling, ICIDS ?08, pages 32?
40, Berlin, Heidelberg. Springer-Verlag.
Lidan Wang and Douglas W. Oard. 2009. Context-based
message expansion for disentanglement of interleaved
text conversations. In HLT-NAACL.
Joseph Weizenbaum. 1966. Eliza: a computer program
for the study of natural language communication be-
tween man and machine. Commun. ACM, 9:36?45,
January.
Yorick Wilks. 2006. Artificial companions as a new kind
of interface to the future internet. In OII Research Re-
port No. 13.
Yuk Wah Wong and Raymond Mooney. 2006. Learning
for semantic parsing with statistical machine transla-
tion. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Main Conference.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
593
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1948?1959,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Regularized Minimum Error Rate Training
Michel Galley
Microsoft Research
mgalley@microsoft.com
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Colin Cherry
National Research Council
colin.cherry@nrc-cnrc.gc.ca
Kristina Toutanova
Microsoft Research
kristout@microsoft.com
Abstract
Minimum Error Rate Training (MERT) re-
mains one of the preferred methods for tun-
ing linear parameters in machine translation
systems, yet it faces significant issues. First,
MERT is an unregularized learner and is there-
fore prone to overfitting. Second, it is com-
monly used on a noisy, non-convex loss func-
tion that becomes more difficult to optimize
as the number of parameters increases. To ad-
dress these issues, we study the addition of
a regularization term to the MERT objective
function. Since standard regularizers such as
`2 are inapplicable to MERT due to the scale
invariance of its objective function, we turn to
two regularizers?`0 and a modification of `2?
and present methods for efficiently integrating
them during search. To improve search in large
parameter spaces, we also present a new direc-
tion finding algorithm that uses the gradient of
expected BLEU to orient MERT?s exact line
searches. Experiments with up to 3600 features
show that these extensions of MERT yield re-
sults comparable to PRO, a learner often used
with large feature sets.
1 Introduction
Minimum Error Rate Training emerged a decade
ago (Och, 2003) as a superior training method for
small numbers of linear model parameters of machine
translation systems, improving over prior work using
maximum likelihood criteria (Och and Ney, 2002).
This technique quickly rose to prominence, becom-
ing standard in many research and commercial MT
systems. Variants operating over lattices (Macherey
et al, 2008) or hypergraphs (Kumar et al, 2009) were
subsequently developed, with the benefit of reducing
the approximation error from n-best lists.
The primary advantages of MERT are twofold. It
directly optimizes the evaluation metric under consid-
eration (e.g., BLEU) instead of some surrogate loss.
Secondly, it offers a globally optimal line search. Un-
fortunately, there are several potential difficulties in
scaling MERT to larger numbers of features, due
to its non-convex loss function and its lack of reg-
ularization. These challenges have prompted some
researchers to move away from MERT, in favor of lin-
early decomposable approximations of the evaluation
metric (Chiang et al, 2009; Hopkins and May, 2011;
Cherry and Foster, 2012), which correspond to easier
optimization problems and which naturally incorpo-
rate regularization. In particular, recent work (Chiang
et al, 2009) has shown that adding thousands or tens
of thousands of features can improve MT quality
when weights are optimized using a margin-based
approximation. On simulated datasets, Hopkins and
May (2011) found that conventional MERT strug-
gles to find reasonable parameter vectors, where a
smooth loss function based on Pairwise Ranking Op-
timization (PRO) performs much better; on real data,
this PRO method appears at least as good as MERT
on small feature sets, and also scales better as the
number of features increases.
In this paper, we seek to preserve the advantages
of MERT while addressing its shortcomings in terms
of regularization and search. The idea of adding a
regularization term to the MERT objective function
can be perplexing at first, because the most common
regularizers, such as `1 and `2, are not directly appli-
cable to MERT. Indeed, these regularizers are scale
sensitive, while the MERT objective function is not:
scaling the weight vector neither changes the predic-
tions of the linear model nor affects the error count.
Hence, MERT can hedge any regularization penalty
by maximally scaling down linear model weights.
The first contribution of this paper is to analyze var-
ious forms of regularization that are not susceptible
to this scaling problem. We analyze and experiment
with `0, a form of regularization that is scale insen-
sitive. We also present new parameterizations of `2
1948
regularization, where we apply `2 regularization to
scale-senstive linear transforms of the original linear
model. In addition, we introduce efficient methods
of incorporating regularization in Och (2003)?s exact
line searches. For all of these regularizers, our meth-
ods let us find the true optimum of the regularized
objective function along the line.
Finally, we address the issue of searching in a
high-dimensional space by using the gradient of ex-
pected BLEU (Smith and Eisner, 2006) to find better
search directions for our line searches. This direction
finder addresses one of the serious concerns raised
by Hopkins and May (2011): MERT widely failed
to reach the optimum of a synthetic linear objective
function. In replicating Hopkins and May?s experi-
ments, we confirm that existing search algorithms for
MERT?including coordinate ascent, Powell?s algo-
rithm (Powell, 1964), and random direction sets (Cer
et al, 2008)?perform poorly in this experimental
condition. However, when using our gradient-based
direction finder, MERT has no problem finding the
true optimum even in a 1000-dimensional space.
Our results suggest that the combination of a reg-
ularized objective function and a gradient-informed
line search algorithm enables MERT to scale well
with a large number of features. Experiments with
up to 3600 features show that these extensions of
MERT yield results comparable to PRO (Hopkins
and May, 2011), a parameter tuning method known
to be effective with large feature sets.
2 Unregularized MERT
Prior to introducing regularized MERT, we briefly
review standard unregularized MERT (Och, 2003).
We use fS1 = {f1 . . . fS} to denote the S input sen-
tences of a given tuning set. For each sentence fs, let
Cs = {es,1 . . . es,M} denote the list of M -best can-
didate translations. Each input and output sentence
pair (fs, es,m) is weighted using a linear model that
applies model parameters w = (w1 . . . wD) ? RD
to D feature functions h1(f , e,?) . . . hD(f , e,?),
where ? is the hidden state associated with the
derivation from f to e, such as phrase segmenta-
tion and alignment. Furthermore, let hs,m ? RD
denote the feature vector representing the translation
pair (fs, es,m).
In MERT, the goal is to minimize a loss function
E(r, e) that scores translation hypotheses against a
set of reference translations rS1 = {r1 . . . rS}. This
yields the following optimization problem:
w? = argmin
w
{ S?
s=1
E(rs, e?(fs;w))
}
=
argmin
w
{ S?
s=1
M?
m=1
E(rs, es,m)?(es,m, e?(fs;w))
}
(1)
where
e?(fs;w) = argmax
m?{1...M}
{
w?hs,m
}
(2)
While the error surface of Equation 1 is only an
approximation of the true error surface of the MT
decoder, the quality of this approximation depends
on the size of the hypothesis space represented by the
M -best list. Therefore, the hypothesis list is grown
iteratively: decoding with an initial parameter vector
seeds the M -best lists; next, parameter estimation
and M -best list gathering alternate until the cumula-
tive M -best list no longer grows, or until changes of
w between two decoding runs are deemed too small.
To increase the size of the hypothesis space, subse-
quent work (Macherey et al, 2008) instead operated
on lattices, but this paper focuses on M -best lists.
A crucial observation is that the unsmoothed error
count represented in Equation 1 is a piecewise con-
stant function. This enabled Och (2003) to devise a
line search algorithm guaranteed to find the optimum
point along the line. To extend the search from one
to multiple dimensions, MERT applies a sequence
of line optimizations along some fixed or variable
set of search directions {dt} until some convergence
criteria are met. Considering a given point wt and
a given direction dt at iteration t, finding the most
probable translation hypothesis in the set of candi-
dates translations Cs = {es,1 . . . es,M} corresponds
to solving the following optimization problem:
e?(fs; ?) = argmax
m?{1...M}
{
(wt + ? ? dt)
?hs,m
}
(3)
The function in this equation is piecewise linear (Pa-
pineni, 1999), which enables an efficient exhaustive
computation. Specifically, this function is optimized
by enumerating the up to M hypotheses that form
the upper envelope of the model score function. The
error count, then, is a piecewise constant function
1949
defined by the points ?fs1 < ? ? ? < ?
fs
M at which an in-
crease in ? causes a change of optimum in Equation 3.
Error counts for the whole corpus are simply the sums
of sentence-level piecewise constant functions aggre-
gated over all sentences of the corpus.1 The optimal ?
is finally computed by enumerating all piecewise con-
stant intervals of the corpus-level error function, and
by selecting the one that has the lowest error count
(or, correspondingly, highest BLEU score). Assum-
ing the optimum is found in the interval [?k?1, ?k],
we define ?opt = (?k?1 + ?k)/2 and change the pa-
rameters using the update wt+1 = wt + ?opt ? dt.
Finally, this method is turned into a global D-
dimensional search using algorithms that repeat-
edly use the aforementioned exact line search algo-
rithm. Och (2003) first advocated the use of Powell?s
method (Powell, 1964; Press et al, 2007). Pharaoh
(Koehn, 2004) and subsequently Moses (Koehn et al,
2007) instead use coordinate ascent, and more recent
work often uses random search directions (Cer et al,
2008; Macherey et al, 2008). In Section 4, we will
present a novel direction finder for maximum-BLEU
optimization, which uses the gradient of expected
BLEU to find directions where the BLEU score is
most likely to increase.
3 Regularization for MERT
Because MERT is prone to overfitting when a large
number of parameters must be optimized, we study
the addition of a regularization term to the objective
function. One conventional approach is to regularize
the objective function with a penalty based on the
Euclidean norm ||w||2 =
??
iw
2
i , also known as `2
regularization. In the case of MERT, this yields the
following objective function:2
w? = argmin
w
{ S?
s=1
E(rs, e?(fs;w)) +
||w||22
2?2
}
(4)
1This assumes that the sufficient statistics of the metric under
consideration are additively decomposable by sentence, which
is the case with most popular evaluation metrics such as BLEU
(Papineni et al, 2001).
2The `2 regularizer is often used in conjunction with log-
likelihood objectives. The regularization term of Equation 4
could similarly be added to the log of an objective?e.g.,
log(BLEU) instead of BLEU?but we found that the distinc-
tion doesn?t have much of an impact in practice.
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
MERT
Max at 0.225
?
?
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
MERT? `2
Max at -0.018
?
?
?`2
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
?, the step size in the current direction
MERT? `0
Max at 0
?
?
`0
Figure 1: Example MERT values along one coordi-
nate, first unregularized. When regularized with `2, the
piecewise constant function becomes piecewise quadratic.
When using `0, the function remains piecewise constant
with a point discontinuity at 0.
where the regularization term 1/2?2 is a free param-
eter that controls the strength of the regularization
penalty. Similar regularizers have also been used
in conjunction with other norms, such as `1 and `0
norms. The `1 norm, defined as ||w||1 =
?
i |wi|,
applies a constant force toward zero, preferring vec-
tors with fewer non-zero components; `0, defined as
||w||0 = |{i | wi 6= 0}|, simply counts the number of
non-zero components of the weight vector, encoding
a preference for sparse vectors.
Geometrically, `2 is a parabola, `1 is the wedge-
shaped absolute value function, and `0 is an impulse
function with a spike at 0. The original formulation
(Equation 1) of MERT consists of a piecewise con-
stant representation of the loss, as a function of the
step size in a given direction. But with these three reg-
1950
ularization terms, the function respectively becomes
piecewise quadratic, piecewise linear, or piecewise
constant with a potential impulse jump for each dis-
tinct choice of regularizer. Figure 1 demonstrates this
effect graphically.
As discussed in (McAllester and Keshet, 2011),
the problem with optimizing Equation 4 directly is
that the output of the underlying linear classifier, and
therefore the error count, are not sensitive to the scale
of w. Moreover, `2 regularization (as well as `1 reg-
ularization) is scale sensitive, which means any op-
timizer of this function can drive the regularization
term down to zero by scaling down w. As special
treatments for `2, we evaluate three linear transforms
of the weight vector, where the vector w of the regu-
larization term ||w||22/2?
2 is replaced with either:
1. an affine transform: w? w0
2. a vector with only (D ? 1) free parameters, e.g.,
(1, w?2, ? ? ? , w
?
D)
3. an `1 renormalization: w/||w||1
In (1), regularization is biased towards w0, a weight
vector previously optimized using a competitive yet
much smaller feature set, such as core features of
a phrase-based (Koehn et al, 2007) or hierarchical
(Chiang, 2007) system. The requirement that this
feature set be small is to prevent overfitting. Other-
wise, any regularization toward an overfit parameter
vector w0 would defeat the purpose of introducing
a regularization term in the first place.3 In (2), the
transformation is motivated by the observation that
the D-parameter linear model of Equation 2 only
needs (D ? 1) degrees of freedom. Fixing one of
the components of w to any non-zero constant and
allowing the others to vary, the new linear model re-
tains the same modeling power, but the (D ? 1) free
parameters are no longer scale invariant, i.e., scaling
the (D ? 1)-dimensional vector now has an effect on
linear model predictions. In (3), the weight vector
is normalized as to have an `1-norm equal to 1. In
contrast, the `0 norm is scale insensitive, thus not
affected by this problem.
3.1 Exact line search with regularization
Optimizing with a regularized error surface requires
a change in the line search algorithm presented in
3(Gimpel and Smith, 2012, footnote 6) briefly mentions the
use of such a regularizer with its ramp loss objective function.
Section 2, but the other aspects of MERT remain the
same, and we can still use global search algorithms
such as coordinate ascent, Powell, and random di-
rections exactly the same way as with unregularized
MERT. Line search with a regularization term is still
as efficient as in (Och, 2003), and it is still guar-
anteed to find the optimum of the (now regularized)
objective function along the line. Considering again a
given point wt and a given direction dt at line search
iteration t, finding the optimum ?opt corresponds to
finding ? that minimizes:
S?
s=1
E(rs, e?(fs; ?)) +
||wt + ? ? dt||22
2?2
(5)
Since regularization does not affect the points at
which e?(fs; ?) changes its optimum, the points
?fs1 < ? ? ? < ?
fs
M of intersection in the upper enve-
lope remain the same, so the points of discontinuity
in the error surface remain the same. The difference
now is that the error count on each segment [?i?1, ?i]
is no longer constant. This means we need to adjust
the final step of line search, which consists of enu-
merating all [?i?1, ?i], and keeping the optimum of
Equation 5 for each segment. e?(fs; ?) remains con-
stant within the segment, so we only need to consider
the expression ||wt + ? ? dt||22 to select a segment
point. The optimum is either at the left edge, the right
edge, or in the middle if the vertex of the parabola
happens to lie within that segment.4 We compute
this optimum by finding the value ? for which the
derivative of the regularization term is zero. There is
an easy closed-form solution:
d
d?
[
||wt + ? ? dt||22
2?2
]
= 0
d
d?
[
?
i
(w2t,i + 2 ? ? ? wt,i ? dt,i + ?
2 ? d2t,i)
]
= 0
?
i
(2 ? wt,i ? dt,i + 2 ? ? ? d
2
t,i) = 0
? = ?
(?
i
wt,i ? dt,i
)/(?
i
d2t,i
)
= ?
wt?dt
dt?dt
This closed-form solution is computed in time pro-
portional to D, which doesn?t slow down the com-
4When the optimum is either at the left edge ?i?1 or right
edge ?i of a segment, we select a point at a small relative distance
within the segment (.999?i?1 + .001?i, in the former case) to
avoid ties in objective values.
1951
putation of Equation 5 for each segment (the con-
struction of each segment of the upper envelope is
proportional to D anyway).
We also use `0 regularization. While minimiza-
tion of the `0-norm is known to be NP-hard in gen-
eral (Hyder and Mahata, 2009), this optimization is
relatively trivial in the case of a line search. Indeed,
for a given segment, the value in Equation 5 is con-
stant everywhere except where we intersect any of
the coordinate hyperplanes, i.e., where one of the
coordinates is zero. Thus, our method consists of
evaluating Equation 5 at the intersection points be-
tween the line and coordinate hyperplanes, returning
the optimal point within the given segment. For any
segment that doesn?t cross any of these hyperplanes,
we evaluate the objective function at any point of the
segment (since the value is constant across the entire
segment).
4 Direction finding
4.1 A Gradient-based direction finder
Perhaps the greatest obstacle in scaling MERT
to many dimensions is finding good search direc-
tions. In problems of lower dimensions, iterating
through all the coordinates is computationally feasi-
ble, though not guaranteed to find a global maximum
even in the case of a perfect line search. As the
number of dimensions increases by orders of mag-
nitude, this coordinate direction approach becomes
less and less tractable, and the quality of the search
also suffers (Hopkins and May, 2011).
Optimization has traditionally relied on finding the
direction of steepest ascent: the gradient. Unfortu-
nately, the objective function optimized by MERT is
piecewise constant; while it may admit a subgradi-
ent, this direction is generally not very informative.
Instead we may consider a smoothed variation of the
original approximation. While some variants have
been considered (Och, 2003; Flanigan et al, 2013),
we use an expected BLEU approximation, assum-
ing hypotheses are drawn from a log-linear distri-
bution according to their parameter values (Smith
and Eisner, 2006). That is, we assume the proba-
bility of a translation candidate es,m is proportional
to (exp (w?hs,m))
?, where w are the parameters be-
ing optimized, hs,m is the vector of the features for
es,m, and ? is a scaling parameter. As ? approaches
infinity, the distribution places all its weight on the
highest scoring candidate.
The log of the BLEU score may be written as:
min
(
1?
R
C
, 0
)
+
1
N
N?
n=1
(logmn ? log cn)
where R is the sum of reference lengths across the
corpus, C is the sum of candidate lengths, mn is the
number of matched n-grams (potentially clipped),
and cn is the number of n-grams in all candidates.
Given a distribution over candidates, we can use
the expected value of the log of the BLEU score. This
is a smooth approximation to the BLEU score, which
asymptotically approaches the true BLEU score as
the scaling parameter ? approaches infinity. While
this expectation is difficult to compute exactly, we
can compute approximations thereof using Taylor se-
ries. Although prior work demonstrates that a second-
order Taylor approximation is feasible to compute
(Smith and Eisner, 2006), we find that a first-order
approximation is faster and very close to the second-
order approximation.5 The first order Taylor approxi-
mation is as follows:
min
(
1?
R
E[C]
, 0
)
+
1
N
N?
n=1
(logE[mn]? logE[cn])
where E is the expectation operator using the proba-
bility distribution P (h;w, ?).
First we note that the gradient ??wiP (h;w, ?) is
P (h;w, ?)
(
hi ?
?
h?
h?iP (h
?;w, ?)
)
Using the chain rule, the gradient of the first order
approximation to BLEU is as follows:
1
N
N?
n=1
( 1
E[mn]
?
h
mn(h)
?P (h;w, ?)
?wi
?
1
E[cn]
?
h
cn(h)
?P (h;w, ?)
?wi
)
+
{
0 if E[C] > R
R
E[C]2
?
h c1(h)
?P (h;w,?)
?wi
otherwise
5Experimentally, we compared our analytical gradient of
the first-order Taylor approximation with the finite-difference
gradients of the first- and second-order approximations, and we
found these three gradients to be very close in terms of cosine
similarity (> 0.99). We performed these measurements both at
arbitrary points and at points of convergence of MERT.
1952
In the case of `2-regularized MERT, the final gradi-
ent also includes the partial derivative of the regular-
ization penalty of Equation 4, which is wi/?2 for a
given component i of the gradient. We do not update
the gradient in the case of `0 regularization since the
`0-norm is not differentiable.
4.2 Search
Our search strategy consists of looking at the direc-
tions of steepest increase of expected BLEU, which
is similar to that of Smith and Eisner (2006), but with
the difference that we do so in the context of MERT.
We think this difference provides two benefits. First,
while the smooth approximation of BLEU reduces
the likelihood of remaining trapped in a local opti-
mum, we avoid approximation error by retaining the
original objective function. Second, the benefit of
exact line searches in MERT is that there is no need
to be concerned about step size, since step size in
MERT line searches is guaranteed to be optimal with
respect to the direction under consideration.
Finally, our gradient-based search algorithm oper-
ates as follows. Considering the current point wt, we
compute the gradient gt of the first order Taylor ap-
proximation at that point, using the current scaling pa-
rameter ?. (We initialize the search with ? = 0.01.)
We find the optimum along the line wt+? ?gt. When-
ever any given line search yields no improvement
larger than a small tolerance threshold, we multiply
? by two and perform a new line search. The increase
of this parameter ? corresponds to a cooling schedule
(Smith and Eisner, 2006), which progressively sharp-
ens the objective function to get a better estimate of
BLEU as the search converges to an optimum. We
repeatedly perform new line searches until ? exceeds
1000. The inability to improve the current optimum
with a sharp approximation (? > 1000) doesn?t mean
line searches would fail with smaller values, so we
find it helpful to repeat the above procedure until a
full pass of updates of ? from 0.01 to 1000 yields no
improvement.
4.3 Computational complexity
Computing the gradient increases the computational
cost of MERT, though not its asymptotic complexity.
The cost of a single exhaustive line search is
O (SM(D + logM + logS))
where S is the number of sentences, each with M
possible translations, andD is the number of features.
For each sentence, we first identify the model score
as a linear function of the step size, requiring two
dot products for an overall cost of O(SMD).6 Next
we construct the upper envelope for each sentence:
first the equations are sorted in increasing order of
slope, and then they are merged in linear time to form
an envelope, with an overall cost of O(SM logM).
A linear pass through the envelope converts these
into piecewise constant (or linear, or quadratic) repre-
sentations of the (regularized) loss function. Finally
the per-sentence envelopes are merged into a global
representation of the loss along that direction. Our
implementation successively merges adjacent pairs
of piecewise smooth loss function representations
until a single list remains. These logS passes lead to
a merging runtime of O(SM logS).
The time required to compute a gradient is pro-
portional to O(SMD). For each sentence, we first
gather the probability and its gradient, then use this to
compute expected n-gram counts and matches as well
as those gradients in time O(MD). A constant num-
ber of arithmetic operations suffice to compute the
final expected loss value and its gradient. Therefore,
computing the gradient does not increase the algo-
rithmic complexity when compared to conventional
approaches using coordinate ascent and random di-
rections. Likewise the runtime of a single iteration
is competitive with PRO, given that gradient finding
is generally the most expensive part of convex opti-
mization. Of course, it is difficult to compare overall
runtime of convex optimization with that of MERT,
as we know of no way to bound the number of gradi-
ent evaluations required for convergence with MERT.
Therefore, we resort to empirical comparison later in
the paper, and find that the two methods appear to
have comparable runtime.
6In the special case where the difference between the prior
direction and the current direction is sparse, we may update the
individual linear functions in time proportional to the number of
changed dimensions. Coordinate ascent in particular can update
the linear functions in time O(SM): to the intercept of the
equation for each translation, we may add the prior step size
multiplied by the feature value in the prior coordinate, and the
slope becomes the feature value in the new coordinate. However,
this optimization does not appear to be widely adopted, likely
because it does not lead to any speedup when random vectors,
conjugate directions, or other non-sparse directions are used.
1953
Language pair Train Tune Dev Test
G
B
M
Chinese-English 0.99M 1,797 1,000 1,082
(mt02+03) (mt05)
Finnish-English 2.20M 11,935 2,001 4,855
S
pa
rs
eH
R
M Chinese-English 3.51M 1,894 1,664 1,357
(mt05) (mt06) (mt08)
Arabic-English 1.49M 1,663 1,360 1,313
(mt06) (mt08) (mt09)
Table 1: Datasets for the two experimental conditions.
5 Experimental Design
Following Hopkins and May (2011), our experimen-
tal setup utilizes both real and synthetic data. The
motivation for using synthetic data is that it is a way
of gauging the quality of optimization methods, since
the data is constructed knowing the global optimum.
Hopkins and May also note that the use of an ob-
jective function that is linear in some gold weight
vector makes the search much simpler than in a real
translation setting, and they suggest that a learner
that performs poorly in such a simple scenario has
little hope of succeeding in a more complex one.
The setup of our synthetic data experiment is al-
most the same as that performed by Hopkins and
May (2011). We generate feature vectors of dimen-
sionality ranging from 10 to 1000. These features are
generated by drawing random numbers uniformly in
the interval [0, 500]. This synthetic dataset consists
of S=1000 source ?sentences?, and M=500 ?trans-
lation? hypotheses for each sentence. A pseudo
?BLEU? score is then computed for each hypothe-
sis, by computing the dot product between a prede-
fined gold weight vector w? and each feature vector
hs,m. By this linear construction, w? is guaranteed
to be a global optimum.7 The pseudo-BLEU score is
normalized for each M -best list, so that the transla-
tion with highest model score according to w? has
a BLEU score of 1, and so that the translation with
lowest model score for the sentence gets a BLEU of
zero. This normalization has no impact on search,
but makes results more interpretable.
For our translation experiments, we use multi-
stack phrase-based decoding (Koehn et al, 2007).
We report results for two feature sets: non-linear
features induced using Gradient Boosting Machines
(Toutanova and Ahn, 2013) and sparse lexicalized
7The objective function remains piecewise constant, and the
plateau containingw? maps to the optimal value of the function.
reordering features (Cherry, 2013). We exploit these
feature sets (GBM and SparseHRM, respectively) in
two distinct experimental conditions, which we de-
tail in the two next paragraphs. Both GBM and
SparseHRM augment baseline features similar to
Moses?: relative frequency and lexicalized phrase
translation scores for both translation directions; one
or two language model features, depending on the
language pair; distortion penalty; word and phrase
count; six lexicalized reordering features. For both
experimental conditions, phrase tables have maxi-
mum phrase length of 7 words on either side. In
reference to Table 1, we used the training set (Train)
for extracting phrase tables and language models; the
Tune set for optimization with MERT or PRO; the
Dev set for selecting hyperparameters of PRO and
regularized MERT; and the Test set for reporting fi-
nal results. In each experimental condition, we first
trained weights for the base feature sets, and then
decoded the Tune, Dev, and Test datasets, generating
500-best lists for each set. All results report rerank-
ing performance on these lists with different feature
sets and optimization methods, based on lower-cased
BLEU (Papineni et al, 2001).
The GBM feature set (Toutanova and Ahn, 2013)
consists of about 230 features automatically induced
using decision tree weak learners, which derive fea-
tures using various word-level, phrase-level, and mor-
phological attributes. For Chinese-English, the train-
ing corpus consists of approximately one million sen-
tence pairs from the FBIS and Hong Kong portions
of the LDC data for the NIST MT evaluation and the
Tune and Test sets are from NIST competitions. A
4-gram language model was trained on the Xinhua
portion of the English Gigaword corpus and on the
target side of the bitext. For Finnish-English we used
a dataset from a technical domain of software man-
uals. For this language pair we used two language
models: one very large model trained on billions of
words, and another language model trained from the
target side of the parallel training set.
The SparseHRM set (Cherry, 2013) contains 3600
sparse reordering features. For each phrase, the fea-
tures take the form of indicators describing its orienta-
tion in the derivation, and its lexical content in terms
of word clusters or frequent words. For both Chinese-
English and Arabic-English, systems are trained on
data from the NIST 2012 MT evaluation. 4-gram
1954
 0
 0.2
 0.4
 0.6
 0.8
 1
50 100 500 1000 20  200
BL
EU
number of features
expected BLEU gradient
random directionsPowell
coordinate ascent
 0
 0.2
 0.4
 0.6
 0.8
 1
50 100 500 1000 20  200
co
sin
e
number of features
expected BLEU gradient
random directionsPowell
coordinate ascent
Figure 2: Change in BLEU score and cosine similarity
to the gold weight vector w? as the number of features
increases, using the noisy synthetic experiments. The
gradient-based direction finding method is barely affected
by the noise. The increase of the number of dimensions en-
ables our direction finder to find a slightly better optimum,
which moved away from w? due to noise.
language models were trained on the target side of
the parallel training data for both Arabic and Chinese.
The Chinese systems development set is taken from
the NIST mt05 evaluation set, augmented with some
material reserved from our NIST training corpora in
order to better cover newsgroup and weblog domains.
6 Results
We conducted experiments with the synthetic data
scenario described in the previous section, as well
as with noise added to the data (Hopkins and May,
2011). The purpose of adding noise is to make the
optimization task more realistic. Specifically, af-
ter computing all pseudo-BLEU scores, we added
noise to each feature vector hs,m by drawing from
a zero-mean Gaussian with standard deviation 200.
Our results with both noiseless and noisy data yield
the same conclusion as Hopkins and May: standard
MERT struggles with many dimensions, and fails
to recover w?. However, our experiments with the
gradient direction finder of Section 4 are much more
positive. This direction finder not only recovers w?
 40
 50
 60
 70
 80
 90
 100
 1  10  100  1000
BL
EU
line search iteration
expected BLEU gradient(noisy) expected BLEU gradient
coordinate ascent(noisy) coordinate ascent
Figure 3: Comparison of rate of convergence between
coordinate ascent and our expected BLEU direction finder
(D = 500). Noisy refers to the noisy experimental setting.
(cosine > 0.999) even with 1000 dimensions, but its
effectiveness is also visible with noisy data, as seen
in Figure 2. The decrease of its cosine is relatively
small compared to other search algorithms, and this
decrease is not necessarily a sign of search errors
since the addition of noise causes the true optimum
to be different from w?. Finally, Figure 3 shows our
rate of convergence compared to coordinate ascent.
Our experimental results with the GBM feature
set data are shown in Table 2. Each table is di-
vided into three sections corresponding respectively
to MERT (Och, 2003) with Koehn-style coordinate
ascent (Koehn, 2004), PRO, and our optimizer featur-
ing both regularization and the gradient-based direc-
tion finder. All variants of MERT are initialized with
a single starting point, which is either uniform weight
or w0. Instead of providing MERT with additional
random starting points as in Moses, we use random
walks as in (Moore and Quirk, 2008) to attempt to
move out of local optima.8 Since PRO and our opti-
mizer have hyperparameters, we use a held-out set
(Dev) for adjusting them. For PRO, we adjust three
parameters: a regularization penalty for `2, the pa-
rameter ? in the add-? smoothed sentence-level ver-
sion of BLEU (Lin and Och, 2004), and a parameter
for scaling the corpus-level length of the references.
The latter scaling parameter is discussed in (He and
8In the case of the gradient-based direction finder, we also
use the following strategy whenever optimization converges to
a (possibly local) optimum. We run one round of coordinate
ascent, and continue with the gradient direction finder as soon as
the optimum improves. If the none of the coordinate directions
helped, we stop the search.
1955
Chinese-English Finnish-English
Method Starting pt. # feat. Tune Dev Test # feat. Tune Dev Test
MERT uniform 14 33.2 19.9 32.9 15 53.0 52.6 54.8
MERT uniform 224 33.0 19.2 32.1 232 53.2 51.7 53.8
MERT w0 224 34.1 20.1 33.0 232 53.9 52.5 54.7
PRO w0 224 33.4 20.1 33.3 232 53.3 52.9 55.3
`2 MERT (v1: ||w ?w0||) w0 224 33.2 20.3 33.5 232 53.2 52.7 55.2
`2 MERT (v2: D ? 1 dimensions) w0 224 33.0 20.4 33.2 232 52.9 52.6 55.0
`2 MERT (v3: `1-renormalized) w0 224 33.1 20.0 33.3 232 53.1 52.5 55.1
`0 MERT w0 224 33.4 20.3 33.2 232 53.2 52.6 55.1
Table 2: BLEU scores for GBM features. Model parameters were optimized on the Tune set. For PRO and regularized
MERT, we optimized with different hyperparameters (regularization weight, etc.), and retained for each experimental
condition the model that worked best on Dev. The table shows the performance of these retained models.
 51.2
 51.4
 51.6
 51.8
 52
 52.2
 52.4
 52.6
 1e-05  0.0001  0.001  0.01  0.1  1  10
BL
EU
regularization weight
expected BLEU gradient
coordinate ascent
Figure 4: BLEU score on the Finnish Dev set (GBM)
with different values for the 1/2?2 regularization weight.
To enable comparable results, the other hyperparameter
(length) is kept fixed.
Deng, 2012; Nakov et al, 2012) and addresses the
problem that systems tuned with PRO tend to pro-
duce sentences that are too short. On the other hand,
regularized MERT only requires one hyperparameter
to tune: a regularization penalty for `2 or `0. How-
ever, since PRO optimizes translation length on the
Dev dataset and MERT does so using the Tune set, a
comparison of the two systems would yield a discrep-
ancy in length that would be undesirable. Therefore,
we add another hyperparameter to regularized MERT
to tune length in the same manner using the Dev set.
Table 2 offers several findings. First, unregular-
ized MERT can achieve competitive results with a
small set of highly engineered features, but adding a
large set of more than 200 features causes MERT to
perform poorly, particularly on the test set. However,
unregularized MERT can recover much of this drop
of performance if it is given a good sparse initializer
w0. Regularized MERT (v1) provides an increase in
the order of 0.5 BLEU on the test set compared to
the best results with unregularized MERT. Regular-
ized MERT is competitive with PRO, even though the
number of features is relatively large. Using the same
GBM experimental setting, Figure 4 compares regu-
larized MERT using the gradient direction finder and
coordinate ascent. At the best regularization setting,
the two algorithms are comparable in terms of BLEU
(though coordinate ascent is slower due to its lack of
a good direction finder), but our method seems more
robust with suboptimal regularization parameters.
Our results with the SparseHRM feature set data
are shown in Table 3. As with the GBM feature set,
we find again that the version of `2 MERT regular-
ized towards ||w ?w0|| is competitive with PRO,
even though we train MERT with a large set of 3601
features.9 One remaining question is whether MERT
remains practical with large feature sets. As noted
in the complexity analysis of Section 4.3, MERT
has a dependence on the number of features that is
comparable to PRO, i.e., it is linear in both cases.
Practically, we find that optimization time is com-
parable between the two systems. In the case of
Chinese-English for the GBM feature set, one run of
the PRO optimizer took 26 minutes on average, while
regularized MERT with the gradient direction finder
took 37 minutes on average, taking into account the
time to compute w0. In the case of Chinese-English
for the SparseHRM feature set, average optimization
times for PRO and our method were 3.10 hours and
3.84 hours on average, respectively.
9We note that the experimental setup of (Cherry, 2013) inte-
grates the Sparse HRM features into the decoder, while we use
them in an M -best reranking scenario. The reranking setup of
this paper yields smaller improvements for both PRO and MERT
than those of (Cherry, 2013).
1956
Chinese-English Arabic-English
Method Starting pt. # feat. Tune Dev Test # feat. Tune Dev Test
MERT uniform 14 25.7 34.0 27.8 14 43.2 42.8 45.5
MERT uniform 3601 25.4 33.1 27.3 3601 45.7 42.3 44.9
MERT w0 3601 27.7 33.5 27.5 3601 46.0 42.4 45.2
PRO w0 3601 25.9 34.3 28.1 3601 44.6 43.4 46.1
`2 MERT (v1: ||w ?w0||) w0 3601 26.3 34.3 28.3 3601 45.2 43.2 46.0
`2 MERT (v2: D ? 1 dimensions) w0 3601 26.4 34.1 28.2 3601 45.0 43.4 45.9
`2 MERT (v3: `1-renormalized) w0 3601 26.1 34.0 27.9 3601 44.9 43.3 45.7
`0 MERT w0 3601 26.5 34.2 28.1 3601 45.4 43.1 46.0
Table 3: BLEU scores for SparseHRM features. Notes in Table 2 also apply here.
Finally, as shown in Table 2, we see that MERT ex-
periments that rely on a good initial starting point w0
generally perform better than when starting from
a uniform vector. While having to compute w0 in
the first place is a bit of a disadvantage compared
to standard MERT, the need for good initializer is
hardly surprising in the context of non-convex op-
timization. Other non-convex problems in machine
learning, such as deep neural networks (DNN) and
word alignment models, commonly require such ini-
tializers in order to obtain decent performance. In
the case of DNN, extensive research is devoted to the
problem of finding good initializers.10 In the case of
word alignment, it is common practice to initialize
search in non-convex optimization problems?such
as IBM Model 3 and 4 (Brown et al, 1993)?with
solutions of simpler models?such as IBM Model 1.
7 Related work
MERT and its extensions have been the target of ex-
tensive research (Och, 2003; Macherey et al, 2008;
Cer et al, 2008; Moore and Quirk, 2008; Kumar et
al., 2009; Galley and Quirk, 2011). More recent work
has focused on replacing MERT with a linearly de-
composable approximations of the evaluation metric
(Smith and Eisner, 2006; Liang et al, 2006; Watan-
abe et al, 2007; Chiang et al, 2008; Hopkins and
May, 2011; Rosti et al, 2011; Gimpel and Smith,
2012; Cherry and Foster, 2012), which generally
involve a surrogate loss function incorporating a reg-
ularization term such as the `2-norm. While we are
not aware of any previous work adding a penalty on
10For example, (Larochelle et al, 2009) presents a pre-trained
DNN that outperforms a shallow network, but the performance
of the DNN becomes much worse relative to the shallow network
once pre-training is turned off.
the weights in the context of MERT, (Cer et al, 2008)
achieves a related effect. Cer et al?s goal is to achieve
a more regular or smooth objective function, while
ours is to obtain a more regular set of parameters.
The two approaches may be complementary.
More recently, new research has explored direction
finding using a smooth surrogate loss function (Flani-
gan et al, 2013). Although this method is successful
in helping MERT find better directions, it also exac-
erbates the tendency of MERT to overfit.11 As an
indirect way of controlling overfitting on the tuning
set, their line searches are performed over directions
estimated over a separate dataset.
8 Conclusion
In this paper, we have shown that MERT can scale to
a much larger number of features than previously
thought, thanks to regularization and a direction
finder that directs the search towards the greatest
increase of expected BLEU score. While our best
results are comparable to PRO and not significantly
better, we think that this paper provides a deeper un-
derstanding of why standard MERT can fail when
handling an increasingly larger number of features.
Furthermore, this paper complements the analysis
by Hopkins and May (2011) of the differences be-
tween MERT and optimization with a surrogate loss
function.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments and suggestions.
11Indeed, in their Table 3, a comparison between HILS and
HOLS suggests tuning set performance improves substantially,
while held out performance degrades.
1957
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Comput. Linguist., 19(2):263?311.
Daniel Cer, Dan Jurafsky, and Christopher D. Manning.
2008. Regularization and search for minimum error
rate training. In Proceedings of the Third Workshop on
Statistical Machine Translation, pages 26?34.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 427?436.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceedings
of the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 22?31.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and structural
translation features. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 224?233.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 218?226.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013.
Large-scale discriminative training for statistical ma-
chine translation using held-out line search. In Pro-
ceedings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 248?258.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 38?49.
Kevin Gimpel and Noah A. Smith. 2012. Structured ramp
loss minimization for machine translation. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 221?231.
Xiaodong He and Li Deng. 2012. Maximum expected
BLEU training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers - Volume 1, pages 292?301.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362.
M. Hyder and K. Mahata. 2009. An approximate L0
norm minimization algorithm for compressed sens-
ing. In Acoustics, Speech and Signal Processing,
2009. ICASSP 2009. IEEE International Conference
on, pages 3365?3368.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL, Demonstration Session.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation models.
In Proc. of AMTA, pages 115?124.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 163?171.
Hugo Larochelle, Yoshua Bengio, Je?ro?me Louradour, and
Pascal Lamblin. 2009. Exploring strategies for training
deep neural networks. J. Mach. Learn. Res., 10:1?40.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In International Conference on Com-
putational Linguistics and Association for Computa-
tional Linguistics (COLING/ACL).
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation metrics
for machine translation. In Proceedings of the 20th
international conference on Computational Linguistics,
Stroudsburg, PA, USA.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 725?734.
David McAllester and Joseph Keshet. 2011. Generaliza-
tion bounds and consistency for latent structural probit
and ramp loss. In Advances in Neural Information
Processing Systems 24, pages 2205?2212.
Robert C. Moore and Chris Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proceedings of the 22nd International
Conference on Computational Linguistics - Volume 1,
pages 585?592.
1958
Preslav Nakov, Francisco Guzman, and Stephan Vogel.
2012. Optimizing for sentence-level BLEU+1 yields
short translations. In Proceedings of COLING 2012,
pages 1979?1994.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic evalu-
ation of machine translation. In Proc. of ACL.
Kishore Papineni. 1999. Discriminative training via linear
programming. In Proceedings IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP), volume 2, pages 561?564, Vol. 2.
M.J.D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables without
calculating derivatives. Comput. J., 7(2):155?162.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical Recipes:
The Art of Scientific Computing. Cambridge University
Press, 3rd edition.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected BLEU training
for graphs: BBN system description for WMT11 sys-
tem combination task. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
159?165.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 787?794.
Kristina Toutanova and Byung-Gyu Ahn. 2013. Learn-
ing non-linear features for machine translation using
gradient boosting machines. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 406?411.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 764?773.
1959
Book Review
Statistical Machine Translation
Philipp Koehn
(University of Edinburgh)
Cambridge University Press, 2010, xii+433 pp; ISBN 978-0-521-87415-1, $60.00
Reviewed by
Colin Cherry
National Research Council Canada
Statistical Machine Translation provides a comprehensive and clear introduction to the
most prominent techniques employed in the field of the same name (SMT). This text-
book is aimed at students or researchers interested in a thorough entry-point to the
field, and it does an excellent job of providing basic understanding for each of the many
pieces of a statistical translation system. I consider this book to be an essential addition
to any advanced undergraduate course or graduate course on SMT.
The book is divided into three parts: Foundations, Core Methods, and Advanced Topics.
Foundations (75 pages) covers an introduction to translation, working with text, and
probability theory. Core Methods (170 pages) covers the main components of a standard
phrase-based SMT system. Advanced Topics (125 pages) covers discriminative training
and linguistics in SMT, including an in-depth discussion of syntactic SMT. The text as
a whole assumes a certain familiarity with natural language processing; though the
Foundations section provides an effort to fill in the gaps, the book?s focus is decidedly
translation. As such, students unfamiliar with NLP may sometimes need to consult a
general NLP text.
The book aims to provide a thorough introduction to each component of a statistical
translation system, and it definitely succeeds in doing so. Supplementing this core
material for each chapter is a highly inclusive Further Reading section. These sections
provide brief narratives highlighting many relevant papers and alternative techniques
for each topic addressed in the chapter. I suspect many readers will find these literature
pointers to be quite valuable, from students wishing to dive deeper, to experienced SMT
researchers wishing to get started in a new sub-field. Each chapter also closes with a
short list of exercises. Many of these are very challenging (accurately indicated by a
star-rating system), and involve getting your hands dirty with tools downloaded from
the Web. The usefulness of these exercises will depend largely on the instructor?s tastes;
I view them as a bonus rather than a core feature of the book.
1. Chapters 1?3: Foundations
The first three chapters provide foundational knowledge for the rest of the book. In-
troduction provides an overview of the book and a brief history of machine transla-
tion, along with a discussion of applications and an expansive list of resources. The
overview?s structure takes the form of a summary of each chapter. This structure pro-
vides an effective preview of what will be covered and in what order, but it does not
focus on typical introductionmaterial; for example, there is no one place set aside to con-
vince the reader that SMT is a good idea, or to introduce concisely themain philosophies
behind the field. The history section is enjoyable, and I was glad to see a cautionary
Computational Linguistics Volume 36, Number 4
note regarding machine translation?s history of high hopes and disappointments. The
applications section provides an excellent overview of where SMT sees actual use, and
helps the reader understand why translations do not always need to be prefect.
Words, Sentences, Corpora provides a whirlwind tour of NLP basics, briefly touching
on a broad set of topics including Zipf?s law, parts-of-speech, morphology, and a num-
ber of grammar formalisms. To give an idea of just how brief coverage can be, the section
on grammar covers four formalisms in five pages. Nonetheless, these descriptions
should be helpful when the concepts re-appear later in the book. This chapter closes
with a discussion of parallel corpora and sentence alignment. As these are central to
the business of SMT, I feel they might have been better placed in a translation-focused
chapter.
Probability Theory covers the basic statistics needed to understand the ideas through-
out the book. This chapter is clear, and provides strong intuitions on important issues
such as conditional probability. There is a surprisingly large emphasis on binomial and
normal distributions, considering SMT?s heavy reliance on categorical distributions;
however, these are needed to discuss significance testing and some language modeling
techniques covered later.
2. Chapters 4?8: Core Methods
The next five chapters provide detailed descriptions of each of the major components
of a phrase-based SMT system. Word-based Methods discusses the five IBM translation
models, with a brief detour to discuss the noisy channel model that motivates the
IBM approach. This chapter is best taken as a complement to Brown et al (1993)
and Kevin Knight?s (1999) tutorial on the same subject, rather than a replacement. It
provides strong intuitions on what each IBMmodel covers and how each model works,
including the clearest descriptions I have seen of IBM:3?5. However, it does sometimes
make them seem a little mysterious. For example, there is no attempt to explain why
IBM:1 always arrives at a global maximum, or to generalize when one can apply the
mathematical simplification that reduces IBM:1?s exponential sum over products to a
polynomial product over sums. One glaring omission from this chapter is a discussion
of the alignment HMM (Vogel, Ney, and Tillmann 1996). This elegant model is widely
used and widely extended, and I had expected to see it covered in detail.
Chapters 5 and 6 on Phrase-based Models andDecoding cover the major algorithms in
the popular phrase-based SMT paradigm. They are clear and fairly complete; this book
could easily serve as an effective reference for these topics. Phrase-basedModelsmotivates
the use of phrases, and then covers phrase extraction along with the calculation of
phrase features, such as lexical weighting and lexicalized re-orderingmodels. This chap-
ter also marks the beginning of a careful dance, where log-linear models are introduced
without having yet covered SMT evaluation or discriminative training. These topics
are covered in Chapters 8 and 9, respectively. This division of modeling and training
is a reasonable strategy, given the amount of material required to understand the full
pipeline, but a student may need some extra guidance to understand the complete
picture. The Decoding chapter focuses on stack decoding, and it is extremely well-
written, with great explanations of search and pruning strategies. Alternative decoding
formalisms, such as A* or finite-state decoding, are given short but effective summaries.
This chapter makes phrasal SMT decoding feel easy.
The next chapter covers Language Models. Considering that this topic is given in-
depth coverage in other NLP texts, I was surprised to see it covered quite thoroughly
here as well. This chapter covers a number of smoothing techniques, as well as some
774
Book Review
practical tips for handling large models. As usual, the exposition is exceptionally clear,
and each newmethod?s advantages are demonstrated with predicted counts or perplex-
ity scores on Europarl data, which I found to be very useful. For many SMT courses,
this chapter will be sufficient to stand alone as both an introduction and a reference for
language modeling.
Finally, the Core Methods section closes with a discussion of Evaluation. This chapter
discusses human evaluation, motivates automatic evaluation, and then covers themajor
contenders: word error rate, BLEU, and METEOR. The discussion of BLEU?s shortcom-
ings is very even-handed, perhaps a little pessimistic, and acknowledges all of the major
concerns regarding the metric.
3. Chapters 9?11: Advanced Topics
The final three chapters cover advanced topics, which include recent or not universally
adopted advances. So at this point, one might expect that all of the major components of
a baseline phrase-based SMT system have been covered, but the final piece of the puzzle
does not come until Discriminative Training, which includes a discussion of minimum
error rate training (MERT) for the log-linear models introduced in Chapter 5. This
chapter also covers n-best list extraction, n-best list re-ranking, and posterior methods
such as Minimum Bayes Risk Decoding. It also devotes a surprisingly large amount of
time to large-scale discriminative training, where thousands of parameter values can
be learned. There is a lot of ground to cover here; consequently, much of the material
will need to be supplemented with research papers or other texts if the instructor wants
to cover any one topic in depth. The sections covering the learning methods used in
parameter tuning (maximum entropy, MERT) did not feel as clear as the rest of the
book. I suspect that a newcomer to the field will require some guidance to pick out the
essential parts.
Chapter 10 is on Integrating Linguistic Information, which is kind of a grab bag,
covering linguistic pre-processing, syntactic features, and factored translation models.
The pre-processing discussion includes transliteration, morphological normalization,
compound splitting, and even syntactically motivated re-ordering of the input sen-
tence. The syntactic features section mostly covers n-best list re-ranking as done in the
Smorgasbord paper (Och et al 2004). Each of these topics is well motivated, and the text
provides a clear description of a prominent, recent solution.
Finally, the book closes with Tree-based Models. This chapter covers a lot of ground:
first describing synchronous context-free grammars, and then describing both for-
mally syntactic hierarchical grammars and linguistically syntactic synchronous-tree-
substitution grammars in terms of this common formalism. This is a very nicely
presented chapter. It draws a lot of interesting connections between formalisms; for
example, tree-to-tree rule extraction and tree-to-string rule extraction are presented as
simple constraints on hierarchical phrase extraction. The description of chart parsing
for decoding is also very clear, and it draws many useful analogies to the material
presented earlier for phrasal decoding. I get the impression that many insights gained
while adding syntactic SMT into the Moses translation system have found their way
into this chapter.
4. Summary
This book?s existence indicates that the field of SMT has reached a point of maturity
where it makes sense to discuss core and foundational techniques. This book provides
775
Computational Linguistics Volume 36, Number 4
a clear and comprehensive introduction to word, phrase, and tree-based translation
modeling, along with the decoding, training, and evaluation algorithms that make these
models work. The text?s stated goal is to provide a thorough introduction, but I would
also recommend it as an effective reference for anyone interested in writing their own
SMT decoder, be it phrasal or syntactic. Most importantly, this book makes the prospect
of teaching a course devoted to SMT much less daunting, and it should provide a
valuable resource to researchers or students looking to teach themselves.
References
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation:
Parameter estimation. Computational
Linguistics, 19(2):263?312.
Knight, Kevin 1999. A statistical MT
tutorial workbook. Available at:
http://www.isi.edu/?knight/.
Och, Franz Josef, Daniel Gildea, Sanjeev
Khudanpur, Anoop Sarkar, Kenki Yamada,
Alex Fraser, Shankar Kumar, Libin Shen,
David Smith, Katherine Eng, Viren Jain,
Zheng Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical
machine translation. In Proceedings of the
Human Language Technology Conference
of the North American Chapter of the
Association for Computational Linguistics:
HLT-NAACL 2004, pages 161?168,
Boston, MA.
Vogel, Stephan, Hermann Ney, and
Cristoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In Proceedings, 16th International Conference
on Computational Linguistics (COLING),
pages 836?841, Copenhagen.
Colin Cherry is a research officer at the National Research Council Canada. His research interests
include structure prediction and induction, with application to parsing, morphology, pronuncia-
tion, andmachine translation. Cherry?s address is NRC Institute for Information Technology, 1200
Montreal Road, M50:C-318, Ottawa, Ontario, Canada K1A 0R6; e-mail: Colin.Cherry@nrc-cnrc.
gc.ca; URL: https://sites.google.com/site/colinacherry/.
776
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 172?180,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Modeling of Twitter Conversations
Alan Ritter?
Computer Sci. & Eng.
University of Washington
Seattle, WA 98195
aritter@cs.washington.edu
Colin Cherry?
National Research Council Canada
Ottawa, Ontario, K1A 0R6
Colin.Cherry@nrc-cnrc.gc.ca
Bill Dolan
Microsoft Research
Redmond, WA 98052
billdol@microsoft.com
Abstract
We propose the first unsupervised approach
to the problem of modeling dialogue acts in
an open domain. Trained on a corpus of
noisy Twitter conversations, our method dis-
covers dialogue acts by clustering raw utter-
ances. Because it accounts for the sequential
behaviour of these acts, the learned model can
provide insight into the shape of communica-
tion in a new medium. We address the chal-
lenge of evaluating the emergent model with a
qualitative visualization and an intrinsic con-
versation ordering task. This work is inspired
by a corpus of 1.3 million Twitter conversa-
tions, which will be made publicly available.
This huge amount of data, available only be-
cause Twitter blurs the line between chatting
and publishing, highlights the need to be able
to adapt quickly to a new medium.
1 Introduction
Automatic detection of dialogue structure is an im-
portant first step toward deep understanding of hu-
man conversations. Dialogue acts1 provide an
initial level of structure by annotating utterances
with shallow discourse roles such as ?statement?,
?question? and ?answer?. These acts are useful in
many applications, including conversational agents
(Wilks, 2006), dialogue systems (Allen et al, 2007),
dialogue summarization (Murray et al, 2006), and
flirtation detection (Ranganath et al, 2009).
Dialogue act tagging has traditionally followed an
annotate-train-test paradigm, which begins with the
?This work was conducted at Microsoft Research.
1Also called ?speech acts?
design of annotation guidelines, followed by the col-
lection and labeling of corpora (Jurafsky et al, 1997;
Dhillon et al, 2004). Only then can one train a tag-
ger to automatically recognize dialogue acts (Stol-
cke et al, 2000). This paradigm has been quite suc-
cessful, but the labeling process is both slow and
expensive, limiting the amount of data available for
training. The expense is compounded as we con-
sider new methods of communication, which may
require not only new annotations, but new annota-
tion guidelines and new dialogue acts. This issue be-
comes more pressing as the Internet continues to ex-
pand the number of ways in which we communicate,
bringing us e-mail, newsgroups, IRC, forums, blogs,
Facebook, Twitter, and whatever is on the horizon.
Previous work has taken a variety of approaches
to dialogue act tagging in new media. Cohen et al
(2004) develop an inventory of dialogue acts specific
to e-mail in an office domain. They design their in-
ventory by inspecting a large corpus of e-mail, and
refine it during the manual tagging process. Jeong et
al. (2009) use semi-supervised learning to transfer
dialogue acts from labeled speech corpora to the In-
ternet media of forums and e-mail. They manually
restructure the source act inventories in an attempt
to create coarse, domain-independent acts. Each ap-
proach relies on a human designer to inject knowl-
edge into the system through the inventory of avail-
able acts.
As an alternative solution for new media, we pro-
pose a series of unsupervised conversation models,
where the discovery of acts amounts to clustering
utterances with similar conversational roles. This
avoids manual construction of an act inventory, and
allows the learning algorithm to tell us something
about how people converse in a new medium.
172
There is surprisingly little work in unsupervised
dialogue act tagging. Woszczyna and Waibel (1994)
propose an unsupervised Hidden Markov Model
(HMM) for dialogue structure in a meeting schedul-
ing domain, but model dialogue state at the word
level. Crook et al (2009) use Dirichlet process mix-
ture models to cluster utterances into a flexible num-
ber of acts in a travel-planning domain, but do not
examine the sequential structure of dialogue.2
In contrast to previous work, we address the prob-
lem of discovering dialogue acts in an informal,
open-topic domain, where an unsupervised learner
may be distracted by strong topic clusters. We also
train and test our models in a new medium: Twit-
ter. Rather than test against existing dialogue inven-
tories, we evaluate using qualitative visualizations
and a novel conversation ordering task, to ensure our
models have the opportunity to discover dialogue
phenomena unique to this medium.
2 Data
To enable the study of large-data solutions to di-
alogue modeling, we have collected a corpus of
1.3 million conversations drawn from the micro-
blogging service, Twitter. 3 To our knowledge,
this is the largest corpus of naturally occurring chat
data that has been available for study thus far. Sim-
ilar datasets include the NUS SMS corpus (How
and Kan, 2005), several IRC chat corpora (Elsner
and Charniak, 2008; Forsyth and Martell, 2007),
and blog datasets (Yano et al, 2009; Gamon et al,
2008), which can display conversational structure in
the blog comments.
As it characterizes itself as a micro-blog, it should
not be surprising that structurally, Twitter conversa-
tions lie somewhere between chat and blogs. Like
blogs, conversations on Twitter occur in a public en-
vironment, where they can be collected for research
purposes. However, Twitter posts are restricted to be
no longer than 140 characters, which keeps interac-
tions chat-like. Like e-mail and unlike IRC, Twit-
ter conversations are carried out by replying to spe-
cific posts. The Twitter API provides a link from
each reply to the post it is responding to, allowing
2The Crook et al model should be able to be combined with
the models we present here.
3Will be available at http://www.cs.washington.
edu/homes/aritter/twitter_chat/
1 2 3 4 5
0
2
4
6
8
10
12
14
log length
log 
freq
uen
cy
Figure 1: Conversation length versus frequency
accurate thread reconstruction without requiring a
conversation disentanglement step (Elsner and Char-
niak, 2008). The proportion of posts on Twitter that
are conversational in nature are somewhere around
37% (Kelly, 2009).
To collect this corpus, we crawled Twitter using
its publicly available API. We monitored the public
timeline4 to obtain a sample of active Twitter users.
To expand our user list, we also crawled up to 10
users who had engaged in dialogue with each seed
user. For each user, we retrieved all posts, retain-
ing only those that were in reply to some other post.
We recursively followed the chain of replies to re-
cover the entire conversation. A simple function-
word-driven filter was used to remove non-English
conversations.
We crawled Twitter for a 2 month period during
the summer of 2009. The resulting corpus consists
of about 1.3 million conversations, with each con-
versation containing between 2 and 243 posts. The
majority of conversations on Twitter are very short;
those of length 2 (one status post and a reply) ac-
count for 69% of the data. As shown in Figure 1, the
frequencies of conversation lengths follow a power-
law relationship.
While the style of writing used on Twitter is
widely varied, much of the text is very similar to
SMS text messages. This is likely because many
users access Twitter through mobile devices. Posts
are often highly ungrammatical, and filled with
spelling errors. In order to illustrate the spelling
variation found on Twitter, we ran the Jcluster word
clustering algorithm (Goodman, 2001) on our cor-
4http://twitter.com/public_timeline pro-
vides the 20 most recent posts on Twitter
173
coming comming
enough enought enuff enuf
be4 b4 befor before
yuhr yur your yor ur youur yhur
msgs messages
couldnt culdnt cldnt cannae cudnt couldent
about bou abt abour abut bowt
Table 1: A sample of Twitter spelling variation.
pus, and manually picked out clusters of spelling
variants; a sample is displayed in Table 1.
Twitter?s noisy style makes processing Twitter
text more difficult than other domains. While mov-
ing to a new domain (e.g. biomedical text) is a chal-
lenging task, at least the new words found in the
vocabulary are limited mostly to verbs and nouns,
while function words remain constant. On Twit-
ter, even closed-class words such as prepositions and
pronouns are spelled in many different ways.
3 Dialogue Analysis
We propose two models to discover dialogue acts in
an unsupervised manner. An ideal model will give
insight into the sorts of conversations that happen
on Twitter, while providing a useful tool for later
processing. We first introduce the summarization
technology we apply to this task, followed by two
Bayesian extensions.
3.1 Conversation model
Our base model structure is inspired by the con-
tent model proposed by Barzilay and Lee (2004)
for multi-document summarization. Their sentence-
level HMM discovers the sequence of topics used
to describe a particular type of news event, such as
earthquakes. A news story is modeled by first gen-
erating a sequence of hidden topics according to a
Markov model, with each topic generating an ob-
served sentence according to a topic-specific lan-
guage model. These models capture the sequential
structure of news stories, and can be used for sum-
marization tasks such as sentence extraction and or-
dering.
Our goals are not so different: we wish to dis-
cover the sequential dialogue structure of conversa-
tion. Rather than learning a disaster?s location is
followed by its death toll, we instead wish to learn
that a question is followed by an answer. An initial
a
0
a
1
a
2
w
0,j
w
1,j
w
2,j
W
0
W
1
W
2
C
k
Figure 2: Conversation Model
a
0
a
1
a
2
w
0,j
w
1,j
w
2,j
W
0
W
1
W
2
C
k
s
0,j
s
1,j
s
2,j
?
k
?
E
pi
k
Figure 3: Conversation + Topic Model
conversation model can be created by simply apply-
ing the content modeling framework to conversation
data. We rename the hidden states acts, and assume
each post in a Twitter conversation is generated by
a single act.5 During development, we found that a
unigram language model performed best as the act
emission distribution.
The resulting conversation model is shown as a
plate diagram in Figure 2. Each conversation C is
a sequence of acts a, and each act produces a post,
represented by a bag of words shown using the W
plates. The number of acts available to the model
is fixed; we experimented with between 5 and 40.
Starting with a random assignment of acts, we train
our conversation model using EM, with forward-
backward providing act distributions during the ex-
pectation step. The model structure in Figure 2 is
5The short length of Twitter posts makes this assumption
reasonable.
174
sadly no. some pasta bake, but coffee and pasta bake is not a
contender for tea and toast... .
yum! Ground beef tacos? We ?re grilling out. Turkey dogs for
me, a Bubba Burger for my dh, and combo for the kids.
ha! They gotcha! You had to think about Arby?s to write that tweet.
Arby?s is conducting a psychlogical study. Of roast beef.
Rumbly tummy soon to be tamed by Dominos for lunch! Nom
nom nom!
Table 2: Example of a topical cluster discovered by
the EM Conversation Model.
similar to previous HMMs for supervised dialogue
act recognition (Stolcke et al, 2000), but our model
is trained unsupervised.
3.2 Conversation + Topic model
Our conversations are not restricted to any partic-
ular topic: Twitter users can and will talk about
anything. Therefore, there is no guarantee that our
model, charged with discovering clusters of posts
that aid in the prediction of the next cluster, will nec-
essarily discover dialogue acts. The sequence model
could instead partition entire conversations into top-
ics, such as food, computers and music, and then pre-
dict that each topic self-transitions with high proba-
bility: if we begin talking about food, we are likely
to continue to do so. Since we began with a content
model, it is perhaps not surprising that our Conversa-
tion Model tends to discover a mixture of dialogue
and topic structure. Several high probability posts
from a topic-focused cluster discovered by EM are
shown in Table 2. These clusters are undesirable, as
they have little to do with dialogue structure.
In general, unsupervised sentence clustering tech-
niques need some degree of direction when a par-
ticular level of granularity is desired. Barzilay and
Lee (2004) mask named entities in their content
models, forcing their model to cluster topics about
earthquakes in general, and not instances of specific
earthquakes. This solution is not a good fit for Twit-
ter. As explained in Section 2, Twitter?s noisiness
resists off-the-shelf tools, such as named-entity rec-
ognizers and noun-phrase chunkers. Furthermore,
we would require a more drastic form of prepro-
cessing in order to mask all topic words, and not
just alter the topic granularity. During development,
we explored coarse methods to abstract away con-
tent while maintaining syntax, such as replacing to-
kens with either parts-of-speech or automatically-
generated word clusters, but we found that these ap-
proaches degrade model performance.
Another approach to filtering out topic informa-
tion leaves the data intact, but modifies the model
to account for topic. To that end, we adopt a Latent
Dirichlet Allocation, or LDA, framework (Blei et al,
2003) similar to approaches used recently in sum-
marization (Daume? III and Marcu, 2006; Haghighi
and Vanderwende, 2009). The goal of this extended
model is to separate content words from dialogue in-
dicators. Each word in a conversation is generated
from one of three sources:
? The current post?s dialogue act
? The conversation?s topic
? General English
The extended model is shown in Figure 3.6 In addi-
tion to act emission and transition parameters, the
model now includes a conversation-specific word
multinomial ?k that represents the topic, as well as a
universal general English multinomial ?E . A new
hidden variable, s determines the source of each
word, and is drawn from a conversation-specific dis-
tribution over sources pik. Following LDA conven-
tions, we place a symmetric Dirichlet prior over
each of the multinomials. Dirichlet concentration
parameters for act emission, act transition, conver-
sation topic, general English, and source become the
hyper-parameters of our model.
The multinomials ?k, pik and ?E create non-local
dependencies in our model, breaking our HMM dy-
namic programing. Therefore we adopt Gibbs sam-
pling as our inference engine. Each hidden vari-
able is sampled in turn, conditioned on a complete
assignment of all other hidden variables through-
out the data set. Again following LDA convention,
we carry out collapsed sampling, where the various
multinomials are integrated out, and are never ex-
plicitly estimated. This results in a sampling se-
quence where for each post we first sample its act,
and then sample a source for each word in the post.
The hidden act and source variables are sampled ac-
cording to the following transition distributions:
6This figure omits hyperparameters as well as act transition
and emission multinomials to reduce clutter. Dirichlet priors are
placed over all multinomials.
175
Ptrans(ai|a?i, s,w) ?
P (ai|a?i)
Wi?
j=1
P (wi,j |a, s,w?(i,j))
Ptrans(si,j |a, s?(i,j),w) ?
P (si,j |s?(i,j))P (wi,j |a, s,w?(i,j))
These probabilities can be computed analogously to
the calculations used in the collapsed sampler for a
bigram HMM (Goldwater and Griffiths, 2007), and
those used for LDA (Griffiths and Steyvers, 2004).
Note that our model contains five hyperparame-
ters. Rather than attempt to set them using an ex-
pensive grid search, we treat the concentration pa-
rameters as additional hidden variables and sample
each in turn, conditioned on the current assignment
to all other variables. Because these variables are
continuous, we apply slice sampling (Neal, 2003).
Slice sampling is a general technique for drawing
samples from a distribution by sampling uniformly
from the area under its density function.
3.3 Estimating Likelihood on Held-Out Data
In Section 4.2 we evaluate our models by comparing
their probability on held-out test conversations. As
computing this probability exactly is intractable in
our model, we employ a recently proposed Chibb-
style estimator (Murray and Salakhutdinov, 2008;
Wallach et al, 2009). Chibb estimators estimate the
probability of unseen data, P (w) by selecting a high
probability assignment to hidden variables h?, and
taking advantage of the following equality which
can be easily derived from the definition of condi-
tional probability:
P (w) =
P (w,h?)
P (h?|w)
As the numerator can be computed exactly, this re-
duces the problem of estimating P (w) to the eas-
ier problem of estimating P (h?|w). Murray and
Salakhutdinov (2008) provide an unbiased estimator
for P (h?|w), which is calculated using the station-
ary distribution of the Gibbs sampler.
3.4 Bayesian Conversation model
Given the infrastructure necessary for the Conver-
sation+Topic model described above, it is straight-
forward to also implement a Bayesian version of
of the conversation model described in Section 3.1.
This amounts to replacing the add-x smoothing of
dialogue act emission and transition probabilities
with (potentially sparse) Dirichlet priors, and replac-
ing EM with Gibbs sampling. There is reason to
believe that integrating out multinomials and using
sparse priors will improve the performance of the
conversation model, as improvements have been ob-
served when using a Bayesian HMM for unsuper-
vised part-of-speech tagging (Goldwater and Grif-
fiths, 2007).
4 Experiments
Evaluating automatically discovered dialogue acts
is a difficult problem. Unlike previous work, our
model automatically discovers an appropriate set of
dialogue acts for a new medium; these acts will
not necessarily have a close correspondence to di-
alogue act inventories manually designed for other
corpora. Instead of comparing against human anno-
tations, we present a visualization of the automati-
cally discovered dialogue acts, in addition to mea-
suring the ability of our models to predict post order
in unseen conversations. Ideally we would evaluate
performance using an end-use application such as a
conversational agent; however as this is outside the
scope of this paper, we leave such an evaluation to
future work.
For all experiments we train our models on a set of
10,000 randomly sampled conversations with con-
versation length in posts ranging from 3 to 6. Note
that our implementations can likely scale to larger
data by using techniques such as SparseLDA (Yao
et al, 2009). We limit our vocabulary to the 5,000
most frequent words in the corpus.
When using EM, we train for 100 iterations, eval-
uating performance on the test set at each iteration,
and reporting the maximum. Smoothing parameters
are set using grid search on a development set.
When performing inference with Gibbs Sam-
pling, we use 1,000 samples for burn-in and take
10 samples at a lag of 100. Although using multi-
ple samples introduces the possibility of poor results
due to ?act drift?, we found this not to be a problem
in practice; in fact, taking multiple samples substan-
tially improved performance during development.
Recall that we infer hyperparameters using slice
176
sampling. The concentration parameters chosen in
this manner were always sparse (< 1), which pro-
duced a moderate improvement over an uninformed
prior.
4.1 Qualitative Evaluation
We are quite interested in what our models can tell
us about how people converse on Twitter. To vi-
sualize and interpret our competing models, we ex-
amined act-emission distributions, posts with high-
confidence acts, and act-transition diagrams. Of
the three competing systems, we found the Conver-
sation+Topic model by far the easiest to interpret:
the 10-act model has 8 acts that we found intuitive,
while the other 2 are used only with low probabil-
ity. Conversely, the Conversation model, whether
trained by EM or Gibbs sampling, suffered from
the inclusion of general terms and from the confla-
tion of topic and dialogue. For example, the EM-
trained conversation model discovered an ?act? that
was clearly a collection of posts about food, with no
underlying dialogue theme (see Table 2).
In the remainder of this section, we reproduce
our visualization for the 10-act Conversation+Topic
model. Word lists summarizing the discovered dia-
logue acts are shown in Table 3. For each act, the
top 40 words are listed in order of decreasing emis-
sion probability. An example post, drawn from the
set of highest-confidence posts for that act, is also
included. Figure 4 provides a visualization of the
matrix of transition probabilities between dialogue
acts. An arrow is drawn from one act to the next
if the probability of transition is above 0.15.7 Note
that a uniform model would transition to each act
with probability 0.10. In both Table 3 and Figure 4,
we use intuitive names in place of cluster numbers.
These are based on our interpretations of the clus-
ters, and are provided only to benefit the reader when
interpreting the transition diagram.8
From inspecting the transition diagram (Figure 4),
one can see that the model employs three distinct
acts to initiate Twitter conversations. These initial
acts are quite different from one another, and lead to
7After setting this threshold, two Acts were cut off from the
rest of the graph (had no incoming edges), and were therefore
removed
8In some cases, the choice in name is somewhat arbitrary,
ie: answer versus response, reaction versus comment.
Figure 4: Transitions between dialogue acts. See
table 3 for word lists and example posts for each act
different sets of possible responses. We discuss each
of these in turn.
The Status act appears to represent a post in which
the user is broadcasting information about what they
are currently doing. This can be seen by the high
amount of probability mass given to words like I
and my, in addition to verbs such as go and get, as
well as temporal nouns such as today, tomorrow and
tonight.
The Reference Broadcast act consists mostly of
usernames and urls.9 Also prominent is the word rt,
which has special significance on Twitter, indicating
that the user is re-posting another user?s post. This
act represents a user broadcasting an interesting link
or quote to their followers. Also note that this node
transitions to the Reaction act with high probability.
Reaction appears to cover excited or appreciative re-
sponses to new information, assigning high proba-
bility to !, !!, !!!, lol, thanks, and haha.
Finally Question to Followers represents a user
asking a question to their followers. The presence
of the question mark and WH question words indi-
cate a question, while words like anyone and know
indicate that the user is asking for information or an
opinion. Note that this is distinct from the Question
act, which is in response to an initial post.
Another interesting point is the alternation be-
9As part of the preprocessing of our corpus we replaced all
usernames and urls with the special tokens -usr- and -url-.
177
Status I . to ! my , is for up in ... and going was today so at go get back day got this am but Im now tomorrow night work
tonight off morning home had gon need !! be just getting
I just changed my twitter page bkgornd and now I can?t stop looking at it, lol!!
Question to Followers ? you is do I to -url- what -usr- me , know if anyone why who can ? this or of that how does - : on your are need
any rt u should people want get did have would tell
anyone using google voice? just got my invite, should i?? don?t know what it is? -url- for the video and break
down
Reference Broadcast -usr- ! -url- rt : -usr-: - ? my the , is ( you new ? ? !! ) this for at in follow of on ? lol u are twitter your thanks via
!!! by :) here 2 please check
rt -usr-: -usr- word that mac lip gloss give u lock jaw! lol
Question ? you what ! are is how u do the did your that , lol where why or ?? hey about was have who it in so haha on
doing going know good up get like were for there :) can
DWL!! what song is that??
Reaction ! you I :) !! , thanks lol it haha that love so good too your thank is are u !!! was for :d me -usr- ? hope ? my 3 omg
... oh great hey awesome - happy now aww
sweet! im so stoked now!
Comment you I . to , ! do ? it be if me your know have we can get will :) but u that see lol would are so want go let up well
need - come ca make or think them
why are you in tx and why am I just now finding out about it?! i?m in dfw, till I get a job. i?ll have to come to
Htown soon!
Answer . I , you it ? that ? is but do was he the of a they if not would know be did or does think ) like ( as have what in are
- no them said who say ?
my fave was ?keeping on top of other week?
Response . I , it was that lol but is yeah ! haha he my know yes you :) like too did well she so its ... though do had no - one
as im thanks they think would not good oh
nah im out in maryland, leaving for tour in a few days.
Table 3: Word lists and example posts for each Dialogue Act. Words are listed in decreasing order of
probability given the act. Example posts are in italics.
tween the personal pronouns you and I in the acts
due to the focus of conversation and speaker. The
Status act generates the word I with high probability,
whereas the likely response state Question generates
you, followed by Response which again generates I.
4.2 Quantitative Evaluation
Qualitative evaluations are both time-consuming
and subjective. The above visualization is useful for
understanding the Twitter domain, but it is of little
use when comparing model variants or selecting pa-
rameters. Therefore, we also propose a novel quan-
titative evaluation that measures the intrinsic qual-
ity of a conversation model by its ability to predict
the ordering of posts in a conversation. This mea-
sures the model?s predictive power, while requiring
no tagged data, and no commitment to an existing
tag inventory.
Our test set consists of 1,000 randomly selected
conversations not found in the training data. For
each conversation in the test set, we generate all
n! permutations of the posts. The probability of
each permutation is then evaluated as if it were an
unseen conversation, using either the forward algo-
rithm (EM) or the Chibb-style estimator (Gibbs).
Following work from the summarization community
(Barzilay and Lee, 2004), we employ Kendall?s ? to
measure the similarity of the max-probability per-
mutation to the original order.
The Kendall ? rank correlation coefficient mea-
sures the similarity between two permutations based
on their agreement in pairwise orderings:
? =
n+ ? n?
(n
2
)
where n+ is the number of pairs that share the same
order in both permutations, and n? is the number
that do not. This statistic ranges between -1 and +1,
where -1 indicates inverse order, and +1 indicates
identical order. A value greater than 0 indicates a
positive correlation.
Predicting post order on open-domain Twitter
conversations is a much more difficult task than on
topic-focused news data (Barzilay and Lee, 2004).
We found that a simple bigram model baseline does
very poorly at predicting order on Twitter, achieving
only a weak positive correlation of ? = 0.0358 on
our test data as compared with 0.19-0.74 reported by
Barzilay and Lee on news data.
Note that ? is not a perfect measure of model qual-
ity for conversations; in some cases, multiple order-
178
5 10 15 20 25 30 35 40
EM ConversationConversation+TopicBayesian Conversation
# acts
tau
0.0
0.1
0.2
0.3
0.4
Figure 5: Performance at conversation ordering task.
ings of the same set of posts may form a perfectly
acceptable conversation. On the other hand, there
are often strong constraints on the type of response
we might expect to follow a particular dialogue act;
for example, answers follow questions. We would
expect an effective model to use these constraints to
predict order.
Performance at the conversation ordering task
while varying the number of acts for each model is
displayed in Figure 5. In general, we found that us-
ing Bayesian inference outperforms EM. Also note
that the Bayesian Conversation model outperforms
the Conversation+Topic model at predicting conver-
sation order. This is likely because modeling conver-
sation content as a sequence can in some cases help
to predict post ordering; for example, adjacent posts
are more likely to contain similar content words. Re-
call though that we found the Conversation+Topic
model to be far more interpretable.
Additionally we compare the likelihood of these
models on held out test data in Figure 6. Note that
the Bayesian methods produce models with much
higher likelihood.10 For the EM models, likelihood
tends to decrease on held out test data as we increase
the number of hidden states, due to overfitting.
5 Conclusion
We have presented an approach that allows the
unsupervised induction of dialogue structure from
naturally-occurring open-topic conversational data.
10Likelihood of the test data is estimated using the Chibb
Style estimator described in (Murray and Salakhutdinov, 2008;
Wallach et al, 2009). This method under-estimates likelihood
in expectation. The maximum likelihood (EM) likelihoods are
exact.
5 10 15 20 25 30 35 40
EM ConversationConversation+TopicBayesian Conversation
# acts
negat
ive lo
g like
lihood
3100
003
1500
032
0000
3250
003
3000
033
5000
3400
00
Figure 6: Negative log likelihood on held out test
data (smaller values indicate higher likelihood).
By visualizing the learned models, coherent patterns
emerge from a stew of data that human readers find
difficult to follow. We have extended a conversa-
tion sequence model to separate topic and dialogue
words, resulting in an interpretable set of automat-
ically generated dialogue acts. These discovered
acts have interesting differences from those found
in other domains, and reflect Twitter?s nature as a
micro-blog.
We have introduced the task of conversation or-
dering as an intrinsic measure of conversation model
quality. We found this measure quite useful in
the development of our models and algorithms, al-
though our experiments show that it does not nec-
essarily correlate with interpretability. We have di-
rectly compared Bayesian inference to EM on our
conversation ordering task, showing a clear advan-
tage for Bayesian methods.
Finally, we have collected a corpus of 1.3 million
Twitter conversations, which we will make available
to the research community, and which we hope will
be useful beyond the study of dialogue. In the fu-
ture, we wish to scale our models to the full corpus,
and extend them with more complex notions of dis-
course, topic and community. Ultimately, we hope
to put the learned conversation structure to use in the
construction of a data-driven, conversational agent.
Acknowledgements
We are grateful to everyone in the NLP and TMSN
groups at Microsoft Research for helpful discussions
and feedback. We thank Oren Etzioni, Michael Ga-
mon, Mausam and Fei Wu, and the anonymous re-
viewers for helpful comments on a previous draft.
179
References
James Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Taysom. 2007. Plow: a collaborative task
learning agent. In Proceedings of AAAI.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
HLT-NAACL, pages 113?120.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of EMNLP.
Nigel Crook, Ramon Granell, and Stephen Pulman.
2009. Unsupervised classification of dialogue acts us-
ing a Dirichlet process mixture model. In Proceedings
of SIGDIAL, pages 341?348.
Hal Daume? III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of ACL.
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and Eliz-
abeth Shriberg. 2004. Meeting recorder project: Dia-
log act labeling guide. Technical report, International
Computer Science Institute.
Micha Elsner and Eugene Charniak. 2008. You talking
to me? A corpus and algorithm for conversation dis-
entanglement. In Proceedings of ACL-HLT.
Eric N. Forsyth and Craig H. Martell. 2007. Lexical and
discourse analysis of online chat dialog. In Proceed-
ings of ICSC.
Michael Gamon, Sumit Basu, Dmitriy Belenko, Danyel
Fisher, Matthew Hurst, and Arnd Christian Knig.
2008. Blews: Using blogs to provide context for news
articles. In Proceedings of ICWSM.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of ACL, pages 744?751.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. Technical report.
T. L. Griffiths and M. Steyvers. 2004. Finding scientific
topics. Proc Natl Acad Sci, 101 Suppl 1:5228?5235.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of HLT-NAACL, pages 362?370.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Proceedings of HCII.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Semi-supervised speech act recognition in
emails and forums. In Proceedings of EMNLP, pages
1250?1259.
Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.
Switchboard swbd-damsl shallow-discourse-function
annotation coders manual, draft 13. Technical report,
University of Colorado Institute of Cognitive Science.
Ryan Kelly. 2009. Pear analytics twitter study. Whitepa-
per, August.
Iain Murray and Ruslan Salakhutdinov. 2008. Evalu-
ating probabilities under high-dimensional latent vari-
able models. In Proceedings of NIPS, pages 1137?
1144.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2006. Incorporating speaker and discourse
features into speech summarization. In Proceedings of
HLT-NAACL, pages 367?374.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Rajesh Ranganath, Dan Jurafsky, and Dan Mcfarland.
2009. It?s not you, it?s me: Detecting flirting and
its misperception in speed-dates. In Proceedings of
EMNLP, pages 334?342.
Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David M. Mimno. 2009. Evaluation methods for
topic models. In Proceedings of ICML, page 139.
Yorick Wilks. 2006. Artificial companions as a new kind
of interface to the future internet. In OII Research Re-
port No. 13.
M. Woszczyna and A. Waibel. 1994. Inferring linguistic
structure in spoken language. In Proceedings of IC-
SLP.
Tae Yano, William W. Cohen, and Noah A. Smith. 2009.
Predicting response to political blog posts with topic
models. In Proceedings of NAACL, pages 477?485.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference on
streaming document collections. In Proceedings of
KDD, pages 937?946.
180
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 697?700,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Integrating Joint n-gram Features
into a Discriminative Training Framework
Sittichai Jiampojamarn? and Colin Cherry? and Grzegorz Kondrak?
?Department of Computing Science ?National Research Council Canada
University of Alberta 1200 Montreal Road
Edmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6, Canada
{sj,kondrak}@cs.ualberta.ca Colin.Cherry@nrc-cnrc.gc.ca
Abstract
Phonetic string transduction problems, such
as letter-to-phoneme conversion and name
transliteration, have recently received much
attention in the NLP community. In the past
few years, two methods have come to dom-
inate as solutions to supervised string trans-
duction: generative joint n-gram models, and
discriminative sequence models. Both ap-
proaches benefit from their ability to consider
large, flexible spans of source context when
making transduction decisions. However, they
encode this context in different ways, provid-
ing their respective models with different in-
formation. To combine the strengths of these
two systems, we include joint n-gram fea-
tures inside a state-of-the-art discriminative
sequence model. We evaluate our approach
on several letter-to-phoneme and translitera-
tion data sets. Our results indicate an improve-
ment in overall performance with respect to
both the joint n-gram approach and traditional
feature sets for discriminative models.
1 Introduction
Phonetic string transduction transforms a source
string into a target representation according to its
pronunciation. Two important examples of this task
are letter-to-phoneme conversion and name translit-
eration. In general, the problem is challenging be-
cause source orthography does not unambiguously
specify the target representation. When consider-
ing letter-to-phoneme, ambiguities and exceptions
in the pronunciation of orthography complicate con-
version. Transliteration suffers from the same ambi-
guities, but the transformation is further complicated
by restrictions in the target orthography that may not
exist in the source.
Joint n-gram models (Bisani and Ney, 2002;
Chen, 2003; Bisani and Ney, 2008) have been
widely applied to string transduction problems (Li et
al., 2004; Demberg et al, 2007; Jansche and Sproat,
2009). The power of the approach lies in building
a language model over the operations used in the
conversion from source to target. Crucially, this al-
lows the inclusion of source context in the generative
story. Smoothing techniques play an important role
in joint n-gram models, greatly affecting their per-
formance. Although joint n-gram models are capa-
ble of capturing context information in both source
and target, they cannot selectively use only source
or target information, nor can they consider arbitrary
sequences within their context window, as they are
limited by their back-off schedule.
Discriminative sequence models have also been
shown to perform extremely well on string transduc-
tion problems. These begin with a Hidden Markov
Model architecture, augmented with substring op-
erations and discriminative training. The primary
strength of these systems is their ability to include
rich indicator features representing long sequences
of source context. We will assume a specific in-
stance of discriminative sequence modeling, DI-
RECTL (Jiampojamarn et al, 2009), which achieved
the best results on several language pairs in the
NEWS Machine Transliteration Shared Task (Li et
al., 2009). The same system matches or exceeds the
performance of the joint n-gram approach on letter-
to-phoneme conversion (Jiampojamarn et al, 2008).
Its features are optimized by an online, margin-
697
based learning algorithm, specifically, the Margin
Infused Relaxed Algorithm, MIRA (Crammer and
Singer, 2003).
In this paper, we propose an approach that com-
bines these two different paradigms by formulating
the joint n-gram model as a new set of features in the
discriminative model. This leverages an advantage
of discriminative training, in that it can easily and
effectively incorporate arbitrary features. We eval-
uate our approach on several letter-to-phoneme and
transliteration data sets. Our results demonstrate an
improvement in overall performance with respect to
both the generative joint n-gram approach and the
original DIRECTL system.
2 Background
String transduction transforms an input string x into
the desired output string y. The input and output are
different representations of the same entity; for ex-
ample, the spelling and the pronunciation of a word,
or the orthographic forms of a word in two different
writing scripts.
One approach to string transduction is to view
it as a tagging problem where the input charac-
ters are tagged with the output characters. How-
ever, since sounds are often represented by multi-
character units, the relationship between the input
and output characters is often complex. This pre-
vents the straightforward application of standard
tagging techniques, but can be addressed by sub-
string decoders or semi-Markov models.
Because the relationship between x and y is hid-
den, alignments between the input and output char-
acters (or substrings) are often provided in a pre-
processing step. These are usually generated in an
unsupervised fashion using a variant of the EM al-
gorithm. Our system employs the many-to-many
alignment described in (Jiampojamarn et al, 2007).
We trained our system on these aligned examples by
using the online discriminative training of (Jiampo-
jamarn et al, 2009). At each step, the parameter
update is provided by MIRA.
3 Features
Jiampojamarn et al (2009) describe a set of indica-
tor feature templates that include (1) context features
(2) transition features and (3) linear-chain features.
context xi?c yi
. . .
xi+c yi
xi?cxi?c+1 yi
. . .
xi+c?1xi+c yi
. . . . . .
xi?c . . . xi+c yi
transition yi?1 yi
linear-chain xi?c yi?1 yi
. . .
xi+c yi?1 yi
xi?cxi?c+1 yi?1 yi
. . .
xi+c?1xi+c yi?1 yi
. . . . . .
xi?c . . . xi+c, yi?1 yi
joint n-gram xi+1?nyi+1?nxiyi
. . .
xi?1yi?1xiyi
xi+1?nyi+1?nxi+2?nyi+2?nxiyi
. . .
xi?2yi?2xi?1yi?1xiyi
. . . . . .
xi+1?nyi+1?n . . . xi?1yi?1xiyi
Table 1: Feature template
Table 1 summarizes these features and introduces
the new set of joint n-gram features.
The context features represent the source side ev-
idence that surrounds an input substring xi as it gen-
erates the target output yi. These features include
all possible n-grams that fit inside a source-side con-
text windows of size C, each conjoined with yi. The
transition features enforce the cohesion of the gen-
erated output with target-side bigrams. The linear-
chain features conjoin context and transition fea-
tures.
The set of feature templates described above
has been demonstrated to achieve excellent perfor-
mance. The context features express rich informa-
tion on the source side, but no feature template al-
lows target context beyond yi?1,yi. Target and
source context are considered jointly, but only in a
very limited fashion, as provided by the linear chain
features. Jiampojamarn et al (2008) report that con-
text features contribute the most to system perfor-
mance. They also report that increasing the Markov
order in the transition features from bigram to tri-
698
Figure 1: System accuracy as a function of the beam size
gram results in no significant improvement. Intu-
itively, the joint information of both source and tar-
get sides is important in string transduction prob-
lems. By integrating the joint n-gram features into
the online discriminative training framework, we en-
able the system to not only enjoy rich context fea-
tures and long-range dependency linear-chain fea-
tures, but we also take advantage of joint informa-
tion between source and target substring pairs, as
encoded by the joint n-gram template shown in the
bottom of Table 1.
An alternative method to incorporate a joint n-
gram feature would compute the generative joint n-
gram scores, and supply them as a real-valued fea-
ture to the model. As all of the other features in
the DIRECTL framework are indicators, the training
algorithm may have trouble scaling an informative
real-valued feature. Therefore, we represent these
joint n-gram features as binary features that indi-
cate whether the model has seen particular strings
of joint evidence in the previous n ? 1 operations
when generating yi from xi. In this case, the sys-
tem learns a distinct weight for each substring of the
joint n-gram.
In order to accommodate higher-order joint n-
grams, we replace the exact search algorithm of Ji-
ampojamarn et al (2008) with a beam search. Dur-
ing our development experiments, we observed no
significant decrease in accuracy after introducing
this approximation. Figure 1 shows the system per-
formance in terms of the word accuracy as a function
of the beam size on a development set. The perfor-
mance starts to converge quickly and shows no fur-
ther improvement for values grater than 20. In the
remaining experiments we set the beam size to 50.
We also performed development experiments
Figure 2: System accuracy as a function of n-gram size
with a version of the system that includes only joint
n-gram indicators. Figure 2 shows the word ac-
curacy with different values of n. The accuracy
reaches its maximum for n = 4, and actually falls
off for larger values of n. This anomaly is likely
caused by the model using its expanded expressive
power to memorize sequences of operations, overfit-
ting to its training data. Such overfitting is less likely
to happen in the generative joint n-gram model,
which smooths high-order estimates very carefully.
4 Experiments and Results
We evaluate our new approach on two string trans-
duction applications: (1) letter-to-phoneme conver-
sion and (2) name transliteration. For the letter-to-
phoneme conversion, we employ the English Celex,
NETtalk, OALD, CMUdict, and the French Brulex
data sets. In order to perform direct comparison with
the joint n-gram approach, we follow exactly the
same data splits as Bisani and Ney (2008). The train-
ing sizes range from 19K to 106K words. For the
transliteration task, we use three data sets provided
by the NEWS 2009 Machine Transliteration Shared
Task (Li et al, 2009): English-Russian (EnRu),
English-Chinese (EnCh), and English-Hindi (EnHi).
The training sizes range from 10K to 30K words.
We set n = 6 for the joint n-gram features; other pa-
rameters are set on the respective development sets.
Tables 2 and 3 show the performance of our new
system in comparison with the joint n-gram ap-
proach and DIRECTL. The results in the rightmost
column of Table 2 are taken directly from (Bisani
and Ney, 2008), where they were evaluated on the
same data splits. The results in the rightmost col-
umn of Table 3 are from (Jansche and Sproat, 2009),
which was the best performing system based on joint
699
Data set this work DIRECTL joint n-gram
Celex 89.23 88.54 88.58
CMUdict 76.41 75.41 75.47
OALD 85.54 82.43 82.51
NETtalk 73.52 70.18 69.00
Brulex 95.21 95.03 93.75
Table 2: Letter-to-phoneme conversion accuracy
Data set this work DIRECTL joint n-gram
EnRu 61.80 61.30 59.70
EnCh 74.17 73.34 64.60
EnHi 50.30 49.80 41.50
Table 3: Name transliteration accuracy
n-grams at NEWS 2009. We report all results in
terms of the word accuracy, which awards the sys-
tem only for complete matches between system out-
puts and the references.
Our full system outperforms both D IRECTL and
the joint n-gram approach in all data sets. This
shows the utility of adding joint n-gram features to
the DIRECTL system, and confirms an advantage of
discriminative approaches: strong competitors can
simply be folded into the model.
Comparing across tables, one can see that the gap
between the generative joint n-gram and the DI-
RECTL methods is much larger for the transliter-
ation tasks. This could be because joint n-grams
are a poor fit for transliteration, or the gap could
stem from differences between the joint n-gram im-
plementations used for the two tasks. Looking at
the improvements to DIRECTL from joint n-gram
features, we see further evidence that joint n-grams
are better suited to letter-to-phoneme than they are
to transliteration: letter-to-phoneme improvements
range from relative error reductions of 3.6 to 17.3,
while in transliteration, the largest reduction is 3.1.
5 Conclusion
We have presented a new set of joint n-gram features
for the DIRECTL discriminative sequence model.
The resulting system combines two successful ap-
proaches for string transduction ? D IRECTL and
the joint n-gram model. Joint n-gram indicator fea-
tures are efficiently trained using a large margin
method. We have shown that the resulting system
consistently outperforms both DIRECTL and strong
joint n-gram implementations in letter-to-phoneme
conversion and name transliteration, establishing a
new state-of-the-art for these tasks.
Acknowledgements
This research was supported by the Alberta Ingenu-
ity Fund and the Natural Sciences and Engineering
Research Council of Canada.
References
Maximilian Bisani and Hermann Ney. 2002. Investi-
gations on joint-multigram models for grapheme-to-
phoneme conversion. In Proc. ICSLP, pages 105?108.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451.
Stanley F. Chen. 2003. Conditional and joint mod-
els for grapheme-to-phoneme conversion. In Proc.
Eurospeech-2003.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Vera Demberg, Helmut Schmid, and Gregor Mo?hler.
2007. Phonological constraints and morphological
preprocessing for grapheme-to-phoneme conversion.
In Proc. ACL, pages 96?103.
Martin Jansche and Richard Sproat. 2009. Named entity
transcription with pair n-gram models. In Proc. ACL-
IJCNLP Named Entities Workshop, pages 32?35.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme con-
version. In Proc. HLT-NAACL, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc.
ACL, pages 905?913.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language independent approach to translitera-
tion. In Proc. ACL-IJCNLP Named Entities Workshop,
pages 28?31.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source channel model for machine transliteration. In
Proc. ACL, pages 159?166.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and Min
Zhang. 2009. Report of NEWS 2009 machine translit-
eration shared task. In Proc. ACL-IJCNLP Named En-
tities Workshop, pages 1?18.
700
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427?436,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Batch Tuning Strategies for Statistical Machine Translation
Colin Cherry and George Foster
National Research Council Canada
{Colin.Cherry,George.Foster}@nrc-cnrc.gc.ca
Abstract
There has been a proliferation of recent work
on SMT tuning algorithms capable of han-
dling larger feature sets than the traditional
MERT approach. We analyze a number of
these algorithms in terms of their sentence-
level loss functions, which motivates several
new approaches, including a Structured SVM.
We perform empirical comparisons of eight
different tuning strategies, including MERT,
in a variety of settings. Among other results,
we find that a simple and efficient batch ver-
sion of MIRA performs at least as well as
training online, and consistently outperforms
other options.
1 Introduction
The availability of linear models and discriminative
tuning algorithms has been a huge boon to statis-
tical machine translation (SMT), allowing the field
to move beyond the constraints of generative noisy
channels (Och and Ney, 2002). The ability to opti-
mize these models according to an error metric has
become a standard assumption in SMT, due to the
wide-spread adoption ofMinimum Error Rate Train-
ing or MERT (Och, 2003). However, MERT has
trouble scaling to more than 30 features, which has
led to a surge in research on tuning schemes that can
handle high-dimensional feature spaces.
These methods fall into a number of broad cate-
gories. Minimum risk approaches (Och, 2003; Smith
and Eisner, 2006) have been quietly capable of han-
dling many features for some time, but have yet to
see widespread adoption. Online methods (Liang
et al, 2006; Watanabe et al, 2007), are recognized
to be effective, but require substantial implementa-
tion efforts due to difficulties with parallelization.
Pairwise ranking (Shen et al, 2004; Hopkins and
May, 2011) recasts tuning as classification, and can
be very easy to implement, as it fits nicely into the
established MERT infrastructure.
The MERT algorithm optimizes linear weights
relative to a collection of k-best lists or lattices,
which provide an approximation to the true search
space. This optimization is wrapped in an outer
loop that iterates between optimizing weights and
re-decoding with those weights to enhance the ap-
proximation. Our primary contribution is to empiri-
cally compare eight tuning algorithms and variants,
focusing on methods that work within MERT?s es-
tablished outer loop. This is the first comparison to
include all three categories of optimizer.
Furthermore, we introduce three tuners that have
not been previously tested. In particular, we
test variants of Chiang et al?s (2008) hope-fear
MIRA that use k-best or lattice-approximated search
spaces, producing a Batch MIRA that outperforms
a popular mechanism for parallelizing online learn-
ers. We also investigate the direct optimization of
hinge loss on k-best lists, through the use of a Struc-
tured SVM (Tsochantaridis et al, 2004). We review
and organize the existing tuning literature, provid-
ing sentence-level loss functions for minimum risk,
online and pairwise training. Finally, since random-
ization plays a different role in each tuner, we also
suggest a new method for testing an optimizer?s sta-
bility (Clark et al, 2011), which sub-samples the
tuning set instead of varying a random seed.
2 Background
We begin by establishing some notation. We view
our training set as a list of triples [f,R, E ]ni=1, where
f is a source-language sentence, R is a set of target-
language reference sentences, and E is the set of
427
all reachable hypotheses; that is, each e ? Ei is a
target-language derivation that can be decoded from
fi. The function ~hi(e) describes e?s relationship to
its source fi using features that decompose into the
decoder. A linear model ~w scores derivations ac-
cording to their features, meaning that the decoder
solves:
ei(~w) = argmax
e?Ei
~w ? ~hi(e) (1)
Assuming we wish to optimize our decoder?s BLEU
score (Papineni et al, 2002), the natural objec-
tive of learning would be to find a ~w such that
BLEU([e(~w), R]n1 ) is maximal. In most machine
learning papers, this would be the point where we
would say, ?unfortunately, this objective is unfeasi-
ble.? But in SMT, we have been happily optimizing
exactly this objective for years using MERT.
However, it is now acknowledged that the MERT
approach is not feasible for more than 30 or so fea-
tures. This is due to two main factors:
1. MERT?s parameter search slows and becomes
less effective as the number of features rises,
stopping it from finding good training scores.
2. BLEU is a scale invariant objective: one can
scale ~w by any positive constant and receive the
same BLEU score.1 This causes MERT to re-
sist standard mechanisms of regularization that
aim to keep ||~w|| small.
The problems with MERT can be addressed
through the use of surrogate loss functions. In this
paper, we focus on linear losses that decompose over
training examples. Using Ri and Ei, each loss `i(~w)
indicates how poorly ~w performs on the ith training
example. This requires a sentence-level approxima-
tion of BLEU, which we re-encode into a cost ?i(e)
on derivations, where a high cost indicates that e re-
ceives a low BLEU score. Unless otherwise stated,
we will assume the use of sentence BLEU with add-
1 smoothing (Lin and Och, 2004). The learners dif-
fer in their definition of ` and ?, and in how they
employ their loss functions to tune their weights.
1This is true of any evaluation metric that considers only the
ranking of hypotheses and not their model scores; ie, it is true
of all common MT metrics.
2.1 Margin Infused Relaxed Algorithm
First employed in SMT by Watanabe et al (2007),
and refined by Chiang et al (2008; 2009), the Mar-
gin Infused Relaxed Algorithm (MIRA) employs a
structured hinge loss:
`i(~w) = max
e?Ei
[
?i(e) + ~w ?
(
~hi(e) ? ~hi(e
?
i )
)]
(2)
where e?i is an oracle derivation, and cost is de-
fined as ?i(e) = BLEUi(e?i ) ? BLEUi(e), so that
?i(e?i ) = 0. The loss `i(~w) is 0 only if ~w separates
each e ? Ei from e?i by a margin proportional to their
BLEU differentials.
MIRA is an instance of online learning, repeating
the following steps: visit an example i, decode ac-
cording to ~w, and update ~w to reduce `i(~w). Each
update makes the smallest change to ~w (subject to a
step-size cap C) that will separate the oracle from a
number of negative hypotheses. The work of Cram-
mer et al (2006) shows that updating away from a
single ?fear? hypothesis that maximizes (2) admits
a closed-form update that performs well. Let e?i be
the e ? Ei that maximizes `i(~w); the update can be
performed in two steps:
?t = min
[
C, `i(~wt)
||~hi(e?i )?
~hi(e?i)||
2
]
~wt+1 = ~wt + ?t
(~hi(e?i ) ? ~hi(e
?
i)
)
(3)
To improve generalization, the average of all
weights seen during learning is used on unseen data.
Chiang et al (2008) take advantage of MIRA?s
online nature to modify each update to better suit
SMT. The cost ?i is defined using a pseudo-
corpus BLEU that tracks the n-gram statistics of
the model-best derivations from the last few up-
dates. This modified cost matches corpus BLEU
better than add-1 smoothing, but it also makes ?i
time-dependent: each update for an example i will
be in the context of a different pseudo-corpus. The
oracle e?i also shifts with each update to ~w, as it
is defined as a ?hope? derivation, which maximizes
~w ? ~hi(e) + BLEUi(e). Hope updating ensures that
MIRA aims for ambitious, reachable derivations.
In our implementation, we make a number of
small, empirically verified deviations from Chiang
et al (2008). These include the above-mentioned
use of a single hope and fear hypothesis, and the use
428
of hope hypotheses (as opposed to model-best hy-
potheses) to build the pseudo-corpus for calculating
BLEUi. These changes were observed to be neu-
tral with respect to translation quality, but resulted in
faster running time and simplified implementation.
2.2 Direct Optimization
With the exception of MIRA, the tuning approaches
discussed in this paper are direct optimizers. That is,
each solves the following optimization problem:
~w? = argmin
~w
?
2
||~w||2 +
?
i
`i(~w) (4)
where the first term provides regularization,
weighted by ?. Throughout this paper, (4) is
optimized with respect to a fixed approximation
of the decoder?s true search space, represented as
a collection of k-best lists. The various methods
differ in their definition of loss and in how they
optimize their objective.
Without the complications added by hope decod-
ing and a time-dependent cost function, unmodified
MIRA can be shown to be carrying out dual coordi-
nate descent for an SVM training objective (Martins
et al, 2010). However, exactly what objective hope-
fear MIRA is optimizing remains an open question.
Gimpel and Smith (2012) discuss these issues in
greater detail, while also providing an interpretable
alternative to MIRA.
2.3 Pairwise Ranking Optimization
Introduced by Hopkins and May (2011), Pairwise
Ranking Optimization (PRO) aims to handle large
feature sets inside the traditional MERT architec-
ture. That is, PRO employs a growing approxima-
tion of Ei by aggregating the k-best hypotheses from
a series of increasingly refined models. This archi-
tecture is desirable, as most groups have infrastruc-
ture to k-best decode their tuning sets in parallel.
For a given approximate E?i, PRO creates a sam-
ple Si of (eg, eb) pairs, such that BLEUi(eg) >
BLEUi(eb). It then uses a binary classifier to sep-
arate each pair. We describe the resulting loss in
terms of an SVM classifier, to highlight similarities
with MIRA. In terms of (4), PRO defines
`i(~w) =
?
(eg ,eb)?Si
2
(
1 + ~w ?
(~hi(eb) ? ~hi(eg)
))+
where (x)+ = max(0, x). The hinge loss is multi-
plied by 2 to account for PRO?s use of two examples
(positive and negative) for each sampled pair. This
sum of hinge-losses is 0 only if each pair is separated
by a model score of 1. Given [S]ni=1, this convex
objective can be optimized using any binary SVM.2
Unlike MIRA, the margin here is fixed to 1; cost en-
ters into PRO through its sampling routine, which
performs a large uniform sample and then selects a
subset of pairs with large BLEU differentials.
The PRO loss uses a sum over pairs in place of
MIRA?s max, which allows PRO to bypass oracle
selection, and to optimize with off-the-shelf classi-
fiers. This sum is potentially a weakness, as PRO
receives credit for each correctly ordered pair in its
sample, and these pairs are not equally relevant to
the final BLEU score.
2.4 Minimum Risk Training
Minimum risk training (MR) interprets ~w as a prob-
abilistic model, and optimizes expected BLEU. We
focus on expected sentence costs (Och, 2003; Zens
et al, 2007; Li and Eisner, 2009), as this risk is sim-
ple to optimize and fits nicely into our mathemati-
cal framework. Variants that use the expected suffi-
cient statistics of BLEU also exist (Smith and Eisner,
2006; Pauls et al, 2009; Rosti et al, 2011).
We again assume a MERT-like tuning architec-
ture. Let ?i(e) = ?BLEUi(e) and let
`i(~w) = EP~w [?i(e)] =
?
e?E?i
[
exp(~w ? ~hi(e))?i(e)
]
?
e??E?i
exp(~w ? ~hi(e?))
This expected cost becomes increasingly small as
greater probability mass is placed on derivations
with high BLEU scores. This smooth, non-convex
objective can be solved to a local minimum using
gradient-based optimizers; we have found stochastic
gradient descent to be quite effective (Bottou, 2010).
Like PRO, MR requires no oracle derivation, and
fits nicely into the established MERT architecture.
The expectations needed to calculate the gradient
EP~w
[
~hi(e)?i(e)
]
? EP~w [?i(e)]EP~w
[
~hi(e)
]
2Hopkins and May (2011) advocate a maximum-entropy
version of PRO, which is what we evaluate in our empirical
comparison. It can be obtained using a logit loss `i(~w) =
P
g,b 2 log
?
1 + exp
?
~w ?
`~hi(eb) ? ~hi(eg)
???
.
429
are trivial to extract from a k-best list of derivations.
Each downward step along this gradient moves the
model toward likely derivations, and away from
likely derivations that incur high costs.
3 Novel Methods
We have reviewed three tuning methods, all of which
address MERT?s weakness with large features by us-
ing surrogate loss functions. Additionally, MIRA
has the following advantages over PRO and MR:
1. Loss is optimized using the true Ei, as opposed
to an approximate search space E?i.
2. Sentence BLEU is calculated with a fluid
pseudo-corpus, instead of add-1 smoothing.
Both of these advantages come at a cost: oper-
ating on the true Ei sacrifices easy parallelization,
while using a fluid pseudo-corpus creates an unsta-
ble learning objective. We develop two large-margin
tuners that explore these trade-offs.
3.1 Batch MIRA
Online training makes it possible to learn with the
decoder in the loop, forgoing the need to approxi-
mate the search space, but it is not necessarily con-
venient to do so. Online algorithms are notoriously
difficult to parallelize, as they assume each example
is visited in sequence. Parallelization is important
for efficient SMT tuning, as decoding is still rela-
tively expensive.
The parallel online updates suggested by Chi-
ang et al (2008) involve substantial inter-process
communication, which may not be easily supported
by all clusters. McDonald et al (2010) suggest
a simpler distributed strategy that is amenable to
map-reduce-like frameworks, which interleaves on-
line training on shards with weight averaging across
shards. This strategy has been adopted by Moses
(Hasler et al, 2011), and it is the one we adopt in
our MIRA implementation.
However, online training using the decoder may
not be necessary for good performance. The success
of MERT, PRO and MR indicates that their shared
search approximation is actually quite reasonable.
Therefore, we propose Batch MIRA, which sits ex-
actly where MERT sits in the standard tuning archi-
tecture, greatly simplifying parallelization:
1. Parallel Decode: [E? ?]n1 = k-best([f, E ]
n
1 , ~w)
2. Aggregate: [E? ]n1 = [E? ]
n
1 ? [E?
?]n1
3. Train: ~w = BatchMIRA([f,R, E? ]n1 , ~w)
4. Repeat
where BatchMIRA() trains the SMT-adapted MIRA
algorithm to completion on the current approxima-
tion E? , without parallelization.3 The only change we
make to MIRA is to replace the hope-fear decoding
of sentences with the hope-fear re-ranking of k-best
lists. Despite its lack of parallelization, each call to
BatchMIRA() is extremely fast, as SMT tuning sets
are small enough to load all of [E? ]n1 into memory. We
test two Batch MIRA variants, which differ in their
representation of E? . Pseudo-code that covers both is
provided in Algorithm 1. Note that if we set E? = E ,
Algorithm 1 also describes online MIRA.
Batch k-best MIRA inherits all of the MERT archi-
tecture. It is very easy to implement; the hope-fear
decoding steps can by carried out by simply evaluat-
ing BLEU score and model score for each hypothe-
sis in the k-best list.
Batch Lattice MIRA replaces k-best decoding in
step 1 with decoding to lattices. To enable loading
all of the lattices into memory at once, we prune to
a density of 50 edges per reference word. The hope-
fear decoding step requires the same oracle lattice
decoding algorithms as online MIRA (Chiang et al,
2008). The lattice aggregation in the outer loop can
be kept reasonable by aggregating only those paths
corresponding to hope or fear derivations.
3.2 Structured SVM
While MIRA takes a series of local hinge-loss re-
ducing steps, it is also possible to directly minimize
the sum of hinge-losses using a batch algorithm, cre-
ating a structured SVM (Tsochantaridis et al, 2004).
To avoid fixing an oracle before optimization begins,
we adapt Yu and Joachim?s (2009) latent SVM to
our task, which allows the oracle derivation for each
sentence to vary during training. Again we assume a
MERT-like architecture, which approximates E with
an E? constructed from aggregated k-best lists.
Inspired by the local oracle of Liang et al (2006),
we define E?i? to be an oracle set:
E?i? = {e|BLEUi(e) is maximal}.
3In our implementation, BatchMIRA() trains for J = 30
passes over [E? ]n1 .
430
Algorithm 1 BatchMIRA
input [f,R, E? ]n1 , ~w, max epochs J , step cap C,
and pseudo-corpus decay ?.
init Pseudo-corpus BG to small positive counts.
init t = 1; ~wt = ~w
for j from 1 to J do
for i from 1 to n in random order do
// Hope-fear decode in E?i
e?t = argmaxe?E?i
[
~wt ? ~hi(e) + BLEUi(e)
]
e?t = argmaxe?E?i
[
~wt ? ~hi(e) ? BLEUi(e)
]
// Update weights
?t = BLEUi(e?t ) ? BLEUi(e
?
t)
?t = min
[
C,
?t+~wt?
(
~hi(e?t)?~hi(e
?
t )
)
||~hi(e?t )?
~hi(e?t)||
2
]
~wt+1 = ~wt + ?t
(~hi(e?t ) ? ~hi(e
?
i)
)
// Update statistics
BG = ?BG+ BLEU stats for e?t and Ri
t = t + 1
end for
~wavgj =
1
nj
?nj
t?=1 ~wt?
end for
return ~wavgj that maximizes training BLEU
Cost is also defined in terms of the maximal BLEU,
?i(e) = max
e??E?i
[
BLEUi(e
?)
]
? BLEUi(e).
Finally, loss is defined as:
`i(~w) = maxe?E?i
[
?i(e) + ~w ? ~hi(e)
? maxe?i ?E?i?
(
~w ? ~hi(e?i )
)]
This loss is 0 only if some hypothesis in the oracle
set is separated from all others by a margin propor-
tional to their BLEUi differentials.
With loss defined in this manner, we can mini-
mize (4) to local minimum by using an alternating
training procedure. For each example i, we select
a fixed e?i ? E?i? that maximizes model score; that
is, ~w is used to break ties in BLEU for oracle selec-
tion. With the oracle fixed, the objective becomes
a standard structured SVM objective, which can be
minimized using a cutting-plane algorithm, as de-
scribed by Tsochantaridis et al (2004). After doing
so, we can drive the loss lower still by iterating this
process: re-select each oracle (breaking ties with the
new ~w), then re-optimize ~w. We do so 10 times. We
were surprised by the impact of these additional iter-
ations on the final loss; for some sentences, E?i? can
be quite large.
Despite the fact that both algorithms use a struc-
tured hinge loss, there are several differences be-
tween our SVM and MIRA. The SVM has an ex-
plicit regularization term ? that is factored into its
global objective, while MIRA regularizes implicitly
by taking small steps. The SVM requires a stable
objective to optimize, meaning that it must forgo the
pseudo-corpus used by MIRA to calculate ?i; in-
stead, the SVM uses an interpolated sentence-level
BLEU (Liang et al, 2006).4 Finally, MIRA?s oracle
is selected with hope decoding. With a sufficiently
large ~w, any e ? E? can potentially become the ora-
cle. In contrast, the SVM?s local oracle is selected
from a small set E??, which was done to more closely
match the assumptions of the Latent SVM.
To solve the necessary quadratic programming
sub-problems, we use a multiclass SVM similar to
LIBLINEAR (Hsieh et al, 2008). Like Batch MIRA
and PRO, the actual optimization is very fast, as the
cutting plane converges quickly and all of [E? ]n1 can
be loaded into memory at once.
3.3 Qualitative Summary
We have reviewed three tuning methods and intro-
duced three tuning methods. All six methods em-
ploy sentence-level loss functions, which in turn em-
ploy sentence-level BLEU approximations. Except
for online MIRA, all methods plug nicely into the
existing MERT architecture. These methods can be
split into two groups: MIRA variants (online, batch
k-best, batch lattice), and direct optimizers (PRO,
MR and SVM). The MIRA variants use pseudo-
corpus BLEU in place of smoothed BLEU, and
provide access to richer hypothesis spaces through
the use of online training or lattices.5 The direct
optimizers have access to a tunable regularization
parameter ?, and do not require special purpose
code for hope and fear lattice decoding. Batch
4SVM training with interpolated BLEU outperformed add-1
BLEU in preliminary testing. A comparison of different BLEU
approximations under different tuning objectives would be an
interesting path for future work.
5MR approaches that use lattices (Li and Eisner, 2009;
Pauls et al, 2009; Rosti et al, 2011) or the complete search
space (Arun et al, 2010) exist, but are not tested here.
431
k-best MIRA straddles the two groups, benefiting
from pseudo-corpus BLEU and easy implementa-
tion, while being restricted to a k-best list.
4 Experimental Design
We evaluated the six tuning strategies described
in this paper, along with two MERT baselines,
on three language pairs
(
French-English (Fr-En),
English-French (En-Fr) and Chinese-English (Zh-
En)
)
, across three different feature-set sizes. Each
setting was run five times over randomized variants
to improve reliability. To cope with the resulting
large number of configurations, we ran all experi-
ments using an efficient phrase-based decoder simi-
lar to Moses (Koehn et al, 2007).
All tuning methods that use an approximate E? per-
form 15 iterations of the outer loop and return the
weights that achieve the best development BLEU
score. When present, ? was coarsely tuned (trying 3
values differing by magnitudes of 10) in our large-
feature Chinese-English setting.
? kb-mert : k-best MERT with 20 random
restarts. All k-best methods use k = 100.
? lb-mert : Lattice MERT (Machery et al, 2008)
using unpruned lattices and aggregating only
those paths on the line search?s upper envelope.
? mira : Online MIRA (?2.1). All MIRA vari-
ants use a pseudo-corpus decay ? = 0.999 and
C = 0.01. Online parallelization follows Mc-
Donald et al (2010), using 8 shards. We tested
20, 15, 10, 8 and 5 shards during development.
? lb-mira : Batch Lattice MIRA (?3.1).
? kb-mira : Batch k-best MIRA (?3.1).
? pro : PRO (?2.3) follows Hopkins and May
(2011); however, we were unable to find set-
tings that performed well in general. Reported
results use MegaM6 with a maximum of 30 it-
erations (as is done in Moses; the early stop-
ping provides a form of regularization) for our
six English/French tests, and MegaM with 100
iterations and a reduced initial uniform sam-
ple (50 pairs instead of 5000) for our three En-
glish/Chinese tests.
? mr : MR as described in ?2.4. We employ a
learning rate of ?0/(1 + ?0?t) for stochastic
6Available at www.cs.utah.edu/?hal/megam/
corpus sentences words (en) words (fr)
train 2,928,759 60,482,232 68,578,459
dev 2,002 40,094 44,603
test1 2,148 42,503 48,064
test2 2,166 44,701 49,986
Table 1: Hansard Corpus (English/French)
corpus sentences words (zh) words (en)
train1 6,677,729 200,706,469 213,175,586
train2 3,378,230 69,232,098 66,510,420
dev 1,506 38,233 40,260
nist04 1,788 53,439 59,944
nist06 1,664 41,782 46,140
nist08 1,357 35,369 42,039
Table 2: NIST09 Corpus (Chinese-English). Train1 cor-
responds to the UN and Hong Kong sub-corpora; train2
to all others.
gradient descent, with ?0 tuned to optimize the
training loss achieved after one epoch (Bottou,
2010). Upon reaching a local optimum, we re-
shuffle our data, re-tune our learning rate, and
re-start from the optimum, repeating this pro-
cess 5 times. We do not sharpen our distribu-
tion with a temperature or otherwise control for
entropy; instead, we trust ? = 50 to maintain a
reasonable distribution.
? svm : Structured SVM (?3.2) with ? = 1000.
4.1 Data
Systems for English/French were trained on Cana-
dian Hansard data (years 2001?2009) summarized
in table 1.7 The dev and test sets were chosen
randomly from among the most recent 5 days of
Hansard transcripts.
The system for Zh-En was trained on data from
the NIST 2009 Chinese MT evaluation, summarized
in table 2. The dev set was taken from the NIST
05 evaluation set, augmented with some material re-
served from other NIST corpora. The NIST 04, 06,
and 08 evaluation sets were used for testing.
4.2 SMT Features
For all language pairs, phrases were extracted with
a length limit of 7 from separate word alignments
7This corpus will be distributed on request.
432
template max fren enfr zhen
tgt unal 50 50 50 31
count bin 11 11 11 11
word pair 6724 1298 1291 1664
length bin 63 63 63 63
total 6848 1422 1415 1769
Table 3: Sparse feature templates used in Big.
performed by IBM2 and HMM models and sym-
metrized using diag-and (Koehn et al, 2003). Con-
ditional phrase probabilities in both directions were
estimated from relative frequencies, and from lexical
probabilities (Zens and Ney, 2004). Language mod-
els were estimated with Kneser-Ney smoothing us-
ing SRILM. Six-feature lexicalized distortion mod-
els were estimated and applied as in Moses.
For each language pair, we defined roughly equiv-
alent systems (exactly equivalent for En-Fr and Fr-
En, which are mirror images) for each of three
nested feature sets: Small, Medium, and Big.
The Small set defines a minimal 7-feature sys-
tem intended to be within easy reach of all tuning
strategies. It comprises 4 TM features, one LM, and
length and distortion features. For the Chinese sys-
tem, the LM is a 5-gram trained on the NIST09 Gi-
gaword corpus; for English/French, it is a 4-gram
trained on the target half of the parallel Hansard.
The Medium set is a more competitive 18-feature
system. It adds 4 TM features, one LM, and 6 lex-
icalized distortion features. For Zh-En, Small?s TM
(trained on both train1 and train2 in table 2) is re-
placed by 2 separate TMs from these sub-corpora;
for En/Fr, the extra TM (4 features) comes from a
forced-decoding alignment of the training corpus, as
proposed by Wuebker et al (2010). For Zh-En, the
extra LM is a 4-gram trained on the target half of the
parallel corpus; for En/Fr, it is a 4-gram trained on
5m sentences of similar parliamentary data.
The Big set adds sparse Boolean features to
Medium, for a maximum of 6,848 features. We used
sparse feature templates that are equivalent to the
PBMT set described in (Hopkins and May, 2011):
tgt unal picks out each of the 50 most frequent tar-
get words to appear unaligned in the phrase table;
count bin uniquely bins joint phrase pair counts with
upper bounds 1,2,4,8,16,32,64,128,1k,10k,?; word
pair fires when each of the 80 most frequent words
in each language appear aligned 1-1 to each other, to
some other word, or not 1-1; and length bin captures
each possible phrase length and length pair. Table 3
summarizes the feature templates, showing the max-
imum number of features each can generate, and the
number of features that received non-zero weights in
the final model tuned by MR for each language pair.
Feature weights are initialized to 1.0 for each of
the TM, LM and distortion penalty features. All
other weights are initialized to 0.0.
4.3 Stability Testing
We follow Clark et al(2011), and perform multiple
randomized replications of each experiment. How-
ever, their method of using different random seeds
is not applicable in our context, since randomization
does not play the same role for all tuning methods.
Our solution was to randomly draw and fix four dif-
ferent sub-samples of each dev set, retaining each
sentence with a probability of 0.9. For each tuning
method and setting, we then optimize on the origi-
nal dev and all sub-samples. The resulting standard
deviations provide an indication of stability.
5 Results
The results of our survey of tuning methods can be
seen in Tables 4, 5 and 6. Results are averaged over
test sets (2 for Fr/En, 3 for Zh/En), and over 5 sub-
sampled runs per test set. The SD column reports the
standard deviation of the average test score across
the 5 sub-samples.
It may be dismaying to see only small score
improvements when transitioning from Medium to
Big. This is partially due to the fact that our Big fea-
ture set affects only phrase-table scores. Our phrase
tables are already strong, through our use of large
data or leave-one-out forced decoding. The impor-
tant baseline when assessing the utility of a method
is Medium k-best MERT. In all language pairs, our
Big systems generally outperform this baseline by
0.4 BLEU points. It is interesting to note that most
methods achieve the bulk of this improvement on the
Medium feature set.8 This indicates that MERT be-
gins to show some problems even in an 18-feature
8One can see the same phenomenon in the results of Hop-
kins and May (2011) as well.
433
Table 4: French to English Translation (Fr-En)
Small Medium Big
Tune Test SD Tune Test SD Tune Test SD
kb-mert 40.50 39.94 0.04 40.75 40.29 0.13 n/a n/a n/a
lb-mert 40.52 39.93 0.11 40.93 40.39 0.08 n/a n/a n/a
mira 40.38 39.94 0.04 40.64 40.59 0.06 41.02 40.74 0.05
kb-mira 40.46 39.97 0.05 40.92 40.64 0.12 41.46 40.75 0.08
lb-mira 40.44 39.98 0.06 40.94 40.65 0.06 41.59 40.78 0.09
pro 40.11 40.05 0.05 40.16 40.07 0.08 40.55 40.21 0.24
mr 40.24 39.88 0.05 40.70 40.57 0.14 41.18 40.60 0.08
svm 40.05 40.20 0.03 40.60 40.56 0.08 41.32 40.52 0.07
Table 5: English to French Translation (En-Fr)
Small Medium Big
Tune Test SD Tune Test SD Tune Test SD
kb-mert 40.47 39.72 0.06 40.70 40.02 0.11 n/a n/a n/a
lb-mert 40.45 39.76 0.08 40.90 40.13 0.10 n/a n/a n/a
mira 40.36 39.83 0.03 40.78 40.44 0.02 40.89 40.45 0.05
kb-mira 40.44 39.83 0.02 40.94 40.35 0.06 41.48 40.52 0.06
lb-mira 40.45 39.83 0.02 41.05 40.45 0.04 41.65 40.59 0.07
pro 40.17 39.57 0.15 40.30 40.01 0.04 40.75 40.22 0.17
mr 40.31 39.65 0.04 40.94 40.30 0.13 41.45 40.47 0.10
svm 39.99 39.55 0.03 40.40 39.96 0.05 41.00 40.21 0.03
Table 6: Chinese to English Translation (Zh-En)
Small Medium Big
Tune Test SD Tune Test SD Tune Test SD
kb-mert 23.97 29.65 0.06 25.74 31.58 0.42 n/a n/a n/a
lb-mert 24.18 29.48 0.15 26.42 32.39 0.22 n/a n/a n/a
mira 23.98 29.54 0.01 26.23 32.58 0.08 25.99 32.52 0.08
kb-mira 24.10 29.51 0.06 26.28 32.50 0.12 26.18 32.61 0.14
lb-mira 24.13 29.59 0.05 26.43 32.77 0.06 26.40 32.82 0.18
pro 23.25 28.74 0.24 25.80 32.42 0.20 26.49 32.18 0.40
mr 23.87 29.55 0.09 26.26 32.52 0.12 26.42 32.79 0.15
svm 23.59 28.91 0.05 26.26 32.70 0.05 27.23 33.04 0.12
40
41
42
43
44
45
lb-mirasvmmr
40
40.2
40.4
40.6
40.8
lb-mira svmmr
Tune Test
Figure 1: French-English test of regularization with an over-fitting feature set. lb-mira varies C ={1, 1e-1, 1e-2, 1e-3}, its default
C is 1e-2; svm varies ? ={1e2, 1e3, 1e4, 1e5}, its default ? is 1e3; mr varies ? ={5, 5e1, 5e2, 5e3}, its default ? is 5e1.
434
setting, which can be mitigated through the use of
Lattice MERT.
When examining score differentials, recall that
the reported scores average over multiple test sets
and sub-sampled tuning runs. Using Small features,
all of the tested methods are mostly indistinguish-
able, but as we move to Medium and Big, Batch
Lattice MIRA emerges as our method of choice. It
is the top scoring system in all Medium settings,
and in two of three Big settings (in Big Zh-En, the
SVM comes first, with batch lattice MIRA placing
second). However, all of the MIRA variants per-
form similarly, though our implementation of on-
line MIRA is an order of magnitude slower, mostly
due to its small number of shards. It is interest-
ing that our batch lattice variant consistently outper-
forms online MIRA. We attribute this to our paral-
lelization strategy, Chiang et al?s (2008) more com-
plex solution may perform better.
There may be settings where an explicit regular-
ization parameter is desirable, thus we also make a
recommendation among the direct optimizers (PRO,
MR and SVM). Though these systems all tend to
show a fair amount of variance across language and
feature sets (likely due to their use sentence-level
BLEU), MR performs the most consistently, and is
always within 0.2 of batch lattice MIRA.
The SVM?s performance on Big Zh-En is an in-
triguing outlier in our results. Note that it not only
performs best on the test set, but also achieves the
best tuning score by a large margin. We suspect
we have simply found a setting where interpolated
BLEU and our choice of ? work particularly well.
We intend to investigate this case to see if this level
of success can be replicated consistently, perhaps
through improved sentence BLEU approximation or
improved oracle selection.
5.1 Impact of Regularization
One main difference between MIRA and the direct
optimizers is the availability of an explicit regular-
ization term ?. To measure the impact of this param-
eter, we designed a feature set explicitly for over-
fitting. This set uses our Big Fr-En features, with the
count bin template modified to distinguish each joint
count observed in the tuning set. These new fea-
tures, which expand the set to 20k+ features, should
generalize poorly.
We tested MR and SVM on our Fr-En data us-
ing this feature set, varying their respective regular-
ization parameters by factors of 10. We compared
this to Batch Lattice MIRA?s step-size cap C, which
controls its regularization (Martins et al, 2010). The
results are shown in Figure 1. Looking at the tuning
scores, one can see that ? affords much greater con-
trol over tuning performance than MIRA?s C. Look-
ing at test scores, MIRA?s narrow band of regular-
ization appears to be just about right; however, there
is no reason to expect this to always be the case.
6 Conclusion
We have presented three new, large-margin tuning
methods for SMT that can handle thousands of fea-
tures. Batch lattice and k-best MIRA carry out their
online training within approximated search spaces,
reducing costs in terms of both implementation and
training time. The Structured SVM optimizes a sum
of hinge losses directly, exposing an explicit reg-
ularization term. We have organized the literature
on tuning, and carried out an extensive comparison
of linear-loss SMT tuners. Our experiments show
Batch Lattice MIRA to be the most consistent of the
tested methods. In the future, we intend to inves-
tigate improved sentence-BLEU approximations to
help narrow the gap between MIRA and the direct
optimizers.
Acknowledgements
Thanks to Mark Hopkins, Zhifei Li and Jonathan
May for their advice while implementing the meth-
ods in this review, and to Kevin Gimpel, Roland
Kuhn and the anonymous reviewers for their valu-
able comments on an earlier draft.
References
Abhishek Arun, Barry Haddow, and Philipp Koehn.
2010. A unified approach to minimum risk training
and decoding. In Proceedings of the Joint Workshop
on Statistical Machine Translation and MetricsMATR,
pages 365?374.
Leon Bottou. 2010. Large-scale machine learning with
stochastic gradient descent. In International Confer-
ence on Computational Statistics, pages 177?187.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP, pages 224?233.
435
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In HLT-NAACL, pages 218?226.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In ACL, pages 176?181.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
HLT-NAACL, Montreal, Canada, June.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2011.
Margin infused relaxed algorithm for moses. The
Prague Bulletin of Mathematical Linguistics, 96:69?
78.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In EMNLP, pages 1352?1362.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In ICML.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL,
pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180, Prague, Czech Republic, June.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP, pages
40?51.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In COLING-ACL,
pages 761?768.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In COLING, pages 501?507.
WolfgangMachery, Franz Josef Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP, pages 725?734.
Andre? F. T. Martins, Kevin Gimpel, Noah A. Smith,
Eric P. Xing, Pedro M. Q. Aguiar, and Ma?rio A. T.
Figueiredo. 2010. Learning structured classifiers with
dual coordinate descent. Technical Report CMU-ML-
10-109, Carnegie Mellon University.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In ACL, pages 456?464.
Franz Joseph Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302,
Philadelphia, PA, July.
Franz Joseph Och. 2003. Minimum error rate training
for statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL, pages 311?318.
Adam Pauls, John Denero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In EMNLP, pages 1418?1427.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected bleu training for
graphs: BBN system description for WMT11 system
combination task. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 159?
165.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
HLT-NAACL, pages 177?184, Boston, Massachusetts,
May 2 - May 7.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In COLING-
ACL, pages 787?794.
Ioannis Tsochantaridis, Thomas Hofman, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In ICML, pages 823?830.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL, pages
764?773.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In ACL.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
ICML.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In HLT-
NAACL, pages 257?264, Boston, USA, May.
Richard Zens, Sa?sa Hasan, and Hermann Ney. 2007. A
systematic comparison of training criteria for statisti-
cal machine translation. In EMNLP, pages 524?532.
436
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21?24,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
MSR SPLAT, a language analysis toolkit 
 
Chris Quirk, Pallavi Choudhury, Jianfeng 
Gao, Hisami Suzuki, Kristina Toutanova, 
Michael Gamon, Wen-tau Yih, Lucy 
Vanderwende 
Colin Cherry 
Microsoft Research National Research Council Canada 
Redmond, WA 98052 USA 1200 Montreal Road 
 Ottawa, Ontario K1A 0R6 
{chrisq, pallavic, jfgao, 
hisamis, kristout, 
mgamon,scottyih, 
lucyv@microsoft.com} 
colin.cherry@nrccnrc.gc.ca 
 
Abstract 
We describe MSR SPLAT, a toolkit for lan-
guage analysis that allows easy access to the 
linguistic analysis tools produced by the NLP 
group at Microsoft Research. The tools in-
clude both traditional linguistic analysis tools 
such as part-of-speech taggers, constituency 
and dependency parsers, and more recent de-
velopments such as sentiment detection and 
linguistically valid morphology. As we ex-
pand the tools we develop for our own re-
search, the set of tools available in MSR 
SPLAT will be extended. The toolkit is acces-
sible as a web service, which can be used 
from a broad set of programming languages. 
1 Introduction 
The availability of annotated data sets that have 
become community standards, such as the Penn 
TreeBank (Marcus et al, 1993) and PropBank 
(Palmer et al, 2005), has enabled many research 
institutions to build core natural language pro-
cessing components, including part-of-speech tag-
gers, chunkers, and parsers. There remain many 
differences in how these components are built, re-
sulting in slight but noticeable variation in the 
component output. In experimental settings, it has 
proved sometimes difficult to distinguish between 
improvements contributed by a specific component 
feature from improvements due to using a differ-
ently-trained linguistic component, such as tokeni-
zation. The community recognizes this difficulty, 
and shared task organizers are now providing ac-
companying parses and other analyses of the 
shared task data. For instance, the BioNLP shared 
task organizers have provided output from a num-
ber of parsers1, alleviating the need for participat-
ing systems to download and run unfamiliar tools. 
On the other hand, many community members 
provide downloads of NLP tools2 to increase ac-
cessibility and replicability of core components.  
Our toolkit is offered in this same spirit. We 
have created well-tested, efficient linguistic tools 
in the course of our research, using commonly 
available resources such as the PTB and PropBank. 
We also have created some tools that are less 
commonly available in the community, for exam-
ple linguistically valid base forms and semantic 
role analyzers. These components are on par with 
other state of the art systems. 
We hope that sharing these tools will enable 
some researchers to carry out their projects without 
having to re-create or download commonly used 
NLP components, or potentially allow researchers 
to compare our results with those of their own 
tools. The further advantage of designing MSR 
SPLAT as a web service is that we can share new 
components on an on-going basis. 
2 Parsing Functionality 
2.1 Constituency Parsing 
                                                          
1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask  for 
the description of other resources made available in addition to 
the shared task data. 
2 See, for example, http://nlp.stanford.edu/software; 
http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; 
http://incubator.apache.org/opennlp 
21
The syntactic parser in MSR SPLAT attempts to 
reconstruct a parse tree according the Penn Tree-
Bank specification (Marcus et al, 1993). This rep-
resentation captures the notion of labeled syntactic 
constituents using a parenthesized representation. 
For instance, the sentence ?Colorless green ideas 
sleep furiously.? could be assigned the following 
parse tree, written in the form of an S expression: 
(TOP (S 
   (NP (JJ Colorless) (JJ green) (NNS ideas)) 
   (VP (VB sleep) (ADVP (RB furiously))) 
   (. .))) 
For instance, this parse tree indicates that ?Color-
less green ideas? is a noun phrase (NP), and ?sleep 
furiously? is a verb phrase (VP). 
Using the Wall Street Journal portion of the 
Penn TreeBank, we estimate a coarse grammar 
over the given grammar symbols. Next, we per-
form a series of refinements to automatically learn 
fine-grained categories that better capture the im-
plicit correlations in the tree using the split-merge 
method of Petrov et al (2006). Each input symbol 
is split into two new symbols, both with a new 
unique symbol label, and the grammar is updated 
to include a copy of each original rule for each 
such refinement, with a small amount of random 
noise added to the probability of each production 
to break ties. We estimate new grammar parame-
ters using an accelerated form of the EM algorithm 
(Salakhutdinov and Roweis, 2003). Then the low-
est 50% of the split symbols (according to their 
estimated contribution to the likelihood of the data) 
are merged back into their original form and the 
parameters are again re-estimated using AEM. We 
found six split-merge iterations produced optimal 
accuracy on the standard development set. 
The best tree for a given input is selected ac-
cording to the max-rule approach (cf. Petrov et al 
2006). Coarse-to-fine parsing with pruning at each 
level helps increase speed; pruning thresholds are 
picked for each level to have minimal impact on 
development set accuracy. However, the initial 
coarse pass still has runtime cubic in the length of 
the sentence. Thus, we limit the search space of the 
coarse parse by closing selected chart cells before 
the parse begins (Roark and Hollingshead, 2008). 
We train a classifier to determine if constituents 
may start or end at each position in the sentence. 
For instance, constituents seldom end at the word 
?the? or begin at a comma. Closing a number of 
chart cells can substantially improve runtime with 
minimal impact on accuracy. 
2.2 Dependency Parsing 
The dependency parses produced by MSR SPLAT 
are unlabeled, directed arcs indicating the syntactic 
governor of each word. 
These dependency trees are computed from the 
output of the constituency parser. First, the head of 
each non-terminal is computed according to a set 
of rules (Collins, 1999). Then, the tree is flattened 
into maximal projections of heads. Finally, we in-
troduce an arc from a parent word p to a child 
word c if the non-terminal headed by p is a parent 
of the non-terminal headed by c. 
2.3 Semantic Role Labeling 
The Semantic Role Labeling component of MSR 
SPLAT labels the semantic roles of verbs accord-
ing to the PropBank specification (Palmer et al, 
2005). The semantic roles represent a level of 
broad-coverage shallow semantic analysis which 
goes beyond syntax, but does not handle phenome-
na like co-reference and quantification.  
For example, in the two sentences ?John broke 
the window? and ?The window broke?, the phrase 
the window will be marked with a THEME label. 
Note that the syntactic role of the phrase in the two 
sentences is different but the semantic role is the 
same. The actual labeling scheme makes use of 
numbered argument labels, like ARG0, ARG1, ?, 
ARG5 for core arguments, and labels like ARGM-
TMP,ARGM-LOC, etc. for adjunct-like argu-
ments. The meaning of the numbered arguments is 
verb-specific, with ARG0 typically representing an 
agent-like role, and ARG1 a patient-like role. 
This implementation of an SRL system follows 
the approach described in (Xue and Palmer, 04), 
and includes two log-linear models for argument 
identification and classification. A single syntax 
tree generated by the MSR SPLAT split-merge 
parser is used as input. Non-overlapping arguments 
are derived using the dynamic programming algo-
rithm by Toutanova et al (2008).  
3 Other Language Analysis Functionality 
3.1 Sentence Boundary / Tokenization 
22
This analyzer identifies sentence boundaries and 
breaks the input into tokens. Both are represented 
as offsets of character ranges. Each token has both 
a raw form from the string and a normalized form 
in the PTB specification, e.g., open and close pa-
rentheses are replaced by -LRB- and -RRB-, re-
spectively, to remove ambiguity with parentheses 
indicating syntactic structure. A finite state ma-
chine using simple rules and abbreviations detects 
sentence boundaries with high accuracy, and a set 
of regular expressions tokenize the input. 
3.2 Stemming / Lemmatization 
We provide three types of stemming: Porter stem-
ming, inflectional morphology and derivational 
morphology. 
3.2.1 Stems  
The stemmer analyzer indicates a stem form for 
each input token, using the standard Porter stem-
ming algorithm (Porter, 1980). These forms are 
known to be useful in applications such as cluster-
ing, as the algorithm assigns the same form ?dai? 
to ?daily? and ?day?, but as these forms are not 
citation forms of these words, presentation to end 
users is known to be problematic. 
3.2.2 Lemmas 
The lemma analyzer uses inflectional morphology 
to indicate the dictionary lookup form of the word. 
For example, the lemma of ?daily? will be ?daily?, 
while the lemma of ?children? will be ?child?. We 
have mined the lemma form of input tokens using 
a broad-coverage grammar NLPwin (Heidorn, 
2000) over very large corpora. 
3.2.3 Bases  
The base analyzer uses derivational morphology to 
indicate the dictionary lookup form of the word; as 
there can be more than one derivation for a given 
word, the base type returns a list of forms. For ex-
ample, the base form of ?daily? will be ?day?, 
while the base form of ?additional? will be ?addi-
tion? and ?add?. We have generated a static list of 
base forms of tokens using a broad-coverage 
grammar NLPwin (Heidorn, 2000) over very large 
corpora. If the token form has not been observed in 
those corpora, we will not return a base form. 
3.3 POS tagging 
We train a maximum entropy Markov Model on 
part-of-speech tags from the Penn TreeBank. This 
optimized implementation has very high accuracy 
(over 96% on the test set) and yet can tag tens of 
thousands of words per second. 
3.4 Chunking 
The chunker (Gao et al, 2001) is based on a Cas-
caded Markov Model, and is trained on the Penn 
TreeBank. With state-of-the-art chunking accuracy 
as evaluated on the benchmark dataset, the chunker 
is also robust and efficient, and has been used to 
process very large corpora of web documents. 
4 The Flexibility of a Web Service 
By making the MSR SPLAT toolkit available as a 
web service, we can provide access to new tools, 
e.g. sentiment analysis. We are in the process of 
building out the tools to provide language analysis 
for languages other than English. One step in this 
direction is a tool for transliterating between Eng-
lish and Katakana words. Following Cherry and 
Suzuki (2009), the toolkit currently outputs the 10-
best transliteration candidates with probabilities for 
both directions.  
Another included service is the Triples analyz-
er, which returns the head of the subject, the verb, 
and the head of the object, whenever such a triple 
is encountered. We found this functionality to be 
useful as we were exploring features for our sys-
tem submitted to the BioNLP shared task. 
5 Programmatic Access 
5.1 Web service reference 
We have designed a web service that accepts a 
batch of text and applies a series of analysis tools 
to that text, returning a bag of analyses. This main 
web service call, named ?Analyze?, requires four 
parameters: the language of the text (such as ?en? 
for English), the raw text to be analyzed, the set of 
analyzers to apply, and an access key to monitor 
and, if necessary, constrain usage. It returns a list 
of analyses, one from each requested analyzer, in a 
23
simple JSON (JavaScript Object Notation) format 
easy to parse in many programming languages. 
In addition, there is a web service call ?Lan-
guages? that enumerates the list of available lan-
guages, and ?Analyzers? to discover the set of 
analyzers available in a given language.  
5.2 Data Formats 
We use a relatively standard set of data representa-
tions for each component. Parse trees are returned 
as S expressions, part-of-speech tags are returned 
as lists, dependency trees are returned as lists of 
parent indices, and so on. The website contains an 
authoritative description of each analysis format. 
5.3 Speed 
Speed of analysis is heavily dependent on the 
component involved. Analyzers for sentence sepa-
ration, tokenization, and part-of-speech tagging 
process thousands of sentences per second; our 
fastest constituency parser handles tens of sentenc-
es per second. Where possible, the user is encour-
aged to send moderate sized requests (perhaps a 
paragraph at a time) to minimize the impact of 
network latency. 
6 Conclusion 
We hope that others will find the tools that we 
have made available as useful as we have. We en-
courage people to send us their feedback so that we 
can improve our tools and increase collaboration in 
the community. 
7 Script Outline 
The interactive UI (Figure 1) allows an arbitrary 
sentence to be entered and the desired levels of 
analysis to be selected as output. As there exist 
other such toolkits, the demonstration is primarily 
aimed at allowing participants to assess the quality, 
utility and speed of the MSR SPLAT tools. 
http://research.microsoft.com/en-us/projects/msrsplat/ 
References  
Colin Cherry and Hisami Suzuki. 2009. Discriminative sub-
string decoding for transliteration. In Proceedings of 
EMNLP. 
Michael Collins. 1999. Head-driven statistical models for 
natural language parsing. PhD Dissertation, University of 
Pennsylvania. 
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming 
Zhou and Chang-Ning Huang. 2001. Improving query 
translation for CLIR using statistical Models. In Proceed-
ings of SIGIR. 
George Heidorn. 2000. Intelligent writing assistance. In R. 
Dale, H. Moisl and H. Somers (eds.), A Handbook of Natu-
ral Language Processing: Techniques and Applications for 
the Processing of Text. New York: Marcel Dekker. 
Mitchell Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a Large Annotated Corpus 
of English: The Penn Treebank. Computational Linguistics 
19(2): 313-330. 
Martha Palmer, Dan Gildea, Paul Kingsbury. 2005. The Prop-
osition Bank: An Annotated Corpus of Semantic Roles. 
Computational Linguistics, 31(1): 71-105 
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3): 130-137. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 
2006. Learning Accurate, Compact, and Interpretable Tree 
Annotation. In Proceedings of ACL. 
Brian Roark and Kristy Hollingshead. 2008. Classifying chart 
cells for quadratic complexity context-free inference. In 
Proceedings of COLING. 
Ruslan Salakhutdinov and Sam Roweis. 2003. Adaptive Over-
relaxed Bound Optimization Methods. In Proceedings of 
ICML. 
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2008. A global joint model for semantic role labeling, 
Computational Linguistics, 34(2): 161-191. 
Nianwen Xue and Martha Palmer. 2004. Calibrating Features 
for Semantic Role Labeling. In Proceedings of EMNLP. 
Munmun de Choudhury, Scott Counts, Michael Gamon. Not 
All Moods are Created Equal! Exploring Human Emotional 
States in Social Media. Accepted for presentation in 
ICWSM 2012 
Munmun de Choudhury, Scott Counts, Michael Gamon. Hap-
py, Nervous, Surprised? Classification of Human Affective 
States in Social Media. Accepted for presentation (short 
paper) in ICWSM 2012 
 
 
Figure 1. Screenshot of the MSR SPLAT interactive UI 
showing selected functionalities which can be toggled 
on and off. This is the interface that we propose to 
demo at NAACL. 
 
 
24
Proceedings of NAACL-HLT 2013, pages 22?31,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improved Reordering for Phrase-Based Translation using Sparse Features
Colin Cherry
National Research Council Canada
Colin.Cherry@nrc-cnrc.gc.ca
Abstract
There have been many recent investigations
into methods to tune SMT systems using large
numbers of sparse features. However, there
have not been nearly so many examples of
helpful sparse features, especially for phrase-
based systems. We use sparse features to ad-
dress reordering, which is often considered a
weak point of phrase-based translation. Using
a hierarchical reordering model as our base-
line, we show that simple features coupling
phrase orientation to frequent words or word-
clusters can improve translation quality, with
boosts of up to 1.2 BLEU points in Chinese-
English and 1.8 in Arabic-English. We com-
pare this solution to a more traditional max-
imum entropy approach, where a probability
model with similar features is trained on word-
aligned bitext. We show that sparse decoder
features outperform maximum entropy hand-
ily, indicating that there are major advantages
to optimizing reordering features directly for
BLEU with the decoder in the loop.
1 Introduction
With the growing adoption of tuning algorithms that
can handle thousands of features (Chiang et al,
2008; Hopkins and May, 2011), SMT system de-
signers now face a choice when incorporating new
ideas into their translation models. Maximum like-
lihood models can be estimated from large word-
aligned bitexts, creating a small number of highly
informative decoder features; or the same ideas can
be incorporated into the decoder?s linear model di-
rectly. There are trade-offs to each approach. Max-
imum likelihood models can be estimated from mil-
lions of sentences of bitext, but optimize a mis-
matched objective, predicting events observed in
word aligned bitext instead of optimizing translation
quality. Sparse decoder features have the opposite
problem; with the decoder in the loop, we can only
tune on small development sets,1 but a translation
error metric directly informs training.
We investigate this trade-off in the context of re-
ordering models for phrase-based decoding. Start-
ing with the intuition that most lexicalized reorder-
ing models do not smooth their orientation distri-
butions intelligently for low-frequency phrase-pairs,
we design features that track the first and last words
(or clusters) of the phrases in a pair. These features
are incorporated into a maximum entropy reorder-
ing model, as well as sparse decoder features, to see
which approach best complements the now-standard
relative-frequency lexicalized reordering model.
We also view our work as an example of strong
sparse features for phrase-based translation. Fea-
tures from hierarchical and syntax-based transla-
tion (Chiang et al, 2009) do not easily transfer
to the phrase-based paradigm, and most work that
has looked at large feature counts in the context of
phrase-based translation has focused on the learn-
ing method, and not the features themselves (Hop-
kins and May, 2011; Cherry and Foster, 2012; Gim-
pel and Smith, 2012). We show that by targeting
reordering, large gains can be made with relatively
simple features.
2 Background
Phrase-based machine translation constructs its tar-
get sentence from left-to-right, with each translation
operation selecting a source phrase and appending
its translation to the growing target sentence, until
1Some systems tune for BLEU on much larger sets (Simi-
aner et al, 2012; He and Deng, 2012), but these require excep-
tional commitments of resources and time.
22
all source words have been covered exactly once.
The first phrase-based translation systems applied
only a distortion penalty to model reordering (Koehn
et al, 2003; Och and Ney, 2004). Any devia-
tion from monotone translation is penalized, with
a single linear weight determining how quickly the
penalty grows.
2.1 Lexicalized Reordering
Implemented in a number of phrase-based decoders,
the lexicalized reordering model (RM) uses word-
aligned data to determine how each phrase-pair
tends to be reordered during translation (Tillmann,
2004; Koehn et al, 2005; Koehn et al, 2007).
The core idea in this RM is to divide reordering
events into three orientations that can be easily deter-
mined both during decoding and from word-aligned
data. The orientations can be described in terms of
the previously translated source phrase (prev) and
the next source phrase to be translated (next):
? Monotone (M): next immediately follows prev.
? Swap (S): prev immediately follows next.
? Discontinuous (D): next and prev are not adja-
cent in the source.
Note that prev and next can be defined for construct-
ing a translation from left-to-right or from right-to-
left. Most decoders incorporate RMs for both direc-
tions; our discussion will generally only cover left-
to-right, with the right-to-left case being implicit and
symmetrical.
As the decoder extends its hypothesis by trans-
lating a source phrase, we can assess the implied
orientations to determine if the resulting reordering
makes sense. This is done using the probability of
an orientation given the phrase pair pp = [src, tgt ]
extending the hypothesis:2
P (o|pp) ?
cnt(o, pp)
?
o cnt(o, pp)
(1)
where o ? {M,S,D}, cnt uses simple heuristics on
word-alignments to count phrase pairs and their ori-
entations, and the ? symbol allows for smoothing.
The log of this probability is easily folded into the
linear models that guide modern decoders. Better
2pp corresponds to the phrase pair translating next for the
left-to-right model, and prev for right-to-left.
performance is achieved by giving each orientation
its own log-linear weight (Koehn et al, 2005).
2.2 Hierarchical Reordering
Introduced by Galley and Manning (2008), the hier-
archical reordering model (HRM) also tracks statis-
tics over orientations, but attempts to increase the
consistency of orientation assignments. To do so,
they remove the emphasis on the previously trans-
lated phrase (prev ), and instead determine orienta-
tion using a compact representation of the full trans-
lation history, as represented by a shift-reduce stack.
Each source span is shifted onto the stack as it is
translated; if the new top is adjacent to the span be-
low it, then a reduction merges the two.
Orientations are determined in terms of the top
of this stack,3 rather than the previously translated
phrase prev. The resulting orientations are more
consistent across different phrasal decompositions
of the same translation, and more consistent with the
statistics extracted from word aligned data. This re-
sults in a general improvement in performance. We
assume the HRM as our baseline reordering model.
It is important to note that although our maximum
entropy and sparse reordering solutions build on the
HRM, the features in this paper can still be applied
without a shift-reduce stack, by using the previously
translated phrase where we use the top of the stack.
2.3 Maximum Entropy Reordering
One frequent observation regarding both the RM and
the HRM is that the statistics used to grade orien-
tations are very sparse. Each orientation predic-
tion P (o|pp) is conditioned on an entire phrase pair.
Koehn et al (2005) experiment with alternatives,
such as conditioning on only the source or the tar-
get, but using the entire pair generally performs best.
The vast majority of phrase pairs found in bitext with
standard extraction heuristics are singletons (more
than 92% in our Arabic-English bitext), and the cor-
responding P (o|pp) estimates are based on a single
observation. Because of these heavy tails, there have
been several attempts to use maximum entropy to
create more flexible distributions.
One straight-forward way to do so is to continue
predicting orientations on phrases, but to use maxi-
3In the case of the right-to-left model, an approximation of
the top of the stack is used instead.
23
mum entropy to consider features of the phrase pair.
This is the approach taken by Xiong et al (2006);
their maximum entropy model chooses between M
and S orientations, which are the only two options
available in their chart-based ITG decoder. Nguyen
et al (2009) build a similar model for a phrase-based
HRM, using syntactic heads and constituent labels
to create a rich feature set. They show gains over an
HRM baseline, albeit on a small training set.
A related approach is to build a reordering model
over words, which is evaluated at phrase bound-
aries at decoding time. Zens and Ney (2006) pro-
pose one such model, with jumps between words
binned very coarsely according to their direction
and distance, testing models that differentiate only
left jumps from right, as well as the cross-product
of {left, right} ? {adjacent, discontinuous}. Their
features consider word identity and automatically-
induced clusters. Green et al (2010) present a sim-
ilar approach, with finer-grained distance bins, us-
ing word-identity and part-of-speech for features.
Yahyaei and Monz (2010) also predict distance bins,
but use much more context, opting to look at both
sides of a reordering jump; they also experiment
with hard constraints based on their model.
Tracking word-level reordering simplifies the ex-
traction of complex models from word alignments;
however, it is not clear if it is possible to enhance
a word reordering model with the stack-based his-
tories used by HRMs. In this work, we construct a
phrase orientation maximum entropy model.
3 Methods
Our primary contribution is a comparison between
the standard HRM and two feature-based alterna-
tives. Since a major motivating concern is smooth-
ing, we begin with a detailed description of our
HRM baseline, followed by our maximum entropy
HRM and our novel sparse reordering features.
3.1 Relative Frequency Model
The standard HRM uses relative frequencies to build
smoothed maximum likelihood estimates of orien-
tation probabilities. Orientation counts for phrase
pairs are collected from bitext, using the method de-
scribed by Galley and Manning (2008). The proba-
bility model P (o|pp = [src, tgt ]) is estimated using
recursive MAP smoothing:
P (o|pp) =
cnt(o, pp) + ?sPs(o|src) + ?tPt(o|tgt)
?
o cnt(o, pp) + ?s + ?t
Ps(o|src) =
?
tgt cnt(o, src, tgt) + ?gPg(o)
?
o,tgt cnt(o, src, tgt) + ?g
Pt(o|tgt) =
?
src cnt(o, src, tgt) + ?gPg(o)
?
o,src cnt(o, src, tgt) + ?g
Pg(o) =
?
pp cnt(o, pp) + ?u/3
?
o,pp cnt(o, pp) + ?u
(2)
where the various ? parameters can be tuned em-
pirically. In practice, the model is not particularly
sensitive to these parameters.4
3.2 Maximum Entropy Model
Next, we describe our implementation of a maxi-
mum entropy HRM. Our goal with this system is
to benefit from modeling features of a phrase pair,
while keeping the system architecture as simple and
replicable as possible. To simplify training, we learn
our model from the same orientation counts that
power the relative-frequency HRM. To simplify de-
coder integration, we limit our feature space to in-
formation from a single phrase pair.
In a maximum entropy model, the probability of
an orientation o given a phrase pair pp is given by a
log-linear model:
P (o|pp) =
exp(w ? f(o, pp))
?
o? exp(w ? f(o?, pp))
(3)
where f(o, pp) returns features of a phrase-pair, and
w is the learned weight vector. We build two models,
one for left-to-right translation, and one for right-
to-left. As with the relative frequency model, we
limit our discussion to the left-to-right model, with
the other direction being symmetrical.
We construct a training example for each unique
phrase-pair type (as opposed to token) found in our
bitext. We use the orientation counts observed for
a phrase pair ppi to construct its example weight:
ci =
?
o cnt(o, ppi). The same counts are used to
construct a target distribution P? (o|ppi), using the
4We use a historically good setting of ?? = 10 throughout.
24
Base:
bias; src ? tgt ; src; tgt
src.first ; src.last ; tgt .first ; tgt .last
clust50(src.first); clust50(src.last)
clust50(tgt .first); clust50(tgt .last)
? Orientation {M,S,D}
Table 1: Features for the Maximum Entropy HRM.
unsmoothed relative frequency estimate in Equa-
tion 1. We then train our weight vector to minimize:
1
2
||w||2+C
?
i
ci
[
log
?
o exp (w ? f(o, ppi))
?
?
o P? (o|ppi) (w ? f(o, ppi))
]
(4)
where C is a hyper-parameter that controls the
amount of emphasis placed on minimizing loss ver-
sus regularizing w.5 Note that this objective is a de-
parture from previous work, which tends to create an
example for each phrase-pair token, effectively as-
signing P? (o|pp) = 1 to a single gold-standard ori-
entation. Instead, our model attempts to reproduce
the target distribution P? for the entire type, where
the penalty ci for missing this target is determined
by the frequency of the phrase pair. The resulting
model will tend to match unsmoothed relative fre-
quency estimates for very frequent phrase pairs, and
will smooth intelligently using features for less fre-
quent phrase pairs.
All of the features returned by f(o|pp) are derived
from the phrase pair pp = [src, tgt ], with the goal
of describing the phrase pair at a variety of granu-
larities. Our features are described in Table 1, using
the following notation: the operators first and last
return the first and last words of phrases,6 while the
operator clust50 maps a word onto its corresponding
cluster from an automatically-induced, determinis-
tic 50-word clustering provided by mkcls (Och,
1999). Our use of words at the corners of phrases
(as opposed to the syntactic head, or the last aligned
word) follows Xiong et al (2006), while our use of
word clusters follows Zens and Ney (2006). Each
feature has the orientation o appended onto it.
To help scale and to encourage smoothing, we
only allow features that occur in at least 5 phrase pair
5Preliminary experiments indicated that the model is robust
to the choice of C; we use C = 0.1 throughout.
6first = last for a single-word phrase
Base:
src.first ; src.last ; tgt .first ; tgt .last
top.src.first ; top.src.last ; top.tgt .last
between words
? Representation
{80-words, 50-clusters, 20-clusters}
? Orientation
{M,S,D}
Table 2: Features for the Sparse Feature HRM.
tokens. Furthermore, to deal with the huge number
of extracted phrase pairs (our Arabic system extracts
roughly 88M distinct phrase pair types), we subsam-
ple pairs that have been observed only once, keeping
only 10% of them. This reduces the number of train-
ing examples from 88M to 19M.
3.3 Sparse Reordering Features
The maximum entropy approach uses features to
model the distribution of orientations found in word
alignments. Alternatively, a number of recent tun-
ing methods, such as MIRA (Chiang et al, 2008)
or PRO (Hopkins and May, 2011), can handle thou-
sands of features. These could be used to tune simi-
lar features to maximize BLEU directly.
Given the appropriate tuning architecture, the
sparse feature approach is actually simpler in many
ways than the maximum entropy approach. There
is no need to scale to millions of training exam-
ples, and there is no question of how to integrate the
trained model into the decoder. Instead, one simply
implements the desired features in the decoder?s fea-
ture API and then tunes as normal. The challenge is
to design features so that the model can be learned
from small tuning sets.
The standard approach for sparse feature design
in SMT is to lexicalize only on extremely fre-
quent words, such as the top-80 words from each
language (Chiang et al, 2009; Hopkins and May,
2011). We take that approach here, but we also
use deterministic clusters to represent words from
both languages, as provided by mkcls. These clus-
ters mirror parts-of-speech quite effectively (Blun-
som and Cohn, 2011), without requiring linguistic
resources. They should provide useful generaliza-
tion for reordering decisions. Inspired by recent suc-
cesses in semi-supervised learning (Koo et al, 2008;
25
corpus sentences words (ar) words (en)
train 1,490,514 46,403,734 47,109,486
dev 1,663 45,243 50,550
mt08 1,360 45,002 51,341
mt09 1,313 40,684 46,813
Table 3: Arabic-English Corpus. For English dev and test
sets, word counts are averaged across 4 references.
Lin and Wu, 2009), we cluster at two granularities
(20 clusters and 50 clusters), and allow the discrim-
inative tuner to determine how to best employ the
various representations.
We add the sparse features in Table 2 to our
decoder to help assess reordering decisions. As
with the maximum entropy model, orientation is ap-
pended to each feature. Furthermore, each feature
has a different version for each of our three word
representations. Like the maximum entropy model,
we describe the phrase pair being added to the hy-
pothesis in terms of the first and last words of its
phrases. Unlike the maximum entropy model, we
make no attempt to use entire phrases or phrase-
pairs as features, as they would be far too sparse for
our small tuning sets. However, due to the sparse
features? direct decoder integration, we have access
to a fair amount of extra context. We represent the
current top of the stack (top) using its first and last
source words (accessible from the HRM stack), and
its last target word (accessible using language model
context). Furthermore, for discontinuous (D) orien-
tations, we can include an indicator for each source
word between the current top of the stack and the
phrase being added.
Because the sparse feature HRM has no access
to phrase-pair or monolingual phrase features, and
because it completely ignores our large supply of
word-aligned training data, we view it as compli-
mentary to the relative frequency HRM. We always
include both when tuning and decoding. Further-
more, we only include sparse features in the left-
to-right translation direction, as the features already
consider context (top) as well as the next phrase.
4 Experimental Design
We test our reordering models in Arabic to English
and Chinese to English translation tasks. Both sys-
tems are trained on data from the NIST 2012 MT
corpus sentences words (ch) words (en)
train 3,505,529 65,917,610 69,453,695
dev 1,894 48,384 53,584
mt06 1,664 39,694 47,143
mt08 1,357 33,701 40,893
Table 4: Chinese-English Corpus. For English dev and
test sets, word counts are averaged across 4 references.
evaluation; the Arabic system is summarized in Ta-
ble 3 and the Chinese in Table 4. The Arabic sys-
tem?s development set is the NIST mt06 test set, and
its test sets are mt08 and mt09. The Chinese sys-
tem?s development set is taken from the NIST mt05
evaluation set, augmented with some material re-
served from our NIST training corpora in order to
better cover newsgroup and weblog domains. Its test
sets are mt06 and mt08.
4.1 Baseline System
For both language pairs, word alignment is per-
formed by GIZA++ (Och and Ney, 2003), with
5 iterations of Model 1, HMM, Model 3 and
Model 4. Phrases are extracted with a length limit
of 7 from alignments symmetrized using grow-
diag-final-and (Koehn et al, 2003). Conditional
phrase probabilities in both directions are estimated
from relative frequencies, and from lexical probabil-
ities (Zens and Ney, 2004). 4-gram language mod-
els are estimated from the target side of the bitext
with Kneser-Ney smoothing. Relative frequency
and maximum entropy RMs are represented with six
features, with separate weights for M, S and D in
both directions (Koehn et al, 2007). HRM orien-
tations are determined using an unrestricted shift-
reduce parser (Cherry et al, 2012). We also em-
ploy a standard distortion penalty incorporating the
minimum completion cost described by Moore and
Quirk (2007). Our multi-stack phrase-based decoder
is quite similar to Moses (Koehn et al, 2007).
For all systems, parameters are tuned with a
batch-lattice variant of hope-fear MIRA (Chiang et
al., 2008; Cherry and Foster, 2012). Preliminary ex-
periments suggested that the sparse reordering fea-
tures have a larger impact when tuned with lattices
as opposed to n-best lists.
26
4.2 Evaluation
We report lower-cased BLEU (Papineni et al, 2002),
evaluated using the same English tokenization used
in training. For our primary results, we perform ran-
dom replications of parameter tuning, as suggested
by Clark et al (2011). Each replication uses a dif-
ferent random seed to determine the order in which
MIRA visits tuning sentences. We test for signifi-
cance using Clark et al?s MultEval tool, which uses
a stratified approximate randomization test to ac-
count for multiple replications.
5 Results
We begin with a comparison of the reordering mod-
els described in this paper: the hierarchical reorder-
ing model (HRM), the maximum entropy HRM
(Maxent HRM) and our sparse reordering features
(Sparse HRM). Results are shown in Table 5.
Our three primary points of comparison have been
tested with 5 replications. We report BLEU scores
averaged across replications as well as standard de-
viations, which indicate optimizer stability. We also
provide unreplicated results for two systems, one us-
ing only the distortion penalty (No RM), and one
using a non-hierarchical reordering model (RM).
These demonstrate that our baseline already has
quite mature reordering capabilities.
The Maxent HRM has very little effect on trans-
lation performance. We found this surprising; we
expected large gains from improving the reorder-
ing distributions of low-frequency phrase-pairs. See
?5.1 for further exploration of this result.
The Sparse HRM, on the other hand, performs
very well. It produces significant BLEU score im-
provements on all test sets, with improvements rang-
ing between 1 and 1.8 BLEU points. Even with
millions of training sentences for our HRM, there
is a large benefit in building HRM-like features that
are tuned to optimize the decoder?s BLEU score on
small tuning sets. We examine the impact of subsets
of these features in ?5.2.
The test sets? standard deviations increase from
0.1 under the baseline to 0.3 under the Sparse HRM
for Chinese-English, indicating a decrease in opti-
mizer stability. With so many features trained on
so few sentences, this is not necessarily surprising.
Fortunately, looking at the actual replications (not
Base:
src.first ; src.last ; tgt .first ; tgt .last
? Representation
{80-words, 50-clusters}
? Orientation
{M,S,D}
Table 6: Intersection of Maxent & Sparse HRM features.
shown), we confirmed that if a replication produced
low scores in one test, it also produced low scores
in the other. This means that one should be able to
outperform the average case by using a dev-test set
to select among replications.
5.1 Maximum Entropy Analysis
The next two sections examine our two solutions
in detail, starting with the Maxent HRM. To avoid
excessive demands on our computing resources, all
experiments report tuning with a single replication
with the same seed. We select Arabic-English for
our analysis, as this pair has high optimizer stability
and fast decoding speeds.
Why does the Maxent HRM help so little? We
begin by investigating some design decisions. One
possibility is that our subsampling of frequency-1
training pairs (see ?3.2) harmed performance. To
test the impact of this decision, we train a Max-
ent HRM without subsampling, taking substantially
longer. The resulting BLEU scores (not shown) are
well within the projected standard deviations for op-
timizer instability (0.1 BLEU from Table 5). This
indicates that subsampling is not the problem. To
confirm our choice of hyperparameters, we conduct
a grid search over the Maxent HRM?s regulariza-
tion parameter C (see Equation 4), covering the set
{1, 0.1, 0.01, 0.001}, where C = 0.1 is the value
used throughout this paper. Again, the resulting
BLEU scores (not shown) are all within 0.1 of the
means reported in Table 5.
Another possibility is that the Maxent HRM has
an inferior feature set. We selected features for our
Maxent and Sparse HRMs to be similar, but also to
play to the strengths of each method. To level the
playing field, we train and test both systems with the
feature set shown in Table 6, which is the intersec-
tion of the features from Tables 1 and 2. The result-
ing average BLEU scores are shown in Table 7. With
27
Chinese-English Arabic-English
Method n tune std mt06 std mt08 std tune std mt08 std mt09 std
No RM 1 24.3 ? 32.0 ? 26.4 ? 41.7 ? 41.4 ? 44.1 ?
RM 1 25.2 ? 33.3 ? 27.4 ? 42.4 ? 42.6 ? 45.2 ?
HRM (baseline) 5 25.6 0.0 34.2 0.1 28.0 0.1 42.9 0.0 42.9 0.1 45.5 0.0
HRM + Maxent HRM 5 25.6 0.0 34.3 0.1 28.1 0.1 43.0 0.0 42.9 0.0 45.6 0.1
HRM + Sparse HRM 5 28.0 0.1 35.4 0.3 29.0 0.3 47.0 0.1 44.6 0.1 47.3 0.1
Table 5: Comparing reordering methods according to BLEU score. n indicates the number of tuning replications,
while standard deviations (std) indicate optimizer stability. Test scores that are significantly higher (p < 0.01) than
the HRM baseline are highlighted in bold.
Method ?HRM +HRM
HRM (baseline) ? 44.2
Original
Maxent HRM 44.2 44.2
Sparse HRM 45.4 46.0
Intersection
Maxent HRM 43.8 44.2
Sparse HRM 45.2 46.0
Table 7: Arabic-English BLEU scores with each system?s
original feature set versus the intersection of the two fea-
ture sets, with and without the relative frequency HRM.
BLEU is averaged across mt08 and mt09.
the baseline HRM included, performance does not
change for either system with the intersected feature
set. Sparse features continue to help, while the max-
imum entropy model does not. Without the HRM,
both systems degrade under the intersection, though
the Sparse HRM still improves over the baseline.
Finally, we examine Maxent HRM performance
as a function of the amount of word-aligned train-
ing data. To do so, we hold all aspects of our sys-
tem constant, except for the amount of bitext used to
train either the baseline HRM or the Maxent HRM.
Importantly, the phrase table always uses the com-
plete bitext. For our reordering training set, we hold
out the final two thousand sentences of bitext to cal-
culate perplexity. This measures the model?s sur-
prise at reordering events drawn from previously un-
seen alignments; lower values are better. We pro-
ceed to subsample sentence pairs from the remain-
ing bitext, in order to produce a series of training
bitexts of increasing size. We subsample with the
probability of accepting a sentence pair, Pa, set to
{0.001, 0.01, 0.1, 1}. It is important to not confuse
this subsampling of sentence pairs with the sub-
sampling of low-frequency phrase pairs (see ?3.2),
which is still carried out by the Maxent HRM for
each training scenario.
Figure 1 shows how BLEU (averaged across both
test sets) and perplexity vary as training data in-
creases from 1.5K sentences to the full 1.5M. At
Pa < 0.1, corresponding to less than 150K sen-
tences, the maximum entropy model actually makes
a substantial difference in terms of BLEU. However,
these deltas narrow to nothing as we reach millions
of training sentences. This is consistent with the re-
sults of Nguyen et al (2009), who report that maxi-
mum entropy reordering outperforms a similar base-
line, but using only 50K sentence pairs.
A related observation is that held-out perplexity
does not seem to predict BLEU in any useful way.
In particular, perplexity does not predict that the two
systems will become similar as data grows, nor does
it predict that maxent?s performance will level off.
Predicting the orientations of unseen alignments is
not the same task as predicting the orientation for a
phrase during translation. We suspect that perplexity
places too much emphasis on rare or previously un-
seen phrase pairs, due to phrase extraction?s heavy
tails. Preliminary attempts to correct for this us-
ing absolute discounting on the test counts did not
resolve these issues. Unfortunately, in maximizing
(regularized or smoothed) likelihood, both maxent
and relative frequency HRMs are chasing the per-
plexity objective, not the BLEU objective.
5.2 Sparse Feature Analysis
The results in Table 7 from ?5.1 already provide
us with a number of insights regarding the Sparse
HRM. First, note that the intersected feature set uses
only information found within a single phrase. The
fact that the Sparse HRM performs so well with
28
1.55	 ?
1.6	 ?
1.65	 ?
1.7	 ?
1.75	 ?
1.8	 ?
1.85	 ?
1.9	 ?
1.95	 ?
2	 ?
0.001	 ? 0.01	 ? 0.1	 ? 1	 ?
Perplexity	 ?as	 ?Data	 ?Grows	 ?
MaxEnt	 ?
RelFreq	 ?
41.5	 ?
42	 ?
42.5	 ?
43	 ?
43.5	 ?
44	 ?
44.5	 ?
0	 ? 0.001	 ? 0.01	 ? 0.1	 ? 1	 ?
BLEU	 ?as	 ?Data	 ?Grows	 ?
Figure 1: Learning curves for Relative Frequency and Maximum Entropy reordering models on Arabic-English.
Feature Group Count BLEU
No Sparse HRM 0 44.2
Between 312 44.4
Stack 1404 45.2
Phrase 1872 45.9
20 Clusters 506 45.4
50 Clusters 1196 45.8
80 Words 1886 45.8
Full Sparse HRM 3588 46.0
Table 8: Versions of the Sparse HRM built using or-
ganized subsets of the complete feature set for Arabic-
English. Count is the number of distinct features, while
BLEU is averaged over mt08 and mt09.
intersected features indicates that modeling context
outside a phrase is not essential for strong perfor-
mance. Furthermore, the ?HRM portion of the ta-
ble indicates that the sparse HRM does not require
the baseline HRM to be present in order to outper-
form it. This is remarkable when one considers that
the Sparse HRM uses less than 4k features to model
phrase orientations, compared to the 530M proba-
bilities7 maintained by the baseline HRM?s relative
frequency model.
To determine which feature groups are most im-
portant, we tested the Sparse HRM on Arabic-
English with a number of feature subsets. We report
BLEU scores averaged over both test sets in Table 8.
First, we break our features into three groups accord-
ing to what part of the hypothesis is used to assess
orientation. For each of these location groups, all
forms of word representation (clusters or frequent
words) are employed. The groups consist of Be-
788.4M phrase pairs ? 3 orientations (M, S and D) ? 2
translation directions (left-to-right and right-to-left).
tween: the words between the top of the stack and
the phrase to be added; Stack: words describing
the current top of the stack; and Phrase: words de-
scribing the phrase pair being added to the hypothe-
sis. Each group was tested alone to measure its use-
fulness. This results in a clear hierarchy, with the
phrase features being the most useful (nearly as use-
ful as the complete system), and the between fea-
tures being the least. Second, we break our features
into three groups according to how words are rep-
resented. For each of these representation groups,
all location groups (Between, Stack and Phrase) are
employed. The groups are quite intuitive: 20 Clus-
ters, 50 Clusters or 80 Words. The differences be-
tween representations are much less dramatic than
the location groups. All representations perform
well on their own, with the finer-grained ones per-
forming better. Including multiple representations
provides a slight boost, but these experiments sug-
gest that a leaner model could certainly drop one or
two representations with little impact.
In its current implementation, the Sparse HRM is
roughly 4 times slower than the baseline decoder.
Our sparse feature infrastructure is designed for flex-
ibility, not speed. To affect reordering, each sparse
feature template is re-applied with each hypothesis
extension. However, the intersected feature set from
?5.1 is only 2 times slower, and could be made faster
still. That feature set uses only within-phrase fea-
tures to asses orientations; therefore, the total weight
for each orientation for each phrase-pair could be
pre-calculated, making its cost comparable to the
baseline.
29
Chinese-English tune mt06 mt08
Base 27.7 39.9 33.7
+Sparse HRM 29.2 41.0 34.1
Arabic-English tune mt08 mt09
Base 49.6 49.1 51.6
+Sparse HRM 51.7 49.9 52.2
Table 9: The effect of Sparse HRMs on complex systems.
5.3 Impact on Competition-Grade SMT
Thus far, we have employed a baseline that has been
designed for both translation quality and replicabil-
ity. We now investigate the impact of our Sparse
HRM on a far more complex baseline: our internal
system used for MT competitions such as NIST.
The Arabic system uses roughly the same bilin-
gual data as our original baseline, but also includes
a 5-gram language model learned from the English
Gigaword. The Chinese system adds the UN bitext
as well as the English Gigaword. Both systems make
heavy use of linear mixtures to create refined transla-
tion and language models, mixing across sources of
corpora, genre and translation direction (Foster and
Kuhn, 2007; Goutte et al, 2009). They also mix
many different sources of word alignments, with
the system adapting across alignment sources us-
ing either binary indicators or linear mixtures. Im-
portantly, these systems already incorporate thou-
sands of sparse features as described by Hopkins and
May (2011). These provide additional information
for each phrase pair through frequency bins, phrase-
length bins, and indicators for frequent alignment
pairs. Both systems include a standard HRM.
The result of adding the Sparse HRM to these sys-
tems is shown in Table 9. Improvements range from
0.4 to 1.1 BLEU, but importantly, all four test sets
improve. The impact of these reordering features is
reduced slightly in the presence of more carefully
tuned translation and language models, but they re-
main a strong contributor to translation quality.
6 Conclusion
We have shown that sparse reordering features can
improve the quality of phrase-based translations,
even in the presence of lexicalized reordering mod-
els that track the same orientations. We have com-
pared this solution to a maximum entropy model,
which does not improve our HRM baseline. Our
analysis of the maximum entropy solution indicates
that smoothing the orientation estimates is not a ma-
jor concern in the presence of millions of sentences
of bitext. This implies that our sparse features are
achieving their improvement because they optimize
BLEU with the decoder in the loop, side-stepping
the objective mismatch that can occur when train-
ing on word-aligned data. The fact that this is possi-
ble with such small tuning corpora is both surprising
and encouraging.
In the future, we would like to investigate how
to incorporate useful future cost estimates for our
sparse reordering features. Previous work has shown
future distortion penalty estimates to be important
for both translation speed and quality (Moore and
Quirk, 2007; Green et al, 2010), but we have ig-
nored future costs in this work. We would also like
to investigate features inspired by transition-based
parsing, such as features that look further down the
reordering stack. Finally, as there is evidence that
ideas from lexicalized reordering can help hierarchi-
cal phrase-based SMT (Huck et al, 2012), it would
be interesting to explore the use of sparse RMs in
that setting.
Acknowledgments
Thanks to George Foster, Roland Kuhn and the
anonymous reviewers for their valuable comments
on an earlier draft.
References
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part of
speech induction. In ACL, pages 865?874, Portland,
Oregon, USA, June.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In HLT-
NAACL, pages 427?436, Montre?al, Canada, June.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In Proceedings of the
Workshop on Statistical Machine Translation, pages
200?209, Montre?al, Canada, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP, pages 224?233.
30
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In HLT-NAACL, pages 218?226.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In ACL, pages 176?181.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Workshop
on Statistical Machine Translation, pages 128?135.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP, pages 848?856, Honolulu, Hawaii.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
HLT-NAACL, Montreal, Canada, June.
Cyril Goutte, David Kurokawa, and Pierre Isabelle.
2009. Improving SMT by learning the translation di-
rection. In EAMT Workshop on Statistical Multilin-
gual Analysis for Retrieval and Translation.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In HLT-NAACL, pages
867?875, Los Angeles, California, June.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 292?301, Jeju Island, Korea, July.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In EMNLP, pages 1352?1362.
Matthias Huck, Stephan Peitz, Markus Freitag, and Her-
mann Ney. 2012. Discriminative reordering exten-
sions for hierarchical phrase-based machine transla-
tion. In Proceedings of the 16th Annual Conference
of the European Association for Machine Translation
(EAMT), pages 313?320, Trento, Italy, May.
Philipp Koehn, Franz Joesef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL, pages 127?133.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings to the International Workshop on Spoken
Language Translation (IWSLT).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180, Prague, Czech Republic, June.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In ACL,
pages 595?603, Columbus, Ohio, June.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the Joint
Conference of the ACL and the AFNLP, pages 1030?
1038, Singapore, August.
Robert C. Moore and Chris Quirk. 2007. Faster beam-
search decoding for phrasal statistical machine trans-
lation. In MT Summit XI, September.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,
and Thai Phuong Nguyen. 2009. Improving a lexi-
calized hierarchical reordering model using maximum
entropy. In MT Summit XII, Ottawa, Canada, August.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1), March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4), December.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In EACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL, pages 311?318.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
ACL, pages 11?21, Jeju Island, Korea, July.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In HLT-NAACL,
pages 101?104, Boston, USA, May.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In COLING-ACL, pages
521?528, Sydney, Australia, July.
Sirvan Yahyaei and Christof Monz. 2010. Dynamic dis-
tortion in a discriminative reordering model for statis-
tical machine translation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 353?360.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In HLT-
NAACL, pages 257?264, Boston, USA, May.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
31
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 47?53,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Reversing Morphological Tokenization in English-to-Arabic SMT
Mohammad Salameh? Colin Cherry? Grzegorz Kondrak?
?Department of Computing Science ?National Research Council Canada
University of Alberta 1200 Montreal Road
Edmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6, Canada
{msalameh,gkondrak}@ualberta.ca Colin.Cherry@nrc-cnrc.gc.ca
Abstract
Morphological tokenization has been used
in machine translation for morphologically
complex languages to reduce lexical sparsity.
Unfortunately, when translating into a mor-
phologically complex language, recombining
segmented tokens to generate original word
forms is not a trivial task, due to morpho-
logical, phonological and orthographic adjust-
ments that occur during tokenization. We re-
view a number of detokenization schemes for
Arabic, such as rule-based and table-based ap-
proaches and show their limitations. We then
propose a novel detokenization scheme that
uses a character-level discriminative string
transducer to predict the original form of a
segmented word. In a comparison to a state-
of-the-art approach, we demonstrate slightly
better detokenization error rates, without the
need for any hand-crafted rules. We also
demonstrate the effectiveness of our approach
in an English-to-Arabic translation task.
1 Introduction
Statistical machine translation (SMT) relies on to-
kenization to split sentences into meaningful units
for easy processing. For morphologically complex
languages, such as Arabic or Turkish, this may in-
volve splitting words into morphemes. Through-
out this paper, we adopt the definition of tokeniza-
tion proposed by Habash (2010), which incorpo-
rates both morphological segmentation as well as
orthographic character transformations. To use an
English example, the word tries would be morpho-
logically tokenized as ?try + s?, which involves
orthographic changes at morpheme boundaries to
match the lexical form of each token. When trans-
lating into a tokenized language, the tokenization
must be reversed to make the generated text read-
able and evaluable. Detokenization is the process
of converting tokenized words into their original or-
thographically and morphologically correct surface
form. This includes concatenating tokens into com-
plete words and reversing any character transforma-
tions that may have taken place.
For languages like Arabic, tokenization can facil-
itate SMT by reducing lexical sparsity. Figure 1
shows how the morphological tokenization of the
Arabic word ???	J?J
?? ?and he will prevent them?
simplifies the correspondence between Arabic and
English tokens, which in turn can improve the qual-
ity of word alignment, rule extraction and decoding.
When translating from Arabic into English, the to-
kenization is a form of preprocessing, and the out-
put translation is readable, space-separated English.
However, when translating from English to Arabic,
the output will be in a tokenized form, which cannot
be compared to the original reference without detok-
enization. Simply concatenating the tokenized mor-
phemes cannot fully reverse this process, because of
character transformations that occurred during tok-
enization.
The techniques that have been proposed for the
detokenization task fall into three categories (Badr
et al, 2008). The simplest detokenization approach
concatenates morphemes based on token markers
without any adjustment. Table-based detokenization
maps tokenized words into their surface form with a
look-up table built by observing the tokenizer?s in-
47
Figure 1: Alignment between tokenized form of
?wsymn?hm? ???	J?J
?? and its English translation.
put and output on large amounts of text. Rule-based
detokenization relies on hand-built rules or regular
expressions to convert the segmented form into the
original surface form. Other techniques use combi-
nations of these approaches. Each approach has its
limitations: rule-based approaches are language spe-
cific and brittle, while table-based approaches fail to
deal with sequences outside of their tables.
We present a new detokenization approach that
applies a discriminative sequence model to predict
the original form of the tokenized word. Like
table-based approaches, our sequence model re-
quires large amounts of tokenizer input-output pairs;
but instead of building a table, we use these pairs
as training data. By using features that consider
large windows of within-word input context, we are
able to intelligently transition between rule-like and
table-like behavior.
Our experimental results on Arabic text demon-
strate an improvement in terms of sentence error
rate1 of 11.9 points over a rule-based approach, and
1.1 points over a table-based approach that backs
off to rules. More importantly, we achieve a slight
improvement over the state-of-the-art approach of
El Kholy and Habash (2012), which combines rules
and tables, using a 5-gram language model to dis-
ambiguate conflicting table entries. In addition, our
detokenization method results in a small BLEU im-
provement over a rule-based approach when applied
to English-to-Arabic SMT.
1Sentence error rate is the percentage of sentences contain-
ing at least one error after detokenization.
2 Arabic Morphology
Compared to English, Arabic has rich and complex
morphology. Arabic base words inflect to eight fea-
tures. Verbs inflect for aspect, mood, person and
voice. Nouns and adjectives inflect for case and
state. Verbs, nouns and adjectives inflect for both
gender and number. Furthermore, inflected base
words can attract various optional clitics. Clitical
prefixes include determiners, particle proclitics, con-
junctions and question particles in strict order. Clit-
ical suffixes include pronominal modifiers. As a re-
sult of clitic attachment, morpho-syntactic interac-
tions sometimes cause changes in spelling or pro-
nunciations.
Several tokenization schemes can be defined for
Arabic, depending on the clitical level that the to-
kenization is applied to. In this paper, we use
Penn Arabic Treebank (PATB) tokenization scheme,
which El Kholy and Habash (2012) report as pro-
ducing the best results for Arabic SMT. The PATB
scheme detaches all clitics except for the definite ar-
ticle Al ?@. Multiple prefix clitics are treated as one
token.
Some Arabic letters present further ambiguity in
text.2 For example, the different forms of Hamzated
Alif ?@ @? are usually written without the Hamza ?Z?.
Likewise, when the letter Ya ?Y? ?
 is present at theend of the word, it is sometimes written in the form
of ?Alif Maqsura? letter ??? ?. Also, short vow-
els in Arabic are represented using diacritics, which
are usually absent in written text. In order to deal
with these ambiguities in SMT, normalization is of-
ten performed as a preprocessing step, which usu-
ally involves converting different forms of Alif and
Ya to a single form. This decreases Arabic?s lexical
sparsity and improves SMT performance.
3 Related Work
Sadat and Habash (2006) address the issue of lex-
ical sparsity by presenting different preprocessing
schemes for Arabic-to-English SMT. The schemes
include simple tokenization, orthographic normal-
ization, and decliticization. The combination of
these schemes results in improved translation out-
2We use Habash-Soudi-Buckwalter transliteration scheme
(Habash, 2007) for all Arabic examples.
48
put. This is one of many studies on normalization
and tokenization for translation from Arabic, which
we will not attempt to review completely here.
Badr et al (2008) show that tokenizing Arabic
also has a positive influence on English-to-Arabic
SMT. They apply two tokenization schemes on
Arabic text, and introduce detokenization schemes
through a rule-based approach, a table-based ap-
proach, and a combination of both. The combina-
tion approach detokenizes words first using the ta-
ble, falling back on rules for sequences not found in
the table.
El Kholy and Habash (2012) extend Badr?s work
by presenting a larger number of tokenization and
detokenization schemes, and comparing their effects
on SMT. They introduce an additional detokeniza-
tion schemes based on the SRILM disambig util-
ity (Stolcke, 2002), which utilizes a 5-gram untok-
enized language model to decide among different al-
ternatives found in the table. They test their schemes
on naturally occurring Arabic text and SMT output.
Their newly introduced detokenization scheme out-
performs the rule-based and table-based approaches
introduced by Badr et al (2008), establishing the
current state-of-the-art.
3.1 Detokenization Schemes in Detail
Rule-based detokenization involves manually defin-
ing a set of transformation rules to convert a se-
quence of segmented tokens into their surface form.
For example, the noun ?llry?ys? ?
KQ?? ?to the pres-
ident? is tokenized as ?l+ Alry?ys? ( l+ ?to? Alry?ys
?the president?) in the PATB tokenization scheme.
Note that the definite article ?Al? ?@ is kept attached
to the noun. In this case, detokenization requires
a character-level transformation after concatenation,
which we can generalize using the rule:
l+Al ? ll.
Table 1 shows the rules provided by El Kholy and
Habash (2012), which we employ throughout this
paper.
There are two principal problems with the rule-
based approach. First, rules fail to account for un-
usual cases. For example, the above rule mishandles
cases where ?Al? ?@ is a basic part of the stem and
not the definite article ?the?. Thus, ?l+ Al?Ab? (l+
?to? Al?Ab ?games?) is erroneously detokenized to
Rule Input Output
l+Al+l? ? ll l+ Alry?ys llry?ys
?+(pron) ? t(pron) Abn?+hA AbnthA
y+(pron) ? A(pron) Alqy+h AlqAh
?+(pron) ? y? AntmA?+hm AntmAy?hm
y+y ? y ?yny+y ?yny
n+n ? n mn+nA mnA
mn+m ? mm mn+mA mmA
?n+m ? ?m ?n+mA ?mA
An+lA ? AlA An+lA AlA
Table 1: Detokenization rules of El Kholy and Habash
(2012), with examples. pron stands for pronominal clitic.
llEAb H. A??? instead of the correct form is ?lAl?Ab?
H. A??B. Second, rules may fail to handle sequences
produced by tokenization errors. For example, the
word ?bslT?? ????. ?with power? can be erro-
neously tokenized as ?b+slT+h?, while the correct
tokenizations is ?b+slT??. The erroneous tokeniza-
tion will be incorrectly detokenized as ?bslTh?.
The table-based approach memorizes mappings
between words and their tokenized form. Such a
table is easily constructed by running the tokenizer
on a large amount of Arabic text, and observing the
input and output. The detokenization process con-
sults this table to retrieve surface forms of tokenized
words. In the case where a tokenized word has sev-
eral observed surface forms, the most frequent form
is selected. This approach fails when the sequence
of tokenized words is not in the table. In morpholog-
ically complex languages like Arabic, an inflected
base word can attrract many optional clitics, and ta-
bles may not include all different forms and inflec-
tions of a word.
The SRILM-disambig scheme introduced by
El Kholy and Habash (2012) extends the table-based
approach to use an untokenized Arabic language
model to disambiguate among the different alter-
natives. Hence, this scheme can make context-
dependent detokenization decisions, rather than al-
ways producing the most frequent surface form.
Both the SRILM-disambig scheme and the table-
based scheme have the option to fall back on either
rules or simple concatenation for sequences missing
from the table.
49
4 Detokenization as String Transduction
We propose to approach detokenization as a string
transduction task. We train a discriminative trans-
ducer on a set of tokenized-detokenized word pairs.
The set of pairs is initially aligned on the charac-
ter level, and the alignment pairs become the opera-
tions that are applied during transduction. For deto-
kenization, most operations simply copy over char-
acters, but more complex rules such as l+ Al ? ll
are learned from the training data as well.
The tool that we use to perform the transduction is
DIRECTL+, a discriminative, character-level string
transducer, which was originally designed for letter-
to-phoneme conversion (Jiampojamarn et al, 2008).
To align the characters in each training example.
DIRECTL+ uses an EM-based M2M-ALIGNER (Ji-
ampojamarn et al, 2007). After alignment is com-
plete, MIRA training repeatedly decodes the train-
ing set to tune the features that determine when each
operation should be applied. The features include
both n-gram source context and HMM-style target
transitions. DIRECTL+ employs a fully discrimina-
tive decoder to learn character transformations and
when they should be applied. The decoder resem-
bles a monotone phrase-based SMT decoder, but is
built to allow for hundreds of thousands of features.
The following example illustrates how string
transduction applies to detokenization. The seg-
mented and surface forms of bbrA?thm ?? D?@Q. K.
?with their skill? constitute a training instance:
b+_brA??_+hm ? bbrA?thm
The instance is aligned during the training phase as:
b+ _b r A ? ?_ + h m
| | | | | | | | |
b b r A ? t  h m
The underscore ?_? indicates a space, while ?? de-
notes an empty string. The following operations are
extracted from the alignment:
b+ ? b, _b ? b, r ? r, A ? A, E ? E, p_ ? t,
+ ? , h ? h, m ? m
During training, weights are assigned to features that
associate operations with context. In our running ex-
ample, the weight assigned to the b+ ? b operation
accounts for the operation itself, for the fact that the
operation appears at the beginning of a word, and for
the fact that it is followed by an underscore; in fact,
we employ a context window of 5 characters to the
left or right of the source substring ?b+?, creating a
feature for each n-gram within that window.
Modeling the tokenization problem as string
transduction has several advantages. The approach
is completely language-independent. The context-
sensitive rules are learned automatically from ex-
amples, without human intervention. The rules
and features can be represented in a more com-
pact way than the full mapping table required by
table-based approaches, while still elegantly han-
dling words that were not seen during training.
Also, since the training data is generalized more
efficiently than in simple memorization of com-
plete tokenized-detokenized pairs, less training data
should be needed to achieve good accuracy.
5 Experiments
This section presents two experiments that evaluate
the effect of the detokenization schemes on both nat-
urally occurring Arabic and SMT output.
5.1 Data
To build our data-driven detokenizers, we use the
Arabic part of 4 Arabic-English parallel datasets
from the Linguistic Data Consortium as train-
ing data. The data sets are: Arabic News
(LDC2004T17), eTIRR (LDC2004E72), English
translation of Arabic Treebank (LDC2005E46), and
Ummah (LDC2004T18). The training data has
107K sentences. The Arabic part of the training data
constitutes around 2.8 million words, 3.3 million to-
kens after tokenization, and 122K word types after
filtering punctuation marks, Latin words and num-
bers (refer to Table 2 for detailed counts).
For training the SMT system?s translation and re-
ordering models, we use the same 4 datasets from
LDC. We also use 200 Million words from LDC
Arabic Gigaword corpus (LDC2011T11) to gener-
ate a 5-gram language model using SRILM toolkit
(Stolcke, 2002).
We use NIST MT 2004 evaluation set for tun-
ing (1075 sentences), and NIST MT 2005 evalua-
tions set for testing (1056 sentences). Both MT04
and MT05 have multiple English references in or-
der to evaluate Arabic-to-English translation. As we
are translating into Arabic, we take the first English
50
Data set Before After
training set 122,720 61,943
MT04 8,201 2,542
MT05 7,719 2,429
Table 2: Type counts before and after tokenization.
translation to be our source in each case. We also
use the Arabic halves of MT04 and MT05 as devel-
opment and test sets for our experiments on natu-
rally occurring Arabic. The tokenized Arabic is our
input, with the original Arabic as our gold-standard
detokenization.
The Arabic text of the training, development, test-
ing set and language model are all tokenized using
MADA 3.2 (Habash et al, 2009) with the Penn Ara-
bic Treebank tokenization scheme. The English text
in the parallel corpus is lower-cased and tokenized
in the traditional sense to strip punctuation marks.
5.2 Experimental Setup
To train the detokenization systems, we generate a
table of mappings from tokenized forms to surface
forms based on the Arabic part of our 4 parallel
datasets, giving us complete coverage of the out-
put vocabulary of our SMT system. In the table-
based approaches, if a tokenized form is mapped to
more than one surface form, we use the most fre-
quent surface form. For out-of-table words, we fall
back on concatenation (in T) or rules (in T+R). For
SRILM-Disambig detokenization, we maintain am-
biguous table entries along with their frequencies,
and we introduce a 5-gram language model to dis-
ambiguate detokenization choices in context. Like
the table-based approaches, the Disambig approach
can back off to either simple concatenation (T+LM)
or rules (T+R+LM) for missing entries. The latter
is a re-implementation of the state-of-the-art system
presented by El Kholy and Habash (2012).
We train our discriminative string transducer us-
ing word types from the 4 LDC catalogs. We
use M2M-ALIGNER to generate a 2-to-1 charac-
ter alignments between tokenized forms and surface
forms. For the decoder, we set Markov order to one,
joint n-gram features to 5, n-gram size to 11, and
context size to 5. This means the decoder can uti-
lize contexts up to 11 characters long, allowing it to
Detokenization WER SER BLEU
Baseline 1.710 34.3 26.30
Rules (R) 0.590 14.0 28.32
Table (T) 0.192 4.9 28.54
Table + Rules (T+R) 0.122 3.2 28.55
Disambig (T+LM) 0.164 4.1 28.53
Disambig (T+R+LM) 0.094 2.4 28.54
DIRECTL+ 0.087 2.1 28.55
Table 3: Word and sentence error rate of detokenization
schemes on the Arabic reference text of NIST MT05.
BLEU score refers to English-Arabic SMT output.
effectively memorize many words. We found these
settings using grid search on the development set,
NIST MT04.
For the SMT experiment, we use GIZA++ for
the alignment between English and tokenized Ara-
bic, and perform the translation using Moses phrase-
based SMT system (Hoang et al, 2007), with a max-
imum phrase length of 5. We apply each detokeniza-
tion scheme on the SMT tokenized Arabic output
test set, and evaluate using the BLEU score (Pap-
ineni et al, 2002).
5.3 Results
Table 3 shows the performance of several detok-
enization schemes. For evaluation, we use the sen-
tence and word error rates on naturally occurring
Arabic text, and BLEU score on tokenized Arabic
output of the SMT system. The baseline scheme,
which is a simple concatenation of morphemes, in-
troduces errors in over a third of all sentences. The
table-based approach outperforms the rule-based ap-
proach, indicating that there are frequent excep-
tions to the rules in Table 1 that require memoriza-
tion. Their combination (T+R) fares better, lever-
aging the strengths of both approaches. The addi-
tion of SRILM-Disambig produces further improve-
ments as it uses a language model context to disam-
biguate the correct detokenized word form. Our sys-
tem outperforms SRILM-Disambig by a very slight
margin, indicating that the two systems are roughly
equal. This is interesting, as it is able to do so by
using only features derived from the tokenized word
itself; unlike SRILM-Disambig, it has no access to
the surrounding words to inform its decisions. In ad-
51
dition, it is able to achieve this level of performance
without any manually constructed rules.
Improvements in detokenization do contribute to
the BLEU score of our SMT system, but only to
a point. Table 3 shows three tiers of performance,
with no detokenization being the worst, the rules be-
ing better, and the various data-driven approaches
performing best. After WER dips below 0.2, further
improvements seem to no longer affect SMT quality.
Note that BLEU scores are much lower overall than
one would expect for the translation in the reverse
direction, because of the morphological complexity
of Arabic, and the use of one (as opposed to four)
references for evaluation.
5.4 Analysis
The sentence error rate of 2.1 represents only 21
errors that our approach makes. Among those 21,
11 errors are caused by changing p to h and vice
versa. This is due to writing p and h interchange-
ably. For example, ?AjmAly+h? was detokenized
as ?AjmAly?? ?J
?A?g. @ instead of ?AjmAlyh? ?J
?A?g. @.Another 4 errors are caused by the lack of dia-
critization, which affects the choice of the Hamza
form. For example,?bnAw?h? ? ?A 	JK. , ?bnAy?h? ?KA 	JK.
and ?bnA?h? ?ZA 	JK. (?its building?) are 3 different
forms of the same word where the choice of Hamza
Z is dependent on its diacritical mark or the mark
of the character that precedes it. Another 3 errors
are attributed to the case of the nominal which it in-
flects for. The case is affected by the context of the
noun which DIRECTL+ has no access to. For ex-
ample, ?mfkry+hm? (?thinkers/Dual-Accusative?)
was detokenized as ?mfkrAhm? ??@Q? 	?? (Dual-
Nominative) instead of ?mfkryhm? ??E
Q? 	??. The
last 3 errors are special cases of ?An +y? which
can be detokenized correctly as either ?Any? ?

	G @ or
?Anny? ?

	? 	K @.
The table-based detokenization scheme fails in
54 cases. Among these instances, 44 cases are not
in the mapping table, hence resolving back to sim-
ple concatenation ended with an error. Our trans-
duction approach succeeds in detokenizing 42 cases
out of the 54. The majority of these cases involves
changing p to h and vice versa, and changing l+Al
to ll. The only 2 instances where the tokenized
word is in the mapping table but DIRECTL+ incor-
rectly detokenizes it are due to hamza case and p
to h case described above. There are 4 instances
of the same word/case where both the table scheme
and DIRECTL+ fails due to error of tokenization
by MADA, where the proper name qwh ??? is er-
roneously tokenized as qw+p. This shows that DI-
RECTL+ handles the OOV words correctly.
The Disambig(T+R+LM) erroneously detok-
enizes 27 instances, where 21 out of them are cor-
rectly tokenized by DIRECTL+. Most of the er-
rors are due to the Hamza and p to h reasons. It
seems that even with a large size language model,
the SRILM utility needs a large mapping table to
perform well. Only 4 instances were erroneously
detokenized by both Disambig and DIRECTL+ due
to Hamza and the case of the nominal.
The analysis shows that using small size training
data, DIRECTL+ can achieve slightly better accu-
racy than SRILM scheme. The limitations of using
table and rules are handled with DIRECTL+ as it is
able to memorize more rules.
6 Conclusion and Future Work
In this paper, we addressed the detokenization prob-
lem for Arabic using DIRECTL+, a discriminative
training model for string transduction. Our system
performs the best among the available systems. It
manages to solve problems caused by limitations of
table-based and rule-based systems. This allows us
to match the performance of the SRILM-disambig
approach without using a language model or hand-
crafted rules. In the future, we plan to test our ap-
proach on other languages that have morphological
characteristics similar to Arabic.
References
Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Seg-
mentation for English-to-Arabic statistical machine
translation. In Proceedings of ACL, pages 153?156.
Ahmed El Kholy and Nizar Habash. 2012. Orthographic
and morphological processing for English-Arabic sta-
tistical machine translation. Machine Translation,
26(1-2):25?45, March.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization, dia-
critization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
52
the Second International Conference on Arabic Lan-
guage Resources and Tools.
Nizar Habash. 2007. Arabic morphological represen-
tations for machine translation. In Arabic Computa-
tional Morphology: Knowledge-based and Empirical
Methods.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Synthesis Lectures on Human
Language Technologies. Morgan & Claypool Publish-
ers.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and On-
drej Bojar. 2007. Moses: Open source toolkit for
statistical machine translation. In Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session, pages 177?180.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and HMMs to letter-to-phoneme conversion. In Pro-
ceedings of NAACL-HLT, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proceed-
ings of ACL-08: HLT, pages 905?913.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318.
Fatiha Sadat and Nizar Habash. 2006. Combination of
Arabic preprocessing schemes for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 1?8.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language Pro-
cessing, pages 901?904.
53
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 742?751,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Lexically-Triggered Hidden Markov Models
for Clinical Document Coding
Svetlana Kiritchenko Colin Cherry
Institute for Information Technology
National Research Council Canada
{Svetlana.Kiritchenko,Colin.Cherry}@nrc-cnrc.gc.ca
Abstract
The automatic coding of clinical documents
is an important task for today?s healthcare
providers. Though it can be viewed as
multi-label document classification, the cod-
ing problem has the interesting property that
most code assignments can be supported by
a single phrase found in the input docu-
ment. We propose a Lexically-Triggered Hid-
den Markov Model (LT-HMM) that leverages
these phrases to improve coding accuracy. The
LT-HMM works in two stages: first, a lexical
match is performed against a term dictionary
to collect a set of candidate codes for a docu-
ment. Next, a discriminative HMM selects the
best subset of codes to assign to the document
by tagging candidates as present or absent.
By confirming codes proposed by a dictio-
nary, the LT-HMM can share features across
codes, enabling strong performance even on
rare codes. In fact, we are able to recover
codes that do not occur in the training set at
all. Our approach achieves the best ever per-
formance on the 2007Medical NLP Challenge
test set, with an F-measure of 89.84.
1 Introduction
The clinical domain presents a number of interesting
challenges for natural language processing. Con-
ventionally, most clinical documentation, such as
doctor?s notes, discharge summaries and referrals,
are written in a free-text form. This narrative form
is flexible, allowing healthcare professionals to ex-
press any kind of concept or event, but it is not
particularly suited for large-scale analysis, search,
or decision support. Converting clinical narratives
into a structured form would support essential activi-
ties such as administrative reporting, quality control,
biosurveillance and biomedical research (Meystre
et al, 2008). One way of representing a docu-
ment is to code the patient?s conditions and the per-
formed procedures into a nomenclature of clinical
codes. The International Classification of Diseases,
9th and 10th revisions, Clinical Modification (ICD-
9-CM, ICD-10-CM) are the official administrative
coding schemes for healthcare organizations in sev-
eral countries, including the US and Canada. Typi-
cally, coding is performed by trained coding profes-
sionals, but this process can be both costly and error-
prone. Automated methods can speed-up the cod-
ing process, improve the accuracy and consistency
of internal documentation, and even result in higher
reimbursement for the healthcare organization (Ben-
son, 2006).
Traditionally, statistical document coding is
viewed as multi-class multi-label document classifi-
cation, where each clinical free-text document is la-
belled with one or several codes from a pre-defined,
possibly very large set of codes (Patrick et al, 2007;
Suominen et al, 2008). One classification model is
learned for each code, and then all models are ap-
plied in turn to a new document to determine which
codes should be assigned to the document. The
drawback of this approach is poor predictive perfor-
mance on low-frequency codes, which are ubiqui-
tous in the clinical domain.
This paper presents a novel approach to document
coding that simultaneously models code-specific as
well as general patterns in the data. This allows
742
us to predict any code label, even codes for which
no training data is available. Our approach, the
lexically-triggered HMM (LT-HMM), is based on
the fact that a code assignment is often indicated
by short lexical triggers in the text. Consequently,
a two-stage coding method is proposed. First, the
LT-HMM identifies candidate codes by matching
terms from a medical terminology dictionary. Then,
it confirms or rejects each of the candidates by ap-
plying a discriminative sequence model. In this ar-
chitecture, low-frequency codes can still be matched
and confirmed using general characteristics of their
trigger?s local context, leading to better prediction
performance on these codes.
2 Document Coding and Lexical Triggers
Document coding is a special case of multi-class
multi-label text classification. Given a fixed set of
possible codes, the ultimate goal is to assign a set of
codes to documents, based on their content. Further-
more, we observe that for each code assigned to a
document, there is generally at least one correspond-
ing trigger term in the text that accounts for the
code?s assignment. For example, if an ICD-9-CM
coding professional were to see ?allergic bronchitis?
somewhere in a clinical narrative, he or she would
immediately consider adding code 493.9 (Asthma,
unspecified) to the document?s code set. The pres-
ence of these trigger terms separates document cod-
ing from text classification tasks, such as topic or
genre classification, where evidence for a particular
label is built up throughout a document. However,
this does not make document coding a term recogni-
tion task, concerned only with the detection of trig-
gers. Codes are assigned to a document as a whole,
and code assignment decisions within a document
may interact. It is an interesting combination of sen-
tence and document-level processing.
Formally, we define the document coding task
as follows: given a set of documents X and a set
of available codes C, assign to each document xi
a subset of codes Ci ? C. We also assume ac-
cess to a (noisy) mechanism to detect candidate trig-
gers in a document. In particular, we will assume
that an (incomplete) dictionary D(c) exists for each
code c ? C, which lists specific code terms asso-
ciated with c.1 To continue our running example:
D(493.9) would include the term ?allergic bron-
chitis?. Each code can have several corresponding
terms while each term indicates the presence of ex-
actly one code. A candidate code c is proposed each
time a term from D(c) is found in a document.
2.1 From triggers to codes
The presence of a term from D(c) does not automat-
ically imply the assignment of code c to a document.
Even with extremely precise dictionaries, there are
three main reasons why a candidate code may not
appear in a document?s code subset.
1. The context of the trigger term might indicate
the irrelevancy of the code. In the clinical do-
main, such irrelevancy can be specified by a
negative or speculative statement (e.g., ?evalu-
ate for pneumonia?) or a family-related context
(e.g., ?family history of diabetes?). Only defi-
nite diagnosis of the patient should be coded.
2. There can be several closely related candidate
codes; yet only one, the best fitted code should
be assigned to the document. For example, the
triggers ?left-sided flank pain? (code 789.09)
and ?abdominal pain? (code 789.00) may both
appear in the same clinical report, but only the
most specific code, 789.09, should end up in
the document code set.
3. The domain can have code dependency rules.
For example, the ICD-9-CM coding rules state
that no symptom codes should be given to
a document if a definite diagnosis is present.
That is, if a document is coded with pneumo-
nia, it should not be coded with a fever or
cough. On the other hand, if the diagnosis is
uncertain, then codes for the symptoms should
be assigned.
This suggests a paradigm where a candidate code,
suggested by a detected trigger term, is assessed
in terms of both its local context (item 1) and the
presence of other candidate codes for the document
(items 2 and 3).
1Note that dictionary-based trigger detection could be re-
placed by tagging approaches similar to those used in named-
entity-recognition or information extraction.
743
2.2 ICD-9-CM Coding
As a specific application we have chosen the task
of assigning ICD-9-CM codes to free-form clinical
narratives. We use the dataset collected for the 2007
Medical NLP Challenge organized by the Compu-
tational Medicine Center in Cincinnati, Ohio, here-
after refereed to as ?CMC Challenge? (Pestian et al,
2007). For this challenge, 1954 radiology reports
on outpatient chest x-ray and renal procedures were
collected, disambiguated, and anonymized. The re-
ports were annotated with ICD-9-CM codes by three
coding companies, and the majority codes were se-
lected as a gold standard. In total, 45 distinct codes
were used.
For this task, our use of a dictionary to detect lex-
ical triggers is quite reasonable. The medical do-
main is rich with manually-created and carefully-
maintained knowledge resources. In particular, the
ICD-9-CM coding guidelines come with an index
file that contains hundreds of thousands of terms
mapped to corresponding codes. Another valuable
resource is Metathesaurus from the Unified Medical
Language System (UMLS) (Lindberg et al, 1993).
It has millions of terms related to medical problems,
procedures, treatments, organizations, etc. Often,
hospitals, clinics, and other healthcare organizations
maintain their own vocabularies to introduce con-
sistency in their internal and external documenta-
tion and to support reporting, reviewing, and meta-
analysis.
This task has some very challenging properties.
As mentioned above, the ICD-9-CM coding rules
create strong code dependencies: codes are assigned
to a document as a set and not individually. Fur-
thermore, the code distribution throughout the CMC
training documents has a very heavy tail; that is,
there are a few heavily-used codes and a large
number of codes that are used only occasionally.
An ideal approach will work well with both high-
frequency and low-frequency codes.
3 Related work
Automated clinical coding has received much atten-
tion in the medical informatics literature. Stanfill et
al. reviewed 113 studies on automated coding pub-
lished in the last 40 years (Stanfill et al, 2010). The
authors conclude that there exists a variety of tools
covering different purposes, healthcare specialties,
and clinical document types; however, these tools
are not generalizable and neither are their evaluation
results. One major obstacle that hinders the progress
in this domain is data privacy issues. To overcome
this obstacle, the CMC Challenge was organized in
2007. The purpose of the challenge was to provide
a common realistic dataset to stimulate the research
in the area and to assess the current level of perfor-
mance on the task. Forty-four teams participated in
the challenge. The top-performing system achieved
micro-averaged F1-score of 0.8908, and the mean
score was 0.7670.
Several teams, including the winner, built pure
symbolic (i.e., hand-crafted rule-based) systems
(e.g., (Goldstein et al, 2007)). This approach is fea-
sible for the small code set used in the challenge,
but it is questionable in real-life settings where thou-
sands of codes need to be considered. Later, the
winning team showed how their hand-crafted rules
can be built in a semi-automatic way: the initial set
of rules adopted from the official coding guidelines
were automatically extended with additional syn-
onyms and code dependency rules generated from
the training data (Farkas and Szarvas, 2008).
Statistical systems trained on only text-derived
features (such as n-grams) did not show good per-
formance due to a wide variety of medical language
and a relatively small training set (Goldstein et al,
2007). This led to the creation of hybrid systems:
symbolic and statistical classifiers used together in
an ensemble or cascade (Aronson et al, 2007; Cram-
mer et al, 2007) or a symbolic component provid-
ing features for a statistical component (Patrick et
al., 2007; Suominen et al, 2008). Strong competi-
tion systems had good answers for dealing with neg-
ative and speculative contexts, taking advantage of
the competition?s limited set of possible code com-
binations, and handling of low-frequency codes.
Our proposed approach is a combination system
as well. We combine a symbolic component that
matches lexical strings of a document against a med-
ical dictionary to determine possible codes (Lussier
et al, 2000; Kevers and Medori, 2010) and a sta-
tistical component that finalizes the assignment of
codes to the document. Our statistical component
is similar to that of Crammer et al (2007), in that
we train a single model for all codes with code-
744
specific and generic features. However, Crammer
et al (2007) did not employ our lexical trigger step
or our sequence-modeling formulation. In fact, they
considered all possible code subsets, which can be
infeasible in real-life settings.
4 Method
To address the task of document coding, our
lexically-triggered HMM operates using a two-stage
procedure:
1. Lexically match text to the dictionary to get a
set of candidate codes;
2. Using features derived from the candidates and
the document, select the best code subset.
In the first stage, dictionary terms are detected in the
document using exact string matching. All codes
corresponding to matches become candidate codes,
and no other codes can be proposed for this docu-
ment.
In the second stage, a single classifier is trained to
select the best code subset from the matched candi-
dates. By training a single classifier, we use all of
the training data to assign binary labels (present or
absent) to candidates. This is the key distinction of
our method from the traditional statistical approach
where a separate classifier is trained for each code.
The LT-HMM allows features learned from a doc-
ument coded with ci to transfer at test time to pre-
dict code cj , provided their respective triggers ap-
pear in similar contexts. Training one common clas-
sifier improves our chances to reliably predict codes
that have few training instances, and even codes that
do not appear at all in the training data.
4.1 Trigger Detection
We have manually assembled a dictionary of terms
for each of the 45 codes used in the CMC chal-
lenge.2 The dictionaries were built by collecting rel-
evant medical terminology from UMLS, the ICD-9-
CM coding guidelines, and the CMC training data.
The test data was not consulted during dictionary
construction. The dictionaries contain 440 terms,
with 9.78 terms per code on average. Given these
dictionaries, the exact-matching of terms to input
2Online at https://sites.google.com/site/
colinacherry/ICD9CM ACL11.txt
documents is straightforward. In our experiments,
this process finds on average 1.83 distinct candidate
codes per document.
The quality of the dictionary significantly affects
the prediction performance of the proposed two-
stage approach. Especially important is the cover-
age of the dictionary. If a trigger term is missing
from the dictionary and, as the result, the code is not
selected as a candidate code, it will not be recov-
ered in the following stage, resulting in a false neg-
ative. Preliminary experiments show that our dictio-
nary recovers 94.42% of the codes in the training set
and 93.20% in the test set. These numbers provide
an upper bound on recall for the overall approach.
4.2 Sequence Construction
After trigger detection, we view the input document
as a sequence of candidate codes, each correspond-
ing to a detected trigger (see Figure 1). By tagging
these candidates in sequence, we can label each can-
didate code as present or absent and use previous
tagging decisions to model code interactions. The
final code subset is constructed by collecting all can-
didate codes tagged as present.
Our training data consists of [document, code set]
pairs, augmented with the trigger terms detected
through dictionary matching. We transform this into
a sequence to be tagged using the following steps:
Ordering: The candidate code sequence is pre-
sented in reverse chronological order, according to
when their corresponding trigger terms appear in the
document. That is, the last candidate to be detected
by the dictionary will be the first code to appear in
our candidate sequence. Reverse order was chosen
because clinical documents often close with a final
(and informative) diagnosis.
Merging: Each detected trigger corresponds to
exactly one code; however, several triggers may be
detected for the same code throughout a document.
If a code has several triggers, we keep only the last
occurrence. When possible, we collect relevant fea-
tures (such as negation information) of all occur-
rences and associate them with this last occurrence.
Labelling: Each candidate code is assigned a bi-
nary label (present or absent) based on whether it
appears in the gold-standard code set. Note that this
745
Cough,	 ?fever	 ?in	 ?9-??year-??
old	 ?male.	 ?IMPRESSION:	 ?
1.	 ?Right	 ?middle	 ?lobe	 ?
pneumonia.	 ?2.	 ?Minimal	 ?
pleural	 ?thickening	 ?on	 ?
the	 ?right	 ?may	 ?represent	 ?
small	 ?pleural	 ?effusion.	 ?	 ?
486	 ?
pneumonia	 ?
context=pos	 ?
sem=disease	 ?
	 ?
N	 ?Y	 ? N	 ?N	 ?
511.9	 ?
pleural	 ?effusion	 ?
context=neg	 ?
sem=disease	 ?
	 ?
780.6	 ?
fever	 ?
context=pos	 ?
sem=symptom	 ?
	 ?
786.2	 ?
cough	 ?
context=pos	 ?
sem=symptom	 ?
	 ?
Gold	 ?code	 ?set:	 ?{486}	 ?
Figure 1: An example document and its corresponding gold-standard tag sequence. The top binary layer is the correct
output tag sequence, which confirms or rejects the presence of candidate codes. The bottom layer shows the candidate
code sequence derived from the text, with corresponding trigger phrases and some prominent features.
process can not introduce gold-standard codes that
were not proposed by the dictionary.
The final output of these steps is depicted in Fig-
ure 1. To the left, we have an input text with un-
derlined trigger phrases, as detected by our dictio-
nary. This implies an input sequence (bottom right),
which consists of detected codes and their corre-
sponding trigger phrases. The gold-standard code
set for the document is used to infer a gold-standard
label sequence for these codes (top right). At test
time, the goal of the classifier is to correctly predict
the correct binary label sequence for new inputs. We
discuss the construction of the features used to make
this prediction in section 4.3.
4.3 Model
We model this sequence data using a discriminative
SVM-HMM (Taskar et al, 2003; Altun et al, 2003).
This allows us to use rich, over-lapping features of
the input while also modeling interactions between
labels. A discriminative HMM has two major cate-
gories of features: emission features, which charac-
terize a candidate?s tag in terms of the input docu-
ment x, and transition features, which characterize
a tag in terms of the tags that have come before it.
We describe these two feature categories and then
our training mechanism. All feature engineering dis-
cussed below was carried out using 10-fold cross-
validation on the training set.
Transition Features
The transition features are modeled as simple in-
dicators over n-grams of present codes, for values of
n up to 10, the largest number of codes proposed by
our dictionary in the training set.3 This allows the
system to learn sequences of codes that are (and are
not) likely to occur in the gold-standard data.
We found it useful to pad our n-grams with ?be-
ginning of document? tokens for sequences when
fewer than n codes have been labelled as present,
but found it harmful to include an end-of-document
tag once labelling is complete. We suspect that the
small training set for the challenge makes the system
prone to over-fit when modeling code-set length.
Emission Features
The vast majority of our training signal comes
from emission features, which carefully model both
the trigger term?s local context and the document as
a whole. For each candidate code, three types of
features are generated: document features, ConText
features, and code-semantics features (Table 1).
Document: Document features include indicators
on all individual words, 2-grams, 3-grams, and 4-
grams found in the document. These n-gram fea-
tures have the candidate code appended to them,
making them similar to features traditionally used
in multiclass document categorization.
ConText: We take advantage of the ConText algo-
rithm?s output. ConText is publicly available soft-
ware that determines the presence of negated, hypo-
thetical, historical, and family-related context for a
given phrase in a clinical text (Harkema et al, 2009).
3We can easily afford such a long history because input se-
quences are generally short and the tagging is binary, resulting
in only a small number of possible histories for a document.
746
Features gen. spec.
Document
n-gram x
ConText
current match
context x x
only in context x x
more than once in context x x
other matches
present x x
present in context = pos x x
code present in context x x
Code Semantics
current match
sem type x
other matches
sem type, context = pos x x
Table 1: The emission features used in LT-HMM.
Typeset words represent variables replaced with spe-
cific values, i.e. context ? {pos,neg}, sem type ?
{symptom,disease}, code is one of 45 challenge codes,
n-gram is a document n-gram. Features can come in
generic and/or code-specific version.
The algorithm is based on regular expression match-
ing of the context to a precompiled list of context
indicators. Regardless of its simplicity, the algo-
rithm has shown very good performance on a vari-
ety of clinical document types. We run ConText for
each trigger term located in the text and produce two
types of features: features related to the candidate
code in question and features related to other candi-
date codes of the document. Negated, hypothetical,
and family-related contexts are clustered into a sin-
gle negative context for the term. Absence of the
negative context implies the positive context.
We used the following ConText derived indicator
features: for the current candidate code, if there is at
least one trigger term found in a positive (negative)
context, if all trigger terms for this code are found
in a positive (negative) context, if there are more
than one trigger terms for the code found in a posi-
tive (negative) context; for other candidate codes of
the document, if there is at least one other candidate
code, if there is another candidate code with at least
one trigger term found in a positive context, if there
is a trigger term for candidate code ci found in a pos-
itive (negative) context.
Code Semantics: We include features that indi-
cate if the code itself corresponds to a disease or a
symptom. This assignment was determined based
on the UMLS semantic type of the code. Like the
ConText features, code features come in two types:
those regarding the candidate code in question and
those regarding other candidate codes from the same
document.
Generic versus Specific: Most of our features
come in two versions: generic and code-specific.
Generic features are concerned with classifying any
candidate as present or absent based on characteris-
tics of its trigger or semantics. Code-specific fea-
tures append the candidate code to the feature. For
example, the feature context=pos represents that
the current candidate has a trigger term in a positive
context, while context=pos:486 adds the infor-
mation that the code in question is 486. Note that
n-grams features are only code-specific, as they are
not connected to any specific trigger term.
To an extent, code-specific features allow us
to replicate the traditional classification approach,
which focuses on one code at a time. Using these
features, the classifier is free to build complex sub-
models for a particular code, provided that this code
has enough training examples. Generic versions of
the features, on the other hand, make it possible to
learn common rules applicable to all codes, includ-
ing low-frequency ones. In this way, even in the ex-
treme case of having zero training examples for a
particular code, the model can still potentially assign
the code to new documents, provided it is detected
by our dictionary. This is impossible in a traditional
document-classification setting.
Training
We train our SVM-HMM with the objective of
separating the correct tag sequence from all others
by a fixed margin of 1, using a primal stochastic
gradient optimization algorithm that follows Shalev-
Shwartz et al (2007). Let S be a set of training
points (x, y), where x is the input and y is the cor-
responding gold-standard tag sequence. Let ?(x, y)
be a function that transforms complete input-output
pairs into feature vectors. We also use ?(x, y?, y)
as shorthand for the difference in features between
747
begin
Input: S, ?, n
Initialize: Set w0 to the 0 vector
for t = 1, 2 . . . , n|S|
Choose (x, y) ? S at random
Set the learning rate: ?t = 1?t
Search:
y? = argmaxy?? [?(y, y
??) + wt ? ?(x, y??)]
Update:
wt+1 = wt + ?t
(
?(x, y, y?) ? ?wt
)
Adjust:
wt+1 = wt+1 ? min
[
1, 1/
?
?
?wt+1?
]
end
Output: wn|S|+1
end
Figure 2: Training an SVM-HMM
two outputs: ?(x, y?, y) = ?(x, y?) ? ?(x, y). With
this notation in place, the SVM-HMM minimizes
the regularized hinge-loss:
min
w
?
2
w2 +
1
|S|
?
(x,y)?S
`(w; (x, y)) (1)
where
`(w; (x, y)) = max
y?
[
?(y, y?) + w ? ?(x, y?, y)
]
(2)
and where ?(y, y?) = 0 when y = y? and 1 oth-
erwise.4 Intuitively, the objective attempts to find
a small weight vector w that separates all incorrect
tag sequences y? from the correct tag sequence y by
a margin of 1. ? controls the trade-off between reg-
ularization and training hinge-loss.
The stochastic gradient descent algorithm used
to optimize this objective is shown in Figure 2. It
bears many similarities to perceptron HMM train-
ing (Collins, 2002), with theoretically-motivated al-
terations, such as selecting training points at ran-
dom5 and the explicit inclusion of a learning rate ?
4We did not experiment with structured versions of ? that
account for the number of incorrect tags in the label sequence
y?, as a fixed margin was already working very well. We intend
to explore structured costs in future work.
5Like many implementations, we make n passes through S,
shuffling S before each pass, rather than sampling from S with
replacement n|S| times.
training test
# of documents 978 976
# of distinct codes 45 45
# of distinct code subsets 94 94
# of codes with < 10 ex. 24 24
avg # of codes per document 1.25 1.23
Table 2: The training and test set characteristics.
and a regularization term ?. The search step can be
carried out with a two-best version of the Viterbi al-
gorithm; if the one-best answer y?1 matches the gold-
standard y, that is ?(y, y?1) = 0, then y
?
2 is checked
to see if its loss is higher.
We tune two hyper-parameters using 10-fold
cross-validation: the regularization parameter ? and
a number of passes n through the training data. Us-
ing F1 as measured by 10-fold cross-validation on
the training set, we found values of ? = 0.1 with
n = 5 to prove optimal. Training time is less than
one minute on modern hardware.
5 Experiments
5.1 Data
For testing purposes, we use the CMC Challenge
dataset. The data consists of 978 training and 976
test medical records labelled with one or more ICD-
9-CM codes from a set of 45 codes. The data statis-
tics are presented in Table 2. The training and test
sets have similar, very imbalanced distributions of
codes. In particular, all codes in the test set have at
least one training example. Moreover, for any code
subset assigned to a test document there is at least
one training document labelled with the same code
subset. Notably, more than half of the codes have
less than 10 instances in both training and test sets.
Following the challenge?s protocol, we use micro-
averaged F1-measure for evaluation.
5.2 Baseline
As the first baseline for comparison, we built a
one-classifier-per-code statistical system. A docu-
ment?s code subset is implied by the set of classi-
fiers that assign it a positive label. The classifiers
use a feature set designed to mimic our LT-HMM
as closely as possible, including n-grams, dictionary
matches, ConText output, and symptom/disease se-
748
mantic types. Each classifier is trained as an SVM
with a linear kernel.
Unlike our approach, this baseline cannot share
features across codes, and it does not allow coding
decisions for a document to inform one another. It
also cannot propose codes that have not been seen in
the training data as it has no model for these codes.
However, one should note that it is a very strong
baseline. Like our proposed system, it is built with
many features derived from dictionary matches and
their contexts, and thus it shares many of our sys-
tem?s strengths. In fact, this baseline system outper-
forms all published statistical approaches tested on
the CMC data.
Our second baseline is a symbolic system, de-
signed to evaluate the quality of our rule-based com-
ponents when used alone. It is based on the same
hand-crafted dictionary, filtered according to the
ConText algorithm and four code dependency rules
from (Farkas and Szarvas, 2008). These rules ad-
dress the problem of overcoding: some symptom
codes should be omitted when a specific disease
code is present.6
This symbolic system has access to the same
hand-crafted resources as our LT-HMM and, there-
fore, has a good chance of predicting low-frequency
and unseen codes. However, it lacks the flexibility of
our statistical solution to accept or reject code candi-
dates based on the whole document text, which pre-
vents it from compensating for dictionary or Con-
Text errors. Similarly, the structure of the code de-
pendency rules may not provide the same flexibility
as our features that look at other detected triggers
and previous code assignments.
5.3 Coding Accuracy
We evaluate the proposed approach on both the
training set (using 10-fold cross-validation) and the
test set (Table 3). The experiments demonstrate the
superiority of the proposed LT-HMM approach over
the one-per-code statistical scheme as well as our
symbolic baseline. Furthermore, the new approach
shows the best results ever achieved on the dataset,
beating the top-performing system in the challenge,
a symbolic method.
6Note that we do not match the performance of the Farkas
and Szarvas system, likely due to our use of a different (and
simpler) dictionary.
Cross-fold Test
Symbolic baseline N/A 85.96
Statistical baseline 87.39 88.26
LT-HMM 89.39 89.84
CMC Best N/A 89.08
Table 3: Micro-averaged F1-scores for statistical and
symbolic baselines, the proposed LT-HMM approach,
and the best CMC hand-crafted rule-based system.
System Prec. Rec. F1
Full 90.91 88.80 89.84
-ConText 88.54 85.89 87.19
-Document 89.89 88.55 89.21
-Code Semantics 90.10 88.38 89.23
-Append code-specific 88.96 88.30 88.63
-Transition 90.79 88.38 89.57
-ConText & Transition 86.91 85.39 86.14
Table 4: Results on the CMC test data with each major
component removed.
5.4 Ablation
Our system employs a number of emission feature
templates. We measure the impact of each by re-
moving the template, re-training, and testing on the
challenge test data, as shown in Table 4. By far the
most important component of our system is the out-
put of the ConText algorithm.
We also tested a version of the system that does
not create a parallel code-specific feature set by ap-
pending the candidate code to emission features.
This system tags code-candidates without any code-
specific components, but it still does very well, out-
performing the baselines.
Removing the sequence-based transition features
from our system has only a small impact on accu-
racy. This is because several of our emission fea-
tures look at features of other candidate codes. This
provides a strong approximation to the actual tag-
ging decisions for these candidates. If we remove
the ConText features, the HMM?s transition features
become more important (compare line 2 of Table 4
to line 7).
5.5 Low-frequency codes
As one can see from Table 2, more than half of the
available codes appear fewer than 10 times in the
749
System Prec. Rec. F1
Symbolic baseline 42.53 56.06 48.37
Statistical baseline 73.33 33.33 45.83
LT-HMM 70.00 53.03 60.34
Table 5: Results on the CMC test set, looking only at the
codes with fewer than 10 examples in the training set.
System Prec. Rec. F1
Symbolic baseline 60.00 80.00 68.57
All training data 72.92 74.47 73.68
One code held out 79.31 48.94 60.53
Table 6: Results on the CMC test set when all instances
of a low-frequency code are held-out during training.
training documents. This does not provide much
training data for a one-classifier-per-code approach,
which has been a major motivating factor in the de-
sign of our LT-HMM. In Table 5, we compare our
system to the baselines on the CMC test set, con-
sidering only these low-frequency codes. We show
a 15-point gain in F1 over the statistical baseline
on these hard cases, brought on by an substantial
increase in recall. Similarly, we improve over the
symbolic baseline, due to a much higher precision.
In this way, the LT-HMM captures the strengths of
both approaches.
Our system also has the ability to predict codes
that have not been seen during training, by labelling
a dictionary match for a code as present according to
its local context. We simulate this setting by drop-
ping training data. For each low-frequency code c,
we hold out all training documents that include c in
their gold-standard code set. We then train our sys-
tem on the reduced training set and measure its abil-
ity to detect c on the unseen test data. 11 of the 24
low-frequency codes have no dictionary matches in
our test data; we omit them from our analysis as we
are unable to predict them. The micro-averaged re-
sults for the remaining 13 low-frequency codes are
shown in Table 6, with the results from the symbolic
baseline and from our system trained on the com-
plete training data provided for comparison.
We were able to recover 49% of the test-time oc-
currences of codes withheld from training, while
maintaining our full system?s precision. Consider-
ing that traditional statistical strategies would lead
to recall dropping uniformly to 0, this is a vast im-
provement. However, the symbolic baseline recalls
80% of occurrences in aggregate, indicating that we
are not yet making optimal use of the dictionary for
cases when a code is missing from the training data.
By holding out only correct occurrences of a code
c, our system becomes biased against it: all trigger
terms for c that are found in the training data must
be labelled absent. Nonetheless, out of the 13 codes
with dictionary matches, there were 9 codes that we
were able to recall at a rate of 50% or more, and 5
codes that achieved 100% recall.
6 Conclusion
We have presented the lexically-triggered HMM, a
novel and effective approach for clinical document
coding. The LT-HMM takes advantage of lexical
triggers for clinical codes by operating in two stages:
first, a lexical match is performed against a trigger
term dictionary to collect a set of candidates codes
for a document; next, a discriminative HMM se-
lects the best subset of codes to assign to the docu-
ment. Using both generic and code-specific features,
the LT-HMM outperforms a traditional one-per-
code statistical classification method, with substan-
tial improvements on low-frequency codes. Also,
it achieves the best ever performance on a common
testbed, beating the top-performer of the 2007 CMC
Challenge, a hand-crafted rule-based system. Fi-
nally, we have demonstrated that the LT-HMM can
correctly predict codes never seen in the training set,
a vital characteristic missing from previous statisti-
cal methods.
In the future, we would like to augment our
dictionary-based matching component with entity-
recognition technology. It would be interesting to
model triggers as latent variables in the document
coding process, in a manner similar to how latent
subjective sentences have been used in document-
level sentiment analysis (Yessenalina et al, 2010).
This would allow us to employ a learned matching
component that is trained to compliment our classi-
fication component.
Acknowledgements
Many thanks to Berry de Bruijn, Joel Martin, and
the ACL-HLT reviewers for their helpful comments.
750
References
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hid-
den Markov support vector machines. In ICML.
A. R. Aronson, O. Bodenreider, D. Demner-Fushman,
K. W. Fung, V. K. Lee, J. G. Mork, A. Nvol, L. Peters,
and W. J. Rogers. 2007. From indexing the biomed-
ical literature to coding clinical text: Experience with
MTI and machine learning approaches. In BioNLP,
pages 105?112.
S. Benson. 2006. Computer assisted coding software
improves documentation, coding, compliance and rev-
enue. Perspectives in Health Information Manage-
ment, CAC Proceedings, Fall.
M. Collins. 2002. Discriminative training methods for
Hidden Markov Models: Theory and experiments with
perceptron algorithms. In EMNLP.
K. Crammer, M. Dredze, K. Ganchev, P. P. Talukdar, and
S. Carroll. 2007. Automatic code assignment to med-
ical text. In BioNLP, pages 129?136.
R. Farkas and G. Szarvas. 2008. Automatic construc-
tion of rule-based ICD-9-CM coding systems. BMC
Bioinformatics, 9(Suppl 3):S10.
I. Goldstein, A. Arzumtsyan, and Uzuner. 2007. Three
approaches to automatic assignment of ICD-9-CM
codes to radiology reports. In AMIA, pages 279?283.
H. Harkema, J. N. Dowling, T. Thornblade, and W. W.
Chapman. 2009. Context: An algorithm for determin-
ing negation, experiencer, and temporal status from
clinical reports. Journal of Biomedical Informatics,
42(5):839?851, October.
L. Kevers and J. Medori. 2010. Symbolic classifica-
tion methods for patient discharge summaries encod-
ing into ICD. In Proceedings of the 7th International
Conference on NLP (IceTAL), pages 197?208, Reyk-
javik, Iceland, August.
D. A. Lindberg, B. L. Humphreys, and A. T. McCray.
1993. The Unified Medical Language System. Meth-
ods of Information in Medicine, 32(4):281?291.
Y. A. Lussier, L. Shagina, and C. Friedman. 2000. Au-
tomating ICD-9-CM encoding using medical language
processing: A feasibility study. In AMIA, page 1072.
S. M. Meystre, G. K. Savova, K. C. Kipper-Schuler, and
J. F. Hurdle. 2008. Extracting information from tex-
tual documents in the electronic health record: a re-
view of recent research. Methods of Information in
Medicine, 47(Suppl 1):128?144.
J. Patrick, Y. Zhang, and Y. Wang. 2007. Developing
feature types for classifying clinical notes. In BioNLP,
pages 191?192.
J. P. Pestian, C. Brew, P. Matykiewicz, D. J. Hovermale,
N. Johnson, K. B. Cohen, and W. Duch. 2007. A
shared task involving multi-label classification of clin-
ical free text. In BioNLP, pages 97?104.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pega-
sos: Primal Estimated sub-GrAdient SOlver for SVM.
In ICML, Corvallis, OR.
M. H. Stanfill, M. Williams, S. H. Fenton, R. A. Jenders,
and W. R. Hersh. 2010. A systematic literature re-
view of automated clinical coding and classification
systems. JAMIA, 17:646?651.
H. Suominen, F. Ginter, S. Pyysalo, A. Airola,
T. Pahikkala, S. Salanter, and T. Salakoski. 2008.
Machine learning to automate the assignment of di-
agnosis codes to free-text radiology reports: a method
description. In Proceedings of the ICML Workshop
on Machine Learning for Health-Care Applications,
Helsinki, Finland.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
markov networks. In Neural Information Processing
Systems Conference (NIPS03), Vancouver, Canada,
December.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In EMNLP.
751
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 200?205,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Joint Training of Dependency Parsing Filters through
Latent Support Vector Machines
Colin Cherry
Institute for Information Technology
National Research Council Canada
colin.cherry@nrc-cnrc.gc.ca
Shane Bergsma
Center for Language and Speech Processing
Johns Hopkins University
sbergsma@jhu.edu
Abstract
Graph-based dependency parsing can be sped
up significantly if implausible arcs are elim-
inated from the search-space before parsing
begins. State-of-the-art methods for arc fil-
tering use separate classifiers to make point-
wise decisions about the tree; they label tokens
with roles such as root, leaf, or attaches-to-
the-left, and then filter arcs accordingly. Be-
cause these classifiers overlap substantially in
their filtering consequences, we propose to
train them jointly, so that each classifier can
focus on the gaps of the others. We inte-
grate the various pointwise decisions as latent
variables in a single arc-level SVM classifier.
This novel framework allows us to combine
nine pointwise filters, and adjust their sensi-
tivity using a shared threshold based on arc
length. Our system filters 32% more arcs than
the independently-trained classifiers, without
reducing filtering speed. This leads to faster
parsing with no reduction in accuracy.
1 Introduction
A dependency tree represents syntactic relationships
between words using directed arcs (Mel?c?uk, 1987).
Each token in the sentence is a node in the tree,
and each arc connects a head to its modifier. There
are two dominant approaches to dependency pars-
ing: graph-based and transition-based, where graph-
based parsing is understood to be slower, but often
more accurate (McDonald and Nivre, 2007).
In the graph-based setting, a complete search
finds the highest-scoring tree under a model that de-
composes over one or two arcs at a time. Much of
the time for parsing is spent scoring each poten-
tial arc in the complete dependency graph (John-
son, 2007), one for each ordered word-pair in the
sentence. Potential arcs are scored using rich linear
models that are discriminatively trained to maximize
parsing accuracy (McDonald et al, 2005). The vast
majority of these arcs are bad; in an n-word sen-
tence, only n of the n2 potential arcs are correct. If
many arcs can be filtered before parsing begins, then
the entire process can be sped up substantially.
Previously, we proposed a cascade of filters to
prune potential arcs (Bergsma and Cherry, 2010).
One stage of this cascade operates one token at a
time, labeling each token t according to various roles
in the tree:
? Not-a-head (NaH ): t is not the head of any arc
? Head-to-left (HtL{1/5/*}): t?s head is to its
left within 1, 5 or any number of words
? Head-to-right (HtR{1/5/*}): as head-to-left
? Root (Root): t is the root node, which elimi-
nates arcs according to projectivity
Similar to Roark and Hollingshead (2008), each role
has a corresponding binary classifier. These token-
role classifierswere shown to be more effective than
vine parsing (Eisner and Smith, 2005; Dreyer et
al., 2006), a competing filtering scheme that filters
arcs based on their length (leveraging the observa-
tion that most dependencies are short).
In this work, we propose a novel filtering frame-
work that integrates all the information used in
token-role classification and vine parsing, but of-
fers a number of advantages. In our previous work,
classifier decisions would often overlap: different
token-role classifiers would agree to filter the same
arc. Based on this observation, we propose a joint
training framework where only the most confident
200
HtR16	 ?
NaH3	 ? HtR*6	 ?
HtR56	 ?
Bob1	 ? ate2	 ? the3	 ? pizza4	 ? with5	 ? his6	 ? fork8	 ?	 ?
NN	 ? VBD	 ? DT	 ? NN	 ? IN	 ? POS	 ? NN	 ?
HtL16	 ?
salad7	 ?
NN	 ?
(T)	 ? (T)	 ?
(T)	 ?
(F)	 ?
(F)	 ?
Figure 1: The dotted arc can be filtered by labeling any of the
boxed roles as True; i.e., predicting that the head the3 is not the
head of any arc, or that the modifier his6 attaches elsewhere.
Role truth values, derived from the gold-standard tree (in grey),
are listed adjacent to the boxes, in parentheses.
classifier is given credit for eliminating an arc. The
identity of the responsible classifier is modeled as
a latent variable, which is filled in during training
using a latent SVM (LSVM) formulation. Our use
of an LSVM to assign credit during joint training
differs substantially from previous LSVM applica-
tions, which have induced latent linguistic structures
(Cherry and Quirk, 2008; Chang et al, 2010) or sen-
tence labels (Yessenalina et al, 2010).
In our framework, each classifier learns to fo-
cus on the cases where the other classifiers are less
confident. Furthermore, the integrated approach di-
rectly optimizes for arc-filtering accuracy (rather
than token-labeling fidelity). We trade-off filtering
precision/recall using two hyperparameters, while
the previous approach trained classifiers for eight
different tasks resulting in sixteen hyperparameters.
Ultimately, the biggest gains in filter quality are
achieved when we jointly train the token-role classi-
fiers together with a dynamic threshold that is based
on arc length and shared across all classifiers.
2 Joint Training of Token Roles
In our previous system, filtering is conducted by
training a separate SVM classifier for each of the
eight token-roles described in Section 1. Each clas-
sifier uses a training set with one example per tree-
bank token, where each token is assigned a binary
label derived from the gold-standard tree. Figure 1
depicts five of the eight token roles, along with their
truth values. The role labelers can be tuned for high
precision with label-specific cost parameters; these
are tuned separately for each classifier. At test time,
each of the eight classifiers assigns a binary label
to each of the n tokens in the sentence. Potential
arcs are then filtered from the complete dependency
graph according to these token labels. In Figure 1,
a positive assignment to any of the indicated token-
roles is sufficient to filter the dotted arc.
In the current work, we maintain almost the same
test-time framework, but we alter training substan-
tially, so that the various token-role classifiers are
trained jointly. To do so, we propose a classifica-
tion scheme focused on arcs.1 During training, each
arc is assigned a filtering event as a latent variable.
Events generalize the token-roles from our previous
system (e.g. NaH 3,HtR?6). Events are assigned bi-
nary labels during filtering; positive events are said
to be detected. In general, events can correspond
to any phenomenon, so long as the following holds:
For each arc a, we must be able to deterministically
construct the set Za of all events that would filter
a if detected.2 Figure 1 shows that Zthe3?his6 =
{NaH 3,HtR?6,HtR56,HtR16,HtL16}.
To detect events, we maintain the eight token-role
classifiers from the previous system, but they be-
come subclassifiers of our joint system. For no-
tational convenience, we pack them into a single
weight vector w?. Thus, the event z = NaH 3 is de-
tected only if w? ? ??(NaH 3) > 0, where ??(z) is z?s
feature vector. Given this notation, we can cast the
filtering decision for an arc a as a maximum. We
filter a only if:
f(Za) > 0 where f(Za) = max
z?Za
[
w? ? ??(z)
]
(1)
We have reformulated our problem, which previ-
ously involved a number of independent token clas-
sifiers, as a single arc classifier f()with an innermax
over latent events. Note the asymmetry inherent in
(1). To filter an arc,
[
w? ? ??(z) > 0
]
must hold for at
least one z ? Za; but to keep an arc,
[
w? ? ??(z) ? 0
]
must hold for all z ? Za. Also note that tokens
have completely disappeared from our formalism:
the classifier is framed only in terms of events and
arcs; token-roles are encapsulated inside events.
To provide a large-margin training objective for
our joint classifier, we adapt the latent SVM (Felzen-
1A joint filtering formalism for CFG parsing or SCFG trans-
lation would likewise focus on hyper-edges or spans.
2This same requirement is also needed by the previous,
independently-trained filters at test time, so that arcs can be fil-
tered according to the roles assigned to tokens.
201
szwalb et al, 2010; Yu and Joachims, 2009) to our
problem. Given a training set A of (a, y) pairs,
where a is an arc in context and y is the correct filter
label for a (1 to filter, 0 otherwise), LSVM training
selects w? to minimize:
1
2
||w?||2+
?
(a,y)?A
Cy max
[
0, 1 + f(Za|?y)? f(Za|y)
]
(2)
where Cy is a label-specific regularization parame-
ter, and the event set Z is now conditioned on the
label y: Za|1 = Za, and Za|0 = {Nonea}. Nonea
is a rejection event, which indicates that a is not
filtered. The rejection event slightly alters our de-
cision rule; rather than thresholding at 0, we now
filter a only if f(Za) > w? ? ??(Nonea). One can set
??(Nonea)? ? for all a to fix the threshold at 0.
Though not convex, (2) can be solved to a lo-
cal minimum with an EM-like alternating minimiza-
tion procedure (Felzenszwalb et al, 2010; Yu and
Joachims, 2009). The learner alternates between
picking the highest-scoring latent event z?a ? Za|y
for each example (a, y), and training a multiclass
SVM to solve an approximation to (2) where Za|y is
replaced with {z?a}. Intuitively, the first step assigns
the event z?a to a, making z?a responsible for a?s ob-
served label. The second step optimizes the model to
ensure that each z?a is detected, leading to the desired
arc-filtering decisions. As the process iterates, event
assignment becomes increasingly refined, leading to
a more accurate joint filter.
The resulting joint filter has only two hyper-
parameters: the label-specific cost parameters C1
and Co. These allow us to tune our system for high
precision by increasing the cost of misclassifying an
arc that should not be filtered (C1  Co).
Joint training also implicitly affects the relative
costs of subclassifier decisions. By minimizing an
arc-level hinge loss with latent events (which in turn
correspond to token-roles), we assign costs to token-
roles based on arc accuracy. Consequently, 1) A
token-level decision that affects multiple arcs im-
pacts multiple instances of hinge loss, and 2) No
extra credit (penalty) is given for multiple decisions
that (in)correctly filter the same arc. Therefore, an
NaH decision that filters thirty arcs is given more
weight than an HtL5 decision that filters only one
(Item 1), unless those thirty arcs are already filtered
NaH3	 ?=	 ?0.5	 ?
The1	 ? big2	 ? dog3	 ? chased4	 ? the5	 ? cat6	 ?
DT	 ? ADJ	 ? NN	 ? VBD	 ? DT	 ? NN	 ?
1.0	 ? 1.1	 ? 0.6	 ? 0.3	 ? 0.2	 ?
Figure 2: A hypothetical example of dynamic threshold-
ing, where a weak assertion that dog3 should not be a head`
w? ? ??(NaH 3) = 0.5
?
is sufficient to rule out two arcs. Each
arc?s threshold
`
w? ? ??(Nonea)
?
is shown next to its arrow.
by higher-scoring subclassifiers (Item 2).
3 Accounting for Arc Length
We can extend our system by expanding our event
set Z. By adding an arc-level event Vinea to each
Za, we can introduce a vine filter to prune long arcs.
Similarly, we have already introduced another arc-
level event, the rejection event Nonea. By assign-
ing features to Nonea, we learn a dynamic thresh-
old on all filters, which considers properties of the
arc before acting on any other event. We parameter-
ize both Vinea and Nonea with the same two fea-
tures, inspired by tag-specific vine parsing (Eisner
and Smith, 2005):
{
Bias : 1
HeadTag ModTag Dir(a) : Len(a)
}
where HeadTag ModTag Dir(a) concatenates the
part-of-speech tags of a?s head and modifier tokens
to its direction (left or right), and Len(a) gives the
unsigned distance between a?s head and modifier.
In the context of Vinea, these two features al-
low the system to learn tag-pair-specific limits on
arc length. In the context of Nonea, these features
protect short arcs and arcs that connect frequently-
linked tag-pairs, allowing our token-role filters to be
more aggressive on arcs that do not have these char-
acteristics. The dynamic threshold also alters our
interpretation of filtering events: where before they
were either active or inactive, events are now as-
signed scores, which are compared with the thresh-
old to make final filtering decisions (Figure 2).3
3Because tokens and arcs are scored independently and cou-
pled only through score comparison, the impact of Vinea and
Nonea on classification speed should be no greater than doing
vine and token-role filtering in sequence. In practice, it is no
slower than running token-role filtering on its own.
202
4 Experiments
We extract dependency structures from the Penn
Treebank using the head rules of Yamada and Mat-
sumoto (2003).4 We divide the Treebank into train
(sections 2?21), development (22) and test (23). We
part-of-speech tag our data using a perceptron tagger
similar to the one described by Collins (2002). The
training set is tagged with jack-knifing: the data is
split into 10 folds and each fold is tagged by a sys-
tem trained on the other 9 folds. Development and
test sets are tagged using the entire training set.
We train our joint filter using an in-house latent
SVM framework, which repeatedly calls a multi-
class exponentiated gradient SVM (Collins et al,
2008). LSVM training was stopped after 4 itera-
tions, as determined during development.5 For the
token-role classifiers, we re-implement the Bergsma
and Cherry (2010) feature set, initializing w? with
high-precision subclassifiers trained independently
for each token-role. Vine and None subclassifiers
are initialized with a zero vector. At test time, we
extract subclassifiers from the joint weight vector,
and use them as parameters in the filtering tools of
Bergsma and Cherry (2010).6
Parsing experiments are carried out using the
MST parser (McDonald et al, 2005),7 which we
have modified to filter arcs before carrying out fea-
ture extraction. It is trained using 5-best MIRA
(Crammer and Singer, 2003).
Following Bergsma and Cherry (2010), we mea-
sure intrinsic filter quality with reduction, the pro-
portion of total arcs removed, and coverage, the pro-
portion of true arcs retained. For parsing results, we
present dependency accuracy, the percentage of to-
kens that are assigned the correct head.
4.1 Impact of Joint Training
Our technical contribution consists of our proposed
joint training scheme for token-role filters, along
4As implemented at http://w3.msi.vxu.se/?nivre/
research/Penn2Malt.html
5The LSVM is well on its way to convergence: fewer than
3% of arcs have event assignments that are still in flux.
6http://code.google.com/p/arcfilter/. Since our
contribution is mainly in better filter training, we were able to
use the arcfilter (testing) code with only small changes. We have
added our new joint filter, along with the Joint P1 model to the
arcfilter package, labeled as ultra filters.
7http://sourceforge.net/projects/mstparser/
Indep. Joint
System Cov. Red. Cov. Red.
Token 99.73 60.5 99.71 59.0
+ Vine 99.62 68.6 99.69 63.3
+ None N/A 99.76 71.6
Table 1: Ablation analysis of intrinsic filter quality.
with two extensions: the addition of vine filters
(Vine) and a dynamic threshold (None). Using pa-
rameters determined to perform well during devel-
opment,8 we examine test-set performance as we in-
corporate each of these components. For the token-
role and vine subclassifiers, we compare against an
independently-trained ensemble of the same classi-
fiers.9 Note that None cannot be trained indepen-
dently, as its shared dynamic threshold considers arc
and token views of the data simultaneously. Results
are shown in Table 1.
Our complete system outperforms all variants in
terms of both coverage and reduction. However, one
can see that neither joint system is able to outper-
form its independently-trained counter-part without
the dynamic threshold provided by None. This is
because the desirable credit-assignment properties
of our joint training procedure are achieved through
duplication (Zadrozny et al, 2003). That is, the
LSVM knows that a specific event is important be-
cause it appears in event sets Za for many arcs from
the same sentence. WithoutNone, the filtering deci-
sions implied by each copy of an event are identical.
Because these replicated events are associated with
arcs that are presented to the LSVM as independent
examples, they appear to be not only important, but
also low-variance, and therefore easy. This leads to
overfitting. We had hoped that the benefits of joint
training would outweigh this drawback, but our re-
sults show that they do not. However, in addition to
its other desirable properties (protecting short arcs),
the dynamic threshold imposed byNone restores in-
dependence between arcs that share a common event
(Figure 2). This alleviates overfitting and enables
strong performance.
8C0=1e-2, C1=1e-5
9Each subclassifier is a token-level SVM trained with token-
role labels extracted from the training treebank. Using develop-
ment data, we search over regularization parameters so that each
classifier yields more than 99.93% arc-level coverage.
203
Filter Intrinsic MST-1 MST-2
Filter Cov. Red. Time Acc. Sent/sec* Acc. Sent/sec*
None 100.00 00.0 0s 91.28 16 92.05 10
B&C R+L 99.70 54.1 7s 91.24 29 92.00 17
Joint P1 99.76 71.6 7s 91.28 38 92.06 22
B&C R+L+Q 99.43 78.3 19s 91.23 35 91.98 22
Joint P2 99.56 77.9 7s 91.29 44 92.05 25
Table 2: Parsing with jointly-trained filters outperforms independently-trained filters (R+L), as well as a more complex
cascade (R+L+Q). *Accounts for total time spent parsing and applying filters, averaged over five runs.
4.2 Comparison to the state of the art
We directly compare our filters to those of Bergsma
and Cherry (2010) in terms of both intrinsic fil-
ter quality and impact on the MST parser. The
B&C system consists of three stages: rules (R), lin-
ear token-role filters (L) and quadratic arc filters
(Q). The Q stage uses rich arc-level features simi-
lar to those of the MST parser. We compare against
independently-trained token-role filters (R+L), as
well as the complete cascade (R+L+Q), using the
models provided online.10 Our comparison points,
Joint P1 and P2 were built by tuning our complete
joint system to roughly match the coverage values
of R+L and R+L+Q on development data.11 Results
are shown in Table 2.
Comparing Joint P1 to R+L, we can see that for
a fixed set of pointwise filters, joint training with
a dynamic threshold outperforms independent train-
ing substantially. We achieve a 32% improvement
in reduction with no impact on coverage and no in-
crease in filtering overhead (time).
Comparing Joint P2 to R+L+Q, we see that Joint
P2 achieves similar levels of reduction with far less
filtering overhead; our filters take only 7 seconds
to apply instead of 19. This increases the speed of
the (already fast) filtered MST-1 parser from 35 sen-
tences per second to 44, resulting in a total speed-
up of 2.75 with respect to the unfiltered parser. The
improvement is less impressive for MST-2, where
the overhead for filter application is a less substan-
tial fraction of parsing time; however, our training
framework also has other benefits with respect to
R+L+Q, including a single unified training algo-
10Results are not identical to those reported in our previous
paper, due to our use of a different part-of-speech tagger. Note
that parsing accuracies for the B&C systems have improved.
11P1: C0=1e-2, C1=1e-5, P2: C0=1e-2, C1=2e-5
rithm, fewer hyper-parameters and a smaller test-
time memory footprint. Finally, the jointly trained
filters have no impact on parsing accuracy, where
both B&C filters have a small negative effect.
The performance of Joint-P2+MST-2 is compa-
rable to the system of Huang and Sagae (2010),
who report a parsing speed of 25 sentences per
second and an accuracy of 92.1 on the same test
set, using a transition-based parser enhanced with
dynamic-programming state combination.12 Graph-
based and transition-based systems tend to make dif-
ferent types of errors (McDonald and Nivre, 2007).
Therefore, having fast, accurate parsers for both ap-
proaches presents an opportunity for large-scale, ro-
bust parser combination.
5 Conclusion
We have presented a novel use of latent SVM
technology to train a number of filters jointly,
with a shared dynamic threshold. By training a
family of dependency filters in this manner, each
subclassifier focuses on the examples where it is
most needed, with our dynamic threshold adjust-
ing filter sensitivity based on arc length. This al-
lows us to outperform a 3-stage filter cascade in
terms of speed-up, while also reducing the im-
pact of filtering on parsing accuracy. Our filter-
ing code and trained models are available online at
http://code.google.com/p/arcfilter. In
the future, we plan to apply our joint training tech-
nique to other rich filtering regimes (Zhang et al,
2010), and to other NLP problems that combine the
predictions of overlapping classifiers.
12The usual caveats for cross-machine, cross-implementation
speed comparisons apply.
204
References
Shane Bergsma and Colin Cherry. 2010. Fast and accu-
rate arc filtering for dependency parsing. In COLING.
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek
Srikumar. 2010. Discriminative learning over con-
strained latent representations. In HLT-NAACL.
Colin Cherry and Chris Quirk. 2008. Discriminative,
syntactic language modeling through latent SVMs. In
AMTA.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponentiated
gradient algorithms for conditional random fields and
max-margin markov networks. JMLR, 9:1775?1822.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In EMNLP.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. JMLR,
3:951?991.
Markus Dreyer, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In CoNLL.
Jason Eisner and Noah A. Smith. 2005. Parsing with soft
and hard constraints on dependency length. In IWPT.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object detec-
tion with discriminatively trained part based models.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 32(9).
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In ACL.
Mark Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In ACL.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In EMNLP-CoNLL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL.
Igor A. Mel?c?uk. 1987. Dependency syntax: theory and
practice. State University of New York Press.
Brian Roark and Kristy Hollingshead. 2008. Classifying
chart cells for quadratic complexity context-free infer-
ence. In COLING.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
IWPT.
Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010.
Multi-level structured models for document-level sen-
timent classification. In EMNLP.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
ICML.
Bianca Zadrozny, John Langford, and Naoki Abe. 2003.
Cost-sensitive learning by cost-proportionate example
weighting. In Third IEEE International Conference on
Data Mining.
Yue Zhang, Byung-Gyu Ahn, Stephen Clark, Curt Van
Wyk, James R. Curran, and Laura Rimell. 2010.
Chart pruning for fast lexicalised-grammar parsing. In
EMNLP.
205
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 100?110,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Lattice Desegmentation for Statistical Machine Translation
Mohammad Salameh
?
Colin Cherry
?
Grzegorz Kondrak
?
?
Department of Computing Science
?
National Research Council Canada
University of Alberta 1200 Montreal Road
Edmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6, Canada
{msalameh,gkondrak}@ualberta.ca Colin.Cherry@nrc-cnrc.gc.ca
Abstract
Morphological segmentation is an effec-
tive sparsity reduction strategy for statis-
tical machine translation (SMT) involv-
ing morphologically complex languages.
When translating into a segmented lan-
guage, an extra step is required to deseg-
ment the output; previous studies have de-
segmented the 1-best output from the de-
coder. In this paper, we expand our trans-
lation options by desegmenting n-best lists
or lattices. Our novel lattice desegmenta-
tion algorithm effectively combines both
segmented and desegmented views of the
target language for a large subspace of
possible translation outputs, which allows
for inclusion of features related to the de-
segmentation process, as well as an un-
segmented language model (LM). We in-
vestigate this technique in the context of
English-to-Arabic and English-to-Finnish
translation, showing significant improve-
ments in translation quality over deseg-
mentation of 1-best decoder outputs.
1 Introduction
Morphological segmentation is considered to be
indispensable when translating between English
and morphologically complex languages such as
Arabic. Morphological complexity leads to much
higher type to token ratios than English, which
can create sparsity problems during translation
model estimation. Morphological segmentation
addresses this issue by splitting surface forms into
meaningful morphemes, while also performing or-
thographic transformations to further reduce spar-
sity. For example, the Arabic noun ??Y?? lldwl
?to the countries? is segmented as l+ ?to? Aldwl
?the countries?. When translating from Arabic,
this segmentation process is performed as input
preprocessing and is otherwise transparent to the
translation system. However, when translating
into Arabic, the decoder produces segmented out-
put, which must be desegmented to produce read-
able text. For example, l+ Aldwl must be con-
verted to lldwl.
Desegmentation is typically performed as a
post-processing step that is independent from the
decoding process. While this division of labor is
useful, the pipeline approach may prevent the de-
segmenter from recovering from errors made by
the decoder. Despite the efforts of the decoder?s
various component models, the system may pro-
duce mismatching segments, such as s+ hzymp,
which pairs the future particle s+ ?will? with a
noun hzymp ?defeat?, instead of a verb. In this sce-
nario, there is no right desegmentation; the post-
processor has been dealt a losing hand.
In this work, we show that it is possible to
maintain the sparsity-reducing benefit of segmen-
tation while translating directly into unsegmented
text. We desegment a large set of possible de-
coder outputs by processing n-best lists or lat-
tices, which allows us to consider both the seg-
mented and desegmented output before locking in
the decoder?s decision. We demonstrate that sig-
nificant improvements in translation quality can be
achieved by training a linear model to re-rank this
transformed translation space.
2 Related Work
Translating into morphologically complex lan-
guages is a challenging and interesting task that
has received much recent attention. Most tech-
niques approach the problem by transforming the
target language in some manner before training the
translation model. They differ in what transforma-
tions are performed and at what stage they are re-
versed. The transformation might take the form of
a morphological analysis or a morphological seg-
mentation.
100
2.1 Morphological Analysis
Many languages have access to morphological an-
alyzers, which annotate surface forms with their
lemmas and morphological features. Bojar (2007)
incorporates such analyses into a factored model,
to either include a language model over target mor-
phological tags, or model the generation of mor-
phological features. Other approaches train an
SMT system to predict lemmas instead of surface
forms, and then inflect the SMT output as a post-
processing step (Minkov et al, 2007; Clifton and
Sarkar, 2011; Fraser et al, 2012; El Kholy and
Habash, 2012b). Alternatively, one can reparame-
terize existing phrase tables as exponential mod-
els, so that translation probabilities account for
source context and morphological features (Jeong
et al, 2010; Subotin, 2011). Of these approaches,
ours is most similar to the translate-then-inflect ap-
proach, except we translate and then desegment.
In particular, Toutanova et al (2008) inflect and
re-rank n-best lists in a similar manner to how we
desegment and re-rank n-best lists or lattices.
2.2 Morphological Segmentation
Instead of producing an abstract feature layer,
morphological segmentation transforms the tar-
get sentence by segmenting relevant morphemes,
which are then handled as regular tokens during
alignment and translation. This is done to reduce
sparsity and to improve correspondence with the
source language (usually English). Such a seg-
mentation can be produced as a byproduct of anal-
ysis (Oflazer and Durgar El-Kahlout, 2007; Badr
et al, 2008; El Kholy and Habash, 2012a), or may
be produced using an unsupervised morphological
segmenter such as Morfessor (Luong et al, 2010;
Clifton and Sarkar, 2011). Work on target lan-
guage morphological segmentation for SMT can
be divided into three subproblems: segmentation,
desegmentation and integration. Our work is con-
cerned primarily with the integration problem, but
we will discuss each subproblem in turn.
The usefulness of a target segmentation de-
pends on its correspondence to the source lan-
guage. If a morphological feature does not man-
ifest itself as a separate token in the source, then
it may be best to leave its corresponding segment
attached to the stem. A number of studies have
looked into what granularity of segmentation is
best suited for a particular language pair (Oflazer
and Durgar El-Kahlout, 2007; Badr et al, 2008;
Clifton and Sarkar, 2011; El Kholy and Habash,
2012a). Since our focus here is on integrating seg-
mentation into the decoding process, we simply
adopt the segmentation strategies recommended
by previous work: the Penn Arabic Treebank
scheme for English-Arabic (El Kholy and Habash,
2012a), and an unsupervised scheme for English-
Finnish (Clifton and Sarkar, 2011).
Desegmentation is the process of converting
segmented words into their original surface form.
For many segmentations, especially unsupervised
ones, this amounts to simple concatenation. How-
ever, more complex segmentations, such as the
Arabic tokenization provided by MADA (Habash
et al, 2009), require further orthographic adjust-
ments to reverse normalizations performed dur-
ing segmentation. Badr et al (2008) present
two Arabic desegmentation schemes: table-based
and rule-based. El Kholy and Habash (2012a)
provide an extensive study on the influence of
segmentation and desegmentation on English-to-
Arabic SMT. They introduce an additional deseg-
mentation technique that augments the table-based
approach with an unsegmented language model.
Salameh et al (2013) replace rule-based deseg-
mentation with a discriminatively-trained char-
acter transducer. In this work, we adopt the
Table+Rules approach of El Kholy and Habash
(2012a) for English-Arabic, while concatenation
is sufficient for English-Finnish.
Work on integration attempts to improve SMT
performance for morphologically complex target
languages by going beyond simple pre- and post-
processing. Oflazer and Durgar El-Kahlout (2007)
desegment 1000-best lists for English-to-Turkish
translation to enable scoring with an unsegmented
language model. Unlike our work, they replace
the segmented language model with the unseg-
mented one, allowing them to tune the linear
model parameters by hand. We use both seg-
mented and unsegmented language models, and
tune automatically to optimize BLEU.
Like us, Luong et al (2010) tune on un-
segmented references,
1
and translate with both
segmented and unsegmented language models
for English-to-Finnish translation. However,
they adopt a scheme of word-boundary-aware
1
Tuning on unsegmented references does not require sub-
stantial modifications to the standard SMT pipeline. For ex-
ample, Badr et al (2008) also tune on unsegmented refer-
ences by simply desegmenting SMT output before MERT
collects sufficient statistics for BLEU.
101
morpheme-level phrase extraction, meaning that
target phrases include only complete words,
though those words are segmented into mor-
phemes. This enables full decoder integration,
where we do n-best and lattice re-ranking. But
it also comes at a substantial cost: when target
phrases include only complete words, the system
can only generate word forms that were seen dur-
ing training. In this setting, the sparsity reduc-
tion from segmentation helps word alignment and
target language modeling, but it does not result
in a more expressive translation model. Further-
more, it becomes substantially more difficult to
have non-adjacent source tokens contribute mor-
phemes to a single target word. For example,
when translating ?with his blue car? into the Ara-
bic ZA

P?
	
Q? @ ?

KPAJ


?
.
bsyArth AlzrqA?, the target word
bsyArth is composed of three tokens: b+ ?with?,
syArp ?car? and +h ?his?. With word-boundary-
aware phrase extraction, a phrase pair containing
all of ?with his blue car? must have been seen in
the parallel data to translate the phrase correctly at
test time. With lattice desegmentation, we need
only to have seen AlzrqA? ?blue? and the three
morphological pieces of bsyArth for the decoder
and desegmenter to assemble the phrase.
3 Methods
Our goal in this work is to benefit from
the sparsity-reducing properties of morphological
segmentation while simultaneously allowing the
system to reason about the final surface forms of
the target language. We approach this problem by
augmenting an SMT system built over target seg-
ments with features that reflect the desegmented
target words. In this section, we describe our vari-
ous strategies for desegmenting the SMT system?s
output space, along with the features that we add
to take advantage of this desegmented view.
3.1 Baselines
The two obvious baseline approaches each decode
using one view of the target language. The un-
segmented approach translates without segment-
ing the target. This trivially allows for an unseg-
mented language model and never makes deseg-
mentation errors. However, it suffers from data
sparsity and poor token-to-token correspondence
with the source language.
The one-best desegmentation approach seg-
ments the target language at training time and
then desegments the one-best output in post-
processing. This resolves the sparsity issue, but
does not allow the decoder to take into account
features of the desegmented target. To the best of
our knowledge, we are the first group to go beyond
one-best desegmentation for English-to-Arabic
translation. In English-to-Finnish, although alter-
native integration strategies have seen some suc-
cess (Luong et al, 2010), the current state-of-
the-art performs one-best-desegmentation (Clifton
and Sarkar, 2011).
3.2 n-best Desegmentation
The one-best approach can be extended easily by
desegmenting n-best lists of segmented decoder
output. Doing so enables the inclusion of an
unsegmented target language model, and with a
small amount of bookkeeping, it also allows the
inclusion of features related to the operations per-
formed during desegmentation (see Section 3.4).
With new features reflecting the desegmented out-
put, we can re-tune our enhanced linear model on
a development set. Following previous work, we
will desegment 1000-best lists (Oflazer and Dur-
gar El-Kahlout, 2007).
Once n-best lists have been desegmented, we
can tune on unsegmented references as a side-
benefit. This could improve translation quality,
as it brings our training scenario closer to our test
scenario (test BLEU is always measured on unseg-
mented references). In particular, it could address
issues with translation length mismatch. Previous
work that has tuned on unsegmented references
has reported mixed results (Badr et al, 2008; Lu-
ong et al, 2010).
3.3 Lattice Desegmentation
An n-best list reflects a tiny portion of a decoder?s
search space, typically fixed at 1000 hypotheses.
Lattices
2
can represent an exponential number of
hypotheses in a compact structure. In this section,
we discuss how a lattice from a multi-stack phrase-
based decoder such as Moses (Koehn et al, 2007)
can be desegmented to enable word-level features.
Finite State Analogy
A phrase-based decoder produces its output from
left to right, with each operation appending
the translation of a source phrase to a grow-
ing target hypothesis. Translation continues un-
2
Or forests for hierarchical and syntactic decoders.
102
0 1b+ 2lEbp
5
+hm
4+hA
3
AlTfl
(a)	 ?
(b)	 ?
(c)	 ?
1
AlTfl:AlTfl
0b+:<epsilon>
2
lEbp:<epsilon>
<epsilon>:blEbp
+hA:blEbthA
+hm:blEbthm
0
5blEbthm
4blEbthA
2
blEbp
3AlTfl
Transduces	 ?
into	 ?
Figure 1: The finite state pipeline for a lattice translating the English fragment ?with the child?s game?.
The input morpheme lattice (a) is desegmented by composing it with the desegmenting transducer (b) to
produce the word lattice (c). The tokens in (a) are: b+ ?with?, lEbp ?game?, +hm ?their?, +hA ?her?,
and AlTfl ?the child?.
til each source word has been covered exactly
once (Koehn et al, 2003).
The search graph of a phrase-based decoder can
be interpreted as a lattice, which can be interpreted
as a finite state acceptor over target strings. In its
most natural form, such an acceptor emits target
phrases on each edge, but it can easily be trans-
formed into a form with one edge per token, as
shown in Figure 1a. This is sometimes referred to
as a word graph (Ueffing et al, 2002), although in
our case the segmented phrase table also produces
tokens that correspond to morphemes.
Our goal is to desegment the decoder?s output
lattice, and in doing so, gain access to a compact,
desegmented view of a large portion of the trans-
lation search space. This can be accomplished by
composing the lattice with a desegmenting trans-
ducer that consumes morphemes and outputs de-
segmented words. This transducer must be able
to consume every word in our lattice?s output vo-
cabulary. We define a word using the following
regular expression:
[prefix]* [stem] [suffix]* | [prefix]+ [suffix]+
(1)
where [prefix], [stem] and [suffix] are non-
overlapping sets of morphemes, whose members
are easily determined using the segmenter?s seg-
ment boundary markers.
3
The second disjunct of
Equation 1 covers words that have no clear stem,
such as the Arabic ?? lh ?for him?, segmented as l+
?for? +h ?him?. Equation 1 may need to be modi-
fied for other languages or segmentation schemes,
but our techniques generalize to any definition that
can be written as a regular expression.
A desegmenting transducer can be constructed
by first encoding our desegmenter as a table that
maps morpheme sequences to words. Regardless
of whether the original desegmenter was based
on concatenation, rules or table-lookup, it can be
encoded as a lattice-specific table by applying it
to an enumeration of all words found in the lat-
tice. We can then transform that table into a fi-
nite state transducer with one path per table en-
try. Finally, we take the closure of this trans-
ducer, so that the resulting machine can transduce
any sequence of words. The desegmenting trans-
3
Throughout this paper, we use ?+? to mark morphemes
as prefixes or suffixes, as in w+ or +h. In Equation 1 only,
we overload ?+? as the Kleene cross: X+ == XX?.
103
ducer for our running example is shown in Fig-
ure 1b. Note that tokens requiring no desegmen-
tation simply emit themselves. The lattice (Fig-
ure 1a) can then be desegmented by composing it
with the transducer (1b), producing a desegmented
lattice (1c). This is a natural place to introduce
features that describe the desegmentation process,
such as scores provided by a desegmentation table,
which can be incorporated into the desegmenting
transducer?s edge weights.
We now have a desegmented lattice, but it has
not been annotated with an unsegmented (word-
level) language model. In order to annotate lattice
edges with an n-gram LM, every path coming into
a node must end with the same sequence of (n?1)
tokens. If this property does not hold, then nodes
must be split until it does.
4
This property is main-
tained by the decoder?s recombination rules for the
segmented LM, but it is not guaranteed for the de-
segmented LM. Indeed, the expanded word-level
context is one of the main benefits of incorporating
a word-level LM. Fortunately, LM annotation as
well as any necessary lattice modifications can be
performed simultaneously by composing the de-
segmented lattice with a finite state acceptor en-
coding the LM (Roark et al, 2011).
In summary, we are given a segmented lattice,
which encodes the decoder?s translation space as
an acceptor over morphemes. We compose this
acceptor with a desegmenting transducer, and then
with an unsegmented LM acceptor, producing a
fully annotated, desegmented lattice. Instead of
using a tool kit such as OpenFst (Allauzen et
al., 2007), we implement both the desegmenting
transducer and the LM acceptor programmatically.
This eliminates the need to construct intermediate
machines, such as the lattice-specific desegmenter
in Figure 1b, and facilitates working with edges
annotated with feature vectors as opposed to sin-
gle weights.
Programmatic Desegmentation
Lattice desegmentation is a non-local lattice trans-
formation. That is, the morphemes forming a word
might span several edges, making desegmentation
non-trivial. Luong et al (2010) address this prob-
lem by forcing the decoder?s phrase table to re-
spect word boundaries, guaranteeing that each de-
segmentable token sequence is local to an edge.
4
Or the LM composition can be done dynamically, ef-
fectively decoding the lattice with a beam or cube-pruned
search (Huang and Chiang, 2007).
Inspired by the use of non-local features in forest
decoding (Huang, 2008), we present an algorithm
to find chains of edges that correspond to deseg-
mentable token sequences, allowing lattice deseg-
mentation with no phrase-table restrictions. This
algorithm can be seen as implicitly constructing a
customized desegmenting transducer and compos-
ing it with the input lattice on the fly.
Before describing the algorithm, we define
some notation. An input morpheme lattice is a
triple ?n
s
,N , E?, where N is a set of nodes, E is
a set of edges, and n
s
? N is the start node that
begins each path through the lattice. Each edge
e ? E is a 4-tuple ?from, to, lex , w?, where from ,
to ? N are head and tail nodes, lex is a single
token accepted by this edge, and w is the (po-
tentially vector-valued) edge weight. Tokens are
drawn from one of three non-overlapping morpho-
syntactic sets: lex ? Prefix ? Stem ? Suffix ,
where tokens that do not require desegmentation,
such as complete words, punctuation and num-
bers, are considered to be in Stem . It is also useful
to consider the set of all outgoing edges for a node
n.out = {e ? E|e.from = n}.
With this notation in place, we can define a
chain c to be a sequence of edges [e
1
. . . e
l
] such
that for 1 ? i < l : e
i
.to = e
i+1
.from . We
denote singleton chains with [e], and when unam-
biguous, we abbreviate longer chains with their
start and end node [e
1
.from ? e
l
.to]. A chain
is valid if it emits the beginning of a word as de-
fined by the regular expression in Equation 1. A
valid chain is complete if its edges form an entire
word, and if it is part of a path through the lat-
tice that consists only of words. In Figure 1a, the
complete chains are [0 ? 2], [0 ? 4], [0 ? 5],
and [2 ? 3]. The path restriction on complete
chains forces words to be bounded by other words
in order to be complete.
5
For example, if we re-
moved the edge 2 ? 3 (AlTfl) from Figure 1a,
then [0? 2] ([b+ lEbp]) would cease to be a com-
plete chain, but it would still be a valid chain. Note
that in the finite-state analogy, the path restriction
is implicit in the composition operation.
Algorithm 1 desegments a lattice by finding all
complete chains and replacing each one with a sin-
gle edge. It maintains a work list of nodes that
lie on the boundary between words, and for each
node on this list, it launches a depth first search
5
Sentence-initial suffix morphemes and sentence-final
prefix morphemes represent a special case that we omit for
the sake of brevity. Lacking stems, they are left segmented.
104
Algorithm 1 Desegment a lattice ?n
s
,N , E?
{Initialize output lattice and work list WL}
n
?
s
= n
s
, N
?
= ?, E
?
= ?, WL = [n
s
]
while n = WL.pop() do
{Work on each node only once}
if n ? N
?
then continue
N
?
= N
?
? {n}
{Initialize the chain stack C}
C = ?
for e ? n.out do
if [e] is valid then C.push([e])
{Depth-first search for complete chains}
while [e
1
, . . . , e
l
] = C.pop() do
{Attempt to extend chain}
for e ? e
l
.to.out do
if [e
1
. . . e
l
, e] is valid then
C.push([e
1
, . . . , e
l
, e])
else
Mark [e
1
, . . . , e
l
] as complete
{Desegment complete chains}
if [e
1
, . . . , e
l
] is complete then
WL.push(e
l
.to)
E
?
= E
?
? {deseg([e
1
, . . . , e
l
])}
return ?n
?
s
,N
?
, E
?
?
to find all complete chains extending from it. The
search recognizes the valid chain c to be complete
by finding an edge e such that c+ e forms a chain,
but not a valid one. By inspection of Equation 1,
this can only happen when a prefix or stem fol-
lows a stem or suffix, which always marks a word
boundary. The chains found by this search are de-
segmented and then added to the output lattice as
edges. The nodes at end points of these chains are
added to the work list, as they lie at word bound-
aries by definition. Note that although this algo-
rithm creates completely new edges, the resulting
node set N
?
will be a subset of the input node set
N . The complementN ?N
?
will consist of nodes
that are word-internal in all paths through the input
lattice, such as node 1 in Figure 1a.
Programmatic LM Integration
Programmatic composition of a lattice with an
n-gram LM acceptor is a well understood prob-
lem. We use a dynamic program to enumerate all
(n ? 1)-word contexts leading into a node, and
then split the node into multiple copies, one for
each context. With each node corresponding to a
single LM context, annotation of outgoing edges
with n-gram LM scores is straightforward.
3.4 Desegmentation Features
Our re-ranker has access to all of the features used
by the decoder, in addition to a number of features
enabled by desegmentation.
Desegmentation Score We use a table-based
desegmentation method for Arabic, which is based
on segmenting an Arabic training corpus and
memorizing the observed transformations to re-
verse them later. Finnish does not require a ta-
ble, as all words can be desegmented with sim-
ple concatenation. The Arabic table consists of
X ? Y entries, where X is a target morpheme
sequence and Y is a desegmented surface form.
Several entries may share the same X , resulting
in multiple desegmentation options. For the sake
of symmetry with the unambiguous Finnish case,
we augment Arabic n-best lists or lattices with
only the most frequent desegmentation Y .
6
We
provide the desegmentation score log p(Y |X)=
log
(
count of X ? Y
count of X
)
as a feature, to indicate the en-
try?s ambiguity in the training data.
7
When an X is
missing from the table, we fall back on a set of de-
segmentation rules (El Kholy and Habash, 2012a)
and this feature is set to 0. This feature is always
0 for English-Finnish.
Contiguity One advantage of our approach is
that it allows discontiguous source words to trans-
late into a single target word. In order to maintain
some control over this powerful capability, we cre-
ate three binary features that indicate the contigu-
ity of a desegmentation. The first feature indicates
that the desegmented morphemes were translated
from contiguous source words. The second indi-
cates that the source words contained a single dis-
contiguity, as in a word-by-word translation of the
?with his blue car? example from Section 2.2. The
third indicates two or more discontiguities.
Unsegmented LM A 5-gram LM trained on un-
segmented target text is used to assess the fluency
of the desegmented word sequence.
4 Experimental Setup
We train our English-to-Arabic system using 1.49
million sentence pairs drawn from the NIST 2012
training set, excluding the UN data. This training
set contains about 40 million Arabic tokens before
6
Allowing the re-ranker to choose between multiple Y s is
a natural avenue for future work.
7
We also experimented on log p(X|Y ) as an additional
feature, but observed no improvement in translation quality.
105
segmentation, and 47 million after segmentation.
We tune on the NIST 2004 evaluation set (1353
sentences) and evaluate on NIST 2005 (1056 sen-
tences). As these evaluation sets are intended for
Arabic-to-English translation, we select the first
English reference to use as our source text.
Our English-to-Finnish system is trained on the
same Europarl corpus as Luong et al (2010) and
Clifton and Sarkar (2011), which has roughly one
million sentence pairs. We also use their develop-
ment and test sets (2000 sentences each).
4.1 Segmentation
For Arabic, morphological segmentation is per-
formed by MADA 3.2 (Habash et al, 2009), using
the Penn Arabic Treebank (PATB) segmentation
scheme as recommended by El Kholy and Habash
(2012a). For both segmented and unsegmented
Arabic, we further normalize the script by convert-
ing different forms of Alif @


@

@ @ and Ya ? ?


to
bare Alif @ and dotless Ya ?. To generate the de-
segmentation table, we analyze the segmentations
from the Arabic side of the parallel training data
to collect mappings from morpheme sequences to
surface forms.
For Finnish, we adopt the Unsup L-match seg-
mentation technique of Clifton and Sarkar (2011),
which uses Morfessor (Creutz and Lagus, 2005)
to analyze the 5,000 most frequent Finnish words.
The analysis is then applied to the Finnish side of
the parallel text, and a list of segmented suffixes
is collected. To improve coverage, words are fur-
ther segmented according to their longest match-
ing suffix from the list. As Morfessor does not
perform any orthographic normalizations, it can be
desegmented with simple concatenation.
4.2 Systems
We align the parallel data with GIZA++ (Och et
al., 2003) and decode using Moses (Koehn et al,
2007). The decoder?s log-linear model includes a
standard feature set. Four translation model fea-
tures encode phrase translation probabilities and
lexical scores in both directions. Seven distor-
tion features encode a standard distortion penalty
as well as a bidirectional lexicalized reordering
model. A KN-smoothed 5-gram language model
is trained on the target side of the parallel data with
SRILM (Stolcke, 2002). Finally, we include word
and phrase penalties. The decoder uses the default
parameters for English-to-Arabic, except that the
maximum phrase length is set to 8. For English-
to-Finnish, we follow Clifton and Sarkar (2011) in
setting the hypothesis stack size to 100, distortion
limit to 6, and maximum phrase length to 20.
The decoder?s log-linear model is tuned with
MERT (Och, 2003). Re-ranking models are tuned
using a batch variant of hope-fear MIRA (Chi-
ang et al, 2008; Cherry and Foster, 2012), us-
ing the n-best variant for n-best desegmentation,
and the lattice variant for lattice desegmentation.
MIRA was selected over MERT because we have
an in-house implementation that can tune on lat-
tices very quickly. During development, we con-
firmed that MERT and MIRA perform similarly,
as is expected with fewer than 20 features. Both
the decoder?s log-linear model and the re-ranking
models are trained on the same development set.
Historically, we have not seen improvements from
using different tuning sets for decoding and re-
ranking. Lattices are pruned to a density of 50
edges per word before re-ranking.
We test four different systems. Our first base-
line is Unsegmented, where we train on unseg-
mented target text, requiring no desegmentation
step. Our second baseline is 1-best Deseg, where
we train on segmented target text and desegment
the decoder?s 1-best output. Starting from the sys-
tem that produced 1-best Deseg, we then output ei-
ther 1000-best lists or lattices to create our two ex-
perimental systems. The 1000-best Deseg system
desegments, augments and re-ranks the decoder?s
1000-best list, while Lattice Deseg does the same
in the lattice. We augment n-best lists and lattices
using the features described in Section 3.4.
8
We evaluate our system using BLEU (Papineni
et al, 2002) and TER (Snover et al, 2006). Fol-
lowing Clark et al (2011), we report average
scores over five random tuning replications to ac-
count for optimizer instability. For the baselines,
this means 5 runs of decoder tuning. For the de-
segmenting re-rankers, this means 5 runs of re-
ranker tuning, each working on n-best lists or lat-
tices produced by the same (representative) de-
coder weights. We measure statistical significance
using MultEval (Clark et al, 2011), which imple-
ments a stratified approximate randomization test
to account for multiple tuning replications.
8
Development experiments on a small-data English-to-
Arabic scenario indicated that the Desegmentation Score was
not particularly useful, so we exclude it from the main com-
parison, but include it in the ablation experiments.
106
5 Results
Tables 1 and 2 report results averaged over 5 tun-
ing replications on English-to-Arabic and English-
to-Finnish, respectively. In all scenarios, both
1000-best Deseg and Lattice Deseg significantly
outperform the 1-best Deseg baseline (p < 0.01).
For English-to-Arabic, 1-best desegmentation
results in a 0.7 BLEU point improvement over
training on unsegmented Arabic. Moving to lat-
tice desegmentation more than doubles that im-
provement, resulting in a BLEU score of 34.4 and
an improvement of 1.0 BLEU point over 1-best
desegmentation. 1000-best desegmentation also
works well, resulting in a 0.6 BLEU point im-
provement over 1-best. Lattice desegmentation is
significantly better (p < 0.01) than 1000-best de-
segmentation.
For English-to-Finnish, the Unsup L-match seg-
mentation with 1-best desegmentation does not
improve over the unsegmented baseline. The seg-
mentation may be addressing issues with model
sparsity, but it is also introducing errors that would
have been impossible had words been left un-
segmented. In fact, even with our lattice deseg-
menter providing a boost, we are unable to see
a significant improvement over the unsegmented
model. As we attempted to replicate the approach
of Clifton and Sarkar (2011) exactly by working
with their segmented data, this difference is likely
due to changes in Moses since the publication of
their result. Nonetheless, the 1000-best and lattice
desegmenters both produce significant improve-
ments over the 1-best desegmentation baseline,
with Lattice Deseg achieving a 1-point improve-
ment in TER. These results match the established
state-of-the-art on this data set, but also indicate
that there is still room for improvement in identi-
fying the best segmentation strategy for English-
to-Finnish translation.
We also tried a similar Morfessor-based seg-
mentation for Arabic, which has an unsegmented
test set BLEU of 32.7. As in Finnish, the 1-best
desegmentation using Morfessor did not surpass
the unsegmented baseline, producing a test BLEU
of only 31.4 (not shown in Table 1). Lattice deseg-
mentation was able to boost this to 32.9, slightly
above 1-best desegmentation, but well below our
best MADA desegmentation result of 34.4. There
appears to be a large advantage to using MADA?s
supervised segmentation in this scenario.
Model Dev Test
BLEU BLEU TER
Unsegmented 24.4 32.7 49.4
1-best Deseg 24.4 33.4 48.6
1000-best Deseg 25.0 34.0 48.0
Lattice Deseg 25.2 34.4 47.7
Table 1: Results for English-to-Arabic translation
using MADA?s PATB segmentation.
Model Dev Test
BLEU BLEU TER
Unsegmented 15.4 15.1 70.8
1-best Deseg 15.3 14.8 71.9
1000-best Deseg 15.4 15.1 71.5
Lattice Deseg 15.5 15.1 70.9
Table 2: Results for English-to-Finnish translation
using unsupervised segmentation.
5.1 Ablation
We conducted an ablation experiment on English-
to-Arabic to measure the impact of the various fea-
tures described in Section 3.4. Table 3 compares
different combinations of features using lattice de-
segmentation. The unsegmented LM alone yields
a 0.4 point improvement over the 1-best deseg-
mentation score. Adding contiguity indicators on
top of the unsegmented LM results in another 0.6
point improvement. As anticipated, the tuner as-
signs negative weights to discontiguous cases, en-
couraging the re-ranker to select a safer transla-
tion path when possible. Judging from the out-
put on the NIST 2005 test set, the system uses
these discontiguous desegmentations very rarely:
only 5% of desegmented tokens align to discon-
tiguous source phrases. Adding the desegmenta-
tion score to these two feature groups does not im-
prove performance, confirming the results we ob-
served during development. The desegmentation
score would likely be useful in a scenario where
we provide multiple desegmentation options to the
re-ranker; for now, it indicates only the ambiguity
of a fixed choice, and is likely redundant with in-
formation provided by the language model.
5.2 Error Analysis
In order to better understand the source of our
improvements in the English-to-Arabic scenario,
we conducted an extensive manual analysis of
the differences between 1-best and lattice deseg-
107
Features dev test
1-best Deseg 24.5 33.4
+ Unsegmented LM 24.9 33.8
+ Contiguity 25.2 34.4
+ Desegmentation Score 25.2 34.3
Table 3: The effect of feature ablation on BLEU
score for English-to-Arabic translation with lattice
desegmentation.
mentation on our test set. We compared the
output of the two systems using the Unix tool
wdiff , which transforms a solution to the longest-
common-subsequence problem into a sequence
of multi-word insertions and deletions (Hunt and
McIlroy, 1976). We considered adjacent insertion-
deletion pairs to be (potentially phrasal) substitu-
tions, and collected them into a file, omitting any
unpaired insertions or deletions. We then sampled
650 cases where the two sides of the substitution
were deemed to be related, and divided these cases
into categories based on how the lattice desegmen-
tation differs from the one-best desegmentation.
We consider a phrase to be correct only if it can
be found in the reference.
Table 4 breaks down per-phrase accuracy ac-
cording to four manually-assigned categories: (1)
clitical ? the two systems agree on a stem, but at
least one clitic, often a prefix denoting a prepo-
sition or determiner, was dropped, added or re-
placed; (2) lexical ? a word was changed to a mor-
phologically unrelated word with a similar mean-
ing; (3) inflectional ? the words have the same
stem, but different inflection due to a change in
gender, number or verb tense; (4) part-of-speech
? the two systems agree on the lemma, but have
selected different parts of speech.
For each case covering a single phrasal differ-
ence, we compare the phrases from each system
to the reference. We report the number of in-
stances where each system matched the reference,
as well as cases where they were both incorrect.
The majority of differences correspond to clitics,
whose correction appears to be a major source of
the improvements obtained by lattice desegmen-
tation. This category is challenging for the de-
coder because English prepositions tend to corre-
spond to multiple possible forms when translated
into Arabic. It also includes the frequent cases
involving the nominal determiner prefix Al ?the?
(left unsegmented by the PATB scheme), and the
Lattice
Correct
1-best
Correct
Both
Incorrect
Clitical 157 71 79
Lexical 61 39 80
Inflectional 37 32 47
Part-of-speech 19 17 11
Table 4: Error analysis for English-to-Arabic
translation based on 650 sampled instances.
sentence-initial conjunction w+ ?and?. The sec-
ond most common category is lexical, where the
unsegmented LM has drastically altered the choice
of translation. The remaining categories show no
major advantage for either system.
6 Conclusion
We have explored deeper integration of morpho-
logical desegmentation into the statistical machine
translation pipeline. We have presented a novel,
finite-state-inspired approach to lattice desegmen-
tation, which allows the system to account for a
desegmented view of many possible translations,
without any modification to the decoder or any
restrictions on phrase extraction. When applied
to English-to-Arabic translation, lattice desegmen-
tation results in a 1.0 BLEU point improvement
over one-best desegmentation, and a 1.7 BLEU
point improvement over unsegmented translation.
We have also applied our approach to English-to-
Finnish translation, and although segmentation in
general does not currently help, we are able to
show significant improvements over a 1-best de-
segmentation baseline.
In the future, we plan to explore introducing
multiple segmentation options into the lattice, and
the application of our method to a full morpho-
logical analysis (as opposed to segmentation) of
the target language. Eventually, we would like
to replace the functionality of factored transla-
tion models (Koehn and Hoang, 2007) with lattice
transformation and augmentation.
Acknowledgments
Thanks to Ann Clifton for generously provid-
ing the data and segmentation for our English-to-
Finnish experiments, and to Marine Carpuat and
Roland Kuhn for their helpful comments on an
earlier draft. This research was supported by the
Natural Sciences and Engineering Research Coun-
cil of Canada.
108
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Ninth International
Conference on Implementation and Application of
Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008.
Segmentation for English-to-Arabic statistical ma-
chine translation. In Proceedings of ACL, pages
153?156.
Ond?rej Bojar. 2007. English-to-Czech factored ma-
chine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
232?239, Prague, Czech Republic, June.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of HLT-NAACL, Montreal, Canada, June.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of
EMNLP, pages 224?233.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Controlling for
optimizer instability. In Proceedings of ACL, pages
176?181.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 32?42, Portland, Oregon, USA, June.
Mathias Creutz and Krista Lagus. 2005. Induc-
ing the morphological lexicon of a natural language
from unannotated text. In In Proceedings of the
International and Interdisciplinary Conference on
Adaptive Knowledge Representation and Reasoning
(AKRR05, pages 106?113.
Ahmed El Kholy and Nizar Habash. 2012a. Ortho-
graphic and morphological processing for English?
Arabic statistical machine translation. Machine
Translation, 26(1-2):25?45, March.
Ahmed El Kholy and Nizar Habash. 2012b. Trans-
late, predict or generate: Modeling rich morphology
in statistical machine translation. Proceeding of the
Meeting of the European Association for Machine
Translation.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inflection and word-
formation in SMT. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 664?674, Avi-
gnon, France, April. Association for Computational
Linguistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools, Cairo, Egypt, April.
The MEDAR Consortium.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151, Prague, Czech Republic, June.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586?594, Columbus, Ohio,
June.
James W. Hunt and M. Douglas McIlroy. 1976. An
algorithm for differential file comparison. Technical
report, Bell Laboratories, June.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A discriminative lexicon
model for complex morphology. In The Ninth Con-
ference of the Association for Machine Translation
in the Americas.
Philipp Koehn and Hieu Hoang. 2007. Factored
translation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 868?
876, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Franz Joesef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation for
machine translation of morphologically rich lan-
guages. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 148?157, Cambridge, MA, October.
109
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 128?135, Prague, Czech Republic, June.
Franz Josef Och, Hermann Ney, Franz Josef, and
Och Hermann Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29.
Franz Joseph Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 25?32, Prague,
Czech Republic, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Brian Roark, Richard Sproat, and Izhak Shafran. 2011.
Lexicographic semirings for exact automata encod-
ing of sequence models. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1?5, Portland, Oregon, USA, June.
Mohammad Salameh, Colin Cherry, and Grzegorz
Kondrak. 2013. Reversing morphological tokeniza-
tion in English-to-Arabic SMT. In Proceedings of
the 2013 NAACL HLT Student Research Workshop,
pages 47?53, Atlanta, Georgia, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language
Processing, pages 901?904.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 230?238, Portland, Ore-
gon, USA, June.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08:
HLT, pages 514?522, Columbus, Ohio, June.
Nicola Ueffing, Franz J. Och, and Hermann Ney. 2002.
Generation of word graphs in statistical machine
translation. In Proceedings of EMNLP, pages 156?
163, Philadelphia, PA, July.
110
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437?442,
Dublin, Ireland, August 23-24, 2014.
NRC-Canada-2014: Detecting Aspects and Sentiment
in Customer Reviews
Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif M. Mohammad
National Research Council Canada
1200 Montreal Rd., Ottawa, ON, Canada
{Svetlana.Kiritchenko, Xiaodan.Zhu, Colin.Cherry, Saif.Mohammad}
@nrc-cnrc.gc.ca
Abstract
Reviews depict sentiments of customers
towards various aspects of a product or
service. Some of these aspects can be
grouped into coarser aspect categories.
SemEval-2014 had a shared task (Task 4)
on aspect-level sentiment analysis, with
over 30 teams participated. In this pa-
per, we describe our submissions, which
stood first in detecting aspect categories,
first in detecting sentiment towards aspect
categories, third in detecting aspect terms,
and first and second in detecting senti-
ment towards aspect terms in the laptop
and restaurant domains, respectively.
1 Introduction
Automatically identifying sentiment expressed in
text has a number of applications, including track-
ing sentiment towards products, movies, politi-
cians, etc.; improving customer relation models;
and detecting happiness and well-being. In many
applications, it is important to associate sentiment
with a particular entity or an aspect of an entity.
For example, in reviews, customers might express
different sentiment towards various aspects of a
product or service they have availed. Consider:
The lasagna was great, but the service
was a bit slow.
The review is for a restaurant, and we can gather
from it that the customer has a positive sentiment
towards the lasagna they serve, but a negative sen-
timent towards the service.
The SemEval-2014 Task 4 (Aspect Based Sen-
timent Analysis) is a shared task where given a
customer review, automatic systems are to deter-
mine aspect terms, aspect categories, and senti-
ment towards these aspect terms and categories.
An aspect term is defined to be an explicit men-
tion of a feature or component of the target prod-
uct or service. The example sentence above has
Restaurants Laptops
Term T-Sent. Cat. C-Sent. Term T-Sent.
3 2 1 1 3 1
Table 1: Rank obtained by NRC-Canada in vari-
ous subtasks of SemEval-2014 Task 4.
the aspect term lasagna. Similar aspect terms can
be grouped into aspect categories. For example,
lasagna and other food items can be grouped into
the aspect category of ?food?. In Task 4, customer
reviews are provided for two domains: restaurants
and laptops. A fixed set of five aspect categories
is defined for the restaurant domain: food, ser-
vice, price, ambiance, and anecdotes. Automatic
systems are to determine if any of those aspect
categories are described in a review. The exam-
ple sentence above describes the aspect categories
of food (positive sentiment) and service (negative
sentiment). For the laptop reviews, there is no as-
pect category detection subtask. Further details of
the task and data can be found in the task descrip-
tion paper (Pontiki et al., 2014).
We present an in-house sequence tagger to de-
tect aspect terms and supervised classifiers to de-
tect aspect categories, sentiment towards aspect
terms, and sentiment towards aspect categories. A
summary of the ranks obtained by our submissions
to the shared task is provided in Table 1.
2 Lexical Resources
2.1 Unlabeled Reviews Corpora
Apart from the training data provided for Task 4,
we compiled large corpora of reviews for restau-
rants and laptops that were not labeled for aspect
terms, aspect categories, or sentiment. We gen-
erated lexicons from these corpora and used them
as a source of additional features in our machine
learning systems.
437
Yelp restaurant reviews corpus: The Yelp
Phoenix Academic Dataset
1
contains customer re-
views posted on the Yelp website. The businesses
for which the reviews are posted are classified into
over 500 categories. Further, many of the busi-
nesses are assigned multiple business categories.
We identified all food-related business categories
(58 categories) that were grouped along with the
category ?restaurant? and extracted all customer
reviews for these categories. We will refer to this
corpus of 183,935 reviews as the Yelp restaurant
reviews corpus.
Amazon laptop reviews corpus: McAuley and
Leskovec (2013) collected reviews posted on
Amazon.com from June 1995 to March 2013. A
subset of this corpus is marked as reviews for elec-
tronic products. We extracted from this subset all
reviews that mention either laptop or notebook.
We will refer to this collection of 124,712 reviews
as the Amazon laptop reviews corpus.
Both the Yelp and the Amazon reviews have one
to five star ratings associated with each review. We
treated the one- and two-star reviews as negative
reviews, and the four- and five-star reviews as pos-
itive reviews.
2.2 Lexicons
Sentiment Lexicons: From the Yelp restaurant
reviews corpus, we automatically created an in-
domain sentiment lexicon for restaurants. Follow-
ing Turney and Littman (2003) and Mohammad
et al. (2013), we calculated a sentiment score for
each term w in the corpus:
score (w) = PMI (w , pos)?PMI (w ,neg) (1)
where pos denotes positive reviews and neg de-
notes negative reviews. PMI stands for pointwise
mutual information:
PMI (w , pos) = log
2
freq (w , pos) ?N
freq (w) ? freq (pos)
(2)
where freq (w, pos) is the number of times a term
w occurs in positive reviews, freq (w) is the to-
tal frequency of term w in the corpus, freq (pos)
is the total number of tokens in positive reviews,
and N is the total number of tokens in the cor-
pus. PMI (w ,neg) was calculated in a similar
way. Since PMI is known to be a poor estima-
tor of association for low-frequency events, we ig-
nored terms that occurred less than five times in
each (positive and negative) groups of reviews.
1
http://www.yelp.com/dataset_challenge
A positive sentiment score indicates a greater
overall association with positive sentiment,
whereas a negative score indicates a greater asso-
ciation with negative sentiment. The magnitude is
indicative of the degree of association.
Negation words (e.g., not, never) can signifi-
cantly affect the sentiment of an expression (Zhu
et al., 2014). Therefore, when generating the sen-
timent lexicons we distinguished terms appearing
in negated contexts (defined as text spans between
a negation word and a punctuation mark) and af-
firmative (non-negated) contexts. The sentiment
scores were then calculated separately for the two
types of contexts. For example, the term good in
affirmative contexts has a sentiment score of 1.2
whereas the same term in negated contexts has a
score of -1.4. We built two lexicons, Yelp Restau-
rant Sentiment AffLex and Yelp Restaurant Senti-
ment NegLex, as described in (Kiritchenko et al.,
2014).
Similarly, we generated in-domain sentiment
lexicons from the Amazon laptop reviews corpus.
In addition, we employed existing out-of-
domain sentiment lexicons: (1) large-coverage au-
tomatic tweet sentiment lexicons, Hashtag Sen-
timent lexicons and Sentiment140 lexicons (Kir-
itchenko et al., 2014), and (2) three manually cre-
ated sentiment lexicons, NRC Emotion Lexicon
(Mohammad and Turney, 2010), Bing Liu?s Lex-
icon (Hu and Liu, 2004), and the MPQA Subjec-
tivity Lexicon (Wilson et al., 2005).
Yelp Restaurant Word?Aspect Association
Lexicon: The Yelp restaurant reviews corpus was
also used to generate a lexicon of terms associated
with the aspect categories of food, price, service,
ambiance, and anecdotes. Each sentence of the
corpus was labeled with zero, one, or more of the
five aspect categories by our aspect category clas-
sification system (described in Section 5). Then,
for each term w and each category c an associa-
tion score was calculated as follows:
score (w , c) = PMI (w , c)? PMI (w ,?c) (3)
2.3 Word Clusters
Word clusters can provide an alternative represen-
tation of text, significantly reducing the sparsity
of the token space. Using Brown clustering algo-
rithm (Brown et al., 1992), we generated 1,000
word clusters from the Yelp restaurant reviews
corpus. Additionally, we used publicly available
438
word clusters generated from 56 million English-
language tweets (Owoputi et al., 2013).
3 Subtask 1: Aspect Term Extraction
The objective of this subtask is to detect aspect
terms in sentences. We approached this problem
using in-house entity-recognition software, very
similar to the system used by de Bruijn et al.
(2011) to detect medical concepts. First, sen-
tences were tokenized to split away punctuation,
and then the token sequence was tagged using a
semi-Markov tagger (Sarawagi and Cohen, 2004).
The tagger had two possible tags: O for outside,
and T for aspect term, where an aspect term could
tag a phrase of up to 5 consecutive tokens. The
tagger was trained using the structured Passive-
Aggressive (PA) algorithm with a maximum step-
size of C = 1 (Crammer et al., 2006).
Our features can be divided into two categories:
emission and transition features. Emission fea-
tures couple the tag sequence y to the input w.
Most of these work on the token level, and con-
join features of each token with the tag covering
that token. If a token is the first or last token cov-
ered by a tag, then we produce a second copy of
each of its features to indicate its special position.
Let w
i
be the token being tagged; its token fea-
ture templates are: token-identity within a win-
dow (w
i?2
. . . w
i+2
), lower-cased token-identity
within a window (lc(w
i?2
) . . . lc(w
i+2
)), and pre-
fixes and suffixes of w
i
(up to 3 characters in
length). There are only two phrase-level emission
feature templates: the cased and uncased identity
of the entire phrase covered by a tag, which al-
low the system to memorize complete terms such
as, ?getting a table? or ?fish and chips.? Transi-
tion features couple tags with tags. Let the cur-
rent tag be y
j
. Its transition feature templates are
short n-grams of tag identities: y
j
; y
j
, y
j?1
; and
y
j
, y
j?1
, y
j?2
.
During development, we experimented with the
training algorithm, trying both PA and the simpler
structured perceptron (Collins, 2002). We also
added the lowercased back-off features. In Ta-
ble 2, we re-test these design decisions on the test
set, revealing that lower-cased back-off features
made a strong contribution, while PA training was
perhaps not as important. Our complete system
achieved an F1-score of 80.19 on the restaurant
domain and 68.57 on the laptop domain, ranking
third among 24 teams in both.
Restaurants
System P R F1
NRC-Canada (All) 84.41 76.37 80.19
All ? lower-casing 83.68 75.49 79.37
All ? PA + percep 83.37 76.45 79.76
Laptops
System P R F1
NRC-Canada (All) 78.77 60.70 68.57
All ? lower-casing 78.11 60.55 68.22
All ? PA + percep 77.76 61.47 68.66
Table 2: Test set ablation experiments for Sub-
task 1: Aspect Term Detection.
4 Subtask 2: Aspect Term Polarity
In this subtask, the goal is to detect sentiment ex-
pressed towards a given aspect term. For example,
in sentence ?The asian salad is barely eatable.? the
aspect term asian salad is referred to with negative
sentiment. There were defined four categories of
sentiment: positive, negative, neutral, or conflict.
The conflict category is assigned to cases where
an aspect term is mentioned with both positive and
negative sentiment.
To address this multi-class classification prob-
lem, we trained a linear SVM classifier using
the LibSVM software (Chang and Lin, 2011).
Sentences were first tokenized and parsed with
the Stanford CoreNLP toolkits
2
to obtain part-of-
speech (POS) tags and (collapsed) typed depen-
dency parse trees (de Marneffe et al., 2006). Then,
features were extracted from (1) the target term it-
self; (2) its surface context, i.e., a window of n
words surrounding the term; (3) the parse context,
i.e., the nodes in the parse tree that are connected
to the target term by at most three edges.
Surface features: (1) unigrams (single words)
and bigrams (2-word sequences) extracted from a
term and its surface context; (2) context-target bi-
grams (i.e., bigrams formed by a word from the
surface context and a word from the term itself).
Lexicon features: (1) the number of posi-
tive/negative tokens; (2) the sum of the tokens?
sentiment scores; (3) the maximal sentiment score.
The lexicon features were calculated for each
manually and automatically created sentiment lex-
icons described in Section 2.2.
Parse features: (1) word- and POS-ngrams in
2
http://nlp.stanford.edu/software/corenlp.shtml
439
Laptops Rest.
System Acc. Acc.
NRC-Canada (All) 70.49 80.16
All ? sentiment lexicons 63.61 77.13
All ? Yelp lexicons 68.65 77.85
All ? Amazon lex. 68.13 80.11
All ? manual lexicons 67.43 78.66
All ? tweet lexicons 69.11 78.57
All ? parse features 69.42 78.40
Table 3: Test set ablation experiments for Sub-
task 2: Aspect Term Polarity.
the parse context; (2) context-target bigrams, i.e.,
bigrams composed of a word from the parse con-
text and a word from the term; (3) all paths that
start or end with the root of the target terms. The
idea behind the use of the parse features is that
sometimes an aspect term is separated from its
modifying sentiment phrase and the surface con-
text is insufficient or even misleading for detect-
ing sentiment expressed towards the aspect. For
example, in sentence ?The food, though different
from what we had last time, is actually great? the
word great is much closer to the word food in the
parse tree than in the surface form. Furthermore,
the features derived from the parse context can
help resolve local syntactic ambiguity (e.g., the
word bad in the phrase ?a bad sushi lover? modi-
fies lover and not sushi).
Table 3 presents the results of our official sub-
mission on the test sets for the laptop and restau-
rant domains. On the laptop dataset, our system
achieved the accuracy of 70.49 and was ranked
first among 32 submissions from 29 teams. From
the ablation experiments we see that the most sig-
nificant gains come from the use of the sentiment
lexicons; without the lexicon features the perfor-
mance of the system drops by 6.88 percentage
points. Observe that the features derived from
the out-of-domain Yelp Restaurant Sentiment lex-
icon are very helpful on the laptop domain. The
parse features proved to be useful as well; they
contribute 1.07 percentage points to the final per-
formance. On the restaurant data, our system ob-
tained the accuracy of 80.16 and was ranked sec-
ond among 36 submissions from 29 teams.
5 Subtask 3: Aspect Category Detection
The objective of this subtask is to detect aspect
categories discussed in a given sentence. There
Restaurants
System P R F1
NRC-Canada (All) 91.04 86.24 88.58
All ? lex. resources 86.53 78.34 82.23
All ?W?A lexicon 88.47 80.10 84.08
All ? word clusters 90.84 86.15 88.43
All ? post-processing 91.47 84.78 88.00
Table 4: Test set ablation experiments for Sub-
task 3: Aspect Category Detection. ?W?A lexicon?
stands for Yelp Restaurant Word?Aspect Associa-
tion Lexicon.
are 5 pre-defined categories for the restaurant do-
main: food, price, service, ambience, and anec-
dotes/miscellaneous. Each sentence can be la-
beled with one or more categories from the pre-
defined set. No aspect categories were defined for
the laptop domain.
We addressed the subtask as a multi-class multi-
label text classification problem. Five binary one-
vs-all Support Vector Machine (SVM) classifiers
were built, one for each category. The parameter C
was optimized through cross-validation separately
for each classifier. Sentences were tokenized
and stemmed with Porter stemmer (Porter, 1980).
Then, the following sets of features were gener-
ated for each sentence: ngrams, stemmed ngrams,
character ngrams, non-contiguous ngrams, word
cluster ngrams, and lexicon features. For the lex-
icon features, we used the Yelp Restaurant Word?
Aspect Association Lexicon and calculated the cu-
mulative scores of all terms appeared in the sen-
tence for each aspect category. Separate scores
were calculated for unigram and bigram entries.
Sentences with no category assigned by any of the
five classifiers went through the post-processing
step. For each such sentence, a category c with the
maximal posterior probability P (c|d) was identi-
fied and the sentence was labeled with the category
c if P (c|d) ? 0.4.
Table 4 presents the results on the restaurant test
set. Our system obtained the F1-score of 88.58
and was ranked first among 21 submissions from
18 teams. Among the lexical resources (lexicons
and word clusters) employed in the system, the
Word?Aspect Association Lexicon provided the
most gains: an increase in F1-score of 4.5 points.
The post-processing step also proved to be benefi-
cial: the recall improved by 1.46 points increasing
the overall F1-score by 0.58 points.
440
6 Subtask 4: Aspect Category Polarity
In the Aspect Category Polarity subtask, the goal
is to detect the sentiment expressed towards a
given aspect category in a given sentence. For
each input pair (sentence, aspect category), the
output is a single sentiment label: positive, neg-
ative, neutral, or conflict.
We trained one multi-class SVM classifier
(Crammer and Singer, 2002) for all aspect cate-
gories. The feature set was extended to incorpo-
rate the information about a given aspect category
c using a domain adaptation technique (Daum?e III,
2007) as follows: each feature f had two copies,
f general (for all the aspect categories) and f c
(for the specific category of the instance). For ex-
ample, for the input pair (?The bread is top notch
as well.?, ?food?) two copies of the unigram top
would be used: top general and top food . With
this setup the classifier can take advantage of the
whole training dataset to learn common sentiment
features (e.g., the word good is associated with
positive sentiment for all aspect categories). At the
same time, aspect-specific sentiment features can
be learned from the training instances pertaining
to a specific aspect category (e.g., the word deli-
cious is associated with positive sentiment for the
category ?food?).
Sentences were tokenized and part-of-speech
tagged with CMU Twitter NLP tool (Gimpel et al.,
2011). Then, each sentence was represented as a
feature vector with the following groups of fea-
tures: ngrams, character ngrams, non-contiguous
ngrams, POS tags, cluster ngrams, and lexicon
features. The lexicon features were calculated as
described in Section 4.
A sentence can refer to more than one aspect
category with different sentiment. For example,
in the sentence ?The pizza was delicious, but the
waiter was rude.?, food is described with posi-
tive sentiment while service with negative. If the
words delicious and rude occur in the training set,
the classifier can learn that delicious usually refers
to food (with positive sentiment) and rude to ser-
vice (with negative sentiment). If these terms do
not appear in the training set, their polarities can
still be inferred from sentiment lexicons. How-
ever, sentiment lexicons do not distinguish among
aspect categories and would treat both words, de-
licious and rude, as equally applicable to both cat-
egories, ?food? and ?service?. To (partially) over-
come this problem, we applied the Yelp Restau-
Restaurants
System Accuracy
NRC-Canada (All) 82.93
All ? lexical resources 74.15
All ? lexicons 75.32
All ? Yelp lexicons 79.22
All ? manual lexicons 82.44
All ? tweet lexicons 84.10
All ? word clusters 82.93
All ? aspect term features 82.54
Table 5: Test set ablation experiments for Sub-
task 4: Aspect Category Polarity.
rant Word?Aspect Association Lexicon to collect
all the terms having a high or moderate associ-
ation with the given aspect category (e.g., pizza,
delicious for the category ?food? and waiter, rude
for the category ?service?). Then, the feature set
described above was augmented with the same
groups of features generated just for the terms as-
sociated with the given category. We call these
features aspect term features.
Table 5 presents the results on the test set for
the restaurant domain. Our system achieved the
accuracy of 82.93 and was ranked first among 23
submissions from 20 teams. The ablation exper-
iments demonstrate the significant impact of the
lexical resources employed in the system: 8.78
percentage point gain in accuracy. The major ad-
vantage comes from the sentiment lexicons, and
specifically from the in-domain Yelp Restaurant
Sentiment lexicons. The out-of-domain tweet sen-
timent lexicons did not prove useful on this sub-
task. Also, word clusters did not offer additional
benefits on top of those provided by the lexicons.
The use of aspect term features resulted in gains
of 0.39.
7 Conclusion
The paper describes supervised machine-learning
approaches to detect aspect terms and aspect cat-
egories and to detect sentiment expressed towards
aspect terms and aspect categories in customer re-
views. Apart from common surface-form features
such as ngrams, our approaches benefit from the
use of existing and newly created lexical resources
such as word?aspect association lexicons and sen-
timent lexicons. Our submissions stood first on 3
out of 4 subtasks, and within the top 3 best results
on all 6 task-domain evaluations.
441
References
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265?292.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, ACL
?07, pages 256 ? 263.
Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko,
Joel Martin, and Xiaodan Zhu. 2011. Machine-
learned solutions for three stages of clinical infor-
mation extraction: the state of the art at i2b2 2010.
Journal of the American Medical Informatics Asso-
ciation, 18(5):557?562.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation, LREC ?06.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, ACL ?11.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-
hammad. 2014. Sentiment analysis of short infor-
mal texts. Journal of Artificial Intelligence Research
(to appear).
Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM conference on Recommender systems, pages
165?172. ACM.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
Mechanical Turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT Workshop on Com-
putational Approaches to Analysis and Generation
of Emotion in Text, LA, California.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the International Workshop on Semantic
Evaluation, SemEval ?13, Atlanta, Georgia, USA,
June.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation,
SemEval ?14, Dublin, Ireland, August.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 3:130?137.
Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems, volume 17, pages 1185?1192.
Peter Turney and Michael L Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21(4).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
Xiaodan Zhu, Hongyu Guo, Saif M. Mohammad, and
Svetlana Kiritchenko. 2014. An empirical study on
the effect of negation words on sentiment. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics, ACL ?14.
442
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 200?209,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
On Hierarchical Re-ordering and Permutation Parsing
for Phrase-based Decoding
Colin Cherry
National Research Council
colin.cherry@nrc-cnrc.gc.ca
Robert C. Moore
Google
bobmoore@google.com
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Abstract
The addition of a deterministic permutation
parser can provide valuable hierarchical in-
formation to a phrase-based statistical ma-
chine translation (PBSMT) system. Permuta-
tion parsers have been used to implement hier-
archical re-ordering models (Galley and Man-
ning, 2008) and to enforce inversion trans-
duction grammar (ITG) constraints (Feng et
al., 2010). We present a number of theoret-
ical results regarding the use of permutation
parsers in PBSMT. In particular, we show that
an existing ITG constraint (Zens et al, 2004)
does not prevent all non-ITG permutations,
and we demonstrate that the hierarchical re-
ordering model can produce analyses during
decoding that are inconsistent with analyses
made during training. Experimentally, we ver-
ify the utility of hierarchical re-ordering, and
compare several theoretically-motivated vari-
ants in terms of both translation quality and
the syntactic complexity of their output.
1 Introduction
Despite the emergence of a number of syntax-based
techniques, phrase-based statistical machine transla-
tion remains a competitive and very efficient trans-
lation paradigm (Galley and Manning, 2010). How-
ever, it lacks the syntactically-informed movement
models and constraints that are provided implicitly
by working with synchronous grammars. There-
fore, re-ordering must be modeled and constrained
explicitly. Movement can be modeled with a dis-
tortion penalty or lexicalized re-ordering probabili-
ties (Koehn et al, 2003; Koehn et al, 2007), while
decoding can be constrained by distortion limits or
by mimicking the restrictions of inversion transduc-
tion grammars (Wu, 1997; Zens et al, 2004).
Recently, we have begun to see deterministic per-
mutation parsers incorporated into phrase-based de-
coders. These efficient parsers analyze the sequence
of phrases used to produce the target, and assem-
ble them into a hierarchical translation history that
can be used to inform re-ordering decisions. Thus
far, they have been used to enable a hierarchical
re-ordering model, or HRM (Galley and Manning,
2008), as well as an ITG constraint (Feng et al,
2010). We discuss each of these techniques in turn,
and then explore the implications of ITG violations
on hierarchical re-ordering.
We present one experimental and four theoreti-
cal contributions. Examining the HRM alone, we
present an improved algorithm for extracting HRM
statistics, reducing the complexity of Galley and
Manning?s solution from O(n4) to O(n2). Examin-
ing ITG constraints alone, we demonstrate that the
three-stack constraint of Feng et al can be reduced
to one augmented stack, and we show that another
phrase-based ITG constraint (Zens et al, 2004) ac-
tually allows some ITG violations to pass. Finally,
we show that in the presence of ITG violations, the
original HRM can fail to produce orientations that
are consistent with the orientations collected during
training. We propose three HRM variants to address
this situation, including an approximate HRM that
requires no permutation parser, and compare them
experimentally. The variants perform similarly to
the original in terms of BLEU score, but differently
in terms of how they permute the source sentence.
200
We begin by establishing some notation. We view
the phrase-based translation process as producing a
sequence of source/target blocks in their target or-
der. For the purposes of this paper, we disregard
the lexical content of these blocks, treating blocks
spanning the same source segment as equivalent.
The block [si, ti] indicates that the source segment
wsi+1, . . . , wti was translated as a unit to produce
the ith target phrase. We index between words;
therefore, a block?s length in tokens is t ? s, and
for a sentence of length n, 0 ? s ? t ? n. Empty
blocks have s = t, and are used only in special cases.
Two blocks [si?1, ti?1] and [si, ti] are adjacent iff
ti?1 = si or ti = si?1. Note that we concern our-
selves only with adjacency in the source. Adjacency
in the target is assumed, as the blocks are in target
order. Figure 1 shows an example block sequence,
where adjacency corresponds to cases where block
corners touch. In the shift-reduce permutation parser
we describe below, the parsing state is encoded as a
stack of these same blocks.
2 Hierarchical Re-ordering
Hierarchical re-ordering models (HRMs) for phrase-
based SMT are an extension of lexicalized re-
ordering models (LRMs), so we begin by briefly
reviewing the LRM (Tillmann, 2004; Koehn et al,
2007). The goal of an LRM is to characterize how
a phrase-pair tends to be placed with respect to the
block that immediately precedes it. Both the LRM
and the HRM track orientations traveling through
the target from left-to-right as well as right-to-left.
For the sake of brevity and clarity, we discuss only
the left-to-right direction except when stated oth-
erwise. Re-ordering is typically categorized into
three orientations, which are determined by exam-
ining two sequential blocks [si?1, ti?1] and [si, ti]:
? Monotone Adjacent (M): ti?1 = si
? Swap Adjacent (S): ti = si?1
? Disjoint (D): otherwise
Figure 1 shows a simple example, where the first
two blocks are placed in monotone orientation, fol-
lowed by a disjoint ?red?, a swapped ?dog? and a
disjoint period. The probability of an orientation
Oi ? {M,S,D} is determined by a conditional
distribution: Pr(Oi|source phrasei, target phrasei).
Em
ily	 ?
	 ?aim
e	 ?	 ?s
on
	 ?	 ?gr
os	 ?
	 ?ch
ien
	 ?	 ?ro
uge
	 ?	 ?	 ?	 ?.
	 ?
[0,	 ?2]	 ?
Emily	 ?	 ?loves	 ?	 ?
[2,	 ?4]	 ?
her	 ?	 ?big	 ?	 ?
[5,6]	 ?
red	 ?	 ?
[4,5]	 ?
dog	 ?
[6,7]	 ?
.	 ?
Figure 1: A French-to-English translation with 5 blocks.
To build this model, orientation counts can be ex-
tracted from aligned parallel text using a simple
heuristic (Koehn et al, 2007).
The HRM (Galley and Manning, 2008) maintains
similar re-ordering statistics, but determines orienta-
tion differently. It is designed to address the LRM?s
dependence on the previous block [si?1, ti?1]. Con-
sider the period [6,7] in Figure 1. If a different seg-
mentation of the source had preceded it, such as one
that translates ?chien rouge? as a single [4,6] block,
the period would have been in monotone orienta-
tion. Galley and Manning (2008) introduce a de-
terministic shift-reduce parser into decoding, so that
the decoder always has access to the largest possible
previous block, given the current translation history.
The parser has two operations: shift places a newly
translated block on the top of the stack. If the top
two blocks are adjacent, then a reduce is immedi-
ately performed, replacing them with a single block
spanning both. Table 1 shows the parser states cor-
responding to our running example. Whether ?chien
rouge? is translated using [5,6],[4,5] or [4,6] alone,
the shift-reduce parser provides a consolidated pre-
vious block of [0,6] at the top of the stack (shown
with dotted lines). Therefore, [6,7] is placed in
monotone orientation in both cases.
The parser can be easily integrated into a phrase-
based decoder?s translation state, so each partial hy-
pothesis carries its own shift-reduce stack. Time and
memory costs for copying and storing stacks can
be kept small by sharing tails across decoder states.
The stack subsumes the coverage vector in that it
contains strictly more information: every covered
201
Op Stack
S [0,2]
S [0,2],[2,4]
R [0,4]
S [0,4],[5,6]
S [0,4],[5,6],[4,5]
R [0,4],[4,6]
R [0,6]
S [0,6],[6,7]
R [0,7]
Table 1: Shift-reduce states corresponding to Figure 1.
word will be present in one of the stack?s blocks.
However, it can be useful to maintain both.
The top item of a parser?s stack can be approxi-
mated using only the coverage vector. The approx-
imate top is the largest block of covered words that
contains the last translated block. This approxima-
tion will always be as large or larger than the true top
of the stack, and it will often match the true top ex-
actly. For example, in Figure 1, after we have trans-
lated [2,4], we can see that the coverage vector con-
tains all of [0,4], making the approximate top [0,4],
which is also the true top. In fact, this approxima-
tion is correct at every time step shown in Figure 1.
Keep this approximation in mind, as we return to it
in Sections 3.2 and 4.3.
We do not use a shift-reduce parser that consumes
source words from right-to-left;1 therefore, we ap-
ply the above approximation to handle the right-to-
left HRM. Before doing so, we re-interpret the de-
coder state to simulate a right-to-left decoder. The
last block becomes [si, ti] and the next block be-
comes [si?1, ti?1], and the coverage vector is in-
verted so that covered words become uncovered and
vice versa. Taken all together, the approximate test
for right-to-left adjacency checks that any gap be-
tween [si?1, ti?1] and [si, ti] is uncovered in the
original coverage vector.2 Figure 2 illustrates how a
monotone right-to-left orientation can be (correctly)
determined for [2, 4] after placing [5, 6] in Figure 1.
Statistics for the HRM can be extracted from
word-aligned training data. Galley and Manning
(2008) propose an algorithm that begins by run-
1This would require a second, right-to-left decoding pass.
2Galley and Manning (2008) present an under-specified ap-
proximation that is consistent with what we present here.
Prev	 ?
2	 ? 4	 ? 5	 ? 7	 ?6	 ?0	 ?
Next	 ?
Coverage	 ?/	 ?Approx	 ?Top	 ?
Next	 ?
2	 ? 4	 ? 5	 ? 7	 ?6	 ?0	 ?
Prev	 ?
Cov	 ?/	 ?Approx	 ?Top	 ?
Le?-??to-??Right	 ?(Disjoint	 ?[5,6])	 ?
Implied	 ?Right-??to-??Le?	 ?(Monotone	 ?[2,4])	 ?
Figure 2: Illustration of the coverage-vector stack ap-
proximation, as applied to right-to-left HRM orientation.
Phrase	 ?Sou
rce	 ?
Target	 ?
??M	 ?
??S	 ? ??M	 ?
??S	 ?
Figure 3: Relevant corners in HRM extraction. ? indi-
cates left-to-right orientation, and? right-to-left.
ning standard phrase extraction (Och and Ney, 2004)
without a phrase-length limit, noting the corners of
each phrase found. Next, the left-to-right and right-
to-left orientation for each phrase of interest (those
within the phrase-length limit) can be determined by
checking to see if any corners noted in the previous
step are adjacent, as shown in Figure 3.
2.1 Efficient Extraction of HRM statistics
The time complexity of phrase extraction is bounded
by the number of phrases to be extracted, which is
determined by the sparsity of the input word align-
ment. Without a limit on phrase length, a sentence
pair with nwords in each language can have as many
as O(n4) phrase-pairs.3 Because it relies on unre-
stricted phrase extraction, the corner collection step
for determining HRM orientation is also O(n4).
By leveraging the fact that the first step col-
lects corners, not phrase-pairs, we can show that
HRM extraction can actually be done inO(n2) time,
through a process we call corner propagation. In-
stead of running unrestricted phrase-extraction, cor-
ner propagation begins by extracting all minimal
3Consider a word-alignment with only one link in the center
of the grid.
202
So
urc
e	 ?
Target	 ?
??M	 ?
??S	 ? ??M	 ?
??S	 ?
??S	 ???M	 ?
??M	 ???S	 ?
Figure 4: Corner Propagation: Each of the four passes
propagates two types of corners along a single dimension.
phrase-pairs; that is, those that do not include un-
aligned words at their boundaries. The complex-
ity of this step is O(n2), as the number of mini-
mal phrases is bounded by the minimum of the num-
ber of monolingual phrases in either language. We
note corners for each minimal pair, as in the orig-
inal HRM extractor. We then carry out four non-
nested propagation steps to handle unaligned words,
traversing the source (target) in forward and reverse
order, with each unaligned row (column) copying
corners from the previous row (column). Each pass
takes O(n2) time, for a total complexity of O(n2).
This process is analogous to the growing step in
phrase extraction, but computational complexity is
minimized because each corner is considered inde-
pendently. Pseudo-code is provided in Algorithm 1,
and the propagation step is diagrammed in Fig-
ure 4. In our implementation, corner propagation is
roughly two-times faster than running unrestricted
phrase-extraction to collect corners.
Note that the trickiest corners to catch are those
that are diagonally separated from their minimal
block (they result from unaligned growth in both
the source and target). These cases are handled cor-
rectly because each corner type is touched by two
propagators, one for the source and one for the tar-
get (see Figure 4). For example, the top-right-corner
array Aq is populated by both propagate-right and
propagate-up. Thus, one propagator can copy a cor-
ner along one dimension, while the next propagator
copies the copies along the other dimension, moving
the original corner diagonally.
Algorithm 1 Corner Propagation
Initialize target-source indexed binary arrays
Aq[m][n], Ay[m][n], Ap[m][n] and Ax[m][n] to
record corners found in minimal phrase-pairs.
{Propagate Right}
for i from 2 to m s.t. target [i] is unaligned do
for j from 1 to n do
Aq[i][j] = True if Aq[i? 1][j] is True
Ay[i][j] = True if Ay[i? 1][j] is True
{Propagate Up}
for j from 2 to n s.t. source[j] is unaligned do
for i from 1 to m do
Ap[i][j] = True if Ap[i][j ? 1] is True
Aq[i][j] = True if Aq[i][j ? 1] is True
{Propagate Left and Down are similar}
return Aq, Ay, Ap and Ax
3 ITG-Constrained Decoding
Phrase-based decoding places no implicit limits on
re-ordering; all n! permutations are theoretically
possible. This is undesirable, as it leads to in-
tractability (Knight, 1999). Therefore, re-ordering is
limited explicitly, typically using a distortion limit.
One particularly well-studied re-ordering constraint
is the ITG constraint, which limits source permu-
tations to those achievable by a binary bracketing
synchronous context-free grammar (Wu, 1997). ITG
constraints are known to stop permutations that gen-
eralize 3142 and 2413,4 and can drastically limit the
re-ordering space for long strings (Zens and Ney,
2003). There are two methods to incorporate ITG
constraints into a phrase-based decoder, one using
the coverage vector (Zens et al, 2004), and the
other using a shift-reduce parser (Feng et al, 2010).
We begin with the latter, returning to the coverage-
vector constraint later in this section.
Feng et al (2010) describe an ITG constraint that
is implemented using the same permutation parser
used in the HRM. To understand their method, it is
important to note that the set of ITG-compliant per-
mutations is exactly the same as those that can be
reduced to a single-item stack using the shift-reduce
permutation parser (Zhang and Gildea, 2007). In
fact, this manner of parsing was introduced to SMT
42413 is shorthand notation that denotes the block sequence
[1,2],[3,4],[0,1],[2,3] as diagrammed in Figure 5a.
203
Sou
rce	 ?
Target	 ?
0[1,2]4	 ?
[2,3]	 ?
[0,1]	 ?
2[3,4]4	 ?
0[1,2]5	 ?
2[2,3]4	 ?
[0,1]	 ?
[3,4]	 ?
Sou
rce	 ?
2[4,5]5	 ?
Target	 ?(a)	 ? (b)	 ?
Figure 5: Two non-ITG permutations. Violations of po-
tential adjacency are indicated with dotted spans. Bounds
for the one-stack constraint are shown as subscripts.
in order to binarize synchronous grammar produc-
tions (Zhang et al, 2006). Therefore, enforcing
an ITG constraint in the presence of a shift-reduce
parser amounts to ensuring that every shifted item
can eventually be reduced. To discuss this con-
straint, we introduce a notion of potential adjacency,
where two blocks are potentially adjacent if any
words separating them have not yet been covered.
Formally, blocks [s, t] and [s?, t?] are potentially ad-
jacent iff one of the following conditions holds:
? they are adjacent (t? = s or t = s?)
? t? < s and [t?, s] is uncovered
? t < s? and [t, s?] is uncovered
Recall that a reduction occurs when the top two
items of the stack are adjacent. To ensure that re-
ductions remain possible, we only shift items onto
the stack that are potentially adjacent to the cur-
rent top. Figure 5 diagrams two non-ITG permu-
tations and highlights where potential adjacency is
violated. Note that no reductions occur in either
of these examples; therefore, each block [si, ti] is
also the top of the stack at time i. Potential ad-
jacency can be confirmed with some overhead us-
ing the stack and coverage vector together, but Feng
et al (2010) present an elegant three-stack solution
that provides potentially adjacent regions in constant
time, without a coverage vector. We improve upon
their method later this section. From this point on,
we abbreviate potential adjacency as PA.
We briefly sketch a proof that maintaining po-
tential adjacency maintains reducibility, by showing
that non-PA shifts produce irreducible stacks, and
that PA shifts are reducible. It is easy to see that ev-
ery non-PA shift leads to an irreducible stack. Let
[s?, t?] be an item to be shifted onto the stack, and
[s, t] be the current top. Assume that t? < s and the
two items are not PA (the case where t < s? is simi-
lar). Because they are not PA, there is some index k
in [t?, s] that has been previously covered. Since it is
covered, k exists somewhere in the stack, buried be-
neath [s, t]. Because k cannot be re-used, no series
of additional shift and reduce operations can extend
[s?, t?] so that it becomes adjacent to [s, t]. Therefore,
[s, t] will never participate in a reduction, and pars-
ing will close with at least two items on the stack.
Similarly, one can easily show that every PA shift is
reducible, because the uncovered space [t?, s] can be
filled by extending the new top toward the previous
top using strictly adjacent shifts.
3.1 A One-stack ITG Constraint
As mentioned earlier, Feng et al (2010) provide a
method to track potential adjacency that does not re-
quire a coverage vector. Instead, they maintain three
stacks, the original stack and two others to track po-
tentially adjacent regions to the left and right respec-
tively. These regions become available to the de-
coder only when the top of the original stack is ad-
jacent to one of the adjacency stacks.
We show that the same goal can be achieved with
even less book-keeping by augmenting the items on
the original stack to track the regions of potential
adjacency around them. The intuition behind this
technique is that on a shift, the new top inherits all
of the constraints on the old top, and the old top be-
comes a constraint itself. Each stack item now has
four fields, the original block [s, t], plus a left and
right adjacency bound, denoted together as `[s, t]r,
where ` and r are indices for the maximal span con-
taining [s, t] that is uncovered except for [s, t]. If the
top of the stack is `[s, t]r, then shifted items must fall
inside one of the two PA regions, [`, s] or [t, r]. The
region shifted into determines new item?s bounds.
The stack is initialized with a special 0[0, 0]n item,
and we then shift unannotated blocks onto the stack.
As we shift [s?, t?] onto the stack, rules derive bounds
`? and r? for the new top based on the old top `[s, t]r:
? Shift-left (t? ? s): `? = `, r? = s
? Shift-right (t ? s?): `? = t, r? = r
204
[2,4]	 ?
[5,7]	 ?
0	 ? 9	 ?
Shi?	 ?[5,7]	 ?
4	 ? 9	 ?
[2,4]	 ? 9	 ?0	 ?
[2,7]	 ?
[4,7]	 ?
0	 ? 9	 ?
Reduce	 ?
4	 ? 9	 ?
[2,4]	 ? 9	 ?0	 ?
(a)	 ? (b)	 ?
Figure 6: Two examples of boundaries for the one-stack solution for potential adjacency. Stacks are built from bottom
to top, blocks indicate [s,t] blocks, while tails are left and right adjacency boundaries.
Meanwhile, when reducing a stack with `? [s
?, t?]r?
at the top and `[s, t]r below it, the new top simply
copies ` and r. The merged item is larger than [s, t],
but it is PA to the same regions. Figure 6 diagrams
a shift-right and a reduce, while Figure 5 annotates
bounds for blocks during its ITG violations.
3.2 The Coverage-Vector ITG Constraint is
Incomplete
The stack-based solution for ITG constraints is el-
egant, but there is also a proposed constraint that
uses only the coverage vector (Zens et al, 2004).
This constraint can be stated with one simple rule:
if the previously translated block is [si?1, ti?1] and
the next block to be translated is [si, ti], one must
be able to travel along the coverage vector from
[si?1, ti?1] to [si, ti] without transitioning from an
uncovered word to a covered word. Feng et al
(2010) compare the two ITG constraints, and show
that they perform similarly, but not identically. They
attribute the discrepancy to differences in when the
constraints are applied, which is strange, as the two
constraints need not be timed differently.
Let us examine the coverage-vector constraint
more carefully, assuming that ti < si?1 (the case
where ti?1 < si is similar). The constraint consists
of two phases: first, starting from si?1, we travel to
the left toward ti, consuming covered words until we
reach the first uncovered word. We then enter into
the second phase, and the path must remain uncov-
ered until we reach ti. The first step over covered
positions corresponds to finding the left boundary
of the largest covered block containing [si?1, ti?1],
which is an approximation to the top of the stack
(Section 2). The second step over uncovered posi-
tions corresponds to determining whether [si, ti] is
PA to the approximate top. That is, the coverage-
vector ITG constraint checks for potential adjacency
using the same top-of-stack approximation as the
right-to-left HRM.
This implicit approximation implies that there
may well be cases where the coverage-vector con-
straint makes the wrong decision. Indeed this is
the case, which we prove by example. Consider
the irreducible sequence 25314, illustrated in Fig-
ure 5b. This non-ITG permutation is allowed by
the coverage-vector approximation, but not by the
stack-based constraint. Both constraints allow the
placement of the first three blocks [1, 2], [4, 5] and
[2, 3]. After adding [0, 1], the stack-based solution
detects a PA-violation. Meanwhile, the vector-based
solution checks the path from 2 to 1 for a transition
from uncovered to covered. This short path touches
only covered words. Similarly, as we add [3, 4], the
path from 1 to 3 is also completely covered. The
entire permutation is accepted without complaint.
The proof provided by Zens et al (2004) misses
this case, as it accounts for phrasal generalizations
of the 2413 ITG-forbidden substructure, but it does
not account for generalizations where the substruc-
ture is interrupted by a discontiguous item, such as
in 25{3}14, where 2413 is revealed not by merging
items but by deleting 3.
4 Inconsistencies in HRM parsing
We have shown that the HRM and the ITG con-
straints for phrase-based decoding use the same de-
terministic shift-reduce parser. The entirety of the
ITG discussion was devoted to preventing the parser
from reaching an irreducible state. However, up
until now, work on the HRM has not addressed
the question of irreducibility (Galley and Manning,
2008; Nguyen et al, 2009).
Irreducible derivations do occur during HRM de-
coding, and when they do, they can create inconsis-
tencies with respect to HRM extraction from word-
205
?  
?  
??
  ?
?  
?  
??
  ?
?  
??
 	 ??	 ?
[4,6]	 ?How	 ?can	 ?
[0,1]	 ?you	 ?
[6,7]	 ?achieve	 ?
[1,2]	 ?the	 ?
[3,4]	 ?economic	 ?and	 ?
[2,3]	 ?tourism	 ?
[7,9]	 ?benefits	 ??	 ?
Figure 7: An example irreducible derivation, drawn from
our Chinese-to-English decoder?s k-best output.
Last translated block 2-red *-red approx
How can [4, 6] [4,6] [4,6] [4,6]
you [0, 1] [0,1] [0,1] [0,1]
achieve [6, 7] [6,7] [6,7] [4,7]
the [1, 2] [1,2] [1,2] [0,2]
economic and [3, 4] [3,4] [3,4] [3,7]
tourism [2, 3] [1,4] [0,7] [0,7]
benefits? [7, 9] [7,9] [0,9] [0,9]
Table 2: Top of stack at each time step in Figure 7, under
2-reduction (as in the original HRM), *-reduction, and
the coverage-vector approximation.
aligned training data. In Figure 7, we show an ir-
reducible block sequence, extracted from a Chinese-
English decoder. The parser can perform a few small
reductions, creating a [1,4] block indicated with a
dashed box, but translation closes with 5 items on
the stack. One can see that [7,9] is assigned a dis-
joint orientation by the HRM. However, if the same
translation and alignment were seen during train-
ing, the unrestricted phrase extractor would find a
phrase at [0,7], indicated with a dotted box, and [7,9]
would be assigned monotone orientation. This in-
consistency penalizes this derivation, as ?benefits ??
is forced into an unlikely disjoint orientation. One
potential implication is that the decoder will tend
to avoid irreducible states, as those states will tend
to force unlikely orientations, resulting in a hidden,
soft ITG-constraint. Indeed, our decoder does not
select this hypothesis, but instead a (worse) transla-
tion that is fully reducible. The impact of these in-
consistencies on translation quality can only be de-
termined empirically. However, to do so, we require
alternatives that address these inconsistencies. We
describe three such variants below.
4.1 ITG-constrained decoding
Perhaps the most obvious way to address irreducible
states is to activate ITG constraints whenever decod-
ing with an HRM. Irreducible derivations will disap-
pear from the decoder, along with the corresponding
inconsistencies in orientation. Since both techniques
require the same parser, there is very little overhead.
However, we will have also limited our decoder?s re-
ordering capabilities.
4.2 Unrestricted shift-reduce parsing
The deterministic shift-reduce parser used through-
out this paper is actually a special case of a general
class of permutation parsers, much in the same way
that a binary ITG is a special case of synchronous
context-free grammar. Zhang and Gildea (2007) de-
scribe a family of k-reducing permutation parsers,
which can reduce the top k items of the stack in-
stead of the top 2. For k ? 2 we can generalize the
adjacency requirement for reduction to a permuta-
tion requirement. Let {[si, ti]|i=1. . . k} be the top k
items of a stack; they are a permutation iff:
max
i
(ti)?min
i
(si) =
?
i
[ti ? si]
That is, every number between the max and min is
present somewhere in the set. Since two adjacent
items always fulfill this property, we know the orig-
inal parser is 2-reducing. k-reducing parsers reduce
by moving progressively deeper in the stack, looking
for the smallest 2 ? i ? k that satisfies the permu-
tation property (see Algorithm 2). As in the original
parser, a k-reduction is performed every time the top
of the stack changes; that is, after each shift and each
successful reduction.
If we set k = ?, the parser will find the small-
est possible reduction without restriction; we refer
to this as a *-reducing parser. This parser will never
reach an irreducible state. In the worst case, it re-
duces the entire permutation as a single n-reduction
after the last shift. This means it will exactly mimic
unrestricted phrase-extraction when predicting ori-
entations, eliminating inconsistencies without re-
stricting our re-ordering space. The disadvantage is
206
Algorithm 2 k-reduce a stack
input stack {[si, ti]|i = 1 . . . l}; i = 1 is the top
input max reduction size k, k ? 2
set s? = s1; t? = t1; size = t1 ? s1
for i from 2 to min(k, l) do
set s? = min(s?, si); t? = max(t?, ti)
set size = size + (ti ? si)
if t? ? s? == size then
pop {[sj , tj ]|j = 1 . . . i} from the stack
push [s?, t?] onto the stack;
return true // successful reduction
return false // failed to reduce
that reduction is no longer a constant-time operation,
but is insteadO(n) in the worst case (consider Algo-
rithm 2 with k =? and l = n items on the stack).5
As a result, we will carefully track the impact of this
parser on decoding speed.
4.3 Coverage vector approximation
One final option is to adopt the top-of-stack approxi-
mation for left-to-right orientations, in addition to its
current use for right-to-left orientations, eliminating
the need for any permutation parser. The next block
[si, ti] is adjacent to the approximate top of the stack
only if any space between [si, ti] and the previous
block [si?1, ti?1] is covered. But before committing
fully to this approximation, we should better under-
stand it. Thus far, we have implied that this approx-
imation can fail to predict correct orientations, but
we have not specified when these failures occur. We
now show that incorrect orientations can only occur
while producing a non-ITG permutation.
Let [si?1, ti?1] be the last translated block, and
[si, ti] be the next block. Recall that the approxima-
tion determines the top of the stack using the largest
block of covered words that contains [si?1, ti?1].
The approximate top always contains the true top,
because they both contain [si?1, ti?1] and the ap-
proximate top is the largest block that does so.
Therefore, the approximation errs on the side of ad-
jacency, meaning it can only make mistakes when
5Zhang and Gildea (2007) provide an efficient algorithm for
*-reduction that uses additional book-keeping so that the num-
ber of permutation checks as one traverses the entire sequence
is linear in aggregate; however, we implement the simpler, less
efficient version here to simplify decoder integration.
Prev	 ? Next	 ?
si-??1	 ? ti-??1	 ? si	 ? ti	 ?t?	 ?
True	 ?top	 ?
Approximate	 ?top	 ?
Breaks	 ?
PA	 ?
Figure 8: Indices for when the coverage approximation
predicts a false M.
assigning an M or S orientation; if it assigns a D, it
is always correct. Let us consider the false M case
(the false S case is similar). If we assign a false M,
then ti?1 < si and si is adjacent to the approximate
top; therefore, all positions between ti?1 and si are
covered. However, since the M is false, the true top
of the stack must end at some t? : ti?1 ? t? < si.
Since we know that every position between t? and si
is covered, [si, ti] cannot be PA to the true top of the
stack, and we must be in the midst of making a non-
ITG permutation. See Figure 8 for an illustration of
the various indices involved. As it turns out, both the
approximation and the 2-reducing parser assign in-
correct orientations only in the presence of ITG vio-
lations. However, the approximation may be prefer-
able, as it requires only a coverage vector.
4.4 Qualitative comparison
Each solution manages its stack differently, and we
illustrate the differences in terms of the top of the
stack at time i in Table 2. The *-reducing parser is
the gold standard, so we highlight deviations from
its decisions in bold. As one can see, the original 2-
reducing parser does fine before and during an ITG
violation, but can create false disjoint orientations
after the violation is complete, as the top of its stack
becomes too small due to missing reductions. Con-
versely, the coverage-vector approximation makes
errors inside the violation: the approximate top be-
comes too large, potentially creating false monotone
or swap orientations. Once the violation is complete,
it recovers nicely.
5 Experiments
We compare the LRM, the HRM and the three HRM
variants suggested in Section 4 on a Chinese-to-
English translation task. We measure the impact on
translation quality in terms of BLEU score (Papineni
et al, 2002), as well as the impact on permutation
207
BLEU NIST 08 Complexity Counts Speed
Method nist04 nist06 nist08 > 2 4 5 6 7 ? 8 sec/sent
LRM 38.00 33.79 27.12 241 146 40 32 12 11 3.187
HRM 2-red 38.53 34.20 27.57 176 113 31 20 8 4 3.353
HRM apprx 38.58 34.09 27.60 280 198 41 26 13 2 3.231
HRM *-red 38.39 34.22 27.41 328 189 71 34 20 14 3.585
HRM itg 38.70 34.26 27.33 0 0 0 0 0 0 3.274
Table 3: Chinese-to-English translation results, comparing the LRM and 4 HRM variants: the original 2-reducing
parser, the coverage vector approximation, the *-reducing parser, and an ITG-constrained decoder.
complexity, as measured by the largest k required to
k-reduce the translations.
5.1 Data
The system was trained on data from the NIST 2009
Chinese MT evaluation, consisting of more than
10M sentence pairs. The training corpora were split
into two phrase tables, one for Hong Kong and UN
data, and one for all other data. The dev set was
taken from the NIST 05 evaluation set, augmented
with some material reserved from other NIST cor-
pora; it consists of 1.5K sentence pairs. The NIST
04, 06, and 08 evaluation sets were used for testing.
5.2 System
We use a phrase-based translation system similar to
Moses (Koehn et al, 2007). In addition to our 8
translation model features (4 for each phrase table),
we have a distortion penalty incorporating the min-
imum possible completion cost described by Moore
and Quirk (2007), a length penalty, a 5-gram lan-
guage model trained on the NIST09 Gigaword cor-
pus, and a 4-gram language model trained on the tar-
get half of the parallel corpus. The LRM and HRM
are represented with six features, with separate
weights for M, S and D in both directions (Koehn et
al., 2007). We employ a gap constraint as our only
distortion limit (Chang and Collins, 2011). This re-
stricts the maximum distance between the start of a
phrase and the earliest uncovered word, and is set to
7 words. Parameters are tuned using a batch-lattice
version of hope-fear MIRA (Chiang et al, 2008;
Cherry and Foster, 2012). We re-tune parameters
for each variant.
5.3 Results
Our results are summarized in Table 3. Speed and
complexity are measured on the NIST08 test set,
which has 1357 sentences. We measure permutation
complexity by parsing the one-best derivations from
each system with an external *-reducing parser, and
noting the largest k-reduction for each derivation.
Therefore, the>2 column counts the number of non-
ITG derivations produced by each system.
Regarding quality, we have verified the effective-
ness of the HRM: each HRM variant outperforms
the LRM, with the 2-reducing HRM doing so by 0.4
BLEU points on average. Unlike Feng et al (2010),
we see no consistent benefit from adding hard ITG
constraints, perhaps because we are building on an
HRM-enabled system. In fact, all HRM variants
perform more or less the same, with no clear win-
ner emerging. Interestingly, the approximate HRM
is included in this pack, which implies that groups
wishing to augment their phrase-based decoder with
an HRM need not incorporate a shift-reduce parser.
Regarding complexity, the 2-reducing HRM pro-
duces about half as many non-ITG derivations as the
*-reducing system, confirming our hypothesis that
a 2-reducing HRM acts as a sort of soft ITG con-
straint. Both the approximate and *-reducing de-
coders produce more violating derivations than the
LRM. This is likely due to their encouragement of
more movement overall. The largest reduction we
observed was k = 11.
Our speed tests show that all of the systems trans-
late at roughly the same speed, with the LRM being
fastest and the *-reducing HRM being slowest. The
*-reducing system is less than 7% slower than the 2-
reducing system, alleviating our concerns regarding
the cost of *-reduction.
208
6 Discussion
We have presented a number of theoretical contribu-
tions on the topic of phrase-based decoding with an
on-board permutation parser. In particular, we have
shown that the coverage-vector ITG constraint is ac-
tually incomplete, and that the original HRM can
produce inconsistent orientations in the presence of
ITG violations. We have presented three HRM vari-
ants that address these inconsistencies, and we have
compared them in terms of both translation quality
and permutation complexity. Though our results in-
dicate that a permutation parser is actually unneces-
sary to reap the benefits of hierarchical re-ordering,
we are excited about the prospects of further ex-
ploring the information provided by these on-board
parsers. In particular, we are interested in using fea-
tures borrowed from transition-based parsing while
decoding.
References
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through la-
grangian relaxation. In EMNLP, pages 26?37, Edin-
burgh, Scotland, UK., July.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In HLT-
NAACL, Montreal, Canada, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP, pages 224?233.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrase-
based machine translation. In COLING, pages 285?
293, Beijing, China, August.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP, pages 848?856, Honolulu, Hawaii,
October.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
HLT-NAACL, pages 966?974, Los Angeles, Califor-
nia, June.
Kevin Knight. 1999. Squibs and discussions: Decod-
ing complexity in word-replacement translation mod-
els. Computational Linguistics, 25(4):607?615, De-
cember.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL,
pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180, Prague, Czech Republic, June.
Robert C. Moore and Chris Quirk. 2007. Faster beam-
search decoding for phrasal statistical machine trans-
lation. In MT Summit XI, September.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,
and Thai Phuong Nguyen. 2009. Improving a lexi-
calized hierarchical reordering model using maximum
entropy. In MT Summit XII, Ottawa, Canada, August.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4), December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL, pages 311?318.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In HLT-NAACL,
pages 101?104, Boston, USA, May.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL, pages 144?151.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In COL-
ING, pages 205?211, Geneva, Switzerland, August.
Hao Zhang and Daniel Gildea. 2007. Factorization
of synchronous context-free grammars in linear time.
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 25?32, Rochester, New York, April.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In HLT-NAACL, pages 256?263, New
York City, USA, June.
209
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362?367,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
A Systematic Comparison of Smoothing Techniques for Sentence-Level
BLEU
Boxing Chen and Colin Cherry
National Research Council Canada
first.last@nrc-cnrc.gc.ca
Abstract
BLEU is the de facto standard machine
translation (MT) evaluation metric. How-
ever, because BLEU computes a geo-
metric mean of n-gram precisions, it of-
ten correlates poorly with human judg-
ment on the sentence-level. There-
fore, several smoothing techniques have
been proposed. This paper systemati-
cally compares 7 smoothing techniques
for sentence-level BLEU. Three of them
are first proposed in this paper, and they
correlate better with human judgments on
the sentence-level than other smoothing
techniques. Moreover, we also compare
the performance of using the 7 smoothing
techniques in statistical machine transla-
tion tuning.
1 Introduction
Since its invention, BLEU (Papineni et al., 2002)
has been the most widely used metric for both
machine translation (MT) evaluation and tuning.
Many other metrics correlate better with human
judgments of translation quality than BLEU, as
shown in recent WMT Evaluation Task reports
(Callison-Burch et al., 2011; Callison-Burch et al.,
2012). However, BLEU remains the de facto stan-
dard evaluation and tuning metric. This is proba-
bly due to the following facts:
1. BLEU is language independent (except for
word segmentation decisions).
2. BLEU can be computed quickly. This is im-
portant when choosing a tuning metric.
3. BLEU seems to be the best tuning metric
from a quality point of view - i.e., models
trained using BLEU obtain the highest scores
from humans and even from other metrics
(Cer et al., 2010).
One of the main criticisms of BLEU is that it
has a poor correlation with human judgments on
the sentence-level. Because it computes a geomet-
ric mean of n-gram precisions, if a higher order
n-gram precision (eg. n = 4) of a sentence is
0, then the BLEU score of the entire sentence is
0, no matter how many 1-grams or 2-grams are
matched. Therefore, several smoothing techniques
for sentence-level BLEU have been proposed (Lin
and Och, 2004; Gao and He, 2013).
In this paper, we systematically compare 7
smoothing techniques for sentence-level BLEU.
Three of them are first proposed in this paper, and
they correlate better with human judgments on the
sentence-level than other smoothing techniques on
the WMT metrics task. Moreover, we compare
the performance of using the 7 smoothing tech-
niques in statistical machine translation tuning on
NIST Chinese-to-English and Arabic-to-English
tasks. We show that when tuning optimizes the
expected sum of these sentence-level metrics (as
advocated by Cherry and Foster (2012) and Gao
and He (2013) among others), all of these metrics
perform similarly in terms of their ability to pro-
duce strong BLEU scores on a held-out test set.
2 BLEU and smoothing
2.1 BLEU
Suppose we have a translation T and its reference
R, BLEU is computed with precision P (N,T,R)
and brevity penalty BP(T,R):
BLEU(N,T,R) = P (N,T,R)? BP(T,R) (1)
where P (N,T,R) is the geometric mean of n-
gram precisions:
P (N,T,R) =
(
N
?
n=1
p
n
)
1
N
(2)
362
and where:
p
n
=
m
n
l
n
(3)
m
n
is the number of matched n-grams between
translation T and its referenceR, and l
n
is the total
number of n-grams in the translation T . BLEU?s
brevity penalty punishes the score if the translation
length len(T ) is shorter than the reference length
len(R), using this equation:
BP(T,R) = min
(
1.0, exp
(
1?
len(R)
len(T )
))
(4)
2.2 Smoothing techniques
The original BLEU was designed for the
document-level; as such, it required no smooth-
ing, as some sentence would have at least one 4-
gram match. We now describe 7 smoothing tech-
niques that work better for sentence-level evalua-
tion. Suppose we consider matching n-grams for
n = 1 . . . N (typically, N = 4). Let m
n
be the
original match count, and m
?
n
be the modified n-
gram match count.
Smoothing 1: if the number of matched n-
grams is 0, we use a small positive value ? to re-
place the 0 for n ranging from 1 toN . The number
? is set empirically.
m
?
n
= ?, if m
n
= 0. (5)
Smoothing 2: this smoothing technique was
proposed in (Lin and Och, 2004). It adds 1 to the
matched n-gram count and the total n-gram count
for n ranging from 2 to N .
m
?
n
= m
n
+ 1, for n in 2 . . . N, (6)
l
?
n
= l
n
+ 1, for n in 2 . . . N. (7)
Smoothing 3: this smoothing technique is im-
plemented in the NIST official BLEU toolkit
mteval-v13a.pl.
1
The algorithm is given below. It
assigns a geometric sequence starting from 1/2 to
the n-grams with 0 matches.
1. invcnt = 1
2. for n in 1 to N
3. if m
n
= 0
4. invcnt = invcnt? 2
5. m
?
n
= 1/invcnt
6. endif
7. endfor
1
available at http://www.itl.nist.gov/iad/mig/tests/mt/2009/
Smoothing 4: this smoothing technique is novel
to this paper. We modify Smoothing 3 to address
the concern that shorter translations may have in-
flated precision values due to having smaller de-
nominators; therefore, we give them proportion-
ally smaller smoothed counts. Instead of scaling
invcnt with a fixed value of 2, we replace line 4 in
Smoothing 3?s algorithm with Equation 8 below.
invcnt = invcnt?
K
ln(len(T ))
(8)
It assigns larger values to invcnt for shorter sen-
tences, resulting in a smaller smoothed count. K
is set empirically.
Smoothing 5: this smoothing technique is also
novel to this paper. It is inspired by the intuition
that matched counts for similar values of n should
be similar. To a calculate the n-gram matched
count, it averages the n ? 1, n and n + 1 ?gram
matched counts. We define m
?
0
= m
1
+ 1, and
calculate m
?
n
for n > 0 as follows:
m
?
n
=
m
?
n?1
+ m
n
+ m
n+1
3
(9)
Smoothing 6: this smoothing technique was
proposed in (Gao and He, 2013). It interpolates
the maximum likelihood estimate of the precision
p
n
with a prior estimate p
0
n
. The prior is estimated
by assuming that the ratio between p
n
and p
n?1
will be the same as that between p
n?1
and p
n?2
.
Formally, the precisions of lower order n-grams,
i.e., p
1
and p
2
, are not smoothed, while the pre-
cisions of higher order n-grams, i.e. n > 2, are
smoothed as follows:
p
n
=
m
n
+ ?p
0
n
l
n
+ ?
(10)
where ? is set empirically, and p
0
n
is computed as
p
0
n
= p
n?1
?
p
n?1
p
n?2
(11)
Smoothing 7: this novel smoothing technique
combines smoothing 4 and smoothing 5. That is,
we first compute a smoothed count for those 0
matched n-gram counts using Smoothing 4, and
then take the average of three counts to set the fi-
nal matched n-gram count as in Equation 9.
3 Experiments
We carried out two series of experiments. The
7 smoothing techniques were first compared in
363
set year lang. #system #seg. pair
dev 2008 xx-en 43 7,804
test1 2012 xx-en 49 34,909
test2 2013 xx-en 94 281,666
test3 2012 en-xx 54 47,875
test4 2013 en-xx 95 220,808
Table 1: Statistics of the WMT dev and test sets.
the metric task as evaluation metrics, then they
were compared as metrics for tuning SMT systems
to maximize the sum of expected sentence-level
BLEU scores.
3.1 Evaluation task
We first compare the correlations with human
judgment for the 7 smoothing techniques onWMT
data; the development set (dev) is the WMT 2008
all-to-English data; the test sets are theWMT 2012
and WMT 2013 all-to-English, and English-to-all
submissions. The languages ?all? (?xx? in Ta-
ble 1) include French, Spanish, German, Czech
and Russian. Table 1 summarizes the dev/test set
statistics.
Following WMT 2013?s metric task (Mach?a?cek
and Bojar, 2013), for the segment level, we use
Kendall?s rank correlation coefficient ? to measure
the correlation with human judgment:
? =
#concordant-pairs?#discordant-pairs
#concordant-pairs + #discordant-pairs
(12)
We extract all pairwise comparisons where one
system?s translation of a particular segment was
judged to be better than the other system?s trans-
lation, i.e., we removed all tied human judg-
ments for a particular segment. If two transla-
tions for a particular segment are assigned the
same BLEU score, then the #concordant-pairs
and #discordant-pairs both get a half count. In
this way, we can keep the number of total pairs
consistent for all different smoothing techniques.
For the system-level, we used Spearman?s rank
correlation coefficient ? and Pearson?s correla-
tion coefficient ? to measure the correlation of
the metric with human judgments of translation.
If we compute document-level BLEU as usual,
all 7 smoothing techniques actually get the same
result, as document-level BLEU does not need
smoothing. We therefore compute the document-
level BLEU as the weighted average of sentence-
level BLEU, with the weights being the reference
Into-English
smooth seg ? sys ? sys ?
crp ? 0.720 0.887
0 0.165 0.759 0.887
1 0.224 0.760 0.887
2 0.226 0.757 0.887
3 0.224 0.760 0.887
4 0.228 0.763 0.887
5 0.234 0.765 0.887
6 0.230 0.754 0.887
7 0.236 0.766 0.887
Table 2: Correlations with human judgment on
WMT data for Into-English task. Results are av-
eraged on 4 test sets. ?crp? is the origianl IBM
corpus-level BLEU.
lengths:
BLEU
d
=
?
D
i=1
len(R
i
)BLEU
i
?
D
i=1
len(R
i
)
(13)
where BLEU
i
is the BLEU score of sentence i,
and D is the size of the document in sentences.
We first set the free parameters of each smooth-
ing method by grid search to optimize the
sentence-level score on the dev set. We set ? to 0.1
for Smoothing 1; K = 5 for Smoothing 4; ? = 5
for Smoothing 6.
Tables 2 and 3 report our results on the met-
rics task. We compared the 7 smoothing tech-
niques described in Section 2.2 to a baseline with
no smoothing (Smoothing 0). All scores match n-
grams n = 1 to 4. Smoothing 3 is implemented
in the standard official NIST evaluation toolkit
(mteval-v13a.pl). Results are averaged across the
4 test sets.
All smoothing techniques improved sentence-
level correlations (? ) over no smoothing. Smooth-
ing method 7 got the best sentence-level results on
both the Into-English and Out-of-English tasks.
On the system-level, our weighted average of
sentence-level BLEU scores (see Equation 13)
achieved a better correlation with human judge-
ment than the original IBM corpus-level BLEU.
However, the choice of which smoothing tech-
nique is used in the average did not make a very
big difference; in particular, the system-level rank
correlation ? did not change for 13 out of 14 cases.
These methods help when comparing one hypoth-
esis to another, but taken as a part of a larger aver-
age, all seven methods assign relatively low scores
364
Out-of-English
smooth seg ? sys ? sys ?
crp ? 0.712 0.744
0 0.119 0.715 0.744
1 0.178 0.722 0.748
2 0.180 0.725 0.744
3 0.178 0.724 0.744
4 0.181 0.727 0.744
5 0.184 0.731 0.744
6 0.182 0.725 0.744
7 0.187 0.734 0.744
Table 3: Correlations with human judgment on
WMT data for Out-of-English task. Results are
averaged on 4 test sets. ?crp? is the origianl IBM
corpus-level BLEU.
to the cases that require smoothing, resulting in
similar system-level rankings.
3.2 Tuning task
In this section, we explore the various BLEU
smoothing methods in the context of SMT param-
eter tuning, which is used to set the decoder?s
linear model weights w. In particular, we use
a tuning method that maximizes the sum of ex-
pected sentence-level BLEU scores, which has
been shown to be a simple and effective method
for tuning with large feature sets by both Cherry
and Foster (2012) and Gao and He (2013), but
which requires a smoothed sentence-level BLEU
approximation. For a source sentence f
i
, the prob-
ability of the k
th
translation hypothesis e
k
i
is its ex-
ponentiated and normalized model score:
P
w
(e
k
i
|f
i
) =
exp(score
w
(e
k
i
, f
i
))
?
k
?
exp(score
w
(e
k
?
i
, f
i
))
where k
?
ranges over all hypotheses in a K-best
list.
2
We then use stochastic gradient descent
(SGD) to minimize:
?||w||
2
?
?
i
[
len(R
i
)? E
P
w
(
BLEU(e
k
i
, f
i
)
)]
Note that we scale the expectation by reference
length to place more emphasis on longer sen-
tences. We set the regularization parameter ?,
which determines the trade-off between a high ex-
pected BLEU and a small norm, to ? = 10.
Following Cherry and Foster (2012), we tune
with a MERT-like batch architecture: fixing a set
2
We useK = 100 in our experiments.
corpus # segs # en tok
Chinese-English
train 10.1M 283M
tune 1,506 161K
MT06 1,664 189K
MT08 1,357 164K
Arabic-English
train 1,512K 47.8M
tune 1,664 202K
MT08 1,360 205K
MT09 1,313 187K
Table 4: Statistics of the NIST Chinese-English
and Arabic-English data.
of K-best lists, optimizing, and then re-decoding
the entire dev set to K-best and aggregating with
previous lists to create a better K-best approxima-
tion. We repeat this outer loop 15 times.
We carried out experiments in two different set-
tings, both involving data from NIST Open MT
2012.
3
The first setting is based on data from the
Chinese-to-English constrained track, comprising
about 283 million English running words. The
second setting uses NIST 2012 Arabic-to-English
data, but excludes the UN data. There are about
47.8 million English running words in these train-
ing data. The dev set (tune) for the Chinese-to-
English task was taken from the NIST 2005 eval-
uation set, augmented with some web-genre mate-
rial reserved from other NIST corpora. We test on
the evaluation sets from NIST 2006 and 2008. For
the Arabic-to-English task, we use the evaluation
sets from NIST 2006, 2008, and 2009 as our dev
set and two test sets, respectively. Table 4 summa-
rizes the training, dev and test sets.
Experiments were carried out with an in-house,
state-of-the-art phrase-based system. Each corpus
was word-aligned using IBM2, HMM, and IBM4
models, and the phrase table was the union of
phrase pairs extracted from these separate align-
ments, with a length limit of 7. The translation
model (TM) was smoothed in both directions with
Kneser-Ney smoothing (Chen et al., 2011). We
use the hierarchical lexicalized reordering model
(RM) (Galley and Manning, 2008), with a dis-
tortion limit of 7. Other features include lexi-
cal weighting in both directions, word count, a
distance-based RM, a 4-gram LM trained on the
target side of the parallel data, and a 6-gram En-
3
http://www.nist.gov/itl/iad/mig/openmt12.cfm
365
Tune std MT06 std MT08 std
0 27.6 0.1 35.6 0.1 29.0 0.2
1 27.6 0.0 35.7 0.1 29.1 0.1
2 27.5 0.1 35.8 0.1 29.1 0.1
3 27.6 0.1 35.8 0.1 29.1 0.1
4 27.6 0.1 35.7 0.2 29.1 0.2
5 27.6 0.1 35.5 0.1 28.9 0.2
6 27.5 0.1 35.7 0.1 29.0 0.2
7 27.6 0.1 35.6 0.1 29.0 0.1
Table 5: Chinese-to-English Results for the small
feature set tuning task. Results are averaged across
5 replications; std is the standard deviation.
glish Gigaword LM.
We also conducted a set of experiments with a
much larger feature set. This system used only
GIZA++ for word alignment, increased the distor-
tion limit from 7 to 9, and is trained on a high-
quality subset of the parallel corpora used ear-
lier. Most importantly, it includes the full set of
sparse phrase-pair features used by both Hopkins
and May (2011) and Cherry and Foster (2012),
which results in nearly 7,000 features.
Our evaluation metric is the original IBM
BLEU, which performs case-insensitive matching
of n-grams up to n = 4. We perform random
replications of parameter tuning, as suggested by
Clark et al. (2011). Each replication uses a differ-
ent random seed to determine the order in which
SGD visits tuning sentences. We test for signifi-
cance using the MultEval tool,
4
which uses a strat-
ified approximate randomization test to account
for multiple replications. We report results aver-
aged across replications as well as standard devia-
tions, which indicate optimizer stability.
Results for the small feature set are shown in
Tables 5 and 6. All 7 smoothing techniques, as
well as the no smoothing baseline, all yield very
similar results on both Chinese and Arabic tasks.
We did not find any two results to be significantly
different. This is somewhat surprising, as other
groups have suggested that choosing an appropri-
ate BLEU approximation is very important. In-
stead, our experiments indicate that the selected
BLEU smoothing method is not very important.
The large-feature experiments were only con-
ducted with the most promising methods accord-
ing to correlation with human judgments:
4
available at https://github.com/jhclark/multeval
Tune std MT08 std MT09 std
0 46.9 0.1 46.5 0.1 49.1 0.1
1 46.9 0.0 46.4 0.1 49.1 0.1
2 46.9 0.0 46.4 0.1 49.0 0.1
3 47.0 0.0 46.5 0.1 49.2 0.1
4 47.0 0.0 46.5 0.1 49.2 0.1
5 46.9 0.0 46.4 0.1 49.1 0.1
6 47.0 0.0 46.4 0.1 49.1 0.1
7 47.0 0.0 46.4 0.1 49.0 0.1
Table 6: Arabic-to-English Results for the small
feature set tuning task. Results are averaged across
5 replications; std is the standard deviation.
Tune std MT06 std MT08 std
mira 29.9 0.1 38.0 0.1 31.0 0.1
0 29.5 0.1 37.9 0.1 31.4 0.3
2 29.6 0.3 38.0 0.2 31.1 0.2
4 29.9 0.2 38.1 0.1 31.2 0.2
6 29.7 0.1 37.9 0.2 31.0 0.2
7 29.7 0.2 38.0 0.2 31.2 0.1
Table 7: Chinese-to-English Results for the large
feature set tuning task. Results are averaged
across 5 replications; std is the standard deviation.
Significant improvements over the no-smoothing
baseline (p ? 0.05) are marked in bold.
0: No smoothing (baseline)
2: Add 1 smoothing (Lin and Och, 2004)
4: Length-scaled pseudo-counts (this paper)
6: Interpolation with a precision prior (Gao and
He, 2013)
7: Combining Smoothing 4 with the match in-
terpolation of Smoothing 5 (this paper)
The results of the large feature set experiments are
shown in Table 7 for Chinese-to-English and Ta-
ble 8 for Arabic-to-English. For a sanity check, we
compared these results to tuning with our very sta-
ble Batch k-best MIRA implementation (Cherry
and Foster, 2012), listed as mira, which shows that
all of our expected BLEU tuners are behaving rea-
sonably, if not better than expected.
Comparing the various smoothing methods in
the large feature scenario, we are able to see signif-
icant improvements over the no-smoothing base-
line. Notably, Method 7 achieves a significant
improvement over the no-smoothing baseline in 3
out of 4 scenarios, more than any other method.
Unfortunately, in the Chinese-English MT08 sce-
nario, the no-smoothing baseline significantly out-
366
Tune std MT08 std MT09 std
mira 47.9 0.1 47.3 0.0 49.3 0.1
0 48.1 0.1 47.2 0.1 49.5 0.1
2 48.0 0.1 47.4 0.1 49.7 0.1
4 48.1 0.2 47.4 0.1 49.6 0.1
6 48.2 0.0 47.3 0.1 49.7 0.1
7 48.1 0.1 47.3 0.1 49.7 0.1
Table 8: Arabic-to-English Results for the large
feature set tuning task. Results are averaged
across 5 replications; std is the standard deviation.
Significant improvements over the no-smoothing
baseline (p ? 0.05) are marked in bold.
performs all smoothed BLEU methods, making it
difficult to draw any conclusions at all from these
experiments. We had hoped to see at least a clear
improvement in the tuning set, and one does see
a nice progression as smoothing improves in the
Chinese-to-English scenario, but no correspond-
ing pattern emerges for Arabic-to-English.
4 Conclusions
In this paper, we compared seven smoothing
techniques for sentence-level BLEU. Three of
them are newly proposed in this paper. The
new smoothing techniques got better sentence-
level correlations with human judgment than other
smoothing techniques. On the other hand, when
we compare the techniques in the context of tun-
ing, using a method that requires sentence-level
BLEU approximations, they all have similar per-
formance.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22?64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical mt system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 555?563, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Boxing Chen, Roland Kuhn, George Foster, and
Howard Johnson. 2011. Unpacking and transform-
ing feature functions: New ways to smooth phrase
tables. In MT Summit 2011.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL 2012.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In ACL 2011.
Michel Galley and C. D. Manning. 2008. A simple
and effective hierarchical phrase reordering model.
In EMNLP 2008, pages 848?856, Hawaii, October.
Jianfeng Gao and Xiaodong He. 2013. Training mrf-
based phrase translation models using gradient as-
cent. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 450?459, Atlanta, Georgia, June.
Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP 2011.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 605?612, Barcelona,
Spain, July.
Matou?s Mach?a?cek and Ond?rej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45?51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
Philadelphia, July. ACL.
367
