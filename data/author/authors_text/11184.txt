Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 681?688
Manchester, August 2008
A Joint Information Model for n-best Ranking 
Patrick Pantel 
Yahoo! Inc. 
Santa Clara, CA 95054 
me@patrickpantel.com 
Vishnu Vyas 
USC Information Sciences Institute 
Marina del Rey, CA 
vishnu@isi.edu 
 
Abstract 
In this paper, we present a method for 
modeling joint information when gene-
rating n-best lists. We apply the method 
to a novel task of characterizing the simi-
larity of a group of terms where only a 
small set of many possible semantic 
properties may be displayed to a user. 
We demonstrate that considering the re-
sults jointly, by accounting for the infor-
mation overlap between results, generates 
better n-best lists than considering them 
independently. We propose an informa-
tion theoretic objective function for mod-
eling the joint information in an n-best 
list and show empirical evidence that 
humans prefer the result sets produced by 
our joint model. Our results show with 
95% confidence that the n-best lists gen-
erated by our joint ranking model are 
significantly different from a baseline in-
dependent model 50.0% ? 3.1% of the 
time, out of which they are preferred 
76.6% ? 5.2% of the time. 
1 Introduction 
Ranking result sets is a pervasive problem in the 
NLP and IR communities, exemplified by key-
word search engines such as Google (Brin and 
Page 1998), machine translation systems (Zhang 
et al 2006), and recommender systems (Sharda-
nand and Maes 1995; Resnick and Varian 1997). 
Consider the lexical semantics task of explain-
ing why a set of terms are similar: given a set of 
terms and a large set of possible explanations for 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
their similarity, one must choose only the best n 
explanations to display to a user. There are many 
ways to explain why terms are similar2; one way 
is to list the semantic properties that are shared 
by the terms. For example, consider the follow-
ing set of terms corresponding to fruit names: 
 {apple, ume, pawpaw, quince} 
Example semantic properties that could be 
used to explain their similarity include: they are 
products, they can be eaten, they are solid (but 
not they are companies, for example). The list of 
such semantic properties can be very large and 
some are much more informative than others. For 
example, the property can-be-eaten is much 
more informative of the similarity of {apple, ume, 
pawpaw, quince} than the property is-solid. Us-
ing a simple measure of association between 
properties and queries, explained in detail later in 
this paper, one can rank each property and obtain 
the following three highest scoring properties for 
explaining the similarity of these terms: 
{they are products, they can be 
imported, they can be exported} 
Even though can be imported and can be ex-
ported are highly ranked explanations, taken 
jointly, once we know one the other does not of-
fer much more information since most things that 
can be imported can also be exported. In other 
words, there is a large overlap in information 
between the two properties. A more informative 
set of explanations could be obtained by replac-
ing one of these two properties with a property 
that scored lower but had less information over-
lap with the others, for example: 
                                                 
2 In (Vyas and Pantel 2008), we explore the task of 
explaining the similarity between terms in detail. In 
this paper, we focus on the task of choosing the best 
set of explanations given a set of candidates. 
681
{they are products, they can be 
imported, they can be eaten} 
Even though, taken alone, the property can be 
eaten may not be as informative as can be ex-
ported, it does indeed add more information to 
the explanation set when considered jointly with 
the other explanations. 
In this paper, we propose an information theo-
retic objective function for modeling the joint 
information in an n-best list. Derived using con-
ditional self-information, we measure the amount 
of information that each property contributes to a 
query. Intuitively, when adding a new property to 
a result set, we should prefer a property that con-
tributes the maximum amount of information to 
the existing set. In our experiments, we show 
empirical evidence that humans prefer our joint 
model?s result sets on the task of explaining why 
a set of terms are similar. 
The remainder of this paper is organized as 
follows. In the next section, we review related 
literature and position our contribution within 
that landscape. Section 3 presents the task of ex-
plaining the similarity of a set of terms and de-
scribes a method for generating candidate expla-
nations from which we will apply our ranking 
model.  In Section 4, we formally define our 
ranking task and present our Joint Information 
Ranking model. Experimental results are pre-
sented in Section 5 and finally, we conclude with 
a discussion and future work. 
2 Related Work 
There are a vast number of applications of 
ranking and its importance to the commercial 
success at companies such as Google and Yahoo 
have fueled a great deal of research in recent 
years. In this paper, we investigate one particular 
aspect of ranking, the importance of considering 
the results in an n-best list jointly because of the 
information overlap issues described in the 
introduction, and one particular application, 
namely explaining why a set of terms are similar. 
Considering results jointly is not a new idea 
and is very similar to the concept of diversity-
based ranking introduced in the IR community 
by Carbonell and Goldstein (1998). In short, se-
lecting an n-best list is a balancing act between 
maximizing the relevance of the list and the in-
formation novelty of its results. One commonly 
used approach is to define a measure of novel-
ty/semantic similarity between documents and to 
apply heuristics to reduce the relevance score of 
a result item (a hit) by a function of the similarity 
of this item to other results in the list (Carbonell 
and Goldstein 1998; Zhu et al 2007). Another 
common approach is to cluster result documents 
according to their semantic similarity and present 
clusters to users instead of individual documents 
(Hearst and Pedersen 1996; Leuski 2001; Liu and 
Croft 2004). In this paper, we argue that the bal-
ance between relevance and novelty can be cap-
tured by a formal model that maximizes the joint 
information content of a result set. Instead of 
ranking documents in an IR setting, we focus in 
this paper on a new task of selecting the best se-
mantic properties that describe the similarity of a 
set of query terms. 
By no means an exhaustive list, the most 
commonly cited ranking and scoring algorithms 
are HITS (Kleinberg 1998) and PageRank (Page 
et al 1998), which rank hyperlinked documents 
using the concepts of hubs and authorities. The 
most well-known keyword scoring methods 
within the IR community are the tf-idf (Salton 
and McGill 1983) and pointwise mutual informa-
tion (Church and Hanks 1989) measures, which 
put more importance on matching keywords that 
occur frequently in a document relative to the 
total number of documents that contain the key-
word (by normalizing term frequencies with in-
verse document frequencies). Various methods 
including tf-idf have been comparatively eva-
luated by Salton and Buckley (1987). Creating n-
best lists using the above algorithms produce 
result sets where each result is considered inde-
pendently. In this paper, we investigate the utility 
of considering the result sets jointly and compare 
our joint method to a pointwise mutual informa-
tion model. 
Within the NLP community, n-best list rank-
ing has been looked at carefully in parsing, ex-
tractive summarization (Barzilay et al 1999; 
Hovy and Lin 1998), and machine translation 
(Zhang et al 2006), to name a few. The problem 
of learning to rank a set of objects by combining 
a given collection of ranking functions using 
boosting techniques is investigated in (Freund et 
al. 2003). This rank boosting technique has been 
used in re-ranking parsers (Collins and Koo 
2000; Charniak and Johnson 2005). Such re-
ranking approaches usually improve the likelih-
ood of candidate results using extraneous fea-
tures and, for example in parsing, the properties 
of the trees. In this paper, we focus on a differ-
ence task: the lexical semantics task of selecting 
the best semantic properties that help explain 
why a set of query terms are similar. Unlike in 
parsing and machine translation, we are not ulti-
682
mately looking for the best single result, but in-
stead the n-best. 
Looking at commercial applications, there are 
many examples showcasing the importance of 
ranking, for example Internet search engines like 
Google and Yahoo (Brin and Page 1998). Anoth-
er application is online recommendation systems 
where suggestions must be ranked before being 
presented to a user (Shardanand and Maes 1995). 
Also, in online social networks such as Facebook 
and LinkedIn, new connections or communities 
are suggested to users by leveraging their social 
connections (Spretus, et al 2005). 
3 Explaining Similarity 
Several applications, such as IR engines, return 
the n-best ranked results to a query. Although we 
expect our joint information model, presented in 
Section 4.2, to generalize to many ranking tasks, 
our focus in this paper is on the task of choosing 
the n-best explanations that describe the similari-
ty of a set of terms. That is, given a set of terms, 
one must choose the best set of characterizations 
of why the terms are similar, chosen from a large 
set of possible explanations. 
Analyzing the different ways in which one can 
explain/characterize the similarity between terms 
is beyond the scope of this paper3. The types of 
explanations that we consider in this paper are 
semantic properties that are shared by the terms. 
For example, consider the query terms {apple, 
ume, pawpaw, quince} presented in Section 1. 
An example set of properties that explains the 
similarity of these words might include {they are 
products, they can be imported, they can be ex-
ported, they are tasty, they grow}. 
The range of possible semantic properties is 
large. For the above example, we may have of-
fered many other properties like {they are enti-
ties, they can be eaten, they have skin, they are 
words, they can be roasted, they can be shipped, 
etc.} Choosing a high quality concise set of 
properties is the goal of this paper. 
Our hypothesis is that considering items in a 
result set jointly for ranking produces better re-
sult sets than considering them independently. 
An important question then is: what is a utility 
function for measuring a better result? We pro-
pose that a result set is considered better than 
another if a person could more easily reconstruct 
the original query from it. Or, in other words, a 
result set is considered better than another if it 
                                                 
3 This topic is the focus of (Vyas and Pantel 2008).  
reduces more the uncertainty of what the original 
query was. Here, reducing the uncertainty means 
making it easier for a human to understand the 
original question (i.e., a good explanation should 
clarify the query).  
Formally, we define our ranking task as: 
Task Definition: Given a query Q = {q1, q2, ?, 
qm} and a set of candidate properties R = {r1, 
r2, ?, rk}, where q is a term and r is a property, 
find the set of properties R' = {r1, r2, ?, rn} that 
most reduces the uncertainty of Q, where n << k. 
Recall from Section 1 the example Q = {apple, 
ume, pawpaw, quince}. The set of properties: 
{they are products, they can be 
imported, they can be eaten} 
is preferred over the set  
{they are products, they can be 
imported, they can be exported} 
since it reduces more the uncertainty of what the 
original query is. That is, if we hid the query 
{apple, ume, pawpaw, quince} from a person, 
the first set of properties would help more that 
person guess the query elements than the second 
properties. 
In Section 4, we describe two models for mea-
suring this uncertainty reduction and in Section 
5.1, we describe an evaluation methodology for 
quantifying this reduction in uncertainty using 
human judgments. 
3.1 Source of Properties 
What is the source of the semantic properties to 
be used as explanations? Following Lin (1998), 
we use syntactic dependencies between words to 
model their semantic properties. The assumption 
here is that some grammatical relations, such as 
subject and object can often yield semantic 
properties of terms. For example, given enough 
corpus occurrences of a phrase like ?students eat 
many apples?, then we can infer the properties 
can-be-eaten for apples and can-eat for students. 
Unfortunately, many grammatical relations do 
not specify semantic properties, such as most 
conjunction relations for example. In this paper, 
we use a combination of corpus statistics and 
manual filters of grammatical relations (such as 
omitting conjunction relations) to uncover 
candidate semantic properties, as described in the 
next section. With this method, we unfortunately 
uncover some non-semantic properties and fail to 
uncover some correct semantic properties. 
683
Improving the candidate lists of semantic 
properties is grounds for further investigation. 
3.2 Extracting Properties 
Given a set of similar terms, we look at the 
overlapping syntactic dependencies between the 
words in the set to form candidate semantic 
properties. Example properties extracted by our 
system (described below) for a random sample of 
two instances from a cluster of food, {apple, 
beef}, include4: 
shredded, sliced, lean, sour, de-
licious, cooked, import, export, 
eat, cook, dice, taste, market, 
consume, slice, ... 
We obtain candidate properties by parsing a 
large textual corpus with the Minipar parser (Lin 
1993)5. For each word in the corpus, we extract 
all of its dependency links, forming a feature 
vector of syntactic dependencies. For example, 
below is a sample of the feature vector for the 
word apple: 
adj-mod:gala, adj-mod:shredded,  
object-of:caramelize, object-of:eat, 
object-of:import, ... 
Intersecting apple?s feature vector with beef?s, 
we are left with the following candidate 
properties: 
adj-mod:shredded, object-of:eat,  
object-of:import, ... 
In this paper, we omit the relation name of the 
syntactic dependencies, and instead write: 
 shredded, eat, import, ... 
This list of syntactic dependencies forms the 
candidate properties for our ranking task defined 
in Section 3.  
In Section 4, we use corpus statistics over 
these syntactic dependencies to find the most 
informative properties that explain the similarity 
of a set of terms. Some syntactic dependencies 
are not reliably descriptive of the similarity of 
words such as conjunctions and determiners. We 
omit these dependency links from our model. 
4 Ranking Models 
In this section, we present our ranking models for 
choosing the n-best results to a query according 
to our task definition from Section 3. The models 
                                                 
4 We omit the syntactic relations for readability. 
5 Section 5.1 describes the specific corpus and method 
that was used to obtain our reported results. 
are expected to generalize to many ranking tasks, 
however in this paper we focus solely on the 
problem of choosing the best semantic properties 
that describe the similarity of a set of terms. 
In the next section, we outline our baseline in-
dependent model, which is based on a commonly 
used ranking metric in lexical semantics for se-
lecting the most informative properties of a term. 
Then in Section 4.2, we propose our new model 
for considering the properties jointly. 
4.1 EIIR: Expected Independent Informa-
tion Ranking Model (Baseline Model) 
Recall the task definition from Section 3. Finding 
a property r that most reduces the uncertainty in 
a query set Q can be modeled by measuring the 
strength of association between r and Q. 
Following Pantel and Lin (2002), we use 
pointwise mutual information (pmi) to measure 
the association strength between two events q 
and r, where q is a term in Q and r is syntactic 
dependency, as follows (Church and Hanks 
1989): 
 ( ) ( )( ) ( )
N
fqc
N
rwc
N
rqc
FfWw
rqpmi ???
=
??
,,
,
log,  (4.1) 
where c(q,r) is the frequency of r in the feature 
vector of q (as defined in Section 3.2), W is the 
set of all words in our corpus, F is the set of all 
syntactic dependencies in our corpus, and  
N = ( )? ?
? ?Ww Ff
fwc , is the total frequency count of 
all features of all words. 
We estimate the association strength between 
a property r and a set of terms Q by taking the 
expected pmi between r and each term in Q as: 
 ( ) ( ) ( )?
?
=
Qq
rqpmiqPrQpmi ,,  (4.2) 
where P(q) is the probability of q in the corpus. 
Finally, the EIIR model chooses an n-best list 
by selecting the n properties from R that have 
highest pmi(Q, r). 
4.2 JIR: Joint Information Ranking Model 
The hypothesis of this paper is that considering 
items in an n-best result set jointly for ranking 
produces better result sets than considering them 
independently, an example of which is shown in 
Section 1. 
Recall our task definition from Section 3: to 
select an n-best list R' from R such that it most 
reduces the uncertainty of Q. Recall that for ex-
plaining the similarity of terms, Q is the set of 
684
query words to be explained and R is the set of 
all properties shared by words in Q. The above 
task of finding R' can be captured by the follow-
ing objective function: 
 ( )RQIR
RR
?=?
??
minarg  (4.3) 
where I(Q|R') is the amount of information in Q 
given R':6 
 ( ) ( ) ( )?
?
??=?
Qq
RqIqPRQI  (4.4) 
where P(q) is the probability of term q in our 
corpus (defined in the Section 4.1) and I(q|R') is 
the amount of information in q given R', which is 
defined as the conditional self-information 
between q and R' (Merhav and Feder 1998): 
 
( ) ( )
( )
( )
( )Rc
Rqc
rrrqP
rrrqIRqI
n
n
?
??=
?=
=?
*,
,
log
,...,,log
,...,,
21
21
 (4.5) 
where c(q,R') is the frequency of all properties in 
R' occurring with word q and * represents all 
possible terms in the corpus7. We have: 
 ( ) ( )?
??
=?
Rr
rqcRqc ,,  and ( ) ( )??
?? ??
=?
Rr Qq
rqcRc ,*,  
where c(q,r) is defined as in Section 4.1 and Q' is 
the set of all words that have all the properties in 
R'. Computing c(*,R') efficiently can be done 
using a reverse index from properties to terms. 
The Joint Information Ranking model (JIR) is 
the objective function in Eq. 4.3. We find a sub-
optimal solution to Eq. 4.3 using a greedy algo-
rithm by starting with an empty set R' and itera-
tively adding one property r at a time into R' such 
that: 
 ( ) ( )?
????
???=
QqRRr
rRqIqPr minarg  (4.6) 
The intuition behind this algorithm is as fol-
lows: when choosing a property r to add to a par-
tial result set, we should choose the r that contri-
butes the maximum amount of information to the 
existing set (where all properties are considered 
jointly). 
                                                 
6 Note that finding the set R' that minimizes the 
amount of information in Q given R' equates to find-
ing the R' that reduces most the uncertainty in Q. 
7 Note that each property in R' is shared by q because 
of the way the candidate properties in R were con-
structed (see Section 3.2). 
A brute force optimal solution to Eq. 4.3 in-
volves computing I(Q|R') for all subsets R' of size 
n of R. In future work, we will investigate heuris-
tic search algorithms for finding better solutions 
to Eq. 4.3, but our experimental results discussed 
in Section 5 show that our greedy solution to Eq. 
4.3 already yields significantly better n-best lists 
than the baseline EIIR model. 
5 Experimental Results 
In this section, we show empirical evidence that 
considering items in an n-best result set jointly 
for ranking produces better result sets than con-
sidering them independently. We validate this 
claim by testing whether or not human judges 
prefer the set of explanations generated by our 
joint model (JIR) over the independent model 
(EIIR). 
5.1 Experimental Setup 
We trained the probabilities described in Section 
4 using corpus statistics extracted from the 
TREC-9 and TREC-2002 Aquaint collections 
consisting of approximately 600 million words. 
We used the Minipar parser (Lin 1993) to ana-
lyze each sentence and we collected the frequen-
cy counts of the grammatical contexts output by 
Minipar and used them to compute the probabili-
ty and pointwise mutual information values from 
Sections 4.1 and 4.2. Given any set of words Q 
from the corpus, our joint and independent mod-
els generate a ranked list of n-best explanations 
(i.e., properties) for the similarity of the words. 
Recall the example set Q = {apple, beef} from 
Section 3.2. Following Section 3.2, all grammat-
ical contexts output by Minipar that both words 
share form a candidate explanation set R for their 
similarity. For {apple, beef}, our systems found 
312 candidate explanations. Applying the inde-
pendent ranking model, EIIR, we obtain the fol-
lowing top-5 best explanations, R': 
product, import of, export, ban 
on, industry 
Using the joint model, JIR, we obtain: 
export, product, eat, ban on, 
from menu 
5.2 Comparing Ranking Models 
In order to obtain a representative set of similar 
terms as queries to our systems, we randomly 
chose 100 concepts from the CBC collection 
(Pantel and Lin 2002) consisting of 1628 clusters 
of nouns. For each of these concepts, we ran-
domly chose a set of cluster instances (nouns), 
685
where the size of each set was randomly chosen 
to consist of two or three noun (chosen to reduce 
the runtime of our algorithm). For example, three 
of our randomly sampled concepts were Music, 
Flowers, and Alcohol and below are the random 
instances selected from these concepts: 
? {concerto, quartet, Fifth Symphony} 
? {daffodil, lily} 
? {gin, alcohol, rum} 
Each of these three samples forms a query. 
Applying both our EIIR and JIR models, we gen-
erated the top-5 explanations for each of the 100 
samples. For example, below are the explana-
tions returned for {daffodil, lily}:  
? EIIR: bulb, bouquet of, yellow, pink, hybr-
id 
? JIR: flowering, bulb, bouquet of, hybrid, 
yellow 
Two judges then independently annotated 500 
test cases using the following scheme. For each 
of the 100 samples, a judge is presented with the 
sample along with the top-1 explanation of both 
systems, randomly ordered for each sample such 
that the judge can never know which system 
generated which explanation. The judge then 
must make one of the following three choices: 
? Explanation 1: The judge prefers the first 
explanation to the second. 
? Explanation 2: The judge prefers the 
second explanation to the first. 
? Equal: The judge cannot determine that 
one explanation is better than the other. 
The judge is then presented with the top-2 ex-
planations from each system, then the top-3, top-
4, and finally the top-5 explanations, making the 
above annotation decision each time. Once the 
judge has seen the top-5 explanations for the 
sample, the judge moves on to the next sample 
and repeats this process until all 100 samples are 
annotated. Allowing the judges to see the top-1, 
top-2, up to top-5 explanations allows us to later 
inspect how our ranking algorithms perform on 
different sizes of explanation sets. 
The above annotation task was performed in-
dependently by two judges and the resulting 
agreement between the judges, using the Kappa 
statistic (Siegel and Castellan Jr. 1988), was ? = 
0.60. Table 1 lists the full confusion matrix on 
the annotation task. On just the annotations of the 
top-5 explanations, the agreement was ? = 0.73. 
Table 2 lists the Kappas for the different sizes of 
explanation sets. It is more difficult for judges to 
determine the quality of smaller explanation sets. 
For the above top-5 explanations for the query 
{daffodil, lily}, both judges preferred the JIR 
properties since flowering was deemed more in-
formative than pink given that we also know the 
property yellow. 
5.2.1 Evaluation Results 
Table 3 shows sample n-best lists generated by 
our system and Table 4 presents the results of the 
experiment described in the previous section. 
Table 4 lists the preferences of the judges for the 
n-best lists generated by the independent and 
joint models, in terms of the percentage of sam-
ples preferred by each judge on each model. We 
report our results on both all 500 annotations and 
on the 100 annotations for the explanation sets of 
size n = 5. Instead of using an adjudicator for 
resolving the two judges? disagreements, we 
weighted each judge?s decision by 0.5. We used 
bootstrap resampling to obtain the 95% confi-
dence intervals. 
The judges significantly preferred the joint 
model over the independent model. Looking at 
all annotated explanation sets (varying n from 1 
to 5), the n-best lists from JIR were preferred 
39.7% of the time. On the 50.0% ? 3.1% test 
cases where one list was preferred over another, 
the JIR lists were preferred overall 76.6% ? 5.2% 
of the time, with 95% confidence. Caution 
should be taken when interpreting the results for 
n < 3 since the annotator agreement for these was 
very low. However, as shown in Figure 1, human 
preference for the JIR model was higher at n ? 3. 
Table 2. Inter-annotator agreement statistics over 
varying explanation set sizes n. 
n AGREEMENT (%) KAPPA (?) 
1 75.0 0.47 
2 70.0 0.50 
3 77.0 0.62 
4 78.0 0.63 
5 84.0 0.73 
 
Table 1. Confusion matrix between the two judges on 
the annotation task over all explanation set sizes 
(n = 1 ? 5). 
 JIR EIIR EQUAL 
JIR 153 2 48 
EIIR 11 33 19 
EQUAL 29 7 198 
 
686
5.2.2 Discussion and Error Analysis 
Figure 1 illustrates the annotated preferences 
over varying sizes of explanation sets, for n ? 
[1 .. 5]. Except in the case where only one expla-
nation is returned, we see consistent preferences 
between the judges. Manual inspection of the 
size 1 explanation sets showed that often one 
property is not enough to understand the similari-
ty of the query words. For example, consider the 
following two explanation sets: {sell} and 
{drink}. If you did not know the original query Q, 
one list would not be much better than the other 
in determining what the query was. But, by add-
ing one more property, we get: {sell, drink} and 
{drink, spike with}. The second explanation list 
reduces much more the uncertainty that the query 
consists of alcoholic beverages, as you probably 
guessed (the first list also reduces the uncertainty, 
but not as much as the second). The above ex-
ample is taken from our random sample list for 
the query words {gin, alcohol, rum} ? the expla-
nation {drink, spike with} was generated using 
the JIR model. 
We manually inspected some of the sample 
queries where both judges preferred the EIIR n-
best list. One such sample query was: {Jerry 
Falwell, Jim Bakker, Pat Robertson}. The n-best 
lists returned by the JIR and EIIR models respec-
tively were {televangelist, evangelist, Rev., tele-
vision, founder} and {evangelist, television, Rev., 
founder, religious}. Both judges preferred the 
EIIR list because of the overlap in information 
between televangelist and evangelist. The prob-
lem here in JIR was that the word televangelist 
was very rare in the corpus and thus few terms 
had both the feature televangelist and evangelist. 
We would expect in a larger corpus to see a larg-
er overlap with the two features, in which case 
evangelist would not be chosen by the JIR model. 
As discussed in Section 2, considering results 
jointly is not a new idea and is very similar to the 
concept of diversity-based ranking introduced in 
the IR community by Carbonell and Goldstein 
(1998). Their proposed technique, called maxim-
al marginal relevance (MMR), forms the basis of 
most schemes used today and works as follows. 
Initially, each result item is scored independently 
of the others. Then, the n-best list is selected by 
iteratively choosing the highest scoring result 
and then discounting each remaining candidate?s 
score by some function of the similarity (or in-
formation gain) between that candidate and the 
currently selected members of the n-best list. In 
practice, these heuristic-based algorithms are fast 
to compute and are used heavily by commercial 
IR engines. The purpose of this paper is to inves-
tigate a principled definition of diversity using 
the concept of maximal joint information. The 
objective function proposed in Eq. 4.3 provides a 
basis for understanding diversity through the lens 
of information theory. Although this paper fo-
Table 3. Five example n-best lists, drawn from our random sample described in Section 5.1, using the joint JIR
model and the independent EIIR model (for n=5). 
Query (Q) JIR n-best (R') EIIR n-best (R') 
{gin, alcohol, rum} drink, spike with, sell, use, consume sell, drink, use, consume, buy 
{Temple University, Michigan State} 
political science at, professor at, 
director at, student at, attend 
professor at, professor, director at, 
student at, student 
{concerto, quartet, Fifth Symphony} Beethoven, his, play, write, performance his, play, write, performance, perform 
{ranch house, loft} 
offer, brick, sprawling, rambling, 
turn-of-the-century 
his, live, her, buy, small 
{dysentery, tuberculosis} morbidity, die of, case, patient, suffer from die of, case, patient, case of, have 
 
Table 4. Percentage of test cases where the judges 
preferred JIR vs. EIIR vs. they had no preference, 
computed over all explanation set sizes (n = 1 ? 5) 
vs. only the explanation sets of size n = 5. 
SYSTEM ALL (95% CONF?) N=5 (95% CONF?) 
JIR 39.7% ? 3.0% 43.7% ? 6.9% 
EIIR 10.4% ? 1.3% 10.1% ? 4.2% 
Equal 50.0% ? 3.1% 45.2% ? 6.9% 
?95% confidence intervals estimated using bootstrap resampling. 
Figure 1. Percentage of human preference for each 
model with varying sizes of explanation sets (n). 
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5
Pr
ef
er
en
ce
Number?of?explanations? (n)
Model?Preference?vs.?Number?of?Explanations
JIR?Preferred EIIR?Baseline?Preferred Equal?(No?Preference)
687
cuses on the task of explaining the similarity of 
terms, we plan in future work to apply our me-
thod to an IR task in order to compare and con-
trast our method with MMR. 
6 Conclusion 
This paper investigates the problem of n-best 
ranking on the lexical semantics task of explain-
ing/characterizing the similarity of a group of 
terms where only a small set of many possible 
semantic properties may be displayed to a user. 
We propose that considering the results jointly, 
by accounting for the information overlap be-
tween results, helps generate better n-best lists. 
We presented an information theoretic objective 
function, called Joint Information Ranking, for 
modeling the joint information in an n-best list. 
On our lexical semantics task, empirical evidence 
shows that humans significantly prefer JIR n-best 
lists over a baseline model that considers the ex-
planations independently. Our results show that 
the n-best lists generated by the joint model are 
judged to be significantly different from those 
generated by the independent model 50.0% ? 
3.1% of the time, out of which they are preferred 
76.6% ? 5.2% of the time, with 95% confidence. 
In future work, we plan to investigate other 
joint models using latent semantic analysis tech-
niques, and to investigate heuristic algorithms to 
both optimize search efficiency and to better ap-
proximate our JIR objective function. Although 
applied only to the task of characterizing the si-
milarity of terms, it is our hope that the JIR mod-
el will generalize well to many ranking tasks, 
from keyword search ranking, to recommenda-
tion systems, to advertisement placements. 
References 
Barzilay, R.; McKeown, K.; and Elhadad, M. 1999. Information 
Fusion in the Context of Multi-Document Summarization. In 
Proceedings of ACL-1999. pp. 550-557. College Park, MD. 
Brin, S. and Page, L. 1998. The Anatomy of a Large-Scale Hyper-
textual Web Search Engine. Computer Networks and ISDN Sys-
tems, 30:107-117. 
Carbonell, J. G. and Goldstein, J. 1998. The Use of MMR, Diversi-
ty-Based Reranking for Reordering Documents and Producing 
Summaries. In Proceedings of SIGIR-1998. pp. 335-336. 
Charniak, E. and Johnson, M. 2005. Coarse-to-fine n-best parsing 
and MaxEnt disciriminative reranking. In Proceedings of ACL-
2005. pp. 173-180. Ann Arbor, MI. 
Church, K. and Hanks, P. 1989. Word association norms, mutual 
information, and lexicography. In Proceedings of ACL-89. pp. 
76-83. Vancouver, Canada. 
Collins, M. and Koo, T. 2000. Discriminative Reranking for Natu-
ral Laguage Parsing. In Proceedings ICML-2000. pp. 175-182. 
Palo Alto, CA 
Freund, Y.; Iyer, R.; Schapier, E.R and Singer, Y. 2003. An effi-
cient boosting algorithm for combining preferences. The Journal 
of Machine Learning Research, 4:933-969. 
Harris, Z. 1985. Distributional structure. In: Katz, J. J. (ed.) The 
Philosophy of Linguistics. New York: Oxford University Press. 
pp. 26-47. 
Hearst, M. A. and Pedersen, J. O. 1996. Reexamining the cluster 
hypothesis: Scatter/gather on retrieval results. In Proceedings of 
SIGIR-1996. pp. 76-84. Zurich, Switzerland. 
Hovy, E.H. and Lin, C.-Y. 1998. Automated Text Summarization in 
SUMMARIST. In M. Maybury and I. Mani (eds), Advances in 
Automatic Text Summarization. Cambridge, MIT Press. 
Kleinberg, J. 1998. Authoritative sources in a hyperlinked environ-
ment. In Proceedings of the Ninth Annual ACM-SIAM Sympo-
sium on Discrete Algorithms. Pp. 668-677. New York, NY. 
Leuski, A. 2001. Evaluating document clustering for interactive 
information retrieval. In Proceedings of CIKM-2001. pp. 33-40. 
Atlanta, GA. 
Lin, D. 1998. Automatic retrieval and clustering of similar words. 
In Proceedings of COLING/ACL-98. pp. 768-774. Montreal, 
Canada. 
Lin, D. 1993. Parsing Without OverGeneration. In Proceedings of 
ACL-93. pp. 112-120. Columbus, OH. 
Liu, X. and Croft, W. B. 2004. Cluster-based retrieval using lan-
guage models. In Proceedings of SIGIR-2004. pp. 186-193. 
Sheffield, UK. 
Merhav, N. and Feder, M. 1998. Universal Prediction. IEEE Trans-
actions on Information Theory, 44(6):2124-2147. 
Page, L.; Brin, S.; Motwani R.; Winograd, T. 1998. The PageRank 
Citation Ranking: Bringing Order to the Web. Stanford Digital 
Library Technologies Project. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. 
In Proceedings of KDD-02. pp. 613-619. Edmonton, Canada. 
Resnick, P. and Varian, H. R. 1997. Recommender Systems. Com-
munications of the ACM, 40(3):56-58. 
Salton, G. and Buckley, C. 1987. Term Weighting Approaches in 
Automatic Text Retrieval. Technical Report:TR81-887, Ithaca, 
NY. 
Salton, G. and McGill, M. J. 1983. Introduction to Modern Infor-
mation Retrieval. McGraw Hill. 
Shardanand, U. and Maes, P. 1995. Social Information Filtering: 
Algorithms for Automating ?Word of Mouth?. In Proceedings 
of ACM CHI-1995. pp. 210-217. New York. 
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for 
the Behavioral Sciences. McGraw-Hill. 
Spretus, E.; Sahami, M.; and Buyukkokten, O. 2005. Evaluating 
Similarity Measures: A Large-Scale Study in the Orkut Social 
Network. In Proceedings of SIGKDD-2005. pp. 678-684. Chi-
cago, IL. 
Vyas, V. and Pantel, P. 2008. Explaining Similarity of Terms. In 
Proceedings of COLING-2008. Manchester, England. 
Zhu, X.; Goldberg, A.; Van Gael, J.; and Andrzejewski, D. 2007. 
Improving Diversity in Ranking using Absorbing Random 
Walks. In Proceedings of NAACL HLT 2007. pp. 97-104. 
Rochester, NY. 
Zhang, Y,; Callan, J.; and Minka, T. 2002. Novelty and redundancy 
detection in adaptive filtering. In Proceedings of SIGIR-2002. 
pp. 81-88. Tampere, Finland. 
Zhang, Y.; Hildebrand, A. S.; and Vogel, S. 2006. Distributed Lan-
guage Modeling for N-best List Re-ranking. In Proceedings of 
EMNLP-2006. Pp. 216-223. Sydney, Australia. 
688
Coling 2008: Companion volume ? Posters and Demonstrations, pages 131?134
Manchester, August 2008
Explaining Similarity of Terms
Vishnu Vyas
USC Information Sciences Institute
Marina del Rey, CA
vishnu@isi.edu
Patrick Pantel
Yahoo! Inc.
Santa Clara, CA 95054
me@patrickpantel.com
Abstract
Computing the similarity between entities
is a core component of many NLP tasks
such as measuring the semantic similarity
of terms for generating a distributional the-
saurus. In this paper, we study the problem
of explaining post-hoc why a set of terms
are similar. Given a set of terms, our task is
to generate a small set of explanations that
best characterizes the similarity of those
terms. Our contributions include: 1) an
information-theoretic objective function
for quantifying the utility of an explana-
tion set; 2) a survey of psycholinguistics
and philosophy for evidence of different
sources of explanations such as descriptive
properties and prototypes; 3) computa-
tional baseline models for automatically
generating various types of explanations;
and 4) a qualitative evaluation of our
explanation generation engine.
1 Introduction
Computing similarity is at the core of many
computer science tasks. Many have developed
algorithms for computing the semantic similarity
of words (Lee, 1999), of expressions to gener-
ate paraphrases (Lin and Pantel, 2001) and of
documents (Salton and McGill, 1983). However,
little investigation has been spent on automatically
explaining why a particular set of elements are
similar to one another.
Explaining similarity is an important part of
various natural language applications such as
question answering and building lexical ontolo-
gies such as WordNet (Fellbaum, 1998). Several
questions must be addressed before one can begin
to explore this topic. First, what constitutes a good
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
explanation and what are the sources of these
explanations? Second, how can we automatically
generate these different types of explanations?
Third, how do we empirically evaluate the quality
of an explanation? In this paper, we propose a first
analysis of these questions.
2 Related Work
The task of generating explanations has been stud-
ied in relation to Question Answering (Hirschman
and Gaizauskas, 2001) and Knowledge Represen-
tation and Reasoning (Cohen et al, 1998). Within
Question Answering, explanations have mostly
been viewed from a deductive framework and
have focused on proof trees and inference traces
as sources of explanations (Moldovan and Rus,
2001). Summarization and text generation from
proof trees have also been explored as explanations
in QA systems (Barker et al, 2004). Lester (1997)
proposed explanation design packages, a hybrid
representation for discourse knowledge that
generates multi-sentential explanations.
Detailed psycholinguistic studies into how
people explain things suggests that people explain
similarity using ?feature complexes? (Fillenbaum,
1969), a bundle of features semantically related
with a term. This suggests considering explana-
tions of similarity as the shared features among
a set of terms. Another competing idea from
linguistic philosophy is the Prototype theory by
Rosch (1975). It is argued that objects within
a semantic category are represented by another,
more commonly used or much simpler member
of the same semantic category, called a prototype.
And, within this view, explanations for similarity
are prototypes from the same semantic category
as the given terms. Deese (1966) investigated
similarity in terms of stimulus-response word
association experiments providing empirical
evidence to consider other semantically similar
words as explanations.
131
3 An Information Theoretic Framework for
Explaining Similarity
In this section, we present an information-theoretic
framework that defines a good explanation using
the intuition that they are highly informative and
reduce the uncertainty in the set of query terms.
For example, consider the set of query terms
{Maybach, Maserati, Renault}. One possible
explanation of their similarity which is very infor-
mative is they are all like a Ford (i.e., a prototype
explanation). Other possible explanations include
they can be driven using a steering wheel and
they have wheels (i.e., descriptive properties as
explanations). Each of these explanations reduces
the uncertainty regarding the semantics of the
original set of terms. In information theory, the
concept of reduction in uncertainty is related to
information gain, and good explanation sets can
be quantified in terms of information gain.
Formally, given a set of query termsQ, and a set
of explanations E, we define the best explanation
set as one which provides maximum information
to the set Q, or in other words,
E = argmax
E
?
??(?)
I(Q;E
?
) (1)
where ? is the set of all explanations (discussed
in detail in Section 4) and ?(X) represents the
power set of X . The problem of choosing the
best explanation set for a given query set is now
reduced to a problem of optimization under I .
3.1 The Information Function
The information function I in Eq. (1) is a set
function which defines the amount of information
contributed by the set of explanations E
?
to the
set of query words Q. There are many possible
information functions, but we would like all of
them to have some common properties.
Consistency
The information function should be consis-
tent. For two sets, E and E
?
, if E ? E
?
then
I(Q;E
?
) ? I(Q;E). In other words, given two
explanation sets E and E
?
, with E
?
containing
extra explanations, not in E, the information
function should assign larger values to E
?
with
respect to Q than it assigns to E.
Explanation Set Cardinality
Another important requirement regarding I , is
the size of the explanation sets. Any consistent
information function would assign larger values
to larger sets of explanations. This leads to a
problem where the optimal solution is always the
set of all explanations. We overcome this by fixing
an upper bound for the size of explanation sets
that are generated by the function I .
Redundancy and Joint Information
Many explanations in an explanation set might
overlap semantically and the information function
has to account for such overlaps. However,
information functions which take such semantic
overlap into account are computationally hard
to optimize. One approach to this problem is to
find approximate solutions using heuristic search
techniques, however, relaxing this constraint lets
us use common association measures such as
mutual information (Cover and Thomas, 1991) as
information functions.
3.2 Marginal Formulation of the Information
Function
Another equivalent formulation of Eq. (1) is to
use marginal information gains. This formulation
also gives a simple greedy algorithm to the op-
timization problem when the size of explanation
set is fixed. Let us define the marginal gain in
information to the set Q, when the explanation e
is added to the set of explanations E as:
IG
Q;E
(e) = I(Q;E ? {e})? I(Q;E)
Then, the best set of explanations of size k can be
recursively defined as
E
0
= {}
E
n
= E
n?1
? {e}
such that
e = argmax
e
?
??
IG
Q;E
n?1
(e
?
)
and
|E
n
| ? k
If our marginal information gain is independent
of the set of explanations to which it is added,
we can rank explanations by their marginal
information gains as added to the empty set. Then,
choosing the top k explanations gives us the k-best
explanation set for the query.
4 Sources for Similarity Explanations
In Section 3, we presented a framework for
quantifying a good explanation set. In this section
we present two sources of explanations, using
descriptive properties and using prototypes.
4.1 Explanations from Descriptive Properties
The concept of essence as discussed by early
empiricists was the first study of using descriptive
properties to explain the similarity of a set of
terms. Descriptive properties are the shared
essential attributes of a set of similar terms and
one way of explaining the similarity of a set of
terms is to generate descriptive properties.
Within our framework in Section 3, let the query
set Q be a set of similar words, and let ?, the
set of all explanations be the set of all properties
132
that are shared by all the words within the query
set. Using mutual information as our measure of
association between properties and terms we can
rewrite our information function I as:
I(Q;E) =
?
q?Q
p(q)
?
e?E
p(e | q) log
p(e | q)
p(e)
The marginal information gain for a single
explanation e is:
IG
Q;E
(e) =
?
q?Q
p(q) ? p(e | q) log
p(e | q)
p(e)
Since the information gain is independent of
the explanation set E, we can find the best set of
size k by greedily choosing explanations until our
explanation set reaches the desired size.
4.2 Explanations from Prototypes
As discussed in Section 2 given a set of query
terms, people can represent their meaning using
other common members from the same semantic
category, called prototypes. Within the framework
of Section 3, let Q be our set of query terms.
To generate the set of all explanations ?, we use
clusters in the CBC resource (Pantel and Lin,
2002) as an approximation to semantic categories
and we collect all possible words that belong to
that cluster which then becomes our candidate set.
Let C
q
denote the cluster to which the query
term q belongs to. Also let the set C(Q) be the
set of all clusters to which the query terms of Q
belong to. Then
? = {w|C
w
? C(Q)}
Now our information function can be written as:
I(Q;E) =
?
q?Q
p(C
q
)
?
e?E
p(e | C
q
) log
p(e | C
q
)
p(e)
The marginal formulation of the above function is:
I
Q;E
(e) =
?
q?Q
p(C
q
) ? p(e | C
q
) log
p(e | C
q
)
p(e)
We can find the optimal set of explanations of size
k using a greedy algorithm as in Section 4.1.
5 Experimental Results
5.1 Experimental Setup
For each source of explanation discussed in
Section 4, we estimated the model probabilities
using corpus statistics extracted from the 1999
AP newswire collection (part of the TREC-2002
Aquaint collection).
In order to obtain a representative set of similar
terms as queries to our systems, we randomly
chose 100 concepts from the CBC collection (Pan-
tel and Lin, 2002) consisting of 1628 clusters of
nouns. For each of these concepts, we randomly
chose a set of cluster instances (nouns), where the
size of each set was randomly chosen to consist of
two to five nouns.
Each of these samples forms a query. For
each explanation source described in Section 4,
we generated explanation sets for the random
samples and in the next section we show a random
selection of these system outputs.
5.2 Examples of Explanations using Descrip-
tive Properties
For the algorithm discussed in Section 4.1, we
derived our descriptive properties using the output
of the dependency analysis generated by the
Minipar (Lin, 1994) dependency parser. We use
syntactic dependencies between words to model
their semantic properties. The assumption here is
that some grammatical relations, such as subject
and object can yield semantic properties of terms.
For example, from a phrase like ?students eat
many apples?, we can infer the properties can-be-
eaten for apples and can-eat for students. In this
paper, we use a combination of corpus statistics
and manual filters for grammatical relations to
uncover candidate semantic properties.
Table 1: Explanations generated using descriptive
properties.
Query Sets Explanations
Palestinian-Israeli,
India-Pakistan
talks(NN), conflict(NN),
dialogue(NN),
relation(NN), peace(NN).
TV, television-station cable(NN), watch(obj),
see(ON), channel(NN),
local(ADJ-MOD)
Britney
Spears, Janet Jackson
like(OBJ),
concert(NN), video(NN),
fan(NN), album(GEN)
Crisis,
Uncertainty, Difficulty
face(OBJ), resolve(OBJ),
overcome(OBJ),
financial(ADJ-MOD),
political(ADJ-MOD)
Intuitively, one would prefer adjectival modi-
fiers and verbal propositions as good descriptive
properties for explanations, and from the exam-
ples, we can see our algorithm generates such
descriptive properties because of the high infor-
mation contribution of such properties to the query
set. However, our algorithm does not try to reduce
the redundancy within the sets of explanations. We
can see redundant explanations for examples in Ta-
ble 1. The reason is that each explanation added to
the set is independent of the ones already present
in the set. In Pantel and Vyas (2008) we propose a
joint information model to overcome this problem.
5.3 Explanations using Prototypes
The algorithm discussed in Section 4.2 uses words
that share the semantic category with words within
the query set as the set of candidate explanations.
133
We can approximate the notion of semantic
categories using clusters of semantically similar
words. For this we used the CBC collection (Pan-
tel and Lin 2002) of nouns. Using these clusters as
semantic categories, the candidate set of all expla-
nations is the set of all the words that belong to the
same cluster. Table 2 shows some system outputs.
Table 2: Explanations generated using prototypes.
Query Sets Explanations
TV, television
station
station, network, radio, channel,
television
Budweiser, Coors
Light
Anheuser-Busch, Heineken, Coors,
San Miguel, Lion Nathan
atom, elec-
tron,photon
particle, molecule, proton, Ion,
isotope
Temple Univer-
sity,Michigan State
University
University of Texas, University of
Massachusetts, University of North
Carolina,University of Virginia,
University of Minnesota
6 Conclusions and Future Work
Computing the similarity between entities forms
the basis of many computer science algorithms.
However, we have little understanding of what
constitutes the underlying similarity. In this
paper, we investigated the problem of explaining
why a set of terms are similar. We proposed
an information-theoretic objective function for
quantifying the utility of an explanation set, by
capturing the intuition that the best explanation
will be the one that is highly informative to the
original query terms. We also explored various
sources of explanations such as descriptive prop-
erties and prototypes. We then proposed baseline
algorithms to automatically generate these types
of explanations and we presented a qualitative
evaluation of the baselines.
However, many other explanation sources were
not addressed. Hypernyms and other hierarchical
relations among words also form good explanation
sources; for example the similarity of the terms
{Ford, Toyota} can be explained using the term
car, a hypernym. Also our current explanation
types would fail for query sets consisting of related
terms such as {bus, road}. More appropriate for
these queries would be identifying the relation
linking the terms or giving analogies such as
{boat, water}. We are working on algorithms
to generate these explanation types within our
information-theoretic framework. We are also
investigating application-level quantitative evalu-
ation methodologies. Candidate applications in-
clude providing answer support by explaining the
answers generated by a QA system and explaining
why a document was returned in an IR system.
References
Barker, K., Chaw, S., Fan, J., Porter, B., Tecuci, D., Yeh,
P. Z., Chaudhri, V., Israel, D., Mishra, S., Romero, P.,
and Clark, P. 2004. A Question-Answering System
for AP Chemistry: Assessing KR&R Technologies. In
Proceedings of the Ninth International Conference on the
Principles of Knowledge Representation and Reasoning
(KR 2004). Whistler, 488-497
Cohen, P., Schrag, R., Jones, E., Pease, A., Lin, A., Start,
B., Gunning, D., and Burke, M. 1998. The DARPA High
Performance Knowledge Bases Project. AI Magazine
19(4): 5-49.
Cover, T. M. and Thomas, J. A. 1991. Elements of
Information Theory. Wiley Interscience, New York.
Deese, J. 1966. The Structure of Associations in Language
and Thought. John Hopkins Press, Oxford, England.
Fellbaum, C. 1998. WordNet: An electronic lexical database.
MIT Press.
Fillenbaum, S. 1969. Words as feature complexes : false
recognition of antonyms and synonyms Journal of
Exp.Psychology, 1969.
L Hirschman, R Gaizauskas - Natural Language Engineering
2001. Natural language question answering: the view
from here. Natural Language Engineering.
Lee, Lillian. 1999. Measures of Distributional Similarity. In
Proceedings of ACL-93. pp. 25-32. College Park, MD
Lester, J. C., and Porter, B. W. 1997. Developing and
Empirically Evaluating Robust Explanation Generators:
The KNIGHT Experiments Computational Linguistics,
v.23(1) p.65-101.
Lin, D. and Pantel, P. 2001. Discovery of Inference Rules
for Question Answering. Natural Language Engineering
7(4):343-360.
Lin, D. 1994. Principar - an efficient, broad-coverage,
principle-based parser. In Proceedings of COLING-94.
pp. 4248. Kyoto, Japan.
Moldovan, D. I., and Rus, V. 2001. Logic Form Trans-
formation of WordNet and its Applicability to Question
Answering Meeting of the Association for Computational
Linguistics, p. 394-401.
Pantel, P. and Lin, D. 2002. Discovering Word Senses
from Text. In Proceedings of SIGKDD-02. pp. 613619.
Edmonton, Canada.
Pantel, P. and Vyas, V. 2008 A Joint Information Model
for n-best Ranking In Procesedings of COLING-2008.
Manchester, UK.
Rosch, E. 1975. Cognitive representations of semantic
categories. Journal of Exp.Psychology: General, 104,
192-233.
Salton, G. and McGill, M. J. 1983. Introduction to Modern
Information Retrieval. McGraw Hill.
134
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938?947,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
 
Web-Scale Distributional Similarity and Entity Set Expansion 
Patrick Pantel?, Eric Crestan?, Arkady Borkovsky?, Ana-Maria Popescu?, Vishnu Vyas? 
?Yahoo! Labs 
Sunnyvale, CA 94089 
{ppantel,ecrestan}@yahoo-inc.com 
{amp,vishnu}@yahoo-inc.com 
?Yandex Labs 
Burlingame, CA 94010 
arkady@yandex-team.ru 
  
 
Abstract 
Computing the pairwise semantic similarity 
between all words on the Web is a compu-
tationally challenging task. Parallelization 
and optimizations are necessary. We pro-
pose a highly scalable implementation 
based on distributional similarity, imple-
mented in the MapReduce framework and 
deployed over a 200 billion word crawl of 
the Web. The pairwise similarity between 
500 million terms is computed in 50 hours 
using 200 quad-core nodes. We apply the 
learned similarity matrix to the task of au-
tomatic set expansion and present a large 
empirical study to quantify the effect on 
expansion performance of corpus size, cor-
pus quality, seed composition and seed 
size. We make public an experimental 
testbed for set expansion analysis that in-
cludes a large collection of diverse entity 
sets extracted from Wikipedia. 
1 Introduction 
Computing the semantic similarity between terms 
has many applications in NLP including word clas-
sification (Turney and Littman 2003), word sense 
disambiguation (Yuret and Yatbaz 2009), context-
spelling correction (Jones and Martin 1997), fact 
extraction (Pa?ca et al 2006), semantic role labe-
ling (Erk 2007), and applications in IR such as 
query expansion (Cao et al 2008) and textual ad-
vertising (Chang et al 2009). 
For commercial engines such as Yahoo! and 
Google, creating lists of named entities found on 
the Web is critical for query analysis, document 
categorization, and ad matching. Computing term 
similarity is typically done by comparing co-
occurrence vectors between all pairs of terms 
(Sarmento et al 2007). Scaling this task to the 
Web requires parallelization and optimizations. 
In this paper, we propose a large-scale term si-
milarity algorithm, based on distributional similari-
ty, implemented in the MapReduce framework and 
deployed over a 200 billion word crawl of the 
Web. The resulting similarity matrix between 500 
million terms is applied to the task of expanding 
lists of named entities (automatic set expansion). 
We provide a detailed empirical analysis of the 
discovered named entities and quantify the effect 
on expansion accuracy of corpus size, corpus 
quality, seed composition, and seed set size. 
2 Related Work 
Below we review relevant work in optimizing si-
milarity computations and automatic set expansion. 
2.1 Computing Term Similarities 
The distributional hypothesis (Harris 1954), which 
links the meaning of words to their contexts, has 
inspired many algorithms for computing term simi-
larities (Lund and Burgess 1996; Lin 1998; Lee 
1999; Erk and Pad? 2008; Agirre et al 2009). 
Brute force similarity computation compares all 
the contexts for each pair of terms, with complexi-
ty O(n2m) where n is the number of terms and m is 
the number of possible contexts. More efficient 
strategies are of three kinds: 
938
Smoothing: Techniques such as Latent Semantic 
Analysis reduce the context space by applying 
truncated Singular Value Decomposition (SVD) 
(Deerwester et al 1990). Computing the matrix 
decomposition however does not scale well to 
web-size term-context matrices. Other currently 
unscalable smoothing techniques include Probabil-
istic Latent Semantic Analysis (Hofmann 1999), 
Iterative Scaling (Ando 2000), and Latent Dirichlet 
Allocation (Blei et al 2003). 
Randomized Algorithms: Randomized tech-
niques for approximating various similarity meas-
ures have been successfully applied to term simi-
larity (Ravichandran et al 2005; Gorman and Cur-
ran 2006). Common techniques include Random 
Indexing based on Sparse Distributed Memory 
(Kanerva 1993) and Locality Sensitive Hashing 
(Broder 1997). 
Optimizations and Distributed Processing: 
Bayardo et al (2007) present a sparse matrix opti-
mization strategy capable of efficiently computing 
the similarity between terms which?s similarity 
exceeds a given threshold. Rychl? and Kilgarriff 
(2007), Elsayed et al (2008) and Agirre et al 
(2009) use reverse indexing and the MapReduce 
framework to distribute the similarity computa-
tions across several machines. Our proposed ap-
proach combines these two strategies and efficient-
ly computes the exact similarity (cosine, Jaccard, 
Dice, and Overlap) between all pairs. 
2.2 Entity extraction and classification 
Building entity lexicons is a task of great interest 
for which structured, semi-structured and unstruc-
tured data have all been explored (GoogleSets; 
Sarmento et al 2007; Wang and Cohen 2007; Bu-
nescu and Mooney 2004; Etzioni et al 2005; Pa?ca 
et al 2006). Our own work focuses on set expan-
sion from unstructured Web text. Apart from the 
choice of a data source, state-of-the-art entity ex-
traction methods differ in their use of numerous, 
few or no labeled examples, the open or targeted 
nature of the extraction as well as the types of fea-
tures employed. Supervised approaches (McCal-
lum and Li 2003, Bunescu and Mooney 2004) rely 
on large sets of labeled examples, perform targeted 
extraction and employ a variety of sentence- and 
corpus-level features. While very precise, these 
methods are typically used for coarse grained enti-
ty classes (People, Organizations, Companies) for 
which large training data sets are available. Unsu-
pervised approaches rely on no labeled data and 
use either bootstrapped class-specific extraction 
patterns (Etzioni et al 2005) to find new elements 
of a given class (for targeted extraction) or corpus-
based term similarity (Pantel and Lin 2002) to find 
term clusters (in an open extraction framework). 
Finally, semi-supervised methods have shown 
great promise for identifying and labeling entities 
(Riloff and Shepherd 1997; Riloff and Jones 1999; 
Banko et al 2007; Downey et al 2007; Pa?ca et al 
2006; Pa?ca 2007a; Pa?ca 2007b; Pa?ca and Durme 
2008). Starting with a set of seed entities, semi-
supervised extraction methods use either class-
specific patterns to populate an entity class or dis-
tributional similarity to find terms similar to the 
seed set (Pa?ca?s work also examines the advan-
tages of combining these approaches). Semi-
supervised methods (including ours) are useful for 
extending finer grain entity classes, for which large 
unlabeled data sets are available. 
2.3 Impact of corpus on system performance 
Previous work has examined the effect of using 
large, sometimes Web-size corpora, on system per-
formance in the case of familiar NLP tasks. Banko 
and Brill (2001) show that Web-scale data helps 
with confusion set disambiguation while Lapata 
and Keller (2005) find that the Web is a good 
source of n-gram counts for unsupervised models. 
Atterer and Schutze (2006) examine the influence 
of corpus size on combining a supervised approach 
with an unsupervised one for relative clause and 
PP-attachment. Etzioni et al (2005) and Pantel et 
al. (2004) show the advantages of using large 
quantities of generic Web text over smaller corpora 
for extracting relations and named entities. Overall, 
corpus size and quality are both found to be impor-
tant for extraction. Our paper adds to this body of 
work by focusing on the task of similarity-based 
set expansion and providing a large empirical 
study quantify the relative corpus effects. 
2.4 Impact of seeds on extraction performance 
Previous extraction systems report on the size and 
quality of the training data or, if semi-supervised, 
the size and quality of entity or pattern seed sets. 
Narrowing the focus to closely related work, Pa?ca 
(2007a; 2007b) and Pa?ca and Durme (2008) show 
the impact of varying the number of instances rep-
resentative of a given class and the size of the 
attribute seed set on the precision of class attribute 
extraction. An example observation is that good 
939
quality class attributes can still be extracted using 
20 or even 10 instances to represent an entity class. 
Among others, Etzioni et al (2005) shows that a 
small pattern set can help bootstrap useful entity 
seed sets and reports on the impact of seed set 
noise on final performance. Unlike previous work, 
empirically quantifying the influence of seed set 
size and quality on extraction performance of ran-
dom entity types is a key objective of this paper. 
3 Large-Scale Similarity Model 
Term semantic models normally invoke the distri-
butional hypothesis (Harris 1985), which links the 
meaning of terms to their contexts. Models are 
built by recording the surrounding contexts for 
each term in a large collection of unstructured text 
and storing them in a term-context matrix. Me-
thods differ in their definition of a context (e.g., 
text window or syntactic relations), or by a means 
to weigh contexts (e.g., frequency, tf-idf, pointwise 
mutual information), or ultimately in measuring 
the similarity between two context vectors (e.g., 
using Euclidean distance, Cosine, Dice). 
In this paper, we adopt the following methodol-
ogy for computing term similarity. Our various 
web crawls, described in Section 6.1, are POS-
tagged using Brill?s tagger (1995) and chunked 
using a variant of the Abney chunker (Abney 
1991). Terms are NP chunks with some modifiers 
removed; their contexts (i.e., features) are defined 
as their rightmost and leftmost stemmed chunks. 
We weigh each context f using pointwise mutual 
information (Church and Hanks 1989). Let PMI(w) 
denote a pointwise mutual information vector, con-
structed for each term as follows: PMI(w) = (pmiw1, 
pmiw2, ?, pmiwm), where pmiwf is the pointwise 
mutual information between term w and feature f: 
 
???
?=
==
m
j
wj
n
i
if
wf
wf
cc
Nc
pmi
11
log
 
where cwf is the frequency of feature f occurring for 
term w, n is the number of unique terms and N is 
the total number of features for all terms. 
Term similarities are computed by comparing 
these pmi context vectors using measures such as 
cosine, Jaccard, and Dice. 
3.1 Large-Scale Implementation  
Computing the similarity between terms on a large 
Web crawl is a non-trivial problem, with a worst 
case cubic running time ? O(n2m) where n is the 
number of terms and m is the dimensionality of the 
feature space. Section 2.1 introduces several opti-
mization techniques; below we propose an algo-
rithm for large-scale term similarity computation 
which calculates exact scores for all pairs of terms, 
generalizes to several different metrics, and is scal-
able to a large crawl of the Web. 
Our optimization strategy follows a generalized 
sparse-matrix multiplication approach (Sarawagi 
and Kirpal 2004), which is based on the well-
known observation that a scalar product of two 
vectors depends only on the coordinates for which 
both vectors have non-zero values. Further, we 
observe that most commonly used similarity scores 
for feature vectors x
r
 and y
r
, such as cosine and 
Dice, can be decomposed into three values: one 
depending only on features of x
r
, another depend-
ing only on features of y
r
, and the third depending 
on the features shared both by x
r
 and y
r
. More for-
mally, commonly used similarity scores ( )yxF rr,  
can be expressed as: 
 ( ) ( ) ( ) ( )??
???
?= ? yfxfyxffyxF
i
ii
rrrr
3210 ,,,,
 
Table 1 defines f0, f1, f2, and f3 for some common 
similarity functions. For each of these scores, f2 = 
f3. In our work, we compute all of these scores, but 
report our results using only the cosine function. 
Let A and B be two matrices of PMI feature vec-
tors. Our task is to compute the similarity between 
all vectors in A and all vectors in B. In computing 
the similarity between all pairs of terms, A = B. 
Figure 1 outlines our algorithm for computing 
the similarity between all elements of A and B. Ef-
ficient computation of the similarity matrix can be 
achieved by leveraging the fact that ( )yxF rr,  is de-
termined solely by the features shared by x
r
 and y
r
 
(i.e., f1(0,x) = f1(x,0) = 0 for any x) and that most of 
Table 1. Definitions for f0, f1, f2, and f3 for commonly used 
similarity scores. 
METRIC ( )zyxf ,,0  ( )yxf ,1  ( ) ( )xfxf rr 32 =  
Overlap x  1 0  
Jaccard* xzy
x
?+
 ( )yx ,min  ?
i
ix
Dice* 
zy
x
+
2
 
yx ?  ?
i
ix
2
Cosine zy
x
?
 
yx ?  ?
i
ix
2
*weighted generalization  
 
940
the feature vectors are very sparse (i.e., most poss-
ible contexts never occur for a given term). In this 
case, calculating f1(x, y) is only required when both 
feature vectors have a shared non-zero feature, sig-
nificantly reducing the cost of computation. De-
termining which vectors share a non-zero feature 
can easily be achieved by first building an inverted 
index for the features. The computational cost of 
this algorithm is ? 2iN , where Ni is the number of 
vectors that have a non-zero ith coordinate. Its 
worst case time complexity is O(ncv) where n is 
the number of terms to be compared, c is the max-
imum number of non-zero coordinates of any vec-
tor, and v is the number of vectors that have a non-
zero ith coordinate where i is the coordinate which 
is non-zero for the most vectors. In other words, 
the algorithm is efficient only when the density of 
the coordinates is low. On our datasets, we ob-
served near linear running time in the corpus size. 
Bayardo et al (2007) described a strategy that 
potentially reduces the cost even further by omit-
ting the coordinates with the highest number of 
non-zero value. However, their algorithm gives a 
significant advantage only when we are interested 
in finding solely the similarity between highly sim-
ilar terms. In our experiments, we compute the ex-
act similarity between all pairs of terms. 
Distributed Implementation 
The pseudo-code in Figure 1 assumes that A can fit 
into memory, which for large A may be impossible. 
Also, as each element of B is processed indepen-
dently, running parallel processes for non-
intersecting subsets of B makes the processing 
faster. In this section, we outline our MapReduce 
implementation of Figure 1 deployed using Ha-
doop1, the open-source software package imple-
menting the MapReduce framework and distri-
buted file system. Hadoop has been shown to scale 
to several thousands of machines, allowing users to 
write simple ?map? and ?reduce? code, and to 
seamlessly manage the sophisticated parallel ex-
ecution of the code. A good primer on MapReduce 
programming is in (Dean and Ghemawat 2008). 
Our implementation employs the MapReduce 
model by using the Map step to start M?N Map 
tasks in parallel, each caching 1/Mth part of A as 
an inverted index and streaming 1/Nth part of B 
through it. The actual inputs are read by the tasks 
                                                 
1 Hadoop, http://lucene.apache.org/hadoop/ 
directly from HDFS (Hadoop Distributed File Sys-
tem). Each part of A is processed N times, and each 
part of B is processed M times. M is determined by 
the amount of memory dedicated for the inverted 
index, and N should be determined by trading off 
the fact that as N increases, more parallelism can 
be obtained at the increased cost of building the 
same inverse index N times. 
The similarity algorithm from Figure 1 is run in 
each task of the Map step of a MapReduce job. 
The Reduce step is used to group the output by bi. 
4 Application to Set Expansion 
Creating lists of named entities is a critical prob-
lem at commercial engines such as Yahoo! and 
Google. The types of entities to be expanded are 
often not known a priori, leaving supervised clas-
sifiers undesirable. Additionally, list creators typi-
cally need the ability to expand sets of varying 
granularity. Semi-supervised approaches are pre-
dominantly adopted since they allow targeted ex-
pansions while requiring only small sets of seed 
entities. State-of-the-art techniques first compute 
term-term similarities for all available terms and 
then select candidates for set expansion from 
amongst the terms most similar to the seeds (Sar-
mento et al 2007). 
Input: Two matrices A and B of feature vectors. 
## Build an inverted index for A (optimiza- 
## tion for data sparseness) 
AA = an empty hash-table 
for i in (1..n): 
   F2[i] = f2(A[i]) ## cache values of f2(x) 
   for k in non-zero features of A[i]: 
      if k not in AA: AA[k] = empty-set 
      ## append <vector-id, feature-value> 
      ## pairs to the set of non-zero 
      ## values for feature k 
      AA[k].append( (i,A[i,k]) ) 
## Process the elements of B 
for b in B: 
   F1 = {} ## the set of Ai that have non-
zero similarity with b 
   for k in non-zero features of b: 
      for i in AA[k]: 
         if i not in sim: sim[i] = 0 
         F1[i] += f1( AA[k][i], b[k]) 
   F3 = f3(b) 
   for i in sim: 
      print i, b, f0( F1[i], F2[i], F3) 
Output: A matrix containing the similarity between 
all elements in A and in B. 
Figure 1. Similarity computation algorithm. 
941
Formally, we define our expansion task as: 
Task Definition: Given a set of seed entities S = 
{s1, s2, ?, sk} of a class C = {s1, s2, ?, sk, ?,, sn} and 
an unlabeled textual corpus T, find all members of 
the class C. 
For example, consider the class of Bottled Water 
Brands. Given the set of seeds S = {Volvic, San 
Pellegrino, Gerolsteiner Brunnen, Bling H2O}, our 
task is to find all other members of this class, such 
as {Agua Vida, Apenta, Culligan, Dasani, Ethos 
Water, Iceland Pure Spring Water, Imsdal, ?} 
4.1 Set Expansion Algorithm 
Our goal is not to propose a new set expansion al-
gorithm, but instead to test the effect of using our 
Web-scale term similarity matrix (enabled by the 
algorithm proposed in Section 3) on a state-of-the-
art distributional set expansion algorithm, namely 
(Sarmento et al 2007). 
We consider S as a set of prototypical examples 
of the underlying entity set. A representation for 
the meaning of S is computed by building a feature 
vector consisting of a weighted average of the fea-
tures of its seed elements s1, s2, ?, sk, a centroid. For 
example, given the seed elements {Volvic, San Pel-
legrino, Gerolsteiner Brunnen, Bling H2O}, the 
resulting centroid consists of (details of the feature 
extraction protocol are in Section 6.1): 
brand, mineral water, monitor, 
lake, water, take over, ? 
Centroids are represented in the same space as 
terms allowing us to compute the similarity be-
tween centroids and all terms in our corpus. A 
scored and ranked set for expansion is ultimately 
generated by sorting all terms according to their 
similarity to the seed set centroid, and applying a 
cutoff on either the similarity score or on the total 
number of retrieved terms. In our reported experi-
ments, we expanded over 22,000 seed sets using 
our Web similarity model from Section 3. 
5 Evaluation Methodology 
In this section, we describe our methodology for 
evaluating Web-scale set expansion. 
5.1 Gold Standard Entity Sets 
Estimating the quality of a set expansion algorithm 
requires a random sample from the universe of all 
entity sets that may ever be expanded, where a set 
represents some concept such as Stage Actors. An 
approximation of this universe can be extracted 
from the ?List of? pages in Wikipedia2. 
Upon inspection of a random sample of the ?List 
of? pages, we found that several lists were compo-
sitions or joins of concepts, for example ?List of 
World War II aces from Denmark? and ?List of 
people who claimed to be God?. We addressed this 
issue by constructing a quasi-random sample as 
follows. We randomly sorted the list of every noun 
occurring in Wikipedia2. Then, for each noun we 
verified whether or not it existed in a Wikipedia 
list, and if so we extracted this list. If a noun be-
longed to multiple lists, the authors chose the list 
that seemed most appropriate. Although this does 
not generate a perfect random sample, diversity is 
ensured by the random selection of nouns and rele-
vancy is ensured by the author adjudication. 
The final gold standard consists of 50 sets, in-
cluding: classical pianists, Spanish provinces, 
Texas counties, male tennis players, first ladies, 
cocktails, bottled water brands, and Archbishops of 
Canterbury. For each set, we then manually 
scraped every instance from Wikipedia keeping 
track also of the listed variants names. 
The gold standard is available for download at: 
http://www.patrickpantel.com/cgi-bin/Web/Tools/getfile.pl?type=data&id=sse-
gold/wikipedia.20071218.goldsets.tgz 
The 50 sets consist on average of 208 instances 
(with a minimum of 11 and a maximum of 1,116) 
for a total of 10,377 instances. 
5.2 Trials 
In order to analyze the corpus and seed effects on 
performance, we created 30 copies of each of the 
50 sets and randomly sorted each copy. Then, for 
each of the 1500 copies, we created a trial for each 
of the following 23 seed sizes: 1, 2, 5, 10, 20, 30, 
40, ?, 200. Each trial of seed size s was created by 
taking the first s entries in each of the 1500 random 
copies. For sets that contained fewer than 200 
items, we only generated trials for seed sizes 
                                                 
2 In this paper, extractions from Wikipedia are taken 
from a snapshot of the resource in December 2008. 
942
smaller than the set size. The resulting trial dataset 
consists of 20,220 trials3. 
5.3 Judgments 
Set expansion systems consist of an expansion al-
gorithm (such as the one described in Section 4.1) 
as well as a corpus (such as Wikipedia, a news 
corpus, or a web crawl). For a given system, each 
of the 20,220 trials described in the previous sec-
tion are expanded. In our work, we limited the total 
number of system expansions, per trial, to 1000. 
Before judgment of an expanded set, we first 
collapse each instance that is a variant of another 
(determined using the variants in our gold stan-
dard) into one single instance (keeping the highest 
system score)4. Then, each expanded instance is 
judged as correct or incorrect automatically 
against the gold standard described in Section 5.1. 
5.4 Analysis Metrics 
Our experiments in Section 6 consist of precision 
vs. recall or precision vs. rank curves, where: 
a) precision is defined as the percentage of correct 
instances in the expansion of a seed set; and 
b) recall is defined as the percentage of non-seed 
gold standard instances retrieved by the system. 
Since the gold standard sets vary significantly in 
size, we also provide the R-precision metric to 
normalize for set size: 
c) R-precision is defined as the average precision 
of all trials where precision is taken at rank R = 
{size of trial?s associated gold standard set}, 
thereby normalizing for set size. 
                                                 
3 Available for download at http://www.patrickpantel.com/cgi-
bin/Web/Tools/getfile.pl?type=data&id=sse-gold/wikipedia.20071218.trials.tgz. 
4 Note also that we do not allow seed instances nor their 
variants to appear in an expansion set. 
For the above metrics, 95% confidence bounds are 
computed using the randomly generated samples 
described in Section 5.2. 
6 Experimental Results 
Our goal is to study the performance gains on set 
expansion using our Web-scale term similarity al-
gorithm from Section 3. We present a large empir-
ical study quantifying the importance of corpus 
and seeds on expansion accuracy. 
6.1 Experimental Setup 
We extracted statistics to build our model from 
Section 3 using four different corpora, outlined in 
Table 2. The Wikipedia corpus consists of a snap-
shot of the English articles in December 20085. 
The Web100 corpus consists of an extraction from 
a large crawl of the Web, from Yahoo!, of over 
600 million English webpages. For each crawled 
document, we removed paragraphs containing 
fewer than 50 tokens (as a rough approximation of 
the narrative part of a webpage) and then removed 
all duplicate sentences. The resulting corpus con-
sists of over 200 billion words. The Web020 cor-
pus is a random sample of 1/5th of the sentences in 
Web100 whereas Web004 is a random sample of 
1/25th of Web100. 
For each corpus, we tagged and chunked each 
sentence as described in Section 3. We then com-
puted the similarity between all noun phrase 
chunks using the model of Section 3.1. 
6.2 Quantitative Analysis 
Our proposed optimization for term similarity 
computation produces exact scores (unlike rando-
mized techniques) for all pairs of terms on a large 
Web crawl. For our largest corpus, Web100, we 
computed the pairwise similarity between over 500 
million words in 50 hours using 200 four-core ma-
chines. Web004 is of similar scale to the largest 
reported randomized technique (Ravichandran et 
al. 2005). On this scale, we compute the exact si-
milarity matrix in a little over two hours whereas 
Ravichandran et al (2005) compute an approxima-
tion in 570 hours. On average they only find 73% 
                                                 
5 To avoid biasing our Wikipedia corpus with the test 
sets, Wikipedia ?List of? pages were omitted from our 
statistics as were any page linked to gold standard list 
members from ?List of? pages. 
Table 2. Corpora used to build our expansion models.
CORPORA 
UNIQUE 
SENTENCES 
(MILLIONS) 
TOKENS 
(MILLIONS) 
UNIQUE 
WORDS 
(MILLIONS) 
Web100 5,201 217,940 542 
Web020? 1040 43,588 108 
Web004? 208 8,717 22 
Wikipedia6 30 721 34 
?Estimated from Web100 statistics. 
 
943
of the top-1000 similar terms of a random term 
whereas we find all of them. 
For set expansion, experiments have been run on 
corpora as large as Web004 and Wikipedia (Sar-
mento et al 2007), a corpora 300 times smaller 
than our Web crawl. Below, we compare the ex-
pansion accuracy of Sarmento et al (2007) on Wi-
kipedia and our Web crawls. 
Figure 2 illustrates the precision and recall tra-
deoff for our four corpora, with 95% confidence 
intervals computed over all 20,220 trials described 
in Section 4.2. Table 3 lists the resulting R-
precision along with the system precisions at ranks 
25, 50, and 100 (see Figure 2 for detailed precision 
analysis). Why are the precision scores so low? 
Compared with previous work that manually select 
entity types for expansion, such as countries and 
companies, our work is the first to evaluate over a 
large set of randomly selected entity types. On just 
the countries class, our R-Precision was 0.816 us-
ing Web100. 
The following sections analyze the effects of 
various expansion variables: corpus size, corpus 
quality, seed size, and seed quality. 
6.2.1 Corpus Size and Corpus Quality Effect 
Not surprisingly, corpus size and quality have a 
significant impact on expansion performance. Fig-
ure 2 and Table 3 quantify this expectation. On our 
Web crawl corpora, we observe that the full 200+ 
billion token crawl (Web100) has an average R-
precision 13% higher than 1/5th of the crawl 
(Web020) and 53% higher than 1/25th of the crawl. 
Figure 2 also illustrates that throughout the full 
precision/recall curve, Web100 significantly out-
performs Web020, which in turn significantly out-
performs Web004. 
The higher text quality Wikipedia corpus, which 
consists of roughly 60 times fewer tokens than 
Web020, performs nearly as well as Web020 (see 
Figure 2). We omitted statistics from Wikipedia 
?List of? pages in order to not bias our evaluation 
to the test set described in Section 5.1. Inspection 
of the precision vs. rank graph (omitted for lack of 
space) revealed that from rank 1 thru 550, Wikipe-
dia had the same precision as Web020. From rank 
550 to 1000, however, Wikipedia?s precision 
dropped off significantly compared with Web020, 
accounting for the fact that the Web corpus con-
tains a higher recall of gold standard instances. The 
R-precision reported in Table 3 shows that this 
precision drop-off results in a significantly lower 
R-precision for Wikipedia compared with Web020. 
6.2.2  The Effect of Seed Selection 
Intuitively, some seeds are better than others. We 
study the impact of seed selection effect by in-
specting the system performance for several ran-
domly selected seed sets of fixed size and we find 
that seed set composition greatly affects perfor-
mance. Figure 3 illustrates the precision vs. recall 
tradeoff on our best performing corpus Web100 for 
30 random seed sets of size 10 for each of our 50 
gold standard sets (i.e., 1500 trials were tested.) 
Each of the trials performed better than the average 
system performance (the double-lined curve lowest 
in Figure 3). Distinguishing between the various 
data series is not important, however important to 
notice is the very large gap between the preci-
sion/recall curves of the best and worst performing 
random seed sets. On average, the best performing 
seed sets had 42% higher precision and 39% higher 
recall than the worst performing seed set. Similar 
Table 3. Corpora analysis: R-precision and Precision at var-
ious ranks. 95% confidence bounds are all below 0.005?. 
CORPORA R-PREC PREC@25 PREC@50 PREC@100 
Web100 0.404 0.407 0.347 0.278 
Web020 0.356 0.377 0.319 0.250 
Web004 0.264 0.353 0.298 0.239 
Wikipedia 0.315 0.372 0.314 0.253 
?95% confidence bounds are computed over all trials described in Section 5.2. 
Figure 2. Corpus size and quality improve performance. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0 0.1 0.2 0.3 0.4 0.5 0.6
Re
ca
ll
Precision
Corpora?Analysis
(Precision?vs.?Recall)
Web100
Web020
Web004
Wikipedia
CORPORA R-PREC PREC@25 PREC@50 PREC@100 
Web100 0.404 0.407 0.347 0.278 
Web020 0.356 0.377 0.319 0.250 
Web004 0.264 0.353 0.298 0.239 
Wikipedia 0.315 0.372 0.314 0.253 
?95% confidence bounds are computed over all trials described in Section 5.2. 
944
curves were observed for inspected seed sets of 
size 5, 20, 30, and 40. 
Although outside of the scope of this paper, we 
are currently investigating ways to automatically 
detect which seed elements are better than others in 
order to reduce the impact of seed selection effect. 
6.2.3 The Effect of Seed Size 
Here we aim to confirm, with a large empirical 
study, the anecdotal claims in (Pa?ca and Durme 
2008) that few seeds are necessary. We found that 
a) very small seed sets of size 1 or 2 are not suffi-
cient for representing the intended entity set; b) 5-
20 seeds yield on average best performance; and c) 
surprisingly, increasing the seed set size beyond 
20 or 30 on average does not find any new correct 
instances. 
We inspected the effect of seed size on R-
precision over the four corpora. Each seed size 
curve is computed by averaging the system per-
formance over the 30 random trials of all 50 sets. 
For each corpus, R-precision increased sharply 
from seed size 1 to 10 and the curve flattened out 
for seed sizes larger than 20 (figure omitted for 
lack of space). Error analysis on the Web100 cor-
pus shows that once our model has seen 10-20 
seeds, the distributional similarity model seems to 
have enough statistics to discover as many new 
correct instances as it could ever find. Some enti-
ties could never be found by the distributional si-
milarity model since they either do not occur or 
infrequently occur in the corpus or they occur in 
contexts that vary a great deal from other set ele-
ments. Figure 4 illustrates this behavior by plotting 
for each seed set size the rate of increase in discov-
ery of new correct instances (i.e., not found in 
smaller seed set sizes). 
We see that most gold standard instances are 
discovered with the first 5-10 seeds. After the 30th 
seed is introduced, no new correct instances are 
found. An important finding is that the error rate 
does not increase with increased seed set size (see 
Figure 5). This study shows that only few seeds 
(10-20) yield best performance and that adding 
more seeds beyond this does not on average affect 
performance in a positive or negative way. 
Figure 3. Seed set composition greatly affects system performance (with 30 different seed samples of size 10). 
Figure 4. Few new instances are discovered with more 
than 5-20 seeds on Web100 (with 95% confidence). 
Figure 5. Percentage of errors does not increase as 
seed size increases on Web100 (with 95% confidence).
0
0.5
1
1.5
2
2.5
3
0 20 40 60 80 100 120 140 160 180 200
Ra
te
?o
f?N
ew
?C
or
re
ct
?
Seed?Size
Rate?of?New?Correct?Expansions
vs.?Seed?Size
0
0.2
0.4
0.6
0.8
1
0 20 40 60 80 100 120 140 160 180 200
%
?o
f?E
rr
or
Seed?Size
Seed?Size?vs.?%?of?Errors
0
0.2
0.4
0.6
0.8
0 0.2 0.4 0.6 0.8 1
Re
ca
ll
Precision
Web100:?Seed?Selection?Effect
Precision?vs.?Recall
Web100 s010
s010.t01 s010.t02
s010.t03 s010.t04
s010.t05 s010.t06
s010.t07 s010.t08
s010.t09 s010.t10
s010.t11 s010.t12
s010.t13 s010.t14
s010.t15 s010.t16
s010.t17 s010.t18
s010.t19 s010.t20
s010.t21 s010.t22
s010.t23 s010.t24
s010.t25 s010.t26
s010.t27 s010.t28
s010.t29 s010.t30
945
7 Conclusion  
We proposed a highly scalable term similarity al-
gorithm, implemented in the MapReduce frame-
work, and deployed over a 200 billion word crawl 
of the Web. The pairwise similarity between 500 
million terms was computed in 50 hours using 200 
quad-core nodes. We evaluated the impact of the 
large similarity matrix on a set expansion task and 
found that the Web similarity matrix gave a large  
performance boost over a state-of-the-art expan-
sion algorithm using Wikipedia. Finally, we re-
lease to the community a testbed for experimental-
ly analyzing automatic set expansion, which in-
cludes a large collection of nearly random entity 
sets extracted from Wikipedia and over 22,000 
randomly sampled seed expansion trials.  
References 
Abney, S. Parsing by Chunks. In: Robert Berwick, Ste-
ven Abney and Carol Tenny (eds.), Principle-Based 
Parsing. Kluwer Academic Publishers, Dordrecht. 
1991. 
Agirre, E.; Alfonseca, E.; Hall, K.; Kravalova, J.; Pasca, 
M.; and Soroa, A.. 2009. A Study on Similarity and 
Relatedness Using Distributional and WordNet-based 
Approaches. In Proceedings of NAACL HLT 09. 
Ando, R. K. 2000. Latent semantic space: Iterative scal-
ing improves precision of interdocument similarity 
measurement. In Proceedings of SIGIR-00. pp. 216?
223. 
Atterer, M. and Schutze, H., 2006. The Effect of Corpus 
Size when Combining Supervised and Unsupervised 
Training for Disambiguation. In Proceedings of ACL-
06. 
Banko, M. and Brill, E. 2001. Mitigating the paucity of 
data problem. In Proceedings of HLT-2001. San Di-
ego, CA. 
Banko, M.; Cafarella, M.; Soderland, S.; Broadhead, M.; 
Etzioni, O. 2007. Open Information Extraction from 
the Web. In Proceedings of IJCAI. 
Bayardo, R. J.; Ma, Y.; Srikant, R. 2007. Scaling Up 
All-Pairs Similarity Search. In Proceedings of WWW-
07. pp. 131-140. Banff, Canada. 
Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent 
Dirichlet Allocation. Journal of Machine Learning 
Research, 3:993?1022. 
Brill, E. 1995. Transformation-Based Error-Driven 
Learning and Natural Language Processing: A Case 
Study in Part of Speech Tagging. Computational 
Linguistics. 
Broder, A. 1997. On the resemblance and containment 
of documents. In Compression and Complexity of 
Sequences. pp. 21-29. 
Bunescu, R. and Mooney, R. 2004 Collective Informa-
tion Extraction with Relational Markov Networks. In 
Proceedings of ACL-04, pp. 438-445. 
Cao, H.; Jiang, D.; Pei, J.; He, Q.; Liao, Z.; Chen, E.; 
and Li, H. 2008. Context-aware query suggestion by 
mining click-through and session data. In Proceed-
ings of KDD-08. pp. 875?883. 
Chang, W.; Pantel, P.; Popescu, A.-M.; and Gabrilovich, 
E. 2009. Towards intent-driven bidterm suggestion. 
In Proceedings of WWW-09 (Short Paper), Madrid, 
Spain. 
Church, K. and Hanks, P. 1989. Word association 
norms, mutual information, and lexicography. In 
Proceedings of ACL89. pp. 76?83. 
Dean, J. and Ghemawat, S. 2008. MapReduce: Simpli-
fied Data Processing on Large Clusters. Communica-
tions of the ACM, 51(1):107-113. 
Deerwester, S. C.; Dumais, S. T.; Landauer, T. K.; Fur-
nas, G. W.; and Harshman, R. A. 1990. Indexing by 
latent semantic analysis. Journal of the American So-
ciety for Information Science, 41(6):391?407. 
Downey, D.; Broadhead, M; Etzioni, O. 2007. Locating 
Complex Named Entities in Web Text. In Proceed-
ings of IJCAI-07.  
Elsayed, T.; Lin, J.; Oard, D. 2008. Pairwise Document 
Similarity in Large Collections with MapReduce. In 
Proceedings of ACL-08: HLT, Short Papers (Com-
panion Volume). pp. 265?268. Columbus, OH. 
Erk, K. 2007. A simple, similarity-based model for se-
lectional preferences. In Proceedings of ACL-07. pp. 
216?223. Prague, Czech Republic. 
Erk, K. and Pad?, S. 2008. A structured vector space 
model for word meaning in context. In Proceedings 
of EMNLP-08. Honolulu, HI. 
Etzioni, O.; Cafarella, M.; Downey. D.; Popescu, A.; 
Shaked, T; Soderland, S.; Weld, D.; Yates, A. 2005. 
Unsupervised named-entity extraction from the Web: 
An Experimental Study. In Artificial Intelligence, 
165(1):91-134. 
Gorman, J. and Curran, J. R. 2006. Scaling distribution-
al similarity to large corpora. In Proceedings of ACL-
06. pp. 361-368. 
946
Harris, Z. 1985. Distributional Structure. In: Katz, J. J. 
(ed.), The Philosophy of Linguistics. New York: Ox-
ford University Press. pp. 26-47. 
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90. pp. 
268?275. Pittsburgh, PA. 
Hofmann, T. 1999. Probabilistic Latent Semantic Index-
ing. In Proceedings of SIGIR-99. pp. 50?57, Berke-
ley, California. 
Kanerva, P. 1993. Sparse distributed memory and re-
lated models. pp. 50-76. 
Lapata, M. and Keller, F., 2005. Web-based Models for 
Natural Language Processing, In ACM Transactions 
on Speech and Language Processing (TSLP), 2(1). 
Lee, Lillian. 1999. Measures of Distributional Similarity. 
In Proceedings of ACL-93. pp. 25-32. College Park, 
MD. 
Lin, D. 1998. Automatic retrieval and clustering of 
similar words. In Proceedings of COLING/ACL-98. 
pp. 768?774. Montreal, Canada. 
Lund, K., and Burgess, C. 1996. Producing high-
dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments, 
and Computers, 28(2):203?208. 
McCallum, A. and Li, W. Early Results for Named 
Entity Recognition with Conditional Random Fields, 
Feature Induction and Enhanced Lexicons. In Pro-
ceedings of CoNLL-03. 
McQueen, J. 1967. Some methods for classification and 
analysis of multivariate observations. In Proceedings 
of 5th Berkeley Symposium on Mathematics, Statistics 
and Probability, 1:281?298. 
Pa?ca, M. 2007a. Weakly-supervised discovery of 
named entities using web search queries. In Proceed-
ings of CIKM-07. pp. 683-690. 
Pa?ca, M. 2007b. Organizing and Searching the World 
Wide Web of Facts ? Step Two: Harnessing the Wis-
dom of the Crowds. In Proceedings of WWW-07. 
Pa?ca, M. and Durme, B.J. 2008. Weakly-supervised 
Acquisition of Open-Domain Classes and Class 
Attributes from Web Documents and Query Logs. In 
Proceedings of ACL-08. 
Pa?ca, M.; Lin, D.; Bigham, J.; Lifchits, A.; Jain, A. 
2006. Names and Similarities on the Web: Fast Ex-
traction in the Fast Lane. In Proceedings of ACL-
2006. pp. 113-120. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses 
from Text. In Proceedings of KDD-02. pp. 613-619. 
Edmonton, Canada. 
Pantel, P.; Ravichandran, D; Hovy, E.H. 2004. Towards 
terascale knowledge acquisition. In proceedings of 
COLING-04. pp 771-777. 
Ravichandran, D.; Pantel, P.; and Hovy, E. 2005. Ran-
domized algorithms and NLP: Using locality sensi-
tive hash function for high speed noun clustering. In 
Proceedings of ACL-05. pp. 622-629. 
Riloff, E. and Jones, R. 1999 Learning Dictionaries for 
Information Extraction by Multi-Level Boostrapping. 
In Proceedings of AAAI/IAAAI-99. 
Riloff, E. and Shepherd, J. 1997. A corpus-based ap-
proach for building semantic lexicons. In Proceed-
ings of EMNLP-97. 
Rychl?, P. and Kilgarriff, A. 2007. An efficient algo-
rithm for building a distributional thesaurus (and oth-
er Sketch Engine developments). In Proceedings of 
ACL-07, demo sessions. Prague, Czech Republic. 
Sarawagi, S. and Kirpal, A. 2004. Efficient set joins on 
similarity predicates. In Proceedings of SIGMOD '04. 
pp. 74 ?754. New York, NY. 
Sarmento, L.; Jijkuon, V.; de Rijke, M.; and Oliveira, E. 
2007. ?More like these?: growing entity classes from 
seeds. In Proceedings of CIKM-07. pp. 959-962. Lis-
bon, Portugal. 
Turney, P. D., and Littman, M. L. 2003. Measuring 
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21(4). 
Wang, R.C. and Cohen, W.W. 2008. Iterative Set Ex-
pansion of Named Entities using the Web. In Pro-
ceedings of ICDM 2008. Pisa, Italy. 
Wang. R.C. and Cohen, W.W. 2007 Language-
Independent Set Expansion of Named Entities Using 
the Web. In Proceedings of ICDM-07. 
Yuret, D., and Yatbaz, M. A. 2009. The noisy channel 
model for unsupervised word sense disambiguation. 
Computational Linguistics. Under review. 
 
947
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 290?298,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Semi-Automatic Entity Set Refinement 
 
 
Vishnu Vyas and Patrick Pantel 
Yahoo! Labs 
Santa Clara, CA 95054 
{vishnu,ppantel}@yahoo-inc.com 
   
 
 
Abstract 
State of the art set expansion algorithms pro-
duce varying quality expansions for different 
entity types. Even for the highest quality ex-
pansions, errors still occur and manual re-
finements are necessary for most practical 
uses. In this paper, we propose algorithms to 
aide this refinement process, greatly reducing 
the amount of manual labor required. The me-
thods rely on the fact that most expansion er-
rors are systematic, often stemming from the 
fact that some seed elements are ambiguous. 
Using our methods, empirical evidence shows 
that average R-precision over random entity 
sets improves by 26% to 51% when given 
from 5 to 10 manually tagged errors. Both 
proposed refinement models have linear time 
complexity in set size allowing for practical 
online use in set expansion systems. 
1 Introduction 
Sets of named entities are extremely useful in a 
variety of natural language and information re-
trieval tasks. For example, companies such as Ya-
hoo! and Google maintain sets of named entities 
such as cities, products and celebrities to improve 
search engine relevance. 
Manually creating and maintaining large sets of 
named entities is expensive and laborious. In re-
sponse, many automatic and semi-automatic me-
thods of creating sets of named entities have been 
proposed, some are supervised (Zhou and Su, 
2001), unsupervised (Pantel and Lin 2002, Nadeau 
et al 2006), and others semi-supervised (Kozareva 
et al 2008). Semi-supervised approaches are often 
used in practice since they allow for targeting spe-
cific entity classes such as European Cities and 
French Impressionist Painters. Methods differ in 
complexity from simple ones using lexico-
syntactic patterns (Hearst 1992) to more compli-
cated techniques based on distributional similarity 
(Pa?ca 2007a). 
Even for state of the art methods, expansion er-
rors inevitably occur and manual refinements are 
necessary for most practical uses requiring high 
precision (such as for query interpretation at com-
mercial search engines). Looking at expansions 
from state of the art systems such as GoogleSets1 , 
we found systematic errors such as those resulting 
from ambiguous seed instances. For example, con-
sider the following seed instances for the target set 
Roman Gods: 
Minerva, Neptune, Baccus, Juno, 
Apollo 
GoogleSet?s expansion as well others employing 
distributional expansion  techniques consists of a 
mishmash of Roman Gods and celestial bodies, 
originating most likely from the fact that Neptune 
is both a Roman God and a Planet. Below is an 
excerpt of the GoogleSet expansion: 
Mars, Venus, *Moon, Mercury, 
*asteroid, Jupiter, *Earth, 
*comet, *Sonne, *Sun, ? 
The inherent semantic similarity between the errors 
can be leveraged to quickly clean up the expan-
sion. For example, given a manually tagged error 
?asteroid?, a distributional similarity thesaurus 
                                                 
1 http://labs.google.com/sets 
290
such as (Lin 1998)2 can identify comet as similar to 
asteroid and therefore potentially also as an error. 
This method has its limitations since a manually 
tagged error such as Earth would correctly remove 
Moon and Sun, but it would also incorrectly re-
move Mars, Venus and Jupiter since they are also 
similar to Earth3. 
In this paper, we propose two algorithms to im-
prove the precision of automatically expanded enti-
ty sets by using minimal human negative 
judgments. The algorithms leverage the fact that 
set expansion errors are systematically caused by 
ambiguous seed instances which attract incorrect 
instances of an unintended entity type. We use dis-
tributional similarity and sense feature modeling to 
identify such unintended entity types in order to 
quickly clean up errors with minimal manual labor. 
We show empirical evidence that average R-
precision over random entity sets improves by 26% 
to 51% when given from 5 to 10 manually tagged 
errors. Both proposed refinement models have li-
near time complexity in set size allowing for prac-
tical online use in set expansion systems. 
The remainder of this paper is organized as fol-
lows. In the next section we review related work 
and position our contribution within its landscape. 
Section 3 presents our task of dynamically model-
ing the similarity of a set of words and describes 
algorithms for refining sets of named entities. The 
datasets and our evaluation methodology used to 
perform our experiments are presented in Section 4 
and in Section 5 we describe experimental results. 
Finally, we conclude with some discussion and 
future work. 
2 Related Work 
There is a large body of work for automatically 
building sets of named entities using various tech-
niques including supervised, unsupervised and 
semi-supervised methods. Supervised techniques 
use large amounts of training data to detect and 
classify entities into coarse grained classes such as 
People, Organizations, and Places (Bunescu and 
Mooney 2004; Etzioni et al 2005). On the other 
hand, unsupervised methods require no training 
                                                 
2 See http://demo.patrickpantel.com/ for a demonstration of 
the distributional thesaurus.  
3 In practice, this problem is rare since most terms that are 
similar in one of their senses tend not to be similar in their 
other senses. 
data and rely on approaches such as clustering, 
targeted patterns and co-occurrences to extract sets 
of entities (Pantel and Lin 2002; Downey et al 
2007). 
Semi-supervised approaches are often used in 
practice since they allow for targeting specific enti-
ty classes. These methods rely on a small set of 
seed examples to extract sets of entities. They ei-
ther are based on distributional approaches or em-
ploy lexico-syntactic patterns to expand a small set 
of seeds to a larger set of candidate expansions. 
Some methods such as (Riloff and Shepherd 1997; 
Riloff and Jones 1999; Banko et al 2007;Pa?ca 
2007a)  use lexico-syntactic patterns to expand a 
set of seeds from web text and query logs. Others 
such as (Pa?ca et al 2006; Pa?ca 2007b; Pa?ca and 
Durme 2008) use distributional approaches. Wang 
and Cohen (2007) use structural cues in semi-
structured text to expand sets of seed elements. In 
all methods however, expansion errors inevitably 
occur. This paper focuses on the task of post 
processing any such system?s expansion output 
using minimal human judgments in order to re-
move expansion errors. 
Using user feedback to improve a system?s per-
formance is a common theme within many infor-
mation retrieval and machine learning tasks. One 
form of user feedback is active learning (Cohn et 
al. 1994), where one or more classifiers are used to 
focus human annotation efforts on the most benefi-
cial test cases. Active learning has been successful-
ly applied to various natural language tasks such as 
parsing (Tang et al 2001), POS tagging (Dagan 
and Engelson 1995) and providing large amounts 
of annotations for common natural language 
processing tasks such as word sense disambigua-
tion (Banko and Brill 2001). Relevance feedback is 
another popular feedback paradigm commonly 
used in information retrieval (Harman 1992), 
where user feedback (either explicit or implicit) is 
used to refine the search results of an IR system. 
Relevance feedback has been successfully applied 
to many IR applications including content-based 
image retrieval (Zhouand Huang 2003) and web 
search (Vishwa et al 2005). Within NLP applica-
tions relevance feedback has also been used to 
generate sense tagged examples for WSD tasks 
(Stevenson et al 2008), and Question Answering 
(Negri 2004). Our methods use relevance feedback 
in the form of negative examples to refine the re-
sults of a set expansion system. 
291
3 Dynamic Similarity Modeling 
The set expansion algorithms discussed in Section 
2 often produce high quality entity sets, however 
inevitably errors are introduced. Applications re-
quiring high precision sets must invest significant-
ly in editorial efforts to clean up the sets. Although 
companies like Yahoo! and Google can afford to 
routinely support such manual labor, there is a 
large opportunity to reduce the refinement cost 
(i.e., number of required human judgments).  
Recall the set expansion example of Roman 
Gods from Section 1. Key to our approach is the 
hypothesis that most expansion errors result from 
some systematic cause. Manual inspection of ex-
pansions from GoogleSets and distributional set 
expansion techniques revealed that most errors are 
due to the inherent ambiguity of seed terms (such 
as Neptune in our example) and data sparseness 
(such as Sonne in our example, a very rare term). 
The former kind of error is systematic and can be 
leveraged by an automatic method by assuming 
that any entity semantically similar to an identified 
error will also be erroneous. 
In this section, we propose two methods for le-
veraging this hypothesis. In the first method, de-
scribed in Section 3.1, we use a simple 
distributional thesaurus and remove all entities 
which are distributionally similar to manually iden-
tified errors. In the second method, described in 
Section 3.2, we model the semantics of the seeds 
using distributional features and then dynamically 
change the feature space according to the manually 
identified errors and rerank the entities in the set. 
Both methods rely on the following two observa-
tions: 
a) Many expansion errors are systematically 
caused by ambiguous seed examples which 
draw in several incorrect entities of its unin-
tended senses (such as seed Neptune in our 
Roman Gods example which drew in celestial 
bodies such as Earth and Sun); 
b) Entities which are similar in one sense are 
usually not similar in their other senses. For 
example, Apple and Sun are similar in their 
Company sense but their other senses (Fruit 
and Celestial Body) are not similar. Our exam-
ple in Section 1 illustrates a rare counterexam-
ple where Neptune and Mercury are similar in 
both their Planets and Roman Gods senses. 
Task Outline: Our task is to remove errors from 
entity sets by using a minimal amount of manual 
judgments. Incorporating feedback into this 
process can be done in multiple ways. The most 
flexible system would allow a judge to iteratively 
remove as many errors as desired and then have 
the system automatically remove other errors in 
each iteration. Because it is intractable to test arbi-
trary numbers of manually identified errors in each 
iteration, we constrain the judge to identify at most 
one error in each iteration. 
Although this paper focuses solely on removing 
errors in an entity set, it is also possible to improve 
expanded sets by using feedback to add new ele-
ments to the sets. We consider this task out of 
scope for this paper.  
3.1 Similarity Method (SIM) 
Our first method directly models observation a) in 
the previous section. Following Lin (1998), we 
model the similarity between entities using the dis-
tributional hypothesis, which states that similar 
terms tend to occur in similar contexts (Harris 
1985). A semantic model can be obtained by re-
cording the surrounding contexts for each term in a 
large collection of unstructured text. Methods dif-
fer in their definition of a context (e.g., text win-
dow or syntactic relations), or a means to weigh 
contexts (e.g., frequency, tf-idf, pointwise mutual 
information), or ultimately in measuring the simi-
larity between two context vectors (e.g., using Euc-
lidean distance, Cosine, Dice). In this paper, we 
use a text window of size 1, we weigh our contexts 
using pointwise mutual information, and we use 
the cosine score to compute the similarity between 
context vectors (i.e., terms). Section 5.1 describes 
our source corpus and extraction details. Compu-
ting the full similarity matrix for many terms over 
a very large corpus is computationally intensive. 
Our specific implementation follows the one pre-
sented in (Bayardo et al 2007). 
The similarity matrix computed above is then 
directly used to refine entity sets. Given a manual-
ly identified error at each iteration, we automatical-
ly remove each entity in the set that is found to be 
semantically similar to the error. The similarity 
threshold was determined by manual inspection 
and is reported in Section 5.1. 
Due to observation b) in the previous section, 
we expect that this method will perform poorly on 
292
entity sets such as the one presented in our exam-
ple of Section 1 where the manual removal of 
Earth would likely remove correct entities such as 
Mars, Venus and Jupiter. The method presented in 
the next section attempts to alleviate this problem. 
3.2 Feature Modification Method (FMM) 
Under the distributional hypothesis, the semantics 
of a term are captured by the contexts in which it 
occurs. The Feature Modification Method (FMM), 
in short, attempts to automatically discover the 
incorrect contexts of the unintended senses of seed 
elements and then filters out expanded terms 
whose contexts do not overlap with the other con-
texts of the seed elements. 
Consider the set of seed terms S and an errone-
ous expanded instance e. In the SIM method of 
Section 3.1 all set elements that have a feature vec-
tor (i.e., context vector) similar to e are removed. 
The Feature Modification Method (FMM) instead 
tries to identify the subset of features of the error e 
which represent the unintended sense of the seed 
terms S. For example, let S = {Minerva, Neptune, 
Baccus, Juno, Apollo}. Looking at the contexts of 
these words in a large corpus, we construct a cen-
troid context vector for S by taking a weighted av-
erage of the contexts of the seeds in S. In 
Wikipedia articles we see contexts (i.e., features) 
such as4: 
attack, kill, *planet, destroy, 
Goddess, *observe, statue, *launch, 
Rome, *orbit, ? 
Given an erroneous expansion such as e = Earth, 
we postulate that removing the intersecting fea-
tures from Earth?s feature vector and the above 
feature vector will remove the unintended Planet 
sense of the seed set caused by the seed element 
Neptune. The intersecting features that are re-
moved are bolded in the above feature vector for S. 
The similarity between this modified feature vector 
for S and all entities in the expansion set can be 
recomputed as described in Section 3.1. Entities 
with a low similarity score are removed from the 
expanded set since they are assumed to be part of 
the unintended semantic class (Planet in this ex-
ample). 
Unlike the SIM method from Section 3.1, this 
method is more stable with respect to observation 
                                                 
4 The full feature vector for these and all other terms in Wiki-
pedia can be found at http://demo.patrickpantel.com/.. 
b) in Section 3. We showed that SIM would incor-
rectly remove expansions such as Mars, Venus and 
Jupiter given the erroneous expansion Earth. The 
FMM method would instead remove the Planet 
features from the seed feature vectors and the re-
maining features would still overlap with Mars, 
Venus and Jupiter?s Roman God sense. 
Efficiency: FMM requires online similarity com-
putations between centroid vectors and all ele-
ments of the expanded set. For large corpora such 
as Wikipedia articles or the Web, feature vectors 
are large and storing them in memory and perform-
ing similarity computations repeatedly for each 
editorial judgment is computationally intensive. 
For example, the size of the feature vector for a 
single word extracted from Wikipedia can be in the 
order of a few gigabytes. Storing the feature vec-
tors for all candidate expansions and the seed set is 
inefficient and too slow for an interactive system. 
The next section proposes a solution that makes 
this computation very fast, requires little memory, 
and produces near perfect approximations of the 
similarity scores. 
3.3 Approximating Cosine Similarity 
There are engineering optimizations that are avail-
able that allow us to perform a near perfect approx-
imation of the similarity computation from the 
previous section. The proposed method requires us 
to only store the shared features between the cen-
troid and the words rather than the complete fea-
ture vectors, thus reducing our space requirements 
dramatically. Also, FMM requires us to repeatedly 
calculate the cosine similarity between a modified 
centroid feature vector and each candidate expan-
sion at each iteration. Without the full context vec-
tors of all candidate expansions, computing the 
exact cosine similarity is impossible. Given, how-
ever, the original cosine scores between the seed 
elements and the candidate expansions before the 
first refinement iteration as well as the shared fea-
tures, we can approximate with very high accuracy 
the updated cosine score between the modified 
centroid and each candidate expansion. Our me-
thod relies on the fact that features (i.e., contexts) 
are only ever removed from the original centroid ? 
no new features are ever added. 
Let ? be the original centroid representing the 
seed instances. Given an expansion error e, FMM 
creates a modified centroid by removing all fea-
293
tures intersecting between e and ?. Let ?' be this 
modified centroid. FMM requires us to compute 
the similarity between ?' and all candidate expan-
sions x as: 
cos x, ? ? ( )= xi ? ? i?
x ? ? ?  
where  i iterates over the feature space. 
In our efficient setting, the only element that we 
do not have for calculating the exact cosine simi-
larity is the norm of x, x . Given that we have the 
original cosine similarity score, cos(x, ?) and that 
we have the shared features between the original 
centroid ? and the candidate expansion x we can 
calculate x  as: 
x = xi?i?? ? cos x,?( )  
Combining the two equations, have: 
cos x, ? ? ( )= cos x,?( )? xi ? ? i?
xi?i? ?
?
? ?  
In the above equation, the modified cosine score 
can be considered as an update to the original co-
sine score, where the update depends only on the 
shared features and the original centroid. The 
above update equation can be used to recalculate 
the similarity scores without resorting to an expen-
sive computation involving complete feature vec-
tors. 
Storing the original centroid is expensive and 
can be approximated instead from only the shared 
features between the centroid and all instances in 
the expanded set. We empirically tested this ap-
proximation by comparing the cosine scores be-
tween the candidate expansions and both the true 
centroid and the approximated centroid. The aver-
age error in cosine score was 9.5E-04 ? 7.83E-05 
(95% confidence interval). 
4 Datasets and Baseline Algorithm 
We evaluate our algorithms against manually 
scraped gold standard sets, which were extracted 
from Wikipedia to represent a random collection of 
concepts. Section 4.1 discusses the gold standard 
sets and the criteria behind their selection. To 
present a statistically significant view of our results 
we generated a set of trials from gold standard sets 
to use as seeds for our seed set expansion algo-
rithm. Also, in section 4.2 we discuss how we can 
simulate editorial feedback using our gold standard 
sets. 
4.1 Gold Standard Entity Sets 
The gold standard sets form an essential part of our 
evaluation. These sets were chosen to represent a 
single concept such as Countries and Archbishops 
of Canterbury. These sets were selected from the 
List of pages from Wikipedia5. We randomly 
sorted the list of every noun occurring in Wikipe-
dia. Then, for each noun we verified whether or 
not it existed in a Wikipedia list, and if so we ex-
tracted this list ? up to a maximum of 50 lists. If a 
noun belonged to multiple lists, the authors chose 
the list that seemed most appropriate. Although 
this does not generate a perfect random sample, 
diversity is ensured by the random selection of 
nouns and relevancy is ensured by the author adju-
dication. 
Lists were then scraped from the Wikipedia 
website and they went through a manual cleanup 
process which included merging variants. . The 50 
sets contain on average 208 elements (with a min-
imum of 11 and a maximum of 1116 elements) for 
a total of 10,377 elements. The final gold standard 
lists contain 50 sets including classical pianists, 
Spanish provinces, Texas counties, male tennis 
players, first ladies, cocktails, bottled water 
brands, and Archbishops of Canterbury6. 
4.2 Generation of Experimental Trials 
To provide a statistically significant view of the 
performance of our algorithm, we created more 
than 1000 trials as follows. For each of the gold 
standard seed sets, we created 30 random sortings. 
These 30 random sortings were then used to gener-
ate trial seed sets with a maximum size of 20 
seeds. 
4.3 Simulating User Feedback and Baseline 
Algorithm 
User feedback forms an integral part of our algo-
rithm. We used the gold standard sets to judge the 
                                                 
5 In this paper, extractions from Wikipedia are taken from a 
snapshot of the resource in December 2007. 
6 The gold standard is available for download at 
http://www.patrickpantel.com/cgi-bin/Web/Tools/getfile.pl? 
type=data&id=sse-gold/wikipedia.20071218.goldsets.tgz 
294
candidate expansions. The judged expansions were 
used to simulate user feedback by marking those 
candidate expansions that were incorrect. The first 
candidate expansion that was marked incorrect in 
each editorial iteration was used as the editor?s 
negative example and was given to the system as 
an error. 
In the next section, we report R-precision gains 
at each iteration in the editorial process for our two 
methods described in Section 3. Our baseline me-
thod simply measures the gains obtained by re-
moving the first incorrect entry in a candidate 
expansion set at each iteration. This simulates the 
process of manually cleaning a set by removing 
one error at a time. 
5 Experimental Results 
5.1 Experimental Setup 
Wikipedia5 served as the source corpus for our al-
gorithms described in Sections 3.1 and 3.2. All 
articles were POS-tagged using (Brill 1995) and 
later chunked using a variant of (Abney 1991). 
Corpus statistics from this processed text were col-
lected to build the similarity matrix for the SIM 
method (Section 3.1) and to extract the features 
required for the FMM method (Section 3.2). In 
both cases corpus statistics were extracted over the 
semi-syntactic contexts (chunks) to approximate  
term meanings. The minimum similarity thresholds 
were experimentally set to 0.15 and 0.11 for the 
SIM and FMM algorithms respectively. 
Each experimental trial described in Section 
4.2, which consists of a set of seed instances of one 
of our 50 random semantic classes, was expanded 
using a variant of the distributional set expansion 
algorithm from Sarmento et al (2007). The expan-
sions were judged against the gold standard and 
each candidate expansion was marked as either 
correct or incorrect. This set of expanded and 
judged candidate files were used as inputs to the 
algorithms described in Sections 3.1 and 3.2. 
Choosing the first candidate expansion that was 
judged as incorrect simulated our user feedback. 
This process was repeated for each iteration of the 
algorithm and results are reported for 10 iterations. 
The outputs of our algorithms were again 
judged against the gold standard lists and the per-
formance was measured in terms of precision gains 
over the baseline at various ranks. Precision gain 
for an algorithm over a baseline is the percentage 
increase in precision for the same values of para-
meters of the algorithm over the baseline. Also, as 
the size of our gold standard lists vary, we report 
another commonly used statistic, R-precision. R-
precision for any set is the precision at the size of 
the gold standard set. For example, if a gold stan-
dard set contains 20 elements, then R-precision for 
any set expansion is measured as the precision at 
rank 20. The average R-precision over each set is 
then reported. 
5.2 Quantitative Analysis 
Table 1 lists the performance of our baseline algo-
rithm (Section 4.3) and our proposed methods SIM 
and FMM (Sections 3.1 and 3.2) in terms of their 
R-precision with 95% confidence bounds over 10 
iterations of each algorithm. 
The FMM of Section 3.2 is the best performing 
method in terms of R-precision reaching a maxi-
mum value of 0.322 after the 10th iteration. For 
small numbers of iterations, however, the SIM me-
thod outperforms FMM since it is bolder in its re-
finements by removing all elements similar to the 
tagged error. Inspection of FMM results showed 
that bad instances get ranked lower in early itera-
tions but it is only after 4 or 5 iterations that they 
get pushed passed the similarity threshold (ac-
counting for the low marginal increase in precision 
gain for FMM in the first 4 to 5 iterations). 
FMM outperforms the SIM method by an aver-
age of 4% increase in performance (13% im-
provement after 10 iterations). However both the 
FMM and the SIM method are able to outperform 
Table 1. R-precision of the three methods with 95% confi-
dence bounds. 
ITERATION BASELINE SIM FMM 
1 0.219?0.012 0.234?0.013 0.220?0.015 
2 0.223?0.013 0.242?0.014 0.227?0.017 
3 0.227?0.013 0.251?0.015 0.235?0.019 
4 0.232?0.013 0.26?0.016 0.252?0.021 
5 0.235?0.014 0.266?0.017 0.267?0.022 
6 0.236?0.014 0.269?0.017 0.282?0.023 
7 0.238?0.014 0.273?0.018 0.294?0.023 
8 0.24?0.014 0.28?0.018 0.303?0.024 
9 0.242?0.014 0.285?0.018 0.315?0.025 
10 0.243?0.014 0.286?0.018 0.322?0.025 
295
the baseline method. Using the FMM method one 
would achieve an average of 17% improvement in 
R-precision over manually cleaning up the set 
(32.5% improvement after 10 iterations). Using the 
SIM method one would achieve an average of 13% 
improvement in R-precision over manually clean-
ing up the set (17.7% improvement after 10 itera-
tions). 
5.3 Intrinsic Analysis of the SIM Algorithm 
Figure 1 shows the precision gain of the similarity 
matrix based algorithm over the baseline algo-
rithm. The results are shown for precision at ranks 
1, 2, 5, 10, 25, 50 and 100, as well as for R-
precision. The results are also shown for the first 
10 iterations of the algorithm.  
SIM outperforms the baseline algorithm for all 
ranks and increases in gain throughout the 10 itera-
tions. As the number of iterations increases the 
change in precision gain levels off. This behavior 
can be attributed to the fact that we start removing 
errors from top to bottom and in each iteration the 
rank of the error candidate provided to the algo-
rithm is lower than in the previous iteration. This 
results in errors which are not similar to any other 
candidate expansions. These are random errors and 
the discriminative capacity of this method reduces 
severely. 
Figure 1 also shows that the precision gain of 
the similarity matrix algorithm over the baseline 
algorithm is higher at ranks 1, 2 and 5.  Also, the 
performance increase drops at ranks 50 and 100. 
This is because low ranks contain candidate expan-
sions that are random errors introduced due to data 
sparsity. Such unsystematic errors are not detecta-
ble by the SIM method. 
5.4 Intrinsic Analysis of the FMM Algorithm 
The feature modification method of Section 3.2 
shows similar behavior to the SIM method, how-
ever as Figure 2 shows, it outperforms SIM me-
thod in terms of precision gain for all values of 
ranks tested. This is because the FMM method is 
able to achieve fine-grained control over what it 
removes and what it doesn?t, as described in Sec-
tion 5.2. 
Another interesting aspect of FMM is illu-
strated in the R-precision curve. There is a sudden 
jump in precision gain after the fifth iteration of 
the algorithm. In the first iterations only few errors 
are pushed beneath the similarity threshold as cen-
troid features intersecting with tagged errors are 
slowly removed. As the feature vector for the cen-
troid gets smaller and smaller, remaining features 
look more and more unambiguous to the target 
entity type and erroneous example have less 
chance of overlapping with the centroid causing 
them to be pushed pass the conservative similarity 
threshold. Different conservative thresholds 
yielded similar curves. High thresholds yield bad 
performance since all but the only very prototypi-
cal set instances are removed as errors. 
The R-precision measure indirectly models re-
call as a function of the target coverage of each set. 
We also directly measured recall at various ranks 
 
Figure 1. Precision gain over baseline algorithm for SIM 
method. 
Figure 2. Precision gain over baseline algorithm for FMM 
method.
296
and FMM outperformed SIM at all ranks and itera-
tions. 
5.5 Discussion 
In this paper we proposed two techniques which 
use user feedback to remove systematic errors in 
set expansion systems caused by ambiguous seed 
instances. Inspection of expansion errors yielded 
other types of errors. 
First, model errors are introduced in candidate 
expansion sets by noise from various pre-
processing steps involved in generating the expan-
sions. Such errors cause incorrect contexts (or fea-
tures) to be extracted for seed instances and 
ultimately can cause erroneous expansions to be 
produced. These errors do not seem to be systemat-
ic and are hence not discoverable by our proposed 
method. 
Other errors are due to data sparsity. As the fea-
ture space can be very large, the difference in simi-
larity between a correct candidate expansion and 
an incorrect expansion can be very small for sparse 
entities. Previous approaches have suggested re-
moving candidate expansions for which too few 
statistics can be extracted, however at the great 
cost of recall (and lower R-precision). 
6 Conclusion 
In this paper we presented two algorithms for im-
proving the precision of automatically expanded 
entity sets by using minimal human negative 
judgments. We showed that systematic errors 
which arise from the semantic ambiguity inherent 
in seed instances can be leveraged to automatically 
refine entity sets. We proposed two techniques: 
SIM which boldly removes instances that are dis-
tributionally similar to errors, and FMM which 
more conservatively removes features from the 
seed set representing its unintended (ambiguous) 
concept in order to rank lower potential errors. 
We showed empirical evidence that average R-
precision over random entity sets improves by 26% 
to 51% when given from 5 to 10 manually tagged 
errors. These results were reported by testing the 
refinement algorithms on a set of 50 randomly 
chosen entity sets expanded using a state of the art 
expansion algorithm. Given very small amounts of 
manual judgments, the SIM method outperformed 
FMM (up to 4 manual judgments). FMM outper-
formed the SIM method given more than 6 manual 
judgments. Both proposed refinement models have 
linear time complexity in set size allowing for 
practical online use in set expansion systems. 
This paper only addresses techniques for re-
moving erroneous entities from expanded entity 
sets. A complimentary way to improve perfor-
mance would be to investigate the addition of rele-
vant candidate expansions that are not already in 
the initial expansion. We are currently investigat-
ing extensions to FMM that can efficiently add 
new candidate expansions to the set by computing 
the similarity between modified centroids and all 
terms occurring in a large body of text. 
We are also investigating ways to use the find-
ings of this work to a priori remove ambiguous 
seed instances (or their ambiguous contexts) before 
running the initial expansion algorithm. It is our 
hope that most of the errors identified in this work 
could be automatically discovered without any 
manual judgments. 
References 
Abney, S. Parsing by Chunks. 1991. In: Robert Ber-
wick, Steven Abney and Carol Tenny (eds.), Prin-
ciple-Based Parsing. Kluwer Academic Publishers, 
Dordrecht. 
Banko, M. and Brill, E. 2001. Scaling to very large cor-
pora for natural language disambiguation. In Pro-
ceedings of ACL-2001.pp 26-33. Morristown, NJ. 
Banko, M.; Cafarella, M.; Soderland, S.; Broadhead, 
M.; Etzioni, O. 2007. Open Information Extraction 
from the Web. In Proceedings of IJCAI-07. 
Bayardo, R. J; Yiming Ma,; Ramakrishnan Srikant.; 
Scaling Up All-Pairs Similarity Search. In Proc. of 
the 16th Int'l Conf. on World Wide Web. pp 131-140 
2007. 
Brill, E. 1995. Transformation-Based Error-Driven 
Learning and Natural Language Processing: A Case 
Study in Part of Speech Tagging. Computational 
Linguistics. 
Bunescu, R. and Mooney, R. 2004 Collective Informa-
tion Extraction with Relational Markov Networks. In 
Proceedings of ACL-04.pp. 438-445. 
Cohn, D. A., Atlas, L., and Ladner, R. E. 1994. Improv-
ing Generalization with Active Learning. Machine 
Learning, 15(2):201-221. Springer, Netherlands. 
Dagan, I. and Engelson, S. P. 1995. Selective Sampling 
in Natural Language Learning. In Proceedings of 
IJCAI-95 Workshop on New Approaches to Learning 
for Natural Language Processing. Montreal, Canada. 
Downey, D.; Broadhead, M; Etzioni, O. 2007. Locating 
Complex Named Entities in Web Text. In Proceed-
ings of IJCAI-07. 
297
Etzioni, O.; Cafarella, M.; Downey. D.; Popescu, A.; 
Shaked, T; Soderland, S.; Weld, D.; Yates, A. 2005. 
Unsupervised named-entity extraction from the Web: 
An Experimental Study. In Artificial Intelligence, 
165(1):91-134. 
Harris, Z. 1985. Distributional Structure. In: Katz, J. J. 
(ed.), The Philosophy of Linguistics. New York: Ox-
ford University Press. pp. 26-47. 
Harman, D. 1992. Relevance feedback revisited. In 
Proceeedings of SIGIR-92. Copenhagen, Denmark. 
Hearst, M. A. 1992.Automatic acquisition of hyponyms 
from large text corpora.In Proceedings of COLING-
92. Nantes, France. 
Kozareva, Z., Riloff, E. and Hovy, E. 2008. Semantic 
Class Learning from the Web with Hyponym Pattern 
Linkage Graphs.In Proceedings of ACL-08.pp 1048-
1056. Columbus, OH 
Lin, D. 1998.Automatic retrieval and clustering of simi-
lar words.In Proceedings of COLING/ACL-98.pp. 
768?774. Montreal, Canada. 
Nadeau, D., Turney, P. D. and Matwin., S. 2006. Unsu-
pervised Named-Entity Recognition: Generating Ga-
zetteers and Resolving Ambiguity. In Advances in 
Artifical Intelligence.pp 266-277. Springer Berlin, 
Heidelberg. 
Negri, M. 2004. Sense-based blind relevance feedback 
for question answering. In Proceedings of SIGIR-04 
Workshop on Information Retrieval For Question 
Answering (IR4QA). Sheffield, UK, 
Pantel, P. and Lin, D. 2002. Discovering Word Senses 
from Text. In Proceedings of KDD-02.pp. 613-619. 
Edmonton, Canada. 
Pa?ca, M. 2007a.Weakly-supervised discovery of 
named entities using web search queries. In Proceed-
ings of CIKM-07.pp. 683-690. 
Pasca, M. 2007b. Organizing and Searching the World 
Wide Web of Facts - Step Two: Harnessing the Wis-
dom of the Crowds. In Proceedings of WWW-07. pp. 
101-110. 
Pa?ca, M.; Lin, D.; Bigham, J.; Lifchits, A.; Jain, A. 
2006. Names and Similarities on the Web: Fact Ex-
traction in the Fast Lane. In Proceedings of ACL-
2006.pp. 113-120. 
Pa?ca, M. and Durme, B.J. 2008. Weakly-supervised 
Acquisition of Open-Domain Classes and Class 
Attributes from Web Documents and Query Logs. In 
Proceedings of ACL-08. 
Riloff, E. and Jones, R. 1999 Learning Dictionaries for 
Information Extraction by Multi-Level Boostrap-
ping.In Proceedings of AAAI/IAAAI-99. 
Riloff, E. and Shepherd, J. 1997. A corpus-based ap-
proach for building semantic lexicons.In Proceedings 
of EMNLP-97. 
Sarmento, L.; Jijkuon, V.; de Rijke, M.; and Oliveira, E. 
2007. ?More like these?: growing entity classes from 
seeds. In Proceedings of CIKM-07. pp. 959-962. Lis-
bon, Portugal. 
Stevenson, M., Guo, Y. and  Gaizauskas, R. 2008. Ac-
quiring Sense Tagged Examples using Relevance 
Feedback. In Proceedings ofCOLING-08. Manches-
ter UK. 
Tang, M., Luo, X., and Roukos, S. 2001. Active learn-
ing for statistical natural language parsing.In Pro-
ceedings of ACL-2001.pp 120 -127. Philadelphia, 
PA. 
Vishwa. V, Wood, K., Milic-Frayling, N. and Cox, I. J. 
2005. Comparing Relevance Feedback Algorithms 
for Web Search. In Proceedings of WWW 2005. Chi-
ba, Japan. 
Wang. R.C. and Cohen, W.W. 2007.Language-
Independent Set Expansion of Named Entities Using 
the Web.In Proceedings of ICDM-07. 
Zhou, X. S. and Huang, S. T. 2003. Relevance Feedback 
in Image Retrieval: A Comprehensive Review - 
Xiang Sean Zhou, Thomas S. Huang Multimedia 
Systems. pp 8:536-544. 
Zhou, G. and Su, J. 2001. Named entity recognition 
using an HMM-based chunk tagger. In Proceedings 
of ACL-2001.pp. 473-480. Morristown, NJ. 
 
 
298
