Language Model Adaptation for Statistical Machine Translation 
with Structured Query Models 
Bing Zhao    Matthias Eck     Stephan Vogel 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA, 15213, USA 
{bzhao, matteck, vogel+}@cs.cmu.edu 
 
 
Abstract 
We explore unsupervised language model 
adaptation techniques for Statistical Machine 
Translation.  The hypotheses from the 
machine translation output are converted into 
queries at different levels of representation 
power and used to extract similar sentences 
from very large monolingual text collection.  
Specific language models are then build from 
the retrieved data and interpolated with a 
general background model.  Experiments 
show significant improvements when 
translating with these adapted language 
models. 
1 Introduction 
Language models (LM) are applied in many 
natural language processing applications, such as 
speech recognition and machine translation, to 
encapsulate syntactic, semantic and pragmatic 
information.  For systems which learn from given 
data we frequently observe a severe drop in 
performance when moving to a new genre or new 
domain.  In speech recognition a number of 
adaptation techniques have been developed to cope 
with this situation.  In statistical machine 
translation we have a similar situation, i.e. estimate 
the model parameter from some data, and use the 
system to translate sentences which may not be 
well covered by the training data.  Therefore, the 
potential of adaptation techniques needs to be 
explored for machine translation applications. 
Statistical machine translation is based on the 
noisy channel model, where the translation 
hypothesis is searched over the space defined by a 
translation model and a target language (Brown et 
al, 1993).  Statistical machine translation can be 
formulated as follows: 
)()|(maxarg)|(maxarg* tPtsPstPt
tt
?==  
where t is the target sentence, and s is the source 
sentence. P(t) is the target language model and 
P(s|t) is the translation model.  The argmax 
operation is the search, which is done by the 
decoder. 
In the current study we modify the target 
language model P(t), to represent the test data 
better, and thereby improve the translation quality.  
(Janiszek, et al 2001) list the following approaches 
to language model adaptation: 
? Linear interpolation of a general and a domain 
specific model (Seymore, Rosenfeld, 1997). 
? Back off of domain specific probabilities with 
those of a specific model (Besling, Meier, 
1995). 
? Retrieval of documents pertinent to the new 
domain and training a language model on-line 
with those data (Iyer, Ostendorf, 1999, 
Mahajan et. al. 1999). 
? Maximum entropy, minimum discrimination 
adaptation (Chen, et. al., 1998). 
? Adaptation by linear transformation of vectors 
of bigram counts in a reduced space (DeMori, 
Federico, 1999). 
? Smoothing and adaptation in a dual space via 
latent semantic analysis, modeling long-term 
semantic dependencies, and trigger 
combinations.  (J. Bellegarda, 2000). 
Our approach can be characterized as 
unsupervised data augmentation by retrieval of 
relevant documents from large monolingual 
corpora, and interpolation of the specific language 
model, build from the retrieved data, with a 
background language model.  To be more specific, 
the following steps are carried out to do the 
language model adaptation.  First, a baseline 
statistical machine translation system, using a large 
general language model, is applied to generate 
initial translations.  Then these translations 
hypotheses are reformulated as queries to retrieve 
similar sentences from a very large text collection.  
A small domain specific language model is build 
using the retrieved sentences and linearly 
interpolated with the background language model.  
This new interpolated language model in applied in 
a second decoding run to produce the final 
translations.  
There are a number of interesting questions 
pertaining to this approach: 
? Which information can and should used to 
generate the queries: the first-best translation 
only, or also translation alternatives. 
? How should we construct the queries, just as 
simple bag-of-words, or can we incorporate more 
structure to make them more powerful. 
? How many documents should be retrieved to 
build the specific language models, and on what 
granularity should this be done, i.e. what is a 
document in the information retrieval process. 
 
The paper is structured as follows:  section 2 
outlines the sentence retrieval approach, and three 
bag-of-words query models are designed and 
explored; structured query models are introduced 
in section 3.  In section 4 we present translation 
experiments are presented for the different query.  
Finally, summary is given in section 5. 
2 LM Adaptation via Sentence Retrieval 
Our language model adaptation is an unsupervised 
data augmentation approach guided by query 
models.  Given a baseline statistical machine 
translation system, the language model adaptation 
is done in several steps shown as follows: 
 
? Generate a set of initial translation 
hypotheses H = {h1 ?hn} for source 
sentences s, using either the baseline MT 
system with the background language 
model or only the translation model 
? Use H  to build query 
? Use query to retrieve relevant sentences 
from the large corpus  
? Build specific language models from 
retrieved sentences 
? Interpolate the specific language model 
with the background language 
? Re-translate sentences s with adapted 
language model 
 
Figure-1: Adaptation Algorithm 
 
The specific language model )|( hwP iA  and the 
general background model )|( hwP iB  are combined 
using linear interpolation: 
)|()1()|()|(? hwPhwPhwP iAiBi ?? ?+=  (1)
The interpolation factor ?  can be simply 
estimated using cross validation or a grid search. 
As an alternative to using translations for the 
baseline system, we will also describe an approach, 
which uses partial translations of the source 
sentence, using the translation model only.  In this 
case, no full translation needs to be carried out in 
the first step; only information from the translation 
model is used.  
Our approach focuses on query model building, 
using different levels of knowledge representations 
from the hypothesis set or from the translation 
model itself.  The quality of the query models is 
crucial to the adapted language model?s 
performance.  Three bag-of-words query models 
are proposed and explained in the following 
sections. 
2.1 Sentence Retrieval Process 
In our sentence retrieval process, the standard tf/idf 
(term frequency and inverse document frequency) 
term weighting scheme is used.  The queries are 
built from the translation hypotheses.  We follow 
(Eck, et al, 2004) in considering each sentence in 
the monolingual corpus as a document, as they 
have shown that this gives better results compared 
to retrieving entire news stories. 
Both the query and the sentences in the text 
corpus are converted into vectors by assigning a 
term weight to each word.  Then the cosine 
similarity is calculated proportional to the inner 
product of the two vectors.  All sentences are 
ranked according to their similarity with the query, 
and the most similar sentences are used as the data 
for building the specific language model.  In our 
experiments we use different numbers of similar 
sentences, ranting from one to several thousand. 
2.2 Bag-of-words Query Models 
Different query models are designed to guide the 
data augmentation efficiently.  We first define  
?bag-of-words? models, based on different levels 
of knowledge collected from the hypotheses of the 
statistical machine translation engine. 
2.2.1 First-best Hypothesis as a Query Model 
The first-best hypothesis is the Viterbi path in the 
search space returned from the statistical machine 
translation decoder.  It is the optimal hypothesis 
the statistical machine translation system can 
generate using the given translation and language 
model, and restricted by the applied pruning 
strategy.  Ignoring word order, the hypothesis is 
converted into a bag-of-words representation, 
which is then used as a query: 
}|),{(),,( 1211 TiiilT VwfwwwwQ ?== L  
where iw is a word in the vocabulary 1TV of the Top-
1 hypothesis. if  is the frequency of iw ?s 
occurrence in the hypothesis.  
The first-best hypothesis is the actual translation 
we want to improve, and usually it captures 
enough correct word translations to secure a sound 
adaptation process.  But it can miss some 
informative translation words, which could lead to 
better-adapted language models.  
2.2.2  N-Best Hypothesis List as a Query Model 
Similar to the first-best hypothesis, the n-best 
hypothesis list is converted into a bag-of-words 
representation.  Words which occurred in several 
translation hypotheses are simply repeated in the 
bag-of-words representations.  
}|),{(
),,;;,,( ,2,1,,12,11,1 1
TNiii
lNNNlTN
Vwfw
wwwwwwQ
N
?=
= LLL   
where TNV  is the combined vocabulary from all n-
best hypotheses and if  is the frequency of iw ?s 
occurrence in the n-best hypothesis list. 
TNQ  has several good characteristics:  First it 
contains translation candidates, and thus is more 
informative than 1TQ .  In addition, the confidently 
translated words usually occur in every hypothesis 
in the n-best list, therefore have a stronger impact 
on the retrieval result due to the higher term 
frequency (tf) in the query.  Thirdly, most of the 
hypotheses are only different from each other in 
one word or two.  This means, there is not so much 
noise and variance introduced in this query model. 
2.2.3 Translation Model as a Query Model 
To fully leverage the available knowledge from the 
translation system, the translation model can be 
used to guide the language model adaptation 
process.  As introduced in section 1, the translation 
model represents the full knowledge of translating 
words, as it encodes all possible translations 
candidates for a given source sentence.  Thus the 
query model based on the translation model, has 
potential advantages over both 1TQ  and TNQ . 
To utilize the translation model, all the n-grams 
from the source sentence are extracted, and the 
corresponding candidate translations are collected 
from the translation model.  These are then 
converted into a bag-of-words representation as 
follows: 
}|),{(
),,;;,,( ,2,1,,2,1, 1111
TMiii
nsssnsssTM
Vwfw
wwwwwwQ
IIII
?=
= LLL   
where is  is a source n-gram, and I is the number of 
n-grams in the source sentence.  jsiw ,  is a candidate 
target word as translation of is .  Thus the 
translation model is converted into a collection of 
target words as a bag-of-word query model. 
There is no decoding process involved to build 
TMQ .  This means TMQ  does not incorporate any 
background language model information at all, 
while both 1TQ  and TNQ  implicitly use the 
background language model to prune the words in 
the query.  Thus TMQ  is a generalization, and 1TQ  
and TNQ  are pruned versions.  This also means TMQ  
is subject to more noise. 
3 Structured Query Models 
Word proximity and word order is closely related 
to syntactic and semantic characteristics.  
However, it is not modeled in the query models 
presented so far, which are simple bag-of-words 
representations.  Incorporating syntactic and 
semantic information into the query models can 
potentially improve the effectiveness of LM 
adaptation. 
The word-proximity and word ordering 
information can be easily extracted from the first-
best hypothesis, the n-best hypothesis list, and the 
translation lattice built from the translation model.  
After extraction of the information, structured 
query models are proposed using the structured 
query language, described in the Section 3.1. 
3.1 Structured Query Language 
This query language essentially enables the use of 
proximity operators (ordered and unordered 
windows) in queries, so that it is possible to model 
the syntactic and semantic information encoded in 
phrases, n-grams, and co-occurred word pairs.  
The InQuery implementation (Lemur 2003) is 
applied.  So far 16 operators are defined in 
InQuery to model word proximity (ordered, 
unordered, phrase level, and passage level).  Four 
of these operators are used specially for our 
language model adaptation: 
Sum Operator: #sum( 1t ? nt ) 
The terms or nodes ( 1t ? nt ) are treated as 
having equal influence on the final retrieval result.  
The belief values provided by the arguments of the 
sum are averaged to produce the belief value of the 
#sum node. 
Weighted Sum Operator: #wsum( 11 : tw , ?) 
The terms or nodes ( 1t ? nt ) contribute 
unequally to the final result according to the 
weight ( iw ) associated with each it .  
Ordered Distance Operator: #N( 1t ? nt ) 
The terms must be found within N words of 
each other in the text in order to contribute to the 
document's belief value.  An n-gram phrase can be 
modeled as an ordered distance operator with N=n. 
Unordered Distance Operator: #uwN( 1t ? nt ) 
The terms contained must be found in any order 
within a window of N words in order for this 
operator to contribute to the belief value of the 
document. 
3.2 Structured Query Models 
Given the representation power of the structured 
query language, the Top-1 hypothesis, Top-N Best 
hypothesis list, and the translation lattice can be 
converted into three Structured Query Models 
respectively. 
For first-best and n-best hypotheses, we collect 
related target n-grams of a given source word 
according to the alignments generated in the 
Viterbi decoding process.  While for the translation 
lattice, similar to the construction of TMQ , we 
collect all the source n-grams, and translate them 
into target n-grams.  In either case, we get a set of 
target n-grams for each source word. The 
structured query model for the whole source 
sentence is a collection of such subsets of target n-
grams. 
},,,{
21 Isssst
tttQ
vLvv=  
is
t
v
is a set of target n-grams for the source word is : 
}}{;},{;},{{ 311211 LLL
v
gramiiigramiigramis ttttttt i ?+??+?=  
In our experiments, we consider up to trigram for 
better retrieval efficiency, but higher order n-grams 
could be used as will.  The second simplification is 
that every source word is equally important, thus 
each n-gram subset 
is
t
v
will have an equal 
contribution to the final retrieval results.  The last 
simplification is each n-gram within the set of 
is
t
v
 
has an equal weight, i.e. we do not use the 
translation probabilities of the translation model.  
If the system is a phrase-based translation system, 
we can encode the phrases using the ordered 
distance operator (#N) with N equals to the number 
of the words of that phrase, which is denoted as the 
#phrase operator in InQuery implementation.  The 
2-grams and 3-grams can be encoded using this 
operator too. 
Thus our final structured query model is a sum 
operator over a set of nodes.  Each node 
corresponds to a source word.  Usually each source 
word has a number of translation candidates 
(unigrams or phrases).  Each node is a weighted 
sum over all translation candidates weighted by 
their frequency in the hypothesis set.  An example 
is shown below, where #phrase indicates the use of 
the ordered distance operator with varying n: 
 
#q=#sum( #wsum(2 eu  2 #phrase(european union) ) 
   #wsum(12 #phrase(the united states) 
1 american 1 #phrase(an american) ) 
   #wsum(4 are 1 is ) 
   #wsum(8 markets  3 market)) 
   #wsum(7 #phrase(the main)  5 primary ) ); 
4 Experiments 
Experiments are carried out on a standard 
statistical machine translation task defined in the 
NIST evaluation in June 2002.  There are 878 test 
sentences in Chinese, and each sentence has four 
human translations as references.  NIST score 
(NIST 2002) and Bleu score (Papineni et. al. 2002) 
of mteval version 9 are reported to evaluate the 
translation quality. 
4.1  Baseline Translation System 
Our baseline system (Vogel et al, 2003) gives 
scores of 7.80 NIST and 0.1952 Bleu for Top-1 
hypothesis, which is comparable to the best results 
reported on this task.  
For the baseline system, we built a translation 
model using 284K parallel sentence pairs, and a 
trigram language model from a 160 million words 
general English news text collection.  This LM is 
the background model to be adapted.  
With the baseline system, the n-best hypotheses 
list and the translation lattice are extracted to build 
the query models.  Experiments are carried out on 
the adapted language model using the three bag-of-
words query models: 1TQ , TNQ  and TMQ , and the 
corresponding structured query models. 
4.2 Data: GigaWord Corpora 
The so-called GigaWord corpora (LDC, 2003) are 
very large English news text collections.  There are 
four distinct international sources of English 
newswire: 
 
AFE Agence France Press English Service 
APW Associated Press Worldstream English Service 
NYT The New York Times Newswire Service 
XIE The Xinhua News Agency English Service 
 
Table-1 shows the size of each part in word counts. 
 
AFE APW NYT XIE 
170,969K 539,665K 914,159K 131,711K 
Table-1: Number of words in the different 
GigaWord corpora 
 
As the Lemur toolkit could not handle the two 
large corpora (APW and NYT) we used only 200 
million words from each of these two corpora. 
In the preprocessing all words are lowercased 
and punctuation is separated.  There is no explicit 
removal of stop words as they usually fade out by 
tf.idf weights, and our experiments showed not 
positive effects when removing stop words. 
4.3 Bag-of-Words Query Models 
Table-2 shows the size of 1TQ , TNQ  and TMQ  in 
terms of number of tokens in the 878 queries: 
 
 1TQ  TNQ  TMQ  
|| Q  25,861 231,834 3,412,512 
Table-2: Query size in number of tokens 
 
As words occurring several times are reduced to 
word-frequency pairs, the size of the queries 
generated from the 100-best translation lists is only 
9 times as big as the queries generated from the 
first-best translations.  The queries generated from 
the translation model contain many more 
translation alternatives, summing up to almost 3.4 
million tokens.  Using the lattices the whole 
information of the translation model is kept.  
4.3.1 Results for Query 1TQ  
In the first experiment we used the first-best 
translations to generate the queries.  For each of 
the 4 corpora different numbers of similar 
sentences (1, 10, 100, and 1000) were retrieved to 
build specific language models.  Figure-2 shows 
the language model adaptation after tuning the 
interpolation factor ?  by a grid search over [0,1]. 
Typically ?  is around 0.80. 
 
1-Best/NIST Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
1-Best/BLEU-Scores
0.1900
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Figure-2: NIST and Bleu scores 
1TQ  
 
We see that each corpus gives an improvement 
over the baseline.  The best NIST score is 7.94, 
and the best Bleu score is 0.2018.  Both best scores 
are realized using top 100 relevant sentences 
corpus per source sentence mined from the AFE. 
4.3.2 Results for Query TNQ  
Figure-3 shows the results for the query model TNQ .  
The best results are 7.99 NIST score, and 0.2022 
Bleu score.  These improvements are statistically 
significant.  Both scores are achieved at the same 
settings as those in 1TQ , i.e. using top 100 retrieved 
relevant sentences mined from the AFE corpus. 
 
100-Best/NIST-Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
100-Best/BLEU-Scores
0.1900
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Figure-3: NIST and Bleu scores from TNQ  
 
Using the translation alternatives to retrieve the 
data for language model adaptation gives an 
improvement over using the first-best translation 
only for query construction.  Using only one 
translation hypothesis to build an adapted language 
model has the tendency to reinforce that 
translation. 
4.3.3 Results for Query TMQ  
The third bag-of-words query model uses all 
translation alternatives for source words and source 
phrases.  Figure-4 shows the results of this query 
model TMQ .  The best results are 7.91 NIST score 
and 0.1995 Bleu.  For this query model best results 
were achieved using the top 1000 relevant 
sentences mined from the AFE corpus per source 
sentence. 
The improvement is not as much as the other 
two query models.  The reason is probably that all 
translation alternatives, even wrong translations 
resulting from errors in the word and phrase 
alignment, contribute alike to retrieve similar 
sentences.  Thereby, an adapted language model is 
built, which reinforces not only good translations, 
but also bad translations. 
All the three query models showed 
improvements over the baseline system in terms of 
NIST and Bleu scores.  The best bag-of-words 
query model is TNQ  built from the N-Best list.  It 
provides a good balance between incorporating 
translation alternatives in the language model 
adaptation process and not reinforcing wrong 
translations. 
 
Lattice/NIST-Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Lattice/BLEU-Scores
0.1900
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Figure-4: NIST and Bleu scores from TMQ  
 
4.4 Structured Query Models 
The next series of experiments was done to 
study if using word order information in 
constructing the queries could help to generate 
more effective adapted language models.  By using 
the structured query language we converted the 
same first-best hypothesis, the 100-best list, and 
the translation lattice into structured query models.  
Results are reported for the AFE corpus only, as 
this corpus gave best translation scores. 
Figure-5 shows the results for all three structured 
query models, built from the first-best hypothesis 
(?1-Best?), the 100 best hypotheses list (?100-
Best?), and translation lattice (?TM-Lattice?).  
Using these query models, different numbers of 
most similar sentences, ranging from 100 to 4000, 
where retrieved from the AFE corpus.  The given 
baseline results are the best results achieved from 
the corresponding bag-of-words query models. 
Consistent improvements were observed on 
NIST and Bleu scores.  Again, optimal 
interpolation factors to interpolate the specific 
language models with the background language 
model were used, which typically were in the 
range of [0.6, 0.7].  Structured query models give 
most improvements when using more sentences for 
language model adaptation.  The effect is more 
pronounced for Bleu then for NIST score. 
 
Structured query/NIST-Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
8.0500
8.1000
8.1500
Baseline Top100 Top500 Top1000 Top2000 Top4000
1-Best
100-Best
TM-Lattice
Structured query/BLEU-Scores
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
0.2060
0.2080
Baseline Top100 Top500 Top1000 Top2000 Top4000
1-Best
100-Best
TM-Lattice
 
Figure-5: NIST and Bleu scores from the  
structured query models 
 
The really interesting result is that the structured 
query model TMQ gives now the best translation 
results.  Adding word order information to the 
queries obviously helps to reduce the noise in the 
retrieved data by selecting sentences, which are 
closer to the good translations,  
The best results using the adapted language 
models are NIST score 8.12 for using the 2000 
most similar sentences, whereas Bleu score goes 
up to 0.2068 when using 4000 sentences for 
language model adaptation. 
4.5 Example 
Table-3 shows translation examples for the 17th 
Chinese sentence in the test set. We applied the 
baseline system (Base), the bag-of-word query 
model (Hyp1), and the structured query model 
(Hyp2) using AFE corpus. 
 
Ref The police has already blockade the scene of the explosion. 
Base At present, the police had cordoned off the explosion. 
Hyp1 At present, police have sealed off the explosion.  
Hyp2 Currently, police have blockade on the scene of the explosion. 
Table-3 Translation examples 
 4.6 Oracle Experiment 
Finally, we run an oracle experiments to see 
how much improvement could be achieved if we 
only selected better data for the specific language 
models. We converted the four available reference 
translations into structured query models and 
retrieved the top 4000 relevant sentences from 
AFE corpus for each source sentence.  Using these 
language models, interpolated with the background 
language model gave a NIST score of 8.67, and a 
Bleu score of 0.2228.  This result indicates that 
there is room for further improvements using this 
language model adaptation technique. 
The oracle experiment suggests that better initial 
translations lead to better language models and 
thereby better 2nd iteration translations.  This lead 
to the question if we can iterate the retrieval 
process several times to get further improvement, 
or if the observed improvement results form using 
for (good) translations, which have more diversity 
than the translations in an n-best list. 
On the other side the oracle experiment also 
shows that the optimally expected improvement is 
limited by the translation model and decoding 
algorithm used in the current SMT system. 
 
5 Summary 
In this paper, we studied language model 
adaptation for statistical machine translation.  
Extracting sentences most similar to the initial 
translations, building specific language models for 
each sentence to be translated, and interpolating 
those with the background language models gives 
significant improvement in translation quality.  
Using structured query models, which capture 
word order information, leads to better results that 
plain bag of words models. 
The results obtained suggest a number of 
extensions of this work:  The first question is if 
more data to retrieve similar sentences from will 
result in even better translation quality.  A second 
interesting question is if the translation 
probabilities can be incorporated into the queries.  
This might be especially useful for structured 
query models generated from the translation 
lattices.  
References  
J. Bellegarda. 2000, Exploiting Latent Semantic 
Information in Statistical Language Modeling. In 
Proceedings of the IEEE, 88(8), pp. 1279-1296. 
S. Besling and H.G. Meier 1995. Language Model 
Speaker Adaptation, Eurospeech 1995, Madrid, 
Spain. 
Peter F Brown., Stephen A Della Pietra., Vincent J. 
Della Pietra and Mercer Robert L., 1993.  The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2), pp. 263?311.  
S.F Chen., K. Seymore, and R. Rosenfeld 1998. Topic 
Adaptation for Language Modeling using 
Unnormalized Exponential Models. IEEE 
International Conference on Acoustics, Speech and 
Signal Processing 1998, Seattle WA.  
Renato DeMori and Marcello Federico 1999. Language 
Model Adaptation, In Computational Models of 
Speech Pattern Processing, Keith Pointing (ed.), 
NATO ASI Series, Springer Verlag.  
Matthias Eck, Stephan Vogel, and Alex Waibel, 2004. 
Language Model Adaptation for Statistical 
Machine Translation based on Information Retrieval, 
International Conference on Language Resources and 
Evaluation, Lisbon, Portugal. 
R. Iyer and M. Ostendorf, 1999. Modeling Long 
Distance Dependence in Language: Topic Mixtures 
vs. Dynamic Cache Models, IEEE Transactions on 
Speech and Audio Processing, SAP-7(1): pp. 30-39. 
David Janiszek, Renato DeMori and Frederic Bechet, 
2001.  Data Augmentation and Language Model 
adaptation, IEEE International Conference on 
Acoustics, Speech and Signal Processing 2001, Salt 
Lake City, UT.  
LDC, Gigaword Corpora. http://wave.ldc.upenn.edu/ 
Catalog/CatalogEntry.jsp?catalogId=LDC2003T05 
Lemur, The Lemur Toolkit for Language Modeling and 
Information Retrieval, http://www.cs.cmu.edu/~ 
lemur/ 
Milind Mahajan, Doug Beeferman and X.D. Huang, 
1999. Improved Topic-Dependent Language 
Modeling Using Information Retrieval Techniques, 
IEEE International Conference on Acoustics, Speech 
and Signal Processing 1999, Phoenix, AZ. 
NIST Report: 2002, Automatic Evaluation of Machine 
Translation Quality Using N-gram Co-Occurrence 
Statistics.  http://www.nist.gov/speech/tests/mt/doc/ 
ngram-study.pdf . 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei- 
Jing Zhu, 2002. BLEU: A Method for Automatic 
Evaluation of Machine Translation. In Proc of the 
40th Annual Meeting of the Association for 
Computational Linguistics. 2002, Philadelphia, PA. 
Kristie Seymore and Ronald Rosenfeld, 1997. Using 
Story Topics for Language Model Adaptation. In 
Proc. Eurospeech 1997, Rhodes, Greece.  
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble, 
Ashish Venogupal, Bing Zhao, Alex Waibel, 2003. 
The CMU Statistical Translation System, Proceedings 
of MT-Summit IX, 2003, New Orleans, LA. 
 
Improving Statistical Machine Translation in the Medical Domain  
using the Unified Medical Language System 
Matthias Eck 
 
 
 
matteck@cs.cmu.edu 
Stephan Vogel 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA, 15213, USA 
vogel+@cs.cmu.edu 
Alex Waibel 
 
 
 
ahw@cs.cmu.edu 
 
Abstract 
Texts from the medical domain are an 
important task for natural language 
processing. This paper investigates the 
usefulness of a large medical database (the 
Unified Medical Language System) for the 
translation of dialogues between doctors and 
patients using a statistical machine translation 
system. We are able to show that the 
extraction of a large dictionary and the usage 
of semantic type information to generalize the 
training data significantly improves the 
translation performance. 
1 Introduction 
Hospitals in the United States have to deal with 
an increasing number of patients who have no 
knowledge of the English language. It is not 
surprising that in this area translation errors can 
lead to severe problems (Neergard, 2003; Flores et 
al. 2003). This is one of the main reasons why the 
medical domain plays an important role in many of 
the current projects involving natural language 
processing. Especially many text or speech 
translation projects include tasks to translate texts 
or dialogues with medical topics.  
The goal of this research was the improvement 
of translation quality in the medical domain using 
a statistical machine translation system. A 
statistical machine translation system deduces 
translation rules from large amounts of parallel 
texts in the source and target language.  
The general approach to gather as much training 
data as possible is usually complicated and 
expensive. So it is necessary to make use of 
already available data and databases and it is 
reasonable to hope that some ideas and special 
methods could actually improve the performance 
in limited domains, like the medical domain.  
The Internet and especially the WWW offers a 
lot of data related to medical topics. Especially 
interesting and promising for us was the Unified 
Medical Language System? (UMLS, 1986-2004) 
available from the US National Library of 
Medicine. It provides a vast amount of information 
concerning medical terms and we extracted 
information from this database to improve an 
existent translation system.  
The paper will first give an introduction into the 
Unified Medical Language system. We will then 
point out which parts could be useful for statistical 
machine translation and later show how the 
baseline system was actually significantly 
improved using this data. 
2 The Unified Medical Language System 
2.1 Introduction 
The Unified Medical Language System (UMLS, 
1986-2004) project was initiated in 1986 by the 
U.S. National Library of Medicine. It integrates 
different knowledge sources into one database (e.g. 
biomedical vocabularies, dictionaries). 
The goal is to help health professionals and 
researchers to use biomedical information from 
these different sources. It is usually updated about 
3 or 4 times per year. 
It consists of three main knowledge repositories, 
the UMLS Metathesaurus, the UMLS Semantic 
Network and the SPECIALIST lexicon. 
Interesting facts about the UMLS, related work 
and further information can be found in 
(Lindbergh, 1990; Kashyap, 2003; Brown et al, 
2003; Friedman et al, 2001; Zweigenbaum et al, 
2003). 
2.2 The UMLS Metathesaurus 
The UMLS Metathesaurus provides a common 
structure for approximately 100 source biomedical 
vocabularies.  
The 2003AB1 version of the Metathesaurus 
contains exactly 900,551 concepts named by 
2,247,457 terms.  It is organized by concept, which 
is a cluster of terms (i.e. synonyms, lexical variants 
                                                    
1
 2003AB was the actual release when the 
experiments described in this paper were executed. The 
most recent version now is 2004AA, which contains 
certain additional and updated information. All numbers 
given in this paper are according to the 2003AB 
version. 
and translations) with the same meaning. 
Translations are present for up to 14 additional 
languages besides English. It is very likely that 
other languages will be added in later releases. 
 
Table 1 shows the distribution of the terms 
according to the 15 different languages. 
 
Language Number of Terms 
English 1860683 
Spanish 73136 
German 71316 
Portuguese 69127 
Russian 44907 
Dutch  38600 
French 38249 
Italian 24992 
Finnish 22382 
Danish 723 
Swedish 723 
Norwegian 722 
Hungarian 718 
Basque 695 
Hebrew 484 
Table 1: Languages in the UMLS 
 
For example the concept ?arm? includes the 
English lexical variant, its plural form, ?arms? and 
with ?bras?, ?arm?, ?braccio?, ?braco?, ?ruka? and 
?brazo? the French, German, Italian, Portuguese, 
Russian and Spanish translations.  
 
Some entries contain case information, too, and 
the entries are not limited to words but some terms 
are also longer phrases like  ?third degree burn of 
lower leg? or ?loss of consciousness?. 
 
It also includes inter-concept relationships 
across the multiple vocabularies. The main 
relationship types are shown in Table 2: 
 
Relationship types 
broader  
narrower 
other related 
like 
parent 
child 
sibling 
is allowed qualifier 
can be qualified by 
is co-occurring with 
Table 2: Relationship types 
 
 
The synonym-relationship is implicitly realized 
by different terms that are affiliated with the same 
concept. 
The co-occurrence relationship refers to 
concepts co-occurring in the MEDLINE-
publications. 
 
In addition each concept is categorized into 
semantic types according to the UMLS Semantic 
Network. 
2.3 The UMLS Semantic Network 
The UMLS Semantic Network categorizes the 
concepts of the UMLS Metathesaurus through 
semantic types and relationships.  
Every concept in the Metathesaurus is part of 
one or more semantic types. 
There are 135 semantic types arranged in a 
generalization hierarchy with the two roots 
?Entity? and ?Event?. This hierarchy is still rather 
abstract (e.g. not deeper than six).  
A more detailed generalization hierarchy is 
realized with the child, parent and sibling 
relationships of the UMLS Metathesaurus. 
 
Figure 1 shows some examples for semantic types. 
 
Entity 
    Physical Object 
        Organism 
        Anatomical Structure 
            Fully Formed Anatomical Structure 
                Body Part, Organ or Organ Component 
        Manufactured Object 
            Medical Device 
                Drug Delivery Device 
            Clinical Drug 
Event 
    Activity 
        Behavior 
            Social Behavior 
        Occupational Activity 
            Health Care Activity 
                Laboratory Procedure 
    Phenomenon or Process 
        Human caused Phenomenon or Process 
Figure 1: Some semantic types 
2.4 The SPECIALIST lexicon 
The SPECIALIST lexicon contains over 30,000 
English words. It is intended to be a general 
English lexicon including many biomedical terms.  
The lexicon entry for each word or term records 
the syntactic, morphological and orthographic 
information. 
 
{base=anesthetic 
spelling_variant=anaesthetic 
entry=E0330018 
        cat=noun 
        variants=reg 
        variants=uncount 
} 
Figure 2: Example entry from the  
Specialist Lexicon 
 
Figure 2 shows the entry for ?anesthetic?. There 
is a spelling variant ?anaesthetic? and an entry 
number. The category in this case is noun (there is 
another entry for ?anesthetic? as an adjective). The 
variants-slot contains a code indicating the 
inflectional morphology of the entry. ?anesthetic? 
can either be a regular count noun (with regular 
plural ?anesthetics?) or an uncountable noun. 
 
3 Machine Translation Experiments 
3.1 The Baseline System 
The Baseline system, which we used to test 
different approaches to improve the translation 
performance, is a statistical machine translation 
system. The task was to facilitate doctor-patient 
dialogues across languages. In this case we chose 
translation from Spanish to English. 
 
The Baseline system was trained using 9,227 
lines of training data (90,012 English words, 
89,432 Spanish words). 3,227 lines of this data are 
?in-domain? data. We collected doctor patient 
dialogues during ongoing research projects in our 
group and used this data as training data. The 
6,000 other lines of training data are out of domain 
data from the C-Star Project. This data also 
consists of dialogues but not from the medical 
domain.  
 
The test data consists of 500 lines with 6,886 
words. The test data was also taken from medical 
dialogues between a doctor and a patient and 
contains a reasonable number of medical terms but 
the language is not very complex. Figure 3 shows 
some example test sentences (from the reference 
data). 
  
 (?) 
Doctor: The symptoms you are describing and 
given your recent change in diet, I believe 
you may be anemic. 
Patient: Anemic? Really? Is that serious? 
Doctor: Anemia can be very serious if left 
untreated. Being anemic means your body 
lacks a sufficient amount of red blood cells 
to carry oxygen through your body. 
 (?) 
Figure 3: Example test sentences (reference) 
 
The Baseline system uses IBM1 lexicon 
transducers and different types of phrase 
transducers (Zhang et al 2003, Vogel et al 1996, 
Vogel et al 2003). The Language model is a 
trigram language model with Good-Turing-
Smoothing built with the SRI-Toolkit (SRI, 1995-
2004) using only the English part of the training 
data. 
 
The Baseline system scores a 0.171 BLEU and 
4.72 NIST. [BLEU and NIST are well known 
scoring methods for measuring machine translation 
quality. Both calculate the precision of a 
translation by comparing it to a reference 
translation and incorporating a length penalty 
(Doddington, 2001; Papineni et al, 2002).] 
3.2 Extracting dictionaries from the UMLS 
The first way to exploit the UMLS database for a 
statistical machine translation system naturally is 
to extract additional Spanish-English lexicons or 
phrasebooks.  
The UMLS Metathesaurus provides translation 
information as we can assume that Spanish and 
English terms that are associated with the same 
concept are respective translations.  For example 
as the English term ?arm? is associated with the 
same concept as the Spanish term ?brazo? we can 
deduce that ?arm? is the English translation of 
?brazo?.  
Unfortunately the UMLS does not contain 
morphological information about languages other 
than English. This means it cannot be 
automatically detected that ?brazo? is the singular 
form and thus the translation of ?arm? and not the 
translation of ?arms?. 
As most of the entries are in singular form we 
just extracted every possible combination of 
Spanish and English terms regardless of possible 
errors like combining the singular ?brazo? and the 
plural ?arms?. 
The resulting (lower-cased) Spanish-English 
lexicon/phrasebook contains 495,248 pairs of 
words and phrases. This means each Spanish term 
is combined with seven English terms on average. 
This seems to be an extremely huge amount but 
it has to be considered that there are terms in the 
UMLS and the resulting lexicon that are probably 
too special to be really useful for the translation of 
dialogues  (e.g. ?1,1,1-trichloropropene-2,3-oxide? 
translating to ?oxido de tricloropropeno?). 
Nevertheless there are lots of meaningful entries 
as the following experiments show.  
 
Applying the dictionaries to the Baseline system 
In the first step we just added this 
lexicon/phrasebook as an additional transducer and 
did not change the language model.  
The experiment showed a nice increase in BLEU 
and NIST performances and scored at 0.180 BLEU 
and 4.86 NIST. 
This system especially has a higher coverage, as 
only 302 words (types) are not covered by the 
training data compared to 411 for the baseline 
system. 
 
Adding the English side to the Language Model 
As the extracted dictionary contained many 
phrases it seemed reasonable to add the English 
side to the language modeling data. This also 
prevents words from the extracted dictionary to be 
treated as ?unknown? by the language model if 
they were not in the language model training data. 
This further improved the BLEU and NIST scores 
to 0.182 BLEU and 4.92 NIST. 
 
It should not be surprising to get an improvement 
in these first two experiments because basically 
just more data was used to train the systems. The 
really interesting ideas will be presented in the 
next sections. 
3.3 Using the Semantic Type Information 
The overall idea to use the semantic type 
information is to generalize the training data. 
The training data contains for example sentence 
pairs like: 
 
Necesito examinar su cabeza. 
I need to examine your head. 
Necesito examinar su brazo. 
I need to examine your arm. 
Necesito examinar su rodilla. 
I need to examine your knee. 
 
If we could generalize these sentences by 
replacing the special body parts like ?head?, ?arm? 
and ?knee? with a general tag e.g. 
?@BODYPART? and especially treat this tag we 
could use one sentence of training data for every 
body part imaginable in this sentence.  
We would just need an additional lexicon that just 
translates body parts. 
 
Necesito examinar su @BODYPART. 
I need to examine your @BODYPART. 
 
 
We could additionally correctly translate possibly 
unseen sentences like ?Necesito examinar su 
antebrazo? (?I need to examine your forearm?) if 
we could automatically deduce that 
?antebrazo/forearm? is a body part and if we just 
knew this translation pair. 
 
Some additional similar sentences in which we 
could apply the same ideas are: 
 
Enseneme que @BODYPART es. 
Show me which @BODYPART. 
?Que @BODYPART le/la duele? 
Which @BODYPART hurts? 
 
(In the last sentence it actually depends on the 
gender of the body part on the Spanish side if the 
sentence is ??Que @BODYPART la duele?? or 
??Que @BODYPART le duele??. But as we are 
translating from Spanish to English this did not 
seem to be a big problem.) 
 
As stated before every concept in the UMLS 
Metathesaurus is categorized into one or more 
semantic types defined in the UMLS Semantic 
Network. 
The two semantic types ?Body Part, Organ, or 
Organ Component? and ?Body Location or 
Region? from the UMLS Semantic Network cover 
pretty closely what we usually affiliate with the 
colloquial meaning of body part. 
[The terminological difference is that the 
semantic type ?Body Part, Organ, or Organ 
Component? is defined by a certain function. For 
example ?liver? and ?eye? are part of this semantic 
type, whereas the semantic type ?Body Location or 
Region? is defined by the topographical location of 
the respective body part. Examples are ?head? and 
?arm?. The function in this case is not as clearly 
defined as the function of a ?liver?.] 
 
This information was used in the next 
experiment. We first filtered the general Spanish-
English dictionary, we had extracted from the 
UMLS, to contain only words and phrases from the 
two semantic types ?Body Part, Organ, or Organ 
Component? and ?Body Location or Region?. This 
gave a dictionary of 11,260 translation entries for 
body parts. Again each Spanish term is combined 
with about seven English terms on average. 
In the next step we replaced every occurrence of a 
word or phrase pair from this new dictionary in the 
training data (i.e. if it occurred on the Spanish and 
English side) with a general body-part-tag. 
527 sentence pairs of the original 9,227 sentence 
pairs contained a word or phrase pair from this 
dictionary. 
 A retraining of the translation system with this 
changed training data resulted in transducer rules 
containing this body-part-tag. 
By using cascaded transducers (Vogel and Ney, 
2000) in the actual translation the first transducer, 
that is applied (in this case the body-part 
dictionary) replaces the Spanish body part with its 
translation pair and the body-part tag.  
The following transducers can apply their 
generalized rules containing the body-part-tag 
instead of the real body part. 
 
E.g. translation of the sentence:  
 
Necesito examinar su antebrazo. 
 
First step apply body-part dictionary rule  
(antebrazo?forearm) 
 
Necesito examinar su @BODYPART(antebrazo?forearm). 
 
Apply generalized transducer rule: (a rule could 
be: Necesito examinar su @BODYPART ? I need 
to examine your @BODYPART) 
 
I need to examine your @BODYPART(antebrazo?forearm). 
 
Resolve tags: 
 
I need to examine your forearm. 
 
By applying this to the whole translation system 
the score improved to 0.188 BLEU/4.94 NIST. 
Using other semantic types  
As the body-part lexicon and the replacement of 
body-parts proved to be helpful we applied two 
more of these replacement strategies. Consider the 
following 4 sentence pairs from the training data. 
 
?Siente dolor cuando respira? 
Do you feel pain when you breathe? 
?Cuando le empezo la fiebre? 
When did the fever start? 
?Podria ser artritis? 
Could this be arthritis? 
?Es grave la anemia, doctor? 
Is anemia serious, doctor? 
The first two sentences contain findings or 
symptoms with the terms ?dolor/pain? and 
?fiebre/fever?. The second two sentences contain 
diseases with ?artritis/arthritis? and 
?anemia/anemia?. The appropriate semantic types 
from the UMLS Semantic Network for these terms 
are ?Finding? and ?Sign or Symptom? for ?pain? 
and ?fever? and ?Disease or Syndrome? for 
?arthritis? and ?anemia? 
Filtering the Spanish-English dictionary resulted 
in 25,987 ?Finding/Sign or Symptom? translation 
pairs (approximately three English terms per 
Spanish term) and 116,793 ?Disease or Syndrome? 
translation pairs (approximately five English terms 
per Spanish term). 
198 sentence pairs from the training data 
contained a ?Finding/Sign or Symptom?-pair and 
127 sentence pairs contained a ?Disease or 
Syndrome?-pair from these dictionaries. 
 
The final translation with those three semantic 
types replaced in the training data and using the 
three filtered dictionaries with the cascaded 
transducer application gave a translation 
performance of 0.190 BLEU/5.02 NIST.  
This shows that although less than 10% of the 
sentences were affected by the replacement with 
the appropriate tags we could nicely improve the 
overall translation performance. 
 Example translations 
Some example translations comparing the baseline 
and the best system with the reference are listed in 
table 3. 
 
1. Sentence  
Reference 
the condition is called tenosynovitis, which 
is an inflammation of the tendon sheath. 
 
Baseline 
this condici?n diagnostic, which is a 
inflammation from the of the tendon. 
 
Best System 
this condition is called tenosynovitis, which 
is a inflammation of tendon sheath. 
2. Sentence  
Reference 
i guess your work involves a lot of repetitive 
movement, huh? 
 
Baseline 
do you I guess your work require plenty 
baby?s, no? 
 
Best System 
i guess you your work require plenty 
repetitive movements, not? 
3. Sentence 
Reference 
you need vitamin c and iron in your blood 
to help your body 
 
Baseline 
you need vitamin c and iron in your blood 
help rescue to  
 
Best System 
you need vitamin c and iron in your blood 
to help the body 
4. Sentence  
Reference 
did you take anything for the pain? 
 
Baseline 
did you sleep taken anything for the pain? 
 
Best System 
did you taken anything for the pain? 
5. Sentence  
Reference 
i can feel it here, behind my breastbone. 
 
Baseline 
i here, behind of the estern?n. 
 
Best System 
i here, behind of sternum. 
Table 3: Example translations 
 
The last example sentence is an interesting case. 
The best system does not get more words right 
compared to the baseline system and so the 
BLEU/NIST-score does not improve. But 
?sternum? is a synonym of the correct 
?breastbone? and a more technical term. This 
supports the claim that the UMLS tends to contain 
more technical terms (like ?tenosynovitis? in the 
first sentence).  
4 Future work 
It is surely possible to use every semantic type 
from the semantic network in the same way like 
the overall five semantic types, which were used in 
the experiments. We did not do this here because 
further semantic types occurred extremely rarely in 
the test and training data. But this could easily be 
done for other test and training data and it is 
reasonable to expect similar improvements. 
Another idea is to use a more specialized 
approach and to make use of the relationships in 
the UMLS Metathesaurus. Each concept could be 
generalized by its parent-concepts instead of its 
semantic type. The generalization hierarchy for the 
concept ?leg? is for example: leg ? lower extremity 
? extremity ? body region ? anatomy. 
This could be especially helpful when translating 
to morphologically richer languages than English 
because the usage of extremities could differ from 
other body parts for example. 
 
In the extracted dictionaries every translation 
pair was given the same translation probability. It 
might be helpful to re-score these probabilities by 
using information from bilingual or monolingual 
texts to improve the translation probabilities for 
usually frequently used terms compared to rarely 
used terms. 
 
As the example translations showed, the 
extracted dictionaries from the UMLS tend to 
contain technical terms instead of colloquial terms 
(translation ?sternum? instead of ?breastbone?). 
We can further assume that a doctor prefers to use 
the more technical terms and a patient prefers the 
more colloquial terms.  Therefore it could be 
interesting to examine if having two different 
translation systems for sentences uttered by a 
doctor and a patient would improve the overall 
translation performance. 
5 Conclusion 
We carried out four different experiments in order 
to improve a Spanish-English medical domain 
translation system. After sequentially applying 
different ideas the final system shows an 11% 
improvement in BLEU and 6% improvement in 
NIST score.  
Table 4 compares the different experiments and 
scores (the 500k dictionary refers to the dictionary 
that was first extracted from the UMLS with 
495,248 word pairs). 
 
System BLEU NIST 
Baseline system 0.171 4.72 
+500k dictionary 0.180 4.86 
+LM improvement 0.182 4.92 
+body part-tags 0.188 4.94 
+sign/symptom/finding 
+disease/syndrome 
0.190 5.02 
Table 4: Experiments and improvements 
 
With more investigation and the ongoing effort 
of the National Library of Medicine to extent the 
UMLS databases it will hopefully be possible to 
further improve the translation performance.  
 
References 
Allen C. Browne, Guy Divita, Alan R. Aronson, 
Alexa T. McGray, 2003.  UMLS Language and 
Vocabulary Tools, Proceedings of the American 
Medical Informatics Association (AMIA) 2003 
Symposium, Washington, DC, USA. 
George Doddington. 2001. Automatic Evaluation 
of Machine Translation Quality using n-Gram 
Cooccurrence Statistics. NIST Washington, DC, 
USA. 
Glenn Flores, M. Barton Laws, Sandra J. Mayo, 
Barry Zuckerman, Milagros Abreu, Leonardo 
Medina,  
Eric J. Hardt, 2003. Errors in medical 
interpretation and their potential clinical 
consequences in pediatric encounters, Pediatrics, 
Jan 2003. 
Carol Friedman, Hongfang Liu, Lyuda Shagina, 
Stephen Johnson, George Hripcsak, 2001. 
Evaluating the UMLS as a Source of Lexical 
Knowledge for Medical Language Processing, 
Proceedings of the AMIA 2001 Symposium, 
Washington, DC, USA. 
Vipul Kashyap, 2003. The UMLS semantic 
network and the semantic web, Proceedings of 
the AMIA 2003 Symposium,  Washington, DC, 
USA. 
C. Lindberg, 1990. The Unified Medical Language 
System (UMLS) of the National Library of 
Medicine, Journal of the American Medical 
Record Association, 1990;61(5):40-42. 
Lauren Neergard, 2003. Hospitals struggle with 
growing language barrier, Associated Press, The 
Charlotte Observer Sept. 2, 2003 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu, 2002. BLEU: a Method for 
Automatic Evaluation of Machine Translation, 
Proceedings of the ACL 2002, Philadelphia, 
USA. 
SRI Speech Technology and Research Laboratory, 
SRI Language Modeling Toolkit, 1995-2004 
(ongoing) 
http://www.speech.sri.com/projects/srilm/ 
UMLS Unified Medical Language System, 
National Library of Medicine, 1986-2004 
(ongoing) 
http://www.nlm.nih.gov/research/umls/ 
Stephan Vogel and Hermann Ney, 2000. 
Translation with Cascaded Finite State 
Transducers. Proceedings of the 38th Annual 
Meeting of the Association for Computational 
Linguistics (ACL 2000), pp. 23-30. Hongkong, 
China, October 2000. 
Stephan Vogel, Hermann Ney, and Christoph Till-
mann, 1996. HMM-based Word Alignment in 
Statistical Translation, Proceedings of COLING 
1996: The 16th International Conference on 
Computational Linguistics, pp. 836-841. 
Copenhagen, August 1996. 
Stephan Vogel, Ying Zhang, Fei Huang, Alicia 
Tribble, Ashish Venogupal, Bing Zhao, Alex 
Waibel, 2003. The CMU Statistical Translation 
System, Proceedings of MT-Summit IX. New 
Orleans, LA. Sep 2003. 
Ying Zhang, Stephan Vogel, Alex Waibel, 2003.  
Integrated Phrase Segmentation and Alignment 
Algorithm for Statistical Machine Translation, 
Proceedings of International Conference on 
Natural Language Processing and Knowledge 
Engineering 2003, Beijing, China, Oct 2003. 
Pierre Zweigenbaum, Robert Baud, Anita Burgun, 
Fiammetta Namer, ?ric Jarrousse, Natalia 
Grabar, Patrick Ruch, Franck Le Duff, Beno?t 
Thirion, St?fan Darmoni, 2003. UMLF: a 
Unified Medical Lexicon for French, 
Proceedings of the AMIA 2003 Symposium, 
Washington, DC, USA. 
 
 
 
 
175
176
177
178
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 483?490, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Mining Key Phrase Translations from Web Corpora 
 
Fei Huang       Ying Zhang       Stephan Vogel 
 
School of Computer Science 
Carnegie Mellon University, Pittsburgh, PA 15213 
{fhuang, joy, vogel}@cs.cmu.edu 
 
 
Abstract 
Key phrases are usually among the most 
information-bearing linguistic structures. 
Translating them correctly will improve 
many natural language processing appli-
cations. We propose a new framework to 
mine key phrase translations from web 
corpora. We submit a source phrase to a 
search engine as a query, then expand 
queries by adding the translations of 
topic-relevant hint words from the re-
turned snippets. We retrieve mixed-
language web pages based on the ex-
panded queries.  Finally, we extract the 
key phrase translation from the second-
round returned web page snippets with 
phonetic, semantic and frequency-
distance features. We achieve 46% phrase 
translation accuracy when using top 10 re-
turned snippets, and 80% accuracy with 
165 snippets. Both results are signifi-
cantly better than several existing meth-
ods. 
1 Introduction 
Key phrases such as named entities (person, loca-
tion and organization names), book and movie ti-
tles, science, medical or military terms and others 
1, are usually among the most information-bearing 
linguistic structures. Translating them correctly 
will improve the performance of cross-lingual in-
formation retrieval, question answering and ma-
chine translation systems. However, these key 
phrases are often domain-specific, and people con-
                                                                                                                    
1 Some name and terminology is a single word, which could 
be regarded as a one-word phrase. 
stantly create new key phrases which are not cov-
ered by existing bilingual dictionaries or parallel 
corpora, therefore standard data-driven or knowl-
edge-based machine translation systems cannot 
translate them correctly. 
 As an increasing amount of web information be-
comes available, exploiting such a huge informa-
tion resource is becoming more attractive. (Resnik 
1999) searched the web for parallel corpora while 
(Lu et al 2002) extracted translation pairs from 
anchor texts pointing to the same webpage. How-
ever, parallel webpages or anchor texts are quite 
limited, and these approaches greatly suffer from 
the lack of data.  
However, there are many web pages containing 
useful bilingual information where key phrases and 
their translations both occur. See the example in 
Figure 1. This example demonstrates web page 
snippets2 containing both a Chinese key phrase ??
??? and its translation, ?Faust?. 
We thus can transform the translation problem 
into a data mining problem by retrieving these 
mixed-language web pages and extracting their 
translations. We propose a new framework to mine 
key phrase translations from web corpora. Given a 
source key phrase (here a Chinese phrase), we first 
retrieve web page snippets containing this phrase 
using the Google search engine. We then expand 
queries by adding the translations of topic-relevant 
hint words from the returned snippets. We submit 
the source key phrase and expanded queries again 
to Google to retrieve mixed-language web page 
snippets.  Finally, we extract the key phrase trans-
lation from the second-round returned snippets 
with phonetic, semantic and frequency-distance 
features.  
2A snippet is a sentence or paragraph containing the key 
phrase, returned with the web page URLs. 
483
  
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Returned mixed-language web page snip-
pets using source query 
 
We achieve 46% phrase translation accuracy 
when using 10 returned snippets, and 80% accu-
racy with 165 snippets. Both results are signifi-
cantly better than several existing methods. 
   The reminder of this paper is organized as fol-
lows: cross-lingual query expansion is discussed in 
section 2; key phrase translation extraction is ad-
dressed in section 3. In section 4 we present ex-
perimental results, which is followed by relevant 
works and conclusions. 
2 Retrieving Web Page Snippets through 
Cross-lingual Query Expansion 
For a Chinese key phrase f, we want to find its 
translation e from the web, more specifically, from 
the mixed-language web pages or web page snip-
pets containing both f and e. As we do not know e, 
we are unable to directly retrieve such mixed-
language web page using (f,e) as the query.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Returned mixed-language web page snip-
pets using cross-lingual query expansion 
However, we observed that when the author of a 
web page lists both f and e in a page, it is very 
likely that f' and e' are listed in the same page, 
where f? is a Chinese hint word topically relevant 
to f, and e? is f?s translation. Therefore if we know 
a Chinese hint word f?, and we know its reliable 
translation, e?, we can send (f, e?) as a query to re-
trieve mixed language web pages containing (f, e).    
For example, to find web pages which contain 
translations of ?????(Faust), we expand the 
query to ????+goethe? since ???? (Goethe) 
is the author of ?????(Faust). Figure 2 illus-
trates retrieved web page snippets with expanded 
queries. We find that newly returned snippets con-
tain more correct translations with higher ranks. 
   To propose a ?good? English hint e' for f, first we 
need to find a Chinese hint word f' that is relevant 
to f. Because f is often an OOV word, it is unlikely 
that such information can be obtained from exist-
ing Chinese monolingual corpora. Instead, we 
484
query Google for web pages containing f. From the 
returned snippets we select Chinese words f' based 
on the following criteria: 
 
1. f' should be relevant to f based on the co-
occurrence frequency. On average, 300 
Chinese words are returned for each query 
f. We only consider those words that occur 
at least twice to be relevant. 
2. f' can be reliably translated given the cur-
rent bilingual resources (e.g. the LDC 
Chinese-English lexicon 3  with 81,945 
translation entries). 
3. The meaning of f' should not be too am-
biguous. Words with many translations 
are not used. 
4. f' should be translated into noun or noun 
phrases. Given the fact that most OOV 
words are noun or noun phrases, we ig-
nore those source words which are trans-
lated into other part-of-speech words. The 
British National Corpus4 is used to gener-
ate the English noun lists. 
 
For each f, the top Chinese words f' with the 
highest frequency are selected. Their correspond-
ing translations are then used as the cross-lingual 
hint words for f. For example, for OOV word f = 
??? (Faust), the top candidate f's are ???
(Goethe)?, ? ?? (introduction)?, ???
(literature)? and ???(tragedy)?. We expand 
the original query ????? to ???? + 
goethe?, ???? + introduction?, ???? + lit-
erature?, ???? + tragic?, and then query Google 
again for web page snippets containing the correct 
translation ?Faust?. 
3 Extracting Key Phrase Translation 
When the Chinese key phrase and its English hint 
words are sent to Google as the query, returned 
web page snippets contain the source query and 
possibly its translation. We preprocess the snippets 
to remove irrelevant information. The preprocess-
ing steps are: 
1. Filter out HTML tags; 
                                                          
3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogI
d=LDC2002L27 
4 http://www.natcorp.ox.ac.uk/ 
2. Convert HTML special characters (e.g., 
?&lt?) to corresponding ASCII code (?>?); 
3. Segment Chinese words based on a maxi-
mum string matching algorithm, which is 
used to calculate the translation probability 
between a Chinese key phrase and an Eng-
lish translation candidate. 
4. Replace punctuation marks with phrase sepa-
rator ?|?; 
5. Replace non-query Chinese words with 
placeholder mark ?+?, as they indicate the 
distance between an English phrase and the 
Chinese key phrase. 
For example, the snippet  
? <b>???? </b>? (the bridges of 
madison county)[review]. ????anjing | 
?????2004-01-25 ??? 02:13 | ?
????? 
is converted into 
| <b> ?  ?  ?  ? </b> | 
the_bridges_of_Madison_county | review | 
++ + | anjing | ++ ++  | 2004-01-25 +++ 02 
13 | + + ++ ++, 
where ?<b>? and ?</b>? mark the start and end 
positions of the Chinese key phrase. The candidate 
English phrases, ?the bridges of madison county?, 
?review? and ?anjing?, will be aligned to the 
source key phrase according to a combined feature 
set using a transliteration model which captures the 
pronunciation similarity, a translation model which 
captures the semantic similarity and a frequency-
distance model reflecting their relevancy. These 
models are described below. 
3.1 Transliteration Model 
The transliteration model captures the phonetic 
similarity between a Chinese phrase and an Eng-
lish translation candidate via string alignment. 
Many key phrases are person and location names, 
which are phonetically translated and whose writ-
ten forms resemble their pronunciations. Therefore 
it is possible to discover these translation pairs 
through their surface strings. Surface string trans-
literation does not need a pronunciation lexicon to 
map words into phoneme sequences; thus it is es-
pecially appealing for OOV word translation. For 
non-Latin languages like Chinese, a romanization 
485
script called ?pinyin? maps each Chinese character 
into Latin letter strings. This normalization makes 
the string alignment possible. 
     We adopt the transliteration model proposed in 
(Huang, et al 2003). This model calculates the 
probabilistic Levinstein distance between a roman-
ized source string and a target string. Unlike the 
traditional Levinstein distance calculation, the 
character alignment cost is not binary (0/1); rather 
it is the logarithm of character alignment probabil-
ity, which ensures that characters with similar pro-
nunciations (e.g. `p` and `b`) have higher 
alignment probabilities and lower cost. These 
probabilities are automatically learned from bilin-
gual name lists using EM. 
Assume the Chinese phrase f has J Chinese 
characters, , and the English candidate 
phrase e has L English words, . The 
transliteration cost between a Chinese query and 
an English translation candidate  is calculated as: 
Jfff ,..., 21
Leee ,...,, 21
f
e
 
 
where is the pinyin of Chinese character ,  
is the i th letter in , and and are their 
aligned English letters, respectively.  
is the letter transliteration probability. The translit-
eration costs between a Chinese phrase and an 
English phrase is approximated by the sum of their 
letter transliteration cost along the optimal align-
ment path, which is identified based on dynamic 
programming.   
jy jf
ijy , jy jae ),( ijae
)|( ,),( jiji yep
3.2 Translation Model 
The translation model measures the semantic 
equivalence between a Chinese phrase and an Eng-
lish candidate. One widely used model is the IBM 
model (Brown et al 1993). The phrase translation 
probability is computed using the IBM model-1 as: 
  
 
 
where is the lexical translation probabili-
ties, which can be calculated according to the IBM 
models. This alignment model is asymmetric, as 
one source word can only be aligned to one target 
word, while one target word can be aligned to mul-
tiple source words. We estimate both  
and , and define the NE translation 
cost as: 
)|( lj efp
)|( efPtrans
)|( fePtrans
).|(log)|(log),( efPfePfeC transtranstrans +=
3.3 Frequency-Distance Model 
The more often a bilingual phrase pair co-occurs, 
or the closer a bilingual phrase pair is within a 
snippet, the more likely they are translations of 
each other. The frequency-distance model meas-
ures this correlation.  
   Suppose S is the set of returned snippets for 
query , and a single returned snippet isf Ssi ? . 
The source phrase occurs in si as  ( since f 
may occur several times in a snippet). The fre-
quency-distance weight of an English candidate 
is  
jif , 1?j
e
??=
i jis f ji efd
ew
,
),(
1
)(
,
 
 
.)|(log)|(log),( ,),(??? =?
j i
jia
j
jatrl yepyepfe where is the distance between phrase   
and e, i.e., how many words are there between the 
two phrases (the separator `|` is not counted).  
),( efd jif ,
3.4 Feature Combination 
Define the confidence measure for the translitera-
tion model as: 
 
 
where e and e? are English candidate phrases, and 
m is the weight of the distance model. We empiri-
cally choose m=2 in our experiments. This 
measure indicates how good the English phrase e is 
compared with other candidates based on translit-
eration model. Similarly the translation model con-
fidence measure is defined as: 
 
 
 
 
   The overall feature cost is the linear combination 
of transliteration cost and translation cost, which 
are weighted by their confidence scores respec-
tively: 
 
 
C
jij
,
)'()],'(exp[
)()],(exp[
)|(
'
?=
e
m
trl
m
trl
trl ewfeC
ewfeC
fe?
.
)'()],'(exp[
)()],(exp[
)|(
'
?=
e
m
trans
m
trans
trans ewfeC
ewfeC
fe???
= =
=
J
j
L
l
ljJtrans efpL
efP
1 1
)|(
1
)|(
486
 ???? the Bridges of Madison-
County                                                                                   
where the linear combination weight ?  is chosen 
empirically. While trl? and trans?  represent the rela-
tive rank of the current candidate among all com-
pared candidates, C and  indicate its 
absolute likelihood, which is useful to reject the 
top 1 incorrect candidate if the true translation does 
not occur in any returned snippets.  
trl transC
                                                          
4 Experiments 
We evaluated our approach by translating a set of 
key phrases from different domains. We selected 
310 Chinese key phrases from 12 domains as the 
test set, which were almost equally distributed 
within these domains. We also manually translated 
them as the reference translations. Table 1 shows 
some typical phrases and their translations, where 
one may find that correct key phrase translations 
need both phonetic transliterations and semantic 
translations. We evaluated inclusion rate, defined 
as the percentage of correct key phrase translations 
which can be retrieved in the returned snippets; 
alignment accuracy, defined as the percentage of 
key phrase translations which can be correctly 
aligned given that these translations are included in 
the snippets; and overall translation accuracy, de-
fined as the percentage of key phrases which can 
be translated correctly. We compared our approach 
with the LiveTrans5 (Cheng et.al. 2004) system, an 
unknown word translator using web corpora, and 
we observed better translation performance using 
our approach. 
4.1 Query Translation Inclusion Rate 
In the first round query search, for each Chinese 
key phrase f, on average 13 unique snippets were 
returned to identify relevant Chinese hint words f?, 
and the top 5 f's were selected to generate hint 
words e?s. In the second round f and e?s were sent 
to Google again to retrieve mixed language snip-
pets, which were used to extract e, the correct 
translation of f. 
Figure 3 shows the inclusion rate vs. the number 
of snippets used for three mixed-language web 
page searching strategies: 
                                                          
5 http://livetrans.iis.sinica.edu.tw/lt.html 
 Table 1. Test set key phrases 
? Search any web pages containing f (Zhang 
and Vines 2004); 
? Only search English web pages6 contain-
ing f (Cheng et al 2004); 
? Search any web pages containing f and 
hint words e?, as proposed in this paper.  
 
   The first search strategy resulted in a relatively 
low inclusion rate; the second achieved a much 
higher inclusion rate. However, because such Eng-
lish pages were limited, and on average only 45 
unique snippets could be found for each f, which 
resulted in a maximum inclusion rate of 85.8%. In 
the case of the cross-lingual query expansion, the 
search space was much larger but more focused 
and we achieved a high inclusion rate of 89.7% 
using 32 mixed-language snippets and 95.2% using 
165 snippets, both from the second round retrieval.  
6 These web pages are labeled by Google as ?English? web 
pages, though they may contain non-English characters. 
Movie Title 
????            Forrest Gump 
Book Title 
???   Dream of the Red Mansion 
???    La Dame aux camellias 
Organization 
Name 
????   University of Notre Dame  
??????????? David and 
Lucile Packard Foundation  
Person 
Name 
???          Ludwig Van Beethoven 
????? Audrey Hepburn 
Location 
Name 
????? Kamchatka 
??????? Taklamakan desert 
Company /
Brand 
???? Lufthansa German 
Airlines 
???? Estee Lauder 
Sci&Tech 
Terms 
???? genetic algorithm 
???? speech recognition  
Specie Term 
??               Aegypius monachus 
???              Manispentadactyla 
Military 
Term 
???              Aegis  
???              Phalcon 
Medical 
Term 
?????? SARS 
???? Arteriosclerosis 
Music Term 
????     Bird-call in the Mountain 
???        Bassoon 
Sports Term 
?????? Houston Rockets 
?????? Tour de France 
)]()|()( ff?? exp[1
)],(exp[)|(),(
eCe
feCfefeC
transtrans
trltrl= ?? +
,?
487
Table 2. Alignment accuracies using different features 
 
These search strategies are further discussed in the 
section 5. 
4.2 Translation Alignment Accuracy 
We evaluated our key phrase extraction model by 
testing queries whose correct translations were in-
cluded in the returned snippets. We used different 
feature combinations on differently sized snippets 
to compare their alignment accuracies. Table 2 
shows the result. Here ?Trl? means using the trans-
literation model, ?Trans? means using the transla-
tion model, and ?Fq-dis? means using Frequency-
Distance model. The frequency-distance model 
seemed to be the strongest single model in both 
cases (with and without hint words), while incor-
porating phonetic and semantic features provided 
additional strength to the overall performance. 
Combining all three features together yielded the 
best accuracy. Note that when more candidate 
translations were available through query expan-
sion, the alignment accuracy improved by 30% 
relative due to the frequency-distance model. 
However, using transliteration and/or translation 
models alone decreased performance because of 
more incorrect translation candidates from returned 
snippets. After incorporating the frequency-
distance model, correct translations have the 
maximum frequency-distance weights and are 
more likely to be selected as the top hypothesis. 
Therefore the combined model obtained the high-
est translation accuracy. 
4.3 Overall Translation Quality  
The overall translation qualities are listed in Table 
3, where we showed the translation accuracies of  
 
No Hints 
(Inc = 44.19%) 
With Hints 
(Inc = 95.16%) 
Table 3. Overall translation accuracy 
the top 5 hypotheses using different number of 
snippets. A hypothesized translation was consid-
ered to be correct when it matched one of the ref-
erence translations.  Using more snippets always 
increased the overall translation accuracy, and with 
all the 165 snippets (on average per query), our 
approach achieved 80% top-1 translation accuracy, 
and 90% top-5 accuracy. 
We compared the translations from a research 
statistical machine translation system (CMU-SMT, 
Vogel et al 2003) and a web-based MT engine 
(BabelFish). Due to the lack of topic-relevant con-
texts and many OOV words occurring in the source 
key phrases, their results were not satisfactory. We 
also compare our system with LiveTrans, which 
only searched within English web pages, thus with 
limited search space and more noises (incorrect 
English candidates). Therefore it was more diffi-
cult to select the correct translation. Table 4 lists 
some example key phrase translations mined from 
web corpora, as well as translations from the Ba-
belFish.  
5 Relevant Work 
Both (Cheng et al 2004) and (Zhang and Vines 
2004) exploited web corpora for translating OOV 
terms and queries. Compared with their work, our 
proposed method differs in both webpage search 
                                                          
7 http://babelfish.altavista.com/ 
Features (avg. snippets = 
10) 
(avg. snip-
pets=130) 
Trl 51.45 17.97 
Trans 51.45 40.68 
Fq-dis 53.62 73.22 
Trl+Trans 63.04 51.36 
Trl+Trans+ 
Fq-dis 65.94 86.73 
Accuracy of the Top-N Hyp. (%) Snippets 
Used Top1 Top2 Top3 Top4 Top5 
10 46.1 55.2 59.0 61.3 62.3 
20 57.4 64.2 69.7 72.6 72.9 
50 63.2 74.5 77.7 79.7 80.6 
100 75.2 84.5 85.8 87.4 87.4 
165 80.0 86.5 89.0 90.0 90.0 
Babel-
Fish7 MT 31.3 N/A N/A N/A N/A 
CMU-
SMT 21.9 N/A N/A N/A N/A 
LiveTrans
(Fast) 20.6 30.0 36.8 41.9 45.2 
LiveTrans
(Smart) 30.0 41.9 48.7 51.0 52.9 
488
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
Figure 3. Inclusion rate vs. the number of snippets used 
 
Examples Category 
Chinese Key Phrase Web-Mining Translation BabelFish? Result 
Movie  
Title ???? 
the Bridges of Madison 
County 
*Love has gone and only good 
memory has left in the dream 
Book  
Title ????? Sense and Sensibility *Reason and emotion 
Organization 
Name 
Woodrow Wilson National 
Fellowship Foundation 
*Wood the Wilson nation gets to-
gether the foundation 
??????????
?? 
Person  ???? Seiji Ozawa *Young Ze drafts you Name 
Location 
Name ????? Tsaidam Basin Qaidam Basin 
Company / ?? Clinique *Attractive blue Brand 
Sci&Tech 
Terms ????? Bayesian network *Shell Ye Si network 
Specie  ?? walrus walrus Term 
Military 
Term ????? stratofortress stratofortress 
Medical 
Term ??? glaucoma glaucoma 
Music  ??? bassoon bassoon Term 
Sports  ?????? Km Tour de France *Link law bicycle match Term 
*: Incorrect translations 
 
Table 4. Key phrase translation from web mining and a MT engine 
 
489
space and translation extraction features. Figure 4 
illustrates three different search strategies. Suppose 
we want to translate the Chinese query ?????. 
(Cheng et al 2004) only searched 188 English web 
pages which contained the source query, and 53% 
of them (100 pages) had the correct translations.  
(Zhang and Vines 2004) searched the whole 
55,100 web pages, 10% of them (5490 pages) had 
the correct translation. Our approach used query 
expansion to search any web pages containing ??
??? and English hint words, which was a larger 
search space than (Cheng et al 2004) and more 
focused compared with (Zhang and Vines 2004), 
as illustrated with the shaded region in Figure 4. 
For translation extraction features, we took advan-
tage of machine transliteration and machine trans-
lation models, and combined them with frequency 
and distance information.  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Web search space strategy comparison 
6 Discussion and Future Work 
In this paper we demonstrated the feasibility of 
the proposed approach by searching for the English 
translation for a given Chinese key phrase, where 
we use punctuations and Chinese words as the 
boundary of candidate English translations. In the 
future we plan to try more flexible translation can-
didate selection methods, and apply them to other 
language pairs. We also would like to test our ap-
proach on more standard test sets, and compare the 
performance with other systems.  
Our approach works on short snippets for query 
expansion and translation extraction, and the com-
putation time is short. Therefore the search en-
gine?s response time is the major factor of 
computational efficiency.  
 
 
7 Conclusion 
We proposed a novel approach to mine key phrase 
translations from web corpora. We used cross-
lingual query expansion to retrieve more relevant 
web pages snippets, and extracted target transla-
tions combining transliteration, translation and fre-
quency-distance models. We achieved significantly 
better results compared to the existing methods.  
8 References  
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra and 
R.L. Mercer. The Mathematics of Machine Translation: 
Parameter Estimation. In Computational Linguistics, vol 
19, number 2. pp.263-311, June, 1993. 
 
P.?J. Cheng, J.-W. Teng, R.-C. Chen, J.-H. Wang, W.-H. 
Lu, and L.-F. Chien. Translating unknown queries with 
web corpora for cross-language information retrieval. In 
the Proceedings of 27th ACM SIGIR, pp146-153. ACM 
Press, 2004. 
 
F. Huang, S.Vogel and A. Waibel. Automatic extraction 
of named entity translingual equivalence based on 
multi-feature cost minimization. In the Proceedings of 
the 41st ACL. Workshop on Multilingual and Mixed-
language Named Entity Recognition, pp124-129, Sap-
poro, Japan, July 2003. 
 
W.-H. Lu, L.-F. Chien, H.-J. Lee. Translation of web 
queries using anchor text mining. ACM Trans. Asian 
Language Information Processing  (TALIP) 1(2): 159-
172 (2002) 
 
P. Resnik and N. A. Smith, The Web as a Parallel Cor-
pus, Computational Linguistics 29(3), pp. 349-380, Sep-
tember 2003 
 
S. Vogel, Y. Zhang, F. Huang, A. Tribble, A. Venogu-
pal, B. Zhao and A. Waibel.  The CMU statistical ma-
chine translation system. In Proceedings of the MT 
Summit IX Conference New Orlean, LA, September, 
2003. 
 
Y. Zhang and P. Vines. Detection and Translation of 
OOV Terms Prior to Query Time, In the Proceedings of 
27th ACM SIGIR. pp524-525, Sheffield, England, 2004. 
 
Y. Zhang and P. Vines 2004, Using the Web for Auto-
mated Translation Extraction in Cross-Language Infor-
mation Retrieval, In Proceedings of 27th ACM SIGIR, 
pp.162-169, Sheffield, United Kingdom, 2004. 
 
Y. Zhang, F. Huang and S. Vogel, Mining Translations 
of OOV Terms from the Web through Cross-lingual 
Query Expansion, in the Proceedings of the 28th ACM 
SIGIR, Salvador, Brazil, August 2005. 
490
Statistical Machine Translation Part I: Hands-On Introduction 
 Stephan VOGEL 
InterACT, LTI 
Carnegie Mellon University 
407 South Craig Street, Pittsburgh, PA 15213
stephan.vogel@cs.cmu.edu 
 
 
 
Abstract 
Statistical machine translation (SMT) is 
currently one of the hot spots in natural 
language processing. Over the last few years 
dramatic improvements have been made, and a 
number of comparative evaluations have shown, 
that SMT gives competitive results to rule-based 
translation systems, requiring significantly less 
development time. This is particularly important 
when building translation systems for new 
language pairs or new domains. 
This workshop is intended to give an 
introduction to statistical machine translation 
with a focus on practical considerations. 
Participants should be able, after attending this 
workshop, to set out building an SMT system 
themselves and achieving good baseline results 
in a short time. 
The tutorial will cover the basics of SMT: 
? architecture of an SMT system  
? word alignment models, esp. IBM1 and 
HMM models  
? phrase alignment, from Viterbi path and 
direct phrase alignment models  
? decoder, including recombination, 
pruning, n-best list generation  
? integrating output from other MT 
engines (multi engine translation)  
? data processing: checking, cleaning, 
normalizing the data  
? evaluation, especially automatic 
evaluation (Bleu, NIST, ...), including 
significance analysis  
Theory will be put into practice. STTK, a 
statistical machine translation tool kit, will be 
introduced and used to build a working 
translation system. STTK has been developed by 
the presenter and co-workers over a number of 
years and is currently used as the basis of 
CMU's SMT system. It has also successfully 
been coupled with rule-based and example 
based machine translation modules to build a 
multi engine machine translation system. The 
source code of the tool kit will be made 
available. 
 
Biography 
Stephan Vogel is research scientist at the 
Language Technologies Institute, Carnegie 
Mellon University.  He is also affiliated to 
InterACT, the International Center for 
Advanced Communication Technologies, a joint 
center between the University of Karlsruhe, 
Germany, and Carnegie Mellon 
University.  After receiving a MSc in Physics 
from Philips University of Marburg, and a 
MPhil in History and Philosophy of Science 
from the University of Cambridge, England, he 
did his doctoral studies at the University of 
Aachen (RWTH), where he started to work on 
Statistical Machine Translation.  This remained 
the focus of his research.  Since he joined CMU 
in 2001 he built a SMT research team, which 
now consists of more then 10 PhD and Master 
students, working on a number of text and 
speech translation projects. 
 
 
275
,PSURYLQJ1DPHG(QWLW\7UDQVODWLRQ&RPELQLQJ3KRQHWLF
DQG6HPDQWLF6LPLODULWLHV
)HL+XDQJ6WHSKDQ9RJHODQG$OH[:DLEHO/DQJXDJH7HFKQRORJLHV,QVWLWXWH6FKRRORI&RPSXWHU6FLHQFHV&DUQHJLH0HOORQ8QLYHUVLW\^IKXDQJYRJHODKZ`#FVFPXHGX
$EVWUDFW
7KLVSDSHUGHVFULEHV DQ DSSURDFK WR WUDQVODWHUDUHO\RFFXUULQJQDPHGHQWLWLHV1(E\FRPELQLQJSKRQHWLFDQGVHPDQWLFVLPLODULWLHV7KHSKRQHWLFVLPLODULW\LVHVWLPDWHGIURPDVXUIDFHVWULQJ WUDQVOLWHUDWLRQPRGHO DQG WKH VHPDQWLFVLPLODULW\ LV FDOFXODWHG IURP D FRQWH[W YHFWRUVHPDQWLFPRGHO*LYHQDVRXUFH&KLQHVH1(DQG LWV FRQWH[W WKLV DSSURDFK ILUVW JHQHUDWHVTXHULHV LQ WKH WDUJHW (QJOLVK ODQJXDJH DFFRUGLQJ WR WKHFRQWH[W WUDQVODWLRQK\SRWKHVHVWKHQ VHDUFKHV IRU UHOHYDQW GRFXPHQWV IURP DWDUJHW ODQJXDJH FRUSXV 7DUJHW 1(V LQ UHWULHYHG GRFXPHQWV DUH FRPSDUHG ZLWK WKHVRXUFH1(EDVHGRQWKHLUSKRQHWLFDQGFRQWH[WXDO VHPDQWLF VLPLODULWLHV DQG WKH EHVWPDWFKHGRQHLVVHOHFWHGDVWKHFRUUHFWWUDQVODWLRQ ([SHULPHQWV VKRZ WKDW WKLV DSSURDFKDFKLHYHV Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 201?204,
New York, June 2006. c?2006 Association for Computational Linguistics
Bridging the Inflection Morphology Gap for Arabic Statistical Machine
Translation
Andreas Zollmann and Ashish Venugopal and Stephan Vogel
School of Computer Science
Carnegie Mellon University
{zollmann,ashishv,stephan.vogel}@cs.cmu.edu
Abstract
Statistical machine translation (SMT) is
based on the ability to effectively learn
word and phrase relationships from par-
allel corpora, a process which is consid-
erably more difficult when the extent of
morphological expression differs signifi-
cantly across the source and target lan-
guages. We present techniques that se-
lect appropriate word segmentations in
the morphologically rich source language
based on contextual relationships in the
target language. Our results take ad-
vantage of existing word level morpho-
logical analysis components to improve
translation quality above state-of-the-art
on a limited-data Arabic to English speech
translation task.
1 Introduction
The problem of translating from a language ex-
hibiting rich inflectional morphology to a language
exhibiting relatively poor inflectional morphology
presents several challenges to the existing compo-
nents of the statistical machine translation (SMT)
process. This inflection gap causes an abundance of
surface word forms 1 in the source language com-
pared with relatively few forms in the target lan-
guage. This mismatch aggravates several issues
1We use the term surface form to refer to a series of charac-
ters separated by whitespace
found in natural language processing: more un-
known words forms in unseen data, more words oc-
curring only once, more distinct words and lower
token-to-type ratios (mean number of occurrences
over all distinct words) in the source language than
in the target language.
Lexical relationships under the standard IBM
models (Brown et al, 1993) do not account for
many-to-many mappings, and phrase extraction re-
lies heavily on the accuracy of the IBM word-to-
word alignment. In this work, we propose an ap-
proach to bridge the inflectional gap that addresses
the issues described above through a series of pre-
processing steps based on the Buckwalter Arabic
Morphological Analyzer (BAMA) tool (Buckwalter,
2004). While (Lee et al, 2003) develop accurate
segmentation models of Arabic surface word forms
using manually segmented data, we rely instead on
the translated context in the target language, lever-
aging the manually constructed lexical gloss from
BAMA to select the appropriate segmented sense for
each Arabic source word.
Our technique, applied as preprocessing to the
source corpus, splits and normalizes surface words
based on the target sentence context. In contrast
to (Popovic and Ney, 2004) and (Nie?en and Ney,
2004), we do not modify the IBM models, and we
leave reordering effects to the decoder. Statistically
significant improvements (Zhang and Vogel, 2004)
in BLEU and NIST translation score over a lightly
stemmed baseline are reported on the available and
well known BTEC IWSLT?05 Arabic-English cor-
pus (Eck and Hori, 2005).
201
2 Arabic Morphology in Recent Work
Arabic-to-English machine translation exemplifies
some of the issues caused by the inflection gap. Re-
fer to (Buckwalter, 2005) and (Larkey et al, 2002)
for examples that highlight morphological inflection
for a simple Modern Standard Arabic (MSA) word
and basic stemming operations that we use as our
baseline system.
(Nie?en and Ney, 2000) tackle the inflection gap
for German-to-English word alignment by perform-
ing a series of morphological operations on the Ger-
man text. They fragment words based on a full
morphological analysis of the sentence, but need to
use domain specific and hand written rules to deal
with ambiguous fragmentation. (Nie?en and Ney,
2004) also extend the corpus by annotating each
source word with morphological information and
building a hierarchical lexicon. The experimental
results show dramatic improvements from sentence-
level restructuring (question inversion, separated
verb prefixes and merging phrases), but limited im-
provement from the hierarchical lexicon, especially
as the size of the training data increases.
We conduct our morphological analysis at the
word level, using Buckwalter Arabic Morphological
Analyzer (BAMA) version 2.0 (Buckwalter, 2004).
BAMA analyzes a given surface word, returning a
set of potential segmentations (order of a dozen) for
the source word into prefixes, stems, and suffixes.
Our techniques select the appropriate splitting from
that set by taking into account the target sides (full
sentences) of that word?s occurrences in the training
corpus. We now describe each splitting technique
that we apply.
2.1 BAMA: Simple fragment splitting
We begin by simply replacing each Arabic word
with the fragments representing the first of the pos-
sible splittings returned by the BAMA tool. BAMA
uses simple word-based heuristics to rank the split-
ting alternatives.
2.2 CONTEXT: Single Sense selection
In the step CONTEXT, we take advantage of the
gloss information provided in BAMA?s lexicon.
Each potential splitting corresponds to a particular
choice of prefix, stem and suffix, all of which exist
in the manually constructed lexicon, along with a set
of possible translations (glosses) for each fragment.
We select a fragmentation (choice of splitting for the
source word) whose corresponding glosses have the
most target side matches in the parallel translation
(of the full sentence). The choice of fragmentation
is saved and used for all occurrences of the surface
form word in training and testing, introducing con-
text sensitivity without parsing solutions. In case of
unseen words during testing, we segment it simply
using the first alternative from the BAMA tool. This
allows us to still translate an unseen test word cor-
rectly even if the surface form was never seen during
training.
2.3 CORRMATCH: Correspondence matching
The Arabic language often encodes linguistic in-
formation within the surface word form that is not
present in English. Word fragments that represent
this missing information are misleading in the trans-
lation process unless explicitly aligned to the NULL
word on the target side. In this step we explicitly
remove fragments that correspond to lexical infor-
mation that is not represented in English. While
(Lee, 2004) builds part of speech models to recog-
nize such elements, we use the fact that their corre-
sponding English translations in the BAMA lexicon
are empty. Examples of such fragments are case and
gender markers. As an example of CORRMATCH
removal, we present the Arabic sentence ? h?*A lA
ya zAl u gayor naZiyf ? (after BAMA only) which
becomes ?h?*A lA ya zAl gayor naZiyf? after the
CORRMATCH stage. The ?u? has been removed.
3 Experimental Framework
We evaluate the impact of inflectional splitting on
the BTEC (Takezawa et al, 2002) IWSLT05 Ara-
bic language data track. The ?Supplied? data track
includes a 20K Arabic/English sentence pair train-
ing set, as well as a development (?DevSet?) and
test (?Test05?) set of 500 Arabic sentences each and
16 reference translations per Arabic sentence. De-
tails regarding the IWSLT evaluation criteria and
data topic and collection methods are available in
(Eck and Hori, 2005). We also evaluate on test and
development data randomly sampled from the com-
plete supplied dev and test data, due to considera-
202
tions noted by (Josep M.Crego, 2005) regarding the
similarity of the development and test data sets.
3.1 System description
Translation experiments were conducted using the
(Vogel et al, 2003) system with reordering and fu-
ture cost estimation. We trained translation parame-
ters for 10 scores (language model, word and phrase
count, and 6 translation model scores from (Vogel,
2005) ) with Minimum Error Rate training on the
development set. We optimized separately for both
the NIST (Doddington, 2002) and the BLEU metrics
(Papineni et al, 2002).
4 Translation Results
Table 1 and 2 shows the results of each stage
of inflectional splitting on the BLEU and NIST
metrics. Basic orthographic normalization serves
as a baseline (merging all Alif, tar marbuta, ee
forms to the base form). The test set NIST scores
show steady improvements of up to 5 percent rel-
ative, as more sophisticated splitting techniques
are used, ie BAMA+CONTEXT+CORRMATCH.
These improvements are statistically significant over
the baseline in both metrics as measured by the tech-
niques in (Zhang and Vogel, 2004).
Our NIST results for all the final stages of inflec-
tional splitting would place us above the top NIST
scores from the ISWLT evaluation on the supplied
test set.2 On both DevSet/Test05 and the randomly
split data, we see more dramatic improvements in
the NIST scores than in BLEU. This might be due to
the NIST metric?s sensitivity to correctly translating
certain high gain words in the test corpus. Inflec-
tional splitting techniques that cause previously un-
known surface form words to be translated correctly
after splitting can significantly impact the overall
score.
5 Conclusion and Future Work
This work shows the potential for significant im-
provements in machine translation quality by di-
rectly bridging the inflectional gap across language
pairs. Our method takes advantage of source and
2The IWSLT evaluation did not allow systems to train sep-
arately for evaluation on BLEU or NIST, but results from the
proceedings indicate that top performers in each metric opti-
mized towards the respective metric.
target language context when conducting morpho-
logical analysis of each surface word form, while
avoiding complex parsing engines or refinements to
the alignment training process. Our results are pre-
sented on moderately sized corpora rather than the
scarce resource domain that has been traditionally
employed to highlight the impact of detailed mor-
phological analysis.
By showing the impact of simple processing steps
we encourage the creation of simple word and gloss
level analysis tools for new languages and show
that small investments in this direction (compared
to high octane context sensitive parsing tools) can
yield dramatic improvements, especially when rapid
development of machine translation tools becomes
increasingly relevant to the research community.
While our work focused on processing the morpho-
logically rich language and then translating ?down?
into the morphologically poor language, we plan to
use the analysis tools developed here to model the
reverse translation process as well, the harder task
of translating ?up? into a highly inflected space.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
Tim Buckwalter. 2004. Buckwalter Arabic Mor-
phological Analyzer Version 2.0. LDC Cata-
log No. LDC2004L02, Linguistic Data Consortium,
www.ldc.upenn.edu/Catalog.
Tim Buckwalter. 2005.
www.qamus.org/morphology.htm.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In In Proc. ARPA Workshop on Human Lan-
guage Technology.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proceedings of
International Workshop on Spoken Language Transla-
tion, pages 11?17.
Jose B.Marino Josep M.Crego, Adria de Gispert. 2005.
The talp ngram-based smt system for iwslt?05. In Pro-
ceedings of International Workshop on Spoken Lan-
guage Translation, pages 191?198.
203
Inflection system NIST ? Dev. NIST ? Test BLEU ? Dev. BLEU ? Test
No preprocessing 9.33 9.44 51.1 49.7
Orthographic normalization (baseline) 9.41 9.51 51.0 49.7
BAMA 9.90 9.76 (+2.5%) 52.0 50.2 (+1%)
BAMA+CONTEXT+CORRMATCH 9.91 10.02 (+5.3%) 52.8 52.0 (+4.7%)
Table 1: Translation results for each stage of inflectional splitting for the merged, sampled dev. and test
data, highest scores in bold, relative improvements in brackets
Inflection system NIST ? Dev. NIST ? Test BLEU ? Dev. BLEU ? Test
No preprocessing 9.46 9.38 51.1 49.6
Orthographic normalization (baseline) 9.58 9.35 52.1 49.8
BAMA 10.10 9.60 (+2.7%) 53.8 48.8 (-2%)
BAMA+CONTEXT+CORRMATCH 10.08 9.79 (+4.7%) 53.7 50.6 (+1.6%)
Table 2: Translation results for each stage of inflectional splitting for the BTEC Supplied DevSet/Test05
data, highest scores in bold, relative improvements in brackets
Leah Larkey, Lisa Ballesteros, and Margaret Connell.
2002. Improving stemming for arabic information re-
trieval: Light stemming and co-occurrence analysis.
In Proc. of the 25th annual international ACM SIGIR
conference on Research and development information
retrieval.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based arabic word segmentation. In ACL, Sap-
poro, Japan, July 6-7.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of the Hu-
man Language Technology and North American As-
sociation for Computational Linguistics Conference
(HLT/NAACL), Boston,MA, May 27-June 1.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In The 18th
International Conference on Computational Linguis-
tics.
Sonja Nie?en and Herman Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Comput. Linguist., 30(2):181?
204.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
Association of Computational Linguistics, pages 311?
318.
H. Popovic and Hermann Ney. 2004. Improving word
alignment quality using morpho-syntactic informa-
tion. In 20th International Conference on Computa-
tional Linguistics (CoLing), Geneva, Switzerland.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world. In
Proc. of LREC 2002, pages 147?152, Las Palmas, Ca-
nary Islands, Spain, May.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venogupal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical translation system. In Pro-
ceedings of MT Summit IX, New Orleans, LA, Septem-
ber.
Stephan Vogel. 2005. PESA: Phrase pair extraction as
sentence splitting. In Proceedings of MT Summit X,
Phuket,Thailand, September.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMII), Baltimore, MD, October.
204
Proceedings of NAACL HLT 2007, pages 364?371,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Log-linear Block Transliteration Model based on Bi-Stream HMMs
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vogel
{bzhao, nbach, ianlane, vogel}@cs.cmu.edu
Language Technologies Institute
School of Computer Science, Carnegie Mellon University
Abstract
We propose a novel HMM-based framework to
accurately transliterate unseen named entities.
The framework leverages features in letter-
alignment and letter n-gram pairs learned from
available bilingual dictionaries. Letter-classes,
such as vowels/non-vowels, are integrated to
further improve transliteration accuracy. The
proposed transliteration system is applied to
out-of-vocabulary named-entities in statistical
machine translation (SMT), and a significant
improvement over traditional transliteration ap-
proach is obtained. Furthermore, by incor-
porating an automatic spell-checker based on
statistics collected from web search engines,
transliteration accuracy is further improved.
The proposed system is implemented within
our SMT system and applied to a real transla-
tion scenario from Arabic to English.
1 Introduction
Cross-lingual natural language applications, such as in-
formation retrieval, question answering, and machine
translation for web-documents (e.g. Google translation),
are becoming increasingly important. However, current
state-of-the-art statistical machine translation (SMT) sys-
tems cannot yet translate named-entities which are not
seen during training. New named-entities, such as per-
son, organization, and location names are continually
emerging on the World-Wide-Web. To realize effective
cross-lingual natural language applications, handling out-
of-vocabulary named-entities is becoming more crucial.
Named entities (NEs) can be translated via transliter-
ation: mapping symbols from one writing system to an-
other. Letters of the source language are typically trans-
formed into the target language with similar pronunci-
ation. Transliteration between languages which share
similar alphabets and sound systems is usually not dif-
ficult, because the majority of letters remain the same.
However, the task is significantly more difficult when the
language pairs are considerably different, for example,
English-Arabic, English-Chinese, and English-Japanese.
In this paper, we focus on forward transliteration from
Arabic to English.
The work in (Arbabi et al, 1994), to our knowledge, is
the first work on machine transliteration of Arabic names
into English, French, and Spanish. The idea is to vow-
elize Arabic names by adding appropriate vowels and uti-
lizing a phonetic look-up table to provide the spelling in
the target language. Their framework is strictly applica-
ble within standard Arabic morphological rules. Knight
and Graehl (1997) introduced finite state transducers that
implement back-transliteration from Japanese to English,
which was then extended to Arabic-English in (Stalls and
Knight, 1998). Al-Onaizan and Knight (2002) translit-
erated named entities in Arabic text to English by com-
bining phonetic-based and spelling-based models, and re-
ranking candidates with full-name web counts, named en-
tities co-reference, and contextual web counts. Huang
(2005) proposed a specific model for Chinese-English
name transliteration with clusterings of names? origins,
and appropriate hypotheses are generated given the ori-
gins. All of these approaches, however, are not based
on a SMT-framework. Technologies developed for SMT
are borrowed in Virga and Khudanpur (2003) and Ab-
dulJaleel and Larkey (2003). Standard SMT alignment
models (Brown et al, 1993) are used to align letter-pairs
within named entity pairs for transliteration. Their ap-
proach are generative models for letter-to-letter transla-
tions, and the letter-alignment is augmented with heuris-
tics. Letter-level contextual information is shown to be
very helpful for transliteration. Oh and Choi (2002)
used conversion units for English-Korean Transliteration;
Goto et al (2003) used conversion units, mapping En-
glish letter-sequence into Japanese Katakana character
string. Li et al (2004) presented a framework allowing
direct orthographical mapping of transliteration units be-
tween English and Chinese, and an extended model is
presented in Ekbal et al (2006).
We propose a block-level transliteration framework, as
shown in Figure 1, to model letter-level context infor-
mation for transliteration at two levels. First, we pro-
pose a bi-stream HMM incorporating letter-clusters to
better model the vowel and non-vowel transliterations
with position-information, i.e., initial and final, to im-
prove the letter-level alignment accuracy. Second, based
on the letter-alignment, we propose letter n-gram (letter-
sequence) alignment models (block) to automatically
learn the mappings from source letter n-grams to target
letter n-grams. A few features specific for transliterations
are explored, and a log-linear model is used to combine
364
Figure 1: Transliteration System Structure. The upper-part is
the two-directional Bi-Stream HMM for letter-alignment; the
lower-part is a log-linear model for combining different feature
functions for block-level transliteration.
these features to learn block-level transliteration-pairs
from training data. The proposed transliteration frame-
work obtained significant improvements over a strong
baseline transliteration approach similar to AbdulJaleel
and Larkey (2003) and Virga and Khudanpur (2003).
The remainder of this paper is organized as follows.
In Section 2, we formulate the transliteration as a general
translation problem; in Section 4, we propose a log-linear
alignment model with a local search algorithm to model
the letter n-gram translation pairs; in Section 5, exper-
iments are presented. Conclusions and discussions are
given in Section 6.
2 Transliteration as Translation
Transliteration can be viewed as a special case of transla-
tion. In this approach, source and target NEs are split into
letter sequences, and each sequence is treated as a pseudo
sentence. The appealing reason of formulating transliter-
ation in this way is to utilize advanced alignment models,
which share ideas applied also within phrase-based sta-
tistical machine translation (Koehn, 2004).
To apply this approach to transliteration, however,
some unique aspects should be considered. First, letters
should be generated from left to right, without any re-
ordering. Thus, the transliteration models can only exe-
cute forward sequential jumps. Second, for unvowelized
languages such as Arabic, a single Arabic letter typically
maps to less than four English letters. Thus, the fertility
for each letter should be recognized to ensure reasonable
length relevance. Third, the position of the letter within
a NE is important. For example, in Arabic, letters such
as ?al? at the beginning of the NE can only be translated
into ?the? or ?al?. Therefore position information should
be considered within the alignment models.
Incorporating the above considerations, transliteration
can be formulated as a noisy channel model. Let fJ1 =
f1f2...fJ denote the source NE with J letters, eI1 =
e1e2...eI be an English transliteration candidate with I
letters. According to Bayesian decision rule:
e?I1=argmax
{eI1}
P (eI1|fJ1 )= argmax
{eI1}
P (fJ1 |eI1)P (eI1), (1)
where P (fJ1 |eI1) is the letter translation model and P (eI1)
is the English letter sequence model corresponding to
the monolingual language models in SMT. In this noisy-
channel scheme, P (fJ1 |eI1) is the key component for
transliteration, in which the transliteration between eI1
and fJ1 can be modeled at either letter-to-letter level, or
letter n-gram transliteration level (block-level).
Our transliteration models are illustrated in Figure 1.
We propose a Bi-Stream HMM of P (fJ1 |eI1) to infer
letter-to-letter alignments in two directions: Arabic-to-
English (F-to-E) and English-to-Arabic (E-to-F), shown
in the upper-part in Figure 1; refined alignment is then
obtained. We propose a log-linear model to extract block-
level transliterations with additional informative features,
as illustrated in the lower-part of Figure 1.
3 Bi-Stream HMMs for Transliteration
Standard IBM translation models (Brown et al, 1993)
can be used to obtain letter-to-letter translations. How-
ever, these models are not directly suitable, because
letter-alignment within NEs is strictly left-to-right. This
sequential property is well suited to HMMs (Vogel et al,
1996), in which the jumps from the current aligned posi-
tion can only be forward.
3.1 Bi-Stream HMMs
We propose a bi-stream HMM for letter-alignment within
NE pairs. For the source NE fJ1 and a target NE eI1, a bi-
stream HMM is defined as follows:
p(fJ1 |eI1)=
?
aJ1
J?
j=1
p(fj |eaj )p(cfj |ceaj )p(aj |aj?1), (2)
where aj maps fj to the English letter eaj at the position
aj in the English named entity. p(aj |aj?1) is the transi-
tion probability distribution assuming first-order Markov
dependency; p(fj |eaj ) is a letter-to-letter translation lex-
icon; cfj is the letter cluster of fj and p(cfj |ceaj ) is a
cluster level translation lexicon. As mentioned in the
above, the vowel/non-vowel linguistic features can be uti-
lized to cluster the letters. The letters from the same clus-
ter tend to share the similar letter transliteration forms.
p(cfj |ceaj ) enables to leverage such letter-correlation in
the transliteration process.
The HMM in Eqn. 2 generates two streams of observa-
tions: the letters together with the letters? classes follow-
ing the distribution of p(fj |eaj ) and p(cfj |ceaj ) at each
365
Figure 2: Block of letters for transliteration. A block is defined
by the left- and right- boundaries in the NE-pair.
state, respectively. To be in accordance with the mono-
tone nature of the NE?s alignment mentioned before, we
enforce the following constraints in Eqn. 3, so that the
transition can only jump forward or stay at the same state:
aj?aj?1?0 ?j ? [1, J ]. (3)
Since the two streams are conditionally independent
given the current state, the extended EM is straight-
forward, with only small modifications of the standard
forward-backward algorithm (Zhao et al, 2005), for pa-
rameter estimation.
3.2 Designing Letter-Classes
Pronunciation is typically highly structured. For in-
stance, in English the pronunciation structure of ?cvc?
(consonant-vowel-consonant) is common. By incorpo-
rating letter classes into the proposed two-stream HMM,
the models? expressiveness and robustness can be im-
proved. In this work, we focus on transliteration of Ara-
bic NEs into English. We define six non-overlapping
letter classes: vowel, consonant, initial, final, noclass,
and unknown. Initial and final classes represent semantic
markers at the beginning or end of NEs such as ?Al? and
?wAl? (in romanization form). Noclass signifies letters
which can be pronounced as both a vowel and a conso-
nant depending on context, for example, the English let-
ter ?y?. The unknown class is reserved for punctuations
and letters that we do not have enough linguistic clues for
mapping them to phonemes.
4 Transliteration Blocks
To further leverage the information from the letter-
context beyond the letter-classes incorporated in our bi-
stream HMM in Eqn. 2, we define letter n-grams, which
consist of n consecutive letters, as the basic transliter-
ation unit. A block is defined as a pair of such letter
n-grams which are transliterations of each other. Dur-
ing decoding of unseen NEs, transliteration is performed
block-by-block, rather than letter-by-letter. The goal of
transliteration model is to learn high-quality translitera-
tion blocks from the training data in a unsupervised fash-
ion.
Specifically, a block X can be represented by its left
and right boundaries in the source and target NEs shown
in Figure 2:
X = (f j+lj , ei+ki ), (4)
where f j+lj is the source letter-ngram with (l+1) letters
in source language, and its projection of ei+ki in the En-
glish NE with left boundary at the position of i, and right
boundary at (i+ k).
We formulate the block extraction as a local search
problem following the work in Zhao and Waibel (2005):
given a source letter n-gram f j+lj , search for the pro-
jected boundaries of candidate target letter n-gram ei+ki
according to a weighted combination of the diverse fea-
tures in a log-linear model detailed in ?4.3. The log-linear
model serves as a performance measure to guide the local
search, which, in our setup, is randomized hill-climbing,
to extract bilingual letter n-gram transliteration pairs.
4.1 Features for Block Transliteration
Three features: fertility, distortion, and lexical transla-
tion are investigated for inferring transliteration blocks
from the NE pairs. Each feature corresponds to one as-
pect of the block within the context of a given NE pair.
4.1.1 Letter n-gram Fertility
The fertility P (?|e) of a target letter e specifies the
probability of generating ? source letters for translitera-
tion. The fertilities can be easily read-off from the letter-
alignment, i.e., the output from the Bi-stream HMM.
Given letter fertility model P (?|ei), a target letter n-gram
eI1, and a source n-gram fJ1 of length J , we compute a
probability of letter n-gram length relevance: P (J |eI1)
via a dynamic programming.
The probability of generating J letters by the English
letter n-gram eI1 is defined:
P (J |eI1) = max{?I1,J=?Ii=1 ?i}
I?
i=1
P (?i|ei). (5)
The recursively updated cost ?[j, i] in dynamic program-
ming is defined as follows:
?[j, i] = max
?
???
???
?[j, i? 1] + logPNull(0|ei)
?[j ? 1, i? 1] + logP?(1|ei)
?[j ? 2, i? 1] + logP?(2|ei)
?[j ? 3, i? 1] + logP?(3|ei)
, (6)
where PNull(0|ei) is the probability of generating a Null
letter from ei; P?(k=1|ei) is the letter-fertility model of
generating one source letter from ei; ?[j, i] is the cost
366
so far for generating j letters from i consecutive English
letters (letter n-gram) ei1 : e1, ? ? ? , ei.
After computing the cost of ?[J, I], the probability
P (J |eI1) is computed for generating the length of the
source NE fJ1 from the English NE eI1 shown in Eqn. 5.
With this letter n-gram fertility model, for every block,
we can compute a fertility score to estimate how relevant
the lengths of the transliteration-pairs are.
4.1.2 Distortion of Centers
When aligning blocks of letters within transliteration
pairs, we expect most of them are close to the diagonal
due to the monotone alignment nature. Thus, a simple
position metric is proposed for each block considering
the relative positions within NE-pairs.
The center ?fj+lj of the source phrase f
j+l
j with a
length of (l + 1) is simply a normalized relative position
in the source entity defined as follows:
?fj+lj =
1
l + 1
j?=j+l?
j?=j
j?
l + 1 . (7)
For the center of English letter-phrase ei+ki , we first
define the expected corresponding relative center for ev-
ery source letter fj? using the lexicalized position score
as follows:
?ei+ki (fj?) =
1
k + 1 ?
?(i+k)
i?=i i? ? P (fj? |ei?)?(i+k)
i?=i P (fj? |ei?)
, (8)
where P (fj? |ei) is the letter translation lexicon estimated
in IBM Models 1?5. i is the position index, which
is weighted by the letter-level translation probabilities;
the term of
?i+k
i?=i P (fj? |ei?) provides a normalization so
that the expected center is within the range of the target
length. The expected center for ei+ki is simply the aver-
age of the ?ei+ki (fj?):
?ei+ki =
1
l + 1
j+l?
j?=j
?ei+ki (fj?) (9)
Given the estimated centers of ?fj+lj and ?ei+ki , we
can compute how close they are via the probability of
P (?fj+lj |?ei+ki ). In our case, because of the mono-
tone alignment nature of transliteration pairs, a simple
gaussian model is employed to enforce that the point
(?ei+ki ,?fj+lj ) is not far away from the diagonal.
4.1.3 Letter Lexical Transliteration
Similar to IBM Model-1 (Brown et al, 1993), we use
a ?bag-of-letter? generative model within a block to ap-
proximate the lexical transliteration equivalence:
P (f j+lj |ei+ki )=
j+l?
j?=j
i+k?
i?=i
P (fj? |ei?)P (ei? |ei+ki ), (10)
where P (ei? |ei+ki ) ' 1/(k+1) is approximated by a bag-
of-word unigram. Since named entities are usually rela-
tively short, this approximation works reasonably well in
practice.
4.2 Extended Feature Functions
Because of the underlying nature of the noisy-channel
model in our proposed transliteration approach in Section
2, the three base feature functions are extended to cover
the directions both from target-to-source and source-to-
target. Therefore, we have in total six feature functions
for inferring transliteration blocks from a named entity
pair.
Besides the above six feature functions, we also com-
pute the average letter-alignment links per block. We
count the number of letter-alignment links within the
block, and normalize the number by the length of the
source letter-ngram. Note that, we can refine the letter-
alignment by growing the intersections of the two di-
rection letter-alignments from Bi-stream HMM via ad-
ditional aligned letter-pairs seen in the union of the two.
In a way, this approach is similar to those of refining the
word-level alignment for SMT in (Och and Ney, 2003).
This step is shown in the upper-part in Figure 1.
Overall, our proposed feature functions cover rela-
tively different aspects for transliteration blocks: the
block level length relevance probability in Eqn. 5, lexical
translation equivalence, and positions? distortion from a
gaussian distribution in Eqn. 8, in both directions; and
the average number of letter-alignment links within the
block. Also, these feature functions are positive and
bounded within [0, 1]. Therefore, it is suitable to apply a
log-linear model (in ?4.3) to combine the weighted indi-
vidual strengths from the proposed feature functions for
better modeling the quality of the candidate translitera-
tion blocks. This log-linear model will serve as a per-
formance measure in a local-search in ?4.4 for inferring
transliteration blocks.
4.3 Log-Linear Transliteration Model
We propose a log-linear model to combine the seven fea-
ture functions in ?4.1 with proper weights as in Eqn. 11:
Pr(X|e, f)= exp(
?M
m=1 ?m?m(X, e, f))?
{X?} exp(
?M
m=1 ?m?m(X ?, e, f))
,
(11)
where ?m(X, e, f) are the real-valued bounded feature
functions corresponding to the seven models introduced
in ?4.1. The log-linear model?s parameters are the
weights {?m} associated with each feature function.
With hand-labeled data, {?m} can be learnt via gen-
eralized iterative scaling algorithm (GIS) (Darroch and
Ratcliff, 1972) or improved iterative scaling (IIS) (Berger
367
et al, 1996). However, as these algorithms are computa-
tionally expensive, we apply an alternative approach us-
ing a simplex down-hill algorithm to optimize the weights
toward better F-measure of block transliterations. Each
feature function corresponds to one dimension in the sim-
plex, and the local optimum only happens at a vertex of
the simplex. Simplex-downhill has several advantages:
it is an efficient approach for optimizing multi-variables
given some performance measure. We compute the F-
measure against a gold-standard block set extracted from
hand-labeled letter-alignment.
To build gold-standard blocks from hand-labeled
letter-alignment, we propose the block transliteration co-
herence in a two-stage fashion. First is the forward pro-
jection: for each candidate source letter-ngram f j+nj ,
search for its left-most el and right-most er projected
positions in the target NE according to the given letter-
alignment. Second is the backward projection: for the
target letter-gram erl , search for its left-most fl? and right-
most fr? projected positions in the source NE. Now if
l??j and r??j+n, i.e. frl is contained within the source
letter-ngram f j+nj , then this block X = (f j+nj , erl ) is de-
fined as coherent for the aligned pairs: (f j+nj , erl ) . We
accept coherent X as gold-standard blocks. This block
transliteration coherence is generally sound for extracting
the gold-blocks mostly because of the the monotone left-
to-right nature of the letter-alignment for transliteration.
A related coherence assumption can be found in (Fox,
2002), where their assumption on phrase-pairs for sta-
tistical machine translation is shown to be somewhat re-
strictive for SMT. This is mainly because the word align-
ment is often non-monotone, especially for langauge-
pairs from different families such as Arabic-English and
Chinese-English.
4.4 Aligning Letter-Blocks: a Local Search
Aligning the blocks within NE pairs can be formulated
as a local search given the heuristic function defined in
Eqn. 11. To be more specific: given a Arabic letter-ngram
f j+lj , our algorithm searches for the best translation can-
didate ei+ki in the target named entities. In our implemen-
tation, we use stochastic hill-climbing with Eqn. 11 as the
performance measure. Down-hill moves are accepted to
allow one or two left and right null letters to be attached
to ei+ki to expand the table of transliteration-blocks.
To make the local search more effective, we normal-
ize the letter translation lexicon p(f |e) within the parallel
entity pair as in:
P? (f |e) = P (f |e)?J
j?=1 P (fj? |e)
. (12)
In this way, the distribution of P? (f |e) is sharper and more
focused in the context of an entity pair.
Overall, given the parallel NE pairs, we can train the
letter level translation models in both directions via the
Bi-stream HMM in Eqn. 2. From the letter-alignment,
we can build the letter translation lexicons and fertility
tables. With these tables, the base feature functions are
then computed for each candidate block, and the features
are combined in the log-linear model in Eqn. 11. Given
a named-entity pair in the training data, we rank all the
transliteration blocks by the scores using the log-linear
model. This step is shown in the lower-part in Figure 1.
4.5 Decoding Unseen NEs
The decoding of NEs is an extension to the noisy-channel
scheme in Eqn. 1. In our configurations for NE translit-
eration, the extracted transliteration blocks are used. Our
letter ngram is a standard letter-ngram model trained us-
ing the SriLM toolkit (Stolcke, 2002). To transliterate the
unseen NEs, the decoder (Hewavitharana et al, 2005) is
configured for monotone decoding. It loads the transliter-
ation blocks and the letter-ngram LM, and it decodes the
unseen Arabic named entities with block-based translit-
eration from left to right.
5 Experiments
5.1 The Data
We have 74,887 bilingual geographic names from
LDC2005G01-NGA, 11,212 bilingual person names
from LDC2005G021, and about 6,000 bilingual names
extracted from the BAMA2 dictionary. In total, there are
92,099 NE pairs. We split them into three parts: 91,459
pairs as the training dataset, 100 pairs as the development
dataset, and 540 unique NE pairs as the held-out dataset.
An additional test set is collected from the TIDES 2003
Arabic-English machine translation evaluation test set.
The 663 sentences contain 286 unique words, which were
not covered by the available training data. From this set
of untranslated words, we manually labeled the entities of
persons, locations and organizations, giving a total of 97
unique un-translated NEs. The BAMA toolkit was used
to romanize the Arabic words. Some names from this test
set are shown in Figure 1.
These untranslated NEs make up only a very small
fraction of all words in the test set. Therefore, having
correct transliterations would give only small improve-
ments in terms of BLEU (Papineni et al, 2002) and NIST
scores. However, successfully translating these unknown
NEs is very crucial for cross-lingual distillation tasks or
question-answering based on the MT-output.
1The corpus is provided as FOUO (for official use only) in
the DARPA-GALE project
2LDC2004L02: Buckwalter Arabic Morphological Ana-
lyzer version 2.0
368
Table 1: Test Set Examples.
To evaluate the transliteration performance, we use
edit-distance between the hypothesis against a reference
set. This is to count the number of insertions, dele-
tions, and substitutions required to correct the hypoth-
esis to match the given reference. An edit-distance of
zero is a perfect match. However, NEs typically have
more than one correct variant. For example, the Arabic
name ?mHmd? (in romanized form) can be transliterated
as Muhammad or Mohammed; both are considered as
correct transliterations. Ideally, we want to have all vari-
ants as reference transliterations. To enable our translit-
eration evaluation to be more informative given only one
reference, edit-distance of one between hypothesis and
reference is considered to be an acceptable match.
5.2 Comparison of Transliteration Models
We compare the performance of three systems within our
proposed framework in Figure.1: the baseline Block sys-
tem, a system in which we use a log-linear combination
of alignment features as described in ?4.3, we call the the
L-Block system, and finally a system, which also uses
the bi-stream HMM alignment model as described in ?3.
This last system will be denoted LCBE system.
The baseline is based on the refined letter-alignment
from the two directions of IBM-Model-4, trained with a
scheme of 15h545 using GIZA++ (Och and Ney, 2004).
The final alignment was obtained by growing the inter-
sections between Arabic-to-English (AE) and English-
to-Arabic (EA) alignments with additional aligned letter-
pairs seen in the union. This is to compensate for the
inherent asymmetry in alignment models. Blocks (letter-
ngram pairs) were collected directly from the refined
letter-alignment, using the same algorithm as described
in ?4.3 for extracting gold-standard letter blocks. There is
no length restrictions to the letter-ngram extracted in our
system. All the blocks were then scored using relative
frequencies and lexical scores in both directions, similar
to the scoring of phrase-pairs in SMT (Koehn, 2004).
In the L-Block system additional feature functions as
defined in ?4.1 were computed on top of the letter-level
alignment obtained from the baseline system. A log-
linear model combining these features was learned with
the gold-blocks described in ?4.3. Transliteration blocks
were extracted using the local-search ?4.4. The other
Table 2: Transliteration accuracy for different translitera-
tion models.
System Accuracy
Baseline 39.18%
L-Block 41.24%
LCBE 46.39%
components remained the same as in the baseline system.
The LCBE system is an extension to both the baseline
and the L-Block system. The key difference in LCBE
is that our proposed bi-stream HMM in Eqn. 2 was ap-
plied in both directions with extended letter-classes. The
resulting combined alignment was used together with all
features of the L-Block system to guide the local-search
for extracting the blocks. The same procedure of decod-
ing was then carried out for the unseen NEs using the
extracted blocks.
To build the letter language model for the decoding
process, we first split the English entities into charac-
ters; additional position indicators ? begin? and ? end?
were added to the begin and end position of the named-
entity; ? middle? was added between the first name and
last name. A letter-trigram language model with SRI LM
toolkit (Stolcke, 2002) was then built using the target side
(English) of NE pairs tagged with the above position in-
formation.
Table 2 shows that the baseline system gives an accu-
racy of 39.18%, while the extended systems L-Block and
LCBE give 41.24% and 46.39%, respectively. These re-
sults show that the additional features besides the letter-
alignment are helpful. The L-Block system, which uses
these features, outperforms the baseline system signifi-
cantly by 2.1% absolute in accuracy. The results also
show that the bi-stream HMM alignment, which uses not
only the letters but also the letter-classes, leads to signif-
icant improvement. It outperforms the L-Block system,
which does not leverage the letter-classes and monotone
alignment, by 4.15% absolute.
5.3 Incorporation of Spell Checking
Our spelling-checker is based on the suggested word-
forms from web search engines for ambiguous candi-
dates. We collected web statistics frequency for both the
proposed transliteration candidates from our system, and
also the suggested candidates from web-search engines.
All the candidates were re-ranked by their frequencies.
Figure 3 shows the performances on the held-out set,
using system LCBE augmented with a spell-checker
(LCBE+Spell), with varying sizes of N-best hypotheses
lists. The held-out set contains 540 unique named entity
pairs. We show accuracy when exact match is requested
and when an edit distances of one is allowed.
369
Figure 3: Transliteration accuracy of LCBE and LCBE+Spell
models for 540 named entity pairs in the held-out set.
Figure 4: Transliteration accuracy of N-best hypotheses for
LCBE and LCBE+Spell models it the MT-03 test set.
Figure 4 shows the performances in the unseen test set
of LCBE and LCBE+Spell, with varying sizes of N-best
hypotheses lists. LCBE+Spell reaches 52% accuracy in
1-best hypothesis. In the 5-best and 10-best cases, the ac-
curacies of LCBE+Spell system archive the highest per-
formances with 66% and 72.16% respectively. The spell-
checker increases the 1-best accuracy by 11.12% and the
10-best accuracy by 7.69%. All these improvements are
statistically significant. These results are also comparable
to other state-of-the-art statistical Arabic name transliter-
ation systems such as (Al-Onaizan and Knight, 2002).
5.4 Comparison with the Google Web Translation
We finally compared our best system with the
state-of-the-art Arabic-English Google Web Translation
(Google). Table 3 shows transliteration examples from
our best system in comparison with Google (as in June
20, 2006)3. The Google system achieved 45.36% accu-
racy for the 1-best hypothesis, which is comparable to
the results when using the LCBE transliteration system,
while LCBE+Spell archived 52%.
3http://www.google.com/translate t
Table 3: Transliteration examples between LCBE+Spell
and Google web translation.
6 Conclusions and Discussions
In this paper we proposed a novel transliteration model.
Viewing transliteration as a translation task we adopt
alignment and decoding techniques used in a phrase-
based statistical machine translation system to work on
letter sequences instead of word sequences. To improve
the performance we extended the HMM alignment model
into a bi-stream HMM alignment by incorporating letter-
classes into the alignment process. We also showed that a
block-extraction approach, which uses a log-linear com-
bination of multiple alignment features, can give signif-
icant improvements in transliteration accuracy. Finally,
spell-checking based on work occurrence statistics ob-
tained from the web gave an additional boost in translit-
eration accuracy.
The goal for this work is to improve the quality of ma-
chine translation, esp. when used in cross-lingual infor-
mation retrieval and distillation tasks, by incorporating
the proposed framework to handle unknown words. Fig-
ure 5 gives an example of the difference named entity
transliteration can make. Shown are the original SMT
system output, the translation when the proposed translit-
eration models are used to translate the unknown named-
entities, and the reference translation. A comparison of
the two SMT outputs indicates that integrating the pro-
posed transliteration model into our machine translation
system can significantly improve translation utility.
Acknowledgment
This work was partially supported by grants from
DARPA (GALE project) and NFS (Str-Dust project).
References
Nasreen AbdulJaleel and Leah Larkey. 2003. Statistical
transliteration for English-Arabic cross language informa-
tion retrieval. In Proceedings of the 12th International
Conference on Information and Knowledge Management,
New Orleans, LA, USA, November.
370
Figure 5: Incorporation of the transliteration model to our
SMT System.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine translit-
eration of names in Arabic text. In Proceedings of ACL
Workshop on Computational Approaches to Semitic Lan-
guages, Philadelphia, PA, USA.
Mansur Arbabi, Scott M. Fischthal, Vincent C. Cheng, and
Elizabeth Bart. 1994. Algorithms for Arabic name translit-
eration. In IBM Journal of Research and Development,
volume 38(2), pages 183?193.
Adam L. Berger, Vincent Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. In Computational Linguistics,
volume 22 of 1, pages 39?71, March.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. In Computa-
tional Linguistics, volume 19(2), pages 263?331.
J.N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. In Annals of Mathematical
Statistics, volume 43, pages 1470?1480.
Asif Ekbal, S. Naskar, and S. Bandyopadhyay. 2006. A modi-
fied joint source channel model for machine transliteration.
In Proceedings of COLING/ACL, pages 191?198, Australia.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, pages 304?311,
Philadelphia, PA, July 6-7.
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Terumasa
Ehara. 2003. Transliteration considering context informa-
tion based on the maximum entropy method. In Proceedings
of MT-Summit IX, New Orleans, Louisiana, USA.
Sanjika Hewavitharana, Bing Zhao, Almut Silja Hildebrand,
Matthias Eck, Chiori Hori, Stephan Vogel, and Alex Waibel.
2005. The CMU statistical machine translation system
for IWSLT2005. In The 2005 International Workshop on
Spoken Language Translation.
Fei Huang. 2005. Cluster-specific name transliteration. In
Proceedings of the HLT-EMNLP 2005, Vancouver, BC,
Canada, October.
Kevin Knight and Jonathan Graehl. 1997. Machine transliter-
ation. In Proceedings of the Conference of the Association
for Computational Linguistics (ACL), Madrid, Spain.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based smt. In Proceedings of the Conference of
the Association for Machine Translation in the Americans
(AMTA), Washington DC, USA.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-
channel model for machine transliteration. In Proceedings
of 42nd ACL, pages 159?166, Barcelona, Spain.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 1:29, pages 19?51.
Franz J. Och and Hermann Ney. 2004. The alignment template
approach to statistical machine translation. In Computa-
tional Linguistics, volume 30, pages 417?449.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-Korean
transliteration model using pronunciation and contextual
rules. In Proceedings of COLING-2002, pages 1?7, Taipei,
Taiwan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proc. of the 40th Annual Conf. of the
Association for Computational Linguistics (ACL 02), pages
311?318, Philadelphia, PA, July.
Bonnie Stalls and Kevin Knight. 1998. Translating names
and technical terms in Arabic text. In Proceedings of the
COLING/ACL Workshop on Computational Approaches to
Semitic Languages, Montreal, Quebec, Canada.
Andreas Stolcke. 2002. SRILM ? An extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver.
Paola Virga and Sanjeev Khudanpur. 2003. Transliteration
of proper names in cross-lingual information retrieval. In
Proceedings of the ACL Workshop on Multi-lingual Named
Entity Recognition, Edmonton, Canada.
Stephan. Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM based word alignment in statistical machine
translation. In Proc. The 16th Int. Conf. on Computational
Lingustics, (COLING-1996), pages 836?841, Copenhagen,
Denmark.
Bing Zhao and Alex Waibel. 2005. Learning a log-linear
model with bilingual phrase-pair features for statistical
machine translation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, Jeju Island,
Korean, October.
Bing Zhao, Eric P. Xing, and Alex Waibel. 2005. Bilingual
word spectral clustering for statistical machine translation.
In Proceedings of the ACL Workshop on Building and Using
Parallel Texts, pages 25?32, Ann Arbor, Michigan, June.
371
Proceedings of NAACL HLT 2007, Companion Volume, pages 21?24,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Translation Model Pruning via Usage Statistics  
for Statistical Machine Translation 
 
 
Matthias Eck, Stephan Vogel, and Alex Waibel 
InterACT Research 
Carnegie Mellon University, Pittsburgh, USA 
matteck@cs.cmu.edu, vogel+@cs.cmu.edu, ahw@cs.cmu.edu 
 
 
 
Abstract 
We describe a new pruning approach to 
remove phrase pairs from translation mod-
els of statistical machine translation sys-
tems. The approach applies the original 
translation system to a large amount of text 
and calculates usage statistics for the 
phrase pairs. Using these statistics the rele-
vance of each phrase pair can be estimated. 
The approach is tested against a strong 
baseline based on previous work and shows 
significant improvements.  
1 Introduction 
A relatively new device for translation systems are 
small portable devices like cell phones, PDAs and 
handheld game consoles. The idea here is to have a 
lightweight and convenient translation device e.g. 
for tourists that can be easily carried. Other appli-
cations include medical, relief, and military scenar-
ios.  
Preferably such a device will offer speech-to-
speech translation for both (or multiple) translation 
directions. These devices have been researched and 
are starting to become commercially available (e.g. 
Isotani et al, 2003). The main challenges here are 
the severe restrictions regarding both memory and 
computing power on such a small portable device. 
1.1 Statistical Machine Translation  
Generally statistical machine translation systems 
have recently outperformed other translation ap-
proaches so it seems natural to also apply them in 
these scenarios.  
A main component of every statistical machine 
translation system is the translation model. The 
translation model assigns translation probabilities 
to phrase1 pairs of source and target phrases ex-
tracted from a parallel bilingual text. These phrase 
pairs are applied during the decoding process and 
their target sides are combined to form the final 
translation. A variety of algorithms to extract 
phrase pairs has been proposed. (e.g. Och and Ney, 
2000 and Vogel, 2005). 
Our proposed approach now tries to remove 
phrase pairs, which have little influence on the fi-
nal translation performance, from a translation sys-
tem (pruning of the translation model2). The goal 
is to reduce the number of phrase pairs and in turn 
the memory requirement of the whole translation 
system, while not impacting the translation per-
formance too heavily.  
The approach does not depend on the actual al-
gorithm used to extract the phrase pairs and can be 
applied to every imaginable method that assigns 
probabilities to phrase pairs. We assume that the 
phrase pairs were pre-extracted before decoding. 
(in contrast to the proposed approaches to ?online 
phrase extraction? (Zhang and Vogel, 2005; Calli-
son-Burch et al, 2005)). 
The task now is to remove enough pre-extracted 
phrase pairs in order to accommodate the possibly 
strict memory limitations of a portable device 
while restricting performance degradation as much 
as possible.  
We will not specifically address the computing 
power limitations of the portable devices in this 
paper.  
                                                          
1
 A ?phrase? here can also refer to a single word. 
2
 Small language models are also desirable and the approaches 
could be applied as well but this was not investigated yet. 
21
2 Previous work 
Previous work mainly introduced two natural ideas 
to prune phrase pairs. Both are for example di-
rectly available in the Pharaoh decoder (Koehn, 
2004). 
Probability threshold 
A very simple way to prune phrase pairs from a 
translation model is to use a probability threshold 
and remove all pairs for which the translation 
probability is below the threshold. The reasoning 
for this is that it is very unlikely that a translation 
with a very low probability will be chosen (over 
another translation candidate with a higher prob-
ability).  
Translation variety threshold 
Another way to prune phrase pairs is to impose a 
limit on the number of translation candidates for a 
certain phrase. That means the pruned translation 
model can only have equal or fewer possible trans-
lations for a given source phrase than the thresh-
old. This is accomplished by sorting the phrase 
pairs for each source phrase according to their 
probability and eliminating low probability ones 
until the threshold is reached. 
3 Pruning via Usage Statistics  
The approach presented here uses a different idea 
inspired by the Optimal Brain Damage algorithm 
for neural networks (Le Cun et al, 1990). 
The Optimal Brain Damage algorithm for neural 
networks computes a saliency for each network 
element. The saliency is the relevance for the per-
formance of the network. In each pruning step the 
element with the smallest saliency is removed, and 
the network is re-trained and all saliencies are re-
calculated etc. 
We can analogously view each phrase pair in the 
translation system as such a network element. The 
question is of course how to calculate the relevance 
for the performance for each phrase pair.  
A simple approximation was already done in the 
previous work using a probability or variety 
threshold. Here the relevance is estimated using the 
phrase pair probability or the phrase pair rank as 
relevance indicators.  
But these are not the only factors that influence 
the final selection of a phrase pair and most of 
these factors are not established during the training 
and phrase extraction process. Especially the fol-
lowing two additional factors play a major role in 
the importance of a phrase pair. 
Frequency of the source phrase  
We can clearly say that a phrase pair with a very 
common source phrase will be much more impor-
tant than a phrase pair where the source phrase oc-
curs only very rarely. 
Actual use of the phrase-pair 
But even phrase-pairs with very common source 
phrases might not be used for the final translation 
hypothesis. It is for example possible that it is part 
of a longer phrase pair that gets a higher probabil-
ity so that the shorter phrase pair is not used.  
 
Generally there are a lot of different factors influ-
encing the estimated importance of a phrase pair 
and it seems hard to consider every influence sepa-
rately. Hence the proposed idea does not use a 
combination of features to estimate the phrase pair 
importance. Instead the idea is to just apply the 
translation system to a large amount of text and see 
how often a phrase pair is actually used (i.e. influ-
ences the translation performance). If the translated 
text is large enough this will give a good statistics 
of the relevance of this respective phrase pair. This 
leads to the following algorithm: 
Algorithm 
Translate a large amount of (in-domain) data with 
the translation system (tuned on a development set) 
and collect the following two statistics for each 
phrase pair in the translation model. 
? c(phrase pair) = Count how often a phrase pair 
was considered during decoding (i.e. was 
added to the translation lattice) 
? u(phrase pair) = Count how often a phrase pair 
was used in the final translation (i.e. in the 
chosen path through the lattice). 
The overall score for a phrase pair with simple 
smoothing (+1) is calculated as:  
 
[ ] [ ]1)(*)1)(log(
)(
pair phrasepair phrase
pair phrase
++
=
uc
score
 
 
We use the logarithm function to limit the influ-
ence of the c value. The u value is more important 
as this measures how often a phrase was actually 
used in a translation hypothesis. This scoring func-
22
tion was empirically found after experimenting 
with a variety of possible scoring terms. 
The phrase pairs can then be sorted according to 
this score and the top n phrase pairs can be selected 
for a smaller phrase translation model. 
4 Data and Experiments 
4.1 Experimental Setup & Baseline 
Translation system 
The translation system that was used for the ex-
periments is a state-of-the-art statistical machine 
translation system (Eck et al 2006). The system 
uses a phrase extraction method described in Vogel 
(2005) and a 6-gram language model.  
Training and testing data 
The training data for all experiments consisted of 
the BTEC corpus (Takezawa et al, 2002) with 
162,318 lines of parallel Japanese-English text. All 
translations were done from Japanese to English. 
The language model was trained on the English 
part of the training data.   
The test set from the evaluation campaign of 
IWSLT 2004 (Akiba et al, 2004) was used as test-
ing data. This data consists of 500 lines of tourism 
data. 16 reference translations to English were 
available.  
Extracted phrases 
Phrase pairs for n-grams up to length 10 were ex-
tracted (with low frequency thresholds for higher 
n-grams). This gave 4,684,044 phrase pairs 
(273,459 distinct source phrases). The baseline 
score using all phrase pairs was 59.11 (BLEU, 
Papineni et al, 2002) with a 95% confidence inter-
val of [57.13, 61.09].  
Baseline pruning 
The approaches presented in previous work served 
as a baseline. The probability threshold was tested 
for 8 values (0 (no pruning), 0.0001, 0.0005, 0.001, 
0.005, 0.01, 0.05, 0.1) while the variety threshold 
tested for 14 values (1, 2, 3, 4, 5, 6, 8, 10, 15, 20, 
50, 100, 200, 500 (no pruning in this case)) and all 
combinations thereof. The final translation scores 
for different settings are very fluctuating. For that 
reason we defined the baseline score for each pos-
sible size as the best score that was reached with 
equal or less phrase pairs than the given size in any 
of the tested combinations.  
4.2 Results for  
Pruning via Usage Statistics 
For the proposed approach ?Pruning via Usage 
Statistics?, the translation system was applied to 
the 162,318 lines of Japanese training data. 
As explained in section 3 it was now counted for 
each phrase pair how often it occurred in a transla-
tion lattice and how often it was used for the final 
translation. The phrase pairs were then sorted ac-
cording to their relevance estimation and the top n 
phrase pairs were chosen for different values of n. 
The pruned phrase table was then used to translate 
the IWSLT 2004 test set. Table 1 shows the results 
comparing the baseline scores with the results us-
ing the described pruning. Figure 1 illustrates the 
scores. The plateaus in the baseline graph are due 
to the baseline definition as stated above. 
 
 BLEU scores  
# of Phrase  
Pairs (n) 
Baseline 
 
Pruning 
 
Relative score  
improvement 
100,000 - 0.4735 - 
200,000 0.3162 0.5008 58.38% 
300,000 0.4235 0.5154 21.70% 
400,000 0.4743 0.5241 10.50% 
500,000 0.4743 0.5269 11.09% 
600,000 0.4890 0.5359 9.59% 
800,000 0.5194 0.5394 3.85% 
1,000,000 0.5355 0.5442 1.62% 
1,500,000 0.5413 0.5523 2.03% 
2,000,000 0.5630 0.5749 2.11% 
3,000,000 0.5778 0.5798 0.35% 
4,000,000 0.5855 0.5865 0.17% 
4,684,044 0.5911 0.5911 0.00% 
Table 1: BLEU scores at different levels of pruning 
(Baseline: Best score with equal or less phrase 
pairs) 
 
For more than 1 million phrase pairs the differ-
ences are not very pronounced. However the trans-
lation score for the proposed pruning algorithm is 
still not significantly lower than the 59.11 score at 
2 million phrase pairs while the baseline drops 
slightly faster. For less than 1 million phrase pairs 
the differences become much more pronounced 
with relative improvements of up to 58% at 
200,000 phrase pairs. It is interesting to note that 
the improved pruning removes infrequent source 
23
phrases and to a lesser extent source vocabulary 
even for larger numbers of phrase pairs. 
 
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0 1,000,000 2,000,000 3,000,000 4,000,000
phrase pairs
B
LE
U 
sc
o
re
Baseline Pruning
 
Figure 1: Pruning and baseline comparison 
5 Conclusions and Future Work 
The proposed pruning algorithm is able to outper-
form a strong baseline based on previously intro-
duced threshold pruning ideas. Over 50% of phrase 
pairs can be pruned without a significant loss of 
performance. Even for very low memory situations 
the improved pruning remains a viable option 
while the baseline pruning performance drops 
heavily.  
One idea to improve this new pruning approach 
is to exchange the used count with the count of the 
phrase occurring in the best path of the lattice ac-
cording to a scoring metric. This would require 
having a reference translation available to be able 
to tell which path is the actual best one (metric-
best path). It would be interesting to compare the 
performance if the statistics is done using the met-
ric-best path on a smaller amount of data to the 
performance if the statistics is done using the 
model-best path on a larger amount (as there is no 
reference translation necessary). 
The Optimal Brain Damage algorithm recalcu-
lates the saliency after removing each network 
element. It could also be beneficial to sequentially 
prune the phrase pairs and always re-calculate the 
statistics after removing a certain number of phrase 
pairs. 
6 Acknowledgements 
This work was partly supported by the US DARPA 
under the programs GALE and TRANSTAC. 
7 References  
Yasuhiro Akiba, Marcello Federico, Noriko Kando, 
Hiromi Nakaiwa, Michael Paul, and Jun'ichi Tsujii}. 
2004. Overview of the IWSLT04 Evaluation Cam-
paign. Proceedings of IWSLT 2004, Kyoto, Japan. 
Chris Callison-Burch, Colin Bannard, and Josh Schroe-
der. 2005. Scaling Phrase-Based Statistical Machine 
Translation to Larger Corpora and Longer Phrases. 
Proceedings of ACL 2005, Ann Arbor, MI, USA. 
Yann Le Cun, John S. Denker, and Sara A. Solla. 1990. 
Optimal brain damage. In Advances in Neural In-
formation Processing Systems 2, pages 598-605. 
Morgan Kaufmann, 1990. 
Matthias Eck, Ian Lane, Nguyen Bach, Sanjika He-
wavitharana, Muntsin Kolss, Bing Zhao, Almut Silja 
Hildebrand, Stephan Vogel, and Alex Waibel. 2006. 
The UKA/CMU Statistical Machine Translation Sys-
tem for IWSLT 2006. Proceedings of IWSLT 2006, 
Kyoto, Japan.  
Ryosuke Isotani, Kyoshi Yamabana, Shinichi Ando, 
Ken Hanazawa, Shin-ya Ishikawa and Ken.ichi Iso. 
2003. Speech-to-speech translation software on 
PDAs for travel conversation. NEC research & de-
velopment, Tokyo, Japan. 
Philipp Koehn. 2004. A Beam Search Decoder for Sta-
tistical Machine Translation Models. Proceedings of 
AMTA 2004, Baltimore, MD, USA. 
Franz Josef Och and Hermann Ney, 2000. Improved 
statistical alignment models, Proceedings of ACL 
2000, Hongkong, China. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. Proceedings of 
ACL 2002, Philadelphia, PA, USA. 
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, 
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002. 
Toward a Broad-coverage Bilingual Corpus for 
Speech Translation of Travel Conversation in the 
Real World. Proceedings of LREC 2002, Las Palmas, 
Spain. 
Stephan Vogel. 2005. PESA: Phrase Pair Extraction as 
Sentence Splitting. Proceedings of MTSummit X, 
Phuket, Thailand. 
Ying Zhang and Stephan Vogel. 2005. An Efficient 
Phrase-to-Phrase Alignment Model for Arbitrarily 
Long Phrases and Large Corpora. Proceedings of 
EAMT 2005, Budapest, Hungary. 
 
24
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236?244,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Preference Grammars: Softening Syntactic Constraints
to Improve Statistical Machine Translation
Ashish Venugopal Andreas Zollmann Noah A. Smith Stephan Vogel
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{ashishv,zollmann,nasmith,vogel}@cs.cmu.edu
Abstract
We propose a novel probabilistic syn-
choronous context-free grammar formalism
for statistical machine translation, in which
syntactic nonterminal labels are represented
as ?soft? preferences rather than as ?hard?
matching constraints. This formalism allows
us to efficiently score unlabeled synchronous
derivations without forgoing traditional
syntactic constraints. Using this score as a
feature in a log-linear model, we are able
to approximate the selection of the most
likely unlabeled derivation. This helps
reduce fragmentation of probability across
differently labeled derivations of the same
translation. It also allows the importance of
syntactic preferences to be learned alongside
other features (e.g., the language model)
and for particular labeling procedures. We
show improvements in translation quality on
small and medium sized Chinese-to-English
translation tasks.
1 Introduction
Probabilistic synchronous context-free grammars
(PSCFGs) define weighted production rules that are
automatically learned from parallel training data. As
in classical CFGs, these rules make use of nontermi-
nal symbols to generalize beyond lexical modeling
of sentences. In MT, this permits translation and re-
ordering to be conditioned on more abstract notions
of context. For example,
VP? ne VB1 pas # do not VB1
represents the discontiguous translation of the
French words ?ne? and ?pas? to ?do not?, in the con-
text of the labeled nonterminal symbol ?VB? (rep-
resenting syntactic category ?verb?). Translation
with PSCFGs is typically expressed as the problem
of finding the maximum-weighted derivation consis-
tent with the source sentence, where the scores are
defined (at least in part) by R-valued weights asso-
ciated with the rules. A PSCFG derivation is a syn-
chronous parse tree.
Defining the translation function as finding the
best derivation has the unfortunate side effect of
forcing differently-derived versions of the same tar-
get sentence to compete with each other. In other
words, the true score of each translation is ?frag-
mented? across many derivations, so that each trans-
lation?s most probable derivation is the only one that
matters. The more Bayesian approach of finding
the most probable translation (integrating out the
derivations) instantiates an NP-hard inference prob-
lem even for simple word-based models (Knight,
1999); for grammar-based translation it is known
as the consensus problem (Casacuberta and de la
Higuera, 2000; Sima?an, 2002).
With weights interpreted as probabilities, the
maximum-weighted derivation is the maximum a
posteriori (MAP) derivation:
e? ? argmax
e
max
d
p(e, d | f)
where f is the source sentence, e ranges over tar-
get sentences, and d ranges over PSCFG deriva-
tions (synchronous trees). This is often described
as an approximation to the most probable transla-
tion, argmaxe
?
d p(e, d | f). In this paper, we
will describe a technique that aims to find the most
probable equivalence class of unlabeled derivations,
rather than a single labeled derivation, reducing the
fragmentation problem. Solving this problem ex-
actly is still an NP-hard consensus problem, but we
provide approximations that build on well-known
PSCFG decoding methods. Our model falls some-
where between PSCFGs that extract nonterminal
symbols from parse trees and treat them as part of
236
the derivation (Zollmann and Venugopal, 2006) and
unlabeled hierarchical structures (Chiang, 2005); we
treat nonterminal labels as random variables chosen
at each node, with each (unlabeled) rule express-
ing ?preferences? for particular nonterminal labels,
learned from data.
The paper is organized as follows. In Section 2,
we summarize the use of PSCFG grammars for
translation. We describe our model (Section 3).
Section 4 explains the preference-related calcula-
tions, and Section 5 addresses decoding. Experi-
mental results using preference grammars in a log-
linear translation model are presented for two stan-
dard Chinese-to-English tasks in Section 6. We re-
view related work (Section 7) and conclude.
2 PSCFGs for Machine Translation
Probabilistic synchronous context-free grammars
(PSCFGs) are defined by a source terminal set
(source vocabulary) TS , a target terminal set (target
vocabulary) TT , a shared nonterminal set N and a
setR of rules of the form: X ? ??, ?,w? where
? X ? N is a labeled nonterminal referred to as the
left-hand-side of the rule.
? ? ? (N ? TS)? is the source side of the rule.
? ? ? (N ? TT )? is the target side of the rule.
? w ? [0,?) is a nonnegative real-valued weight
assigned to the rule.
For visual clarity, we will use the # character to sep-
arate the source side of the rule ? from the target
side ?. PSCFG rules also have an implied one-to-
one mapping between nonterminal symbols in ? and
nonterminals symbols in ?. Chiang (2005), Zoll-
mann and Venugopal (2006) and Galley et al (2006)
all use parameterizations of this PSCFG formalism1.
Given a source sentence f and a PSCFG G, the
translation task can be expressed similarly to mono-
lingual parsing with a PCFG. We aim to find the
most likely derivation d of the input source sentence
and read off the English translation, identified by
composing ? from each rule used in the derivation.
This search for the most likely translation under the
1Galley et al (2006) rules are formally defined as tree trans-
ducers but have equivalent PSCFG forms.
MAP approximation can be defined as:
e? = tgt
(
argmax
d?D(G):src(d)=f
p(d)
)
(1)
where tgt(d) is the target-side yield of a derivation
d, and D(G) is the set of G?s derivations. Using an
n-gram language model to score derivations and rule
labels to constraint the rules that form derivations,
we define p(d) as log-linear model in terms of the
rules r ? R used in d as:
p(d) = pLM(tgt(d))?0 ?
( m?
i=1
pi(d)?i
)
?psyn(d)?m+1/Z(~?)
pi(d) =
?
r?R
hi(r)freq(r;d) (2)
psyn(d) =
{ 1 if d respects label constraints
0 otherwise (3)
where ~? = ?0 ? ? ??m+1 are weights that reflect the
relative importance of features in the model. The
features include the n-gram language model (LM)
score of the target yield sequence, a collection of m
rule feature functions hi : R ? R?0, and a ?syn-
tax? feature that (redundantly) requires every non-
terminal token to be expanded by a rule with that
nonterminal on its left-hand side. freq(r; d) denotes
the frequency of the rule r in the derivation d. Note
that ?m+1 can be effectively ignored when psyn is
defined as in Equation 3. Z(~?) is a normalization
constant that does not need to be computed during
search under the argmax search criterion in Equa-
tion 1. Feature weights ~? are trained discrimina-
tively in concert with the language model weight
to maximize the BLEU (Papineni et al, 2002) au-
tomatic evaluation metric via Minimum Error Rate
Training (MERT) (Och, 2003).
We use the open-source PSCFG rule extraction
framework and decoder from Zollmann et al (2008)
as the framework for our experiments. The asymp-
totic runtime of this decoder is:
O
(
|f |3
[
|N ||TT |2(n?1)
]K) (4)
where K is the maximum number of nonterminal
symbols per rule, |f | the source sentence length, and
237
n is the order of the n-gram LM that is used to com-
pute pLM. This constant factor in Equation 4 arises
from the dynamic programming item structure used
to perform search under this model. Using notation
from Chiang (2007), the corresponding item struc-
ture is:
[X, i, j, q(?)] : w (5)
whereX is the nonterminal label of a derivation, i, j
define a span in the source sentence, and q(?) main-
tains state required to compute pLM(?). Under the
MAP criterion we can discard derivations of lower
weight that share this item structure, but in practice
we often require additional lossy pruning to limit the
number of items produced. The Syntax-Augmented
MT model of Zollmann and Venugopal (2006), for
instance, produces a very large nonterminal set us-
ing ?slash? (NP/NN? the great) and ?plus? labels
(NP+VB ? she went) to assign syntactically mo-
tivated labels for rules whose target words do not
correspond to constituents in phrase structure parse
trees. These labels lead to fragmentation of prob-
ability across many derivations for the same target
sentence, worsening the impact of the MAP approx-
imation. In this work we address the increased frag-
mentation resulting from rules with labeled nonter-
minals compared to unlabeled rules (Chiang, 2005).
3 Preference Grammars
We extend the PSCFG formalism to include soft ?la-
bel preferences? for unlabeled rules that correspond
to alternative labelings that have been encountered
in training data for the unlabeled rule form. These
preferences, estimated via relative frequency counts
from rule occurrence data, are used to estimate the
feature psyn(d), the probability that an unlabeled
derivation can be generated under traditional syn-
tactic constraints. In classic PSCFG, psyn(d) en-
forces a hard syntactic constraint (Equation 3). In
our approach, label preferences influence the value
of psyn(d).
3.1 Motivating example
Consider the following labeled Chinese-to-English
PSCFG rules:
(4) S ? (?? VB1 #
a place where I can VB1
(3) S ? (?? VP1 #
a place where I can VP1
(2) SBAR ? (?? VP1 #
a place where I can VP1
(1) FRAG ? (?? AUX1 #
a place where I can AUX1
(8) VB ? m # eat
(1) VP ? m # eat
(1) NP ? m # eat
(10) NN ? m # dish
where the numbers are frequencies of the rule from
the training corpus. In classical PSCFG we can think
of the nonterminals mentioned in the rules as hard
constraints on which rules can be used to expand a
particular node; e.g., a VP can only be expanded by
a VP rule. In Equation 2, psyn(d) explicitly enforces
this hard constraint. Instead, we propose softening
these constraints. In the rules below, labels are rep-
resented as soft preferences.
(10) X ? (?? X1 #
a place where I can X1?
?
?
p(H0 = S, H1 = VB | r) = 0.4
p(H0 = S, H1 = VP | r) = 0.3
p(H0 = SBAR, H1 = VP | r) = 0.2
p(H0 = FRAG, H1 = AUX | r) = 0.1
?
?
?
(10) X ? m # eat{ p(H0 = VB | r) = 0.8
p(H0 = VP | r) = 0.1
p(H0 = NP | r) = 0.1
}
(10) X ? m # dish
{ p(H0 = NN | r) = 1.0 }
Each unlabeled form of the rule has an associated
distribution over labels for the nonterminals refer-
enced in the rule; the labels are random variables
Hi, with H0 the left-hand-side label. These un-
labeled rule forms are simply packed representa-
tions of the original labeled PSCFG rules. In ad-
dition to the usual features hi(r) for each rule, esti-
mated based on unlabeled rule frequencies, we now
238
have label preference distributions. These are esti-
mated as relative frequencies from the labelings of
the base, unlabeled rule. Our primary contribution
is how we compute psyn(d)?the probability that
an unlabeled derivation adheres to traditional syn-
tactic constraints?for derivations built from prefer-
ence grammar rules. By using psyn(d) as a feature
in the log-linear model, we allow the MERT frame-
work to evaluate the importance of syntactic struc-
ture relative to other features.
The example rules above highlight the potential
for psyn(d) to affect the choice of translation. The
translation of the Chinese word sequence  ( ?
? m can be performed by expanding the non-
terminal in the rule ?a place where I can X1? with
either ?eat? or ?dish.? A hierarchical system (Chi-
ang, 2005) would allow either expansion, relying on
features like pLM to select the best translation since
both expansions occurred the same number of times
in the data.
A richly-labeled PSCFG as in Zollmann and
Venugopal (2006) would immediately reject the rule
generating ?dish? due to hard label matching con-
straints, but would produce three identical, compet-
ing derivations. Two of these derivations would pro-
duce S as a root symbol, while one derivation would
produce SBAR. The two S-labeled derivations com-
pete, rather than reinforce the choice of the word
?eat,? which they both make. They will also com-
pete for consideration by any decoder that prunes
derivations to keep runtime down.
The rule preferences indicate that VB and VP are
both valid labels for the rule translating to ?eat?, and
both of these labels are compatible with the argu-
ments expected by ?a place where I can X1?. Al-
ternatively, ?dish? produces a NN label which is
not compatible with the arguments of this higher-
up rule. We design psyn(d) to reflect compatibility
between two rules (one expanding a right-hand side
nonterminal in the other), based on label preference
distributions.
3.2 Formal definition
Probabilistic synchronous context-free preference
grammars are defined as PSCFGs with the follow-
ing additional elements:
? H: a set of implicit labels, not to be confused
with the explicit label set N .
? pi: H ? N , a function that associates each im-
plicit label with a single explicit label. We can
therefore think ofH symbols as refinements of
the nonterminals inN (Matsusaki et al, 2005).
? For each rule r, we define a probability distri-
bution over vectors ~h of implicit label bindings
for its nonterminals, denoted ppref(~h | r). ~h
includes bindings for the left-hand side nonter-
minal (h0) as well as each right-hand side non-
terminal (h1, ..., h|~h|). Each hi ? H.
When N ,H are defined to include just a single
generic symbol as in (Chiang, 2005), we produce the
unlabeled grammar discussed above. In this work,
we define
? N = {S,X}
? H = {NP,DT,NN ? ? ? } = NSAMT
where N corresponds to the generic labels of Chi-
ang (2005) and H corresponds to the syntactically
motivated SAMT labels from (Zollmann and Venu-
gopal, 2006), and pi maps all elements of H to
X . We will use hargs(r) to denote the set of all
~h = ?h0, h1, ..., hk? ? Hk+1 that are valid bindings
for the rule with nonzero preference probability.
The preference distributions ppref from each rule
used in d are used to compute psyn(d) as described
next.
4 Computing feature psyn(d)
Let us view a derivation d as a collection of nonter-
minal tokens nj , j ? {1, ..., |d|}. Each nj takes an
explicit label in N . The score psyn(d) is a product,
with one factor per nj in the derivation d:
psyn(d) =
|d|?
j=1
?j (6)
Each ?j factor considers the two rules that nj partic-
ipates in. We will refer to the rule above nonterminal
token nj as rj (the nonterminal is a child in this rule)
and the rule that expands nonterminal token j as rj .
The intuition is that derivations in which these
two rules agree (at each j) about the implicit label
239
for nj , in H are preferable to derivations in which
they do not. Rather than making a decision about
the implicit label, we want to reward psyn when rj
and rj are consistent. Our way of measuring this
consistency is an inner product of preference distri-
butions:
?j ?
?
h?H
ppref(h | rj)ppref(h | rj) (7)
This is not quite the whole story, because ppref(? | r)
is defined as a joint distribution of all the implicit
labels within a rule; the implicit labels are not in-
dependent of each other. Indeed, we want the im-
plicit labels within each rule to be mutually consis-
tent, i.e., to correspond to one of the rule?s preferred
labelings, for both hargs(r) and hargs(r).
Our approach to calculating psyn within the dy-
namic programming algorithm is to recursively cal-
culate preferences for each chart item based on (a)
the smaller items used to construct the item and
(b) the rule that permits combination of the smaller
items into the larger one. We describe how the pref-
erences for chart items are calculated. Let a chart
item be denoted [X, i, j, u, ...] where X ? N and i
and j are positions in the source sentence, and
u : {h ? H | pi(h) = X} ? [0, 1]
(where ?h u(h) = 1) denotes a distribution over
possible X-refinement labels. We will refer to it
below as the left-hand-side preference distribution.
Additional information (such as language model
state) may also be included; it is not relevant here.
The simplest case is for a nonterminal token nj
that has no nonterminal children. Here the left-hand-
side preference distribution is simply given by
u(h) = ppref(h | rj) .
and we define the psyn factor to be ?j = 1.
Now consider the dynamic programming step
of combining an already-built item [X, i, j, u, ...]
rooted by explicit nonterminal X , spanning source
sentence positions i to j, with left-hand-side prefer-
ence distribution u, to build a larger item rooted by
Y through a rule r = Y ? ??X1??, ?X1??, w?with
preferences ppref(? | r).2 The new item will have
2We assume for the discussion that ?, ?? ? T ?S and ?, ?? ?
signature [Y, i ? |?|, j + |??|, v, ...]. The left-hand-
side preferences v for the new item are calculated as
follows:
v(h) = v?(h)?
h? v?(h?)
where (8)
v?(h) = ?
h??H:?h,h???hargs(r)
ppref(?h, h?? | r)? u(h?)
Renormalizing keeps the preference vectors on the
same scale as those in the rules. The psyn factor ?,
which is factored into the value of the new item, is
calculated as:
? = ?
h??H:?h,h???hargs(r)
u(h?) (9)
so that the value considered for the new item is w ?
? ? ..., where factors relating to pLM, for example,
may also be included. Coming back to our example,
if we let r be the leaf rule producing ?eat? at shared
nonterminal n1, we generate an item with:
u = ?u(VB) = 0.8, u(VP) = 0.1, u(NP) = 0.1?
?1 = 1
Combining this item with X ? ?  ( ? ? X1
# a place where I can X1 ? as r2 at nonterminal n2
generates a new target item with translation ?a place
where I can eat?, ?2 = 0.9 and v as calculated in
Fig. 1. In contrast, ?2 = 0 for the derivation where
r is the leaf rule that produces ?dish?.
This calculation can be seen as a kind of single-
pass, bottom-up message passing inference method
embedded within the usual dynamic programming
search.
5 Decoding Approximations
As defined above, accurately computing psyn(d) re-
quires extending the chart item structure with u. For
models that use the n-gram LM feature, the item
structure would be:
[X, i, j, q(?), u] : w (10)
Since u effectively summarizes the choice of rules
in a derivation, this extension would partition the
T ?T . If there are multiple nonterminals on the right-hand side
of the rule, we sum over the longer sequences in hargs(r) and
include appropriate values from the additional ?child? items?
preference vectors in the product.
240
v?(S) = ppref (?h = S, h? = VB? | r)u(VB) + ppref (?h = S, h? = VP? | r)u(VP) = (0.4? 0.8) + (0.3? 0.1) = 0.35
v?(SBAR) = p(?h = SBAR, h? = VP? | r)u(VP) = (0.2? 0.1) = 0.02
v = ?v(S) = 0.35/(v?(S) + v?(SBAR)), v(SBAR) = 0.02/v?(S) + v?(SBAR)? = ?v(S) = 0.35/0.37, v(SBAR) = 0.02/0.37?
?2 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9
Figure 1: Calculating v and ?2 for the running example.
search space further. To prevent this partitioning, we
follow the approach of Venugopal et al (2007). We
keep track of u for the best performing derivation
from the set of derivations that share [X, i, j, q(?)]
in a first-pass decoding. In a second top-down pass
similar to Huang and Chiang (2007), we can recal-
culate psyn(d) for alternative derivations in the hy-
pergraph; potentially correcting search errors made
in the first pass.
We face another significant practical challenge
during decoding. In real data conditions, the size
of the preference vector for a single rule can be very
high, especially for rules that include multiple non-
terminal symbols that are located on the left and
right boundaries of ?. For example, the Chinese-
to-English rule X ? ? X1 ? X2 # X1 ?s X2 ? has
over 24K elements in hargs(r) when learned for the
medium-sized NIST task used below. In order to
limit the explosive growth of nonterminals during
decoding for both memory and runtime reasons, we
define the following label pruning parameters:
? ?R: This parameter limits the size of hargs(r) to
the ?R top-scoring preferences, defaulting other
values to zero.
? ?L: This parameter is the same as ?R but applied
only to rules with no nonterminals. The stricter of
?L and ?R is applied if both thresholds apply.
? ?P : This parameter limits the number labels in
item preference vectors (Equation 8) to the ?P
most likely labels during decoding, defaulting
other preferences to zero.
6 Empirical Results
We evaluate our preference grammar model on
small (IWSLT) and medium (NIST) data Chinese-
to-English translation tasks (described in Table 1).
IWSLT is a limited domain, limited resource task
(Paul, 2006), while NIST is a broadcast news task
with wide genre and domain coverage. We use a
subset of the full training data (67M words of En-
glish text) from the annual NIST MT Evaluation.
Development corpora are used to train model pa-
rameters via MERT. We use a variant of MERT that
prefers sparse solutions where ?i = 0 for as many
features as possible. At each MERT iteration, a sub-
set of features ? are assigned 0 weight and optimiza-
tion is repeated. If the resulting BLEU score is not
lower, these features are left at zero.
All systems are built on the SAMT framework
described in Zollmann et al (2008), using a tri-
gram LM during search and the full-order LM dur-
ing a second hypergraph rescoring pass. Reorder-
ing limits are set to 10 words for all systems. Prun-
ing parameters during decoding limit the number of
derivations at each source span to 300.
The system ?Hier.? uses a grammar with a single
nonterminal label as in Chiang (2005). The system
?Syntax? applies the grammar from Zollmann and
Venugopal (2006) that generates a large number of
syntactically motivated nonterminal labels. For the
NIST task, rare rules are discarded based on their
frequency in the training data. Purely lexical rules
(that include no terminal symbols) that occur less
than 2 times, or non-lexical rules that occur less than
4 times are discarded.
IWSLT task: We evaluate the preference gram-
mar system ?Pref.? with parameters ?R = 100,
?L = 5, ?P = 2. Results comparing systems Pref.
to Hier. and Syntax are shown in Table 2. Auto-
matic evaluation results using the preference gram-
mar translation model are positive. The preference
grammar system shows improvements over both the
Hier. and Syntax based systems on both unseen eval-
uation sets IWSLT 2007 and 2008. The improve-
ments are clearest on the BLEU metric (matching
the MERT training criteria). On 2007 test data,
Pref. shows a 1.2-point improvement over Hier.,
while on the 2008 data, there is a 0.6-point improve-
ment. For the IWSLT task, we report additional au-
241
System Name Words in Target Text LM singleton 1-n-grams (n) Dev. Test
IWSLT 632K 431K (5) IWSLT06 IWSLT07,08
NIST 67M 102M (4) MT05 MT06
Table 1: Training data configurations used to evaluate preference grammars. The number of words in the target text
and the number of singleton 1-n-grams represented in the complete model are the defining statistics that characterize
the scale of each task. For each LM we also indicate the order of the n-gram model.
System Dev
BLEU
(lpen) ?
2007
BLEU
(lpen) ?
2008
BLEU
(lpen) ?
2008
WER ?
2008 PER
?
2008
MET. ?
2008
GTM ?
Hier. 28.0
(0.89)
37.0
(0.89)
45.9
(0.91)
44.5 39.9 61.8 70.7
Syntax 30.9
(0.96)
35.5
(0.94)
45.3
(0.95)
45.7 40.4 62.1 71.5
Pref. 28.3
(0.88)
38.2
(0.90)
46.3
(0.91)
43.8 40.0 61.7 71.2
Table 2: Translation quality metrics on the IWSLT translation task, with IWSLT 2006 as the development corpora, and
IWSLT 2007 and 2008 as test corpora. Each metric is annotated with an ? if increases in the metric value correspond
to increase in translation quality and a ? if the opposite is true. We also list length penalties for the BLEU metric to
show that improvements are not due to length optimizations alone.
tomatic evaluation metrics that generally rank the
Pref. system higher than Hier. and Syntax. As a fur-
ther confirmation, our feature selection based MERT
chooses to retain ?m+1 in the model. While the
IWSLT results are promising, we perform a more
complete evaluation on the NIST translation task.
NIST task: This task generates much larger rule
preference vectors than the IWSLT task simply due
to the size of the training corpora. We build sys-
tems with both ?R = 100, 10 varying ?P . Vary-
ing ?P isolates the relative impact of propagating
alternative nonterminal labels within the preference
grammar model. ?L = 5 for all NIST systems. Pa-
rameters ? are trained via MERT on the ?R = 100,
?L = 5, ?P = 2 system. BLEU scores for each
preference grammar and baseline system are shown
in Table 3, along with translation times on the test
corpus. We also report length penalties to show that
improvements are not simply due to better tuning of
output length.
The preference grammar systems outperform the
Hier. baseline by 0.5 points on development data,
and upto 0.8 points on unseen test data. While sys-
tems with ?R = 100 take significantly longer to
translate the test data than Hier., setting ?R = 10
takes approximately as long as the Syntax based sys-
tem but produces better slightly better results (0.3
points).
The improvements in translation quality with the
preference grammar are encouraging, but how much
of this improvement can simply be attributed to
MERT finding a better local optimum for parame-
ters ?? To answer this question, we use parameters
?? optimized by MERT for the preference grammar
system to run a purely hierarchical system, denoted
Hier.(??), which ignores the value of ?m+1 during
decoding. While almost half of the improvement
comes from better parameters learned via MERT for
the preference grammar systems, 0.5 points can be
still be attributed purely to the feature psyn. In addi-
tion, MERT does not set parameter ?m+1 to 0, cor-
roborating the value of the psyn feature again. Note
that Hier.(??) achieves better scores than the Hier.
system which was trained via MERT without psyn.
This highlights the local nature of MERT parameter
search, but also points to the possibility that train-
ing with the feature psyn produced a more diverse
derivation space, resulting in better parameters ?.
We see a very small improvement (0.1 point) by al-
lowing the runtime propagation of more than 1 non-
terminal label in the left-hand side posterior distribu-
tion, but the improvement doesn?t extend to ?P = 5.
Improved integration of the feature psyn(d) into de-
coding might help to widen this gap.
242
Test
Dev. Test time
System BLEU (lpen) BLEU (lpen) (h:mm)
Baseline Systems
Hier. 34.1 (0.99) 31.8 (0.95) 0:12
Syntax 34.7 (0.99) 32.3 (0.95) 0:45
Hier.(??) - 32.1 (0.95) 0:12
Preference Grammar: ?R = 100
?P = 1 - 32.5 (0.96) 3:00
?P = 2 34.6 (0.99) 32.6 (0.95) 3:00
?P = 5 - 32.5 (0.95) 3:20
Preference Grammar: ?R = 10
?P = 1 - 32.5 (0.95) 1:03
?P = 2 - 32.6 (0.95) 1:10
?P = 5 - 32.5 (0.95) 1:10
Table 3: Translation quality and test set translation time
(using 50 machines with 2 tasks per machine) measured
by the BLEU metric for the NIST task. NIST 2006 is
used as the development (Dev.) corpus and NIST 2007 is
used as the unseen evaluation corpus (Test). Dev. scores
are reported for systems that have been separately MERT
trained, Pref. systems share parameters from a single
MERT training. Systems are described in the text.
7 Related Work
There have been significant efforts in the both the
monolingual parsing and machine translation liter-
ature to address the impact of the MAP approxi-
mation and the choice of labels in their respective
models; we survey the work most closely related to
our approach. May and Knight (2006) extract n-
best lists containing unique translations rather than
unique derivations, while Kumar and Byrne (2004)
use the Minimum Bayes Risk decision rule to se-
lect the lowest risk (highest BLEU score) translation
rather than derivation from an n-best list. Tromble
et al (2008) extend this work to lattice structures.
All of these approaches only marginalize over alter-
native candidate derivations generated by a MAP-
driven decoding process. More recently, work by
Blunsom et al (2007) propose a purely discrimina-
tive model whose decoding step approximates the
selection of the most likely translation via beam
search. Matsusaki et al (2005) and Petrov et al
(2006) propose automatically learning annotations
that add information to categories to improve mono-
lingual parsing quality. Since the parsing task re-
quires selecting the most non-annotated tree, the an-
notations add an additional level of structure that
must be marginalized during search. They demon-
strate improvements in parse quality only when a
variational approximation is used to select the most
likely unannotated tree rather than simply stripping
annotations from the MAP annotated tree. In our
work, we focused on approximating the selection of
the most likely unlabeled derivation during search,
rather than as a post-processing operation; the meth-
ods described above might improve this approxima-
tion, at some computational expense.
8 Conclusions and Future Work
We have proposed a novel grammar formalism that
replaces hard syntactic constraints with ?soft? pref-
erences. These preferences are used to compute a
machine translation feature (psyn(d)) that scores un-
labeled derivations, taking into account traditional
syntactic constraints. Representing syntactic con-
straints as a feature allows MERT to train the cor-
responding weight for this feature relative to others
in the model, allowing systems to learn the relative
importance of labels for particular resource and lan-
guage scenarios as well as for alternative approaches
to labeling PSCFG rules.
This approach takes a step toward addressing
the fragmentation problems of decoding based on
maximum-weighted derivations, by summing the
contributions of compatible label configurations
rather than forcing them to compete. We have sug-
gested an efficient technique to approximate psyn(d)
that takes advantage of a natural factoring of deriva-
tion scores. Our approach results in improvements
in translation quality on small and medium resource
translation tasks. In future work we plan to focus on
methods to improve on the integration of the psyn(d)
feature during decoding and techniques that allow us
consider more of the search space through less prun-
ing.
Acknowledgements
We appreciate helpful comments from three anony-
mous reviewers. Venugopal and Zollmann were sup-
ported by a Google Research Award. Smith was sup-
ported by NSF grant IIS-0836431.
243
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2007.
A discriminative latent variable model for statistical
machine translation. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Francisco Casacuberta and Colin de la Higuera. 2000.
Computational complexity of problems on probabilis-
tic grammars and transducers. In Proc. of the 5th Inter-
national Colloquium on Grammatical Inference: Al-
gorithms and Applications.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compua-
tional Linguistics (ACL).
David Chiang. 2007. Hierarchical phrase based transla-
tion. Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proceed-
ings of the Annual Meeting of the Association for Com-
puational Linguistics (ACL), Sydney, Australia.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the Annual Meeting of the Association
for Compuational Linguistics (ACL).
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, Squibs and Discussion.
Shankar Kumar and William Byrne. 2004. Min-
imum Bayes-risk decoding for statistical machine
translation. In Proceedings of the Human Lan-
guage Technology and North American Association for
Computational Linguistics Conference (HLT/NAACL),
Boston,MA, May 27-June 1.
Takuya Matsusaki, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Jonathan May and Kevin Knight. 2006. A better N-best
list: Practical determinization of weighted finite tree
automata. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
Conference (HLT/NAACL).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of the Association for Compuational
Linguistics (ACL).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Compuational
Linguistics (ACL).
Michael Paul. 2006. Overview of the IWSLT 2006 eval-
uation campaign. In Proceedings of the International
Workshop on Spoken Language Translation (IWSLT).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the Annual
Meeting of the Association for Compuational Linguis-
tics (ACL).
Khalil Sima?an. 2002. Computational complexity of
probabilistic disambiguation. Grammars, 5(2):125?
151.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum Bayes-risk decod-
ing for statistical machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
Synchronous-CFG driven statistical MT. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics Conference (HLT/NAACL).
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, HLT/NAACL, New York, June.
Andreas Zollmann, Ashish Venugopal, Franz J. Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of the Conference on Computa-
tional Linguistics (COLING).
244
Proceedings of NAACL HLT 2009: Short Papers, pages 1?4,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Cohesive Constraints in A Beam Search Phrase-based Decoder
Nguyen Bach and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, stephan.vogel}@cs.cmu.edu
Colin Cherry
Microsoft Research
One Microsoft Way
Redmond, WA, 98052, USA
collinc@microsoft.com
Abstract
Cohesive constraints allow the phrase-based decoder
to employ arbitrary, non-syntactic phrases, and en-
courage it to translate those phrases in an order that
respects the source dependency tree structure. We
present extensions of the cohesive constraints, such
as exhaustive interruption count and rich interrup-
tion check. We show that the cohesion-enhanced de-
coder significantly outperforms the standard phrase-
based decoder on English?Spanish. Improvements
between 0.5 and 1.2 BLEU point are obtained on
English?Iraqi system.
1 Introduction
Phrase-based machine translation is driven by a phrasal
translation model, which relates phrases (contiguous seg-
ments of words) in the source to phrases in the tar-
get. This translation model can be derived from a word-
aligned bitext. Translation candidates are scored accord-
ing to a linear model combining several informative fea-
ture functions. Crucially, this model incorporates trans-
lation model scores and n-gram language model scores.
The component features are weighted to minimize a
translation error criterion on a development set (Och,
2003). Decoding the source sentence takes the form of
a beam search through the translation space, with inter-
mediate states corresponding to partial translations. The
decoding process advances by extending a state with the
translation of a source phrase, until each source word has
been translated exactly once. Re-ordering occurs when
the source phrase to be translated does not immediately
follow the previously translated phrase. This is penalized
with a discriminatively-trained distortion penalty. In or-
der to calculate the current translation score, each state
can be represented by a triple:
? A coverage vector HC indicates which source words
have already been translated.
? A span f? indicates the last source phrase translated
to create this state.
? A target word sequence stores context needed by the
target language model.
As cohesion concerns only movement in the source, we
can completely ignore the language model context, mak-
ing state effectively an (f? ,HC ) tuple.
To enforce cohesion during the state expansion pro-
cess, cohesive phrasal decoding has been proposed in
(Cherry, 2008; Yamamoto et al, 2008). The cohesion-
enhanced decoder enforces the following constraint: once
the decoder begins translating any part of a source sub-
tree, it must cover all the words under that subtree before
it can translate anything outside of it. This notion can be
applied to any projective tree structure, but we use de-
pendency trees, which have been shown to demonstrate
greater cross-lingual cohesion than other structures (Fox,
2002). We use a tree data structure to store the depen-
dency tree. Each node in the tree contains surface word
form, word position, parent position, dependency type
and POS tag. We use T to stand for our dependency tree,
and T (n) to stand for the subtree rooted at node n. Each
subtree T (n) covers a span of contiguous source words;
for subspan f? covered by T (n), we say f? ? T (n).
Cohesion is checked as we extend a state (f?h,HC h)
with the translation of f?h+1, creating a new state
(f?h+1,HC h+1). Algorithm 1 presents the cohesion
check described by Cherry (2008). Line 2 selects focal
points, based on the last translated phrase. Line 4 climbs
from each focal point to find the largest subtree that needs
to be completed before the translation process can move
elsewhere in the tree. Line 5 checks each such subtree
for completion. Since there are a constant number of fo-
cal points (always 2) and the tree climb and completion
checks are both linear in the size of the source, the entire
check can be shown to take linear time.
The selection of only two focal points is motivated by
a ?violation free? assumption. If one assumes that the
1
Algorithm 1 Interruption Check (Coh1) (Cherry, 2008)
Input: Source tree T , previous phrase f?h, current
phrase f?h+1, coverage vector HC
1: Interruption ? False
2: F ? the left and right-most tokens of f?h
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists and T (n) is not covered in HCh+1
then
6: Interruption ? True
7: end if
8: end for
9: Return Interruption
Figure 1: A candidate translation where Coh1 does not fire
translation represented by (f?h,HC h) contains no cohe-
sion violations, then checking only the end-points of f?h
is sufficient to maintain cohesion. However, once a soft
cohesion constraint has been implemented, this assump-
tion no longer holds.
2 Extensions of Cohesive Constraints
2.1 Exhaustive Interruption Check (Coh2)
Because of the ?violation free? assumption, Algorithm 1
implements the design decision to only suffer a violation
penalty once, when cohesion is initially broken. How-
ever, this is not necessarily the best approach, as the de-
coder does not receive any further incentive to return to
the partially translated subtree and complete it.
For example, Figure 1 illustrates a translation candi-
date of the English sentence ?the presidential election
of the united states begins tomorrow? into French. We
consider f?4 = ?begins?, f?5 = ?tomorrow?. The decoder
already translated ?the presidential election? making the
coverage vector HC 5 = ?1 1 1 0 0 0 0 1 1?. Algorithm 1
tells the decoder that no violation has been made by trans-
lating ?tomorrow? while the decoder should be informed
that there exists an outstanding violation. Algorithm 1
found the violation when the decoder previously jumped
from ?presidential? to ?begins?, and will not find another
violation when it jumps from ?begins? to ?tomorrow?.
Algorithm 2 is a modification of Algorithm 1, chang-
ing only line 2. The resulting system checks all previ-
Algorithm 2 Exhaustive Interruption Check (Coh2)
Input: Source tree T , previous phrase fh, current
phrase fh+1, coverage vector HC
1: Interruption ? False
2: F ? {f |HCh(f) = 1}
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists and T (n) is not covered in HC h+1
then
6: Interruption ? True
7: end if
8: end for
9: Return Interruption
Algorithm 3 Interruption Count (Coh3)
Input: Source tree T , previous phrase f?h, current
phrase f?h+1, coverage vector HC
1: ICount ? 0
2: F ? the left and right-most tokens of f?h
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists then
6: for each of e ? T (n) and HCh+1(e) = 0 do
7: ICount = ICount+ 1
8: end for
9: end if
10: end for
11: Return ICount
ously covered tokens, instead of only the left and right-
most tokens of f?h+1, and therefore makes no violation-
free assumption. For the example above, Algorithm 2
will inform the decoder that translating ?tomorrow? also
incurs a violation. Because |F | is no longer constant,
the time complexity of Coh2 is worse than Coh1. How-
ever, we can speed up the interruption check algorithm
by hashing cohesion checks, so we only need to run Al-
gorithm 2 once per (f?h+1,HC h+1) .
2.2 Interruption Count (Coh3) and Exhaustive
Interruption Count (Coh4)
Algorithm 1 and 2 described above interpret an inter-
ruption as a binary event. As it is possible to leave several
words untranslated with a single jump, some interrup-
tions may be worse than others. To implement this obser-
vation, an interruption count is used to assign a penalty
to cohesion violations, based on the number of words left
uncovered in the interrupted subtree. We initialize the in-
terruption count with zero. At any search state when the
cohesion violation is detected the count is incremented by
2
Algorithm 4 Exhaustive Interruption Count (Coh4)
Input: Source tree T , previous phrase fh, current
phrase fh+1, coverage vector HC
1: ICount ? 0
2: F ? {f |HCh(f) = 1}
3: for each of f ? F do
4: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
5: if n exists then
6: for each of e ? T (n) and HCh+1(e) = 0 do
7: ICount = ICount+ 1
8: end for
9: end if
10: end for
11: Return ICount
one. The modification of Algorithm 1 and 2 lead to Inter-
ruption Count (Coh3) and Exhaustive Interruption Count
(Coh4) algorithms, respectively. The changes only hap-
pen in lines 1, 5 and 6. We use an additional bit vector
to make sure that if a node has been reached once during
an interruption check, it should not be counted again. For
the example in Section 2.1, Algorithm 4 will return 4 for
ICount (?of?; ?the?; ?united?; ?states?).
2.3 Rich Interruption Constraints (Coh5)
The cohesion constraints in Sections 2.1 and 2.2 do not
leverage node information in the dependency tree struc-
tures. We propose the rich interruption constraints (Coh5)
algorithm to combine four constraints which are Interrup-
tion, Interruption Count, Verb Count and Noun Count.
The first two constraints are identical to what was de-
scribed above. Verb and Noun count constraints are en-
forcing the following rule: a cohesion violation will be
penalized more in terms of the number of verb and noun
words that have not been covered. For example, we want
to translate the English sentence ?the presidential elec-
tion of the united states begins tomorrow? to French with
the dependency structure as in Figure 1. We consider f?h
= ?the united states?, f?h+1 = ?begins?. The coverage bit
vector HC h+1 is ?0 0 0 0 1 1 1 1 0?. Algorithm 5 will re-
turn true for Interruption, 4 for ICount (?the?; ?pres-
idential?; ?election?; ?of?), 0 for V erbCount and 1 for
NounCount (?election?).
3 Experiments
We built baseline systems using GIZA++ (Och and Ney,
2003), Moses? phrase extraction with grow-diag-final-
end heuristic (Koehn et al, 2007), a standard phrase-
based decoder (Vogel, 2003), the SRI LM toolkit (Stol-
cke, 2002), the suffix-array language model (Zhang and
Vogel, 2005), a distance-based word reordering model
Algorithm 5 Rich Interruption Constraints (Coh5)
Input: Source tree T , previous phrase f?h, current
phrase f?h+1, coverage vector HC
1: Interruption ? False
2: ICount, V erbCount,NounCount ? 0
3: F ? the left and right-most tokens of f?h
4: for each of f ? F do
5: Climb the dependency tree from f until you reach
the highest node n such that f?h+1 /? T (n).
6: if n exists then
7: for each of e ? T (n) and HCh+1(e) = 0 do
8: Interruption ? True
9: ICount = ICount+ 1
10: if POS of e is ?VB? then
11: V erbCount ? V erbCount+ 1
12: else if POS of e is ?NN? then
13: NounCount ? NounCount+ 1
14: end if
15: end for
16: end if
17: end for
18: Return Interruption, ICount, V erbCount,
NounCount
with a window of 3, and the maximum number of target
phrases restricted to 10. Results are reported using low-
ercase BLEU (Papineni et al, 2002). All model weights
were trained on development sets via minimum-error rate
training (MERT) (Och, 2003) with 200 unique n-best lists
and optimizing toward BLEU. We used the MALT parser
(Nivre et al, 2006) to obtain source English dependency
trees and the Stanford parser for Arabic (Marneffe et al,
2006). In order to decide whether the translation output
of one MT engine is significantly better than another one,
we used the bootstrap method (Zhang et al, 2004) with
1000 samples (p < 0.05). We perform experiments on
English?Iraqi and English?Spanish. Detailed corpus
statistics are shown in Table 1. Table 2 shows results in
lowercase BLEU and bold type is used to indicate high-
est scores. An italic text indicates the score is statistically
significant better than the baseline.
English?Iraqi English?Spanish
English Iraqi English Spanish
sentence pairs 654,556 1,310,127
unique sent. pairs 510,314 1,287,016
avg. sentence length 8.4 5.9 27.4 28.6
# words 5.5 M 3.8 M 35.8 M 37.4 M
vocabulary 34 K 109 K 117 K 173 K
Table 1: Corpus statistics
Our English-Iraqi data come from the DARPA
TransTac program. We used TransTac T2T July 2007
3
English?Iraqi English?Spanish
july07 june08 ncd07 nct07
Baseline 31.58 23.58 33.18 32.04
+Coh1 32.63 24.45 33.49 32.72
+Coh2 32.51 24.73 33.52 32.81
+Coh3 32.43 24.19 33.37 32.87
+Coh4 32.32 24.66 33.47 33.20
+Coh5 31.98 24.42 33.54 33.27
Table 2: Scores of baseline and cohesion-enhanced systems on
English?Iraqi and English?Spanish systems
(july07) as the development set and TransTac T2T June
2008 (june08) as the held-out evaluation set. Each test set
has 4 reference translation. We applied the suffix-array
LM up to 6-gram with Good-Turing smoothing. Our co-
hesion constraints produced improvements ranging be-
tween 0.5 and 1.2 BLEU point on the held-out evaluation
set.
We used the Europarl and News-Commentary parallel
corpora for English?Spanish as provided in the ACL-
WMT 2008 shared task evaluation. The baseline sys-
tem used the parallel corpus restricting sentence length
to 100 words for word alignment and a 4-gram SRI
LM with modified Kneyser-Ney smoothing. We used
nc-devtest2007(ncd07) as the development set and nc-
test2007(nct07) as the held-out evaluation set. Each test
set has 1 translation reference. We obtained improve-
ments ranging between 0.7 and 1.2 BLEU. All cohesion
constraints perform statistically significant better than the
baseline on the held-out evaluation set.
4 Conclusions
In this paper, we explored cohesive phrasal decoding, fo-
cusing on variants of cohesive constraints. We proposed
four novel cohesive constraints namely exhaustive inter-
ruption check (Coh2), interruption count (Coh3), exhaus-
tive interruption count (Coh4) and rich interruption con-
straints (Coh5). Our experimental results show that with
cohesive constraints the system generates better transla-
tions in comparison with strong baselines. To ensure the
robustness and effectiveness of the proposed approaches,
we conducted experiments on 2 different language pairs,
namely English?Iraqi and English?Spanish. These ex-
periments also covered a wide range of training corpus
sizes, ranging from 600K sentence pairs up to 1.3 mil-
lion sentence pairs. All five proposed approaches give
positive results. The improvements on English?Spanish
are statistically significant at the 95% level. We observe
a consistent pattern indicating that the improvements are
stable in both language pairs.
Acknowledgments
This work is in part supported by the US DARPA TransTac pro-
grams. Any opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the views of DARPA. We would like
to thank Qin Gao and Matthias Eck for helpful conversations,
Johan Hall and Joakim Nirve for helpful suggestions on train-
ing and using the English dependency model. We also thanks
the anonymous reviewers for helpful comments.
References
Colin Cherry. 2008. Cohesive phrase-based decoding for statis-
tical machine translation. In Proceedings of ACL-08: HLT,
pages 72?80, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of EMNLP?02, pages 304?311,
Philadelphia, PA, July 6-7.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL?07, pages 177?180, Prague,
Czech Republic, June.
Marie-Catherine Marneffe, Bill MacCartney, and Christopher
Manning. 2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC?06, Genoa,
Italy.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. MaltParser:
A data-driven parser-generator for dependency parsing. In
Proceedings of LREC?06, Genoa, Italy.
Franz J. Och and Hermann Ney. 2003. A systematic compar-
ison of various statistical alignment models. In Computa-
tional Linguistics, volume 1:29, pages 19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of ACL?03, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: A method for automatic evaluation of
machine translation. In Proceedings of ACL?02, pages 311?
318, Philadelphia, PA, July.
Andreas Stolcke. 2002. SRILM ? An extensible language mod-
eling toolkit. In Proc. Intl. Conf. on Spoken Language Pro-
cessing, volume 2, pages 901?904, Denver.
Stephan Vogel. 2003. SMT decoder dissected: Word reorder-
ing. In Proceedings of NLP-KE?03, pages 561?566, Bejing,
China, Oct.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro Sumita.
2008. Imposing constraints from the source tree on ITG
constraints for SMT. In Proceedings of the ACL-08: HLT,
SSST-2, pages 1?9, Columbus, Ohio, June. Association for
Computational Linguistics.
Ying Zhang and Stephan Vogel. 2005. An efficient phrase-to-
phrase alignment model for arbitrarily long phrase and large
corpora. In Proceedings of EAMT?05, Budapest, Hungary,
May. The European Association for Machine Translation.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do we
need to have a better system? In Proceedings of LREC?04,
pages 2051?2054.
4
Proceedings of NAACL HLT 2009: Short Papers, pages 149?152,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Incremental Adaptation of Speech-to-Speech Translation
Nguyen Bach, Roger Hsiao, Matthias Eck, Paisarn Charoenpornsawat, Stephan Vogel,
Tanja Schultz, Ian Lane, Alex Waibel and Alan W. Black
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, wrhsiao, matteck, paisarn, stephan.vogel, tanja, ianlane, ahw, awb}@cs.cmu.edu
Abstract
In building practical two-way speech-to-speech
translation systems the end user will always wish
to use the system in an environment different from
the original training data. As with all speech sys-
tems, it is important to allow the system to adapt
to the actual usage situations. This paper investi-
gates how a speech-to-speech translation system can
adapt day-to-day from collected data on day one to
improve performance on day two. The platform is
the CMU Iraqi-English portable two-way speech-
to-speech system as developed under the DARPA
TransTac program. We show how machine transla-
tion, speech recognition and overall system perfor-
mance can be improved on day 2 after adapting from
day 1 in both a supervised and unsupervised way.
1 Introduction
As speech-to-speech translation systems move from the
laboratory into field deployment, we quickly see that mis-
match in training data with field use can degrade the per-
formance of the system. Retraining based on field us-
age is a common technique used in all speech systems
to improve performance. In the case of speech-to-speech
translation we would particularly like to be able to adapt
the system based on its usage automatically without hav-
ing to ship data back to the laboratory for retraining. This
paper investigates the scenario of a two-day event. We
wish to improve the system for the second day based on
the data collected on the first day.
Our system is designed for eyes-free use and hence
provides no graphical user interface. This allows the user
to concentrate on his surrounding environment during an
operation. The system only provides audio control and
feedback. Additionally the system operates on a push-to-
talk method. Previously the system (Hsiao et al, 2006;
Bach et al, 2007) needed 2 buttons to operate, one for the
English speaker and the other one for the Iraqi speaker.
W i i c o n t r o l l e r
M i c & L i g h t
L o u d  s p e a k e r
Figure 1: The users interact with the system
To make the system easier and faster to use, we propose
to use a single button which can be controlled by the En-
glish speaker. We mounted a microphone and a Wii re-
mote controller together as shown in 1.
Since the Wii controller has an accelerometer which
can be used to detect the orientation of the controller, this
feature can be applied to identify who is speaking. When
the English speaker points towards himself, the system
will switch to English-Iraqi translation. However, when
the Wii is pointed towards somebody else, the system will
switch to Iraqi-English translation. In addition, we attach
a light on the Wii controller providing visual feedback.
This can inform an Iraqi speaker when to start speaking.
The overall system is composed of five major compo-
nents: two automatic speech recognition (ASR) systems,
a bidirectional statistical machine translation (SMT) sys-
tem and two text-to-speech (TTS) systems.
2 Data Scenario
The standard data that is available for the TransTac
project was collected by recording human interpreter
mediated dialogs between war fighters and Iraqi native
speakers in various scenarios. The dialog partners were
aware that the data was being collected for training ma-
chine based translation devices, but would often talk di-
rectly to the human interpreter rather than pretending it
was an automatic device. This means that the dialog
149
partners soon ignored the recording equipment and used
a mostly natural language, using informal pronunciation
and longer sentences with more disfluencies than we find
in machine mediated translation dialogs.
Most users mismatch their language when they com-
municate using an automatic speech-to-speech transla-
tion system. They often switch to a clearer pronuncia-
tion and use shorter and simpler sentences with less dis-
fluency. This change could have a significant impact on
speech recognition and machine translation performance
if a system was originally trained on data from the inter-
preter mediated dialogs.
For this reason, additional data was collected during
the TransTac meeting in June of 2008. This data was
collected with dialog partners using the speech-to-speech
translation systems from 4 developer participants in the
TransTac program. The dialog partners were given a de-
scription of the specific scenario in form of a rough script
and had to speak their sentences into the translation sys-
tems. The dialog partners were not asked to actually react
to the potentially incorrect translations but just followed
the script, ignoring the output of the translation system.
This has the effect that the dialog partners are no longer
talking to a human interpreter, but to a machine, press-
ing push-to-talk buttons etc. and will change their speech
patterns accordingly.
The data was collected over two days, with around 2
hours of actual speech per day. This data was transcribed
and translated, resulting in 864 and 824 utterance pairs
on day 1 and 2, respectively.
3 ASR LM Adaptation
This section describes the Iraqi ASR system and how we
perform LM adaptation on the day 1 data to improve ASR
performance on day 2. The CMU Iraqi ASR system is
trained with around 350 hours of audio data collected un-
der the TransTac program. The acoustic model is speaker
independent but incremental unsupervised MLLR adap-
tation is performed to improve recognition. The acous-
tic model has 6000 codebooks and each codebook has
at most 64 Gaussian mixtures determined by merge-and-
split training. Semi-tied covariance and boosted MMI
discriminative training is performed to improve the model
(Povey et al, 2009). The features for the acoustic model
is the standard 39-dimension MFCC and we concatenate
adjacent 15 frames and perform LDA to reduce the di-
mension to 42 for the final feature vectors. The language
model of the ASR system is a trigram LM trained on the
audio transcripts with around three million words with
Kneser-Ney smoothing (Stolcke, 2002).
To perform LM adaptation for the ASR system, we use
the ASR hypotheses from day 1 to build a LM. This LM
is then interpolated with the original trigram LM to pro-
duce an adapted LM for day 2. We also evaluate the effect
of having transcribers provide accurate transcription ref-
erences for day 1 data, and see how it may improve the
performance on day 2. We compare unigram, bigram and
trigram LMs for adaptation. Since the amount of day 1
data is much smaller than the whole training set and we
do not assume transcription of day 1 is always available,
the interpolation weight is chosen of be 0.9 for the orig-
inal trigram LM and 0.1 for the new LM built from the
day 1 data. The WER of baseline ASR system on day 1
is 32.0%.
Base 1-g hypo 2-g hypo 3-g hypo 1-g ref 2-g ref 3-g ref
31.3 30.9 31.2 31.1 30.6 30.5 30.4
Table 1: Iraqi ASR?s WER on day 2 using different adaptation
schemes for day 1 data
The results in Table 1 show that the ASR benefits from
LM adaptation. Adapting day 1 data can slightly improve
the performance of day 2. The improvement is larger
when day 1 transcript is available which is expected. The
result also shows that the unigram LM is the most robust
model for adaptation as it works reasonably well when
transcripts are not available, whereas bigram and trigram
LM are more sensitive to the ASR errors made on day 1.
Day 1 Day 2
No ASR adaptation 29.39 27.41
Unsupervised ASR adaptation 31.55 27.66
Supervised ASR adaptation 32.19 27.65
Table 2: Impact of ASR adaptation to SMT
Table 2 shows the impact of ASR adaptation on the
performance of the translation system in BLEU (Papineni
et al, 2002). In these experiments we only performed
adaptation on ASR and still using the baseline SMT com-
ponent. There is no obvious difference between unsuper-
vised and supervised ASR adaptation on performance of
SMT on day 2. However, we can see that the difference
in WER on day 2 of unsupervised and supervised ASR
adaptation is relatively small.
4 SMT Adaptation
The Iraqi-English SMT system is trained with around
650K sentence pairs collected under the TransTac pro-
gram. We used PESA phrase extraction (Vogel, 2005)
and a suffix array language model (Zhang and Vogel,
2005). To adapt SMT components one approach is to op-
timize LM interpolation weights by minimizing perplex-
ity of the 1-best translation output (Bulyko et al, 2007).
Related work including (Eck et al, 2004) attempts to use
information retrieval to select training sentences similar
to those in the test set. To adapt the SMT components
we use a domain-specific LM on top of the background
150
language models. This approach is similar to the work
in (Chen et al, 2008). sThe adaptation framework is 1)
create a domain-specific LM via an n-best list of day 1
machine translation hypothesis, or day 1 translation ref-
erences; 2) re-tune the translation system on day 1 via
minimum error rate training (MERT) (Venugopal and Vo-
gel, 2005).
Use Day 1 Day 2
Baseline 29.39 27.41
500 Best 1gramLM 29.18 27.23
MT Hypos 2gramLM 29.53 27.50
3gramLM 29.36 27.23
Table 3: Performance in BLEU of unsupervised adaptation.
The first question we would like to address is whether
our adaptation obtains improvements via an unsupervised
manner. We take day 1 baseline ASR hypothesis and use
the baseline SMT to get the MT hypothesis and a 500-
best list. We train a domain LM using the 500-best list
and use the MT hypotheses as the reference in MERT. We
treat day 1 as a development set and day 2 as an unseen
test set. In Table 3 we compare the performance of four
systems: the baseline which does not have any adaptation
steps; and 3 adapted systems using unigram, bigram and
trigram LMs build from 500-best MT hypotheses.
Use Day 1 Day 2
Baseline (no tune) 29.39 27.41
Baseline (tune) 29.49 27.30
500 Best 1gramLM 30.27 28.29
MT Hypos 2gramLM 30.39 28.30
3gramLM 28.36 24.64
MT Ref 1gramLM MT Ref 30.53 28.35
Table 4: Performance in BLEU of supervised adaptation.
Experimental results from unsupervised adaptation did
not show consistent improvements but suggest we may
obtain gains via supervised adaptation. In supervised
adaptation, we assume we have day 1 translation refer-
ences. The references are used in MERT. In Table 4 we
show performances of two additional systems which are
the baseline system without adaptation but tuned toward
day 1, and the adapted system which used day 1 trans-
lation references to train a unigram LM (1gramLM MT
Ref). The unigram and bigram LMs from 500-best and
unigram LM from MT day 1 references perform rela-
tively similar on day 2. Using a trigram 500-best LM
returned a large degradation and this LM is sensitive to
the translation errors on day1
5 Joint Adaptation
In Sections 3 and 4 we saw that individual adaptation
helps ASR to reduce WER and SMT to increase BLEU
ASR SMT Day 1 Day 2
No adaptation No adaptation 29.39 27.41
Unsupervised ASR 1gramLM 500-Best 32.07 28.65
adaptation with MT Hypo
1gramLM ASR hypo 1gramLM MT Ref 31.76 28.83
Supervised ASR 1gramLM 500-Best 32.48 28.59
adaptation with MT Hypo
1gramLM transcription 1gramLM MT Ref 32.68 28.60
Table 5: Performance in BLEU of joint adaptation.
score. The next step in validating the adaptation frame-
work was to check if the joint adaptation of ASR and
SMT on day 1 data will lead to improvements on day
2. Table 5 shows the combination of ASR and SMT
adaptation methods. Improvements are obtained by us-
ing both ASR and SMT adaptation. Joint adaptation con-
sistently gained more than one BLEU point improvement
on day 2. Our best system is unsupervised ASR adapta-
tion via 1gramLM of ASR day 1 transcription coupled
with supervised SMT adaptation via 1gramLM of day
1 translation references. An interesting result is that to
have a better result on day 2 our approach only requires
translation references on day 1. We selected 1gramLM
of 500-best MT hypotheses to conduct the experiments
since there is no significant difference between 1gramLM
and 2gramLM on day 2 as showed in Table 3.
6 Selective Adaptation
The previous results indicate that we require human
translation references on day 1 data to get improved per-
formance on day 2. However, our goal is to make a better
system on day 2 but try to minimize human efforts on day
1. Therefore, we raise two questions: 1) Can we still ob-
tain improvements by not using all of day 1 data? and 2)
Can we obtain more improvements?
To answer these questions we performed oracle exper-
iments when we take the translation hypotheses on day
1 of the baseline SMT and compare them with transla-
tion references, then select sentences which have BLEU
scores higher than a threshold. The subset of day 1 sen-
tences is used to perform supervised adaptation in a sim-
ilar way showed in section 5. These experiments also
simulate the situation when we have a perfect confidence
score for machine translation hypothesis selection. Table
6 shows results when we use various portions of day 1 to
perform adaptation. By using day 1 sentences which have
smoothed sentence BLEU scores higher than 10 or 20 we
have very close performance with adaptation by using all
day 1 data. The results also show that by using 416 sen-
tences which have sentence BLEU score higher than 40
on day 1, our adapted translation components outperform
the baseline. Performance starts degrading after 50. Ex-
perimental results lead to the answer for question 1) that
151
by using less day 1 data our adapted translation compo-
nents still obtain improvements compare with the base-
line, and 2) we did not see that using less data will lead
us to a better performance compare with using all day 1
data.
No. sents Day 1 Day 2
Baseline 29.39 27.41
? 0 864 30.27 28.29
? 10 797 31.15 28.27
? 20 747 30.81 28.24
? 30 585 30.04 27.71
? 40 416 29.72 27.65
? 50 296 30.06 27.04
Correct 98 29.18 27.19
Table 6: Performance in BLEU of selective adaptation
W i c o n t r o l e M & L g
h r n u
d
r c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  
h r n u c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  Effective Phrase Translation Extraction from Alignment Models
Ashish Venugopal
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
ashishv@cs.cmu.edu
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
vogel+@cs.cmu.edu
Alex Waibel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
ahw@cs.cmu.edu
Abstract
Phrase level translation models are ef-
fective in improving translation qual-
ity by addressing the problem of local
re-ordering across language boundaries.
Methods that attempt to fundamentally
modify the traditional IBM translation
model to incorporate phrases typically do
so at a prohibitive computational cost. We
present a technique that begins with im-
proved IBM models to create phrase level
knowledge sources that effectively repre-
sent local as well as global phrasal con-
text. Our method is robust to noisy align-
ments at both the sentence and corpus
level, delivering high quality phrase level
translation pairs that contribute to signif-
icant improvements in translation quality
(as measured by the BLEU metric) over
word based lexica as well as a competing
alignment based method.
1 Introduction
Statistical Machine Translation defines the task
of translating a source language sentence
 
		

into a target language sentence
 


		
. The traditional framework presented in
(Brown et al, 1993) assumes a generative process
where the source sentence is passed through a noisy
stochastic process to produce the target sentence.
The task can be formally stated as finding the 

s.t 

= Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 77?80,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Recent Improvements in the
CMU Large Scale Chinese-English SMT System
Almut Silja Hildebrand, Kay Rottmann, Mohamed Noamany, Qin Gao,
Sanjika Hewavitharana, Nguyen Bach and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
silja, kayrm, mfn, qing, sanjika, nbach, vogel+@cs.cmu.edu
Abstract
In this paper we describe recent improvements
to components and methods used in our statis-
tical machine translation system for Chinese-
English used in the January 2008 GALE eval-
uation. Main improvements are results of
consistent data processing, larger statistical
models and a POS-based word reordering ap-
proach.
1 Introduction
Building a full scale Statistical Machine Transla-
tion (SMT) system involves many preparation and
training steps and it consists of several components,
each of which contribute to the overall system per-
formance. Between 2007 and 2008 our system im-
proved by 5 points in BLEU from 26.60 to 31.85
for the unseen MT06 test set, which can be mainly
attributed to two major points.
The fast growth of computing resources over
the years make it possible to use larger and larger
amounts of data in training. In Section 3 we show
how parallelizing model training can reduce training
time by an order of magnitude and how using larger
training data as well as more extensive models im-
prove translation quality.
Word reordering is still a difficult problem in
SMT. In Section 4 we apply a Part Of Speech (POS)
based syntactic reordering model successfully to our
large Chinese system.
1.1 Decoder
Our translation system is based on the CMU
SMT decoder as described in (Hewavitharana et
al., 2005). Our decoder is a phrase-based beam
search decoder, which combines multiple models
e.g. phrase tables, several language models, a dis-
tortion model ect. in a log-linear fashion. In order
to find an optimal set of weights, we use MER train-
ing as described in (Venugopal et al, 2005), which
uses rescoring of the top n hypotheses to maximize
an evaluation metric like BLEU or TER.
1.2 Evaluation
In this paper we report results using the BLEU met-
ric (Papineni et al, 2002), however as the evaluation
criterion in GALE is HTER (Snover et al, 2006), we
also report in TER (Snover et al, 2005).
We used the test sets from the NIST MT evalua-
tions from the years 2003 and 2006 as development
and unseen test data.
1.3 Training Data
In translation model training we used the Chinese-
English bilingual corpora relevant to GALE avail-
able through the LDC1. After sentence alignment
these sources add up to 10.7 million sentences with
301 million running words on the English side. Our
preprocessing steps include tokenization on the En-
glish side and for Chinese: automatic word segmen-
tation using the revised version of the Stanford Chi-
nese Word Segmenter2 (Tseng et al, 2005) from
2007, replacement of traditional by simplified Chi-
nese characters and 2-byte to 1-byte ASCII charac-
ter normalization. After data cleaning steps like e.g.
removal of sentence pairs with very unbalanced sen-
1http://projects.ldc.upenn.edu/gale/data/catalog.html
2http://nlp.stanford.edu/software/segmenter.shtml
77
tence length etc., we used the remaining 10 million
sentences with 260 million words (English) in trans-
lation model training (260M system).
2 Number Tagging
Systematic tagging and pre-translation of numbers
had shown significant improvements for our Arabic-
English system, so we investigated this for Chinese-
English. The baseline for these experiments was a
smaller system with 67 million words (67M) bilin-
gual training data (English) and a 500 million word
3-gram LM with a BLEU score of 27.61 on MT06.
First we pre-translated all numbers in the testdata
only, thus forcing the decoder to treat the numbers as
unknown words. Probably because the system could
not match longer phrases across the pre-translated
numbers, the overall translation quality degraded by
1.6 BLEU to 26.05 (see Table 1).
We then tagged all numbers in the training corpus,
replaced them with a placeholder tag and re-trained
the translation model. This reduced the vocabu-
lary and enabled the decoder to generalize longer
phrases across numbers. This strategy did not lead to
the expected result, the BLEU score for MT06 only
reached 25.97 BLEU.
System MT03 MT06
67M baseline 31.45/60.93 27.61/62.18
test data tagged ? 26.06/63.36
training data tagged 29.07/62.52 25.97/63.39
Table 1: Number tagging experiments, BLEU/TER
Analysing this in more detail, we found, the rea-
son for this degradation in translation quality could
be the unbalanced occurrence of number tags in the
training data. From the bilingual sentence pairs,
which contain number tags, 66.52% do not contain
the same number of tags on the Chinese and the En-
glish side. As a consequence 52% of the phrase pairs
in the phrase table, which contain number tags had
to be removed, because the tags were unbalanced.
This hurts system performance considerably.
3 Scaling up to Large Data
3.1 Language Model
Due to the availability of more computing resources,
we were able to extend the language model history
from 4- to 5-gram, which improved translation qual-
ity from 29.49 BLEU to 30.22 BLEU for our large
scale 260M system (see Table 2). This shows, that
longer LM histories help if we are able to use enough
data in model training.
System MT03 MT06
260M, 4gram 31.20/61.00 29.49/61.00
260M, 5gram 32.20/60.59 30.22/60.81
Table 2: 4- and 5-gram LM,260M system, BLEU/TER
The language model was trained on the sources
from the English Gigaword Corpus V3, which con-
tains several newspapers for the years between 1994
to 2006. We also included the English side of the
bilingual training data, resulting in a total of 2.7 bil-
lion running words after tokenization.
We trained separate open vocabulary language
models for each source and interpolated them using
the SRI Language Modeling Toolkit (Stolcke, 2002).
Table 3 shows the interpolation weights for the dif-
ferent sources. Apart from the English part of the
bilingual data, the newswire data from the Chinese
Xinhua News Agency and the Agence France Press
have the largest weights. This reflects the makeup of
the test data, which comes in large parts from these
sources. Other sources, as for example the UN par-
lamentary speeches or the New York Times, differ
significantly in style and vocabulary from the test
data and therefore get small weights.
xin 0.30 cna 0.06 nyt 0.03
bil 0.26 un 0.07 ltw 0.01
afp 0.21 apw 0.05
Table 3: LM interpolation weights per source
3.2 Speeding up Model Training
To accelerate the training of word alignment
models we implemented a distributed version of
GIZA++ (Och and Ney, 2003), based on the latest
version of GIZA++ and a parallel version developed
at Peking University (Lin et al, 2006). We divide the
bilingual training data in equal parts and distribute it
over several processing nodes, which perform align-
ment independently. In each iteration the nodes read
the model from the previous step and output all nec-
essary counts from the data for the models, e.g. the
78
co-occurrence or fertility model. A master process
collects the counts from the nodes, normalizes them
and outputs the intermediate model for each itera-
tion.
This distributed GIZA++ version finished training
the word alignment up to IBM Model 4 for both lan-
guage directions on the full bilingual corpus (260
million words, English) in 39 hours. On average
about 11 CPUs were running concurrently. In com-
parison the standard GIZA++ implementation fin-
ished the same training in 169 hours running on 2
CPUs, one for each language direction.
We used the Pharaoh/Moses package (Koehn et
al., 2007) to extract and score phrase pairs using the
grow-diag-final extraction method.
3.3 Translation Model
We trained two systems, one on the full data and one
without the out-of-domain corpora: UN parlament,
HK hansard and HK law parallel texts. These parla-
mentary sessions and law texts are very different in
genre and style from the MT test data, which con-
sists mainly of newspaper texts and in recent years
also of weblogs, broadcast news and broadcast con-
versation. The in-domain training data had 3.8 mil-
lion sentences and 67 million words (English). The
67 million word system reached a BLEU score of
29.65 on the unseeen MT06 testset. Even though the
full 260M system was trained on almost four times
as many running words, the baseline score for MT06
only increased by 0.6 to 30.22 BLEU (see Table 4).
System MT03 MT06
67M in-domain 32.42/60.26 29.65/61.22
260M full 32.20/60.59 30.22/60.81
Table 4: In-domain only or all training data, BLEU/TER
The 67M system could not translate 752 Chinese
words out of 38937, the number of unknown words
decreased to 564 for the 260M system. To increase
the unigram coverage of the phrase table, we added
the lexicon entries that were not in the phrase table
as one-word translations. This lowered the number
of unknown words further to 410, but did not effect
the translation score.
4 POS-based Reordering
As Chinese and English have very different word
order, reordering over a rather limited distance dur-
ing decoding is not sufficient. Also using a simple
distance based distortion probability leaves it essen-
tially to the language model to select among dif-
ferent reorderings. An alternative is to apply auto-
matically learned reordering rules to the test sen-
tences before decoding (Crego and Marino, 2006).
We create a word lattice, which encodes many re-
orderings and allows long distance reordering. This
keeps the translation process in the decoder mono-
tone and makes it significantly faster compared to
allowing long distance reordering at decoding time.
4.1 Learning Reordering Rules
We tag both language sides of the bilingual corpus
with POS information using the Stanford Parser3
and extract POS based reordering patterns from
word alignment information. We use the context in
which a reordering pattern is seen in the training data
as an additional feature. Context refers to the words
or tags to the left or to the right of the sequence for
which a reordering pattern is extracted.
Relative frequencies are computed for every rule
that has been seen more than n times in the training
corpus (we observed good results for n > 5).
For the Chinese system we used only 350k bilin-
gual sentence pairs to extract rules with length of
up to 15. We did not reorder the training corpus
to retrain the translation model on modified Chinese
word order.
4.2 Applying Reordering Rules
To avoid hard decisions, we build a lattice struc-
ture for each source sentence as input for our de-
coder, which contains reordering alternatives consis-
tent with the previously extracted rules.
Longer reordering patterns are applied first.
Thereby shorter patterns can match along new paths,
creating short distance reordering on top of long dis-
tance reordering. Every outgoing edge of a node is
scored with the relative frequency of the pattern used
on the following sub path (For details see (Rottmann
and Vogel, 2007)). These model scores give this re-
3http://nlp.stanford.edu/software/lex-parser.shtml
79
ordering approach an advantage over a simple jump
model with a sliding window.
System MT03 MT06
260M, standard 32.20/60.59 30.22/60.81
260M, lattice 33.53/59.74 31.74/59.59
Table 5: Reordering lattice decoding in BLEU/TER
The system with reordering lattice input outper-
forms the system with a reordering window of 4
words by 1.5 BLEU (see Table 5).
5 Summary
The recent improvements to our Chinese-English
SMT system (see Fig. 1) can be mainly attributed to
a POS based word reordering method and the possi-
bility to work with larger statistical models.
We used the lattice translation functionality of our
decoder to translate reordering lattices. They are
built using reordering rules extracted from tagged
and aligned parallel data. There is further potential
for improvement in this approach, as we did not yet
reorder the training corpus and retrain the translation
model on modified Chinese word order.Improvements in BLEU
242526
272829
303132
33
2007 67M+3gr 260M+3gr 260M+4gr 260M+5gr 260M+RO
Figure 1: Improvements for MT06 in BLEU
We modified GIZA++ to run in parallel, which en-
abled us to include especially longer sentences into
translation model training. We also extended our de-
coder to use 5-gram language models and were able
to train an interpolated LM from all sources of the
English GigaWord Corpus.
Acknowledgments
This work was partly funded by DARPA under
the project GALE (Grant number #HR0011-06-2-
0001).
References
Josep M. Crego and Jose B. Marino. 2006. Reordering
Experiments for N-Gram-Based SMT. Spoken Lan-
guage Technology Workshop, Palm Beach, Aruba.
Sanjika Hewavitharana, Bing Zhao, Almut Silja Hilde-
brand, Matthias Eck, Chiori Hori, Stephan Vogel and
Alex Waibel. 2005. The CMU Statistical Machine
Translation System for IWSLT 2005. IWSLT 2005,
Pittsburgh, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL 2007,
Demonstration Session, Prague, Czech Republic.
Xiaojun Lin, Xinhao Wang, and Xihong Wu. 2006.
NLMP System Description for the 2006 NIST MT
Evaluation. NIST 2006 MT Evaluation.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Poukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. ACL 2002, Philadel-
phia, USA.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
based Distortion Model. TMI-2007: 11th Interna-
tional Conference on Theoretical and Methodological
Issues in MT, Skvde, Sweden.
Mathew Snover, Bonnie Dorr, Richard Schwartz, John
Makhoul, Linnea Micciula and Ralph Weischedel.
2005. A Study of Translation Error Rate with Tar-
geted Human Annotation. LAMP-TR-126, University
of Maryland, College Park and BBN Technologies.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. 7th Conference of AMTA, Cambridge, Mas-
sachusetts, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. ICSLP, Denver, Colorado.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky and Christopher Manning. 2005. A Con-
ditional Random Field Word Segmenter. Fourth
SIGHAN Workshop on Chinese Language Processing.
Ashish Venugopal, Andreas Zollman and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. ACL 2005,
WPT-05, Ann Arbor, MI
80
Word Alignment Based on Bilingual Bracketing
Bing Zhao
Language Technologies Institute
Carnegie Mellon University
bzhao@cs.cmu.edu
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
vogel+@cs.cmu.edu
Abstract
In this paper, an improved word alignment
based on bilingual bracketing is described. The
explored approaches include using Model-1
conditional probability, a boosting strategy for
lexicon probabilities based on importance sam-
pling, applying Parts of Speech to discriminate
English words and incorporating information
of English base noun phrase. The results of
the shared task on French-English, Romanian-
English and Chinese-English word alignments
are presented and discussed.
1 Introduction
Bilingual parsing based word alignment is promising
but still difficult. The goal is to extract structure in-
formation from parallel sentences, and thereby improve
word/phrase alignment via bilingual constraint transfer.
This approach can be generalized to the automatic acqui-
sition of a translation lexicon and phrase translations esp.
for languages for which resources are relatively scarce
compared with English.
The parallel sentences in building Statistical Machine
Translation (SMT) systems are mostly unrestricted text
where full parsing often fails, and robustness with respect
to the inherent noise of the parallel data is important.
Bilingual Bracketing [Wu 1997] is one of the bilingual
shallow parsing approaches studied for Chinese-English
word alignment. It uses a translation lexicon within a
probabilistic context free grammar (PCFG) as a genera-
tive model to analyze the parallel sentences with weak
order constraints. This provides a framework to incorpo-
rate knowledge from the English side such as POS, phrase
structure and potentially more detailed parsing results.
In this paper, we use a simplified bilingual bracket-
ing grammar together with a statistical translation lexicon
such as the Model-1 lexicon [Brown 1993] to do the bilin-
gual bracketing. A boosting strategy is studied and ap-
plied to the statistical lexicon training. English POS and
Base Noun Phrase (NP) detection are used to further im-
prove the alignment performance. Word alignments and
phrase alignments are extracted from the parsing results
as post processing. The settings of different translation
lexicons within the bilingual bracketing framework are
studied and experiments on word-alignment are carried
out on Chinese-English, French-English, and Romanian-
English language pairs.
The paper is structured as follows: in section 2, the
simplified bilingual bracketing used in our system is de-
scribed; in section 3, the boosting strategy based on im-
portance sampling for IBM Model-1 lexicon is intro-
duced; in section 4, English POS and English Base Noun
Phrase are used to constrain the alignments ; in section
5, the experimental results are shown; summary and con-
clusions are given in section 6.
2 Bilingual Bracketing
In [Wu 1997], the Bilingual Bracketing PCFG was intro-
duced, which can be simplified as the following produc-
tion rules:
A ? [AA] (1)
A ? < AA > (2)
A ? f/e (3)
A ? f/null (4)
A ? null/e (5)
Where f and e are words in the target vocabulary Vf and
source vocabulary Ve respectively. A is the alignment
of texts. There are two operators for bracketing: direct
bracketing denoted by [ ], and inverse bracketing, de-
noted by <>. The A-productions are divided into two
classes: syntactic {(1),(2)}and lexical rules {(3),(4),(5)}.
Each A-production rule has a probability.
In our algorithm, we use the same PCFG. However,
instead of estimating the probabilities for the production
rules via EM as described in [Wu 1997], we assign the
probabilities to the rules using the Model-1 statistical
translation lexicon [Brown et al 1993].
Because the syntactic A-production rules do not com-
pete with the lexical rules, we can set them some default
values. Also we make no assumptions which bracketing
direction is more likely to occur, thus the probabilities
for [ ] and <> are set to be equal. As for the lexical
rules, we experimented with the conditional probabilities
p(e|f), p(f |e) and the interpolation of p(f |e, epos) and
p(f |e) (described in section 4.1). As for these probabil-
ities of aligning a word to the null word or to unknown
words, they are set to be 1e-7, which is the default small
value used in training Model-1.
The word alignment can then be done via maximizing
the likelihood of matched words subject to the bracketing
grammar using dynamic programming.
The result of the parsing gives bracketing for both in-
put sentences as well as bracket algnments indicating the
corresponding brackets between the sentence pairs. The
bracket algnment includes a word alignment as a by-
product. One example for French-English (the test set
sentence pair #18) is shown as below:
[[it1 is2 ] [quite3 [understandable4 .5 ]]]
[[ce1 est2 ] [tout3 [[?4 [fait5 comprihensible6 ] ] .7]]]
[[it1/ce1 is2/est2 ] [quite3/tout3 [[e/?4 [e/fait5
understandable4/comprihensible6 ] ] .5/.7]]]
3 Boosting Strategy of Model-1 Lexicon
The probabilities for the lexical rules are Model-1 condi-
tional probabilities p(f |e), which can be estimated using
available toolkits such as [Franz 2000].
This strategy is a three-pass training of Model-1, which
was shown to be effective in our Chinese-English align-
ment experiments. The first two passes are carried out to
get Viterbi word alignments based on Model-1?s param-
eters in both directions: from source to target and then
vice versa. An intersection of the two Viterbi word align-
ments is then calculated. The highly frequent word-pairs
in the intersection set are considered to be important sam-
ples supporting the alignment of that word-pair. This ap-
proach, which is similar to importance sampling, can be
summarized as follows:
Denote a sample as a co-occurred word-pair as
x = (ei, fj) with its observed frequency: C(x) =
freq(ei, fj); Denote I(x) = freq(ei, fj) as the fre-
quency of that word-pair x observed in the intersection
of the two Viterbi alignments.
? Build I(x) = freq(ei, fj) from the intersection of
alignments in two directions.
? Generate x = (ei, fj) and its C(x) = freq(ei, fj)
observed from a given parallel corpus;
? Generate random variable u from uniform [0,1] dis-
tribution independent of x;
? If I(x)M ?C(x) ? u, then accept x, where M is a finite
known constant M > 0;
? Re-weight sample x: Cb(x) = C(x)?(1+?), ? > 0)
The modified counts (weighted samples) are re-
normalized to get a proper probability distribution, which
is used in the next iteration of EM training. The constant
M is a threshold to remove the potential noise from the
intersection set. M ?s value is related to the size of the
training corpus, the larger its size, the larger M should
be. ? is chosen as a small positive value. The overall idea
is to collect those word-pairs which are reliable and give
an additional pseudo count to them.
4 Incorporating English Grammatical
Constraints
There are several POS taggers, base noun phrase detec-
tors and parsers available for English. Both the shallow
and full parsing information of English sentences can be
used as constraints in Bilingual Bracketing. Here, we
explored utilizing English POS and English base noun
phrase boundaries.
4.1 Incorporating English POS
The correctly aligned words from two languages are very
likely to have the same POS. For example, a Chinese
noun is very likely to be aligned with a English noun.
While the English POS tagging is often reliable and ac-
curate, the POS tagging for other languages is usually not
easily acquired nor accurate enough. Modelling only the
English POS in word alignment is usually a practical way.
Given POS information for only the English side, we
can discriminate English words and thus disambiguate
the translation lexicon. We tagged each English word
in the parallel corpus, so that each English word is as-
sociated with its POS denoted as epos. The English
word and its POS were concatenated into one pseudo
word. For example: beginning/NN and beginning/VBG
are two pseudo words which occurred in our training
corpus. Then the Model-1 training was carried out on
this concatenated parallel corpus to get estimations of
p(f |e, epos).
One potential problem is the estimation of p(f |e, epos).
When we concatenated the word with its POS, we im-
plicitly increased the vocabulary size. For example, for
French-English training set, the English vocabulary in-
creased from 57703 to 65549. This may not cause a prob-
lem when the training data?s size is large. But for small
parallel corpora, some correct word-pair?s p(f |e, epos)
will be underestimated due to the sparse data, and some
word-pairs become unknown in p(f |e, epos). So in our
system, we actually interpolated p(f |e, epos) with p(f |e)
as a mixture model for robustness:
P (A ? f/e|A) = ??P (f |e)+(1??)?P (f |e, epos) (6)
Where ? can be estimated by EM for this two-mixture
model on the training data, or a grid search via cross-
validation.
4.2 Incorporating English Base Noun Boundaries
The English sentence is bracketed according to the syn-
tactic A-production rules. This bracketing can break an
English noun phrase into separated pieces, which are
not in accordance with results from standard base noun
phrase detectors. Though the word-alignments may still
be correct, but for the phrase level alignment, it is not
desired.
One solution is to constrain the syntactic A-production
rules to penalize bracketing English noun phrases into
separated pieces. The phrase boundaries can be obtained
by using a base noun phrase detection toolkit [Ramshaw
1995], and the boundaries are loaded into the bracketing
program. During the dynamic programming, before ap-
plying a syntactic A-production rule, the program checks
if the brackets defined by the syntactic rule violate the
noun phrase boundaries. If so, an additional penalty is
attached to this rule.
5 Experiments
All the settings described so far are based on our pre-
vious experiments on Chinese-English (CE) alignment.
These settings are then used directly without any ad-
justment of the parameters for the French-English (FE)
and Romanian-English (RE) word alignment tasks. In
this section, we will first describe our experiments on
Chinese-English alignment, and then the results for the
shared task on French-English and Romanian-English.
For Chinese-English alignment, 365 sentence-pairs are
randomly sampled from the Chinese Tree-bank provided
by the Linguistic Data Consortium. Three persons man-
ually aligned the word-pairs independently, and the con-
sistent alignments from all of them were used as the ref-
erence alignments. There are totally 4094 word-pairs in
the reference set. Our way of alignment is very similar
to the ?SURE? (S) alignment defined in the shared task.
The training data we used is 16K parallel sentence-pairs
from Hong-Kong news data. The English POS tagger we
used is Brill?s POS tagger [Brill 1994]. The base noun
detector is [Ramshaw 1995]. The alignment is evaluated
in terms of precision, recall, F-measure and alignment er-
ror rate (AER) defined in the shared task. The results are
shown in Table-1:
Table-1. Chinese-English Word-Alignment
CE precision recall F-measure AER
No-Boost 50.88 58.77 54.54 45.46
Boosted 52.19 60.33 55.96 44.04
+POS 54.77 63.34 58.71 41.29
+NP 55.16 63.75 59.14 40.86
Table-1 shows the effectiveness of using each setting
on this small size training data. Here the boosted model
gives a noticeable improvement over the baseline. How-
ever, our observations on the trial/test data showed very
similar results for boosted and non-boosted models, so
we present only the non-boosted results(standard Model-
1) for the shared task of EF and RE word alignment.
Adding POS further improved the performance signif-
icantly. The AER drops from 44.04 to 41.29. Adding
additional base noun phrase boundaries did not give as
much improvement as we hoped. There is only slight
improvement in terms of AER and F-measure. One rea-
son is that noun phrase boundaries is more directly re-
lated to phrase alignment than word-alignment. A close
examination showed that with wrong phrase-alignment,
word-alignment can still be correct. Another reason is
that using the noun phrase boundaries this way may not
be powerful enough to leverage the English structure in-
formation in Bilingual Bracketing. More suitable ways
could be bilingual chunk parsing, and refining the brack-
eting grammar as described in [Wu 1997].
In the shared task experiments, we restricted the train-
ing data to sentences upto 60 words. The statistics for the
training sets are shown in Table-2. (French/Romanian are
source and English is target language).
Table-2.Training Set Statistics
French-English Romanian-English
Sent-pairs 1028382 45456
Src Voc 79601 45880
Tgt Voc 57703 26904
There are 447 test sentence pairs for English-French
and 248 test sentence pairs for Romanian-English. After
the bilingual bracketing, we extracted only the explicit
word alignment from lexical rules: A ? e/f , where nei-
ther e nor f is the null(empty) word. These explicit word
alignments are more directly related to the translation
quality in our SMT system than the null-word alignments.
Also the explicit word alignments is in accordance with
the ?SURE? (S) alignment defined in the shared tasks.
However the Bilingual Bracketing system is not adapted
to the ?PROBABLE? (P) alignment because of the inher-
ent one-to-one mapping. All the AERs in the following
tables are calculated based solely on S alignment without
any null alignments collected from the bracketing results.
Table-3. Limited Resource French-English
FE precision recall F-measure AER
p(f |e) 49.85 79.45 61.26 23.87
p(e|f) 51.46 82.42 63.36 20.95
inter 63.03 74.59 68.32 19.26
Table-4. Unlimited Resource French-English
FE precision recall F-measure AER
p(f |e) 50.21 80.36 61.80 23.07
p(e|f) 51.91 83.26 63.95 19.96
inter 66.34 74.86 70.34 17.77
For the limited resource task, we trained Model-1 lex-
icons in both directions: from source to target denoted as
p(f |e) and from target to source denoted as p(e|f). These
two lexicons are then plugged into the Bilingual Brack-
eting algorithm separately to get two sets of bilingual
bracketing word alignments. The intersection of these
two sets of word alignments is then collected. The result-
ing AERs are shown in Table-3 and Table-5 respectively.
For the unlimited resource task, we again tagged the
English sentences and base noun phrase boundaries as
mentioned before. Then corresponding Model-1 lexicon
was trained and Bilingual Bracketing carried out. Using
the same strategies as in the limited resource task, we got
the results shown in Table-4 and Table-6.
The table above show that adding English POS and
base noun detection gave a consistent improvement for
all conditions in the French-to-English alignment. The
intersection of the two alignments greatly improves the
precision, paired with a reduction in recall, still resulting
in an overall improvement in F-measure and AER.
For the Romanian-English alignment the POS tagging
and noun phrase boundaries did not help. On the small
corpus the increase in vocabulary resulted in addition un-
known words in the test sentences which introduces ad-
ditional alignment errors.
Comparing the results of the French-English and
Romanian-English alignment tasks we see a striking dif-
ference in precision and recall. Whereas the French-
English alignment has a low precision and a high recall
its the opposite for the Romanian-English alignment. The
cause lays in different styles for the manual alignments.
The French-English reference set contains both S and P
alignments, whereas the Romanian-English reference set
was annotated with only S alignments. As a result, there
are on average only 0.5 S alignments per word in the FE
reference set, but 1.5 S alignments per word in the RE
test set.
6 Summary
In this paper we presented our word alignment system
based on bilingual bracketing. We introduced a technique
Table-5. Limited Resource Romanian-English
RE precision recall F-measure AER
p(r|e) 70.65 55.75 62.32 37.66
p(e|r) 71.39 55.00 62.13 37.87
inter 85.48 48.64 62.01 37.99
Table-6. Unlimited Resource Romanian-English
RE precision recall F-measure AER
p(r|e) 69.63 54.65 61.24 38.76
p(e|r) 70.36 55.50 62.05 37.95
inter 82.09 48.73 61.15 38.85
to boost lexical probabilities for more reliable word pairs
in the statistical lexicon. In addition, we investigated the
effects of using POS and noun phrase detection on the
English side of the bilingual corpus as constraints for the
alignment. We applied these techniques to the French-
English and Romanian-English alignment tasks, and in
addition to Chinese-English alignment. For Chinese-
English and French-English alignments these additional
knowledge sources resulted in improvements in align-
ment quality. Best results were obtained by using the
intersection of the source to target and target to source
bilingual bracketing alignments. The results show very
different behavior of the alignment system on the French-
English and Romanian-English tasks which is due to dif-
ferent characteristics of the manually aligned test data.
This indicates that establishing a good golden standard
for word alignment evaluation is still an open issue.
References
Brown, P. F. and Della Pietra, S. A. and Della Pietra, V. J.
and Mercer, R. L. 1993. The Mathematics of Statisti-
cal Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19-2, pp 263-311.
Erik Brill. 1994. Some advances in rule-based part of
speech tagging. Proceedings of the Twelfth National
Conference on Artificial Intelligence (AAAI-94), Seat-
tle, Wa., 1994.
Franz Josef Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models. Proceedings of ACL-00,
pp. 440-447, Hongkong, China.
Lance Ramshaw and Mitchell Marcus 1995. Text Chunk-
ing Using Transformation-Based Learning. Proceed-
ings of the Third ACL Workshop on Very Large Cor-
pora, MIT, June, 1995.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics 23(3):377-404, Sep. 1997.
Efficient Optimization for Bilingual Sentence Alignment  
Based on Linear Regression 
 
Bing Zhao 
 
Language Technologies 
Institute 
Carnegie Mellon University 
bzhao@cs.cmu.edu 
Klaus Zechner 
 
Educational Testing Service 
Rosedale Road, Princeton, 
NJ 08541 
kzechner@ets.org 
Stephan Vogel 
 
Language Technologies 
Institute 
Carnegie Mellon University 
vogel+@cs.cmu.edu 
Alex Waibel 
 
Language Technologies 
Institute 
Carnegie Mellon University 
ahw@cs.cmu.edu 
 
Abstract 
This paper presents a study on optimizing sen-
tence pair alignment scores of a bilingual sen-
tence alignment module. Five candidate 
scores based on perplexity and sentence 
length are introduced and tested. Then a linear 
regression model based on those candidates is 
proposed and trained to predict sentence pairs? 
alignment quality scores solicited from human 
subjects. Experiments are carried out on data 
automatically collected from Internet. The 
correlation between the scores generated by 
the linear regression model and the scores 
from human subjects is in the range of the in-
ter-subject agreement score correlations. Pear-
son's correlation ranges from 0.53 up to 0.72 
in our experiments.  
1 Introduction 
In many instances, multilingual natural language 
systems like machine translation systems are developed 
and trained on parallel corpora.  When faced with a dif-
ferent, unseen text genre, however, translation perform-
ance usually drops noticeably.  One way to remedy this 
situation is to adapt and retrain the system parameters 
based on bilingual data from the same source or at least 
a closely related source.  A bilingual sentence alignment 
program (Gale and Church, 1991, and Brown et al, 
1991) is the crucial part in this adaptation procedure, in 
that it collects bilingual document pairs from the Inter-
net, and identifies sentence pairs, which should have a 
high likelihood of being correct translations of each 
other.  The set of identified bilingual parallel sentence 
pairs is then added to the training set for parameter re-
estimation. 
As is well known, text mined from the Internet is 
very noisy.  Even after careful html parsing and filtering 
for text size and language, the text from comparable 
html-page pairs still contains mismatches of content or 
non-parallel junk text, and the sentence order can be too 
different to be aligned.  Together with a large mismatch 
of vocabulary, the aligned sentence pairs, which are 
extracted from these collected comparable html-page 
pairs, contain a number of low translation quality 
alignments.  These need to be removed before the re-
training of the MT system. 
In this paper, we present an approach to automati-
cally optimizing the alignment scores of such a bilingual 
sentence alignment program.  The alignment score is a 
combination (by linear regression) of two word transla-
tion lexicon scores and three sentence length scores and 
predicts the translation quality scores from a set of hu-
man annotators.  We also present experiments analyzing 
how many different human scorers are needed for good 
prediction and also how many sentence pairs should be 
scored per human annotator. 
The paper is structured as follows: in section 2, the 
text mining system is briefly described.  In section 3, 
five sentence alignment models based on lexical infor-
mation and sentence length are explained. In section 4, a 
regression model is proposed to combine the five mod-
els to get further improvement in predicting alignment 
quality.  We describe alignment experiments in section 
5, focusing on the correlation between the alignment 
scores predicted by the sentence alignment models and 
by humans.  Conclusions are given in section 6. 
2 System of Mining Parallel Text 
One crucial component of statistical machine trans-
lation (SMT) system is the parallel text mining from 
Internet. Several processing modules are applied to col-
lect, extract, convert, and clean the text from Internet.  
The components in our system include: 
? A web crawler, which collects potential parallel 
html documents based on link information follow-
ing (Philip Resnik 1999); 
? A bilingual html parser (based on flex for effi-
ciency), which is designed for both Chinese and 
English html documents.  The paragraphs? bounda-
ries within the html structure are kept.  
? A character encoding detector, which judges if the 
Chinese html document is GB2312 encoding or 
BIG5 encoding.  
? An encoding converter, which converts the BIG5 
documents to GB2312 encoding.  
? A language identifier to ensure that source and tar-
get documents are both of the proper language. 
(Noord?s Implementation).  
? A Chinese word segmenter, which parses the Chi-
nese strings into Chinese words.   
? A document alignment program, which judges if 
the document pair is close translation candidates, 
and filters out those non-translation pairs. 
? A sentence boundary detector, which is based on 
punctuation and capitalized characters; 
? And the key component, a sentence alignment pro-
gram, which aligns and extracts potential parallel 
sentence pairs from the candidate document pairs. 
   
After sentence alignment, each candidate of a par-
allel sentence pair is then re-scored by the regression 
models (to be described in section 5). These scores are 
used to judge the quality of the aligned sentences.  Thus 
one can select the aligned sentence pairs, which have 
high alignment quality scores, to re-estimate the sys-
tem?s parameters.  
2.1 Sentence Alignment 
Our sentence alignment program uses IBM Model-1 
based perplexity (section 2.2) to calculate the similarity 
of each sentence pair. Dynamic programming is applied 
to find Viterbi path for sentence alignments of the bilin-
gual comparable document pair. In our dynamic pro-
gramming implementation, we allow for seven 
alignment types between English and Chinese sentences: 
 
? 1:1 ? exact match, where one sentence is the trans-
lation of the other one; 
? 2:2 ? the break point between two sentences in the 
source document is different from the segmentation 
in the target document.  E.g. part of sentence one in 
the source might be translated as part of the second 
sentence in the target; 
? 2:1, 1:2, and 3:1 ? these cases are similar to the 
case before: they handle differences in how a text is 
split into sentences. The case 1:3 has not been used 
in the final configuration of the system, as this type 
did not occur in any significant number; 
? 1:0 (deletion) and (0:1) insertion ? a sentence in the 
source document is missing in the translation or 
vice versa. 
 
The deletion and insertion types are discarded, and 
the remaining types are extracted to be used as potential 
parallel data. In general, one Chinese sentence corre-
sponds to several English sentences. In (Bing and 
Stephan, 2002), experiments on a 10-year XinHua news 
story collection from the Linguistic Data Consortium 
(LDC) show that alignment types like (2:1) and (3:1) 
are common, and this 7-type alignment is shown to be 
reliable for English-Chinese sentence alignment.  How-
ever, only a small part of the whole 10-year collection 
was pre-aligned (Xiaoyi, 1999) and extracted for sen-
tence alignment.   
The picture can be very different when directly min-
ing the data from Internet. Due to the mismatch between 
the training data and the data collected from Internet, 
the vocabulary coverage can be very low; the data is 
very noisy; and the data aligned is not strictly parallel. 
The percentage of alignment types of insertion (0:1) and 
deletion (1:0) become very high as shown in section 5. 
The aligned sentence pairs are subject to many align-
ment errors. The alignment errors are not desired in the 
re-training of the system, and need to be removed.  
Though the sentence alignment outputs a score from 
Viterbi path for each of the aligned sentence pairs, this 
score is only a rough estimation of the alignment quality. 
A more reliable re-scoring of the data is desirable to 
estimate the alignment quality as a post processing step 
to filter out the errors and noise from the aligned data.  
2.2 Statistical Translation Lexicon 
We use a statistical translation lexicon known as IBM 
Model-1 in (Brown et al, 1993) for both efficiency and 
simplicity.  
In our approach, Model-1 is the conditional probabil-
ity that a word f in the source language is translated 
given word e in the target language, t(f|e). This prob-
ability can be reliably estimated using the expectation-
maximization (EM) algorithm (Cavnar, W. B. and J. M. 
Trenkle, 1994). 
Given training data consisting of parallel sen-
tences: }..1),,{( )()( Sief ii = , our Model-1 training for 
t(f|e) is as follows: 
?
=
?
=
S
s
ss
e efefceft
1
)()(1 ),;|()|( ?  
Where 1?
e? is a normalization factor such that 
0.1)|( =
?
j
j eft  
),;|( )()( ss efefc denotes the expected number of times 
that word e connects to word f.  
??
?
==
=
=
l
i
i
m
j
jl
k
k
ss eeff
eft
eft
efefc
11
1
)()( ),(),(
)|(
)|(),;|( ??  
With the conditional probability t(f|e), the probability 
for an alignment of foreign string F given English string 
E is in (1): 
??
= =
+
=
m
j
n
i
ijm eftlEFP 1 0
)|()1(
1)|(  (1) 
The probability of alignment F given E: )|( EFP is 
shown to achieve the global maximum under this EM 
framework as stated in (Brown et al,1993).  
In our approach, equation (1) is further normalized 
so that the probability for different lengths of F is com-
parable at the word level: 
m
m
j
n
i
ijm eftlEFP
/1
1 0
)|()1(
1)|(
?
?
?
?
?
?
+
= ??
= =
 (2) 
 
The alignment models described in (Brown et al, 
1993) are all based on the notion that an alignment 
aligns each source word to exactly one target word.  
This makes this type of alignment models asymmetric.  
Thus by using the conditional probability t(e|f) trans-
lation lexicon trained from English (source) to Chinese 
(target), different aspects of the bilingual lexical 
information can be captured. A similar probability to (2) 
can be defined based on this reverse translation lexicon: 
n
m
i
n
j
jim fetlFEP
/1
1 0
)|()1(
1)|(
?
?
?
?
?
?
+
= ??
= =
 (3) 
 
Starting from the Hong Kong news corpora provided 
by LDC, we trained the translation lexicons to be used 
in the parallel sentence alignment.  Each sentence pair 
has a perplexity, which is calculated using the minus log 
of the probability eg. equation (2).  
3 Alignment Models 
The alignment model is aimed at automatically pre-
dicting the alignment scores of a bilingual sentence 
alignment program. By scoring the alignment quality of 
the sentence pairs, we can filter out those mis-aligned 
sentence pairs, and save our SMT system from being 
corrupted by mis-aligned data. 
3.1 Lexicon Based Models 
It is necessary to include lexical features in the 
aligned quality evaluation. One way is to use the trans-
lation lexicon based perplexity as in our sentence 
alignment program.  
For each of the aligned sentence pairs, the sentence 
alignment generated a score, which is solely based on 
equation (2). Using this score only, we can do a simple 
filtering by setting a threshold of perplexity. The sen-
tence pairs which have a higher perplexity than the 
threshold will be removed. However the perplexity 
based on (2) is definitely not discriminative enough to 
evaluate the quality of aligned sentence pairs.  
In our experiment, it showed that perplexity (3) has 
more discriminative power in judging the quality of the 
aligned sentence pairs for Chinese-English sentence 
alignment. It is also possible that equation (2) is more 
suitable for other language pairs.  Both (2) and (3) are 
applied in our sentence alignment quality judgment, 
which is to be explained in section 4.  
3.2 Sentence Length Models 
As was shown in the sentence alignment literature 
(Church, K.W. 1993), the sentence length ratio is also a 
very good indication of the alignment of a sentence pair 
for languages from a similar family such as French and 
English.  For language pairs from very different families 
such as Chinese and English, the sentence length ratio is 
also a good indication of alignment quality as shown in 
our experiments.  
For the language pair of Chinese and English, the 
sentence length can be defined in several different ways.  
3.2.1 Sentence Length 
In general, a Chinese sentence does not have word 
boundary information; so one way to define Chinese 
sentence length is to count the number of bytes of the 
sentence. Another way is to first segment the Chinese 
sentence into words (section 3.2.2) and count how many 
words are in the sentence. For English sentences, we 
can similarly define the length in bytes and in words.  
The length ratio is assumed to be a Gaussian distri-
bution. The mean and variance are calculated from the 
parallel training corpus, which, in our case, is the Hong 
Kong parallel corpus with 290K parallel sentence pairs.  
3.2.2 A Chinese Word Segmenter 
The word segmenter for Chinese is to parse the Chi-
nese string into words. Different word segmenters can 
generate different numbers of words for the same Chi-
nese sentence.  
There are many word segmenters publicly available. 
In our experiments, we applied a two-pass strategy to 
segment the word according to the dictionary of the 
LDC bilingual dictionary of Chinese-English. The two-
pass started first from left to right, and then from right 
back to left, to calculate the maximum word frequency 
and select one best path to segment the words.  
In general, the sentence length is not sensitive to the 
segmenters used. But for reliability, we want each seg-
mented word can have an English translation, thus we 
used the LDC bilingual dictionary as a reference word 
list for segmentation.  
3.2.3 Sentence Length Model 
Assume the alignment probability of ),|( tsAP  is 
only related to the length of source sentence s and target 
sentence t: 
|))||,(|(~
|)||(|~
||)||,|||||(|~
),||||(|),|(
tsP
tsP
tstsP
tstsPtsAP
?=
?=
?=
?=
 
where || s and || t are the sentence lengths of s and t.  
The difference of the length |)||,(| ts? is assumed 
to be a Gaussian distribution (Church, K.W. 1993) and 
can be normalized as follows: 
)1,0(~
)1|(|
||||
2
N
s
cst
?
?
+
?
=  (4) 
where c is a constant indicating the mean length ratios 
between source and target sentences and 2? is the vari-
ance of the length ratios.  
In our case, we applied three length models de-
scribed in the following Table 1: 
 
Table 1. Three Length Models description 
L-1 Both English and Chinese sentence are meas-
ured in bytes 
L-2 Both English and Chinese sentence are meas-
ured in words 
L-3 English sentence is measured in words and 
Chinese sentence is measured in bytes 
 
The means and 2? of the length ratios for each of the 
length models are calculated from Hong Kong news 
parallel corpus. The statistics of the three sentence 
length models are shown in Table 2. 
 
Table 2. Sentence length ratio statistics 
  L-1  L-2 L-3: 
Mean 1.59 1.01 0.33 
Var 3.82 0.79 0.71 
 
In general, the smaller the variance, the better the 
sentence length model can be. From Table 2 we observe 
that the bytes based length ratio model has significantly 
larger variance (3.82) than the other two models (L-2: 
0.79, L-3: 0.71).  This means L1 is not as reliable as L2 
and L3. Both L2 and L3 have similar variance, which 
indicates measuring English sentences in words will 
entail smaller variance in length model; measuring Chi-
nese sentences in bytes or words entails only a slight 
difference in variance. This also indicates that the length 
model is not so sensitive to the Chinese word segmenter 
applied. L-1, L-2 and L-3 capture the length relationship 
of parallel sentence in different views. Their modeling 
power has overlap, but they also compensate each other 
in capturing the parallel characteristics of good transla-
tion quality. A combination of these models can poten-
tially bring further improvement, which is shown in our 
experiment in section 6.  
4 Regression Model 
Rather than doing a binary decision (classification) that 
the aligned sentence pair is either good or not, the re-
gression can give a confidence score indicating how 
good the alignment can be, thus offering more flexibil-
ity in decisions.   Predicting the alignment quality using 
the candidate models is considered as a regression prob-
lem in that different scores are combined together.   
There are many ways such as genetic programming, 
to combine the candidate models, and regression is one 
of the straight forward and efficient ones.  So in this 
work, we explored linear regression. 
4.1 Candidate Models 
We have five candidate models described in section 
3. They are: PP1, the perplexity based on the word pair 
conditional probability p(f|e) in equation (2); PP2, the 
perplexity based on the reverse word pair conditional 
probability p(e|f) in equation (3); L-1, Length ratio 
model measured in bytes (mean=1.59, var=3.82); L-2, 
length ratio model measured in words (mean=1.01, 
var=0.79); L-3, length ratio model, where the English 
sentence is measured in words and the Chinese sentence 
is measured in bytes (mean=0.33, var=0.71).  These five 
models capture different aspects of the aligned quality 
of the sentence pair. The idea is to combine these five 
models together to get better prediction of the aligned 
quality. 
Linear regression is applied to combine these five 
models. It is trained from the observation of the five 
models together with the label of human judgment on a 
training set. 
4.2 Regression Model Training 
The linear regression model tries to discover the 
equation for a line that most nearly fits the given data 
(Trevor Hastie et al 2001). That linear equation is then 
used to predict values for the data.  
Now given human subject judgment of the aligned 
translation quality of sentence pairs, we can train a re-
gression model based on the five models we described 
in section 4.1 under the objective of least square errors.  
The human evaluation is measures translation qual-
ity of aligned pairs on a discrete 6-point scale between 1 
(very bad) and 5 (perfect translation). The score 0 was 
used for alignments that were not genuine translation 
e.g., both sentences were from the same language. We 
will use n for the number of total sentence pairs labeled 
by humans and used in training.  
Let A= [PP1, PP2, L-1, L-2, L-3] be the machine-
generated scores for each of the sentence pairs. In our 
case, A is a 5?n  matrix.  
Let H= [Human-Judgment-Score] be the human 
evaluation of the sentence pairs on a 6-point scale. In 
our case, H is a 1?n  matrix. 
In linear regression modeling, a linear transforma-
tion matrix W should satisfy the least square error crite-
rion: 
||}{||min* HAWW
w
?=  (5) 
where W is in fact a 5x1 weight matrix. The equation 
can be solved as:  
HAAAW TT 1* )( ?=  (6) 
The inverse of matrix AAT  is usually calculated using 
singular vector decomposition (SVD). After W is calcu-
lated, the predicted score from the regression model is: 
*
' AWH =  (7) 
where 'H  is the final predicted alignment quality score 
of the regression model. We can also view 'H  as a 
weighted sum of the five models shown in section 4.1. 
The calculation of 'H  reduces to a linear weighted 
summation, which is very efficient to compute.  
5 Experiments 
1500 pairs of comparable html document pairs were 
obtained from bilingual web pages crawled from Inter-
net. After preprocessing, filtering, and sentence align-
ment, the alignment types were distributed as shown in 
Table 3. Ignoring the alignment type of insertion (0:1) 
and deletion (1:0), we extracted around 5941 parallel 
sentences.  
 
Table 3. Alignment types? distribution of mined 
data from noisy web data crawled 
 1:0 0:1 1:1 2:1 1:2 2:2 3:1 
% 23.7 41.9 29.4 1.99 0.01 0.02 2.79 
 
From Table 3, we see the data is very noisy, con-
taining a large portion of insertions (23.7%) and dele-
tions (41.9%).  This is very different from the LDC 
XinHua pre-aligned collection provided by LDC, which 
is relatively clean.  
For this set of English-Chinese bilingual sentences, 
we randomly selected 200 sentence pairs, focusing on 
Viterbi alignment scores below 12.0 from sentence 
alignment, which was an empirically determined 
threshold (The alignment scores here were purely re-
flecting the Model-1 parameters using equation (2)).  
Three human subjects then had to score the 'translation 
quality' of every sentence pair, using a 6 point scale 
described in section 4.2. We further excluded very short 
sentences from consideration and evaluated 168 remain-
ing sentences. 
Pearson R correlation is applied to calculate the mag-
nitude of the association between two variables (human-
human or human-machine in our case) that are on an 
interval or ratio scale. The correlation coefficients 
(Pearson R) between human subjects were in Table 4 
(all are statistically significant): 
 
Table 4. Correlation between Human Subjects 
 H2 H3 
H1 0.786 0.615 
H2 ---- 0.568 
 
Overall, more than 2/3 of the human scores are identical 
or differ by only 1 (between subjects). 
For the automatic score prediction, the five compo-
nent scores described in section 4.1 are used, which are 
then combined using a standard Linear Regression as 
described in section 4.2. Table 5 shows the correlation 
between alignment scores based on Model X and human 
subjects' predicted quality scores: 
 
Table 5. Correlation between optimization models 
and human subjects 
Model human-1 human -2 human -3 
PP-1 .57 .53 .32 
PP-2 .60 .58 .46 
L-1 .42 .41 .30 
L-2 .46 .41 .40 
L-3 .40 .38 .29 
Na?ve .58 .56 .38 
Regression  .72 .68 .53 
 
The data we used in our training of the lexicon is Hong 
Kong news parallel data from LDC. There are 290K 
parallel sentence pairs, with 7 million words of English 
and 7.3 million Chinese words after segmentation. The 
IBM Model-1 for PP-1 and PP-2 are both trained using 
5 EM iterations. The other three length models are also 
calculated from the same 290K sentence pairs. Punctua-
tion is removed before the calculation of all automatic 
score prediction models. 
The regression model here is the standard linear re-
gression using the observations from three human sub-
jects as described in section 4.1. The average 
performance of the regression model is shown in the 
bottom line of the above Table 5. The average correla-
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
Average of 3-human judgement 5-scale score (5-best, 0-non)
Pearson r Correlation
tion varies from 0.53 upto 0.72, which shows that the 
regression model has a very strong positive correlation 
with the human judgment.  
Also from Table 5, we see both lexicon based mod-
els: PP-1 and PP-2 are better than the length models in 
term of correlation with human scorer. Model PP-2 has 
the largest correlation, and is slightly better than PP-1. 
PP-2 is based on the conditional probability of p(e|f), 
which models the generation of an English word from a 
Chinese word. The vocabulary size of Chinese is usu-
ally smaller than English vocabulary size, so this model 
can be more reliably estimated than the reverse direction 
of p(f|e). This explains why PP-2 is slightly better than 
PP-1.  
For sentence length models, we see L-2, for which 
the lengths of both the English sentence and the Chinese 
sentence are measured in words, has the best perform-
ance among the three settings of a sentence length 
model. This indicates that the length model measured in 
words is more reliable.  
Also shown in Table 5, the na?ve interpolation of 
these different models, i.e. just using each model with 
equal weight, resulted in lower correlation than the best 
single alignment model. 
We also performed correlation experiments with 
varied numbers of training sentences from either Hu-
man-1/Human-2/Human-3 or from all of the three hu-
man subjects.  We picked the first 30/60/90/120 labeled 
sentence pairs for training and saved the last 48 sen-
tence pairs for testing.  The average performance of the 
regression model is as follows: 
 
Table 6. Correlation between different training set 
sizes and human scorers. 
Training  
set size 
Human-1 Human -2 Human ?3 
30 .686 .639 .447 
60 .750 .707 .452 
90 .765 .721 .456 
120 .760 .721 .464 
 
The average correlation of the regression models 
showed here increased noticeably when the training set 
was increased from 30 sentence pairs to 90 sentence 
pairs. More sentence pairs caused no or only marginal 
improvements (esp. for the third human subject).  
Figure 1 shows a scatter plot, which illustrates a 
good correlation (here: Pearson R=0.74) between our 
regression model predictors and the human scorers. 
6 Conclusion 
In this paper, we have demonstrated ways to effi-
ciently optimize a sentence alignment module, such that 
it is able to select aligned sentence pairs of high transla-
tion quality automatically. This procedure of alignment 
score optimization requires (a) a small number of hu-
man subjects who annotate a set of about 100 sentence 
pairs each for translation quality; and (b) a set of align-
ment scores, based on perplexity and sentence length 
ratio, to be able to learn to predict the human scores. 
Based on the learned predictions, by means of linear 
regression, the alignment program can choose the best  
sentence pair candidates to be included in the training 
data for the SMT system re-estimation. 
Our experiments showed that, for Chinese-English 
language pair, perplexity based on the reverse word pair 
conditional probability p(e|f) (PP-2) gives the most reli-
able prediction among the five models proposed in this 
paper; the regression model, which combines those five 
models, give the best correlation between human score 
and automatic predictions. Our approach needs only a 
fairly limited number of human labeled sentences pairs, 
and is an efficient optimization of the sentence 
alignment system. 
 
Figure 1. Correlation between regression model and 
human scorers, Pearson R=0.74. 
 
References 
Bing Zhao, Stephan Vogel. 2002. Adaptive Parallel 
Sentences Mining from Web Bilingual News Collec-
tion. IEEE International Conference on Data Mining 
(ICDM 02) , pp. 745-748. Japan. 
Brown, P., Lai, J. C., and Mercer, R. 1991. Aligning 
Sentences in Parallel Corpora. In Proceedings of 
ACL-91, Berkeley CA. 1991 
Cavnar, W. B. and J. M. Trenkle. 1994.  N-Gram-Based 
Text Categorization.  Proceedings of Third Annual 
Symposium on Document Analysis and Information 
Retrieval, Las Vegas, NV, UNLV Publica-
tions/Reprographics, pp. 161-175, 11-13. 
Stanley Chen. 1993. Aligning sentences in Bilingual 
corpora using lexical information. In proceedings of 
the 31st Annual Conference of the Association for 
computational linguistics, pages 9-16, Columbus, 
Ohio, June 1993 
Church, K. W. 1993. Char_align: A Program for Align-
ing Parallel Texts at the Character Level. Proceed-
ings of ACL-93, Columbus OH. 
Gale, W. A. and Church, K. W.  1991. A Program for 
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of ACL-91, Berkeley CA. 1991. 
Melamed, I.D. 1996. A Geometric Approach to Map-
ping Bitext Correspondence. In Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, PA. 1996 
Noord?s Implementation of Textcat: http://odur.let. 
rug.nl/~vannoord/TextCat/index.html 
Peter F. Brown, Stephan A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer.  1993. The 
Mathmatics of Statistical Machine Translation: Pa-
rameter estimation. Computational Linguistics, vol 
19, no.2 , pp.263-311. 
Philip Resnik. 1999. Mining the Web for Bilingual Text. 
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL'99), University of Maryland, 
College Park, Maryland. 
Trevor Hastie, Robert Tibshirani, Jerome Friedman. 
2001. The Elements of Statistical Learning: Data 
Mining, Inference and Prediction. Springer Publisher. 
Xiaoyi Ma, Mark Y. Liberman, ?BITS: A Method for 
Bilingual Text Search over the Web?. Machine 
Translation Summit VII, 1999 
 
$XWRPDWLF([WUDFWLRQRI1DPHG(QWLW\7UDQVOLQJXDO(TXLYDOHQFH%DVHGRQ0XOWL)HDWXUH&RVW0LQLPL]DWLRQ)HL+XDQJ6WHSKDQ9RJHODQG$OH[:DLEHO/DQJXDJH7HFKQRORJLHV,QVWLWXWH&DUQHJLH0HOORQ8QLYHUVLW\3LWWVEXUJK3$^IKXDQJYRJHODKZ`#FVFPXHGX $EVWUDFW7UDQVOLQJXDO HTXLYDOHQFH UHIHUV WR WKH UHODWLRQVKLSEHWZHHQH[SUHVVLRQVRIWKHVDPHPHDQLQJIURPGLIIHUHQWODQJXDJHV ,GHQWLI\LQJ WUDQVOLQJXDO HTXLYDOHQFH RIQDPHG HQWLWLHV 1( FDQ VLJQLILFDQWO\ FRQWULEXWH WRPXOWLOLQJXDO QDWXUDO ODQJXDJH SURFHVVLQJ VXFK DVFURVVOLQJXDO LQIRUPDWLRQ UHWULHYDO FURVVOLQJXDOLQIRUPDWLRQ H[WUDFWLRQ DQG VWDWLVWLFDO PDFKLQHWUDQVODWLRQ ,Q WKLV SDSHU ZH SUHVHQW DQ LQWHJUDWHGDSSURDFKWRH[WUDFW1(WUDQVOLQJXDOHTXLYDOHQFHIURPDSDUDOOHO&KLQHVH(QJOLVKFRUSXV6WDUWLQJ IURP D ELOLQJXDO FRUSXV ZKHUH 1(V DUHDXWRPDWLFDOO\ WDJJHG IRU HDFK ODQJXDJH 1( SDLUV DUHDOLJQHG LQ RUGHU WR PLQLPL]H WKH RYHUDOO PXOWLIHDWXUHDOLJQPHQW FRVW  $Q 1( WUDQVOLWHUDWLRQ PRGHO LVSUHVHQWHG DQG LWHUDWLYHO\ WUDLQHG XVLQJ QDPHG HQWLW\SDLUV H[WUDFWHG IURP D ELOLQJXDO GLFWLRQDU\ 7KHWUDQVOLWHUDWLRQ FRVW FRPELQHG ZLWK WKH QDPHG HQWLW\WDJJLQJFRVWDQGZRUGEDVHGWUDQVODWLRQFRVWFRQVWLWXWHWKH PXOWLIHDWXUH DOLJQPHQW FRVW 7KHVH IHDWXUHV DUHGHULYHG IURP VHYHUDO LQIRUPDWLRQ VRXUFHV XVLQJXQVXSHUYLVHGDQGSDUWO\VXSHUYLVHGPHWKRGV $JUHHG\VHDUFK DOJRULWKP LV DSSOLHG WR PLQLPL]H WKH DOLJQPHQWFRVW ([SHULPHQWV VKRZ WKDW WKH SURSRVHG DSSURDFKH[WUDFWV1(WUDQVOLQJXDOHTXLYDOHQFHZLWK)VFRUHDQGLPSURYHVWKHWUDQVODWLRQVFRUHIURPPhrase Pair Rescoring with Term Weightings for  
Statistical Machine Translation 
Bing Zhao   Stephan Vogel   Alex Waibel 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA, 15213, USA 
{bzhao, vogel+, ahw}@cs.cmu.edu 
 
Abstract 
We propose to score phrase translation 
pairs for statistical machine translation 
using term weight based models.  These 
models employ tf.idf to encode the 
weights of content and non-content words 
in phrase translation pairs.  The transla-
tion probability is then modeled by simi-
larity functions defined in a vector space.  
Two similarity functions are compared.  
Using these models in a statistical ma-
chine translation task shows significant 
improvements. 
1 Introduction 
Words can be classified as content and func-
tional words.  Content words like verbs and 
proper nouns are more informative than function 
words like "to'' and "the''.  In machine translation, 
intuitively, the informative content words should 
be emphasized more for better adequacy of the 
translation quality.  However, the standard statis-
tical translation approach does not take account 
how informative and thereby, how important a 
word is, in its translation model.  One reason is 
the difficulty to measure how informative a word 
is. Another problem is to integrate it naturally 
into the existing statistical machine translation 
framework, which typically is built on word 
alignment models, like the well-known IBM 
alignment models (Brown et al1993).  
In recent years there has been a strong ten-
dency to incorporate phrasal translation into sta-
tistical machine translation.  It directly translates 
an n-gram from the source language into an m-
gram in the target language.  The advantages are 
obvious:  It has built-in local context modeling, 
and provides reliable local word reordering.  It 
has multi-word translations, and models a word?s 
conditional fertility given a local context.  It cap-
tures idiomatic phrase translations and can be 
easily enriched with bilingual dictionaries. In 
addition, it can compensate for the segmentation 
errors made during preprocessing, i.e. word seg-
mentation errors of Chinese.  The advantage of 
using phrase-based translation in a statistical 
framework has been shown in many studies such 
as (Koehn et al 2003; Vogel et al 2003; Zens et 
al. 2002; Marcu and Wong, 2002).  However, the 
phrase translation pairs are typically extracted 
from a parallel corpus based on the Viterbi align-
ment of some word alignment models.  The leads 
to the question what probability should be as-
signed to those phrase translations.  Different 
approaches have been suggested as using relative 
frequencies (Zens et al 2002), calculate prob-
abilities based on a statistical word-to-word dic-
tionary (Vogel et al 2003) or use a linear 
interpolation of these scores (Koehn et al 2003).  
In this paper we investigate a different ap-
proach with takes the information content of 
words better into account.  Term weighting based 
vector models are proposed to encode the transla-
tion quality.  The advantage is that term weights, 
such as tf.idf, are useful to model the informa-
tiveness of words.  Highly informative content 
words usually have high tf.idf scores.  In informa-
tion retrieval this has been successfully applied to 
capture the relevance of a document to a query, 
by representing both query and documents as 
term weight vectors and use for example the 
cosine distance to calculate the similarity be-
tween query vector and document vector.  The 
idea now is to consider the source phrase as a 
?query?, and the different target phrases ex-
tracted from the bilingual corpus as translation 
candidates as a relevant ?documents?.  The co-
sine distance is then a natural choice to model the 
translation probability.  
Our approach is to apply term weighting 
schemes to transform source and target phrases 
into term vectors.  Usually content words in both 
source and target languages will be emphasized 
by large term weights.  Thus, good phrase trans-
lation pairs will share similar contours, or, to ex-
press it in a different way, will be close to each 
other in the term weight vector space.  A similar-
ity function is then defined to approximate trans-
lation probability in the vector space. 
The paper is structured as follows:  in Section 
2, our phrase-based statistical machine translation 
system is introduced; in Section 3, a phrase trans-
lation score function based on word translation 
probabilities is explained, as this will be used as a 
baseline system;  in Section 4, a vector model 
based on tf.idf is proposed together with two 
similarity functions;  in Section 5, length regu-
larization and smoothing schemes are explained 
briefly;  in Section 6, the translation experiments 
are presented; and Section 7 concludes with a 
discussion.  
2 Phrase-based Machine Translation 
In this section, the phrase-based machine transla-
tion system used in the experiments is briefly 
described: the phrase based translation models 
and the decoding algorithm, which allows for 
local word reordering.  
2.1 Translation Model 
The phrase-based statistical translation systems 
use not only word-to-word translation, extracted 
from bilingual data, but also phrase-to phrase 
translations. . Different types of extraction ap-
proaches have been described in the literature: 
syntax-based, word-alignment-based, and genu-
ine phrase alignment models.  The syntax-based 
approach has the advantage to model the gram-
mar structures using models of more or less 
structural richness, such as the syntax-based 
alignment model in (Yamada and Knight, 2001) 
or the Bilingual Bracketing in (Wu, 1997).  Popu-
lar word-alignment-based approaches usually 
rely on initial word alignments from the IBM and 
HMM alignment models (Och and Ney, 2000), 
from which the phrase pairs are then extracted.  
(Marcu and Wong 2002) and (Zhang et al 2003) 
do not rely on word alignment but model directly 
the phrase alignment. 
Because all statistical machine translation sys-
tems search for a globally optimal translation 
using the language and translation model, a trans-
lation probability has to be assigned to each 
phrase translation pair.  This score should be 
meaningful in that better translations have a 
higher probability assigned to them, and balanced 
with respect to word translations.  Bad phrase 
translations should not win over better word for 
word translations, only because they are phrases. 
Our focus here is not phrase extraction, but 
how to estimate a reasonable probability (or 
score) to better represent the translation quality 
of the extracted phrase pairs.  One major problem 
is that most phrase pairs are seen only several 
times, even in a very large corpus.  A reliable and 
effective estimation approach is explained in sec-
tion 3, and the proposed models are introduced in 
section 4.   
In our system, a collection of phrase transla-
tions is called a transducer.  Different phrase ex-
traction methods result in different transducers.  
A manual dictionary can be added to the system 
as just another transducer.  Typically, one source 
phrase is aligned with several candidate target 
phrases, with a score attached to each candidate 
representing the translation quality.  
2.2 Decoding Algorithm 
Given a set of transducers as the translation 
model (i.e. phrase translation pairs together with 
the scores of their translation quality), decoding 
is divided into several steps. 
The first step is to build a lattice by applying 
the transducers to the input source sentence.  We 
start from a lattice, which has as its only path the 
source sentence.  Then for each word or sequence 
of words in the source sentence for which we 
have an entry in the transducer new edges are 
generated and inserted into the lattice, spanning 
over the source phrase.  One new edge is created 
for each translation candidate, and the translation 
score is assigned to this edge.  The resulting lat-
tice has then all the information available from 
the translation model. 
The second step is search for a best path 
through this lattice, but not only based on the 
translation model scores but applying also the 
language model.  We start with an initial special 
sentence begin hypothesis at the first node in the 
lattice.  Hypotheses are then expanded over the 
edges, applying the language model to the partial 
translations attached to the edges.  The following 
algorithm summarizes the decoding process 
when not considering word reordering:  
 
Current node n, previous node n?; edge e 
Language model state L, L? 
Hypothesis h, h? 
Foreach node n in the lattice 
  Foreach incoming edge e in n 
      phrase = word sequence at e 
      n?     = FromNode(e) 
      foreach L in n? 
         foreach h with LMstate L 
           LMcost = 0.0 
           foreach word w in phrase 
             LMcost += -log p(w|L) 
             L? = NewState(L,w) 
             L  = L? 
           end 
           Cost= LMcost+TMcost(e) 
           TotalCost=TotalCost(h)+Cost 
          h? = (L,e,h,TotalCost) 
          store h?in Hypotheses(n,L) 
 
The updated hypothesis h? at the current node 
stores the pointer to the previous hypothesis and 
the edge (labeled with the target phrase) over 
which it was expanded.  Thus, at the final step, 
one can trace back to get the path associated with 
the minimum cost, i.e. the best hypothesis. 
Other operators such as local word reordering 
are incorporated into this dynamic programming 
search (Vogel, 2003). 
3 Phrase Pair Translation Probability  
As stated in the previous section, one of the 
major problems is how to assign a reasonable 
probability for the extracted phrase pair to repre-
sent the translation quality. 
Most of the phrase pairs are seen only once or 
twice in the training data.  This is especially true 
for longer phrases.  Therefore, phrase pair co-
occurrence counts collected from the training 
corpus are not reliable and have little discrimina-
tive power.  In (Vogel et al 2003) a different es-
timation approach was proposed.  Similar as in 
the IBM models, it is assumed that each source 
word si in the source phrase ),,( 21 Issss L
v =  is 
aligned to every target word tj in the target phrase 
),,( 21 Jtttt L
v =  with probability )|Pr( ij st .  The 
total phrase translation probability is then calcu-
lated according to the following generative 
model:  
? ?
= =
=
I
i
J
j
ji tsts
1 1
))|Pr(()|Pr(
vv  (1) 
 
This is essentially the lexical probability as 
calculated in the IBM1 alignment model, without 
considering position alignment probabilities.  
Any statistical translation can be used in (1) to 
calculate the phrase translation probability.  
However, in our experiment we typically see now 
significant difference in translation results when 
using lexicons trained from different alignment 
models. 
Also Equation (1) was confirmed to be robust 
and effective in parallel sentence mining from a 
very large and noisy comparable corpus (Zhao 
and Vogel, 2002).  
Equation (1) does not explicitly discriminate 
content words from non-content words.  As non-
content words such as high frequency functional 
words tend to occur in nearly every parallel sen-
tence pair, they co-occur with most of the source 
words in the vocabulary with non-trivial transla-
tion probabilities.  This noise propagates via (1) 
into the phrase translations probabilities, increas-
ing the chance that non-optimal phrase transla-
tion candidates get high probabilities and better 
translations are often not in the top ranks.  
We propose a vector model to better distin-
guish between content words and non-content 
words with the goal to emphasize content words 
in the translation.  This model will be used to 
rescore the phrase translation pairs, and to get a 
normalized score representing the translation 
probability.  
4 Vector Model for Phrase Translation 
Probability 
Term weighting models such as tf.idf are ap-
plied successfully in information retrieval.  The 
duality of term frequency (tf) and inverse docu-
ment frequency (idf), document space and collec-
tion space respectively, can smoothly predict the 
probability of terms being informative (Roelleke, 
2003).  Naturally, tf.idf is suitable to model con-
tent words as these words in general have large 
tf.idf weights. 
4.1 Phrase Pair as Bag-of-Words 
Our translation model: (transducer, as defined 
in 2.1), is a collection of phrase translation pairs 
together with scores representing the translation 
quality.  Each phrase translation pair, which can 
be represented as a triple },{ pts vv? , is now con-
verted into a ?Bag-of-Words? D consisting of a 
collection of both source and target words ap-
pearing in the phrase pair, as shown in (2):  
},,,,,{},{ 2121 JI tttsssDpts LL
vv =??  (2) 
 
Given each phrase pair as one document, the 
whole transducer is a collection of such docu-
ments.  We can calculate tf.idf for each is  and jt , 
and represent source and target phrases by vec-
tors of sv
v  and tv
v   as in Equation (3):  
},,,{
21 Issss
wwwv Lv =  
},,,{
21 Jtttt
wwwv Lv =  (3) 
where
is
w and
jt
w are tf.idf for is or jt respectively.  
This vector representation can be justified by 
word co-occurrence considerations.  As the 
phrase translation pairs are extracted from paral-
lel sentences, the source words is  and target 
words jt  in the source and target phrases must 
co-occur in the training data.  The co-occurring 
words should share similar term frequency and 
document frequency statistics.  Therefore, the 
vectors  sv
v and tv
v  have similar term weight con-
tours corresponding to the co-occurring word 
pairs.  So the vector representations of a phrase 
translation pair can reflect the translation quality.  
In addition, the content words and non-content 
words are modeled explicitly by using term 
weights.  An over-simplified example would be 
that a rare word in the source language usually 
translates into a rare word in the target language. 
4.2 Term Weighting Schemes 
Given the transducer, it is straightforward to 
calculate term weights for source and target 
words.  There are several versions of tf.idf.  The 
smooth ones are preferred, because phrase trans-
lation pairs are rare events collected from train-
ing data.  
The idf model selected is as in Equation (4): 
)
5.0
5.0log( +
+?=
df
dfNidf  (4)
where N is the total number of documents in the 
transducer, i.e. the total number of translation 
pairs, and df is the document frequency, i.e. in 
how many phrase pairs a given word occurs.  The 
constant of 0.5 acts as smoothing. 
Because most of the phrases are short, such as 
2 to 8 words, the term frequency in the bag of 
words representation is usually 1, and some times 
2.  This, in general, does not bring much dis-
crimination in representing translation quality.  
The following version of tf is chosen, so that 
longer target phrases with more words than aver-
age will be slightly down-weighted: 
)(/)(5.15.0
'
vavglenvlentf
tftf vv?++=  (5)
where tf is the term frequency, )(vlen v  is the 
length in words of the phrase vv , and )(vavglen v  is 
the average length of source or target phrase cal-
culated from the transducer.  Again, the values of 
0.5 and 1.5 are constants used in IR tasks acting 
as smoothing.  
Thus after a transducer is extracted from a par-
allel corpus, tf and df are counted from the collec-
tion of the ?bag-of-words'' phrase alignment 
representations.  For each word in the phrase pair 
translation its tf.idf weight is assigned and the 
source and target phrase are transformed into 
vectors as shown in Equation (3).  These vectors 
reserve the translation quality information and 
also model the content and non-content words by 
the term weighting model of tf.idf. 
4.3 Vector Space Alignment 
Given the vector representations in Equation 
(3), a similarity between the two vectors can not 
directly be calculated.  The dimensions I and J 
are not guaranteed to be the same.  The goal is to 
transform the source vector into a vector having 
the same dimensions as the target vector, i.e. to 
map the source vector into the space of the target 
vector, so that a similarity distance can be calcu-
lated.  Using the same reasoning as used to moti-
vate Equation (1), it is assumed that every source 
word is  contributes some probability mass to 
each target word jt .  That is to say, given a term 
weight for jt , all source term weights are aligned 
to it with some probability.  So we can calculate 
a transformed vector from the source vectors by 
calculating weights jtaw  using a translation lexi-
con )|Pr( st  as in Equation (6): 
?
=
?=
I
i
sij
t
a i
j wstw
1
)|Pr(  (6) 
 
Now the target vector and the mapped vector 
av
v  have the same dimensions as shown in (7):  
},,,{ 21 Jta
t
a
t
aa wwwv L
v =  
},,,{
21 Jtttt
wwwv Lv =  (7) 
 
4.4 Similarity Functions 
As explained in section 4.1, intuitively, if sv  
and t
v  is a good translation pair, then the corre-
sponding vectors of av
v  and tv
v  should be similar 
to each other in the vector space.   
Cosine distance 
The standard cosine distance is defined as the 
inner product of the two vectors av
v  and tv
v  nor-
malized by their norms.  Based on Equation (6), 
it is easy to derive the similarity as follows:  
)()(
)|(
)|(1
1),(),(
1
2
1
2
1 1
1 1
1
cos
??
? ?
? ?
?
==
= =
= =
=
=
=
==
J
j
t
a
J
j
t
J
j
I
i
sijt
J
j
I
i
sijt
t
t
a
J
j
t
t
a
t
t
at
t
a
t
t
a
t
t
a
j
j
ij
ij
j
j
wsqrtwsqrt
wstPw
wstPw
vv
ww
vvvv
vvvvd
 
(8)
where I and J are the length of the source and 
target phrases; 
is
w  and 
jt
w  are term weights for 
source word and target words;  jtaw  is the trans-
formed weight mapped from all source words to 
the target dimension at word jt .   
BM25 distance 
TREC tests show that bm25 (Robertson and 
Walker, 1997) is one of the best-known distance 
schemes.  This distance metric is given in Equa-
tion (9). The constants of 31 ,, kbk are set to be 1, 1 
and 1000 respectively.  
)(
)1(
)(
)1(
3
3
1
1
25 j
j
j
j
t
a
t
a
J
j t
t
bm wk
wk
wK
wk
wd +
+
+
+=?
=
 
)5.0/()5.0( ++?==
jjj ttt
dfdfNidfw  
))(/)1((1 lavgJbkK +?=  
(9)
where avg(l) is the average target phrase length 
in words given the same source phrase. 
Our experiments confirmed the bm25 distance 
is slightly better than the cosine distance, though 
the difference is not really significant.  One ad-
vantage of bm25 distance is that the set of free 
parameters 31 ,, kbk can be tuned to get better per-
formance e.g. via n-fold cross validation.  
4.5 Integrated Translation Score 
Our goal is to rescore the phrase translation 
pairs by using additional evidence of the transla-
tion quality in the vector space.   
The vector based scores (8) & (9) provide a 
distinct view of the translation quality in the vec-
tor space.  Equation (1) provides a evidence of 
the translation quality based on the word align-
ment probability, and can be assumed to be dif-
ferent from the evidences in vector space.  Thus, 
a natural way of integrating them together is a 
geometric interpolation shown in (10) or equiva-
lently a linear interpolation in the log domain.  
)|(Pr),( 1int tsstdd vec
vvvv ?? ??=  (10)
where ),( stdvec
vv is the score from the cosine or 
bm25 vector distance, normalized within [0, 1], 
like a probability. 
0.1),( =?
t
vec stdv
vv   
The parameter ? can be tuned using held-out 
data.  In our cross validation experiments 5.0=?  
gave the best performance in most cases.  There-
fore, Equation (10) can be simplified into: 
)|Pr(),(int tsstdd vec
vvvv ?=  (11)
 
The phrase translation score functions in (1) 
and (11) are non-symmetric.  This is because the 
statistical lexicon Pr(s|t) is non-symmetric.  One 
can easily re-write all the distances by using 
Pr(t|s).  But in our experiments this reverse di-
rection of using Pr(t|s) gives trivially difference.  
So in all the experimental results reported in this 
paper, the distances defined in (1) and (11) are 
used. 
5 Length Regularization  
Phrase pair extraction does not work perfectly 
and sometimes a short source phrase is aligned to 
a long target phrase or vice versa.  Length regu-
larization can be applied to penalize too long or 
too short candidate translations.  Similar to the 
sentence alignment work in (Gale and Church, 
1991), the phrase length ratio is assumed to be a 
Gaussian distribution as given in Equation (12):  
)))(/)((5.0exp(),( 2
2
?
????? sltlstl
vvvv  (12)
where l(t) is the target sentence  length.  Mean ?  
and variance ?  can be estimated using a parallel 
corpus using a Maximum Likelihood criteria. 
The regularized score is the product of (11) and 
(12).  
6 Experiments  
Experiments were carried out on the so-called 
large data track Chinese-English TIDES transla-
tion task, using the June 2002 test data.  The 
training data used to train the statistical lexicon 
and to extract the phrase translation pairs was 
selected from a 120 million word parallel corpus 
in such a way as to cover the phrases in test sen-
tences.  The restricted training corpus contained 
then approximately 10 million words..  A trigram 
model was built on 20 million words of general 
newswire text, using the SRILM toolkit (Stolcke, 
2002).  Decoding was carried out as described in 
section 2.2.  The test data consists of 878 Chinese 
sentences or 24,337 words after word segmenta-
tion.  There are four human translations per Chi-
nese sentence as references.  Both NIST score 
and Bleu score (in percentage) are reported for 
adequacy and fluency aspects of the translation 
quality. 
6.1 Transducers 
Four transducers were used in our experi-
ments: LDC, BiBr, HMM, and ISA.  
LDC was built from the LDC Chinese-English 
dictionary in two steps: first, morphological 
variations are created.  For nouns and noun 
phrases plural forms and entries with definite and 
indefinite determiners were generated.  For verbs 
additional word forms with -s -ed and -ing were 
generated, and the infinitive form with 'to'.  Sec-
ond, a large monolingual English corpus was 
used to filter out the new word forms.  If they did 
not appear in the corpus, the new entries were not 
added to the transducer (Vogel, 2004). 
BiBr extracts sub-tree mappings from Bilin-
gual Bracketing alignments (Wu, 1997);  HMM 
extracts partial path mappings from the Viterbi 
path in the Hidden Markov Model alignments 
(Vogel et. al., 1996).  ISA is an integrated seg-
mentation and alignment for phrases (Zhang et.al, 
2003), which is an extension of (Marcu and 
Wong, 2002).  
 LDC BiBr HMM ISA 
)(KN  425K 137K 349K 263K 
)/( srctgt llavg  1.80 1.11 1.09 1.20 
Table-1 statistics of transducers 
 
Table-1 shows some statistics of the four 
transducers extracted for the translation task. N  
is the total number of phrase pairs in the trans-
ducer.  LDC is the largest one having 425K en-
tries, as the other transducers are restricted to 
?useful? entries, i.e. those translation pairs where 
the source phrase matches a sequence of words in 
one of the test sentence.  Notice that the LDC 
dictionary has a large number of long transla-
tions, leading to a high source to target length 
ratio. 
6.2 Cosine vs BM25 
The normalized cosine and bm25 distances de-
fined in (8) and (9) respectively, are plugged into 
(11) to calculate the translation probabilities.  
Initial experiments are reported on the LDC 
transducer, which gives already a good transla-
tion, and therefore allows for fast and yet mean-
ingful experimentation.  
Four baselines (Uniform, Base-m1, Base-m4, 
and Base-m4S) are presented in Table-2.   
 
NIST Bleu 
Uniform 6.69 13.82 
Base-m1 7.08 14.84 
Base-m4 7.04 14.91 
Base-m4S 6.91 14.44 
cosine 7.17 15.30 
bm25 7.19 15.51 
bm25-len 7.21 15.64 
Table-2 Comparisons of different score functions 
 
In the first uniform probabilities are assigned 
to each phrase pair in the transducer.  The second 
one (Base-m1) is using Equation (1) with a statis-
tical lexicon trained using IBM Model-1, and 
Base-m4 is using the lexicon from IBM Model-4.  
Base-m4S is using IBM Model-4, but we skipped 
194 high frequency English stop words in the 
calculation of Equation (1). 
Table-2 shows that the translation score de-
fined by Equation (1) is much better than a uni-
form model, as expected.  Base-m4 is slightly 
worse than Base-m1.on NIST score, but slightly 
better using the Bleu metric.  Both differences 
are not statistically significant.  The result for 
Base-m4S shows that skipping English stop 
words in Equation (1) gives a disadvantage.  One 
reason is that skipping ignores too much non-
trivial statistics from parallel corpus especially 
for short phrases.  These high frequency words 
actually account already for more than 40% of 
the tokens in the corpus.  
Using the vector model, both with the cosine 
cosd  and the bm25 25bmd  distance, is significantly 
better than Base-m1 and Base-m4 models, which 
confirms our intuition of the vector model as an 
additional useful evidence for translation quality. 
The length regularization (12) helps only slightly 
for LDC.  Since bm25?s parameters could be 
tuned for potentially better performance, we se-
lected bm25 with length regularization as the 
model tested in further experiments.  
A full-loaded system is tested using the 
LM020 with and without word-reordering in de-
coding.  The results are presented in Table-3.  
Table-3 shows consistent improvements on all 
configurations: the individual transducers, com-
binations of transducers, and different decoder 
settings of word-reordering. Because each phrase 
pair is treated as a ?bag-of-words?, the grammar 
structure is not well represented in the vector 
model.  Thus our model is more tuned towards 
the adequacy aspect, corresponding to NIST 
score improvement. 
Because the transducers of BiBr, HMM, and 
ISA are extracted from the same training data, 
they have significant overlaps with each other.  
This is why we observe only small improvements 
when adding more transducers.   
The final NIST score of the full system is 8.24, 
and the Bleu score is 22.37.  This corresponds to 
3.1% and 11.8% relative improvements over the 
baseline.  These improvements are statistically 
significant according to a previous study (Zhang 
et.al., 2004), which shows that a 2% improve-
ment in NIST score and a 5% improvement in 
Bleu score is significant for our translation sys-
tem on the June 2002 test data. 
6.3 Mean Reciprocal Rank 
To further investigate the effects of the rescor-
ing function in (11), Mean Reciprocal Rank 
(MRR) experiments were carried out.  MRR for a 
labeled set is the mean of the reciprocal rank of 
the individual phrase pair, at which the best can-
didate translation is found (Kantor and Voorhees, 
1996).  
Totally 9,641 phrase pairs were selected con-
taining 216 distinct source phrases.  Each source 
phrase was labeled with its best translation can-
didate without ambiguity.  The rank of the la-
beled candidate is calculated according to 
translation scores. The results are shown in Ta-
ble-4. 
 baseline cosine bm25 
MRR 0.40 0.58 0.75 
Table-4 Mean Reciprocal Rank 
 
The rescore functions improve the MRR from 
0.40 to 0.58 using cosine distance, and to 0.75 
using bm25.  This confirms our intuitions that 
good translation candidates move up in the rank 
after the rescoring.  
Decoder settings without word reordering with word reordering 
baseline bm25 baseline bm25 Scores (%) NIST Bleu NIST Bleu NIST Bleu NIST Bleu 
LDC 7.08 14.84 7.21 15.64 7.13 15.10 7.26 15.98 
LDC+ISA 7.73 19.60 7.99 19.58 7.86 20.80 8.13 20.93 
LDC+ISA+HMM 7.86 19.08 8.14 20.70 7.95 19.84 8.19 21.60 
LDC+ISA+HMM+BiBr 7.87 19.23 8.14 21.48 7.99 20.01 8.24 22.37 
Table-3 Translation using bm25 rescore function with different decoder settings 
 
7 Conclusion and Discussion  
In this work, we proposed a way of using term 
weight based models in a vector space as addi-
tional evidences for translation quality, and inte-
grated the model into an existing phrase-based 
statistical machine translation system.  The mod-
el shows significant improvements when using it 
to score a manual dictionary as well as when us-
ing different phrase transducers or a combination 
of all available translation information.  Addi-
tional experiments also confirmed the effective-
ness of the proposed model in terms of of 
improved Mean Reciprocal Rank of good transla-
tions. 
Our future work is to explore alternatives such 
as the reranking work in (Collins, 2002) and in-
clude more knowledge such as syntax informa-
tion in rescoring the phrase translation pairs.  
References 
A. Stolcke. 2002. SRILM -- An Extensible Language 
Modeling Toolkit. In 2002 Proc. Intl. Conf. on 
Spoken Language Processing, Denver. 
Peter F. Brown, Stephen A. Della Pietra, Vincent 
J.Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation, Computational Linguistics, 
vol. 19, no. 2, pp. 263?311. 
Michael Collins. 2000. Discriminative Reranking for 
Natural Language Parsing. Proc. 17th International 
Conf. on Machine Learning. pp. 175-182. 
William A. Gale and Kenneth W. Church. 1991. A 
Program for Aligning Sentences in Bilingual Cor-
pora.  In Computational Linguistics, vol.19 pp. 75-
102.  
Paul B. Kantor, Ellen Voorhees. 1996. Report on the 
TREC-5 Confusion Track. The Fifth Text Retrieval 
Conference. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. Pro-
ceedings of HLT-NAACL. Edmonton, Canada.  
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. Proceedings of EMNLP-2002, 
Philadelphia, PA. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. Proceedings of ACL-
00, pp. 440-447, Hongkong, China. 
Thomas Roelleke. 2003. A Frequency-based and a 
Poisson-based Definition of the Probability of Be-
ing Informative. Proceedings of the 26th annual in-
ternational ACM SIGIR. pp. 227-234.  
S.E. Robertson, and S. Walker. 1997. On relevance 
weights with little relevance information. In 1997 
Proceedings of ACM SIGIR. pp. 16-24. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora, 
Computational Linguistics 23(3): pp.377-404.  
K. Yamada and K. Knight. 2001. A Syntax-Based 
Statistical Translation Model. Proceedings of the 
39th Annual Meeting of the Association for Compu-
tational Linguistics. pp. 523-529. Toulouse, France.  
Richard Zens, Franz Josef Och and Hermann Ney. 
2002. Phrase-Based Statistical Machine Transla-
tion. Proceedings of the 25th Annual German Con-
ference on AI: Advances in Artificial Intelligence. 
pp. 18-22. 
Stephan Vogel, Hermann Ney, Christian Tillmann. 
1996. HMM-based word alignment in statistical 
translation. In: COLING '96: The 16th Int. Conf. on 
Computational Linguistics, Copenhagen, Denmark 
(1996) pp. 836-841. 
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venogupal, Bing Zhao, Alex Waibel. 
2003. The CMU Statistical Translation System, 
Proceedings of MT-Summit IX. New Orleans, LA. 
Stephan Vogel. 2003. SMT decoder dissected: word 
reordering, In 2003 Proceedings of Natural Lan-
guage Processing and Knowledge Engineering, 
(NLP-KE'03) Beijing, China.  
Stephan Vogel. 2004. Augmenting Manual Dictionar-
ies for Statistical Machine Translation Systems, In 
2003 Proceedings of LREC, Lisbon, Portugal.  pp. 
1593-1596. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST Scores : How much Im-
provement Do We Need to Have a Better System? 
In 2004 Proceedings of LREC, Lisbon, Portugal. 
pp. 2051-2054. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2003. "In-
tegrated Phrase Segmentation and Alignment Algo-
rithm for Statistical Machine Translation," in the 
Proceedings of NLP-KE'03, Beijing, China. 
Bing Zhao, Stephan Vogel. 2002. Adaptative Parallel 
Sentences Mining from web bilingual news collec-
tion. In 2002 IEEE International Conference on 
Data Mining, Maebashi City, Japan.  
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 141?144,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
A Generalized Alignment-Free Phrase Extraction
Bing Zhao
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA-15213
bzhao@cs.cmu.edu
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA-15213
vogel+@cs.cmu.edu
Abstract
In this paper, we present a phrase ex-
traction algorithm using a translation lex-
icon, a fertility model, and a simple dis-
tortion model. Except these models, we
do not need explicit word alignments for
phrase extraction. For each phrase pair (a
block), a bilingual lexicon based score is
computed to estimate the translation qual-
ity between the source and target phrase
pairs; a fertility score is computed to es-
timate how good the lengths are matched
between phrase pairs; a center distortion
score is computed to estimate the relative
position divergence between the phrase
pairs. We presented the results and our
experience in the shared tasks on French-
English.
1 Introduction
Phrase extraction becomes a key component in to-
day?s state-of-the-art statistical machine translation
systems. With a longer context than unigram, phrase
translation models have flexibilities of modelling lo-
cal word-reordering, and are less sensitive to the er-
rors made from preprocessing steps including word
segmentations and tokenization. However, most of
the phrase extraction algorithms rely on good word
alignments. A widely practiced approach explained
in details in (Koehn, 2004), (Och and Ney, 2003)
and (Tillmann, 2003) is to get word alignments from
two directions: source to target and target to source;
the intersection or union operation is applied to get
refined word alignment with pre-designed heuristics
fixing the unaligned words. With this refined word
alignment, the phrase extraction for a given source
phrase is essentially to extract the target candidate
phrases in the target sentence by searching the left
and right projected boundaries.
In (Vogel et al, 2004), they treat phrase align-
ment as a sentence splitting problem: given a source
phrase, find the boundaries of the target phrase such
that the overall sentence alignment lexicon probabil-
ity is optimal. We generalize it in various ways, esp.
by using a fertility model to get a better estimation of
phrase lengths, and a phrase level distortion model.
In our proposed algorithm, we do not need ex-
plicit word alignment for phrase extraction. Thereby
it avoids the burden of testing and comparing differ-
ent heuristics especially for some language specific
ones. On the other hand, the algorithm has such flex-
ibilities that one can incorporate word alignment and
heuristics in several possible stages within this pro-
posed framework to further improve the quality of
phrase pairs. In this way, our proposed algorithm
is more generalized than the usual word alignment
based phrase extraction algorithms.
The paper is structured as follows: in section 2,
The concept of blocks is explained; in section 3, a
dynamic programming approach is model the width
of the block; in section 4, a simple center distortion
of the block; in section 5, the lexicon model; the
complete algorithm is in section 6; in section 7, our
experience and results using the proposed approach.
2 Blocks
We consider each phrase pair as a block within a
given parallel sentence pair, as shown in Figure 1.
The y-axis is the source sentence, indexed word
by word from bottom to top; the x-axis is the target
sentence, indexed word by word from left to right.
The block is defined by the source phrase and its pro-
jection. The source phrase is bounded by the start
and the end positions in the source sentence. The
projection of the source phrase is defined as the left
and right boundaries in the target sentence. Usually,
the boundaries can be inferred according to word
alignment as the left most and right most aligned
positions from the words in the source phrase. In
141
Start
End
Right boundaryLeft boundary
Width
src center
tgt center
Figure 1: Blocks with ?width? and ?centers?
this paper, we provide another view of the block,
which is defined by the centers of source and target
phrases, and the width of the target phrase.
Phrase extraction algorithms in general search
for the left and right projected boundaries of each
source phrase according to some score metric com-
puted for the given parallel sentence pairs. We
present here three models: a phrase level fertility
model score for phrase pairs? length mismatch, a
simple center-based distortion model score for the
divergence of phrase pairs? relative positions, and
a phrase level translation score to approximate the
phrase pairs? translational equivalence. Given a
source phrase, we can search for the best possible
block with the highest combined scores from the
three models.
3 Length Model: Dynamic Programming
Given the word fertility definitions in IBM Mod-
els (Brown et al, 1993), we can compute a prob-
ability to predict phrase length: given the candi-
date target phrase (English) eI1, and a source phrase
(French) of length J , the model gives the estima-
tion of P (J |eI1) via a dynamic programming algo-
rithm using the source word fertilities. Figure 2
shows an example fertility trellis of an English tri-
gram. Each edge between two nodes represents one
English word ei. The arc between two nodes rep-
resents one candidate non-zero fertility for ei. The
fertility of zero (i.e. generating a NULL word) cor-
responds to the direct edge between two nodes, and
in this way, the NULL word is naturally incorpo-
rated into this model?s representation. Each arc is
e1 e2 e3
1
3
2
0 0
2
0
e1 e2 e3
??
?.
1
2
3
4
3
1
3
1
2
Figure 2: An example of fertility trellis for dynamic
programming
associated with a English word fertility probability
P (?i|ei). A path ?I1 through the trellis represents
the number of French words ?i generated by each
English word ei. Thus, the probability of generating
J words from the English phrase along the Viterbi
path is:
P (J |eI1) = max
{?I1,J=
?I
i=1 ?i}
I
?
i=1
P (?i|ei) (1)
The Viterbi path is inferred via dynamic program-
ming in the trellis of the lower panel in Figure 2:
?[j, i] = max
?
?
?
?
?
?
?
?[j, i ? 1] + log PNULL(0|ei)
?[j ? 1, i ? 1] + log P?(1|ei)
?[j ? 2, i ? 1] + log P?(2|ei)
?[j ? 3, i ? 1] + log P?(3|ei)
where PNULL(0|ei) is the probability of generating
a NULL word from ei; P?(k = 1|ei) is the usual
word fertility model of generating one French word
from the word ei; ?[j, i] is the cost so far for gener-
ating j words from i English words ei1 : e1, ? ? ? , ei.
After computing the cost of ?[J, I], we can trace
back the Viterbi path, along which the probability
P (J |eI1) of generating J French words from the En-
glish phrase eI1 as shown in Eqn. 1.
142
With this phrase length model, for every candidate
block, we can compute a phrase level fertility score
to estimate to how good the phrase pairs are match
in their lengthes.
4 Distortion of Centers
The centers of source and target phrases are both il-
lustrated in Figure 1. We compute a simple distor-
tion score to estimate how far away the two centers
are in a parallel sentence pair in a sense the block is
close to the diagonal.
In our algorithm, the source center fj+lj of the
phrase f j+lj with length l +1 is simply a normalized
relative position defined as follows:
fj+lj =
1
|F |
j?=j+l
?
j?=j
j?
l + 1 (2)
where |F | is the French sentence length.
For the center of English phrase ei+ki in the target
sentence, we first define the expected corresponding
relative center for every French word fj? using the
lexicalized position score as follows:
ei+ki (fj?) =
1
|E| ?
?(i+k)
i?=i i? ? P (fj? |ei?)
?(i+k)
i?=i P (fj? |ei?)
(3)
where |E| is the English sentence length. P (fj? |ei)
is the word translation lexicon estimated in IBM
Models. i is the position index, which is weighted
by the word level translation probabilities; the term
of ?Ii=1 P (fj? |ei) provides a normalization so that
the expected center is within the range of target sen-
tence length. The expected center for ei+ki is simply
a average of ei+ki (fj?):
ei+ki =
1
l + 1
j+l
?
j?=j
ei+ki (fj?) (4)
This is a general framework, and one can certainly
plug in other kinds of score schemes or even word
alignments to get better estimations.
Given the estimated centers of fj+lj and
ei+ki , we can compute how close they are bythe probability of P (ei+ki |fj+lj ). To estimate
P (ei+ki |fj+lj ), one can start with a flat gaussian
model to enforce the point of (ei+ki ,fj+lj ) not toofar off the diagonal and build an initial list of phrase
pairs, and then compute the histogram to approxi-
mate P (ei+ki |fj+lj ).
5 Lexicon Model
Similar to (Vogel et al, 2004), we compute for each
candidate block a score within a given sentence pair
using a word level lexicon P (f |e) as follows:
P (f j+lj |ei+ki ) =
?
j??[j,j+l]
?
i??[i,i+k]
P (fj? |ei?)
k + 1
?
?
j? /?[j,j+l]
?
i? /?[i,i+k]
P (fj? |ei?)
|E| ? k ? 1
6 Algorithm
Our phrase extraction is described in Algorithm
1. The input parameters are essentially from IBM
Model-4: the word level lexicon P (f |e), the English
word level fertility P?(?e = k|e), and the center
based distortion P (ei+ki |fj+lj ).
Overall, for each source phrase f j+lj , the algo-
rithm first estimates its normalized relative center
in the source sentence, its projected relative cen-
ter in the target sentence. The scores of the phrase
length, center-based distortion, and a lexicon based
score are computed for each candidate block A lo-
cal greedy search is carried out for the best scored
phrase pair (f j+lj , ei+ki ).
In our submitted system, we computed the
following seven base scores for phrase pairs:
Pef (f j+lj |ei+ki ), Pfe(ei+ki |f j+lj ), sharing similar
function form in Eqn. 5.
Pef (f j+lj |ei+ki ) =
?
j?
?
i?
P (fj? |ei?)P (ei? |ei+ki )
=
?
j?
?
i?
P (fj? |ei?)
k + 1 (5)
We compute phrase level relative frequency in both
directions: Prf (f j+lj |ei+ki ) and Prf (ei+ki |f j+lj ). We
compute two other lexicon scores which were also
used in (Vogel et al, 2004): S1(f j+lj |ei+ki ) and
S2(ei+ki |f
j+l
j ) using the similar function in Eqn. 6:
S(f j+lj |ei+ki ) =
?
j?
?
i?
P (fj? |ei?) (6)
143
In addition, we put the phrase level fertility score
computed in section 3 via dynamic programming to
be as one additional score for decoding.
Algorithm 1 A Generalized Alignment-free Phrase
Extraction
1: Input: Pre-trained models: P?(?e = k|e) ,
P (E |F ) , and P (f |e).
2: Output: PhraseSet: Phrase pair collections.
3: Loop over the next sentence pair
4: for j : 0 ? |F | ? 1,
5: for l : 0 ? MaxLength,
6: foreach f j+lj
7: compute f and E
8: left = E ? |E|-MaxLength,
9: right= E ? |E|+MaxLength,
10: for i : left ? right,
11: for k : 0 ? right,
12: compute e of ei+ki ,
13: score the phrase pair (f j+lj , ei+ki ), where
score = P (e|f )P (l|ei+ki )P (f j+lj |ei+ki )
14: add top-n {(f j+lj , ei+ki )} into PhraseSet.
7 Experimental Results
Our system is based on the IBM Model-4 param-
eters. We train IBM Model 4 with a scheme of
1720h73043 using GIZA++ (Och and Ney, 2003).
The maximum fertility for an English word is 3. All
the data is used as given, i.e. we do not have any
preprocessing of the English-French data. The word
alignment provided in the workshop is not used in
our evaluations. The language model is provided
by the workshop, and we do not use other language
models.
The French phrases up to 8-gram in the devel-
opment and test sets are extracted with top-3 can-
didate English phrases. There are in total 2.6 mil-
lion phrase pairs 1 extracted for both development
set and the unseen test set. We did minimal tuning
of the parameters in the pharaoh decoder (Koehn,
2004) settings, simply to balance the length penalty
for Bleu score. Most of the weights are left as they
are given: [ttable-limit]=20, [ttable-threshold]=0.01,
1Our phrase table is to be released to public in this workshop
[stack]=100, [beam-threshold]=0.01, [distortion-
limit]=4, [weight-d]=0.5, [weight-l]=1.0, [weight-
w]=-0.5. Table 1 shows the algorithm?s performance
on several settings for the seven basic scores pro-
vided in section 6.
settings Dev.Bleu Tst.Bleu
s1 27.44 27.65
s2 27.62 28.25
Table 1: Pharaoh Decoder Settings
In Table 1, setting s1 was our submission
without using the inverse relative frequency of
Prf (ei+ki |f
j+l
j ). s2 is using all the seven scores.
8 Discussions
In this paper, we propose a generalized phrase ex-
traction algorithm towards word alignment-free uti-
lizing the fertility model to predict the width of the
block, a distortion model to predict how close the
centers of source and target phrases are, and a lex-
icon model for translational equivalence. The algo-
rithm is a general framework, in which one could
plug in other scores and word alignment to get bet-
ter results.
References
P.F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. In Computational Linguistics, volume 19(2),
pages 263?331.
Philip Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based smt. In Proceedings of the Confer-
ence of the Association for Machine Translation in the
Americans (AMTA).
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 29, pages 19?51.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
Stephan Vogel, Sanjika Hewavitharana, Muntsin Kolss,
and Alex Waibel. 2004. The ISL statistical translation
system for spoken language translation. In Proc. of the
International Workshop on Spoken Language Transla-
tion, pages 65?72, Kyoto, Japan.
144
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 159?162,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Competitive Grouping in Integrated Phrase Segmentation
and Alignment Model
Ying Zhang Stephan Vogel
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
{joy+,vogel+}@cs.cmu.edu
Abstract
This article describes the competitive
grouping algorithm at the core of our Inte-
grated Segmentation and Alignment (ISA)
model. ISA extracts phrase pairs from a
bilingual corpus without requiring the pre-
calculated word alignment as many other
phrase alignment models do. Experiments
conducted within the WPT-05 shared task
on statistical machine translation demon-
strate the simplicity and effectiveness of
this approach.
1 Introduction
In recent years, various phrase translation ap-
proaches (Marcu and Wong, 2002; Och et al, 1999;
Koehn et al, 2003) have been shown to outper-
form word-to-word translation models (Brown et al,
1993). Many of these phrase alignment strategies
rely on the pre-calculated word alignment and use
different heuristics to extract the phrase pairs from
the Viterbi word alignment path. The Integrated
Segmentation and Alignment (ISA) model (Zhang
et al, 2003) does not require such word alignment.
ISA segments the sentence into phrases and finds
their alignment simultaneously. ISA is simple and
fast. Translation experiments have shown compara-
ble performance to other phrase alignment strategies
which require complicated statistical model training.
In this paper, we describe the key idea behind this
model and connect it with the competitive linking al-
gorithm (Melamed, 1997) which was developed for
word-to-word alignment.
2 Translation Likelihood as a Statistical
Test
Given a bilingual corpus of language pair F (For-
eign, source language) and E (English, target lan-
guage), if we know the word alignment for each sen-
tence pair we can calculate the co-occurrence fre-
quency for each source/target word pair type C(f, e)
and the marginal frequency C(f) = ?e C(f, e) and
C(e) = ?f C(f, e). We can apply various sta-
tistical tests (Manning and Schu?tze, 1999) to mea-
sure how likely is the association between f and
e, in other words how likely they are mutual trans-
lations. In the following sections, we will use ?2
statistics to measure the the mutual translation like-
lihood (Church and Hanks, 1990).
3 The Core of the Integrated Phrase
Segmentation and Alignment
The competitive linking algorithm (CLA)
(Melamed, 1997) is a greedy word alignment
algorithm. It was designed to overcome the problem
of indirect associations using a simple heuristic:
whenever several word tokens fi in one half of the
bilingual corpus co-occur with a particular word to-
ken e in the other half of the corpus, the word that is
most likely to be e?s translation is the one for which
the likelihood L(f, e) of translational equivalence
is highest. The simplicity of this algorithm depends
on a one-to-one alignment assumption. Each word
translates to at most one other word. Thus when
one pair {f, e} is ?linked?, neither f nor e can be
aligned with any other words. This assumption
renders CLA unusable in phrase level alignment.
159
We propose an extension, the competitive grouping,
as the core component in the ISA model.
3.1 Competitive Grouping Algorithm (CGA)
The key modification to the competitive linking al-
gorithm is to make it less greedy. When a word pair
is found to be the winner of the competition, we al-
low it to invite its neighbors to join the ?winner?s
club? and group them together as an aligned phrase
pair. The one-to-one assumption is thus discarded
in CGA. In addition, we introduce the locality as-
sumption for phrase alignment. Locality states that a
source phrase of adjacent words can only be aligned
to a target phrase composed of adjacent words. This
is not true of most language pairs in cases such as
the relative clause, passive tense, and prepositional
clause, etc.; however this assumption renders the
problem tractable. Here is a description of CGA:
For a sentence pair {f , e}, represent the word pair
statistics for each word pair {f, e} in a two dimen-
sional matrix LI?J , where L(i, j) = ?2(fi, ej) in
our implementation. 1
Figure 1: Expanding the current phrase pair
Denote an aligned phrase pair {f? , e?} as
a tuple [istart, iend, jstart, jend] where f? is
fistart , fistart+1 , . . . , fiend and similarly for e?.
1. Find i? and j? such that L(i?, j?) is the highest.
Create a seed phrase pair [i?, i?, j?, j?] which is
simply the word pair {fi? , ej?} itself.
2. Expand the current phrase pair
[istart, iend, jstart, jend] to the neighboring
territory to include adjacent source and target
words in the phrase alignment group. There
1?2 statistics were found to be more discriminative in our
experiments than other symmetric word association measures,
such as the averaged mutual information, ?2 statistics and Dice-
coefficient.
are 8 ways to group new words into the phrase
pair. For example, one can expand to the
north by including an additional source word
fistart?1 to be aligned with all the target words
in the current group; or one can expand to the
northeast by including fistart?1 and ejend+1
(Figure 1).
Two criteria have to be satisfied for each expan-
sion:
(a) If a new source word fi? is to be grouped,
maxjstart?j?jend L(i?, j) should be no
smaller than max1?j?J L(i?, j). Since
CGA is a greedy algorithm as described
below, this is to guarantee that fi? will not
?regret? the decision of joining the phrase
pair because it does not have other ?better?
target words to be aligned with. Similar
constraint is applied if a new target word
ej? is to be grouped.
(b) The highest value in the newly-expanded
area needs to be ?similar? to the seed value
L(i?, j?).
Expand the current phrase pair to the largest ex-
tend possible as long as both criteria are satis-
fied.
3. The locality assumption means that the aligned
phrase cannot be aligned again. Therefore, all
the source and target words in the phrase pair
are marked as ?invalid? and will be skipped in
the following steps.
4. If there is another valid pair {fi, ej}, then re-
peat from Step 1.
Figure 2 and Figure 3 show a simple example
of applying CGA on the sentence pair {je de?clare
reprise la session/i declare resumed the session}.
Figure 2: Seed pair {je / i}, no expansion allowed
160
Figure 3: Seed pair {session/session}, expanded to
{la session/the session}
3.2 Exploring all possible groupings
The similarity criterion 2-(b) described previously
is used to control the granularity of phrase pairs.
In cases where the pairs {f1f2, e1e2}, {f1, e1} and
{f2, e2} are all valid translations pairs, similar-
ity is used to control whether we want to align
{f1f2, e1e2} as one phrase pair or two shorter ones.
The granularity of the phrase pairs is hard to op-
timize especially when the test data is unknown. On
the one hand, we prefer long phrases since inter-
action among the words in the phrase, for example
word sense, morphology and local reordering could
be encapsulated. On the other hand, long phrase
pairs are less likely to occur in the test data than the
shorter ones and may lead to low coverage. To have
both long and short phrases in the alignment, we ap-
ply a range of similarity thresholds for each of the
expansion operations. By applying a low similarity
threshold, the expanded phrase pairs tend to be large,
while a higher similarity threshold results in shorter
phrase pairs. As described above, CGA is a greedy
algorithm and the expansion of the seed pair restricts
the possible alignments for the rest of the sentence.
Figure 4 shows an example as we explore all the pos-
sible grouping choices in a depth-first search. In the
end, all unique phrase pairs along the path traveled
are output as phrase translation candidates for the
current sentence pair.
3.3 Phrase translation probabilities
Each aligned phrase pair {f? , e?} is assigned a likeli-
hood score L(f? , e?), defined as:
?
i maxj logL(fi, ej) +
?
j maxi logL(fi, ej)
|f? |+ |e?|
where i ranges over all words in f? and similarly j in
e?.
Given the collected phrase pairs and their likeli-
hood, we estimate the phrase translation probability
Figure 4: Depth-first itinerary of all possible group-
ing choices.
by their weighted frequency:
P (f? |e?) = count(f? , e?) ? L(f? , e?)?
f? count(f? , e?) ? L(f? , e?)
No smoothing is applied to the probabilities.
4 Learning co-occurrence information
In most cases, word alignment information is not
given and is treated as a hidden parameter in the
training process. We initialize a word pair co-
occurrence frequency by assuming uniform align-
ment for each sentence pair, i.e. for sentence pair
(f , e) where f has I words and e has J words, each
word pair {f, e} is considered to be aligned with fre-
quency 1I?J . These co-occurrence frequencies will
be accumulated over the whole corpus to calculate
the initial L(f, e). Then we iterate the ISA model:
1. Apply the competitive grouping algorithm to
each sentence pair to find all possible phrase
pairs.
2. For each identified phrase pair {f? , e?}, increase
the co-occurrence counts for all word pairs in-
side {f? , e?} with weight 1|f? |?|e?| .
3. Calculate L(f, e) again and goto Step 1 for sev-
eral iterations.
5 Experiments
We participated the shared task in the WPT05 work-
shop2 and applied ISA to all four language pairs
2http://www.statmt.org/wpt05/mt-shared-task/
161
(French-English, Finnish-English, German-English
and Spanish-English). Table 1 shows the n-gram
coverage of the dev-test set. French and Spanish
data are better covered by the training data com-
pared to the German and Finnish sets. Since our
phrase alignment is constrained by the locality as-
sumption and we can only extract phrase pairs of
adjacent words, lower n-gram coverage will result in
lower translation scores. We used the training data
Dev-test DE ES FI FR
N=1 99.2 99.6 98.2 99.8
N=2 88.2 93.3 73.0 94.7
N=3 59.4 71.7 38.2 76.0
N=4 30.0 42.9 17.0 50.6
N=5 13.0 21.7 6.8 29.8
N=16 (8) (65) (1) (101)
N=19 (1) (23) (34)
N=23 (1) (1)
Table 1: Percentage of dev-test n-grams covered by
the training data. Numbers in parenthesis are the
actual counts of n-gram tokens in the dev-test data.
and the language model as provided and manually
tuned the parameters of the Pharaoh decoder3 to op-
timize BLEU scores. Table 2 shows the translation
results on the dev-test and the test set of WPT05.
The BLEU scores appear comparable to those of
other state-of-the-art phrase alignment systems, in
spite of the simplicity of the ISA model and ease of
training.
DE ES FI FR
Dev-test 18.63 26.20 12.88 26.20
Test 18.93 26.14 12.66 26.71
Table 2: BLEU scores of ISA in WPT05
6 Conclusion
In this paper, we introduced the competitive group-
ing algorithm which is at the core of the ISA phrase
alignment model. As an extension to the competitive
linking algorithm which is used for word-to-word
alignment, CGA overcomes the assumption of one-
to-one mapping and makes it possible to align phrase
3http://www.isi.edu/licensed-sw/pharaoh/
pairs. Despite its simplicity, the ISA model has
achieved competitive translation results. We plan to
release ISA toolkit4 to the community in the near
future.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Comput. Linguist., 16(1):22?29.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguistics
Conference (HLT/NAACL), Edomonton, Canada, May
27-June 1.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. of the Conference on Empirical Meth-
ods in Natural Language Processing, Philadephia, PA,
July 6-7.
I. Dan Melamed. 1997. A word-to-word model of trans-
lational equivalence. In Proceedings of the 8-th con-
ference on EACL, pages 490?497, Morristown, NJ,
USA. Association for Computational Linguistics.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proc. of the Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 20?28, University of
Maryland, College Park, MD, June.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2003. In-
tegrated phrase segmentation and alignment algorithm
for statistical machine translation. In Proceedings of
NLP-KE?03, Beijing, China, October.
4http://projectile.is.cs.cmu.edu/research/public/isa/index.htm
162
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 216?223,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Distributed Language Modeling for N -best List Re-ranking
Ying Zhang Almut Silja Hildebrand Stephan Vogel
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Ave. Pittsburgh, PA 15213, U.S.A.
{joy+, silja+, vogel+}@cs.cmu.edu
Abstract
In this paper we describe a novel dis-
tributed language model for N -best list
re-ranking. The model is based on the
client/server paradigm where each server
hosts a portion of the data and provides
information to the client. This model al-
lows for using an arbitrarily large corpus
in a very efficient way. It also provides
a natural platform for relevance weighting
and selection. We applied this model on
a 2.97 billion-word corpus and re-ranked
the N -best list from Hiero, a state-of-the-
art phrase-based system. Using BLEU as a
metric, the re-ranked translation achieves
a relative improvement of 4.8%, signifi-
cantly better than the model-best transla-
tion.
1 Introduction
Statistical language modeling has been widely
used in natural language processing applications
such as Automatic Speech Recognition (ASR),
Statistical Machine Translation (SMT) (Brown et
al., 1993) and Information Retrieval (IR) (Ponte
and Croft, 1998).
Conventional n-gram language modeling
counts the frequency of all the n-grams in a
corpus and calculates the conditional probabilities
of a word given its history of n ? 1 words
P (wi|wi?1i?n+1). As the corpus size increases,
building a high order language model offline
becomes very expensive if it is still possible
(Goodman, 2000).
In this paper, we describe a new approach of
language modeling using a distributed comput-
ing paradigm. Distributed language modeling can
make use of arbitrarily large training corpora and
provides a natural way for language model adap-
tation.
We applied the distributed LM to the task of re-
ranking the N -best list in statistical machine trans-
lation and achieved significantly better translation
quality when measured by the BLEU metric (Pap-
ineni et al, 2001).
2 N -best list re-ranking
When translating a source language sentence f
into English, the SMT decoder first builds a trans-
lation lattice over the source words by applying the
translation model and then explores the lattice and
searches for an optimal path as the best translation.
The decoder uses different models, such as the
translation model, n-gram language model, fertil-
ity model, and combines multiple model scores to
calculate the objective function value which favors
one translation hypothesis over the other (Och et
al., 2004).
Instead of outputting the top hypothesis e(1)
based on the decoder model, the decoder can out-
put N (usually N = 1000) alternative hypotheses
{e(r)|r = 1, . . . , N} for one source sentence and
rank them according to their model scores.
Figure 1 shows an example of the output from a
SMT system. In this example, alternative hypoth-
esis e(2) is a better translations than e(1) according
to the reference (Ref) although its model score is
lower.
SMT models are not perfect, it is unavoidable
to have a sub-optimal translation output as the
model-best by the decoder. The objective of N -
best list re-ranking is then to re-rank the trans-
lation hypotheses using features which are not
used during decoding so that better translations
can emerge as ?optimal? translations. Our exper-
216
f : , 2001#?)?I9]??{/G??
Ref: Since the terrorist attacks on the United States in 2001
e(1): since 200 year , the united states after the terrorist
attacks in the incident
e(2): since 2001 after the incident of the terrorist attacks on
the united states
e(3): since the united states 2001 threats of terrorist attacks
after the incident
e(4): since 2001 the terrorist attacks after the incident
e(5): since 200 year , the united states after the terrorist
attacks in the incident
Figure 1: An example of N -best list.
iments (section 5.1) have shown that the oracle-
best translation from a typical N -best list could be
6 to 10 BLEU points better than the model-best
translation.
In this paper we use the distributed language
model on very large data to re-rank the N -best list.
2.1 Sentence likelihood
The goal of a language model is to determine
the probability, or in general the ?likelihood? of
a word sequence w1 . . . wm (wm1 for short) given
some training data. The standard language model-
ing approach breaks the sentence probability down
into:
P (wm1 ) =
?
i
P (wi|wi?11 ) (1)
Under the Markov or higher order Markov process
assumption that only the closest n? 1 words have
real impact on the choice of wi, equation 1 is ap-
proximated to:
P (wm1 ) =
?
i
P (wi|wi?1i?n+1) (2)
The probability of a word given its history can be
approximated with the maximum likelihood esti-
mate (MLE) without any smoothing:
P (wi|wi?1i?n+1) ?
C(wii?n+1)
C(wi?1i?n+1)
(3)
In addition to the standard n-gram probability
estimation, we propose 3 sentence likelihood met-
rics.
? L0: Number of n-grams matched.
The simplest metric for sentence likelihood is
to count how many n-grams in this sentence
can be found in the corpus.
L0(wm1 ) =
?
i,j
i?j
?(wji ) (4)
?(wji ) =
{
1 : C(wji ) > 0
0 : C(wji ) = 0
(5)
For example, L0 for sentence in figure 2 is 52
because 52 n-grams have non-zero counts.
? Ln1 : Average interpolated n-gram conditional
probability.
Ln1 (wm1 ) =
( m?
i=1
n?
k=1
?kP (wi|wi?1i?k+1)
) 1
m
(6)
P (wi|wi?1i?k+1) is approximated from the n-
gram counts (Eq. 3) without any smoothing.
?k is the weight for k-gram conditional prob-
ability,
??k = 1.
Ln1 is similar to the standard n-gram LM
except the probability is averaged over the
words in the sentence to prevent shorter sen-
tences being favored unfairly.
? L2: Sum of n-gram?s non-compositionality
For each matched n-gram, we consider all
the possibilities to cut/decompose it into two
short n-grams, for example ?the terrorist at-
tacks on the united states? could be decom-
posed into (?the?, ?terrorist attacks on the
united states?) or (?the terrorist?, ?attacks
on the united states?), ... , or (?the ter-
rorist attacks on the united?, ?states?). For
each cut, calculate the point-wise mutual in-
formation (PMI) between the two short n-
grams. The one with the minimal PMI
is the most ?natural? cut for this n-gram.
The PMI over the natural cut quantifies the
non-compositionality Inc of an n-gram wji .
The higher the value of Inc(wji ) the more
likely wji is a meaningful constituent, in other
words, it is less likely that wji is composed
from two short n-grams just by chance (Ya-
mamoto and Church, 2001).
Define L2 formally as:
L2(wm1 ) =
?
i,j
i?j
Inc(wji ) (7)
217
Inc(wji ) =
?
?
?
min
k
I(wki ;wjk+1) : C(wji ) > 0
0 : C(wji ) = 0
(8)
I(wki ;wjk+1) = log
P (wji )
P (wki )P (wjk+1)
(9)
3 Distributed language model
The fundamental information required to calculate
the likelihood of a sentence is the frequency of n-
grams in the corpus. In conventional LM train-
ing, all the counts are collected from the corpus D
and saved to disk for probability estimation. When
the size of D becomes large and/or n is increased
to capture more context, the count file can be too
large to be processed.
Instead of collecting n-gram counts offline, we
index D using a suffix array (Manber and Myers,
1993) and count the occurrences of wii?n+1 in D
on the fly.
3.1 Calculate n-gram frequency using suffix
array
For a corpus D with N words, locating all the oc-
currences of wii?n+1 takes O(logN ). Zhang and
Vogel (2005) introduce a search algorithm which
locates all the m(m+ 1)/2 embedded n-grams in
a sentence of m words within O(m ? logN ) time.
Figure 2 shows the frequencies of all the embed-
ded n-grams in sentence ?since 2001 after the in-
cident of the terrorist attacks on the united states?
matched against a 26 million words corpus. For
example, unigram ?after? occurs 4.43?104 times,
trigram ?after the incident? occurs 106 times. The
longest n-gram that can be matched is the 8-gram
?of the terrorist attacks on the united states? which
occurs 7 times in the corpus.
3.2 Client/Server paradigm
To load the corpus and its suffix array index into
the memory, each word token needs 8 bytes. For
example, if the corpus has 50 million words,
400MB memory is required. For the English1 Gi-
gaWord2 corpus which has 2.7 billion words, the
1Though we used English data for our experiments in this
paper, the approach described here is language independent.
2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2005T12
total memory required is 22GB. It is practically
impossible to fit such data into the memory of any
single machine.
To make use of the large amount of data, we
developed a distributed client/server architecture
for language modeling. Client/server is the most
common paradigm of distributed computing at
present (Leopold, 2001). The paradigm describes
an asymmetric relationship between two type of
processes, of which one is the client, and the other
is the server. The server process manages some re-
sources and offers a service which can be used by
other processes. The client is a process that needs
the service in order to accomplish its task. It sends
a request to the server and asks for the execution
of a task that is covered by the service.
We split the large corpus D into d non-
overlapping chunks. One can easily verify that for
any n-gram wii?n+1 the count of its occurrences in
D is the sum of its occurrences in all the chunks,
i.e.,
C(wii?n+1)|D =
?
d
C(wii?n+1)|Dd (10)
Each server3 loads one chunk of the corpus with
its suffix array index. The client sends an English
sentence w1 . . . wm to each of the servers and re-
quests for the count information of all the n-grams
in the sentence. The client collects the count infor-
mation from all the servers, sums up the counts for
each n-gram and then calculates the likelihood of
the sentence.
The client communicates with the servers via
TCP/IP sockets. In our experiments, we used
150 servers running on 26 computers to serve one
client. Multiple clients can be served at the same
time if needed. The process of collecting counts
and calculating the sentence probabilities takes
about 1 to 2 ms for each English sentence (average
length 23.5 words). With this architecture, we can
easily make use of larger corpora by adding addi-
tional data servers. In our experiments, we used all
the 2.7 billion word data in the English Gigaword
corpus without any technical difficulties.
3A server is a special program that provides services to
client processes. It runs on a physical computer but the con-
cept of server should not be confused with the actual machine
that runs it. In practice, one computer usually hosts multiple
servers at the same time.
218
n since 2001 after the incident of the terrorist attacks on the united states
1 2.19?104 7559 4.43?104 1.67?106 2989 6.9?105 1.67?106 6160 9278 2.7?105 1.67?106 5.1?104 3.78?104
2 165 105 1.19?104 1892 34 2.07?105 807 1398 1656 5.64?104 3.72?104 3.29?104
3 6 56 106 6 3 162 181 216 545 605 2.58?104
4 0 0 0 1 0 35 67 111 239 424
5 0 0 0 0 0 15 34 77 232
6 0 0 0 0 0 10 23 76
7 0 0 0 0 0 7 23
8 0 0 0 0 0 7
Figure 2: Frequencies of all the embedded n-grams in sentence ?since 2001 after the incident of the
terrorist attacks on the united states.?
4 ?More data is better data? or
?Relevant data is better data?
Although statistical systems usually improve with
more data, performance can decrease if additional
data does not fit the test data. There have been
debates in the data-driven NLP community as to
whether ?more data is better data? or ?relevant
data is better data?. For N -best list re-ranking, the
question becomes: ?should we use all the data to
re-rank the hypotheses for one source sentence, or
select some corpus chunks that are believed to be
relevant to this sentence??
Various relevance measures are proposed in
(Iyer and Ostendorf, 1999) including content-
based relevance criteria and style-based criteria. In
this paper, we use a very simple relevance metric.
Define corporaDd?s relevance to a source sentence
ft as:
R(Dd, ft) =
N?
r=1
L0(e(r)t )|Dd (11)
R(Dd, ft) estimates how well a corpus Dd can
cover the n-grams in the N -best list of a source
sentence. The higher the coverage, the more rele-
vant Dd is.
In the distributed LM architecture, the client
first sends N translations of ft to all the servers.
From the returned n-gram matching information,
client calculates R(Dd, ft) for each server, and
choose the most relevant (e.g., 20) servers for ft.
The n-gram counts returned from these relevant
servers are summed up for calculating the likeli-
hood of ft. One could also assign weights to the n-
gram counts returned from different servers during
the summation so that the relevant data has more
impact than the less-relevant ones.
5 Experiments
We used the N -best list generated by the Hiero
SMT system (Chiang, 2005). Hiero is a statis-
tical phrase-based translation model that uses hi-
erarchical phrases. The decoder uses a trigram
language model trained with modified Kneser-Ney
smoothing (Kneser and Ney, 1995) on a 200 mil-
lion words corpus. The 1000-best list was gen-
erated on 919 sentences from the MT03 Chinese-
English evaluation set.
All the data from the English Gigaword corpus
plus the English side of the Chinese-English bilin-
gual data available from LDC are used. The 2.97
billion words data is split into 150 chunks, each
has about 20 million words. The original order
is kept so that each chunk contains data from the
same news source and a certain period of time.
For example, chunk Xinhua2003 has all the Xin-
hua News data from year 2003 and NYT9499 038
has the last 20 million words from the New York
Times 1994-1999 corpus. One could split the
data into larger(smaller) chunks which will require
less(more) servers. We choose 20 million words as
the size for each chunk because it can be loaded by
our smallest machine and it is a reasonable granu-
larity for selection.
In total, 150 corpus information servers run on
26 machines connected by the standard Ethernet
LAN. One client sends each English hypothesis
translations to all 150 servers and uses the returned
information to re-rank. The whole process takes
about 600 seconds to finish.
We use BLEU scores to measure the transla-
tion accuracy. A bootstrapping method is used to
calculate the 95% confidence intervals for BLEU
(Koehn, 2004; Zhang and Vogel, 2004).
5.1 Oracle score of the N -best list
Because of the spurious ambiguity, there are only
24,612 unique hypotheses in the 1000-best list, on
average 27 per source sentence. This limits the po-
tential of N -best re-ranking. Spurious ambiguity
is created by the decoder where two hypotheses
generated from different decoding path are con-
sidered different even though they have identical
word sequences. For example, ?the terrorist at-
tacks on the united states? could be the output of
decoding path [the terrorist attacks][on the united
219
states] and [the terrorist attacks on] [the united
states].
We first calculate the oracle score from the N -
best list to verify that there are alternative hypothe-
ses better than the model-best translation. The or-
acle best translations are created by selecting the
hypothesis which has the highest sentence BLEU
score for each source sentence. Yet a critical prob-
lem with BLEU score is that it is a function of
the entire test set and does not give meaningful
scores for single sentences. We followed the ap-
proximation described in (Collins et al, 2005) to
get around this problem. Given a test set with T
sentences, N hypotheses are generated for each
source sentence ft. Denote e(r)t as the r-th ranked
hypothesis for ft. e(1)t is the model-best hypoth-
esis for this sentence. The baseline BLEU scores
are calculated based on the model-best translation
set {e(1)t |t = 1, . . . , T}.
Define the BLEU sentence-level gain for e(r)t
as:
GBLEUe(r)t =
BLEU{e(1)1 , e(1)2 , . . . , e(r)t , . . . , e(r)T }
? BLEU{e(1)1 , e(1)2 , . . . , e(1)t , . . . , e(r)T }
GBLEUe(r)t calculates the gain if we switch the
model-best hypothesis e(1)t using e(r)t for sentence
ft and keep the translations for the rest of the test
set untouched.
With the estimated sentence level gain for each
hypothesis, we can construct the oracle best trans-
lation set by selecting the hypotheses with the
highest BLEU gain for each sentence. Oracle best
BLEU translation set is: {e(r?t )t |t = 1, . . . , T}
where r?t = argmaxr GBLEUe(r)t .
Model-best
Score Confidence Interval Oracle
BLEU 31.44 [30.49, 32.33] 37.48
Table 1: BLEU scores for the model-best and
oracle-best translations.
Table 1 shows the BLEU score of the approxi-
mated oracle best translation. The oracle score is
7 points higher than the model-best scores even
though there are only 27 unique hypotheses for
each sentence on average. This confirms our ob-
servation that there are indeed better translations
in the N -best list.
5.2 Training standard n-gram LM on large
data for comparison
Besides comparing the distributed language model
re-ranked translations with the model-best transla-
tions, we also want to compare the distributed LM
with the the standard 3-gram and 4-gram language
models on the N -best list re-ranking task.
Training a standard n-gram model for a 2.9 bil-
lion words corpora is much more complicated and
tedious than setting up the distributed LM. Be-
cause of the huge size of the corpora, we could
only manage to train a test-set specific n-gram LM
for this experiment.
First, we split the corpora into smaller chunks
and generate n-gram count files for each chunk.
Each count file is then sub-sampled to entries
where all the words are listed in the vocabulary
of the N -best list (5,522 word types). We merge
all the sub-sampled count files into one and train
the standard language model based on it.
We manage to train a 3-gram LM using the
2.97 billion-word corpus. Resulting LM requires
2.3GB memory to be loaded for the re-ranking ex-
periment.
A 4-gram LM for this N -best list is of 13 GB
in size and can not be fit into the memory. We
split the N -best list into 9 parts to reduce the vo-
cabulary size of each sub N -best list to be around
1000 words. The 4-gram LM tailored for each sub
N -best list is around 1.5 to 2 GB in size.
Training higher order standard n-gram LMs
with this method requires even more partitions of
the N -best list to get smaller vocabularies. When
the vocabulary becomes too small, the smoothing
could fail and results in unreliable LM probabili-
ties.
Adapting the standard n-gram LM for each in-
dividual source sentence is almost infeasible given
our limited computing resources. Thus we do not
have equivalent n-gram LMs to be compared with
the distributed LM for conditions where the most
relevant data chunks are used to re-rank the N -best
list for a particular source sentence.
5.3 Results
Table 2 lists results of the re-ranking experiments
under different conditions. The re-ranked trans-
lation improved the BLEU score from 31.44 to
220
32.64, significantly better than the model-best
translation.
Different metrics are used under the same data
situation for comparison. L0, though extremely
simple, gives quite nice results on N -best list re-
ranking. With only one corpus chunk (the most
relevant one) for each source sentence, L0 im-
proved the BLEU score to 32.22. We suspect that
L0 works well because it is inline with the nature
of BLEU score. BLEU measures the similarity be-
tween the translation hypothesis and human refer-
ence by counting how many n-grams in MT can
be found in the references.
Instead of assigning weights 1 to all the
matched n-grams in L0, L2 weights each n-gram
by its non-compositionality. For all data condi-
tions, L2 consistently gives the best results.
Metric family L1 is close to the standard n-gram
LM probability estimation. Because no smoothing
is used, L31 performance (32.00) is slightly worse
than the standard 3-gram LM result (32.22). On
the other hand, increasing the length of the history
in L1 generally improves the performance.
Figure 3 shows the BLEU score of the re-ranked
translation when using different numbers of rele-
vant data chunks for each sentence. The selected
data chunks may differ for each sentences. For
example, the 2 most relevant corpora for sentence
1 are Xinhua2002 and Xinhua2003 while for sen-
tence 2 APW2003A and NYT2002D are more rel-
evant. When we use the most relevant data chunk
(about 20 million words) to re-rank the N -best list,
36 chunks of data will be used at least once for
919 different sentences, which accounts for about
720 million words in total. Thus the x-axis in fig-
ure 3 should not be interpreted as the total amount
of data used but the number of the most relevant
corpora used for each sentence.
All three metrics in figure 3 show that using
all data together (150 chunks, 2.97 billion words)
does not give better discriminative powers than us-
ing only some relevant chunks. This supports our
argument in section 4 that relevance selection is
helpful in N -best list re-ranking. In some cases
the re-ranked N -best list has a higher BLEU score
after adding a supposedly ?less-relevant? corpus
chunk and a lower BLEU score after adding a
?more-relevant? chunk. This indicates that the rel-
evance measurement (Eq. 11) is not fully reflect-
ing the real ?relevance? of a data chunk for a sen-
tence. With a better relevance measurement one
 32.15
 32.2
 32.25
 32.3
 32.35
 32.4
 32.45
 32.5
 32.55
 32.6
 32.65
 32.7
 0  20  40  60  80  100  120  140  160
Ble
u S
cor
e
Number of corpus chunks used for each source sentence (*20M=corpus size used)
"L0"
"L1"
"L2"
Figure 3: BLEU score of the re-ranked best hy-
pothesis vs. the number of the most relevant cor-
pus chunks used to re-rank the n-best list for each
sentences. L0: number of n-grams matched; L1:
average interpolated n-gram conditional probabil-
ity; L2: sum of n-grams? non-compositionality.
would expect to see the curves in figure 3 to be
much smoother.
6 Related work and discussion
Yamamoto and Church (2001) used suffix arrays
to compute the frequency and location of an n-
gram in a corpus. The frequencies are used to find
?interesting? substrings which have high mutual
information.
Soricut et al (2002) build a Finite State Ac-
ceptor (FSA) to compactly represent all possible
English translations of a source sentence accord-
ing to the translation model. All sentences in a
big monolingual English corpus are then scanned
by this FSA and those accepted by the FSA are
considered as possible translations for the source
sentence. The corpus is split into hundreds of
chunks for parallel processing. All the sentences
in one chunk are scanned by the FSA on one pro-
cessor. Matched sentences from all chunks are
then used together as possible translations. The
assumption of this work that possible translations
of a source sentence can be found as exact match
in a big monolingual corpus is weak even for very
large corpus. This method can easily fail to find
any possible translation and return zero proposed
translations.
Kirchhoff and Yang (2005) used a factored 3-
gram model and a 4-gram LM (modified KN
smoothing) together with seven system scores to
re-rank an SMT N -best. They improved the
translation quality of their best baseline (Spanish-
221
# of Relevant Chunks per. Sent 1 2 5 10 20 150
3-gram KN 32.22 32.08
4-gram KN 32.22 32.53
L0 32.27 32.38 32.40 32.47 32.51 32.48
L31 32.00 32.14 32.14 32.15 32.16
L41 32.18 32.36 32.28 32.44 32.41
L51 32.21 32.33 32.35 32.41 32.37
L61 32.19 32.22 32.37 32.45 32.40 32.41
L71 32.22 32.29 32.37 32.44 32.40
L2 32.29 32.52 32.61 32.55 32.64 32.56
Table 2: BLEU scores of the re-ranked translations. Baseline score = 31.44
English) from BLEU 30.5 to BLEU 31.0.
Iyer and Ostendorf (1999) select and weight
data to train language modeling for ASR. The data
is selected based on its relevance for a topic or the
similarity to data known to be in the same domain
as the test data. Each additional document is clas-
sified to be in-domain or out-of-domain accord-
ing to cosine distance with TF-IDF term weights,
POS-tag LM and a 3-gram word LM. n-gram
counts from the in-domain and the additionally se-
lected out-of-domain data are then combined with
an weighting factor. The combined counts are
used to estimate a LM with standard smoothing.
Hildebrand et al (2005) use information re-
trieval to select relevant data to train adapted trans-
lation and language models for an SMT system.
Si et al (2002) use unigram distribution simi-
larity to select the document collection which is
most relevant to the query documents. Their work
is mainly focused on information retrieval appli-
cation.
7 Conclusion and future work
In this paper, we presented a novel distributed
language modeling solution. The distributed LM
is capable of using an arbitrarily large corpus
to estimate the n-gram probability for arbitrarily
long histories. We applied the distributed lan-
guage model to N -best re-ranking and improved
the translation quality by 4.8% when evaluated by
the BLEU metric. The distributed LM provides a
flexible architecture for relevance selection, which
makes it possible to select data for each individual
test sentence. Our experiments have shown that
relevant data has better discriminative power than
using all the data.
We will investigate different relevance weight-
ing schemes to better combine n-gram statistics
from different data sources. We are planning to
integrate the distributed LM in the statistical ma-
chine translation decoder in the near future.
8 Acknowledgement
We would like to thank Necip Fazil Ayan and
Philip Resnik for providing Hiero system?s N -best
list and allowing us to use it for this work.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL 2005, pages 263?270, Ann Arbor,
MI, June 2005. ACL.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL 2005, pages
531?540, Ann Arbor, MI, June.
J. Goodman. 2000. A bit of progress in language
modeling. Technical report, Microsoft Research, 56
Fuchun Peng.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
EAMT conference ?Practical applications of ma-
chine translation?, pages 133?142, Budapest, May.
R. Iyer and M. Ostendorf. 1999. Relevance weighting
for combining multi-domain data for n-gram lan-
guage modeling. Comptuer Speech and Language,
13(3):267?282.
222
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation.
In Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 125?128, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume 1,
pages 181?184.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, Barcelona, Spain, July.
Claudia Leopold. 2001. Parallel and Distributed Com-
puting: A Survey of Models, Paradigms and Ap-
proaches. John Wiley & Sons, Inc., New York, NY,
USA.
Udi Manber and Gene Myers. 1993. Suffix arrays:
a new method for on-line string searches. SIAM J.
Comput., 22(5):935?948.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical machine
translation. In Proceedings of the 2004 Meeting of
the North American chapter of the Association for
Computational Linguistics (NAACL-04), Boston.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176(W0109-
022), IBM Research Division, Thomas J. Watson
Research Center.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Re-
search and Development in Information Retrieval,
pages 275?281.
Luo Si, Rong Jin, Jamie Callan, and Paul Ogilvie.
2002. A language modeling framework for resource
selection and results merging. In CIKM ?02: Pro-
ceedings of the eleventh international conference
on Information and knowledge management, pages
391?397, New York, NY, USA. ACM Press.
Radu Soricut, Kevin Knight, and Daniel Marcu. 2002.
Using a large monolingual corpus to improve trans-
lation accuracy. In AMTA ?02: Proceedings of
the 5th Conference of the Association for Machine
Translation in the Americas on Machine Transla-
tion: From Research to Real Users, pages 155?164,
London, UK. Springer-Verlag.
Mikio Yamamoto and Kenneth W. Church. 2001. Us-
ing suffix arrays to compute term frequency and doc-
ument frequency for all substrings in a corpus. Com-
put. Linguist., 27(1):1?30.
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for the machine translation evalu-
ation metrics. In Proceedings of The 10th Interna-
tional Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, October.
Ying Zhang and Stephan Vogel. 2005. An effi-
cient phrase-to-phrase alignment model for arbitrar-
ily long phrase and large corpora. In Proceedings
of the Tenth Conference of the European Associa-
tion for Machine Translation (EAMT-05), Budapest,
Hungary, May. The European Association for Ma-
chine Translation.
223
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 72?79,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
A Walk on the Other Side:  
Adding Statistical Components to a Transfer-Based Translation System 
Ariadna Font Llitj?s 
Carnegie Mellon University 
5000 Forbes Ave. 
 Pittsburgh, PA, 15213 
aria@cs.cmu.edu 
Stephan Vogel 
Carnegie Mellon University 
5000 Forbes Ave. 
 Pittsburgh, PA, 15213 
vogel+@cs.cmu.edu 
  
Abstract 
This paper seeks to complement the cur-
rent trend of adding more structure to Sta-
tistical Machine Translation systems, by 
exploring the opposite direction: adding 
statistical components to a Transfer-Based 
MT system. Initial results on the BTEC 
data show significant improvement ac-
cording to three automatic evaluation 
metrics (BLEU, NIST and METEOR). 
1 Introduction 
In recent years the machine translation research 
community has seen a remarkable paradigm shift.  
It is not the first one, but it has been a very dra-
matic one: statistical machine translation has taken 
the center stage. Conferences like ACL or HLT are 
virtually flooded with papers on various flavors of 
SMT.  In international machine translation evalua-
tion like NIST (NIST MT Evaluation), TC-Star 
(TC-STAR Evaluation) or IWSLT (IWSLT 2006) 
evaluations, most participating systems are SMT 
systems, with a few Example-Based systems sprin-
kled in. Rule-Based systems seem to have for the 
most part disappeared. There may be many reasons 
for this paradigm shift. One obvious reason is the 
comparable ease, which with data-driven systems 
can be built once some parallel data is available. 
Another reason is that the performance of statisti-
cal translation systems has dramatically improved 
over the last 5 to 10 years. 
Does this mean that work on grammar-based 
systems should be stopped?  Should all the insight 
into the structure of languages be neglected?  This 
might be too drastic a reaction. Actually, now that 
SMT has reached some maturity, we see several 
attempts to integrate more structure into these sys-
tems, ranging from simple hierarchical alignment 
models (Wu 1997, Chiang 2005) to syntax-based 
statistical systems (Yamada and Knight 2001, 
Zollmann and Venugopal 2006). What can tradi-
tional Rule-Based translation systems learn from 
these approaches? And would it not make sense to 
work from both sides towards that common goal:  
structurally rich statistical translation models.  In 
this paper we study some enhancements for a 
Transfer-Based translation system, using tech-
niques and even components developed for statisti-
cal machine translation.  While the core engine 
remains virtually untouched, additional features are 
added to re-score the n-best list generated by the 
transfer engine. Statistical alignment techniques 
are used to lower the burden in building a lexicon 
for a new domain. Minimum error rate training is 
used to optimize the system. We show that this 
leads to significant improvements in performance. 
2 A Transfer-Based Translation System 
2.1 The Lexicon and Grammar 
In our Rule-Based MT (RBMT) system, translation 
rules include parsing, transfer, and generation in-
formation, similar to the modified transfer ap-
proach used in the early Metal system (Hutchins 
and Somers, 1992).  
The initial lexicon (479 entries) and grammar 
(40 rules) used in our experiments were manually 
written to cover the syntactic structures and the 
vocabulary of the first 400 sentences of the 
AVENUE Elicitation Corpus (Probst et al2001). 
The Elicitation Corpus contains sets of minimal 
pairs in English and it was designed to cover a va-
riety of linguistic phenomena. Building these two 
language-dependent components took a computa-
tional linguist 2-3 months. Figures 1 and 2 show 
72
examples of a translation rules in the grammar and 
the lexicon. 
 
{S,4} 
S::S : [NP VP] -> [NP VP] 
( (X1::Y1)  (X2::Y2) 
  (x0 = x2) 
  ((y2 subj) = -) 
  ((y1 case) = nom) 
  ((y1 agr) = (x1 agr)) 
  ((y2 tense) = (x2 tense)) 
  ((y2 agr pers) = (y1 agr pers)) 
  ((y2 agr num) = (y1 agr num)) ) 
 
Figure 1: English?Spanish translation rule with 
agreement constraints for subject (NP) and verb 
(VP). 
 
V::V |: ["prefer"] -> ["prefiero"] 
((X1::Y1) 
((x0 form) = prefer) 
((x0 tense) = pres) 
((y0 agr pers) = 1) 
((y0 agr num) = sg)) 
 
Figure 2: English?Spanish lexical entry for the 
verb ?prefer?. 
2.2 Refined MT System 
The original grammar and lexicon were automati-
cally improved with an Automatic Rule Refiner, 
guided by a few bilingual speaker corrections 
(Font Llitj?s & Ridmann 2007). In this approach, 
automatic refinements only affect the target lan-
guage side of translation rules, namely transfer and 
generation information. 
The refined MT system used in our experiments 
is the result of adding 30 agreement constraints to 
the grammar rules, which makes the grammar 
tighter (leading to an increase in precision), as well 
as adding three new rules to cover new syntactic 
structures and five lexical entries for new senses 
and forms of existing words (leading to an increase 
in recall). 
2.3 The Transfer Engine 
The Transfer Engine, or Xfer engine for short, 
combines the translation grammar and lexicon in 
order to produce translations of a source language 
sentence into a target language. The Xfer engine 
incorporates the three main processes involved in 
Transfer-based MT: parsing of the source language 
input, transfer of the parsed constituents of the 
source sentence to their corresponding structured 
constituents on the target language side, and gen-
eration of the target sentence. 
The currently implemented algorithm is similar 
to bottom-up chart parsing as described for exam-
ple in Allen (1995). A chart is first populated with 
all constituent structures that were created in the 
course of parsing the source language sentence 
with the source-side portion of the transfer gram-
mar. Transfer and generation are applied to each 
constituent entry. The transfer rules associated 
with each entry in the chart are used in order to 
determine the corresponding constituent structure 
on the target language side. At the word level, lexi-
cal transfer rules are used in order to get the differ-
ent lexical choices. 
Often, no parse for the entire source sentence 
can be found. Partial parses are concatenated se-
quentially to generate complete translations. 
In the current version of the Xfer system, the 
output can be a first-best translation or a n-best list, 
which can be used for additional n-best list rescor-
ing. The alternatives arise from lexical ambiguity 
and multiple synonymous choices for lexical items 
in the dictionary, but also from syntactic ambiguity 
and multiple competing hypotheses from the 
grammar. 
For our experiments, we used version 3 of the 
Xfer engine. An older version of the Xfer engine is 
described in detail in Peterson (2002).  
2.4 Ranking Translations 
The Xfer engine can generate multiple translations.  
This requires a quality score to be assigned to all 
the alternatives. Based on these scores, the 1-best 
translation will be selected by the system. 
Fragmentation Penalty 
In the original Xfer system the only score used to 
rank translation alternatives was a heuristic frag-
mentation penalty. The fragmentation penalty is 
essentially the number of different chunks (rules or 
lexical entries not embedded in another rule) that 
span the whole translation. The intuition behind 
this score is that the more partial parses are neces-
sary to span the entire sentence the less likely the 
resulting translation will be a good one. 
N-gram LM 
The fragmentation feature is rather weak. It does 
not distinguish between words which are more 
likely to be seen in the target language and words 
which are less likely to be used.  To generate sen-
73
tences which are not only grammatically correct, 
but also use words and word sequences that are 
more natural and more common, data-driven ma-
chine translation systems use a n-gram language 
model.  To get the same benefit in the Xfer system, 
an n-gram LM has been integrated with the engine.   
This has the advantage that in the case of prun-
ing, the LM score can be used to avoid pruning 
good hypotheses, in addition to re re-rank the final 
translations. 
For our experiments, a suffix array language 
model based on the SALM toolkit (Zhang & Vo-
gel, 2006) is used. 
Length Model 
To adjust for the length of the translations gener-
ated by the system, the difference between the 
number of words generated and the expected num-
ber of words is added as a very simple feature. The 
expected length is calculated by multiplying the 
source sentence length by the ratio of the number 
of target and source words in the training corpus. 
The effect of this feature is to balance globally the 
length of the translations. 
2.5 Pruning 
To deal with the combinatorial explosion during 
the parsing/translation process, pruning has to be 
applied. Only the n top-ranking hypotheses are 
kept in each cell of the chart. The ranking of these 
partial translations is based on their language 
model score, which at this time is only an ap-
proximation, as the true history has not been seen 
and cannot be taken into account. 
3 Building a Xfer System for a New Do-
main 
A major bottleneck in developing a RBMT system 
for a new translation task (a new language pair or a 
new domain) is writing the grammar and building 
the lexicon. Automatic grammar induction using 
statistical alignments has been studied in (Probst 
2005).   
Here, we start with an existing grammar and 
augment the baseline lexicon with entries to cover 
the new domain. We explore semi-automatic lexi-
con generation for fast adaptation to the travel do-
main (Section 3.2). 
3.1 Test Data: BTEC Corpus 
For initial evaluation on unseen data, we selected 
the Basic Travel Expression Corpus (BTEC) 
(Takezawa et al 2002), which has been used in the 
evaluation campaigns in connection with the Inter-
national Workshop on Spoken Language Transla-
tion (IWSLT 2006). Besides still being currently 
used to build real systems (Shimizu et al 2006; 
Nakamura, et al 2006), this corpus contains rela-
tively simple sentences that are comparable to the 
ones initially corrected by users, and which are 
covered by the baseline manual grammar. 
As our test set, we used 506 English sentences 
for which two sets of Spanish reference transla-
tions were available. Table 1 shows corpus statis-
tics for the BTEC data. 
 
Data  English 
Sentences Pairs 123,416 
Sentence Length   7.3 
Word Tokens  903,525 
 
Train 
Word Types  12,578 
Sentence Pairs 506 
Word Tokens 3,764 
Word Types 776 
 
 
BTEC 
 
 
 
 
 
Test  
Coverage Test 756 (97%) 
Table 1: Corpus Statistics for the BTEC corpus 
3.2 Semi-Automatic Generation of the 
Transfer Lexicon 
The Transfer-Based system relies on a lexicon that 
contains POS, gender and number agreement, 
among other linguistic features. To adjust the sys-
tem quickly to a new task, we decided to leverage 
from statistical alignment models to generate word 
and phrase alignments as candidates for the trans-
fer lexicon. 
In the first step, we trained statistical lexicons 
using the well-known IBM1 word alignment 
model: one for the directions Spanish to English, 
and one for the direction English to Spanish. As 
multi-word entries, are often needed ([valuables] 
? [objetos de valor], [reception desk] ?[recep-
ci?n], [air conditioner]?[aire acondicionado]), we 
used phrase alignment techniques to create transla-
tion candidates for words and 2-word phrases. The 
phrase alignment also generates multi-word trans-
lations for single source words. With reasonably 
tight pruning, a manageable phrase translation ta-
74
ble was generated. This first step took about 5 
hours. 
The next step, manually cleaning the translation 
table, annotating them with parts-of-speech, and 
with agreement and tense constraints, was initially 
restricted to those items that overlapped with the 
vocabulary of our development test set, and took 
two days. 
The statistically generated lexicon comprises 
1,248 lexical entries, whereas the initial manual 
lexicon contained 479 lexical entries. For our 
BTEC experiments, we combined both lexicons. 
3.3 Xfer Results with No Ranking 
To determine how the Xfer system would perform 
only on the basis of the lexicon and grammar, we 
ran one translation experiment in which no lan-
guage model was used. This experiment was also 
intended to see if the refined grammar would lead 
to better translations. We took the first-best transla-
tion output by the system without using any statis-
tical components to rank alternative translations. 
 
System METEOR BLEU NIST
Baseline 0.5666 0.2745 5.88 
Refined 0.5676 0.2559 5.62 
Table 2: Automatic metric scores for a purely 
Rule-Based MT System. 
 
Table 2 shows that, in this crude setting, differ-
ent automatic metrics do not agree on the transla-
tion accuracy of both systems. On one hand, 
METEOR (Lavie et al 2004), which has been 
shown to correlate well with human judgments 
(Snover et al 2006), indicates that the refined sys-
tem outperforms the baseline system (as measured 
by the latest version v0.5.1,).  On the other hand, 
both BLEU (Papineni et al, 2002) and NIST 
(Doddington 2002) scores are higher for the base-
line system (mteval-v11b.pl).  
However, human inspection revealed that the re-
fined grammar is able to augment the n-best list 
with correct translations that the baseline system 
was not able to generate. This suggests that these 
results reflect poor re-ranking and not n-best list 
quality. In the next section, we describe an oracle 
experiment to measure n-best list quality of both 
systems.  
3.4 Oracle Experiment 
Oracle scores provide an upper-bound in perform-
ance. For the BTEC test set, we approximated a 
human oracle by calculating automatic metric 
scores for METEOR and for BLEU and NIST. 
Given 100-best lists for each source language 
sentence, we selected the best translation hypothe-
sis for each automatic metric separately. 
These scores reflect the fact that automatic re-
finements are able to feed the n-best list with better 
translations, as evulated by comparison against 
human reference translations. Even with a small set 
of independent user corrections, the refined system 
shows potential improved translation quality as 
indicated by higher scores for all three automatic 
evaluation metrics in Table 3. 
 
System METEOR BLEU NIST
Baseline 0.6863 0.4068 7.42 
Refined 0.6954 0.4215 7.51 
Table 3: Automatic metric oracle scores based on a 
100-best list 
 
Moreover, oracle scores provide the margin that 
we can gain when improving on the re-ranking of 
the n-best list produced by the Xfer engine. 
3.5 Xfer Results with Initial Ranking 
As expected, when the Xfer system is run in com-
bination with a LM1 as well as the fragmentation 
penalty, automatic metric scores for the 1-best hy-
pothesis are significantly higher (Table 4), than 
when just using the first translation output by the 
Xfer system alone (Table 2). 
 
System METEOR BLEU NIST
Baseline 0.6176 0.3425 6.53 
Refined 0.6222 0.3513 6.56 
Table 4: Automatic metric scores for 1-best de-
coder hypothesis. 
 
These results are lower than the oracle scores for 
both the baseline and the refined system (Table 3), 
which is also to be expected. However, the impor-
tant thing to notice from these results is that, like in 
the oracle case, the refined system consistently 
outperforms the baseline MT system for all three 
automatic metrics. 
                                                     
1 The Suffix Array Language Model (SALM) was built using 
the 123,416 Spanish sentences from the training data. 
75
The difference between the baseline and the re-
fined system in terms of 1-best scores is slightly 
smaller than the difference between oracle scores, 
which means that the decoder can not fully lever-
age the improvements made in the grammar. This 
indicates that the decoder fails to select the best 
translation in most cases. 
4 Adding Statistical Components to a Re-
Ranker 
The information used in the Xfer system to rank 
alternative translations is limited.  Essentially, it is 
the n-gram LM, which is the most important com-
ponent, a simple sentence length model, and the 
fragmentation score, which measures if a com-
pletely spanning parse could be found or if the 
translation is glued together from partial parses. 
Given an n-best list of translations for each source 
sentence, we can apply additional models to re-
rank these n-best list, hopefully pushing more good 
translations into the first rank. We studied the ef-
fect of adding different features to the n-best lists: 
lexical features and rule (type) probability features. 
4.1 Word-To-Word Probabilities 
In SMT systems, rescoring with an IBM1 model-
like word alignment score has become a standard 
feature. We use two word-to-word lexicons (Eng-
lish?Spanish and Spanish?English) to calculate 
sentence translation probabilities, based on word-
to-word probabilities: 
??= )|(1)|( jiI sepJseP       Eq.1 
and: 
??= )|(1)|( ijJ espIesP       Eq.2 
 
Here, we denote the English words with e, the 
Spanish words with s, the sentence lengths are 
given by I and J.  In the IBM1 alignment model, 
the position alignment is a uniform distribution p( i 
| j ) = 1/I for Spanish to English and p( j | i ) = 1/J 
for English to Spanish.  For Spanish to English, we 
have the additional factor of (1/I)J, i.e. longer 
translations get a smaller probability, and for En-
Sp we have (1/J)I, which again gives a bias to-
wards shorter translations.  To compensate for this 
bias, we use probabilities normalized to the sen-
tence length. Table 5 shows that adding the lexical 
probabilities improves the 1-best translation score.  
However, there is no significant difference when 
using different normalization of the lexicon prob-
abilities. The length bias introduced by different 
lexicon features can be balanced by the decoder?s 
length feature. 
 
 BLEU NIST 
Refined 0.3513 6.56 
+Lex Prob 0.3755 6.88 
Table 5: Comparing 1-best scores with scores 
result of rescoring the n-best list with lexical fea-
tures. 
4.2 Rule Probabilities 
The Xfer MT system can display the derivation 
tree showing the rules applied during translation. 
This allows rescoring the translations with rule 
probabilities. However, there is no annotated cor-
pus from which the rule probabilities could be es-
timated. As an approximation to such a training 
corpus, we decided to run the Xfer system over the 
training data and to generate n-best lists with trans-
lations and translation trees. Overall, about 6 mil-
lion parse trees were generated.  Using this data to 
estimate rule probabilities is definitely not ideal, as 
the translation on the training data are far from per-
fect, especially as not all the vocabulary has so far 
been added to the Xfer lexicon.  By averaging over 
all n-best translations a reasonable smoothing is to 
be expected. 
We used this information in three ways.  We es-
timated conditional probabilities rule r given rule-
type R, i.e. the distribution over different VP rules 
or NP rules. For each derivation D the overall 
probability was then calculated as: 
?= )|()( RrpDP                             Eq. 3 
As an alternative, we just build n-gram language 
models, one on the rule level and on the rule type 
level: 
? ??= )...|()( 1rrrpDP n                      Eq. 4 
? ??= )...|()( 1RRRpDP n                   Eq. 5 
 
Overall, 1,685 different rules and 19 rule types 
were seen in the training data. For models 2 and 3, 
we used the suffix array LM once again to allow 
for arbitrary long histories. Even though it often 
backs-off to 3-gram, 2-gram or even unigram prob-
abilities. 
76
In Table 6, we can see the effect of adding these 
LMs as additional features to the system and run-
ning MER training. 
 
 BLEU NIST
Refined 0.3513 6.56 
Lex. Prob. 0.3755 6.88 
Cond. Prob. 0.3728 6.81 
Rule LM 0.3717 6.74 
Rule Type LM 0.3736 6.78 
Table 6: BLEU scores when rescoring the n-
best list with different rule probability features (as 
well as the n-gram LM). 
5 MER Training 
Like in SMT systems, in the Xfer engine transla-
tions are ranked to their total cost, which is a 
weighted linear combination of the individual 
costs. When adding more features to the translation 
system, a careful balancing of the individual con-
tributions can make a significant difference. How-
ever, with each feature added, manually tuning the 
system becomes less and less practical, and auto-
matic optimization becomes necessary. 
Different optimization techniques are available, 
like the Simplex algorithm or the special Minimum 
Error Training as described in (Och 2003). In 
Minimum Error Rate (MER) training, the n-best 
list generated by the translation system is used to 
find feature weight, thereby re-ranking the n-best 
list. This improves the match between the 1-best 
translation and given reference translations. Opti-
mization can use any metric as objective function.  
Typically, systems are tuned towards high BLEU 
or high NIST scores, more recently also towards 
METEOR or TER (Snover et al 2006). 
We used a MER training module (Venugopal), 
originally developed for an SMT system, to run 
MER training on the n-best lists generated by the 
Xfer system. This implementation allows for opti-
mization towards BLEU and NIST mteval metrics. 
5.1 Results 
In Table 7, we summarize some of the results from 
different n-best list rescoring experiments.  Using 
only the Xfer engine, without language model, 
gives a very low score, as the selection is based 
only on the fragmentation score. 
Adding the n-gram language model gives a huge 
improvement. Adding additional features leads to 
more then 2 BLEU points improvement. However, 
there is not much difference when using different 
feature combinations. It seems that the rather small 
size of the n-best list is a limiting factor.  
When setting the optimal weights in the Xfer 
engine for the LM and fragmentation penalty 
scores obtained from MER training, both the base-
line and the refined system get higher scores, not 
only according to BLEU, which was used as the 
objective function, but also according to METEOR 
and NIST automatic evaluation metrics (Table 8). 
 
 System + Statistical Components 1-best 
Rule Based Xfer 0.2559 
+ Stat. Comp. Xfer + LM + Frag 0.3513 
POS LM 0.3180 
Rule Probabilities (Prob.) 0.2593 
LM + Rule Type LM 0.3736 
LM + Frag/Len + Rule Type LM 0.3737 
LM + POS + Rule LM 0.3744 
LM + Frag + Rule Type LM + Cond. Rule Prob. 0.3743 
LM + Len + Rule Type LM + Cond. Rule Prob. 0.3745 
LM + POS + Rule LM + Cond. Rule Prob. 0.3741 
LM + Frag + Len + Rule Type LM + Rule Prob. 0.3746 
 
 
 
Optimizing 
weights 
with 
MER training 
LM + Frag + Len + POS + Rule LM + Rule Prob. 0.3741 
Table 7:  BLEU scores for the Refined MT System as the weights for the different statistical components 
described in Section 2.4 and 4 are optimized with MER Training. 
 
77
Moreover, the difference between the Baseline 
and the Refined system after MER training is sta-
tistically significant2, whereas this was not the case 
for the initial ranking results (Table 4). 
 
 
 
Table 8: Automatic metric scores for 1-best de-
coder hypothesis, after LM and Fragmentation 
weights have been optimized. 
 
Table 9 shows a few examples from the BTEC cor-
pus with 1-best translations output by the Refined 
MT system before (No Optimization) and after 
(With Optimization) MER training, given LM and 
Fragmentation penalty scores. From these exam-
ples, it can be observed that re-ranking improves 
after optimizing the LM and fragmentation 
weights. In particular, order issues get resolved 
(examples 1, 2 and 4), which result in correct de-
terminer agreement (1 and 2); determiner insertion 
(3); correct verb form (5 and 7) and omission of 
incorrect pronouns (6 and 7).   
6 Conclusion 
Starting from a Transfer-Based translation system, 
we explored techniques currently used in statistical 
translation systems to rapidly adapt to a new do-
main and to improve its performance.  Using word 
and phrase alignment techniques allowed us to 
quickly augment the transfer lexicon. Adding a 
statistical language model is crucial in selecting 
good translations from the n-best lists generated by 
the Xfer engine. Adding additional features, such 
as word-to-word probabilities and rule (type) prob-
abilities, further improves performance. 
While this information would ideally be used in 
the parsing and transfer steps of the translation sys-
tem, our initial experiments were targeted at using 
this in an n-best list rescoring setup. As rule prob-
abilities were estimated from noisy training data, 
these models are far from optimal.   
To facilitate the experiments with the Xfer sys-
tem, especially when adding more and more fea-
tures, we added a Minimum Error Rate training 
                                                     
2 According to the standard paired two-tailed t-Test, the de-
coder METEOR scores with optimized weights are statisti-
cally significant, with a p value of 0.0051. 
component. Having such a component will defi-
nitely boost the development of the Xfer engine. 
We see statistically significant improvements 
over the baseline system when using optimized 
weights for the word-level language model and the 
fragmentation score.  
System METEOR BLEU NIST
Baseline 0.6184 0.3609 6.68 
Refined 0.6231 0.3780  6.79  1 Source: where is the boarding gate ?   
   NO: d?nde est? el embarque puerta ? 
   WO: d?nde est? la puerta embarque ? 
2 Src: where is the bus stop for city hall ? 
  NO: d?nde est? el autob?s parada para ayuntamiento ? 
  WO: d?nde est? la parada autob?s para ayuntamiento ? 
3 Src: i would like a twin room with a bath please . 
   NO: me gustar?a habitaci?n una cama doble con un 
           ba?o por favor . 
   WO: me gustar?a una habitaci?n cama doble con un 
            ba?o por favor . 
4 Src:  i would like to buy some duty-free items . 
  NO: me gustar?a  comprar algunos duty-free productos. 
  WO: me gustar?a  comprar algunos art?culos duty-free . 
5 Src: does he speak japanese ? 
   NO: ?l hablar a japon?s ? 
   WO: habla japon?s ? 
6 Src: it is just round the corner . 
   NO: lo es simplemente a la vuelta de la esquina . 
   WO: es simplemente a la vuelta de la esquina . 
7  Src: do you sell duty-free items ?   
    NO: te venden art?culos duty-free ? 
    WO: vend?is art?culos duty-free ? 
Table 9: 1-best translations from the BTEC test set 
output by the Refined MT system before and after 
MER training. NO stands for No Optimization of 
LM and Fragmentation weights, and WO stands 
for With Optimization of weights. 
7 Future Work 
Using rule probabilities has shown to be a promis-
ing extension to the current Xfer system.  We plan 
to improve these models by selecting the oracle 
best translations from the n-best list generated on 
the training data. This will reduce the noise in the 
training stage. Ultimately, the rule probabilities 
should be applied not as an n-best list rescoring 
step, but directly in the Xfer engine decoder. 
Analyzing the translation results, one important 
shortcoming became obvious. Currently the trans-
lation lexicon only covers about 88% of the words 
that appear in the reference translations. This se-
verely limits as to what kind of BLEU score we 
can achieve. When we generated the phrasal lexi-
con from the BTEC training data, we deliberately 
78
chose to only include few alternatives, mainly to 
limit the manual labor when adding POS and con-
straint. We expect that the Xfer system will sig-
nificantly benefit from further expanding the 
lexicon. 
References 
Allen, J. 1995. Natural Language Understanding. Sec-
ond Edition ed. Benjamin Cummings. 
 Chiang, D. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proceedings of 
the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL), Ann Arbor, USA. 
Doddington G. 2002. Automatic evaluation of machine 
translation quality using n-gram co-occurrence sta-
tistics. In Proc. of the HLT 2002, San Diego, USA. 
Hutchins, W. J., and H. L. Somers. 1992. An Introduc-
tion to Machine Translation. London: Academic 
Press. 
Font Llitj?s, A. and W. Ridmann. 2007 The Inner 
Works of an Automatic Rule Refiner for Machine 
Translation. METIS-II Workshop, Leuven, Belgium. 
IWSLT 2006: http://www.slt.atr.jp/IWSLT2006/
Lavie, A., K. Sagae and S. Jayaraman. 2004. The Sig-
nificance of Recall in Automatic Metrics for MT 
Evaluation. AMTA, Washington DC, USA. 
Nakamura, S., K. Markov, H. Nakaiwa, G. Kikui, H. 
Kawai, T. Jitsuhiro, J. Zhang, H. Yamamoto, E. 
Sumita, and S. Yamamoto. 2006. The ATR multilin-
gual speech-to-speech translation system. IEEE 
Trans. on Audio, Speech, and Language Processing, 
14, No.2:365?376. 
NIST MT Evaluations: 
http://www.nist.gov/speech/tests/mt/
Och, F. J. 2003. Minimum error rate training in statisti-
cal machine translation.  In Proc. of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics (ACL), Sapporo, Japan. 
Papineni, K, S. Roukos, T. Ward, and W. Zhu. 2002. 
BLEU: a method for automatic evaluation of ma-
chine translation. In Proc. of the 40th ACL, Phila-
delphia, USA. 
Peterson, E. 2002. Adapting a transfer engine for rapid 
machine translation development. M.S. Thesis, 
Georgetown University. 
Probst, K., Brown, R., Carbonell, J., Lavie, A. Levin, 
and L., Peterson, E., 2001. Design and Implementa-
tion of Controlled Elicitation for Machine Transla-
tion of Low density Languages. Proceedings of the 
MT2001 workshop at MT Summit, Santiago de 
Compostela, Spain. 
SALM Toolkit: 
http://projectile.is.cs.cmu.edu/research/public/tools/s
alm/salm.htm 
Shimizu T., Y. Ashikari, E. Sumita, H. Kashioka and  S. 
Nakamura. 2006. Development of client-server 
speech translation system on a multi-lingual speech 
communication platform. IWSLT, Kyoto, Japan. 
Snover, M; B. Dorr, R. Schwartz, L. Micciulla, 2006. 
Targeted Human Annotation. AMTA, Boston, USA.  
Takezawa, T, E. Sumita, F. Sugaya, H. Yamamoto, and 
S. Yamamoto, 2002. Toward a Broad-Coverage Bi-
lingual Corpus for Speech Translation of Travel 
Conversations in the Real World. In Proceedings of 
3rd LREC, Las Palmas, Spain. 
TC-STAR Evaluations: http://www.tc-star.org/
Venugopal, A.:  MER Training Toolkit. 
http://www.cs.cmu.edu/~ashishv/mer.html
Wu, D. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23:377?404. 
Yamada, Kenji and Kevin Knight. 2001. A syntax-based 
statistical translation model. In Proceedings of the 
39th Annual Meeting of the ACL, Toulouse, France. 
Zhang, Y and S. Vogel. 2006. Suffix Array and its Ap-
plications in Empirical Natural Language Process-
ing,. Technical Report CMU-LTI-06-010, Pittsburgh 
PA, USA. 
Zhang, Y, A. S. Hildebrand and S. Vogel. 2006.  Dis-
tributed Language Modeling for N-best List Re-
ranking. Empirical Methods in Natural Language 
Processing (EMNLP), Sydney, Australia.  
Zollmann A. and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In 
Proc. of NAACL 2006 - Workshop on Statistical 
Machine Translation, New York, USA. 
79
Proceedings of the Second Workshop on Statistical Machine Translation, pages 197?202,
Prague, June 2007. c?2007 Association for Computational Linguistics
The ISL Phrase-Based MT System for the 2007 ACL Workshop on
Statistical Machine Translation
M. Paulik1,2, K. Rottmann2, J. Niehues2, S. Hildebrand 1,2 and S. Vogel1
1Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, USA
2Institut fu?r Theoretische Informatik, Universita?t Karlsruhe (TH), Karlsruhe, Germany
{paulik|silja|vogel}@cs.cmu.edu ; {jniehues|rottmann}@ira.uka.de
Abstract
In this paper we describe the Interactive Sys-
tems Laboratories (ISL) phrase-based ma-
chine translation system used in the shared
task ?Machine Translation for European
Languages? of the ACL 2007 Workshop on
Statistical Machine Translation. We present
results for a system combination of the
ISL syntax-augmented MT system and the
ISL phrase-based system by combining and
rescoring the n-best lists of the two systems.
We also investigate the combination of two
of our phrase-based systems translating from
different source languages, namely Spanish
and German, into their common target lan-
guage, English.
1 Introduction
The shared task of the ACL 2007 Workshop on Sta-
tistical Machine Translation focuses on the auto-
matic translation of European language pairs. The
workshop provides common training sets for trans-
lation model training and language model training
to allow for easy comparison of results between the
participants.
Interactive Systems Laboratories participated in the
English ? Spanish Europarl and News Commen-
tary task as well as in the English ? German Eu-
roparl task. This paper describes the phrase-based
machine translation (MT) system that was applied
to these tasks. We also investigate the feasibility
of combining the ISL syntax-augmented MT system
(Zollmann et al, 2007) with our phrase-based sys-
tem by combining and rescoring the n-best lists pro-
duced by both systems for the Spanish ? English
Europarl task. Furthermore, we apply the same com-
bination technique to combine two of our phrase-
based systems that operate on different source lan-
guages (Spanish and German), but share the same
target language (English).
The paper is organized as follows. In section 2 we
give a general description of our phrase-based sta-
tistical machine translation system. Section 3 gives
an overview of the data and of the final systems
used for the English ? Spanish Europarl and News
Commentary tasks, along with corresponding per-
formance numbers. Section 4 shows the data, final
systems and results for the English ? German Eu-
roparl task. In Section 5, we present our experiments
involving a combination of the syntax-augmented
MT system with the phrase-based MT system and a
combination of the Spanish ? English and German
? English phrase-based systems.
2 The ISL Phrase-Based MT System
2.1 Word and Phrase Alignment
Phrase-to-phrase translation pairs are extracted by
training IBM Model-4 word alignments in both di-
rections, using the GIZA++ toolkit (Och and Ney,
2000), and then extracting phrase pair candidates
which are consistent with these alignments, start-
ing from the intersection of both alignments. This
is done with the help of phrase model training
code provided by University of Edinburgh during
the NAACL 2006 Workshop on Statistical Machine
Translation (Koehn and Monz, 2006). The raw rel-
197
ative frequency estimates found in the phrase trans-
lation tables are then smoothed by applying modi-
fied Kneser-Ney discounting as explained in (Foster
et al, 2006). The resulting phrase translation tables
are pruned by using the combined translation model
score as determined by Minimum Error Rate (MER)
optimization on the development set.
2.2 Word Reordering
We apply a part-of-speech (POS) based reordering
scheme (J. M. Crego et al, 2006) to the POS-tagged
source sentences before decoding. For this, we use
the GIZA++ alignments and the POS-tagged source
side of the training corpus to learn reordering rules
that achieve a (locally) monotone alignment. Fig-
ure 1 shows an example in which three reordering
rules are extracted from the POS tags of an En-
glish source sentence and its corresponding Span-
ish GIZA++ alignment. Before translation, we con-
struct lattices for every source sentence. The lattices
include the original source sentence along with all
the reorderings that are consistent with the learned
rules. All incoming edges of the lattice are anno-
tated with distortion model scores. Figure 2 gives an
example of such a lattice. In the subsequent lattice
decoding step, we apply either monotone decoding
or decoding with a reduced local reordering window,
typically of size 2.
2.3 Decoder and MER Training
The ISL beam search decoder (Vogel, 2003) com-
bines all the different model scores to find the best
translation. Here, the following models were used:
? The translation model, i.e. the phrase-to-
phrase translations extracted from the bilingual
corpus, annoted with four translation model
scores. These four scores are the smoothed for-
ward and backward phrase translation proba-
bilities and the forward and backward lexical
weights.
? A 4-gram language model. The SRI language
model toolkit was used to train the language
model and we applied modified Kneser-Ney
smoothing.
? An internal word reordering model in addition
to the already described POS-based reordering.
  
We all agree on thatPRP DT VB IN DTEn {4} esto {5} estamos {1} todos {2} de {} acuerdo {3}
? PRP DT VB IN DT :   4 ? 5 ? 1 ? 2 ? 3? PRP DT VB:   2 ? 3 ? 1 ? PRP DT VB IN:   3 ? 4 ? 1 ? 2
Figure 1: Rule extraction for the POS-based reorder-
ing scheme.
This internal reordering model assigns higher
costs to longer distance reordering.
? Simple word and phrase count models. The
former is essentially used to compensate for
the tendency of the language model to prefer
shorter translations, while the latter can give
preference to longer phrases, potentially im-
proving fluency.
The ISL SMT decoder is capable of loading
several language models (LMs) at the same time,
namely n-gram SRI language models with n up to
4 and suffix array language models (Zhang and Vo-
gel, 2006) of arbitrary length. While we typically
see gains in performance for using suffix array LMs
with longer histories, we restricted ourselves here to
one 4-gram SRI LM only, due to a limited amount
of available LM training data. The decoding process
itself is organized in two stages. First, all available
word and phrase translations are found and inserted
into a so-called translation lattice. Then the best
combination of these partial translations is found
by doing a best path search through the translation
lattice, where we also allow for word reorderings
within a predefined local reordering window.
To optimize the system towards a maximal BLEU
or NIST score, we use Minimum Error Rate (MER)
Training as described in (Och, 2003). For each
model weight, MER applies a multi-linear search
on the development set n-best list produced by the
system. Due to the limited numbers of translations
in the n-best list, these new model weights are sub-
optimal. To compensate for this, a new full trans-
lation is done. The resulting new n-best list is then
merged with the old n-best list and the optimization
process is repeated. Typically, the translation quality
converges after three iterations.
198
1
20 3
honourable1.0000
Members1.0000 honourable0.3299
Members0.6701 6
75 8
we1.0000
have1.0000
have0.9175
a0.08254,1.0000 a1.0000
?
?Honourable Members, we have a challenging agenda?
Figure 2: Example for a source sentence lattice from
the POS-based reordering scheme.
English Spanish
sentence pairs 1259914
unique sent. pairs 1240151
sentence length 25.3 26.3
words 31.84 M 33.16 M
vocabulary 266.9 K 346.3 K
Table 1: Corpus statistics for the English/Spanish
Europarl corpus.
3 Spanish? English Europarl and News
Commentary Task
3.1 Data and Translation Tasks
The systems for the English ? Spanish translation
tasks were trained on the sentence-aligned Europarl
corpus (Koehn, 2005). Detailed corpus statistics can
be found in Table 1. The available parallel News
Commentary training data of approximately 1 mil-
lion running words for both languages was only
used as additional language model training data, to
adapt our in-domain (Europarl) system to the out-of-
domain (News Commentary) task.
The development sets consist of 2000 Europarl
sentences (dev-EU) and 1057 News Commentary
sentences (dev-NC). The available development-
test data consists of 2 x 2000 Europarl sentences
(devtest-EU and test06-EU) and 1064 News Com-
mentary sentences (test06-NC). All development
and development-test sets have only one reference
translation per sentence.
3.2 Data Normalization
The ACL shared task is very close in form and con-
tent to the Final Text Editions (FTE) task of the TC-
STAR (TC-STAR, 2004) evaluation. For this rea-
son, we decided to apply a similar normalization
scheme to the training data as was applied in our TC-
STAR verbatim SMT system. Although trained on
?verbatimized? data that did not contain any num-
bers, but rather had all numbers and dates spelled
out, it yielded consistently better results than our
TC-STAR FTE SMT system. When translating FTE
content, the verbatim system treated all numbers as
unknown words, i.e. they were left unchanged dur-
ing translation. To compensate for this, we applied
extended postprocessing to the translations that con-
ducts the necessary conversions between Spanish
and English numbers, e.g. the conversion of deci-
mal comma in Spanish to decimal point in English.
Other key points which we adopted from this nor-
malization scheme were the tokenization of punc-
tuation marks, the true-casing of the first word of
each sentence, as well as extended cleaning of the
training data. The latter mainly consisted of the re-
moval of sections with a highly unbalanced source
to target words ratio and the removal of unusual
string combinations and document references, like
for example ?B5-0918/2000?, ?(COM(2000) 335 -
C5-0386/2000 - 2000/0143(CNS))?, etc.
Based on this normalization scheme, we trained and
optimized a baseline in-domain system on accord-
ingly normalized source and reference sentences.
For optimization, we combined the available de-
velopment sets for the Europarl task and the News
Commentary task. In order to further improve
the applied normalization scheme, we experimented
with replacing all numbers with the string ?NMBR?,
rather than spelling them out and by replacing all
document identifiers with the string ?DCMNT?,
rather than deleting them. This was first done for
the language model training data only, and then for
all data, i.e. for the bilingual training data and for
the development set source and reference sentences.
In the latter case, the respective tags were again re-
placed by the correct numbers and document identi-
fiers during postprocessing. Table 2 shows the case
sensitive BLEU scores for the three normalization
approaches on the English ? Spanish Europarl and
News Commentary development sets. These scores
were computed with the official NIST scoring script
against the original (not normalized) references.
3.3 In-domain System
As mentioned above, we combined the Europarl and
News Commentary development sets when optimiz-
ing the in-domain system. This resulted in only one
199
Task baseline LM only all data
Europarl 30.94 31.20 31.26
News Com. 31.28 31.39 31.73
Table 2: Case sensitive BLEU scores on the in-
domain and out-of-domain development sets for the
three different normalization schemes.
Task Eng ? Spa Spa ? Eng
dev-EU 31.29 31.77
dev-NC 31.81 31.12
devtest-EU 31.01 31.40
test06-EU 31.87 31.76
test06-NC 30.23 29.22
Table 3: Case sensitive BLEU scores for the final
English ? Spanish in-domain systems.
set of scaling factors, i.e. the in-domain system
applies the same scaling factors for translating in-
domain data as for translating out-of-domain data.
Our baseline system applied only monotone lattice
decoding. For our final in-domain system, we used a
local reordering window of length 2, which accounts
for the slightly higher scores when compared to the
baseline system. The BLEU scores for both trans-
lation directions on the different development and
development-test sets can be found in Table 3.
3.4 Out-of-domain System
In order to adapt our in-domain system towards the
out-of-domain News Commentary task, we consid-
ered two approaches based on language model adap-
tation. First, we interpolated the in-domain LM
with an out-of-domain LM computed on the avail-
able News Commentary training data. The inter-
polation weights were chosen such as to achieve a
minimal LM perplexity on the out-of-domain de-
velopment set. For both languages, the interpo-
lation weights were approximately 0.5. Our sec-
ond approach was to simply load the out-of-domain
LM as an additional LM into our decoder. In both
cases, we optimized the translation system on the
out-of-domain development data only. For the sec-
ond approach, MER optimization assigned three to
four times higher scaling factors to the consider-
ably smaller out-domain LM than to the original in-
domain LM. Table 4 shows the results in BLEU on
the out-of-domain development and development-
test sets for both translation directions. While load-
Eng ? Spa Spa ? Eng
Task interp 2 LMs interp 2 LMs
dev-NC 33.31 33.28 32.61 32.70
test06-NC 32.55 32.15 30.73 30.55
Table 4: Case sensitive BLEU scores for the final
English ? Spanish out-of-domain systems.
ing a second LM gives similar or slightly better re-
sults on the development set during MER optimiza-
tion, we see consistently worse results on the unseen
development-test set. This, in the context of the rela-
tively small amount of development data, can be ex-
plained by stronger overfitting during optimization.
4 English? German Europarl Task
The systems for the English ? German translation
tasks were trained on the sentence-aligned Europarl
corpus only. The complete corpus consists of ap-
proximately 32 million English and 30 million Ger-
man words.
We applied a similar normalization scheme to the
training data as for the English ? Spanish system.
The main difference was that we did not replace
numbers and that we removed all document refer-
ences. In the translation process, the document ref-
erences were treated as unknown words and there-
fore left unchanged. As above, we trained and op-
timized a first baseline system on the normalized
source and reference sentences. However, we used
only the Europarl task development set during opti-
mization. To achieve further improvements on the
German ? English task, we applied a compound
splitting technique. The compound splitting was
based on (Koehn and Knight, 2003) and was applied
on the lowercased source sentences. The words gen-
erated by the compound splitting were afterwards
true-cased. Instead of replacing a compound by
its separate parts, we added a parallel path into the
source sentence lattices used for translation. The
source sentence lattices were augmented with scores
on their edges indicating whether each edge repre-
sents a word of the original text or if it was gener-
ated during compound splitting.
Table 5 shows the case-sensitive BLEU scores for
the final German ? English systems. In contrast
to the English ? Spanish systems, we used only
monotonous decoding on the lattices containing the
200
task Eng ? Ger Ger ? Eng
dev-EU 18.58 23.85
devtest-EU 18.50 23.87
test06-EU 18.39 23.88
Table 5: Case sensitive BLEU scores for the final
English ? German in-domain systems.
syntactical reorderings.
5 System Combination via n-best List
Combination and Rescoring
5.1 N-best List Rescoring
For n-best list rescoring we used unique 500-best
lists, which may have less than 500 entries for
some sentences. In this evaluation, we used sev-
eral features computed from different information
sources such as features from the translation sys-
tem, additional language models, IBM-1 word lex-
ica and the n-best list itself. We calculated 4 fea-
tures from the IBM-1 word lexica: the word proba-
bility sum as well as the maximum word probabil-
ity in both language directions. From the n-best list
itself, we calculated three different sets of scores.
A position-dependent word agreement score as de-
scribed in (Ueffing and Ney, 2005) with a position
window instead of the Levenshtein alignment, the
n-best list n-gram probability as described in (Zens
and Ney, 2006) and a position-independent n-gram
agreement, which is a variation on the first two. To
tune the feature combination weights, we used MER
optimization.
Rescoring the n-best lists from our individual sys-
tems did not give significant improvements on the
available unseen development-test data. For this rea-
son, we did not apply n-best list rescoring to the indi-
vidual systems. However, we investigated the feasi-
bility of combining two different systems by rescor-
ing the joint n-best lists of both systems. The corre-
sponding results are described in the following sec-
tions.
5.2 Combining Syntax-Augmented MT and
Phrase-Based MT
On the Spanish ? English in-domain task, we par-
ticipated not only with the ISL phrase-based SMT
system as described in this paper, but also with
the ISL syntax-augmented system. The syntax-
task PHRA SYNT COMB
dev-EU 31.77 32.48 32.77
test06-EU 31.76 32.15 32.27
Table 6: Results for combining the syntax-
augmented system (SYNT) with the phrase-based
system (PHRA).
augmented system was trained on the same normal-
ized data as the phrase-based system. However, it
was optimized on the in-domain development set
only. More details on the syntax-augmented system
can be found in (Zollmann et al, 2007). Table 6
lists the respective BLEU scores of both systems as
well as the BLEU score achieved by combining and
rescoring the individual 500-best lists.
5.3 Combining MT Systems with Different
Source Languages
(Och and Ney, 2001) describes methods for trans-
lating text given in multiple source languages into a
single target language. The ultimate goal is to im-
prove the translation quality when translating from
one source language, for example English into mul-
tiple target languages, such as Spanish and German.
This can be done by first translating the English doc-
ument into German and then using the translation as
an additional source, when translating to Spanish.
Another scenario where a multi-source translation
becomes desirable was described in (Paulik et al,
2005). The goal was to improve the quality of au-
tomatic speech recognition (ASR) systems by em-
ploying human-provided simultaneous translations.
By using automatic speech translation systems to
translate the speech of the human interpreters back
into the source language, it is possible to bias the
source language ASR system with the additional
knowledge. Having these two frameworks in mind,
we investigated the possibility of combining our in-
domain German ? English and Spanish ? English
translation systems using n-best list rescoring. Ta-
ble 7 shows the corresponding results. Even though
the German ? English translation performance was
approximately 8 BLEU below the translation perfor-
mance of the Spanish ? English system, we were
able to improve the final translation performance by
up to 1 BLEU.
201
task Spa ? Eng Ger ? Eng Comb.
dev-EU 31.77 23.85 32.76
devtest-EU 31.40 23.87 32.41
test06-EU 31.76 23.88 32.51
Table 7: Results for combining the Spanish ? En-
glish and German ? English phrase-based systems
on the in-domain tasks.
6 Conclusion
We described the ISL phrase-based statistical ma-
chine translation systems that were used for the 2007
ACL Workshop on Statistical Machine Translation.
Using the available out-of-domain News Commen-
tary task training data for language model adapta-
tion, we were able to significantly increase the per-
formance on the out-of-domain task by 2.3 BLEU
for English ? Spanish and by 1.3 BLEU for Span-
ish ? English. We also showed the feasibility of
combining different MT systems by combining and
rescoring their resprective n-best lists. In particular,
we focused on the combination of our phrase-based
and syntax-augmented systems and the combination
of two phrase-based systems operating on different
source languages. While we saw only a minimal im-
provement of 0.1 BLEU for the phrase-based and
syntax-augmented combination, we gained up to 1
BLEU, in case of the multi-source translation.
References
G. Foster, R. Kuhn, and H. Johnson. 2006. Phrasetable
Smoothing for Statistical Machine Translation. In
Proc. of Empirical Methods in Natural Language Pro-
cessing, Sydney, Australia.
J. M. Crego et al 2006. N-gram-based SMT System
Enhanced with Reordering Patterns. In Proc. of the
Workshop on Statistical Machine Translation, pages
162?165, New York, USA.
P. Koehn and K. Knight. 2003. Empirical Methods for
Compound Splitting. In Proc. of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 187?193, Budapest, Hungary.
P. Koehn and C. Monz. 2006. Manual and Automatic
Evaluation of Machine Translation between European
Langauges. In Proc. of the Workshop on Statisti-
cal Machine Translation, pages 102?121, New York,
USA.
P. Koehn. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation. In Proc. of Machine Trans-
lation Summit.
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proc. of the 38th Annual Meet-
ing of the Association for Computational Linguistics,
Hongkong, China.
F. J. Och and H. Ney. 2001. Statistical Multi-Source
Translation. In Proc. of Machine Translation Summit,
pages 253?258, Santiago de Compostela, Spain.
F. J. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In Proc. of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 160 ? 167, Sapporo, Japan.
M. Paulik, S. Stueker, C. Fuegen, T. Schultz, T. Schaaf,
and A. Waibel. 2005. Speech Translation Enhanced
Automatic Speech Recognition. In Proc. of the Work-
shop on Automatic Speech Recognition and Under-
standing, San Juan, Puerto Rico.
TC-STAR. 2004. Technology and Corpora for Speech to
Speech Translation. http://www.tc-star.org.
N. Ueffing and H. Ney. 2005. Word-Level Con-
fidence Estimation for Machine Translation using
Phrase-Based Translation Models. In Proc. of HLT
and EMNLP, pages 763?770, Vancouver, British
Columbia, Canada.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Proc. of Int. Conf. on Natural Lan-
guage Processing and Knowledge Engineering, Bei-
jing, China.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 72?77, New York, USA.
Y. Zhang and S. Vogel. 2006. Suffix Array and its Ap-
plications in Empirical Natural Language Processing.
In the Technical Report CMU-LTI-06-010, Pittsburgh,
USA.
A. Zollmann, A. Venugopal, M. Paulik, and S. Vogel.
2007. The Syntax Augmented MT (SAMT) system
at the Shared Task for the 2007 ACL Workshop on
Statistical Machine Translation. In Proc. of ACL 2007
Workshop on Statistical MachineTranslation, Prague,
Czech Republic.
202
Proceedings of the Third Workshop on Statistical Machine Translation, pages 18?25,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Discriminative Word Alignment via Alignment Matrix Modeling
Jan Niehues
Institut fu?r Theoretische Informatik
Universita?t Karlsruhe (TH)
Karlsruhe, Germany
jniehues@ira.uka.de
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
stephan.vogel@cs.cmu.edu
Abstract
In this paper a new discriminative word align-
ment method is presented. This approach
models directly the alignment matrix by a con-
ditional random field (CRF) and so no restric-
tions to the alignments have to be made. Fur-
thermore, it is easy to add features and so all
available information can be used. Since the
structure of the CRFs can get complex, the in-
ference can only be done approximately and
the standard algorithms had to be adapted. In
addition, different methods to train the model
have been developed. Using this approach the
alignment quality could be improved by up
to 23 percent for 3 different language pairs
compared to a combination of both IBM4-
alignments. Furthermore the word alignment
was used to generate new phrase tables. These
could improve the translation quality signifi-
cantly.
1 Introduction
In machine translation parallel corpora are one very
important knowledge source. These corpora are of-
ten aligned at the sentence level, but to use them
in the systems in most cases a word alignment is
needed. Therefore, for a given source sentence fJ1
and a given target sentence eI1 a set of links (j, i) has
to be found, which describes which source word fj
is translated into which target word ei.
Most SMT systems use the freely available
GIZA++-Toolkit to generate the word alignment.
This toolkit implements the IBM- and HMM-
models introduced in (Brown et al, 1993; Vogel et
al., 1996). They have the advantage that they are
trained unsupervised and are well suited for a noisy-
channel approach. But it is difficult to include addi-
tional features into these models.
In recent years several authors (Moore et al,
2006; Lacoste-Julien et al, 2006; Blunsom and
Cohn, 2006) proposed discriminative word align-
ment frameworks and showed that this leads to im-
proved alignment quality. In contrast to generative
models, these models need a small amount of hand-
aligned data. But it is easy to add features to these
models, so all available knowledge sources can be
used to find the best alignment.
The discriminative model presented in this pa-
per uses a conditional random field (CRF) to model
the alignment matrix. By modeling the matrix no
restrictions to the alignment are required and even
n:m alignments can be generated. Furthermore, this
makes the model symmetric, so the model will pro-
duce the same alignment no matter which language
is selected as source and which as target language.
In contrast, in generative models the alignment is a
function where a source word aligns to at most one
target word. So the alignment is asymmetric.
The training of this discriminative model has to be
done on hand-aligned data. Different methods were
tested. First, the common maximum-likelihood ap-
proach was used. In addition to this, a method to
optimize the weights directly towards a word align-
ment metric was developed.
The paper is structured as follows: Section 2 and
3 present the model and the training. In Section 4
the model is evaluated in the word alignment task as
well as in the translation task. The related work and
the conclusion are given in Sections 5 and 6.
18
Figure 1: Alignment Example
2 The Model
In the approach presented here the word alignment
matrix is modeled by a conditional random field
(CRF). A CRF is an unidirectional graphical model.
It models the conditional distribution over random
variables. In most applications like (Tseng et al,
2005; Sha and Pereira, 2003), a sequential model is
used. But to model the alignment matrix the graphi-
cal structure of the model is more complex.
The alignment matrix is described by a random
variable yji for every source and target word pair
(fj , ei). These variables can have two values, 0
and 1, indicating whether these words are transla-
tions of each other or not. An example is shown
in Figure 1. Gray circles represent variables with
value 1, white circles stand for variables with value
0. Consequently, a word with zero fertility is indi-
rectly modeled by setting all associated random vari-
ables to a value of 0.
The structure of the CRF is described by a fac-
tored graph like it was done, for example, in (Lan
et al, 2006). In this bipartite graph there are two
different types of nodes. First, there are hidden
nodes, which correspond to the random variables.
The second type of nodes are the factored nodes c
. These are not drawn in Figure 1 to keep the pic-
ture clear, but they are shown in Figure 2. They
define a potential ?c on the random variables Vc
they are connected to. This potential is used to
describe the probability of an alignment based on
the information encoded in the features. This po-
tential is a log-linear combination of some features
Fc(Vc) = (f1(Vc), . . . , fn(Vc)) and it can be written
as:
?c(Vc) = exp(? ? Fc(Vc)) = exp(
?
k
?k ? fk(Vc))
(1)
with the weights ?. Then the probability of an
assignment of the random variables, which corre-
sponds to a word alignment, can be expressed as:
p?(y|e, f) =
1
Z(e, f)
?
c?VFN
?c(Vc) (2)
with VFN the set of all factored nodes in the graph,
and the normalization factor Z(e, f) defined as:
Z(e, f) =
?
Y
?
c?VFN
?c(Vc) (3)
where Y is the set of all possible alignments.
In the presented model there are four different
types of factored nodes corresponding to four groups
of features.
2.1 Features
One main advantage of the discriminative frame-
work is the ability to use all available knowledge
sources by introducing additional features. Differ-
ent features have been developed to capture different
aspects of the word-alignment.
The first group of features are those that depend
only on the source and target words and may there-
fore be called local features. Consequently, the
factored node corresponding to such a feature is
connected to one random variable only (see Figure
2(a)). The lexical features, which represent the lexi-
cal translation probability of the words belong to this
group. In our experiments the IBM4-lexica in both
directions were used. Furthermore, there are source
and target normalized lexical features for every lexi-
con. The source normalized feature, for example, is
normalized in a way, that all translation probabilities
of one source word to target words in the sentences
sum up to one as shown in equation 4.
psourceN (fj , ei) =
plex(fj , ei)
?
1?j?J plex(fj , ei)
(4)
ptargetN (fj , ei) =
plex(fj , ei)
?
1?i?I plex(fj , ei)
(5)
19
Figure 2: Different features
(a) Local features (b) Fertility features (c) First order features
They compare the possible translations in one sen-
tence similar to the rank feature used in the approach
presented by Moore (2006). In addition, the follow-
ing local features are used: The relative distance of
the sentence positions of both words. This should
help to aligned words that occur several times in the
sentence. The relative edit distance between source
and target word was used to improve the align-
ment of cognates. Furthermore a feature indicating
if source and target words are identical was added
to the system. This helps to align dates, numbers
and names, which are quite difficult to align using
only lexical features since they occur quite rarely.
In some of our experiments the links of the IBM4-
alignments are used as an additional local feature.
In the experiments this leads to 22 features. Lastly,
there are indicator features for every possible com-
bination of Parts-of-Speech(POS)-tags and for Nw
high frequency words. In the experiments the 50
most frequent words were used, which lead to 2500
features and around 1440 POS-based features were
used. The POS-feature can help to align words, for
which the lexical features are weak.
The next group of features are the fertility fea-
tures. They model the probability that a word trans-
lates into one, two, three or more words, or does not
have any translation at all. The corresponding fac-
tored node for a source word is connected to all I
random variables representing the links to the target
words, and the node for a target word is connected
to all the J nodes for the links to source words (s.
Figure 2(b)). In this group of features there are two
different types. First, there are indicator features for
the different fertilities. To reduce the complexity of
the calculation this is only done up to a given max-
imal fertility Nf and there is an additional indicator
feature for all fertilities larger than Nf . This is an
extension of the empty word indicator feature used
in other discriminative word alignment models. Fur-
thermore, there is a real-valued feature, which can
use the GIZA++ probabilities for the different fer-
tilities. This has the advantage compared to the in-
dicator feature that the fertility probabilities are not
the same for all words. But here again, all fertilities
larger than a givenNf are not considered separately.
In the evaluation Nf = 3 was selected. So 12 fertil-
ity features were used in the experiments.
The first-order features model the first-order de-
pendencies between the different links. They are
grouped into different directions. The factored node
for the direction (s, t) is connected to the variable
nodes yji and y(j+s)(i+t). For example, the most
common direction is (1, 1), which describes the sit-
uation that if the words at positions j and i are
aligned, also the immediate successor words in both
sentences are aligned as shown in Figure 2(c). In
the default configuration the directions (1, 1), (2, 1),
(1, 2) and (1,?1) are used. So this feature is able to
explicitly model short jumps in the alignment, like
in the directions (2, 1) and (1, 2) as well as crossing
links like in the directions (1,?1). Furthermore, it
can be used to improve the fertility modeling. If a
word has got a fertility of two, it is often aligned to
two consecutive words. Therefore, for example in
the Chinese-English system the directions (1, 0) and
(0, 1) were used in addition. This does not mean,
that other directions in the alignment are not possi-
ble, but other jumps in the alignment do not improve
the probability of the alignment. For every direction,
an indicator feature that both links are active and an
additional one, which also depends on the POS-pair
of the first word pair is used. For a configuration
with 4 directions this leads to 4 indicator features
and, for example, 5760 POS-based features.
20
The last group of features are phrase features,
which are introduced to model context dependen-
cies. First a training corpus is aligned. Then, groups
of source and target words are extracted. Words
build a group, if all source words in the group are
aligned to all target words. The relative frequency
of this alignment is used as the feature and indicator
features for 1 : 1, 1 : n, n : 1 and n : m alignments.
The corresponding factored node is connected to all
links that are important for this group.
2.2 Alignment
The structure of the described CRF is quite complex
and there are many loops in the graphical structure,
so the inference cannot be done exactly. For exam-
ple, the random variables y(1,1) and y(1,2) as well as
y(2,1) and y(2,2) are connected by the source fertil-
ity nodes of the words f1 and f2. Furthermore the
variables y(1,1) and y(2,1) as well as y(1,2) and y(2,2)
are connected by the target fertility nodes. So these
nodes build a loop as shown in Figure 2(b). The first
order feature nodes generate loops as well. Conse-
quently an approximation algorithm has to be used.
We use the belief propagation algorithm introduced
in (Pearl, 1966). In this algorithm messages consist-
ing of a pair of two values are passed along the edges
between the factored and hidden nodes for several it-
erations. In each iterations first messages from the
hidden nodes to the connected factored nodes are
sent. These messages describe the belief about the
value of the hidden node calculated from the incom-
ing messages of the other connected factored nodes.
Afterwards the messages from the factored nodes
to the connected hidden nodes are send. They are
calculated from the potential and the other incom-
ing messages. This algorithm is not exact in loopy
graphs and it is not even possible to prove that it con-
verges, but in (Yedidia et al, 2003) it was shown,
that this algorithm leads to good results.
The algorithm cannot be used directly, since the
calculation of the message sent from a factored node
to a random variable has an exponential runtime
in the number of connected random variables. Al-
though we limit the number of considered fertili-
ties, the number of connected random variables can
still be quite large for the fertility features and the
phrase features, especially in long sentences. To re-
duce this complexity, we leverage the fact that the
potential can only have a small number of different
values. This will be shown for the fertility feature
node. For a more detailed description we refer to
(Niehues, 2007). The message sent from a factored
node to a random variable is defined in the algorithm
as:
mc?(j,i)(v) =
?
Vc/v
?c(Vc) (6)
?
(j,i)??N(c)/(j,i)
n(j,i)??c(v
?)
where Vc is the set of random variables connected
to the factored node and
?
Vc/v is the sum over all
possible values of Vc where the random variable yji
has the the value v. So the value for the message is
calculated by looking at every possible combination
of the other incoming messages. Then the belief for
this combination is multiplied with the potential of
this combination. This can be rewritten, since the
potential only depends on how many links are active,
not on which ones are active.
mc?(j,i)(v) =
Nf?
n=0
?c(n+ v) ? ?(n) (7)
+ ?c(Nf + 1) ? ?(Nf + 1)
with ?(n) the belief for a fertility of n of the other
connected nodes and ?(Nf+1) the belief for a fertil-
ity bigger than Nf with ?c(Nf + 1) the correspond-
ing potential. The belief for a configuration of some
random variables is calculated by the product over
all out-going messages. So ?(n) is calculated by the
sum over all possible configurations that lead to a
fertility of n over these products.
?(n) =
?
Vc/v:|Vc|=n
?
(j,i)??Vc/(j,i)
n(j,i)??c(v
?)
?(Nf + 1) =
?
Vc/v:|Vc|>Nf
?
(j,i)??Vc/(j,i)
n(j,i)??c(v
?)
The values of the sums can be calculated in linear
time using dynamic programming.
3 Training
The weights of the CRFs are trained using a gradient
descent for a fixed number of iterations, since this
approach leads already to quite good results. In the
21
experiments 200 iterations turned out to be a good
number.
The default criteria to train CRFs is to maximize
the log-likelihood of the correct solution, which is
given by a manually created gold standard align-
ment. Therefore, the feature values of the gold stan-
dard alignment and the expectation values have to be
calculated for every factored node. This can be done
using again the belief propagation algorithm.
Often, this hand-aligned data is annotated with
sure and possible links and it would be nice, if the
training method could use this additional informa-
tion. So we developed a method to optimize the
CRFs towards the alignment error rate (AER) or the
F-score with sure and possible links as introduced
in (Fraser and Marcu, 2007). The advantage of the
F-score is, that there is an additional parameter ?,
which allows to bias the metric more towards pre-
cision or more towards recall. To be able to use
a gradient descent method to optimize the weights,
the derivation of the word alignment metric with re-
spect to these weights must be computed. This can-
not be done for the mentioned metrics since they are
not smooth functions. We follow (Gao et al, 2006;
Suzuki et al, 2006) and approximate the metrics us-
ing the sigmoid function. The sigmoid function uses
the probabilities for every link calculated by the be-
lief propagation algorithm.
In our experiments we compared the maximum
likelihood method and the optimization towards the
AER. We also tested combinations of both. The best
results were obtained when the weights were first
trained using the ML method and the resulting fac-
tors were used as initial values for the AER opti-
mization. Another problem is that the POS-based
features and high frequency word features have a
lot more parameters than all other features and with
these two types of features overfitting seems to be a
bigger problem. Therefore, these features are only
used in a third optimization step, in which they are
optimized towards the AER, keeping all other fea-
ture weights constant. Initial results using a Gaus-
sian prior showed no improvement.
4 Evaluation
The word alignment quality of this approach was
tested on three different language pairs. On the
Spanish-English task the hand-aligned data provided
by the TALP Research Center (Lambert et al, 2005)
was used. As proposed, 100 sentences were used as
development data and 400 as test data. The so called
?Final Text Edition of the European Parliament Pro-
ceedings? consisting of 1.4 million sentences and
this hand-aligned data was used as training corpus.
The POS-tags were generated by the Brill-Tagger
(Brill, 1995) and the FreeLing-Tagger (Asterias et
al., 2006) for the English and the Spanish text re-
spectively. To limit the number of different tags for
Spanish we grouped them according to the first 2
characters in the tag names.
A second group of experiments was done on
an English-French text. The data from the 2003
NAACL shared task (Mihalcea and Pedersen, 2003)
was used. This data consists of 1.1 million sen-
tences, a validation set of 37 sentences and a test
set of 447 sentences, which have been hand-aligned
(Och and Ney, 2003). For the English POS-tags
again the Brill Tagger was used. For the French side,
the TreeTagger (Schmid, 1994) was used.
Finally, to test our alignment approach with lan-
guages that differ more in structure a Chinese-
English task was selected. As hand-aligned data
3160 sentences aligned only with sure links were
used (LDC2006E93). This was split up into 2000
sentences of test data and 1160 sentences of devel-
opment data. In some experiments only the first
200 sentences of the development data were used to
speed up the training process. The FBIS-corpus was
used as training corpus and all Chinese sentences
were word segmented with the Stanford Segmenter
(Tseng et al, 2005). The POS-tags for both sides
were generated with the Stanford Parser (Klein and
Manning, 2003).
4.1 Word alignment quality
The GIZA++-toolkit was used to train a baseline
system. The models and alignment information
were then used as additional knowledge source for
the discriminative word alignment. For the first two
tasks, all heuristics of the Pharaoh-Toolkit (Koehn
et al, 2003) as well as the refined heuristic (Och and
Ney, 2003) to combine both IBM4-alignments were
tested and the best ones are shown in the tables. For
the Chinese task only the grow-diag-final heuristic
was used.
22
Table 1: AER-Results on EN-ES task
Name Dev Test
IBM4 Source-Target 21.49
IBM4 Target-Source 19.23
IBM4 grow-diag 16.48
DWA IBM1 15.26 20.82
+ IBM4 14.23 18.67
+ GIZA-fert. 13.28 18.02
+ Link feature 12.26 15.97
+ POS 9.21 15.36
+ Phrase feature 8.84 14.77
Table 2: AER-Results on EN-FR task
Name Dev Test
IBM4 Source-Target 8.6
IBM4 Target-Source 9.86
IBM4 intersection 5.38
DWA IBM1 5.54 6.37
+ HFRQ/POS 3.67 5.57
+ Link Feature 3.13 4.80
+ IBM4 3.60 4.60
+ Phrase feature 3.32 4.30
The results measured in AER of the discrimina-
tive word alignment for the English-Spanish task are
shown in Table 1. In the experiments systems using
different knowledge sources were evaluated. The
first system used only the IBM1-lexica of both di-
rections as well as the high frequent word features.
Then the IBM4-lexica were used instead and in
the next system the GIZA++-fertilities were added.
As next knowledge source the links of both IBM4-
alignments were added. Furthermore, the system
could be improved by using also the POS-tags. For
the last system, the whole EPPS-corpus was aligned
with the previous system and the phrases were ex-
tracted. Using them as additional features, the best
AER of 14.77 could be reached. This is an improve-
ment of 1.71 AER points or 10% relative to the best
baseline system.
Similar experiments have also been done for the
English-French task. The results measured in AER
are shown in Table 2. The IBM4 system uses
the IBM4 lexica and links instead of the IBM1s
Table 3: AER-Results on CH-EN task
Name Test
IBM4 Source-target 44.94
IBM4 Target-source 37.43
IBM4 Grow-diag-final 35.04
DWA IBM4 30.97
- similarity 30.24
+ Add. directions 27.96
+ Big dev 27.26
+ Phrase feature 27.00
+ Phrase feature(high P.) 26.90
and adds the GIZA++-fertilities. For the ?phrase
feature?-system the corpus was aligned with the
?IBM4?-system and the phrases were extracted.
This led to the best result with an AER of 4.30. This
is 1.08 points or 20% relative improvement over the
best generative system. One reason, why less knowl-
edge sources are needed to be as good as the base-
line system, may be that there are many possible
links in the reference alignment and the discrimina-
tive framework can better adapt to this style. So a
system using only features generated by the IBM1-
model could already reach an AER of 4.80.
In Table 3 results for the Chinese-English align-
ment task are shown1. The first system was only
trained on the smaller development set and used the
same knowledge source than the ?IBM4?-systems
in the last experiment. The system could be im-
proved a little bit by removing the similarity fea-
ture and adding the directions (0, 1) and (1, 0) to
the model. Then the same system was trained on
the bigger development set. Again the parallel cor-
pus was aligned with the discriminative word align-
ment system, once trained towards AER and once
more towards precision, and phrases were extracted.
Overall, an improvement by 8.14 points or 23% over
the baseline system could be achieved.
These experiments show, that every knowledge
source that is available should be used. For all lan-
guages pairs additional knowledge sources lead to
an improvement in the word alignment quality. A
problem of the discriminative framework is, that
hand-aligned data is needed for training. So the
1For this task no results on the development task are given
since different development sets were used
23
Table 4: Translation results for EN-ES
Name Dev Test
Baseline 40.04 47.73
DWA 41.62 48.13
Table 5: Translation results for CH-EN
Name Dev Test
Baseline 27.13 22.56
AER 27.63 23.85?
F0.3 26.34 22.35
F0.7 26.40 23.52?
Phrase feature AER 25.84 23.42?
Phrase feature F0.7 26.41 23.92?
French-English dev set may be too small, since the
best system on the development set does not cor-
respond to the best system on the test set. And as
shown in the Chinese-English task additional data
can improve the alignment quality.
4.2 Translation quality
Since the main application of the word alignment is
statistical machine translation, the aim was not only
to generate better alignments measured in AER, but
also to generate better translations. Therefore, the
word alignment was used to extract phrases and use
them then in the translation system. In all translation
experiments the beam decoder as described in (Vo-
gel, 2003) was used together with a 3-gram language
model and the results are reported in the BLUE met-
ric. For test set translations the statistical signifi-
cance of the results was tested using the bootstrap
technique as described in (Zhang and Vogel, 2004).
The baseline system used the phrases build with the
Pharaoh-Toolkit.
The new word alignment was tested on the
English-Spanish translation task using the TC-Star
07 development and test data. The discriminative
word alignment (DWA) used the configuration de-
noted by +POS system in Table 1. With this con-
figuration it took around 4 hours to align 100K sen-
tences. But, of course, generating the alignment can
be parallelized to speed up the process. As shown
in Table 4 the new word alignment could generate
better translations as measured in BLEU scores.
For the Chinese-English task some experiments
were made to study the effect of different training
schemes. Results are shown in Table 5. The sys-
tems used the MT?03 eval set as development data
and the NIST part of the MT?06 eval set was used as
test set. Scores significantly better than the baseline
system are mark by a ?. The first three systems used
a discriminative word alignment generated with the
configuration as the one described as ?+ big dev?-
system in Table 3. The first one was optimized to-
wards AER, the other two were trained towards the
F-score with an ?-value of 0.3 (recall-biased) and
0.7 (precision-biased) respectively. A higher pre-
cision word alignment generates fewer alignment
links, but a larger phrase table. For this task, the
precision seems to be more important. So the sys-
tem trained towards the AER and the F-score with
an ?-value of 0.7 performed better than the other
systems. The phrase features gave improved perfor-
mance only when optimized towards the F-score, but
not when optimized towards the AER.
5 Comparison to other work
Several discriminative word alignment approaches
have been presented in recent years. The one most
similar to ours is the one presented by Blunsom
and Cohn (2006). They also used CRFs, but they
used two linear-chain CRFs, one for every direc-
tions. Consequently, they could find the optimal so-
lution for each individual CRF, but they still needed
the heuristics to combine both alignments. They
reached an AER of 5.29 using the IBM4-alignment
on the English-French task (compared to 4.30 of our
approach).
Lacoste-Julien et al (2006) enriched the bipartite
matching problem to model also larger fertilities and
first-or der dependencies. They could reach an AER
of 3.8 on the same task, but only if they also included
the posteriors of the model of Liang et al (2006).
Using only the IBM4-alignment they generated an
alignment with an AER of 4.5. But they did not use
any POS-based features in their experiments.
Finally, Moore et al (2006) used a log-linear
model for the features and performed a beam search.
They could reach an AER as low as 3.7 with both
types of alignment information. But they presented
no results using only the IBM4-alignment features.
24
6 Conclusion
In this paper a new discriminative word alignment
model was presented. It uses a conditional random
field to model directly the alignment matrix. There-
fore, the algorithms used in the CRFs had to be
adapted to be able to model dependencies between
many random variables. Different methods to train
the model have been developed. Optimizing the F-
score allows to generate alignments focusing more
on precision or on recall. For the model a multitude
of features using the different knowledge sources
have been developed. The experiments showed that
the performance could be improved by using these
additional knowledge sources. Furthermore, the use
of a general machine learning framework like the
CRFs enables this alignment approach to benefit
from future improvements in CRFs in other areas.
Experiments on 3 different language pairs have
shown that word alignment quality as well as trans-
lation quality could be improved. In the translation
experiments it was shown that the improvement is
significant at a significance level of 5%.
References
Atserias, J., B. Casas, E. Comelles, M. Gonza?lez, L.
Padro? and M. Padro?. 2006. FreeLing 1.3: Syntactic
and semantic services in an open-source NLP library.
In LREC?06. Genoa, Italy.
P. Blunsom and T. Cohn. 2006. Discriminative word
alignment with conditional random fields. In ACL?06,
pp. 65-72. Sydney, Australia.
E. Brill. 1995. Transformation-based error-driven learn-
ing and natural language processing: A case study in
part of speech tagging. Computational Linguistics,
21(4):543-565.
P.F. Brown, S. Della Pietra, V. J. Della Pietra, R. L. Mer-
cer. 1993. The Mathematic of Statistical Machine
Translation: Parameter Estimation. Computational
Linguistics, 19(2):263-311.
A. Fraser, D. Marcu. 2007. Measuring Word Alignment
Quality for Statistical Machine Translation Computa-
tional Linguistics, 33(3):293-303.
S. Gao, W. Wu, C. Lee, T. Chua. 2006. A maximal
figure-of-merit (MFoM)-learning approach to robust
classifier design for text categorization. ACM Trans.
Inf. Syst., 24(2):190-218.
D. Klein and C.D. Manning. 2003. Fast Exact Inference
with a Factored Model for Natural Language Parsing.
Advances in Neural Information Processing Systems
15 (NIPS 2002), pp. 3-10.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical
phrase-based translation. In HTL-NAACL?03, pp. 48-
54. Morristown, New Jersey, USA.
S. Lacoste-Julien, B. Taskar, D. Klein, M. I. Jordan.
2006. Word alignment via quadratic assignment. In
HTL-NAACL?06. New York, USA.
P. Lambert, A. de Gispert, R. Banchs and J. b. Marino.
2005. Guidelines for Word Alignment Evaluation and
Manual Alignment. Language Resources and Evalua-
tion, pp. 267-285, Springer.
X. Lan and S. Roth, D. P. Huttenlocher, M. J. Black.
2006. Efficient Belief Propagation with Learned
Higher-Order Markov Random Fields. ECCV (2),
Lecture Notes in Computer Science, pp. 269-282.
P. Liang, B. Taskar, D. Klein. 2006. Alignment by agree-
ment. In HTL-NAACL?06, pp. 104-110. New York,
USA.
R. Mihalcea, T. Pedersen. 2003. An Evaluation Exer-
cise for Word Alignment. In HLT-NAACL 2003 Work-
shop, Building and Using Parallel Texts: Data Driven
Machine Translation and Beyond, pp. 1-6. Edmon-
ton,Canada.
R. C. Moore, W. Yih, A. Bode. 2006. Improved dis-
criminative bilingual word alignment. In ACL?06, pp.
513-520. Sydney, Australia.
J. Niehues. 2007. Discriminative Word Alignment Mod-
els. Diplomarbeit at Universita?t Karlsruhe(TH).
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguist,29(1):19-51.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In NEMLAP?94. Manchester,
UK.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In HLT-NAACL?03, pp. 134?141.
Edmonton, Canada.
J. Suzuki, E. McDermott, H. Isozaki. 2006. Training
conditional random fields with multivariate evaluation
measures In ACL?06, pp 217-224. Sydney, Australia.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky and C. Man-
ning. 2005. A Conditional Random Field Word Seg-
menter. In SIGHAN-4. Jeju, Korea.
S. Vogel, H. Ney, C. Tillmann. 1996. HMM-based word
alignment in statistical translation. In COLING?96,
pp. 836-841. Copenhagen, Denmark.
S. Vogel. 2003. SMT Decoder Dissected: Word Reorder-
ing. In NLP-KE?03. Bejing, China.
J. S. Yedidia, W. T. Freeman, Y. Weiss. 2003. Un-
derstanding belief propagation and its generalizations.
Exploring artificial intelligence in the new millennium.
Y. Zhang and S. Vogel. 2004. Measuring Confidence
Intervals for MT Evaluation Metrics. In TMI 2004.
Baltimore, MD, USA.
25
Proceedings of the Third Workshop on Statistical Machine Translation, pages 151?154,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving Word Alignment with Language Model Based Confidence Scores
Nguyen Bach, Qin Gao, Stephan Vogel
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, qing, vogel+}@cs.cmu.edu
Abstract
This paper describes the statistical machine trans-
lation systems submitted to the ACL-WMT 2008
shared translation task. Systems were submitted for
two translation directions: English?Spanish and
Spanish?English. Using sentence pair confidence
scores estimated with source and target language
models, improvements are observed on the News-
Commentary test sets. Genre-dependent sentence
pair confidence score and integration of sentence
pair confidence score into phrase table are also in-
vestigated.
1 Introduction
Word alignment models are a crucial component in sta-
tistical machine translation systems. When estimating
the parameters of the word alignment models, the sen-
tence pair probability is an important factor in the objec-
tive function and is approximated by the empirical prob-
ability. The empirical probability for each sentence pair
is estimated by maximum likelihood estimation over the
training data (Brown et al, 1993). Due to the limitation of
training data, most sentence pairs occur only once, which
makes the empirical probability almost uniform. This is
a rather weak approximation of the true distribution.
In this paper, we investigate the methods of weighting
sentence pairs using language models, and extended the
general weighting method to genre-dependent weight. A
method of integrating the weight directly into the phrase
table is also explored.
2 The Baseline Phrase-Based MT System
The ACL-WMT08 organizers provided Europarl and
News-Commentary parallel corpora for English ? Span-
ish. Detailed corpus statistics is given in Table 1. Follow-
ing the guidelines of the workshop we built baseline sys-
tems, using the lower-cased Europarl parallel corpus (re-
stricting sentence length to 40 words), GIZA++ (Och and
Ney, 2003), Moses (Koehn et al, 2007), and the SRI LM
toolkit (Stolcke, 2002) to build 5-gram LMs. Since no
News development sets were available we chose News-
Commentary sets as replacements. We used test-2006
(E06) and nc-devtest2007 (NCd) as development sets for
Europarl and News-Commentary; test-2007 (E07) and
nc-test2007 (NCt) as held-out evaluation sets.
English Spanish
Europarl (E)
sentence pairs 1,258,778
unique sent. pairs 1,235,134
avg. sentence length 27.9 29.0
# words 35.14 M 36.54 M
vocabulary 108.7 K 164.8 K
News-Commentary (NC)
sentence pairs 64,308
unique sent. pairs 64,205
avg. sentence length 24.0 27.4
# words 1.54 M 1.76 M
vocabulary 44.2 K 56.9 K
Table 1: Statistics of English?Spanish Europarl and News-
Commentary corpora
To improve the baseline performance we trained sys-
tems on all true-cased training data with sentence length
up to 100. We used two language models, a 5-gram LM
build from the Europarl corpus and a 3-gram LM build
from the News-Commentary data. Instead of interpolat-
ing the two language models, we explicitly used them in
the decoder and optimized their weights via minimum-
error-rate (MER) training (Och, 2003). To shorten the
training time, a multi-threaded GIZA++ version was used
to utilize multi-processor servers (Gao and Vogel, 2008).
Other parameters were the same as the baseline sys-
tem. Table 2 shows results in lowercase BLEU (Pap-
ineni et al, 2002) for both the baseline (B) and the im-
proved baseline systems (B5) on development and held-
151
out evaluation sets. We observed significant gains for the
News-Commentary test sets. Our improved baseline sys-
tems obtained a comparable performance with the best
English?Spanish systems in 2007 (Callison-Burch et al,
2007).
Pairs Europarl NC
E06 E07 NCd NCt
En?Es B 33.00 32.21 31.84 30.56B5 33.33 32.25 35.10 34.08
Es?En B 33.08 33.23 31.18 31.34B5 33.26 33.23 36.06 35.56
Table 2: NIST-BLEU scores of baseline and improved baseline
systems experiments on English?Spanish
3 Weighting Sentence Pairs
3.1 Problem Definition
The quality of word alignment is crucial for the perfor-
mance of the machine translation system.
In the well-known so-called IBM word alignment
models (Brown et al, 1993), re-estimating the model pa-
rameters depends on the empirical probability P? (ek, fk)
for each sentence pair (ek, fk). During the EM train-
ing, all counts of events, e.g. word pair counts, distortion
model counts, etc., are weighted by P? (ek, fk). For ex-
ample, in IBM Model 1 the lexicon probability of source
word f given target word e is calculated as (Och and Ney,
2003):
p(f |e) =
?
k c(f |e; ek, fk)?
k,f c(f |e; ek, fk)
(1)
c(f |e; ek, fk) =
?
ek,fk
P? (ek, fk)
?
a
P (a|ek, fk) ? (2)
?
j
?(f , fkj )?(e, ekaj )
Therefore, the distribution of P? (ek, fk) will affect the
alignment results. In Eqn. 2, P? (ek, fk) determines
how much the alignments of sentence pair (ek, fk) con-
tribute to the model parameters. It will be helpful if
the P? (ek, fk) can approximate the true distribution of
P (ek, fk).
Consider that we are drawing sentence pairs from a
given data source, and each unique sentence pair (ek, fk)
has a probability P (ek, fk) to be observed. If the training
corpora size is infinite, the normalized frequency of each
unique sentence pair will converge to P (ek, fk). In that
case, equally assigning a number to each occurrence of
(ek, fk) and normalizing it will be valid. However, the
assumption is invalid if the data source is finite. As we
can observe in the training corpora, most sentences occur
only one time, and thus P? (ek, fk) will be uniform.
To get a more informative P? (ek, fk), we explored
methods of weighting sentence pairs. We investigated
three sets of features: sentence pair confidence (sc),
genre-dependent sentence pair confidence (gdsc) and
phrase alignment confidence (pc) scores. These features
were calculated over an entire training corpus and could
be easily integrated into the phrase-based machine trans-
lation system.
3.2 Sentence Pair Confidence
We can hardly compute the joint probability of P (ek, fk)
without knowing the conditional probability P (ek|fk)
which is estimated during the alignment process. There-
fore, to estimate P (ek, fk) before alignment, we make an
assumption that P? (ek, fk) = P (ek)P (fk), which means
the two sides of sentence pair are independent of each
other. P (ek) and P (fk) can be obtained by using lan-
guage models. P (ek) or P (fk), however, can be small
when the sentence is long. Consequently, long sentence
pairs will be assigned low scores and have negligible ef-
fect on the training process. Given limited training data,
ignoring these long sentences may hurt the alignment re-
sult. To compensate this, we normalize the probability by
the sentence length. We propose the following method
to weighting sentence pairs in the corpora. We trained
language models for source and target language, and the
average log likelihood (AVG-LL) of each sentence pair
was calculated by applying the corresponding language
model. For each sentence pair (ek, fk), the AVG-LL
L(ek, fk) is
L(ek) = 1|ek|
?
eki ?ek logP (e
k
i |h)
L(fk) = 1|fk|
?
fkj ?fk logP (f
k
j |h)
L(ek, fk) = [L(ek) + L(fk)]/2
(3)
where P (eki |h) and P (fkj |h) are ngram probabilities.
The sentence pair confidence score is then given by:
sc(ek, fk) = exp(L(ek, fk)). (4)
3.3 Genre-Dependent Sentence Pair Confidence
Genre adaptation is one of the major challenges in statis-
tical machine translation since translation models suffer
from data sparseness (Koehn and Schroeder, 2007). To
overcome these problems previous works have focused
on explicitly modeling topics and on using multiple lan-
guage and translation models. Using a mixture of topic-
dependent Viterbi alignments was proposed in (Civera
and Juan, 2007). Language and translation model adap-
tation to Europarl and News-Commentary have been ex-
plored in (Paulik et al, 2007).
Given the sentence pair weighting method, it is pos-
sible to adopt genre-specific language models into the
152
weighting process. The genre-dependent sentence pair
confidence gdsc simulates weighting the training sen-
tences again from different data sources, thus, given
genre g, it can be formulated as:
gdsc(ek, fk) = sc(ek, fk|g) (5)
where P (eki |h) and P (fkj |h) are estimated by genre-
specific language models.
The score generally represents the likelihood of the
sentence pair to be in a specific genre. Thus, if both sides
of the sentence pair show a high probability according
to the genre-specific language models, alignments in the
pair should be more possible to occur in that particular
domain, and put more weight may contribute to a better
alignment for that genre.
3.4 Phrase Alignment Confidence
So far the confidence scores are used only in the train-
ing of the word alignment models. Tracking from which
sentence pairs each phrase pair was extracted, we can use
the sentence level confidence scores to assign confidence
scores to the phrase pairs. Let S(e?, f?) denote the set of
sentences pairs from which the phrase pair (e?, f?) was ex-
tracted. We calculate then a phrase alignment confidence
score pc as:
pc(e?, f?) = exp
?
(ek,fk)?S(e?,f?) log sc(ek, fk)
|S(e?, f?)| (6)
This score is used as an additional feature of the phrase
pair. The feature weight is estimated in MER training.
4 Experimental Results
The first step in validating the proposed approach was
to check if the different language models do assign dif-
ferent weights to the sentence pairs in the training cor-
pora. Using the different language models NC (News-
Commentary), EP (Europarl), NC+EP (both NC and EP)
the genre-specific sentence pair confidence scores were
calculated. Figure 1 shows the distributions of the dif-
ferences in these scores across the two corpora. As ex-
pected, the language model build from the NC corpus as-
signs - on average - higher weights to sentence pairs in the
NC corpus and lower weights to sentence pairs in the EP
corpus (Figure 1a). The opposite is true for the EP LM.
When comparing the scores calculated from the NC LM
and the combined NC+EP LM we still see a clear sep-
aration (Figure 1b). No marked difference can be seen
between using the EP LM and the NC+EP LM (Figure
1c), which again is expected, as the NC corpus is very
small compared to the EP corpus.
The next step was to retrain the word alignment mod-
els using sentences weights according to the various con-
?0.06 ?0.04 ?0.02 0 0.02 0.04 0.060
0.005
0.01
0.015
(a) Difference in Weight (NC?EP)
Pro
port
ion 
in C
orpo
ra
?0.06 ?0.04 ?0.02 0 0.02 0.04 0.060
0.005
0.01
0.015
0.02
(b) Difference in Weight (NC?NE)
Pro
port
ion 
in C
orpo
ra
?0.06 ?0.04 ?0.02 0 0.02 0.04 0.060
0.005
0.01
0.015
0.02
(c) Difference in Weight (NE?EP)
Pro
port
ion 
in C
orpo
ra
Europal DataNews Commentary Data
Europal DataNews Commentary Data
Europal DataNews Commentary Data
Figure 1: Histogram of weight differences genre specific con-
fidence scores on NC and EP training corpora
fidence scores. Table 3 shows training and test set per-
plexities for IBM model 4 for both training directions.
Not only do we see a drop in training set perplexities,
but also in test set perplexities. Using the genre specific
confidence scores leads to lower perplexities on the cor-
responding test set, which means that using the proposed
method does lead to small, but consistent adjustments in
the alignment models.
Uniform NC+EP NC EP
train En?Es 46.76 42.36 42.97 44.47Es?En 70.18 62.81 62.95 65.86
test
NC(En?Es) 53.04 53.44 51.09 55.94
EP(En?Es) 91.13 90.89 91.84 90.77
NC(Es?En) 81.39 81.28 78.23 80.33
EP(Es?En) 126.56 125.96 123.23 122.11
Table 3: IBM model 4 training and test set perplexities using
genre specific sentence pair confidence scores.
In the final step the specific alignment models were
used to generate various phrase tables, which were then
used in translation experiments. Results are shown in Ta-
ble 4. We report lower-cased Bleu scores. We used nc-
dev2007 (NCt1) as an additional held-out evaluation set.
Bold cells indicate highest scores.
As we can see from the results, improvements are ob-
tained by using sentence pair confidence scores. Us-
ing confidence scores calculated from the EP LM gave
overall the best performance. While we observe only a
small improvement on Europarl sets, improvements on
News-Commentary sets are more pronounced, especially
on held-out evaluation sets NCt and NCt1. The exper-
iments do not give evidence that genre-dependent con-
fidence can improve over using the general confidence
153
Test Set
E06 E07 NCd NCt NCt1
Es?En
B5 33.26 33.23 36.06 35.56 35.64
NC+EP 33.23 32.29 36.12 35.47 35.97
NC 33.43 33.39 36.14 35.27 35.68
EP 33.36 33.39 36.16 35.63 36.17
En?Es
B5 33.33 32.25 35.10 34.08 34.43
NC+EP 33.23 32.29 35.12 34.56 34.89
NC 33.30 32.27 34.91 34.07 34.29
EP 33.08 32.29 35.05 34.52 35.03
Table 4: Translation results (NIST-BLEU) using gdsc with dif-
ferent genre-specific language models for Es?En systems
score. As the News-Commentary language model was
trained on a very small amount of data further work is
required to study this in more detail.
Test Set
E06 E07 NCd NCt NCt1
Es?En
B5 33.26 33.23 36.06 35.56 35.64
NC+EP+pc 33.54 33.39 36.07 35.38 35.85
NC+pc 33.17 33.31 35.96 35.74 36.04
EP+pc 33.44 32.87 36.22 35.63 36.09
En?Es
B5 33.33 32.25 35.10 34.08 34.43
NC+EP+pc 33.28 32.45 34.82 33.68 33.86
NC+pc 33.13 32.47 34.01 34.34 34.98
EP+pc 32.97 32.20 34.26 33.99 34.34
Table 5: Translation results (NIST-BLEU) using pc with differ-
ent genre-specific language models for Es?En systems
Table 5 shows experiments results in NIST-BLEU us-
ing pc score as an additional feature on phrase tables
in Es?En systems. We observed that across develop-
ment and held-out sets the gains from pc are inconsistent,
therefore our submissions are selected from the B5+EP
system.
5 Conclusion
In the ACL-WMT 2008, our major innovations are meth-
ods to estimate sentence pair confidence via language
models. We proposed to use source and target language
models to weight the sentence pairs. We developed sen-
tence pair confidence (sc), genre-dependent sentence pair
confidence (gdsc) and phrase alignment confidence (pc)
scores. Our experimental results shown that we had a bet-
ter word alignment and translation performance by using
gdsc. We did not observe consistent improvements by
using phrase pair confidence scores in our systems.
Acknowledgments
This work is in part supported by the US DARPA under the
GALE program. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the au-
thors and do not necessarily reflect the views of DARPA.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statisti-
cal machine translation: Parameter estimation. In Computa-
tional Linguistics, volume 19(2), pages 263?331.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-) evalua-
tion of machine translation. In Proc. of the ACL 2007 Second
Workshop on Statistical Machine Translation, Prague, Czech
Republic.
Jorge Civera and Alfons Juan. 2007. Domain adaptation in sta-
tistical translation with mixture modelling. In Proc. of the
ACL 2007 Second Workshop on Statistical Machine Transla-
tion, Prague, Czech Republic.
Qin Gao and Stephan Vogel. 2008. Parallel implementations
of word alignment tool. In Proc. of the ACL 2008 Soft-
ware Engineering, Testing, and Quality Assurance Work-
shop, Columbus, Ohio, USA.
Philipp Koehn and Josh Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation. In Proc.
of the ACL 2007 Second Workshop on Statistical Machine
Translation, Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics, demo sessions, pages 177?
180, Prague, Czech Republic, June.
Franz J. Och and Hermann Ney. 2003. A systematic compar-
ison of various statistical alignment models. In Computa-
tional Linguistics, volume 1:29, pages 19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Erhard Hinrichs and Dan Roth,
editors, Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proc. of the 40th Annual Conf. of the
Association for Computational Linguistics (ACL 02), pages
311?318, Philadelphia, PA, July.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja Hildebrand,
and Stephan Vogel. 2007. The ISL phrase-based mt system
for the 2007 ACL workshop on statistical machine transla-
tion. In In Proc. of the ACL 2007 Second Workshop on Sta-
tistical Machine Translation, Prague, Czech Republic.
Andreas Stolcke. 2002. SRILM ? An extensible language mod-
eling toolkit. In Proc. Intl. Conf. on Spoken Language Pro-
cessing, volume 2, pages 901?904, Denver.
154
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49?57,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Parallel Implementations of Word Alignment Tool
Qin Gao and Stephan Vogel
Language Technology Institution
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{qing, stephan.vogel}@cs.cmu.edu
Abstract
Training word alignment models on large cor-
pora is a very time-consuming processes. This
paper describes two parallel implementations
of GIZA++ that accelerate this word align-
ment process. One of the implementations
runs on computer clusters, the other runs on
multi-processor system using multi-threading
technology. Results show a near-linear speed-
up according to the number of CPUs used, and
alignment quality is preserved.
1 Introduction
Training state-of-the-art phrase-based statistical ma-
chine translation (SMT) systems requires several
steps. First, word alignment models are trained on
the bilingual parallel training corpora. The most
widely used tool to perform this training step is the
well-known GIZA++(Och and Ney, 2003). The re-
sulting word alignment is then used to extract phrase
pairs and perhaps other information to be used in
translation systems, such as block reordering mod-
els. Among the procedures, more than 2/3 of the
time is consumed by word alignment (Koehn et al,
2007). Speeding up the word alignment step can
dramatically reduces the overall training time, and in
turn accelerates the development of SMT systems.
With the rapid development of computing hard-
ware, multi-processor servers and clusters become
widely available. With parallel computing, process-
ing time (wall time) can often be cut down by one
or two orders of magnitude. Tasks, which require
several weeks on a single CPU machine may take
only a few hours on a cluster. However, GIZA++
was designed to be single-process and single-thread.
To make more efficient use of available computing
resources and thereby speed up the training of our
SMT system, we decided to modify GIZA++ so that
it can run in parallel on multiple CPUs.
The word alignment models implemented in
GIZA++, the so-called IBM (Brown et al, 1993) and
HMM alignment models (Vogel et al, 1996) are typ-
ical implementation of the EM algorithm (Dempster
et al, 1977). That is to say that each of these mod-
els run for a number of iterations. In each iteration
it first calculates the best word alignment for each
sentence pairs in the corpus, accumulating various
counts, and then normalizes the counts to generate
the model parameters for the next iteration. The
word alignment stage is the most time-consuming
part, especially when the size of training corpus is
large. During the aligning stage, all sentences can
be aligned independently of each other, as model
parameters are only updated after all sentence pairs
have been aligned. Making use of this property, the
alignment procedure can be parallelized. The basic
idea is to have multiple processes or threads aligning
portions of corpus independently and then merge the
counts and perform normalization.
The paper implements two parallelization meth-
ods. The PGIZA++ implementation, which is based
on (Lin et al 2006), uses multiple aligning pro-
cesses. When all the processes finish, a master pro-
cess starts to collect the counts and normalizes them
to produce updated models. Child processes are then
restarted for the new iteration. The PGIZA++ does
not limit the number of CPUs being used, whereas
it needs to transfer (in some cases) large amounts
49
of data between processes. Therefore its perfor-
mance also depends on the speed of the network in-
frastructure. The MGIZA++ implementation, on the
other hand, starts multiple threads on a common ad-
dress space, and uses a mutual locking mechanism
to synchronize the access to the memory. Although
MGIZA++ can only utilize a single multi-processor
computer, which limits the number of CPUs it can
use, it avoids the overhead of slow network I/O. That
makes it an equally efficient solution for many tasks.
The two versions of alignment tools are available on-
line at http://www.cs.cmu.edu/q?ing/giza.
The paper will be organized as follows, section 2
provides the basic algorithm of GIZA++, and sec-
tion 3 describes the PGIZA++ implementation. Sec-
tion 4 presents the MGIZA++ implementation, fol-
lowed by the profile and evaluation results of both
systems in section 5. Finally, conclusion and future
work are presented in section 6.
2 Outline of GIZA++
2.1 Statistical Word Alignment Models
GIZA++ aligns words based on statistical models.
Given a source string fJ1 = f1, ? ? ? , fj , ? ? ? , fJ and a
target string eI1 = e1, ? ? ? , ei, ? ? ? , eI , an alignment A
of the two strings is defined as(Och and Ney, 2003):
A ? {(j, i) : j = 1, ? ? ? , J ; i = 0, ? ? ? , I} (1)
in case that i = 0 in some (j, i) ? A, it represents
that the source word j aligns to an ?empty? target
word e0.
In statistical world alignment, the probability of a
source sentence given target sentence is written as:
P (fJ1 |eI1) =
?
aJ1
P (fJ1 , aJ1 |eI1) (2)
in which aJ1 denotes the alignment on the sen-
tence pair. In order to express the probability in
statistical way, several different parametric forms of
P (fJ1 , aJ1 |eI1) = p?(fJ1 , aJ1 |eI1) have been proposed,
and the parameters ? can be estimated using maxi-
mum likelihood estimation(MLE) on a training cor-
pus(Och and Ney, 2003).
?? = arg max
?
S
?
s=1
?
a
p?(fs, a|es) (3)
The best alignment of the sentence pair,
a?J1 = arg max
aJ1
p??(f
J
1 , aJ1 |eI1) (4)
is called Viterbi alignment.
2.2 Implementation of GIZA++
GIZA++ is an implementation of ML estimators for
several statistical alignment models, including IBM
Model 1 through 5 (Brown et al, 1993), HMM (Vo-
gel et al, 1996) and Model 6 (Och and Ney, 2003).
Although IBM Model 5 and Model 6 are sophisti-
cated, they do not give much improvement to align-
ment quality. IBM Model 2 has been shown to be
inferior to the HMM alignment model in the sense
of providing a good starting point for more complex
models. (Och and Ney, 2003) So in this paper we
focus on Model 1, HMM, Model 3 and 4.
When estimating the parameters, the EM (Demp-
ster et al, 1977) algorithm is employed. In the
E-step the counts for all the parameters are col-
lected, and the counts are normalized in M-step.
Figure 1 shows a high-level view of the procedure
in GIZA++. Theoretically the E-step requires sum-
ming over all the alignments of one sentence pair,
which could be (I + 1)J alignments in total. While
(Och and Ney, 2003) presents algorithm to imple-
ment counting over all the alignments for Model 1,2
and HMM, it is prohibitive to do that for Models 3
through 6. Therefore, the counts are only collected
for a subset of alignments. For example, (Brown
et al, 1993) suggested two different methods: us-
ing only the alignment with the maximum probabil-
ity, the so-called Viterbi alignment, or generating a
set of alignments by starting from the Viterbi align-
ment and making changes, which keep the align-
ment probability high. The later is called ?pegging?.
(Al-Onaizan et al, 1999) proposed to use the neigh-
bor alignments of the Viterbi alignment, and it yields
good results with a minor speed overhead.
During training we starts from simple models use
the simple models to bootstrap the more complex
ones. Usually people use the following sequence:
Model 1, HMM, Model 3 and finally Model 4. Table
1 lists all the parameter tables needed in each stage
and their data structures1. Among these models, the
1In filename, prefix is a user specified parameter, and n is
the number of the iteration.
50
Figure 1: High-level algorithm of GIZA++
lexicon probability table (TTable) is the largest. It
should contain all the p(fi, ej) entries, which means
the table will have an entry for every distinct source
and target word pair fi, ej that co-occurs in at least
one sentence pair in the corpus. However, to keep
the size of this table manageable, low probability en-
tries are pruned. Still, when training the alignment
models on large corpora this statistical lexicon often
consumes several giga bytes of memory.
The computation time of aligning a sentence pair
obviously depends on the sentence length. E.g. for
IBM 1 that alignment is O(J ? I), for the HMM
alignment it is O(J + I2), with J the number of
words in the source sentence and I the number of
words in the target sentence. However, given that
the maximum sentence length is fixed, the time com-
plexity of the E-step grows linearly with the num-
ber of sentence pairs. The time needed to perform
the M-step is dominated by re-normalizing the lexi-
con probabilities. The worst case time complexity is
O(|VF | ? |VE |), where |VF | is the size of the source
vocabulary and |VE | is the size of the target vocabu-
lary. Therefore, the time complexity of the M-step is
polynomial in the vocabulary size, which typically
grows logarithmic in corpus size. As a result, the
alignment stage consumes most of the overall pro-
cessing time when the number of sentences is large.
Because the parameters are only updated during
the M-step, it will be no difference in the result
whether we perform the word alignment in the E-
step sequentially or in parallel2. These character-
2However, the rounding problem will make a small differ-
istics make it possible to build parallel versions of
GIZA++. Figure 2 shows the basic idea of parallel
GIZA++.
Figure 2: Basic idea of Parallel GIZA++
While working on the required modification to
GIZA++ to run the alignment step in parallel we
identified a bug, which needed to be fixed. When
training the HMM model, the matrix for the HMM
trellis will not be initialized if the target sentence has
only one word. Therefore some random numbers
are added to the counts. This bug will also crash
the system when linking against pthread library. We
observe different alignment and slightly lower per-
plexity after fixing the bug 3.
3 Multi-process version - PGIZA++
3.1 Overview
A natural idea of parallelizing GIZA++ is to sep-
arate the alignment and normalization procedures,
and spawn multiple alignment processes. Each pro-
cess aligns a chunk of the pre-partitioned corpus and
outputs partial counts. A master process takes these
counts and combines them, and produces the nor-
malized model parameters for the next iteration. The
architecture of PGIZA++ is shown in Figure 3.
ence in the results even when processing the sentences sequen-
tially, but in different order.
3The details of the bug can be found in: http://www.mail-
archive.com/moses-support@mit.edu/msg00292.html
51
Model Parameter tables Filename Description Data structure
Model 1 TTable prefix.t1.n Lexicon Probability Array of Array
HMM TTable prefix.thmm.n
ATable prefix.ahmm.n Align Table 4-D Array
HMMTable prefix.hhmm.n HMM Jump Map
Model 3/4 TTable prefix.t3.n
ATable prefix.a3.n Align Table
NTable prefix.n3.n Fertility Table 2-D Array
DTable prefix.d3.n Distortion Table 4-D Array
pz prefix.p0 3.n Probability for null words p0 Scalar
(Model 4 only) D4Table prefix.d4.n prefix.D4.n Distortion Table for Model 4 Map
Table 1: Model tables created during training
Figure 3: Architecture of PGIZA++
3.2 Implementation
3.2.1 I/O of the Parameter Tables
In order to ensure that the next iteration has the
correct model, all the information that may affect the
alignment needs to be stored and shared. It includes
model files and statistics over the training corpus.
Table 1 is a summary of tables used in each model.
Step Without With
Pruning(MB) Pruning(MB)
Model 1, Step 1 1,273 494
HMM , Step 5 1,275 293
Model 4 , Step 3 1,280 129
Table 2: Comparison of the size of count tables for the
lexicon probabilities
In addition to these models, the summation of
?sentence weight? of the whole corpus should be
stored. GIZA++ allows assigning a weight wi for
each sentence pair si sto indicate the number of oc-
currence of the sentence pair. The weight is normal-
ized by pi = wi/
?
i wi, so that
?
i pi = 1. Then
the pi serves as a prior probability in the objective
function. As each child processes only see a portion
of training data, it is required to calculate and share
the
?
i wi among the children so the values can be
consistent.
The tables and count tables of the lexicon proba-
bilities (TTable) can be extremely large if not pruned
before being written out. Pruning the count tables
when writing them into a file will make the result
slightly different. However, as we will see in Sec-
tion 5, the difference does not hurt translation per-
formance significantly. Table 2 shows the size of
count tables written by each child process in an ex-
periment with 10 million sentence pairs, remember
there are more than 10 children writing the the count
tables, and the master would have to read all these
tables, the amount of I/O is significantly reduced by
pruning the count tables.
3.2.2 Master Control Script
The other issue is the master control script. The
script should be able to start processes in other
nodes. Therefore the implementation varies accord-
ing to the software environment. We implemented
three versions of scripts based on secure shell, Con-
dor (Thain et al, 2005) and Maui.
Also, the master must be notified when a child
process finishes. In our implementation, we use sig-
nal files in the network file system. When the child
process finishes, it will touch a predefined file in a
shared folder. The script keeps watching the folder
and when all the children have finished, the script
runs the normalization process and then starts the
next iteration.
52
3.3 Advantages and Disadvantages
One of the advantages of PGIZA++ is its scalability,
it is not limited by the number of CPUs of a sin-
gle machine. By adding more nodes, the alignment
speed can be arbitrarily fast4. Also, by splitting the
corpora into multiple segments, each child process
only needs part of the lexicon, which saves mem-
ory. The other advantage is that it can adopt differ-
ent resource management systems, such as Condor
and Maui/Torque. By splitting the corpus into very
small segments, and submitting them to a scheduler,
we can get most out of clusters.
However, PGIZA++ also has significant draw-
backs. First of all, each process needs to load the
models of the previous iteration, and store the counts
of the current step on shared storage. Therefore,
I/O becomes a bottleneck, especially when the num-
ber of child processes is large. Also, the normal-
ization procedure needs to read all the count files
from network storage. As the number of child pro-
cesses increases, the time spent on reading/writing
will also increase. Given the fact that the I/O de-
mand will not increase as fast as the size of corpus
grows, PGIZA++ can only provide significant speed
up when the size of each training corpus chunk is
large enough so that the alignment time is signifi-
cantly longer than normalization time.
Also, one obvious drawback of PGIZA++ is its
complexity in setting up the environment. One has
to write scripts specially for the scheduler/resource
management software.
Balancing the load of each child process is an-
other issue. If any one of the corpus chunks takes
longer to complete, the master has to wait for it. In
other words, the speed of PGIZA++ is actually de-
termined by the slowest child process.
4 Multi-thread version - MGIZA++
4.1 Overview
Another implementation of parallelism is to run sev-
eral alignment threads in a single process. The
threads share the same address space, which means
it can access the model parameters concurrently
without any I/O overhead.
4The normalization process will be slower when the number
of nodes increases
The architecture of MGIZA++ is shown in Figure
4.
Data Sentence 
Provider
Thread 1 Thread 2 Thread n
Synchronized Assignment of 
Sentence Pairs
Model
Synchronized 
Count Storage
Main Thread
Normalization
Figure 4: Architecture of MGIZA++
4.2 Implementation
The main thread spawns a number of threads, us-
ing the same entry function. Each thread will ask
a provider for the next sentence pair. The sentence
provider is synchronized. The request of sentences
are queued, and each sentence pair is guaranteed to
be assigned to only one thread.
The threads do alignment in their own stacks, and
read required probabilities from global parameter ta-
bles, such as the TTable, which reside on the heap.
Because no update on these global tables will be per-
formed during this stage, the reading can be concur-
rent. After aligning the sentence pairs, the counts
need to be collected. For HMMTable and D4Table,
which use maps as their data structure, we cannot
allow concurrent read/write to the table, because the
map structure may be changed when inserting a new
entry. So we must either put mutual locks to post-
pone reading until writing is complete, or dupli-
cate the tables for each thread and merge them af-
terwards. Locking can be very inefficient because
it may block other threads, so the duplicate/merge
method is a much better solution. However, for the
TTable the size is too large to have multiple copies.
Instead, we put a lock on every target word, so only
when two thread try to write counts for the same tar-
get word will a collisions happen. We also have to
put mutual locks on the accumulators used to calcu-
late the alignment perplexity.
53
Table Synchronizations Method
TTable Write lock on every target words
ATable Duplicate/Merge
HMMTable Duplicate/Merge
DTable Duplicate/Merge
NTable Duplicate/Merge
D4Table Duplicate /Merge
Perplexity Mutual lock
Table 3: Synchronizations for tables in MGIZA++
Each thread outputs the alignment into its own
output file. Sentences in these files are not in sequen-
tial order. Therefore, we cannot simply concatenate
them but rather have to merge them according to the
sentence id.
4.3 Advantages and Disadvantages
Because all the threads within a process share the
same address space, no data needs to be transferred,
which saves the I/O time significantly. MGIZA++ is
more resource-thrifty comparing to PGIZA++, it do
not need to load copies of models into memory.
In contrast to PGIZA++, MGIZA++ has a much
simpler interface and can be treated as a drop-in
replacement for GIZA++, except that one needs
to run a script to merge the final alignment files.
This property makes it very simple to integrate
MGIZA++ into machine translation packages, such
as Moses(Koehn et al, 2007).
One major disadvantage of MGIZA++ is also ob-
vious: lack of scalability. Accelerating is limited
by the number of CPUs the node has. Compared
to PGIZA++ on the speed-up factor by each addi-
tional CPU, MGIZA++ also shows some deficiency.
Due to the need for synchronization, there are al-
ways some CPU time wasted in waiting.
5 Experiments
5.1 Experiments on PGIZA++
For PGIZA++ we performed training on an Chinese-
English translation task. The dataset consists of ap-
proximately 10 million sentence pairs with 231 mil-
lion Chinese words and 258 million English words.
We ran both GIZA++ and PGIZA++ on the same
training corpus with the same parameters, then ran
Pharaoh phrase extraction on the resulting align-
ments. Finally, we tuned our translation systems on
the NIST MT03 test set and evaluate them on NIST
MT06 test set. The experiment was performed on
a cluster of several Xeon CPUs, the storage of cor-
pora and models are on a central NFS server. The
PGIZA++ uses Condor as its scheduler, splitting the
training data into 30 fragments, and ran training in
both direction (Ch-En, En-Ch) concurrently. The
scheduler assigns 11 CPUs on average to the tasks.
We ran 5 iterations of Model 1 training, 5 iteration
of HMM, 3 Model 3 iterations and 3 Model 4 iter-
ations. To compare the performance of system, we
recorded the total training time and the BLEU score,
which is a standard automatic measurement of the
translation quality(Papineni et al, 2002). The train-
ing time and BLEU scores are shown in Table 4: 5
Running (TUNE) (TEST)
Time MT03 MT06 CPUs
GIZA++ 169h 32.34 29.43 2
PGIZA++ 39h 32.20 30.14 11
Table 4: Comparison of GIZA++ and PGIZA++
The results show similar BLEU scores when us-
ing GIZA++ and PGIZA++, and a 4 times speed up.
Also, we calculated the time used in normaliza-
tion. The average time of each normalization step is
shown in Table 5.
Per-iteration (Avg) Total
Model 1 47.0min 235min (3.9h)
HMM 31.8min 159min (2.6h)
Model 3/4 25.2 min 151min (2.5h)
Table 5: Normalization time in each stage
As we can see, if we rule out the time spent in
normalization, the speed up is almost linear. Higher
order models require less time in the normalization
step mainly due to the fact that the lexicon becomes
smaller and smaller with each models (see Table 2.
PGIZA++, in small amount of data,
5.2 Experiment on MGIZA++
Because MGIZA++ is more convenient to integrate
into other packages, we modified the Moses sys-
tem to use MGIZA++. We use the Europal English-
Spanish dataset as training data, which contains 900
thousand sentence pairs, 20 million English words
and 20 million Spanish words. We trained the
5All the BLEU scores in the paper are case insensitive.
54
English-to-Spanish system, and tuned the system
on two datasets, the WSMT 2006 Europal test set
(TUNE1) and the WSMT news commentary dev-
test set 2007 (TUNE2). Then we used the first pa-
rameter set to decode WSMT 2006 Europal test set
(TEST1) and used the second on WSMT news com-
mentary test set 2007 (TEST2)6. Table 6 shows the
comparison of BLEU scores of both systems. listed
in Table 6:
TUNE1 TEST1 TUNE2 TEST2
GIZA++ 33.00 32.21 31.84 30.56
MGIZA++ 32.74 32.26 31.35 30.63
Table 6: BLEU Score of GIZA++ and MGIZA++
Note that when decoding using the phrase table
resulting from training with MGIZA++, we used
the parameter tuned for a phrase table generated
from GIZA++ alignment, which may be the cause
of lower BLEU score in the tuning set. However,
the major difference in the training comes from fix-
ing the HMM bug in GIZA++, as mentioned before.
To profile the speed of the system according to
the number of CPUs it use, we ran MGIZA++ on
1, 2 and 4 CPUs of the same speed. When it runs
on 1 CPU, the speed is the same as for the original
GIZA++. Table 7 and Figure 5 show the running
time of each stage:
4000
5000
6000
7000
8000
m
e(
s)
Model 1
HMM
Model3/4
0
1000
2000
3000
1 2 3 4
Ti
m
CPUS
Figure 5: Speed up of MGIZA++
When using 4 CPUs, the system uses only 41%
time comparing to one thread. Comparing to
PGIZA++, MGIZA++ does not have as high an ac-
6http://www.statmt.org/wmt08/shared-task.html
CPUs M1(s) HMM(s) M3,M4(s) Total(s)
1 2167 5101 7615 14913
2 1352 3049 4418 8854
(62%) (59%) (58%) (59%)
4 928 2240 2947 6140
(43%) (44%) (38%) (41%)
Table 7: Speed of MGIZA++
celeration rate. That is mainly because of the re-
quired locking mechanism. However the accelera-
tion is also significant, especially for small training
corpora, as we will see in next experiment.
5.3 Comparison of MGIZA++ and PGIZA++
In order to compare the acceleration rate of
PGIZA++ and MGIZA++, we also ran PGIZA++ in
the same dataset as described in the previous section
with 4 children. To avoid the delay of starting the
children processes, we chose to use ssh to start re-
mote tasks directly, instead of using schedulers. The
results are listed in Table 8.
M1(s) HMM(s) M3,M4(s) Total(s)
MGIZA+1CPU 2167 5101 7615 14913
MGIZA+4CPUs 928 2240 2947 6140
PGIZA+4Nodes 3719 4324 4920 12963
Table 8: Speed of PGIZA++ on Small Corpus
There is nearly no speed-up observed, and in
Model 1 training, we observe a loss in the speed.
Again, by investigating the time spent in normaliza-
tion, the phenomenon can be explained (Table 9):
Even after ruling out the normalization time, the
speed up factor is smaller than MGIZA++. That
is because of reading models when child processes
start and writing models when child processes finish.
From the experiment we can conclude that
PGIZA++ is more suited to train on large corpora
than on small or moderate size corpora. It is also im-
portant to determine whether to use PGIZA++ rather
than MGIZA++ according to the speed of network
storage infrastructure.
5.4 Difference in Alignment
To compare the difference in final Viterbi alignment
output, we counted the number of sentences that
have different alignments in these systems. We use
55
Per-iteration (Avg) Total
Model 1 8.4min 41min (0.68h)
HMM 7.2min 36min (0.60h)
Model 3/4 5.7 min 34min (0.57h)
Total 111min (1.85h)
Table 9: Normalization time in each stage : small data
GIZA++ with the bug fixed as the reference. The
results of all other systems are listed in Table 10:
Diff Lines Diff Percent
GIZA++(origin) 100,848 10.19%
MGIZA++(4CPU) 189 0.019%
PGIZA++(4Nodes) 18,453 1.86%
Table 10: Difference in Viterbi alignment (GIZA++ with
the bug fixed as reference)
From the comparison we can see that PGIZA++
has larger difference in the generated alignment.
That is partially because of the pruning on count ta-
bles.
To also compare the alignment score in the differ-
ent systems. For each sentence pair i = 1, 2, ? ? ? , N ,
assume two systems b and c have Viterbi alignment
scores Sbi , Sci . We define the residual R as:
R = 2
?
i
( |Sbi ? Sci |
(Sbi + Sci )
)
/N (5)
The residuals of the three systems are listed in Table
11. The residual result shows that the MGIZA++ has
a very small (less than 0.2%) difference in alignment
scores, while PGIZA++ has a larger residual.
The results of experiments show the efficiency
and also the fidelity of the alignment generated by
the two versions of parallel GIZA++. However,
there are still small differences in the final align-
ment result, especially for PGIZA++. Therefore,
one should consider which version to choose when
building systems. Generally speaking, MGIZA++
provides smoother integration into other packages:
easy to set up and also more precise. PGIZA++ will
not perform as good as MGIZA++ on small-size cor-
pora. However, PGIZA++ has good performance on
large data, and should be considered when building
very large scale systems.
6 Conclusion
The paper describes two parallel implementations
of the well-known and widely used word alignment
R
GIZA++(origin) 0.6503
MGIZA++(4CPU) 0.0017
PGIZA++(4Nodes) 0.0371
Table 11: Residual in Viterbi alignment scores (GIZA++
with the bug fixed as reference)
tool GIZA++. PGIZA++ does alignment on a num-
ber of independent processes, uses network file sys-
tem to collect counts, and performs normalization by
a master process. MGIZA++ uses a multi-threading
mechanism to utilize multiple cores and avoid net-
work transportation. The experiments show that the
two implementation produces similar results with
original GIZA++, but lead to a significant speed-up
in the training process.
With compatible interface, MGIZA++ is suit-
able for a drop-in replacement for GIZA++, while
PGIZA++ can utilize huge computation resources,
which is suitable for building large scale systems
that cannot be built using a single machine.
However, improvements can be made on both
versions. First, a combination of the two imple-
mentation is reasonable, i.e. running multi-threaded
child processes inside PGIZA++?s architecture. This
could reduce the I/O significantly when using the
same number of CPUs. Secondly, the mechanism
of assigning sentence pairs to the child processes can
be improved in PGIZA++. A server can take respon-
sibility to assign sentence pairs to available child
processes dynamically. This would avoid wasting
any computation resource by waiting for other pro-
cesses to finish. Finally, the huge model files, which
are responsible for a high I/O volume can be reduced
by using binary formats. A first implementation of a
simple binary format for the TTable resulted in files
only about 1/3 in size on disk compared to the plain
text format.
The recent development of MapReduce frame-
work shows its capability to parallelize a variety of
machine learning algorithms, and we are attempting
to port word alignment tools to this framework. Cur-
rently, the problems to be addressed is the I/O bot-
tlenecks and memory usage, and an attempt to use
distributed structured storage such as HyperTable to
enable fast access to large tables and also performing
filtering on the tables to alleviate the memory issue.
56
References
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum Likelihood From Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39(1):138
Douglas Thain, Todd Tannenbaum, and Miron Livny.
2005. Distributed Computing in Practice: The Con-
dor Experience. Concurrency and Computation: Prac-
tice and Experience, 17(2-4):323-356
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19-51
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL
2007, Demonstration Session, Prague, Czech Repub-
lic
Peter F. Brown, Stephan A. Della Pietra, Vincent J. Della
Pietra, Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263-311
Stephan Vogel, Hermann Ney and Christoph Tillmann.
1996. HMM-based Word Alignment in Statistical
Translation. In COLING ?96: The 16th International
Conference on Computational Linguistics, pp. 836-
841, Copenhagen, Denmark.
Xiaojun Lin, Xinhao Wang and Xihong Wu. 2006.
NLMP System Description for the 2006 NIST MT
Evaluation. NIST 2006 Machine Translation Evalu-
ation
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John D. Lafferty, I. Dan Melamed, David
Purdy, Franz J. Och, Noah A. Smith and David
Yarowsky. 1999. Statistical Machine Trans-
lation. Final Report JHU Workshop, Available at
http://www.clsp.jhu.edu/ws99/projects/mt/final report/mt-
final-reports.ps
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu 2002. BLEU: a Method for Automatic Eval-
uation of machine translation. Proc. of the 40th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 02), pp. 311-318, Philadelphia, PA
57
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 47?50,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
CMU System Combination for WMT?09
Almut Silja Hildebrand
Carnegie Mellon University
Pittsburgh, USA
silja@cs.cmu.edu
Stephan Vogel
Carnegie Mellon University
Pittsburgh, USA
vogel@cs.cmu.edu
Abstract
This paper describes the CMU entry for
the system combination shared task at
WMT?09. Our combination method is hy-
pothesis selection, which uses information
from n-best lists from several MT systems.
The sentence level features are indepen-
dent from the MT systems involved. To
compensate for various n-best list sizes in
the workshop shared task including first-
best-only entries, we normalize one of our
high-impact features for varying sub-list
size. We combined restricted data track
entries in French - English, German - En-
glish and Hungarian - English using pro-
vided data only.
1 Introduction
For the combination of machine translation sys-
tems there have been two main approaches de-
scribed in recent publications. One uses confusion
network decoding to combine translation systems
as described in (Rosti et al, 2008) and (Karakos et
al., 2008). The other approach selects whole hy-
potheses from a combined n-best list (Hildebrand
and Vogel, 2008).
Our setup follows the approach described in
(Hildebrand and Vogel, 2008). We combine the
output from the available translation systems into
one joint n-best list, then calculate a set of fea-
tures consistently for all hypotheses. We use MER
training on a development set to determine feature
weights and re-rank the joint n-best list.
2 Features
For our entries to the WMT?09 we used the fol-
lowing feature groups:
? Language model score
? Word lexicon scores
? Sentence length features
? Rank feature
? Normalized n-gram agreement
The details on language model and word lexi-
con scores can be found in (Hildebrand and Vogel,
2008). We use two sentence length features, which
are the ratio of the hypothesis length to the length
of the source sentence and the difference between
the hypothesis length and the average length of
the hypotheses in the n-best list for the respec-
tive source sentence. We also use the rank of the
hypothesis in the original system?s n-best list as a
feature.
2.1 Normalized N-gram Agreement
The participants of the WMT?09 shared transla-
tion task provided output from their translation
systems in various sizes. Most submission were
1st-best translation only, some submitted 10-best
up to 300-best lists.
In preliminary experiments we saw that adding
a high scoring 1st-best translation to a joint n-best
list composed of several larger n-best lists does not
yield the desired improvement. This might be due
to the fact, that hypotheses within an n-best list
originating from one single system (sub-list) tend
to be much more similar to each other than to hy-
potheses from another system. This leads to hy-
potheses from larger sub-lists scoring higher in the
n-best list based features, e.g. because they collect
more n-gram matches within their sub-list, which
?supports? them the more the larger it is.
Previous experiments on Chinese-English
showed, that the two feature groups with the
highest impact on the combination result are the
language model and the n-best list based n-gram
agreement. Therefore we decided to focus on the
n-best list n-gram agreement for exploring sub-list
47
size normalization to adapt to the data situation
with various n-best list sizes.
The n-gram agreement score of each n-gram in
the target sentence is the relative frequency of tar-
get sentences in the n-best list for one source sen-
tence that contain the n-gram e, independent of
the position of the n-gram in the sentence. This
feature represents the percentage of the transla-
tion hypotheses, which contain the respective n-
gram. If a hypothesis contains an n-gram more
than once, it is only counted once, hence the max-
imum for the agreement score a(e) is 1.0 (100%).
The agreement score a(e) for each n-gram e is:
a(e) =
C
L
(1)
where C is the count of the hypotheses containing
the n-gram and L is the size of the n-best list for
this source sentence.
To compensate for the various n-best list sizes
provided to us we modified the n-best list n-gram
agreement by normalizing the count of hypotheses
that contain the n-gram by the size of the sub-list
it came from. It can be viewed as either collecting
fractional counts for each n-gram match, or as cal-
culating the n-gram agreement percentage for each
sub-list and then interpolating them. The normal-
ized n-gram agreement score anorm(e) for each n-
gram e is:
anorm(e) =
1
P
P?
j=1
Cj
Lj
(2)
where P is the number of systems, Cj is the count
of the hypotheses containing the n-gram e in the
sublist pj and Lj is the size of the sublist pj .
For the extreme case of a sub-list size of one
the fact of finding an n-gram in that hypothesis
or not has a rather strong impact on the normal-
ized agreement score. Therefore we introduce a
smoothing factor ? in a way that it has an increas-
ing influence the smaller the sub-list is:
asmooth(e) =
1
P
P?
j=1
[
Cj
Lj
(1?
?
Lj
)
]
+
[
Lj ? Cj
Lj
?
Lj
] (3)
where P is the number of systems, Cj is the count
of the hypotheses containing the n-gram in the
sublist pj and Lj is the size of the sublist pj . We
used an initial value of ? = 0.1 for our experi-
ments.
In all three cases the score for the whole hypoth-
esis is the sum over the word scores normalized
by the sentence length. We use n-gram lengths
n = 1..6 as six separate features.
3 Preliminary Experiments
Arabic-English
For the development of the modification on the n-
best list n-gram agreement feature we used n-best
lists from three large scale Arabic to English trans-
lation systems. We evaluate using the case insen-
sitive BLEU score for the MT08 test set with four
references, which was unseen data for the individ-
ual systems as well as the system combination. Ta-
ble 1 shows the initial scores of the three input sys-
tems.
system MT08
A 47.47
B 46.33
C 44.42
Table 1: Arabic-English Baselines: BLEU
To compare the behavior of the combination
result for different n-best list sizes we combined
the 100-best lists from systems A and C and then
added three n-best list sizes from the middle sys-
tem B into the combination: 1-best, 10-best and
full 100-best. For each of these four combination
options we ran the hypothesis selection using the
plain version of the n-gram agreement feature a as
well as the normalized version without anorm and
with smoothing asmooth .
combination a anorm asmooth
A & C 48.04 48.09 48.13
A & C & B1 47.84 48.34 48.21
A & C & B10 48.29 48.33 48.47
A & C & B100 48.91 48.95 49.02
Table 2: Combination results: BLEU on MT08
The modified feature has as expected no impact
on the combination of n-best lists of the same size
(see Table 2), however it shows an improvement
of BLEU +0.5 for the combination with the 1st-
best from system B. The smoothing seems to have
no significant impact for this dataset, but differ-
ent smoothing factors will be investigated in the
future.
48
4 Workshop Results
To train our language models and word lexica
we only used provided data. Therefore we ex-
cluded systems from the combination, which were
to our knowledge using unrestricted training data
(google). We did not include any contrastive sys-
tems.
We trained the statistical word lexica on the par-
allel data provided for each language pair1. For
each combination we used two language models,
a 1.2 giga-word 3-gram language model, trained
on the provided monolingual English data and a 4-
gram language model trained on the English part
of the parallel training data of the respective lan-
guages. We used the SRILM toolkit (Stolcke,
2002) for training.
For each of the three language pairs we submit-
ted a combination that used the plain version of the
n-gram agreement feature as well as one using the
normalized smoothed version.
The provided system combination development
set, which we used for tuning our feature weights,
was the same for all language pairs, 502 sentences
with only one reference.
For combination we tokenized and lowercased
all data, because the n-best lists were submitted
in various formats. Therefore we report the case
insensitive scores here. The combination was op-
timized toward the BLEU metric, therefore results
for TER and METEOR are not very meaningful
here and only reported for completeness.
4.1 French-English
14 systems were submitted to the restricted data
track for the French-English translation task. The
scores on the combination development set range
from BLEU 27.56 to 15.09 (case insensitive eval-
uation).
We received n-best lists from five systems, a
300-best, a 200-best two 100-best and one 10-best
list. We included up to 100 hypotheses per system
in our joint n-best list.
For our workshop submission we combined the
top nine systems with the last system scoring
24.23 as well as all 14 systems. Comparing the
results for the two combinations of all 14 systems
(see Table 3), the one with the sub-list normaliza-
tion for the n-gram agreement feature gains +0.8
1http://www.statmt.org/wmt09/translation-
task.html#training
BLEU on unseen data compared to the one with-
out normalization.
system dev test TER Meteor
best single 27.56 26.88 56.32 52.68
top 9 asmooth 29.85 28.07 55.23 53.90
all 14 asmooth 30.39 28.46 55.12 54.35
all 14 29.49 27.65 55.41 53.74
Table 3: French-English Results: BLEU
Our system combination via hypothesis selec-
tion could improve the translation quality by +1.6
BLEU on the unseen test set compared to the best
single system.
A  177 B* 434 C  104
177 434 104
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100% N     7
M   18
L    16
K    12
J    10
I*  264
H    41
G  110
F* 423
E* 584
D* 562
C  104
B* 434
A  177*
*
*
*
*
Figure 1: Contributions of the individual systems
to the final translation.
Figure 1 shows, how many hypotheses were
contributed by the individual systems to the fi-
nal translation (unseen data). The systems A to
N are ordered by their BLEU score on the devel-
opment set. The systems which provided n-best
lists, marked with a star in the diagram, clearly
dominate the selection. The low scoring systems
contribute very little as expected.
4.2 German-English
14 systems were submitted to the restricted data
track for the German-English translation task. The
scores on the combination development set range
49
from BLEU 27.56 to 7 (case insensitive evalua-
tion). The two lowest scoring systems at BLEU
11 and 7 were so far from the rest of the systems
that we decided to exclude them, assuming an er-
ror had occurred.
Within the remaining 12 submissions were four
n-best lists, three 100-best and one 10-best.
For our submissions we combined the top seven
systems between BLEU 22.91 and 20.24 as well as
the top 12 systems where the last one of those was
scoring BLEU 16.00 on the development set. For
this language pair the combination with the nor-
malized n-gram agreement also outperforms the
one without by +0.8 BLEU (see Table 4).
system dev test TER Meteor
best single 22.91 21.03 61.87 47.96
top 7 asmooth 25.13 22.86 60.73 49.71
top 12 asmooth 25.32 22.98 60.72 50.01
top 12 25.12 22.20 60.95 49.33
Table 4: German-English Results: BLEU
Our system combination via hypothesis selec-
tion could improve translation quality by +1.95
BLEU on the unseen test set over the best single
system.
4.3 Hungarian-English
Only three systems were submitted for the
Hungarian-English translation task. Scores on the
combination development set ranged from BLEU
13.63 to 10.04 (case insensitive evaluation). Only
the top system provided an n-best list. We used
100-best hypotheses.
system dev test TER Meteor
best single 13.63 12.73 68.75 36.76
3 sys asmooth 14.98 13.74 72.34 38.20
3 sys 14.14 13.18 74.29 37.52
Table 5: Hungarian-English Results: BLEU
We submitted combinations of the three systems
by using the modified smoothed n-gram agree-
ment feature and the plain version of the n-gram
agreement feature. Here also the normalized ver-
sion of the feature gives an improvement of +0.56
BLEU with an overall improvement of +1.0 BLEU
over the best single system (see Table 5).
5 Summary
It is beneficial to include more systems, even if
they are more than 7 points BLEU behind the best
system, as the comparison to the combinations
with fewer systems shows.
In the mixed size data situation of the workshop
the modified feature shows a clear improvement
for all three language pairs. Different smoothing
factors should be investigated for these data sets
in the future.
Acknowledgments
We would like to thank the participants in the
WMT?09 workshop shared translation task for
providing their data, especially n-best lists.
References
Almut Silja Hildebrand and Stephan Vogel. 2008.
Combination of machine translation systems via hy-
pothesis selection from combined n-best lists. In
MT at work: Proceedings of the Eighth Confer-
ence of the Association for Machine Translation in
the Americas, pages 254?261, Waikiki, Hawaii, Oc-
tober. Association for Machine Translation in the
Americas.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using itg-based alignments. In
Proceedings of ACL-08: HLT, Short Papers, pages
81?84, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hy-
pothesis alignment for building confusion networks
with application to machine translation system com-
bination. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 183?186,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference for Spoken Language Processing,
Denver, Colorado, September.
50
Const ruct ion  of  a Hierarchica l  \ ]?ans la t ion  Memory  
S. Vogel, H. Ney 
Lehrs tuh l  flit In tbrmat ik  VI, Computer  Science Depar tment  
1{1~7~111 Aach(',n Univers i ty  of T(~chnology 
1)-52056 Aachen,  Gerinm~y 
Elnaih voge l@in format ik ,  rwth -aachen,  de 
Abst rac t  
_q}:anslation memories are t)ronfising devi('es for 
artt;omati(- translation. Their main weakuess, 
however, is poor coverage, on llllSeell {;ex|;. \]ill 
this t)at)er, l;he use of a hierarchical ;ransla- 
tion memory, (:onsisting of a ('as(:ade of finite 
si;~d;e transducers, is t)rot)os(;d. A mmfl)er of 
tr~nsdu(:e, rs is al)l)\]ied to (;onverl; s(;ni;enee 1)airs 
fl:om a t)ilingual cortms into translat ion pat- 
terns, which are then used as a translat ion me, m- 
ory. Pr(;l iminary results on the (\]erman English 
V ERBMOIIIL ('orl)us a,re given. 
1 In t roduct ion  
In reeenl; years, exa,int)le-1)ased t;l"ansl~l;i<)l~ has 
been 1)rol)osed as an efli<:ient ~n(;t;llo<l for auto- 
m~d;i(: translation (Sal;o and Nag;to, 1990; Ki- 
tan(), 1993; Brown, \]99(i). 'lli'anslations are 
sl;()l;ed il l a t ra l l s la i ; i ( ) l l  l l le. l l lory tloll(t llso, d t;o coi1- 
SI;YllCI; trauslations for new sealten(:e.s. In its sin> 
1)lest version, examl)le-1)ased translat ion boils 
down to l l S i l lg  a (tat;fl)ase of SOllrce sell(;el l(;es 
with their l;rmlslations. For many translat;ion 
tasks, esl)eeially in coml)ul;er assisl;cd |;ransla- 
tion, this at)l)roa(:h works with greal; success. 
For flflly aul;onlat;i(" l;ranslal;ion the main t ) ro t )  - 
\ ]em is t )oor  COVel'a~e oi1 l leW data. To overco l I le  
this weakness, it hierar(:hi(:al translation llleln- 
ory is prot)osed. Al)plying a cascade of tinite 
sti%te |;ra, i ls(hl( 'ers~ a~ SOllrce Se l l te l l ce  is {;ralis- 
laW, d into the tin:get language. 
2 The  Transducers  
2.1 Overv iew 
A translat;ion lnemory is siml)ly a eolle(:|;ion 
of source-l;arge3; string i)airs. As a tirst Stel) ~ 
these translat ion examt)les (:all be (:onverted 
inl;o translat ion 1)atl;(.q:ns t)y lilt;reducing cate- 
gory \]abels, e.g. tbr prol)er nmnes or numbers. 
3.'0 make the translat ion patterns even more use- 
ful, not only single words but comph;x phrases 
can be replace.d by category labels. Which 
phrases t;o select for categorization depends on 
the aplflication, l,br example, the corpus lls0.d 
for this si;udy coal;alas many time and date ex- 
pressions. Therefore, a specialized |;ransduce, r 
was constructed to recognize and translal;e such 
e, xl)ressions. 
Each transducer is a se~ of quadrut)les of the 
tbrm: 
label # source pal;t;ern # l;arget; t)atl;ern # score, 
Som'ce l)al;terns and target patterns may con- 
tain category labels. We call su(:h l)atterns 
~(:ompomldL ~.l.~:ansdueea's working only on the 
word level are (:ailed 'simple'. \]if a la:ans- 
dll(:e,r coal;alas recursive p:tl;terns, e.g. \])ATE # 
\])NPE lind \])ATE # I)AS?I'~ and \])ATI'3 # -3.0, it; 
has |;o be. apl)lied re.cursively t;o t;he input;. 
The scores a,t|;a(:hed to the translation t)at -
terns can be viewed ns translat ion scores. They 
are llse, d to bi~ts towards 1;he selection of lollg(;r 
part;eras and towards lliore likely translations 
in I;hose cases where several targol; patterns are 
associated with ()lie SOl l rce  t)a,l;i;ern. 
'.l'he transducers can be applied in 1)oth di- 
rections, i.e. for a given language pair, each 
language can be viewed as source language. 
Thcrel)y, bil ingual abeling is possilfle. This can 
l)e applied to convert a bilingual corlms into a 
selection of translat ion l)atterns which are. for- 
mulated in terms of words and ('ategory lal)els. 
2.2 Const ruct ion  o f  the  Transducers  
The transducers should t)e selected in such a 
way am to minimize l;he lle, ed tbr recursive ap- 
t)li(:al;ion in order l;o lint)rove efficiency. There- 
tbre, |;11(' l)atl;erns to search tbr are l)artitioned to 
forln a ('as(:ade of t;ransducers. Sonic trans(luc- 
ers analys(,' l)arts of the senten(:e and rel)la(:e it 
1131 
by a category label, which is then used at a later 
step by another transducer. The labeling of the 
days of the week or the names of the months is 
a prerequisite to apply more complex patterns 
for date expressions. The transducers currently 
used are listed in Table 1. 
'Fable 1: List of transducers. 
1. names (persons, towns, places, events, etc) 
2. spelling (e.g. 'D A double L') 
3. numbers (ordinal, cardinal, fractions, etc) 
4. time and date expressions 
5. parts of speech (tbr certain word classes) 
6. grammar (noun phrases, verb phrases) 
Some transducers are general in scope, e.g. 
the transducers for numbers, part of speech tags 
and grammar. Others are costumized towards 
the domain tbr which the translation system is 
developed. In tile VERBMOBIL corpus, which is 
used for the experiments, time and date expres- 
sions are very prominent. To recognize these 
expressions, a small grammar has been devel- 
oped and coded as finite state transducer. Ac- 
tually, two transducers are used. On the first 
level, words are replaced by labels, like DAY- 
OFWEEK = { Montag, Dienstag, ...}. On the 
second level, these labels are used to t'orm com- 
plex time and date expressions. This second 
transducer works recursively, as simpler expres- 
sions are used to build more complex expres- 
sions. 
Finally, a small grammar based on POS (part 
of speech) tags has been crafted mamlally. The 
purpose of this grammar is to recognize simple 
noun phrases. Extensions to handle the differ- 
ent word ordering in the verb phrases arc under 
development. 
2.3 Scoring 
The scores attached to the translation patterns 
can be viewed as a kind of translation scores. 
In the current implementation a rather crude 
heuristic together with some manual tuning in 
the grammar transducer is applied. The idea 
is to give preference to longer translation pat- 
terns as they take more context into account 
and encode word reordering in an explicit man- 
ner. Thus, fbr simple and compound translation 
patterns the score is exponential to the length 
of the source pattern. Tile scores are negative 
by convention: not translating a word gives zero 
cost, translating it gives a benefit, i.e. negative 
costs. In future, scoring will be refined by using 
corpus statistics to assign probabilities to the 
translation patterns. 
2.4 Bilingual Labeling 
The sentence pairs ill the bilingual training cor- 
pus can be segmented into shorter segments 
with the help of an alignment progrmn (Och et 
al., 1999). This collection of segments could be 
used directly as a translation memory. However, 
to improve the coverage on unseen data, these 
segnmnts are labeled. Applying the transducers 
as given in Table 1 transfbrms these segments 
into compound t)hrases. 
The procedure is as follows: 
1. For each transducer taken from the com- 
plete cascade - as given in Table 1 ap- 
lilY the transducer to both, the source and 
tlm target sentences of the bilingual train- 
ing cortms. 
2. Find those sentence pairs which contain 
equal number and types of category labels 
tbr both sentences. 
3. For sentence pairs which do not match in 
mmflmr and type of the category labels 
keep the original sentence pair. 
Table 2 shows examples of some translation 
patterns which resulted flom bilingual abeling. 
3 Applying the Transducers 
The working of the transducers i best described 
as tile construction of a translation graph. That 
is to say, the sentence to be translated is viewed 
as a graph which is traversed fi'om left to right. 
For each matching source pattern, as encoded 
in the transducers, a new edge is added to the 
graph. The edge is labeled with the category la- 
bel of the translation pattern. The translation 
and the translation score are attached to the 
edge. In this way a translation graph is con- 
structed. In those cases, where a source pattern 
has several translations, one edge tbr each trans- 
lation is added to the graph. 
Tim left right search on the graph is orga- 
nized in such a way that all paths are traversed 
1132 
Table 2: Coml)ound translation t)atterns (CTP). 
CTP ~ DATE_DAY ginge es wiedcr 
CTP ~ SURNAME am A1)i)~rat 
CTP ~ NP dauert DATE 
CTP @ nehmen PPER NP DATE 
@ DATE_DAY it is possible again :~ -4.6 
~/: this is SURNAME st)caking @ -3.3 
NP takes DATE :~ -3.3 
let PPER take NP DATE @ -4.6 
in parallel and tile patterns l;ored in the trans- 
ducer are matched synchronously. For each 
~lo(te n and each edge e leading to n, all patterns 
in tile transducer starting with the label of e arc 
attached to n. This gives a mmlber of hypothe- 
ses describing partially matching patterns. Al- 
ready started hypotheses are expanded with tile 
lal)el of the edge running ti'om the l)revious node 
to the current node. This procedure is shown in 
l~'igul'e 1. For a selection of t;rmmlation patterns 
from the siml)le , word-1)ased translation mem- 
ory the hyt)otheses tbr 1)artially matching pat- 
terns generated uring the left--right traversal 
are shown as well as the resulting new edges. 
The result of applying all transducers is a 
graph where each path is a (partial) transla- 
tion of the source sentence. The 1)ath with the 
best overall score is used to construct the fi- 
nal translation. For good result;s, not; only the 
scores from t;he transducers houl(l 1)e used in 
selecting the best t)ath, but a language model 
of the target language should l)e inchlde(l. 
1 llIIl # al, on, at the  
9 Montag# Monday 
17 waere  es so  moeglich # would that  be possilflc 
18 wic ist cs bel lhncn # how about you 
19 wie waerc es # how al)out 
20 wie wacrc cs denn # how about 
21 wie waere es denn am Montag # how about Monday 
22 wie wacrc es am Montag # Imw about Monday 
Figure 1: Ext)ansion of Pattern Hypotheses 
3.1 Error Tolerant Match  
To improve tile coverage on unseen test data, 
it may be avantageous to allow tbr approxima- 
tivc matching. The idea is, to apply longer seg- 
ments tbr syntactically better translations with- 
out loosing to much as far as tile content of the 
sentences i concerned. 
We us(; weighted edit distance, i.e. each er- 
ror (insertion, deletion, substitution) is assici- 
ated with an individual score. Thereby, the 
deletion or insertion of typical filler words can 
be allowed, whereas the deletion or insertion of 
content words is avoided. 
3.2 Translation on Word Lat t i ces  
The approach described so far can be used for 
a tight integration of speech recognition and 
translation. Speech recognition systems typi- 
cally 1)ro(luce wor(l lattices which encode the 
most likely word sequences in an e.flicient lllall- 
net. A direct translation on the lattice has, 
compared to transforming the lattice, into an n- 
best list;, translating each word sequence, mM 
selecting the overall best translation, a nulnber 
of advantages: 
? all the paths can be covered, whereas in 
an n-best approach typically only a small 
fraction of tile paths is considered; 
? partial translation hypotheses are reused; 
? acoustic scores can be taken into account 
when calculating an overall score for each 
translation hypothesis. 
4 Exper iments  and Resu l ts  
In this section, we will report on first expert- 
ments and results obtained with the cascaded 
transducer approach. Experiments were per- 
tbrmed on the VERBMOBIL corpus. This cor- 
pus consists of spontaneously spoken dialogs in 
the appointment scheduling domain (Wahlster, 
1993). The vocabulary comprises 7335 German 
1133 
words and 4382 English words. A test corI)us 
of 147 sentences with a total of 1 968 words was 
used to test the coverage of tile transducers and 
to run preliminary translation experiments. 
In Table 3 the sizes of the transducers are 
given. 
Table 3: Number of translation t)atterns of tile 
transducers. 
Transducer Patterns 
Nalne 
Spell 
Number 
Date 
POS Tags 
~ralnnlar 
442 
60 
342 
334 
671.4 
124 
4.1 Coverage 
In a first series of experiments, the coverage 
of the cascaded transducers was tested. TILe 
sentences pairs Dora the training corpus were 
segmented into shorter segments. This resulted 
in 43609 bilingual phrases running from 1 word 
up to 82 words in length. The longest phrases 
were discarded as it is very unlikely that they 
will match other sentences. Thus, for the ex- 
periments only 40000 sentence pairs were used, 
the longest sentences containing sixteen source 
words. 
Starting fi'om those simple phrases, succes- 
sively more transducers were applied 1lt) to the 
fllll cascade. In Table 4 the coverage for each 
level is shown. As expected, the coverage in- 
creases and nearly flfll coverage on the test 
sentences is reached. In tile final step, the 
POS transducer and the grammer transducer 
are both applied. 
The first cohnnn shows which transducers 
have been applied. In each step, one additional 
transducer is applied tbr bilingual labeling and 
tbr translation. Bilingual labeling reduces the 
number of distinct patterns in the translation 
memory, whereas the immber of compound pat- 
terns increases. The last column shows the 
number of words in the test sentences not cov- 
ered by the patterns ill tile translation mmory. 
As can be seen, the coverage increases which 
each step. The large improvement in the final 
Table 4: Efl'ect of selected transducers oi1 cov- 
erage on test corpus. 
%'ansdncers Patterns Coln- not 
pound covered 
NOlle 
Name 
+ Spell 
+ Number 
+ Date 
+ Gramnlar 
40000 
39624 
39508 
38669 
36118 
35519 
1.259 
1468 
11181 
14684 
15682 
273 
254 
249 
238 
215 
9 
step results froln applying tile POS-tag trans- 
ducer whidl coveres a large part of the vocabu- 
lary. 
4.2 Translat ion 
First experiments have been performed to test 
tile approach tbr translation. So far, no lan- 
guage model tbr the target language is applied 
to score the different ranslations. 
For the sentence 'Samstag und Februar sind 
gut, aber der siebzehnte ware besser' the best 
t)ath through the resnlting translation graph 
gives a structure as shown in Figure 2. IlL Ta- 
ble 5, some translation examples for test sen- 
tences not seen ill the training corpus arc given. 
Table 5: Three translations generated t'rom the 
hierarchical translation memory. 
Ich werde lnit dem Fhlgzeug kolnnmn. 
I will come with the plane. 
Ja, wunderbar. Machen wir das so, und 
dann treflbn wir uns daim ill Hamburg. 
Vielen Dank und auf WiederhSren. 
Well, excellent. Shall we fix this, and 
then we will meet then in Hanfl)urg. 
Thank you very much goodbye. 
Das kann ich nicht einrichten. Ich habe 
eine Chance ab dreimldzwanzigsten 
Oktober. Ist es da bei Ihnen m6glich? 
It can I not arrange. I have 
a chance froln twenty-third of 
October. Is it as for you possible? 
1134 
I C_PHRASE 
the fourth would be better 
-7.4 
~DATE \] 
Saturday and February 
-4,2 
DATE DATE 
Saturday February 
-0.6 -0.6 
DAYWEEK 
Saturday 
-0.5 
Samstag I 
MONTH 
February 
-0.5 
Feb  ruar  4 
I DATE 
the fourth 
-4.1 
~ DATEDAY the fourth 
-4.0 
are   ood but  
-2 1 -0 1 
~a_ere ~ A  
Figure 2: \[\[~'m~slation example 
5 Sun'nnary  and  conc lus ions  
In this t)npcr a translation at)pronch 1)asexl on 
cascaded tin|re state l;ra,nsducers has l)een pre- 
sen|ext. A mm~l\] mm~l)er of simple l;rmlsdut'- 
(;rs is handcrafted and then used to convert; n 
bilingual cortms in|;o a translation memory con- 
sisting of som:(:c l)al;tcrn target; i)a,l;l;(;rn p~tirs, 
which inchuh; category lnlmls. Trmlslni;ion is 
then lmrformcd by applying l;he comtflel;e cas- 
ca(le of l;rans(luce.rs. 
First (;xl)e.rim(mts ha,v(; shown l;lm \])ot,cnl;i;J 
of this ai)l)ro~u:h for m~tchine l;ransla,tion. Good 
coverag(~ on mlse,(m test data ('ould 1)e ol)l;aine(l. 
The. main ditficulty in this nt)l)roach is to (te- 
l|he a (:onsistenl; scoring s('heme thr the (litt'e,r- 
ent transdu(:('rs. Especially, ~ good l)M~m('e t)(;- 
tween the grammm: and th(', word-t)as(;d |,ransb> 
lion m('mory is n(',c(;ssary. 'Phis will t)e th(' main 
focus for futur(', work. 
As Mrea(ty mentioned, ;~ l~tngmtge modal for 
th(; tnrget l~mguag(; has to bc integrated into 
t;h(, scoring of the translation hyl)othes(,s. Fi- 
mflly, the l, rmmdu('er based al)t)roadl to transla- 
tion will 1)e tested on word lattice.s as i)rodu(:ed 
by spee,(:h recognition systeans. 
Acknowledgement .  This work was partly 
SUl)t)orted l)y the German Fede.ral Ministry of 
E(tuc~ttion, S(:ie.n(:e, ll.es(;m:ch mM 3b.(:hnoh)gy 
under the. Contract Nulnl)er 01 IV 701 Td 
(vl m vonu,). 
References  
R. 1). Brown. i\[996. Exmut)lc-1)ase, d machine 
translation in the pangloss system, l"rocc, cd- 
ings of the 16th, international Co~@rencc, on 
Computational Linguistics, 169-174, Copcn- 
tm,ge, n, l)emnark, August. 
It. l(itmJo. 1993. A COml)rehensive mM prn(> 
ti(-M model of memory-ha,seal machine trmls- 
la.tion, l~mcccdi,ng.~ of the 13th, hzl, c'r,natio'nal 
Joint Co'nfere, nce, o'n Art{/icial bl, tclligc,'n, ce, 
vohmm 2. 1276 1282. Morgmt Ka.ufmmm. 
F..\]. Och, C. Tillmmm, mM H. Ney. 1999. lm- 
prove, d aligmnent models for statistical ma- 
chilw, I;ranslation. Procceding,s of the Joint 
SIGDAT Co~@rcncc on Empirical Meth, ods 
in Na, t,wral Language PTwccs.sin9 and Very 
Large, Corpora, 20 28, University of Mm:y~ 
land, College Park, MD, USA, June. 
S. Sato and M. Nagao. 1990. Towmd memory- 
based tnmslation. P'rocc, edings of the 13th 
International Cm@rcnce on Computational 
Lingui,~tics, vol. 3, 24:7 ~252, Hclsinki, Fin- 
land. 
W. Wahlster. 1993. Vert)mobil: %'anslation of 
t'a(:c-to-fac(; dialogs. Proceedings of th, e MT 
Summit IV, 1.27 135, Kol)e, Jal)mL 
1135 
The RWTH System for Statistical Translation of Spoken
Dialogues
H. Ney, F. J. Och, S. Vogel
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen, University of Technology
D-52056 Aachen, Germany
ABSTRACT
This paper gives an overview of our work on statistical ma-
chine translation of spoken dialogues, in particular in the
framework of the Verbmobil project. The goal of the
Verbmobil project is the translation of spoken dialogues
in the domains of appointment scheduling and travel plan-
ning. Starting with the Bayes decision rule as in speech
recognition, we show how the required probability distri-
butions can be structured into three parts: the language
model, the alignment model and the lexicon model. We
describe the components of the system and report results
on the Verbmobil task. The experience obtained in the
Verbmobil project, in particular a large-scale end-to-end
evaluation, showed that the statistical approach resulted in
significantly lower error rates than three competing transla-
tion approaches: the sentence error rate was 29% in compar-
ison with 52% to 62% for the other translation approaches.
1. INTRODUCTION
In comparison with written language, speech and espe-
cially spontaneous speech poses additional difficulties for
the task of automatic translation. Typically, these difficul-
ties are caused by errors of the recognition process, which is
carried out before the translation process. As a result, the
sentence to be translated is not necessarily well-formed from
a syntactic point-of-view. Even without recognition errors,
speech translation has to cope with a lack of conventional
syntactic structures because the structures of spontaneous
speech differ from that of written language.
The statistical approach shows the potential to tackle
these problems for the following reasons. First, the statisti-
cal approach is able to avoid hard decisions at any level of
the translation process. Second, for any source sentence, a
translated sentence in the target language is guaranteed to
be generated. In most cases, this will be hopefully a syn-
tactically perfect sentence in the target language; but even
if this is not the case, in most cases, the translated sentence
will convey the meaning of the spoken sentence.
.
Whereas statistical modelling is widely used in speech
recognition, there are so far only a few research groups that
apply statistical modelling to language translation. The pre-
sentation here is based on work carried out in the framework
of the EuTrans project [8] and the Verbmobil project [25].
2. STATISTICAL DECISION THEORY
AND LINGUISTICS
2.1 The Statistical Approach
The use of statistics in computational linguistics has been
extremely controversial for more than three decades. The
controversy is very well summarized by the statement of
Chomsky in 1969 [6]:
?It must be recognized that the notion of a ?probability
of a sentence? is an entirely useless one, under any
interpretation of this term?.
This statement was considered to be true by the major-
ity of experts from artificial intelligence and computational
linguistics, and the concept of statistics was banned from
computational linguistics for many years.
What is overlooked in this statement is the fact that, in an
automatic system for speech recognition or text translation,
we are faced with the problem of taking decisions. It is
exactly here where statistical decision theory comes in. In
speech recognition, the success of the statistical approach is
based on the equation:
Speech Recognition = Acoustic?Linguistic Modelling
+ Statistical Decision Theory
Similarly, for machine translation, the statistical approach
is expressed by the equation:
Machine Translation = Linguistic Modelling
+ Statistical Decision Theory
For the ?low-level? description of speech and image signals,
it is widely accepted that the statistical framework allows
an efficient coupling between the observations and the mod-
els, which is often described by the buzz word ?subsymbolic
processing?. But there is another advantage in using prob-
ability distributions in that they offer an explicit formalism
for expressing and combining hypothesis scores:
? The probabilities are directly used as scores: These
scores are normalized, which is a desirable property:
when increasing the score for a certain element in the
set of all hypotheses, there must be one or several other
elements whose scores are reduced at the same time.
? It is straightforward to combine scores: depending
on the task, the probabilities are either multiplied or
added.
? Weak and vague dependencies can be modelled eas-
ily. Especially in spoken and written natural language,
there are nuances and shades that require ?grey levels?
between 0 and 1.
2.2 Bayes Decision Rule and
System Architecture
In machine translation, the goal is the translation of a
text given in a source language into a target language. We
are given a source string fJ1 = f1...fj ...fJ , which is to be
translated into a target string eI1 = e1...ei...eI . In this arti-
cle, the term word always refers to a full-form word. Among
all possible target strings, we will choose the string with the
highest probability which is given by Bayes decision rule [5]:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)} .
Here, Pr(eI1) is the language model of the target language,
and Pr(fJ1 |eI1) is the string translation model which will be
decomposed into lexicon and alignment models. The argmax
operation denotes the search problem, i.e. the generation
of the output sentence in the target language. The overall
architecture of the statistical translation approach is sum-
marized in Figure 1.
In general, as shown in this figure, there may be additional
transformations to make the translation task simpler for the
algorithm. The transformations may range from the cate-
gorization of single words and word groups to more complex
preprocessing steps that require some parsing of the source
string. We have to keep in mind that in the search procedure
both the language and the translation model are applied af-
ter the text transformation steps. However, to keep the
notation simple, we will not make this explicit distinction in
the subsequent exposition.
3. ALIGNMENT MODELLING
3.1 Concept
A key issue in modelling the string translation probabil-
ity Pr(fJ1 |eI1) is the question of how we define the corre-
spondence between the words of the target sentence and the
words of the source sentence. In typical cases, we can as-
sume a sort of pairwise dependence by considering all word
pairs (fj , ei) for a given sentence pair (fJ1 ; eI1). Here, we will
further constrain this model by assigning each source word
to exactly one target word. Later, this requirement will be
relaxed. Models describing these types of dependencies are
referred to as alignment models [5, 24].
When aligning the words in parallel texts, we typically
observe a strong localization effect. Figure 2 illustrates this
effect for the language pair German?English. In many cases,
although not always, there is an additional property: over
large portions of the source string, the alignment is mono-
tone.
Source Language Text
Transformation
 Lexicon Model
Language Model
Global Search:
 
 
Target Language Text
 
over
 
 Pr(f1  J  |e1I )
 
 
 Pr(   e1I )
 
 
 Pr(f1  J  |e1I )   Pr(   e1I )
  
e1I
f1 J
maximize  Alignment Model
Transformation
Figure 1: Architecture of the translation approach
based on Bayes decision rule.
3.2 Basic Models
To arrive at a quantitative specification, we define the
alignment mapping: j ? i = aj , which assigns a word fj
in position j to a word ei in position i = aj . We rewrite
the probability for the translation model by introducing the
?hidden? alignments aJ1 := a1...aj ...aJ for each sentence pair
(fJ1 ; eI1). To structure this probability distribution, we fac-
torize it over the positions in the source sentence and limit
the alignment dependencies to a first-order dependence:
Pr(fJ1 |eI1) = p(J |I) ?
X
aJ1
J
Y
j=1
[p(aj |aj?1, I, J) ? p(fj |eaj )] .
Here, we have the following probability distributions:
? the sentence length probability: p(J |I), which is in-
cluded here for completeness, but can be omitted with-
out loss of performance;
? the lexicon probability: p(f |e);
? the alignment probability: p(aj |aj?1, I, J).
By making the alignment probability p(aj |aj?1, I, J) depen-
dent on the jump width aj ? aj?1 instead of the absolute
positions aj , we obtain the so-called homogeneous hidden
Markov model, for short HMM [24].
We can also use a zero-order model p(aj |j, I, J), where
there is only a dependence on the absolute position index j
of the source string. This is the so-called model IBM-2 [5].
Assuming a uniform alignment probability p(aj |j, I, J) =
1/I, we arrive at the so-called model IBM-1.
These models can be extended to allow for source words
having no counterpart in the translation. Formally, this
is incorporated into the alignment models by adding a so-
called ?empty word? at position i = 0 to the target sentence
and aligning all source words without a direct translation to
this empty word.
well
I
think
if
we
can
make
it
at
eight
on
both
days
ja ich den
ke
we
nn wir das
hin
kri
ege
n an
bei
den
Ta
ge
n
ac
ht Uh
r
Figure 2: Word-to-word alignment.
In [5], more refined alignment models are introduced by
using the concept of fertility. The idea is that often a word
in the target language may be aligned to several words in
the source language. This is the so-called model IBM-3. Us-
ing, in addition, first-order alignment probabilities along the
positions of the source string leads us to model IBM-4. Al-
though these models take one-to-many alignments explicitly
into account, the lexicon probabilities p(f |e) are still based
on single words in each of the two languages.
In systematic experiments, it was found that the qual-
ity of the alignments determined from the bilingual training
corpus has a direct effect on the translation quality [14].
3.3 Alignment Template Approach
A general shortcoming of the baseline alignment models
is that they are mainly designed to model the lexicon de-
pendences between single words. Therefore, we extend the
approach to handle word groups or phrases rather than sin-
gle words as the basis for the alignment models [15]. In
other words, a whole group of adjacent words in the source
sentence may be aligned with a whole group of adjacent
words in the target language. As a result, the context of
words tends to be explicitly taken into account, and the
differences in local word orders between source and target
languages can be learned explicitly. Figure 3 shows some of
the extracted alignment templates for a sentence pair from
the Verbmobil training corpus. The training algorithm for
the alignment templates extracts all phrase pairs which are
aligned in the training corpus up to a maximum length of 7
words. To improve the generalization capability of the align-
ment templates, the templates are determined for bilingual
word classes rather than words directly. These word classes
are determined by an automatic clustering procedure [13].
4. SEARCH
The task of the search algorithm is to generate the most
likely target sentence eI1 of unknown length I for an observed
source sentence fJ1 . The search must make use of all three
knowledge sources as illustrated by Figure 4: the alignment
model, the lexicon model and the language model. All three
okay
,
how
about
the
nineteenth
at
maybe
,
two
o?clock
in
the
afternoon
?
oka
y ,
wie
sie
ht es am
ne
un
ze
hnt
en aus ,
vie
lle
ich
t um
zw
ei Uhr
na
chm
itt
ags ?
Figure 3: Example of a word alignment and of ex-
tracted alignment templates.
of them must contribute in the final decision about the words
in the target language.
To illustrate the specific details of the search problem, we
slightly change the definitions of the alignments:
? we use inverted alignments as in the model IBM-4 [5]
which define a mapping from target to source positions
rather the other way round.
? we allow several positions in the source language to be
covered, i.e. we consider mappings B of the form:
B : i ? Bi ? {1, ...j, ...J}
We replace the sum over all alignments by the best
alignment, which is referred to as maximum approxima-
tion in speech recognition. Using a trigram language model
p(ei|, ei?2, ei?1), we obtain the following search criterion:
max
BI1 ,eI1
I
Y
i=1
2
4[p(ei|ei?1i?2) ? p(Bi|Bi?1, I, J) ?
Y
j?Bi
p(fj |ei)]
3
5
Considering this criterion, we can see that we can build
up hypotheses of partial target sentences in a bottom-to-
top strategy over the positions i of the target sentence ei1
as illustrated in Figure 5. An important constraint for the
alignment is that all positions of the source sentence should
be covered exactly once. This constraint is similar to that
of the travelling salesman problem where each city has to
be visited exactly once. Details on various search strategies
can be found in [4, 9, 12, 21].
In order to take long context dependences into account,
we use a class-based five-gram language model with backing-
off. Beam-search is used to handle the huge search space. To
normalize the costs of partial hypotheses covering different
parts of the input sentence, an (optimistic) estimation of the
remaining cost is added to the current accumulated cost as
follows. For each word in the source sentence, a lower bound
on its translation cost is determined beforehand. Using this
SENTENCE INSOURCE LANGUAGE
TRANSFORMATION
SENTENCE GENERATEDIN TARGET LANGUAGE
SENTENCE 
 
KNOWLEDGE SOURCESSEARCH: INTERACTION OF 
                            KNOWLEDGE SOURCES
WORD + POSITION
ALIGNMENT 
     
LANGUAGE MODEL
BILINGUAL LEXICON 
ALIGNMENTMODELWORD RE-ORDERING
SYNTACTIC ANDSEMANTIC ANALYSIS
LEXICAL CHOICE
HYPOTHESES
HYPOTHESES
HYPOTHESES
TRANSFORMATION
Figure 4: Illustration of search in statistical trans-
lation.
lower bound, it is possible to achieve an efficient estimation
of the remaining cost.
5. EXPERIMENTAL RESULTS
5.1 The Task and the Corpus
Within the Verbmobil project, spoken dialogues were
recorded. These dialogues were manually transcribed and
later manually translated by Verbmobil partners (Hildes-
heim for Phase I and Tu?bingen for Phase II). Since different
human translators were involved, there is great variability
in the translations.
Each of these so-called dialogues turns may consist of sev-
eral sentences spoken by the same speaker and is sometimes
SOURCE POSITION
TA
RG
ET
 P
O
SI
TI
O
N
i
i-1
j
Figure 5: Illustration of bottom-to-top search.
rather long. As a result, there is no one-to-one correspon-
dence between source and target sentences. To achieve a
one-to-one correspondence, the dialogue turns are split into
shorter segments using punctuation marks as potential split
points. Since the punctuation marks in source and target
sentences are not necessarily identical, a dynamic program-
ming approach is used to find the optimal segmentation
points. The number of segments in the source sentence and
in the test sentence can be different. The segmentation is
scored using a word-based alignment model, and the seg-
mentation with the best score is selected. This segmented
corpus is the starting point for the training of translation
and language models. Alignment models of increasing com-
plexity are trained on this bilingual corpus [14].
A standard vocabulary had been defined for the various
speech recognizers used in Verbmobil. However, not all
words of this vocabulary were observed in the training cor-
pus. Therefore, the translation vocabulary was extended
semi-automatically by adding about 13 000 German?English
word pairs from an online bilingual lexicon available on the
web. The resulting lexicon contained not only word-word
entries, but also multi-word translations, especially for the
large number of German compound words. To counteract
the sparseness of the training data, a couple of straightfor-
ward rule-based preprocessing steps were applied before any
other type of processing:
? categorization of proper names for persons and cities,
? normalization of:
? numbers,
? time and date phrases,
? spelling: don?t ? do not,...
? splitting of
German compound words.
Table 1 gives the characteristics of the training corpus
and the lexicon. The 58 000 sentence pairs comprise about
half a million running words for each language of the bilin-
gual training corpus. The vocabulary size is the number of
distinct full-form words seen in the training corpus. Punctu-
ation marks are treated as regular words in the translation
approach. Notice the large number of word singletons, i. e.
words seen only once. The extended vocabulary is the vo-
cabulary after adding the manual bilingual lexicon.
5.2 Offline Results
During the progress of the Verbmobil project, different
variants of statistical translation were implemented, and ex-
Table 1: Bilingual training corpus, recognition lex-
icon and translation lexicon (PM = punctuation
mark).
German English
Training Text Sentences 58 332
Words (+PMs) 519 523 549 921
Vocabulary 7 940 4 673
Singletons 44.8% 37.6%
Recognition Vocabulary 10 157 6 871
Translation Manual Pairs 12 779
Ext. Vocab. 11 501 6 867
perimental tests were performed for both text and speech
input. To summarize these experimental tests, we briefly
report experimental offline results for the following transla-
tion approaches:
? single-word based approach [20];
? alignment template approach [15];
? cascaded transducer approach [23]:
unlike the other two-approaches, this approach re-
quires a semi-automatic training procedure, in which
the structure of the finite state transducers is designed
manually. For more details, see [23].
The offline tests were performed on text input for the trans-
lation direction from German to English. The test set con-
sisted of 251 sentences, which comprised 2197 words and 430
punctuation marks. The results are shown in Table 2. To
judge and compare the quality of different translation ap-
proaches in offline tests, we typically use the following error
measures [11]:
? mWER (multi-reference word error rate):
For each test sentence sk in the source language, there
are several reference translationsRk = {rk1, . . . , rknk}
in the target language. For each translation of the test
sentence sk, the edit distances (number of substitu-
tions, deletions and insertions as in speech recognition)
to all sentences in Rk are calculated, and the smallest
distance is selected and used as error measure.
? SSER (subjective sentence error rate):
Each translated sentence is judged by a human exam-
iner according to an error scale from 0.0 (semantically
and syntactically correct) to 1.0 (completely wrong).
Both error measures are reported in Table 2. Although
the experiments with the cascaded transducers [23] were not
fully optimized yet, the preliminary results indicated that
this semi-automatic approach does not generalize as well
as the other two fully automatic approaches. Among these
two, the alignment template approach was found to work
consistently better across different test sets (and also tasks
different from Verbmobil). Therefore, the alignment tem-
plate approach was used in the final Verbmobil prototype
system.
5.3 Disambiguation Examples
In the statistical translation approach as we have pre-
sented it, no explicit word sense disambiguation is per-
formed. However, a kind of implicit disambiguation is pos-
sible due to the context information of the alignment tem-
plates and the language model as shown by the examples
in Table 3. The first two groups of sentences contain the
Table 2: Comparison of three statistical translation
approaches (test on text input: 251 sentences =
2197 words + 430 punctuation marks).
Translation mWER SSER
Approach [%] [%]
Single-Word Based 38.2 35.7
Alignment Template 36.0 29.0
Cascaded Transducers >40.0 >40.0
verbs ?gehen? and ?annehmen? which have different transla-
tions, some of which are rather collocational. The correct
translation is only possible by taking the whole sentence
into account. Some improvement can be achieved by ap-
plying morpho-syntactic analysis, e.g handling of the sepa-
rated verb prefixes in German [10]. The last two sentences
show the implicit disambiguation of the temporal and spa-
tial sense for the German preposition ?vor?. Although the
system has not been tailored to handle such types of disam-
biguation, the translated sentences are all acceptable, apart
from the sentence: The meeting is to five.
5.4 Integration into the Verbmobil Prototype
System
The statistical approach to machine translation is em-
bodied in the stattrans module which is integrated into the
Verbmobil prototype system. We briefly review those as-
pects of it that are relevant for the statistical translation ap-
proach. The implementation supports the translation direc-
tions from German to English and from English to German.
In regular processing mode, the stattrans module receives
its input from the repair module [18]. At that time, the
word lattices and best hypotheses from the speech recogni-
tion systems have already been prosodically annotated, i.e.
information about prosodic segment boundaries, sentence
mode and accentuated syllables are added to each edge in
the word lattice [2]. The translation is performed on the
single best sentence hypothesis of the recognizer.
The prosodic boundaries and the sentence mode informa-
tion are utilized by the stattrans module as follows. If there
is a major phrase boundary, a full stop or question mark is
inserted into the word sequence, depending on the sentence
mode as indicated by the prosody module. Additional com-
mas are inserted for other types of segment boundaries. The
prosody module calculates probabilities for segment bound-
aries, and thresholds are used to decide if the sentence marks
are to be inserted. These thresholds have been selected in
such a way that, on the average, for each dialogue turn, a
good segmentation is obtained. The segment boundaries re-
strict possible word reordering between source and target
language. This not only improves translation quality, but
also restricts the search space and thereby speeds up the
translation process.
5.5 Large-Scale End-to-End Evaluation
Whereas the offline tests reported above were important
for the optimization and tuning of the system, the most
important evaluation was the final evaluation of the Verb-
mobil prototype in spring 2000. This end-to-end evaluation
of the Verbmobil system was performed at the University
of Hamburg [19]. In each session of this evaluation, two
native speakers conducted a dialogue. They did not have
any direct contact and could only interact by speaking and
listening to the Verbmobil system.
Three other translation approaches had been integrated
into the Verbmobil prototype system:
? a classical transfer approach [3, 7, 22],
which is based on a manually designed analysis gram-
mar, a set of transfer rules, and a generation grammar,
? a dialogue act based approach [16],
which amounts to a sort of slot filling by classifying
Table 3: Disambiguation examples (?: using morpho-syntactic analysis).
Ambiguous Word Text Input Translation
gehen Wir gehen ins Theater. We will go to the theater.
Mir geht es gut. I am fine.
Es geht um Geld. It is about money.
Geht es bei Ihnen am Montag? Is it possible for you on Monday?
Das Treffen geht bis 5 Uhr. The meeting is to five.
annehmen Wir sollten das Angebot annehmen. We should accept that offer.
Ich nehme das Schlimmste an. I will assume the worst.?
vor Wir treffen uns vor dem Fru?hstu?ck. We meet before the breakfast.
Wir treffen uns vor dem Hotel. We will meet in front of the hotel.
each sentence into one out of a small number of possi-
ble sentence patterns and filling in the slot values,
? an example-based approach [1],
where a sort of nearest neighbour concept is applied to
the set of bilingual training sentence pairs after suit-
able preprocessing.
In the final end-to-end evaluation, human evaluators
judged the translation quality for each of the four trans-
lation results using the following criterion:
Is the sentence approximatively correct: yes/no?
The evaluators were asked to pay particular attention to
the semantic information (e.g. date and place of meeting,
participants etc) contained in the translation. A missing
translation as it may happen for the transfer approach or
other approaches was counted as wrong translation. The
evaluation was based on 5069 dialogue turns for the trans-
lation from German to English and on 4136 dialogue turns
for the translation from English to German. The speech
recognizers used had a word error rate of about 25%. The
overall sentence error rates, i.e. resulting from recognition
and translation, are summarized in Table 4. As we can see,
the error rates for the statistical approach are smaller by a
factor of about 2 in comparison with the other approaches.
In agreement with other evaluation experiments, these ex-
periments show that the statistical modelling approach may
be comparable to or better than the conventional rule-based
approach. In particular, the statistical approach seems to
have the advantage if robustness is important, e.g. when
the input string is not grammatically correct or when it is
corrupted by recognition errors.
Although both text and speech input are translated with
good quality on the average by the statistical approach,
Table 4: Sentence error rates of end-to-end evalua-
tion (speech recognizer with WER=25%; corpus of
5069 and 4136 dialogue turns for translation Ger-
man to English and English to German, respec-
tively).
Translation Method Error [%]
Semantic Transfer 62
Dialogue Act Based 60
Example Based 52
Statistical 29
there are examples where the syntactic structure of the pro-
duced sentence is not correct. Some of these syntactic errors
are related to long range dependencies and syntactic struc-
tures that are not captured by the m-gram language model
used. To cope with these problems, morpho-syntactic anal-
ysis [10] and grammar-based language models [17] are cur-
rently being studied.
6. SUMMARY
In this paper, we have given an overview of the statistical
approach to machine translation and especially its imple-
mentation in the Verbmobil prototype system. The sta-
tistical system has been trained on about 500 000 running
words from a bilingual German?English corpus. Transla-
tions are performed for both directions, i.e. from German
to English and from English to German. Comparative eval-
uations with other translation approaches of the Verbmo-
bil prototype system show that the statistical translation
is superior, especially in the presence of speech input and
ungrammatical input.
Acknowledgment
The work reported here was supported partly by the Verb-
mobil project (contract number 01 IV 701 T4) by the Ger-
man Federal Ministry of Education, Science, Research and
Technology and as part of the EuTrans project (ESPRIT
project number 30268) by the European Community.
Training Toolkit
In a follow-up project of the statistical machine translation
project during the 1999 Johns Hopkins University workshop,
we have developped a publically available toolkit for the
training of different alignment models, including the models
IBM-1 to IBM-5 [5] and an HMM alignment model [14, 24].
The software can be downloaded at
http://www-i6.Informatik.RWTH-Aachen.DE/
~och/software/GIZA++.html.
7. REFERENCES
[1] M. Auerswald: Example-based machine translation
with templates. In [25], pp. 418?427.
[2] A. Batliner, J. Buckow, H. Niemann, E. No?th,
V. Warnke: The prosody module. In [25], pp. 106?
121.
[3] T. Becker, A. Kilger, P. Lopez, P. Poller: The
Verbmobil generation component VM-GECO. In [25],
pp. 481?496.
[4] A. L. Berger, P. F. Brown, J. Cocke, S. A. Della Pietra,
V. J. Della Pietra, J. R. Gillett, J. D. Lafferty,
R. L. Mercer, H. Printz,L. Ures: The Candide Sys-
tem for Machine Translation. ARPA Human Lan-
guage Technology Workshop, Plainsboro, NJ, Morgan
Kaufmann Publishers, pp. 152-157, San Mateo, CA,
March 1994.
[5] P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
R. L. Mercer: The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, Vol. 19, No. 2, pp. 263?311, 1993.
[6] N. Chomsky: ?Quine?s Empirical Assumptions?, in
D. Davidson, J. Hintikka (eds.): Words and objections.
Essays on the work of W. V. Quine, Reidel, Dordrecht,
The Netherlands, 1969.
[7] M. C. Emele, M. Dorna, A. Lu?deling, H. Zinsmeister,
C. Rohrer: Semantic-based transfer. In [25], pp. 359?
376.
[8] EuTrans Project; Instituto Tecnolo?gico de Informa?tica
(ITI, Spain), Fondazione Ugo Bordoni (FUB, Italy),
RWTH Aachen, Lehrstuhl f. Informatik VI (Ger-
many), Zeres GmbH Bochum (Germany): Example-
Based Language Translation Systems. Final report of
the EuTrans project (EU project number 30268), July
2000.
[9] H. Ney, S. Nie?en, F. J. Och, H. Sawaf, C. Tillmann,
S. Vogel: Algorithms for statistical translation of spo-
ken language. IEEE Trans. on Speech and Audio Pro-
cessing Vol. 8, No. 1, pp. 24?36, Jan. 2000.
[10] S. Nie?en, H. Ney: Improving SMT quality with
morpho-syntactic analysis. 18th Int. Conf. on Compu-
tational Linguistics, pp. 1081-1085, Saarbru?cken, Ger-
many, July 2000.
[11] S. Nie?en, F.-J. Och, G. Leusch, H. Ney: An evalua-
tion tool for machine translation: Fast evaluation for
MT research. 2nd Int. Conf. on Language Resources
and Evaluation, pp.39?45, Athens, Greece, May 2000.
[12] S. Nie?en, S. Vogel, H. Ney, C. Tillmann: A DP
based search algorithm for statistical machine transla-
tion. COLING?ACL ?98: 36th Annual Meeting of the
Association for Computational Linguistics and 17th
Int. Conf. on Computational Linguistics, pp. 960?967,
Montreal, Canada, Aug. 1998.
[13] F. J. Och: An efficient method to determine bilingual
word classes. 9th Conf. of the European Chapter of the
Association for Computational Linguistics, pp. 71?76,
Bergen, Norway, June 1999.
[14] F. J. Och, H. Ney: A comparison of alignment
models for statistical machine translation. 18th Int.
Conf. on Computational Linguistics, pp. 1086-1090,
Saarbru?cken, Germany, July 2000.
[15] F. J. Och, C. Tillmann, H. Ney: Improved alignment
models for statistical machine translation. Joint SIG-
DAT Conf. on Empirical Methods in Natural Language
Processing and Very Large Corpora, 20?28, University
of Maryland, College Park, MD, June 1999.
[16] N. Reithinger, R. Engel: Robust content extraction for
translation and dialog processing. In [25], pp. 428?437.
[17] H. Sawaf, K. Schu?tz, H. Ney: On the use of grammar
based language models for statistical machine trans-
lation. 6th Int. Workshop on Parsing Technologies,
pp. 231?241, Trento, Italy, Feb. 2000.
[18] J. Spilker, M. Klarner, G. Go?rz: Processing self-
corrections in a speech-to-speech system. In [25],
pp. 131?140.
[19] L. Tessiore, W. v. Hahn: Functional validation of
a machine translation system: Verbmobil. In [25],
pp. 611?631.
[20] C. Tillmann, H. Ney: Word re-ordering in a DP-based
approach to statistical MT. 18th Int. Conf. on Com-
putational Linguistics 2000, Saarbru?cken, Germany,
pp. 850-856, Aug. 2000.
[21] C. Tillmann, S. Vogel, H. Ney, A. Zubiaga: A DP-
based search using monotone alignments in statisti-
cal translation. 35th Annual Conf. of the Association
for Computational Linguistics, pp. 289?296, Madrid,
Spain, July 1997.
[22] H. Uszkoreit, D. Flickinger, W. Kasper, I. A. Sag:
Deep linguistic analysis with HPSG. In [25], pp. 216?
263.
[23] S. Vogel, H. Ney: Translation with Cascaded Finite-
State Transducers. ACL Conf. (Assoc. for Comput.
Linguistics), Hongkong, pp. 23-30, Oct. 2000.
[24] S. Vogel, H. Ney, C. Tillmann: HMM-based word
alignment in statistical translation. 16th Int. Conf. on
Computational Linguistics, pp. 836?841, Copenhagen,
Denmark, August 1996.
[25] W. Wahlster (Ed.): Verbmobil: Foundations of speech-
to-speech translations. Springer-Verlag, Berlin, Ger-
many, 2000.
 
	ffProceedings of NAACL HLT 2007, pages 500?507,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An Efficient Two-Pass Approach to Synchronous-CFG Driven Statistical MT
Ashish Venugopal and Andreas Zollmann and Stephan Vogel
School of Computer Science, Carnegie Mellon University, Pittsburgh
interACT Lab, University of Karlsruhe
{ashishv,zollmann,vogel+}@cs.cmu.edu
Abstract
We present an efficient, novel two-pass
approach to mitigate the computational
impact resulting from online intersection
of an n-gram language model (LM) and
a probabilistic synchronous context-free
grammar (PSCFG) for statistical machine
translation. In first pass CYK-style decod-
ing, we consider first-best chart item ap-
proximations, generating a hypergraph of
sentence spanning target language deriva-
tions. In the second stage, we instantiate
specific alternative derivations from this
hypergraph, using the LM to drive this
search process, recovering from search er-
rors made in the first pass. Model search
errors in our approach are comparable to
those made by the state-of-the-art ?Cube
Pruning? approach in (Chiang, 2007) un-
der comparable pruning conditions evalu-
ated on both hierarchical and syntax-based
grammars.
1 Introduction
Syntax-driven (Galley et al, 2006) and hierarchi-
cal translation models (Chiang, 2005) take advan-
tage of probabilistic synchronous context free gram-
mars (PSCFGs) to represent structured, lexical re-
ordering constraints during the decoding process.
These models extend the domain of locality (over
phrase-based models) during decoding, represent-
ing a significantly larger search space of possible
translation derivations. While PSCFG models are
often induced with the goal of producing grammati-
cally correct target translations as an implicit syntax-
structured language model, we acknowledge the
value of n-gram language models (LM) in phrase-
based approaches.
Integrating n-gram LMs into PSCFGs based de-
coding can be viewed as online intersection of the
PSCFG grammar with the finite state machine rep-
resented by the n-gram LM, dramatically increasing
the effective number of nonterminals in the decoding
grammar, rendering the decoding process essentially
infeasible without severe, beam-based lossy prun-
ing. The alternative, simply decoding without the
n-gram LM and rescoring N-best alternative transla-
tions, results in substantially more search errors, as
shown in (Zollmann and Venugopal, 2006).
Our two-pass approach involves fast, approximate
synchronous parsing in a first stage, followed by a
second, detailed exploration through the resulting
hypergraph of sentence spanning derivations, using
the n-gram LM to drive that search. This achieves
search errors comparable to a strong ?Cube Pruning?
(Chiang, 2007), single-pass baseline. The first pass
corresponds to a severe parameterization of Cube
Pruning considering only the first-best (LM inte-
grated) chart item in each cell while maintaining un-
explored alternatives for second-pass consideration.
Our second stage allows the integration of long dis-
tance and flexible history n-gram LMs to drive the
search process, rather than simply using such mod-
els for hypothesis rescoring.
We begin by discussing the PSCFG model for
statistical machine translation, motivating the need
500
for effective n-gram LM integration during decod-
ing. We then present our two-pass approach and
discuss Cube Pruning as a state-of-the-art baseline.
We present results in the form of search error analy-
sis and translation quality as measured by the BLEU
score (Papineni et al, 2002) on the IWSLT 06 text
translation task (Eck and Hori, 2005)1, comparing
Cube Pruning with our two-pass approach.
2 Synchronous Parsing for SMT
Probabilistic Synchronous Context Free Grammar
(PSCFG) approaches to statistical machine transla-
tion use a source terminal set (source vocabulary)
TS , a target terminal set (target vocabulary) TT and
a shared nonterminal set N and induce rules of the
form
X ? ??, ?,?, w?
where (i) X ? N is a nonterminal, (ii) ? ? (N ?
TS)? is a sequence of nonterminals and source ter-
minals, (iii) ? ? (N ?TT )? is a sequence of nonter-
minals and target terminals, (iv) the number cnt(?)
of nonterminal occurrences in ? is equal to the num-
ber cnt(?) of nonterminal occurrences in ?, (v)
?: {1, . . . , cnt(?)} ? {1, . . . , cnt(?)} is a one-to-
one mapping from nonterminal occurrences in ? to
nonterminal occurrences in ?, and (vi) w ? [0,?)
is a non-negative real-valued weight assigned to the
rule. We will assume ? to be implicitly defined by
indexing the NT occurrences in ? from left to right
starting with 1, and by indexing the NT occurrences
in ? by the indices of their corresponding counter-
parts in ?. Syntax-oriented PSCFG approaches typ-
ically ignore source structure, instead focussing on
generating syntactically well formed target deriva-
tions. (Galley et al, 2006) use syntactic constituents
for the PSCFG nonterminal set and (Zollmann and
Venugopal, 2006) take advantage of CCG (Steed-
man, 1999) categories, while (Chiang, 2005) uses
a single generic nonterminal. PSCFG derivations
function analogously to CFG derivations. Given
a source sentence f , the translation task under a
PSCFG grammar can be expressed as
1While IWSLT represents a limited resource translation task
(120K sentences of training data for Chinese-English), the prob-
lem of efficient n-gram LM integration is still critically impor-
tant to efficient decoding, and our contributions can be expected
to have an even more significant impact when decoding with
grammars induced from larger corpora.
e? = argmax
{e | ?D. src(D)=f,tgt(D)=e}
P (D)
where tgt(D) refers to the target terminal symbols
generated by the derivation D and src(D) refers to
the source terminal symbols spanned by D. The
score (also laxly called probability, since we never
need to compute the partition function) of a deriva-
tion D under a log-linear model, referring to the
rules r used in D, is:
P (D) =
1
Z
PLM (tgt(D))
?LM ?
?
i
?
r?D
?i(r)
?i
where ?i refers to features defined on each rule,
and PLM is a g-gram LM probability applied to the
target terminal symbols generated by the derivation
D. Introducing the LM feature defines dependen-
cies across adjacent rules used in each derivation,
and requires modifications to the decoding strategy.
Viewing the LM as a finite-state machine, the de-
coding process involves performing an intersection
between the PSCFG grammar and the g-gram LM
(Bar-Hillel et al, 1964). We present our work under
the construction in (Wu, 1996), following notation
from (Chiang, 2007), extending the formal descrip-
tion to reflect grammars with an arbitrary number of
nonterminals in each rule.
2.1 Decoding Strategies
In Figure 1, we reproduce the decoding algorithm
from (Chiang, 2007) that applies a PSCFG to
translate a source sentence in the same notation (as
a deductive proof system (Shieber et al, 1995)),
generalized to handle more than two non-terminal
pairs. Chart items [X, i, j, e] : w span j ? i words
in the source sentence f1 ? ? ? fn, starting at position
i + 1, and have weight w (equivalent to P (D)), and
e ? (TT ? {?})? is a sequence of target terminals,
with possible elided parts, marked by ?. Functions
p, q whose domain is TT ? {?} are defined in
(Chiang, 2007) and are repeated here for clarity.
p(a1 ? ? ? am) =
Y
g?i?m,?/?ai?g+1???ai?1
PLM (ai|ai?g+1 ? ? ? ai?1)
q(a1 ? ? ? am) =
(
a1 ? ? ? ag?1 ? am?g+2 ? ? ? am if m ? g
a1 ? ? ? am else
The function q elides elements from a target lan-
guage terminal sequence, leaving the leftmost and
rightmost g ? 1 words, replacing the elided words
501
X ? ??, ?? : w
(X ? ??, ?, w?) ? G
X ? ?f ji+1, ?? : w
[X, i, j; q(?)] : wp(?)
Z ? ?f i1i+1(X
1)1f
i2
j1+1
? ? ? (Xm?1)m?1f
im
jm?1+1
(Xm)mf
j
jm+1
, ?? : w
?
X1, i1, j1; e1
?
: w1 ? ? ? [Xm, im, jm; em] : wm
[Z, i, j, q(??)] : ww1 ? ? ?wmp(??) (where ?? = ? [e1/(X1)1, . . . , em/(Xm)m])
Goal item:
?
S, 0, n; ?s?g?1 ? ?\s?g?1
?
Figure 1. CYK parsing with integrated g-gram LM. The inference rules are explored in ascending order of j ? i. Here
? [e/Yi] is the string ? where the NT occurrence Yi is replaced by e. Functions q and p are explained in the text.
with a single ? symbol. The function p returns g-
gram LM probabilities for target terminal sequences,
where the ? delineates context boundaries, prevent-
ing the calculation from spanning this boundary. We
add a distinguished start nonterminal S to gener-
ate sentences spanning target translations beginning
with g ? 1 ?s? symbols and ending with g ? 1 ?\s?
symbols. This can e.g. be achieved by adding for
each nonterminal X a PSCFG rule
S ? ?X, ?s?g?1X?\s?g?1, 1?
We are only searching for the derivation of highest
probability, so we can discard identical chart items
that have lower weight. Since chart items are de-
fined by their left-hand side nonterminal production,
span, and the LM contexts e, we can safely discard
these identical items since q has retained all context
that could possibly impact the LM calculation. This
process is commonly referred to as item recombina-
tion. Backpointers to antecedent cells are typically
retained to allow N -Best extraction using an algo-
rithm such as (Huang and Chiang, 2005).
The impact of g-gram LM intersection during de-
coding is apparent in the final deduction step. Gen-
erating the set of consequent Z chart items involves
combining m previously produced chart cells. Since
each of these chart cells with given source span [i, j]
is identified by nonterminal symbol X and LM con-
text e, we have at worst |N | ? |TT |2(g?1) such chart
cells in a span. The runtime of this algorithm is thus
O
(
n3
[
|N ||TT |
2(g?1)
]K
)
where K is the maximum number of NT pairs per
rule and n the source sentence length. Without se-
vere pruning, this runtime is prohibitive for even the
smallest induced grammars. Traditional pruning ap-
proaches that limit the number of consequents after
they are produced are not effective since they first re-
quire that the cost of each consequent be computed
(which requires calls to the g-gram LM).
Restrictions to the grammar afford alternative de-
coding strategies to reduce the runtime cost of syn-
chronous parsing. (Zhang et al, 2006) ?binarize?
grammars into CNF normal form, while (Watan-
abe et al, 2006) allow only Griebach-Normal form
grammars. (Wellington et al, 2006) argue that these
restrictions reduce our ability to model translation
equivalence effectively. We take an agnostic view
on the issue; directly addressing the question of effi-
cient LM intersection rather than grammar construc-
tion.
3 Two-pass LM Intersection
We propose a two-pass solution to the problem of
online g-gram LM intersection. A naive two-pass
approach would simply ignore the LM interactions
during parsing, extract a set of N derivations from
the sentence spanning hypergraph and rescore these
derivations with the g-gram LM. In practice, this ap-
proach performs poorly (Chiang, 2007; Zollmann
and Venugopal, 2006). While parsing time is dra-
matically reduced (and N -best extraction time is
negligible), N is typically significantly less than the
complete number of possible derivations and sub-
stantial search errors remain. We propose an ap-
proach that builds upon the concept of a second pass
but uses the g-gram LM to search for alternative,
better translations.
502
3.1 First pass: parsing
We begin by relaxing the criterion that determines
when two chart items are equivalent during parsing.
We consider two chart items to be equivalent (and
therefore candidates for recombination) if they have
matching left-hand side nonterminals, and span. We
no longer require them to have the same LM con-
text e. We do however propagate the e, w for the
chart item with highest score, causing the algorithm
to still compute LM probabilities during parsing. As
a point of notation, we refer to such a chart item by
annotating its e, w as e1, w1, and we refer to them
as approximate items (since they have made a first-
best approximation for the purposes of LM calcula-
tion). These approximate items labeled with e1, w1
are used in consequent parse calculations.
The parsing algorithm under this approximation
stays unchanged, but parsing time is dramatically re-
duced. The runtime complexity of this algorithm is
now O
(
n3|N |K
)
at the cost of significant search
errors (since we ignored most LM contexts that we
encountered).
This relaxation is different from approaches that
do not use the LM during parsing. The sentence
spanning item does have LM probabilities associ-
ated with it (but potentially valuable chart items
were not considered during parsing). Like in tra-
ditional parsing, we retain the recombined items in
the cell to allow us to explore new derivations in a
second stage.
3.2 Second pass: hypergraph search
The goal item of the parsing step represents a sen-
tence spanning hypergraph of alternative deriva-
tions. Exploring alternatives from this hyper-
graph and updating LM probabilities can now reveal
derivations with higher scores that were not consid-
ered in the first pass. Exploring the whole space of
alternative derivations in this hypergraph is clearly
infeasible. We propose a g-gram LM driven heuris-
tic search ?H.Search? of this space that allows the g-
gram LM to decide which section of the hypergraph
to explore. By construction, traversing a particular
derivation item from the parse chart in target-side
left-to-right, depth-first order yields the correctly or-
dered sequence of target terminals that is the transla-
tion represented by this item. Now consider a partial
traversal of the item in that order, where we gener-
ate only the first M target terminals, leaving the rest
of the item in its original backpointer form. We in-
formally define our second pass algorithm based on
these partial derivation items.
Consider a simple example, where we have parsed
a source sentence, and arrived at a sentence spanning
item obtained from a rule with the following target
side:
NP2 VP3 PP1
and that the item?s best-score estimate is w. A par-
tial traversal of this item would replace NP2 with
one of the translations available in the chart cell un-
derlying NP2 (called ?unwinding?), and recalculate
the weights associated with this item, taking into
account the alternative target terminals. Assuming
?the nice man? was the target side of the best scoring
item in NP2, the respective traversal would main-
tain the same weight. An alternative item at NP2
might yield ?a nice man?. This partial traversal rep-
resents a possible item that we did not consider dur-
ing parsing, and recalculating LM probabilities for
this new item (based on approximate items VP3 and
PP1) yields weight w2:
the nice man VP3 PP1 : w1 = w
a nice man VP3 PP1 : w2
Alternative derivation items that obtain a higher
score than the best-score estimates represent recov-
ery from search errors. Our algorithm is based on
the premise that these items should be traversed fur-
ther, with the LM continuing to score newly gener-
ated target words. These partially traversed items
are placed on an agenda (sorted by score). At each
step of the second pass search, we select those items
from the agenda that are within a search beam of Z
from the best item, and perform the unwind opera-
tion on each of these items. Since we unwind partial
items from left-to-right the g-gram LM is able to in-
fluence the search through the space of alternative
derivations.
Applying the g-gram LM on partial items with
leading only-terminal symbols allows the integra-
tion of high- / flexible-order LMs during this sec-
ond stage process, and has the advantage of explor-
ing only those alternatives that participate in sen-
tence spanning, high scoring (considering both LM
and translation model scores) derivations. While
503
we do not evaluate such models here, we note that
H.Search was developed specifically for the integra-
tion of such models during search.
We further note that partial items that have gen-
erated translations that differ only in the word po-
sitions up to g ? 1 words before the first nonter-
minal site can be recombined (for the same rea-
sons as during LM intersected parsing). For exam-
ple, when considering a 3-gram LM, the two par-
tial items above can be recombined into one equiv-
alence class, since partial item LM costs resulting
from these items would only depend on ?nice man?,
but not on ?a? vs. ?the?. Even if two partial items
are candidates for recombination due to their termi-
nal words, they must also have identical backpoint-
ers (representing a set of approximate parse deci-
sions for the rest of the sentence, in our example
VP3PP1 ). Items that are filed into existing equiv-
alence classes with a lower score are not put onto
the agenda, while those that are better, or have cre-
ated new equivalence classes are scheduled onto the
agenda. For each newly created partial derivation,
we also add a backpointer to the ?parent? partial
derivation that was unwound to create it.
This equivalence classing operation transforms
the original left-hand side NT based hypergraph into
an (ordinary) graph of partial items. Each equiva-
lence class is a node in this new graph, and recom-
bined items are the edges. Thus, N -best extraction
can now be performed on this graph. We use the
extraction method from (Huang and Chiang, 2005).
The expensive portion of our algorithm lies in the
unwinding step, in which we generate a new par-
tial item for each alternative at the non-terminal site
that we are ?unwinding?. For each new partial item,
we factor out LM estimates and rule weights that
were used to score the parent item, and factor in
the LM probabilities and rule weights of the alter-
native choice that we are considering. In addition,
we must also update the new item?s LM estimates
for the remaining non-terminal and terminal sym-
bols that depend on this new left context of termi-
nals. Fortunately, the number of LM calculations
per new item is constant, i.e., does not dependent on
the length of the partial derivation, or how unwound
it is. Only (g ? 1) ? 2 LM probabilities have to be
re-evaluated per partial item. We now define this
?unwind-recombine? algorithm formally.
3.2.1 The unwind-recombine algorithm
Going back to the first-pass parsing algorithm
(Figure 1), remember that each application of a
grammar rule containing nonterminals corresponds
to an application of the third inference rule of the
algorithm. We can assign chart items C created by
the third inference rule a back-pointer (BP) target
side as follows: When applying the third inference
rule, each nonterminal occurrence (Xk)k in the cor-
responding Z ? ??, ?? grammar rule corresponds
to a chart cell [Xk, ik, jk] used as an antecedent for
the inference rule. We assign a BP target side for C
by replacing NT occurrences in ? (from the rule that
created C) with backpointers to their corresponding
antecedent chart cells. Further we define the distin-
guished backpointer PS as the pointer to the goal
cell [S, 0, n] : w?.
The deductive program for our second-pass al-
gorithm is presented in Figure 2. It makes use of
two kind of items. The first, {P ? ?; e1} : w,
links a backpointer P to a BP target side, storing
current-item vs. best-item correction terms in form
of an LM context e1 and a relative score w. The
second item form [[e;?]] in this algorithm corre-
sponds to partial left-to-right traversal states as de-
scribed above, where e is the LM context of the tra-
versed and unwound translation part, and ? the part
that is yet to be traversed and whose backpointers
are still to be unwound. The first deduction rule
presents the logical axioms, creating BP items for
each backpointer used in a NT inference rule appli-
cation during the first-pass parsing step. The sec-
ond deduction rule represents the unwinding step
as discussed in the example above. These deduc-
tions govern a search for derivations through the hy-
pergraph that is driven by updates of rule weights
and LM probabilities when unwinding non-first-best
hypotheses. The functions p and q are as defined
in Section 2, except that the domain of q is ex-
tended to BP target sides by first replacing each
back-pointer with its corresponding chart cell?s LM
context and then applying the original q on the re-
sulting sequence of target-terminals and ? symbols.2
Note that w?, which was computed by the first de-
duction rule, adjusts the current hypothesis? weight
2Note also that p(?s?g?1 ??\s?g?1) = 1 as the product over
the empty set is one.
504
{P ? ?; e1} : w?/w
(P back-points to 1st-pass cell [X, i, j; e1] : w; ? and w? are BP-target-side and weight of one of that cell?s items)
[[e;P?end]] : w
?
P ? ?lex?mid; e1
?
: w?
[[q(e?lex);?mid?end]] : ww?p[eq(?lex?mid)]p[q(e?lex?mid)q(?end)]/p(ee1)/p[q(ee1)q(?end)]
?
?lex contains no BPs and
?mid = P ??? or ?mid = ?
?
Figure 2. Left-to-right LM driven hypergraph search of the sentence spanning hypergraph; ? denotes the empty word.
Non-logical (Start) axiom: [[?;PS ]] : w?; Goal item: [[?s?g?1 ? ?\s?g?1; ?]] : w
that is based on the first-best instance of P to the
actually chosen instance?s weight. Further, the ra-
tio p(eq(?lex?mid))/p(ee1) adjusts the LM prob-
abilities of P ?s instantiation given its left context,
and p[q(e?lex?mid)q(?end)]/p[q(ee1)q(?end)] ad-
justs the LM probabilities of the g ? 1 words right
of P .
4 Alternative Approaches
We evaluate our two pass hypergraph search
?H.Search? against the strong single pass Cube
Pruning (CP) baseline as mentioned in (Chiang,
2005) and detailed in (Chiang, 2007). In the latter
work, the author shows that CP clearly outperforms
both the naive single pass solution of severe prun-
ing as well as the naive two-pass rescoring approach.
Thus, we focus on comparing our approach to CP.
CP is an optimization to the intersected LM pars-
ing algorithm presented in Figure 1. It addresses
the creation of the
?K
k=1 | [Xk, ik, jk, ?] | chart items
when generating consequent items. CP amounts to
an early termination condition when generating the
set of possible consequents. Instead of generating
all consequents, and then pruning away the poor per-
formers, CP uses the K-Best extraction approach of
(Huang and Chiang, 2005) to select the best K con-
sequents only, at the cost of potential search errors.
CP?s termination condition can be defined in terms
of an absolute number of consequents to generate, or
by terminating the generation process when a newly
generated item is worse (by ?) than the current best
item for the same left-hand side and span. To sim-
ulate comparable pruning criteria we parameterize
each method with soft-threshold based criteria only
(? for CP and Z for H.Search) since counter based
limits like K have different effects in CP (selecting
e labeled items) vs H.Search (selecting rules since
items are not labeled with e).
5 Experimental Framework
We present results on the IWSLT 2006 Chinese to
English translation task, based on the Full BTEC
corpus of travel expressions with 120K parallel sen-
tences (906K source words and 1.2m target words).
The evaluation test set contains 500 sentences with
an average length of 10.3 source words.
Grammar rules were induced with the syntax-
based SMT system ?SAMT? described in (Zoll-
mann and Venugopal, 2006), which requires ini-
tial phrase alignments that we generated with
?GIZA++? (Koehn et al, 2003), and syntactic parse
trees of the target training sentences, generated by
the Stanford Parser (D. Klein, 2003) pre-trained on
the Penn Treebank. All these systems are freely
available on the web.
We experiment with 2 grammars, one syntax-
based (3688 nonterminals, 0.3m rules), and one
purely hierarchical (1 generic nonterminal, 0.05m
rules) as in (Chiang, 2005). The large number of
nonterminals in the syntax based systems is due to
the CCG extension over the original 75 Penn Tree-
bank nonterminals. Parameters ? used to calculate
P (D) are trained using MER training (Och, 2003)
on development data.
6 Comparison of Approaches
We evaluate each approach by considering both
search errors made on the development data for a
fixed set of model parameters, and the BLEU metric
to judge translation quality.
6.1 Search Error Analysis
While it is common to evaluate MT quality using the
BLEU score, we would like to evaluate search errors
made as a function of ?effort? made by each algo-
rithm to produce a first-best translation. We con-
sider two metrics of effort made by each algorithm.
505
We first evaluate search errors as a function of novel
queries made to the g-gram LM (since LM calls tend
to be the dominant component of runtime in large
MT systems). We consider novel queries as those
that have not already been queried for a particular
sentence, since the repeated calls are typically effi-
ciently cached in memory and do not affect runtime
significantly. Our goal is to develop techniques that
can achieve low search error with the fewest novel
queries to the g-gram LM.
To appreciate the practical impact of each algo-
rithm, we also measure search errors as a function of
the number of seconds required to translate a fixed
unseen test set. This second metric is more sensitive
to implementation and, as it turned out, even com-
piler memory management decisions.
We define search errors based on the weight of
the best sentence spanning item. Treating weights as
negative log probabilities (costs), we accumulate the
value of the lowest cost derivation for each sentence
in the testing data as we vary pruning settings ap-
propriate to each method. Search errors are reduced
when we are able to lower this accumulated model
cost. We prefer approaches that yield lowmodel cost
with the least number of LM calls or number of sec-
onds spent decoding.
It is important to note that model cost
(? log(P (D))) is a function of the parameters
? which have been trained using MER training. The
parameters used for these experiments were trained
with the CP approach; in practice we find that either
approach is effective for MER training.
6.2 Results
Figure 3 and Figure 4 plot model cost as a function
of LM cache misses for the IWSLT Hierarchical and
Syntax based systems, while Figure 5 plots decod-
ing time. The plots are based on accumulated model
cost, decoding time and LM cache misses over the
IWSLT Test 06 set. For H.Search, we vary the beam
parameter Z for a fixed value of ? = 5 during pars-
ing while for CP, we vary ?. We also limit the total
number of items on the agenda at any time to 1000
for H.Search as a memory consideration. We plot
each method until we see no change in BLEU score
for that method. BLEU scores for each parameter
setting are also noted on the plots.
For both the hierarchical and syntax based gram-
mars we see that the H.Search method achieves a
given model cost ?earlier? in terms of novel LM
calls for most of the plotted region, but ultimately
fails to achieve the same lowest model cost as the
CP method.3 While the search beam of Z mit-
igates the impact of the estimated scores during
H.Search?s second pass, the score is still not an ad-
missible heuristic for error-free search. We suspect
that simple methods to ?underestimate? the score of
a partial derivation?s remaining nonterminals could
bridge this gap in search error. BLEU scores in
the regions of lowest model cost tend to be reason-
ably stable and reflect comparable translation per-
formance for both methods.
Under both H.Search and CP, the hierarchical
grammar ultimately achieves a BLEU score of
19.1%, while the syntactic grammar?s score is ap-
proximately 1.5 points higher at 20.7%. The hierar-
chical grammar demonstrates a greater variance of
BLEU score for both CP and H.Search compared
to the syntax-based grammar. The use of syntac-
tic structure serves as an additional model of target
language fluency, and can explain the fact that syn-
tax based translation quality is more robust to differ-
ences in the number of g-gram LM options explored.
Decoding time plots shows a similar result,
but with diminished relative improvement for the
H.Search method. Profiling analysis of the H.Search
method shows that significant time is spent simply
on allocating and deallocating memory for partial
derivations on top of the scoring times for these
items. We expect to be able to reduce this overhead
significantly in future work.
7 Conclusion
We presented an novel two-pass decoding approach
for PSCFG-based machine translation that achieves
search errors comparable to the state of the art Cube
Pruning method. By maintaining comparable, sen-
tence spanning derivations we allow easy integration
of high or flexible order LMs as well as sentence
level syntactic features during the search process.
We plan to evaluate the impact of these more power-
ful models in future work. We also hope to address
the question of how much search error is tolerable to
3Analysis of total LM calls made by each method (not pre-
sented here) shows the H.Search makes significantly fewer (1/2)
total LM calls than CP to achieve each model cost.
506
IWSLT - LM Cache Misses Hierarchical
-3200-3100
-3000-2900
-2800-2700
-2600-2500
0.0E+00 2.5E+06 5.0E+06 7.5E+06Number of LM Misses
Model Cost
CPH.Search
0.175
0.178
0.181
0.191
0.188
0.177
0.180
0.182
0.186
0.191
0.191
0.174
Figure 3. LM caches misses for
IWSLT hierarchical grammar and
BLEU scores for varied pruning pa-
rameters
IWSLT - LM Cache Misses Syntax
3740037425
3745037475
37500
2.0E+05 7.0E+05 1.2E+06Number of LM Misses
Model Cost
CPH.Search0.205
0.206
0.206
0.2
0.2
0.206
0.207
0.207
0.207
Figure 4. LM caches misses
for IWSLT syntactic grammar and
BLEU scores for varied pruning pa-
rameters
IWSLT - Syntax Decoding Time
3740037420
3744037460
3748037500
9.0E+02 9.8E+02 1.1E+03 1.1E+03 1.2E+03Decoding Time (s)
Model Cost
CPH.Search0.205
0.206
0.207
0.205
0.206
0.206
0.206
0.207 0.207
Figure 5. Decoding time (s)
for IWSLT syntactic grammar and
BLEU scores for varied pruning pa-
rameters
run MER training and still generate parameters that
generalize well to test data. This point is particularly
relevant to evaluate the use of search error analysis.
References
Bar-Hillel, M.Perles, and E.Shamir. 1964. An efficient
context-free parsing algorithm. Communications of
the Assocation for Computing Machinery.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL.
David Chiang. 2007. Hierarchical phrase based transla-
tion. To Appear in the Journal of Computational Lin-
guistics.
C. Manning D. Klein. 2003. Accurate unlexicalized
parsing. In Proc. of ACL.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proc. of Inter-
national Workshop on Spoken Language Translation,
pages 11?17.
Michael Galley, M. Hopkins, Kevin Knight, and Daniel
Marcu. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proc. of
NAACL-HLT.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of the 9th International Workshop on
Parsing Technologies.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT/NAACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, Sap-
poro, Japan, July 6-7.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3?15.
Mark Steedman. 1999. Alternative quantifier scope in
CCG. In Proc. of ACL.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase based translation. In ACL.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the com-
plexity of translational equivalence. In ACL.
Dekai Wu. 1996. A polynomial time algorithm for statis-
tical machine translation. In Proc. of the Association
for Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. of the Workshop on Statistical Machine Transla-
tion, HLT/NAACL, New York, June.
507
Proceedings of the Second Workshop on Statistical Machine Translation, pages 216?219,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Syntax Augmented MT (SAMT) System for the Shared Task in the 2007
ACL Workshop on Statistical Machine Translation
Andreas Zollmann and Ashish Venugopal and Matthias Paulik and Stephan Vogel
School of Computer Science, Carnegie Mellon University, Pittsburgh
interACT Lab, University of Karlsruhe
{ashishv,zollmann,paulik,vogel+}@cs.cmu.edu
Abstract
We describe the CMU-UKA Syntax Augmented
Machine Translation system ?SAMT? used for the
shared task ?Machine Translation for European Lan-
guages? at the ACL 2007 Workshop on Statistical
Machine Translation. Following an overview of syn-
tax augmented machine translation, we describe pa-
rameters for components in our open-source SAMT
toolkit that were used to generate translation results
for the Spanish to English in-domain track of the
shared task and discuss relative performance against
our phrase-based submission.
1 Introduction
As Chiang (2005) and Koehn et al (2003) note,
purely lexical ?phrase-based? translation models
suffer from sparse data effects when translating con-
ceptual elements that span or skip across several
source language words. Phrase-based models also
rely on distance and lexical distortion models to rep-
resent the reordering effects across language pairs.
However, such models are typically applied over
limited source sentence ranges to prevent errors in-
troduced by these models and to maintain efficient
decoding (Och and Ney, 2004).
To address these concerns, hierarchically struc-
tured models as in Chiang (2005) define weighted
transduction rules, interpretable as components of
a probabilistic synchronous grammar (Aho and Ull-
man, 1969) that represent translation and reordering
operations. In this work, we describe results from
the open-source Syntax Augmented Machine Trans-
lation (SAMT) toolkit (Zollmann and Venugopal,
2006) applied to the Spanish-to-English in-domain
translation task of the ACL?07 workshop on statisti-
cal machine translation.
We begin by describing the probabilistic model of
translation applied by the SAMT toolkit. We then
present settings for the pipeline of SAMT tools that
we used in our shared task submission. Finally, we
compare our translation results to the CMU-UKA
phrase-based SMT system and discuss relative per-
formance.
2 Synchronous Grammars for SMT
Probabilistic synchronous context-free grammars
(PSCFGs) are defined by a source terminal set
(source vocabulary) TS , a target terminal set (target
vocabulary) TT , a shared nonterminal setN and pro-
duction rules of the form
X ? ??, ?,?, w?
where following (Chiang, 2005)
? X ? N is a nonterminal
? ? ? (N ?TS)? : sequence of source nonterminals
and terminals
? ? ? (N ? TT )? : sequence of target nonterminals
and terminals
? the count #NT(?) of nonterminal tokens in ? is
equal to the count #NT(?) of nonterminal tokens
in ?,
? ?: {1, . . . ,#NT(?)} ? {1, . . . ,#NT(?)} one-
to-one mapping from nonterminal tokens in ? to
nonterminal tokens in ?
? w ? [0,?) : nonnegative real-valued weight
Chiang (2005) uses a single nonterminal cate-
gory, Galley et al (2004) use syntactic constituents
for the PSCFG nonterminal set, and Zollmann and
Venugopal (2006) take advantage of CCG (Combi-
natorial Categorical Grammar) (Steedman, 1999) in-
spired ?slash? and ?plus? categories, focusing on tar-
get (rather than source side) categories to generate
well formed translations.
We now describe the identification and estima-
tion of PSCFG rules from parallel sentence aligned
corpora under the framework proposed by Zollmann
and Venugopal (2006).
216
2.1 Grammar Induction
Zollmann and Venugopal (2006) describe a process
to generate a PSCFG given parallel sentence pairs
?f, e?, a parse tree pi for each e, the maximum a
posteriori word alignment a over ?f, e?, and phrase
pairs Phrases(a) identified by any alignment-driven
phrase induction technique such as e.g. (Och and
Ney, 2004).
Each phrase in Phrases(a) (phrases identifiable
from a) is first annotated with a syntactic category
to produce initial rules. If the target span of the
phrase does not match a constituent in pi, heuristics
are used to assign categories that correspond to par-
tial rewriting of the tree. These heuristics first con-
sider concatenation operations, forming categories
like ?NP+VP?, and then resort to CCG style ?slash?
categories like ?NP/NN? giving preference to cate-
gories found closer to the leaves of the tree.
To illustrate this process, consider the following
French-English sentence pair and selected phrase
pairs obtained by phrase induction on an automat-
ically produced alignment a, and matching target
spans with pi.
f = il ne va pas
e = he does not go
PRP ? il, he
VB ? va, go
RB+VB ? ne va pas, not go
S ? il ne va pas, he does not go
The alignment a with the associated target side
parse tree is shown in Fig. 1 in the alignment visual-
ization style defined by Galley et al (2004).
Following the Data-Oriented Parsing inspired
rule generalization technique proposed by Chiang
(2005), one can now generalize each identified
rule (initial or already partially generalized) N ?
f1 . . . fm/e1 . . . en for which there is an initial rule
M ? fi . . . fu/ej . . . ev where 1 ? i < u ? m and
1 ? j < v ? n, to obtain a new rule
N ? f1 . . . fi?1Mkfu+1 . . . fm/e1 . . . ej?1Mkev+1 . . . en
where k is an index for the nonterminal M that in-
dicates the one-to-one correspondence between the
new M tokens on the two sides (it is not in the space
of word indices like i, j, u, v,m, n). The initial rules
listed above can be generalized to additionally ex-
tract the following rules from f, e.
S ? PRP1 ne va pas , PRP1 does not go
S ? il ne VB1 pas , he does not VB1
S ? il RB+VB1, he does RB+VB1
S ? PRP1 RB+VB2, PRP1 does RB+VB2
RB+VB ? ne VB1 pas , not VB1
Fig. 2 uses regions to identify the labeled, source
and target side span for all initial rules extracted on
our example sentence pair and parse. Under this rep-
resentation, generalization can be viewed as a pro-
cess that selects a region, and proceeds to subtract
out any sub-region to form a generalized rule.
S
q
q
q
q
q
q
q
M
M
M
M
M
M
M
NP VP
q
q
q
q
q
q
q
M
M
M
M
M
M
M
PRN AUX RB VB
he does not
q
q
q
q
q
q
q
M
M
M
M
M
M
M
go
q
q
q
q
q
q
q
il ne va pas
Figure 1: Alignment graph (word alignment and target parse
tree) for a French-English sentence pair.
il 1 ne 2 va 3 pas 4
he 1
does 2
not 3
go 4

ffi
ff
ff
9
S
RB+VB
VB
VP
NP+AUX
NP
Figure 2: Spans of initial lexical phrases w.r.t. f, e. Each phrase
is labeled with a category derived from the tree in Fig. 1.
2.2 Decoding
Given a source sentence f , the translation task under
a PSCFG grammar can be expressed analogously to
monolingual parsing with a CFG. We find the most
likely derivation D with source-side f and read off
the English translation from this derivation:
e? = tgt
(
argmax
D:src(D)=f
p(D)
)
(1)
where tgt(D) refers to the target terminals and
src(D) to the source terminals generated by deriva-
tion D.
Our distribution p over derivations is defined by a
log-linear model. The probability of a derivation D
217
is defined in terms of the rules r that are used in D:
p(D) =
pLM (tgt(D))?LM
?
r?D
?
i ?i(r)
?i
Z(?)
(2)
where ?i refers to features defined on each rule,
pLM is a language model (LM) probability applied to
the target terminal symbols generated by the deriva-
tion D, and Z(?) is a normalization constant cho-
sen such that the probabilities sum up to one. The
computational challenges of this search task (com-
pounded by the integration of the LM) are addressed
in (Chiang, 2007; Venugopal et al, 2007). The
feature weights ?i are trained in concert with the
LM weight via minimum error rate (MER) training
(Och, 2003).
We now describe the parameters for the SAMT
implementation of the model described above.
3 SAMT Components
SAMT provides tools to perform grammar induc-
tion ( ?extractrules?, ?filterrules?), from bilingual
phrase pairs and target language parse trees, as well
as translation (?FastTranslateChart?) of source sen-
tences given an induced grammar.
3.1 extractrules
extractrules is the first step of the grammar induc-
tion pipeline, where rules are identified based on the
process described in section 2.1. This tool works on
a per sentence basis, considering phrases extracted
for the training sentence pair ?si, ti? and the corre-
sponding target parse tree pii. extractrules outputs
identified rules for each input sentence pair, along
with associated statistics that play a role in the esti-
mation of the rule features ?. These statistics take
the form of real-valued feature vectors for each rule
as well as summary information collected over the
corpus, such as the frequency of each nonterminal
symbol, or unique rule source sides encountered.
For the shared task evaluation, we ran extrac-
trules with the following extraction parameter
settings to limit the scope and number of rules
extracted. These settings produce the same initial
phrase table as the CMU-UKA phrase based sys-
tem. We limit the source-side length of the phrase
pairs considered as initial rules to 8 (parameter
MaxSourceLength). Further we set the max-
imum number of source and target terminals per
rule (MaxSource/MaxTargetWordCount)
to 5 and 8 respectively with 2 of nonter-
minal pairs (i.e., substitution sites) per rule
(MaxSubstititionCount). We limit the
total number of symbols in each rule to 8
(MaxSource/TargetSymbolCount) and
require all rules to contain at least one source-side
terminal symbol (noAllowAbstractRules,
noAllowRulesWithOnlyTargetTerminals)
since this reduces decoding time considerably. Ad-
ditionally, we discard all rules that contain source
word sequences that do not exist in the development
and test sets provided for the shared task (parameter
-r).
3.2 filterrules
This tool takes as input the rules identified by ex-
tractrules, and associates each rule with a feature
vector ?, representing multiple criteria by which the
decoding process can judge the quality of each rule
and, by extension, each derivation. filterrules is also
in charge of pruning the resulting PSCFG to ensure
tractable decoding.
? contains both real and Boolean valued features
for each rule. The following probabilistic features
are generated by filterrules:
? p?(r| lhs(X)) : Probability of a rule given its left-
hand-side (?result?) nonterminal
? p?(r| src(r)) : Prob. of a rule given its source side
? p?(ul(src(r)),ul(tgt(r))|ul(src(r)) : Probability
of the unlabeled source and target side of the rule
given its unlabeled source side.
Here, the function ul removes all syntactic la-
bels from its arguments, but retains ordering nota-
tion, producing relative frequencies similar to those
used in purely hierarchical systems. As in phrase-
based translation model estimation, ? also contains
two lexical weights (Koehn et al, 2003), counters
for number of target terminals generated. ? also
boolean features that describe rule types (i.e. purely
terminal vs purely nonterminal).
For the shared task submission, we pruned away
rules that share the same source side based on
p?(r| src(r)) (the source conditioned relative fre-
quency). We prune away a rule if this value is
less that 0.5 times the one of the best performing
rule (parameters BeamFactorLexicalRules,
BeamFactorNonlexicalRules).
3.3 FastTranslateChart
The FastTranslateChart decoder is a chart parser
based on the CYK+(Chappelier and Rajman, 1998)
algorithm. Translation experiments in this paper
are performed with a 4-gram SRI language model
trained on the target side of the corpus. Fast-
TranslateChart implements both methods of han-
dling the LM intersection described in (Venugopal
et al, 2007). For this submission, we use the Cube-
Pruning (Chiang, 2007) approach (the default set-
ting). LM and rule feature parameters ? are trained
with the included MER training tool. Our prun-
ing settings allow up to 200 chart items per cell
218
with left-hand side nonterminal ? S? (the reserved
sentence spanning nonterminal), and 100 items per
cell for each other nonterminal. Beam pruning
based on an (LM-scaled) additive beam of neg-
lob probability 5 is used to prune the search fur-
ther. These pruning settings correspond to setting
?PruningMap=0-100-5-@_S-200-5?.
4 Empirical Results
We trained our system on the Spanish-English in-
domain training data provided for the workshop. Ini-
tial data processing and normalizing is described
in the workshop paper for the CMU-UKA ISL
phrase-based system. NIST-BLEU scores are re-
ported on the 2K sentence development ?dev06? and
test ?test06? corpora as per the workshop guide-
lines (case sensitive, de-tokenized). We compare
our scores against the CMU-UKA ISL phrase-based
submission, a state-of-the art phrase-based SMT
system with part-of-speech (POS) based word re-
ordering (Paulik et al, 2007).
4.1 Translation Results
The SAMT system achieves a BLEU score of
32.48% on the ?dev06? development corpus and
32.15% on the unseen ?test06? corpus. This is
slightly better than the score of the CMU-UKA
phrase-based system, which achieves 32.20% and
31.85% when trained and tuned under the same in-
domain conditions. 1
To understand why the syntax augmented ap-
proach has limited additional impact on the Spanish-
to-English task, we consider the impact of reorder-
ing within our phrase-based system. Table 1 shows
the impact of increasing reordering window length
(Koehn et al, 2003) on translation quality for the
?dev06? data.2 Increasing the reordering window
past 2 has minimal impact on translation quality,
implying that most of the reordering effects across
Spanish and English are well modeled at the local or
phrase level. The benefit of syntax-based systems to
capture long-distance reordering phenomena based
on syntactic structure seems to be of limited value
for the Spanish to English translation task.
5 Conclusions
In this work, we briefly summarized the Syntax-
augmented MT model, described how we trained
and ran our implementation of that model on
1The CMU-UKA phrase-based workshop submission was
tuned on out-of-domain data as well.
2Variant of the CMU-UKA ISL phrase-based system with-
out POS based reordering. With POS-based reordering turned
on, additional window-based reordering even for window length
1 had no improvement in NIST-BLEU.
ReOrder 1 2 3 4 POS SAMT
BLEU 31.98 32.24 32.30 32.26 32.20 32.48
Table 1: Impact of phrase based reordering model settings com-
pared to SAMT on the ?dev06? corpus measured by NIST-
BLEU
the MT?07 Spanish-to-English translation task.
We compared SAMT translation results to
a strong phrase-based system trained under
the same conditions. Our system is available
open-source under the GNU General Pub-
lic License (GPL) and can be downloaded at
www.cs.cmu.edu/?zollmann/samt
References
Alfred Aho and Jeffrey Ullman. 1969. Syntax directed
translations and the pushdown assembler. Journal of
Computer and System Sciences.
Jean-Cedric. Chappelier and Martin Rajman. 1998.
A generalized CYK algorithm for parsing stochastic
CFG. In Proc. of Tabulation in Parsing and Deduction
(TAPD?98), Paris, France.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL.
David Chiang. 2007. Hierarchical phrase based transla-
tion. Computational Linguistics. To appear.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of HLT/NAACL, Boston, Massachusetts.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT/NAACL, Edmonton,Canada.
Franz Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Com-
put. Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, Sap-
poro, Japan, July 6-7.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja
Hildebrand, and Stephan Vogel. 2007. The ISL
phrase-based MT system for the 2007 ACL work-
shop on statistical MT. In Proc. of the Association
of Computational Linguistics Workshop on Statistical
Machine Translation.
Mark Steedman. 1999. Alternative quantifier scope in
CCG. In Proc. of ACL, College Park, Maryland.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to syn-
chronous CFG driven MT. In Proc. of HLT/NAACL,
Rochester, NY.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. of the Workshop on Statistical Machine Transla-
tion, HLT/NAACL, New York, June.
219
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 349?357,
Beijing, August 2010
EMDC: A Semi-supervised Approach for Word Alignment
Qin Gao
Language Technologies Institute
Carnegie Mellon University
qing@cs.cmu.edu
Francisco Guzman
Centro de Sistemas Inteligentes
Tecnolo?gico de Monterrey
guzmanhe@gmail.com
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
stephan.vogel@cs.cmu.edu
Abstract
This paper proposes a novel semi-
supervised word alignment technique
called EMDC that integrates discrimina-
tive and generative methods. A discrim-
inative aligner is used to find high preci-
sion partial alignments that serve as con-
straints for a generative aligner which
implements a constrained version of the
EM algorithm. Experiments on small-size
Chinese and Arabic tasks show consistent
improvements on AER. We also experi-
mented with moderate-size Chinese ma-
chine translation tasks and got an aver-
age of 0.5 point improvement on BLEU
scores across five standard NIST test sets
and four other test sets.
1 Introduction
Word alignment is a crucial component in sta-
tistical machine translation (SMT). From a Ma-
chine Learning perspective, the models for word
alignment can be roughly categorized as gener-
ative models and discriminative models. The
widely used word alignment tool, i.e. GIZA++
(Och and Ney, 2003), implements the well-known
IBM models (Brown et al, 1993) and the HMM
model (Vogel et al, 1996), which are genera-
tive models. For language pairs such as Chinese-
English, the word alignment quality is often un-
satisfactory. There has been increasing interest on
using manual alignments in word alignment tasks,
which has resulted in several discriminative mod-
els. Ittycheriah and Roukos (2005) proposed to
use only manual alignment links in a maximum
entropy model, which is considered supervised.
Also, a number of semi-supervised word align-
ers have been proposed (Taskar et al, 2005; Liu
et al, 2005; Moore, 2005; Blunsom and Cohn,
2006; Niehues and Vogel, 2008). These methods
use held-out manual alignments to tune weights
for discriminative models, while using the model
parameters, model scores or alignment links from
unsupervised word aligners as features. Callison-
Burch et. al. (2004) proposed a method to interpo-
late the parameters estimated by sentence-aligned
and word-aligned corpus. Also, there are recent
attempts to combine multiple alignment sources
using alignment confidence measures so as to im-
prove the alignment quality (Huang, 2009).
In this paper, the question we address is
whether we can jointly improve discriminative
models and generative models by feeding the in-
formation we get from the discriminative aligner
back into the generative aligner. Examples of
this line of research include Model 6 (Och and
Ney, 2003) and the EMD training approach pro-
posed by Fraser and Marcu (2006) and its ex-
tension called LEAF aligner (Fraser and Marcu,
2007). These approaches use labeled data to tune
additional parameters to weight different compo-
nents of the IBM models such as the lexical trans-
lation model, the distortion model and the fertility
model. These methods are proven to be effective
in improving the quality of alignments. However,
the discriminative training in these methods is re-
stricted in using the model components of gener-
ative models, in other words, incorporating new
features is difficult.
Instead of using discriminative training meth-
ods to tune the weights of generative models,
in this paper we propose to use a discrimina-
tive word aligner to produce reliable constraints
for the EM algorithm. We call this new train-
ing scheme EMDC (Expectation-Maximization-
Discrimination-Constraint). The methodology
can be viewed as a variation of bootstrapping. It
enables the generative models to interact with dis-
criminative models at the data level instead of the
model level. Furthermore, with a discriminative
349
word aligner that uses generative word aligner?s
output as features, we create a feedback loop that
can iteratively improve the quality of both align-
ers. The major contributions of this paper are: 1)
The EMDC training scheme, which ties the gen-
erative and discriminative aligners together and
enables future research on integrating other dis-
criminative aligners. 2) An extended generative
aligner based on GIZA++ that allows to perform
constrained EM training.
In Section 2, we present the EMDC training
scheme. Section 3 provides details of the con-
strained EM algorithm. In Section 4, we intro-
duce the discriminative aligner and link filtering.
Section 5 provides the experiment set-up and the
results. Section 6 concludes the paper.
2 EMDC Training Scheme
The EMDC training scheme consists of
three parts, namely EM, Discrimination, and
Constraints. As illustrated in Figure 1, a large
unlabeled training set is first aligned with a gen-
erative aligner (GIZA++ for the purpose of this
paper). The generative aligner outputs the model
parameters and the Viterbi alignments for both
source-to-target and target-to-source directions.
Afterwards, a discriminative aligner (we use the
one described in (Niehues and Vogel, 2008)),
takes the lexical translation model, fertility model
and Viterbi alignments from both directions as
features, and is tuned to optimize the AER on a
small manually aligned tuning set. Afterwards,
the alignment links generated by the discrimina-
tive aligner are filtered according to their likeli-
hood, resulting in a subset of links that has high
precision and low recall. The next step is to put
these high precision alignment links back into the
generative aligner as constraints. A conventional
generative word aligner does not support this type
of constraints. Thus we developed a constrained
EM algorithm that can use the links from a partial
alignment as constraints and estimate the model
parameters by marginalizing likelihoods.
After the constrained EM training is performed,
we repeat the procedure and put the updated gen-
erative models and Viterbi alignment back into the
discriminative aligner. We can either fix the num-
ber of iterations, or stop the procedure when the
gain on AER of a small held-out test set drops be-
Figure 1: Illustration of EMDC training scheme
low a threshold.
The key components for the system are:
1. A generative aligner that can make use of re-
liable alignment links as constraints and im-
prove the models/alignments.
2. A discriminative aligner that outputs con-
fidence scores for alignment links, which
allows to obtain high-precision-low-recall
alignments.
While in this paper we derive the reliable links
by filtering the alignment generated by a discrimi-
native aligner, such partial alignments may be ob-
tained from other sources as well: manual align-
ments, specific named entity aligner, noun-phrase
aligner, etc.
As we mentioned in Section 1, the discrimina-
tive aligner is not restricted to use features param-
eters of generative models and Viterbi alignments.
However, including the features from generative
models is required for iterative training, because
the improvement on the quality of these features
can in turn improve the discriminative aligner. In
our experiments, the discriminative aligner makes
heavy use of the Viterbi alignment and the model
parameters from the generative aligner. Nonethe-
less, one can easily replace the discriminative
aligner or add new features to it without modify-
ing the training scheme. The open-ended prop-
erty of the training scheme makes it a promising
method to integrate different aligners.
In the next two sections, we will describe the
key components of this framework in detail.
3 Constrained EM algorithm
In this section we will briefly introduce the con-
strained EM algorithm we used in the experiment,
350
further details of the algorithm can be found in
(Gao et al, 2010).
The IBM Models (Brown et al, 1993) are a
series of generative models for word alignment.
GIZA++ (Och and Ney, 2003), the most widely
used implementation of IBM models and HMM
(Vogel et al, 1996), employs EM algorithm to es-
timate the model parameters. For simpler models
such as Model 1 and Model 2, it is possible to
obtain sufficient statistics from all possible align-
ments in the E-step. However, for fertility-based
models such as Models 3, 4, and 5, enumerating
all possible alignments is NP-complete. To over-
come this limitation, GIZA++ adopts a greedy
hill-climbing algorithm, which uses simpler mod-
els such as HMM or Model 2 to generate a ?center
alignment? and then tries to find better alignments
among its neighbors. The neighbors of an align-
ment aJ1 = [a1, a2, ? ? ? , aJ ] with aj ? [0, I] are
defined as alignments that can be generated from
aJ1 by one of the following two operators:
1. The move operator m[i,j], that changes aj :=
i, i.e. arbitrarily sets word fj in the target
sentence to align to the word ei in source sen-
tence;
2. The swap operator s[j1,j2] that exchanges aj1
and aj2 .
The algorithm will update the center alignment
as long as a better alignment can be found, and
finally outputs a local optimal alignment. The
neighbor alignments of the final center alignment
are then used in collecting the counts for the M-
Step. Och and Ney (2003) proposed a fast imple-
mentation of the hill-climbing algorithm that em-
ploys two matrices, i.e. Moving MatrixMI?J and
Swapping Matrix SJ?J . Each cell of the matrices
stores the value of likelihood difference after ap-
plying the corresponding operator.
We define a partial alignment constraint of a
sentence pair (fJ1 , eI1) as a set of links: ?JI =
{(i, j)|0 ? i < I, 0 ? j < J}. Given a set of
constraints, an alignment aJ1 = [a1, a2, ? ? ? , aj ]
on the sentence pair fJ1 , eI1, the translation proba-
bility of Pr(fJ1 |eI1) will be zero if the alignment
is inconsistent with the constraints. Constraints
(0, j) or (i, 0) are used to explicitly represent that
word fj or ei is aligned to the empty word.
Under the assumptions of the IBM models,
there are two situations that aJ1 is inconsistent with
?JI :
1. Target word misalignment: The IBM mod-
els assume that one target word can only be
aligned to one source word. Therefore, if the
target word fj aligns to a source word ei,
while the constraint ?JI suggests fj should be
aligned to ei? , the alignment violates the con-
straint and thus is considered inconsistent.
2. Source word to empty word misalignment: if
a source word is aligned to the empty word,
it cannot be aligned to any concrete target
word.
However, the partial alignments, which allow
n-to-n alignments, may already violate the 1-to-n
alignment restriction of the IBM models. In these
cases, we relax the condition in situation 1 that if
the alignment link aj? is consistent with any one
of the conflicting target-to-source constraints, it
will be considered consistent. Also, we arbitrarily
assign the source word to empty word constraints
higher priorities than other constraints, because
unlike situation 1, it does not have the problem
of conflicting with other constraints.
3.1 Constrained hill-climbing algorithm
To ensure that resulting center alignment be
consistent with the constraints, we need to split
the hill-climbing algorithm into two stages: 1) op-
timize towards the constraints and 2) optimize to-
wards the optimal alignment under the constraints.
From a seed alignment, we first move the align-
ment towards the constraints by choosing a move
or swap operator that:
1. produces the alignment that has the highest
likelihood among alignments generated by
other operators,
2. eliminates at least one inconsistent link.
We iteratively update the alignment until no
other inconsistent link can be removed. The algo-
rithm implies that we force the seed alignment to
be closer to the constraints while trying to find the
best consistent alignment. Figure 2 demonstrates
the idea, given the constraints shown in (a), and
the seed alignment shown as solid links in (b), we
351
2005?
?
??
the
summer
of
2005Manual Alignment Link
(a)
2005?
?
??
the
summer
Of
2005Seed Alignment Consistent Alignment Center Alignment
(b)                    (c)
2005?
?
??
the
summer
of
2005
Figure 2: Illustration of Algorithm 1
move the inconsistent link to the dashed link by a
move operation.
After we find the consistent alignment, we pro-
ceed to optimize towards the optimal alignment
under the constraints. The algorithm sets the value
of the cells in moving/swapping matrices to nega-
tive if the corresponding operators will lead to an
inconsistent alignment. The moving matrix needs
to be processed only once, whereas the swapping
matrix needs to be updated every iteration, since
once the alignment is updated, the possible viola-
tions will also change.
If a source word ei is aligned to the empty word,
we set Mi,j = ?1,?j. The swapping matrix does
not need to be modified in this case because the
swapping operator will not introduce new links.
Because the cells that can lead to violations are
set to negative, the operators will never be picked
when updating the center alignments. This en-
sures the consistency of the final center alignment.
3.2 Count Collection
After finding the center alignment, we need to
collect counts from neighbor alignments so that
the M-step can normalize the counts to produce
the model parameters for the next step. In this
stage, we want to make sure all the inconsistent
alignments in the neighbor set of the center align-
ment be ruled out from the sufficient statistics, i.e.
have zero probability. Similar to the constrained
hill climbing algorithm, we can manipulate the
moving/swapping matrices to effectively exclude
inconsistent alignments. Since the original count
collection algorithm depends only on moving and
swapping matrices, we just need to bypass all the
cells which hold negative values, i.e. represent in-
consistent alignments.
We can also view the algorithm as forcing
the posteriors of inconsistent alignments to zero,
and therefore increase the posteriors of consistent
alignments. When no constraint is given, the algo-
rithm falls back to conventional EM, and when all
the alignments are known, the algorithm becomes
fully supervised. And if the alignment quality
can be improved if high-precision partial align-
ment links is given as constraints. In (Gao et al,
2010) we experimented with using a dictionary to
generate such constraints, and in (Gao and Vogel,
2010) we experimented with manual word align-
ments from Mechanical Turk. And in this paper
we try to use an alternative method that uses a dis-
criminative aligner and link filtering to generate
such constraints.
4 Discriminative Aligner and Link
Filtering
We employ the CRF-based discriminative word
aligner described in (Niehues and Vogel, 2008).
The aligner can use a variety of knowledge
sources as features, such as: the fertility and lex-
ical translation model parameters from GIZA++,
the Viterbi alignment from both source-to-target
and target-to-source directions. It can also make
use of first-order features which model the depen-
dency between different links, the Parts-of-Speech
tagging features, the word form similarity feature
and the phrase features. In this paper we use all
the features mentioned above except the POS and
phrase features.
The aligner is trained using a belief-
propagation (BP) algorithm, and can be optimized
to maximize likelihood or directly optimize to-
wards AER on a tuning set. The aligner outputs
confidence scores for alignment links, which
allows us to control the precision and recall
rate of the resulting alignment. Guzman et al
(2009) experimented with different alignments
produced by adjusting the filtering threshold for
the alignment links and showed that they could
get high-precision-low-recall alignments by hav-
ing a higher threshold. Therefore, we replicated
the confidence filtering procedures to produce
the partial alignment constraints. Afterwards
we iterate by putting the partial alignments back
to the constrained word alignment algorithm
described in section 3.
Although the discriminative aligner performs
well in supplying high precision constraints, it
does not model the null alignment explicitly.
352
Num. of
Sentences
Num. of Words Num. of
LinksSource Target
Ch-En 21,863 424,683 524,882 687,247
Ar-En 29,876 630,101 821,938 830,349
Table 1: Corpus statistics of the manual aligned
corpora
Threshold P R AER
Ch-En
0.6 71.30 58.12 35.96
0.7 75.24 54.03 37.11
0.8 85.66 44.19 41.70
0.9 93.70 37.95 45.98
Ar-En
0.6 72.35 59.87 34.48
0.7 77.55 55.58 35.25
0.8 80.07 50.89 37.77
0.9 83.74 44.16 42.17
Table 2: The qualities of the constraints
Hence we are currently not able to provide source
word to empty word alignment constraints which
have been proven to be effective in improving the
alignment quality in (Gao et al, 2010). Due to
space limitation, please refer to: (Niehues and Vo-
gel, 2008; Guzman et al, 2009) for further details
of the aligner and link filtering, respectively.
5 Experiments
To validate the proposed training scheme, we
performed two sets of experiments. First of all,
we experimented with a small manually aligned
corpus to evaluate the ability of the algorithm to
improve the AER. The experiment was performed
on Chinese to English and Arabic to English tasks.
Secondly, we experimented with a moderate size
corpus and performed translation tasks to observe
the effects in translation quality.
5.1 Effects on AER
In order to measure the effects of EMDC in
alignment quality, we experimented with Chinese-
English and Arabic-English manually aligned cor-
pora. The statistics of these sets are shown in Ta-
ble 1. We split the data into two fragments, the
first 100 sentences (Set A) and the remaining (Set
B). We trained generative IBM models using the
Set B, and tuned the discriminative aligner using
the Set A. We evaluated the AER on Set B, but in
any of the training steps the manual alignments of
Set B were not used.
In each iteration of EDMC, we load the model
parameters from the previous step and continue
training using the new constraints. Therefore, it is
important to compare the performance of contin-
uous training against an unconstrained baseline,
because variation in alignment quality could be
attributed to either the effect of more training it-
erations or to the effect of semi-supervised train-
ing scheme. In Figures 3 and 4 we show the
alignment quality for each iteration. Iteration 0 is
the baseline, which comes from standard GIZA++
training1. The grey dash curves represent uncon-
strained Model 4 training, and the curves with
start, circle, cross and diamond markers are con-
strained EM alignments with 0.6, 0.7, 0.8 and
0.9 filtering thresholds respectively. As we can
see from the results, when comparing only the
mono-directional trainings, the alignment quali-
ties improve over the unconstrained training in all
the metrics (precision, recall and AER). From Ta-
ble 2, we observe that the quality of discrimina-
tive aligner also improved. Nonetheless, when
we consider the heuristically symmetrized align-
ment2, we observe mixed results. For instance,
for the Chinese-English case we observe that AER
improves over iterations, but this is the result of
a increasingly higher recall rate in detriment of
precision. Ayan and Dorr (2006) pointed out
that grow-diag-final symmetrization tends to out-
put alignments with high recall and low precision.
However this does not fully explain the tendency
we observed between iterations. The character-
istics of the alignment modified by EDMC that
lead to larger improvements in mono-directional
trainings but a precision drop with symmetrization
heuristics needs to be addressed in future work.
Another observation is how the filtering thresh-
olds affect the results. As we can see in Table 3,
for Chinese to English word alignment, the largest
gain on the alignment quality is observed when
the threshold was set to 0.8, while for Arabic to
English, the threshold of 0.7 or 0.6 works better.
Table 2 shows the precision, recall, and AER of
the constraint links used in the constrained EM al-
1We run 5, 5, 3, 3 iterations of Model 1, HMM, Model 3
and Model 4 respectively.
2We used grow-diag-final-and
353
0 2 4 6 860
6264
66
%
Precision
 
 
0 2 4 6 85052
5456
5860
Recall
0 2 4 6 838
4042
4446
AER
UnconstrainedFiltered 0.6Filtered 0.7Filtered 0.8Filtered 0.9
(a) Arabic-English
0 2 4 6 859
6061
62
%
Precision
 
 
0 2 4 6 864
6668
7072
Recall
0 2 4 6 83334
3536
3738
39 AER
(b) English-Arabic
0 2 4 6 860.5
6161.5
6262.5
63
%
Precision
 
 
0 2 4 6 866
68
70
72 Recall
0 2 4 6 83233
3435
3637
AER
(c) Heuristically-symmetrized
Figure 3: Alignment qualities of each iteration for Arabic-English word alignment task. The grey dash
curves represent unconstrained Model 4 training, and the curves with star, circle, cross and diamond
markers are constrained EM alignments with 0.6, 0.7, 0.8 and 0.9 filtering thresholds respectively.
Source-Target Target-Source Heuristic Discriminative
P R AER P R AER P R AER P R AER
Ch
BL 68.22 46.88 44.43 65.35 55.05 40.25 69.15 57.47 37.23 67.45 59.77 36.62
NC +0.73 +0.71 -0.74 +1.14 +1.14 -1.15 +0.06 +1.07 -0.66 +0.15 +0.64 -0.42
0.6 +2.17 +2.28 -2.32 +1.17 +2.51 -1.97 -0.64 +2.65 -1.27 -0.39 +1.89 -0.87
0.7 +2.57 +2.32 -2.48 +1.94 +2.34 -2.19 -0.34 +2.30 -1.20 -0.28 +1.60 -0.76
0.8 +3.78 +3.27 -3.55 +2.94 +3.32 -3.18 -0.52 +3.32 -1.70 +0.69 +0.14 -0.89
0.9 +0.98 +1.13 -1.11 +1.48 +1.85 -1.71 -0.55 +1.94 -0.90 -0.58 +1.45 -0.54
Ar
BL 58.41 50.42 45.88 59.08 64.84 38.17 60.35 66.99 36.50 68.93 63.94 33.66
NC +2.98 +2.92 -2.96 +1.40 +2.06 -1.70 +0.97 +2.14 -1.49 -0.87 +2.37 -0.83
0.6 +6.69 +8.02 -7.47 +3.45 +6.70 -4.90 +2.62 +4.71 -3.55 +0.58 -0.55 +0.03
0.7 +8.38 +7.93 -8.16 +3.65 +5.26 -4.38 +2.83 +4.70 -3.67 +2.46 -0.42 -0.88
0.8 +6.48 +6.27 -6.39 +2.18 +3.54 -2.80 +1.81 +3.81 -2.70 +1.67 +2.30 -2.01
0.9 +4.02 +4.07 -4.07 +1.70 +3.10 -2.33 +0.62 +3.82 -2.03 +1.33 +2.70 -2.06
Table 3: Improvement on word alignment quality on small corpus after 8 iterations. BL stands for
baseline, and NC represents unconstrained Model 4 training, and 0.9, 0.8, 0.7, 0.6 are the thresholds
used in alignment link filtering.
gorithm, the numbers are averaged across all iter-
ations, the actual numbers of each iteration only
have small differences. Although one might ex-
pect that the quality of resulting alignment from
constrained EM be proportional to the quality of
constraints, from the numbers in Table 2 and 3,
we are not able to induce a clear relationship be-
tween them, and it could be language- or corpus-
dependent. However, in practice we nonetheless
use a held-out test set to tune this parameter. The
354
0 2 4 6 869
7071
72
%
Precision
 
 
0 2 4 6 84647
4849
5051
Recall
0 2 4 6 84041
4243
4445
AER
UnconstrainedFiltered 0.6Filtered 0.7Filtered 0.8Filtered 0.9
(a) Chinese-English
0 2 4 6 865.5
6666.5
6767.5
68
%
Precision
 
 
0 2 4 6 855
5657
5859
Recall
0 2 4 6 837
3839
4041
AER
(b) English-Chinese
0 2 4 6 868.6
68.869
69.2
%
Precision
 
 
0 2 4 6 857
5859
6061
Recall
0 2 4 6 835
36
37
38 AER
(c) Heuristically-symmetrized
Figure 4: Alignment qualities of each iteration for Chinese-English word alignment task. The grey dash
curves represent unconstrained Model 4 training, and the curves with star, circle, cross and diamond
markers are constrained EM alignments with 0.6, 0.7, 0.8 and 0.9 filtering thresholds respectively.
Ch-En En-Ch Heuristic Discriminative
P R AER P R AER P R AER P R AER
BL 73.51 50.14 40.38 68.82 57.66 37.31 72.98 60.23 34.01 72.10 61.63 33.55
NC 73.23 50.38 40.30 68.30 58.00 37.27 72.39 60.99 33.80 72.07 61.81 33.45
0.8 76.27 52.90 37.53 70.26 60.26 35.11 72.75 63.49 32.19 72.64 63.29 32.35
Table 4: Improvement on word alignment quality on moderate-size corpus, where BL and NC represents
baseline and non-constrained Model 4 training
relationship between quality of constraints and
alignment results is an interesting topic for future
research.
5.2 Effects on translation quality
In this experiment we run the whole machine
translation pipeline and evaluate the system on
BLEU score. We used the corpus LDC2006G05
which contains 25 million words as training set,
the same discriminative tuning set as previously
used (100 sentence pairs) and the remaining
21,763 sentence pairs from the hand-aligned cor-
pus of the previous experiment are held-out test
set for alignment qualities. A 4-gram language
model trained from English GigaWord V1 and V2
corpus was used. The AER scores on the held-
out test set are also provided for every iteration.
Based on the observation in last experiment, we
adopt the filtering threshold of 0.8.
Similar to previous experiment, the heuristi-
cally symmetrized alignments have lower preci-
sions than their EMDC counterparts, however the
gaps are smaller as shown in Table 4. We observe
2.85 and 2.21 absolute AER reduction on two di-
rections, after symmetrization the gain on AER
is 1.82. Continuing Model 4 training appears to
have minimal effect on AER, and the improve-
355
I M NIST GALE
mt06 mt02 mt03 mt04 mt05 mt08 ain db-nw db-wb dd-nw dd-wb aia
0 G 31.00 31.80 29.89 32.63 29.33 24.24 26.92 24.48 28.44 24.26
1 D 30.65 31.60 30.04 32.89 29.34 24.52 0.12 27.43 24.72 28.32 24.30 0.14G 31.35 31.91 30.35 32.75 29.40 24.16 0.15 27.39 24.50 28.22 24.60 0.15
2 D 31.61 32.31 30.40 33.06 29.49 24.11 0.33 28.17 24.42 28.58 24.36 0.34G 31.14 31.94 30.42 32.86 29.49 24.15 0.20 27.31 24.51 27.50 24.02 0.03
3 D 31.29 32.39 30.28 33.19 29.60 24.41 0.43 27.64 25.32 28.55 24.71 0.47G 30.94 31.95 30.15 32.71 29.38 24.22 0.12 27.63 24.61 28.80 25.05 0.29
4 D 30.80 32.04 30.51 33.24 29.49 24.61 0.46 27.61 25.27 28.72 24.98 0.53G 30.68 31.81 30.33 33.05 29.28 24.41 0.26 27.20 24.79 28.43 24.50 0.24
5 D 30.93 31.89 29.96 32.89 29.37 24.50 0.17 27.75 24.50 29.05 24.90 0.33G 31.16 32.28 30.72 33.30 29.83 24.30 0.51 27.32 25.05 28.60 25.44 0.54
Table 5: Improvement on translation alignment quality on moderate-size corpus, The column ain shows
the average improvement of BLEU scores for all NIST test sets (excluding the tuning set MT06), and
column aia is the average improvement on all unseen test sets. The column M indicates the alignment
source, G means the alignment comes from generative aligner, and D means discriminative aligner
respectively. The number of iterations is shown in column I.
ment mainly comes from the constraints.
In the experiment, we use the Moses toolkit to
extract phrases, tune parameters and decode. We
use the NIST MT06 test set as the tuning set,
NIST MT02-05 and MT08 as unseen test sets.
We also include results for four additional unseen
test sets used in GALE evaluations: DEV07-Dev
newswire part (dd-nw, 278 sentences) and We-
blog part (dd-wb, 345 sentences), Dev07-Blind
newswire part (db-nw, 276 sentences and Weblog
part (db-wb, 312 sentences). Table 5 presents the
average improvement on BLEU scores in each it-
eration. As we can see from the results, in all iter-
ations we got improvement on BLEU scores, and
the largest gain we have gotten is on the fifth it-
eration, which has 0.51 average improvement on
five NIST test sets, and 0.54 average improvement
across all nine test sets.
6 Conclusion
In this paper we presented a novel training
scheme for word alignment task called EMDC.
We also presented an extension of GIZA++ that
can perform constrained EM training. By inte-
grating it with a CRF-based discriminative word
aligner and alignment link filtering, we can im-
prove the alignment quality of both aligners itera-
tively. We experimented with small-size Chinese-
English and Arabic English and moderate-size
Chinese-English word alignment tasks, and ob-
served in all four mono-directional alignments
more than 3% absolute reduction on AER, with
the largest improvement being 8.16% absolute on
Arabic-to-English comparing to the baseline, and
5.90% comparing to Model 4 training with the
same numbers of iterations. On a moderate-size
Chinese-to-English tasks we also evaluated the
impact of the improved alignment on translation
quality across nine test sets. The 2% absolute
AER reduction resulted in 0.5 average improve-
ment on BLEU score.
Observations on the results raise several inter-
esting questions for future research, such as 1)
What is the relationship between the precision of
the constraints and the quality of resulting align-
ments after iterations, 2) The effect of using dif-
ferent discriminative aligners, 3) Using aligners
that explicitly model empty words and null align-
ments to provide additional constraints. We will
continue exploration on these directions.
The extended GIZA++ is released to the re-
search community as a branch of MGIZA++ (Gao
and Vogel, 2008), which is available online3.
Acknowledgement
This work is supported by NSF CluE Project
(NSF 08-560) and DARPA GALE project.
3Accessible on Source Forge, with the URL:
http://sourceforge.net/projects/mgizapp/
356
References
Ayan, Necip Fazil and Bonnie J. Dorr. 2006. Going
beyond aer: an extensive analysis of word align-
ments and their impact on mt. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 9?16.
Blunsom, Phil and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 65?72.
Brown, Peter F., Vincent J.Della Pietra, Stephen
A. Della Pietra, and Robert. L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. In Computational Linguistics,
volume 19(2), pages 263?331.
Callison-Burch, C., D. Talbot, and M. Osborne.
2004. Statistical machine translation with word-
and sentence-aligned parallel corpora. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 175?183.
Fraser, Alexander and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 769?776.
Fraser, Alexander and Daniel Marcu. 2007. Get-
ting the structure right for word alignment: LEAF.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 51?60.
Gao, Qin and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In Proceedings
of the ACL 2008 Software Engineering, Testing, and
Quality Assurance Workshop, pages 49?57.
Gao, Qin and Stephan Vogel. 2010. Consensus ver-
sus expertise : A case study of word alignment with
mechanical turk. In NAACL 2010 Workshop on Cre-
ating Speech and Language Data With Mechanical
Turk, pages 30?34.
Gao, Qin, Nguyen Bach, and Stephan Vogel. 2010.
A semi-supervised word alignment algorithm with
partial manual alignments. In In Proceedings of
the ACL 2010 joint Fifth Workshop on Statistical
Machine Translation and Metrics MATR (ACL-2010
WMT).
Guzman, Francisco, Qin Gao, and Stephan Vogel.
2009. Reassessment of the role of phrase extrac-
tion in pbsmt. In The twelfth Machine Translation
Summit.
Huang, Fei. 2009. Confidence measure for word
alignment. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 932?940.
Ittycheriah, Abraham and Salim Roukos. 2005. A
maximum entropy word aligner for arabic-english
machine translation. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 89?96.
Liu, Yang, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 459?466.
Moore, Robert C. 2005. A discriminative frame-
work for bilingual word alignment. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 81?88.
Niehues, Jan. and Stephan. Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 18?25.
Och, Franz Joseph and Hermann Ney. 2003. A
systematic comparison of various statistical align-
ment models. In Computational Linguistics, vol-
ume 1:29, pages 19?51.
Taskar, Ben, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 73?80.
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM based word alignment in statis-
tical machine translation. In Proceedings of 16th In-
ternational Conference on Computational Linguis-
tics), pages 836?841.
357
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 815?823,
Beijing, August 2010
Nonparametric Word Segmentation for Machine Translation
ThuyLinh Nguyen Stephan Vogel Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
{thuylinh,vogel,nasmith}@cs.cmu.edu
Abstract
We present an unsupervised word seg-
mentation model for machine translation.
The model uses existing monolingual seg-
mentation techniques and models the joint
distribution over source sentence segmen-
tations and alignments to the target sen-
tence. During inference, the monolin-
gual segmentation model and the bilin-
gual word alignment model are coupled
so that the alignments to the target sen-
tence guide the segmentation of the source
sentence. The experiments show improve-
ments on Arabic-English and Chinese-
English translation tasks.
1 Introduction
In statistical machine translation, the smallest unit
is usually the word, defined as a token delimited
by spaces. Given a parallel corpus of source and
target text, the training procedure first builds a
word alignment, then extracts phrase pairs from
this word alignment. However, in some languages
(e.g., Chinese) there are no spaces between words.
The same problem arises when translating be-
tween two very different languages, such as from
a language with rich morphology like Hungarian
or Arabic to a language with poor morphology
like English or Chinese. A single word in a mor-
phologically rich language is often the composi-
tion of several morphemes, which correspond to
separate words in English.1
1We will use the terms word segmentation, morphologi-
cal analysis, and tokenization more or less interchangeably.
Often some preprocessing is applied involving
word segmentation or morphological analysis of
the source and/or target text. Such preprocess-
ing tokenizes the text into morphemes or words,
which linguists consider the smallest meaning-
bearing units of the language. Take as an ex-
ample the Arabic word ?fktbwha? and its En-
glish translation ?so they wrote it?. The preferred
segmentation of ?fktbwha? would be ?f-ktb-w-ha
(so-wrote-they-it),? which would allow for a one-
to-one mapping between tokens in the two lan-
guages. However, the translation of the phrase in
Hebrew is ?wktbw ath?. Now the best segmen-
tation of the Arabic words would be ?fktbw-ha,?
corresponding to the two Hebrew words. This ex-
ample shows that there may not be one correct
segmentation that can be established in a prepro-
cessing step. Rather, tokenization depends on the
language we want to translate into and needs to
be tied in with the alignment process. In short,
we want to find the tokenization yielding the best
alignment, and thereby the best translation sys-
tem.
We propose an unsupervised tokenization
method for machine translation by formulating a
generative Bayesian model to ?explain? the bilin-
gual training data. Generation of a sentence pair
is described as follows: first a monolingual to-
kenization model generates the source sentence,
then the alignment model generates the target sen-
tence through the alignments with the source sen-
tence. Breaking this generation process into two
steps provides flexibility to incorporate existing
monolingual morphological segmentation mod-
els such as those of Mochihashi et al (2009) or
Creutz and Lagus (2007). Using nonparametric
815
models and the Bayesian framework makes it pos-
sible to incorporate linguistic knowledge as prior
distributions and obtain the posterior distribution
through inference techniques such as MCMC or
variational inference.
As new test source sentences do not have trans-
lations which can help to infer the best segmenta-
tion, we decode the source string according to the
posterior distribution from the inference step.
In summary, our segmentation technique con-
sists of the following steps:
? A joint model of segmented source text and
its target translation.
? Inference of the posterior distribution of the
model given the training data.
? A decoding algorithm for segmenting source
text.
? Experiments in translation using the prepro-
cessed source text.
Our experiments show that the proposed seg-
mentation method leads to improvements on
Arabic-English and Chinese-English translation
tasks.
In the next section we will discuss related work.
Section 3 will describe our model in detail. The
inference will be covered in Section 4, and decod-
ing in Section 5. Experiments and results will be
presented in Section 6.
2 Related Work
The problem of segmentation for machine trans-
lation has been studied extensively in recent lit-
erature. Most of the work used some linguistic
knowledge about the source and the target lan-
guages (Nie?en and Ney, 2004; Goldwater and
McClosky, 2005). Sadat and Habash (2006) ex-
perimented with a wide range of tokenization
schemes for Arabic-English translation. These
experiments further show that even for a single
language pair, different tokenizations are needed
depending on the training corpus size. The ex-
periments are very expensive to conduct and do
not generalize to other language pairs. Recently,
Dyer (2009) created manually crafted lattices for
a subset of source words as references for seg-
mentation when translating into English, and then
learned the segmentation of the source words to
optimize the translation with respect to these ref-
erences. He showed that the parameters of the
model can be applied to similar languages when
translating into English. However, manually cre-
ating these lattices is time-consuming and requires
a bilingual person with some knowledge of the un-
derlying statistical machine translation system.
There have been some attempts to apply un-
supervised methods for tokenization in machine
translation (Chung and Gildea, 2009; Xu et al,
2008). The alignment model of Chung and
Gildea (2009) forces every source word to align
with a target word. Xu et al (2008) mod-
eled the source-to-null alignment as in the source
word to target word model. Their models are
special cases of our proposed model when the
source model2 is a unigram model. Like Xu et
al. (2008), we use Gibbs sampling for inference.
Chung and Gildea (2009) applied efficient dy-
namic programming-based variational inference
algorithms.
We benefit from existing unsupervised mono-
lingual segmentation. The source model uses the
nested Pitman-Yor model as described by Mochi-
hashi et al (2009). When sampling each potential
word boundary, our inference technique is a bilin-
gual extension of what is described by Goldwater
et al (2006) for monolingual segmentation.
Nonparametric models have received attention
in machine translation recently. For example,
DeNero et al (2008) proposed a hierarchical
Dirichlet process model to learn the weights of
phrase pairs to address the degeneration in phrase
extraction. Teh (2006) used a hierarchical Pitman-
Yor process as a smoothing method for language
models.
Recent work on multilingual language learning
successfully used nonparametric models for lan-
guage induction tasks such as grammar induction
(Snyder et al, 2009; Cohen et al, 2010), morpho-
logical segmentation (Goldwater et al, 2006; Sny-
der and Barzilay, 2008), and part-of-speech tag-
ging (Goldwater and Griffiths, 2007; Snyder et al,
2Note that ?source model? here means a model of source
text, not a source model in the noisy channel paradigm.
816
2008).
3 Models
We start with the generative process for a source
sentence and its alignment with a target sentence.
Then we describe individual models employed by
this generation scheme.
3.1 Generative Story
A source sentence is a sequence of word tokens,
and each word is either aligned or not aligned. We
focus only on the segmentation problem and not
reordering source words; therefore, the model will
not generate the order of the target word tokens.
A sentence pair and its alignment are captured by
four components:
? a sequence of words in the source sentence,
? a set of null-aligned source tokens,
? a set of null-aligned target tokens, and
? a set of (source word to target word) align-
ment pairs.
We will start with a high-level story of how the
segmentation of the source sentence and the align-
ment are generated.
1. A source language monolingual segmenta-
tion model generates the source sentence.
2. Generate alignments:
(a) Given the sequence of words of the
source sentence already generated in
step 1, the alignment model marks each
source word as either aligned or un-
aligned. If a source word is aligned, the
model also generates the target word.
(b) Unaligned target words are generated.
The model defines the joint probability of a seg-
mented source language sentence and its align-
ment. During inference, the two parts are cou-
pled, so that the alignment will influence which
segmentation is selected. However, there are sev-
eral advantages in breaking the generation process
into two steps.
First of all, in principle the model can incor-
porate any existing probabilistic monolingual seg-
mentation to generate the source sentence. For
example, the source model can be the nested
Pitman-Yor process as described by Mochihashi et
al. (2009), the minimum description length model
presented by Creutz and Lagus (2007), or some-
thing else. Also the source model can incorporate
linguistic knowledge from a rule-based or statisti-
cal morphological disambiguator.
The model generates the alignment after the
source sentence with word boundaries already
generated. Therefore, the alignment model can
be any existing word alignment model (Brown
et al, 1993; Vogel et al, 1996). Even though
the choices of source model or alignment model
can lead to different inference methods, the model
we propose here is highly extensible. Note that
we assume that the alignment consists of at most
one-to-one mappings between source and target
words, with null alignments possible on both
sides.
Another advantage of a separate source model
lies in the segmentation of an unseen test set. In
section 5 we will show how to apply the source
model distribution learned from training data to
find the best segmentation of an unseen test set.
Notation and Parameters
We will use bold font for a sequence or bags
of words and regular font for an individual word.
A source sentence s is a sequence of |s| words
si:
(
s1, . . . , s|s|
); the translation of sentence s is
the target sentence t of |t| words (t1, . . . , t|t|
).
In sentence s the list of unaligned words is snal
and the list of aligned source words is sal. In
the target sentence t the list of unaligned words
is tnal and the list of target words having one-
to-one alignment with source words sal is tal.
The alignment a of s and t is represented by
{?si, null? | si ? snal} ? {?si, tai? | si ? sal; tai ?
tal} ? {?null, tj? | tj ? tnal} where ai denotes
the index in t of the word aligned to si.
The probability of a sequence or a set is denoted
by P (.), probability at the word level is p (.). For
example, the probability of sentence s is P (s), the
probability of a word s is p (s), the probability
that the target word t aligns to an aligned source
817
word s is p (t |s).
A sentence pair and its alignment are generated
from the following models:
? The source model generates sentence s with
probability P (s).
? The source-to-null alignment model de-
cides independently for each word s
whether it is unaligned with probability
p (null | si) or aligned with probabil-
ity: 1 ? p (null | si). The probability
of this step, for all source words, is:
P (snal, sal | s) = ?si?snal p (null | si) ??
si?sal (1 ? p (null | si)) .
We will also refer to the source-to-null model
as the deletion model, since words in snal are
effectively deleted for the purposes of align-
ment.
? The source-to-target algnment model gen-
erates a bag of target words tal aligned
to the source words sal with probability:
P (tal |sal) = ?si?sal;tai?tal p (tai |si). Notethat we do not need to be concerned with
generating a explicitly, since we do not
model word order on the target side.
? The null-to-target algnment model gen-
erates the list of unaligned target words
tnal given aligned target words tal with
P (tnal |tal) as follows:
? Generate the number of unaligned tar-
get words |tnal| given the number of
aligned target words |tal| with probabil-
ity P (|tnal| | |tal|).
? Generate |tnal| unaligned words t ?
tnal independently, each with probabil-
ity p (t |null).
The resulting null-to-target proba-
bility is therefore: P (tnal | tal) =
P (|tnal| | |tal|)?t?tnal p (t |null) .
We also call the null-to-target model the in-
sertion model.
The above generation process defines the joint
probability of source sentence s and its alignment
a as follows:
P (s, a) = P (s)????
source model
? P (a | s)? ?? ?
alignment model
(1)
P (a | s) = P (tal |sal) ? P (tnal |tal) (2)
?
?
si?snal
p (null | si) ?
?
si?sal
(1 ? p (null | si))
3.2 Source Model
Our generative process provides the flexibility of
incorporating different monolingual models into
the probability distribution of a sentence pair.
In particular we use the existing state-of-the-art
nested Pitman-Yor n-gram language model as de-
scribed by Mochihashi et al (2009). The proba-
bility of s is given by
P (s) = P (|s|)
|s|?
i=1
p (si |si?n, . . . , si?1) (3)
where the n-gram probability is a hierarchical
Pitman-Yor language model using (n ? 1)-gram
as the base distribution.
At the unigram level, the model uses the base
distribution p (s) as the infinite-gram character-
level Pitman-Yor language model.
3.3 Modeling Null-Aligned Source Words
The probability that a source word aligns to null
p (null | s) is defined by a binomial distribution
with Beta prior Beta (?p, ? (1 ? p)), where ?
and p are model parameters. When p ? 0 and
? ? ? the probability p (null | s) converges to 0
forcing each source words align to a target word.
We fixed p = 0.1 and ? = 20 in our experiment.
Xu et al (2008) view the null word as another
target word, hence in their model the probability
that a source word aligns to null can only depend
on itself.
By modeling the source-to-null alignment sep-
arately, our model lets the distribution depend
on the word?s n-gram context as in the source
model. p (null | si?n, . . . , si) stands for the prob-
ability that the word si is not aligned given its con-
text (si?n, . . . , si?1).
The n-gram source-to-null distribution
p (null | si?n, . . . , si) is defined similarly to
818
p (null | si) definition above in which the base
distribution p now becomes the (n ? 1)-gram:
p (null | si?n+1, . . . , si).3
3.4 Source-Target Alignment Model
The probability p (t |s) that a target word t aligns
to a source word s is a Pitman-Yor process:
t | s ? PY (d, ?, p0 (t |s))
here d and ? are the input parameters, and
p0 (t |s) is the base distribution.
Let |s, ?| denote the number of times s is aligned
to any t in the corpus and let |s, t| denote the num-
ber of times s is aligned to t anywhere in the cor-
pus. And let ty(s) denote the number of different
target words t the word s is aligned to anywhere
in the corpus. In the Chinese Restaurant Process
metaphor, there is one restaurant for each source
word s, the s restaurant has ty(s) tables and total
|s, ?| customers; table t has |s, t| customers.
Then, at a given time in the generative process
for the corpus, we can write the probability that t
is generated by the word s as:
? if |s, t| > 0:
p (t |s) =
|s, t| ? d + [? + dty(s)]p0 (t |s)
|s, ?| + ?
? if |s, t| = 0:
p (t |s) = [? + dty(s)]p0 (t |s)|s, ?| + ?
For language pairs with similar character sets
such as English and French, words with similar
surface form are often translations of each other.
The base distribution can be defined based on
the edit distance between two words (Snyder and
Barzilay, 2008).
We are working with diverse language pairs
(Arabic-English and Chinese-English), so we
use the base distribution as the flat distribution
p0 (t |s) = 1T ; T is the number of distinct targetwords in the training set. In our experiment, the
model parameters are ? = 20 and d = .5.
3We also might have conditioned this decision on words
following si, since those have all been generated already at
this stage.
3.5 Modeling Null-Aligned Target Words
The null-aligned target words are modeled condi-
tioned on previously generated target words as:
P (tnal |tal) = P (|tnal| | |tal|)
?
t?tnal
p (t |null)
This model uses two probability distributions:
? the number of unaligned target words:
P (|tnal| | |tal|), and
? the probability that each word in tnal is gen-
erated by null: p (t |null).
We model the number of unaligned target
words similarly to the distribution in the IBM3
word alignment model (Brown et al, 1993).
IBM3 assumes that each aligned target words gen-
erates a null-aligned target word with probabil-
ity p0 and fails to generate a target word with
probability 1 ? p0. So the parameter p0 can
be used to control the number of unaligned tar-
get words. In our experiments, we fix p0 =
.05. Following this assumption, the probability of
|tnal| unaligned target words generated from |tal|
words is: P (|tnal| | |tal|) =
( |tal|
|tnal|
)
p|tnal|0 (1 ?
p0)|tal|?|tnal|.
The probability that a target word t aligns to
null, p (t |null), also has a Pitman-Yor process
prior. The base distribution of the model is similar
to the source-to-target model?s base distribution
which is the flat distribution over target words.
4 Inference
We have defined a probabilistic generative model
to describe how a corpus of alignments and seg-
mentations can be generated jointly. In this sec-
tion we discuss how to obtain the posterior distri-
butions of the missing alignments and segmenta-
tions given the training corpus, using Gibbs sam-
pling.
Suppose we are provided a morphological
disambiguator for the source language such as
MADA morphology tokenization toolkit (Sadat
and Habash, 2006) for Arabic.4 The morpho-
logical disambiguator segments a source word to
4MADA provides several segmentation schemes; among
them the MADA-D3 scheme seeks to separate all mor-
phemes of each word.
819
morphemes of smallest meaning-bearing units of
the source language. Therefore, a target word is
equivalent to one or several morphemes. Given
a morphological disambiguation toolkit, we use
its output to bias our inference by not consider-
ing word boundaries after every character but only
considering potential word boundaries as a subset
of the morpheme boundaries set. In this way, the
inference uses the morphological disambiguation
toolkit to limit its search space.
The inference starts with an initial segmenta-
tion of the source corpus and also its alignment
to the target corpus. The Gibbs sampler consid-
ers one potential word boundary at a time. There
are two hypotheses at any given boundary posi-
tion of a sentence pair (s, t): the merge hypothe-
sis stands for no word boundary and the resulting
source sentence smerge has a word s spanning over
the sample point; the split hypothesis indicates the
resulting source sentence ssplit has a word bound-
ary at the sample point separating two words s1s2.
Similar to Goldwater et al (2006) for monolingual
segmentation, the sampler randomly chooses the
boundary according to the relative probabilities of
the merge hypothesis and the split hypothesis.
The model consists of source and alignment
model variables; given the training corpora size of
a machine translation system, the number of vari-
ables is large. So if the Gibbs sampler samples
both source variables and alignment variables, the
inference requires many iterations until the sam-
pler mixes. Xu et al (2008) fixed this by repeat-
edly applying GIZA++ word alignment after each
sampling iteration through the training corpora.
Our inference technique is not precisely Gibbs
sampling. Rather than sampling the alignment or
attempting to collapse it out (by summing over
all possible alignments when calculating the rel-
ative probabilities of the merge and split hypothe-
ses), we seek the best alignment for each hypoth-
esis. In other words, for each hypothesis, we per-
form a local search for a high-probability align-
ment of the merged word or split words, given
the rest of alignment for the sentence. Up to one
word may be displaced and realigned. This ?local-
best? alignment is used to score the hypothesis,
and after sampling merge or split, we keep that
best alignment.
This inference technique is motivated by run-
time demands, but we do not yet know of a the-
oretical justification for combining random steps
with maximization over some variables. A more
complete analysis is left to future work.
5 Decoding for Unseen Test Sentences
Section 4 described how to get the model?s pos-
terior distribution and the segmentation and align-
ment of the training data under the model. We are
left with the problem of decoding or finding the
segmentation of test sentences where the transla-
tions are not available. This is needed when we
want to translate new sentences. Here, tokeniza-
tion is performed as a preprocessing step, decou-
pled from the subsequent translation steps.
The decoding step uses the model?s posterior
distribution for the training data to segment un-
seen source sentences. Because of the clear sep-
aration of the source model and the alignment
model, the source model distribution learned from
the Gibbs sampling directly represents the distri-
bution over the source language and can therefore
also handle the segmentation of unknown words
in new test sentences. Only the source model is
used in preprocessing.
The best segmentation s? of a string of charac-
ters c = (c1, . . . , c|c|
) according to the n-gram
source model is:
s? = argmax
s from c
p (|s|)
i=|s|?
i=1
p (si |si?n, . . . , si?1)
We use a stochastic finite-state machine for de-
coding. This is possible by composition of the fol-
lowing two finite state machines:
? Acceptor Ac. The string of characters c is
represented as an finite state acceptor ma-
chine where any path through the machine
represents an unweighted segmentation of c.
? Source model weighted finite state trans-
ducer Lc. Knight and Al-Onaizan (1998)
show how to build an n-gram language
model by a weighted finite state machine.
The states of the transducer are (n ? 1)-
gram history, the edges are words from the
language. The arc si coming from state
820
(si?n, . . . , si?1) to state (si?n+1, . . . , si) has
weight p (si |si?n, . . . , si?1).
The best segmentation s? is given as s? =
BestPath(Ac ? Lc).
6 Experiments
This section presents experimental results on
Arabic-English and Chinese-English translation
tasks using the proposed segmentation technique.
6.1 Arabic-English
As a training set we use the BTEC corpus dis-
tributed by the International Workshop on Spo-
ken Language Translation (IWSLT) (Matthias and
Chiori, 2005). The corpus is a collection of
conversation transcripts from the travel domain.
The ?Supplied Data? track consists of nearly 20K
Arabic-English sentence pairs. The development
set consists of 506 sentences from the IWSLT04
evaluation test set and the unseen set consists of
500 sentences from the IWSLT05 evaluation test
set. Both development set and test set have 16 ref-
erences per Arabic sentence.
6.2 Chinese-English
The training set for Chinese-English translation
task is also distributed by the IWSLT evaluation
campaign. It consists of 67K Chinese-English
sentence pairs. The development set and the test
set each have 489 Chinese sentences and each sen-
tence has 7 English references.
6.3 Results
We will report the translation results where the
preprocessing of the source text are our unigram,
bigram, and trigram source models and source-to-
null model.
The MCMC inference algorithm starts with an
initial segmentation of the source text into full
word forms. For Chinese, we use the original
word segmentation as distributed by IWSLT. To
get an initial alignment, we generate the IBM4
Viterbi alignments in both directions using the
GIZA++ toolkit (Och and Ney, 2003) and com-
bine them using the ?grow-diag-final-and? heuris-
tic. The output of combining GIZA++ align-
ment for a sentence pair is a sequence of si-tj
entries where i is an index of the source sen-
tence and j is an index of the target sentence.
As our model allows only one-to-one mappings
between the words in the source and target sen-
tences, we remove si-tj from the sequence if ei-
ther the source word si or target word tj is al-
ready in a previous entry of the combined align-
ment sequence. The resulting alignment is our ini-
tial alignment for the inference.
We also apply the MADA morphology seg-
mentation toolkit (Habash and Rambow, 2005) to
preprocess the Arabic corpus. We use the D3
scheme (each Arabic word is segmented into mor-
phemes in sequence [CONJ+ [PART+ [Al+ BASE
+PRON]]]), mark the morpheme boundaries, and
then combine the morphemes again to have words
in their original full word form. During inference,
we only sample over these morpheme boundaries
as potential word boundaries. In this way, we
limit the search space, allowing only segmenta-
tions consistent with MADA-D3.
The inference samples 150 iterations through
the whole training set and uses the posterior prob-
ability distribution from the last iteration for de-
coding. The decoding process is then applied
to the entire training set as well as to the devel-
opment and test sets to generate a consistent to-
kenization across all three data sets. We used
the OpenFST toolkit (Allauzen et al, 2007) for
finite-state machine implementation and opera-
tions. The output of the decoding is the pre-
processed data for translation. We use the open
source Moses phrase-based MT system (Koehn et
al., 2007) to test the impact of the preprocessing
technique on translation quality.5
6.3.1 Arabic-English Translation Results
We consider the Arabic-English setting. We
use two baselines: original full word form
and MADA-D3 tokenization scheme for Arabic-
English translation. Table 1 compares the trans-
lation results of our segmentation methods with
these baselines. Our segmentation method shows
improvement over the two baselines on both the
development and test sets. According to Sadat
and Habash (2006), the MADA-D3 scheme per-
5The Moses translation alignment is the output of
GIZA++, not from our MCMC inference.
821
Dev. Test
Original 59.21 54.00
MADA-D3 58.28 54.92
Unigram 59.44 56.18
Bigram 58.88 56.18
Trigram 58.76 56.82
Table 1: Arabic-English translation results
(BLEU).
forms best for their Arabic-English translation es-
pecially for small and moderate data sizes. In our
experiments, we see an improvement when using
the MADA-D3 preprocessing over using the orig-
inal Arabic corpus on the unseen test set, but not
on the development set.
The Gibbs sampler only samples on the mor-
phology boundary points of MADA-D3, so the
improvement resulting from our segmentation
technique does not come from removing unknown
words. It is due to a better matching between
the source and target sentences by integrating seg-
mentation and alignment. We therefore expect the
same impact on a larger training data set in future
experiments.
6.3.2 Chinese-English Translation Results
Dev. Test
Whole word 23.75 29.02
Character 23.39 27.74
Unigram 24.90 28.97
Trigram 23.98 28.20
Table 2: Chinese-English translation result in
BLEU score metric.
We next consider the Chinese-English setting.
The translation performance using our word seg-
mentation technique is shown in Table 2. There
are two baselines for Chinese-English translation:
(a) the source text in the full word form distributed
by the IWSLT evaluation and (b) no segmentation
of the source text, which is equivalent to interpret-
ing each Chinese character as a single word.
Taking development and test sets into account,
the best Chinese-English translation system re-
sults from our unigram model. It is significantly
better than other systems on the development set
and performs almost equally well with the IWSLT
segmentation on the test set. Note that the seg-
mentation distributed by IWSLT is a manual seg-
mentation for the translation task.
Chung and Gildea (2009) and Xu et al (2008)
also showed improvement over a simple mono-
lingual segmentation for Chinese-English trans-
lation. Our character-based translation result is
comparable to their monolingual segmentations.
Both trigram and unigram translation results out-
perform the character-based translation.
We also observe that there are no additional
gains for Chinese-English translation when using
a higher n-gram model. Our Gibbs sampler has
the advantage that the samples are guaranteed to
converge eventually to the model?s posterior dis-
tributions, but in each step the modification to the
current hypothesis is small and local. In itera-
tions 100?150, the average number of boundary
changes for the unigram model is 14K boundaries
versus only 1.5K boundary changes for the tri-
gram model. With 150 iterations, the inference
output of trigram model might not yet represent
its posterior distribution. We leave a more de-
tailed investigation of convergence behavior to fu-
ture work.
Conclusion and Future Work
We presented an unsupervised segmentation
method for machine translation and presented
experiments for Arabic-English and Chinese-
English translation tasks. The model can incor-
porate existing monolingual segmentation mod-
els and seeks to learn a segmenter appropriate for
a particular translation task (target language and
dataset).
Acknowledgements
We thank Kevin Gimpel for interesting discus-
sions and technical advice. We also thank the
anonymous reviewers for useful feedback. This
work was supported by DARPA Gale project,
NSF grants 0844507 and 0915187.
822
References
Allauzen, C., M. Riley, J. Schalkwyk, W. Skut, and
M. Mohri. 2007. OpenFst: A General and Efficient
Weighted Finite-State Transducer Library. In Pro-
ceedings of the CIAA 2007, volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Brown, Peter F., Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Comput. Linguist., 19(2):263?
311.
Chung, T. and D. Gildea. 2009. Unsupervised Tok-
enization for Machine Translation. In Proceedings
of EMNLP 2009, pages 718?726, Singapore, Au-
gust. Association for Computational Linguistics.
Cohen, S. B., D. M. Blei, and N. A. Smith. 2010. Vari-
ational Inference for Adaptor Grammars. In Pro-
ceedings of NAACL-HLT, pages 564?572, June.
Creutz, Mathias and Krista Lagus. 2007. Unsu-
pervised Models for Morpheme Segmentation and
Morphology Learning. ACM Trans. Speech Lang.
Process., 4(1):1?34.
DeNero, J., A. Bouchard-Co?te?, and D. Klein. 2008.
Sampling Alignment Structure under a Bayesian
Translation Model. In Proceedings of EMNLP
2008, pages 314?323, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Dyer, C. 2009. Using a Maximum Entropy model to
build segmentation lattices for MT. In Proceedings
of HLT 2009, pages 406?414, Boulder, Colorado,
June.
Goldwater, S. and T. L. Griffiths. 2007. A Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. In Proceedings of ACL.
Goldwater, S. and D. McClosky. 2005. Improving Sta-
tistical Machine Translation Through Morphologi-
cal Analysis. In Proc. of EMNLP.
Goldwater, S., T. L. Griffiths, and M. Johnson. 2006.
Contextual Dependencies in Unsupervised Word
Segmentation. In Proc. of COLING-ACL.
Habash, N. and O. Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging, and Morpholog-
ical Disambiguation in One Fell Swoop. In Proc. of
ACL.
Knight, K. and Y. Al-Onaizan. 1998. Translation
with Finite-State Devices. In Proceedings of AMTA,
pages 421?437.
Koehn, P., H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
of ACL (demo session).
Matthias, E. and H. Chiori. 2005. Overview of the
IWSLT 2005 Evaluation Campaign. In Proceedings
of IWSLT.
Mochihashi, D., T. Yamada, and N. Ueda. 2009.
Bayesian Unsupervised Word Segmentation with
Nested Pitman-Yor Language Modeling. In Pro-
ceedings of 47th ACL, pages 100?108, Suntec, Sin-
gapore, August.
Nie?en, S. and H. Ney. 2004. Statistical Machine
Translation with Scarce Resources Using Morpho-
Syntactic Information. Computational Linguistics,
30(2), June.
Och, F. and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1).
Sadat, F. and N. Habash. 2006. Combination of Ara-
bic Preprocessing Schemes for Statistical Machine
Translation. In Proceedings of the ACL, pages 1?8.
Snyder, B. and R. Barzilay. 2008. Unsupervised Mul-
tilingual Learning for Morphological Segmentation.
In Proceedings of ACL-08: HLT, pages 737?745,
June.
Snyder, B., T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised Multilingual Learning for POS
Tagging. In Proceedings of EMNLP.
Snyder, B., T. Naseem, and R. Barzilay. 2009. Unsu-
pervised Multilingual Grammar Induction. In Pro-
ceedings of ACL-09, pages 73?81, August.
Teh, Y. W. 2006. A Hierarchical Bayesian Language
Model Based On Pitman-Yor Processes. In Pro-
ceedings of ACL, pages 985?992, July.
Vogel, S., H. Ney, and C. Tillmann. 1996. HMM-
Based Word Alignment in Statistical Translation. In
Proceedings of COLING, pages 836?841.
Xu, J., J. Gao, K. Toutanova, and H. Ney. 2008.
Bayesian Semi-Supervised Chinese Word Segmen-
tation for Statistical Machine Translation. In
Proceedings of (Coling 2008), pages 1017?1024,
Manchester, UK, August.
823
Proceedings of the ACL 2010 Conference Short Papers, pages 147?150,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Fixed Length Word Suffix for Factored 
Statistical Machine Translation 
 
Narges Sharif Razavian 
School of Computer Science 
Carnegie Mellon Universiy 
Pittsburgh, USA 
nsharifr@cs.cmu.edu 
Stephan Vogel 
School of Computer Science 
Carnegie Mellon Universiy 
Pittsburgh, USA 
stephan.vogel@cs.cmu.edu 
 
 
 
 
Abstract 
Factored Statistical Machine Translation ex-
tends the Phrase Based SMT model by al-
lowing each word to be a vector of factors. 
Experiments have shown effectiveness of 
many factors, including the Part of Speech 
tags in improving the grammaticality of the 
output. However, high quality part of 
speech taggers are not available in open 
domain for many languages. In this paper 
we used fixed length word suffix as a new 
factor in the Factored SMT, and were able 
to achieve significant improvements in three 
set of experiments: large NIST Arabic to 
English system, medium WMT Spanish to 
English system, and small TRANSTAC 
English to Iraqi system. 
1 Introduction 
Statistical Machine Translation(SMT) is current-
ly the state of the art solution to the machine 
translation. Phrase based SMT is also among the 
top performing approaches available as of today. 
This approach is a purely lexical approach, using 
surface forms of the words in the parallel corpus 
to generate the translations and estimate proba-
bilities. It is possible to incorporate syntactical 
information into this framework through differ-
ent ways. Source side syntax based re-ordering 
as preprocessing step, dependency based reorder-
ing models, cohesive decoding features are 
among many available successful attempts for 
the integration of syntax into the translation 
model. Factored translation modeling is another 
way to achieve this goal. These models allow 
each word to be represented as a vector of factors 
rather than a single surface form.  Factors can 
represent richer expression power on each word. 
Any factors such as word stems, gender, part of 
speech, tense, etc. can be easily used in this 
framework.  
   Previous work in factored translation modeling 
have reported consistent improvements from Part 
of Speech(POS) tags, morphology, gender, and 
case factors (Koehn et. a. 2007). In another work, 
Birch et. al. 2007 have achieved improvement 
using Combinational Categorial Grammar (CCG) 
super-tag factors. Creating the factors is done as 
a preprocessing step, and so far, most of the ex-
periments have assumed existence of external 
tools for the creation of these factors (i. e. Part of 
speech taggers, CCG parsers, etc.). Unfortunately 
high quality language processing tools, especial-
ly for the open domain, are not available for most 
languages. 
   While linguistically identifiable representations 
(i.e. POS tags, CCG supertags, etc) have been 
very frequently used as factors in many applica-
tions including MT, simpler representations have 
also been effective in achieving the same result 
in other application areas. Grzymala-Busse and 
Old 1997, DINCER et.al. 2008, were able to use 
fixed length suffixes as features for training a 
POS tagging. In another work Saberi and Perrot 
1999 showed that reversing middle chunks of the 
words while keeping the first and last part intact, 
does not decrease listeners? recognition ability. 
This result is very relevant to Machine Transla-
tion, suggesting that inaccurate context which is 
usually modeled with n-gram language models, 
can still be as effective as accurate surface forms. 
Another research (Rawlinson 1997) confirms this 
finding; this time in textual domain, observing 
that randomization of letters in the middle of 
words has little or no effect on the ability of 
skilled readers to understand the text. These re-
sults suggest that the inexpensive representation-
al factors which do not need unavailable tools 
might also be worth investigating. 
   These results encouraged us to introduce lan-
guage independent simple factors for machine 
translation. In this paper, following the work of 
Grzymala-Busse et. al. we used fixed length suf-
147
fix as word factor, to lower the perplexity of the 
language model, and have the factors roughly 
function as part of speech tags, thus increasing 
the grammaticality of the translation results. We 
were able to obtain consistent, significant im-
provements over our baseline in 3 different expe-
riments, large NIST Arabic to English system, 
medium WMT Spanish to English system, and 
small TRANSTAC English to Iraqi system.  
   The rest of this paper is as follows. Section 2 
briefly reviews the Factored Translation Models. 
In section 3 we will introduce our model, and 
section 4 will contain the experiments and the 
analysis of the results, and finally, we will con-
clude this paper in section 5. 
2 Factored Translation Model  
Statistical Machine Translation uses the log li-
near combination of a number of features, to 
compute the highest probable hypothesis as the 
translation.   
 
e = argmaxe p(e|f) = argmaxe p exp ?i=1
n ?i hi(e,f) 
 
   In phrase based SMT, assuming the source and 
target phrase segmentation as {(fi,ei)}, the most 
important features include: the Language Model 
feature hlm(e,f) = plm(e); the phrase translation 
feature ht(e,f) defined as product of translation 
probabilities, lexical probabilities and phrase pe-
nalty; and the reordering probability, hd(e,f), 
usually defined as ?i=1
n d(starti,endi-1) over the 
source phrase reordering events. 
   Factored Translation Model, recently intro-
duced by (Koehn et. al. 2007), allow words to 
have a vector representation. The model can then 
extend the definition of each of the features from 
a uni-dimensional value to an arbitrary joint and 
conditional combination of features. Phrase 
based SMT is in fact a special case of Factored 
SMT.  
   The factored features are defined as an exten-
sion of phrase translation features. The function 
?(fj,ej), which was defined for a phrase pair be-
fore, can now be extended as a log linear combi-
nation ?f ?f(fjf,ejf).  The model also allows for a 
generation feature, defining the relationship be-
tween final surface form and target factors. Other 
features include additional language model fea-
tures over individual factors, and factored reor-
dering features.  
   Figure 1 shows an example of a possible fac-
tored model.  
 
Figure 1: An example of a Factored Translation and 
Generation Model 
 
   In this particular model, words on both source 
and target side are represented as a vector of four 
factors: surface form, lemma, part of speech 
(POS) and the morphology. The target phrase is 
generated as follows: Source word lemma gene-
rates target word lemma. Source word's Part of 
speech and morphology together generate the 
target word's part of speech and morphology, and 
from its lemma, part of speech and morphology 
the surface form of the target word is finally gen-
erated. This model has been able to result in 
higher translation BLEU score as well as gram-
matical coherency for English to German, Eng-
lish to Spanish, English to Czech, English to 
Chinese, Chinese to English and German to Eng-
lish. 
3 Fixed Length Suffix Factors for Fac-
tored Translation Modeling 
Part of speech tagging, constituent and depen-
dency parsing, combinatory categorical grammar 
super tagging are used extensively in most appli-
cations when syntactic representations are 
needed. However training these tools require 
medium size treebanks and tagged data, which 
for most languages will not be available for a 
while. On the other hand, many simple words 
features, such as their character n-grams, have in 
fact proven to be comparably as effective in 
many applications.  
(Keikha et. al. 2008) did an experiment on text 
classification on noisy data, and compared sever-
al word representations. They compared surface 
form, stemmed words, character n-grams, and 
semantic relationships, and found that for noisy 
and open domain text, character-ngrams outper-
form other representations when used for text 
classification. In another work (Dincer et al
2009) showed that using fixed length word end-
ing outperforms whole word representation for 
training a part of speech tagger for Turkish lan-
guage.  
148
Based on this result, we proposed a suffix fac-
tored model for translation, which is shown in 
Figure 2.  
 
 
 
 
 
 
 
Figure 2: Suffix Factored model: Source word de-
termines factor vectors (target word, target word suf-
fix) and each factor will be associated with its 
language model. 
 
Based on this model, the final probability of 
the translation hypothesis will be the log linear 
combination of phrase probabilities, reordering 
model probabilities, and each of the language 
models? probabilities.  
 
P(e|f) ~  plm-word(eword)* plm-suffix(esuffix) 
 * ?i=1
n  p(eword-j & esuffix-j|fj)  
 * ?i=1
n p(fj | eword-j & esuffix-j) 
 
Where plm-word is the n-gram language model 
probability over the word surface sequence, with 
the language model built from the surface forms. 
Similarly, plm-suffix(esuffix) is the language model 
probability over suffix sequences.  p(eword-j & 
esuffix-j|fj) and p(fj | eword-j & esuffix-j) are translation 
probabilities for each phrase pair i , used in by 
the decoder. This probability is estimated after 
the phrase extraction step which is based on 
grow-diag heuristic at this stage. 
4 Experiments and Results 
We used Moses implementation of the factored 
model for training the feature weights, and SRI 
toolkit for building n-gram language models. The 
baseline for all systems included the moses sys-
tem with lexicalized re-ordering, SRI 5-gram 
language models. 
 
4.1 Small System from Dialog Domain: 
English to Iraqi 
 
This system was TRANSTAC system, which 
was built on about 650K sentence pairs with the 
average sentence length of 5.9 words. After 
choosing length 3 for suffixes, we built a new 
parallel corpus, and SRI 5-gram language models 
for each factor. Vocabulary size for the surface 
form was 110K whereas the word suffixes had 
about 8K distinct words. Table 1 shows the result 
(BLEU Score) of the system compared to the 
baseline. 
 
System Tune on  
Set-
July07 
Test on  
Set-
June08 
Test on  
Set-
Nov08 
Baseline 27.74 21.73 15.62 
Factored 28.83 22.84 16.41 
Improvement 1.09 1.11 0.79 
Table 1: BLEU score, English to Iraqi Transtac sys-
tem, comparing Factored and Baseline systems. 
 
As you can see, this improvement is consistent 
over multiple unseen datasets. Arabic cases and 
numbers show up as the word suffix. Also, verb 
numbers usually appear partly as word suffix and 
in some cases as word prefix. Defining a lan-
guage model over the word endings increases the 
probability of sequences which have this case 
and number agreement, favoring correct agree-
ments over the incorrect ones.  
 
4.2 Medium System on Travel Domain: 
Spanish to English 
 
This system is the WMT08 system, on a corpus 
of 1.2 million sentence pairs with average sen-
tence length 27.9 words. Like the previous expe-
riment, we defined the 3 character suffix of the 
words as the second factor, and built the lan-
guage model and reordering model on the joint 
event of (surface, suffix) pairs. We built 5-gram 
language models for each factor. The system had 
about 97K distinct vocabulary in the surface lan-
guage model, which was reduced to 8K using the 
suffix corpus. Having defined the baseline, the 
system results are as follows.  
 
 
System Tune-
WMT06 
Test set-
WMT08 
Baseline 33.34 32.53 
Factored 33.60 32.84 
Improvement 0.26 0.32 
Table 2: BLEU score, Spanish to English WMT sys-
tem, comparing Factored and Baseline systems. 
 
Here, we see improvement with the suffix fac-
tors compared to the baseline system. Word end-
ings in English language are major indicators of 
word?s part of speech in the sentence. In fact 
 Word Language Model 
Suffix Language Model 
 LM 
  Word 
Word ? 
Suffix ?  
Source Target 
149
most common stemming algorithm, Porter?s 
Stemmer, works by removing word?s suffix. 
Having a language model on these suffixes push-
es the common patterns of these suffixes to the 
top, making the more grammatically coherent 
sentences to achieve a better probability.  
 
4.3 Large NIST 2009 System: Arabic to 
English 
 
We used NIST2009 system as our baseline in 
this experiment. The corpus had about 3.8 Mil-
lion sentence pairs, with average sentence length 
of 33.4 words. The baseline defined the lexica-
lized reordering model. As before we defined 3 
character long word endings, and built 5-gram 
SRI language models for each factor. The result 
of this experiment is shown in table 3.  
 
System Tune 
on  
MT06 
Test on  
Dev07 
News
Wire 
Test on  
Dev07 
Weblog 
Test 
on 
MT08 
Baseline 43.06 48.87 37.84 41.70 
Factored 44.20 50.39 39.93 42.74 
Improve
ment 
1.14 1.52 2.09 1.04 
Table 3: BLEU score, Arabic to English NIST 2009 
system, comparing Factored and Baseline systems. 
 
This result confirms the positive effect of the 
suffix factors even on large systems. As men-
tioned before we believe that this result is due to 
the ability of the suffix to reduce the word into a 
very simple but rough grammatical representa-
tion. Defining language models for this factor 
forces the decoder to prefer sentences with more 
probable suffix sequences, which is believed to 
increase the grammaticality of the result. Future 
error analysis will show us more insight of the 
exact effect of this factor on the outcome. 
5 Conclusion 
In this paper we introduced a simple yet very 
effective factor: fixed length word suffix, to use 
in Factored Translation Models. This simple fac-
tor has been shown to be effective as a rough 
replacement for part of speech. We tested our 
factors in three experiments in a small, English to 
Iraqi system, a medium sized system of Spanish 
to English, and a large system, NIST09 Arabic to 
English. We observed consistent and significant 
improvements over the baseline. This result, ob-
tained from the language independent and inex-
pensive factor, shows promising new 
opportunities for all language pairs.  
References  
Birch, A., Osborne, M., and Koehn, P. CCG supertags 
in factored statistical machine translation. Proceed-
ings of the Second Workshop on Statistical Ma-
chine Translation, pages 9?16, Prague, Czech 
Republic. Association for Computational Linguis-
tics, 2007. 
Dincer T., Karaoglan B. and Kisla T., A Suffix Based 
Part-Of-Speech Tagger For Turkish, Fifth Interna-
tional Conference on Information Technology: 
New Generations, 2008. 
Grzymala-Busse J.W., Old L.J. A machine learning 
experiment to determine part of speech from word-
endings, Lecture Notes in Computer Science, 
Communications Session 6B Learning and Discov-
ery Systems, 1997.  
Keikha M., Sharif Razavian N, Oroumchian F., and 
Seyed Razi H., Document Representation and 
Quality of Text: An Analysis, Chapter 12, Survey 
of Text Mining II, Springer London, 2008. 
Koehn Ph., Hoang H., Factored Translation Models, 
Proceedings of 45th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), 2007. 
Rawlinson G. E., The significance of letter position in 
word recognition, PhD Thesis, Psychology De-
partment, University of Nottingham, Nottingham 
UK, 1976. 
Saberi K and Perrot D R, Cognitive restoration of 
reversed speech, Nature (London) 1999. 
 
150
Proceedings of the ACL 2010 Conference Short Papers, pages 365?370,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Active Learning-Based Elicitation for Semi-Supervised Word Alignment
Vamshi Ambati, Stephan Vogel and Jaime Carbonell
{vamshi,vogel,jgc}@cs.cmu.edu
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
Semi-supervised word alignment aims to
improve the accuracy of automatic word
alignment by incorporating full or par-
tial manual alignments. Motivated by
standard active learning query sampling
frameworks like uncertainty-, margin- and
query-by-committee sampling we propose
multiple query strategies for the alignment
link selection task. Our experiments show
that by active selection of uncertain and
informative links, we reduce the overall
manual effort involved in elicitation of
alignment link data for training a semi-
supervised word aligner.
1 Introduction
Corpus-based approaches to machine translation
have become predominant, with phrase-based sta-
tistical machine translation (PB-SMT) (Koehn et
al., 2003) being the most actively progressing area.
The success of statistical approaches to MT can
be attributed to the IBM models (Brown et al,
1993) that characterize word-level alignments in
parallel corpora. Parameters of these alignment
models are learnt in an unsupervised manner us-
ing the EM algorithm over sentence-level aligned
parallel corpora. While the ease of automati-
cally aligning sentences at the word-level with
tools like GIZA++ (Och and Ney, 2003) has en-
abled fast development of SMT systems for vari-
ous language pairs, the quality of alignment is typ-
ically quite low for language pairs like Chinese-
English, Arabic-English that diverge from the in-
dependence assumptions made by the generative
models. Increased parallel data enables better es-
timation of the model parameters, but a large num-
ber of language pairs still lack such resources.
Two directions of research have been pursued
for improving generative word alignment. The
first is to relax or update the independence as-
sumptions based on more information, usually
syntactic, from the language pairs (Cherry and
Lin, 2006; Fraser and Marcu, 2007a). The sec-
ond is to use extra annotation, typically word-level
human alignment for some sentence pairs, in con-
junction with the parallel data to learn alignment
in a semi-supervised manner. Our research is in
the direction of the latter, and aims to reduce the
effort involved in hand-generation of word align-
ments by using active learning strategies for care-
ful selection of word pairs to seek alignment.
Active learning for MT has not yet been ex-
plored to its full potential. Much of the litera-
ture has explored one task ? selecting sentences
to translate and add to the training corpus (Haf-
fari and Sarkar, 2009). In this paper we explore
active learning for word alignment, where the in-
put to the active learner is a sentence pair (S, T )
and the annotation elicited from human is a set of
links {aij , ?si ? S, tj ? T}. Unlike previous ap-
proaches, our work does not require elicitation of
full alignment for the sentence pair, which could
be effort-intensive. We propose active learning
query strategies to selectively elicit partial align-
ment information. Experiments in Section 5 show
that our selection strategies reduce alignment error
rates significantly over baseline.
2 Related Work
Researchers have begun to explore models that
use both labeled and unlabeled data to build
word-alignment models for MT. Fraser and Marcu
(2006) pose the problem of alignment as a search
problem in log-linear space with features com-
ing from the IBM alignment models. The log-
365
linear model is trained on available labeled data
to improve performance. They propose a semi-
supervised training algorithm which alternates be-
tween discriminative error training on the la-
beled data to learn the weighting parameters and
maximum-likelihood EM training on unlabeled
data to estimate the parameters. Callison-Burch
et al (2004) also improve alignment by interpolat-
ing human alignments with automatic alignments.
They observe that while working with such data
sets, alignments of higher quality should be given
a much higher weight than the lower-quality align-
ments. Wu et al (2006) learn separate models
from labeled and unlabeled data using the standard
EM algorithm. The two models are then interpo-
lated to use as a learner in the semi-supervised
algorithm to improve word alignment. To our
knowledge, there is no prior work that has looked
at reducing human effort by selective elicitation of
partial word alignment using active learning tech-
niques.
3 Active Learning for Word Alignment
Active learning attempts to optimize performance
by selecting the most informative instances to la-
bel where ?informativeness? is defined as maximal
expected improvement in accuracy. The objective
is to select optimal instance for an external expert
to label and then run the learning method on the
newly-labeled and previously-labeled instances to
minimize prediction or translation error, repeat-
ing until either the maximal number of external
queries is reached or a desired accuracy level is
achieved. Several studies (Tong and Koller, 2002;
Nguyen and Smeulders, 2004; Donmez and Car-
bonell, 2008) show that active learning greatly
helps to reduce the labeling effort in various clas-
sification tasks.
3.1 Active Learning Setup
We discuss our active learning setup for word
alignment in Algorithm 1. We start with an un-
labeled dataset U = {(Sk, Tk)}, indexed by k,
and a seed pool of partial alignment links A0 =
{akij , ?si ? Sk, tj ? Tk}. This is usually an empty
set at iteration t = 0. We iterate for T itera-
tions. We take a pool-based active learning strat-
egy, where we have access to all the automatically
aligned links and we can score the links based
on our active learning query strategy. The query
strategy uses the automatically trained alignment
model Mt from current iteration t for scoring the
links. Re-training and re-tuning an SMT system
for each link at a time is computationally infeasi-
ble. We therefore perform batch learning by se-
lecting a set of N links scored high by our query
strategy. We seek manual corrections for the se-
lected links and add the alignment data to the
current labeled data set. The word-level aligned
labeled data is provided to our semi-supervised
word alignment algorithm for training an align-
ment model Mt+1 over U .
Algorithm 1 AL FOR WORD ALIGNMENT
1: Unlabeled Data Set: U = {(Sk, Tk)}
2: Manual Alignment Set : A0 = {akij ,?si ?
Sk, tj ? Tk}
3: Train Semi-supervised Word Alignment using
(U , A0)?M0
4: N : batch size
5: for t = 0 to T do
6: Lt = LinkSelection(U ,At,Mt,N )
7: Request Human Alignment for Lt
8: At+1 = At + Lt
9: Re-train Semi-Supervised Word Align-
ment on (U,At+1)?Mt+1
10: end for
We can iteratively perform the algorithm for a
defined number of iterations T or until a certain
desired performance is reached, which is mea-
sured by alignment error rate (AER) (Fraser and
Marcu, 2007b) in the case of word alignment. In
a more typical scenario, since reducing human ef-
fort or cost of elicitation is the objective, we iterate
until the available budget is exhausted.
3.2 Semi-Supervised Word Alignment
We use an extended version of MGIZA++ (Gao
and Vogel, 2008) to perform the constrained semi-
supervised word alignment. Manual alignments
are incorporated in the EM training phase of these
models as constraints that restrict the summation
over all possible alignment paths. Typically in the
EM procedure for IBM models, the training pro-
cedure requires for each source sentence position,
the summation over all positions in the target sen-
tence. The manual alignments allow for one-to-
many alignments and many-to-many alignments
in both directions. For each position i in the source
sentence, there can be more than one manually
aligned target word. The restricted training will
allow only those paths, which are consistent with
366
the manual alignments. Therefore, the restriction
of the alignment paths reduces to restricting the
summation in EM.
4 Query Strategies for Link Selection
We propose multiple query selection strategies for
our active learning setup. The scoring criteria is
designed to select alignment links across sentence
pairs that are highly uncertain under current au-
tomatic translation models. These links are diffi-
cult to align correctly by automatic alignment and
will cause incorrect phrase pairs to be extracted in
the translation model, in turn hurting the transla-
tion quality of the SMT system. Manual correc-
tion of such links produces the maximal benefit to
the model. We would ideally like to elicit the least
number of manual corrections possible in order to
reduce the cost of data acquisition. In this section
we discuss our link selection strategies based on
the standard active learning paradigm of ?uncer-
tainty sampling?(Lewis and Catlett, 1994). We use
the automatically trained translation model ?t for
scoring each link for uncertainty, which consists of
bidirectional translation lexicon tables computed
from the bidirectional alignments.
4.1 Uncertainty Sampling: Bidirectional
Alignment Scores
The automatic Viterbi alignment produced by
the alignment models is used to obtain transla-
tion lexicons. These lexicons capture the condi-
tional distributions of source-given-target P (s/t)
and target-given-source P (t/s) probabilities at the
word level where si ? S and tj ? T . We de-
fine certainty of a link as the harmonic mean of the
bidirectional probabilities. The selection strategy
selects the least scoring links according to the for-
mula below which corresponds to links with max-
imum uncertainty:
Score(aij/s
I
1, t1
J) =
2 ? P (tj/si) ? P (si/tj)
P (tj/si) + P (si/tj)
(1)
4.2 Confidence Sampling: Posterior
Alignment probabilities
Confidence estimation for MT output is an in-
teresting area with meaningful initial exploration
(Blatz et al, 2004; Ueffing and Ney, 2007). Given
a sentence pair (sI1, t
J
1 ) and its word alignment,
we compute two confidence metrics at alignment
link level ? based on the posterior link probability
as seen in Equation 5. We select the alignment
links that the initial word aligner is least confi-
dent according to our metric and seek manual cor-
rection of the links. We use t2s to denote com-
putation using higher order (IBM4) target-given-
source models and s2t to denote source-given-
target models. Targeting some of the uncertain
parts of word alignment has already been shown
to improve translation quality in SMT (Huang,
2009). We use confidence metrics as an active
learning sampling strategy to obtain most informa-
tive links. We also experimented with other con-
fidence metrics as discussed in (Ueffing and Ney,
2007), especially the IBM 1 model score metric,
but it did not show significant improvement in this
task.
Pt2s(aij , t
J
1 /s
I
1) =
pt2s(tj/si,aij?A)
?M
i pt2s(tj/si)
(2)
Ps2t(aij , s
I
1/t
J
1 ) =
ps2t(si/tj ,aij?A)
?N
i ps2t(si/tj)
(3)
Conf1(aij/S, T ) =
2?Pt2s?Ps2t
Pt2s+Ps2t
(4)
(5)
4.3 Query by Committee
The generative alignments produced differ based
on the choice of direction of the language pair. We
useAs2t to denote alignment in the source to target
direction and At2s to denote the target to source
direction. We consider these alignments to be two
experts that have two different views of the align-
ment process. We formulate our query strategy
to select links where the agreement differs across
these two alignments. In general query by com-
mittee is a standard sampling strategy in active
learning(Freund et al, 1997), where the commit-
tee consists of any number of experts, in this case
alignments, with varying opinions. We formulate
a query by committee sampling strategy for word
alignment as shown in Equation 6. In order to
break ties, we extend this approach to select the
link with higher average frequency of occurrence
of words involved in the link.
Score(aij) = ? (6)
where ? =
?
?
?
2 aij ? As2t ?At2s
1 aij ? As2t ?At2s
0 otherwise
4.4 Margin Sampling
The strategy for confidence based sampling only
considers information about the best scoring link
367
conf(aij/S, T ). However we could benefit from
information about the second best scoring link as
well. In typical multi-class classification prob-
lems, earlier work shows success using such a
?margin based? approach (Scheffer et al, 2001),
where the difference between the probabilities as-
signed by the underlying model to the first best
and second best labels is used as a sampling cri-
teria. We adapt such a margin-based approach to
link-selection using the Conf1 scoring function
discussed in the earlier sub-section. Our margin
technique is formulated below, where a?1ij and
a?2ij are potential first best and second best scor-
ing alignment links for a word at position i in the
source sentence S with translation T . The word
with minimum margin value is chosen for human
alignment. Intuitively such a word is a possible
candidate for mis-alignment due to the inherent
confusion in its target translation.
Margin(i) =
Conf1(a?1ij/S, T ) ?Conf1(a?2ij/S, T )
5 Experiments
5.1 Data Setup
Our aim in this paper is to show that active learn-
ing can help select the most informative alignment
links that have high uncertainty according to a
given automatically trained model. We also show
that fixing such alignments leads to the maximum
reduction of error in word alignment, as measured
by AER. We compare this with a baseline where
links are selected at random for manual correction.
To run our experiments iteratively, we automate
the setup by using a parallel corpus for which the
gold-standard human alignment is already avail-
able. We select the Chinese-English language pair,
where we have access to 21,863 sentence pairs
along with complete manual alignment.
5.2 Results
We first automatically align the Cn-En corpus us-
ing GIZA++ (Och and Ney, 2003). We then
use the learned model in running our link selec-
tion algorithm over the entire corpus to determine
the most uncertain links according to each active
learning strategy. The links are then looked up in
the gold-standard human alignment database and
corrected. In case a link is not present in the
gold-standard data, we introduce a NULL align-
ment, else we propose the alignment as given in
Figure 1: Performance of active sampling strate-
gies for link selection
the gold standard. We select the partial align-
ment as a set of alignment links and provide it to
our semi-supervised word aligner. We plot per-
formance curves as number of links used in each
iteration vs. the overall reduction of AER on the
corpus.
Query by committee performs worse than ran-
dom indicating that two alignments differing in
direction are not sufficient in deciding for uncer-
tainty. We will be exploring alternative formula-
tions to this strategy. We observe that confidence
based metrics perform significantly better than the
baseline. From the scatter plots in Figure 1 1 we
can say that using our best selection strategy one
achieves similar performance to the baseline, but
at a much lower cost of elicitation assuming cost
per link is uniform.
We also perform end-to-end machine transla-
tion experiments to show that our improvement
of alignment quality leads to an improvement of
translation scores. For this experiment, we train
a standard phrase-based SMT system (Koehn et
al., 2007) over the entire parallel corpus. We tune
on the MT-Eval 2004 dataset and test on a subset
of MT-Eval 2004 dataset consisting of 631 sen-
tences. We first obtain the baseline score where
no manual alignment was used. We also train a
configuration using gold standard manual align-
ment data for the parallel corpus. This is the max-
imum translation accuracy that we can achieve by
any link selection algorithm. We now take the
best link selection criteria, which is the confidence
1X axis has number of links elicited on a log-scale
368
System BLEU METEOR
Baseline 18.82 42.70
Human Alignment 19.96 44.22
Active Selection 20% 19.34 43.25
Table 1: Alignment and Translation Quality
based method and train a system by only selecting
20% of all the links. We observe that at this point
we have reduced the AER from 37.09 AER to
26.57 AER. The translation accuracy as measured
by BLEU (Papineni et al, 2002) and METEOR
(Lavie and Agarwal, 2007) also shows improve-
ment over baseline and approaches gold standard
quality. Therefore we achieve 45% of the possible
improvement by only using 20% elicitation effort.
5.3 Batch Selection
Re-training the word alignment models after elic-
iting every individual alignment link is infeasible.
In our data set of 21,863 sentences with 588,075
links, it would be computationally intensive to re-
train after eliciting even 100 links in a batch. We
therefore sample links as a discrete batch, and train
alignment models to report performance at fixed
points. Such a batch selection is only going to be
sub-optimal as the underlying model changes with
every alignment link and therefore becomes ?stale?
for future selections. We observe that in some sce-
narios while fixing one alignment link could po-
tentially fix all the mis-alignments in a sentence
pair, our batch selection mechanism still samples
from the rest of the links in the sentence pair. We
experimented with an exponential decay function
over the number of links previously selected, in
order to discourage repeated sampling from the
same sentence pair. We performed an experiment
by selecting one of our best performing selection
strategies (conf ) and ran it in both configurations
- one with the decay parameter (batchdecay) and
one without it (batch). As seen in Figure 2, the
decay function has an effect in the initial part of
the curve where sampling is sparse but the effect
gradually fades away as we observe more samples.
In the reported results we do not use batch decay,
but an optimal estimation of ?staleness? could lead
to better gains in batch link selection using active
learning.
Figure 2: Batch decay effects on Conf-posterior
sampling strategy
6 Conclusion and Future Work
Word-Alignment is a particularly challenging
problem and has been addressed in a completely
unsupervised manner thus far (Brown et al, 1993).
While generative alignment models have been suc-
cessful, lack of sufficient data, model assump-
tions and local optimum during training are well
known problems. Semi-supervised techniques use
partial manual alignment data to address some of
these issues. We have shown that active learning
strategies can reduce the effort involved in elicit-
ing human alignment data. The reduction in ef-
fort is due to careful selection of maximally un-
certain links that provide the most benefit to the
alignment model when used in a semi-supervised
training fashion. Experiments on Chinese-English
have shown considerable improvements. In future
we wish to work with word alignments for other
language pairs like Arabic and English. We have
tested out the feasibility of obtaining human word
alignment data using Amazon Mechanical Turk
and plan to obtain more data reduce the cost of
annotation.
Acknowledgments
This research was partially supported by DARPA
under grant NBCHC080097. Any opinions, find-
ings, and conclusions expressed in this paper are
those of the authors and do not necessarily reflect
the views of the DARPA. The first author would
like to thank Qin Gao for the semi-supervised
word alignment software and help with running
experiments.
369
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2004. Confidence estimation for machine
translation. In Proceedings of Coling 2004, pages 315?
321, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, David Talbot, and Miles Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In ACL 2004, page
175, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic con-
straints for word alignment through discriminative train-
ing. In Proceedings of the COLING/ACL on Main con-
ference poster sessions, pages 105?112, Morristown, NJ,
USA.
Pinar Donmez and Jaime G. Carbonell. 2008. Optimizing es-
timated loss reduction for active sampling in rank learning.
In ICML ?08: Proceedings of the 25th international con-
ference on Machine learning, pages 248?255, New York,
NY, USA. ACM.
Alexander Fraser and Daniel Marcu. 2006. Semi-supervised
training for statistical word alignment. In ACL-44: Pro-
ceedings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, pages 769?
776, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Alexander Fraser and Daniel Marcu. 2007a. Getting the
structure right for word alignment: LEAF. In Proceedings
of the 2007 Joint Conference on EMNLP-CoNLL, pages
51?60.
Alexander Fraser and Daniel Marcu. 2007b. Measuring word
alignment quality for statistical machine translation. Com-
put. Linguist., 33(3):293?303.
Yoav Freund, Sebastian H. Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query by com-
mittee algorithm. Machine. Learning., 28(2-3):133?168.
Qin Gao and Stephan Vogel. 2008. Parallel implementa-
tions of word alignment tool. In Software Engineering,
Testing, and Quality Assurance for Natural Language Pro-
cessing, pages 49?57, Columbus, Ohio, June. Association
for Computational Linguistics.
Gholamreza Haffari and Anoop Sarkar. 2009. Active learn-
ing for multilingual statistical machine translation. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the AFNLP,
pages 181?189, Suntec, Singapore, August. Association
for Computational Linguistics.
Fei Huang. 2009. Confidence measure for word alignment.
In Proceedings of the Joint ACL and IJCNLP, pages 932?
940, Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of the
HLT/NAACL, Edomonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL Demon-
stration Session.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an auto-
matic metric for mt evaluation with high levels of corre-
lation with human judgments. In WMT 2007, pages 228?
231, Morristown, NJ, USA.
David D. Lewis and Jason Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In In Proceed-
ings of the Eleventh International Conference on Machine
Learning, pages 148?156. Morgan Kaufmann.
Hieu T. Nguyen and Arnold Smeulders. 2004. Active learn-
ing using pre-clustering. In ICML.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, pages 19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In ACL 2002, pages 311?318, Mor-
ristown, NJ, USA.
Tobias Scheffer, Christian Decomain, and Stefan Wrobel.
2001. Active hidden markov models for information ex-
traction. In IDA ?01: Proceedings of the 4th Interna-
tional Conference on Advances in Intelligent Data Anal-
ysis, pages 309?318, London, UK. Springer-Verlag.
Simon Tong and Daphne Koller. 2002. Support vector ma-
chine active learning with applications to text classifica-
tion. Journal of Machine Learning, pages 45?66.
Nicola Ueffing and Hermann Ney. 2007. Word-level con-
fidence estimation for machine translation. Comput. Lin-
guist., 33(1):9?40.
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2006. Boost-
ing statistical word alignment using labeled and unlabeled
data. In Proceedings of the COLING/ACL on Main con-
ference poster sessions, pages 913?920, Morristown, NJ,
USA. Association for Computational Linguistics.
370
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1?11,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Word-Class Approach to Labeling PSCFG Rules for Machine Translation
Andreas Zollmann and Stephan Vogel
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{zollmann,vogel+}@cs.cmu.edu
Abstract
In this work we propose methods to label
probabilistic synchronous context-free gram-
mar (PSCFG) rules using only word tags,
generated by either part-of-speech analysis
or unsupervised word class induction. The
proposals range from simple tag-combination
schemes to a phrase clustering model that can
incorporate an arbitrary number of features.
Our models improve translation quality over
the single generic label approach of Chiang
(2005) and perform on par with the syntacti-
cally motivated approach from Zollmann and
Venugopal (2006) on the NIST large Chinese-
to-English translation task. These results per-
sist when using automatically learned word
tags, suggesting broad applicability of our
technique across diverse language pairs for
which syntactic resources are not available.
1 Introduction
The Probabilistic Synchronous Context Free Gram-
mar (PSCFG) formalism suggests an intuitive ap-
proach to model the long-distance and lexically sen-
sitive reordering phenomena that often occur across
language pairs considered for statistical machine
translation. As in monolingual parsing, nonterminal
symbols in translation rules are used to generalize
beyond purely lexical operations. Labels on these
nonterminal symbols are often used to enforce syn-
tactic constraints in the generation of bilingual sen-
tences and imply conditional independence assump-
tions in the translation model. Several techniques
have been recently proposed to automatically iden-
tify and estimate parameters for PSCFGs (or related
synchronous grammars) from parallel corpora (Gal-
ley et al, 2004; Chiang, 2005; Zollmann and Venu-
gopal, 2006; Liu et al, 2006; Marcu et al, 2006).
While all of these techniques rely on word-
alignments to suggest lexical relationships, they dif-
fer in the way in which they assign labels to non-
terminal symbols of PSCFG rules. Chiang (2005)
describes a procedure to extract PSCFG rules from
word-aligned (Brown et al, 1993) corpora, where
all nonterminals share the same generic label X . In
Galley et al (2004) and Marcu et al (2006), tar-
get language parse trees are used to identify rules
and label their nonterminal symbols, while Liu et al
(2006) use source language parse trees instead. Zoll-
mann and Venugopal (2006) directly extend the rule
extraction procedure from Chiang (2005) to heuristi-
cally label any phrase pair based on target language
parse trees. Label-based approaches have resulted
in improvements in translation quality over the sin-
gleX label approach (Zollmann et al, 2008; Mi and
Huang, 2008); however, all the works cited here rely
on stochastic parsers that have been trained on man-
ually created syntactic treebanks. These treebanks
are difficult and expensive to produce and exist for a
limited set of languages only.
In this work, we propose a labeling approach that
is based merely on part-of-speech analysis of the
source or target language (or even both). To-
wards the ultimate goal of building end-to-end ma-
chine translation systems without any human anno-
tations, we also experiment with automatically in-
ferred word classes using distributional clustering
(Kneser and Ney, 1993). Since the number of classes
is a parameter of the clustering method and the re-
sulting nonterminal size of our grammar is a func-
tion of the number of word classes, the PSCFG
grammar complexity can be adjusted to the specific
translation task at hand.
Finally, we introduce a more flexible labeling ap-
proach based on K-means clustering, which allows
1
the incorporation of an arbitrary number of word-
class based features, including phrasal contexts, can
make use of multiple tagging schemes, and also al-
lows non-class features such as phrase sizes.
2 PSCFG-based translation
In this work we experiment with PSCFGs that have
been automatically learned from word-aligned par-
allel corpora. PSCFGs are defined by a source ter-
minal set (source vocabulary) TS , a target terminal
set (target vocabulary) TT , a shared nonterminal set
N and rules of the form: A? ??, ?,w? where
? A ? N is a labeled nonterminal referred to as the
left-hand-side of the rule,
? ? ? (N ? TS)? is the source side of the rule,
? ? ? (N ? TT )? is the target side of the rule,
? w ? [0,?) is a non-negative real-valued weight
assigned to the rule; in our model,w is the product
of features ?i raised to the power of weight ?i.
Chiang (2005) learns a single-nonterminal PSCFG
from a bilingual corpus by first identifying initial
phrase pairs using the technique from Koehn et al
(2003), and then performing a generalization opera-
tion to generate phrase pairs with gaps, which can be
viewed as PSCFG rules with generic ?X? nontermi-
nal left-hand-sides and substitution sites. Bilingual
features ?i that judge the quality of each rule are es-
timated based on rule extraction frequency counts.
3 Hard rule labeling from word classes
We now describe a simple method of inducing a
multi-nonterminal PSCFG from a parallel corpus
with word-tagged target side sentences. The same
procedure can straightforwardly be applied to a cor-
pus with tagged source side sentences. We use the
simple term ?tag? to stand for any kind of word-level
analysis?a syntactic, statistical, or other means of
grouping word types or tokens into classes, possibly
based on their position and context in the sentence,
POS tagging being the most obvious example.
As in Chiang?s hierarchical system, we rely on
an external phrase-extraction procedure such as the
one of Koehn et al (2003) to provide us with a set
of phrase pairs for each sentence pair in the train-
ing corpus, annotated with their respective start and
end positions in the source and target sentences.
Let f = f1 ? ? ? fm be the current source sentence,
e = e1 ? ? ? en the current target sentence, and t =
t1 ? ? ? tn its corresponding target tag sequence. We
convert each extracted phrase pair, represented by
its source span ?i, j? and target span ?k, `?, into an
initial rule
tk-t` ? fi ? ? ? fj | ek ? ? ? e`
by assigning it a nonterminal ?tk-t`? constructed by
combining the tag of the target phrase?s left-most
word with the tag of its right-most word.
The creation of complex rules based on all initial
rules obtained from the current sentence now pro-
ceeds just as in Chiang?s model.
Consider the target-tagged example sentence pair:
Ich habe ihn gesehen | I/PRP saw/VBD him/PRP
Then (depending on the extracted phrase pairs), the
resulting initial rules could be:
1: PRP-PRP? Ich | I
2: PRP-PRP? ihn | him
3: VBD-VBD? gesehen | saw
4: VBD-PRP? habe ihn gesehen | saw him
5: PRP-PRP? Ich habe ihn gesehen | I saw him
Now, by abstracting-out initial rule 2 from initial
rule 4, we obtain the complex rule:
VBD-PRP? habe PRP-PRP1 gesehen | saw PRP-PRP1
Intuitively, the labeling of initial rules with tags
marking the boundary of their target sides results in
complex rules whose nonterminal occurrences im-
pose weak syntactic constraints on the rules eligi-
ble for substitution in a PSCFG derivation: The left
and right boundary word tags of the inserted rule?s
target side have to match the respective boundary
word tags of the phrase pair that was replaced by
a nonterminal when the complex rule was created
from a training sentence pair. Since consecutive
words within a rule stem from consecutive words in
the training corpus and thus are already consistent,
the boundary word tags are more informative than
tags of words between the boundaries for the task
of combining different rules in a derivation, and are
therefore a more appropriate choice for the creation
of grammar labels than tags of inside words.
Accounting for phrase size A drawback of the
current approach is that a single-word rule such as
PRP-PRP? Ich | I
2
can have the same left-hand-side nonterminal as a
long rule with identical left and right boundary tags,
such as (when using target-side tags):
PRP-PRP? Ich habe ihn gesehen | I saw him
We therefore introduce a means of distinguishing
between one-word, two-word, and multiple-word
phrases as follows: Each one-word phrase with tag
T simply receives the label T , instead of T -T . Two-
word phrases with tag sequence T1T2 are labeled
T1-T2 as before. Phrases of length greater two with
tag sequence T1 ? ? ?Tn are labeled T1..Tn to denote
that tags were omitted from the phrase?s tag se-
quence. The resulting number of grammar nonter-
minals based on a tag vocabulary of size t is thus
given by 2t2 + t.
An alternative way of accounting for phrase size
is presented by Chiang et al (2008), who intro-
duce structural distortion features into a hierarchi-
cal phrase-based model, aimed at modeling nonter-
minal reordering given source span length. Our
approach instead uses distinct grammar rules and
labels to discriminate phrase size, with the advan-
tage of enabling all translation models to estimate
distinct weights for distinct size classes and avoid-
ing the need of additional models in the log-linear
framework; however, the increase in the number of
labels and thus grammar rules decreases the relia-
bility of estimated models for rare events due to in-
creased data sparseness.
Extension to a bilingually tagged corpus While
the availability of syntactic annotations for both
source and target language is unlikely in most trans-
lation scenarios, some form of word tags, be it part-
of-speech tags or learned word clusters (cf. Sec-
tion 3) might be available on both sides. In this case,
our grammar extraction procedure can be easily ex-
tended to impose both source and target constraints
on the eligible substitutions simultaneously.
Let Nf be the nonterminal label that would be
assigned to a given initial rule when utilizing the
source-side tag sequence, and Ne the assigned la-
bel according to the target-side tag sequence. Then
our bilingual tag-based model assigns ?Nf + Ne?
to the initial rule. The extraction of complex rules
proceeds as before. The number of nonterminals
in this model, based on a source tag vocabulary of
size s and a target tag vocabulary of size t, is thus
given by s2t2 for the regular labeling method and
(2s2 + s)(2t2 + t) when accounting for phrase size.
Consider again our example sentence pair (now
also annotated with source-side part-of-speech tags):
Ich/PRP habe/AUX ihn/PRP gesehen/VBN
I/PRP saw/VBD him/PRP
Given the same phrase extraction method as before,
the resulting initial rules for our bilingual model,
when also accounting for phrase size, are as follows:
1: PRP+PRP? Ich | I
2: PRP+PRP? ihn | him
3: VBN+VBD? gesehen | saw
4: AUX..VBN+VBD-PRP ? habe ihn
gesehen | saw him
5: PRP..VBN+PRP..PRP ? Ich habe ihn
gesehen | I saw him
Abstracting-out rule 2 from rule 4, for instance,
leads to the complex rule:
AUX..VBN+VBD-PRP ? habe PRP+PRP1
gesehen | saw PRP+PRP1
Unsupervised word class assignment by cluster-
ing As an alternative to POS tags, we experiment
with unsupervised word clustering methods based
on the exchange algorithm (Kneser and Ney, 1993).
Its objective function is maximizing the likelihood
n?
i=1
P (wi|w1, . . . , wi?1)
of the training data w = w1, . . . , wn given a par-
tially class-based bigram model of the form
P (wi|w1, . . . , wi?1) ? p(c(wi)|wi?1) ?p(wi|c(wi))
where c : V ? {1, . . . , N} maps a word (type, not
token) w to its class c(w), V is the vocabulary, and
N the fixed number of classes, which has to be cho-
sen a priori. We use the publicly available imple-
mentation MKCLS (Och, 1999) to train this model.
As training data we use the respective side of the
parallel training data for the translation system.
We also experiment with the extension of this
model by Clark (2003), who incorporated morpho-
logical information by imposing a Bayesian prior
on the class mapping c, based on N individual dis-
tributions over strings, one for each word class.
Each such distribution is a character-based hidden
Markov model, thus encouraging the grouping of
morphologically similar words into the same class.
3
4 Clustering phrase pairs directly using
the K-means algorithm
Even though we have only made use of the first and
last words? classes in the labeling methods described
so far, the number of resulting grammar nontermi-
nals quickly explodes. Using a scheme based on
source and target phrases with accounting for phrase
size, with 36 word classes (the size of the Penn En-
glish POS tag set) for both languages, yields a gram-
mar with (36+2?362)2 = 6.9m nonterminal labels.
Quite plausibly, phrase labeling should be in-
formed by more than just the classes of the first and
last words of the phrase. Taking phrase context into
account, for example, can aid the learning of syn-
tactic properties: a phrase beginning with a deter-
miner and ending with a noun, with a verb as right
context, is more likely to be a noun phrase than the
same phrase with another noun as right context. In
the current scheme, there is no way of distinguish-
ing between these two cases. Similarly, it is con-
ceivable that using non-boundary words inside the
phrase might aid the labeling process.
When relying on unsupervised learning of the
word classes, we are forced to chose a fixed num-
ber of classes. A smaller number of word clusters
will result in smaller number of grammar nonter-
minals, and thus more reliable feature estimation,
while a larger number has the potential to discover
more subtle syntactic properties. Using multiple
word clusterings simultaneously, each based on a
different number of classes, could turn this global,
hard trade-off into a local, soft one, informed by the
number of phrase pair instances available for a given
granularity.
Lastly, our method of accounting for phrase size
is somewhat displeasing: While there is a hard par-
titioning of one-word and two-word phrases, no dis-
tinction is made between phrases of length greater
than two. Marking phrase sizes greater than two
explicitly by length, however, would create many
sparse, low-frequency rules, and one of the strengths
of PSCFG-based translation is the ability to sub-
stitute flexible-length spans into nonterminals of a
derivation. A partitioning where phrase size is in-
stead merely a feature informing the labeling pro-
cess seems more desirable.
We thus propose to represent each phrase pair in-
stance (including its bilingual one-word contexts) as
feature vectors, i.e., points of a vector space. We
then use these data points to partition the space into
clusters, and subsequently assign each phrase pair
instance the cluster of its corresponding feature vec-
tor as label.
The feature mapping Consider the phrase pair in-
stance
(f0)f1 ? ? ? fm(fm+1) | (e0)e1 ? ? ? en(en+1)
(where f0, fm+1, e0, en+1 are the left and right,
source and target side contexts, respectively). We
begin with the case of only a single, target-side
word class scheme (either a tagger or an unsuper-
vised word clustering/POS induction method). Let
C = {c1, . . . , cN} be its set of word classes. Fur-
ther, let c0 be a short-hand for the result of looking
up the class of a word that is out of bounds (e.g., the
left context of the first word of a sentence, or the sec-
ond word of a one-word phrase). We now map our
phrase pair instance to the real-valued vector (where
1[P ] is the indicator function defined as 1 if property
P is true, and 0 otherwise):
?
1[e1=c0], . . . ,1[e1=cN ],1[en=c0], . . . ,1[en=cN ],
?sec1[e2=c0], . . . , ?sec1[e2=cN ],
?sec1[en?1=c0], . . . , ?sec1[en?1=cN ],
?ins
?n
i=1 1[ei=c0]
n
, . . . ,
?ins
?n
i=1 1[ei=cN ]
n
,
?cntxt1[e0=c0], . . . , ?cntxt1[e0=cN ],
?cntxt1[en+1=c0], . . . , ?cntxt1[en+1=cN ],
?phrsize
?
N + 1 log10(n)
?
The ? parameters determine the influence of the dif-
ferent types of information. The elements in the first
line represent the phrase boundary word classes, the
next two lines the classes of the second and penul-
timate word, followed by a line representing the ac-
cumulated contents of the whole phrase, followed by
two lines pertaining to the context word classes. The
final element of the vector is proportional to the log-
arithm of the phrase length.1 We chose the logarithm
assuming that length deviation of syntactic phrasal
units is not constant, but proportional to the average
length. Thus, all other features being equal, the dis-
tance between a two-word and a four-word phrase is
1The
?
N + 1 factor serves to make the feature?s influence in-
dependent of the number of word classes by yielding the same
distance (under L2) as N + 1 identical copies of the feature.
4
the same as the distance between a four-word and an
eight-word phrase.
We will mainly use the Euclidean (L2) distance to
compare points for clustering purposes. Our feature
space is thus the Euclidean vector space R7N+8.
To additionally make use of source-side word
classes, we append elements analogous to the ones
above to the vector, all further multiplied by a pa-
rameter ?src that allows trading off the relevance
of source-side and target-side information. In the
same fashion, we can incorporate multiple tagging
schemes (e.g., word clusterings of different gran-
ularities) into the same feature vector. As finer-
grained schemes have more elements in the fea-
ture vector than coarser-grained ones, and thus ex-
ert more influence, we set the ? parameter for each
scheme to 1/N (where N is the number of word
classes of the scheme).
The K-means algorithm To create the clusters,
we chose the K-means algorithm (Steinhaus, 1956;
MacQueen, 1967) for both its computational effi-
ciency and ease of implementation and paralleliza-
tion. Given an initial mapping from the data points
to K clusters, the procedure alternates between (i)
computing the centroid of each cluster and (ii) re-
allocating each data point to the closest cluster cen-
troid, until convergence.
We implemented two commonly used initializa-
tion methods: Forgy and Random Partition. The
Forgy method randomly chooses K observations
from the data set and uses these as the initial means.
The Random Partition method first randomly as-
signs a cluster to each observation and then proceeds
straight to step (ii). Forgy tends to spread the ini-
tial means out, while Random Partition places all
of them close to the center of the data set. As the
resulting clusters looked similar, and Random Parti-
tion sometimes led to a high rate of empty clusters,
we settled for Forgy.
5 Experiments
We evaluate our approach by comparing translation
quality, as evaluated by the IBM-BLEU (Papineni
et al, 2002) metric on the NIST Chinese-to-English
translation task using MT04 as development set to
train the model parameters ?, and MT05, MT06 and
MT08 as test sets. Even though a key advantage
of our method is its applicability to resource-poor
languages, we used a language pair for which lin-
guistic resources are available in order to determine
how close translation performance can get to a fully
syntax-based system. Accordingly, we use Chiang?s
hierarchical phrase based translation model (Chiang,
2007) as a base line, and the syntax-augmented MT
model (Zollmann and Venugopal, 2006) as a ?target
line?, a model that would not be applicable for lan-
guage pairs without linguistic resources.
We perform PSCFG rule extraction and decoding
using the open-source ?SAMT? system (Venugopal
and Zollmann, 2009), using the provided implemen-
tations for the hierarchical and syntax-augmented
grammars. Apart from the language model, the lex-
ical, phrasal, and (for the syntax grammar) label-
conditioned features, and the rule, target word,
and glue operation counters, Venugopal and Zoll-
mann (2009) also provide both the hierarchical and
syntax-augmented grammars with a rareness penalty
1/ cnt(r), where cnt(r) is the occurrence count of
rule r in the training corpus, allowing the system to
learn penalization of low-frequency rules, as well as
three indicator features firing if the rule has one, two
unswapped, and two swapped nonterminal pairs, re-
spectively.2 Further, to mitigate badly estimated
PSCFG derivations based on low-frequency rules of
the much sparser syntax model, the syntax grammar
also contains the hierarchical grammar as a back-
bone (cf. Zollmann and Vogel (2010) for details and
empirical analysis).
We implemented our rule labeling approach
within the SAMT rule extraction pipeline, resulting
in comparable features across all systems. For all
systems, we use the bottom-up chart parsing decoder
implemented in the SAMT toolkit with a reorder-
ing limit of 15 source words, and correspondingly
extract rules from initial phrase pairs of maximum
source length 15. All rules have at most two non-
terminal symbols, which must be non-consecutive
on the source side, and rules must contain at least
one source-side terminal symbol. The beam set-
tings for the hierarchical system are 600 items per
?X? (generic rule) cell, and 600 per ?S? (glue) cell.3
Due to memory limitations, the multi-nonterminal
grammars have to be pruned more harshly: We al-
2Penalization or reward of purely-lexical rules can be indirectly
learned by trading off these features with the rule counter fea-
ture.
3For comparison, Chiang (2007) uses 30 and 15, respectively,
and further prunes items that deviate too much in score from
the best item. He extracts initial phrases of maximum length
10.
5
low 100 ?S? items, and a total of 500 non-?S? items,
but maximally 40 items per nonterminal. For all sys-
tems, we further discard non-initial rules occurring
only once.4 For the multi-nonterminal systems, we
generally further discard all non-generic non-initial
rules occurring less than 6 times, but we additionally
give results for a ?slow? version of the Syntax target-
line system and our best word class based systems,
where only single-occurrences were removed.
For parameter tuning, we use the L0-regularized
minimum-error-rate training tool provided by the
SAMT toolkit. Each system is trained separately to
adapt the parameters to its specific properties (size
of nonterminal set, grammar complexity, features
sparseness, reliance on the language model, etc.).
The parallel training data comprises of 9.6M
sentence pairs (206M Chinese and 228M English
words). The source and target language parses for
the syntax-augmented grammar, as well as the POS
tags for our POS-based grammars were generated by
the Stanford parser (Klein and Manning, 2003).
The results are given in Table 1. Results for the
Syntax system are consistent with previous results
(Zollmann et al, 2008), indicating improvements
over the hierarchical system. Our approach, using
target POS tags (?POS-tgt (no phr. s.)?), outper-
forms the hierarchical system on all three tests sets,
and gains further improvements when accounting
for phrase size (?POS-tgt?). The latter approach is
roughly on par with the corresponding Syntax sys-
tem, slightly outperforming it on average, but not
consistently across all test sets. The same is true for
the ?slow? version (?POS-tgt-slow?).
The model based on bilingually tagged training
instances (?POS-src&tgt?) does not gain further im-
provements over the merely target-based one, but
actually performs worse. We assume this is due to
the huge number of nonterminals of ?POS-src&tgt?
((2 ? 332 + 33)(2 ? 362 + 36) = 5.8M in princi-
ple) compared to ?POS-tgt? (2 ? 362 + 36 = 2628),
increasing the sparseness of the grammar and thus
leading to less reliable statistical estimates.
We also experimented with a source-tag based
model (?POS-src?). In line with previous findings
for syntax-augmented grammars (Zollmann and Vo-
gel, 2010), the source-side-based grammar does not
reach the translation quality of its target-based coun-
terpart; however, the model still outperforms the hi-
4As shown in Zollmann et al (2008), the impact of these rules
on translation quality is negligible.
erarchical system on all test sets. Further, decod-
ing is much faster than for ?POS-ext-tgt? and even
slightly faster than ?Hierarchical?. This is due to
the fact that for the source-tag based approach, a
given chart cell in the CYK decoder, represented by
a start and end position in the source sentence, al-
most uniquely determines the nonterminal any hy-
pothesis in this cell can have: Disregarding part-
of-speech tag ambiguity and phrase size accounting,
that nonterminal will be the composition of the tags
of the start and end source words spanned by that
cell. At the same time, this demonstrates that there
is hence less of a role for the nonterminal labels to
resolve translational ambiguity in the source based
model than in the target based model.
Performance of the word-clustering based mod-
els To empirically validate the unsupervised clus-
tering approaches, we first need to decide how to de-
termine the number of word classes, N . A straight-
forward approach is to run experiments and report
test set results for many different N . While this
would allow us to reliably conclude the optimal
number N , a comparison of that best-performing
clustering method to the hierarchical, syntax, and
POS systems would be tainted by the fact that N
was effectively tuned on the test sets. We there-
fore chooseN merely based on development set per-
formance. Unfortunately, variance in development
set BLEU scores tends to be higher than test set
scores, despite of SAMT MERT?s inbuilt algorithms
to overcome local optima, such as random restarts
and zeroing-out. We have noticed that using an L0-
penalized BLEU score5 as MERT?s objective on the
merged n-best lists over all iterations is more stable
and will therefore use this score to determine N .
Figure 1 (left) shows the performance of the
distributional clustering model (?Clust?) and its
morphology-sensitive extension (?Clust-morph?) ac-
cording to this score for varying values of N =
1, . . . , 36 (the number Penn treebank POS tags, used
for the ?POS? models, is 36).6 For ?Clust?, we see a
comfortably wide plateau of nearly-identical scores
from N = 7, . . . , 15. Scores for ?Clust-morph? are
lower throughout, and peak at N = 7.
Looking back at Table 1, we now compare the
clustering models chosen by the procedure above?
5Given by: BLEU?? ? |{i ? {1, . . . ,K}|?i 6= 0}|, where
?1, . . . , ?K are the feature weights and the constant ? (which
we set to 0.00001) is the regularization penalty.
6All these models account for phrase size.
6
Dev (MT04) MT05 MT06 MT08 TestAvg Time
Hierarchical 38.63 36.51 33.26 25.77 31.85 14.3
Syntax 39.39 37.09 34.01 26.53 32.54 18.1
Syntax-slow 39.69 37.56 34.66 26.93 33.05 34.6
POS-tgt (no phr. s.) 39.31 37.29 33.79 26.13 32.40 27.7
POS-tgt 39.14 37.29 33.97 26.77 32.68 19.2
POS-src 38.74 36.75 33.85 26.76 32.45 12.2
POS-src&tgt 38.78 36.71 33.65 26.52 32.29 18.8
POS-tgt-slow 39.86 37.78 34.37 27.14 33.10 44.6
Clust-7-tgt 39.24 36.74 34.00 26.93 32.56 24.3
Clust-7-morph-tgt 39.08 36.57 33.81 26.40 32.26 23.6
Clust-7-src 38.68 36.17 33.23 26.55 31.98 11.1
Clust-7-src&tgt 38.71 36.49 33.65 26.33 32.16 15.8
Clust-7-tgt-slow 39.48 37.70 34.31 27.24 33.08 45.2
kmeans-POS-src&tgt 39.11 37.23 33.92 26.80 32.65 18.5
kmeans-POS-src&tgt-L1 39.33 36.92 33.81 26.59 32.44 17.6
kmeans-POS-src&tgt-cosine 39.15 37.07 33.98 26.68 32.58 17.7
kmeans-POS-src&tgt (?ins = .5) 39.07 36.88 33.71 26.26 32.28 16.5
kmeans-Clust-7-src&tgt 39.19 36.96 34.26 26.97 32.73 19.3
kmeans-Clust-7..36-src&tgt 39.09 36.93 34.24 26.92 32.70 17.3
kmeans-POS-src&tgt-slow 39.28 37.16 34.38 27.11 32.88 36.3
kmeans-Clust-7..36-s&t-slow 39.18 37.12 34.13 27.35 32.87 34.3
Table 1: Translation quality in % case-insensitive IBM-BLEU (i.e., brevity penalty based on closest reference length)
for Chinese-English NIST-large translation tasks, comparing baseline Hierarchical and Syntax systems with POS and
clustering based approaches proposed in this work. ?TestAvg? shows the average score over the three test sets. ?Time?
is the average decoding time per sentence in seconds on one CPU.
resulting in N = 7 for the morphology-unaware
model (?Clust-7-tgt?) as well as the morphology-
aware model (?Clust-7-morph-tgt?)?to the other
systems. ?Clust-7-tgt? improves over the hierarchi-
cal base line on all three test sets and is on par
with the corresponding Syntax and POS target lines.
The same holds for the ?Clust-7-tgt-slow? version.
We also experimented with a model variant based
on seven source and seven target language clusters
(?Clust-7-src&tgt?) and a source-only labeled model
(?Clust-7-src?)?both performing worse.
Surprisingly, the morphology-sensitive cluster-
ing model (?Clust-7-morph-tgt?), while still improv-
ing over the hierarchical system, performs worse
than the morphology-unaware model. An in-
spection of the trained word clusters showed that
the model, while far superior to the morphology-
unaware model in e.g. mapping all numbers to
the same class, is overzealous in discovering mor-
phological regularities (such as the ?-ed? suffix) to
partition functionally only slightly dissimilar words
(such present-tense and past-tense verbs) into dif-
ferent classes. While these subtle distinctions make
for good partitionings when the number of clusters
is large, they appear to lead to inferior results for
our task that relies on coarse-grained partitionings
of the vocabulary. Note that there are no ?src? or
?src&tgt? systems for ?Clust-morph?, as Chinese, be-
ing a monosyllabic writing system, does not lend it-
self to morphology-sensitive clustering.
K-means clustering based models To establish
suitable values for the ? parameters and investigate
the impact of the number of clusters, we looked at
the development performance over various param-
eter combinations for a K-means model based on
source and/or target part-of-speech tags.7 As can
be seen from Figure 1 (right), our method reaches
its peak performance at around 50 clusters and then
levels off slightly. Encouragingly, in contrast to
the hard labeling procedure, K-means actually im-
proves when adding source-side information. The
optimal ratio of weighting source and target classes
is 0.5:1, corresponding to ?src = .5. Incorporat-
ing context information also helps, and does best for
?cntxt = 0.25, i.e. when giving contexts 1/4 the in-
fluence of the phrase boundary words.
7We set ?sec = .25, ?ins = 0, and ?phrsize = .5 throughout.
7
Figure 1: Left: Performance of the distributional clustering model ?Clust? and its morphology-sensitive extension
?Clust-morph? according to L0-penalized development set BLEU score for varying numbers N of word classes. For
each data point N , its corresponding n.o. nonterminals of the induced grammar is stated in parentheses.
Right: Dev. set performance of K-means for various n.o. labels and values of ?src and ?cntxt.
Entry ?kmeans-POS-src&tgt? in Table 1 shows
the test set results for the development-set best K-
means configuration (i.e., ?src = .5, ?cntxt = 0.25,
and using 500 clusters). While beating the hier-
archical baseline, it is only minimally better than
the much simpler target-based hard labeling method
?POS-tgt?. We also tried K-means variants in which
the Euclidean distance metric is replaced by the
city block distance L1 and the cosine dissimilarity,
respectively, with slightly worse outcomes. Con-
figuration ?kmeans-POS-src&tgt (?ins = .5)? in-
vestigates the incorporation of non-boundary word
tags inside the phrase. Unfortunately, these features
appear to deteriorate performance, presumably be-
cause given a fixed number of clusters, accounting
for contents inside the phrase comes at the cost of
neglect of boundary words, which are more relevant
to producing correctly reordered translations.
The two completely unsupervised systems
?kmeans-Clust-7-src&tgt? (based on 7-class
MKCLS distributional word clustering) and
?kmeans-Clust-7..36-src&tgt? (using six different
word clustering models simultaneously: all the
MKCLS models from Figure 1 (left) except for the
two-, three- and five-class models) have the best
results, outperforming the other K-means models as
well as ?Syntax? and ?POS-tgt? on average, but not
on all test sets.
Lastly, we give results for ?slow? K-means config-
urations (?kmeans-POS-src&tgt-slow? and ?kmeans-
Clust-7..36-s&t-slow?). Unfortunately (or fortu-
nately, from a pragmatic viewpoint), the models are
outperformed by the much simpler ?POS-tgt-slow?
and ?Clust-7-tgt-slow? models.
6 Related work
Hassan et al (2007) improve the statistical phrase-
based MT model by injecting supertags, lexical in-
formation such as the POS tag of the word and its
subcategorization information, into the phrase table,
resulting in generalized phrases with placeholders in
them. The supertags are also injected into the lan-
guage model. Our approach also generates phrase
labels and placeholders based on word tags (albeit
in a different manner and without the use of subcat-
egorization information), but produces PSCFG rules
for use in a parsing-based decoding system.
Unsupervised synchronous grammar induction,
apart from the contribution of Chiang (2005) dis-
cussed earlier, has been proposed by Wu (1997) for
inversion transduction grammars, but as Chiang?s
model only uses a single generic nonterminal la-
bel. Blunsom et al (2009) present a nonparamet-
ric PSCFG translation model that directly induces
a grammar from parallel sentences without the use
of or constraints from a word-alignment model, and
8
Cohn and Blunsom (2009) achieve the same for
tree-to-string grammars, with encouraging results
on small data. Our more humble approach treats
the training sentences? word alignments and phrase
pairs, obtained from external modules, as ground
truth and employs a straight-forward generalization
of Chiang?s popular rule extraction approach to la-
beled phrase pairs, resulting in a PSCFG with mul-
tiple nonterminal labels.
Our phrase pair clustering approach is similar in
spirit to the work of Lin and Wu (2009), who use K-
means to cluster (monolingual) phrases and use the
resulting clusters as features in discriminative clas-
sifiers for a named-entity-recognition and a query
classification task. Phrases are represented in terms
of their contexts, which can be more than one word
long; words within the phrase are not considered.
Further, each context contributes one dimension per
vocabulary word (not per word class as in our ap-
proach) to the feature space, allowing for the dis-
covery of subtle semantic similarities in the phrases,
but at much greater computational expense. Another
distinction is that Lin and Wu (2009) work with
phrase types instead of phrase instances, obtaining
a phrase type?s contexts by averaging the contexts
of all its phrase instances.
Nagata et al (2006) present a reordering model
for machine translation, and make use of clustered
phrase pairs to cope with data sparseness in the
model. They achieve the clustering by reducing
phrases to their head words and then applying the
MKCLS tool to these pseudo-words.
Kuhn et al (2010) cluster the phrase pairs of
an SMT phrase table based on their co-occurrence
counts and edit distances in order to arrive at seman-
tically similar phrases for the purpose of phrase table
smoothing. The clustering proceeds in a bottom-up
fashion, gradually merging similar phrases while al-
ternating back and forth between the two languages.
7 Conclusion and discussion
In this work we proposed methods of labeling phrase
pairs to create automatically learned PSCFG rules
for machine translation. Crucially, our methods only
rely on ?shallow? lexical tags, either generated by
POS taggers or by automatic clustering of words into
classes. Evaluated on a Chinese-to-English transla-
tion task, our approach improves translation qual-
ity over a popular PSCFG baseline?the hierarchi-
cal model of Chiang (2005) ?and performs on par
with the model of Zollmann and Venugopal (2006),
using heuristically generated labels from parse trees.
Using automatically obtained word clusters instead
of POS tags yields essentially the same results, thus
making our methods applicable to all languages
pairs with parallel corpora, whether syntactic re-
sources are available for them or not.
We also propose a more flexible way of obtaining
the phrase labels from word classes using K-means
clustering. While currently the simple hard-labeling
methods perform just as well, we hope that the ease
of incorporating new features into the K-means la-
beling method will spur interesting future research.
When considering the constraints and indepen-
dence relationships implied by each labeling ap-
proach, we can distinguish between approaches that
label rules differently within the context of the sen-
tence that they were extracted from, and those that
do not. The Syntax system from Zollmann and
Venugopal (2006) is at one end of this extreme. A
given target span might be labeled differently de-
pending on the syntactic analysis of the sentence
that it is a part of. On the other extreme, the clus-
tering based approach labels phrases based on the
contained words alone.8 The POS grammar repre-
sents an intermediate point on this spectrum, since
POS tags can change based on surrounding words in
the sentence; and the position of the K-means model
depends on the influence of the phrase contexts on
the clustering process. Context insensitive labeling
has the advantage that there are less alternative left-
hand-side labels for initial rules, producing gram-
mars with less rules, whose weights can be more
accurately estimated. This could explain the strong
performance of the word-clustering based labeling
approach.
All source code underlying this work is available
under the GNU Lesser General Public License as
part of the Hadoop-based ?SAMT? system at:
www.cs.cmu.edu/?zollmann/samt
Acknowledgments
We thank Jakob Uszkoreit and Ashish Venugopal for
helpful comments and suggestions and Yahoo! for
the access to the M45 supercomputing cluster.
8Note, however, that the creation of clusters itself did take the
context of the clustered words into account.
9
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of ACL,
Singapore, August.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2).
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, Honolulu, Hawaii, October.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
David Chiang. 2007. Hierarchical phrase based transla-
tion. Computational Linguistics, 33(2).
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the European chapter of the
Association for Computational Linguistics (EACL),
pages 59?66.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Singapore.
Michael Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics Conference
(HLT/NAACL).
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, Prague, Czech
Republic, June.
Dan Klein and Christoper Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Reinhard Kneser and Hermann Ney. 1993. Improved
clustering techniques for class-based statistical lan-
guage modelling. In Proceedings of the 3rd European
Conference on Speech Communication and Technol-
ogy, pages 973?976, Berlin, Germany.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics Conference (HLT/NAACL).
Roland Kuhn, Boxing Chen, George Foster, and Evan
Stratford. 2010. Phrase clustering for smoothing
TM probabilities - or, how to extract paraphrases from
phrase tables. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 608?616, Beijing, China, August.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL).
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics.
J. B. MacQueen. 1967. Some methods for classification
and analysis of multivariate observations. In L. M. Le
Cam and J. Neyman, editors, Proc. of the fifth Berkeley
Symposium on Mathematical Statistics and Probabil-
ity, volume 1, pages 281?297. University of California
Press.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Syd-
ney, Australia.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP).
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global phrase
reordering model for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
ACL-44, pages 713?720.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
the European chapter of the Association for Computa-
tional Linguistics (EACL), pages 71?76.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL).
Hugo Steinhaus. 1956. Sur la division des corps
mate?riels en parties. Bull. Acad. Polon. Sci. Cl. III.
4, pages 801?804.
10
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. The Prague
Bulletin of Mathematical Linguistics, 91:67?78.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, HLT/NAACL.
Andreas Zollmann and Stephan Vogel. 2010. New
parameterizations and features for PSCFG-based ma-
chine translation. In Proceedings of the 4th Work-
shop on Syntax and Structure in Statistical Translation
(SSST), Beijing, China.
Andreas Zollmann, Ashish Venugopal, Franz J. Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of the Conference on Computa-
tional Linguistics (COLING).
11
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 294?298,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Corpus Expansion for Statistical Machine Translation with
Semantic Role Label Substitution Rules
Qin Gao and Stephan Vogel
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
{qing, stephan.vogel}@cs.cmu.edu
Abstract
We present an approach of expanding paral-
lel corpora for machine translation. By uti-
lizing Semantic role labeling (SRL) on one
side of the language pair, we extract SRL sub-
stitution rules from existing parallel corpus.
The rules are then used for generating new
sentence pairs. An SVM classifier is built to
filter the generated sentence pairs. The fil-
tered corpus is used for training phrase-based
translation models, which can be used directly
in translation tasks or combined with base-
line models. Experimental results on Chinese-
English machine translation tasks show an av-
erage improvement of 0.45 BLEU and 1.22
TER points across 5 different NIST test sets.
1 Introduction
Statistical machine translation (SMT) relies on par-
allel corpus. Aside from collecting parallel cor-
pus, we have seen interesting research on automat-
ically generating corpus from existing resources.
Typical examples are paraphrasing using bilingual
(Callison-Burch et al, 2006) or monolingual (Quirk
et al, 2004) data. In this paper, we propose a dif-
ferent methodology of generating additional parallel
corpus. The basic idea of paraphrasing is to find al-
ternative ways that convey the same information.
In contrast, we propose to build new parallel sen-
tences that convey different information, yet retain
correct grammatical and semantic structures.
The basic idea of the proposed method is to sub-
stitute source and target phrase pairs in a sentence
pair with phrase pairs from other sentences. The
problem is how to identify where a substitution
should happen and which phrase pairs are valid can-
didates for the substitution. While syntactical con-
straints have been proven to helpful in identifying
good paraphrases (Callison-Burch, 2008), it is in-
sufficient in our task because it cannot properly filter
the candidates for the replacement. If we allow all
the NPs to be replaced with other NPs, each sen-
tence pair can generate huge number of new sen-
tences. Instead, we resort to Semantic Role Labeling
(Palmer et al, 2005) to provide more lexicalized and
semantic constraints to select the candidates. The
method only requires running SRL labeling on ei-
ther side of the language pair, and that enables ap-
plications on low resource languages. Even with the
SRL constraints, the generated corpus may still be
large and noisy. Hence, we apply an additional fil-
tering stage on the generated corpus. We used an
SVM classifier with features derived from standard
phrase based translation models and bilingual lan-
guage models to identify high quality sentence pairs,
and use these sentence pairs in the SMT training. In
the remaining part of the paper, we introduce the ap-
proach and present experimental results on Chinese-
to-English translation tasks, which showed improve-
ments across 5 NIST test sets.
2 The Proposed Approach
The objective of the method is to generate new syn-
tactically and semantically well-formed parallel sen-
tences from existing corpus. To achieve this, we first
collect a set of rules as the candidates for the substi-
tution. We also need to know where we should put in
the replacements and whether the resulting sentence
pairs are grammatical.
First, standard word alignment and phrase extrac-
tion are performed on existing corpus. Afterwards,
we apply an SRL labeler on either the source or tar-
get language, whichever has a better SRL labeler.
Third, we extract SRL substitution rules (SSRs)
from the corpus. The rules carry information of se-
mantic frames, semantic roles, and corresponding
294
.! !   &
1

2
	 

 ,+,2+
% ,
+ ",  &
 ,+,1+
% ,+ ",.! %(" #
'!% ")"Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 379?383,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment
Shujian Huang
State Key Laboratory for
Novel Software Technology
Nanjing University
huangsj@nlp.nju.edu.cn
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
vogel@cs.cmu.edu
Jiajun Chen
State Key Laboratory for
Novel Software Technology
Nanjing University
chenjj@nlp.nju.edu.cn
Abstract
Word alignment has an exponentially large
search space, which often makes exact infer-
ence infeasible. Recent studies have shown
that inversion transduction grammars are rea-
sonable constraints for word alignment, and
that the constrained space could be efficiently
searched using synchronous parsing algo-
rithms. However, spurious ambiguity may oc-
cur in synchronous parsing and cause prob-
lems in both search efficiency and accuracy. In
this paper, we conduct a detailed study of the
causes of spurious ambiguity and how it ef-
fects parsing and discriminative learning. We
also propose a variant of the grammar which
eliminates those ambiguities. Our grammar
shows advantages over previous grammars in
both synthetic and real-world experiments.
1 Introduction
In statistical machine translation, word alignment at-
tempts to find word correspondences in parallel sen-
tence pairs. The search space of word alignment
will grow exponentially with the length of source
and target sentences, which makes the inference for
complex models infeasible (Brown et al, 1993). Re-
cently, inversion transduction grammars (Wu, 1997),
namely ITG, have been used to constrain the search
space for word alignment (Zhang and Gildea, 2005;
Cherry and Lin, 2007; Haghighi et al, 2009; Liu et
al., 2010). ITG is a family of grammars in which the
right hand side of the rule is either two nonterminals
or a terminal sequence. The most general case of the
ITG family is the bracketing transduction grammar
A? [AA] | ?AA? | e/f | /f | e/
Figure 1: BTG rules. [AA] denotes a monotone concate-
nation and ?AA? denotes an inverted concatenation.
(BTG, Figure 1), which has only one nonterminal
symbol.
Synchronous parsing of ITG may generate a large
number of different derivations for the same under-
lying word alignment. This is often referred to as
the spurious ambiguity problem. Calculating and
saving those derivations will slow down the parsing
speed significantly. Furthermore, spurious deriva-
tions may fill up the n-best list and supersede po-
tentially good results, making it harder to find the
best alignment. Besides, over-counting those spu-
rious derivations will also affect the likelihood es-
timation. In order to reduce spurious derivations,
Wu (1997), Haghighi et al (2009), Liu et al (2010)
propose different variations of the grammar. These
grammars have different behaviors in parsing effi-
ciency and accuracy, but so far no detailed compari-
son between them has been done.
In this paper, we formally analyze alignments un-
der ITG constraints and the different causes of spu-
rious ambiguity for those alignments. We do an em-
pirical study of the influence of spurious ambiguity
on parsing and discriminative learning by compar-
ing different grammars in both synthetic and real-
data experiments. To our knowledge, this is the first
in-depth analysis on this specific issue. A new vari-
ant of the grammar is proposed, which efficiently re-
moves all spurious ambiguities. Our grammar shows
advantages over previous ones in both experiments.
379
AA
A A
A
e1 e2 e3
f1 f2 f3
A
A A
A A
e1 e2 e3
f1 f2 f3
A
A
A A
A
e1 e2 e3
f1 f2 f3
A
A A
A A
e1 e2 e3
f1 f2 f3
Figure 2: Possible monotone/inverted t-splits (dashed
lines) under BTG, causing branching ambiguities.
2 ITG Alignment Family
By lexical rules like A ? e/f , each ITG derivation
actually represents a unique alignment between the
two sequences. Thus the family of ITG derivations
represents a family of word alignment.
Definition 1. The ITG alignment family is a set of
word alignments that has at least one BTG deriva-
tion.
ITG alignment family is only a subset of word
alignments because there are cases, known as inside-
outside alignments (Wu, 1997), that could not be
represented by any ITG derivation. On the other
hand, an ITG alignment may have multiple deriva-
tions.
Definition 2. For a given grammar G, spurious am-
biguity in word alignment is the case where two or
more derivations d1, d2, ... dk of G have the same
underlying word alignmentA. A grammarG is non-
spurious if for any given word alignment, there exist
at most one derivation under G.
In any given derivation, an ITG rule applies by ei-
ther generating a bilingual word pair (lexical rules)
or splitting the current alignment into two parts,
which will recursively generate two sub-derivations
(transition rules).
Definition 3. Applying a monotone (or inverted)
concatenation transition rule forms a monotone t-
split (or inverted t-split) of the original alignment
(Figure 2).
3 Causes of Spurious Ambiguity
3.1 Branching Ambiguity
As shown in Figure 2, left-branching and right-
branching will produce different derivations under
A? [AB] | [BB] | [CB] | [AC] | [BC] | [CC]
B ? ?AA? | ?BA? | ?CA? | ?AC? | ?BC? | ?CC?
C ? e/f | /f | e/
Figure 3: A Left heavy Grammar (LG).
BTG, but yield the same word alignment. Branching
ambiguity was identified and solved in Wu (1997),
using the grammar in Figure 3, denoted as LG. LG
uses two separate non-terminals for monotone and
inverted concatenation, respectively. It only allows
left branching of such non-terminals, by excluding
rules like A? [BA].
Theorem 1. For each ITG alignment A, in which
all the words are aligned, LG will produce a unique
derivation.
Proof: Induction on n, the length of A. Case n=1
is trivial. Induction hypothesis: the theorem holds
for any A with length less than n.
For A of length n, let s be the right most t-split
which splits A into S1 and S2. s exists because A is
an ITG alignment. Assume that there exists another
t-split s?, splitting A into S11 and (S12S2). Because
A is fixed and fully aligned, it is easy to see that if
s is a monotone t-split, s? could only be monotone,
and S12 and S2 in the right sub-derivation of t-split s?
could only be combined by monotone concatenation
as well. So s? will have a right branching of mono-
tone concatenation, which contradicts with the def-
inition of LG because right branching of monotone
concatenations is prohibited. A similar contradic-
tion occurs if s is an inverted t-split. Thus s should
be the unique t-split forA. By I.H., S1 and S2 have a
unique derivation, because their lengths are less than
n. Thus the derivation for A will be unique.
3.2 Null-word Attachment Ambiguity
Definition 4. For any given sentence pair (e, f) and
its alignment A, let (e?, f ?) be the sentence pairs
with all null-aligned words removed from (e, f).
The alignment skeletonAS is the alignment between
(e?, f ?) that preserves all links in A.
From Theorem 1 we know that every ITG align-
ment has a unique LG derivation for its alignment
skeleton (Figure 4 (c)).
However, because of the lexical or syntactic dif-
ferences between languages, some words may have
380
AC B
A
C C
C
e1/ e2 e3 e4
f1 f2 f3
(a)
B
A
A
C C
C
C
e1/ e2 e3 e4
f1 f2 f3
(b)
B
A
C
C01
Ct C01
C
C
e1/ e2 e3 e4
f1 f2 f3
(c)
Figure 4: Null-word attachment for the same alignment.
((a) and (b) are spurious derivations under LG caused
by null-aligned words attachment. (c) shows the unique
derivation under LGFN. The dotted lines have omitted
some unary rules for simplicity. The dashed box marks
the alignment skeleton.)
A? [AB] | [BB] | [CB] | [AC] | [BC] | [CC]
B ? ?AA? | ?BA? | ?CA? | ?AC? | ?BC? | ?CC?
C ? C01 | [Cs C]
C01 ? C00 | [Ct C01]
C00 ? e/f, Ct ? e/, Cs ? /f
Figure 5: A Left heavy Grammar with Fixed Null-word
attachment (LGFN).
no explicit correspondence in the other language and
tend to stay unaligned. These null-aligned words,
also called singletons, should be attached to some
other nodes in the derivation. It will produce dif-
ferent derivations if those null-aligned words are at-
tached by different rules, or to different nodes.
Haghighi et al (2009) give some restrictions on
null-aligned word attachment. However, they fail to
restrict the node to which the null-aligned word is
attached, e.g. the cases (a) and (b) in Figure 4.
3.3 LGFN Grammar
We propose here a new variant of ITG, denoted as
LGFN (Figure 5). Our grammar takes similar tran-
sition rules as LG and efficiently constrains the at-
tachment of null-aligned words. We will empirically
compare those different grammars in the next sec-
tion.
Lemma 1. LGFN has a unique mapping from the
derivation of any given ITG alignment A to the
derivation of its alignment skeleton AS .
Proof: LGFN maps the null-aligned source word
sequence, Cs1 , Cs2 , ..., Csk , the null-aligned target
word sequence, Ct1 , Ct2 , ..., Ctk? , together with the
aligned word-pair C00 that directly follows, to the
nodeC exactly in the way of Equation 1. The brack-
ets indicate monotone concatenations.
C ? [Cs1 ...[Csk [Ct1 ...[Ctk?C00]...]]...] (1)
The mapping exists when every null-aligned se-
quence has an aligned word-pair after it. Thus it
requires an artificial word at the end of the sentence.
Note that our grammar attaches null-aligned
words in a right-branching manner, which means it
builds the span only when there is an aligned word-
pair. After initialization, any newly-built span will
contain at least one aligned word-pair. Compara-
tively, the grammar in Liu et al (2010) uses a left-
branching manner. It may generate more spans that
only contain null-aligned words, which makes it less
efficient than ours.
Theorem 2. LGFN has a unique derivation for each
ITG alignment, i.e. LGFN is non-spurious.
Proof: Derived directly from Definition 4, Theo-
rem 1 and Lemma 1.
4 Experiments
4.1 Synthetic Experiments
We automatically generated 1000 fully aligned ITG
alignments of length 20 by generating random per-
mutations first and checking ITG constraints using a
linear time algorithm (Zhang et al, 2006). Sparser
alignments were generated by random removal of
alignment links according to a given null-aligned
word ratio. Four grammars were used to parse these
alignments, namely LG (Wu, 1997), HaG (Haghighi
et al, 2009), LiuG (Liu et al, 2010) and LGFN (Sec-
tion 3.3).
Table 1 shows the average number of derivations
per alignment generated under LG and HaG. The
number of derivations produced by LG increased
dramatically because LG has no restrictions on null-
aligned word attachment. HaG also produced a large
number of spurious derivations as the number of
null-aligned words increased. Both LiuG and LGFN
produced a unique derivation for each alignment, as
expected. One interpretation is that in order to get
381
% 0 5 10 15 20 25
LG 1 42.2 1920.8 9914.1+ 10000+ 10000+
HaG 1 3.5 10.9 34.1 89.2 219.9
Table 1: Average #derivations per alignment for LG and
HaG v.s. Percentage of unaligned words. (+ marked
parses have reached the beam size limit of 10000.)
600
s)
Ha
G
Liu
G
200300400500
sing time (
Ha
G
Liu
G
LF
G
LG
0100
0
5
10
15
20
25
Par
P
t
f
ll
li
d
d
Per
cen
tag
e o
f n
ull
-al
ign
ed 
wo
rds
 
Figure 6: Total parsing time (in seconds) v.s. Percentage
of un-aligned words.
the 10-best alignments for sentence pairs that have
10% of words unaligned, the top 109 HaG deriva-
tions should be generated, while the top 10 LiuG or
LGFN derivations are already enough.
Figure 6 shows the total parsing time using each
grammar. LG and HaG showed better performances
when most of the words were aligned because their
grammars are simpler and less constrained. How-
ever, when the number of null-aligned words in-
creased, the parsing times for LG and HaG became
much longer, caused by the calculation of the large
number of spurious derivations. Parsings using LG
for 10 and 15 percent of null-aligned words took
around 15 and 80 minutes, respectively, which can-
not be plotted in the same scale with other gram-
mars. The parsing times of LGFN and LiuG also
slowly increased, but parsing LGFN consistently
took less time than LiuG.
It should be noticed that the above results came
from parsing according to some given alignment.
When searching without knowing the correct align-
ment, it is possible for every word to stay unaligned,
which makes spurious ambiguity a much more seri-
ous issue.
4.2 Discriminative Learning Experiments
To further study how spurious ambiguity affects the
discriminative learning, we implemented a frame-
work following Haghighi et al (2009). We used
a log-linear model, with features like IBM model1
020.21 01
7
0.180.190.
2
AE R
0.150.160.1
7
1
6
11
16
A
Ha
G-
20be
st
LF
G-
1bes
t
LF
G-
20be
st
Num
ber o
f it
era
tio
ns
Figure 7: Test set AER after each iteration.
probabilities (collected from FBIS data), relative
distances, matchings of high frequency words,
matchings of pos-tags, etc. Online training was
performed using the margin infused relaxed algo-
rithm (Crammer et al, 2006), MIRA. For each
sentence pair (e, f), we optimized with alignment
results generated from the nbest parsing results.
Alignment error rate (Och and Ney, 2003), AER,
was used as the loss function. We ran MIRA train-
ing for 20 iterations and evaluated the alignments of
the best-scored derivations on the test set using the
average weights.
We used the manually aligned Chinese-English
corpus in NIST MT02 evaluation. The first 200 sen-
tence pairs were used for training, and the last 150
for testing. There are, on average, 10.3% words stay
null-aligned in each sentence, but if restricted to sure
links the average ratio increases to 22.6%.
We compared training using LGFN with 1-best,
20-best and HaG with 20-best (Figure 7). Train-
ing with HaG only obtained similar results with 1-
best trained LGFN, which demonstrated that spu-
rious ambiguity highly affected the nbest list here,
resulting in a less accurate training. Actually, the
20-best parsing using HaG only generated 4.53 dif-
ferent alignments on average. 20-best training us-
ing LGFN converged quickly after the first few it-
erations and obtained an AER score (17.23) better
than other systems, which is also lower than the re-
fined IBM Model 4 result (19.07).
We also trained a similar discriminative model but
extended the lexical rule of LGFN to accept at max-
imum 3 consecutive words. The model was used
to align FBIS data for machine translation exper-
iments. Without initializing by phrases extracted
from existing alignments (Cherry and Lin, 2007) or
using complicated block features (Haghighi et al,
382
2009), we further reduced AER on the test set to
12.25. An average improvement of 0.52 BLEU (Pa-
pineni et al, 2002) score and 2.05 TER (Snover
et al, 2006) score over 5 test sets for a typical
phrase-based translation system, Moses (Koehn et
al., 2003), validated the effectiveness of our experi-
ments.
5 Conclusion
Great efforts have been made in reducing spurious
ambiguities in parsing combinatory categorial gram-
mar (Karttunen, 1986; Eisner, 1996). However, to
our knowledge, we give the first detailed analysis on
spurious ambiguity of word alignment. Empirical
comparisons between different grammars also vali-
dates our analysis.
This paper makes its own contribution in demon-
strating that spurious ambiguity has a negative im-
pact on discriminative learning. We will continue
working on this line of research and improve our
discriminative learning model in the future, for ex-
ample, by adding more phrase level features.
It is worth noting that the definition of spuri-
ous ambiguity actually varies for different tasks. In
some cases, e.g. bilingual chunking, keeping differ-
ent null-aligned word attachments could be useful.
It will also be interesting to explore spurious ambi-
guity and its effects in those different tasks.
Acknowledgments
The authors would like to thank Alon Lavie, Qin
Gao and the anonymous reviewers for their valu-
able comments. This work is supported by the Na-
tional Natural Science Foundation of China (No.
61003112), the National Fundamental Research
Program of China (2010CB327903) and by NSF un-
der the CluE program, award IIS 084450.
References
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of the NAACL-HLT 2007/AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion, SSST ?07, pages 17?24, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585, December.
Jason Eisner. 1996. Efficient normal-form parsing for
combinatory categorial grammar. In Proceedings of
the 34th annual meeting on Association for Compu-
tational Linguistics, ACL ?96, pages 79?86, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Aria Haghighi, John Blitzer, and Dan Klein. 2009. Bet-
ter word alignments with supervised itg models. In
Association for Computational Linguistics, Singapore.
Lauri Karttunen. 1986. Radical lexicalism. Technical
Report CSLI-86-68, Stanford University.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Shujie Liu, Chi-Ho Li, and Ming Zhou. 2010. Dis-
criminative pruning for discriminative itg alignment.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ?10,
pages 316?324, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Matthew Snover, Bonnie J. Dorr, and Richard Schwartz.
2006. A study of translation edit rate with targeted
human annotation. In Proceedings of AMTA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23:377?403, September.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexi-
calized inversion transduction grammar for alignment.
In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, ACL ?05,
pages 475?482, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Compu-
tational Linguistics, pages 256?263, Morristown, NJ,
USA. Association for Computational Linguistics.
383
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1587?1596,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Integrating Phrase-based Reordering Features into a Chart-based
Decoder for Machine Translation
ThuyLinh Nguyen
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
thuylinh@cs.cmu.edu
Stephan Vogel
Qatar Computing Research Institute
Tornado Tower
Doha, Qatar
svogel@qf.org.qa
Abstract
Hiero translation models have two lim-
itations compared to phrase-based mod-
els: 1) Limited hypothesis space; 2) No
lexicalized reordering model. We pro-
pose an extension of Hiero called Phrasal-
Hiero to address Hiero?s second problem.
Phrasal-Hiero still has the same hypoth-
esis space as the original Hiero but in-
corporates a phrase-based distance cost
feature and lexicalized reodering features
into the chart decoder. The work consists
of two parts: 1) for each Hiero transla-
tion derivation, find its corresponding dis-
continuous phrase-based path. 2) Extend
the chart decoder to incorporate features
from the phrase-based path. We achieve
significant improvement over both Hiero
and phrase-based baselines for Arabic-
English, Chinese-English and German-
English translation.
1 Introduction
Phrase-based and tree-based translation model are
the two main streams in state-of-the-art machine
translation. The tree-based translation model, by
using a synchronous context-free grammar for-
malism, can capture longer reordering between
source and target language. Yet, tree-based trans-
lation often underperforms phrase-based transla-
tion in language pairs with short range reordering
such as Arabic-English translation (Zollmann et
al., 2008; Birch et al, 2009).
We follow Koehn et al (2003) for our phrase-
based system and Chiang (2005) for our Hiero sys-
tem. In both systems, the translation of a source
sentence f is the target sentence e? that maximizes
a linear combination of features and weights:
?e?,a?? = argmax
?e,a??H(f)
?
m?M
?mhm (e, f ,a) . (1)
where
? a is a translation path of f . In the phrase-
based system, aph represents a segmentation
of e and f and a correspondance of phrases.
In the Hiero system, atr is a derivation of a
parallel parse tree of f and e, each nontermi-
nal representing a rule in the derivation.
? H (f) is the hypothesis space of the sentence
f . We denote Hph (f) as the phrase-based
hypothesis space of f and Htr (f) as its tree-
based hypothesis space. Galley and Manning
(2010) point out that due to the hard con-
straints of rule combination, the tree-based
system does not have the same excessive hy-
pothesis space as the phrase-based system.
? M is the set of feature indexes used in the
decoder. Many features are shared between
phrase-based and tree-based systems includ-
ing language model, word count, and trans-
lation model features. Phrase-based systems
often use a lexical reordering model in addi-
tion to the distance cost feature.
The biggest difference in a Hiero system and a
phrase-based system is in how the reordering is
modeled. In the Hiero system, the reordering de-
cision is encoded in weighted translation rules, de-
termined by nonterminal mappings. For example,
the rule X ? ne X1 pas ; not X1 : w indicates
the translation of the phrase between ne and pas to
be after the English word not with scorew. During
decoding, the system parses the source sentence
and synchronously generates the target output.
To achieve reordering, the phrase-based sys-
tem translates source phrases out of order. A re-
ordering distance limit is imposed to avoid search
space explosion. Most phrase-based systems are
equipped with a distance reordering cost feature
to tune the system towards the right amount of
reordering, but then also a lexicalized reordering
1587
model to model the direction of adjacent source
phrases reordering as either monotone, swap or
discontinuous.
There are two reasons to explain the shortcom-
ings of the current Hiero system:
1. A limited hypothesis space because the syn-
chronous context-free grammar is not appli-
cable to non-projective dependencies.
2. It does not have the expressive lexicalized re-
ordering model and distance cost features of
the phrase-based system.
When comparing phrase-based and Hiero trans-
lation models, most of previous work on tree-
based translation addresses its limited hypothesis
space problem. Huck et al (2012) add new rules
into the Hiero system, Carreras and Collins (2009)
apply the tree adjoining grammar formalism to al-
low highly flexible reordering. On the other hand,
the Hiero model has the advantage of capturing
long distance and structure reordering. Galley
and Manning (2010) extend phrase-based trans-
lation by allowing gaps within phrases such as
?ne . . . pas, not?, so the decoder still has the dis-
criminative reordering features of phrase-based,
but also uses on average longer phrases. How-
ever, these phrase pairs with gaps do not capture
structure reordering as do Hiero rules with non-
terminal mappings. For example, the rule X ?
ne X1 pas ; not X1 explicitly places the transla-
tion of the phrase between ne and pas behind the
English word not through nonterminal X1. This
is important for language pairs with strict reorder-
ing. In our Chinese-English experiment, the Hiero
system still outperforms the discontinuous phrase-
based system.
We address the second problem of the origi-
nal Hiero decoder by mapping Hiero translation
derivations to corresponding phrase-based paths,
which not only have the same output but also pre-
serve structure distortion of the Hiero translation.
We then include phrase-based features into the Hi-
ero decoder.
A phrase-based translation path is the sequence
of phrase-pairs, whose source sides cover the
source sentence and whose target sides generate
the target sentence from left to right. If we look at
the leaves of a Hiero derivation tree, the lexicals
also form a segmentation of the source and target
sentence, thus also form a discontinuous phrase-
based translation path. As an example, let us look
at the translation of the French sentence je ne parle
pas le franc?aise into English i don?t speak french
in Figure 1. The Hiero decoder translates the sen-
tence using a derivation of three rules:
? r1 = X? parle ; speak.
? r2 = X? ne X1 pas ; don?t X1.
? r3 = X?
Je X1 le Franc?ais ; I X1 french.
From this Hiero derivation, we have a seg-
mentation of the sentence pairs into phrase
pairs according to the word alignments, as
shown on the left side of Figure 1. Or-
dering these phrase pairs according the word
sequence on the target side, shown on the
right side of Figure 1, we have a phrase-
based translation path consisting of four phrase
pairs: (je, i) , (ne . . . pas, not) , (parle, speak) ,
(lefrancaise, french) that has the same output
as the Hiero system. Note that even though the
Hiero decoder uses a composition of three rules,
the corresponding phrase-based path consists of
four phrase pairs. We name this new variant of the
Hiero decoder, which uses phrase-based features,
Phrasal-Hiero.
Our Phrasal-Hiero addresses the shortcomming
of the original Hiero system by incorporating
phrase-based features. Let us revisit machine
translation?s loglinear model combination of fea-
tures in equation 1. We denote ph(a) as the corre-
sponding phrase-based path of a Hiero derivation
a, and MPh\H as the indexes of phrase-based fea-
tures currently not applicable to the Hiero decoder.
Our Phrasal-Hiero decoder seeks to find the trans-
lation, which optimizes:
?e?,a?? = argmax
?e,a??Htr(f)
( ?
m?MH
?mhm (e, f ,a) +
+
?
m??MPh\H
?m?hm? (e, f , ph(a))
)
.
We focus on improving the modelling of re-
ordering within Hiero and include discriminative
reordering features (Tillmann, 2004) and a dis-
tance cost feature, both of which are not modeled
in the original Hiero system. Chiang et al (2008)
added structure distortion features into their de-
coder and showed improvements in their Chinese-
English experiment. To our knowledge, Phrasal-
Hiero is the first system, which directly integrates
phrase-based and Hiero features into one model.
1588
Figure 1: Example of French-English Hiero Translation on the left and its corresponding discontinuous
phrase-based translation on the right.
Rules Alignments Phrase pairs & nonterminals
r1 = X? parle ; speak. 0-0 (parle ; speak)
r2 = X? ne X1 pas ; don?t X1. 0-0 1-1 2-0 (ne . . . pas ; don?t) ; X1
r3 = X? Je X1 le Francais ; I X1 French 0-0 1-1 3-2 (Je ; I) ; X1 ; (le Francais; french)
r4 = X? je X1 le X2 ; i X1 X2 0-0 1-1 3-2 Not Applicable
Table 1: Rules and their sequences of phrase pairs and nonterminals
Previous work has attempted to weaken the con-
text free assumption of the synchronous context
free grammar formalism, for example using syn-
tactic non-terminals (Zollmann and Venugopal,
2006). Our approach can be viewed as applying
soft context constraint to make the probability of
substituting a nonterminal by a subtree depending
on the corresponding phrase-based reordering fea-
tures.
In the next section, we explain the model in de-
tail.
2 Phrasal-Hiero Model
Phrasal-Hiero maps a Hiero derivation into a dis-
continuous phrase-based translation path by the
following two steps:
1. Training: Represent each rule as a sequence
of phrase pairs and nonterminals.
2. Decoding: Use the rules? sequences of
phrase pairs and nonterminals to find the
corresponding phrase-based path of a Hiero
derivation and calculate its feature scores.
2.1 Map Rule to A Sequence of Phrase Pairs
and Nonterminals
We segment the rules? lexical items into phrase
pairs. These phrase pairs will be part of the phrase-
based translation path in the decoding step. The
rules? nonterminals are also preserved in the se-
quence, during the decoding they will be substi-
tuted by other rules? phrase pairs. We now explain
how to map a rule to a sequence of phrase pairs
and nonterminals.
Let r = X ?
s0X1s1 . . . Xksk ; t0X?(1)t1 . . . X?(k)tk be a rule
of k nonterminals, ?(.) defines the sequence of
nonterminals on the target. si or ti , i = 0 . . . k
are phrases between nonterminals, they can be
empty because nonterminals can be at the border
of the rule or two nonterminals are adjacent. For
example the rule X ? ne X1 pas ; not X1
has k = 1, s0 = ne, s1 = pas, t0 = not, t1 is
an empty phrase because the target X1 is at the
rightmost position.
Phrasal-Hiero retains both nonterminals and
lexical alignments of Hiero rules instead of only
nonterminal mappings as in (Chiang, 2005). A
1589
rule?s lexical alignment is the most frequent one
in the training data. We use the lexical alignments
of a rule to decide how source phrases and tar-
get phrases are connected. In the rule r, a source
phrase si is connected to a target phrase ti? if at
least one word in si aligns to a target word in ti? . In
the rule X? Je X1 le Franc?ais ; I X1 french
extract from sentence pair in Figure 1, the phrase
le Franc?ais connects to the phrase french because
the French word Franc?ais aligns with the English
word french even though le is unaligned.
We then group the source phrases and target
phrases into phrase pairs such that only phrases
that are connected to each other are in the same
phrase pair. So phrase pairs still preserve the lexi-
cal dependency of the rule. Phrase pairs and non-
terminals are then ordered according to the target
side of the rule. Table 1 shows an example of rules,
alignments and their sequences of phrase pairs and
nonterminals on the last column.
Figure 2: Alignment of a sentence pair.
There are Hiero rules in which one of its source
phrases or target phrases is not aligned. For exam-
ple in the rule r4 = X ? je X1 le X2 ; i X1 X2
extracted from the sentence pair in Figure 2, the
phrase le is not aligned. In our Arabic-English
experiment, rules without nonaligned phrases ac-
count for only 48.54% of the total rules. We com-
pared the baseline Hiero translation from the full
set of rules and the translation from only rules
without nonaligned phrases. The later translation
is faster and Table 2 1 shows that it outperforms
the translation with the whole set of rules. We
therefore decided to not use rules with nonaligned
phrases in Phrasal-Hiero.
It is important to note that there are different
ways to use all the rules and map rules with un-
aligned phrases into a sequence of phrase pairs.
1The dataset and experiment setting description are in sec-
tion 4.
Test set MT04 MT05 MT09
All rules 48.17 47.85 42.37
Phrasal Hiero 48.52 47.78 42.8
Table 2: Arabic-English pilot experiment. Com-
pare BLEU scores of translation using all ex-
tracted rules (the first row) and translation using
only rules without nonaligned subphrases (the sec-
ond row).
For example, adding these unaligned phrases to
the previous phrase pair i.e. the rule r4 has one dis-
continuous phrase pair (je . . . le, i) or treat these
unaligned phrases as deletion/insertion phrases.
We started the work with Arabic-English transla-
tion and decided not to use rules with nonaligned
phrases in Phrasal-Hiero. In the experiment sec-
tion, we will discuss the impact of removing
rules with nonaligned sub-phrases in our German-
English and Chinese-English experiments.
2.2 Training: Lexicalized Reordering Table
Phrasal-Hiero needs a phrase-based lexicalized re-
ordering table to calculate the features. The lexi-
calized reordering table could be from a discontin-
uous phrase-based system. To guarantee the lexi-
calized reordering table to cover all phrase pairs
of the rule table, we extract phrase-pairs and their
reordering directions during rule extraction.
Let (s, t) be a sentence pair in the training data
and r = X? s0X1s1 . . . Xksk ; t0X1t1 . . . Xktk
be a rule extracted from the sentence. The lex-
ical phrase pair corresponding to the rule r is
ph = (s0 . . . s1 . . . sk, t0 . . . t1 . . . tk), with non-
terminals are replaced by the gaps. Because the
nonterminal could be at the border of the rule, the
lexical phrase pair might have smaller coverage
than the rule. For example, the training sentence
pair in Figure 2 generates the rule r2 = X ?
ne X1 pas ; don?t X1 spanning (1 . . . 3, 1 . . . 2)
but its lexical phrase pair (ne . . . pas, not) only
spans (1 . . . 3, 1 . . . 1).
Also, two different rules can have the same
lexical phrase pairs. In Phrasal-Hiero, each lex-
ical phrase pair is only generated once for a
sentence. Look at the example of the train-
ing sentence pair in Figure 2, the rule X ?
je ; I spanning (0 . . . 1, 0 . . . 1) and the rule X ?
je X1 ; I X1 spanning (0 . . . 3, 0 . . . 2) are both
sharing the same lexical phrase pair (je, i) span-
ning (0 . . . 1, 0 . . . 1). But Phrasal-Hiero only gen-
1590
erates (je, i) once for the sentence. Phrase pairs
are generated together with phrase-based reorder-
ing orientations to build lexicalized reordering ta-
ble.
3 Decoding
Chiang (2007) applied bottom up chart parsing to
parse the source sentence and project on the tar-
get side for the best translation. Each chart cell
[X, i, j, r] indicates a subtree with rule r at the root
covers the translation of the i-th word upto the j-th
word of the source sentence. We extend the chart
parsing, mapping the subtree to the equivalent dis-
continuous phrase-based path and includes phrase-
based features to the log-linear model.
In Phrasal-Hiero, each chart cell [X, i, j, r] also
stores the first phrase pair and the last phrase pair
of the phrase-based translation path covered the i-
th to the j-th word of the source sentence. These
two phrase pairs are the back pointers to calcu-
late reordering features of later larger spans. Be-
cause the distance cost feature and phrase-based
discriminative reordering feature calculation are
both only required the source coverage of two ad-
jacent phrase pairs, we explain here the distance
cost calculation.
We will again use three rules r1, r2, r3 in Ta-
ble 1 and the translation je ne parle pas le franc?ais
into I don?t speak French to present the technique.
Table 3 shows the distance cost calculation.
First, when the rule r has only terminals, the
rule?s sequence of phrase pairs and nonterminals
consists of only a phrase pair. No calculation is
needed, the first phrase pair and the last phrase
pair are the same. The chart cell X1 : 2 . . . 2 in
Table 3 shows the translation with the rule r1 =
X ? parle ; speak. The first phrase pair and the
last phrase pair point to the phrase (parle, speak)
spanning 2 . . . 2 of the source sentence.
When the translation rule?s right hand side has
nonterminals, the nonterminals in the sequence
belong to smaller chart cells that we already found
phrase-based paths and calculated their features
before. The decoder then substitute these paths
into the rule?s sequence of phrase pairs and non-
terminals to form the complete path for the current
span.
We now demonstrate finding the phrase based
path and calculate distance cost of the chart
cell X2 spanning 1 . . . 3. The next phrase pair
of (ne . . . pas, don?t) is the first phrase pair
of the chart cell X1 which is (parle, speak).
The distance cost of these two phrase pairs ac-
cording to discontinuous phrase-based model is
|2? 3? 1| = 2. The distance cost of the
whole chart cell X2 also includes the cost of the
translation path covered by chart cell X1 which
is 0, therefore the distance cost for X2 is 2 +
dist(X1) = 2. We then update the first phrase
pair and the last phrase pair of cell X2. The first
phrase pair of X2 is (ne . . . pas, don?t), the last
phrase pair is also the last phrase pair of cell X1
which is (parle, speak).
Similarly, finding the phrase-based path and
calculate its distortion features in the chart cell
X3 include calculate the feature values for mov-
ing from the phrase pair (je, I) to the first
phrase pair of chart cell X2 and also from last
phrase pair of chart cell X2 to the phrase pair
(le franc?aise, french).
4 Experiment Results
In all experiments we use phrase-orientation lex-
icalized reordering (Galley and Manning, 2008)2
which models monotone, swap, discontinuous
orientations from both reordering with previous
phrase pair and with the next phrase pair. There
are total six features in lexicalized reordering
model.
We will report the impact of integrating phrase-
based features into Hiero systems for three lan-
guage pairs: Arabic-English, Chinese-English and
German-English.
4.1 System Setup
We are using the following three baselines:
? Phrase-based without lexicalized reodering
features. (PB+nolex)
? Phrase-based with lexicalized reordering fea-
tures.(PB+lex)
? Hiero system with all rules extracted from
training data. (Hiero)
We use Moses phrase-based and chart decoder
(Koehn et al, 2007) for the baselines. The score
difference between PB+nolex and PB+lex results
indicates the impact of lexicalized reordering fea-
tures on phrase-based system. In Phrasal-Hiero we
2Galley and Manning (2008) introduce three orientation
models for lexicalized reordering: word-based, phrase-based
and hierarchical orientation model. We apply phrase-based
orientation in all experiment using lexicalized reordering.
1591
Chart Cell Rule?s phrase pairs & NTs Distance First Phrase Pair Last Phrase Pair
X1 : 2 . . . 2 (parle, speak) ? 2 . . . 2 (parle, speak)
X2 : 1 . . . 3 (ne . . . pas, don?t) ; X1 2 + dist (X1) 1 . . . 3 2 . . . 2 (parle, speak)= 2 (ne . . . pas, don?t)
X3 : 0 . . . 5 (Je ; I) ; X2 ; 0 + dist (X2) 0 . . . 0 (je, I) 4 . . . 5(le Franc?ais; french) +1 = 3 (le Franc?ais; french)
Table 3: Phrasal-Hiero Decoding Example: Calculate distance cost feature for the translation in Figure 1.
will compare if these improvements still carry on
into Hiero systems.
The original Hiero system with all rules ex-
tracted from training data (Hiero) is the most rele-
vant baseline. We will evaluate the difference be-
tween this Hiero baseline and our Phrasal-Hiero.
To implement Phrasal-Hiero, we extented
Moses chart decoder (Koehn et al, 2007) to in-
clude distance-based reordering as well as the lex-
icalized phrase orientation reordering model. We
will report the following results for Phrasal-Hiero:
? Hiero translation results on the subset of rules
without unaligned phrases. (we denote this in
the table scores as P.H.)
? Phrasal-Hiero with phrase-based distance
cost feature (P.H.+dist).
? Phrasal-Hiero with phrase-based lexicalized
reordering features(P.H.+lex).
? Phrasal-Hiero with distance cost and lexical-
ized reordering features(P.H.+dist+lex).
4.2 Arabic-English Results
The Arabic-English system was trained from
264K sentence pairs with true case English. The
Arabic is in ATB morphology format. The lan-
guage model is the interpolation of 5-gram lan-
guage models built from news corpora of the NIST
2012 evaluation. We tuned the parameters on
the MT06 NIST test set (1664 sentences) and re-
port the BLEU scores on three unseen test sets:
MT04 (1353 sentences), MT05 (1056 sentences)
and MT09 (1313 sentences). All test sets have four
references per each sentence.
The results are in Table 4. The three
rows in the first block are the baseline scores.
Phrase-based with lexicalized reordering fea-
tures(PB+lex) shows significant improvement on
all test sets over the simple phrase-based system
without lexicalized reordering (PB+nolex). On av-
erage the improvement is 1.07 BLEU score (45.66
MT04 MT05 MT09 Avg.
PB+nolex 47.40 46.83 42.75 45.66
PB+lex 48.62 48.07 43.51 46.73
Hiero 48.17 47.85 42.37 46.13
P.H. 48.52 47.78 42.80 46.37(48.54% rules)
P.H.+dist 48.46 47.92 42.62 46.33
P.H. +lex 48.70 48.59 43.84 47.04
P.H +lex+dist 49.35 49.07 43.40 47.27
Improv. over 0.73 1.00 0.34 0.54PB+lex
Improv. over 0.83 1.29 1.04 0.90P.H.
Improv. over 1.18 1.22 1.47 1.14Hiero
Table 4: Arabic-English true case translation
scores in BLEU metric. The three rows in the first
block are the baseline scores. The next four rows
in the second block are Phrasal-Hiero scores, the
best scores are in boldface. The three rows in the
last block are the Phrasal-Hiero improvements.
versus 46.73). We make the same observation as
Zollmann et al (2008), i.e, that the Hiero baseline
system underperforms compared to the phrase-
based system with lexicalized phrase-based re-
ordering for Arabic-English in all test sets, on av-
erage by about 0.60 BLEU points (46.13 versus
46.73). This is because Arabic language has rel-
ative free reordering, but mostly short distance,
which is better captured by discriminative reorder-
ing features.
The next four rows in the second block of Ta-
ble 4 show Phrasal-Hiero results. The P.H. line is
the result of Hiero experiment on only a subset of
rules without nonaligned phrases. As mentioned
in section 2.1, Phrasal-Hiero only uses 48.54% of
the rules but achieves as good or even better per-
formance (on average 0.24 BLEU points better)
compared to the original Hiero system using the
full set of rules.
We do not benefit from adding only the
1592
distance-based reordering feature (P.H+dist) to the
Arabic-English experiment but get significant im-
provements when adding the six features of the
lexicalized reordering (P.H+lex). Table 4 shows
that the P.H.+lex system gains on average 0.67
BLEU points (47.04 versus 46.37). Even though
the baseline Hiero underperforms phrase-based
system with lexicalized reordering(P.B+lex), the
P.H.+lex system already outperforms P.B+lex in
all test sets (on average 47.04 versus 46.73).
Adding both distance cost and lexicalized re-
ordering features (P.H.+dist+lex) performs the
best. On average P.H.+dist+lex improves 0.90
BLEU points over P.H. without new phrase-based
features and 1.14 BLEU score over the base-
line Hiero system. Note that Hiero rules already
have lexical context in the reordering, but adding
phrase-based lexicalized reordering features to the
system still gives us about as much improvement
as the phrase-based system gets from lexicalized
reordering features, here 1.07 BLEU points. And
our best Phrasal-Hiero significantly improves over
the best phrase-based baseline by 0.54 BLEU
points. This shows that the underperformance of
the Hiero system is due to its lack of lexicalized
reordering features rather than a limited hypothe-
sis space.
4.3 Chinese-English Results
The Chinese-English system was trained on FBIS
corpora of 384K sentence pairs, the English cor-
pus is lower case. The language model is the tri-
gram SRI language model built from Xinhua cor-
pus of 180 millions words. We tuned the parame-
ters on MT06 NIST test set of 1664 sentences and
report the results of MT04, MT05 and MT08 un-
seen test sets. The results are in Table 5.
We also make the same observation as Zoll-
mann et al (2008) on the baselines for Chinese-
English translation. Even though the phrase-
based system benefits from lexicalized reordering,
PB+lex on average outperforms PB+nolex by 1.16
BLEU points (25.87 versus 27.03), it is the Hiero
system that has the best baseline scores across all
test sets, with and average of 27.70 BLEU points.
Phrasal Hiero scores are given in the second
block of Table 5. It uses 84.19% of the total train-
ing rules, but unlike the Arabic-English system,
using a subset of the rules costs Phrasal-Hiero on
all test sets and on average it loses 0.49 BLEU
points (27.21 versus 27.70). Similar to Chiang
MT04 MT05 MT08 Avg.
PB+nolex 29.99 26.4 21.23 25.87
PB+lex 31.03 27.57 22.41 27.03
Hiero 32.49 28.06 22.57 27.70
P.H. 31.83 27.66 22.16 27.21(84.19% rules)
P.H.+dist 32.18 28.25 22.46 27.63
P.H.+lex 32.55 28.51 23.08 28.05
P.H+lex+dist 33.06 28.78 23.23 28.35
Improv. over 2.03 1.21 0.82 1.32PB+lex
Improv. over 1.23 1.12 1.07 1.14P.H.
Improv. over 0.57 0.72 0.66 0.65Hiero
Table 5: Chinese-English lower case translation
scores in BLEU metric.
et al (2008) in their Chinese-English experiment,
we benefit by adding the distance cost feature.
PH.+dist outperforms P.H. on all test sets. We
have better improvements when adding the six fea-
tures of the lexicalized reordering model: P.H.+lex
on average has 28.05 BLEU points, i.e. gains
0.84 over P.H.. The P.H.+lex system is even better
than the best Hiero baseline using the whole set of
rules.
We again get the best translation when adding
both the distance cost feature and the lexicalized
reordering features. The P.H+dist+lex has the best
score across all the test sets and on average gains
1.14 BLEU points over P.H. So adding phrase-
based features to the Hiero system yields nearly
the same improvement as adding lexicalized re-
ordering features to the phrase-based system. This
shows that a strong Chinese-English Hiero system
still benefits from phrase-based features. Further
more, the P.H+dist+lex also outperforms the Hi-
ero baseline using all rules from training data.
4.4 German-English Results
We next consider German-English translation.
The systems were trained on 1.8 million sentence
pairs using the Europarl corpora. The language
model is three-gram SRILM trained from the tar-
get side of the training corpora. We use WMT
2010 (2489 sentences) as development set and
report scores on WMT 2008 (2051 sentences),
WMT 2009 (2525 sentences), WMT 2011 (3003
sentences). All test sets have one reference per
test sentence. The results are in Table 6.
1593
WMT test 08 09 11 Avg.
PB+nolex 17.46 17.38 16.76 17.20
PB+lex 18.16 17.85 17.18 17.73
Hiero 18.20 18.23 17.46 17.96
P.H. 18.24 18.15 17.39 17.92(80.54% rules)
P.H. +dist 18.19 17.97 17.41 17.85
P.H. +lex 18.59 18.46 17.69 18.24
P.H.+lex+dist 18.70 18.53 17.81 18.34
Improv. over 0.54 0.68 0.63 0.61PB+lex
Improv. over 0.46 0.38 0.42 0.42P.H.
Improv. over 0.50 0.30 0.35 0.38Hiero
Table 6: German-English lower case translation
scores in BLEU metric.
The Hiero baseline performs on average 0.26
BLEU points better than the phrase-based sys-
tem with lexicalized reordering features (PB+lex).
The hrasal-Hiero system used 80.54% of the total
training rules, but on average the P.H. system has
the same performance as the Hiero system using
all the rules extracted from training data. Similar
to the Arabic-English experiment, Phrasal-Hiero
does not benefit from adding the distance cost fea-
ture. We do, however, see improvements on all
test sets when adding lexicalized reordering fea-
tures. On average the P.H.+lex results are 0.32
BLEU points higher than the P.H. results. The
best scores are achieved with P.H+lex+dist. The
German-English translations on average gain 0.38
BLEU score by adding both distance cost and dis-
criminative reordering features.
4.5 Impact of segment rules into phrase pairs
Phrasal Hiero is the first system using rules? lexi-
cal alignments. If lexical alignments are not avail-
able, we can not divide the rules? lexicals into
phrase pairs without losing their dependancies. An
alternative approach would be combining all lex-
icals of a rule into one phrase pair. We run an
addition experiment for this approach on Arabic-
English dataset. Table 7 shows the examples rules
and its new sequence of nonterminals and phrase
pairs. The rules r1 and r2 have the same se-
quences as in Table 1. Without segment rules into
phrase pairs, the rule r3 has only one phrase pair:
ph = (Je . . . le Francaise ; I . . . french) and
ph is repeated twice in r3?s sequence of phrase
pairs and nonterminals. The new experiment uses
the complete set of rules so the rule r4 is included.
According to the new sequence of phrase pairs
and nonterminals, during decoding the rule r3 has
discontinous translation directions on both from
phrase pair ph to the nonterminal X1 and from
X1 to ph. But using lexical alignment and divide
the rule into phrase pairs as in section 2.1 , the
sequence preserves the translation order of r3 as
two monotone translations from (je; I) to X1 and
from X1 to (le Francaise ; french).
Avg
Hiero 46.13
Hiero+lex 46.45 ( +0.32)(no lex. alignments)
P.H 46.37
P.H.+lex 47.04 (+0.67)(with lex. alignments)
Table 8: Average of Arabic-English translation
scores in BLEU metric. Compare the improve-
ment of using rules? lexical alignments (2nd
block) and not using rules? lexical alignments (1st
block).
Table 8 compares the two experiments results.
The additional experiment is denoted as Hiero+lex
in the table. The first block shows an improvement
of 0.32 BLEU score when adding discriminated
reordering features on Hiero (using the whole set
of rules and no rule segmentation). The second
block is the impact of adding discriminated re-
ordering features on Phrasal Hiero (using a sub-
set of rules and segment rules into phrase pairs).
Here the improvement of P.H+lex over P.H is 0.67
BLEU score. It shows the benefit of segment rules
into phrase pairs.
4.6 Rules without unaligned phrases
A-E C-E G-E
Hiero 46.13 27.70 17.96
P.H. 46.36 27.21 17.92
%Rules used 48.54% 84.19% 80.54%
P.H.+lex+dist 47.27 28.35 18.34
Table 9: The impact of using only rules without
nonaligned phrases on Phrasal-Hiero results.
Table 9 summarizes the impact of using only
rules without nonaligned phrases on Phrasal-
1594
Rules Phrase pairs & nonterminals
r1 = X? parle ; speak. (parle ; speak)
r2 = X? ne X1 pas ; don?t X1. (ne . . . pas ; don?t) ; X1
r3 = X? Je X1 le Francais ; I X1 French (Je . . . le Francais ; I . . . french) ; X1 ;
(Je . . . le Francais ; I . . . french)
r4 = X? je X1 le X2 ; i X1 X2 (je . . . le ; i) ; X1 ; X2
Table 7: Example of translation rules and their sequences of phrase pairs and nonterminals when lexical
alignments are not available.
Hiero. Using only rules without nonaligned
phrases can get the same performance with trans-
lation with full set of rules for Arabic-English and
German-English experiments but underperforms
for the Chinese-English system. We suggest the
difference might come from the linguistic diver-
gences of source and target languages.
Phrasal Hiero includes all lexical rules (rules
without nonterminal) therefore it still has the same
lexical coverage as the original Hiero system.
In the Arabic-English system, the Arabic is in
ATB format, therefore most English words should
have alignments in the ATB source, rules with
nonaligned phrases could be the results of bad
alignments or non-informative rules, therefore we
could have better performance by using a subset of
rules in Phrasal-Hiero.
As Chinese and English are highly divergent,
we expect many phrases in one language correctly
unaligned in the other language. So leaving out
the rules with nonaligned phrases could degrade
the system. Even though the current Phrasal-Hiero
with extra phrase-based features outperforms the
Hiero baseline, future work for Phrasal-Hiero will
focus on including all rules extracted from training
corpora.
4.7 Discontinuous Phrase-Based
C-E G-E
PB+lex 27.03 17.73
PB+lex+gap 27.11 17.55
Hiero 27.70 17.96
P.H.+lex+dist 28.35 18.34
Table 10: Comparing Phrasal-Hiero with transla-
tion with gap for Chinese-English and German-
English. The numbers are average BLEU scores
of all test sets.
We compare Phrasal-Hiero with a discontinu-
ous phrase-based system introduced by Galley and
Manning (2010) for Chinese-English and German-
English system. Table 10 shows the average re-
sults. We used Phrasal decoder (Cer et al, 2010)
for phrase-based with gaps (PB+lex+gap) results.
While we do not focus on the differences in the
toolkits, our Phrasal-Hiero still outperforms the
phrase-based with gaps experiments.
Conclusion
We have presented a technique to combine phrase-
based features and tree-based features into one
model. Adding a distance cost feature, we only
get better translation for Chinese-English transla-
tion. Phrasal-Hiero benefits from adding discrim-
inative reodering features in all experiment. We
achieved the best result when adding both distance
cost and lexicalized reordering features. Phrasal-
Hiero currently uses only a subset of rules from
training data. A future work on the model can in-
clude complete rule sets together with word inser-
tion/deletion features for nonaligned phrases.
References
A. Birch, P. Blunsom, and M. Osborne. 2009. A
Quantitative Analysis of Reordering Phenomena. In
StatMT ?09: Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 197?205.
X. Carreras and M. Collins. 2009. Non-Projective
Parsing for Statistical Machine Translation. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 -
Volume 1, EMNLP ?09, pages 200?209.
D. Cer, M. Galley, D. Jurafsky, and C. Manning. 2010.
Phrasal: A Statistical Machine Translation Toolkit
for Exploring New Model Features. In Proceedings
of the NAACL HLT 2010 Demonstration Session,
pages 9?12. Association for Computational Linguis-
tics, June.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online
Large-Margin Training of Syntactic and Structural
Translation Features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
1595
Processing, pages 224?233. Association for Com-
putational Linguistics.
D. Chiang. 2005. A Hierarchical Phrase-Based Model
for Statistical Machine Translation. In Proc. of ACL.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
M. Galley and C. Manning. 2008. A Simple and Effec-
tive Hierarchical Phrase Reordering Model. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 847?
855, Honolulu, Hawaii, October.
M. Galley and C. D. Manning. 2010. Accurate Non-
Hierarchical Phrase-Based Translation. In Proceed-
ings of NAACL-HLT, pages 966?974.
M. Huck, S. Peitz, M. Freitag, and H. Ney. 2012. Dis-
criminative Reordering Extensions for Hierarchical
Phrase-Based Machine Translation. In EAMT, pages
313?320.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In Proc. of HLT-NAACL,
pages 127?133.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL demon-
stration session.
C. Tillmann. 2004. A Unigram Orientation Model for
Statistical Machine Translation. In Proceedings of
HLT-NAACL: Short Papers, pages 101?104.
A. Zollmann and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
Proc. of NAACL 2006 - Workshop on Statistical Ma-
chine Translation.
A. Zollmann, A. Venugopal, F. Och, and J. Ponte.
2008. A Systematic Comparison of Phrase-Based,
Hierarchical and Syntax-Augmented Statistical MT.
In Proceedings of the Conference on Computational
Linguistics (COLING).
1596
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 12?17,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Tale about PRO and Monsters
Preslav Nakov, Francisco Guzma?n and Stephan Vogel
Qatar Computing Research Institute, Qatar Foundation
Tornado Tower, floor 10, PO box 5825
Doha, Qatar
{pnakov,fherrera,svogel}@qf.org.qa
Abstract
While experimenting with tuning on long
sentences, we made an unexpected discov-
ery: that PRO falls victim to monsters ?
overly long negative examples with very
low BLEU+1 scores, which are unsuitable
for learning and can cause testing BLEU
to drop by several points absolute. We
propose several effective ways to address
the problem, using length- and BLEU+1-
based cut-offs, outlier filters, stochastic
sampling, and random acceptance. The
best of these fixes not only slay and pro-
tect against monsters, but also yield higher
stability for PRO as well as improved test-
time BLEU scores. Thus, we recommend
them to anybody using PRO, monster-
believer or not.
1 Once Upon a Time...
For years, the standard way to do statistical ma-
chine translation parameter tuning has been to
use minimum error-rate training, or MERT (Och,
2003). However, as researchers started using mod-
els with thousands of parameters, new scalable op-
timization algorithms such as MIRA (Watanabe et
al., 2007; Chiang et al, 2008) and PRO (Hopkins
and May, 2011) have emerged. As these algo-
rithms are relatively new, they are still not quite
well understood, and studying their properties is
an active area of research.
For example, Nakov et al (2012) have pointed
out that PRO tends to generate translations that
are consistently shorter than desired. They
have blamed this on inadequate smoothing in
PRO?s optimization objective, namely sentence-
level BLEU+1, and they have addressed the prob-
lem using more sensible smoothing. We wondered
whether the issue could be partially relieved sim-
ply by tuning on longer sentences, for which the
effect of smoothing would naturally be smaller.
To our surprise, tuning on the longer 50% of the
tuning sentences had a disastrous effect on PRO,
causing an absolute drop of three BLEU points
on testing; at the same time, MERT and MIRA
did not have such a problem. While investigating
the reasons, we discovered hundreds of monsters
creeping under PRO?s surface...
Our tale continues as follows. We first explain
what monsters are in Section 2, then we present a
theory about how they can be slayed in Section 3,
we put this theory to test in practice in Section 4,
and we discuss some related efforts in Section 5.
Finally, we present the moral of our tale, and we
hint at some planned future battles in Section 6.
2 Monsters, Inc.
PRO uses pairwise ranking optimization, where
the learning task is to classify pairs of hypotheses
into correctly or incorrectly ordered (Hopkins and
May, 2011). It searches for a vector of weights
w such that higher evaluation metric scores cor-
respond to higher model scores and vice versa.
More formally, PRO looks for weights w such that
g(i, j) > g(i, j?) ? hw(i, j) > hw(i, j?), where
g is a local scoring function (typically, sentence-
level BLEU+1) and hw are the model scores for
a given input sentence i and two candidate hy-
potheses j and j? that were obtained using w. If
g(i, j) > g(i, j?), we will refer to j and j? as the
positive and the negative example in the pair.
Learning good parameter values requires nega-
tive examples that are comparable to the positive
ones. Instead, tuning on long sentences quickly
introduces monsters, i.e., corrupted negative ex-
amples that are unsuitable for learning: they are
(i) much longer than the respective positive ex-
amples and the references, and (ii) have very low
BLEU+1 scores compared to the positive exam-
ples and in absolute terms. The low BLEU+1
means that PRO effectively has to learn from pos-
itive examples only.
12
Avg. Lengths Avg. BLEU+1
iter. pos neg ref. pos neg
1 45.2 44.6 46.5 52.5 37.6
2 46.4 70.5 53.2 52.8 14.5
3 46.4 261.0 53.4 52.4 2.19
4 46.4 250.0 53.0 52.0 2.30
5 46.3 248.0 53.0 52.1 2.34
. . . . . . . . . . . . . . . . . .
25 47.9 229.0 52.5 52.2 2.81
Table 1: PRO iterations, tuning on long sentences.
Table 1 shows an optimization run of PRO when
tuning on long sentences. We can see monsters
after iterations in which positive examples are on
average longer than negative ones (e.g., iter. 1).
As a result, PRO learns to generate longer sen-
tences, but it overshoots too much (iter. 2), which
gives rise to monsters. Ideally, the learning algo-
rithm should be able to recover from overshoot-
ing. However, once monsters are encountered,
they quickly start dominating, with no chance for
PRO to recover since it accumulates n-best lists,
and thus also monsters, over iterations. As a result,
PRO keeps jumping up and down and converges to
random values, as Figure 1 shows.
By default, PRO?s parameters are averaged
over iterations, and thus the final result is quite
mediocre, but selecting the highest tuning score
does not solve the problem either: for example,
on Figure 1, PRO never achieves a BLEU better
than that for the default initialization parameters.
?
?
? ?
?
?
?
? ?
?
? ? ? ? ? ? ? ? ? ? ? ?
?
? ?
5 10 15 20 250
10
20
30
40
iteration
BLE
U sc
ore
?
?
?
?
? ?
?
? ?
?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 1
2
3
4
5
Leng
th ra
tio
Figure 1: PRO tuning results on long sentences
across iterations. The dark-gray line shows the
tuning BLEU (left axis), the light-gray one is the
hypothesis/reference length ratio (right axis).
Figure 2 shows the translations after iterations
1, 3 and 4; the last two are monsters. The monster
at iteration 3 is potentially useful, but that at itera-
tion 4 is clearly unsuitable as a negative example.
Optimizer Objective BLEU
PRO sent-BLEU+1 44.57
MERT corpus-BLEU 47.53
MIRA pseudo-doc-BLEU 47.80
PRO (6= objective) pseudo-doc-BLEU 21.35
MIRA (6= objective) sent-BLEU+1 47.59
PRO, PC-smooth, ground fixed sent-BLEU+1 45.71
Table 2: PRO vs. MERT vs. MIRA.
We also checked whether other popular opti-
mizers yield very low BLEU scores at test time
when tuned on long sentences. Lines 2-3 in Ta-
ble 2 show that this is not the case for MERT and
MIRA. Since they optimize objectives that are dif-
ferent from PRO?s,1 we further experimented with
plugging MIRA?s objective into PRO and PRO?s
objective into MIRA. The resulting MIRA scores
were not much different from before, while PRO?s
score dropped even further; we also found mon-
sters. Next, we applied the length fix for PRO
proposed in (Nakov et al, 2012); this helped a
bit, but still left PRO two BLEU points behind
MERT2 and MIRA, and the monsters did not go
away. We can conclude that the monster problem
is PRO-specific, cannot be blamed on the objective
function, and is different from the length bias.
Note also that monsters are not specific to a
dataset or language pair. We found them when
tuning on the top-50% of WMT10 and testing on
WMT11 for Spanish-English; this yielded a drop
in BLEU from 29.63 (MERT) to 27.12 (PRO).
From run 110 /home/guzmanhe/NIST12/ems/preslav-mada-atb/tuning/tmp.110
**REF**: but we have to close ranks with each other and realize that in 
unity there is strength while in division there is weakness . 
-----------------------------------------------------
**IT1**: but we are that we add our ranks to some of us and that we know 
that in the strength and weakness in
**IT3**:, we are the but of the that that the , and , of ranks the the on 
the the our the our the some of we can include , and , of to the of we know 
the the our in of the of some people , force of the that that the in of the 
that that the the weakness Union the the , and
**IT4**: namely Dr Heba Handossah and Dr Mona been pushed aside because a 
larger story EU Ambassador to Egypt Ian Burg highlighted 've dragged us 
backwards and dragged our speaking , never balme your defaulting a December 
7th 1941 in Pearl Harbor ) we can include ranks will be joined by all 've 
dragged us backwards and dragged our $ 3.8 billion in tourism income 
proceeds Chamber are divided among themselves : some 've dragged us 
backwards and dragged our were exaggerated . Al @-@ Hakim namely Dr Heba 
Handossah and Dr Mona December 7th 1941 in Pearl Harbor ) cases might be 
known to us December 7th 1941 in Pearl Harbor ) platform depends on 
combating all liberal policies Track and Field Federation shortened strength 
as well face several challenges , namely Dr Heba Handossah and Dr Mona 
platform depends on combating all liberal policies the report forecast that 
the weak structure
**IT7**: , the sakes of our on and the , the we can include however , the Al 
ranks the the on the , to the = last of we , the long of the part of some of 
to the affect that the of some is the with ] us our to the affect that the 
with ] us our of the in baker , the cook , the on and the , the we know , 
has are in the heaven of to the affect that the of weakness of @-@ Ittihad 
@-@ Al the force , to 
Figure 2: Example reference translation and hy-
pothesis translations after iterations 1, 3 and 4.
The last two hypotheses are monsters.
1See (Cherry and Foster, 2012) for details on objectives.
2Also, using PRO to initialize MERT, as implemented in
Moses, yields 46.52 BLEU and monsters, but using MERT to
initialize PRO yields 47.55 and no monsters.
13
3 Slaying Monsters: Theory
Below we explain what monsters are and where
they come from. Then, we propose various mon-
ster slaying techniques to be applied during PRO?s
selection and acceptance steps.
3.1 What is PRO?
PRO is a batch optimizer that iterates between
(i) translation: using the current parameter values,
generate k-best translations, and (ii) optimization:
using the translations from all previous iterations,
find new parameter values. The optimization step
has four substeps:
1. Sampling: For each sentence, sample uni-
formly at random ? = 5000 pairs from the
set of all candidate translations for that sen-
tence from all previous iterations.
2. Selection: From these sampled pairs, select
those for which the absolute difference be-
tween their BLEU+1 scores is higher than
? = 0.05 (note: this is 5 BLEU+1 points).
3. Acceptance: For each sentence, accept the
? = 50 selected pairs with the highest abso-
lute difference in their BLEU+1 scores.
4. Learning: Assemble the accepted pairs for
all sentences into a single set and use it to
train a ranker to prefer the higher-scoring
sentence in each pair.
We believe that monsters are nurtured by PRO?s
selection and acceptance policies. PRO?s selec-
tion step filters pairs involving hypotheses that dif-
fer by less than five BLEU+1 points, but it does
not cut-off ones that differ too much based on
BLEU+1 or length. PRO?s acceptance step selects
? = 50 pairs with the highest BLEU+1 differ-
entials, which creates breeding ground for mon-
sters since these pairs are very likely to include
one monster and one good hypothesis.
Below we discuss monster slaying geared to-
wards the selection and acceptance steps of PRO.
3.2 Slaying at Selection
In the selection step, PRO filters pairs for which
the difference in BLEU+1 is less than five points,
but it has no cut-off on the maximum BLEU+1 dif-
ferentials nor cut-offs based on absolute length or
difference in length. Here, we propose several se-
lection filters, both deterministic and probabilistic.
Cut-offs. A cut-off is a deterministic rule that
filters out pairs that do not comply with some cri-
teria. We experiment with a maximal cut-off on
(a) the difference in BLEU+1 scores and (b) the
difference in lengths. These are relative cut-offs
because they refer to the pair, but absolute cut-offs
that apply to each of the elements in the pair are
also possible (not explored here). Cut-offs (a) and
(b) slay monsters by not allowing the negative ex-
amples to get much worse in BLEU+1 or in length
than the positive example in the pair.
Filtering outliers. Outliers are rare or extreme
observations in a sample. We assume normal dis-
tribution of the BLEU+1 scores (or of the lengths)
of the translation hypotheses for the same source
sentence, and we define as outliers hypotheses
whose BLEU+1 (or length) is more than ? stan-
dard deviations away from the sample average.
We apply the outlier filter to both the positive and
the negative example in a pair, but it is more im-
portant for the latter. We experiment with values
of ? like 2 and 3. This filtering slays monsters be-
cause they are likely outliers. However, it will not
work if the population gets riddled with monsters,
in which case they would become the norm.
Stochastic sampling. Instead of filtering ex-
treme examples, we can randomly sample pairs
according to their probability of being typical. Let
us assume that the values of the local scoring func-
tions, i.e., the BLEU+1 scores, are distributed nor-
mally: g(i, j) ? N(?, ?2). Given a sample of hy-
pothesis translations {j} of the same source sen-
tence i, we can estimate ? empirically. Then,
the difference ? = g(i, j) ? g(i, j?) would be
distributed normally with mean zero and variance
2?2. Now, given a pair of examples, we can calcu-
late their ?, and we can choose to select the pair
with some probability, according to N(0, 2?2).
3.3 Slaying at Acceptance
Another problem is caused by the acceptance
mechanism of PRO: among all selected pairs, it
accepts the top-? with the highest BLEU+1 dif-
ferentials. It is easy to see that these differentials
are highest for nonmonster?monster pairs if such
pairs exist. One way to avoid focusing primarily
on such pairs is to accept a random set of ? pairs,
among the ones that survived the selection step.
One possible caveat is that we can lose some of
the discriminative power of PRO by focusing on
examples that are not different enough.
14
TESTING TUNING (run 1, it. 25, avg.) TEST(tune:full)
Avg. for 3 reruns Lengths BLEU+1 Avg. for 3 reruns
PRO fix BLEU StdDev Pos Neg Ref Pos Neg BLEU StdDev
PRO (baseline) 44.70 0.266 47.9 229.0 52.5 52.2 2.8 47.80 0.052
Max diff. cut-off BLEU+1 max=10 ? 47.94 0.165 47.9 49.6 49.4 49.4 39.9 47.77 0.035
BLEU+1 max=20 ? 47.73 0.136 47.7 55.5 51.1 49.8 32.7 47.85 0.049
LEN max=5 ? 48.09 0.021 46.8 47.0 47.9 52.9 37.8 47.73 0.051
LEN max=10 ? 47.99 0.025 47.3 48.5 48.7 52.5 35.6 47.80 0.056
Outliers BLEU+1 ?=2.0 ? 48.05 0.119 46.8 47.2 47.7 52.2 39.5 47.47 0.090
BLEU+1 ?=3.0 47.12 1.348 47.6 168.0 53.0 51.7 3.9 47.53 0.038
LEN ?=2.0 46.68 2.005 49.3 82.7 53.1 52.3 5.3 47.49 0.085
LEN ?=3.0 47.02 0.727 48.2 163.0 51.4 51.4 4.2 47.65 0.096
Stoch. sampl. ? BLEU+1 46.33 1.000 46.8 216.0 53.3 53.1 2.4 47.74 0.035
? LEN 46.36 1.281 47.4 201.0 52.9 53.4 2.9 47.78 0.081
Table 3: Some fixes to PRO (select pairs with highest BLEU+1 differential, also require at least 5
BLEU+1 points difference). A dagger (?) indicates selection fixes that successfully get rid of monsters.
4 Attacking Monsters: Practice
Below, we first present our general experimental
setup. Then, we present the results for the var-
ious selection alternatives, both with the original
acceptance strategy and with random acceptance.
4.1 Experimental Setup
We used a phrase-based SMT model (Koehn et al,
2003) as implemented in the Moses toolkit (Koehn
et al, 2007). We trained on all Arabic-English
data for NIST 2012 except for UN, we tuned on
(the longest-50% of) the MT06 sentences, and we
tested on MT09. We used the MADA ATB seg-
mentation for Arabic (Roth et al, 2008) and true-
casing for English, phrases of maximal length 7,
Kneser-Ney smoothing, and lexicalized reorder-
ing (Koehn et al, 2005), and a 5-gram language
model, trained on GigaWord v.5 using KenLM
(Heafield, 2011). We dropped unknown words
both at tuning and testing, and we used minimum
Bayes risk decoding at testing (Kumar and Byrne,
2004). We evaluated the output with NIST?s scor-
ing tool v.13a, cased.
We used the Moses implementations of MERT,
PRO and batch MIRA, with the ?return-best-dev
parameter for the latter. We ran these optimizers
for up to 25 iterations and we used 1000-best lists.
For stability (Foster and Kuhn, 2009), we per-
formed three reruns of each experiment (tuning +
evaluation), and we report averaged scores.
4.2 Selection Alternatives
Table 3 presents the results for different selection
alternatives. The first two columns show the test-
ing results: average BLEU and standard deviation
over three reruns.
The following five columns show statistics
about the last iteration (it. 25) of PRO?s tuning
for the worst rerun: average lengths of the positive
and the negative examples and average effective
reference length, followed by average BLEU+1
scores for the positive and the negative examples
in the pairs. The last two columns present the re-
sults when tuning on the full tuning set. These are
included to verify the behavior of PRO in a non-
monster prone environment.
We can see in Table 3 that all selection mech-
anisms considerably improve BLEU compared to
the baseline PRO, by 2-3 BLEU points. However,
not every selection alternative gets rid of monsters,
which can be seen by the large lengths and low
BLEU+1 for the negative examples (in bold).
The max cut-offs for BLEU+1 and for lengths
both slay the monsters, but the latter yields much
lower standard deviation (thirteen times lower than
for the baseline PRO!), thus considerably increas-
ing PRO?s stability. On the full dataset, BLEU
scores are about the same as for the original PRO
(with small improvement for BLEU+1 max=20),
but the standard deviations are slightly better.
Rejecting outliers using BLEU+1 and ? = 3 is
not strong enough to filter out monsters, but mak-
ing this criterion more strict by setting ? = 2,
yields competitive BLEU and kills the monsters.
Rejecting outliers based on length does not
work as effectively though. We can think of two
possible reasons: (i) lengths are not normally dis-
tributed, they are more Poisson-like, and (ii) the
acceptance criterion is based on the top-? differ-
entials based on BLEU+1, not based on length.
On the full dataset, rejecting outliers, BLEU+1
and length, yields lower BLEU and less stability.
15
TESTING TUNING (run 1, it. 25, avg.) TEST(tune:full)
Avg. for 3 reruns Lengths BLEU+1 Avg. for 3 reruns
PRO fix BLEU StdDev Pos Neg Ref Pos Neg BLEU StdDev
PRO (baseline) 44.70 0.266 47.9 229.0 52.5 52.2 2.8 47.80 0.052
Rand. accept PRO, rand ?? 47.87 0.147 47.7 48.5 48.70 47.7 42.9 47.59 0.114
Outliers BLEU+1 ?=2.0, rand? 47.85 0.078 48.2 48.4 48.9 47.5 43.6 47.62 0.091
BLEU+1 ?=3.0, rand 47.97 0.168 47.6 47.6 48.4 47.8 43.6 47.44 0.070
LEN ?=2.0, rand? 47.69 0.114 47.8 47.8 48.6 47.9 43.6 47.48 0.046
LEN ?=3.0, rand 47.89 0.235 47.8 48.0 48.7 47.7 43.1 47.64 0.090
Stoch. sampl. ? BLEU+1, rand? 47.99 0.087 47.9 48.0 48.7 47.8 43.5 47.67 0.096
? LEN, rand? 47.94 0.060 47.8 47.9 48.6 47.8 43.6 47.65 0.097
Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (??) indicates that
random acceptance kills monsters. The asterisk (?) indicates improved stability over random acceptance.
Reasons (i) and (ii) arguably also apply to
stochastic sampling of differentials (for BLEU+1
or for length), which fails to kill the monsters,
maybe because it gives them some probability of
being selected by design. To alleviate this, we test
the above settings with random acceptance.
4.3 Random Acceptance
Table 4 shows the results for accepting training
pairs for PRO uniformly at random. To eliminate
possible biases, we also removed the min=0.05
BLEU+1 selection criterion. Surprisingly, this
setup effectively eliminated the monster problem.
Further coupling this with the distributional cri-
teria can also yield increased stability, and even
small further increase in test BLEU. For instance,
rejecting BLEU outliers with ? = 2 yields com-
parable average test BLEU, but with only half the
standard deviation.
On the other hand, using the stochastic sam-
pling of differentials based on either BLEU+1 or
lengths improves the test BLEU score while in-
creasing the stability across runs. The random
acceptance has a caveat though: it generally de-
creases the discriminative power of PRO, yielding
worse results when tuning on the full, nonmonster
prone tuning dataset. Stochastic selection does
help to alleviate this problem. Yet, the results are
not as good as when using a max cut-off for the
length. Therefore, we recommend using the latter
as a default setting.
5 Related Work
We are not aware of previous work that discusses
the issue of monsters, but there has been work on
a different, length problem with PRO (Nakov et
al., 2012). We have seen that its solution, fix the
smoothing in BLEU+1, did not work for us.
The stability of MERT has been improved using
regularization (Cer et al, 2008), random restarts
(Moore and Quirk, 2008), multiple replications
(Clark et al, 2011), and parameter aggregation
(Cettolo et al, 2011).
With the emergence of new optimization tech-
niques, there have been studies that compare sta-
bility between MIRA?MERT (Chiang et al, 2008;
Chiang et al, 2009; Cherry and Foster, 2012),
PRO?MERT (Hopkins and May, 2011), MIRA?
PRO?MERT (Cherry and Foster, 2012; Gimpel
and Smith, 2012; Nakov et al, 2012).
Pathological verbosity can be an issue when
tuning MERT on recall-oriented metrics such
as METEOR (Lavie and Denkowski, 2009;
Denkowski and Lavie, 2011). Large variance be-
tween the results obtained with MIRA has also
been reported (Simianer et al, 2012). However,
none of this work has focused on monsters.
6 Tale?s Moral and Future Battles
We have studied a problem with PRO, namely that
it can fall victim to monsters, overly long negative
examples with very low BLEU+1 scores, which
are unsuitable for learning. We have proposed sev-
eral effective ways to address this problem, based
on length- and BLEU+1-based cut-offs, outlier fil-
ters and stochastic sampling. The best of these
fixes have not only slayed the monsters, but have
also brought much higher stability to PRO as well
as improved test-time BLEU scores. These bene-
fits are less visible on the full dataset, but we still
recommend them to everybody who uses PRO as
protection against monsters. Monsters are inher-
ent in PRO; they just do not always take over.
In future work, we plan a deeper look at the
mechanism of monster creation in PRO and its
possible connection to PRO?s length bias.
16
References
Daniel Cer, Daniel Jurafsky, and Christopher Manning.
2008. Regularization and search for minimum error
rate training. In Proc. of Workshop on Statistical
Machine Translation, WMT ?08, pages 26?34.
Mauro Cettolo, Nicola Bertoldi, and Marcello Fed-
erico. 2011. Methods for smoothing the optimizer
instability in SMT. MT Summit XIII: the Machine
Translation Summit, pages 32?39.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL-HLT ?12, pages 427?436.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 224?233.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proc. of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, NAACL-HLT ?09, pages 218?226.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better hypothesis testing for statis-
tical machine translation: Controlling for optimizer
instability. In Proceedings of the Meeting of the As-
sociation for Computational Linguistics, ACL ?11,
pages 176?181.
Michael Denkowski and Alon Lavie. 2011. Meteor-
tuned phrase-based SMT: CMU French-English and
Haitian-English systems for WMT 2011. Techni-
cal report, CMU-LTI-11-011, Language Technolo-
gies Institute, Carnegie Mellon University.
George Foster and Roland Kuhn. 2009. Stabiliz-
ing minimum error rate training. In Proceedings
of the Workshop on Statistical Machine Translation,
StatMT ?09, pages 242?249.
Kevin Gimpel and Noah Smith. 2012. Structured ramp
loss minimization for machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL-HLT ?12, pages 221?231.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Workshop on Statistical
Machine Translation, WMT ?11, pages 187?197.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 1352?1362.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics on Human Language Technology, HLT-
NAACL ?03, pages 48?54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, IWSLT ?05.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the Meeting of the Association for Compu-
tational Linguistics, ACL ?07, pages 177?180.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
HLT-NAACL ?04, pages 169?176.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23:105?115.
Robert Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statisti-
cal machine translation. In Proceedings of the Inter-
national Conference on Computational Linguistics,
COLING ?08, pages 585?592.
Preslav Nakov, Francisco Guzma?n, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proceedings of the Inter-
national Conference on Computational Linguistics,
COLING ?12, pages 1979?1994.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Meeting of the Association for Computational Lin-
guistics, ACL ?03, pages 160?167.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic morphological
tagging, diacritization, and lemmatization using lex-
eme models and feature ranking. In Proceedings
of the Meeting of the Association for Computational
Linguistics, ACL ?08, pages 117?120.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the Meeting of the Association for
Computational Linguistics, ACL ?12, pages 11?21.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, EMNLP-CoNLL ?07, pages
764?773.
17
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 135?142
Manchester, August 2008
Context-based Arabic Morphological Analysis for Machine Translation
ThuyLinh Nguyen
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
thuylinh@cs.cmu.edu
Stephan Vogel
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
vogel@cs.cmu.edu
Abstract
In this paper, we present a novel morphol-
ogy preprocessing technique for Arabic-
English translation. We exploit the Arabic
morphology-English alignment to learn a
model removing nonaligned Arabic mor-
phemes. The model is an instance of
the Conditional Random Field (Lafferty et
al., 2001) model; it deletes a morpheme
based on the morpheme?s context. We
achieved around two BLEU points im-
provement over the original Arabic trans-
lation for both a travel-domain system
trained on 20K sentence pairs and a news
domain system trained on 177K sentence
pairs, and showed a potential improvement
for a large-scale SMT system trained on 5
million sentence pairs.
1 Introduction
Statistical machine translation (SMT) relies heav-
ily on the word alignment model of the source
and the target language. However, there is a
mismatch between a rich morphology language
(e.g Arabic, Czech) and a poor morphology lan-
guage (e.g English). An Arabic source word of-
ten corresponds to several English words. Pre-
vious research has focused on attempting to ap-
ply morphological analysis to machine translation
in order to reduce unknown words of highly in-
flected languages. Nie?en and Ney (2004) rep-
resented a word as a vector of morphemes and
gained improvement over word-based system for
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
German-English translation. Goldwater and Mc-
closky (2005) improved Czech-English translation
by applying different heuristics to increase the
equivalence of Czech and English text.
Specially for Arabic-English translation, Lee
(2004) used the Arabic part of speech and English
parts of speech (POS) alignment probabilities to
retain an Arabic affix, drop it from the corpus or
merge it back to a stem. The resulting system
outperformed the original Arabic system trained
on 3.3 million sentence pairs corpora when using
monotone decoding. However, an improvement
in monotone decoding is no guarantee for an im-
provement over the best baseline achievable with
full word forms. Our experiments showed that an
SMT phrase-based translation using 4 words dis-
tance reordering could gain four BLEU points over
monotone decoding. Sadat and Habash (2006) ex-
plored a wide range of Arabic word-level prepro-
cessing and produced better translation results for
a system trained on 5 million Arabic words.
What all the above methodologies do not pro-
vide is a means to disambiguate morphologi-
cal analysis for machine translation based on the
words? contexts. That is, for an Arabic word anal-
ysis of the form prefix*-stem-suffix* a morpheme
only is either always retained, always dropped off
or always merged to the stem regardless of its
surrounding text. In the example in Figure (1),
the Arabic word ?AlnAfi*h?(?window? in English)
was segmented as ?Al nAfi* ap?. The morpheme
?ap? is removed so that ?Al nAfi*? aligned to ?the
window? of the English sentence. In the sentence
?hl ldyk mqAEd bjwAr AlnAf*h ?? (?do you have
window tables ?? in English) the word ?AlnAfi*h?
is also segmented as ?Al nAfi* ap?. But in this
sentence, morphological preprocessing should re-
move both ?Al? and ?ap? so that only the remain-
135
nu  riyd  u  |||  mA}id  ap  |||  bi  jAnib  |||  Al  nAfi* ap  |||  .
we  want  to  have  a  table  near  the  window  .
nu  riyd  u  mA}id  ap  bi  jAnib  Al  nAfi* .
nryd   mA}dh   bjAnb   AlnAf*h   .
(c)
(a)
(b)
(d)
Figure 1: (a) Romanization of original Arabic sentence, (b) Output of morphological analysis toolkit?
words are separated by ?|||?, (c) English translation and its alignment with full morphological analysis
(d) Morphological analysis after removing unaligned morphemes.
ing morpheme ?nAfi*? aligned to the word ?win-
dow? of the English translation. Thus an appropri-
ate preprocessing technique should be guided by
English translation and bring the word context into
account.
In this paper we describe a context-based mor-
phological analysis for Arabic-English translation
that take full account morphemes alignment to En-
glish text. The preprocessing uses the Arabic mor-
phology disambiguation in (Smith et al, 2005) for
full morphological analysis and learns the remov-
ing morphemes model based on the Viterbi align-
ment of English to full morphological analysis. We
tested the model with two training corpora of 5.2
millions Arabic words(177K sentences) in news
domain and 159K Arabic words (20K sentences)
in travel conversation domain and gain improve-
ment over the original Arabic translation in both
experiments. The system that trained on a sub-
sample corpora of 5 millions sentence pairs cor-
pora also showed one BLEU score improvement
over the original Arabic system on unseen test set.
We will explain our technique in the next section
and briefly review the phrase based SMT model in
section 3. The experiment results will be presented
in section 4.
2 Methodology
We first preprocess the Arabic training corpus and
segment words into morpheme sequences of the
form prefix* stem suffix*. Stems are verbs, adjec-
tives, nouns, pronouns, etc., carrying the content
of the sentence. Prefixes and suffixes are func-
tional morphemes such as gender and case mark-
ers, prepositions, etc. Because case makers do not
exist in English, we remove case marker suffixes
from the morphology output. The output of this
process is a full morphological analysis corpus.
Even after removing case markers, the token count
of the full morphology corpus still doubles the
original Arabic?s word token count and is approx-
imately 1.7 times the number of tokens of the En-
glish corpus. As stated above, using original Ara-
bic for translation introduces more unknown words
in test data and causes multiple English words to
map to one Arabic word. At the morpheme level,
an English word would correspond to a morpheme
in the full morphology corpus but some prefixes
and suffixes in the full morphology corpus may not
be aligned with any English words at all. For ex-
ample, the Arabic article ?Al? (?the? in English)
prefixes to both adjectives and nouns, while En-
glish has only one determiner in a simple noun
phrase. Using the full morphological analysis cor-
pus for translation would introduce redundant mor-
phemes in the source side.
The goal of our morphological analysis method
for machine translation is removing nonaligned
prefixes and suffixes from the full morphology cor-
pus using a data-driven approach. We use the word
alignment output of the full morphology corpus to
the English corpus to delete morphemes in a sen-
tence. If an affix is not aligned to an English word
in the word alignment output, the affix should be
removed from the morphology corpus for better
one-to-one alignment of source and target corpora.
However, given an unseen test sentence, the En-
glish translation of the sentence is not available to
remove affixes based on the word alignment out-
put. We therefore learn a model removing non-
aligned morphemes from the full morphology Ara-
bic training corpus and its alignment to the English
corpus. To obtain consistency between training
corpus and test set, we applied the model to both
Arabic training corpus and test set, obtaining pre-
processed morphology corpora for the translation
task.
In this section, we will explain in detail each
steps of our preprocessing methodology:
136
? Apply word segmentation to the Arabic train-
ing corpus to get the full morphological anal-
ysis corpus.
? Annotate the full morphological analysis cor-
pus based on its word alignment to the En-
glish training corpus. We tag a morpheme as
?Deleted? if it should be removed from the
corpus, and ?Retained? otherwise.
? Learn the morphology tagger model.
? Apply the model to both Arabic training cor-
pus and Arabic test corpus to get prepro-
cessed corpus for translation.
2.1 Arabic Word Segmentation
Smith et al (2005) applies a source-channel model
to the problem of morphology disambiguation.
The source model is a uniform model that de-
fines the set of analyses. For Arabic morphology
disambiguation, the source model uses the list of
un-weighted word analyses generated by BAMA
toolkit (Buckwalter, 2004). The channel model
disambiguates the morphology alternatives. It is a
log-linear combination of features, which capture
the morphemes? context including tri-gram mor-
pheme histories, tri-gram part-of-speech histories
and combinations of the two.
The BAMA toolkit and hence (Smith et al,
2005) do not specify if a morpheme is an affix or
a stem in the output. Given a segmentation of an
original Arabic word, we considered a morpheme
a
i
as a stem if its parts of speech p
i
is either a
noun, pronoun, verb, adjective, question, punctua-
tion, number or abbreviation. A morpheme on the
left of its word?s stem is a prefix and it is a suffix
if otherwise. We removed case marker morphemes
and got the full morphology corpus.
2.2 Annotate Morphemes
To extract the Arabic morphemes that align to
English text, we use English as the source cor-
pus and aligned to Arabic morpheme corpus us-
ing GIZA++ (Och and Ney, 2003) toolkit. The
IBM3 and IBM4 (Brown et al, 1994) word align-
ment models select each word in the source sen-
tence, generate fertility and a list of target words
that connect to it. This generative process would
constrain source words to find alignments in the
target sentence. Using English as source corpus,
the alignment models force English words to gen-
erate their alignments in the Arabic morphemes.
GIZA++ outputs Viterbi alignment for every sen-
tence pair in the training corpus as depicted in (b)
and (c) of Figure (1). In our experiment, only 5%
of English words are not aligned to any Arabic
morpheme in the Viterbi alignment. From Viterbi
English-morpheme alignment output, we annotate
morphemes either to be deleted or retained as fol-
lows:
? Annotate stem morphemes as ?Retained?(R),
in dependant of word alignment output.
? Annotate a prefix or a suffix as ?Retained? (R)
if it is aligned to an English word.
? Annotate a prefix or a suffix as ?Deleted? (D)
if it is not aligned to an English word.
Note that the model does not assume that
GIZA++ outputs accurate word alignments. We
lessen the impact of the GIZA++ errors by only
using the word alignment output of prefix and suf-
fix morphemes.
Furthermore, because the full morphology sen-
tence is longer, each English word could align to a
separate morpheme. Our procedure of annotating
morphemes also constrains morphemes tagged as
?Retained? to be aligned to English words. Thus
if we remove ?Deleted? morphemes from the mor-
phology corpus, the reduced corpus and English
corpus have the property of one-to-one mapping
we prefer for source-target corpora in machine
translation.
2.3 Reduced Morphology Model
The reduced morphology corpus would be the
best choice of morphological analysis for machine
translation. Because it is impossible to tag mor-
phemes of a test sentence without the English ref-
erence based on Viterbi word alignment, we need
to learn a morpheme tagging model. The model
estimates the distributions of tagging sequences
given a morphologically analysed sentence using
the previous step?s annotated training data.
The task of tagging morphemes to be either
?Deleted? or ?Retained? belongs to the set of se-
quence labelling problems. The conditional ran-
dom fields (CRF) (Lafferty et al, 2001) model has
shown great benefits in similar applications of nat-
ural language processing such as part-of-speech
tagging, noun phrase chunking (Sha and Pereira,
2003), morphology disambiguation(Smith et al,
2005). We apply the CRF model to our morpheme
tagging problem.
137
Let A = {(A,T)} be the full morphology train-
ing corpus whereA = a
1
|p
1
a
2
|p
2
. . . a
m
|p
m
is a
morphology Arabic sentence, a
i
is a morpheme in
the sentence and p
i
is its POS;T = t
1
t
2
. . . t
m
is
the tag sequence of A, each t
i
is either ?Deleted?
or ?Retained? . The CRF model estimates param-
eter ?
?
maximizing the conditional probability of
the sequences of tags given the observed data:
?
?
= argmax
?
?
(A,T)?A
(1)
p? ((A,T)) log p
(
T|A, ?
)
where p? ((A,T)) is the empirical distribution of
the sentence (A,T) in the training data, ? are the
model parameters. The model?s log conditional
probability log p
(
T|A, ?
)
is the linear combina-
tion of feature weights:
log p
(
T|A, ?
)
=
?
k
?
k
f
k
((A
q
,T
q
)) (2)
The feature functions {f
k
} are defined on any sub-
set of the sentence A
q
? A and T
q
? T. CRFs
can accommodate many closely related features
of the input. In our morpheme tagging model,
we use morpheme features, part-of-speech features
and combinations of both. The features capture
the local contexts of morphemes. The lexical mor-
pheme features are the combinations of the current
morpheme and up to 2 previous and 2 following
morphemes. The part-of-speech features are the
combinations of the current part of speech and up
to 3 previous part of speeches. The part of speech,
morpheme combination features capture the de-
pendencies of current morphemes and up to its 3
previous parts of speech.
2.4 Preprocessed Data
Given a full morphology sentence A, we use the
morpheme tagging model learnt as described in the
previous section to decode A into the most proba-
ble sequence of tags T? = t
1
t
2
. . . t
m
.
T
?
= argmax
T
Pr
(
T|A, ?
?
)
(3)
If a t
i
is ?Deleted?, the morpheme a
i
is removed
from the morphology sentence A. The same pro-
cedure is applied to both training Arabic corpus
and test corpus to get preprocessed data for transla-
tion. We call a morphology sentence after remov-
ing ?Deleted? tag a reduced morphology sentence.
In our experiments, we used the freely available
CRF++1 toolkit to train and decode with the mor-
pheme tagging model. The CRF model smoothed
the parameters by assigning them Gaussian prior
distributions.
3 Phrase-based SMT System
We used the open source Moses (Koehn, 2007)
phrase-based MT system to test the impact of the
preprocessing technique on translation results. We
kept the default parameter settings of Moses for
translation model generation. The system used the
?grow-diag-final? alignment combination heuris-
tic. The phrase table consisted of phrase pairs up to
seven words long. The system used a tri-gram lan-
guage model built from SRI (Stolcke, 2002) toolkit
with modified Kneser-Ney interpolation smooth-
ing technique (Chen and Goodman, 1996). By de-
fault, the Moses decoder uses 6 tokens distance re-
ordering windows.
4 Experiment Results
In this section we present experiment results using
our Arabic morphology preprocessing technique.
4.1 Data Sets
We tested our morphology technique on a small
data set of 20K sentence pairs and a medium size
data set of 177K sentence pairs.
4.1.1 BTEC Data
As small training data set we used the BTEC
corpus (Takezawa et al, 2002) distributed by
the International Workshop on Spoken Language
Translation (IWSLT) (Eck and Hori, 2005). The
corpus is a collection of conversation transcripts
from the travel domain. Table 1 gives some de-
Arabic EngOri Full Reduced
Sentences 19972
Tokens 159K 258K 183K 183K
Types 17084 8207 8207 7298
Table 1: BTEC corpus statistics
tails for this corpus, which consists of nearly 20K
sentence pairs with lower case on the English side.
There is an imbalance of word types and word to-
kens between original Arabic and English. The
1http://crfpp.sourceforge.net/
138
original Arabic sentences are on average shorter
than the English sentences whereas the Arabic vo-
cabulary is more than twice the size of the English
vocabulary. The word segmentation reduced the
number of word types in the corpus to be closed
to English side but also increased word tokens
quite substantially. By removing nonaligned mor-
phemes, the reduced corpus is well balanced with
the English corpus.
The BTEC experiments used the 2004 IWSLT
Evaluation Test set as development set and 2005
IWSLT Evaluation Test set as unseen test data.
Table 2 gives the details of the two test sets. Both
of them had 16 reference translations per source
sentence. The English side of the training corpus
was used to build the language model. To optimize
the parameters of the decoder, we performed min-
imum error rate training on IWSLT04 optimizing
for the IBM-BLEU metric (Papineni et al, 2002).
4.1.2 Newswire Corpora
We also tested the impact of our morphology
technique on parallel corpus in the news domain.
The corpora were collected from LDC?s full Ara-
bic news translation corpora and a small portion
of UN data. The details of the data are give in
Table 3. The data consists of 177K sentence pairs,
5.2M words on the Arabic and 6M words on the
English side.
Arabic EngOri Full Reduced
Sentences 177035
Tokens 5.2M 9.3M 6.2M 6.2M
Types 155K 47K 47K 68K
Table 3: Newswire corpus statistics
We used two test sets from past NIST evalua-
tions as test data. NIST MT03 was used as devel-
opment set for optimizing parameters with respect
to the IBM-BLEU metric, NIST MT06 was used
as unseen test set. Both test sets have 4 references
per test sentence. Table 4 describes the data statis-
tics of the two test sets. All Newswire translation
experiments used the same language model esti-
mated from 200 million words collected from the
Xinhua section of the GIGA word corpus.
4.2 Translation Results
4.2.1 BTEC
We evaluated the machine translation accord-
ing to the case-insensitive BLEU metric. Table 5
shows the BTEC results when translated with de-
fault Moses setting of distance-based reordering
window size 6. The original Arabic word trans-
lation was the baseline of the evaluation. The
second row contains translation scores using the
full morphology translation. Our new technique of
context-based morphological analysis is shown in
the last row.
IWSLT04 IWSLT05
Ori 58.20 54.50
Full 58.55 55.87
Reduced 60.28 56.03
Table 5: BTEC translations results on IBM-BLEU
metrics(Case insensitive and 6 tokens distance re-
ordering window). The boldface marks scores sig-
nificantly higher than the original Arabic transla-
tion scores.
The full morphology translation performed sim-
ilar to the baseline on the development set but
outperformed the baseline on the unseen test set.
The reduced corpus showed significant improve-
ments over the baseline on the development set
(IWSLT04) and gave an additional small improve-
ment over the full morphology score over the un-
seen data (IWSLT05).
So why did the reduced morphology translation
not outperform more significantly the full mor-
phology translation on unseen set IWSLT05? To
analysis this in more detail, we selected good
full morphology translations and compared them
with the corresponding reduced morphology trans-
lations. Figure 2 shows one of these examples.
Typically, the reduced morphology translations
Figure 2: An example of BTEC translation output.
are shorter than both the references and the full
morphology outputs. Table 2 shows that for the
IWSLT05 test set, the ratio of the average En-
glish reference sentence length and the source sen-
139
IWSLT04 (Dev set) IWSLT05 (Unseen set)
Arabic English Arabic EnglishOri Full Reduced Ori Full Reduced
Sentences 500 8000 506 8096
Words 3261 5243 3732 64896 3253 5155 3713 66286
Avg Sent Length 6.52 10.48 7.46 8.11 6.43 10.19 7.34 8.18
Table 2: BTEC test set statistics
MT03 (Dev set) MT06 (Unseen set)
Arabic English Arabic EnglishOri Full Reduced Ori Full Reduced
Sentences 663 2652 1797 7188
Words 16268 27888 18888 79163 41059 71497 48716 222750
Avg Sent Length 24.53 42.06 28.49 29.85 22.85 39.79 27.1 30.98
Table 4: Newswire test set statistics
tence length is slightly higher than the correspond-
ing ratio for IWSLT04. Using the parameters op-
timised for IWSLT04 to translate IWSLT05 sen-
tences would generate hypotheses slightly shorter
than the IWSLT05 references resulting in brevity
penalties in the BLEU metric. The IWSLT05
brevity penalties for original Arabic, reduced mor-
phology and full morphology are 0.969, 0.978 and
0.988 respectively. Note that the BTEC corpus and
test sets are in the travel conversation domain, the
English reference sentences contain a large num-
ber of high frequency words. The full morpho-
logical analysis with additional prefixes and suf-
fixes outputs longer translations containing high
frequency words resulting in a high n-gram match
and lower BLEU brevity penalty. The reduced
translation method could generate translations that
are comparable but do not have the same effect on
BLEU metrics.
4.2.2 Newswire results
Table 6 presents the translation results for the
Newswire corpus. Even though morphology seg-
mentation reduced the number of unseen words,
the translation results of full morphological anal-
ysis are slightly lower than the original Arabic
scores in both development set MT03 and unseen
test set MT06. This is consistent with the result
achieved in previous literature (Sadat and Habash,
2006). Morphology preprocessing only helps with
small corpora, but the advantage decreases for
larger data sets.
Our context dependent preprocessing technique
MT03 MT06
Ori 45.55 32.09
Full 45.30 31.54
Reduced 47.69 34.13
Table 6: Newswire translation results on IBM-
BLEU metrics(Case insensitive and 6 tokens dis-
tance reordering wondow). The boldface marks
scores significantly higher than the original Arabic
translation?s scores.
shows significant improvements on both develop-
ment and unseen test sets. Moreover, while the ad-
vantage of morphology segmentation diminishes
for the full morphology translation, we achieve an
improvement of more than two BLEU points over
the original Arabic translations in both develop-
ment set and unseen test set.
4.3 Unknown Words Reduction
A clear advantage of using morphology based
translation over original word translation is the
reduction in the number of untranslated words.
Table 7 compares the number of unknown Arabic
tokens for original Arabic translation and reduced
morphology translation. In all the test sets, mor-
phology translations reduced the number of un-
known tokens by more than a factor of two.
4.4 The Impact of Reordering Distance Limit
The reordering window length is determined based
on the movements of the source phrases. On an
average, an original Arabic word has two mor-
140
Reorder Window 0 2 3 4 5 6 7 8 9
IWSLT04
Ori 57.21 57.92 58.01 58.31 58.16 58.20 58.20 58.12 58.01
Full 56.89 57.54 58.62 58.39 58.32 58.55 58.55 58.55 58.57
Reduced 58.36 59.56 60.05 60.70 60.32 60.28 60.46 60.30 60.55
MT03
Ori 41.75 43.84 45.24 45.61 45.40 45.55 45.21 45.22 45.19
Full 41.45 43.12 44.32 44.71 45.30 45.80 45.88 45.82
Reduced 44.08 45.28 46.50 47.40 47.41 47.69 47.59 47.75 47.79
Table 8: The impact of reordering limits on BTEC ?s development set IWSLT04 and Newswire?s devel-
opment set MT03. The translation scores are IBM-BLEU metric
Test Set Ori Reduced
IWSLT04 242 100
IWSLT05 219 97
MT03 1463 553
MT06 3734 1342
Table 7: Unknown tokens count
phemes. The full morphology translation with a
6-word reordering window has the same impact
as a 3-word reordering when translating the orig-
inal Arabic. To fully benefit from word reorder-
ing, the full morphology translation requires a
longer reorder distance limit. However, in current
phrase based translations, reordering models are
not strong enough to guide long distance source-
word movements. This shows an additional advan-
tage of the nonaligned morpheme removal tech-
nique.
We carried out experiments from monotone de-
coding up to 9 word distance reordering limit for
the two development sets IWSLT04 and MT03.
The results are given in Table 8. The BTEC data
set does not benefit from a larger reordering win-
dow. Using only a 2-word reordering window
the score of the original Arabic translations(57.92)
was comparable to the best score (58.31) obtained
by using a 4-word reordering window. On the
other hand, the reordering limit showed a signifi-
cant impact on Newswire data. The MT03 original
Arabic translation using a 4-word re-ordering win-
dow resulted in an improvement of 4 BLEU points
over monotone decoding. Large Arabic corpora
usually contain data from the news domain. The
decoder might not effectively reorder very long
distance morphemes for these data sets. This ex-
plains why machine translation does not benefit
from word-based morphological segmentation for
large data sets which adequately cover the vocabu-
lary of the test set.
4.5 Large Training Corpora Results
We wanted to test the impact of our preprocess-
ing technique on a system trained on 5 million
sentence pairs (128 million Arabic words). Un-
fortunately, the CRF++ toolkit exceeded memory
limits when executed even on a 24GB server. We
created smaller corpora by sub-sampling the large
corpus for the source side of MT03 and MT06
test sets. The sub-sampled corpus have 500K sen-
tence pairs and cover all source phrases of MT03
and MT06 which can be found in the large cor-
pus. In these experiments, we used a lexical re-
ordering model into translation model. The lan-
guage model was the 5-gram SRI language model
built from the whole GIGA word corpus. Table 9
MT03 MT06
5M Ori 56.22 42.17
Sub-sample Ori 54.54 41.59
Sub-sample Full 51.47 40.84
Sub-sample Reduced 54.78 43.20
Table 9: Translation results of large corpora(Case
insensitive, IBM-BLEU metric). The boldface
marks score significantly higher than the original
Arabic translation score.
presents the translation result of original Arabic
system trained on the full 5M sentence pairs cor-
pus and the three systems trained on the 500K sen-
tence pairs sub-sampled corpus. The sub-sampled
full morphology system scores degraded for both
development set and unseen test set. On devel-
opment set, the sub-sampled reduced morphology
system score was slightly better than baseline. On
the unseen test set, it significantly outperformed
both the baseline on sub-sampled training data and
even outperformed the system trained on the entire
141
5M sentence pairs.
5 Conclusion and Future Work
In this paper, we presented a context-dependent
morphology preprocessing technique for Arabic-
English translation. The model significantly out-
performed the original Arabic systems on small
and mid-size corpora and unseen test set on large
training corpora. The model treats morphology
processing task as a sequence labelling problem.
Therefore, other machine learning techniques such
as perceptron (Collins, 2002) could also be applied
for this problem.
The paper also discussed the relation between
the size of the reordering window and morphol-
ogy processing. In future investigations, we plan
to extend the model such that merging morphemes
is included. We also intent to study the impact of
phrase length and phrase extraction heuristics.
Acknowledgement
We thank Noah Smith for useful comments and
suggestions and providing us with the morphol-
ogy disambiguation toolkit. We also thank Sameer
Badaskar for help on editing the paper. We also
thank anonymous reviewers for helpful comments.
The research was supported by the GALE project.
References
Brown, Peter F., Stephen Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1994. The Mathematic of Statisti-
cal Machine Translation: Parameter Estimation. Compu-
tational Linguistics, 19(2):263?311.
Buckwalter, T. 2004. Arabic Morphological Analyzer ver-
sion 2.0. LDC2004L02.
Chen, Stanley F. and Joshua Goodman. 1996. An Empirical
Study of Smoothing Techniques for Language Modeling.
In Proceedings of the ACL, pages 310?318.
Collins, Michael. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments with
Perceptron Algorithms. In Proceedings of EMNLP ?02,
pages 1?8.
Eck, M. and C. Hori. 2005. Overview of the IWSLT 2005
Evaluation Campaign. In Proceedings of IWSLT, pages
11?17.
Goldwater, Sharon and David Mcclosky. 2005. Improv-
ing Statistical MT through Morphological Analysis. In
Proceedings of HLT/EMNLP, pages 676?683, Vancouver,
British Columbia, Canada.
Koehn, et al 2007. Moses: Open Source Toolkit for Sta-
tistical Machine Translation. In Annual Meeting of ACL,
demonstration session.
Lafferty, John, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Models
for Segmenting and Labeling Sequence Data. In Proceed-
ings of 18th ICML, pages 282?289.
Lee, Young S. 2004. Morphological Analysis for Statistical
Machine Translation. In HLT-NAACL 2004: Short Papers,
pages 57?60, Boston, Massachusetts, USA.
Nie?en, Sonja and Hermann Ney. 2004. Statistical Ma-
chine Translation with Scarce Resources Using Morpho-
Syntactic Information. Computational Linguistics, 30(2),
June.
Och, Franz Josef and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Evaluation
of Machine Translation. In Proceedings of the 40th ACL,
pages 311?318, Philadelphia.
Sadat, Fatiha and Nizar Habash. 2006. Combination of Ara-
bic Preprocessing Schemes for Statistical Machine Trans-
lation. In Proceedings of the ACL, pages 1?8, Sydney,
Australia. Association for Computational Linguistics.
Sha, Fei and Fernando Pereira. 2003. Shallow Parsing with
Conditional Random Fields. In Proceedings of NAACL
?03, pages 134?141, Morristown, NJ, USA.
Smith, Noah A., David A. Smith, and Roy W. Tromble. 2005.
Context-Based Morphological Disambiguation with Ran-
dom Fields. In Proceedings of HLT/EMNLP, pages 475?
482, Vancouver, British Columbia, Canada, October. As-
sociation for Computational Linguistics.
Stolcke, A. 2002. SRILM ? an Extensible Language Model-
ing Toolkit. In Intl. Conf. on Spoken Language Process-
ing.
Takezawa, Toshiyuki, Eiichiro Sumita, Fumiaki Sugaya, Hi-
rofumi Yamamoto, and Seiichi Yamamoto. 2002. Toward
a Broad-Coverage Bilingual Corpus for Speech Transla-
tion of Travel Conversations in the Real World. In Pro-
ceedings of LREC 2002, pages 147?152.
142
Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 10?17,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Active Semi-Supervised Learning for Improving Word Alignment
Vamshi Ambati, Stephan Vogel and Jaime Carbonell
{vamshi,vogel,jgc}@cs.cmu.edu
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
Word alignment models form an important
part of building statistical machine transla-
tion systems. Semi-supervised word align-
ment aims to improve the accuracy of auto-
matic word alignment by incorporating full
or partial alignments acquired from humans.
Such dedicated elicitation effort is often ex-
pensive and depends on availability of bilin-
gual speakers for the language-pair. In this
paper we study active learning query strate-
gies to carefully identify highly uncertain or
most informative alignment links that are pro-
posed under an unsupervised word alignment
model. Manual correction of such informative
links can then be applied to create a labeled
dataset used by a semi-supervised word align-
ment model. Our experiments show that using
active learning leads to maximal reduction of
alignment error rates with reduced human ef-
fort.
1 Introduction
The success of statistical approaches to Machine
Translation (MT) can be attributed to the IBM mod-
els (Brown et al, 1993) that characterize word-
level alignments in parallel corpora. Parameters of
these alignment models are learnt in an unsupervised
manner using the EM algorithm over sentence-level
aligned parallel corpora. While the ease of auto-
matically aligning sentences at the word-level with
tools like GIZA++ (Och and Ney, 2003) has enabled
fast development of statistical machine translation
(SMT) systems for various language pairs, the qual-
ity of alignment is typically quite low for language
pairs that diverge from the independence assump-
tions made by the generative models. Also, an im-
mense amount of parallel data enables better estima-
tion of the model parameters, but a large number of
language pairs still lack parallel data.
Two directions of research have been pursued for
improving generative word alignment. The first is to
relax or update the independence assumptions based
on more information, usually syntactic, from the
language pairs (Cherry and Lin, 2006). The sec-
ond is to use extra annotation, typically word-level
human alignment for some sentence pairs, in con-
junction with the parallel data to learn alignment in
a semi-supervised manner. Our research is in the
direction of the latter, and aims to reduce the effort
involved in hand-generation of word alignments by
using active learning strategies for careful selection
of word pairs to seek alignment.
Active learning for MT has not yet been explored
to its full potential. Much of the literature has ex-
plored one task ? selecting sentences to translate
and add to the training corpus (Haffari et al, 2009).
In this paper we explore active learning for word
alignment, where the input to the active learner is
a sentence pair (sJ1 , t
I
1), present in two different lan-
guages S = {s?} and T = {t?}, and the annotation
elicited from human is a set of links {(j, i) : j =
0 ? ? ? J ; i = 0 ? ? ? I}. Unlike previous approaches,
our work does not require elicitation of full align-
ment for the sentence pair, which could be effort-
intensive. We use standard active learning query
strategies to selectively elicit partial alignment infor-
mation. This partial alignment information is then
fed into a semi-supervised word aligner which per-
10
forms an improved word alignment over the entire
parallel corpus.
Rest of the paper is organized as follows. We
present related work in Section 2. Section 3 gives
an overview of unsupervised word alignment mod-
els and its semi-supervised improvisation. Section 4
details our active learning framework with discus-
sion of the link selection strategies in Section 5. Ex-
periments in Section 6 have shown that our selection
strategies reduce alignment error rates significantly
over baseline. We conclude with discussion on fu-
ture work.
2 Related Work
Semi-supervised learning is a broader area of Ma-
chine Learning, focusing on improving the learn-
ing process by usage of unlabeled data in conjunc-
tion with labeled data (Chapelle et al, 2006). Many
semi-supervised learning algorithms use co-training
framework, which assumes that the dataset has mul-
tiple views, and training different classifiers on a
non-overlapping subset of these features provides
additional labeled data (Zhu, 2005). Active query
selection for training a semi-supervised learning al-
gorithm is an interesting method that has been ap-
plied to clustering problems. Tomanek and Hahn
(2009) applied active semi supervised learning to
the sequence-labeling problem. Tur et al (2005) de-
scribe active and semi-supervised learning methods
for reducing labeling effort for spoken language un-
derstanding. They train supervised classification al-
gorithms for the task of call classification and apply
it to a large unlabeled dataset to select the least con-
fident instances for human labeling.
Researchers have begun to explore semi-
supervised word alignment models that use both
labeled and unlabeled data. Fraser and Marcu
(2006) pose the problem of alignment as a search
problem in log-linear space with features coming
from the IBM alignment models. The log-linear
model is trained on the available labeled data
to improve performance. They propose a semi-
supervised training algorithm which alternates
between discriminative error training on the la-
beled data to learn the weighting parameters and
maximum-likelihood EM training on unlabeled
data to estimate the parameters. Callison-Burch et
al. (2004) also improve alignment by interpolating
human alignments with automatic alignments. They
observe that while working with such datasets,
alignments of higher quality should be given a much
higher weight than the lower-quality alignments.
Wu et al (2006) learn separate models from labeled
and unlabeled data using the standard EM algo-
rithm. The two models are then interpolated as a
learner in the semi-supervised AdaBoost algorithm
to improve word alignment.
Active learning has been applied to various fields
of Natural Language Processing like statistical pars-
ing, entity recognition among others (Hwa, 2004;
Tang et al, 2001; Shen et al, 2004). In case of
MT, the potential of active learning has remained
largely unexplored. For Statistical Machine Transla-
tion, application of active learning has been focused
on the task of selecting the most informative sen-
tences to train the model, in order to reduce cost
of data acquisition. Recent work in this area dis-
cussed multiple query selection strategies for a Sta-
tistical Phrase Based Translation system (Haffari et
al., 2009). Their framework requires source text to
be translated by the system and the translated data
is used in a self-training setting to train MT models.
To our knowledge, we are not aware of any work
that has looked at reducing human effort by selec-
tive elicitation of alignment information using active
learning techniques.
3 Word Alignment
3.1 IBM models
IBM models provide a generative framework for
performing word alignment of parallel corpus.
Given two strings from source and target languages
sJ1 = s1, ? ? ? , sj , ? ? ? sJ and t
I
1 = t1, ? ? ? , ti, ? ? ? tI ,
an alignment A is defined as a subset of the Carte-
sian product of the word indices as shown in Eq 1.
In IBM models, since alignment is treated as a func-
tion, all the source positions must be covered exactly
once (Brown et al, 1993).
A ? {(j, i) : j = 0 ? ? ? J ; i = 0 ? ? ? I} (1)
For the task of translation, we would ideally want
to model P (sI1|t
J
1 ), which is the probability of ob-
serving source sentence sI1 given target sentence t
J
1 .
This requires a lot of parallel corpus for estimation
11
and so it is then factored over the word alignment
A for the sentence pair, which is a hidden variable.
Word alignment is therefore a by-product in the pro-
cess of modeling translation. We can also represent
the same under some parameterization of ?, which
is the model we are interested to estimate.
P (sJ1 |t
I
1) =
?
aJ1
Pr(sJ1 , A|t
J
1 ) (2)
=
?
A
p?(s
J
1 , A|t
I
1) (3)
Given a parallel corpus U of sentence pairs
{(sk, tk) : k = 1, ? ? ? ,K} the parameters can be
estimated by maximizing the conditional likelihood
over the data. IBM models (Brown et al, 1993) from
1 to 5 are different ways of factoring the probability
model to estimate the parameter set ?. For example
in the simplest of the models, IBM model 1, only the
lexical translation probability is considered treating
each word being translated independent of the other
words.
?? = argmax
?
K?
k=1
?
A
p?(sk, A|tk) (4)
The parameters of the model above are estimated
as ??, using the EM algorithm. We can also extract
the Viterbi alignment ,A?, for all the sentence pairs,
which is the alignment with the highest probability
under the current model parameters ?:
A? = argmax
A
p??(s
J
1 , A|t
I
1) (5)
The alignment models are asymmetric and dif-
fer with the choice of translation direction. We can
therefore perform the above after switching the di-
rection of the language pair and obtain models and
Viterbi alignments for the corpus as represented be-
low:
?? = argmax
?
K?
k=1
?
a
p?(tk, a|sk) (6)
A? = argmax
A
p??(t
I
1, A|s
J
1 ) (7)
Given the Viterbi alignment for each sentence
pair in the parallel corpus, we can also compute the
word-level alignment probabilities using simple rel-
ative likelihood estimation for both the directions.
As we will discuss in Section 5, the alignments and
the computed lexicons form an important part of our
link selection strategies.
P (sj/ti) =
?
s count(ti, sj ; A?)?
s count(ti)
(8)
P (ti/sj) =
?
s count(ti, sj ; A?)?
s count(sj)
(9)
We perform all our experiments on a symmetrized
alignment that combines the bidirectional align-
ments using heuristics as discussed in (Koehn et al,
2007). We represent this alignment as A = {aij :
i = 0 ? ? ? J ? sJ1 ; j = 0 ? ? ? I ? t
I
1}.
3.2 Semi-Supervised Word Alignment
We use an extended version of MGIZA++ (Gao
and Vogel, 2008) to perform the constrained semi-
supervised word alignment. To get full benefit
from the manual alignments, MGIZA++ modifies all
alignment models used in the standard training pro-
cedure, i.e. the IBM1, HMM, IBM3 and IBM4 mod-
els. Manual alignments are incorporated in the EM
training phase of these models as constraints that
restrict the summation over all possible alignment
paths. Typically in the EM procedure for IBM mod-
els, the training procedure requires for each source
sentence position, the summation over all positions
in the target sentence. The manual alignments al-
low for one-to-many alignments and many-to-many
alignments in both directions. For each position i
in the source sentence, there can be more than one
manually aligned target word. The restricted train-
ing will allow only those paths, which are consistent
with the manual alignments. Therefore, the restric-
tion of the alignment paths reduces to restricting the
summation in EM.
4 Active Learning for Word Alignment
Active learning attempts to optimize performance
by selecting the most informative instances to la-
bel, where ?informativeness? is defined as maximal
expected improvement in accuracy. The objective
is to select optimal instance for an external expert
to label and then run the learning method on the
newly-labeled and previously-labeled instances to
minimize prediction or translation error, repeating
until either the maximal number of external queries
12
is reached or a desired accuracy level is achieved.
Several studies (Tong and Koller, 2002; Nguyen
and Smeulders, 2004; Donmez and Carbonell, 2008)
show that active learning greatly helps to reduce the
labeling effort in various classification tasks.
We discuss our active learning setup for word
alignment in Algorithm 1. We start with an un-
labeled dataset U = {(Sk, Tk)}, indexed by k,
and a seed pool of partial alignment links A0 =
{akij , ?si ? Sk, tj ? Tk}. Each a
k
ij represents an
alignment link from a sentence pair k that connects
source word si with tj .
This is usually an empty set at iteration t = 0. We
iterate for T iterations. We take a pool-based active
learning strategy, where we have access to all the au-
tomatically aligned links and we can score the links
based on our active learning query strategy. The
query strategy uses the automatically trained align-
ment model ?t from the current iteration t, for scor-
ing the links. Re-training and re-tuning an SMT sys-
tem for each link at a time is computationally infea-
sible. We therefore perform batch learning by se-
lecting a set of N links scored high by our query
strategy. We seek manual corrections for the se-
lected links and add the alignment data to the cur-
rent labeled dataset. The word-level aligned labeled
dataset is then provided to our semi-supervised word
alignment algorithm, which uses it to produces the
alignment model ?t+1 for U .
Algorithm 1 AL FOR WORD ALIGNMENT
1: Unlabeled Data Set: U = {(sk, tk)}
2: Manual Alignment Set : A0 = {akij ,?si ?
Sk, tj ? Tk}
3: Train Semi-supervised Word Alignment using
(U , A0)? ?0
4: N : batch size
5: for t = 0 to T do
6: Lt = LinkSelection(U ,At,?t,N )
7: Request Human Alignment for Lt
8: At+1 = At + Lt
9: Re-train Semi-Supervised Word Align-
ment on (U,At+1)? ?t+1
10: end for
We can iteratively perform the algorithm for a de-
fined number of iterations T or until a certain desired
performance is reached, which is measured by align-
ment error rate (AER) (Fraser and Marcu, 2007) in
the case of word alignment. In a more typical sce-
nario, since reducing human effort or cost of elici-
tation is the objective, we iterate until the available
budget is exhausted.
5 Query Strategies for Link Selection
We propose multiple query selection strategies for
our active learning setup. The scoring criteria is
designed to select alignment links across sentence
pairs that are highly uncertain under current au-
tomatic translation models. These links are diffi-
cult to align correctly by automatic alignment and
will cause incorrect phrase pairs to be extracted in
the translation model, in turn hurting the transla-
tion quality of the SMT system. Manual correction
of such links produces the maximal benefit to the
model. We would ideally like to elicit the least num-
ber of manual corrections possible in order to reduce
the cost of data acquisition. In this section we dis-
cuss our link selection strategies based on the stan-
dard active learning paradigm of ?uncertainty sam-
pling?(Lewis and Catlett, 1994). We use the au-
tomatically trained translation model ?t for scoring
each link for uncertainty. In particular ?t consists of
bidirectional lexicon tables computed from the bidi-
rectional alignments as discussed in Section 3.
5.1 Uncertainty based: Bidirectional
Alignment Scores
The automatic Viterbi alignment produced by the
alignment models is used to obtain translation lexi-
cons, as discussed in Section 3. These lexicons cap-
ture the conditional distributions of source-given-
target P (s/t) and target-given-source P (t/s) prob-
abilities at the word level where si ? S and tj ? T .
We define certainty of a link as the harmonic mean
of the bidirectional probabilities. The selection strat-
egy selects the least scoring links according to the
formula below which corresponds to links with max-
imum uncertainty:
Score(aij/sI1, t
J
1 ) =
2 ? P (tj/si) ? P (si/tj)
P (tj/si) + P (si/tj)
(10)
5.2 Confidence Based: Posterior Alignment
probabilities
Confidence estimation for MT output is an interest-
ing area with meaningful initial exploration (Blatz
13
et al, 2004; Ueffing and Ney, 2007). Given a sen-
tence pair (sI1, t
J
1 ) and its word alignment, we com-
pute two confidence metrics at alignment link level ?
based on the posterior link probability and a simple
IBM Model 1 as seen in Equation 13. We select the
alignment links that the initial word aligner is least
confident according to our metric and seek manual
correction of the links. We use t2s to denote com-
putation using higher order (IBM4) target-given-
source models and s2t to denote source-given-target
models. Targeting some of the uncertain parts of
word alignment has already been shown to improve
translation quality in SMT (Huang, 2009). In our
current work, we use confidence metrics as an ac-
tive learning sampling strategy to obtain most infor-
mative links. We also experiment with other con-
fidence metrics as discussed in (Ueffing and Ney,
2007), especially the IBM 1 model score metric
which showed some improvement as well.
Pt2s(aij , tJ1 /s
I
1) =
pt2s(tj/si, aij ? A)
?M
i pt2s(tj/si)
(11)
Ps2t(aij , sI1/t
J
1 ) =
ps2t(si/tj , aij ? A)
?N
i pt2s(tj/si)
(12)
Conf(aij/S, T ) =
2 ? Pt2s ? Ps2t
Pt2s + Ps2t
(13)
5.3 Agreement Based: Query by Committee
The generative alignments produced differ based on
the choice of direction of the language pair. We use
As2t to denote alignment in the source to target di-
rection and At2s to denote the target to source direc-
tion. We consider these alignments to be two experts
that have two different views of the alignment pro-
cess. We formulate our query strategy to select links,
where the agreement differs across these two align-
ments. In general query by committee is a standard
sampling strategy in active learning(Freund et al,
1997), where the committee consists of any number
of experts with varying opinions, in this case align-
ments in different directions. We formulate a query
by committee sampling strategy for word alignment
as shown in Equation 14. In order to break ties, we
extend this approach to select the link with higher
average frequency of occurrence of words involved
in the link.
Language Sentences Words
Src Tgt
Ch-En 21,863 424,683 524,882
Ar-En 29,876 630,101 821,938
Table 1: Corpus Statistics of Human Data
Alignment Automatic Links Manual Links
Ch-En 491,887 588,075
Ar-En 786,223 712,583
Table 2: Alignment Statistics of Human Data
Score(aij) = ? where (14)
? =
?
?
?
2 aij ? At2s ?At2s
1 aij ? At2s ?At2s
0 otherwise
6 Experiments
6.1 Data Analysis
To run our active learning and semi-supervised word
alignment experiments iteratively, we simulate the
setup by using a parallel corpus for which the
gold standard human alignment is already available.
We experiment with two language pairs - Chinese-
English and Arabic-English. Corpus-level statistics
for both language pairs can be seen in Table 1 and
their alignment link level statistics can be seen in
Table 2. Both datasets were released by LDC as part
of the GALE project.
Chinese-English dataset consists of 21,863 sen-
tence pairs with complete manual alignment. The
human alignment for this dataset is much denser
than the automatic word alignment. On an aver-
age each source word is linked to more than one
target word. Similarly, the Arabic-English dataset
consisting of 29,876 sentence pairs also has a denser
manual alignment. Automatic word alignment in
both cases was computed as a symmetrized version
of the bidirectional alignments obtained from using
GIZA++ (Och and Ney, 2003) in each direction sep-
arately.
6.2 Word Alignment Results
We first perform an unsupervised word alignment of
the parallel corpus. We then use the learned model
14
Figure 1: Chinese-English: Link Selection Results
in running our link selection algorithm over the en-
tire alignments to determine the most uncertain links
according to each active learning strategy. The links
are then looked up in the gold standard human align-
ment database and corrected. In scenarios where
an alignment link is not present in the gold stan-
dard data for the source word, we introduce a NULL
alignment constraint, else we select all the links as
given in the gold standard. The aim of our work is to
show that active learning can help in selecting infor-
mative alignment links, which if manually labeled
can reduce the overall alignment error rate of the
given corpus. We, therefore measure the reduction
of alignment error rate (AER) of a semi-supervised
word aligner that uses this extra information to align
the corpus. We plot performance curves for both
Chinese-English, Figure 1 and Arabic-English, Fig-
ure 2, with number of manual links elicited on x-axis
and AER on y-axis. In each iteration of the experi-
ment, we gradually increase the number of links se-
lected from gold standard and make them available
to the semi-supervised word aligner and measure the
overall reduction of AER on the corpus. We com-
pare our link selection strategies to a baseline ap-
proach, where links are selected at random for man-
ual correction.
All our approaches perform equally or better than
the baseline for both language pairs. Query by
committee (qbc) performs similar to the baseline in
Chinese-English and only slightly better for Arabic-
Figure 2: Arabic-English: Link Selection Results
English. This could be due to our committee con-
sisting of two alignments that differ only in direc-
tion and so are not sufficient in deciding for uncer-
tainty. We will be exploring alternative formulations
to this strategy. Confidence based and uncertainty
based metrics perform significantly better than the
baseline in both language pairs. We can interpret the
improvements in two ways. For the same number
of manual alignments elicited, our selection strate-
gies select links that provide higher reduction of er-
ror when compared to the baseline. An alternative
interpretation is that assuming a uniform cost per
link, our best selection strategy achieves similar per-
formance to the baseline, at a much lower cost of
elicitation.
6.3 Translation Results
We also perform end-to-end machine translation ex-
periments to show that our improvement of align-
ment quality leads to an improvement of translation
scores. For Chinese-English, we train a standard
phrase-based SMT system (Koehn et al, 2007) over
the available 21,863 sentences. We tune on the MT-
Eval 2004 dataset and test on a subset of MT-Eval
2005 dataset consisting of 631 sentences. The lan-
guage model we use is built using only the English
side of the parallel corpus. We understand that this
language model is not the optimal choice, but we
are interested in testing the word alignment accu-
racy, which primarily affects the translation model.
15
Cn-En BLEU METEOR
Baseline 18.82 42.70
Human Alignment 19.96 44.22
Active Selection 20% 19.34 43.25
Table 3: Effect of Alignment on Translation Quality
We first obtain the baseline score by training in an
unsupervised manner, where no manual alignment
is used. We also train a configuration, where we
substitute the final word alignment with gold stan-
dard manual alignment for the entire parallel corpus.
This is an upper bound on the translation accuracy
that can be achieved by any alignment link selec-
tion algorithm for this dataset. We now take our
best link selection criteria, which is the confidence
based method and re-train the MT system after elic-
iting manual information for only 20% of the align-
ment links. We observe that at this point we have
reduced the AER from 37.09 to 26.57. The trans-
lation accuracy reported in Table 3, as measured by
BLEU (Papineni et al, 2002) and METEOR (Lavie
and Agarwal, 2007), also shows significant improve-
ment and approaches the quality achieved using gold
standard data. We did not perform MT experiments
with Arabic-English dataset due to the incompatibil-
ity of tokenization schemes between the manually
aligned parallel corpora and publicly available eval-
uation sets.
7 Conclusion
Word-Alignment is a particularly challenging prob-
lem and has been addressed in a completely unsuper-
vised manner thus far (Brown et al, 1993). While
generative alignment models have been successful,
lack of sufficient data, model assumptions and lo-
cal optimum during training are well known prob-
lems. Semi-supervised techniques use partial man-
ual alignment data to address some of these issues.
We have shown that active learning strategies can
reduce the effort involved in eliciting human align-
ment data. The reduction in effort is due to care-
ful selection of maximally uncertain links that pro-
vide the most benefit to the alignment model when
used in a semi-supervised training fashion. Experi-
ments on Chinese-English have shown considerable
improvements.
8 Future Work
In future, we wish to work with word alignments for
other language pairs as well as study the effect of
manual alignments by varying the size of available
parallel data. We also plan to obtain alignments from
non-experts over online marketplaces like Amazon
Mechanical Turk to further reduce the cost of an-
notation. We will be experimenting with obtain-
ing full-alignment vs. partial alignment from non-
experts. Our hypothesis is that, humans are good
at performing tasks of smaller size and so we can
extract high quality alignments in the partial align-
ment case. Cost of link annotation in our current
work is assumed to be uniform, but this needs to
be revisited. We will also experiment with active
learning techniques for identifying sentence pairs
with very low alignment confidence, where obtain-
ing full-alignment is equivalent to obtaining multi-
ple partial alignments.
Acknowledgments
This research was partially supported by DARPA
under grant NBCHC080097. Any opinions, find-
ings, and conclusions expressed in this paper are
those of the authors and do not necessarily reflect the
views of the DARPA. The first author would like to
thank Qin Gao for the semi-supervised word align-
ment software and help with running experiments.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of Coling 2004,
pages 315?321, Geneva, Switzerland, Aug 23?Aug
27. COLING.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, David Talbot, and Miles Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In ACL 2004, page
175, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
16
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discriminative
training. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 105?112, Morris-
town, NJ, USA.
Pinar Donmez and Jaime G. Carbonell. 2008. Opti-
mizing estimated loss reduction for active sampling in
rank learning. In ICML ?08: Proceedings of the 25th
international conference on Machine learning, pages
248?255, New York, NY, USA. ACM.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
ACL-44: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 769?776, Morristown, NJ, USA.
Association for Computational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33(3):293?303.
Yoav Freund, Sebastian H. Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the query
by committee algorithm. Machine. Learning., 28(2-
3):133?168.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based ma-
chine translation. In Proceedings of HLT NAACL
2009, pages 415?423, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Fei Huang. 2009. Confidence measure for word align-
ment. In Proceedings of the Joint ACL and IJCNLP,
pages 932?940, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Comput. Linguist., 30(3):253?276.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL Demonstration Session.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In WMT 2007,
pages 228?231, Morristown, NJ, USA.
David D. Lewis and Jason Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning. In In
Proceedings of the Eleventh International Conference
on Machine Learning, pages 148?156. Morgan Kauf-
mann.
Hieu T. Nguyen and Arnold Smeulders. 2004. Active
learning using pre-clustering. In ICML.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, pages 19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL 2002, pages 311?
318, Morristown, NJ, USA.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In ACL ?04: Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 589, Morristown, NJ,
USA. Association for Computational Linguistics.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2001. Ac-
tive learning for statistical natural language parsing. In
ACL ?02, pages 120?127, Morristown, NJ, USA.
Katrin Tomanek and Udo Hahn. 2009. Semi-supervised
active learning for sequence labeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
1039?1047, Suntec, Singapore, August. Association
for Computational Linguistics.
Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text clas-
sification. Journal of Machine Learning, pages 45?66.
Gokhan Tur, Dilek Hakkani-Tr, and Robert E. Schapire.
2005. Combining active and semi-supervised learning
for spoken language understanding. Speech Commu-
nication, 45(2):171 ? 186.
Nicola Ueffing and Hermann Ney. 2007. Word-level
confidence estimation for machine translation. Com-
put. Linguist., 33(1):9?40.
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2006. Boosting
statistical word alignment using labeled and unlabeled
data. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 913?920, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
X. Zhu. 2005. Semi-Supervised Learning Lit-
erature Survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
17
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 30?34,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Consensus versus Expertise : A Case Study of Word Alignment with
Mechanical Turk
Qin Gao and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh PA, 15213
{qing,stephan.vogel}@cs.cmu.edu
Abstract
Word alignment is an important preprocessing step
for machine translation. The project aims at incorpo-
rating manual alignments from Amazon Mechanical
Turk (MTurk) to help improve word alignment qual-
ity. As a global crowdsourcing service, MTurk can
provide flexible and abundant labor force and there-
fore reduce the cost of obtaining labels. An easy-
to-use interface is developed to simplify the labeling
process. We compare the alignment results by Turk-
ers to that by experts, and incorporate the alignments
in a semi-supervised word alignment tool to improve
the quality of the labels. We also compared two pric-
ing strategies for word alignment task. Experimental
results show high precision of the alignments pro-
vided by Turkers and the semi-supervised approach
achieved 0.5% absolute reduction on alignment error
rate.
1 Introduction
Word alignment is used in various natural language
processing tasks. Most state-of-the-art statistical machine
translation systems rely on word alignment as a prepro-
cessing step. The quality of word alignment is usually
measured by AER, which is loosely related to BLEU
score (Lopez and Resnik, 2006). There has been re-
search on utilizing manually aligned corpus to assist auto-
matic word alignment, and obtains encouraging results on
alignment error rate. (Callison-Burch et al, 2004; Blun-
som and Cohn, 2006; Fraser and Marcu, 2006; Niehues
and Vogel, 2008; Taskar et al, 2005; Liu et al, 2005;
Moore, 2005). However, how to obtain large amount of
alignments with good quality is problematic. Labeling
word-aligned parallel corpora requires significant amount
of labor. In this paper we explore the possibility of us-
ing Amazon Mechanical Turk (MTurk) to obtain manual
word alignment faster, cheaper, with high quality.
Crowdsourcing is a way of getting random labor force
on-line with low cost. MTurk is one of the leading
providers for crowdsourcing marketplace. There have
been several research papers on using MTurk to help nat-
ural language processing tasks, Callison-Burch (2009)
used MTurk to evaluate machine translation results. Kit-
tur et al (2008) showed the importance of validation
data set, the task is evaluating quality of Wikipedia arti-
cles. There are also experiments use the annotation from
MTurk in place of training data. For example (Kaisser et
al., 2008) and (Kaisser and Lowe, 2008) used MTurk to
build question answering datasets and choose summary
lengths that suite the need of the users.
Word alignment is a relatively complicate task for in-
experienced workers. The fact puts us in a dilemma,
we can either provide lengthy instructions and train the
workers, or we must face the problem that workers may
have their own standards. The former solution is im-
practical in the context of crowdsourcing because heavily
trained workers will expect higher payment, which de-
feats economical nature of crowdsourcing. Therefore we
are forced to face the uncertainty, and ask ourselves the
following questions: First, how consistent would the la-
bels from random labelers be, given minimal or no in-
structions? Second, how consistent would these intuitive
labels be consistent with the labels from expert labelers?
Third, if there is certain level of consistency between the
intuitive labels and the labels from experts, can we extract
most reliable links from the former? Last but not least,
given the alignment links, can we utilize them to help au-
tomatic word alignment without further human efforts?
The statistics on the data we get shows the internal
consistency among multiple MTurk alignments is greater
than 70%, and the precision is greater than 84% when
consider all the links. By applying majority vote and
consensus strategies, we can select links that have greater
than 95% accuracy. When applying the alignment links
on a new aligner that can perform constrained EM al-
gorithm for IBM models we observe 0.5% absolute im-
provements on alignment error rate. The average per-
word cost is about 2 cent per word.
The paper will be organized as follows, first we will
discuss the design principle of the task and the implemen-
tation of the application for word alignment in section
2. Section 3 describes the algorithm used in utilizing the
manual alignments. Section 4 presents the analysis on the
harvested data and the expert labels, and the the experi-
ment results of semi-supervised word alignment. Section
5 concludes the paper.
30
2 Design of the task
In this task, we want to collect manual word alignment
data from MTurk workers, Figure 2 shows an example of
word alignment. There are two sentences which are trans-
lation of each other. There are links between words in two
sentences, indicating the words are translation pairs. No-
tice that one word can be aligned to zero or more words,
if a word is aligned to zero word, we can assume it is
aligned to a virtual empty word. Therefore, given a sen-
tence pair, we want workers to link words in source sen-
tence to one or more target words or the empty word.
In our experiment, we use a Chinese-English parallel
corpus and ask workers to alignment the words in Chi-
nese sentence to the words in English sentence. We do
not provide any alignment links from automatic aligner.
2.1 Guidelines of design
MTurk represents a new pattern of market that has
yet be thoroughly studied. Mason and Watts (2009)
shows that higher payment does not guarantee results
with higher quality. Also, one should be aware that the
web-based interface is vulnerable to automatic scripts
that generate highly consistent yet meaningless results.
To ensure a better result, several measures must be com-
bined: 1) Require workers to take qualifications before
they can accept the tasks. 2) Implement an interface less
vulnerable to automatic scripts. 3) Build quality control
mechanism that filters inaccurate results, and finally 4)
Redesign the interface so that the time spent by careful
and careless workers does not differ too much, so there
is less incentives for workers to submit random results.
With these guidelines in mind, we put together several
elements into the HIT.
Qualifications
We require the workers to take qualifications, which
requires them to pick correct translation of five Chinese
words. The Chinese word is rendered in bitmap.
Interface implementation
We implemented the word alignment interface on
top of Google Web Toolkit, which enables developing
Javascript based Web application in Java. Because
all the content of the interface, including the content in
the final result, is generated dynamically in the run time,
it is much more difficult to hack than plain HTML forms.
Figure 1 shows a snapshot of the interface1. The labeling
procedure requires only mouse click. The worker need
to label all the words with a golden background2. To
complete the task, the worker needs to: 1) Click on the
1A demo of the latest version can be found at http://
alt-aligner.appspot.com, the source code of the
aligner is distributed under Apache License 2.0 on http://
code.google.com/p/alt-aligner/
2If the document is printed in greyscale, the lightest background (ex-
cept the white one) is actually golden, the second lightest one is red and
the darkest one is dark blue.
Chinese word he want to label. 2) Click on the English
words he want the Chinese word to be linked, or click on
the empty word to the end of the sentence. 3) If he want to
delete a link, he need to click on the English word again,
otherwise he can move on to next unlabeled word, or to
modify links on another labeled word. 4) Only when all
required words are labeled, the user would be allowed to
click on submit button.
The interface has two more functionalities, first, it al-
lows to specify a subset of words in the sentence for user
to label, as shown in the snapshot, words with white back-
ground are not required to label. Secondly it supports
providing initial alignment on the sentence.
Quality control
Quality control is a crucial component of the system.
For problems that have clear gold standard answers to a
portion of data, the quality control can be done by min-
gling the known into the unknown, and rejecting the sub-
missions with low qualities on known samples. However
in our situation it is not easy to do so because although
we have fully manual aligned sentences, we do not have
corpus in which the sentences are partially aligned, there-
fore if we want to use the method we have to let worker
label an additional sentence, which may double the effort
for the workers. Also we do not provide thorough stan-
dard for users, therefore before we know the divergence
of the alignments, we actually do not know how to set the
threshold, even with given gold standard labels. In addi-
tion, if the method will be applied on languages with low
resource, we cannot assume availability of gold standard
answers. Therefore, we only try to filter out answers base
on the consensus. The quality control works as follows.
Firstly we assign an alignment task to 2n + 1 workers.
For these submissions, we first try to build a majority an-
swer from these assignments. For each alignment link,
if it appears in more than n submissions. Then every in-
dividual assignments will be compared to the majority
alignment, so we can get the precision and recall rates.
If either precision or recall rate is lower than a threshold,
we will reject the submission.
Figure 1: A snapshot of the labeling interface.
2.2 Pricing and worker base
We tried two pricing strategies. The first one fixes the
number of words that a worker need to label for each
HIT, and fix the rate for each HIT. The second one always
31
asks workers to label every word in the sentence, in the
mean time we vary the rate for each HIT according to the
lengths of source sentences. For each strategy we tried
different rates, starting from 10 words per cent. However
we did not get enough workers even after the price raised
to 2 words per cent. The result indicates a limited worker
base of Chinese speakers.
3 Utilizing the manual alignments
As we can expect, given no explicit guideline for word
alignments, the variance of different assignments can be
fairly large, a question will raise what can we do with
the disagreements? As we will see later in the experi-
ment part, the labels are more likely to be consistent with
expert labels if more workers agree on it. Therefore, a
simple strategy is to use only the links that more workers
have consensus on them.
2005? ? ??
The   summer   of    2005
Figure 2: Partial and full alignments
However the method instantly gives rise to a prob-
lem. Now the alignment is not ?full alignments?, instead,
they are ?partial?. The claim seems to be trivial but they
have completely different underlying assumptions. Fig-
ure 2 shows the comparison of partial alignments (the
bold link) and full alignments (the dashed and the bold
links). In the example, if full alignment is given, we can
assert 2005 is only aligned to 2005#, not to {or ,
but we cannot do that if only partial alignment is given.
In this paper we experiment with a novel method which
uses the partial alignment to constraint the EM algorithm
in the parameter estimation of IBM models.
IBM Models (Brown et. al., 1993) are a series of gen-
erative models for word alignment. GIZA++ (Och and
Ney, 2003) is the most widely used implementation of
IBM models and HMM (Vogel et al, 1996) where EM
algorithm is employed to estimate the model parameters.
In the E-step, it is possible to obtain sufficient statistics
from all possible alignments for simple models such as
Model 1 and Model 2. Meanwhile for fertility-based
models such as Model 3, 4, 5, enumerating all possible
alignments is NP-complete. In practice, we use sim-
pler models such as HMM or Model 2 to generate a
?center alignment? and then try to find better alignments
among the neighbors of it. The neighbors of an alignment
aJ1 = [a1, a2, ? ? ? , aJ ], aj ? [0, I] is defined as align-
ments that can be generated from aJ1 by one of the oper-
ators: 1) Move operator m[i,j], that changes aj := i, i.e.
arbitrarily set word fj in source sentence to align to word
ei in target sentence; 2) Swap operator s[j1,j2] that ex-
changes aj1 and aj2 . The algorithm will update the center
alignment as long as a better alignment can be found, and
finally outputs a local optimal alignment. The neighbor
alignments of the alignment are then used in collecting
the counts for the M Step.
In order to use partial manual alignments to constrain
the search space, we separate the algorithm into two
stages, first the seed alignment will be optimized towards
the constraints. Each iteration we only pick a new center
alignment with less inconsistent links than the original
one, until the alignment is consistent with all constraints.
After that, in each iteration we pick the alignment with
highest likelihood but does not introduce any inconsistent
links. The algorithm will output a local optimal align-
ment consistent with the partial alignment. When col-
lecting the counts for M-step, we also need to exclude all
alignments that are not consistent with the partial man-
ual alignment. The task can also be done by skipping the
inconsistent alignments in the neighborhood of the local
optimal alignment.
4 Experiment and analysis
In this section we will show the analysis of the har-
vested MTurk alignments and the results of the semi-
supervised word alignment experiments.
4.1 Consistency of the manual alignments
We first examine the internal consistency of the MTurk
alignments. We calculate the internal consistency rate
in both results. Because we requested three assignments
for every question, we classify the links in two different
ways. First, if a link appear in all three submissions, we
classify it as ?consensus link?. Second, if a link appear in
more than one submissions, we classify it as ?majority?,
otherwise it is classified as ?minority?. Table 1 presents
the statistics of partial alignment and full alignment tasks.
Note that by spending the same amount of money, we get
more sentences aligned because for fixed rate partial sen-
tence alignment tasks, sometimes we may have overlaps
between tasks. Therefore we also calculate a subset of
full alignment tasks that consists of all the sentences in
partial alignment tasks. The statistics shows that although
generally full alignment tasks generates more links, the
partial alignment tasks gives denser alignments. It is in-
teresting to know whether the denser alignments lead to
higher recall rate or lower precision.
4.2 Comparing MTurk and expert alignments
To exam the quality of alignments, we compared them
with expert alignments. Table 2 lists the precision, recall
and F-1 scores for partial and full alignment tasks. We
compare the consistency of all links, the links in majority
group and the consensus links.
As we can observe from the results, the Turkers tend
to label less links than the experts, Interestingly, the over-
all quality of partial alignment tasks is significantly better
than full alignment tasks. Despite the lower recall rate, it
is encouraging that the majority vote and consensus links
32
Partial Full Full-Int
Number of sentences 135 239 135
Number of words 2,008 3,241 2,008
Consensus words 13,03 2,299 1,426
Consensus rate(%) 64.89 70.93 71.02
Total Links 7,508 9,767 6,114
Consensus Links 5,625 7,755 4,854
Consensus Rate(%) 74.92 79.40 79.39
Total Unique Links 3,186 3,989 2,506
Consensus Links 1,875 2,585 1,618
Consensus Rate(%) 58.85 64.80 64.54
In majority group 2,447 3,193 1,426
Majority rate(%) 76.80 80.04 71.06
Table 1: Internal consistency of manual alignments, here
Full-Int means statistics of full alignment tasks on the
sentences that also aligned using partial alignment task
All Links Majority Links Consensus Links
P. R. F. P. R. F. P. R. F.
P 0.84 0.88 0.86 0.95 0.76 0.84 0.98 0.60 0.74
F 0.88 0.70 0.78 0.96 0.61 0.75 0.99 0.51 0.68
I 0.87 0.71 0.79 0.95 0.62 0.75 0.98 0.52 0.68
Table 2: Consistency of MTurk alignments with expert
alignments, showing precision (P), recall (R) and F1 (F)
between MTurk and expert alignments. P, F, and I corre-
spond to Partial, Full and Full-Int in Table 1
yield very high precisions against expert alignments. Ta-
ble 3 lists the words with most errors. Most errors occur
on function words. A manual review shows that more
than 85% errors have function words on either Chinese
side or English side. The result, however, is as expected
because these words are hard to label and we did not pro-
vide clear rule for function words.
4.3 Results of semi-supervised word alignment
In this experiment we try to use the alignment links in
the semi-supervised word alignment algorithm. We use
Chinese-English manually aligned corpus in the exper-
iments, which contains 21,863 sentence pairs, 424,683
Chinese words and 524,882 English words. First, we use
the parallel corpus to train IBM models without any man-
ual alignments, we run 5 iterations of model 1 and HMM,
Chinese English
FN FP FN FP
64 { 16 , 122 the 15 ,
26 4 11 ? 67 NULL 11 a
19 , 9 4 43 of 6 the
17 ? 3 ? 36 to 6 is
16 ?? 3 ? 24 a 4 to
Table 3: Words that most errors occur, FN means a false
negative error occurred on the word, i.e. a link to this
word or from this word is missing. FP means false pos-
itive, accordingly. The manual alignment links comes
from majority vote.
3 iterations of model 3 and 6 iterations of model 4. Then
we resume the training procedure from the third itera-
tions of model 4. This time we load the manual alignment
links and perform 3 iterations of constrained EM. We also
experiment with 3 different sets of alignments. Table 4
presents the improvements on the alignment quality.
Unsupervised
Ch-En En-Ch
Prec. Recall AER Prec. Recall AER
68.22 46.88 44.43 65.35 55.05 40.24
All Links
Partial 68.28 47.09 44.26 65.86 55.63 39.68
Full-Int 68.28 47.09 44.26 65.85 55.63 39.69
Full 68.37 47.15 44.19 65.90 55.67 39.65
Majority Links
Partial 68.28 47.08 44.27 65.84 55.62 39.70
Full-Int 68.28 47.08 44.27 65.84 55.61 39.71
Full 68.37 47.13 44.20 65.88 55.65 39.67
Consensus Links
Partial 68.24 47.06 44.30 65.83 55.60 39.71
Full-Int 68.25 47.06 44.29 65.83 55.60 39.72
Full 68.31 47.10 44.25 65.86 55.63 39.68
Table 4: The performance of using manual alignments in
semi-supervised word alignment
From the result we can see that given the same amount
of links the improvement of alignment error rate is gen-
erally the same for partial and full alignment tasks, how-
ever, if we consider the amount of money spent on the
task, the full alignment task collect much more data than
partial alignments, we consider full sentence alignment
more cost efficient in this sense.
5 Conclusion
In this pilot experiment, we explore the possibility of
using Amazon Mechanical Turk (MTurk) to collect bilin-
gual word alignment data to assist automatic word align-
ment. We develop a system including a word align-
ment interface based on Javascript and a quality control
scheme. To utilize the manual alignments, we develop a
semi-supervised word alignment algorithm that can per-
form constrained EM with partial alignments. The algo-
rithm enables us to use only the most reliable links by
majority vote or consensus. The effectiveness of these
methods is proven by small-scale experiments. The re-
sults show the manual alignments from MTurk have high
precision with expert word alignment, especially when
filtered by majority vote or consensus. We get small im-
provement on semi-supervised word alignment. Given
the promising results, it is interesting to see if the ten-
dency will carry on when we scale up the experiments.
However the experiment also shows some problems,
first the coverage of worker base on MTurk is limited.
Given small worker base for specific languages, the cost
efficiency for NLP tasks in those languages is question-
able.
33
References
P. Blunsom and T. Cohn. 2006. Discriminative word align-
ment with conditional random fields. In Proceedings of the
21st International Conference on Computational Linguistics
and the 44th annual meeting of the Association for Compu-
tational Linguistics, pages 65?72.
P. F. Brown et. al. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. In Computational
Linguistics, volume 19(2), pages 263?331.
C. Callison-Burch, D. Talbot, and D. Osborne. 2004. Statistical
machine translation with word- and sentence-aligned parallel
corpora. In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics (ACL-2004).
C. Callison-Burch. 2009. Fast, cheap, and creative: Evaluat-
ing translation quality using Amazon?s Mechanical Turk. In
Proceedings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 286?295. Associa-
tion for Computational Linguistics.
A. Fraser and D. Marcu. 2006. Semi-supervised training for
statistical word alignment. In ACL-44: Proceedings of the
21st International Conference on Computational Linguistics
and the 44th annual meeting of the Association for Compu-
tational Linguistics, pages 769?776.
M. Kaisser and J. B. Lowe. 2008. A research collection of
question answer sentence pairs. In Proceedings of The 6th
Language Resources and Evaluation Conference.
M. Kaisser, M. Hearst, and J.B. Lowe. 2008. Evidence for
varying search results summary lengths. In Proceedings of
the 46th Annual Meeting of the Association for Computa-
tional Linguistics.
A. Kittur, E. H. Chi, and B Suh. 2008. Crowdsourcing user
studies with mechanical turk. In CHI ?08: Proceeding of the
twenty-sixth annual SIGCHI conference on Human factors in
computing systems, pages 453?456.
Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models for word
alignment. In ACL ?05: Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistics, pages
459?466.
A. Lopez and P. Resnik. 2006. Word-based alignment, phrase-
based translation: What?s the link? with philip resnik. In
Proceedings of the 7th Biennial Conference of the Associa-
tion for Machine Translation in the Americas (AMTA-2006).
W. Mason and D. J. Watts. 2009. Financial incentives and the
?performance of crowds?. In HCOMP ?09: Proceedings of
the ACM SIGKDD Workshop on Human Computation, pages
77?85.
R. C Moore. 2005. A discriminative framework for bilingual
word alignment. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in Natu-
ral Language Processing, pages 81?88.
J. Niehues and S. Vogel. 2008. Discriminative word alignment
via alignment matrix modeling. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages 18?25.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. In Computational Linguis-
tics, volume 1:29, pages 19?51.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discrim-
inative matching approach to word alignment. In Proceed-
ings of the conference on Human Language Technology and
Empirical Methods in Natural Language Processing, pages
73?80.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based word
alignment in statistical machine translation. In Proceedings
of 16th International Conference on Computational Linguis-
tics), pages 836?841.
34
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 62?65,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Can Crowds Build Parallel Corpora for Machine Translation Systems?
Vamshi Ambati and Stephan Vogel
{vamshi,vogel}@cs.cmu.edu
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
Corpus based approaches to machine transla-
tion (MT) rely on the availability of parallel
corpora. In this paper we explore the effec-
tiveness of Mechanical Turk for creating par-
allel corpora. We explore the task of sen-
tence translation, both into and out of a lan-
guage. We also perform preliminary experi-
ments for the task of phrase translation, where
ambiguous phrases are provided to the turker
for translation in isolation and in the context
of the sentence it originated from.
1 Introduction
Large scale parallel data generation for new lan-
guage pairs requires intensive human effort and
availability of bilingual speakers. Only a few lan-
guages in the world enjoy sustained research inter-
est and continuous financial support for develop-
ment of automatic translation systems. For most
remaining languages there is very little interest or
funding available and limited or expensive access to
experts for data elicitation. Crowd-sourcing com-
pensates for the lack of experts with a large pool of
expert/non-expert crowd. However, crowd-sourcing
has thus far been explored in the context of elicit-
ing annotations for a supervised classification task,
typically monolingual in nature (Snow et al, 2008).
In this shared task we test the feasibility of eliciting
parallel data for Machine Translation (MT) using
Mechanical Turk (MTurk). MT poses an interesting
challenge as we require turkers to have understand-
ing/writing skills in both the languages. Our work is
similar to some recent work on crowd-sourcing and
machine translation (Ambati et al, 2010; Callison-
Burch, 2009), but focuses primarily on the setup and
design of translation tasks on MTurk with varying
granularity levels, both at sentence- and phrase-level
translation.
2 Language Landscape on MTurk
We first conduct a pilot study by posting 25 sen-
tences each from a variety of language pairs and
probing to see the reception on MTurk. Language-
pair selection was based on number of speakers in
the language and Internet presence of the popula-
tion. Languages like Spanish, Chinese, English,
Arabic are spoken by many and have a large pres-
ence of users on the Internet. Those like Urdu,
Tamil, Telugu although spoken by many are not well
represented on the Web. Languages like Swahili,
Zulu, Haiti are neither spoken by many nor have
a great presence on the Web. For this pilot study
we selected Spanish, Chinese, English, Urdu, Tel-
ugu, Hindi, Haitian Creole languages. We do not
select German, French and other language pairs as
they have already been explored by Callison-Burch
(2009). Our pilot study helped us calibrate the costs
for different language pairs as well as helped us se-
lect the languages to pursue further experiments. We
found that at lower pay rates like 1 cent, it is difficult
to find a sufficient number of translators to complete
the task. For example, we could not find turkers
to complete the translation from English to Haitian-
Creole even after a period of 10 days. Haitian creole
is spoken by a small population and it seems that
only a very small portion of that was on MTurk. For
a few other languages pairs, while we could find a
62
Pair Cost per sen Days
Spanish-Eng $0.01 1
Telugu-Eng $0.02 2
Eng-Creole $0.06 -
Urdu-Eng $0.03 1
Hindi-Eng $0.03 1
Chinese-Eng $0.02 1
Table 1: Cost vs. Completion for Language pairs
few turkers attempting the task, the price had to be
increased to attract any attention. Table 1 shows the
findings of our pilot study. We show the minimum
cost at which we could start getting turkers to pro-
vide translations and the number of days they took to
complete the task. MTurk has so far been a suppli-
ers? market, and translation of rare-languages shows
how a limited supply of turkers leads to a buyer?s
market; only fair.
3 Challenges for Crowd-Sourcing and
Machine Translation
We use MTurk for all our crowd-sourcing experi-
ments. In case of MT, a HIT on MTurk is one or
more sentences in the source language that need to
be translated to a target language. Making sure that
the workers understand the task is the first step to-
wards a successful elicitation using the crowd. We
provide detailed instructions on the HIT for both
completion of the task and its evaluation. Mechan-
ical turk also has a provision to seek annotations
from qualified workers, from a specific location with
a specific success rate in their past HITs. For all
our HITs we set the worker qualification threshold
to 90%. We use the terms HIT vs. task and turker
vs. translator interchangeably.
3.1 Quality Assurance
Quality assurance is a concern with an online crowd
where the expertise of the turkers is unknown. We
also notice from the datasets we receive that consis-
tently poor and noisy translators exist. Problems like
blank annotations, mis-spelling, copy-pasting of in-
put are prevalent, but easy to identify. Turkers who
do not understand the task but attempt it anyway are
the more difficult ones to identify, but this is to be
expected with non-experts. Redundancy of transla-
tions for the input and computing majority consen-
sus translation is agreed to be an effective solution to
identify and prune low quality translation. We dis-
cuss in following section computation of majority
vote using fuzzy matching.
For a language pair like Urdu-English, we noticed
a strange scenario, where the translations from two
turkers were significantly worse in quality, but con-
sistently matched each other, there by falsely boost-
ing the majority vote. We suspect this to be a case of
cheating, but this exposes a loop in majority voting
which needs to be addressed, perhaps by also using
gold standard data.
Turking Machines: We also have the problem
of machines posing as turkers ? ?Turking machine?
problem. With the availability of online translation
systems like Google translate, Yahoo translate (Ba-
belfish) and Babylon, translation tasks on MTurk
become easy targets to this problem. Turkers ei-
ther use automatic scripts to get/post data from au-
tomatic MT systems, or make slight modifications
to disguise the fact. This defeats the purpose of the
task, as the resulting corpus would then be biased to-
wards some existing automatic MT system. It is ex-
tremely important to keep gamers in check; not only
do they pollute the quality of the crowd data, but
their completion of a HIT means it becomes unavail-
able to genuine turkers who are willing to provide
valuable translations. We, therefore, collect transla-
tions from existing automatic MT services and use
them to match and block submissions from gamers.
We rely on some gold-standard to identify genuine
matches with automatic translation services.
3.2 Output Space and Fuzzy Matching
Due to the natural variability in style of turkers, there
could be multiple different, but perfectly valid trans-
lations for a given sentence. Therefore it is dif-
ficult to match translation outputs from two turk-
ers or even with gold standard data. We there-
fore need a fuzzy matching algorithm to account
for lexical choices, synonymy, word ordering and
morphological variations. This problem is similar
to the task of automatic translation output evalua-
tion and so we use METEOR (Lavie and Agarwal,
2007), an automatic MT evaluation metric for com-
paring two sentences. METEOR has an internal
aligner that matches words in the sentences given
63
and scores them separately based on whether the
match was supported by synonymy, exact match or
fuzzy match. The scores are then combined to pro-
vide a global matching score. If the score is above a
threshold ?, we treat the sentences to be equivalent
translations of the source sentence. We can set the
? parameter to different values, based on what is ac-
ceptable to the application. In our experiments, we
set ? = 0.7. We did not choose BLEU scoring met-
ric as it is strongly oriented towards exact matching
and high precision, than towards robust matching for
high recall.
4 Sentence Translation
The first task we setup on MTurk was to translate
full sentences from a source language into a tar-
get language. The population we were interested in
was native speakers of one of the languages. We
worked with four languages - English, Spanish, Tel-
ugu and Urdu. We chose 100 sentences for each
language-pair and requested three different transla-
tions for each sentence. The Spanish data was taken
from BTEC (Takezawa et al, 2002) corpus, consist-
ing of short sentences in the travel domain. Telugu
data was taken from the sports and politics section
of a regional newspaper. For Urdu, we used the
NIST-Urdu Evaluation 2008 data. We report results
in Table 2. Both Spanish and Urdu had gold stan-
dard translations, as they were taken from parallel
corpora created by language experts. As the data
sets are small, we chose to perform manual inspec-
tion rather than use automatic metrics like BLEU to
score match against gold-standard data.
4.1 Translating into English
The first batch of HITs were posted to collect trans-
lations into English. We noticed from manual in-
spection of the quality of translations that most of
our translators were non-native speakers of English.
This calls for adept and adequate methods for evalu-
ating the translation quality. For example more than
50% of the Spanish-English tasks were completed in
India, and in some cases a direct output of automatic
translation services.
4.2 Translating out of English
The second set of experiments were to test the ef-
fectiveness of translating out of English. The ideal
Language Pair Cost #Days #Turkers
Spanish-English $0.01 1 16
Telugu-English $0.02 4 12
Urdu-English $0.03 2 13
English-Spanish $0.01 1 19
English-Telugu $0.02 3 35
English-Urdu $0.03 2 21
Table 2: Sentence translation data
target population for this task were native speakers
of the target language who also understood English.
Most participant turkers who provided Urdu and Tel-
ugu translations, were from India and USA and were
non-native speakers of English. However, one prob-
lem with enabling this task was the writing system.
Most turkers do not have the tools to create content
in their native language. We used ?Google Translit-
erate? API 1 to enable production of non-English
content. This turned out to be an interesting HIT
for the turkers, as they were excited to create their
native language content. This is evident from the
increased number of participant turkers. Manual in-
spection of translations revealed that this direction
resulted in higher quality translations for both Urdu
and Telugu and slightly lower quality for Spanish.
5 Phrase Translation
Phrase translation is useful in reducing the cost
and effort of eliciting translations by focusing on
those parts of the sentence that are difficult to
translate. It fits well into the paradigm of crowd-
sourcing where small tasks can be provided to a lot
of translators. For this task, we were interested in
understanding how well non-experts translate sub-
sentential segments, and whether exposure to ?con-
text? was helpful. For this set of experiments we use
the Spanish-English language pair, where the turk-
ers were presented with Spanish phrases to trans-
late. The phrases were selected from the standard
phrase tables produced by statistical phrase-based
MT (Koehn et al, 2007), that was trained on the en-
tire 128K BTEC corpus for Spanish-English. We
computed an entropy score for each entry in the
phrase table under the translation probability distri-
butions in both directions and picked the set of 50
1http://www.google.com/transliterate/
64
Type %Agreement %Gold match
Out of Context 64% 32%
In Context 68% 33%
Table 3: Phrase Translation: Spanish-English
Length Count Example
1 2 cierras
2 11 vienes aqu
3 26 hay una en
4 8 a conocer su decisin
5 4 viene bien a esa hora
Table 4: Details of Spanish-English phrases used
most ambiguous phrases according to this metric.
Table 4 shows sample and the length distribution of
the phrases selected for this task.
5.1 In Context vs. Out of Context
We performed two kinds of experiments to study
phrase translation and role of context. In the first
case, the task was designed to be as simple as possi-
ble with each phrase to be translated as an individual
HIT. We provided a source phrase and request turk-
ers to translate a phrase under any hypothesized con-
text. For the second task, we gave a phrase associ-
ated with the sentence that it originated from and re-
quested the turkers to translate the phrase only in the
context of the sentence. For both cases, we analyzed
the data for inter-translator agreement;% of cases
where there was a consensus translation), and agree-
ment with the gold standard; % of times the trans-
lated phrase was present in the gold standard transla-
tion of the source sentence it came from. As shown
in Table 3, translating in-context produced a better
match with gold standard data and scored slightly
better on the inter-translator agreement. We think
that when translating out of context, most translators
choose as appropriate for a context in their mind and
so the inter-translator agreement could be lower, but
when translating within the context of a sentence,
they make translation choices to suit the sentence
which could lead to better agreement scores. In fu-
ture, we will extend these experiments to other lan-
guage pairs and choose phrases not by entropy met-
ric, but to study specific language phenomenon.
6 Conclusion
Our experiments helped us better understand the
formulation of translation tasks on MTurk and its
challenges. We experimented with both translating
into and out of English and use transliteration for
addressing the writing system issue. We also ex-
periment with in-context and out-of-context phrase
translation task. While working with non-expert
translators it is important to address quality concerns
alongside keeping in check any usage of automatic
translation services. At the end of the shared task we
have sampled the ?language landscape? on MTurk
and have a better understanding of what to expect
when building MT systems for different language
pairs.
References
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Proceedings of the LREC 2010,
Malta, May.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP 2009, pages 286?295, Sin-
gapore, August. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL Demonstration Session.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In WMT 2007,
pages 228?231, Morristown, NJ, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the EMNLP 2008, pages 254?
263, Honolulu, Hawaii, October.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Towards a broad-coverage bilingual corpus for speech
translation of travel conversation in the real world. In
Proceedings of LREC 2002, Las Palmas, Spain.
65
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 1?10,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Semi-supervised Word Alignment Algorithm
with Partial Manual Alignments
Qin Gao, Nguyen Bach and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh PA, 15213
{qing, nbach, stephan.vogel}@cs.cmu.edu
Abstract
We present a word alignment framework
that can incorporate partial manual align-
ments. The core of the approach is a
novel semi-supervised algorithm extend-
ing the widely used IBM Models with
a constrained EM algorithm. The par-
tial manual alignments can be obtained
by human labelling or automatically by
high-precision-low-recall heuristics. We
demonstrate the usages of both methods
by selecting alignment links from manu-
ally aligned corpus and apply links gen-
erated from bilingual dictionary on unla-
belled data. For the first method, we con-
duct controlled experiments on Chinese-
English and Arabic-English translation
tasks to compare the quality of word align-
ment, and to measure effects of two differ-
ent methods in selecting alignment links
from manually aligned corpus. For the
second method, we experimented with
moderate-scale Chinese-English transla-
tion task. The experiment results show an
average improvement of 0.33 BLEU point
across 8 test sets.
1 Introduction
Word alignment is used in various natural lan-
guage processing applications, and most statistical
machine translation systems rely on word align-
ment as a preprocessing step. Traditionally the
word alignment model is trained in an unsuper-
vised manner, e.g. the most widely used tool
GIZA++ (Och and Ney, 2003), which implements
the IBM Models (Brown et. al., 1993) and the
HMM model (Vogel et al, 1996). However, for
language pairs such as Chinese-English, the word
alignment quality is often unsatisfactory (Guzman
et al, 2009). There has been increasing interest on
using manual alignments in word alignment tasks.
Ittycheriah and Roukos (2005) proposed to use
only manual alignment links in a maximum en-
tropy model. A number of semi-supervised word
aligners are proposed (Blunsom and Cohn, 2006;
Niehues and Vogel, 2008; Taskar et al, 2005; Liu
et al, 2005; Moore, 2005). These approaches use
held-out manual alignments to tune the weights
for discriminative models, with the model param-
eters, model scores or alignment links from un-
supervised word aligners as features. Also, sev-
eral models are proposed to address the prob-
lem of improving generative models with small
amount of manual data, including Model 6 (Och
and Ney, 2003) and the model proposed by Fraser
and Marcu (2006) and its extension called LEAF
aligner (Fraser and Marcu, 2007). The approaches
use labelled data to tune parameters to combine
different components of the IBM Models.
2005? ? ??
2005nian     de      xiatian
The   summer   of    2005
Figure 1: Partial and full alignments
An interesting question is, if we only have par-
tial alignments of sentences, can we make use of
them? Figure 1 shows the comparison of par-
tial alignments (the bold link) and full alignments
(both of the dashed and the bold links). A partial
alignment of a sentence only provides a portion of
links of the full alignment. Although it seems to be
trivial, they actually convey different information.
In the example, if the full alignment is given, we
can assert 2005 is only aligned to 2005nian, not to
de or xiatian, but if only the partial alignment is
given we cannot make such assertion.
Partial alignments can be obtained from vari-
ous sources, for example, we can fetch them by
manually correcting unsupervised alignments, by
simple heuristics such as dictionaries of technical
1
terms, by rule-based alignment systems that have
high accuracy but low recall rate. The function-
ality is considered useful in many scenarios. For
example, the researchers can analyse the align-
ments generated by GIZA++ and fix common
error patterns, and perform training again. On
another way, an application can combine active
learning (Arora et al, 2009) and crowdsourcing,
asking non-expertise such as workers of Amazon
Mechanical Turk to label crucial alignment links
that can improve the system with low cost, which
is now a promising methodology in NLP areas
(Callison-Burch, 2009).
In this paper, we propose a semi-supervised ex-
tension of the IBM Models that can utilize partial
alignment links. More specifically, we are seeking
answers for the following questions:
? Given the partial alignment of a sentence,
how to find the most probable alignment that
is consistent with the partial alignment.
? Given a set of partially aligned sentences,
how to get the parameters that maximize the
likelihood of the sentence pairs with align-
ments consistent with the partial alignments
? Given a set of partially aligned sentences,
with conflicting partial alignments, how to
answer the two questions above.
In the proposed approach, the manual partial
alignment links are treated as ground truth, there-
fore, they will be fixed. However, for all other
links we make no additional assumption. When
using manual alignments, there can be links con-
flicting with each other. These conflicting evi-
dences are treated as options and the generative
model will choose the most probable alignment
from them. An efficient training algorithm for
fertility-based models is proposed. The algorithm
manipulates the Moving and Swapping matrices
used in the hill-climbing algorithm (Och and Ney,
2003) to rule out inconsistent alignments in both
E-step and M-step of the training.
A similar attempt has been made by Callison-
Burch et al (2004), where the authors interpo-
late the parameters estimated by sentence-aligned
and word-aligned corpus. Our approach is differ-
ent from their method that we do not require fully
aligned data and we do not need to interpolate two
parameter sets. All the training is done within a
unified framework. Our approach is also different
from LEAF (Fraser and Marcu, 2007) and Model
6 (Och and Ney, 2003) that we do not use these
additional links to tune additional parameters to
combine model components, as a result, it is not
limited to fully aligned corpus.
A question may raise why the proposed method
is superior over using the partial alignment links
as features in discriminative aligners? There are
three possible explanations. First, the method pre-
serves the power of the generative model in which
the algorithm utilizes large amount of unlabeled
data. More importantly, the additional information
can propagate over the whole corpus through bet-
ter estimation of model parameters. In contrast, if
we use the alignment links in discriminative align-
ers as a feature, one link can only affect the par-
ticular word, or at most the sentence. Second, al-
though the discriminative word alignment meth-
ods provide flexibility to utilize labeled data, most
of them still rely on generative aligners. Some
rely on the model parameters of the IBM Mod-
els (Liu et al, 2005; Blunsom and Cohn, 2006),
others rely on the alignment links from GIZA++
as features or as training data (Taskar et al, 2005),
or use both the model parameters and the align-
ment links (Niehues and Vogel, 2008). Therefore,
improving the generative aligner is still important
even when using discriminative aligners. Third,
these methods require full alignment of sentences
to provide positive (aligned) and negative (non-
aligned) information, which limits the availability
of data (Niehues and Vogel, 2008).
The proposed method has been successfully ap-
plied on various tasks, such as utilizing manual
alignments harvested from Amazon Mechanical
Turk (Gao and Vogel, 2010), and active learning
methods for improving word alignment (Ambati
et al, 2010). This paper provides the detailed al-
gorithm of the method and controlled experiments
to demonstrate its behavior.
The paper is organized as follows, in section
2 we describe the proposed model as well as the
modified training algorithm. Section 3 presents
two approaches of obtaining manual alignment
links, The experimental results will be shown in
section 4. We conclude the paper in section 5.
2 Semi-supervised word alignment
2.1 Problem Setup
The IBM Models (Brown et. al., 1993) are a
series of generative models for word alignment.
GIZA++ (Och and Ney, 2003) is the most widely
used implementation of the IBM Models and the
2
HMM model (Vogel et al, 1996). Given two
strings from target and source languages fJ1 =
f1, ? ? ? , fj , ? ? ? fJ and eI1 = e1, ? ? ? , ei, ? ? ? eI , an
alignment of the sentence pair is defined as aJ1 =
[a1, a2, ? ? ? , aJ ], aj ? [0, I]. The IBM Models
assume all the target words must be covered ex-
actly once (Brown et. al., 1993). We try to model
P (fJ1 |e
I
1), which is the probability of observing
source sentence given target sentence eI1. In sta-
tistical models a hidden alignment variable is in-
troduced, so that we can write the probability as
P (fJ1 |e
I
1) =
?
aJ1
Pr(fJ1 , a
J
1 |e
J
1 , ?), where Pr(?)
is the estimated probability given the parameter set
?. The IBM Models define several different set of
parameters, from Model 1 to Model 5. Starting
from Model 3, the fertility model is introduced.
EM algorithm is employed to estimate the
model parameters of the IBM Models. In E-step,
it is possible to obtain sufficient statistics from
all possible alignments with simplified formulas
for simple models such as Model 1 and Model 2.
Meanwhile for fertility-based models, enumerat-
ing all possibilities is NP-complete and hence it
cannot be carried out for long sentences. A solu-
tion is to explore only the ?neighbors? of Viterbi
alignments. However, obtaining Viterbi align-
ments itself is NP-complete for these models. In
practice, a greedy algorithm is employed to find
a local optimal alignments based on Viterbi align-
ments generated by simpler models.
First, we define the neighbor alignments of a as
the set of alignments that differ by one of the two
operators from the original ?center alignment?.
? Move operator m[i,j], that changes aj := i,
i.e. arbitrarily set word fj in source sentence
to align to word fi in target sentence.
? Swap operator s[j1,j2] that exchanges aj1 and
aj2 .
We denote the neighbor alignments set of
current center alignment a as nb(a). In
each step of hill-climbing algorithm, we find
the alignment b(a) in nb(a), s.t. b(a) =
argmaxa??nb(a) p(a
?|e, f), and update the current
center alignment. The algorithm iterates until
there is no update could be made. The statistics of
the neighbor alignments of the final center align-
ment will be collected for normalization step (M-
step). The algorithm is greedy, so a reasonable
start point is important. In practice GIZA++ uses
Model 2 or HMM to generate the seed alignment.
To improve the speed of hill climbing, GIZA++
caches the cost of all possible move and swap op-
erations in two matrices. In the so called Moving
Matrix M , the element Mij stores the likelihood
difference of a move operator aj = i:
Mij =
Pr(m[i,j](a)|e, f)
Pr(a|e, f)
? (1? ?(aj , i)) (1)
and in the Swapping Matrix S, the element Sjj?
stores the likelihood difference of a swap operator
between aj and aj? :
Sjj? =
{
Pr(S[j,j?](a)|e,f)
Pr(a|e,f) ? (1? ?(aj , aj?)) if j < j
?
0 otherwise
(2)
The matrices will be updated whenever an oper-
ator is made, but the update is limited to the rows
and columns involved in the operator.
We define a partial alignment of a sentence
pair (fJ1 , e
I
1) as ?
J
I = {(i, j), 0 ? i < I, 0 ?
j < J}, note that the partial alignment does not
assume 1-to-N restriction on either side, and the
word from neither source nor target side need to be
covered with links. If an index is missing, it does
not mean the word is aligned to the empty word.
Instead it just means no information is provided.
We use a link (0, j) or (i, 0) to explicitly represent
the information that word fj or ei is aligned to the
empty word.
In order to find the most probable align-
ment that is consistent the partial alignments,
we treat the partial alignment as constraints, i.e.
for an alignment aJ1 = [a1, a2, ? ? ? , aj ] on the
sentence pair fJ1 , e
I
1, the translation probability
Pr(fJ1 , a
J
1 |e
I
1, ?
J
I ) will be zero if the alignment is
inconsistent with the partial alignments.
Pr(fJ1 |e
I
1, a
J
1 , ?
J
I ) =
{
0, aJ1 is inconsistent with?
J
I
Pr(fJ1 |e
I
1, a
J
1 , ?), otherwise
(3)
Under the constraints of the IBM Models, there
are two situations that aJ1 is inconsistent with ?
J
I :
1. Target word misalignment: The IBM Models
assume one target word can only be aligned
to one source word. Therefore, if the target
word fj aligns to a source word ei, while the
constraint ?JI suggests fj should be aligned
to ei? , the alignment violates the constraint
and thus is considered inconsistent.
3
2005?
?
??
the
summer
of
2005Manual Alignment Link
(a)
2005?
?
??
the
summer
Of
2005Seed Alignment Consistent Alignment Center Alignment
(b)                    (c)
2005?
?
??
the
summer
of
2005
Figure 2: Illustration of Algorithm 1
2. Source word to empty word misalignment:
Since one source word can be aligned to mul-
tiple target words, it is hard to constrain the
alignments of source words. However, if a
source word is aligned to the empty word,
it cannot be aligned to any concrete target
word.
However, we are facing the problem of con-
flicting evidences. The problem is not necessar-
ily caused by errors in manual alignments, but
the assumption of the IBM Models that one tar-
get word can only be aligned to one source word.
This assumption causes multiple alignment links
from one target word conflict with each other. In
this case, we relax the constraints of situation 1
that if the alignment link aj? is consistent with any
target-to-source links (i, j) that j = j?, it will be
considered consistent. Also, we arbitrarily assign
the source word to empty word constraints higher
priorities than other constraints.
In EM algorithm, to ensure the final model be
marginalized on the fixed alignment links, and
the final Viterbi alignment is consistent with the
fixed alignment links, we need to guarantee that
no statistics from inconsistent alignments be col-
lected into the sufficient statistics. On fertility-
based models, we have to make sure:
1. The hill-climbing algorithm outputs align-
ment links consistent with the fixed align-
ment links.
2. The count collection algorithm rules out all
the inconsistent statistics.
With the constrained hill-climbing algorithm
and count collection algorithm which will be de-
scribed below, the above two criteria are satisfied.
2.2 Constrained hill-climbing algorithm
Algorithm 1 shows the algorithm outline of con-
strained hill-climbing. First, similar to the original
hill-climbing algorithm described above, HMM
(or Model 2) is used to obtain a seed alignment.
To ensure the resulting center alignment be con-
sistent with manual alignment, we need to split the
Algorithm 1 Constrained Hill-Climbing
1: Calculate the seed alignment a0 using HMM model
2: while ic(a0) > 0 do
3: if {a : ic(a) < ic(a0)} = ? then
4: break
5: end if
6: a0 := argmaxa?nb(a0),ic(a)<ic(a0) Pr(f |e, a)
7: end while
8: Mij := ?1 if (i, j) 6? ?JI or (i, 0) ? ?
J
I
9: loop
10: Sjj? := ?1 if (j, aj?) 6? ?
J
I or (j
?, aj) 6? ?JI
11: Mi1j1 = argmaxMij ; Sj1j?1 = argmaxSij
12: if Mi1j1 ? 1 and Sj1j?1 ? 1 then
13: Break
14: end if
15: if Mi1j1 > Sj1j?1 then
16: Update Mi1?,Mj1?,M?i1 ,M?j1
and Si1?, Sj1?, S?i1 , S?j1 , set a0 := Mi1j1(a0)
17: else
18: Update Mj1?,Mj?1?,M?j1 ,M?j?1
and Sj?1?, Sj1?, S?j?1 , S?j1 , set a0 := Sj1j?1(a0)
19: end if
20: end loop
21: Return a0
hill-climbing algorithm into two stages, i.e. opti-
mize towards the constraints and towards the opti-
mal alignment under the constraints.
From a seed alignment, we first try to move the
alignment towards the constraints by choosing a
move or swap operator that:
1. has highest likelihood among alignments
generated by other operators, excluding the
original alignment,
2. eliminates at least one inconsistent link.
The first step reflects in line 2 through 7 in the
algorithm, where we use ic(?) to denote the total
number of inconsistent links in the alignment, and
nb(?) to denote the neighbor alignments.
We iteratively update the alignment until no ad-
ditional inconsistent link can be removed. The al-
gorithm implies that we force the seed alignment
to become closer to the constraints while trying
to find the best consistent alignment. Figure 2
demonstrates the idea, given the manual alignment
link shown in (a), and the seed alignment shown as
solid links in (b), we move the inconsistent link to
the dashed link by a move operation.
After we find the consistent alignment, we pro-
ceed to optimize towards the optimal alignment
within the constraints. The algorithm sets the cells
to negative if the corresponding operations are not
allowed. The Moving matrix only need to be up-
dated once, as in line 8 of the algorithm. Whereas
the swapping matrix need to be updated every it-
4
eration, Since once the alignment is updated, the
possible violations will also change. This is done
in line 10.
If source words ik are aligned to the empty
word, we set Mik,j = ?1,?j, as shown in line 8.
The swapping matrix does not need to be modified
in this case because the swapping operator will not
introduce new links. Again, Figure 2 demonstrates
the optimization step in (c), two move operators
or one swap operator can move the link marked
with cross to the dashed line, which can be a bet-
ter alignment.
Because the cells that can lead to violations are
set to negative, the operators will never be picked
in line 11, therefore we effectively ensure the con-
sistency of the final center alignment.
The algorithm will end when no better update
can be made (line 12 through 14), otherwise, we
pick the new update with highest likelihood as new
center alignment and update the cells in the Mov-
ing and Swapping matrices that will be affected
by the update. Line 15 through line 19 perform
the operation.
2.3 Count Collection
After finding the center alignment, we collect
counts from the neighbor alignments so that the
M-step can normalize the counts to produce the
model parameters for the next step. All statis-
tics from inconsistent alignments are ruled out to
ensure the final sufficient statistics marginalized
on the fixed alignment links. Similar to the con-
strained hill climbing algorithm, we can manipu-
late the Moving/Swapping matrices to effectively
exclude inconsistent alignments. We just need to
bypass all the cells whose values are negative, i.e.
represent inconsistent alignments.
By combining the constrained EM algorithm
and the count collection, the Viterbi alignment is
guaranteed to be consistent with the fixed align-
ment links, and the sufficient statistics is guar-
anteed to contain no statistics from inconsistent
alignments.
2.4 Training scheme
We extend the multi-thread GIZA++ (Gao and
Vogel, 2008) to load the alignments from a mod-
ified corpus file. The links are appended to the
end of each sentence in the corpus file in the form
of indices pairs, which will be read by the aligner
during training. In practice, we first training un-
constrained models up to Model 4, and then switch
to constrained Model 4 and continue training for
several iterations, the actual number of training
order is: 5 iterations of Model 1, 5 iterations of
HMM, 3 iterations of Model 3, 3 iterations of
unconstrained Model 4 and 3 iterations of con-
strained Model 4. Because here we actually have
more Model 4 iterations, to make the comparison
fair, in all the experiments below we perform 6 it-
erations of Model 4 in the baseline systems.
3 Obtaining alignment links
Given the algorithm described in the Section 2,
we still face the problem of obtaining alignment
links to constrain the system. In this section, we
describe two approaches to obtain the links, the
first is to resort to human labels, while the second
applies high-precision-low-recall heuristic-based
aligner on large unsupervised corpus.
3.1 Using manual alignment links
Using manual alignment links is simple and
straight-forward, however the problem is how to
select links for human to label given that labelling
the whole corpus is impossible. We propose two
link selectors, the first is the random selector in
which every links in the manual alignment has
equal probability of being selected. Obviously,
the random selecting method is far from optimal
because it pays no attention on the quality of ex-
isting links. In order to demonstrate that by select-
ing links carefully we can achieve better alignment
quality with less manual alignment links, we pro-
pose the second selector based on disagreements
of alignments from two directions. We first clas-
sify the source and target words fj and ei into
three categories. Use fj as an example, the cat-
egories are:
? C1: fj aligns to ei, i > 0 in e ? f ,1 but in
reversed direction ei does not align to fj but
to another word.
? C2: fj aligns to ei, i > 0, in f ? e, but in
reversed direction (e ? f ), fj aligns to the
empty word.
? C3: no word aligns to fj , in f ? e, but in
reversed direction fj aligns to ei, i > 0.2
The criteria of ei are the same as fj after swap-
ping the definitions of ?source? and ?target?.
We prioritize the links ?JI = (i, j) by looking at
the classes of the source/target words. The order of
1Recall that fj can align to only one word.
2This class is different from C1 that whether ei aligns to
concrete words or the empty word.
5
Order Criterion Order Criterion
1 fj ? C1 5 ei ? C2
2 fj ? C2 4 ei ? C1
3 fj ? C3 6 ei ? C3
Table 1: The priorities of alignment links
priorities is shown in Table 1. All the links not in
the six classes will have the lowest priorities. The
links with higher priorities will be selected first,
but the order of two links in a same priority class
is not defined and they will be selected randomly.
3.2 Using heuristics on unlabelled data
Another possible way of getting alignment links
is to make use of heuristics to generate high-
precision-low-recall links and feed them into the
aligner. The heuristics can be number map-
ping, person name translator or more sophisticated
methods such as alignment confidence measure
(Huang, 2009). In this paper we propose to use
manual dictionaries to generate alignment links.
First we filter out from the dictionary the en-
tries with high frequency in the source side, and
then build an aligner based on it. The aligner out-
put links between words if them match an entry
in the dictionary. The method can be applied on
large unlabelled corpus and generate large num-
ber of links, after that we use the links as manual
alignment links in proposed method.
The readers may notice that GIZA++ supports
utilizing manual dictionary as well, however it is
different from our method. The dictionary is used
in GIZA++ only in the initialization step of Model
1, where only the statistics of the word pairs ap-
peared in the dictionary will be collected and nor-
malized. Given the fact that Model 1 converges to
global optimal, the effect will fade out after sev-
eral iterations. In contrast, our method impose
a hard constraint on the alignments. Also, our
method can be used side-by-side with the method
in GIZA++.
4 Experiments
4.1 Experiments on manual link selectors
We designed a set of controlled experiments to
show that the algorithm acts as desired. Particu-
larly, with a number of manual alignment links fed
into the aligner, we should be able to correct more
misaligned alignment links than the manual align-
ment links through better alignment models. Also,
carefully selected alignment links should outper-
form randomly selected alignment links.
We used Chinese-English and Arabic-English
manually aligned corpus in the experiments. Ta-
ble 2 shows the statistics of the corpora:
Number of Num. of Words Alignment
Sentences Source Target Links
Ch-En 21,863 424,683 524,882 687,247
Ar-En 29,876 630,101 821,938 830,349
Table 2: Corpus statistics of the corpora
First the corpora is trained as unlabelled data
to serve as baselines, and then we feed a portion
of alignment links into the proposed aligner. We
experimented with different methods of choosing
alignment links and adjust the number of links vis-
ible to the aligner. Because of the limitations of
the IBM Models, such as no N-to-1 alignments,
the manual alignment is not reachable from ei-
ther direction. We then define the best align-
ment that the IBM Models can express ?oracle
alignment?, which can be obtained by dropping
all N-to-1 links from manual alignment. Also, to
show the upper-bound performance, we feed all
the manual alignment links to our aligner, and call
the alignment ?force alignment?. Table 3 shows
the alignment qualities of oracle alignments and
force alignments of both systems. For force align-
ments, we show the scores with and without im-
plicit empty links derived from the manual align-
ment.3 The oracle alignments are the performance
upper-bounds of all aligners under IBM Model?s
1-to-N assumption. The result from Table 3 shows
that, if we include the derived empty links, the
force alignments are close to the oracle results.
Then the question is how fast we can approach the
upper-bound.
To answer the question, we gradually increase
the number of links being fed into the aligner. In
these experiments the seeds for random number
generator are fixed so that the links selected in
later experiments are always superset of that of
earlier experiments. The comparison of the align-
ment quality is shown in Figure 3 and 4. To show
the actual improvement brought in by the algo-
rithm instead of the manual alignment links them-
selves, we compare the alignment results of the
proposed method with directly fixing the align-
ments from original GIZA++ training. By fix-
ing alignments we mean that first the conventional
3We can derive empty links if one word has no alignment
link from the full alignment we have access to.
6
0 1 2 3 4 5 6
x 105
70
75
80
85
90
95
100
Number of Links
Pre
cisio
n
Precision?Number of links (Chinese?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
48
50
52
54
56
58
60
62
Number of Links
Rec
all
Recall?Number of links (Chinese?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
5
10
15
20
25
30
35
40
Number of Links
AER
AER?Number of links (Chinese?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
70
75
80
85
90
95
100
Number of Links
Pre
cisio
n
Precision?Number of links (English?Chinese)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
60
65
70
75
80
Number of Links
Rec
all
Recall?Number of links (English?Chinese)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
5
10
15
20
25
30
35
40
Number of Links
AER
AER?Number of links (English?Chinese)
 
 NNWNDFFRFD
0 1 2 3 4 5 6
x 105
70
75
80
85
90
95
Number of Links
Pre
cisio
n
Precision?Number of links (Combined Ch/En)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
60
65
70
75
80
85
90
Number of Links
Rec
all
Recall?Number of links (Combined Ch/En)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
5
10
15
20
25
30
35
Number of Links
AER
AER?Number of links (Combined Ch/En)
 
 NNWNDFFRFD
Figure 3: Alignment qualities of Chinese-English word alignment, NN: Random selector without empty
links, WN: Random seletor with empty links, DF: Disagreement selector, FR: Directly fixing the align-
ments with random selector, FD: Directly fixing the alignments with disagreement selector. Each row
shows the precision, recall and AER when applying different number of manual alignment links. The
three rows are for Chinese-English, English-Chinese and heuristically symmetrized alignments (grow-
diag-final-and) accordingly.
GIZA++ training is performed and then we add the
manual alignment links to the resulting alignment.
In case that the 1-to-N restriction of the IBM Mod-
els is violated, we keep the manual alignment links
and remove the links from GIZA++.
We show the results as FR (dashed curves with
diamond markers) and FD (dashed curves with
square markers) in the plots, corresponding to
alignments selected from the random link selector
and the disagreement-based link selector. These
two curves serve as baseline, and the gaps between
the FR curves and the WN curves (dotted curves
with cross markers) and the gaps between the FD
curves and the DF curves (solid curves) show the
amount of improvement we achieved using the
method in addition to the manual alignment links.
Therefore, they represent the effectiveness of the
proposed alignment approach. Also the gaps be-
tween DF and WN curves indicate the differences
in the performance of two link selectors.
The plots illustrate that when the number of
links is small, the WN and DF curves are al-
ways higher than the FR/FD curves. It proves
that our system does not just fix the links pro-
vided by manual alignments, instead the informa-
tion propagates to other links. The largest gap
between FD and DF is 8% absolute in com-
bined alignment of Chinese-English system with
200,000 manual alignment links. Also, we can
see that the disagreement-based link selector (DF)
always outperform the random selector (WN). It
suggest that, if we want to harvest manual align-
ment links, it is possible to apply active learning
method to minimize the user labelling effort while
maximizing the improvement on word alignment
qualities. Especially, notice that in the lower parts
7
0 1 2 3 4 5 6 7 8
x 105
60
65
70
75
80
85
Number of Links
Pre
cisio
n
Precision?Number of links (Arabic?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
55
60
65
70
Number of Links
Rec
all
Recall?Number of links (Arabic?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
5
10
15
20
25
30
35
40
45
Number of Links
AER
AER?Number of links (Arabic?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
60
65
70
75
80
85
Number of Links
Pre
cisio
n
Precision?Number of links (English?Arabic)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
70
75
80
85
90
Number of Links
Rec
all
Recall?Number of links (English?Arabic)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
5
10
15
20
25
30
35
Number of Links
AER
AER?Number of links (English?Arabic)
 
 NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
65
70
75
80
85
Number of Links
Pre
cisio
n
Precision?Number of links (Combined En/Ar)
 
 NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
70
75
80
85
90
95
100
Number of Links
Rec
all
Recall?Number of links (Combined En/Ar)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
5
10
15
20
25
30
35
Number of Links
AER
AER?Number of links (Combined En/Ar)
 
 
NNWNDFFRFD
Figure 4: Alignment qualities of Arabic-English word alignment, NN: Random selector without empty
links, WN: Random selector with empty links, DF: Disagreement selector, FR: Directly fixing the align-
ments with random selector, FD: Directly fixing the alignments with disagreement selector. Each row
shows the precision, recall and AER when applying different number of manual alignment links. The
three rows are for Arabic-English, English-Arabic and heuristically symmetrized alignments (grow-diag-
final-and) accordingly.
of the curves, with a small number of manual
alignment links, we can already improve the align-
ment quality by a large gap. This observation can
benefit low-resource word alignment tasks.
4.2 Experiment on using heuristics
The previous experiment shows the potential of
using the method on manual aligned corpus, here
we demonstrate another possible usage of the pro-
posed method that uses heuristics to generate high-
precision-low-recall links. We use LDC Chinese-
English dictionary as an example. The entries with
single Chinese character and more than six En-
glish words are filtered out. The heuristic-based
aligner yields alignment that has 79.48% preci-
sion and 17.36% recall rate on the test set we used
in 4.1. By applying the links as manual links,
we run proposed method on the same Chinese-
English test data presented in 4.1, and the results
of alignment qualities are shown in 5. As we can
see, the AER reduced by 1.64 from 37.23 to 35.61
on symmetrized alignment.
We also experimented with translation tasks
with moderate-size corpus. We used the corpus
LDC2006G05 with 25 million words. The train-
ing scheme is the same as previous experiments,
where the filtered LDC dictionary is used. After
word alignment, standard Moses phrase extraction
tool (Och and Ney, 2004) is used to build the trans-
lation models and finally Moses (Koehn et. al.,
2007) is used to tune and decode.
We tune the system on the NIST MT06 test
set (1664 sentences), and test on the MT08 (1357
sentences) and the DEV075 (1211 sentences) test
sets, which are further divided into two sources
(newswire and web data). A trigram language
5It is a test set used by GALE Rosseta Team
8
MT02 MT03 MT04 MT05 MT08-NW MT08-WB Dev07NW Dev07WB
Baseline 28.87 27.82 30.08 26.77 25.09 17.72 24.88 21.76
Dict-Link 29.59 27.67 31.01 27.13 25.14 17.96 25.51 21.88
Table 4: Comparison of the performance of baseline and the alignment generated by new aligner with
dictionary links in BLEU scores
Precision Recall AER
ORL 100.00 62.61 23.00
Ch-En F/NE 89.25 62.47 26.50
F/WE 99.59 62.47 23.22
ORL 100.00 80.98 10.51
En-Ch F/NE 93.49 80.79 13.32
F/WE 99.82 80.79 10.70
F/NE 90.79 87.49 10.89Comb F/WE 99.78 87.23 6.92
ORL 100.00 72.07 16.23
Ar-En F/NE 82.46 72.00 23.13
F/WE 94.25 72.00 18.36
ORL 100.00 90.14 5.18
En-Ar F/NE 79.81 90.06 15.37
F/WE 93.27 90.10 8.34
F/NE 78.91 93.07 14.59Comb F/WE 94.64 93.21 6.08
Table 3: Alignment quality of oracle alignment
and force alignment, the rows with ?ORL? in the
second column are oracle alignments, ?F/NE? and
?F/WE? represent force alignments with empty
links and without empty links correspondingly.
For ?F/NE? and ?F/WE? we also listed the
scores of heuristically symmetrized alignment4.
(?Comb?)
model trained from GigaWord V1 and V2 cor-
pora is used. Table 4 shows the comparison of
the performances on BLEU metric (Papineni et
al., 2002). As we can observe from the results,
the proposed method outperforms the baseline on
all test sets except MT03, and has significant6
improvement on MT02 (+0.72), MT04 (+0.93),
and Dev07NW(+0.63). The average improvement
across all test sets is 0.35 BLEU points.
As a summary, the purpose of the this experi-
ment is to demonstrate an important characteris-
tic of the proposed method. Even with imperfect
manual alignment links, we can get better align-
ment by applying our method. This characteristic
opens a possibility to integrate other more sophis-
ticated aligners.
5 Conclusion
In this study, our major contribution is a novel
generative model extended from IBM Model 4 to
6We used the confidence measurement described in
(Zhang and Vogel, 2004)
Chinese-English
Precision Recall AER
Baseline 68.22 46.88 44.43
Dict-Link 69.93 48.28 42.88
English-Chinese
Precision Recall AER
Baseline 65.35 55.05 40.24
Dict-Link 66.70 56.45 38.85
grow-diag-final-and
Precision Recall AER
Baseline 69.15 57.47 37.23
Dict-Link 70.11 59.54 35.61
Table 5: Comparison on alignment error rate by
using alignment links generated by dictionaries
utilize partial manual alignments. The proposed
method enables us to efficiently enforce subtle
alignment constraints into the EM training. We
performed experiments on manually aligned cor-
pora to prove the validity. We also demonstrated
using the method with simple heuristics to boost
the translation quality on moderate size unlabelled
corpus. The results show that our method is ef-
fective in promoting the word alignment quali-
ties with small amounts of partial alignments and
with high-precision-low-recall heuristics. Also the
method of using dictionary to generate manual
alignment links showed an average improvement
of 0.35 BLEU points across 8 test sets.
The algorithm has small impact on the speed of
GIZA++, and can easily be added to current multi-
thread implementation of GIZA++. Therefore it is
suitable for large scale training.
Future work includes applying the proposed ap-
proach on low resource language pairs and in-
tegrating the algorithm with other rule-based or
discriminative aligners that can generate high-
precision-low-recall partial alignments.
Acknowledgement
This work is supported by DARPA GALE
project and NSF CluE project.
9
References
V. Ambati, S. Vogel, and J. Carbonell. 2010. Ac-
tive semi-supervised learning for improving word
alignment. In Proceedings of the NAACL HLT 2010
Workshop on Active Learning for Natural Language
Processing.
S. Arora, E. Nyberg, and C. P. Rose?. 2009. Estimat-
ing annotation cost for active learning in a multi-
annotator environment. In HLT ?09: Proceedings of
the NAACL HLT 2009 Workshop on Active Learning
for Natural Language Processing, pages 18?26.
P. Blunsom and T. Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 65?72.
P. F. Brown et. al. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation.
In Computational Linguistics, volume 19(2), pages
263?331.
C. Callison-Burch, D. Talbot, and M. Osborne.
2004. Statistical machine translation with word-and
sentence-aligned parallel corpora. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, pages 175?183.
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazons me-
chanical turk. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 286?295.
A. Fraser and D. Marcu. 2006. Semi-supervised
training for statistical word alignment. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 769?776.
A. Fraser and D. Marcu. 2007. Getting the struc-
ture right for word alignment: LEAF. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 51?60.
Q. Gao and S. Vogel. 2008. Parallel implementations
of word alignment tool. In Proceedings of the ACL
2008 Software Engineering, Testing, and Quality As-
surance Workshop, pages 49?57.
Q. Gao and S. Vogel. 2010. Consensus versus exper-
tise : A case study of word alignment with mechan-
ical turk. In NAACL 2010 Workshop on Creating
Speech and Language Data With Mechanical Turk,
pages 30?34.
F. Guzman, Q. Gao, and S. Vogel. 2009. Reassessment
of the role of phrase extraction in pbsmt. In The
twelfth Machine Translation Summit.
F. Huang. 2009. Confidence measure for word align-
ment. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 932?940. Association
for Computational Linguistics.
A. Ittycheriah and S. Roukos. 2005. A maximum en-
tropy word aligner for arabic-english machine trans-
lation. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 89?96.
P. Koehn et. al. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180.
Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models
for word alignment. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 459?466.
R. C Moore. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of the
conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81?88.
J. Niehues and S. Vogel. 2008. Discriminative word
alignment via alignment matrix modeling. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 18?25.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. In Compu-
tational Linguistics, volume 1:29, pages 19?51.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. In Com-
putational Linguistics, volume 30, pages 417?449.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proceedings of ACL?02, pages
311?318, Philadelphia, PA, July.
B. Taskar, S. Lacoste-Julien, and Klein D. 2005. A dis-
criminative matching approach to word alignment.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Nat-
ural Language Processing, pages 73?80.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM
based word alignment in statistical machine trans-
lation. In Proceedings of 16th International Confer-
ence on Computational Linguistics), pages 836?841.
Y. Zhang and S. Vogel. 2004. Measuring confidence
intervals for the machine translation evaluation met-
rics. In Proceedings of The 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation, October.
10
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 307?310,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
CMU System Combination via Hypothesis Selection for WMT?10
Almut Silja Hildebrand
Carnegie Mellon University
Pittsburgh, USA
silja@cs.cmu.edu
Stephan Vogel
Carnegie Mellon University
Pittsburgh, USA
vogel@cs.cmu.edu
Abstract
This paper describes the CMU entry for
the system combination shared task at
WMT?10. Our combination method is hy-
pothesis selection, which uses information
from n-best lists from the input MT sys-
tems, where available. The sentence level
features used are independent from the
MT systems involved. Compared to the
baseline we added source-to-target word
alignment based features and trained sys-
tem weights to our feature set. We com-
bined MT systems for French - English
and German - English using provided data
only.
1 Introduction
For the combination of machine translation sys-
tems there have been several approaches described
in recent publications. One uses confusion net-
works formed along a skeleton sentence to com-
bine translation systems as described in (Rosti et
al., 2008) and (Karakos et al, 2008). A different
approach described in (Heafield et al, 2009) is not
keeping the skeleton fixed when aligning the sys-
tems. Another approach selects whole hypotheses
from a combined n-best list (Hildebrand and Vo-
gel, 2008).
Our setup follows the latter approach. We com-
bine the output from the submitted translation sys-
tems, including n-best lists where available, into
one joint n-best list, then calculate a set of fea-
tures consistently for all hypotheses. We use MER
training on the provided development data to de-
termine feature weights and re-rank the joint n-
best list. We train to maximize BLEU.
2 Features
For our entries to the WMT?09 we used the follow-
ing feature groups (in parenthesis are the number
of separate feature values per group):
? Language model scores (3)
? Word lexicon scores (6)
? Sentence length features (3)
? Rank feature (1)
? Normalized n-gram agreement (6)
? Source-target word alignment features (6)
? Trained system weights (no. of systems)
The details on language model and word lexi-
con scores can be found in (Hildebrand and Vogel,
2008) and details on the rank feature and the nor-
malized n-gram agreement can be found in (Hilde-
brand and Vogel, 2009). We use three sentence
length features, which are the ratio of the hypoth-
esis length to the length of the source sentence,
the diversion of this ratio from the overall length
ratio of the bilingual training data and the differ-
ence between the hypothesis length and the av-
erage length of the hypotheses in the n-best list
for the respective source sentence. The system
weights are trained together with the other feature
weights during MERT using a binary feature per
system. To the feature vector for each hypothe-
sis one feature per input system is added; for each
hypothesis one of the features is one, indicating
which system it came from, all others are zero.
2.1 Source-Target Word Alignment Features
We trained the IBM word alignment models up
to model 4 using the GIZA++ toolkit (Och and
Ney, 2003) on the bilingual training corpus. Then
a forced alignment algorithm utilizes the trained
models to align each source sentence to each trans-
lation hypothesis in its respective n-best list.
We use the alignment score given by the word
alignment models, the number of unaligned words
307
and the number of NULL aligned words, all nor-
malized by the sentence length, as three separate
features. We calculate these alignability features
for both language directions.
3 Experiments
In the WMT shared translation task only a very
small number of participants submitted n-best
lists, e.g. in the German-English track there were
only four n-best lists among the 16 submissions.
Our combination method is proven to work signif-
icantly better when n-best lists are available.
For all our experiments on the data from
WMT?09, which was available for system combi-
nation development as well as the WMT?10 shared
task data we used the same setup and the same sta-
tistical models.
To train our language models and word lexica
we only used provided data. We trained the sta-
tistical word lexica on the parallel data provided
for each language pair1. For each combination we
used three language models: a 4-gram language
model trained on the English part of the parallel
training data, a 1.2 giga-word 3-gram language
model trained on the provided monolingual En-
glish data, and an interpolated 5-gram language
model trained on the English GigaWord corpus.
We used the SRILM toolkit (Stolcke, 2002) for
training. We chose to train three separate LMs
for the three corpora, so the feature weight train-
ing can automatically determine the importance of
each corpus for this task. The reason for training
only a 3-gram LM from the wmt10 monolingual
data was simply that there were not sufficient time
and resources available to train a bigger model.
For each of the two language pairs we compared
a combination that used the word alignment fea-
tures, or trained system weights or both of these
feature groups in addition to the features described
in (Hildebrand and Vogel, 2009) which serves a
baseline for this set of experiments.
For combination we tokenized and lowercased
all data, because the n-best lists were submitted
in various formats. Therefore we report the case
insensitive scores here. The combination was op-
timized toward the BLEU metric, therefore TER
results might not be very meaningful here and are
only reported for completeness.
1http://www.statmt.org/wmt10/translation-
task.html#training
3.1 French-English data from WMT?09
We used 14 systems from the restricted data track
of the WMT?09 including five n-best lists. The
scores of the individual systems for the combina-
tion tuning set range from BLEU 27.93 for the best
to 15.09 for the lowest ranked individual system
(case insensitive evaluation).
system tune test
best single 27.93 / 56.53 27.21 / 56.99
baseline 30.17 / 54.76 28.89 / 55.74
+ wrd al 30.67 / 54.34 28.69 / 55.67
+ sys weights 29.71 / 55.45 28.07 / 56.18
all features 30.30 / 54.53 28.37 / 55.77
Table 1: French-English Results: BLEU / TER
The combination outperforms the best single
system by 1.7 BLEU points. Here adding the 14
binary features for training system weights with
MERT hurts the combinations performance on the
unseen data. The reason for this might be the
rather small tuning set of 502 sentences with one
reference. Adding the word alignment features
does not improve the result either, the difference
to the baseline is at the noise level.
3.2 German-English data from WMT?09
For our experiments on the development data for
German-English we used the top 12 systems, scor-
ing between BLEU 23.01 and BLEU 16.06, ex-
cluding systems known to use data beyond the pro-
vided data. Within those 12 system outputs were
four n-best lists, three of which were 100-best and
one was 10-best.
system tune test
best single 23.01 / 60.52 21.44 / 62.33
baseline 26.28 / 58.69 23.62 / 60.49
+ wrd al 26.25 / 59.13 23.42 / 61.11
+ sys weights 26.78 / 58.48 23.28 / 60.80
all features 26.81 / 58.12 23.51 / 60.25
Table 2: German-English Results: BLEU / TER
Our system combination via hypothesis selec-
tion could improve translation quality by +2.2
BLEU over the best single system on the unseen
test set. Again, the differences between the four
different feature sets are not significant on the un-
seen test set.
308
3.3 French-English WMT?10 system
combination shared task
Out of 14 systems submitted to the French-English
translation task, we combined the top 11 systems,
the best of which scored 28.58 BLEU and the last
24.16 BLEU on the tuning set. There were only
three n-best lists among the submissions. We in-
cluded up to 100 hypotheses per system in our
joint n-best list.
system tune test
best sys. 28.58 / 54.17 29.98 / 52.62 / 53.88
baseline 30.67 / 52.62 29.94 / 52.53 / -
+ w. al 30.69 / 52.76 29.97 / 52.76 / 53.76
+ sys w. 30.90 / 52.44 29.79 / 52.84 / 54.05
all feat. 31.10 / 52.06 29.80 / 52.86 / 53.67
Table 3: French-English Results: BLEU / TER /
MaxSim
Our system combination via hypothesis selec-
tion could not improve the translation quality com-
pared to the best single system on the unseen data.
Adding any of the new feature groups to the base-
line does not change the result of the combination
significantly. This result could be explained by the
fact, that due to computational problems and time
constraints we were not able to train our models on
the whole provided French-English training data.
This should only affect the lexicon and word align-
ment feature groups though.
3.4 German-English WMT?10 system
combination shared task
For the German-English combination we used 13
out of the 16 submitted systems, which scored be-
tween BLEU 25.01 to BLEU 19.76 on the tuning
set. Our combination could improve translation
quality by +1.64 BLEU compared to the best sys-
tem.
system tune test
best sys. 25.01 / 58.34 23.89 / 59.14 / 51.10
baseline 26.47 / 56.89 25.44 / 57.96 / -
+ w. al 26.37 / 57.02 25.25 / 58.34 / 50.72
+ sys w. 27.67 / 56.05 25.53 / 57.70 / 51.06
all feat. 27.66 / 56.35 25.25 / 57.86 / 50.83
Table 4: German-English Results: BLEU / TER /
MaxSim
The word alignment features seem to hurt per-
formance slightly, which might be due to the more
	 
	   	 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 110?117,
COLING 2010, Beijing, August 2010.
New Parameterizations and Features for PSCFG-Based Machine
Translation
Andreas Zollmann Stephan Vogel
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{zollmann,vogel}@cs.cmu.edu
Abstract
We propose several improvements to the
hierarchical phrase-based MT model of
Chiang (2005) and its syntax-based exten-
sion by Zollmann and Venugopal (2006).
We add a source-span variance model
that, for each rule utilized in a prob-
abilistic synchronous context-free gram-
mar (PSCFG) derivation, gives a confi-
dence estimate in the rule based on the
number of source words spanned by the
rule and its substituted child rules, with
the distributions of these source span sizes
estimated during training time.
We further propose different methods of
combining hierarchical and syntax-based
PSCFG models, by merging the grammars
as well as by interpolating the translation
models.
Finally, we compare syntax-augmented
MT, which extracts rules based on target-
side syntax, to a corresponding variant
based on source-side syntax, and experi-
ment with a model extension that jointly
takes source and target syntax into ac-
count.
1 Introduction
The Probabilistic Synchronous Context Free
Grammar (PSCFG) formalism suggests an intu-
itive approach to model the long-distance and lex-
ically sensitive reordering phenomena that often
occur across language pairs considered for statis-
tical machine translation. As in monolingual pars-
ing, nonterminal symbols in translation rules are
used to generalize beyond purely lexical opera-
tions. Labels on these nonterminal symbols are
often used to enforce syntactic constraints in the
generation of bilingual sentences and imply con-
ditional independence assumptions in the statis-
tical translation model. Several techniques have
been recently proposed to automatically iden-
tify and estimate parameters for PSCFGs (or re-
lated synchronous grammars) from parallel cor-
pora (Galley et al, 2004; Chiang, 2005; Zollmann
and Venugopal, 2006; Liu et al, 2006; Marcu et
al., 2006).
In this work, we propose several improvements
to the hierarchical phrase-based MT model of
Chiang (2005) and its syntax-based extension by
Zollmann and Venugopal (2006). We add a source
span variance model that, for each rule utilized
in a probabilistic synchronous context-free gram-
mar (PSCFG) derivation, gives a confidence es-
timate in the rule based on the number of source
words spanned by the rule and its substituted child
rules, with the distributions of these source span
sizes estimated during training (i.e., rule extrac-
tion) time.
We further propose different methods of com-
bining hierarchical and syntax-based PSCFG
models, by merging the grammars as well as by
interpolating the translation models.
Finally, we compare syntax-augmented MT,
which extracts rules based on target-side syntax,
to a corresponding variant based on source-side
syntax, and experiment with a model extension
based on source and target syntax.
We evaluate the different models on the
NIST large resource Chinese-to-English transla-
tion task.
110
2 Related work
Chiang et al (2008) introduce structural dis-
tortion features into a hierarchical phrase-based
model, aimed at modeling nonterminal reordering
given source span length, by estimating for each
possible source span length ` a Bernoulli distribu-
tion p(R|`) where R takes value one if reorder-
ing takes place and zero otherwise. Maximum-
likelihood estimation of the distribution amounts
to simply counting the relative frequency of non-
terminal reorderings over all extracted rule in-
stances that incurred a substitution of span length
`. In a more fine-grained approach they add a sep-
arate binary feature ?R, `? for each combination of
reordering truth value R and span length ` (where
all ` ? 10 are merged into a single value), and
then tune the feature weights discriminatively on a
development set. Our approach differs from Chi-
ang et al (2008) in that we estimate one source
span length distribution for each substitution site
of each grammar rule, resulting in unique distri-
butions for each rule, estimated from all instances
of the rule in the training data. This enables our
model to condition reordering range on the in-
dividual rules used in a derivation, and even al-
lows to distinguish between two rules r1 and r2
that both reorder arguments with identical mean
span lengths `, but where the span lengths encoun-
tered in extracted instances of r1 are all close to `,
whereas span length instances for r2 vary widely.
Chen and Eisele (2010) propose a hypbrid ap-
proach between hierarchical phrase based MT
and a rule based MT system, reporting improve-
ment over each individual model on an English-
to-German translation task. Essentially, the rule
based system is converted to a single-nonterminal
PSCFG, and hence can be combined with the
hierarchical model, another single-nonterminal
PSCFG, by taking the union of the rule sets
and augmenting the feature vectors, adding zero-
values for rules that only exist in one of the two
grammars. We face the challenge of combining
the single-nonterminal hierarchical grammar with
a multi-nonterminal syntax-augmented grammar.
Thus one hierarchical rule typically corresponds
to many syntax-augmented rules. The SAMT sys-
tem used by Zollmann et al (2008) adds hierar-
chical rules separately to the syntax-augmented
grammar, resulting in a backbone grammar of
well-estimated hierarchical rules supporting the
sparser syntactic rules. They allow the model
preference between hierarchical and syntax rules
to be learned from development data by adding
an indicator feature to all rules, which is one
for hierarchical rules and zero for syntax rules.
However, no empirical comparison is given be-
tween the purely syntax-augmented and the hy-
brid grammar. We aim to fill this gap by experi-
menting with both models, and further refine the
hybrid approach by adding interpolated probabil-
ity models to the syntax rules.
Chiang (2010) augments a hierarchical phrase-
based MT model with binary syntax features rep-
resenting the source and target syntactic con-
stituents of a given rule?s instantiations during
training, thus taking source and target syntax
into account while avoiding the data-sparseness
and decoding-complexity problems of multi-
nonterminal PSCFG models. In our approach, the
source- and target-side syntax directly determines
the grammar, resulting in a nonterminal set de-
rived from the labels underlying the source- and
target-language treebanks.
3 PSCFG-based translation
Given a source language sentence f , statistical
machine translation defines the translation task as
selecting the most likely target translation e under
a model P (e|f), i.e.:
e?(f) = argmax
e
P (e|f) = argmax
e
m?
i=1
hi(e, f)?i
where the argmax operation denotes a search
through a structured space of translation outputs
in the target language, hi(e, f) are bilingual fea-
tures of e and f and monolingual features of
e, and weights ?i are typically trained discrim-
inatively to maximize translation quality (based
on automatic metrics) on held out data, e.g., us-
ing minimum-error-rate training (MERT) (Och,
2003).
In PSCFG-based systems, the search space is
structured by automatically extracted rules that
model both translation and re-ordering operations.
111
Most large scale systems approximate the search
above by simply searching for the most likely
derivation of rules, rather than searching for the
most likely translated output. There are efficient
algorithms to perform this search (Kasami, 1965;
Chappelier and Rajman, 1998) that have been ex-
tended to efficiently integrate n-gram language
model features (Chiang, 2007; Venugopal et al,
2007; Huang and Chiang, 2007; Zollmann et al,
2008; Petrov et al, 2008).
In this work we experiment with PSCFGs
that have been automatically learned from word-
aligned parallel corpora. PSCFGs are defined by a
source terminal set (source vocabulary) TS , a tar-
get terminal set (target vocabulary) TT , a shared
nonterminal set N and rules of the form: X ?
??, ?,w? where
? X ? N is a labeled nonterminal referred to as
the left-hand-side of the rule.
? ? ? (N ? TS)? is the source side of the rule.
? ? ? (N ? TT )? is the target side of the rule.
? w ? [0,?) is a non-negative real-valued
weight assigned to the rule; in our model, w is
the exponential function of the inner product of
features h and weights ?.
3.1 Hierarchical phrase-based MT
Building upon the success of phrase-based meth-
ods, Chiang (2005) presents a PSCFG model of
translation that uses the bilingual phrase pairs
of phrase-based MT as starting point to learn
hierarchical rules. For each training sentence
pair?s set of extracted phrase pairs, the set of in-
duced PSCFG rules can be generated as follows:
First, each phrase pair is assigned a generic X-
nonterminal as left-hand-side, making it an initial
rule. We can now recursively generalize each al-
ready obtained rule (initial or including nontermi-
nals)
N ? f1 . . . fm/e1 . . . en
for which there is an initial rule
M ? fi . . . fu/ej . . . ev
where 1 ? i < u ? m and 1 ? j < v ? n, to
obtain a new rule
N ? f i?11 Xkfmu+1/ej?11 Xkenv+1
where e.g. f i?11 is short-hand for f1 . . . fi?1, and
where k is an index for the nonterminal X that
indicates the one-to-one correspondence between
the new X tokens on the two sides (it is not in
the space of word indices like i, j, u, v,m, n). The
recursive form of this generalization operation al-
lows the generation of rules with multiple nonter-
minal pairs.
Chiang (2005) uses features analogous to the
ones used in phrase-based translation: a lan-
guage model neg-log probability, a ?rule given
source-side? neg-log-probability, a ?rule given
target-side? neg-log-probability, source- and tar-
get conditioned ?lexical? neg-log-probabilities
based on word-to-word co-occurrences (Koehn et
al., 2003), as well as rule, target word, and glue
operation counters. We follow Venugopal and
Zollmann (2009) to further add a rareness penalty,
1/ count(r)
where count(r) is the occurrence count of rule
r in the training corpus, allowing the system to
learn penalization of low-frequency rules, as well
as three indicator features firing if the rule has
one, two unswapped, and two swapped nontermi-
nal pairs, respectively.1
3.2 Syntax Augmented MT
Syntax Augmented MT (SAMT) (Zollmann and
Venugopal, 2006) extends Chiang (2005) to in-
clude nonterminal symbols from target language
phrase structure parse trees. Each target sentence
in the training corpus is parsed with a stochas-
tic parser to produce constituent labels for target
spans. Phrase pairs (extracted from a particular
sentence pair) are assigned left-hand-side nonter-
minal symbols based on the target side parse tree
constituent spans.
Phrase pairs whose target side corresponds to
a constituent span are assigned that constituent?s
label as their left-hand-side nonterminal. If the
target side of the phrase pair is not spanned by
a single constituent in the corresponding parse
tree, we use the labels of subsuming, subsumed,
and neighboring parse tree constituents to assign
1Penalization or reward of purely-lexical rules can be in-
directly learned by trading off these features with the rule
counter feature.
112
an extended label of the form C1 + C2, C1/C2,
or C2\C1 (the latter two being motivated from
the operations in combinatory categorial gram-
mar (CCG) (Steedman, 2000)), indicating that the
phrase pair?s target side spans two adjacent syn-
tactic categories (e.g., she went: NP+VB), a par-
tial syntactic category C1 missing aC2 at the right
(e.g., the great: NP/NN), or a partial C1 missing
a C2 at the left (e.g., great wall: DT\NP), respec-
tively. The label assignment is attempted in the or-
der just described, i.e., assembling labels based on
?+? concatenation of two subsumed constituents is
preferred, as smaller constituents tend to be more
accurately labeled. If no label is assignable by ei-
ther of these three methods, a default label ?FAIL?
is assigned.
In addition to the features used in hierarchical
phrase-based MT, SAMT introduces a relative-
frequency estimated probability of the rule given
its left-hand-side nonterminal.
4 Modeling Source Span Length of
PSCFG Rule Substitution Sites
Extracting a rule with k right-hand-side nonter-
minal pairs, i.e., substitution sites, (from now on
called order-k rule) by the method described in
Section 3 involves k + 1 phrase pairs: one phrase
pair used as initial rule and k phrase pairs that are
sub phrase pairs of the first and replaced by non-
terminal pairs. Conversely, during translation, ap-
plying this rule amounts to combining k hypothe-
ses from k different chart cells, each represented
by a source span and a nonterminal, to form a new
hypothesis and file it into a chart cell. Intuitively,
we want the source span lengths of these k + 1
chart cells to be close to the source side lengths of
the k+1 phrase pairs from the training corpus that
were involved in extracting the rule. Of course,
each rule generally was extracted from multiple
training corpus locations, with different involved
phrase pairs of different lengths. We therefore
model k + 1 source span length distributions for
each order-k rule in the grammar.
Ignoring the discreteness of source span length
for the sake of easier estimation, we assume the
distribution to be log-normal. This is motivated
by the fact that source span length is positive and
that we expect its deviation between instances of
the same rule to be greater for long phrase pairs
than for short ones.
We can now add k? + 1 features to the transla-
tion framework, where k? is the maximum num-
ber of PSCFG rule nonterminal pairs, in our case
two. Each feature is computed during translation
time. Ideally, it should represent the probabil-
ity of the hypothesized rule given the respective
chart cell span length. However, as each com-
peting rule underlies a different distribution, this
would require a Bayesian setting, in which priors
over distributions are specified. In this prelimi-
nary work we take a simpler approach: Based on
the rule?s span distribution, we compute the prob-
ability that a span length no likelier than the one
encountered was generated from the distribution.
This probability thus yields a confidence estimate
for the rule. More formally, let ? be the mean and
? the standard deviation of the logarithm of the
span length random variableX concerned, and let
x be the span length encountered during decoding.
Then the computed confidence estimate is given
by
P (| ln(X)? ?| ? | ln(x)? ?|)
= 2 ? Z (?(| ln(x)? ?|)/?)
where Z is the cumulative density function of the
normal distribution with mean zero and variance
one.
The confidence estimate is one if the encoun-
tered span length is equal to the mean of the dis-
tribution, and decreases as the encountered span
length deviates further from the mean. The sever-
ity of that decline is determined by the distribution
variance: the higher the variance, the less a devia-
tion from the mean is penalized.
Mean and variance of log source span length are
sufficient statistics of the log-normal distribution.
As we extract rules in a distributed fashion, we
use a straightforward parallelization of the online
algorithm of Welford (1962) and its improvement
by West (1979) to compute the sample variance
over all instances of a rule.
113
5 Merging a Hierarchical and a
Syntax-Based Model
While syntax-based grammars allow for more re-
fined statistical models and guide the search by
constraining substitution possibilitites in a gram-
mar derivation, grammar sizes tend to be much
greater than for hierarchical grammars. Therefore
the average occurrence count of a syntax rule is
much lower than that of a hierarchical rule, and
thus estimated probabilitites are less reliable.
We propose to augment the syntax-based ?rule
given source side? and ?rule given target side? dis-
tributions by hierarchical counterparts obtained by
marginalizing over the left-hand-side and right-
hand-side rule nonterminals. For example, the
hierarchical equivalent of the ?rule given source
side? probability is obtained by summing occur-
rence counts over all rules that have the same
source and target terminals and substitution posi-
tions but possibly differ in the left- and/or right-
hand side nonterminal labels, divided by the sum
of occurrence counts of all rules that have the
same source side terminals and source side substi-
tution positions. Similarly, an alternative rareness
penalty based on the combined frequency of all
rules with the same terminals and substitution po-
sitions is obtained.
Using these syntax and hierarchical features
side by side amounts to interpolation of the re-
spective probability models in log-space, with
minimum-error-rate training (MERT) determining
the optimal interpolation coefficient. We also add
respective models interpolated with coefficient .5
in probability-space as additional features to the
system.
We further experiment with adding hierarchical
rules separately to the syntax-augmented gram-
mar, as proposed in Zollmann et al (2008), with
the respective syntax-specific features set to zero.
A ?hierarchical-indicator? feature is added to all
rules, which is one for hierarchical rules and zero
for syntax rules, allowing the joint model to trade
of hierarchical against syntactic rules. During
translation, the hierarchical and syntax worlds are
bridged by glue rules, which allow monotonic
concatenation of hierarchical and syntactic partial
sentence hypotheses. We separate the glue feature
used in hierarchical and syntax-augmented trans-
lation into a glue feature that only fires when a hi-
erarchical rule is glued, and a distinct glue feature
firing when gluing a syntax-augmented rule.
6 Extension of SAMT to a bilingually
parsed corpus
Syntax-based MT models have been proposed
both based on target-side syntactic annotations
(Galley et al, 2004; Zollmann and Venugopal,
2006) as well source-side annotations (Liu et al,
2006). Syntactic annotations for both source and
target language are available for popular language
pairs such as Chinese-English. In this case, our
grammar extraction procedure can be easily ex-
tended to impose both source and target con-
straints on the eligible substitutions simultane-
ously.
Let Nf be the nonterminal label that would be
assigned to a given initial rule when utilizing the
source-side parse tree, and Ne the assigned label
according to the target-side parse. Then our bilin-
gual model assigns ?Nf + Ne? to the initial rule.
The extraction of complex rules proceeds as be-
fore. The number of nonterminals in this model,
based on a source-model label set of size s and a
target label set of size t, is thus given by st.
7 Experiments
We evaluate our approaches by comparing trans-
lation quality according to the IBM-BLEU (Pap-
ineni et al, 2002) metric on the NIST Chinese-
to-English translation task using MT04 as devel-
opment set to train the model parameters ?, and
MT05, MT06 and MT08 as test sets.
We perform PSCFG rule extraction and de-
coding using the open-source ?SAMT? system
(Venugopal and Zollmann, 2009), using the pro-
vided implementations for the hierarchical and
syntax-augmented grammars. For all systems, we
use the bottom-up chart parsing decoder imple-
mented in the SAMT toolkit with a reordering
limit of 15 source words, and correspondingly ex-
tract rules from initial phrase pairs of maximum
source length 15. All rules have at most two non-
terminal symbols, which must be non-consecutive
on the source side, and rules must contain at least
114
one source-side terminal symbol.
For parameter tuning, we use the L0-
regularized minimum-error-rate training tool pro-
vided by the SAMT toolkit.
The parallel training data comprises of 9.6M
sentence pairs (206M Chinese Words, 228M En-
glish words). The source and target language
parses for the syntax-augmented grammar were
generated by the Stanford parser (Klein and Man-
ning, 2003).
The results are given in Table 1. The source
span models (indicated by +span) achieve small
test set improvements of 0.15 BLEU points on av-
erage for the hierarchical and 0.26 BLEU points
for the syntax-augmented system, but these are
not statistically significant.
Augmenting a syntax-augmented grammar
with hierarchical features (?Syntax+hiermodels?)
results in average test set improvements of 0.5
BLEU points. These improvements are not sta-
tistically significant either, but persist across all
three test sets. This demonstrates the benefit of
more reliable feature estimation. Further aug-
menting the hierarchical rules to the grammar
(?Syntax+hiermodels+hierrules?) does not yield
additional improvements.
The use of bilingual syntactic parses (?Syn-
tax/src&tgt?) turns out detrimental to translation
quality. We assume this is due to the huge number
of nonterminals in these grammars and the great
amount of badly-estimated low-occurrence-count
rules. Perhaps merging this grammar with a regu-
lar syntax-augmented grammar could yield better
results.
We also experimented with a source-parse
based model (?Syntax/src?). While not being able
to match translation quality of its target-based
counterpart, the model still outperforms the hier-
archical system on all test sets.
8 Conclusion
We proposed several improvements to the hierar-
chical phrase-based MT model of Chiang (2005)
and its syntax-based extension by Zollmann and
Venugopal (2006). We added a source span length
model that, for each rule utilized in a probabilis-
tic synchronous context-free grammar (PSCFG)
derivation, gives a confidence estimate in the rule
based on the number of source words spanned by
the rule and its substituted child rules, resulting in
small improvements for hierarchical phrase-based
as well as syntax-augmented MT.
We further demonstrated the utility of combin-
ing hierarchical and syntax-based PSCFG models
and grammars.
Finally, we compared syntax-augmented MT,
which extracts rules based on target-side syntax,
to a corresponding variant based on source-side
syntax, showing that target syntax is more ben-
efitial, and unsuccessfully experimented with a
model extension that jointly takes source and tar-
get syntax into account.
Hierarchical phrase-based MT suffers from
spurious ambiguity: A single translation for a
given source sentence can usually be accom-
plished by many different PSCFG derivations.
This problem is exacerbated by syntax-augmented
MT with its thousands of nonterminals, and made
even worse by its joint source-and-target exten-
sion. Future research should apply the work of
Blunsom et al (2008) and Blunsom and Osborne
(2008), who marginalize over derivations to find
the most probable translation rather than the most
probable derivation, to these multi-nonterminal
grammars.
All source code underlying this work is avail-
able under the GNU Lesser General Public Li-
cense as part of the ?SAMT? system at:
www.cs.cmu.edu/?zollmann/samt
Acknowledgements
This work is in part supported by NSF un-
der the Cluster Exploratory program (grant NSF
0844507), and in part by the US DARPA GALE
program. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of NSF or DARPA.
References
Blunsom, Phil and Miles Osborne. 2008. Probabilistic
inference for machine translation. In EMNLP ?08:
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 215?
223, Morristown, NJ, USA. Association for Com-
putational Linguistics.
115
Dev (MT04) MT05 MT06 MT08 TestAvg Time
Hierarchical 38.63 36.51 33.26 25.77 31.85 14.3
Hier+span 39.03 36.44 33.29 26.26 32.00 16.7
Syntax 39.17 37.17 33.87 26.81 32.62 59
Syntax+hiermodels 39.61 37.74 34.30 27.30 33.11 68.4
Syntax+hiermodels+hierrules 39.69 37.56 34.66 26.93 33.05 34.6
Syntax+span+hiermodels+hierrules 39.81 38.02 34.50 27.41 33.31 39.6
Syntax/src+span+hiermodels+hierrules 39.62 37.25 33.99 26.44 32.56 20.1
Syntax/src&tgt+span+hiermodels+hierrules 39.15 36.92 33.70 26.24 32.29 17.5
Table 1: Translation quality in % case-insensitive IBM-BLEU (i.e., brevity penalty based on closest reference length) for
different systems on Chinese-English NIST-large translation tasks. ?TestAvg? shows the average score over the three test sets.
?Time? is the average decoding time per sentence in seconds on one CPU.
Blunsom, Phil, Trevor Cohn, and Miles Osborne.
2008. A discriminative latent variable model for
statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Brown, Peter F., Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2).
Chappelier, J.C. and M. Rajman. 1998. A general-
ized CYK algorithm for parsing stochastic CFG. In
Proceedings of Tabulation in Parsing and Deduction
(TAPD), pages 133?137, Paris.
Chen, Yu and Andreas Eisele. 2010. Hierarchical hy-
brid translation between english and german. In
Hansen, Viggo and Francois Yvon, editors, Pro-
ceedings of the 14th Annual Conference of the Eu-
ropean Association for Machine Translation, pages
90?97. EAMT, EAMT, 5.
Chiang, David, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 224?233, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Chiang, David. 2007. Hierarchical phrase based trans-
lation. Computational Linguistics, 33(2).
Chiang, David. 2010. Learning to translate with
source and target syntax. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1443?1452, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Galley, Michael, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics Confer-
ence (HLT/NAACL).
Huang, Liang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Kasami, T. 1965. An efficient recognition
and syntax-analysis algorithm for context-free lan-
guages. Technical report, Air Force Cambridge Re-
search Lab.
Klein, Dan and Christoper Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Koehn, Philipp, Franz J. Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics Conference
(HLT/NAACL).
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics.
Marcu, Daniel, Wei Wang, Abdessamad Echihabi,
and Kevin Knight. 2006. SPMT: Statistical ma-
chine translation with syntactified target language
phrases. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Sydney, Australia.
116
Och, Franz J. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Petrov, Slav, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).
Steedman, Mark. 2000. The Syntactic Process. MIT
Press.
Venugopal, Ashish and Andreas Zollmann. 2009.
Grammar based statistical MT on Hadoop: An end-
to-end toolkit for large scale PSCFG based MT.
The Prague Bulletin of Mathematical Linguistics,
91:67?78.
Venugopal, Ashish, Andreas Zollmann, and Stephan
Vogel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics Conference
(HLT/NAACL).
Welford, B. P. 1962. Note on a method for calculating
corrected sums of squares and products. Techno-
metrics, 4(3):419?420.
West, D. H. D. 1979. Updating mean and variance
estimates: an improved method. Commun. ACM,
22(9):532?535.
Zollmann, Andreas and Ashish Venugopal. 2006.
Syntax augmented machine translation via chart
parsing. In Proceedings of the Workshop on Sta-
tistical Machine Translation, HLT/NAACL.
Zollmann, Andreas, Ashish Venugopal, Franz J. Och,
and Jay Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and syntax-augmented
statistical MT. In Proceedings of the Conference on
Computational Linguistics (COLING).
117
A Multi-layer Chinese Word Segmentation System Optimized for
Out-of-domain Tasks
Qin Gao
Language Technologies Institute
Carnegie Mellon University
qing@cs.cmu.edu
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
stephan.vogel@cs.cmu.edu
Abstract
State-of-the-art Chinese word segmenta-
tion systems have achieved high perfor-
mance when training data and testing data
are from the same domain. However, they
suffer from the generalizability problem
when applied on test data from different
domains. We introduce a multi-layer Chi-
nese word segmentation system which can
integrate the outputs from multiple hetero-
geneous segmentation systems. By train-
ing a second layer of large margin clas-
sifier on top of the outputs from several
Conditional Random Fields classifiers, it
can utilize a small amount of in-domain
training data to improve the performance.
Experimental results show consistent im-
provement on F1 scores and OOV recall
rates by applying the approach.
1 Introduction
The Chinese word segmentation problem has been
intensively investigated in the past two decades.
From lexicon-based methods such as Bi-Directed
Maximum Match (BDMM) (Chen et al, 2005) to
statistical models such as Hidden Markove Model
(HMM) (Zhang et al, 2003), a broad spectrum
of approaches have been experimented. By cast-
ing the problem as a character labeling task, se-
quence labeling models such as Conditional Ran-
dom Fields can be applied on the problem (Xue
and Shen, 2003). State-of-the-art CRF-based sys-
tems have achieved good performance. However,
like many machine learning problems, generaliz-
ability is crucial for a domain-independent seg-
mentation system. Because the training data usu-
ally come from limited domains, when the domain
of test data is different from the training data, the
results are still not satisfactory.
A straight-forward solution is to obtain more la-
beled data in the domain we want to test. However
this is not easily achievable because the amount
of data needed to train a segmentation system are
large. In this paper, we focus on improving the
system performance by using a relatively small
amount of manually labeled in-domain data to-
gether with larger out-of-domain corpus1. The
effect of mingling the small in-domain data into
large out-of-domain data may be neglectable due
to the difference in data size. Hence, we try to
explore an alternative way that put a second layer
of classifier on top of the segmentation systems
built on out-of-domain corpus (we will call them
sub-systems). The classifier should be able to uti-
lize the information from the sub-systems and op-
timize the performance with a small amount of in-
domain data.
The basic idea of our method is to integrate
a number of different sub-systems whose per-
formance varies on the new domain. Figure 1
demonstrates the system architecture. There are
two layers in the system. In the lower layer,
the out-of-domain corpora are used, together with
other resources to produce heterogeneous sub-
systems. In the second layer the outputs of the
sub-systems in the first layer are treated as input
to the classifier. We train the classifier with small
in-domain data. All the sub-systems should have
1From this point, we use the term out-of-domain corpus
to refer to the general and large training data that are not
related to the test domain, and the term in-domain corpus
to refer to small amount of data that comes from the same
domain of the test data
reasonable performance on all domains, but their
performance on different domains may vary. The
job of the second layer is to find the best decision
boundary on the target domain, in presence of all
the decisions made by the sub-systems.
Num
ber
?
Tag
?Fea
ture
Clas
sifie
r?1
Trai
nin
g?da
ta?
Cha
ract
er?
Typ
e?Fe
atu
re
g
(in?
dom
ain)
Ent
rop
y?
Fea
ture
Clas
sifie
r?2
Trai
nin
gda
ta
Clas
sifie
r3
Inte
gra
ted
?
Trai
nin
g?da
ta?
(ou
t?of
?
dom
ain)
Clas
sifie
r?3
clas
sifie
r
Wo
rd?l
ist?1
Clas
sifie
r?4
Wo
rd?l
ist?2
Clas
sifie
r5
Wo
rd?l
ist?2
Clas
sifie
r?5
Figure 1: The architecture of the system, the first
layer (sub-systems) is trained on general out-of-
domain corpus and various resources, while the
second layer of the classifier is trained on in-
domain corpus.
Conditional Random Fields (CRF) (Lafferty et
al., 2001) has been applied on Chinese word seg-
mentation and achieved high performance. How-
ever, because of its conditional nature the small
amount of in-domain corpus will not significantly
change the distributions of the model parame-
ters trained on out-of-domain corpus, it is more
suitable to be used in the sub-systems than in
the second-layer classifier. Large margin models
such as Support Vector Machine (SVM) (Vapnik,
1995) can be trained on small corpus and gener-
alize well. Therefore we chose to use CRF in
building sub-systems and SVM in building the
second-layer. We built multiple CRF-based Chi-
nese word segmentation systems using different
features, and then use the marginal probability of
each tag of all the systems as features in SVM.
The SVM is then trained on small in-domain cor-
pus, results in a decision hyperplane that mini-
mizes the loss in the small training data. To in-
tegrate the dependencies of output tags, we use
SVM-HMM (Altun et al, 2003) to capture the in-
teractions between tags and features. By apply-
ing SVM-HMM we can bias our decision towards
most informative CRF-based system w.r.t. the tar-
get domain. Our methodology is similar to (Co-
hen and Carvalho, 2005), who applied a cross-
validation-like method to train sequential stacking
models, while we directly use small amount of in-
domain data to train the second-layer classifiers.
The paper is organized as follows, first we will
discuss the CRF-based sub-systems we used in
section 2, and then the SVM-based system com-
bination method in section 3. Finally, in section 4
the experimental results are presented.
2 CRF-based sub-systems
In this section we describe the sub-systems we
used in system. All of the sub-systems are based
on CRF with different features. The tag set we
use is the 6-tag (B1, B2, B3, M, E, S) set pro-
posed by Zhao et al(2006). All of the sub-systems
use the same tag set, however as we will see later,
the second-layer classifier in our system does not
require the sub-systems to have a common tag
set. Also, all of the sub-systems include a com-
mon set of character features proposed in (Zhao
and Kit, 2008). The offsets and concatenations
of the six n-gram features (the feature template)
are: C?1, C0, C1, C?1C0, C0C1, C?1C1. In the
remaining part of the section we will introduce
other features that we employed in different sub-
systems.
2.1 Character type features
By simply classify the characters into four types:
Punctuation (P), Digits (D), Roman Letters (L)
and Chinese characters (C), we can assign char-
acter type tags to every character. The idea is
straight-forward. We denote the feature as CTF .
Similar to character feature, we also use differ-
ent offsets and concatenations for character type
features. The feature template is identical to
character feature, i.e. CTF?1, CTF0, CTF1,
CTF?1CTF0, CTF0CTF1, CTF?1CTF1 are
used as features in CRF training.
2.2 Number tag feature
Numbers take a large portion of the OOV words,
which can easily be detected by regular expres-
sions or Finite State Automata. However there
are often ambiguities on the boundary of numbers.
Therefore, instead of using detected numbers as
final answers, we use them as features. The num-
ber detector we developed finds the longest sub-
strings in a sentence that are:
? Chinese Numbers (N)
? Chinese Ordinals (O)
? Chinese Dates (D)
For each character of the detected num-
bers/ordinal/date, we assign a tag that reflects the
position of the character in the detected num-
ber/ordinal/date. We adopt the four-tag set (B, M,
E, S). The position tags are appended to end of
the number/ordinal/date tags to form the number
tag feature of that character. I.e. there are totally
13 possible values for the number tag feature, as
listed in Table 1.2
Number Ordinal Date Other
Begin NB OB DB
Middle NM OM DM
End NE OE DE XX
Single NS OS? DS?
Table 1: The feature values used in the number tag
feature, note that OS and DS are never observed
because there is no single character ordinal/date
by our definition.
Similar to character feature and character type
feature, the feature template mention before is
also applied on the number tag feature. We de-
note the number tag features as NF .
2.3 Conditional Entropy Feature
We define the Forward Conditional Entropy of
a character C by the entropy of all the charac-
ters that follow C in a given corpus, and the
Backward Conditional Entropy as the entropy
of all the characters that precede C in a given
corpus. The conditional entropy can be com-
puted easily from a character bigram list gener-
ated from the corpus. Assume we have a bigram
2Two of the tags, OS and DS are never observed.
list B = {B1, B2, ? ? ? , BN}, where every bigram
entry Bk = {cik , cjk , nk} is a triplet of the two
consecutive characters cik and cjk and the count of
the bigram in the corpus, nk. The Forward Condi-
tional Entropy of the character C is defined by:
Hf (C) :=
?
cik=C
nk
Z
log
nk
Z
where Z =
?
cik=C
nk is the normalization fac-
tor.
And the Backward Conditional Entropy can be
computed similarly.
We assign labels to every character based on
the conditional entropy of it. If the conditional
entropy value is less than 1.0, we assign fea-
ture value 0 to the character, and for region
[1.0, 2.0), we assign feature value 1. Similarly we
define the region-to-value mappings as follows:
[2.0, 3.5) ? 2, [3.5, 5.0) ? 4, [5.0, 7.0) ? 5,
[7.0,+?) ? 6. The forward and backward con-
ditional entropy forms two features. We will refer
to these features as EF .
2.4 Lexical Features
Lexical features are the most important features to
make sub-systems output different results on dif-
ferent domains. We adopt the definition of the fea-
tures partially from (Shi and Wang, 2007). In our
system we use only the Lbegin(C0) and Lend(C0)
features, omitting the LmidC0 feature. The two
features represent the maximum length of words
found in the lexicon that contain the current char-
acter as the first or last character, correspondingly.
For feature values equal or greater than 6, we
group them into one value.
Although we can find a number of Chinese lex-
icons available, they may or may not be gener-
ated according to the same standard as the train-
ing data. Concatenating them into one may bring
in noise and undermine the performance. There-
fore, every lexicon will generate its own lexical
features.
3 SVM-based System Combination
Generalization is a fundamental problem of Chi-
nese word segmentation. Since the training data
may come from different domains than the test
data, the vocabulary and the distribution can also
be different. Ideally, if we can have labeled data
from the same domain, we can train segmenters
specific to the domain. However obtaining suffi-
cient amount of labeled data in the target domain
is time-consuming and expensive. In the mean
time, if we only label a small amount of data in the
target domain and put them into the training data,
the effect may be too small because the size of
out-of-domain data can overwhelm the in-domain
data.
In this paper we propose a different way of
utilizing small amount of in-domain corpus. We
put a second-layer classifier on top of the CRF-
based sub-systems, the output of CRF-based sub-
systems are treated as features in an SVM-HMM
(Altun et al, 2003) classifier. We can train the
SVM-HMM classifier on a small amount of in-
domain data. The training procedure can be
viewed as finding the optimal decision boundary
that minimize the hinge loss on the in-domain
data. Because the number of features for SVM-
HMM is significantly smaller than CRF, we can
train the model with as few as several hundred
sentences.
Similar to CRF, the SVM-HMM classifier still
treats the Chinese word segmentation problem as
character tagging. However, because of the limi-
tation of training data size, we try to minimize the
number of classes. We chose to adopt the two-tag
set, i.e. class 1 indicates the character is the end of
a word and class 2 means otherwise. Also, due to
limited amount of training data, we do not use any
character features, instead, the features comes di-
rectly from the output of sub-systems. The SVM-
HMM can use any real value features, which en-
ables integration of a wide range of segmenters.
In this paper we use only the CRF-based seg-
menters, and the features are the marginal prob-
abilities (Sutton and McCallum, 2006) of all the
tags in the tag set for each character. As an ex-
ample, for a CRF-based sub-system that outputs
six tags, it will output six features for each char-
acter for the SVM-HMM classifier, corresponding
to the marginal probability of the character given
the CRF model. The marginal probabilities for
the same tag (e.g. B1, S, etc) come from differ-
ent CRF-based sub-systems are treated as distinct
features.
Features Lexicons
S1 CF, CTF None
S2 CF, NF ADSO, CTB6
S3 CF, CTF, NF ADSO
S4 CF, CTF, NF, EF ADSO, CTB6
S5 CF, EF None
S6 CF, NF None
S7 CF, CTF ADSO
S8 CF, CTF CTB6
Table 2: The configurations of CRF-based sub-
systems. S1 to S4 are used in the final submission
of the Bake-off, S5 through S8 are also presented
to show the effects of individual features.
When we encounter data from a new domain,
we first use one of the CRF-based sub-system to
segment a portion of the data, and manually cor-
rect obvious segmentation errors. The manually
labeled data are then processed by all the CRF-
based sub-systems, so as to obtain features of ev-
ery character. After that, we train the SVM-HMM
model using these features.
During decoding, the Chinese input will also be
processed by all of the CRF-based sub-systems,
and the outputs will be fed into the SVM-HMM
classifier. The final decisions of word boundaries
are based solely on the classified labels of SVM-
HMM model.
For the Bake-off system, we labeled two hun-
dred sentences in each of the unsegmented train-
ing set (A and B). Since only one submission is
allowed, the SVM-HMM model of the final sys-
tem was trained on the concatenation of the two
training sets, i.e. four hundred sentences.
The CRF-based sub-systems are trained using
CRF++ toolkit (Kudo, 2003), and the SVM-HMM
trained by the SVMstruct toolkit (Joachims et al,
2009).
4 Experiments
To evaluate the effectiveness of the proposed sys-
tem combination method, we performed two ex-
periments. First, we evaluate the system combina-
tion method on provided training data in the way
that is similar to cross-validation. Second, we ex-
perimented with training the SVM-HMM model
with the manually labeled data come from cor-
Micro-Average Macro-Average
P R F1 OOV-R P R F1 OOV-R
S1 0.962 0.960 0.961 0.722 0.962 0.960 0.960 0.720
S2 0.965 0.966 0.966 0.725 0.965 0.966 0.966 0.723
S3 0.966 0.967 0.967 0.731 0.966 0.967 0.967 0.729
S4 0.968 0.969 0.968 0.731 0.967 0.969 0.969 0.729
S5 0.962 0.960 0.961 0.720 0.962 0.960 0.960 0.718
S6 0.963 0.961 0.962 0.730 0.963 0.961 0.961 0.729
S7 0.966 0.967 0.966 0.723 0.966 0.967 0.967 0.720
S8 0.963 0.960 0.962 0.727 0.963 0.960 0.960 0.726
CB 0.969 0.969 0.969 0.741 0.969 0.969 0.969 0.739
Table 3: The performance of individual sub-systems and combined system. The Micro-Average results
come from concatenating all the outputs of the ten-fold systems and then compute the scores, and the
Macro-Average results are calculated by first compute the scores in every of the ten-fold systems and
then average the scores.
Set A Set B
P R F1 OOV-R P R F1 OOV-R
S1 0.925 0.920 0.923 0.625 0.936 0.938 0.937 0.805
S2 0.934 0.934 0.934 0.641 0.941 0.930 0.935 0.751
S3 0.940 0.937 0.938 0.677 0.938 0.926 0.932 0.752
S4 0.942 0.940 0.941 0.688 0.944 0.929 0.936 0.776
CB1 0.943 0.941 0.942 0.688 0.948 0.936 0.942 0.794
CB2 0.941 0.940 0.941 0.692 0.939 0.949 0.944 0.821
CB3 0.943 0.939 0.941 0.699 0.950 0.950 0.950 0.820
Table 4: The performance of individual systems and system combination on Bake-off test data, CB1,
CB2, and CB3 are system combination trained on labeled data from domain A, B, and the concatenation
of the data from both domains.
responding domains, and tested the resulting sys-
tems on the Bake-off test data.
For experiment 1, We divide the training set
into 11 segments, segment 0 through 9 contains
1733 sentences, and segment 10 has 1724 sen-
tence. We perform 10-fold cross-validation on
segment 0 to 9. Every time we pick one segment
from segment 0 to 9 as test set and the remain-
ing 9 segments are used to train CRF-based sub-
systems. Segment 10 is used as the training set for
SVM-HMM model. The sub-systems we used is
listed in Table 2.
In Table 3 we provide the micro-level and
macro-level average of performance the ten-fold
evaluation, including both the combined system
and all the individual sub-systems. Because
the system combination uses more data than its
sub-systems (segment 10), in order to have a
fair comparison, when evaluating individual sub-
systems, segment 10 is appended to the training
data of CRF model. Therefore, the individual sub-
systems and system combination have exactly the
same set of training data.
As we can see in the results in Table 3, the sys-
tem combination method (Row CB) has improve-
ment over the best sub-system (S4) on both F1
and OOV recall rate, and the OOV recall rate im-
proved by 1%. We should notice that in this exper-
iment we actually did not deal with any data from
different domains, the advantage of the proposed
method is therefore not prominent.
We continue to present the experiment results
of the second experiment. In the experiment
we labeled 200 sentences from each of the unla-
beled bake-off training set A and B, and trained
the SVM-HMM model on the labeled data. We
compare the performance of the four sub-systems
and the performance of the system combination
method trained on: 1) 200 sentences from A, 2)
200 sentences from B, and 3) the concatenation
of the 400 sentences from both A and B. We show
the scores on the bake-off test set A and B in Table
4.
As we can see from the results in Table 4, the
system combination method outperforms all the
individual systems, and the best performance is
observed when using both of the labeled data from
domain A and B, which indicates the potential of
further improvement by increasing the amount of
in-domain training data. Also, the individual sub-
systems with the best performance on the two do-
mains are different. System 1 performs well on
Set B but not on Set A, so does System 4, which
tops on Set A but not as good as System 1 on Set
B. The system combination results appear to be
much more stable on the two domains, which is a
preferable characteristic if the segmentation sys-
tem needs to deal with data from various domains.
5 Conclusion
In this paper we discussed a system combina-
tion method based on SVM-HMM for the Chinese
word segmentation problem. The method can uti-
lize small amount of training data in target do-
mains to improve the performance over individ-
ual sub-systems trained on data from different do-
mains. Experimental results show that the method
is effective in improving the performance with a
small amount of in-domain training data.
Future work includes adding more heteroge-
neous sub-systems other than CRF-based ones
into the system and investigate the effects on the
performance. Automatic domain adaptation for
Chinese word segmentation can also be an out-
come of the method, which may be an interesting
research topic in the future.
References
Altun, Yasemin, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden markov support vector
machines. In Proceedings of International Confer-
ence on Machine Learning (ICML).
Chen, Yaodong, Ting Wang, and Huowang Chen.
2005. Using directed graph based bdmm algorithm
for chinese word segmentation. pages 214?217.
Cohen, William W. and Vitor Carvalho. 2005. Stacked
sequential learning. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence
(IJ-CAI).
Joachims, Thorsten, Thomas Finley, and Chun-
Nam John Yu. 2009. Cutting-plane training of
structural svms. Machine Learning, 77(1):27?59.
Kudo, Taku. 2003. CRF++: Yet another crf toolkit.
Web page: http://crfpp.sourceforge.net/.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference on Machine Learning (ICML).
Shi, Yanxin and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence (IJ-CAI).
Sutton, Charles and Andrew McCallum, 2006. Intro-
duction to Statistical Relational Learning, chapter
An Introduction to Conditional Random Fields for
Relational Learning. MIT Press.
Vapnik, Vladimir N. 1995. The Nature of Statistical
Learning Theory. Springer.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
the second SIGHAN workshop on Chinese language
processing, pages 176?179.
Zhang, Huaping, Qun Liu, Xueqi Cheng, Hao Zhang,
and Hongkui Yu. 2003. Chinese lexical analysis us-
ing hierarchical hidden markov model. In Proceed-
ings of the second SIGHAN workshop on Chinese
language processing, pages 63?70.
Zhao, Hai and Chunyu Kit. 2008. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named entity
recognition. In The Sixth SIGHAN Workshop on
Chinese Language Processing (SIGHAN-6), pages
106?111.
Zhao, Hai, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in chinese
word segmentation via conditional random field
modeling. In Proceedings of the 20th Pacific Asia
Conference on Language, Information and Compu-
tation (PACLIC-20), pages 87?94.
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 107?115,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Utilizing Target-Side Semantic Role Labels to Assist Hierarchical
Phrase-based Machine Translation
Qin Gao and Stephan Vogel
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
{qing, stephan.vogel}@cs.cmu.edu
Abstract
In this paper we present a novel approach
of utilizing Semantic Role Labeling (SRL)
information to improve Hierarchical Phrase-
based Machine Translation. We propose an
algorithm to extract SRL-aware Synchronous
Context-Free Grammar (SCFG) rules. Con-
ventional Hiero-style SCFG rules will also be
extracted in the same framework. Special con-
version rules are applied to ensure that when
SRL-aware SCFG rules are used in deriva-
tion, the decoder only generates hypotheses
with complete semantic structures. We per-
form machine translation experiments using 9
different Chinese-English test-sets. Our ap-
proach achieved an average BLEU score im-
provement of 0.49 as well as 1.21 point reduc-
tion in TER.
1 Introduction
Syntax-based Machine Translation methods have
achieved comparable performance to Phrase-based
systems. Hierarchical Phrase-based Machine Trans-
lation, proposed by Chiang (Chiang, 2007), uses a
general non-terminal label X but does not use lin-
guistic information from the source or the target lan-
guage. There have been efforts to include linguis-
tic information into machine translation. Liu et al
(2006) experimented with tree-to-string translation
models that utilize source side parse trees, and later
improved the method by using the Packed Forest
data structure to reduce the impact of parsing errors
(Liu and Huang, 2010). The string-to-tree (Galley
et al 2006) and tree-to-tree (Chiang, 2010) meth-
ods have also been the subject of experimentation, as
well as other formalisms such as Dependency Trees
(Shen et al, 2008).
One problem that arises by using full syntactic la-
bels is that they require an exact match of the con-
stituents in extracted phrases, so it faces the risk
of losing coverage of the rules. SAMT (Zollmann
and Venugopal, 2006) and Tree Sequence Align-
ment (Zhang et al, 2008) are proposed to amend this
problem by allowing non-constituent phrases to be
extracted. The reported results show that while uti-
lizing linguistic information helps, the coverage is
more important (Chiang, 2010). When dealing with
formalisms such as semantic role labeling, the cov-
erage problem is also critical. In this paper we fol-
low Chiang?s observation and use SRL labels to aug-
ment the extraction of SCFG rules. I.e., the formal-
ism provides additional information and more rules
instead of restrictions that remove existing rules.
This preserves the coverage of rules.
Recently there has been increased attention to use
semantic information in machine translation. Liu
and Gildea (2008; 2010) proposed using Semantic
Role Labels (SRL) in their tree-to-string machine
translation system and demonstrated improvement
over conventional tree-to-string methods. Wu and
Fung (2009) developed a framework to reorder the
output using information from both the source and
the target SRL labels. In this paper, we explore an
approach of using the target side SRL information
in addition to a Hierarchical Phrase-based Machine
Translation framework. The proposed method ex-
tracts initial phrases with two different heuristics:
The first heuristic is used to extract rules that have
a general left-hand-side (LHS) non-terminal tag X ,
107
Second we must build a flood prevention system , strengthen pre-flood inspections and implement flood prevention measures 
arg0 mod 
arg0 mod 
arg0 mod 
pred 
pred 
arg1 
arg1 
pred 
arg1 
Figure 1: Example of predicate-argument structure in a sentence
i.e., Hiero rules. The second will extract phrases that
contain information of SRL structures. The pred-
icate and arguments that the phrase covers will be
represented in the LHS non-terminal tags. After
that, we obtain rules from the initial phrases in the
same way as the Hiero extraction algorithm, which
replaces nesting phrases with their corresponding
non-terminals.
By applying this scheme, we will obtain rules that
contain SRL information, without sacrificing the
coverage of rules. In this paper, we call such rules
SRL-aware SCFG rules. During decoding, both the
conventional Hiero-style SCFG rules with general
tag X and SRL-aware SCFG rules are used in a syn-
chronous Chart Parsing algorithm. Special conver-
sion rules are introduced to ensure that whenever
SRL-aware SCFG rules are used in the derivation,
a complete predicate-argument structure is built.
The main contributions are:
1. an algorithm to extract SRL-aware SCFG rules
using target side SRL information.
2. an approach to use Hiero rules side-by-side
with information-rich SRL-aware SCFG rules,
which improves the quality of translation re-
sults.
In section 2 we briefly review SCFG-based ma-
chine translation and SRL. In section 3, we describe
the SRL-aware SCFG rules. Section 4 provides
the detail of the rule extraction algorithm. Section
5 presents two alternative methods how to utilize
the SRL information. The experimental results are
given in Section 6, followed by analysis and conclu-
sion in Section 7.
2 Background
2.1 Hierarchical Phrase-based Machine
Translation
Proposed by Chiang (2005), the Hierarchical
Phrase-based Machine Translation model (com-
monly known as the Hiero model) has achieved re-
sults comparable, if not superior, to conventional
Phrase-based approaches. The basic idea is to treat
the translation as a synchronous parsing problem.
Using the source side terminals as input, the decoder
tries to build a parse tree and synchronously generate
target side terminals. The rules that generates such
synchronous parse trees are in the following form:
X ? (f1 X1 f2 X2 f3, e1 X2 e2 X1 e3)
where X1 and X2 are non-terminals, and the sub-
scripts represents the correspondence between the
non-terminals. In Chiang?s Hiero model all non-
terminals will have the same tag, i.e. X . The formal-
ism, known as Synchronous Context-Free Grammar
(SCFG) does not require the non-terminals to have a
unique tag name. Instead, they may have tags with
syntactic or semantic meanings, such as NP or V P .
2.2 Semantic Role Labeling and Machine
Translation
The task of semantic role labeling is to label the se-
mantic relationships between predicates and argu-
ments. This relationship can be treated as a depen-
dency structure called ?Predicate-Argument Struc-
ture? (PA structure for short). Figure 1 depicts ex-
amples of multiple PA structures in a sentence. The
lines indicate the span of the predicates and argu-
ments of each PA structure, and the tags attached to
these lines show their role labels.
Despite the similarity between PA structure and
dependency trees, SRL offers a structure that posses
better granularity. Instead of trying to analyze all
links between words in the sentences, PA structure
only deals with the relationships between verbs and
constituents that are arguments of the predicates.
This information is useful in preserving the mean-
ing of the sentence during the translation process.
However, using semantic role representation in
machine translation has its own set of problems.
108
First, we face the coverage problem. Some sen-
tences might not have semantic structure at all, if,
for instance they consist of single noun phrases or
contain only rare predicates that are not covered by
the semantic role labeler. Moreover, the PA struc-
tures are not guaranteed to cover the whole sentence.
This is especially true when two or more predicates
are presented in a coordinated structure. In this case,
the arguments of other predicates will not be covered
in the PA structure of the predicate.
The second problem is that the SRL labels are
only on the constituents of predicate and arguments.
There is no analysis conducted inside the augments.
That is different from syntactic parsing or depen-
dency parsing, which both provide a complete tree
from the sentence to every individual word. As
we can see in Figure 1, words such as ?Second?
and ?and? are not covered. Inside the NPs such
as ?a flood prevention system?, SRL will not pro-
vide more information. Therefore it is hard to build
a self-contained formalization based only on SRL
labels. Most work on SRL labels is built upon
or assisted by other formalisms. For instance, Liu
and Gildea (2010) integrated SRL label into a tree-
to-string translation system. Wu and Fung (2009)
used SRL labels for reordering the n-best output of
phrase-based translation systems. Similarly, in our
work we also adopt the methodology of using SRL
information to assist existing formalism. The dif-
ference of our method from Wu and Fung is that
we embed the SRL information directly into the de-
code, instead of doing two-pass decoding. Also, our
method is different from Liu and Gildea (2010) that
we utilize target side SRL information instead of the
source side.
As we will see in section 3, we define a mapping
function from the SRL structures that a phrase cov-
ers to a non-terminal tag before extracting the SCFG
rules. The tags will restrict the derivation of the tar-
get side parse tree to accept only SRL structures we
have seen in the training corpus. The mapping from
SRL structures to non-terminal tags can be defined
according to the SRL annotation set.
In this paper we adopt the PropBank (Palmer et
al., 2005) annotation set of semantic labels, because
the annotation set is relatively simple and easy to
parse. The small set of argument tags also makes
the number of LHS non-terminal tags small, which
alleviates the problem of data scarcity. However the
methodology of this paper is not limited to Prop-
Bank tags. By defining appropriate mapping, it is
also possible to use other annotation sets, such as
FrameNet (Baker et al, 2002).
3 SRL-aware SCFG Rules
The SRL-aware SCFG rules are SCFG rules. They
contain at least one non-terminal label with infor-
mation about the PA structure that is covered by the
non-terminal. The labels are called SRL-aware la-
bels, and the non-terminal itself is called SRL-aware
non-terminal. The non-terminal can be on the left
hand side or right hand side or the rule, and we do
not require all the non-terminals in the rules be SRL-
aware, thus, the general tag X can also be used. In
this paper, we assign SRL-aware labels based on the
SRL structure they cover. The label contains the fol-
lowing components:
1. The predicate frame; that is the predicate whose
predicate argument structure belongs to the
SRL-aware non-terminal.
2. The set of complete arguments the SRL-aware
non-terminal covers.
In practice, the predicates are stemmed. For ex-
ample, if we have a target side phrase: She beats
eggs today, where She will be labeled as ARG0 of the
predicate beat, and eggs will be labeled as ARG1, to-
day will be labeled as ARG-TMP, respectively. The
SRL-aware label that covers this phrase is:
#beat/0 1 TMP
There are two notes for the definition. Firstly,
the order of arguments is not important in the la-
bel. #beat/0 1 TMP is treated identically to
#beat/0 TMP 1. Secondly, as we always require
the predicate to be represented, an SRL-aware non-
terminal should always cover the predicate. This
property will be re-emphasized when we discuss
the rule extraction algorithm in Section 3. Figure
2 shows some examples of the SRL-aware SCFG
rules.
When the RHS non-terminal is an SRL-aware
non-terminal, we define the rule as a conversion rule.
A conversion rule is only generated when the right
109
Xin
jia
ng
?s
Yil
i
hol
ds
pro
pag
and
a
dri
ve
??
??
???
??
???
??
??
[#Ho
ld/1]
[#Ho
ld/0_
1]
[#Ho
ld/0][
#Hol
d]
[X]
[X]
[X]
[X][X]
[X]
Some
 SRL-aw
are Ru
les :  
[#Hold/0_1] 
?( [#Ho
ld/0] 
???
??
??,
 [#Hold/0] p
ropag
anda 
drive)
[#Hold/0_1] 
?( ?
??
??
??
[#Hold/1], Xin
jiang?s
 Yili[#Hold
/1])
[#Hold/0_1] 
?( ?
?[X 1
]??
?[#Hol
d/1], Xinjia
ng?s [
X 1] [#Hol
d/1])
[#Hold/0_1] 
?( [
X 1]ho
ld [X 2
], [X 1]
hold [X
2])
[#Hold/1]
?([#Hold
] 
??
??
??
?, [#Ho
ld] pro
pagan
da dr
ive)
[#Hold/0] 
?(?
??
??
??
[#Hold], Xin
jiang?s
 Yili[#Hold
])
[#Hold] 
?(?
?, h
olds)
Spec
ial SRL-
aware
 conv
ersion
 rule:  
[X
] ?[#Ho
ld/0_1]
Figure 2: Example SRL structure with word alignment
hand side is a complete SRL structure. For exam-
ple, #hold/0 is not a complete SRL structure in
Figure 2, because it lacks of a required argument,
while #hold/0 1 is a complete SRL structure. In
this case, the conversion rule X ? #hold/0 1
will be extracted from the example shown in Fig-
ure 2, but not the other. Together with the glue
rules that commonly used in Hiero decoder, i.e.
S ? (S X1, S X1) and S ? (X1, X1), the conver-
sion rules ensures that whenever SRL-aware SCFG
rules are used in parsing, the output parse tree con-
tains only complete SRL structures. This is because
only complete SRL structures that we have observed
in the training data can be converted back to the gen-
eral tag X .
After we have extracted the SRL-aware SCFG
rules, derivation can be done on the input of source
sentence. For example, the sentence ?? ???
????????? 1 can generate the parse tree
and translation in Figure 3a) using the rules shown
in Figure 2. Also, we can see in Figure 3b) that in-
complete SRL structures cannot be generated due to
the absence of a proper conversion rule.
1The translation is Xinjiang?s Yili holds propaganda drive
and the Pinyin transliteration is Xinjiang daguimo kaizhan
mianduimian xuanjiang huodong

  
	
	
 

	 

	   

 
	
 

	 
  
	
Extracting Parallel Phrases from Comparable Data
Sanjika Hewavitharana and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{sanjika,vogel+}@cs.cmu.edu
Abstract
Mining parallel data from comparable corpora
is a promising approach for overcoming the
data sparseness in statistical machine trans-
lation and other NLP applications. Even if
two comparable documents have few or no
parallel sentence pairs, there is still poten-
tial for parallelism in the sub-sentential level.
The ability to detect these phrases creates a
valuable resource, especially for low-resource
languages. In this paper we explore three
phrase alignment approaches to detect paral-
lel phrase pairs embedded in comparable sen-
tences: the standard phrase extraction algo-
rithm, which relies on the Viterbi path; a
phrase extraction approach that does not rely
on the Viterbi path, but uses only lexical fea-
tures; and a binary classifier that detects par-
allel phrase pairs when presented with a large
collection of phrase pair candidates. We eval-
uate the effectiveness of these approaches in
detecting alignments for phrase pairs that have
a known alignment in comparable sentence
pairs. The results show that the Non-Viterbi
alignment approach outperforms the other two
approaches on F1 measure.
1 Introduction
Statistical Machine Translation (SMT), like many
natural language processing tasks, relies primarily
on parallel corpora. The translation performance of
SMT systems directly depends on the quantity and
the quality of the available parallel data. However,
such corpora are only available in large quantities
for a handful of languages, including English, Ara-
bic, Chinese and some European languages. Much
of this data is derived from parliamentary proceed-
ings, though a limited amount of newswire text is
also available. For most other languages, especially
for less commonly used languages, parallel data is
virtually non-existent.
Comparable corpora provide a possible solution
to this data sparseness problem. Comparable doc-
uments are not strictly parallel, but contain rough
translations of each other, with overlapping infor-
mation. A good example for comparable documents
is the newswire text produced by multilingual news
organizations such as AFP or Reuters. The de-
gree of parallelism can vary greatly, ranging from
noisy parallel documents that contain many paral-
lel sentences, to quasi parallel documents that may
cover different topics (Fung and Cheung, 2004).
The Web is by far the largest source of compara-
ble data. Resnik and Smith (2003) exploit the sim-
ilarities in URL structure, document structure and
other clues for mining the Web for parallel docu-
ments. Wikipedia has become an attractive source of
comparable documents in more recent work (Smith
et al, 2010).
Comparable corpora may contain parallel data in
different levels of granularity. This includes: par-
allel documents, parallel sentence pairs, or parallel
sub-sentential fragments. To simplify the process
and reduce the computational overhead, the paral-
lel sentence extraction is typically divided into two
tasks. First, a document level alignment is iden-
tified between comparable documents, and second,
the parallel sentences are detected within the iden-
tified document pairs. Cross-lingual information re-
trieval methods (Munteanu and Marcu, 2005) and
61
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 61?68,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Figure 1: Sample comparable sentences that contain parallel phrases
other similarity measures (Fung and Cheung, 2004)
have been used for the document alignment task.
Zhao and Vogel (2002) have extended parallel sen-
tence alignment algorithms to identify parallel sen-
tence pairs within comparable news corpora. Till-
mann and Xu (2009) introduced a system that per-
forms both tasks in a single run without any doc-
ument level pre-filtering. Such a system is useful
when document level boundaries are not available in
the comparable corpus.
Even if two comparable documents have few or
no parallel sentence pairs, there could still be paral-
lel sub-sentential fragments, including word transla-
tion pairs, named entities, and long phrase pairs. The
ability to identify these pairs would create a valu-
able resource for SMT, especially for low-resource
languages. The first attempt to detect sub-sentential
fragments from comparable sentences is (Munteanu
and Marcu, 2006). Quirk et al (2007) later ex-
tended this work by proposing two generative mod-
els for comparable sentences and showed improve-
ments when applied to cross-domain test data. In
both these approaches the extracted fragment data
was used as additional training data to train align-
ment models. Kumano et al (2007) have proposed a
phrasal alignment approach for comparable corpora
using the joint probability SMT model. While this
approach is appealing for low-resource scenarios as
it does not require any seed parallel corpus, the high
computational cost is a deterrent in its applicability
to large corpora.
In this paper we explore several phrase alignment
approaches to detect parallel phrase pairs embedded
in comparable sentence pairs. We assume that com-
parable sentence pairs have already been detected.
Our intention is to use the extracted phrases directly
in the translation process, along with other phrase
pairs extracted from parallel corpora. In particular,
we study three alignment approaches:
? the standard phrase extraction algorithm, which
relies on the Viterbi path of the word alignment;
? a phrase extraction approach that does not rely
on the Viterbi path, but only uses lexical fea-
tures;
? and a binary classifier to detect parallel phrase
pairs when presented with a large collection of
phrase pair candidates.
We evaluate the effectiveness of these approaches
in detecting alignments for phrase pairs that have a
known translation a comparable sentence pair. Sec-
tion 2 introduces the phrase alignment problem in
comparable sentences and discusses some of the
challenges involved. It also explains the differ-
ent alignment approaches we explore. Section 3
presents the experimental setup and the results of
the evaluation. We conclude, in section 4, with an
analysis of the results and some directions for future
work.
62
Figure 2: Word-to-word alignment pattern for (a) a parallel sentence pair (b) a non-parallel sentence pair
2 Parallel Phrase Extraction
Figure 1 shows three sample sentences that were ex-
tracted from Gigaword Arabic and Gigaword En-
glish collections. For each comparable sentence
pair, the Arabic sentence is shown first, followed by
its literal English translation (in Italics). The English
sentence is shown next. The parallel sections in each
sentence are marked in boldface. In the first two sen-
tences pairs, the English sentence contains the full
translation of the Arabic sentence, but there are addi-
tional phrases on the English side that are not present
on the Arabic sentence. These phrases appear at the
beginning of sentence 1 and at the end of sentence
2. In sentence 3, there are parallel phrases as well
as phrases that appear only on one side. The phrase
?to Iraq? appears only on the Arabic sentence while
the phrase ?the former Egyptian foreign minister?
appears only on the English side.
Standard word alignment and phrase alignment
algorithms are formulated to work on parallel sen-
tence pairs. Therefore, these standard algorithms are
not well suited to operate on partially parallel sen-
tence pairs. Presence of non-parallel phrases may
result in undesirable alignments.
Figure 2 illustrates this phenomenon. It compares
a typical word alignment pattern in a parallel sen-
tence pair (a) to one in a non-parallel sentence pair
(b). The darkness of a square indicates the strength
of the word alignment probability between the corre-
sponding word pair. In 2(a), we observe high proba-
bility word-to-word alignments (dark squares) over
the entire length of the sentences. In 2(b), we see
one dark area above ?weapons of mass destruction?,
corresponding to the parallel phrase pair, and some
scattered dark spots, where high frequency English
words pair with high frequency Arabic words. This
spurious alignments pose problems to the phrase
alignment, and indicate that word alignment prob-
abilities alone might not be sufficient.
Our aim is to identify such parallel phrase pairs
from comparable sentence pairs. In the following
subsections we briefly explain the different phrase
alignment approaches we use.
2.1 Viterbi Alignment
Here we use the typical phrase extraction approach
used by Statistical Machine Translation systems:
obtain word alignment models for both directions
(source to target and target to source), combine the
Viterbi paths using one of many heuristics, and ex-
tract phrase pairs from the combined alignment. We
used Moses toolkit (Koehn et al, 2007) for this task.
To obtain the word alignments for comparable sen-
tence pairs, we performed a forced alignment using
the trained models.
2.2 Binary Classifier
We used a Maximum Entropy classifier as our
second approach to extract parallel phrase pairs
from comparable sentences. Such classifiers have
been used in the past to detect parallel sentence
pairs in large collections of comparable documents
(Munteanu and Marcu, 2005). Our classifier is sim-
ilar, but we apply it at phrase level rather than at
sentence level. The classifier probability is defined
63
as:
p(c|S, T ) = exp (
?n
i=1 ?ifi(c, S, T ))
Z(S, T ) , (1)
where S = sL1 is a source phrase of length L and
T = tK1 is a target phrase of length K. c ? {0, 1}
is a binary variable representing the two classes of
phrases: parallel and not parallel. p(c|S, T ) ? [0, 1]
is the probability where a value p(c = 1|S, T ) close
to 1.0 indicates that S and T are translations of each
other. fi(c, S, T ) are feature functions that are co-
indexed with respect to the class variable c. The pa-
rameters ?i are the weights for the feature functions
obtained during training. Z(S, T ) is the normal-
ization factor. In the feature vector for phrase pair
(S, T ), each feature appears twice, once for each
class c ? {0, 1}.
The feature set we use is inspired by Munteanu
and Marcu (2005) who define the features based
on IBM Model-1 (Brown et al, 1993) alignments
for source and target pairs. However, in our ex-
periments, the features are computed primarily on
IBM Model-1 probabilities (i.e. lexicon). We do
not explicitly compute IBM Model-1 alignments. To
compute coverage features, we identify alignment
points for which IBM Model-1 probability is above
a threshold. We produce two sets of features based
on IBM Model-1 probabilities obtained by training
in both directions. All the features have been nor-
malized with respect to the source phrase length L
or the target phrase length K. We use the following
11 features:
1. Lexical probability (2): IBM Model-1 log
probabilities p(S|T ) and p(T |S)
2. Phrase length ratio (2): source length ratio
K/L and target length ratio L/K
3. Phrase length difference (1): source length mi-
nus target length, L?K
4. Number of words covered (2): A source word
s is said to be covered if there is a target word
t ? T such that p(s|t) > , where  = 0.5.
Target word coverage is defined accordingly.
5. Number of words not covered (2): This is com-
puted similarly to 4. above, but this time count-
ing the number of positions that are not cov-
ered.
6. Length of the longest covered sequence of
words (2)
To train the classifier, we used parallel phrases
pairs extracted from a manually word-aligned cor-
pus. In selecting negative examples, we followed the
same approach as in (Munteanu and Marcu, 2005):
pairing all source phrases with all target phrases, but
filter out the parallel pairs and those that have high
length difference or a low lexical overlap, and then
randomly select a subset of phrase pairs as the neg-
ative training set. The model parameters are esti-
mated using the GIS algorithm.
2.3 Non-Viterbi (PESA) Alignment
A phrase alignment algorithm called ?PESA? that
does not rely on the Viterbi path is described in (Vo-
gel, 2005). PESA identifies the boundaries of the
target phrase by aligning words inside the source
phrase with words inside the target phrase, and sim-
ilarly for the words outside the boundaries of the
phrase pair. It does not attempt to generate phrase
alignments for the full sentence. Rather, it identifies
the best target phrase that matches a given source
phrase. PESA requires a statistical word-to-word
lexicon. A seed parallel corpus is required to au-
tomatically build this lexicon.
This algorithm seems particularly well suited in
extracting phrase pairs from comparable sentence
pairs, as it is designed to not generate a complete
word alignment for the entire sentences, but to find
only the target side for a phrase embedded in the
sentence. We briefly explain the PESA alignment
approach below.
Instead of searching for all possible phrase align-
ments in a parallel sentence pair, this approach
finds the alignment for a single source phrase S =
s1 . . . sl. Assume that we have a parallel sentence
pair (sJ1 , tI1) which contains the source phrase S in
the source sentence sJ1 . Now we want to find the
target phrase T = t1 . . . tk in the target sentence tI1
which is the translation of the source phrase.
A constrained IBM Model-1 alignment is now ap-
plied as follows:
? Source words inside phrase boundary are
aligned only with the target words inside the
phrase boundary. Source words outside the
64
phrase boundary are only aligned with target
words outside the phrase boundary.
? Position alignment probability for the sentence,
which is 1/I in IBM Model-1, is modified to be
1/k inside the source phrase and to 1/(I ? k)
outside the phrase.
Figure 3 shows the different regions. Given the
source sentence and the source phrase from position
j1 to j2, we want to find the boundaries of the target
phrase, i1 and i2. The dark area in the middle is the
phrase we want to align. The size of the blobs in
each box indicates the lexical strength of the word
pair.
Figure 3: PESA Phrase alignment
The constrained alignment probability is calculated
as follows:
p(s|t) =
?
?
j1?1?
j=1
?
i/?(i1...i2)
1
I ? kp(sj |ti)
?
?
?
?
?
j2?
j=j1
i2?
i=i1
1
kp(sj |ti)
?
? (2)
?
?
?
J?
j=j2+1
?
i/?(i1...i2)
1
I ? kp(sj |ti)
?
?
p(t|s) is similarly calculated by switching source
and target sides in equation 2:
p(t|s) =
?
?
i1?1?
i=1
?
i/?(j1...j2)
1
J ? l p(ti|sj)
?
?
?
?
?
i2?
i=i1
j2?
j=j1
1
l p(ti|sj)
?
? (3)
?
?
?
I?
i=i2+1
?
j /?(j1...j2)
1
J ? l p(ti|sj)
?
?
To find the optimal target phrase boundaries, we in-
terpolate the two probabilities in equations 2 and 3
and select the boundary (i1, i2) that gives the highest
probability.
(i1, i2) = argmax
i1,i2
{(1? ?) log(p(s|t))
+ ? log(p(t|s))} (4)
The value of ? is estimated using held-out data.
PESA can be used to identify all possible phrase
pairs in a given parallel sentence pair by iterating
over every source phrase. An important difference is
that each phrase is found independently of any other
phrase pair, whereas in the standard phrase extrac-
tion they are tied through the word alignment of the
sentence pair.
There are several ways we can adapt the non-Viterbi
phrase extraction to comparable sentence.
? Apply the same approach assuming the sen-
tence pair as parallel. The inside of the source
phrase is aligned to the inside of the target
phrase, and the outside, which can be non-
parallel, is aligned the same way.
? Disregard the words that are outside the phrase
we are interested in. Find the best target phrase
by aligning only the inside of the phrase. This
will considerably speed-up the alignment pro-
cess.
3 Experimental Results
3.1 Evaluation Setup
We want to compare the performance of the differ-
ent phrase alignment methods in identifying paral-
lel phrases embedded in comparable sentence pairs.
65
1 2 3 4 5 6 7 8 9 10 All
test set 2,826 3,665 3,447 3,048 2,718 2,414 2,076 1,759 1,527 1,378 24,858
test set (found) 2,746 2,655 1,168 373 87 29 7 2 1 0 7,068
Table 1: N-gram type distribution of manually aligned phrases set
Using a manually aligned parallel corpus, and two
monolingual corpora, we obtained a test corpus as
follows: From the manually aligned corpus, we ob-
tain parallel phrase pairs (S, T ). Given a source lan-
guage corpus S and a target language corpus T , for
each parallel phrase pair (S, T ) we select a sentence
s from S which contains S and a target sentence
t from T which contains T . These sentence pairs
are then non-parallel, but contain parallel phrases,
and for each sentence pair the correct phrase pair
is known. This makes it easy to evaluate different
phrase alignment algorithms.
Ideally, we would like to see the correct target
phrase T extracted for a source phrase S. How-
ever, even if the boundaries of the target phrase do
not match exactly, and only a partially correct trans-
lation is generated, this could still be useful to im-
prove translation quality. We therefore will evaluate
the phrase pair extraction from non-parallel sentence
pairs also in terms of partial matches.
To give credit to partial matches, we define pre-
cision and recall as follows: Let W and G denote
the extracted target phrase and the correct reference
phrase, respectively. Let M denote the tokens in W
that are also found in the reference G. Then
Precision = |M ||W | ? 100 (5)
Recall = |M ||G| ? 100 (6)
These scores are computed for each extracted phrase
pair, and are averaged to produce precision and re-
call for the complete test set. Finally, precision and
recall are combined to generated the F-1 score in the
standard way:
F1 = 2 ? Precision ?RecallPrecision+Recall (7)
3.2 Evaluation
We conducted our experiments on Arabic-English
language pair. We obtained manual alignments for
663 Arabic-English sentence pairs. From this, we
selected 300 sentences, and extracted phrase pairs
up to 10 words long that are consistent with the un-
derlying word alignment. From the resulting list of
phrase pairs, we removed the 50 most frequently
occurring pairs as well as those only consisting of
punctuations. Almost all high frequency phrases are
function words, which are typically covered by the
translation lexicon. Line 1 in Table 1 gives the n-
gram type distribution for the source phrases.
Using the phrase pairs extracted from the manu-
ally aligned sentences, we constructed a comparable
corpus as follows:
1. For each Arabic phrase, we search the Arabic
Gigaword1 corpus for sentences that contain
the phrase and select up to 5 sentences. Sim-
ilarly, for each corresponding English phrase
we select up to 5 sentences from English Gi-
gaword2.
2. For each phrase pair, we generate the Cartesian
product of the sentences and produce a sen-
tence pair collection. I.e. up to 25 comparable
sentence pairs were constructed for each phrase
pair.
3. We only select sentences up to 100 words long,
resulting in a final comparable corpus consist-
ing of 170K sentence pairs.
Line 2 in Table 1 gives the n-gram type distribu-
tion for the phrase pairs for which we found both a
source sentence and a target sentence in the mono-
lingual corpora. As expected, the longer the phrases,
the less likely it is to find them in even larger cor-
pora.
We consider the resulting set as our comparable
corpus which we will use to evaluate all alignment
approaches. In most sentence pairs, except for the
phrase pair that we are interested in, the rest of the
sentence does not typically match the other side.
1Arabic Gigaword Fourth Edition (LDC2009T30)
2English Gigaword Fourth Edition (LDC2009T13)
66
Lexicon Viterbi Classifier PESA
Exact P R F1 Exact P R F1 Exact P R F1
Lex-Full 43.56 65.71 57.99 61.61 54.46 81.79 85.29 85.29 67.94 93.34 86.80 90.22
Lex-1/3 42.95 65.68 56.69 60.85 53.57 81.32 88.34 84.69 67.28 93.23 86.17 89.56
Lex-1/9 41.10 63.60 51.15 56.70 52.38 80.30 86.64 83.35 65.81 91.95 84.73 88.19
Lex-1/27 41.02 62.10 49.38 55.01 52.51 80.51 83.84 82.14 63.23 89.41 82.06 85.57
Lex-BTEC 19.10 26.94 23.63 25.18 18.76 45.90 36.17 40.46 17.45 46.70 36.28 40.83
Table 2: Results for Alignment Evaluation of test phrases
We obtained the Viterbi alignment using stan-
dard word alignment techniques: IBM4 word align-
ment for both directions, Viterbi path combination
using heuristics (?grow-diag-final?) and phrase ex-
traction from two-sided training, as implemented in
the Moses package (Koehn et al, 2007). Because
the non-parallel segments will lead the word align-
ment astray, this may have a negative effect on the
alignment in the parallel sections. Alignment mod-
els trained on parallel data are used to generate the
Viterbi alignment for the comparable sentences. We
then extract the target phrases that are aligned to
the embedded source phrases. A phrase pair is ex-
tracted only when the alignment does not conflict
with other word alignments in the sentence pair. The
alignments are not constrained to produce contigu-
ous phrases. We allow unaligned words to be present
in the phrase pair. For each source phrase we se-
lected the target phrase that has the least number of
unaligned words.
The classifier is applied at the phrase level. We
generate the phrase pair candidates as follows: For
a given target sentence we generate all n-grams up
to length 10. We pair each n-gram with the source
phrase embedded in the corresponding source sen-
tence to generate a phrase pair. From the 170 thou-
sand sentence pairs, we obtained 15.6 million phrase
pair candidates. The maximum entropy classifier is
then applied to the phrase pairs. For each source
phrase, we pick the target candidate for which p(c =
1, S, T ) has the highest value.
For the PESA alignment we used both inside and
outside alignments, using only lexical probabilities.
For each source phrase pair, we select the best scor-
ing target phrase.
As our goal is to use these methods to extract
parallel data for low resource situations, we tested
each method with several lexica, trained on differ-
ent amounts of initial parallel data. Starting from the
full corpus with 127 million English tokens, we gen-
erated three additional parallel corpora with 1/3, 1/9
and 1/27 of the original size. The 1/9 and 1/27 cor-
pora (with 13 million and 4 million English words)
can be considered medium and small sized corpora,
respectively. These two corpora are a better match
to the resource levels for many languages. We also
used data from the BTEC (Kikui et al, 2003) cor-
pus. This corpus contains conversational data from
the travel domain, which is from a different genre
than the document collections. Compared to other
corpora, it is much smaller (about 190 thousand En-
glish tokens).
Table 2 gives the results for all three alignment ap-
proaches. Results are presented as percentages of:
exact matches found (Exact), precision (P), recall
(R) and F1. The Viterbi alignment gives the lowest
performance. This shows that the standard phrase
extraction procedure, which works well for parallel
sentence, is ill-suited for partially parallel sentences.
Despite the fact that the classifier incorporates sev-
eral features including the lexical features, the per-
formance of the PESA alignment, which uses only
the lexical features, has consistently higher precision
and recall than the classifier. This demonstrates that
computing both inside and outside probabilities for
the sentence pair helps the phrase extraction. The
classifier lacks this ability because the phrase pair
is evaluated in isolation, without the context of the
sentence.
Except for the BTEC corpus, the performance
degradation is minimal as the lexicon size is re-
duced. This shows that the approaches are robust
for smaller parallel amounts of parallel data.
Instead of using token precision, an alternative
67
method of evaluating partial matches, is to give
credit based on the length of the overlap between
the extracted phrase and the reference. Precision and
recall can then be defined based on the longest com-
mon contiguous subsequence, similar to (Bourdail-
let et al, 2010). Results obtained using this methods
were similar to the results in Table 2.
4 Conclusion and Future Work
In this paper we explored several phrase alignment
approaches for extracting phrase pairs that are em-
bedded inside comparable sentence pairs. We used
the standard Viterbi phrase alignment, a maximum
entropy classifier that works on phrase pairs, and a
non-Viterbi PESA alignment in the evaluation pro-
cess. The results show that PESA outperforms both
the Viterbi approach and the classifier, in both preci-
sion and recall.
We plan to extend the PESA framework to use
not only lexical features, but other features similar
to the ones used in the classifier. We believe this
will further improve the alignment accuracy.
While this paper focuses on comparisons of dif-
ferent phrase alignment approaches in a realistic, yet
controlled manner by selecting appropriate compa-
rable sentence pairs for given phrase pairs, future
experiments will focus on finding new phrase pairs
from comparable corpora and evaluating the poten-
tial utility of the extracted data in the context of an
end-to-end machine translation system.
References
Julien Bourdaillet, Ste?phane Huet, Philippe Langlais, and
Guy Lapalme. 2010. TransSearch: from a bilingual
concordancer to a translation finder. Machine Trans-
lation, 24(3-4):241?271, dec.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Pascale Fung and Percy Cheung. 2004. Mining very
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 57?63, Barcelona, Spain.
Genichiro Kikui, Eiichiro Sumita, Toshiyuki Takezawa,
and Seiichi Yamamoto. 2003. Creating corpora
for speech-to-speech translation. In In Proc. of EU-
ROSPEECH 2003, pages 381?384, Geneva.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic,
June.
Tadashi Kumano, Hideki Tanaka, and Takenobu Toku-
naga. 2007. Extracting phrasal alignments from com-
parable corpora by using joint probability smt model.
In In Proceedings of the International Conference on
Theoretical and Methodological Issues in Machine
Translation, Skvde, Sweden, September.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 81?88, Sydney, Aus-
tralia.
Chris Quirk, Raghavendra U. Udupa, and Arul Menezes.
2007. Generative models of noisy translations with
applications to parallel fragment extraction. In Pro-
ceedings of the Machine Translation Summit XI, pages
377?384, Copenhagen, Denmark.
Philip Resnik and Noah Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29(3):349?
380.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. In Pro-
ceedings of the Human Language Technologies/North
American Association for Computational Linguistics,
pages 403?411.
Christoph Tillmann and Jian-Ming Xu. 2009. A sim-
ple sentence-level extraction algorithm for comparable
data. In Companion Vol. of NAACL HLT 09, Boulder,
CA, June.
Stephan Vogel. 2005. PESA: Phrase pair extraction
as sentence splitting. In Proceedings of the Machine
Translation Summit X, Phuket, Thailand, September.
Bing Zhao and Stephan Vogel. 2002. Full-text story
alignment models for chinese-english bilingual news
corpora. In Proceedings of the ICSLP ?02, September.
68
Active Learning with Multiple Annotations for Comparable Data
Classification Task
Vamshi Ambati, Sanjika Hewavitharana, Stephan Vogel and Jaime Carbonell
{vamshi,sanjika,vogel,jgc}@cs.cmu.edu
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
Abstract
Supervised learning algorithms for identify-
ing comparable sentence pairs from a domi-
nantly non-parallel corpora require resources
for computing feature functions as well as
training the classifier. In this paper we pro-
pose active learning techniques for addressing
the problem of building comparable data for
low-resource languages. In particular we pro-
pose strategies to elicit two kinds of annota-
tions from comparable sentence pairs: class
label assignment and parallel segment extrac-
tion. We also propose an active learning strat-
egy for these two annotations that performs
significantly better than when sampling for ei-
ther of the annotations independently.
1 Introduction
The state-of-the-art Machine Translation (MT) sys-
tems are statistical, requiring large amounts of paral-
lel corpora. Such corpora needs to be carefully cre-
ated by language experts or speakers, which makes
building MT systems feasible only for those lan-
guage pairs with sufficient public interest or finan-
cial support. With the increasing rate of social media
creation and the quick growth of web media in lan-
guages other than English makes it relevant for lan-
guage research community to explore the feasibility
of Internet as a source for parallel data. (Resnik and
Smith, 2003) show that parallel corpora for a variety
of languages can be harvested on the Internet. It is to
be observed that a major portion of the multilingual
web documents are created independent of one an-
other and so are only mildly parallel at the document
level.
There are multiple challenges in building compa-
rable corpora for consumption by the MT systems.
The first challenge is to identify the parallelism be-
tween documents of different languages which has
been reliably done using cross lingual information
retrieval techniques. Once we have identified a sub-
set of documents that are potentially parallel, the
second challenge is to identify comparable sentence
pairs. This is an interesting challenge as the avail-
ability of completely parallel sentences on the inter-
net is quite low in most language-pairs, but one can
observe very few comparable sentences among com-
parable documents for a given language-pair. Our
work tries to address this problem by posing the
identification of comparable sentences from com-
parable data as a supervised classification problem.
Unlike earlier research (Munteanu and Marcu, 2005)
where the authors try to identify parallel sentences
among a pool of comparable documents, we try to
first identify comparable sentences in a pool with
dominantly non-parallel sentences. We then build
a supervised classifier that learns from user annota-
tions for comparable corpora identification. Train-
ing such a classifier requires reliably annotated data
that may be unavailable for low-resource language
pairs. Involving a human expert to perform such
annotations is expensive for low-resource languages
and so we propose active learning as a suitable tech-
nique to reduce the labeling effort.
There is yet one other issue that needs to be solved
in order for our classification based approach to
work for truly low-resource language pairs. As we
will describe later in the paper, our comparable sen-
tence classifier relies on the availability of an ini-
69
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 69?77,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
tial seed lexicon that can either be provided by a hu-
man or can be statistically trained from parallel cor-
pora (Och and Ney, 2003). Experiments show that a
broad coverage lexicon provides us with better cov-
erage for effective identification of comparable cor-
pora. However, availability of such a resource can
not be expected in very low-resource language pairs,
or even if present may not be of good quality. This
opens an interesting research question - Can we also
elicit such information effectively at low costs? We
propose active learning strategies for identifying the
most informative comparable sentence pairs which a
human can then extract parallel segments from.
While the first form of supervision provides us
with class labels that can be used for tuning the fea-
ture weights of our classifier, the second form of su-
pervision enables us to better estimate the feature
functions. For the comparable sentence classifier to
perform well, we show that both forms of supervi-
sion are needed and we introduce an active learning
protocol to combine the two forms of supervision
under a single joint active learning strategy.
The rest of the paper is organized as follows. In
Section 2 we survey earlier research as relevant to
the scope of the paper. In Section 3 we discuss the
supervised training setup for our classifier. In Sec-
tion 4 we discuss the application of active learning to
the classification task. Section 5 discusses the case
of active learning with two different annotations and
proposes an approach for combining them. Section 6
presents experimental results and the effectiveness
of the active learning strategies. We conclude with
further discussion and future work.
2 Related Work
There has been a lot of interest in using compara-
ble corpora for MT, primarily on extracting paral-
lel sentence pairs from comparable sources (Zhao
and Vogel, 2002; Fung and Yee, 1998). Some work
has gone beyond this focussing on extracting sub-
sentential fragments from noisier comparable data
(Munteanu and Marcu, 2006; Quirk et al, 2007).
The research conducted in this paper has two pri-
mary contributions and so we will discuss the related
work as relevant to each of them.
Our first contribution in this paper is the appli-
cation of active learning for acquiring comparable
data in the low-resource scenario, especially rele-
vant when working with low-resource languages.
There is some earlier work highlighting the need
for techniques to deal with low-resource scenar-
ios.(Munteanu and Marcu, 2005) propose bootstrap-
ping using an existing classifier for collecting new
data. However, this approach works when there is
a classifier of reasonable performance. In the ab-
sence of parallel corpora to train lexicons human
constructed dictionaries were used as an alternative
which may, however, not be available for a large
number of languages. Our proposal of active learn-
ing in this paper is suitable for highly impoverished
scenarios that require support from a human.
The second contribution of the paper is to ex-
tend the traditional active learning setup that is suit-
able for eliciting a single annotation. We highlight
the needs of the comparable corpora scenario where
we have two kinds of annotations - class label as-
signment and parallel segment extraction and pro-
pose strategies in active learning that involve multi-
ple annotations. A relevant setup is multitask learn-
ing (Caruana, 1997) which is increasingly becom-
ing popular in natural language processing for learn-
ing from multiple learning tasks. There has been
very less work in the area of multitask active learn-
ing. (Reichart et al, 2008) proposes an extension of
the single-sided active elicitation task to a multi-task
scenario, where data elicitation is performed for two
or more independent tasks at the same time. (Settles
et al, 2008) propose elicitation of annotations for
image segmentation under a multi-instance learning
framework.
Active learning with multiple annotations also has
similarities to the recent body of work in learn-
ing from instance feedback and feature feedback
(Melville et al, 2005). (Druck et al, 2009) pro-
pose active learning extensions to the gradient ap-
proach of learning from feature and instance feed-
back. However, in the comparable corpora problem
although the second annotation is geared towards
learning better features by enhancing the coverage
of the lexicon, the annotation itself is not on the fea-
tures but for extracting training data that is then used
to train the lexicon.
70
3 Supervised Comparable Sentence
Classification
In this section we discuss our supervised training
setup and the classification algorithm. Our classifier
tries to identify comparable sentences from among a
large pool of noisy comparable sentences. In this pa-
per we define comparable sentences as being trans-
lations that have around fifty percent or more trans-
lation equivalence. In future we will evaluate the ro-
bustness of the classifier by varying levels of noise
at the sentence level.
3.1 Training the Classifier
Following (Munteanu and Marcu, 2005), we use a
Maximum Entropy classifier to identify comparable
sentences. The classifier probability can be defined
as:
Pr(ci|S, T ) = 1Z(S, T )exp
?
?
n?
j=1
?jfij(ci, S, T )
?
?
where (S, T ) is a sentence pair, ci is the class, fij
are feature functions and Z(S) is a normalizing fac-
tor. The parameters ?i are the weights for the fea-
ture functions and are estimated by optimizing on a
training data set. For the task of classifying a sen-
tence pair, there are two classes, c0 = comparable
and c1 = non parallel. A value closer to one for
Pr(c1|S, T ) indicates that (S, T ) are comparable.
To train the classifier we need comparable sen-
tence pairs and non-parallel sentence pairs. While
it is easy to find negative examples online, ac-
quiring comparable sentences is non-trivial and re-
quires human intervention. (Munteanu and Marcu,
2005) construct negative examples automatically
from positive examples by pairing all source sen-
tences with all target sentences. We, however, as-
sume the availability of both positive and negative
examples to train the classifier. We use the GIS
learning algorithm for tuning the model parameters.
3.2 Feature Computation
The features are defined primarily based on trans-
lation lexicon probabilities. Rather than computing
word alignment between the two sentences, we use
lexical probabilities to determine alignment points
as follows: a source word s is aligned to a target
word t if p(s|t) > 0.5. Target word alignment is
computed similarly. Long contiguous sections of
aligned words indicate parallelism. We use the fol-
lowing features:
? Source and target sentence length ratio
? Source and target sentence length difference
? Lexical probability score, similar to IBM
model 1
? Number of aligned words
? Longest aligned word sequence
? Number of un-aligned words
Lexical probability score, and alignment features
generate two sets of features based on translation
lexica obtained by training in both directions. Fea-
tures are normalized with respect to the sentence
length.
Figure 1: Seed parallel corpora size vs. Classifier perfor-
mance in Urdu-English language pair
In our experiments we observe that the most in-
formative features are the ones involving the prob-
abilistic lexicon. However, the comparable corpora
obtained for training the classifier cannot be used for
automatically training a lexicon. We, therefore, re-
quire the availability of an initial seed parallel cor-
pus that can be used for computing the lexicon and
the associated feature functions. We notice that the
size of the seed corpus has a large influence on the
accuracy of the classifier. Figure 1 shows a plot with
71
the initial size of the corpus used to construct the
probabilistic lexicon on x-axis and its effect on the
accuracy of the classifier on y-axis. The sentences
were drawn randomly from a large pool of Urdu-
English parallel corpus and it is clear that a larger
pool of parallel sentences leads to a better lexicon
and an improved classifier.
4 Active Learning with Multiple
Annotations
4.1 Cost Motivation
Lack of existing annotated data requires reliable
human annotation that is expensive and effort-
intensive. We propose active learning for the prob-
lem of effectively acquiring multiple annotations
starting with unlabeled data. In active learning, the
learner has access to a large pool of unlabeled data
and sometimes a small portion of seed labeled data.
The objective of the active learner is then to se-
lect the most informative instances from the unla-
beled data and seek annotations from a human ex-
pert, which it then uses to retrain the underlying su-
pervised model for improving performance.
A meaningful setup to study multi annotation ac-
tive learning is to take into account the cost involved
for each of the annotations. In the case of compara-
ble corpora we have two annotation tasks, each with
cost modelsCost1 andCost2 respectively. The goal
of multi annotation active learning is to select the
optimal set of instances for each annotation so as to
maximize the benefit to the classifier. Unlike the tra-
ditional active learning, where we optimize the num-
ber of instances we label, here we optimize the se-
lection under a provided budget Bk per iteration of
the active learning algorithm.
4.2 Active Learning Setup
We now discuss our active learning framework for
building comparable corpora as shown in Algo-
rithm 1. We start with an unlabeled dataset U0 =
{xj =< sj , tj >} and a seed labeled dataset L0 =
{(< sj , tj >, ci)}, where c ? 0, 1 are class la-
bels with 0 being the non-parallel class and 1 being
the comparable data class. We also have T0 = {<
sk, tk >} which corresponds to parallel segments
or sentences identified from L0 that will be used in
training the probabilistic lexicon. Both T0 and L0
can be very small in size at the start of the active
learning loop. In our experiments, we tried with as
few as 50 to 100 sentences for each of the datasets.
We perform an iterative budget motivated active
learning loop for acquiring labeled data over k it-
erations. We start the active learning loop by first
training a lexicon with the available Tk and then us-
ing that we train the classifier over Lk. We, then
score all the sentences in the Uk using the model ?
and apply our selection strategy to retrieve the best
scoring instance or a small batch of instances. In the
simplest case we annotate this instance and add it
back to the tuning set Ck for re-training the classi-
fier. If the instance was a comparable sentence pair,
then we could also perform the second annotation
conditioned upon the availability of the budget. The
identified sub-segments (ssi , tti) are added back to
the training data Tk used for training the lexicon in
the subsequent iterations.
Algorithm 1 ACTIVE LEARNING SETUP
1: Given Unlabeled Comparable Corpus: U0
2: Given Seed Parallel Corpus: T0
3: Given Tuning Corpus: L0
4: for k = 0 to K do
5: Train Lexicon using Tk
6: ? = Tune Classifier using Ck
7: while Cost < Bk do
8: i = Query(Uk,Lk,Tk,?)
9: ci = Human Annotation-1 (si, ti)
10: (ssi ,tti) = Human Annotation-2 xi
11: Lk = Ck ? (si, ti, ci)
12: Tk = Tk ? (ssi, tti)
13: Uk = Uk - xi
14: Cost = Cost1 + Cost2
15: end while
16: end for
5 Sampling Strategies for Active Learning
5.1 Acquiring Training Data for Classifier
Our selection strategies for obtaining class labels for
training the classifier uses the model in its current
state to decide on the informative instances for the
next round of iterative training. We propose the fol-
lowing two sampling strategies for this task.
72
5.1.1 Certainty Sampling
This strategy selects instances where the current
model is highly confident. While this may seem
redundant at the outset, we argue that this crite-
ria can be a good sampling strategy when the clas-
sifier is weak or trained in an impoverished data
scenario. Certainty sampling strategy is a lot sim-
ilar to the idea of unsupervised approaches like
boosting or self-training. However, we make it a
semi-supervised approach by having a human in the
loop to provide affirmation for the selected instance.
Consider the following scenario. If we select an
instance that our current model prefers and obtain
a contradicting label from the human, then this in-
stance has a maximal impact on the decision bound-
ary of the classifier. On the other hand, if the label
is reaffirmed by a human, the overall variance re-
duces and in the process, it also helps in assigning
higher preference for the configuration of the deci-
sion boundary. (Melville et al, 2005) introduce a
certainty sampling strategy for the task of feature
labeling in a text categorization task. Inspired by
the same we borrow the name and also apply this
as an instance sampling approach. Given an in-
stance x and the classifier posterior distribution for
the classes as P (.), we select the most informative
instance as follows:
x? = argmaxxP (c = 1|x)
5.1.2 Margin-based Sampling
The certainty sampling strategy only considers the
instance that has the best score for the comparable
sentence class. However we could benefit from in-
formation about the second best class assigned to
the same instance. In the typical multi-class clas-
sification problems, earlier work shows success us-
ing such a ?margin based? approach (Scheffer et al,
2001), where the difference between the probabil-
ities assigned by the underlying model to the first
best and second best classes is used as the sampling
criteria.
Given a classifier with posterior distribution
over classes for an instance P (c = 1|x),
the margin based strategy is framed as x? =
argminxP (c1|x)? P (c2|x), where c1 is the best
prediction for the class and c2 is the second best
prediction under the model. It should be noted that
for binary classification tasks with two classes, the
margin sampling approach reduces to an uncertainty
sampling approach (Lewis and Catlett, 1994).
5.2 Acquiring Parallel Segments for Lexicon
Training
We now propose two sampling strategies for the sec-
ond annotation. Our goal is to select instances that
could potentially provide parallel segments for im-
proved lexical coverage and feature computation.
5.2.1 Diversity Sampling
We are interested in acquiring clean parallel seg-
ments for training a lexicon that can be used in fea-
ture computation. It is not clear how one could use a
comparable sentence pair to decide the potential for
extracting a parallel segment. However, it is highly
likely that if such a sentence pair has new cover-
age on the source side, then it increases the chances
of obtaining new coverage. We, therefore, propose
a diversity based sampling for extracting instances
that provide new vocabulary coverage . The scor-
ing function tc score(s) is defined below, where
V oc(s) is defined as the vocabulary of source sen-
tence s for an instance xi =< si, ti >, T is the set
of parallel sentences or segments extracted so far.
tc score(s) =
|T |?
s=1
sim(s, s?) ? 1|T | (1)
sim(s, s?) = |(V oc(s) ? V oc(s?)| (2)
5.2.2 Alignment Ratio
We also propose a strategy that provides direct in-
sight into the coverage of the underlying lexicon and
prefers a sentence pair that is more likely to be com-
parable. We call this alignment ratio and it can be
easily computed from the available set of features
discussed in Section 3 as below:
a score(s) = #unalignedwords#alignedwords (3)
s? = argmaxsa score(s) (4)
This strategy is quite similar to the diversity based
approach as both prefer selecting sentences that have
73
a potential to offer new vocabulary from the com-
parable sentence pair. However while the diver-
sity approach looks only at the source side coverage
and does not depend upon the underlying lexicon,
the alignment ratio utilizes the model for computing
coverage. It should also be noted that while we have
coverage for a word in the sentence pair, it may not
make it to the probabilistically trained and extracted
lexicon.
5.3 Combining Multiple Annotations
Finally, given two annotations and corresponding
sampling strategies, we try to jointly select the sen-
tence that is best suitable for obtaining both the an-
notations and is maximally beneficial to the classi-
fier. We select a single instance by combining the
scores from the different selection strategies as a
geometric mean. For instance, we consider a mar-
gin based sampling (margin) for the first annota-
tion and a diversity sampling (tc score) for the sec-
ond annotation, we can jointly select a sentence that
maximizes the combined score as shown below:
total score(s) = margin(s) ? tc score(s) (5)
s? = argmaxstotal score(s) (6)
6 Experiments and Results
6.1 Data
This research primarily focuses on identifying com-
parable sentences from a pool of dominantly non-
parallel sentences. To our knowledge, there is a
dearth of publicly available comparable corpora of
this nature. We, therefore, simulate a low-resource
scenario by using realistic assumptions of noise
and parallelism at both the corpus-level and the
sentence-level. In this section we discuss the pro-
cess and assumptions involved in the creation of our
datasets and try to mimic the properties of real-world
comparable corpora harvested from the web.
We first start with a sentence-aligned parallel cor-
pus available for the language pair. We then divide
the corpus into three parts. The first part is called
the ?sampling pool? and is set aside to use for draw-
ing sentences at random. The second part is used
to act as a non-parallel corpus. We achieve non-
parallelism by randomizing the mapping of the tar-
get sentences with the source sentences. This is a
slight variation of the strategy used in (Munteanu
and Marcu, 2005) for generating negative examples
for their classifier. The third part is used to synthe-
size a comparable corpus at the sentence-level. We
perform this by first selecting a parallel sentence-
pair and then padding either sides by a source and
target segment drawn independently from the sam-
pling pool. We control the length of the non-parallel
portion that is appended to be lesser than or equal
to the original length of the sentence. Therefore, the
resulting synthesized comparable sentence pairs are
guaranteed to contain at least 50% parallelism.
We use this dataset as the unlabeled pool from
which the active learner selects instances for label-
ing. Since the gold-standard labels for this corpus
are already available, which gives us better control
over automating the active learning process, which
typically requires a human in the loop. However,
our active learning strategies are in no way limited
by the simulated data setup and can generalize to the
real world scenario with an expert providing the la-
bels for each instance.
We perform our experiments with data from two
language pairs: Urdu-English and Spanish-English.
For Urdu-English, we use the parallel corpus NIST
2008 dataset released for the translation shared task.
We start with 50,000 parallel sentence corpus from
the released training data to create a corpus of
25,000 sentence pairs with 12,500 each of compa-
rable and non-parallel sentence pairs. Similarly, we
use 50,000 parallel sentences from the training data
released by the WMT 2008 datasets for Spanish-
English to create a corpus of 25,000 sentence pairs.
We also use two held-out data sets for training and
tuning the classifier, consisting of 1000 sentence
pairs (500 non-parallel and 500 comparable).
6.2 Results
We perform two kinds of evaluations: the first, to
show that our active learning strategies perform well
across language pairs and the second, to show that
multi annotation active learning leads to a good im-
provement in performance of the classifier.
6.2.1 How does the Active Learning perform?
In section 5, we proposed multiple active learn-
ing strategies for both eliciting both kinds of annota-
tions. A good active learning strategy should select
74
instances that contribute to the maximal improve-
ment of the classifier. The effectiveness of active
learning is typically tested by the number of queries
the learner asks and the resultant improvement in
the performance of the classifier. The classifier per-
formance in the comparable sentence classification
task can be computed as the F-score on the held out
dataset. For this work, we assume that both the an-
notations require the same effort level and so assign
uniform cost for eliciting each of them. Therefore
the number of queries is equivalent to the total cost
of supervision.
Figure 2: Active learning performance for the compara-
ble corpora classification in Urdu-English language-pair
Figure 3: Active learning performance for the compara-
ble corpora classification in Spanish-English language-
pair
Figure 2 shows our results for the Urdu-English
language pair, and Figure 3 plots the Spanish-
English results with the x-axis showing the total
number of queries posed to obtain annotations and
the y-axis shows the resultant improvement in accu-
racy of the classifier. In these experiments we do
not actively select for the second annotation but ac-
quire the parallel segment from the same sentence.
We compare this over a random baseline where the
sentence pair is selected at random and used for elic-
iting both annotations at the same time.
Firstly, we notice that both our active learn-
ing strategies: certainty sampling and margin-based
sampling perform better than the random baseline.
For the Urdu-English language pair we can see that
for the same effort expended (i.e 2000 queries) the
classifier has an increase in accuracy of 8 absolute
points. For Spanish-English language pair the ac-
curacy improvement is 6 points over random base-
line. Another observation from Figure 3 is that for
the classifier to reach an fixed accuracy of 68 points,
the random sampling method requires 2000 queries
while the from the active selection strategies require
significantly less effort of about 500 queries.
6.2.2 Performance of Joint Selection with
Multiple Annotations
We now evaluate our joint selection strategy that
tries to select the best possible instance for both
the annotations. Figure 4 shows our results for the
Urdu-English language pair, and Figure 5 plots the
Spanish-English results for active learning with mul-
tiple annotations. As before, the x-axis shows the
total number of queries posed, equivalent to the cu-
mulative effort for obtaining the annotations and the
y-axis shows the resultant improvement in accuracy
of the classifier.
We evaluate the multi annotation active learning
against two single-sided baselines where the sam-
pling focus is on selecting instances according to
strategies suitable for one annotation at a time. The
best performing active learning strategy for the class
label annotations is the certainty sampling (annot1)
and so for one single-sided baseline, we use this
baseline. We also obtain the second annotation for
the same instance. By doing so, we might be se-
lecting an instance that is sub-optimal for the sec-
ond annotation and therefore the resultant lexicon
may not maximally benefit from the instance. We
also observe, from our experiments, that the diver-
sity based sampling works well for the second anno-
75
tation and alignment ratio does not perform as well.
So, for the second single-sided baseline we use the
diversity based sampling strategy (annot2) and get
the first annotation for the same instance. Finally
we compare this with the joint selection approach
proposed earlier that combines both the annotation
strategies (annot1+annot2). In both the language
pairs we notice that joint selection for both anno-
tations performs better than the baselines.
Figure 4: Active learning with multiple annotations and
classification performance in Urdu-English
Figure 5: Active learning with multiple annotations and
classification performance in Spanish-English
7 Conclusion and Future Work
In this paper, we proposed active learning with mul-
tiple annotations for the challenge of building com-
parable corpora in low-resource scenarios. In par-
ticular, we identified two kinds of annotations: class
labels (for identifying comparable vs. non-parallel
data) and clean parallel segments within the com-
parable sentences. We implemented multiple inde-
pendent strategies for obtaining each of the abve in
a cost-effective manner. Our active learning experi-
ments in a simulated low-resource comparable cor-
pora scenario across two language pairs show signif-
icant results over strong baselines. Finally we also
proposed a joint selection strategy that selects a sin-
gle instance which is beneficial to both the annota-
tions. The results indicate an improvement over sin-
gle strategy baselines.
There are several interesting questions for future
work. Throughout the paper we assumed uniform
costs for both the annotations, which will need to
be verified with human subjects. We also hypoth-
esize that obtaining both annotations for the same
sentence may be cheaper than getting them from two
different sentences due to the overhead of context
switching. Another assumption is that of the exis-
tence of a single contiguous parallel segment in a
comparable sentence pair, which needs to be veri-
fied for corpora on the web.
Finally, active learning assumes availability of an
expert to answer the queries. Availability of an ex-
pert for low-resource languages and feasibility of
running large scale experiments is difficult. We,
therefore, have started working on crowdsourcing
these annotation tasks on Amazon Mechanical Turk
(MTurk) where it is easy to find people and quickly
run experiments with real people.
Acknowledgement
This material is based upon work supported in part
by the U. S. Army Research Laboratory and the U.
S. Army Research Office under grant W911NF-10-
1-0533, and in part by NSF under grant IIS 0916866.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Rich Caruana. 1997. Multitask learning. In Machine
Learning, pages 41?75.
Gregory Druck, Burr Settles, and Andrew McCallum.
2009. Active learning by labeling features. In Pro-
ceedings of Conference on Empirical Methods in Nat-
76
ural Language Processing (EMNLP 2009), pages 81?
90.
Jenny Rose Finkel and Christopher D. Manning. 2010.
Hierarchical joint learning: Improving joint parsing
and named entity recognition with non-jointly labeled
data. In Proceedings of ACL 2010.
Pascale Fung and Lo Yen Yee. 1998. An IR approach for
translating new words from nonparallel, comparable
texts. In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics, pages
414?420, Montreal, Canada.
David D. Lewis and Jason Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning. In In
Proceedings of the Eleventh International Conference
on Machine Learning, pages 148?156. Morgan Kauf-
mann.
Prem Melville, Foster Provost, Maytal Saar-Tsechansky,
and Raymond Mooney. 2005. Economical active
feature-value acquisition through expected utility esti-
mation. In UBDM ?05: Proceedings of the 1st interna-
tional workshop on Utility-based data mining, pages
10?16, New York, NY, USA. ACM.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 81?88, Sydney, Aus-
tralia.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345?1359, October.
Chris Quirk, Raghavendra U. Udupa, and Arul Menezes.
2007. Generative models of noisy translations with
applications to parallel fragment extraction. In Pro-
ceedings of the Machine Translation Summit XI, pages
377?384, Copenhagen, Denmark.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-
poport. 2008. Multi-task active learning for linguis-
tic annotations. In Proceedings of ACL-08: HLT,
pages 861?869, Columbus, Ohio, June. Association
for Computational Linguistics.
Philip Resnik and Noah A. Smith. 2003. The web as a
parallel corpus. Comput. Linguist., 29(3):349?380.
Tobias Scheffer, Christian Decomain, and Stefan Wro-
bel. 2001. Active hidden markov models for informa-
tion extraction. In IDA ?01: Proceedings of the 4th
International Conference on Advances in Intelligent
Data Analysis, pages 309?318, London, UK. Springer-
Verlag.
Burr Settles, Mark Craven, and Soumya Ray. 2008.
Multiple-instance active learning. In In Advances in
Neural Information Processing Systems (NIPS, pages
1289?1296. MIT Press.
Bing Zhao and Stephan Vogel. 2002. Full-text story
alignment models for chinese-english bilingual news
corpora. In Proceedings of the ICSLP ?02, September.
77
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 198?206,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Wider Context by Using Bilingual Language Models in Machine Translation
Jan Niehues1, Teresa Herrmann1, Stephan Vogel2 and Alex Waibel1,2
1Institute for Anthropomatics, KIT - Karlsruhe Institute of Technology, Germany
2 Language Techonolgies Institute, Carnegie Mellon University, USA
1firstname.lastname@kit.edu 2lastname@cs.cmu.edu
Abstract
In past Evaluations for Machine Translation of
European Languages, it could be shown that
the translation performance of SMT systems
can be increased by integrating a bilingual lan-
guage model into a phrase-based SMT system.
In the bilingual language model, target words
with their aligned source words build the to-
kens of an n-gram based language model. We
analyzed the effect of bilingual language mod-
els and show where they could help to bet-
ter model the translation process. We could
show improvements of translation quality on
German-to-English and Arabic-to-English. In
addition, for the Arabic-to-English task, train-
ing an extra bilingual language model on the
POS tags instead of the surface word forms
led to further improvements.
1 Introduction
In many state-of-the art SMT systems, the phrase-
based (Koehn et al, 2003) approach is used. In
this approach, instead of building the translation by
translating word by word, sequences of source and
target words, so-called phrase pairs, are used as the
basic translation unit. A table of correspondences
between source and target phrases forms the transla-
tion model in this approach. Target language fluency
is modeled by a language model storing monolin-
gual n-gram occurrences. A log-linear combination
of these main models as well as additional features
is used to score the different translation hypotheses.
Then the decoder searches for the translation with
the highest score.
A different approach to SMT is to use a stochas-
tic finite state transducer based on bilingual n-
grams (Casacuberta and Vidal, 2004). This ap-
proach was for example successfully applied by Al-
lauzen et al (2010) on the French-English trans-
lation task. In this so-called n-gram approach the
translation model is trained by using an n-gram lan-
guage model of pairs of source and target words,
called tuples. While the phrase-based approach cap-
tures only bilingual context within the phrase pairs,
in the n-gram approach the n-gram model trained on
the tuples is used to capture bilingual context be-
tween the tuples. As in the phrase-based approach,
the translation model can also be combined with ad-
ditional models like, for example, language models
using log-linear combination.
Inspired by the n-gram-based approach, we in-
troduce a bilingual language model that extends
the translation model of the phrase-based SMT ap-
proach by providing bilingual word context. In ad-
dition to the bilingual word context, this approach
enables us also to integrate a bilingual context based
on part of speech (POS) into the translation model.
When using phrase pairs it is complicated to use
different kinds of bilingual contexts, since the con-
text of the POS-based phrase pairs should be bigger
than the word-based ones to make the most use of
them. But there is no straightforward way to inte-
grate phrase pairs of different lengths into the trans-
lation model in the phrase-based approach, while it
is quite easy to use n-gram models with different
context lengths on the tuples. We show how we can
use bilingual POS-based language models to capture
longer bilingual context in phrase-based translation
198
systems.
This paper is structured in the following way: In
the next section, we will present some related work.
Afterwards, in Section 3, a motivation for using the
bilingual language model will be given. In the fol-
lowing section the bilingual language model is de-
scribed in detail. In Section 5, the results and an
analysis of the translation results is given, followed
by a conclusion.
2 Related Work
The n-gram approach presented in Mari?o et al
(2006) has been derived from the work of Casacu-
berta and Vidal (2004), which used finite state trans-
ducers for statistical machine translation. In this ap-
proach, units of source and target words are used as
basic translation units. Then the translation model is
implemented as an n-gram model over the tuples. As
it is also done in phrase-based translations, the dif-
ferent translations are scored by a log-linear combi-
nation of the translation model and additional mod-
els.
Crego and Yvon (2010) extended the approach to
be able to handle different word factors. They used
factored language models introduced by Bilmes and
Kirchhoff (2003) to integrate different word factors
into the translation process. In contrast, we use a
log-linear combination of language models on dif-
ferent factors in our approach.
A first approach of integrating the idea presented
in the n-gram approach into phrase-based machine
translation was described in Matusov et al (2006).
In contrast to our work, they used the bilingual units
as defined in the original approach and they did not
use additional word factors.
Hasan et al (2008) used lexicalized triplets to in-
troduce bilingual context into the translation pro-
cess. These triplets include source words from out-
side the phrase and form and additional probability
p(f |e, e?) that modifies the conventional word prob-
ability of f given e depending on trigger words e? in
the sentence enabling a context-based translation of
ambiguous phrases.
Other approaches address this problem by inte-
grating word sense disambiguation engines into a
phrase-based SMT system. In Chan and Ng (2007)
a classifier exploits information such as local col-
locations, parts-of-speech or surrounding words to
determine the lexical choice of target words, while
Carpuat and Wu (2007) use rich context features
based on position, syntax and local collocations to
dynamically adapt the lexicons for each sentence
and facilitate the choice of longer phrases.
In this work we present a method to extend the
locally limited context of phrase pairs and n-grams
by using bilingual language models. We keep the
phrase-based approach as the main SMT framework
and introduce an n-gram language model trained in a
similar way as the one used in the finite state trans-
ducer approach as an additional feature in the log-
linear model.
3 Motivation
To motivate the introduction of the bilingual lan-
guage model, we will analyze the bilingual context
that is used when selecting the target words. In a
phrase-based system, this context is limited by the
phrase boundaries. No bilingual information outside
the phrase pair is used for selecting the target word.
The effect can be shown in the following example
sentence:
Ein gemeinsames Merkmal aller extremen
Rechten in Europa ist ihr Rassismus
und die Tatsache, dass sie das Einwan-
derungsproblem als politischen Hebel be-
nutzen.
Using our phrase-based SMT system, we get the
following segmentation into phrases on the source
side: ein gemeinsames, Merkmal, aller, extremen
Rechten. That means, that the translation of Merk-
mal is not influenced by the source words gemein-
sames or aller.
However, apart from this segmentation, other
phrases could have been conceivable for building a
translation:
ein, ein gemeinsames, ein gemeinsames Merk-
mal, gemeinsames, gemeinsames Merkmal, Merk-
mal aller, aller, extremen, extremen Rechten and
Rechten.
As shown in Figure 1 the translation for the
first three words ein gemeinsames Merkmal into a
common feature can be created by segmenting it
into ein gemeinsames and Merkmal as done by the
199
Figure 1: Alternative Segmentations
phrase-based system or by segmenting it into ein and
gemeinsames Merkmal. In the phrase-based system,
the decoder cannot make use of the fact that both
segmentation variants lead to the same translation,
but has to select one and use only this information
for scoring the hypothesis.
Consequently, if the first segmentation is cho-
sen, the fact that gemeinsames is translated to com-
mon does effect the translation of Merkmal only by
means of the language model, but no bilingual con-
text can be carried over the segmentation bound-
aries.
To overcome this drawback of the phrase-based
approach, we introduce a bilingual language model
into the phrase-based SMT system. Table 1 shows
the source and target words and demonstrates how
the bilingual phrases are constructed and how the
source context stays available over segment bound-
aries in the calculation of the language model score
for the sentence. For example, when calculating the
language model score for the word feature P ( fea-
ture_Merkmal | common_gemeinsames) we can see
that through the bilingual tokens not only the previ-
ous target word but also the previous source word is
known and can influence the translation even though
it is in a different segment.
4 Bilingual Language Model
The bilingual language model is a standard n-gram-
based language model trained on bilingual tokens in-
stead of simple words. These bilingual tokens are
motivated by the tuples used in n-gram approaches
to machine translation. We use different basic units
for the n-gram model compared to the n-gram ap-
proach, in order to be able to integrate them into a
phrase-based translation system.
In this context, a bilingual token consists of a tar-
get word and all source words that it is aligned to.
More formally, given a sentence pair eI1 = e1...eI
and fJ1 = f1...fJ and the corresponding word align-
ment A = {(i, j)} the following tokens are created:
tj = {fj} ? {ei|(i, j) ? A} (1)
Therefore, the number of bilingual tokens in a
sentence equals the number of target words. If a
source word is aligned to two target words like the
word aller in the example sentence, two bilingual to-
kens are created: all_aller and the_aller. If, in con-
trast, a target word is aligned to two source words,
only one bilingual token is created consisting of the
target word and both source words.
The existence of unaligned words is handled in
the following way. If a target word is not aligned
to any source word, the corresponding bilingual to-
ken consists only of the target word. In contrast, if a
source word is not aligned to any word in the target
language sentence, this word is ignored in the bilin-
gual language model.
Using this definition of bilingual tokens the trans-
lation probability of source and target sentence and
the word alignment is then defined by:
p(eI1, f
J
1 , A) =
J?
j=1
P (tj |tj?1...tj?n) (2)
This probability is then used in the log-linear com-
bination of a phrase-based translation system as an
additional feature. It is worth mentioning that al-
though it is modeled like a conventional language
model, the bilingual language model is an extension
to the translation model, since the translation for the
source words is modeled and not the fluency of the
target text.
To train the model a corpus of bilingual tokens can
be created in a straightforward way. In the genera-
tion of this corpus the order of the target words de-
fines the order of the bilingual tokens. Then we can
use the common language modeling tools to train
the bilingual language model. As it was done for
the normal language model, we used Kneser-Ney
smoothing.
4.1 Comparison to Tuples
While the bilingual tokens are motivated by the tu-
ples in the n-gram approach, there are quite some
differences. They are mainly due to the fact that the
200
Source Target Bi-word LM Prob
ein a a_ein P(a_ein | <s>)
gemeinsames common common_gemeinsames P(common_gemeinsames | a_ein, <s>)
Merkmal feature feature_Merkmal P(feature_Merkmal | common_gemeinsames)
of of_ P(of_ | feature_Merkmal)
aller all all_aller P(all_aller | of_)
aller the the_aller P(the_aller | all_aller, of_)
extremen extreme extreme_extremen P(extreme_extremen)
Rechten right right_Rechten P(right_Rechten | extreme_extremen)
Table 1: Example Sentence: Segmentation and Bilingual Tokens
tuples are also used to guide the search in the n-gram
approach, while the search in the phrase-based ap-
proach is guided by the phrase pairs and the bilin-
gual tokens are only used as an additional feature in
scoring.
While no word inside a tuple can be aligned to
a word outside the tuple, the bilingual tokens are
created based on the target words. Consequently,
source words of one bilingual token can also be
aligned to target words inside another bilingual to-
ken. Therefore, we do not have the problems of em-
bedded words, where there is no independent trans-
lation probability.
Since we do not create a a monotonic segmenta-
tion of the bilingual sentence, but only use the seg-
mentation according to the target word order, it is
not clear where to put source words, which have no
correspondence on the target side. As mentioned be-
fore, they are ignored in the model.
But an advantage of this approach is that we have
no problem handling unaligned target words. We
just create bilingual tokens with an empty source
side. Here, the placing order of the unaligned tar-
get words is guided by the segmentation into phrase
pairs.
Furthermore, we need no additional pruning of
the vocabulary due to computation cost, since this is
already done by the pruning of the phrase pairs. In
our phrase-based system, we allow only for twenty
translations of one source phrase.
4.2 Comparison to Phrase Pairs
Using the definition of the bilingual language model,
we can again have a look at the introductory example
sentence. We saw that when translating the phrase
ein gemeinsames Merkmal using a phrase-based sys-
tem, the translation of gemeinsames into common
can only be influenced by either the preceeding ein
# a or by the succeeding Merkmal # feature, but
not by both of them at the same time, since either
the phrase ein gemeinsames or the phrase gemein-
sames Merkmal has to be chosen when segmenting
the source sentence for translation. If we now look
at the context that can be used when translating this
segment applying the bilingual language model, we
see that the translation of gemeinsames into com-
mon is on the one hand influenced by the translation
of the token ein # a within the bilingual language
model probability P (common_gemeinsames | a_ein,
<s>).
On the other hand, it is also influenced by the
translation of the word Merkmal into feature en-
coded into the probability P (feature_Merkmal |
common_gemeinsames). In contrast to the phrase-
based translation model, this additional model is ca-
pable of using context information from both sides
to score the translation hypothesis. In this way,
when building the target sentence, the information
of aligned source words can be considered even be-
yond phrase boundaries.
4.3 POS-based Bilingual Language Models
When translating with the phrase-based approach,
the decoder evaluates different hypotheses with dif-
ferent segmentations of the source sentence into
phrases. The segmentation depends on available
phrase pair combinations but for one hypothesis
translation the segmentation into phrases is fixed.
This leads to problems, when integrating parallel
POS-based information. Since the amount of differ-
201
ent POS tags in a language is very small compared to
the number of words in a language, we could man-
age much longer phrase pairs based on POS tags
compared to the possible length of phrase pairs on
the word level.
In a phrase-based translation system the average
phrase length is often around two words. For POS
sequences, in contrast, sequences of 4 tokens can
often be matched. Consequently, this information
can only help, if a different segmentation could be
chosen for POS-based phrases and for word-based
phrases. Unfortunately, there is no straightforward
way to integrate this into the decoder.
If we now look at how the bilingual language
model is applied, it is much easier to integrate the
POS-based information. In addition to the bilin-
gual token for every target word we can generate a
bilingual token based on the POS information of the
source and target words. Using this bilingual POS
token, we can train an additional bilingual POS-
based language model and apply it during transla-
tion. In this case it is no longer problematic if the
context of the POS-based bilingual language model
is longer than the one based on the word informa-
tion, because word and POS sequences are scored
separately by two different language models which
cover different n-gram lengths.
The training of the bilingual POS language model
is straightforward. We can build the corpus of bilin-
gual POS tokens based on the parallel corpus of
POS tags generated by running a POS tagger over
both source and target side of the initial parallel cor-
pus and the alignment information for the respective
words in the text corpora.
During decoding, we then also need to know the
POS tag for every source and target word. Since
we build the sentence incrementally, we cannot use
the tagger directly. Instead, we store also the POS
source and target sequences during the phrase ex-
traction. When creating the bilingual phrase pair
with POS information, there might be different pos-
sibilities of POS sequences for the source and target
phrases. But we keep only the most probable one for
each phrase pair. For the Arabic-to-English trans-
lation task, we compared the generated target tags
with the tags created by the tagger on the automatic
translations. They are different on less than 5% of
the words.
Using the alignment information as well as the
source and target POS sequences we can then create
the POS-based bilingual tokens for every phrase pair
and store it in addition to the normal phrase pairs.
At decoding time, the most frequent POS tags in the
bilingual phrases are used as tags for the input sen-
tence and the translation is done based on the bilin-
gual POS tokens built from these tags together with
their alignment information.
5 Results
We evaluated and analyzed the influence of the bilin-
gual language model on different languages. On
the one hand, we measured the performance of the
bilingual language model on German-to-English on
the News translation task. On the other hand, we
evaluated the approach on the Arabic-to-English di-
rection on News and Web data. Additionally, we
present the impact of the bilingual language model
on the English-to-German, German-to-English and
French-to-English systems with which we partici-
pated in the WMT 2011.
5.1 System Description
The German-to-English translation system was
trained on the European Parliament corpus, News
Commentary corpus and small amounts of addi-
tional Web data. The data was preprocessed and
compound splitting was applied. Afterwards the dis-
criminative word alignment approach as described
in (Niehues and Vogel, 2008) was applied to gener-
ate the alignments between source and target words.
The phrase table was built using the scripts from the
Moses package (Koehn et al, 2007). The language
model was trained on the target side of the paral-
lel data as well as on additional monolingual News
data. The translation model as well as the language
model was adapted towards the target domain in a
log-linear way.
The Arabic-to-English system was trained us-
ing GALE Arabic data, which contains 6.1M sen-
tences. The word alignment is generated using
EMDC, which is a combination of a discriminative
approach and the IBM Models as described in Gao
et al (2010). The phrase table is generated using
Chaski as described in Gao and Vogel (2010). The
language model data we trained on the GIGAWord
202
V3 data plus BBN English data. After splitting the
corpus according to sources, individual models were
trained. Then the individual models were interpo-
lated to minimize the perplexity on the MT03/MT04
data.
For both tasks the reordering was performed as a
preprocessing step using POS information from the
TreeTagger (Schmid, 1994) for German and using
the Amira Tagger (Diab, 2009) for Arabic. For Ara-
bic the approach described in Rottmann and Vogel
(2007) was used covering short-range reorderings.
For the German-to-English translation task the ex-
tended approach described in Niehues et al (2009)
was used to cover also the long-range reorderings
typical when translating between German and En-
glish.
For both directions an in-house phrase-based de-
coder (Vogel, 2003) was used to generate the transla-
tion hypotheses and the optimization was performed
using MER training. The performance on the test-
sets were measured in case-insensitive BLEU and
TER scores.
5.2 German to English
We evaluated the approach on two different test sets
from the News Commentary domain. The first con-
sists of 2000 sentences with one reference. It will
be referred to as Test 1. The second test set consists
of 1000 sentences with two references and will be
called Test 2.
5.2.1 Translation Quality
In Tables 2 and 3 the results for translation per-
formance on the German-to-English translation task
are summarized.
As it can been seen, the improvements of transla-
tion quality vary considerably between the two dif-
ferent test sets. While using the bilingual language
model improves the translation by only 0.15 BLEU
and 0.21 TER points on Test 1, the improvement on
Test 2 is nearly 1 BLEU point and 0.5 TER points.
5.2.2 Context Length
One intention of using the bilingual language
model is its capability to capture the bilingual con-
texts in a different way. To see, whether additional
bilingual context is used during decoding, we ana-
lyzed the context used by the phrase pairs and by
the n-gram bilingual language model.
However, a comparison of the different context
lengths is not straightforward. The context of an n-
gram language model is normally described by the
average length of applied n-grams. For phrase pairs,
normally the average target phrase pair length (avg.
Target PL) is used as an indicator for the size of the
context. And these two numbers cannot be com-
pared directly.
To be able to compare the context used by the
phrase pairs to the context used in the n-gram lan-
guage model, we calculated the average left context
that is used for every target word where the word
itself is included, i.e. the context of a single word
is 1. In case of the bilingual language model the
score for the average left context is exactly the aver-
age length of applied n-grams in a given translation.
For phrase pairs the average left context can be cal-
culated in the following way: A phrase pair of length
1 gets a left context score of 1. In a phrase pair of
length 2, the first word has a left context score of 1,
since it is not influenced by any target word to the
left. The second word in that phrase pair gets a left
context count of 2, because it is influenced by the
first word in the phrase. Correspondingly, the left
context score of a phrase pair of length 3 is 6 (com-
posed of the score 1 for the first word, score 2 for
the second word and score 3 for the third word). To
get the average left context for the whole translation,
the context scores of all phrases are summed up and
divided by the number of words in the translation.
The scores for the average left contexts for the two
test sets are shown in Tables 2 and 3. They are called
avg. PP Left Context. As it can be seen, the con-
text used by the bilingual n-gram language model is
longer than the one by the phrase pairs. The average
n-gram length increases from 1.58 and 1.57, respec-
tively to 2.21 and 2.18 for the two given test sets.
If we compare the average n-gram length of the
bilingual language model to the one of the target
language model, the n-gram length of the first is of
course smaller, since the number of possible bilin-
gual tokens is higher than the number of possible
monolingual words. This can also be seen when
looking at the perplexities of the two language mod-
els on the generated translations. While the perplex-
ity of the target language model is 99 and 101 on
Test 1 and 2, respectively, the perplexity of the bilin-
203
gual language model is 512 and 538.
Metric No BiLM BiLM
BLEU 30.37 30.52
TER 50.27 50.06
avg. Target PL 1.66 1.66
avg. PP Left Context 1.57 1.58
avg. Target LM N-Gram 3.28 3.27
avg. BiLM N-Gram 2.21
Table 2: German-to-English results (Test 1)
Metric No BiLM BiLM
BLEU 44.16 45.09
TER 41.02 40.52
avg. Target PL 1.65 1.65
avg. PP Left Context 1.56 1.57
avg. Target LM N-Gram 3.25 3.23
avg. BiLM N-Gram 2.18
Table 3: German-to-English results (Test 2)
5.2.3 Overlapping Context
An additional advantage of the n-gram-based ap-
proach is the possibility to have overlapping con-
text. If we would always use phrase pairs of length
2 only half of the adjacent words would influence
each other in the translation. The others are only
influenced by the other target words through the lan-
guage model. If we in contrast would have a bilin-
gual language model which uses an n-gram length
of 2, this means that every choice of word influences
the previous and the following word.
To analyze this influence, we counted how many
borders of phrase pairs are covered by a bilingual
n-gram. For Test 1, 16783 of the 27785 borders
between phrase pairs are covered by a bilingual n-
gram. For Test 2, 9995 of 16735 borders are cov-
ered. Consequently, in both cases at around 60 per-
cent of the borders additional information can be
used by the bilingual n-gram language model.
5.2.4 Bilingual N-Gram Length
For the German-to-English translation task we
performed an additional experiment comparing dif-
ferent n-gram lengths of the bilingual language
BiLM Length aNGL BLEU TER
No 30.37 50.27
1 1 29.67 49.73
2 1.78 30.36 50.05
3 2.11 30.47 50.08
4 2.21 30.52 50.06
5 2.23 30.52 50.07
6 2.24 30.52 50.07
Table 4: Different N-Gram Lengths (Test 1)
BiLM Length aNGL BLEU TER
No 44.16 41.02
1 1 44.22 40.53
2 1.78 45.11 40.38
3 2.09 45.18 40.51
4 2.18 45.09 40.52
5 2.21 45.10 40.52
6 2.21 45.10 40.52
Table 5: Different N-Gram Lengths (Test 2)
model. To ensure comparability between the exper-
iments and avoid additional noise due to different
optimization results, we did not perform separate
optimization runs for for each of the system vari-
ants with different n-gram length, but used the same
scaling factors for all of them. Of course, the sys-
tem using no bilingual language model was trained
independently. In Tables 4 and 5 we can see that the
length of the actually applied n-grams as well as the
BLEU score increased until the bilingual language
model reaches an order of 4. For higher order bilin-
gual language models, nearly no additional n-grams
can be found in the language models. Also the trans-
lation quality does not increase further when using
longer n-grams.
5.3 Arabic to English
The Arabic-to-English system was optimized on the
MT06 data. As test set the Rosetta in-house test set
DEV07-nw (News) and wb (Web Data) was used.
The results for the Arabic-to-English translation
task are summarized in Tables 6 and 7. The perfor-
mance was tested on two different domains, transla-
tion of News and Web documents. On both tasks,
the translation could be improved by more than 1
204
BLEU point. Measuring the performance in TER
also shows an improvement by 0.7 and 0.5 points.
By adding a POS-based bilingual language
model, the performance could be improved further.
An additional gain of 0.2 BLEU points and decrease
of 0.3 points in TER could be reached. Conse-
quently, an overall improvement of up to 1.7 BLEU
points could be achieved by integrating two bilin-
gual language models, one based on surface word
forms and one based on parts-of-speech.
System
Dev Test
BLEU TER BLEU
NoBiLM 48.42 40.77 52.05
+ BiLM 49.29 40.04 53.51
+ POS BiLM 49.56 39.85 53.71
Table 6: Results on Arabic to English: Translation of
News
System
Dev Test
BLEU TER BLEU
NoBiLM 48.42 47.14 41.90
+ BiLM 49.29 46.66 43.12
+ POS BiLM 49.56 46.40 43.28
Table 7: Results on Arabic to English: Translation of
Web documents
As it was done for the German-to-English system,
we also compared the context used by the different
models for this translation direction. The results are
summarized in Table 8 for the News test set and in
Table 9 for the translation of Web data. It can be seen
like it was for the other language pair that the context
used in the bilingual language model is bigger than
the one used by the phrase-based translation model.
Furthermore, it is worth mentioning that shorter
phrase pairs are used, when using the POS-based
bilingual language model. Both bilingual language
models seem to model the context quite good, so that
less long phrase pairs are needed to build the trans-
lation. Instead, the more frequent short phrases can
be used to generate the translation.
5.4 Shared Translation Task @ WMT2011
The bilingual language model was included in 3
systems built for the WMT2011 Shared Translation
Metric No BiLM POS BiLM
BLEU 52.05 53.51 53.71
avg. Target PL 2.12 2.03 1.79
avg. PP Left Context 1.92 1.85 1.69
avg. BiLM N-Gram 2.66 2.65
avg. POS BiLM 4.91
Table 8: Bilingual Context in Arabic-to-English results
(News)
Metric No BiLM POS BiLM
BLEU 41.90 43.12 43.28
avg. Target PL 1.82 1.80 1.57
avg. PP Left Context 1.72 1.69 1.53
avg. BiLM N-Gram 2.33 2.31
avg. POS BiLM 4.49
Table 9: Bilingual Context in Arabic-to-English results
(Web data)
Task evaluation. A phrase-based system similar to
the one described before for the German-to-English
results was used. A detailed system description can
be found in Herrmann et al (2011). The results are
summarized in Table 10. The performance of com-
petitive systems could be improved in all three lan-
guages by up to 0.4 BLEU points.
Language Pair No BiLM BiLM
German-English 24.12 24.52
English-German 16.89 17.01
French-English 28.17 28.34
Table 10: Preformance of Bilingual language model at
WMT2011
6 Conclusion
In this work we showed how a feature of the n-gram-
based approach can be integrated into a phrase-
based statistical translation system. We performed
a detailed analysis on how this influences the scor-
ing of the translation system. We could show im-
provements on a variety of translation tasks cover-
ing different languages and domains. Furthermore,
we could show that additional bilingual context in-
formation is used.
Furthermore, the additional feature can easily be
205
extended to additional word factors such as part-of-
speech, which showed improvements for the Arabic-
to-English translation task.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Fran?ois Yvon. 2010. LIMSI?s Statisti-
cal Translation Systems for WMT?10. In Fifth Work-
shop on Statistical Machine Translation (WMT 2010),
Uppsala, Sweden.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Fac-
tored language models and generalized parallel back-
off. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 4?6, Stroudsburg, PA, USA.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation using Word Sense Disam-
biguation. In In The 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning.
Francisco Casacuberta and Enrique Vidal. 2004. Ma-
chine Translation with Inferred Stochastic Finite-State
Transducers. Comput. Linguist., 30:205?225, June.
Yee Seng Chan and Hwee Tou Ng. 2007. Word Sense
Disambiguation improves Statistical Machine Trans-
lation. In In 45th Annual Meeting of the Association
for Computational Linguistics (ACL-07, pages 33?40.
Josep M. Crego and Fran?ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, 24, June.
Mona Diab. 2009. Second Generation Tools (AMIRA
2.0): Fast and Robust Tokenization, POS tagging, and
Base Phrase Chunking. In Proc. of the Second Interna-
tional Conference on Arabic Language Resources and
Tools, Cairo, Egypt, April.
Qin Gao and Stephan Vogel. 2010. Training Phrase-
Based Machine Translation Models on the Cloud:
Open Source Machine Translation Toolkit Chaski. In
The Prague Bulletin of Mathematical Linguistics No.
93.
Qin Gao, Francisco Guzman, and Stephan Vogel.
2010. EMDC: A Semi-supervised Approach for Word
Alignment. In Proc. of the 23rd International Confer-
ence on Computational Linguistics, Beijing, China.
Sa?a Hasan, Juri Ganitkevitch, Hermann Ney, and Jes?s
Andr?s-Ferrer. 2008. Triplet Lexicon Models for Sta-
tistical Machine Translation. In Proc. of Conference
on Empirical Methods in NLP, Honolulu, USA.
Teresa Herrmann, Mohammed Mediani, Jan Niehues,
and Alex Waibel. 2011. The Karlsruhe Institute of
Technology Translation Systems for the WMT 2011.
In Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinbugh, U.K.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48?
54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007, Demonstration Session, Prague, Czech Repub-
lic, June 23.
Jos? B. Mari?o, Rafael E. Banchs, Josep M. Crego, Adri?
de Gispert, Patrik Lambert, Jos? A. R. Fonollosa, and
Marta R. Costa-juss?. 2006. N-gram-based machine
translation. Comput. Linguist., 32, December.
Evgeny Matusov, Richard Zens, David Vilar, Arne
Mauser, Maja Popovic?, Sa?a Hasan, and Hermann
Ney. 2006. The rwth machine translation system. In
TC-STAR Workshop on Speech-to-Speech Translation,
pages 31?36, Barcelona, Spain, June.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In
Proc. of Third ACL Workshop on Statistical Machine
Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and Alex
Waibel. 2009. The Universit?t Karlsruhe Translation
System for the EACL-WMT 2009. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
Based Distortion Model. In TMI, Sk?vde, Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing,
Manchester, UK.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Pro-
cessing and Knowledge Engineering, Beijing, China.
206
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 386?392,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
CMU Haitian Creole-English Translation System for WMT 2011
Sanjika Hewavitharana, Nguyen Bach, Qin Gao, Vamshi Ambati, Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{sanjika,nbach,qing,vamshi,vogel+}@cs.cmu.edu
Abstract
This paper describes the statistical machine
translation system submitted to the WMT11
Featured Translation Task, which involves
translating Haitian Creole SMS messages into
English. In our experiments we try to ad-
dress the issue of noise in the training data,
as well as the lack of parallel training data.
Spelling normalization is applied to reduce
out-of-vocabulary words in the corpus. Us-
ing Semantic Role Labeling rules we expand
the available training corpus. Additionally we
investigate extracting parallel sentences from
comparable data to enhance the available par-
allel data.
1 Introduction
In this paper we describe the CMU-SMT Haitian
Creole-English translation system that was built as
part of the Featured Translation Task of the WMT11.
The task involved translating text (SMS) messages
that were collected during the humanitarian opera-
tions in the aftermath of the earthquake in Haiti in
2010.
Due to the circumstances of this situation, the
SMS messages were often noisy, and contained in-
complete information. Additionally they sometimes
contained text from other languages (e.g. French).
As is typical in SMS messages, abbreviated text (as
well as misspelled words) were present. Further,
since the Haitian Creole orthography is not fully
standardized (Allen, 1998), the text inherently con-
tained several different spelling variants.
These messages were translated into English by
a group of volunteers during the disaster response.
The background and the details of this crowdsourc-
ing translation effort is discussed in Munro (2010).
Some translations contain additional annotations
which are not part of the original SMS, possibly
added by the translators to clarify certain issues with
the original message. Along with the noise, spelling
variants, and fragmented nature of the SMS mes-
sages, the annotations contribute to the overall diffi-
culty in building a machine translation system with
this type of data. We aim to address some of these
issues in out effort.
Another challenge with building a Haitian Creole-
English translation system is the lack of parallel
data. As Haitian Creole is a less commonly spo-
ken language, the available resources are limited.
Other than the manually translated SMS messages,
the available Haitian Creole-English parallel data
is about 2 million tokens, which is considerably
smaller than the parallel data available for the Stan-
dard Translation Task of the WMT11.
Lewis (2010) details the effort quickly put
forth by the Microsoft Translator team in building
a Haitian Creole-English translation system from
scratch, as part of the relief effort in Haiti. We took
a similar approach to this shared task: rapidly build-
ing a translation system to a new language pair uti-
lizing available resources. Within a short span (of
about one week), we built a baseline translation sys-
tem, identified the problems with the system, and
exploited several approaches to rectify them and im-
prove its overall performance. We addressed the is-
sues above (namely: noise in the data and sparsity of
parallel data) when building our translation system
for Haitian Creole-English task. We also normalized
386
different spelling variations to reduce the number of
out-of-vocabulary (OOV) tokens in the corpus. We
used Semantic Role Labeling to expand the available
training corpus. Additionally we exploited other re-
sources, such as comparable corpora, to extract par-
allel data to enhance the limited amount of available
parallel data.
The paper is organized as follows: Section 2
presents the baseline system used, along with a de-
scription of training and testing data used. Section 3
explains different preprocessing schemes that were
tested for SMS data, and their effect on the trans-
lation performance. Corpus expansion approach is
given in Section 4. Parallel data extraction from
comparable corpora is presented in section 5. We
present our concluding remarks in Section 6.
2 System Architecture
The WMT11 has provided a collection of Haitian
Creole-English parallel data from a variety of
sources, including data from CMU1. A summary
of the data is given in Table 1. The primary in-
domain data comprises the translated (noisy) SMS
messages. The additional data contains newswire
text, medical dialogs, the Bible, several bilingual
dictionaries, and parallel sentences from Wikipedia.
Corpus Sentences Tokens (HT/EN)
SMS messages 16,676 351K / 324K
Newswire text 13,517 336K / 292K
Medical dialog 1,619 10K / 10K
Dictionaries 42,178 97K / 92K
Other 41,872 939K / 865K
Wikipedia 8,476 77K / 90K
Total 124,338 1.81M / 1.67M
Table 1: Haitian Creole (HT) and English (EN) parallel
data provide by WMT11
We preprocessed the data by separating the punc-
tuations, and converting both sides into lower case.
SMS data was further processed to normalize quo-
tations and other punctuation marks, and to remove
all markups.
To build a baseline translation system we fol-
lowed the recommended steps: generate word align-
1www.speech.cs.cmu.edu/haitian/
ments using GIZA++ (Och and Ney, 2003) and
phrase extraction using Moses (Koehn et al, 2007).
We built a 4-gram language model with the SRI
LM toolkit (Stolcke, 2002) using English side of
the training corpus. Model parameters for the lan-
guage model, phrase table, and lexicalized reorder-
ing model were optimized via minimum error-rate
(MER) training (Och, 2003).
The SMS test sets were provided in two formats:
raw (r) and cleaned (cl), where the latter had been
manually cleaned. We used the SMS dev clean to op-
timize the decoder parameters and the SMS devtest
clean and SMS devtest raw as held-out evaluation sets.
Each set contains 900 sentences. A separate SMS
test, with 1274 sentences, was used as the unseen
test set in the final evaluation. For each experiment
we report the case-insensitive BLEU (Papineni et
al., 2002) score.
Using the available training data we built several
baseline systems: The first system (Parallel-OOD),
uses all the out-of-domain parallel data except the
Wikipedia sentences. The second system, in addi-
tion, includes Wikipedia data. The third system uses
all available parallel training data (including both the
out-of-domain data as well as in-domain SMS data).
We used the third system as the baseline for later
experiments.
dev (cl) devtest (cl) devtest (r)
Parallel-OOD 23.84 22.28 17.32
+Wikipedia 23.89 22.42 17.37
+SMS 32.28 33.49 29.95
Table 2: Translation results in BLEU for different corpora
Translation results for different test sets using the
three systems are presented in Table 2. No signifi-
cant difference in BLEU was observed with the ad-
dition of Wikipedia data. However, a significant
improvement in performance can be seen when in-
domain SMS data is added, despite the fact that this
is noisy data. Because of this, we paid special atten-
tion to clean the noisy SMS data.
3 Preprocessing of SMS Data
In this section we explain two approaches that we
explored to reduce the noise in the SMS data.
387
3.1 Lexicon-based Collapsing of OOV Words
We observed that a number of words in the raw SMS
data consisted of asterisks or special character sym-
bols. This seems to occur because either users had
to type with a phone-based keyboard or simply due
to processing errors in the pipeline. Our aim, there-
fore, was to collapse these incorrectly spelled words
to their closest vocabulary entires from the rest of
the data.
We first built a lexicon of words using the entire
data provided for the Featured Task. We then built
a second probabilistic lexicon by cross-referencing
SMS dev raw with the cleaned-up SMS dev clean.
The first resource can be treated as a dictionary
while the second is a look-up table. We processed
incoming text by first selecting all the words with
special characters in the text, and then computing
an edit distance with each of the words in the first
lexicon. We return the most frequent word that is
the closest match as a substitute. For all words that
don?t have a closest match, we looked them up in the
probabilistic dictionary and return a potential substi-
tution if it exists. As the probabilistic dictionary is
constructed using a very small amount of data, the
two-level lookup helps to place less trust in it and
use it only as a back-off option for a missing match
in the larger lexicon.
This approach only collapses words with special
characters to their closest in-vocabulary words. It
does not make a significant difference to the OOV
ratios, but reduces the number of tokens in the
dataset. Using this approach we were able to col-
lapse about 80% of the words with special characters
to existing vocabulary entries.
3.2 Spelling Normalization
One of the most problematic issues in Haitian Cre-
ole SMS translation system is misspelled words.
When training data contains misspelled words, the
translation system performance will be affected at
several levels, such as word alignment, phrase/rule
extractions, and tuning parameters (Bertoldi et al,
2010). Therefore, it is desirable to perform spelling
correction on the data. Spelling correction based
on the noisy channel model has been explored in
(Kernighan et al, 1990; Brill and Moore, 2000;
Toutanova and Moore, 2002). The model is gener-
ally presented in the following form:
p(c?|h) = argmax
?c
p(h|c)p(c) (1)
where h is the Haitian Creole word, and c is a pos-
sible correction. p(c) is a source model which is a
prior of word probabilities. p(h|c) is an error model
or noisy channel model that accounts for spelling
transformations on letter sequences.
Unfortunately, in the case of Haitian Creole SMS
we do not have sufficient data to estimate p(h|c)
and p(c). However, we can assume p(c|h) ? p(c)
and c is in the French vocabulary and is not an En-
glish word. The rationale for this, from linguistic
point of view, is that Haitian Creole developed from
the 18th century French. As a result, an important
part of the Haitian Creole lexicon is directly derived
from French. Furthermore, SMS messages some-
times were mixed with English words. Therefore,
we ignore c if it appears in an English dictionary.
Given h, how do we get a list of possible normal-
ization c and estimate p(c)? We use edit distance
of 1 between h and c. An edit can be a deletion,
transposition, substitution, or insertion. If a word
has l characters, there will be 66l+31 possible cor-
rections2. It may result in a large list. However,
we only keep possible normalizations which appear
in a French dictionary and do not appear in an En-
glish dictionary3. To approximate p(c), we use the
French parallel Giga training data from the Shared
Task of the WMT11. p(c) is estimated by MLE. Fi-
nally, our system chooses the French word with the
highest probability.
dev (cl) devtest (cl) test (cl)
Before 2.6 ; 16 2.7 ; 16 2.6 ; 16
After 2.2 ; 13.63 2.3 ; 13.95 2.2 ; 14.3
Table 3: Percentage of OOV tokens and types in test sets
before and after performing spelling normalization.
Table 3 shows that spelling normalization helps
to bring down the percentage of OOV tokens and
types by 0.4% and 2% respectively on the three test
2l deletions, l-1 transpositions, 32l substitutions, and 32(l+1)
insertions; Haitian Creole orthography has 32 forms.
3The English dictionary was created from the English Gigaword
corpus.
388
sets. Some examples of Haitian Creole words and
their French normalization are (tropikal:tropical),
(economiques:economique), (irjan:iran), (idanti-
fie:identifie).
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
S1 32.18 30.22 25.45
S2 28.9 31.06 27.69
Table 4: Translation results in BLEU with/without
spelling correction
Given the encouraging OOV reductions, we ap-
plied the spelling normalization for the full corpus,
and built new translation systems. Our baseline sys-
tem has no spelling correction (for the training cor-
pus or the test sets); in S1, the spelling corrections
is applied to all words; in S2, the spelling correc-
tion is only applied to Haitian Creole words that oc-
cur only once or twice in the data. In S1, 11.5% of
Haitian Creole words had been mapped to French,
including high frequency words. Meanwhile, 4.5%
Haitian Creole words on training data were mapped
to French words in S2. Table 4 presents a compar-
ison of translation performance of the baseline, S1
and S2 for the SMS test sets. Unfortunately, none of
systems with spelling normalization outperformed
the system trained on the original data. Restricting
the spelling correction only to infrequent words (S2)
performed better for the devtest sets, but not for the
dev set, although all the test sets come from the same
domain.
4 Corpus Expansion using Semantic Role
Labeling
To address the problem of limited resources, we
tried to expand the training corpus by applying the
corpus expansion method described in (Gao and Vo-
gel, 2011). First, we parsed and labeled the semantic
roles of the English side of the corpus, using the AS-
SERT labeler (Pradhan et al, 2004). Next, using the
word alignment models of the parallel corpus, we
extracted Semantic Role Label (SRL) substitution
rules. SRL rules consist of source and target phrases
that cover whole constituents of semantic roles, the
verb frames they belong to, and the role labels of
the constituents. The source and target phrases must
comply with the restrictions detailed in (Gao and Vo-
gel, 2011). Third, for each sentence, we replaced
one of embedded SRL substitution rules with equiv-
alent rules that have the same verb frame and the
same role label.
The original method includes an additional but
crucial step of filtering out the grammatically incor-
rect sentences using an SVM classifier, trained with
labeled samples. However, we were unable to find
Haitian Creole speakers who could manually label
training data for the filtering step. Therefore, we
were forced to skip this filtering step. We expanded
the full training corpus which contained 124K sen-
tence pairs, resulting in an expanded corpus with
505K sentences. The expanded corpus was force-
aligned using the word alignment models trained
on the original unexpanded corpus. A new trans-
lation system was built using the original plus the
expanded corpus. As seen in Table 5, we observed
a small improvement with the expanded corpus for
the raw devtest. This method did not improve per-
formance for the other two test sets.
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
+Expanded 31.79 32.98 30.1
Table 5: Translation results in BLEU with/without corpus
expansion
A possible explanation for this, in addition to
the missing component of filtering, is the low qual-
ity of SRL parsing on the SMS corpus. We ob-
served a very small ratio of expansions in the
Haitian Creole-English data, when compared to the
Chinese-English experiment shown in (Gao and Vo-
gel, 2011). The latter used a high quality corpus for
the expansion and the expanded corpus was 20 times
larger than the original one. Due to the noisy nature
of the available parallel data, only 61K of the 124K
sentences were successfully parsed and SRL-labeled
by the labeler.
389
5 Extracting Parallel Data from
Comparable Data
As we only have a limited amount of parallel data,
we focused on automatically extracting additional
parallel data from other available resources, such as
comparable corpora. We were not able to find com-
parable news articles in Haitian Creole and English.
However, we found several hundred Haitian Creole
medical articles on the Web which were linked to
comparable English articles4. Although some of the
medical articles seemed to be direct translations of
each other, converting the original pdf formats into
text did not produce sentence aligned parallel arti-
cles. Rather, it produced sentence fragments (some-
times in different orders) due to the structural dif-
ferences in the article pair. Hence a parallel sen-
tence detection technique was necessary to process
the data. Because the SMS messages are related to
the disaster relief effort, which may include many
words in the medical domain, we believe the newly
extracted data may help improve translation perfor-
mance.
Following Munteanu and Marcu (2005), we used
a Maximum Entropy classifier to identify compara-
ble sentence. To avoid the problem of having dif-
ferent sentence orderings in the article pair, we take
every source-target sentence pair in the two articles,
and apply the classifier to detect if they are paral-
lel. The classifier approach is appealing to a low-
resource language such as Haitian Creole, because
the features for the classifier can be generated with
minimal translation resources (i.e. a translation lex-
icon).
5.1 Maximum Entropy Classifier
The classifier probability can be defined as:
Pr(ci|S, T ) =
exp
(?n
j=1 ?jfij(ci, S, T )
)
Z(S, T )
(2)
where (S, T ) is a sentence pair, ci is the class, fij
are feature functions and Z(S) is a normalizing fac-
tor. The parameters ?i are the weights for the feature
functions and are estimated by optimizing on a train-
ing data set. For the task of classifying a sentence
pair, there are two classes, c0 = non ? parallel
4Two main sources were: www.rhin.org and www.nlm.nih.gov
and c1 = parallel . A value closer to one for
Pr(c1|S, T ) indicates that (S, T ) are parallel.
The features are defined primarily based on trans-
lation lexicon probabilities. Rather than computing
word alignment between the two sentences, we use
lexical probabilities to determine alignment points
as follows: a source word s is aligned to a tar-
get word t if p(s|t) > 0.5. Target word align-
ment is computed similarly. We defined a feature set
which includes: length ratio and length difference
between source and target sentences, lexical proba-
bility scores similar to IBM model 1 (Brown et al,
1993), number of aligned/unaligned words and the
length of the longest aligned word sequence. Lexi-
cal probability score, and alignment features gener-
ate two sets of features based on translation lexica
obtained by training in both directions. Features are
normalized with respect to the sentence length.
5.2 Training and Testing the Classifier
To train the model we need training examples that
belong to each of the two classes: parallel and non-
parallel. Initially we used a subset of the available
parallel data as training examples for the classifier.
This data was primarily sourced from medical con-
versations and newswire text, whereas the compa-
rable data was found in medical articles. This mis-
match in domain resulted in poor classification per-
formance. Therefore we manually aligned a set of
250 Haitian Creole-English sentence pairs from the
medical articles and divided them in to a training set
(175 sentences) and a test set (100 sentences).
The parallel sentence pairs were directly used as
positive examples. In selecting negative examples,
we followed the same approach as in (Munteanu
and Marcu, 2005): pairing all source phrases with
all target phrases, but filter out the parallel pairs and
those that have high length difference or a low lex-
ical overlap, and then randomly select a subset of
phrase pairs as the negative training set. The test
set was generated in a similar manner. The model
parameters were estimated using the GIS algorithm.
We used the trained ME model to classify the sen-
tences in the test set into the two classes, and notice
how many instances are classified correctly.
Classification results are as given in Table 6. We
notice that even with a smaller training set, the clas-
sifier produces results with high precision. Using
390
Precision Recall F-1 Score
Training Set 93.90 77.00 84.61
Test Set 85.53 74.29 79.52
Table 6: Performance of the Classifier
the trained classifier, we processed 220 article pairs
which contained a total of 20K source sentences
and 18K target sentences. The classifier selected
about 10K sentences as parallel. From these, we se-
lected sentences where pr(c1|S, T ) > 0.7 for trans-
lation experiments. The extracted data expanded the
source vocabulary by about 5%.
We built a second translation system by combin-
ing the baseline parallel corpus and the extracted
corpus. Table 7 shows the translation results for this
system.
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
+Extracted 32.29 33.29 29.89
Table 7: Translation results in BLEU with/without ex-
tracted data
The results indicate that there is no significant per-
formance difference in using the extracted data. This
may be due to the relatively small size of the com-
parable corpus we used when extract the data.
6 Conclusion
Building an MT system to translate Haitian Creole
SMS messages involved several challenges. There
was only a limited amount of parallel data to train
the models. The SMS messages tend to be quite
noisy. After building a baseline MT system, we
investigated several approaches to improve its per-
formance. In particular, we tried collapsing OOV
words using a lexicon generated with clean data, and
normalize different variations in spelling. However,
these methods did not results in improved translation
performance.
We tried to address the data sparseness problem
with two approaches: expanding the corpus using
SRL rules, and extracting parallel sentences from
a collection of comparable documents. Corpus ex-
pansion showed a small improvement for the raw
devtest. Both corpus expansion and parallel data
extraction did not have a positive impact on other
test sets. Both these methods have shown significant
performance improvement in the past in large data
scenarios (for Chinese-English and Arabic-English),
but failed to show improvements in the current low-
data scenario. Thus, we need further investigations
in handling noisy data, especially in low-resource
scenarios.
Acknowledgment
We thank Julianne Mentzer for assisting with editing
and proofreading the final version of the paper. We
also thank the anonymous reviewers for their valu-
able comments.
References
Jeff Allen. 1998. Lexical variation in haitian cre-
ole and orthographic issues for machine translation
(MT) and optical character recognition (OCR) appli-
cations. In Proceedings of the First Workshop on Em-
bedded Machine Translation systems of AMTA confer-
ence, Philadelphia, Pennsylvania, USA, October.
Nicola Bertoldi, Mauro Cettolo, and Marcello Federico.
2010. Statistical machine translation of texts with mis-
spelled words. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Los Angeles, California, June.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics (ACL 2000), pages
286?293.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Qin Gao and Stephan Vogel. 2011. Corpus expansion
for statistical machine translation with semantic role
label substitution rules. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Portland,
Oregon, USA, June.
Mark D. Kernighan, Kenneth W. Church, and William A.
Gale. 1990. A spelling correction program based on
a noisy channel model. In Proceedings of the 13th
conference on Computational linguistics - Volume 2,
COLING ?90, pages 205?210.
391
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic,
June.
William Lewis. 2010. Haitian Creole: How to build and
ship an mt engine from scratch in 4 days, 17 hours, &
30 minutes. In Proceedings of the 14th Annual confer-
ence of the European Association for Machine Trans-
lation (EAMT), Saint-Raphae?l, France, May.
Robert Munro. 2010. Crowdsourced translation for
emergency response in haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collab-
orative Crowdsourcing for Translation, Denver, Col-
orado, USA, October-November.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low semantic parsing using support vector machines.
In Proceedings of the Human Language Technology
Conference/North American chapter of the Associa-
tion for Computational Linguistics annual meeting
(HLT/NAACL-2004).
Andreas Stolcke. 2002. An extensible language model-
ing toolkit. In Proc. of International Conference on
Spoken Language Processing, volume 2, pages 901?
904, Denver, CO, September.
Kristina Toutanova and Robert Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2002).
392
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 501?511,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Crisis MT: Developing A Cookbook for MT in Crisis Situations
William D. Lewis
Microsoft Research
Redmond, WA 98052
wilewis@microsoft.com
Robert Munro
Stanford University
Stanford, CA 94305
rmunro@stanford.edu
Stephan Vogel
Carnegie Mellon University
Pittsburgh, PA 15213
stephan.vogel@cmu.edu
Abstract
In this paper, we propose that MT is an im-
portant technology in crisis events, something
that can and should be an integral part of a
rapid-response infrastructure. By integrating
MT services directly into a messaging infras-
tructure (whatever the type of messages being
serviced, e.g., text messages, Twitter feeds,
blog postings, etc.), MT can be used to pro-
vide first pass translations into a majority lan-
guage, which can be more effectively triaged
and then routed to the appropriate aid agen-
cies. If done right, MT can dramatically in-
crease the speed by which relief can be pro-
vided. To ensure that MT is a standard tool
in the arsenal of tools needed in crisis events,
we propose a preliminary Crisis Cookbook,
the contents of which could be translated into
the relevant language(s) by volunteers imme-
diately after a crisis event occurs. The result-
ing data could then be made available to relief
groups on the ground, as well as to providers
of MT services. We also note that there
are significant contributions that our commu-
nity can make to relief efforts through con-
tinued work on our research, especially that
research which makes MT more viable for
under-resourced languages.
1 Introduction
The connected world contains approximately 5000
languages ? at least that is how many languages you
could find at the other end of your phone right now.
However, the majority of these languages are under-
resourced, and they have few or no digital resources.
In the event of a sudden onset crisis, people will
immediately begin using their communication tech-
nologies ? and their languages ? to report their situ-
ations, request help, and seek out loved ones. Yet,
in the event that such a crisis occurs in a region
of the world where an under-resourced language is
spoken, delivery of support or aid could be affected
due to the inability to communicate. This was felt
most strongly in the wake of the January 12, 2010
earthquake in Haiti. Local emergency response ser-
vices were inoperable, but 70-80% of cell-towers
were quickly restored. With 83% of men and 67% of
women possessing cellphones, the nation remained
largely connected. People within Haiti were texting,
calling, and interacting with social media, primarily
in Haitian Kreyo`l (Munro, 2011). Yet, most of the
aid that was being delivered to the country ? initially,
soley by the American Military ? was being deliv-
ered by groups that did not communicate in Kreyo`l.
It was the first time that the world has seen a large-
scale sudden onset crisis in a region with productive
digital communications in an under-resourced lan-
guage, but it certainly will not be the last.
We strongly believe that MT is an important tech-
nology to facilitate communication in crisis situa-
tions, crucially since it can make content in a lan-
guage spoken or written by a local population ac-
cessible to those that do not know the language, in
particular aid agencies. Multiple groups saw MT
as a grand challenge in the Haitian crisis, and they
set to work to make MT available as soon as pos-
sible after the crisis. Within two weeks of the cri-
sis, the first two MT engines were built and were
available to those who needed them. We believe that
we can make MT available just as quickly in future
crises, and, with the right preparation, tightly inte-
grate MT into the communication infrastructure that
is deployed (e.g., the text messaging infrastructure).
The challenge is doing the work now to make this
vision possible.
In this paper, we describe the technologies that
came to play in the Haitian crisis, how Haitian
Kreyo`l MT was developed, the problems of surprise
languages and low resource MT, and detail the re-
search and technologies, cast as a ?Crisis MT Cook-
book?, that will be essential for MT to form a core
role in future crises. In Sections 2, 3, and 4 we dis-
cuss Mission 4636 and the technologies that came
501
into play in Haiti and other recent crises, and the
role that technologies can and should play in future
crises. In Section 5, we discuss what made Haitian
Kreyo`l a special case of a ?surprise language?, and
how MT was developed for the language. In Sec-
tion 6, we review the NLP and MT research areas
that will likely net big returns for under-resourced
languages. In Section 7, we review the need for an
MT Crisis Cookbook, and what the data and infras-
tructural components of the Cookbook should be.
Finally, in Section 8 we review a sample crisis time-
line, and how a crisis might play out with all the
components of the Cookbook available. Section 9
wraps up the paper.
2 Mission 4636
In Haiti, crowdsourced translation enabled com-
munications between the Kreyo`l-speaking Haitian
population and English-speaking emergency respon-
ders. A small group of international aid workers
established a phone-number,?4636?1 , that people
were able to send text messages to for free within
Haiti. The actual translations were made by about
2000 Kreyo`l2 and French speaking volunteers col-
laborating on an online microtasking platform that
they used to translate, categorize, identify missing
people and geolocate information on a map (Munro,
2010).3 After a month, this work was gradually
transferred to paid workers in Mirebalais, Haiti.
These messages, about 80,000 in total, were used
as part of the shared task for the 2011 Workshop
on Machine Translation. About 3,000 of the mes-
sages had the categories and coordinates refined by a
third workforce working with the Ushahidi platform
out of Boston.4 They published this information on
an online crisis map and worked directly with the
main emergency responder, the American Military,
to identify actionable information.
1See the ?Mission 4636? website at
http://www.mission4636.org for more information about
the organization and its efforts in Haiti.
2We use the term Kreyo`l for the Creole spoken in Haiti to
differentiate it from other Creoles. This is also in concord with
customary usage in Haiti.
3An author of this paper, Robert Munro, coordinated this
process and is a founding member and translation coordinator
for the Standby Task Force, which is discussed later in the paper.
4For more information on Ushahidi, see
http://www.ushahidi.org.
The strategy for translation was extremely effec-
tive - 80,000 messages equates to about 10 novels of
information, translated in real-time, lifting a burden
off people in Haiti. One high-ranking official de-
scribed the translation process as a ?perfect match?
of social media and traditional emergency response
(Anderson, 2010).
To meet the scale of translation needs, machine
translation services were quickly shipped. A mem-
ber of Mission 4636 built a high-precision, low-
coverage dictionary-based system that was used by
a number of translators. A couple of days later,
the world?s first publically accessible Stastical Ma-
chine Translation (SMT) engine for Kreyo`l was de-
veloped by Microsoft Research, with Google Re-
search following several days later with their own
engine.5 Although the statistical translation engines
were not used directly in the SMS translation ef-
fort, there is evidence they were used by those who
were involved in the relief effort, as determined by
blog postings and a review of translation logs show-
ing relief-centric translations. Although Kreyo`l is
not a high traffic language?it was not expected that
it would be?about 5% of the traffic in the weeks
and the months following the earthquake appeared
to be relief-related, suggesting that machine transla-
tion was being used those who needed it most.6 Had
MT been integrated directly into the text messaging
infrastructure used in Haiti, this percentage would
have been significantly larger.
3 Translation and crisis response - a
quickly changing field
To establish a ready-workforce to aid information
processing in relief efforts an organization called the
5A rough timeline of these developments can
be seen in the commentary posted to the Lan-
guage Log website (see specifically the archive at:
http://languagelog.ldc.upenn.edu/nll/?p=2068).
6The logs output by Microsoft Translator?s engine were ex-
amined, and categorized roughly into broad categories describ-
ing the type of content. These categories were: Relief Related
(suspected), Colloquial or Common Expressions (which could,
in fact, have been relief related), Chat, and Unknown. The anal-
ysis was done by hand on a random sample of 200 messages
from the many thousands of messages received within a couple
of months of the quake. There were a large number of strings
that were difficult to categorize, including many partial strings,
and a bias against Relief Related when it was not clear. Thus,
the 5% estimate is likely a conservative one.
502
Standby Task Force was established in late 2010. Its
founding members had worked together in the Haiti
and/or subsequent Pakistan response efforts. It cur-
rently has several hundred members who special-
ize in tasks like report mapping, verification, media
monitoring and translation. Of all the different tasks
that volunteers can perform, translation is the least
transferable from one crisis to the next.
Following from the lessons learned in Haiti,
crowdsourced and machine translation have been
combined for a number of aid efforts: vote monitor-
ing for the referendem in Southern Sudan (Arabic);
a UN-led earthquake simulation in Colombia (Span-
ish); and for crisis mapping following the tsunami in
Japan (Japanese).
When information is immediately translated into
a high resource language it can be quickly triaged by
a greater number of people. The more time-intensive
task of manually correcting any mistranslations can
be performed in parallel. This workflow of combin-
ing machine and crowdsourced translation is largely
a succesful one and is likely to become common
practice in humanitarian information processing.
The combination of manual and machine-
translation was found to be effective across unpre-
dictable input:
?An email came into the Sudan Vote Mon-
itor platform in Indonesian - your plugin
did a good job of translating it into English
and Arabic?
Helena Puig Larrauri, volunteer for Sudan
Vote Monitor (P.C.)
But not without errors, especially across vital
phrases like location names:
?Names of neighborhoods such as Salitre
or Puerta al Llano were not recognized as
such and unnecessarily being translated.?
Marta Poblet, volunteer for Colombia
earthquake simulation (P.C.)
When the uprisings hit Libya in early 2011 the
United Nations did not have the capacity to col-
lect vital ground-truth data in the lead up to their
involvement. Information about refugee numbers
and needs were on web-accessible articles and so-
cial media, as were reports about the movements of
government and rebel troops and vunerable popula-
tions within the country. But there simply wasn?t the
workforce within the UN to aggregate and verify so
much information. This was the first time the United
Nations directly engaged a volunteer workforce for
large-scale information processing, requesting the
Standby Tasks Force?s deployment. It was also
the first time that so much information had come
from social media, a potentially large but unstruc-
tured data source, but it gave the UN a huge head-
start in their efforts (Verity, 2011). Crowdsourced
and machine translation were also combined here,
but in this case by directly engaging Arabic speak-
ers in media monitoring and by using reports from
Meedan.7
In a crisis, it will now be more common than not
that the volume of available digital information will
surpass the volume of information that aid-workers
can collect directly from the ground. This rapid
change is being quickly met by a rapid change in
cloud-based and automated solutions to language
processing, especially machine translation.
4 Translation and low-resource languages
We were fortunate that Arabic, Spanish and
Japanese are high resource languages for which
online machine translation services already exist.
Speakers of low resource languages cannot currently
benefit from this kind of translation service and yet
low resource languages are disproportionally spoken
by the world?s most vunerable populations. Over the
last 12 months many problems have been solved re-
garding the workflow of managing crisis data, but
one of the biggest remaining problems is the abil-
ity to quickly deploy machine-translation systems to
augment relief efforts.
While translation is not widely discussed aspect
of crisis response, it is ?a perennial hidden issue?
(Disaster 2.0, 2011):
?Go and look at any evaluation from the
last ten or fifteen years. ?Recommenda-
tion: make effective information available
7Meedan is an NGO that seeks to create greater understand-
ing between the Arabic and English speaking world by translat-
ing media reports and blogs between the languages, combining
quick machine-translation with corrections by a volunteer com-
munity.
503
to the government and the population in
their own language.? We didn?t do it . . . It
is a consistent thing across emergencies.?
Brendan McDonald, UN OCHA in (Dis-
aster 2.0, 2011)
Beyond the particular use case of small-to-
medium scale emergency information processing,
machine translation can also contribute to aid ef-
forts when the scale of information is beyond any
manual processing. In addition to the Libya deploy-
ment, a recent Red Cross survey (2010) found that
nearly half the respondents would use social media
to report emergencies. It simply would not be possi-
ble to translate all real-time reports when expressed
through social media, but translation into a high re-
source language could aid semi-automated methods
for discovering and prioritizing information.
There is, therefore, a great need to explore meth-
ods for rapid deployment of machine-translation
systems into minority languages. The questions that
we seek to address in this paper is how we as a com-
munity can prepare for the eventuality of the next
crisis, can draw from the lessons we learned in the
Haitian crisis, and might significantly impact the aid
effort in the next and future crises.
5 Surprise Languages: What Made Haiti
Different?
On January 19th, 2010, the Microsoft Research
Translator team received an e-mail from the field re-
questing that they develop an MT engine for Haitian
Kreyo`l to assist in the relief effort. At the time, no
publically available MT engine existed for Kreyo`l.
In less than five days, the Microsoft Translator site
was supporting the language. Given that it can take
weeks to months to develop an MT engine for a new
language, it would not seem possible that an engine
could be developed so quickly, especially for a low-
resource, minority language. The reasons this was
possible are varied, and are in some ways unique to
Kreyo`l.
Haitian Kreyo`l, as it turns out, has proven to be an
exceptional case for a surprise language. Unlike the
languages in Surprise Language Exercises of nearly
a decade ago (Oard, 2003; Oard and Och, 2003),
in which participants were given a month to collect
data and build language technologies for previously
unknown languages, including Machine Translation
systems, there was a surprising amount of data for
Kreyo`l at the start of the Haitian crisis, and it be-
came available relatively quickly. Partly, this is due
to the growth of the Web, which has proven to be
a surpisingly diverse multi-lingual resource. But it
also stems crucially from work that had been done
in the past on Kreyo`l, specifically, the work that was
done in the DIPLOMAT and NESPOLE! projects at
CMU (Frederking et al, 1997). It was possible to
assemble a reasonable sample of data for the lan-
guage in very short order (i.e., days). Further, since
the language itself is fairly reduced morphologically,
it is an easier target for SMT. In contrast, if one
were to sample a language at random from the set
of the 7,000 languages spoken on the earth, one is
more likely to find a language that is morpholog-
ically richer (e.g., fusional, aggutinating, polysyn-
thetic). Morphological richness compounds the data
sparsity problem, reducing the quality of the result-
ing SMT engines.
In other words, a combination of a simple
morphology combined with reasonably accessible
sources of data made the rapid deployment of MT
for Kreyo`l far more likely. That is not to say
that there weren?t problems. First, Kreyo`l is fairly
?young? as a written language8 , and is still in the
early stages of orthographic standardization and nor-
malization (Allen, 1998). This has led to inconsis-
tencies in the orthography that increases data sparse-
ness and noise. Further, Kreyo`l has multiple regis-
ters in its written form: a ?high? register that uses
full forms for pronouns and a set of function words,
and a ?low? register that corresponds more closely
to its spoken form, and is written with many con-
tractions. For example, the Kreyo`l word for the first
person pronoun is mwen. It can be written as mwen
(the high register), or contracted to m? (the low regis-
ter). The form can either be attached to the succeed-
ing word or written with a following space. Like-
wise, the first person possessive is also mwen which
is written following the word that is possessed. This
8Although Haitian Kreyo`l in written form goes back as far as
the late 18th century (see Lefebvre (1998) for material on some
of these texts), Kreyo`l as a written language did not become
more commonplace until the 20th century, not achieving official
status in Haiti until 1961.
504
can be written as ?m, and can be attached to the word
or delimited by a space. Both m? and ?m appear in
some texts as just m. The same patterns hold for all
pronouns, and some function words as well. See Ta-
ble 1 for a list of these reductions.
Table 1: Sample Pronouns and Reductions
Pronoun Gloss Appears as
mwen I, me, mine m, ?m, m?
nou you (pl), us n, ?n, n?
ou you w, w?
li he, she, it l, l?, ?l
Additionally, writers of Kreyo`l use a large num-
ber of abbreviated forms for common expressions, a
kind of shorthand. For example, ave`n can be used to
represent ave`k nou, mandem can be used for mande
mwen, etc. Overall, the number of alternations and
multi-way ambiguities also increases the level of
noise and data sparsity. 9
So, even with a morphologically reduced lan-
guage like Kreyo`l, one has issues with data sparsity
beyond the mere lack of availability of data. This
compounds the low-data aspect of the language.
Adding in a multitude of morphological variants, as
one might encounter in a Turkic language, or worse,
in an Inuit language, would only make the problem
more severe. The big challenge for Crisis MT is not
only to deal with the data availability problem, but
once one has the data in hand, to deal with the re-
duction in the utility of that data caused by noise
and the multiplication of word forms. These pose
major challenges to our community, which can be
countered through additional research, a motivated
and active community, and scores of rapidly applied
heuristics and data repairs.
6 Research Areas to Counter Data
Sparsity
As noted, the major problems with low-resource MT
is the lack of data and various data issues that in-
crease the sparsity of data already in short supply.
What are the research challenges? How can we
make MT viable quickly for low-resource and si-
multaneously morphologically rich languages?
9For more details of the Haitian Kreyo`l translation systems
developed at Microsoft Research, please see Lewis (2010).
The following constitutes a rough list of solu-
tions, many of which map to very interesting re-
search problems:
? Crowdsourcing ? Beyond the use of crowd-
sourcing in the crisis context itself (e.g., to
translate or process text messages, much as
what was done by Mission 4636), novel tech-
niques for tapping the crowd could also be used
to add or repair data:
? Repairing and evaluation ? In this sce-
nario, the crowd would be used to repair
data that is obviously noisy, evaluate prob-
lems with particular data points, or even
make simple determinations as to whether
the data in question is actually in the lan-
guage(s) of interest or too noisy to use.
? Translating content, generating new data ?
Given crowd sourced, micro-tasking plat-
forms such as Amazon?s Mechanical Turk
and Crowdflower, one can now easily tap
the crowd to generate new data. The ma-
jor challenge will be identifying if speak-
ers of the target language(s) are available
on the desired platform, and if not, if they
could be motivated to particpate.10 Like-
wise, infrastructure and resources will be
needed to evaluate the quality of the re-
sulting translations (Zaidan and Callison-
Burch, 2011).
? Active Crowd Translation ? This method
combines active learning with crowd-
sourcing for annotation of parallel data in
comparable resources, and can be used
to increase the amount of data that is
found (Ambati et al, 2011). Active learn-
ing might be applicable to other crowd-
sourcing tasks as well, such as being used
in crowdsourcing for translating content
or repairing translated content.
? Tapping non-traditional sources ? Critical to
traditional approaches of SMT is parallel train-
ing data. Parallel data is difficult to impossible
to come by for a large number of the world?s
10Based on the results of an informal survey, there may be
speakers of a hundred or more languages on Mechanical Turk.
See http://www.junglelightspeed.com/amt language/ for a list
of the languages that may be available on Turk.
505
languages. Tapping non-traditional sources of
data can help increase the supply of ever valu-
able training data for a language:
? Mining comparable sources of data ? min-
ing comparable data for parallel data has
a long history, including mining compara-
ble sources for named entities (Udupa et
al., 2009; Irvine et al, 2010; Hewavitha-
rana and Vogel, 2008; Hewavitharana and
Vogel, 2011), mining Wikipedia for paral-
lel content, including sentences (Smith et
al., 2010), and many more too numerous
to list. There is always room for improve-
ment and hybridization in this space, as
well as tapping additional sources of data,
such as the volumes of noisy comparable
data on the Web.
? Monolingual ? More recent work has fo-
cused on mining monolingual sources of
data, treating MT as a decipherment prob-
lem (Ravi and Knight, 2011), rather than
a source-target mapping problem.
? Dictionary bootstraps and backoffs ? De-
spite the absence of context, dictionar-
ies can be useful, especially for resolving
out-of-vocabulary items (OOVs). Many
bilingual dictionaries also contain exam-
ple sentences, which can be harvested and
used in training.
? Field data from linguists ? Given that lin-
guists have variously studied a large per-
centage of the world?s languages, tapping
the supply of data that they have accu-
mulated could prove quite fruitful. Some
recent work tapping annotated bitexts (at
this time, for over 1,200 languages) pro-
duced by linguists may prove useful in
the future (Lewis and Xia, 2010), if for
nothing more than to provide information
about linguistic structure (e.g., morpho-
logical complexity or divergences, poten-
tial distortion rates, and structural diver-
gence (a la Fox (2002))). Engaging with
the documentary linguistic community
and providing tools to facilitate the col-
lection of data might produce additional
data, especially data where alignment is
assisted through human input (Monson et
al., 2008).
? Novel ways of countering data sparsity
? Systematizing data cleaning heuristics ?
Undoubtedly, the same kinds of filtra-
tion and data cleaning heuristics used for
Kreyo`l could prove useful for speeding
up the processing of data for new lan-
guages. Applying Machine Learning tech-
niques to data filtration and data cleaning
could aid and generalize the process, thus
decreasing overall latency from acquisi-
tion to training.
? Strategies to make the source look more
like the target (or vice versa) ? A corol-
lary to data sparsity is faulty word align-
ment, where low frequency words fail to
get good alignments because there is not
enough data to reinforce fairly weak hy-
potheses, or where source-target distor-
tion is high. Both problems disfavor what
alignments do exist. If the source and tar-
get are reordered so that one side more
closely matches the other, or one side is
?enriched? to be more like the other, one
can reduce distortion related effects, and
might also counter the large number of
forms in morphologically rich languages
(e.g., (Yeniterzi and Oflazer, 2010; Gen-
zel, 2010), and many others).
? Strategies to systematically deal with complex
morphology ? this is one on-going area of re-
search that could still net large returns, since,
even with some relatively high-data languages,
such as Finnish, data is made sparser due to the
multiplication of possible forms. There is too
long a literature to really do justice here, but
some recent work includes discrimitative lexi-
cons (Jeong et al, 2010), sub-word alignment
strategies (Bodrumlu et al, 2009), learning the
morphological variants in a language (Oflazer
and El-kahlout, 2007), using off-the-shelf mor-
phological tools, e.g., Morfessor 11, etc.
? Use syntax or linguistic knowledge in the
translation task ? By reducing the hypothe-
sis space for possible alignments, syntax-based
11http://www.cis.hut.fi/projects/morpho/
506
approaches can do better in lower-data situa-
tions and can handle source-target discontinu-
ities better than straight phrase-based systems
(e.g., (Quirk and Menezes, 2006; Li et al,
2010)).
7 The MT Crisis Cookbook
Given the relatively narrow domain context of Cri-
sis MT?generally the needed vocabulary and data
should be centered on relief work, medical in-
teractions, and communicating with the affected
populations?it may be possible to approach Crisis
MT as we would MT for any domain (e.g., news,
government, etc.). With enough data relevant to a
particular domain or sub-domain (e.g., earthquake,
tsunami, nuclear disaster, flooding, etc.), it would
be possible to build the relevant translation memo-
ries (TMs) and train highly domain-specific MT en-
gines to produce translations of reasonable quality
and utility. Even with highly inflected languages, a
domain-specific approach may get around many of
the data sparsity issues.
It is also crucial that no data be thrown out. Re-
lief specific content that was relevant to an earlier
crisis can certainly contribute to subsequent crises.
Among these data are difficult to replicate sources
of data, such as SMS messages. This data would
constitute a highly domain specific set of data which
would only grow over time.
7.1 Outline of the Cookbook
The recipe for the MT Crisis Cookbook consists of
two parts:
1. The content that would be most useful in crisis
situations. This consists of relief-centric vocab-
ulary, phrases, sentences, and other material. It
should be in some common ?source? language,
likely English (English is a reasonable ?pivot?
in and out of many other languages, given the
ubiquity of English-to-X content).
2. The infrastructure to support relief workers,
aid agencies, and the affected population. As
made obvious in Haiti, an SMS messaging
infrastructure integrated into a crowd-sourced
translation infrastructure, proved to be crucial.
For future crises, this infrastructure should be
streamlined and have public MT APIs inte-
grated directly into it (to support first pass MT).
7.2 Cookbook Data
As noted in Section 5, one way to counter the data
sparsity problem is to build domain specific engines,
with a set of data ready-to-go in the event of a crisis.
This data, which would exist in English and possibly
other languages, would be translated into the target
language (if needed), distributed to to aid organiza-
tions (as needed), and used to train MT engines and
other language processing resources. The following
list constitutes a set of possible sources. It is by no
means complete (for instance, some resources spe-
cific to particular crisis types, e.g., floods, nuclear
disasters, etc. are not included), but it does repre-
sent a good central core of resources that should be
part of any Crisis Cookbook12 :
? Where There is No Doctor ? This is one of the
most recognized and widely used and useful
references in under-resourced regions around
the world. The publisher of the text, the Hespe-
rian Foundation, has already had the text trans-
lated into 75 languages, and it is available in
PDF as a free download from their website.13
? CMU Medical Domain Phrases, Sentences,
and Glossary ? Collected under the jointly
NSF/EU funded NESPOLE! and DIPLOMAT
projects (Frederking et al, 1997), this data con-
sists of common phrases and sentences that
would be useful in a crisis medical scenario,
and would be quite useful for training MT, as
it was for training the Kreyo`l engines. Only the
English side of this data would be relevant to
future crises.
? Anonymized Crisis-related SMS Messages ?
Relief-related SMS messages may be particu-
larly useful in future crises, since those col-
lected in a crisis scenario are likely to contain
content that transfers readily to similar crises.
A selected sample of the 80,000+ messages re-
sulting from the Haitian crisis could constitute
12Some of the resources listed here are under copyright.
There may need to be some negotiation with the copyright own-
ers to ensure that the texts can be used, and how they can be
used (e.g., to train MT, to be used in TMs, to be distributed in
hardcopy form, etc.).
13http://hesperian.org/
507
a reasonable core of SMS messages that could
be added to over time.
? Red Cross Emergency Multilingual Phrase-
book ? A small, but highly focused, set of
phrases and questions useful in an emergency
medical context. Available in multiple lan-
guages.
? Emergency and Crisis Communication Vocab-
ulary ? An example bilingual set was prepared
by the Canadian Government in both French
and English14 , consisting of a small list of
?official? terms needed in crisis situations, and
their associated descriptions. Although the
terms on the Canadian site are translated and
defined only in English and French and have
a bias to the Canadian government nomecla-
ture, having such a list of terms from multi-
ple government agencies and their definitions
could prove useful for relief vocabulary as well
as for vocabulary needed for official announce-
ments.
? High Frequency Wikipedia Disaster Content
? This would consist of vocabulary that re-
curs across multiple related crisis pages on
wikipedia. The idea is to harvest those terms
that repeat across multiple pages of the same
?sub-domain? (e.g., those that cover events
with floods, earthquakes, nuclear disasters,
etc.), but document disasters in different lo-
cales, where cross-page repeated vocabulary is
favored (substracting out high-frequency vo-
cabulary that occurs elsewhere). This vocab-
ulary could be distilled automatically from a
set of relevant pages, and would likely contain
core vocabulary for specific crisis and disas-
ter contexts. For instance, shared vocabulary
between the Japanese, Indonesian, Pakistani,
and Haitian Earthquake pages might contain a
reasonable set of vocabulary relevant to earth-
quake crises as a whole.
7.3 Cookbook Infrastructure
The Cookbook infrastructure draws directly on what
was found to be useful in the Haitian Crisis. Here are
the infrastructural components we see as crucial:
14http://www.btb.gc.ca/publications/documents/crise-
crisis.pdf
? A crowd sourced microtasking infrastructure
to translate and route messages from the field.
This proved to be essential in Haiti. Hav-
ing such an infrastructure ready-to-go for fu-
ture crises would shave days off implementa-
tion and likely have profound effects on the ra-
pidity of the response.
? Integration of the APIs for the publically avail-
able MT services, such as Microsoft Transla-
tor and Google Translate, into the microtask-
ing and messaging infrastructure, enabling pro-
cessing of SMS messages, Twitter feeds, etc.
In this way, when any of these services deploy
MT for a given crisis language, the switch can
be flipped and first-pass can be MT activated at
a moment?s notice.
? A ready-to-go smart phone app that acts as a
crisis Translation Memory, which can be pop-
ulated with Cookbook content as it becomes
available. In this manner, rather than relying
on the distribution of paper copies of Cook-
book materials, relief workers on the ground
could just sync-up their mobile devices to get
the latest content. This is particularly impor-
tant in crisis locales where ?data plan? access
is limited, and phones will thus not necessarily
have online access to cloud based resources on
a regular basis.
8 A Sample Crisis Timeline
The following timeline is only meant to demonstrate
what might be possible with the right infrastructure
in place and the community fully engaged. The
mantra of ?every crisis is different? applies, and this
timeline should not be interpreted as a ?cookbook?
for a future event. All place and entity names are
intended to add realism; there was no intention to
leave anyone in or out.
Day 0 ? A massive earthquake hits the island nation
of Palladi.
Day 1 ? The first aid organizations arrive on the is-
land with food and humanitarian aid, although
only the two major cities are directly accessi-
ble. Thousands of Palladians are not reach-
able by aid organizations, and the exact num-
bers that are affected and their locations are not
known.
508
The native population of Palladians is nearly
80% monolingual. There is a dire need for Pal-
ladian interpreters, but also of translated Pal-
ladian content. Notified of the need for Pal-
ladian translations, MT community volunteers
begin efforts to collect and license data in Pal-
ladian. The relief community responds by ac-
tivating the crowd sourcing infrastructure used
in other relief scenarios. Researchers and disas-
ter response teams are notified at Microsoft Re-
search and Google Research of the critical need
for crisis content to be translated into Palladian.
Native Palladian speakers are being looked for
by all parties.
Day 2 ? As with the Haitian crisis, a text messag-
ing infrastructure is put in place such that text
messages can be received from the population
and routed to a crowd of rapidly assembling
volunteers. Since there is some internet ac-
cess, including via mobile phones, twitter feeds
are monitored. Until messages start arriving, a
small crowd of Palladian speakers begin trans-
lating content into Palladian, focused specifi-
cally on the Cookbook and off-the-shelf SMS
content.
The first text messages start arriving by late af-
ternoon. These text messages are routed di-
rectly to the text messaging and microtasking
infrastructure. The small but growing crowd
of Palladian translators begin translating this
growing tide of messages.
Day 3 ? The humanitarian information processing
community, with the support of many organiza-
tions and volunteers, releases the first sections
of the Crisis Cookbook. The Crisis Cookbook
is transmitted directly to aid organizations on
the ground in Palladi, and soft- and hard-copies
are distributed to aid workers as quickly as fea-
sible.
AT&T puts into place several cell towers with
satellite connectivity for areas that do not have
cell coverage. Within hours, text and twitter
messages from the field increase dramatically.
Day 4 ? Microsoft and Google release the first ver-
sions of their Palladian-English translators,
with ready access via their public APIs. Since
the text messaging infrastructure already has
both APIs integrated directly into the micro-
tasking and message processing infrastructure,
both engines are activated immediately, and all
messages are translated first by one or the other
engine, and the MT?d content along with the
original message are handed to volunteers.15
Translations are repaired, and routed directly to
aid organizations, and to the Google and Mi-
crosoft teams (for retraining models).
Day 5 ? Additional cookbook materials are trans-
lated. Researchers at Johns Hopkins locate a
stash of Palladian data at the Palladian Cen-
tral University. This data is posted at the CMU
site, and is immediately consumed by all par-
ties working on the MT problem.
Day 6 ? Researchers at University of Edinburgh de-
velop a novel algorithm for dealing with Palla-
dian vowel harmony, which has been a major
problem with Palladian MT, since data sparsity
is exacerbated by the problem. The Edinburgh
researchers publish the algorithm immediately
to their Web site, and notify both Microsoft and
Google.
Day 10 ? Armed with algorithmic improvements
and an increasing volume of data, machine
translated content is now achieving sufficient
quality to warrant passing it directly to aid or-
ganizations. Palladian volunteers now work
principally on the hard to translate cases (those
with high OOVs), and on post-response data
clean-up. The fruits of their labor result in iter-
ative improvements on the various MT engines
that have been deployed.
Day 11+ ? The deployment of language technolo-
gies, specifically MT, in the Palladian crisis re-
sults in saving untold thousands of lives. The
lessons learned in the Palladian earthquake will
be applied to future crises, and the translated
content produced by volunteers will be added
to the cookbook for use in the next crisis.
15Determining which engine to send translations to is a prob-
lem that should be resolved in advance. A combination of either
random selection or on-the-fly OOV calculations could be used
to determine routing.
509
9 Conclusion
In this paper, we propose that MT is an important
technology in crisis events, something that can and
should be an integral part of the rapid-response in-
frastructure. By integrating MT services directly
into a messaging infrastructure (whatever the type of
messages being serviced, e.g., text messages, Twit-
ter feeds, blog postings, etc.), MT can be used to
provide first pass translations into a majority lan-
guage, which can assist in triaging messages and
routing them to appropriate aid agencies. If done
right, MT can dramatically increase the speed by
which relief can be provided. To ensure that MT
is a standard tool in the arsenal of tools used in cri-
sis events, we propose a preliminary Crisis Cook-
book, the data contents of which could be translated
into the relevant language(s) by volunteers imme-
diately after a crisis event takes place. The result-
ing data can then be made available to relief groups
on the ground, as well as to providers of MT ser-
vices. We also note that there are significant con-
tributions that our community can make to relief ef-
forts through continued work on our research, espe-
cially that research which makes MT more viable for
under-resourced languages.
Credits
This paper is dedicated to the thousands of volun-
teers who worked selflessly for many, many hours
in aid of the people of Haiti. Without their help,
many hundreds more would have perished. We also
wish to express our deepest appreciation to all those
who have devoted their lives to aid people in need,
especially the first responders in crisis events. It is
our sincerest hope that that the small measures our
community can take to assist in relief efforts will
help make your jobs more effective, and that our ef-
forts will ultimately assist you and those you strive
to help.
References
Jeffrey Allen. 1998. Lexical variation in Haitian Cre-
ole and orthographic issues for Machine Translation
(MT) and Optical Character Recognition (OCR) appli-
cations. In Association for Machine Translation in the
Americas (AMTA) Workshop on Embedded MT Sys-
tems: Design, Construction, and Evaluation of Sys-
tems with an MT Component, Langhorne, Pennsylva-
nia.
Vamshi Ambati, Sanjika Hewavitharana, Stephan Vogel,
and Jaime Carbonell. 2011. Active Learning with
Multiple Annotations for Comparable Data Classifi-
cation Task. In Proceedings of ACL 2011, Portland,
Oregon, June.
Sharon Anderson. 2010. Talking with Adm. James G.
Stavridis Supreme Allied Commander, Europe Com-
mander, U.S. European Command. CHIPS - The De-
partment of the Navy Information Technology Maga-
zine, 28.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009.
A New Objective Function for Word Alignment. In
Proceedings of the NAACL/HLT Workshop on Inte-
ger Programming for Natural Language Processing,
Boulder, Colorado.
Disaster 2.0. 2011. Disaster Relief 2.0: The Future
of Information Sharing in Humanitarian Emergencies.
United Nations Foundation, UN Office for the Coor-
dination of Humanitarian Affairs (UN OCHA), Voda-
fone Foundation, Harvard Humanitarian Initiative.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP 2002,
Philadelphia, Pennsylvania.
Robert Frederking, Alexander Rudnicky, and Christopher
Hogan. 1997. Interactive speech translation in the
diplomat project. In Workshop on Spoken Language
Translation at ACL-97, Madrid.
Dmitriy Genzel. 2010. Automatically Learning Source-
side Reordering Rules for Large Scale Machine Trans-
lation. In Proceedings of COLING 2010, Beijing, Au-
gust.
Sanjika Hewavitharana and Stephan Vogel. 2008. En-
hancing a Statistical Machine Translation System by
using an Automatically Extracted Parallel Corpus
from Comparable Sources. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC 2008), Workshop on Comparable
Corpora, Marrakech, Morocco, May.
Sanjika Hewavitharana and Stephan Vogel. 2011. Ex-
tracting Parallel Phrases from Comparable Data. In
Proceedings of ACL 2011, Portland, Oregon, June.
Ann Irvine, Chris Callison-Burch, and Alexandre Kle-
mentiev. 2010. Transliterating from all languages. In
510
Proceedings of the Ninth Conference of the Associa-
tion for Machine Translation in the Americas (AMTA
2010), Denver.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, and
Chris Quirk. 2010. A Discriminative Lexicon Model
for Complex Morphology. In Proceedings of the Ninth
Conference of the Association for Machine Translation
in the Americas (AMTA 2010), Denver.
Claire Lefebvre. 1998. Creole Genesis and the Acquisi-
tion of Grammar: The case of Haitian Creole. Cam-
bridge University Press, Cambridge, England.
William D. Lewis and Fei Xia. 2010. Developing
ODIN: A Multilingual Repository of Annotated
Language Data for Hundreds of the World?s Lan-
guages. Literary and Linguistic Computing. See:
http://research.microsoft.com/apps/pubs/default.aspx?
id=138757.
William D. Lewis. 2010. Haitian Creole: How to Build
and Ship an MT Engine from Scratch in 4 Days, 17
Hours, & 30 Minutes. In EAMT 2010: Proceedings of
the 14th Annual conference of the European Associa-
tion for Machine Translation, Saint Raphae?l, France,
May.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Lane Schwartz, Wren N. G.
Thornton, Ziyuan Wang, Jonathan Weese, and Omar F.
Zaidan. 2010. Joshua 2.0: A Toolkit for Parsing-
Based Machine Translation with Syntax, Semirings,
Discriminative Training and Other Goodies. In In Pro-
ceedings of Workshop on Statistical Machine Transla-
tion (WMT10), Uppsala, Sweden.
Christian Monson, Ariadna Font Llitjos, Vamshi Ambati,
Lori Levin, Alon Lavie, Alison Alvarez, Robert Fred-
erking Roberto Aranovich, Jaime Carbonell, Erik Pe-
terson, and Katharina Probst. 2008. Linguistic Struc-
ture and Bilingual Informants Help Induce Machine
Translation of Lesser-Resourced Languages. In Pro-
ceedings of the 6th International Conference on Lan-
guage Resources and Evaluation (LREC 2008), Mar-
rakech, Morocco.
Robert Munro. 2010. Crowdsourced translation for
emergency response in Haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collabo-
rative Crowdsourcing for Translation.
Robert Munro. 2011. Subword and spatiotemporal mod-
els for identifying actionable information in Haitian
Kreyol. In Fifteenth Conference on Computational
Natural Language Learning (CoNLL 2011), Portland.
Douglas W. Oard and Franz Josef Och. 2003. Rapid-
Response Machine Translation for Unexpected Lan-
guages. In MT Summit IX, New Orleans.
Douglas W. Oard. 2003. The Surprise Language Exer-
cises. ACM Transactions on Asian Language Informa-
tion Processing - TALIP, 2(2):79?84.
Kemal Oflazer and Ilknur Durgar El-kahlout. 2007. Ex-
ploring Different Representational Units in English-
to-Turkish Statistical Machine Translation. In In Pro-
ceedings of the Statistical Machine Translation Work-
shop, ACL 2007, Prague.
Chris Quirk and Arul Menezes. 2006. Dependency
Treelet Translation: The convergence of statistical and
example-based machine translation? Machine Trans-
lation, 20:43?65.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of ACL 2011, Portland,
Oregon, June.
RC. 2010. The American Red Cross: Social Media in
Disasters and Emergencies. Presentation.
Jason Smith, Chris Quirk, and Kristina Toutanova. 2010.
Extracting parallel sentences from comparable cor-
pora using document level alignment. In Proceedings
of Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the ACL,
Los Angeles.
Raghavendra Udupa, K Saravanan, A Kumaran, and Ja-
gadeesh Jagarlamudi. 2009. MINT: A Method for Ef-
fective and Scalable Mining of Named Entity Translit-
erations from Large Comparable Corpora. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2009), Athens, Greece.
Andrej Verity. 2011. What the UN could not have
done without the Volunteer Technical Community. In
United Nations Dispatch. The Disaster Relief 2.0 Blog
Series.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
Morphology Mapping in Factored Phrase-Based Sta-
tistical Machine Translation from English to Turkish.
In Proceedings of the ACL 2010, Uppsala, Sweden.
Omar Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing Translation: Professional Quality from Non-
Professionals. In Proceedings of ACL 2011, Portland,
Oregon, June.
511
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 298?303,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
QCRI at WMT12: Experiments in Spanish-English and German-English
Machine Translation of News Text
Francisco Guzma?n, Preslav Nakov, Ahmed Thabet, Stephan Vogel
Qatar Computing Research Institute
Qatar Foundation
Tornado Tower, floor 10, PO box 5825
Doha, Qatar
{fguzman,pnakov,ahawad,svogel}@qf.org.qa
Abstract
We describe the systems developed by the
team of the Qatar Computing Research Insti-
tute for the WMT12 Shared Translation Task.
We used a phrase-based statistical machine
translation model with several non-standard
settings, most notably tuning data selection
and phrase table combination. The evaluation
results show that we rank second in BLEU and
TER for Spanish-English, and in the top tier
for German-English.
1 Introduction
The team of the Qatar Computing Research Insti-
tute (QCRI) participated in the Shared Translation
Task of WMT12 for two language pairs:1 Spanish-
English and German-English. We used the state-of-
the-art phrase-based model (Koehn et al, 2003) for
statistical machine translation (SMT) with several
non-standard settings, e.g., data selection and phrase
table combination. The evaluation results show that
we rank second in BLEU (Papineni et al, 2002) and
TER (Snover et al, 2006) for Spanish-English, and
in the top tier for German-English.
In Section 2, we describe the parameters of our
baseline system and the non-standard settings we
experimented with. In Section 3, we discuss our
primary and secondary submissions for the two lan-
guage pairs. Finally, in Section 4, we provide a short
summary.
1The WMT12 organizers invited systems translating be-
tween English and four other European languages, in both di-
rections: French, Spanish, German, and Czech. However, we
only participated in Spanish?English and German?English.
2 System Description
Below, in Section 2.1, we first describe our initial
configuration; then, we discuss our incremental im-
provements. We explored several non-standard set-
tings and extensions and we evaluated their impact
with respect to different baselines. These baselines
are denoted in the tables below by a #number that
corresponds to systems in Figures 1 for Spanish-
English and in Figure 2 for German-English.
We report case insensitive BLEU calculated on
the news2011 testing data using the NIST scoring
tool v.11b.
2.1 Initial Configuration
Our baseline system can be summarized as follows:
? Training: News Commentary + Europarl train-
ing bi-texts;
? Tuning: news2010;
? Testing: news2011;
? Tokenization: splitting words containing a
dash, e.g., first-order becomes first @-@ order;
? Maximum sentence length: 100 tokens;
? Truecasing: convert sentence-initial words to
their most frequent case in the training dataset;
? Word alignments: directed IBM model 4
(Brown et al, 1993) alignments in both direc-
tions, then grow-diag-final-and heuristics;
? Maximum phrase length: 7 tokens;
? Phrase table scores: forward & reverse phrase
translation probabilities, forward & reverse lex-
ical translation probabilities, phrase penalty;
298
? Language model: 5-gram, trained on the target
side of the two training bi-texts;
? Reordering: lexicalized, msd-bidirectional-fe;
? Detokenization: reconnecting words that were
split around dashes;
? Model parameter optimization: minimum error
rate training (MERT), optimizing BLEU.
2.2 Phrase Tables
We experimented with two non-standard settings:
Smoothing. The four standard scores associated
with each phrase pair in the phrase table (forward
& reverse phrase translation probabilities, forward
& reverse lexical translation probabilities) are nor-
mally used unsmoothed. We also experimented with
Good-Turing and Kneser-Ney smoothing (Chen and
Goodman, 1999). As Table 1 shows, the latter works
a bit better for both Spanish-English and German-
English.
es-en de-en
Baseline (es:#3,de:#4) 29.98 22.03
Good Turing 29.98 22.07
Kneser-Ney 30.16 22.30
Table 1: Phrase table smoothing.
Phrase table combination. We built two phrase
tables, one for News Commentary + Europarl and an
additional one for the UN bi-text. We then merged
them,2 adding additional features to each entry in
the merged phrase table: F1, F2, and F3. The
value of F1/F2 is 1 if the phrase pair came from the
first/second phrase table, and 0.5 otherwise, while
F3 is 1 if the phrase pair was in both tables, and 0.5
otherwise. We optimized the weights for all features,
including the additional ones, using MERT.3 Table 2
shows that this improves by +0.42 BLEU points.
2In theory, we should also re-normalize the conditional
probabilities (forward/reverse phrase translation probability,
and forward/reverse lexicalized phrase translation probability)
since they may not sum to one anymore. In practice, this is
not that important since the log-linear phrase-based SMT model
does not require that the phrase table features be probabilities
(e.g., F1, F2, F3, and the phrase penalty are not probabilities);
moreover, we have extra features whose impact is bigger.
3This is similar but different from (Nakov, 2008): when a
phrase pair appeared in both tables, they only kept the entry
from the first table, while we keep the entries from both tables.
es-en
Baseline (es:#7) 30.94
Merging (1) News+EP with (2) UN 31.36
Table 2: Phrase table merging.
2.3 Language Models
We built the language models (LM) for our systems
using a probabilistic 5-gram model with Kneser-
Ney (KN) smoothing. We experimented with LMs
trained on different training datasets. We used the
SRILM toolkit (Stolcke, 2002) for training the lan-
guage models, and the KenLM toolkit (Heafield
and Lavie, 2010) for binarizing the resulting ARPA
models for faster loading with the Moses decoder
(Koehn et al, 2007).
2.3.1 Using WMT12 Corpora Only
We trained 5-gram LMs on datasets provided by
the task organizers. The results are presented in
Table 3. The first line reports the baseline BLEU
scores using a language model trained on the target
side of the News Commentary + Europarl training
bi-texts. The second line shows the results when us-
ing an interpolation (minimizing the perplexity on
the news2010 tuning dataset) of different language
models, trained on the following corpora:
? the monolingual News Commentary corpus
plus the English sides of all training News
Commentary v.7 bi-texts (for French-English,
Spanish-English, German-English, and Czech-
English), with duplicate sentences removed
(5.5M word tokens; one LM);
? the News Crawl 2007-2011 corpora, (1213M
word tokens; separate LM for each of these five
years);
? the Europarl v.7 monolingual corpus (60M
word tokens; one LM);
? the English side of the Spanish-English UN bi-
text (360M word tokens; one LM).
The last line in Table 3 shows the results when
using an additional 5-gram LM in the interpolation,
one trained on the English side of the 109 French-
English bi-text (662M word tokens).
299
We can see that using these interpolations yields
very sizable improvements of 1.7-2.5 BLEU points
over the baseline. However, while the impact of
adding the 109 bi-text to the interpolation is clearly
visible for Spanish-English (+0.47 BLEU), it is al-
most negligible for German-English (+0.06 BLEU).
Corpora es-en de-en
Baseline (es:#1, de:#2) 27.34 20.01
News + EP + UN (interp.) 29.36 21.66
News + EP + UN + 109 (interp.) 29.83 21.72
Table 3: LMs using the provided corpora only.
2.3.2 Using Gigaword
In addition to the WMT12 data, we used the LDC
Gigaword v.5 corpus. We divided the corpus into
reasonably-sized chunks of text of about 2GB per
chunk, and we built a separate intermediate language
model for each chunk. Then, we interpolated these
language models, minimizing the perplexity on the
news2010 development set as with the previous
LMs. We experimented with two different strate-
gies for creating the chunks by segmenting the cor-
pus according to (a) data source, e.g., AFP, Xinhua,
etc., and (b) year of release. We thus compared the
advantages of interpolating epoch-consistent LMs
vs. source-coherent LMs. We trained individual
LMs for each of the segments and we added them
to a pool. Finally, we selected the ten most relevant
ones from this pool based on their perplexity on the
news2010 devset, and we interpolated them.
The results are shown in Table 4. The first line
shows the baseline, which uses an interpolation of
the nine LMs from the previous subsection. The
following two lines show the results when using an
LM trained on Gigaword only. We can see that for
Spanish-English, interpolation by year performs bet-
ter, while for German-English, it is better to use the
by-source chunks. However, the following two lines
show that when we translate with two LMs, one built
from the WMT12 data only and one built using Gi-
gaword data only, interpolation by year is preferable
for Gigaword for both language pairs. For our sub-
mitted systems, we used the LMs shown in bold in
Table 4: we used a single LM for Spanish-English
and two LMs for German-English.
Language Models es-en de-en
Baseline (es:#5, de:#6) 30.31 22.48
GW by year 30.68 22.32
GW by source 30.52 22.56
News-etc + GW by year 30.60 22.71
News-etc + GW by source 30.55 22.54
Table 4: LMs using Gigaword.
2.4 Parameter Tuning and Data Selection
Parameter tuning is a very important step in SMT.
The standard procedure consists of performing a se-
ries of iterations of MERT to choose the model pa-
rameters that maximize the translation quality on a
development set, e.g., as measured by BLEU. While
the procedure is widely adopted, it is also recognized
that the selection of an appropriate development set
is important since it biases the parameters towards
specific types of translations. This is illustrated in
Table 5, which shows BLEU on the news2011 testset
when using different development sets for MERT.
Devset es-en
news2008 29.47
news2009 29.14
news2010 29.61
Table 5: Using different tuning sets for MERT.
To address this problem, we performed a selection
of development data using an n-gram-based similar-
ity ranking. The selection was performed over a pool
of candidate sentences drawn from the news2008,
news2009, and news2010 tuning datasets. The sim-
ilarity metric was defined as follows:
sim(f, g) = 2match(f, g) ? lenpen(f, g) (1)
where 2match represents the number of bi-gram
matches between sentences f and g, and lenpen is
a length penalty to discourage unbalanced matches.
We penalized the length difference using an
inverted-squared sigmoid function:
lenpen(f, g) = 3? 4 ? sig
([
|f | ? |g|
?
]2
)
(2)
300
where |.| denotes the length of a sentence in num-
ber of words, ? controls the maximal tolerance to
differences, and sig is the sigmoid function.
To generate a suitable development set, we av-
eraged the similarity scores of candidate sentences
w.r.t. to the target testset. For instance:
sf =
1
|G|
?
g?G
sim(f, g) (3)
where G is the set of the test sentences.
Finally, we selected a pool of candidates f from
news2008, news2009 and news2011 to generate a
2000-best tuning set. The results when using each of
the above penalty functions are presented on Table 6.
devset es-en
baseline (es:#6) 30.68
selection (? = 5) 30.94
selection (? = 10) 30.90
Table 6: Selecting sentences for MERT.
The average length of the source-side sentences
in our selected sentence pairs was smaller than in
our baseline, the news2011 development dataset.
This means that our selected source-side sentences
tended to be shorter than in the baseline. Moreover,
the standard deviation of the sentence lengths was
smaller for our samples as well, which means that
there were fewer long sentences; this is good since
long sentences can take very long to translate. As
a result, we observed sizable speedup in parameter
tuning when running MERT on our selected tuning
datasets.
2.5 Decoding and Hypothesis Reranking
We experimented with two decoding settings:
(1) monotone at punctuation reordering (Tillmann
and Ney, 2003), and (2) minimum Bayes risk decod-
ing (Kumar and Byrne, 2004). The results are shown
in Table 7. We can see that both yield improvements
in BLEU, even if small.
2.6 System Combination
As the final step in our translation system, we per-
formed hypothesis re-combination of the output of
several of our systems using the Multi-Engine MT
system (MEMT) (Heafield and Lavie, 2010).
es-en de-en
Baseline (es:#2,de:#3) 29.83 21.72
+MP 29.98 22.03
Baseline (es:#4,de:#5) 30.16 22.30
+MBR 30.31 22.48
Table 7: Decoding parameters. Experiments with
monotone at punctuation (MP) reordering, and minimum
Bayes risk (MBR) decoding.
The results for the actual news2012 testset are
shown in Table 8: the system combination results
are our primary submission. We can see that system
combination yielded 0.4 BLEU points of improve-
ment for Spanish-English and 0.2-0.3 BLEU points
for German-English.
3 Our Submissions
Here we briefly describe the cumulative improve-
ments when applying the above modifications to our
baseline system, leading to our official submissions
for the WMT12 Shared Translation Task.
3.1 Spanish-English
The development of our final Spanish-English sys-
tem involved several incremental improvements,
which have been described above and which are
summarized in Figure 1. We started with a base-
line system (see Section 2.1), which scored 27.34
BLEU points. From there, using a large inter-
polated language model trained on the provided
data (see Section 2.3.1) yielded +2.49 BLEU points
of improvement. Monotone-at-punctuation de-
coding contributed an additional improvement of
+0.15, smoothing the phrase table using Kneser-Ney
boosted the score by +0.18, and using minimum
Bayes risk decoding added another +0.15 BLEU
points. Changing the language model to one trained
on Gigaword v.5 and interpolated by year yielded
+0.37 additional points of improvement. Another
+0.26 points came from tuning data selection. Fi-
nally, using the UN data in a merged phrase ta-
ble (see Section 2.2) yielded another +0.42 BLEU
points. Overall, we achieve a total improvement
over our initial baseline of about 4 BLEU points.
301
27.34 
29.83 29.98 30.16 30.31 
30.68 
30.94 
31.36 
25 
26 
27 
28 
29 
30 
31 
32 
1:BA
SELI
NE 
2:+W
MT-L
M 
3:+M
P 
4:+K
N 
5:+M
BR 
6:*G
IGA 
V5-L
M 
7:+T
UNE
-SEL
 
8:+P
T-ME
RGE
 
!
BL
EU
 v12
  sc
ore
  (n
ew
s-2
011
) 
Figure 1: Incremental improvements for the Spanish-English system.
3.2 German-English
Figure 2 shows a similar sequence of improvements
for our German-English system. We started with a
baseline (see Section 2.1) that scored 19.79 BLEU
points. Next, we performed compound splitting for
the German side of the training, the development
and the testing bi-texts, which yielded +0.22 BLEU
points of improvement. Using a large interpolated
language model trained on the provided corpora (see
Section 2.3.1) added another +1.71. Monotone-at-
punctuation decoding contributed +0.31, smoothing
the phrase table using Kneser-Ney boosted the score
by +0.27, and using minimum Bayes risk decoding
added another +0.18 BLEU points. Finally, adding a
second language model trained on the Gigaword v.5
corpus interpolated by year yielded +0.23 additional
BLEU points. Overall, we achieved about 3 BLEU
points of total improvement over our initial baseline.
3.3 Final Submissions
For both language pairs, our primary submission
was a combination of the output of several of our
best systems shown in Figures 1 and 2, which use
different experimental settings; our secondary sub-
mission was our best individual system, i.e., the
right-most one in Figures 1 and 2.
The official BLEU scores, both cased and lower-
cased, for our primary and secondary submissions,
as evaluated on the news2012 dataset, are shown
in Table 8. For Spanish-English, we achieved the
second highest BLEU and TER scores, while for
German-English we were ranked in the top tier.
news2012
lower cased
Spanish-English
Primary 34.0 32.9
Secondary 33.6 32.5
German-English
Primary 23.9 22.6
Secondary 23.6 22.4
Table 8: The official BLEU scores for our submissions
to the WMT12 Shared Translation Task.
4 Conclusion
We have described the primary and the secondary
systems developed by the team of the Qatar Com-
puting Research Institute for Spanish-English and
German-English machine translation of news text
for the WMT12 Shared Translation Task.
We experimented with phrase-based SMT, explor-
ing a number of non-standard settings, most notably
tuning data selection and phrase table combination,
which we described and evaluated in a cumulative
fashion. The automatic evaluation metrics,4 have
ranked our system second for Spanish-English and
in the top tier for German-English.
We plan to continue our work on data selection
for phrase table and the language model training, in
addition to data selection for tuning.
4The evaluation scores for WMT12 are available online:
http://matrix.statmt.org/
302
19.79 20.01 
21.72 
22.03 22.30 
22.48 
22.71 
18 
18.5 
19 
19.5 
20 
20.5 
21 
21.5 
22 
22.5 
23 
1:BA
SELI
NE 
2:+S
PLIT
 
3:WM
T-LM
 
4:+M
P 
5:+K
N 
6:+M
BR 
7:+G
IGA+
WMT
 LM 
BL
EU
 v12
  sc
ore
   (n
ew
s-2
011
) 
Figure 2: Incremental improvements for the German-English system.
Acknowledgments
We would like to thank the anonymous reviewers
for their useful comments, which have helped us im-
prove the text of this paper.
References
Peter Brown, Vincent Della Pietra, Stephen Della Pietra,
and Robert Mercer. 1993. The mathematics of statis-
tical machine translation: parameter estimation. Com-
putational Linguistics, 19(2):263?311.
Stanley Chen and Joshua Goodman. 1999. An empirical
study of smoothing techniques for language modeling.
Computer Speech & Language, 13(4):359?393.
Kenneth Heafield and Alon Lavie. 2010. Combin-
ing machine translation output with open source:
The Carnegie Mellon multi-engine machine transla-
tion scheme. The Prague Bulletin of Mathematical
Linguistics, 93(1):27?36.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL. Demonstration session, ACL ?07, pages
177?180, Prague, Czech Republic.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Susan Dumais, Daniel Marcu, and Salim
Roukos, editors, Proceedings of the Annual Meeting
of the North American chapter of the Association for
Computational Linguistics, HLT-NAACL ?04, pages
169?176, Boston, MA.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: Experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of the Third Workshop
on Statistical Machine Translation, WMT ?07, pages
147?150, Prague, Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics, ACL ?02, pages 311?318, Philadelphia,
PA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Annual Meetig of the Associa-
tion for Machine Translation in the Americas, AMTA
?06, pages 223?231.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of Intl. Conf.
on Spoken Language Processing, volume 2 of ICSLP
?02, pages 901?904, Denver, CO.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97?133.
303
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 373?379,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
MT Quality Estimation: The CMU System for WMT?13
Almut Silja Hildebrand
Carnegie Mellon University
Pittsburgh, USA
silja@cs.cmu.edu
Stephan Vogel
Qatar Computing Research Institute
Doha, Qatar
svogel@qf.org.qa
Abstract
In this paper we present our entry to the
WMT?13 shared task: Quality Estima-
tion (QE) for machine translation (MT).
We participated in the 1.1, 1.2 and 1.3
sub-tasks with our QE system trained on
features from diverse information sources
like MT decoder features, n-best lists,
mono- and bi-lingual corpora and giza
training models. Our system shows com-
petitive results in the workshop shared
task.
1 Introduction
As MT becomes more and more reliable, more
people are inclined to use automatically translated
texts. If coming across a passage that is obviously
a mistranslation, any reader would probably start
to doubt the reliability of the information in the
whole article, even though the rest might be ad-
equately translated. If the MT system had a QE
component to mark translations as reliable or pos-
sibly erroneous, the reader would know to use in-
formation from passages marked as bad transla-
tions with caution, while still being able to trust
other passages. In post editing a human translator
could use translation quality annotation as an indi-
cation to whether editing the MT output or trans-
lating from scratch might be faster. Or he could
use this information to decide where to start in or-
der to improve the worst passages first or skip ac-
ceptable passages altogether in order to save time.
Confidence scores can also be useful for applica-
tions such as cross lingual information retrieval or
question answering. Translation quality could be
a valuable ranking feature there.
Most previous work in the field estimates con-
fidence on the sentence level (e.g. Quirk et
al. (2004)), some operate on the word level (e.g.
Ueffing and Ney (2007), Sanchis et al (2007),
and Bach et al (2011)), whereas Soricut and Echi-
habi (2010) use the document level.
Various classifiers and regression models have
been used in QE in the past. Gandrabur and Foster
(2003) compare single layer to Multi Layer Per-
ceptron (MLP), Quirk et al (2004) report that Lin-
ear Regression (LR) produced the best results in
a comparison of LR, MLP and SVM, Gamon et
al. (2005) use SVM, Soricut and Echihabi (2010)
find the M5P tree works best among a number of
regression models, while Bach et al (2011) define
the problem as a word sequence labeling task and
use MIRA.
The QE shared task was added to the WMT
evaluation campaign in 2012 (Callison-Burch et
al., 2012), providing standard training and test
data for system development.
2 WMT?13 Shared Task
In this WMT Shared Task for Quality Estima-
tion1 there were tasks for sentence and word level
QE. We participated in all sub-tasks for Task 1:
Sentence-level QE.
Task 1.1: Scoring and ranking for post-editing
effort focuses on predicting HTER per segment
for the translations of one specific MT system.
Task 1.2: System selection/ranking required to
predict a ranking for up to five translations of
the same source sentence by different MT sys-
tems. The training data provided manual labels for
ranking including ties. Task 1.3: Predicting post-
editing time participants are asked to predict the
time in seconds a professional translator will take
to post edit each segment.
1http://www.statmt.org/wmt13/quality-estimation-
task.html
373
Besides the training data with labels, for each
of these tasks additional resources were provided.
These include bilingual training corpora, language
models, 1000-best lists, models from giza and
moses training and various other statistics and
models depending on task and language pair.
3 Features
3.1 Language Models
To calculate language model (LM) features, we
train traditional n-gram language models with n-
gram lengths of four and five using the SRILM
toolkit (Stolcke, 2002). We calculate our features
using the KenLM toolkit (Heafield, 2011). We
normalize all our features with the target sentence
length to get an average word feature score, which
is comparable for translation hypotheses of differ-
ent length. In addition to the LM probability we
record the average n-gram length found in the lan-
guage model for the sentence, the total number of
LM OOVs and OOVs per word, as well as the
maximum and the minimum word probability of
the sentence, six features total.
We use language models trained on source lan-
guage data and target language data to measure
source sentence difficulty as well as translation
fluency.
3.2 Distortion Model
The moses decoder uses one feature from a dis-
tance based reordering model and six features
from a lexicalized reordering model: Given a
phrase pair, this model considers three events
Monotone, Swap, and Discontinuous in two direc-
tions Left and Right. This results in six events:
LM (left-monotone), LS (left-swap), LD (left-
discontinuous) and RM (right-monotone), RS,
RD.
These distortion features are calculated for each
phrase. For a total sentence score we normalize by
the phrase count for each of the seven features.
3.3 Phrase Table
From the phrase table we use the features from
the moses decoder output: inverse phrase trans-
lation probability, inverse lexical weighting, di-
rect phrase translation probability and direct lex-
ical weighting. For a total sentence score we nor-
malize by the phrase count. We use the number
of phrases used to generate the hypothesis and the
average phrase length as additional features, six
features total.
3.4 Statistical Word Lexica
From giza training we use IBM-4 statistical word
lexica in both directions. We use six probabil-
ity based features as described in Hildebrand and
Vogel (2008): Normalized probability, maximum
word probability and word deletion count from
each language direction.
To judge the translation difficulty of each word
in the source sentence we collect the number of
lexicon entries for each word similar to Gandrabur
and Foster (2003). The intuition is, that a word
with many translation alternatives in the word-to-
word lexicon is difficult to translate while a word
with only a few translation choices is easy to trans-
late.
In fact it is not quite this straight forward. There
are words in the lexicon, which have many lex-
icon entries, but the probability for them is not
very equally distributed. One entry has a very
high probability while all others have a very low
one - not much ambiguity there. Other words
on the other hand have several senses in one lan-
guage and therefore are translated frequently into
two or three different words in the target language.
There the top entries in the lexicon might each
have about 30% probability. To capture this be-
havior we do not only count the total number of
entries but also the number of entries with a prob-
ability over a threshold of 0.01.
For example one word with a rather high num-
ber of different translations in the English-Spanish
statistical lexicon is the period (.) with 1570 en-
tries. It has only one translation with a probability
over the threshold which is the period (.) in Span-
ish at a probability of 0.9768. This shows a clear
choice and rather little ambiguity despite the high
number of different translations in the lexicon.
For each word we collect the number of lexi-
con entries, the number of lexicon entries over the
threshold, the highest probability from the lexicon
and whether or not the word is OOV. If a word has
no lexicon entry with a probability over the thresh-
old we exclude the word from the lexicon for this
purpose and count it as an OOV. As sentence level
features we use the sum of the word level features
normalized by the sentence length as well as the
total OOV count for the sentence, which results in
five features.
374
3.5 Sentence Length Features
The translation difficulty of a source sentence is
often closely related to the sentence length, as
longer sentences tend to have a more complex
structure. Also a skewed ratio between the length
of the source sentence and its translation can be an
indicator for a bad translation.
We use plain sentence length features, namely
the source sentence length, the translation hypoth-
esis length and their ratio as introduced in Quirk
(2004).
Similar to Blatz et al (2004) we use the n-best
list as an information source. We calculate the av-
erage hypothesis length in the n-best list for one
source sentence. Then we compare the current hy-
pothesis to that and calculate both the diversion
from that average as well as their ratio. We also
calculate the source-target ratio to this average hy-
pothesis length.
To get a representative information on the
length relationship of translations from the source
and target languages in question, we use the par-
allel training corpus. We calculate the overall lan-
guage pair source to target sentence length ratio
and record the diversion of the current hypothesis?
source-target ratio from that.
The way sentences are translated from one lan-
guage to another might differ depending on how
complex the information is, that needs to be con-
veyed, which in turn might be related to the sen-
tence length and the ratio between source and
translation. As a simple way of capturing this
phenomenon we divide the parallel training cor-
pus into three classes (short, medium, long) by
the length of the source language sentence. The
boundaries of these classes are the mean 26.84
plus and minus the standard deviation 14.54 of the
source sentence lengths seen in the parallel cor-
pus. We calculate the source/target length ratio for
each of the three classes separately. The resulting
statistics for the parallel training corpora can be
found in Table 1. For English - Spanish the ratio
for all classes is close to one, for other language
pairs these differ more clearly.
As features for each hypothesis we use a binary
indicator for its membership to each class and its
deviation from the length ratio of its class. This
results in 12 sentence length related features in to-
tal.
En train
number of sentences 1,714,385
average length 26.84
standard deviation 14.54
class short 0 - 12.29
class medium 12.29 - 41.38
class long 41.38 - 100
s/t ratio overall 0.9624
s/t ratio for short 0.9315
s/t ratio for medium 0.9559
s/t ratio for long 0.9817
Table 1: Sentence Length Statistics for the
English-Spanish Parallel Corpus
3.6 Source Language Word and Bi-gram
Frequency Features
The length of words is often related to whether
they are content words and how frequently they
are used in the language. Therefore we use the
maximum and average word length as features.
Similar to Blatz et al (2004) we sort the vo-
cabulary of the source side of the training corpus
by occurrence frequency and then divide it into
four parts, each of which covers 25% of all to-
kens. As features we use the percentage of words
in the source sentence that fall in each quartile.
Additionally we use the number and percentage of
source words in the source sentence that are OOV
or very low frequency, using count 2 as threshold.
We also collect all bigram statistics for the cor-
pus and calculate the corresponding features for
the source sentence based on bigrams. This adds
up to fourteen features from source word and cor-
pus statistics.
3.7 N-Best List Agreement & Diversity
We use the three types of n-best list based features
described in Hildebrand and Vogel (2008): Posi-
tion Dependent N-best List Word Agreement, Po-
sition independent N-best List N-gram Agreement
and N-best List N-gram Probability.
To measure n-best list diversity, we compare
the top hypothesis to the 5th, 10th, 100th, 200th,
300th, 400th and 500th entry in the n-best list
(where they exist) to see how much the transla-
tion changes throughout the n-best list. We calcu-
late the Levenshtein distance (Levenshtein, 1966)
between the top hypothesis and the three lower
ranked ones and normalize by the sentence length
375
of the first hypothesis. We also record the n-best
list size and the size of the vocabulary in the n-
best list for each source sentence normalized by
the source sentence length.
Fifteen agreement based and nine diversity
based features add up to 24 n-best list based fea-
tures.
3.8 Source Parse Features
The intuition is that a sentence is harder to trans-
late, if its structure is more complicated. A sim-
ple indicator for a more complex sentence struc-
ture is the presence of subclauses and also the
length of any clauses and subclauses. To obtain the
clause structure, we parse the source language sen-
tence using the Stanford Parser2 (Klein and Man-
ning, 2003). Features are: The number of clauses
and subclauses, the average clause length, and the
number of sentence fragments found. If the parse
does not contain a clause tag, it is treated as one
clause which is a fragment.
3.9 Source-Target Word Alignment Features
A forced alignment algorithm utilizes the trained
alignment models from the MT systems GIZA
(Och and Ney, 2003) training to align each source
sentence to each translation hypothesis.
We use the score given by the word alignment
models, the number of unaligned words and the
number of NULL aligned words, all normalized
by the sentence length, as three separate features.
We calculate those for both language directions.
Hildebrand and Vogel (2010) successfully applied
these features in n-best list re-ranking.
3.10 Cohesion Penalty
Following the cohesion constraints described in
Bach et al (2009) we calculate a cohesion penalty
for the translation based on the dependency parse
structure of the source sentence and the word
alignment to the translation hypothesis. To obtain
these we use the Stanford dependency parser (de
Marneffe et al, 2006) and the forced alignment
from Section 3.9.
For each head word we collect all dependent
words and also their dependents to form each com-
plete sub-tree. Then we project each sub-tree onto
the translation hypothesis using the alignment. We
test for each sub-tree, whether all projected words
in the translation are next to each other (cohesive)
2http://nlp.stanford.edu/software/lex-parser.shtml
or if there are gaps. From the collected gaps we
subtract any unaligned words. Then we count the
number of gaps as cohesion violations as well as
how many words are in each gap. We go recur-
sively up the tree, always including all sub-trees
for each head word. If there was a violation in
one of the sub-trees it might be resolved by adding
in its siblings, but if the violation persists, it is
counted again.
4 Classifiers
For all experiments we used the Weka3 data min-
ing toolkit described in Hall et. al. (2009) to com-
pare four different classifiers: Linear Regression
(LR), M5P tree (M5Ptree), Multi Layer Percep-
tron (MLP) and Support Vector Machine for Re-
gression (SVM). Each of these has been identi-
fied as effective in previous publications. All but
one of the Weka default settings proved reliable,
changing the learning rate for the MLP from de-
fault: 0.3 to 0.01 improved the performance con-
siderably. We report Mean Absolute Error (MAE)
and Root Mean Squared Error (RMSE) for all re-
sults.
5 Experiment Results
For Tasks 1.1 and 1.3 we used the 1000-best out-
put provided. As first step we removed duplicate
entries in these n-best list. This brought the size
down to an average of 152.9 hypotheses per source
sentence for the Task 1.1 training data, 172.7 on
the WMT12 tests set and 204.3 hypotheses per
source sentence on the WMT13 blind test data.
The training data for task 1.3 has on average 129.0
hypothesis per source sentence, the WMT13 blind
test data 129.8.
In addition to our own features described above
we extracted the 17 features used in the WMT12
baseline for all sub-tasks via the software provided
for the WMT12-QE shared task.
5.1 Task 1.1
Task 1.1 is to give a quality score between 0 and
1 for each segment in the test set, predicting the
HTER score for the segment and also to give a
rank for each segment, sorting the entire test set
from best quality of translation to worst.
For Task 1.1 our main focus was the scoring
task. We did submit a ranking for the blind test
3http://www.cs.waikato.ac.nz/ml/weka/
376
wmt12-test: WMT12 manual quality labels
WMT12 best system: Language Weaver 0.61 - 0.75
WMT12 baseline system 0.69 - 0.82
feat. set #feat LR M5Pt MLP SVM
full 117 0.617 - 0.755 0.618 - 0.756 0.619 - 0.773 0.609 - 0.750
no WMT12-base 100 0.618 - 0.766 0.618 - 0.767 0.603 - 0.757 0.611 - 0.761
slim 69 0.621 - 0.767 0.621 - 0.766 0.614 - 0.768 0.627 - 0.773
wmt12-test: HTER
full 117 0.125 - 0.162 0.126 - 0.163 0.122 - 0.156 0.121 - 0.156
no WMT12-base 100 0.124 - 0.160 0.123 - 0.159 0.125 - 0.159 0.121 - 0.155
slim 69 0.125 - 0.161 0.126 - 0.161 0.124 - 0.159 0.123 - 0.158
wmt13-test: HTER
WMT12 baseline system 0.148 - 0.182
full 117 0.146 - 0.183 0.147 - 0.185 0.156 - 0.199 0.142 - 0.180
no WMT12-base 100 0.144 - 0.180 0.144 - 0.180 0.156 - 0.203 0.139 - 0.176
slim 69 0.147 - 0.182 0.147 - 0.181 0.153 - 0.194 0.142 - 0.177
Table 2: Task 1.1: Results in MAE and RMSE on the WMT12 test set for WMT12 manual labels as well
as WMT13 HTER as target class and the WMT13 test set for HTER
set as well, which resulted from simply sorting the
test set by the estimated HTER per segment.
In Table 2 we show the results for some ex-
periments comparing the performance of differ-
ent feature sets and classifiers. For development
we used the WMT12-QE test set and both the
WMT12 manual labels as well as HTER as target
class. We compared the impact of removing the
17 WMT12-baseline features ?no WMT12-base?
and training a ?slim? system by removing nearly
half the features, which showed to have a smaller
impact on the overall performance in preliminary
experiments. Among the removed features are
n-best list based features, redundant features be-
tween ours, the moses based and the base17 fea-
tures and some less reliable features like e.g. the
lexicon deletion features, who?s thresholds need to
be calibrated carefully for each new language pair.
We submitted the full+MLP and the no-WMT12-
base+SVM output to the shared task, shown in
bold in the table.
The official result for our system for task 1.1
on the WMT13 blind data is MAE 13.84, RMSE
17.46 for the no-WMT12-base+SVM system and
MAE 15.25 RMSE 18.97 for the full+MLP sys-
tem. Surprising here is the fact that our full system
clearly outperforms the 17-feature baseline on the
WMT12 test set, but is behind it on the WMT13
blind test set. (Baseline bb17 SVM: MAE 14.81,
RMSE 18.22) Looking at the WMT13 test set re-
sults, we should have chosen the slim+SVM sys-
tem variant.
5.2 Task 1.2
Task 1.2 asks to rank different MT systems by
translation quality on a segment by segment basis.
Since the manually annotated ranks in task 1.2
allowed ties, we treated them as quality scores and
ran the same QE system on this data as we did
for task 1.1. We submitted the full-MLP output
with the only difference that for this data set the
decoder based features were not available. We
rounded the predicted ranks to integer. Since the
training data contains many ties we did not employ
a strategy to resolve ties.
As a contrastive approach we ran the hypothe-
sis selection system described in Hildebrand and
Vogel (2010) using the BLEU MT metric as rank-
ing criteria. For this system it would have been
very beneficial to have access to the n-best lists
for the different system?s translations. The BLEU
score for the translation listed as the first system
for each source sentence would be 30.34 on the
entire training data. We ran n-best list re-ranking
using MERT (Och, 2003) for two feature sets: The
full feature set, 100 features in total and a slim fea-
ture set with 59 features. For the slim feature set
we removed all features that are solely based on
377
the source sentence, since those have no impact on
re-ranking an n-best list. The BLEU score for the
training set improved to 45.25 for the full feature
set and to 45.76 for the slim system. Therefore
we submitted the output of the slim system to the
shared task. This system does not predict ranks
directly, but estimates ranking according to BLEU
gain on the test set. Therefore the new ranking is
always ranks 1-5 without ties.
The official result uses Kendalls tau with and
without ties penalized. Our two submissions
score: ?0.11 /?0.11 for the BLEU optimized sys-
tem and?0.63 / 0.23 for the classifier system. The
classifier system is the best submission in the ?ties
ignored? category.
5.3 Task 1.3
Task 1.3 is to estimate post editing time on a per
segment basis.
In absence of a development test set we used
10-fold cross-validation on the training data to de-
termine the best feature set and classifier for the
two submissions. Table 3 shows the results on our
preliminary tests for four classifiers and three fea-
ture sets. The ?no pr.? differs from the full fea-
ture set only by removing the provided features, in
this case the 17 WMT12-baseline features and the
?translator ID? and ?nth in doc? features. For the
?slim? system run the feature set size was cut in
half in order to prevent overfitting to the training
data since the training data set is relatively small.
We used the same criteria as in Task 1.1. For
the shared task we submitted the full+SVM and
slim+LR variants, shown in bold in the table.
The official result for our entries on the WMT13
blind set in MAE and RMSE are: 53.59 - 92.21 for
the full system and 51.59 - 84.75 for the slim sys-
tem. The slim system ranks 3rd for both metrics
and outperforms the baseline at 51.93 - 93.36.
6 Conclusions
In this WMT?13 QE shared task we submitted to
the 1.1, 1.2 and 1.3 sub-tasks. In development we
focused on the scoring type tasks.
In general there don?t seem to be significant dif-
ferences between the different classifiers.
Surprising is the fact that our full system for
task 1.1 clearly outperforms the 17-feature base-
line on the WMT12 test set, but is behind it on
the WMT13 blind test set. This calls into ques-
tion whether the performance on the WMT12 test
set was the right criterium for selecting a system
variant for submission.
The relative success of the ?slim? system vari-
ant over the full feature set shows that our system
would most likely benefit from a sophisticated fea-
ture selection method. We plan to explore this in
future work.
References
Nguyen Bach, Stephan Vogel, and Colin Cherry. 2009.
Cohesive constraints in a beam search phrase-based
decoder. In HLT-NAACL (Short Papers), pages 1?4.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: A method for measuring machine
translation confidence. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 211?219, Portland, Oregon, USA, June. As-
sociation for Computational Linguistics.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. Technical report,
Final report JHU / CLSP 2003 Summer Workshop,
Baltimore.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC-06.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level mt evaluation without refer-
ence translations: Beyond language modeling. In
In European Association for Machine Translation
(EAMT.
Simona Gandrabur and George Foster. 2003. Con-
fidence estimation for translation prediction. In In
Proceedings of CoNLL-2003, page 102.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10?18.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
378
feat. set #feat class. 10-fold cross train WMT13 test
full 119 LR 45.73 - 73.52 39.74 - 63.92 54.45 - 88.68
full 119 M5Pt 44.49 - 74.05 35.81 - 57.36 50.05 - 85.22
full 119 MLP 48.05 - 75.68 41.03 - 68.70 54.38 - 88.93
full 119 SVM 40.88 - 73.61 34.70 - 69.69 53.74 - 92.26
no pr 100 LR 46.06 - 74.94 40.39 - 66.00 52.13 - 86.68
no pr 100 M5Pt 43.80 - 74.30 36.80 - 59.47 50.86 - 87.42
no pr 100 MLP 47.70 - 75.41 39.85 - 68.30 52.39 - 87.93
no pr 100 SVM 41.35 - 74.68 35.59 - 70.99 52.87 - 92.22
slim 59 LR 44.72 - 73.86 41.14 - 67.44 51.71 - 84.83
slim 59 M5Pt 43.77 - 74.43 35.26 - 56.84 57.75 - 102.68
slim 59 MLP 46.98 - 74.38 40.35 - 69.79 51.06 - 85.48
slim 59 SVM 40.42 - 74.47 36.88 - 71.59 51.09 - 90.18
Table 3: Task 1.3: Results in MAE and RMSE for 10-fold cross validation and the whole training set as
well as the WMT13 blind test set
Almut Silja Hildebrand and Stephan Vogel. 2008.
Combination of machine translation systems via hy-
pothesis selection from combined n-best lists. In
MT at work: Proceedings of the Eighth Confer-
ence of the Association for Machine Translation in
the Americas, pages 254?261, Waikiki, Hawaii, Oc-
tober. Association for Machine Translation in the
Americas.
Almut Silja Hildebrand and Stephan Vogel. 2010.
CMU system combination via hypothesis selec-
tion for WMT?10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 307?310. Association for
Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Erhard Hinrichs and
Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 423?430.
Vladimir Iosifovich Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions, and re-
versals. Soviet Physics Doklady, 10(8):707?710.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Christopher B. Quirk. 2004. Training a sentence-
level machine translation confidence measure. In
Proceedings of the Fourth International Conference
on Language Resources and Evaluation, pages 825?
828, Lisbon, Portugal, May. LREC.
Alberto Sanchis, Alfons Juan, Enrique Vidal, and De-
partament De Sistemes Informtics. 2007. Estima-
tion of confidence measures for machine translation.
In In Procedings of Machine Translation Summit XI.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference for Spoken Language Processing,
Denver, Colorado, September.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33(1):9?40.
379
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 207?216,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Unsupervised Word Segmentation Improves
Dialectal Arabic to English Machine Translation
Kamla Al-Mannai
1
, Hassan Sajjad
1
, Alaa Khader
2
, Fahad Al Obaidli
1
,
Preslav Nakov
1
, Stephan Vogel
1
Qatar Computing Research Institute
1
, Carnegie Mellon University in Qatar
2
{kamlmannai,hsajjad,faalobaidli,pnakov,svogel}@qf.org.qa
1
, akhader@cmu.edu
2
Abstract
We demonstrate the feasibility of using
unsupervised morphological segmentation
for dialects of Arabic, which are poor in
linguistics resources. Our experiments us-
ing a Qatari Arabic to English machine
translation system show that unsupervised
segmentation helps to improve the transla-
tion quality as compared to using no seg-
mentation or to using ATB segmentation,
which was especially designed for Mod-
ern Standard Arabic (MSA). We use MSA
and other dialects to improve Qatari Ara-
bic to English machine translation, and we
show that a uniform segmentation scheme
across them yields an improvement of 1.5
BLEU points over using no segmentation.
1 Introduction
The Arabic language has many varieties, where
the Modern Standard Arabic (MSA) coexists with
various dialects. Dialects differ from MSA and
from each other lexically, phonologically, mor-
phologically and syntactically. MSA has stan-
dard orthography and is used in formal contexts
(e.g., publications, newspaper articles, etc.), while
the dialects are usually limited to daily verbal in-
teractions. However, with the recent rise of social
media, it has become increasingly common to use
dialects in written communication as well, which
has constituted the research in dialectal Arabic
(DA) as a separate field within the broader field
of natural language processing (NLP).
As DA NLP is still in its infancy, there is lack
of basic computational resources and tools, which
are needed in order to apply standard NLP ap-
proaches to the dialects of Arabic. For instance,
statistical approaches need a lot of training data,
which makes it very hard, if not impossible, to
apply them to resource-poor languages; this is
especially true for statistical machine translation
(SMT) of Arabic dialects.
The Arabic language and its dialects are highly
inflectional, and a word can appear in many more
inflected forms compared to English. Consider the
Arabic words

IJ
.
?? ,I
.
??K


,I
.
??

K, and
	
??J
.
??K


: they
all belong to one root word I
.
?? ?playing? /lEb/.
Each morphological variation is derived from a
root word with different affixes addressing differ-
ent functions. This causes data sparseness, and
covering all possible word forms of a root word
may not be always possible. Considering the dif-
ferent variants of Arabic, the problem is exacer-
abated as dialects could use different choices of af-
fixes for the same function. For example, the MSA
word
	
??J
.
??K


/yalEabuwn/, meaning ?they are play-
ing?, could be found as
	
??J
.
??K


/ylEbuwn/ in Gulf,
as @?J
.
??K


?? /Eam yilEabuA/ in Levantine, and as
@?J
.
??J


K
.
/biylEabwA/ in Egyptian Arabic.
One possible solution is to use a morphological
segmenter that segments words into simpler units
such as stems and affixes, which might be covered
in the training set (Zollmann et al., 2006; Tsai et
al., 2010). When applied to dialects, this may re-
duce the lexical gap between dialects and MSA by
matching the common stems. Unfortunately, there
are no standard morphological segmentation tools
for dialects. Due to the difference in morphology,
tools designed for MSA do not work well for di-
alects. Developing rule-based segmenters for each
dialect might appear to be the ideal solution, but,
as the orthography of dialects is not standardized,
crafting linguistic rules for them is very hard.
In this paper, we focus on training an unsuper-
vised model for word segmentation, which we ap-
ply to SMT for a given Arabic dialect. We train a
pre-existing unsupervised segmentation model on
the Arabic side of the training bi-text (and on some
other monolingual data), and then we optimize its
parameters based on the resulting SMT quality.
Similarly, a multi-dialectal word segmenter could
be developed by training on multi-dialectal data.
207
In particular, we develop a Qatari Arabic to En-
glish (QA-EN) SMT system, which we train on a
small pre-existing bi-text. As part of the devel-
opment of the unsupervised segmentation model,
we also collected some additional monolingual
data for Qatari Arabic. Qatari Arabic is a subdi-
alect of the more general Gulf dialect, among with
Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we
collected additional monologual data for each of
these subdialects, and we release this data to the
research community.
We train an unsupervised segmentation tool,
Morphessor, and its MAP model (Creutz and La-
gus, 2007), using different variations of the col-
lected Qatari data. We optimize the single hy-
perparameter of the MAP model by maximizing
the translation quality of the QA-EN SMT sys-
tem in terms of BLEU. Our experimental results
demonstrate that the resulting unsupervised seg-
menter yields improvements in translation quality
when compared to (i) using no segmentation and
(ii) using an MSA-based ATB segmenter.
We further develop a multi-dialectal word seg-
mentation model, which we train on the Arabic
side of the multi-dialectal training data, which
consists of Qatari Arabic, Egyptian Arabic (EGY),
Levantine Arabic (LEV) and MSA to English,
i.e., a scaled combination of all the available par-
allel data. We train a QA-EN SMT system using
the segmented multi-dialectal data, and we show
an absolute gain of 1.5 BLEU points compared to
a baseline that uses no segmentation.
The rest of the paper is organized as follows:
First, we provide an overview of related work on
Dialectal Arabic NLP (Section 2). Next, we dis-
cuss and we illustrate the linguistic differences be-
tween different Arabic dialects in comparison with
and with a focus on Qatari Arabic (Section 3).
Then, we provide statistics about the corpora we
collected and used in our experiments, followed by
an illustration of the orthographic normalization
schemes we applied (Section 4). We next provide
a high-level description of our approach, which
uses morphological segmentation to combine re-
sources for other Arabic dialects in a QA-EN SMT
system effectively (Section 4.3). We also explain
our experimental setup and we present the results
(Section 5). We then discuss translating in the
reverse direction, i.e., into Qatari Arabic (Section
6). Finally, we point to possible directions for fu-
ture work and we conclude the paper (Section 7).
2 Related Work
NLP for DA is still in its early stages of develop-
ment and many challenges need to be overcomed
such as the lack of suitable tools and resources.
Collecting resources for dialectal Arabic:
Several researchers have directed efforts to de-
velop DA computational resources (Maamouri et
al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and
Callison-Burch, 2011; Salama et al., 2014). Zbib
et al. (2012) built two dialectal Arabic-English
parallel corpora for Egyptian and Levantine Ara-
bic using crowdsourcing. Bouamor et al. (2014)
presented a multi-dialectal Arabic parallel corpus,
which covers five Arabic dialects besides MSA
and English. Mubarak and Darwish (2014) col-
lected a multi-dialectal corpus using Twitter. Un-
like previous work, we focus on Gulf subdialects,
particularly Qatari Arabic. The monolingual data
that we collected is a high-quality dialectal re-
source and originates from dialect-specific sources
such as novels and forums.
Adapting SMT resources for other Arabic di-
alects: Many researchers have explored the po-
tential of using MSA as a pivot language for im-
proving SMT of Arabic dialects (Bakr et al., 2008;
Sawaf, 2010; Salloum and Habash, 2011; Sajjad et
al., 2013a; Jeblee et al., 2014). This often involves
DA-MSA conversion schemes as an alternative in
the absence of DA-MSA parallel resources. In
contrast, limited work has been done on lever-
aging available resources for other dialects. Re-
cently, Zbib et al. (2012) have shown that using
a small amount of dialectal data could yield great
improvements for SMT. Here, we investigate the
potential of improving the resource adaptability of
Arabic dialects. Our work is different as we use
an unsupervised segmenter that helps in improv-
ing the lexical overlap between dialects and MSA.
Building morphological segmenters for the
Arabic dialects: Researchers have already fo-
cused efforts on crafting and extending existing
MSA tools to DA by mainly using a set of rules
(Habash et al., 2012). Habash and Rambow
(2006) presented MAGEAD, a knowledge-based
morphological analyzer and generator for Egyp-
tian and Levantine Arabic. Chiang et al. (2006)
developed a Levantine morphological analyzer on
top of an existing MSA analyzer using an explicit
knowledge base.
208
Riesa and Yarowsky (2006) trained a supervised
trie-based model using a small lexicon of dialec-
tal affixes. In our work, we eliminate the need
for linguistic knowledge by training an unsuper-
vised model using available resources. The unsu-
pervised mode of learning allowed us to develop a
multi-dialectal morphological segmenter.
3 Arabic Dialects
In this section, we highlight some of the linguis-
tic differences between Arabic dialects and MSA,
with a focus on the Qatari dialect.
3.1 Phonological Variations
The Gulf dialect often preserves the phonological
representation of MSA, which is not the case with
many other Arabic dialects. For example, in Egyp-
tian (EGY) and in some Levantine (LEV) dialects,
the MSA consonants

H /v/,

? /q/, and
	
X /*/ are
realized as

H /t/, glottal stop /?/, and
	
? /Z/, re-
spectively. While, their MSA pronunciations are
preserved in Gulf Arabic.
In Gulf Arabic, there are some phonological dif-
ferences between countries such as Kuwait (KW),
Saudi Arabia (SA), Bahrain (BH), Qatar (QA),
United Arab Emirates (AE), and Oman (OM).
Here, we focus our discussion on Qatari Arabic,
and we compare it to MSA and other dialects.
The QA dialect borrows two Persian characters
namely h

/J/ and

? /V/. For instance, the MSA
letter h
.
/j/ is converted to /J/ in QA, e.g., ?A?

Jk
.
@

?meeting? is pronounced as /<jtimAE/ in MSA
and /<JtimAE/ in QA. The Persian character h

/J/
is also used in place of ? /k/ in some MSA words
when they are used in QA. For example, ??
?
?
?fish?
/samak/ is pronounced i

?
?
?
/smaJ/ in QA, while
the EGY and the LEV dialects maintain the MSA
pronunciation. The Persian

? /V/ is used to map
the sound of the English letter ?v? in borrowed for-
eign words, e.g., ?K


YJ


	
? ?video? is pronounced as
?K


YJ



? /Viydyw/ as opposed to /fiydywu/; the form
in which it is written in MSA.
The MSA consonant
	
? /D/ is not used in the
QA dialect. It is substituted by
	
? /Z/ in Qatari. For
example, the MSA pronunciation /HaD/ of
	
?k
?to encourage? is transformed to
	
?k /HaZ/ in QA,
but it is maintained in EGY.
Meanwhile, the MSA consonant
	
? /Z/ is re-
alized as /D/ in EGY. For example, the MSA
pronunciation /HaZ/ of
	
?k ?luck? is maintained
in QA and transformed to /HaD/ in EGY. This
change is consistent in all words within each di-
alect. However, such phonological variations be-
tween dialects have the potential to add ambiguity
to dialectal Arabic.
The MSA consonant h
.
/j/ can be used to distin-
guish between different dialects, particularly Gulf
subdialects. h
.
/j/ is pronounced as ?


/y/ in KW,
BH, QA, AE,

? /q/ in OM, much like in EGY,
and h
.
/j/ in SA, much like in LEV. For exam-
ple, the MSA word Yj
.
?? ?mosque? /masjid/ is
pronounced as /masjid/ in MSA, SA, LEV, Y

???
/masqid/ in OM, EGY, YJ


?? /masyid/ in KW,
BH, QA, AE, while the MSA pronunciation is
preserved in SA. This change does not apply to
names. However, we should note that it is not con-
sistent in QA, e.g., the MSA pronunciation of h
.
/j/
in ?J
.
k
.
?mountain? /jabal/ and h
.
QK
.
?tower? /burj/ is
preserved in QA.
3.2 Morphological Variations
In Arabic, a root can produce surface wordforms
by means of inflectional and derivational morpho-
logical processes (Habash, 2010).
An inflectional word form is a variant of a root
word with the same meaning but expressing a dif-
ferent function, e.g., gender, number, case. It is
usually formed by adding a prefix, a suffix, or a
circumfix to a stem word. Note that Arabic di-
alects can make different lexical choices for affix-
ations compared to MSA. For example, the MSA
future prefix ? /s/ is replaced by H
.
/b/ in QA
and by ? /h/ in EGY and LEV. Thus, the MSA
word ??

AJ


? ?he will eat? /say>kul/ becomes ?? AJ


K
.
/biyAkil/ in QA and ?? AJ


? /hayAkul/ in EGY and
LEV.
A derivational word form is formed by applying
a pattern to a root word, e.g., ?player? is derived
from ?play? using the pattern noun + ?er?. An
example of an Arabic derivational form is ??
	
?

K
?do? /tafaE?al/. The root is ??
	
? /faEal/ and it uses
the imperative pattern

H+??
	
?. In EGY, @ /A/ is
added as a prefix; so, it becomes ??
	
?

K@ /AitfaE
?
il/.
209
Meanwhile, the original form is preserved in QA.
Changing the structure of a pattern in a dialect
will result in producing a new dialect-specific or-
thography for every word that is represented by the
structure. For example, the MSA word ??
?

K
?learn?
/taEal?am/ becomes ??
?

K
@ /AitEalim/ in EGY, while
the MSA form is preserved in QA.
3.3 Lexical Variations
Lexical variations are among the most obvious
differences between Arabic dialects. For exam-
ple, the MSA word @
	
XA? ?what? /mA*A/ would be
found as ?

? /$uw/ in LEV, ?K


@

/<yh/ in EGY, and
?
	
J

? /$nuw/ in GLF. We can find lexical variations
in subdialects as well. For example, the MSA
negation word
	
?? /lan/, ?not?, is expressed as I
.
?
/mab/ in QA, as ?? /muw/ in KW, and as I
.
?? /ma-
hab/ in SA.
3.4 Orthographic Variations
Due to the lack of orthographic standardization of
dialectal Arabic, some MSA words can be found
in dialectal text with both MSA and phonologi-
cal spellings. For example, the MSA word

???
g
.
?gathering? /jamEap/ can be also spelled as ???
?

/yamEah/, which is a phonetic variation in QA.
Some dialectal words also vary in spelling due
to variation in their pronunciation, e.g.,
	
??

?

@
/A$uwf/, a QA word meaning ?I see?, can be also
spelled as
	
??k
.
@ /Ajuwf/.
In dialectal Arabic, different orthographic
forms are also possible for entire phrases. For
instance, words followed or preceded by pro-
nouns are commonly reduced to a single word,
e.g., A??

I?

? /glt lahA/ ?I told her? is written as
A??

J?

?. Also, commonly used religious phrases can
be found written as a single unit, e.g., ?<? @ Z A

? A?
/mA $A? A
?
lah/ ?God has willed it? as ?<?A

??.
4 Methodology
In the section, we present some statistics about the
Arabic dialectal data that we have collected. We
processed it to remove orthographic inconsisten-
cies. Then, we used a pre-existing unsupervised
morphological segmenter, Morfessor, in order to
segment the text.
Corpus QCA AVIA
QA
AVIA
O
Sents 14.7 0.9 2
Tokens 115 6.7 15
Table 1: Statistics about the collected parallel cor-
pora (in thousands). AVIA
O
shows the statistics
about the AVIA corpus excluding Qatari data.
4.1 Data Collection
We did an extensive search for available monolin-
gual and bilingual resources for the Gulf dialect,
with a focus on Qatari Arabic. Tables 1 and 2
present some statistics about the corpora we col-
lected. More detailed description follows below.
Bilingual corpora:
? The QCA speech corpus, comprises 14.7k
sentences that are phonetically transcribed from
TV broadcasts in Qatari Arabic and translated to
English; see (Elmahdy et al., 2014) for more de-
tail. The corpus was designed for speech recog-
nition and we faced several normalization-related
issues that we had to resolve before it could be
used for machine translation and language mod-
eling. One example is the usage of five Per-
sian characters to represent some sounds in Ara-
bic words. Moreover, the English side had some
grammatical and spelling errors. We normalized
the Arabic side and corrected the English side of
the corpus as described in Section 4.2. The cor-
pus can be found at http://sprosig.isle.
illinois.edu/corpora/1.
? The AVIA corpus
1
is designed as a refer-
ence source of dialectal Arabic. It consists of 3k
sentences in four Gulf subdialects: Emirati (AE),
Kuwaiti (KW), Qatari (QA), and Hejazi (SA).
2
The data consists of dialectal sentences that con-
tain words commonly used in daily conversation.
Monolingual corpora: We further collected
monolingual corpora consisting of a total of 2.7M
tokens for various Gulf subdialects. The Qatari
part of the data consists of 470K tokens. Most of
the corpus is a collection of novels, belonging to
the romance genre.
3
For the Qatari dialect, we also
collected Qatari forum data.
4
1
http://terpconnect.umd.edu/
?
nlynn/
AVIA/Level3/
2
The website also contains small parallel corpora for
MSA, EGY and LEV to English, but here we focus on Gulf
subdialects only.
3
http://forum.te3p.com/264311-52.html
4
www.qatarshares.com/vb/index.php
210
Corpus Novel Forum
AE BH KW OM QA SA QA
Tokens 573 244 178 372 412 614 69
Types 43 22 27 27 43 71 15
Table 2: Statistics about the collected monolingual
corpora (in thousands of words).
To the best of our knowledge, this is the first
collection of monolingual corpora for Gulf Ara-
bic subdialects. It can be helpful for, e.g., lan-
guage modeling when translating into Arabic, for
learning the similarities and differences between
Gulf subdialects, etc. Table 2 shows some statis-
tics about the data after punctuation tokenization.
4.2 Orthographic Normalization
The inconsistency in the orthographic spelling of
the same word can increase data sparseness. Thus,
we normalize the Arabic text in the collected re-
sources by applying the reduced orthographic nor-
malization scheme, e.g., Tah Marbota is reduced to
Hah. We also normalize extended lines between
letters, e.g., Q?? ?sugar? /sukar/ is changed
to Q??, and we reduce character elongations to
be just two characters long. In order to main-
tain consistency among different resources, we re-
move supplementary diacritics, e.g.,

Y


?

? ?knots?
/Euqad/ is normalized to Y

??, and we map Per-
sian letters to their phonological correspondences
in Qatari Arabic
5
, i.e., ? /G/ to

? /g/,

? /V/ to
	
?
/f/, H

/P/ to H
.
/b/, and

P and h

/J/ to h
.
/j/.
For the English texts, the orthographic varia-
tions were already normalized. However, the En-
glish side of the QCA corpus had some spelling
and grammatical errors, which we corrected man-
ually. On the grammatical side, we only corrected
a subset of the data, which we used for tuning and
testing our SMT system (see Section 5).
4.3 Morphological Decomposition
There is no general Arabic morphological seg-
menter that works for all variations of Arabic. The
most commonly used segmenters for Arabic were
designed for MSA (Habash et al., 2009; Green and
DeNero, 2012). Due to the lexical and morpholog-
ical differences between dialects and MSA, these
MSA-based morphological tools do not work well
for dialects.
5
This issue relates to the QCA corpus.
In this work, we used an unsupervised morpho-
logical segmenter, Morfessor-categories MAP
6
,
an unsupervised model with a single hyper-
parameter (Creutz and Lagus, 2007). We chose
Morfessor because of its superior performance on
Arabic compared to other unsupervised models
(Siivola et al., 2007; Poon et al., 2009).
The model has a single hyperparameter, the per-
plexity threshold parameter B, which controls the
granularity of segmentation. The recommended
value ranges from 1 to 400 where 1 means max-
imum fine-grained segmentation, and 400 restricts
it to the least segmented output. We set the thresh-
old empirically to 70, as shown in Section 5.1.
5 Experimental Setup
We performed an extrinsic evaluation of the varia-
tions in segmentation by building a Qatari Arabic
to English machine translation system on each of
them. We also tested Morfessor on other available
dialects and on MSA, and we will show below how
a uniform segmentation can help to better adapt re-
sources for dialects and MSA for SMT. This sec-
tion describes our experimental setup.
Datasets: We divided the QCA corpus into 1k
sentences each for development and testing, and
we used the remaining 12k for training.
We adapted parallel corpora for Egyptian, Lev-
antine and MSA to English to be used for Qatari
Arabic to English SMT. For MSA, we used par-
allel corpora of TED talks (Cettolo et al., 2012)
and the AMARA corpus (Abdelali et al., 2014),
which consists of educational videos. Since the
QCA corpus is in the speech domain, we believe
that an MSA corpus of spoken domain would be
more helpful than a text domain such as News. For
Egyptian and Levantine, we used the parallel cor-
pus provided by Zbib et al. (2012). There is no
Gulf?English parallel data available in the litera-
ture. The data that we found was a very small col-
lection of subdialects of Gulf Arabic; we did not
use it for MT experiments. However, we used the
Qatari part of the AVIA corpus to train Morfessor.
Machine translation system settings: We used
a phrase-based statistical machine translation
model as implemented in the Moses toolkit
(Koehn et al., 2007) for machine translation.
6
This is an extension of the basic Morfessor method and
is based on a Maximum a Posteriori model.
211
We built separate directed word alignments
for source-to-target and target-to-source using
IBM model 4 (Brown et al., 1993), and we
symmetrized them using the grow-diag-final-and
heuristics (Koehn et al., 2003). We then extracted
phrase pairs with a maximum length of seven, and
we scored them using maximum likelihood esti-
mation with Kneser-Ney smoothing (Kneser and
Ney, 1995). We also built a lexicalized reordering
model, msd-bidirectional-fe. We built a 5-gram
language model on the English side of QCA-train
using KenLM (Heafield, 2011). Finally, we built a
log-linear model using the above features.
We tuned the model weights by optimizing
BLEU (Papineni et al., 2002) on the tuning set, us-
ing PRO (Hopkins and May, 2011) with sentence-
level BLEU+1 optimization (Nakov et al., 2012).
In testing, we used minimum Bayes risk decoding
(Kumar and Byrne, 2004), cube pruning, and the
operation sequence model (Durrani et al., 2011).
Baseline: Our baseline Qatari Arabic to English
MT system is trained on the QCA bitext without
any segmentation of Qatari Arabic. For the exper-
iments described in this paper, we used the English
side of the QCA corpus for language modeling.
5.1 Experimental Results
In this section, we first present our work on using
Morfessor for segmenting Qatari Arabic. We tried
different values of its parameter, and we trained it
using corpora of different sizes to find balanced
settings that improve SMT quality as compared
with no segmentation and with segmentation us-
ing the Stanford ATB segmenter. We further ap-
plied our selected settings to segment MSA, EGY
and LEV and used them for Qatari Arabic to En-
glish machine translation. Our results show that a
uniform segmentation scheme across different di-
alects improves machine translation.
Morfessor training variations: We trained
Morfessor using three corpora: (i) QCA,
(ii) AVIA
QA
plus Qatari Novels, and (iii) a com-
bination thereof. Table 3 shows the results for
our SMT system when trained on the QCA par-
allel corpus, which was segmented using different
training models of Morfessor with B = 40. The
result for segmented Qatari Arabic is always bet-
ter than the baseline, irrespective of the training
model used for segmentation. We can see that the
Morfessor model trained on a large monolingual
corpus, i.e., on (ii) or (iii), yields better results.
Morfessor BLEU OOV%
Baseline 12.2 16.6
QCA 12.5 0.6
AVIA
QA
, Novels 13.5 0.8
QCA, AVIA
QA
, Novels 13.4 0.7
Table 3: Study of the effect of varying the train-
ing datasets for Morfessor on the Qatari to English
SMT. ?Baseline? shows the output of the MT sys-
tem with no segmentation.
B 10 40 70 100 130
BLEU 13.3 13.5 13.8 12.9 12.6
OOV 0.3 0.8 1.4 2.8 2.8
After merging
BLEU 12.5 13.4 13.7 12.8 12.3
OOV 1.5 1.9 3.9 6.5 9.8
Table 4: The effect of varying the perplexity
threshold parameter B of Morfessor on SMT qual-
ity. ?After merging? are the results using the post-
processed Qatari segmented data.
The high reduction in OOV in Table 3 is be-
cause of the fine-grained segmentation. We tried
different values for the perplexity parameter B
in order to find a good balance between better
BLEU scores and linguistically correct segmen-
tations. The first part of Table 4 shows the ef-
fect of different values of B on the quality of the
machine translation system trained on AVIA
QA
,
Qatari Novels. We achieved the best SMT score at
B = 70.
We further analyzed the output of Morfessor
at B = 70 and we noticed that it tends to gener-
ate very small segments of length two and three
characters long. The segmentation produces more
than one stem in a word and does not generate le-
gal word units. For example, the word

??A
	
J??@?
?and the industry? /wAlSinAEp/ is segmented as
PRE/? + PRE/?@ + STM/? + PRE/
	
? + PRE/ @
+ STM/? + SUF/

?. We apply a post-processing
step that merges all stems in a word and affixes
between them to one stem. So, a word can have
only one stem. For example, the word

??A
	
J??@?
would be segmented as PRE/?@? + STM/?A
	
J? +
SUF/

?. This yielded linguistically correct segmen-
tations in many cases. The second part of Table
4 shows the effect of the post-processing on the
BLEU score. We can see that it remains almost
the same with an increase in OOV rate.
212
For rest of the experiments in this paper, we
used a value of 70 for the perplexity threshold
parameter plus the post-processing on segmenta-
tion. We trained Morfessor on the concatenation
of QCA, AVIA
Q
A and Novels.
7
Using other Arabic variations: In this section,
we present experiments using MSA, EGY and
LEV to English bitexts combined with the QCA
bitext for Qatari Arabic to English machine trans-
lation. We explored three segmentation options for
the Arabic side of the data: (i) no segmentation,
(ii) ATB segmentation, and (iii) unsupervised seg-
mentation using Morfessor.
The QCA corpus is of much smaller size com-
pared to other Arabic variants, say MSA. It is pos-
sible that in the training of the machine transla-
tion models, the large corpus dominates the QCA
corpus. In order to avoid that, we balanced the
two corpora by replicating the smaller corpus X
number of times in order to make it approximately
equal to the large corpus (Nakov and Ng, 2009).
8
The complete procedure is described below.
In a nutshell, for building a machine transla-
tion system using the MSA plus Qatari corpus, we
first balanced the Qatari corpus to make it approx-
imately equal to MSA and concatenated them. For
training Morfessor, the Qatari Arabic data con-
sisted of QCA, Novels and AVIA
QA
, while for
SMT, it consisted of QCA only. In both cases,
we balanced it to be approximately equal to MSA.
We then trained Morfessor on the balanced (QCA,
Novels, AVIA
QA
) plus MSA data and we seg-
mented the Arabic side of the balanced QCA plus
MSA training data for machine translation. We
built a machine translation system on the seg-
mented data. We segmented the testing and tuning
data sets similarly. We used the same balancing
when we combined EGY-EN and LEV-EN with
the Qatari Arabic ? English data.
We also tried training multiple unsupervised
models, but this yielded lower SMT quality com-
pared to using a single model trained on multi-
dialects. Using different models could result
in having different segmentation schemes, which
might not help in reducing the vocabulary mis-
match between different variants of Arabic.
7
We did not see a big difference in training Morfessor
with and without the QCA corpus, and we decided to use
the complete data for training.
8
Due to the spoken nature of the QCA corpus, it contains
shorter sentences. Thus, we balanced the corpora based on
the number of tokens rather than on the number of sentences.
Train NONE ATB Morfessor
QCA 12.2 12.9 13.7
?QCA,MSA 12.7 13.3 14.6
?QCA,EGY 13.0 13.5 14.5
?QCA,LEV 13.8 13.7 15.2
Table 5: BLEU scores for Qatari Arabic to English
SMT using three different segmentation settings.
?QCA means the modified QCA corpus with num-
ber of tokens approximately equal to MSA, EGY
and LEV in the respective experiments.
Table 5 shows the results. There are two things
to point here. First, the SMT systems that used
the unsupervised morphological segmenter, Mor-
fessor, outperformed the systems that used no seg-
mentation and those using the ATB segmentation.
The Morfessor-based systems showed consistent
improvements compared to the ATB-based sys-
tems over the no-segmentation systems. This val-
idates our point that unsupervised morphological
segmentation generalizes well for a variety of di-
alects and these SMT results complement that.
The second observation is that adding a bitext for
other dialects and MSA improves machine trans-
lation quality for Qatari?English SMT.
6 Translation into Qatari Arabic
Our monolingual corpora of Gulf subdialects
could be also helpful when translating English into
Qatari Arabic. We conducted a few basic experi-
ments in this direction but without segmentation.
We trained an English to Qatari Arabic SMT
system on the QCA bitext, using the same settings
as described in Section 5. We then normalized the
output of the translation system using the QCRI-
Normalizer (Sajjad et al., 2013b).
9
As a language
model, we used the Arabic side of the QCA cor-
pus, novels and forum data, standalone and to-
gether. Table 6 presents the results of the effect of
varying the language model on the quality of the
SMT system. The best system shows an improve-
ment of 0.22 BLEU points absolute compared to
the baseline system that only uses the Arabic side
of the QCA corpus for LM training.
The SMT system achieved the largest gain when
adding QA forum data to the QCA data. SA and
AE monolingual data also showed good improve-
ments. This might be due to their relatively large
sizes; we need further investigation.
9
http://alt.qcri.org/tools/
213
LM BLEU
QCA 2.78
QCA+QA-Novels 2.64
QCA+QA-Novels+BH-Novels 2.86
QCA+QA-Novels+KW-Novels 2.78
QCA+QA-Novels+AE-Novels 2.92
QCA+QA-Novels+SA-Novels 2.96
QCA+ALL-Novels 2.80
QCA+QA-Novels+QForum 3.00
Table 6: Results for English to Qatari SMT for
varying language models. In all cases, the transla-
tion model is trained on the QCA bitext only.
Note the quite low BLEU scores, especially
compared to the reverse translation direction. One
reason is the morphologically rich nature of Qatari
Arabic, which makes translating into it a hard
problem. The small amount of training data fur-
ther adds to it. We expect to see larger gains com-
pared to Qatari Arabic to English machine transla-
tion when segmentation is used.
7 Conclusion and Future Work
We have demonstrated the feasibility of using
an unsupervised morphological segmenter to in-
crease the resource adaptability of Arabic variants.
We evaluated the segmentation on a Qatari dialect
by building a Qatari Arabic to English machine
translation system. We further adapted MSA,
EGY and LEV in the simplest machine translation
settings and we showed a consistent improvement
of 1.5 BLEU points when compared to the respec-
tive baseline system that uses no segmentation.
In the future, we would like to explore the
impact of segmentation on both the translation
model and the language model when translating
into Qatari Arabic. This involves greater chal-
lenges, as a desegmenter is required for the trans-
lation output with every segmentation scheme.
References
Ahmed Abdelali, Francisco Guzman, Hassan Sajjad,
and Stephan Vogel. 2014. The AMARA corpus:
Building parallel language resources for the educa-
tional domain. In Proceedings of the 9th Interna-
tional Conference on Language Resources and Eval-
uation, Reykjavik, Iceland, May.
Rania Al-Sabbagh and Roxana Girju. 2010. Mining
the web for the induction of a dialectical Arabic lexi-
con. In Proceedings of the 7th International Confer-
ence on Language Resources and Evaluation, Val-
letta, Malta, May.
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for convert-
ing written Egyptian colloquial dialect into dia-
critized Arabic. In Proceedings of the 6th Inter-
national Conference on Informatics and Systems,
Cairo, Egypt, March.
Houda Bouamor, Nizar Habash, and Kemal Oflazer.
2014. A multidialectal parallel corpus of Arabic. In
Proceedings of the 9th edition of the Language Re-
sources and Evaluation Conference, Reykjavik, Ice-
land, May.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2), June.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit
3
: Web inventory of transcribed
and translated talks. In Proceedings of the 16
th
Con-
ference of the European Association for Machine
Translation, Trento, Italy, May.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic
dialects. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics, Trento, Italy, April.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Port-
land, OR, June.
Mohamed Elmahdy, Mark Hasegawa-Johnson, and
Eiman Mustafawi. 2014. Development of a TV
broadcasts speech recognition system for Qatari
Arabic. In Proceedings of the 9th edition of the Lan-
guage Resources and Evaluation Conference, Reyk-
javik, Iceland, May.
Spence Green and John DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, Jeju Island, Korea, July.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
a morphological analyzer and generator for the Ara-
bic dialects. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, Sydney, Australia, July.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, pos
214
tagging, stemming and lemmatization. In Proceed-
ings of the 2nd International Conference on Ara-
bic Language Resources and Tools (MEDAR), Cairo,
Egypt, April.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012. A morphological analyzer for Egyptian Ara-
bic. In Proceedings of the 12th Meeting of the Spe-
cial Interest Group on Computational Morphology
and Phonology, Montreal, Canada, June.
Nizar Y Habash. 2010. Introduction to Arabic natural
language processing. Synthesis Lectures on Human
Language Technologies, 3(1), August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the 6th
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Scotland, UK, July.
Serena Jeblee, Weston Feely, Houda Bouamor, Alon
Lavie, Nizar Habash, and Kemal Oflazer. 2014.
Domain and Dialect Adaptation for Machine Trans-
lation into Egyptian Arabic. In Proceedings of
the Arabic Natural Language Processing Workshop,
Doha, Qatar, October.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for ngram langauge modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, Detroit,
Michigan, May.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguis-
tics Conference, Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Demonstra-
tion Program, Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
Boston, MA, May.
Mohamed Maamouri, Ann Bies, Tim Buckwalter,
Mona Diab, Nizar Habash, Owen Rambow, and
Dalila Tabessi. 2006. Developing and using a pi-
lot dialectal Arabic treebank. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, Genova, Italy, May.
Hamdy Mubarak and Kareem Darwish. 2014. Using
Twitter to collect a multi-dialectal corpus of Arabic.
In Proceedings of the Arabic Natural Language Pro-
cessing Workshop, Doha, Qatar, October.
Preslav Nakov and Hwee Tou Ng. 2009. Improved
statistical machine translation for resource-poor lan-
guages using related resource-rich languages. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Suntec,
Singapore, August.
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proceedings of the
24th International Conference on Computational
Linguistics, Mumbai, India, December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th annual meeting on association for com-
putational linguistics, Philadelphia, PA, July.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics, Denver, CO,
June.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with appli-
cations to machine translation. In Proceedings of
the 7th Conference of the Association for Machine
Translation in the Americas, MA, USA, August.
Hassan Sajjad, Kareem Darwish, and Yonatan Be-
linkov. 2013a. Translating dialectal Arabic to En-
glish. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
Sofia, Bulgaria, August.
Hassan Sajjad, Francisco Guzman, Preslav Nakov,
Ahmed Abdelali, Kenton Murray, Fahad Al Obaidli,
and Stephan Vogel. 2013b. QCRI at IWSLT 2013:
Experiments in Arabic-English and English-Arabic
Spoken Language Translation. In Proceedings of the
10th International Workshop on Spoken Language
Translation, Hiedelberg, Germany, December.
Ahmed Salama, Houda Bouamor, Behrang Mohit, and
Kemal Oflazer. 2014. YouDACC: the youtube di-
alectal Arabic commentary corpus. In Proceedings
of the 9th edition of the Language Resources and
Evaluation Conference, Reykjavik, Iceland, May.
Wael Salloum and Nizar Habash. 2011. Dialectal
to standard Arabic paraphrasing to improve Arabic-
English statistical machine translation. In Proceed-
ings of the First Workshop on Algorithms and Re-
sources for Modelling of Dialects and Language Va-
rieties, Edinburgh, Scotland, July.
215
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Con-
ference of the Association for Machine Translation
in the Americas, Denver, CO, October.
Vesa Siivola, Mathias Creutz, and Mikko Kurimo.
2007. Morfessor and VariKN machine learning
tools for speech and language technology. In
Proceedings of the 8th International Conference
on Speech Communication and Technology (Inter-
speech), Antwerpen, Belgium, August.
Ming-Feng Tsai, Preslav Nakov, and Hwee Tou Ng.
2010. Morphological analysis for resource-poor
machine translation. Technical report, Kent Ridge,
Singapore, December.
Omar F Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers-Volume
2, Portland, OR, June.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine translation of Arabic di-
alects. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Montreal, Canada, June.
Andreas Zollmann, Ashish Venugopal, and Stephan
Vogel. 2006. Bridging the inflection morphol-
ogy gap for Arabic statistical machine translation.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, New York, NY, June.
216
