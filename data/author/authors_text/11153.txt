Coling 2008: Companion volume ? Posters and Demonstrations, pages 169?172
Manchester, August 2008
Multilingual Assistant for Medical Diagnosing and Drug Prescription
Based on Category Ranking
Fernando Ruiz-Rico
University of Alicante
frr@alu.ua.es
Jose-Luis Vicedo
University of Alicante
vicedo@dlsi.ua.es
Mar??a-Consuelo Rubio-Sa?nchez
University of Alicante
mcrs7@alu.ua.es
Abstract
This paper presents a real-world applica-
tion for assisting medical diagnosis and
drug prescription, which relies on the
exclusive use of machine learning tech-
niques. We have automatically processed
an extensive biomedical literature to train
a categorization algorithm in order to pro-
vide it with the capability of matching
symptoms to MeSH descriptors. To in-
teract with the classifier, we have devel-
oped a multilingual web interface so that
professionals in medicine can easily get
some help in their decisions about di-
agnoses (lookfordiagnosis.com) and pre-
scriptions (lookfortherapy.com). We also
demonstrate the effectiveness of this ap-
proach with a test set containing several
hundreds of real clinical histories.
1 Introduction
Text categorization consists of automatically as-
signing documents to pre-defined classes. It has
been extensively applied to many fields and in par-
ticular, some efforts have been focused on MED-
LINE abstracts classification (Ibushi and Tsujii,
1999). However, as far as we are concerned, it
has never been used to assist multilingual medical
diagnosing and drug prescription by using the tex-
tual information provided by biomedical literature
together with patient histories.
Every year, thousands of documents are added
to the National Library of Medicine and the Na-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
tional Institutes of Health databases1 . Most of
them have been manually indexed by assigning
each document to one or several entries in a con-
trolled vocabulary called MeSH2 (Medical Subject
Headings). The MeSH tree is a hierarchical struc-
ture of medical terms which are used to define the
main subjects that a medical article or report is
about. Due to the wide use of this terminology, we
can find translations into several languages such as
Portuguese and Spanish (i.e. DeCS3 - Health Sci-
ence Descriptors). This paper focuses on both the
diseases sub-tree (from C01 to C23) and drugs sub-
tree (from D01 to D20). The first one defines on its
own more than 4,000 pathological states, and also
offers the chance to search for documented case re-
ports related to each of them. The drugs sub-tree
provides the capability of arranging around 8,000
active principles, which can be directly matched to
commercial drugs.
Our proposal tries to estimate a ranked list of di-
agnoses and possible prescriptions from a patient
history. To tackle this problem, we have selected
an existing categorization algorithm, and we have
trained it using the textual information provided
by lots of previously reported cases and labora-
tory findings. This way, a detailed symptomatic
description is sufficient to obtain a list of possible
diseases and prescriptions, along with an estima-
tion of probabilities and bibliography.
We have not used binary decisions from binary
categorization methods, since they might leave
some interesting MeSH entries out, which should
probably be taken into consideration. Instead, we
have chosen a category ranking algorithm to obtain
an ordered list of all possible diagnoses and pre-
1http://www.pubmed.gov
2http://www.nlm.nih.gov/mesh
3http://decs.bvs.br/I/homepagei.htm
169
scriptions so that the user can finally decide which
of them better suits the clinical history.
In this paper, first of all, we will explain the way
we have developed our experiments, including a
full description of the sources and methods used to
get both training and test data. Secondly, we will
provide an example of a patient history and both
the expected and provided diagnoses. We will also
show the suggested drugs recommended by the al-
gorithm for a common disease. And we will finish
by showing and commenting several evaluation re-
sults on.
2 Procedures
2.1 Medical Diagnosis
We have extracted the training data from the
PubMed database1 by selecting every case re-
ports on diseases written in English including ab-
stract and related to human beings. These docu-
ments were extracted by using the ?diseases cat-
egory[MAJR]? query, where [MAJR] stands for
?MeSH Major Topic?, asking the system for re-
trieving only documents whose subject is mainly a
disease. The query provided us with 483,726 doc-
uments4 leading us to 4,024 classes with at least
one training sample each.
With respect to the test set, we have used 400
medical histories from the School of Medicine
of the University of Pittsburgh (Department of
Pathology5). Although, so far the web page con-
tains more than 500 histories4, not all of them are
suitable for our purposes. There are some which
do not provide a concrete diagnosis but only a dis-
cussion about the case, and some others do not
have a direct matching to the MeSH tree. We
have used from each document both the title and
all the clinical history, including radiological find-
ings, gross and microscopic descriptions, etc. To
get the expected output, we extracted the top level
MeSH diseases categories corresponding to the di-
agnoses given on the titles of the ?final diagnosis?
files (dx.html).
As the ranking algorithm, we have chosen the
Sum of Weights (SOW) approach (Ruiz-Rico et
al., 2006), that is more suitable than the rest for its
efficiency, accuracy and incremental training ca-
pacity. Since medical databases are frequently up-
dated and they also grow continuously, we have
preferred using a fast and unattended approach
4Data obtained on February 14th 2007
5http://path.upmc.edu/cases
that lets us perform updates easily with no sub-
stantial performance degradation after increment-
ing the number of categories or training samples.
The restrictive complexity of other classifiers such
as SVM could derivate to an intractable problem,
as stated by (Ruch, 2005).
To evaluate how worth our suggestion is, we
have measured accuracy through three common
ranking performance measures (Ruiz-Rico et al,
2006): Precision at recall = 0 (Pr=0), mean aver-
age precision (AvgP) and Precision/Recall break
even point (BEP). Sometimes, only one diagno-
sis is valid for a particular patient. In these cases,
Pr=0 let us quantify the mistaken answers, since it
indicates the proportion of correct topics given at
the top ranked position. To know about the qual-
ity of the full ranking list, we use the AvgP, since
it goes down the arranged list averaging precision
until all possible answers are covered. BEP is the
value where precision equals recall, that is, when
we consider the maximum number of relevant top-
ics as a threshold. To follow the same procedure as
(Joachims, 1998), the performance evaluation has
been computed over the top diseases level.
2.2 Drug Prescription
Multilingual drug prescription can be achieved
through the international active principles, which
are the constituents of drugs on which the charac-
teristic therapeutic action of the substance largely
depends. The appropriate nomenclature for the ac-
tive principles can be found translated to several
languages from MeSH, and can lead to the final
commercial medicaments in most of the countries
around the world.
To train the algorithm for this new purpose, we
have launched the following query to the PubMed
database:
(?Plant Families and Groups?[majr] OR ?Inorganic
Chemicals?[majr] OR ?Organic Chemicals?[majr] OR
?Heterocyclic Compounds?[majr] OR ?Polycyclic Com-
pounds?[majr] OR ?Macromolecular Substances?[majr]
OR ?Hormones, Hormone Substitutes, and Hormone An-
tagonists?[majr] OR ?Enzymes and Coenzymes?[majr] OR
?Carbohydrates? OR ?Lipids?[majr] OR ?Amino Acids, Pep-
tides, and Proteins?[majr] OR ?Nucleic Acids, Nucleotides,
and Nucleosides?[majr] OR ?Complex Mixtures?[majr])
AND ?therapeutic use?[sh] NOT (?adverse effects?[sh] OR
?contraindications?[sh] OR ?poisoning?[sh] OR ?radiation
effects?[sh] OR ?toxicity?[sh])
After filtering only articles written in English
which have abstract, a total amount of 540,2354
training documents are left.
170
Figure 1: Example of the first level of a hierarchical diagnosis
2.3 Multilingual Environment
Since all training data is written in English, ev-
ery symptom provided to the algorithm must also
be written in English. For this purpose, an au-
tomatic translation tool is used for input data in
other languages than English. We also promote the
translation by using the MeSH vocabulary, which
has been delivered by human experts, and pro-
vides a reliable correspondence of thousands of
non phrases in many language pairs. Although
the automatic translation method is not accurate
enough for natural speaking, it may be capable
of giving quite good results for independent noun
phrases (Ruiz-Rico et al, 2006), which are the
pieces of information the ranking algorithm uses.
2.4 Availability and Requirements
No special hardware nor software is neces-
sary to interact with the assistant. Just an
Internet connection and a standard browser
are enough to access on-line through the fol-
lowing sites: www.lookfordiagnosis.com and
www.lookfortherapy.com.
By using a web interface and by presenting re-
sults in text format, we allow users to access from
many types of portable devices (laptops, PDA?s,
etc.). Moreover, they will always have available
the latest version, with no need of installing spe-
cific applications nor software updates.
3 A Couple of Examples
3.1 Medical Diagnosis
One of the 400 histories included in the test set
looks as follows:
Case 177 ? Headaches, Lethargy and a Sel-
lar/Suprasellar Mass
A 16 year old female presented with two months
of progressively worsening headaches, lethargy
and visual disturbances. Her past medical his-
tory included developmental delay, shunted hydro-
cephalus, and tethered cord release ...
The final diagnosis expected for this clinical his-
tory is: ?Rathke?s Cleft Cyst?, which is a syn-
Figure 2: Output example after manual expansion of high
ranked topics (up) and by selecting the flat diagnosis mode
(down)
rheumatoid arthritis
Figure 3: Example of the drug prescription suggestions for
rheumatoid arthritis (up) and the final medicament (down)
found through the drugs link provided by the assistant.
171
onym of the preferred term ?Central Nervous Sys-
tem Cysts?. Translating this into one or several
of the 23 top MeSH diseases categories we are
led to the following entries: ?Neoplasms?, ?Ner-
vous System Diseases? and ?Congenital, Heredi-
tary, and Neonatal Diseases and Abnormalities?.
In hierarchical mode, our approach provides au-
tomatically a first categorization level with ex-
panding possibilities as shown in figure 1. We pro-
vide navigation capabilities to allow the user to go
down the tree by selecting different branches, de-
pending on the given probabilities and his/her own
criteria. Moreover, a flat diagnosis mode can be
activated to directly obtain a ranked list of all dis-
eases, as shown on the lower part of figure 2.
After an individual evaluation of this case, we
have obtained the following values: Pr=0 = 1,
AvgP= 0.92, and BEP= 0.67, since the right top-
ics in figure 1 are given at positions 1, 2 and 4.
3.2 Drug Prescription
As an example for drug prescription, figure 3
shows the suggestions that the ranking algorithm
provides for rheumatoid arthritis, where the user
obtains a ranked list of active principles. Fi-
nally, we reach the name of one of the possible
medicaments containing the selected active princi-
ple, along with particular recommendations from
pharmacists (secondary effects, etc).
4 Results
Last row in table 1 shows the performance mea-
sures calculated for each medical history and its
diagnosis, averaged afterwards across all the 400
decisions. Pr=0 indicates that we get 69% of the
histories correctly diagnosed with the top ranked
MeSH entry. AvgP value means that the rest of the
list also contains quite valid topics, since it reaches
a value of 73%.
First row in table 1 provides a comparison be-
tween SVM (Joachims, 1998) and sum of weights
(Ruiz-Rico et al, 2006) algorithms using the well
known OHSUMED evaluation benchmark. Even
using a training and test set containing different
document types, BEP indicates that the perfor-
mance is not far away from that achieved in text
classification tasks, meaning that category ranking
can also be effectively applied to our scenario.
Regarding drug prescription tests, we are still
working under the evaluation process, colaborat-
ing with companies such as CMPMedica, which
Table 1: Averaged performance for both text categorization
and diagnosis
Corpus Algor. Pr=0 AvgP BEP
OHSUMED SVM - - 0.66
SOW - - 0.71
Case reports and
patient histories SOW 0.69 0.73 0.62
is in charge of many sites containing drug com-
pendiums (vademecum.es, vidal.fr, cddata.co.uk,
etc.). We have already performed preliminary tests
by using the symptoms and diseases in the MeSH
tree as the input data, and an arranged list of active
principles as the output data. We have reached an
AvgP around 0.9.
5 Conclusions and Further Work
We believe that category ranking algorithms may
help in multilingual medical diagnosing and drug
prescription from clinical histories. Although the
output of the categorization process should not be
directly taken as a medical advice, the accuracy
achieved could be good enough to assist human ex-
perts. However, due to the large amount of new ar-
ticles continuously added to biomedical literature,
it becomes quite difficult for a practitioner to keep
up to date. Further works are focused on providing
bibliographic references for each suggestion of the
classifier. We pretend to select from the PubMed
database those entries most related to the patholog-
ical states entered by the user.
References
Ibushi, Katsutoshi, Collier-Nigel and Jun?ichi Tsujii.
1999. Classification of medline abstracts. Genome
Informatics, volume 10, pages 290?291.
Joachims, Thorsten. 1998. Text categorization with
support vector machines: learning with many rel-
evant features. In Proceedings of ECML-98, 10th
European Conference on Machine Learning, pages
137?142.
Ruch, Patrick. 2005. Automatic assignment of
biomedical categories: toward a generic approach.
Bioinformatics, volume 22 no. 6 2006, pages 658?
664.
Ruiz-Rico, Fernando, Jose Luis Vicedo, and Mar??a-
Consuelo Rubio-Sa?nchez. 2006. Newpar: an au-
tomatic feature selection and weighting schema for
category ranking. In Proceedings of DocEng-06, 6th
ACM symposium on Document engineering, pages
128?137.
172
Importance of Pronominal Anaphora resolution in Question
Answering systems
Jose L. Vicedo and Antonio Ferrandez
Departamento de Lenguajes y Sistemas Informaticos
Universidad de Alicante
Apartado 99. 03080 Alicante, Spain
fvicedo,antoniog@dlsi.ua.es
Abstract
The main aim of this paper is
to analyse the eects of applying
pronominal anaphora resolution to
Question Answering (QA) systems.
For this task a complete QA system
has been implemented. System eval-
uation measures performance im-
provements obtained when informa-
tion that is referenced anaphorically
in documents is not ignored.
1 Introduction
Open domain QA systems are dened as
tools capable of extracting the answer to
user queries directly from unrestricted do-
main documents. Or at least, systems that
can extract text snippets from texts, from
whose content it is possible to infer the an-
swer to a specic question. In both cases,
these systems try to reduce the amount of
time users spend to locate a concrete infor-
mation.
This work is intended to achieve two princi-
pal objectives. First, we analyse several docu-
ment collections to determine the level of in-
formation referenced pronominally in them.
This study gives us an overview about the
amount of information that is discarded when
these references are not solved. As second ob-
jective, we try to measure improvements of
solving this kind of references in QA systems.
With this purpose in mind, a full QA system
has been implemented. Benets obtained by
solving pronominal references are measured
by comparing system performance with and
without taking into account information ref-
erenced pronominally. Evaluation shows that
solving these references improves QA perfor-
mance.
In the following section, the state-of-the-
art of open domain QA systems will be sum-
marised. Afterwards, importance of pronom-
inal references in documents is analysed.
Next, our approach and system components
are described. Finally, evaluation results are
presented and discussed.
2 Background
Interest in open domain QA systems is quite
recent. We had little information about this
kind of systems until the First Question An-
swering Track was held in last TREC confer-
ence (TRE, 1999). In this conference, nearly
twenty dierent systems were evaluated with
very dierent success rates. We can clas-
sify current approaches into two groups: text-
snippet extraction systems and noun-phrase
extraction systems.
Text-snippet extraction approaches are
based on locating and extracting the most rel-
evant sentences or paragraphs to the query by
supposing that this text will contain the cor-
rect answer to the query. This approach has
been the most commonly used by participants
in last TREC QA Track. Examples of these
systems are (Moldovan et al, 1999) (Singhal
et al, 1999) (Prager et al, 1999) (Takaki,
1999) (Hull, 1999) (Cormack et al, 1999).
After reviewing these approaches, we can
notice that there is a general agreement
about the importance of several Natural Lan-
guage Processing (NLP) techniques for QA
task. Pos-tagging, parsing and Name En-
tity recognition are used by most of the sys-
tems. However, few systems apply other NLP
techniques. Particularly, only four systems
model some coreference relations between en-
tities in the query and documents (Morton,
1999)(Breck et al, 1999) (Oard et al, 1999)
(Humphreys et al, 1999). As example, Mor-
ton approach models identity, denite noun-
phrases and non-possessive third person pro-
nouns. Nevertheless, benets of applying
these coreference techniques have not been
analysed and measured separately.
The second group includes noun-phrase ex-
traction systems. These approaches try to
nd the precise information requested by
questions whose answer is dened typically by
a noun phrase.
MURAX is one of these systems (Kupiec,
1999). It can use information from dierent
sentences, paragraphs and even dierent doc-
uments to determine the answer (the most rel-
evant noun-phrase) to the question. However,
this system does not take into account the
information referenced pronominally in docu-
ments. Simply, it is ignored.
With our system, we want to determine the
benets of applying pronominal anaphora res-
olution techniques to QA systems. Therefore,
we apply the developed computational sys-
tem, Slot Unication Parser for Anaphora res-
olution (SUPAR) over documents and queries
(Ferrandez et al, 1999). SUPAR's architec-
ture consists of three independent modules:
lexical analysis, syntactic analysis, and a reso-
lution module for natural language processing
problems, such as pronominal anaphora.
For evaluation, a standard based IR system
and a sentence-extraction QA system have
been implemented. Both are based on Salton
approach (1989). After IR system retrieves
relevant documents, our QA system processes
these documents with and without solving
pronominal references in order to compare -
nal performance.
As results will show, pronominal anaphora
resolution improves greatly QA systems per-
formance. So, we think that this NLP tech-
nique should be considered as part of any
open domain QA system.
3 Importance of pronominal
information in documents
Trying to measure the importance of informa-
tion referenced pronominally in documents,
we have analysed several text collections used
for QA task in TREC-8 Conference as well
as others used frequently for IR system test-
ing. These collections were the following: Los
Angeles Times (LAT), Federal Register (FR),
Financial Times (FT), Federal Bureau Infor-
mation Service (FBIS), TIME, CRANFIELD,
CISI, CACM, MED and LISA. This analy-
sis consists on determining the amount and
type of pronouns used, as well as the number
of sentences containing pronouns in each of
them. As average measure of pronouns used
in a collection, we use the ratio between the
quantity of pronouns and the number of sen-
tences containing pronouns. This measure ap-
proximates the level of information that is ig-
nored if these references are not solved. Fig-
ure 1 shows the results obtained in this anal-
ysis.
As we can see, the amount and type of pro-
nouns used in analysed collections vary de-
pending on the subject the documents talk
about. LAT, FBIS, TIME and FT collections
are composed from news published in dier-
ent newspapers. The ratio of pronominal ref-
erence used in this kind of documents is very
high (from 35,96% to 55,20%). These doc-
uments contain a great number of pronomi-
nal references in third person (he, she, they,
his, her, their) whose antecedents are mainly
people's names. In this type of documents,
pronominal anaphora resolution seems to be
very necessary for a correct modelling of rela-
tions between entities. CISI and MED collec-
tions appear ranked next in decreasing ratio
level order. These collections are composed
by general comments about document man-
aging, classication and indexing and doc-
uments extracted from medical journals re-
spectively. Although the ratio presented by
these collections (24,94% and 22,16%) is also
high, the most important group of pronominal
references used in these collections is formed
by "it" and "its" pronouns. In this case,
TEXT COLLECTION LAT FBIS TIME FT CISI MED CACM LISA FR CRANFIELD
Pronoun type
HE, SHE, THEY 38,59% 29,15% 31,20% 26,20% 15,38% 15,07% 8,59% 12,24% 13,31% 6,54%
HIS, HER, THEIR 25,84% 21,54% 35,01% 20,52% 22,96% 21,46% 15,69% 31,03% 20,70% 10,35%
IT, ITS 26,92% 39,60% 22,43% 46,68% 52,11% 57,41% 67,61% 47,86% 61,06% 79,76%
HIM, THEM 7,04% 7,08% 7,82% 4,44% 6,38% 3,96% 4,87% 6,30% 3,45% 1,60%
HIM, HER,IT(SELF), THEMSELVES 1,61% 2,63% 3,54% 2,17% 3,17% 2,10% 3,25% 2,57% 1,48% 1,75%
Pronouns in Sentences
Containing 0  pronouns 44,80% 48,09% 51,37% 64,04% 75,06% 77,84% 79,06% 83,79% 84,92% 90,95%
Containing 1 pronoun 30,40% 31,37% 29,46% 23,07% 17,17% 15,02% 17,54% 13,01% 11,64% 8,10%
Containing 2 pronouns 14,94% 12,99% 12,26% 8,54% 5,27% 4,75% 2,79% 2,56% 2,57% 0,85%
Containing +2 pronouns 9,86% 7,55% 6,90% 4,34% 2,51% 2,39% 0,60% 0,64% 0,88% 0,09%
Ratio of pronominal reference 55,20% 51,91% 48,63% 35,96% 24,94% 22,16% 20,94% 16,21% 15,08% 9,05%
Figure 1: Pronominal references in text collections
antecedents of these pronominal references
are mainly concepts represented typically by
noun phrases. It seems again important solv-
ing these references for a correct modelling
of relations between concepts expressed by
noun-phrases. The lowest ratio results are
presented by CRANFIELD collection with a
9,05%. The reason of this level of pronominal
use is due to text contents. This collection is
composed by extracts of very high technical
subjects. Between the described percentages
we nd the CACM, LISA and FR collections.
These collections are formed by abstracts and
documents extracted from the Federal Regis-
ter, from the CACM journal and from Library
and Information Science Abstracts, respec-
tively. As general behaviour, we can notice
that as more technical document contents be-
come, the pronouns "it" and "its" become the
most appearing in documents and the ratio
of pronominal references used decreases. An-
other observation can be extracted from this
analysis. Distribution of pronouns within sen-
tences is similar in all collections. Pronouns
appear scattered through sentences contain-
ing one or two pronouns. Using more than
two pronouns in the same sentence is quite
infrequent.
After analysing these results an important
question may arise. Is it worth enough to
solve pronominal references in documents? It
would seem reasonable to think that resolu-
tion of pronominal anaphora would only be
accomplished when the ratio of pronominal
occurrence exceeds a minimum level. How-
ever, we have to take into account that the
cost of solving these references is proportional
to the number of pronouns analysed and con-
sequently, proportional to the amount of in-
formation a system will ignore if these refer-
ences are not solved.
As results above state, it seems reason-
able to solve pronominal references in queries
and documents for QA tasks. At least, when
the ratio of pronouns used in documents rec-
ommend it. Anyway, evaluation and later
analysis (section 5) contribute with empiri-
cal data to conclude that applying pronom-
inal anaphora resolution techniques improve
QA systems performance.
4 Our Approach
Our system is made up of three modules. The
rst one is a standard IR system that retrieves
relevant documents for queries. The second
module will manage with anaphora resolution
in both, queries and retrieved documents. For
this purpose we use SUPAR computational
system (section 4.1). And the third one is
a sentence-extraction QA system that inter-
acts with SUPAR module and ranks sentences
from retrieved documents to locate the an-
swer where the correct answer appears (sec-
tion 4.2).
For the purpose of evaluation an IR sys-
tem has been implemented. This system is
based on the standard information retrieval
approach to document ranking described in
Salton (1989). For QA task, the same ap-
proach has been used as baseline but using
sentences as text unit. Each term in the query
and documents is assigned an inverse docu-
ment frequency (idf ) score based on the same
corpus. This measure is computed as:
idf(t) = log(
N
df(t)
) (1)
where N is the total number of documents
in the collection and df(t) is the number of
documents which contains term t. Query ex-
pansion consists of stemming terms using a
version of the Porter stemmer. Document and
sentence similarity to the query was computed
using the cosine similarity measure. The LAT
corpus has been selected as test collection due
to his high level of pronominal references.
4.1 Solving pronominal anaphora
In this section, the NLP Slot Unication
Parser for Anaphora Resolution (SUPAR)
is briey described (Ferrandez et al, 1999;
Ferrandez et al, 1998). SUPAR's architec-
ture consists of three independent modules
that interact with one other. These modules
are lexical analysis, syntactic analysis, and a
resolution module for Natural Language Pro-
cessing problems.
Lexical analysis module. This module
takes each sentence to parse as input, along
with a tool that provides the system with all
the lexical information for each word of the
sentence. This tool may be either a dictio-
nary or a part-of-speech tagger. In addition,
this module returns a list with all the neces-
sary information for the remaining modules
as output. SUPAR works sentence by sen-
tence from the input text, but stores informa-
tion from previous sentences, which it uses in
other modules, (e.g. the list of antecedents of
previous sentences for anaphora resolution).
Syntactic analysis module. This mod-
ule takes as input the output of lexical analy-
sis module and the syntactic information rep-
resented by means of grammatical formalism
Slot Unication Grammar (SUG). It returns
what is called slot structure, which stores all
necessary information for following modules.
One of the main advantages of this system is
that it allows carrying out either partial or
full parsing of the text.
Module of resolution of NLP prob-
lems. In this module, NLP problems
(e.g. anaphora, extra-position, ellipsis or PP-
attachment) are dealt with. It takes the slot
structure (SS) that corresponds to the parsed
sentence as input. The output is an SS in
which all the anaphors have been resolved. In
this paper, only pronominal anaphora resolu-
tion has been applied.
The kinds of knowledge that are going to
be used in pronominal anaphora resolution in
this paper are: pos-tagger, partial parsing,
statistical knowledge, c-command and mor-
phologic agreement as restrictions and several
heuristics such as syntactic parallelism, pref-
erence for noun-phrases in same sentence as
the pronoun preference for proper nouns.
We should remark that when we work with
unrestricted texts (as it occurs in this paper)
we do not use semantic knowledge (i.e. a
tool such as WorNet). Presently, SUPAR re-
solves both Spanish and English pronominal
anaphora with a success rate of 87% and 84%
respectively.
SUPAR pronominal anaphora resolution
diers from those based on restrictions and
preferences, since the aim of our preferences
is not to sort candidates, but rather to dis-
card candidates. That is to say, preferences
are considered in a similar way to restrictions,
except when no candidate satises a prefer-
ence, in which case no candidate is discarded.
For example in sentence: "Rob was asking us
about John. I replied that Peter saw John yes-
terday. James also saw him." After applying
the restrictions, the following list of candi-
dates is obtained for the pronoun him: [John,
Peter, Rob], which are then sorted according
to their proximity to the anaphora. If pref-
erence for candidates in same sentence as the
anaphora is applied, then no candidate satis-
es it, so the following preference is applied on
the same list of candidates. Next, preference
for candidates in the previous sentence is ap-
plied and the list is reduced to the following
candidates: [John, Peter ]. If syntactic par-
allelism preference is then applied, only one
candidate remains, [John], which will be the
antecedent chosen.
Each kind of anaphora has its own set of
restrictions and preferences, although they all
follow the same general algorithm: rst come
the restrictions, after which the preferences
are applied. For pronominal anaphora, the
set of restrictions and preferences that apply
are described in Figure 2.
Procedure SelectingAntecedent ( INPUT L: ListOfCandidates,
                     OUTPUT Solution: Antecedent )
Apply restrictions to L with a result of L1
Morphologic agreement
C-command constraints
Semantic consistency
Case of:
NumberOfElements (L1) = 1
Solution = TheFirstOne (L1)
NumberOfElements (L1) = 0
Exophora or cataphora
NumberOfElements (L1) > 1
Apply preferences to L1 with a result of L2
1) Candidates in the same sentence as anaphor.
2) Candidates in the previous sentence
3) Preference for proper nouns.
4) Candidates in the same position as the anaphor
with reference to the verb (before or after).
5) Candidates with the same number of parsed
constituents as the anaphora
6) Candidates that have appeared with the verb of
the anaphor more than once
7) Preference for indefinite NPs.
Case of:
NumberOfElements (L2) = 1
       Solution = TheFirstOne (L2)
NumberOfElements (L2) > 1
Extract from L2 in L3 those candidates that have
been repeated most in the text
If NumberOfElements (L3) > 1
Extract from L3 in L4 those candidates that
have appeared most with the verb of the
anaphora
Solution = TheFirstOne (L4)
Else
Solution = TheFirstOne (L3)
EndIf
EndCase
EndCase
EndProcedure
Figure 2: Pronominal anaphora resolution al-
gorithm
The following restrictions are rst applied
to the list of candidates: morphologic agree-
ment, c-command constraints and semantic
consistency. This list is sorted by proximity to
the anaphor. Next, if after applying restric-
tions there is still more than one candidate,
the preferences are then applied, in the order
shown in this gure. This sequence of prefer-
ences (from 1 to 7 ) stops when, after having
applied a preference, only one candidate re-
mains. If after applying preferences there is
still more than one candidate, then the most
repeated candidates
1
in the text are extracted
from the list after applying preferences. After
this is done, if there is still more than one can-
didate, then those candidates that have ap-
peared most frequently with the verb of the
anaphor are extracted from the previous list.
Finally, if after having applied all the previ-
ous preferences, there is still more than one
candidate left, the rst candidate of the re-
sulting list, (the closest one to the anaphor),
is selected.
4.2 Anaphora resolution and QA
Our QA approach provides a second level of
processing for relevant documents: Analysing
matching documents and Sentence ranking.
Analysing Matching Documents. This
step is applied over the best matching docu-
ments retrieved from the IR system. These
documents are analysed by SUPAR module
and pronominal references are solved. As re-
sult, each pronoun is associated with the noun
phrase it refers to in the documents. Then,
documents are split into sentences as basic
text unit for QA purposes. This set of sen-
tences is sent to the sentence ranking stage.
Sentence Ranking. Each term in the
query is assigned a weight. This weight is
the sum of inverse document frequency mea-
sure of terms based on its occurrence in the
LAT collection described earlier. Each docu-
ment sentence is weighted the same way. The
only dierence with baseline is that pronouns
are given the weight of the entity they refer
to. As we only want to analyse the eects
of pronominal reference resolution, no more
changes are introduced in weighting scheme.
For sentence ranking, cosine similarity is used
between query and document sentences.
5 Evaluation
For this evaluation, several people unac-
quainted with this work proposed 150 queries
1
Here, we mean that rstly we obtain the maxi-
mum number of repetitions for an antecedent in the
remaining list. After that, we extract from that list
the antecedents that have this value of repetition.
whose correct answer appeared at least once
into the analysed collection. These queries
were also selected based on their expressing
the user's information need clearly and their
being likely answered in a single sentence.
First, relevant documents for each query
were retrieved using the IR system described
earlier. Only the best 50 matching docu-
ments were selected for QA evaluation. As
the document containing the correct answer
was included into the retrieved sets for only
93 queries (a 62% of the proposed queries),
the remaining 57 queries were excluded for
this evaluation.
Once retrieval of relevant document sets
was accomplished for each query, the sys-
tem applied anaphora resolution algorithm to
these documents. Finally, sentence matching
and ranking was accomplished as described in
section 4.2 and the system presented a ranked
list containing the 10 most relevant sentences
to each query.
For a better understanding of evaluation re-
sults, queries were classied into three groups
depending on the following characteristics:
 Group A. There are no pronominal ref-
erences in the target sentence (sentence
containing the correct answer).
 Group B. The information required as
answer is referenced via pronominal
anaphora in the target sentence.
 Group C. Any term in the query is ref-
erenced pronominally in the target sen-
tence.
Group A was made up by 37 questions.
Groups B and C contained 25 and 31 queries
respectively. Figure 3 shows examples of
queries classied into groups B and C.
Evaluation results are presented in Figure
4 as the number of target sentences appear-
ing into the 10 most relevant sentences re-
turned by the system for each query and also,
the number of these sentences that are con-
sidered a correct answer. An answer is con-
sidered correct if it can be obtained by sim-
ply looking at the target sentence. Results
Question: ?Who is the village head man of Digha ??
Answer: ?He is the sarpanch, or village head man of
Digha, a hamlet or mud-and-straw huts  10
miles from ...?
Group B Example
Anaphora resolution: Ram Bahadu
Question: ?What did Democrats propose for low-income
families??
Answer: ?They also want to provide small subsidies for
low-income families in which both parents work
at outside jobs.?
Group C Example
Anaphora resolution: Democrats
Figure 3: Group B and C query examples
are classied based on question type intro-
duced above. The number of queries pertain-
ing to each group appears in the second col-
umn. Third and fourth columns show base-
line results (without solving anaphora). Fifth
and sixth columns show results obtained when
pronominal references have been solved.
Results show several aspects we have to
take into account. Benets obtained from ap-
plying pronominal anaphora resolution vary
depending on question type. Results for
group A and B queries show us that relevance
to the query is the same as baseline system.
So, it seems that pronominal anaphora res-
olution does not achieve any improvement.
This is true only for group A questions. Al-
though target sentences are ranked similarly,
for group B questions, target sentences re-
turned by baseline can not be considered as
correct because we do not obtain the an-
swer by simply looking at returned sentences.
The correct answer is displayed only when
pronominal anaphora is solved and pronom-
inal references are substituted by the noun
phrase they refer to. Only if pronominal ref-
erences are solved, the user will not need to
read more text to obtain the correct answer.
For noun-phrase extraction QA systems the
improvement is greater. If pronominal ref-
erences are not solved, this information will
                    Baseline              Anaphora solved
Answer Type      Number Target included Correct answer Target included Correct answer
A 37 (39,78%) 18 (48,65%) 18 (48,65%) 18 (48,65%) 18 (48,65%)
B 25 (26,88%) 12 (48,00%) 0 (0,00%) 12 (48,00%) 12 (48,00%)
C 31 (33,33%) 9 (29,03%) 9 (29,03%) 21 (67,74%) 21 (67,74%)
A+B+C 93 (100,00%) 39 (41,94%) 27 (29,03%) 51 (54,84%) 51 (54,84%)
Figure 4: Evaluation results
not be analysed and probably a wrong noun-
phrase will be given as answer to the query.
Results improve again if we analyse group
C queries performance. These queries have
the following characteristic: some of the
query terms were referenced via pronominal
anaphora in the relevant sentence. When
this situation occurs, target sentences are re-
trieved earlier in the nal ranked list than in
the baseline list. This improvement is because
similarity increases between query and target
sentence when pronouns are weighted with
the same score as their referring terms. The
percentage of target sentences obtained in-
creases 38,71 points (from 29,03% to 67,74%).
Aggregate results presented in Figure 4
measure improvement obtained considering
the system as a whole. General percentage
of target sentences obtained increases 12,90
points (from 41,94% to 54,84%) and the level
of correct answers returned by the system in-
creases 25,81 points (from 29,03% to 54,84%).
At this point we need to consider the follow-
ing question: Will these results be the same
for any other question set? We have analysed
test questions in order to determine if results
obtained depend on question test set. We ar-
gue that a well-balanced query set would have
a percentage of target sentences that contain
pronouns (PTSC) similar to the pronominal
reference ratio of the text collection that is
being queried. Besides, we suppose that the
probability of nding an answer in a sentence
is the same for all sentences in the collec-
tion. Comparing LAT ratio of pronominal
reference (55,20%) with the question test set
PTSC we can measure how a question set can
aect results. Our question set PTSC value
is a 60,22%. We obtain as target sentences
containing pronouns only a 5,02% more than
expected when test queries are randomly se-
lected. In order to obtain results according to
a well-balanced question set, we discarded ve
questions from both groups B and C. Figure 5
shows that results for this well-balanced ques-
tion set are similar to previous results. Aggre-
gate results show that general percentage of
target sentences increases 10,84 points when
solving pronominal anaphora and the level
of correct answers retrieved increases 22,89
points (instead of 12,90 and 25,81 obtained
in previous evaluation respectively).
As results show, we can say that pronom-
inal anaphora resolution improves QA sys-
tems performance in several aspects. First,
precision increases when query terms are ref-
erenced anaphorically in the target sentence.
Second, pronominal anaphora resolution re-
duces the amount of text a user has to read
when the answer sentence is displayed and
pronominal references are substituted with
their coreferent noun phrases. And third,
for noun phrase extraction QA systems it is
essential to solve pronominal references if a
good performance is pursued.
6 Conclusions and future research
The analysis of information referenced
pronominally in documents has revealed to
be important to tasks where high level of
recall is required. We have analysed and
measured the eects of applying pronominal
anaphora resolution in QA systems. As
results show, its application improves greatly
QA performance and seems to be essential in
some cases.
Three main areas of future work have ap-
peared while investigation has been devel-
oped. First, IR system used for retrieving
relevant documents has to be adapted for QA
                    Baseline              Anaphora solved
Answer Type      Number Target included Correct answer Target included Correct answer
A 37 (39,78%) 18 (48,65%) 18 (48,65%) 18 (48,65%) 18 (48,65%)
B 20 (21,51%) 10 (50,00%) 0 (0,00%) 10 (50,00%) 10 (50,00%)
C 26 (27,96%) 9 (34,62%) 9 (34,62%) 18 (69,23%) 18 (69,23%)
A+B+C 83 (89,25%) 37 (44,58%) 27 (32,53%) 46 (55,42%) 46 (55,42%)
Figure 5: Well-balanced question set results
tasks. The IR used, obtained the document
containing the target sentence only for 93 of
the 150 proposed queries. Therefore, its preci-
sion needs to be improved. Second, anaphora
resolution algorithm has to be extended to
dierent types of anaphora such as denite
descriptions, surface count, verbal phrase and
one-anaphora. And third, sentence ranking
approach has to be analysed to maximise the
percentage of target sentences included into
the 10 answer sentences presented by the sys-
tem.
References
Eric Breck, John Burger, Lisa Ferro, David House,
Marc Light, and Inderjeet Mani. 1999. A Sys
Called Quanda. In Eighth Text REtrieval Con-
ference (TRE, 1999).
Gordon V. Cormack, Charles L. A. Clarke,
Christopher R. Palmer, and Derek I. E.
Kisman. 1999. Fast Automatic Passage Rank-
ing (MultiText Experiments for TREC-8). In
Eighth Text REtrieval Conference (TRE, 1999).
Antonio Ferrandez, Manuel Palomar, and Lidia
Moreno. 1998. Anaphora resolution in unre-
striced texts with partial parsing. In 36th An-
nual Meeting of the Association for Computa-
tional Linguistics and 17th International Con-
ference on Computational Lingustics COLING-
ACL.
Antonio Ferrandez, Manuel Palomar, and Lidia
Moreno. 1999. An empirical approach to Span-
ish anaphora resolution. To appear in Machine
Translation.
David A. Hull. 1999. Xerox TREC-8 Question
Answering Track Report. In Eighth Text RE-
trieval Conference (TRE, 1999).
Kevin Humphreys, Robert Gaizauskas, Mark
Hepple, and Mark Sanderson. 1999. University
of Sheeld TREC-8 Q&A System. In Eighth
Text REtrieval Conference (TRE, 1999).
Julian Kupiec, 1999. MURAX: Finding and Or-
ganising Answers from Text Search, pages 311{
331. Kluwer Academic, New York.
Dan Moldovan, Sanda Harabagiu, Marius Pasca,
Rada Mihalcea, Richard Goodrum, Roxana
G^rju, and Vasile Rus. 1999. LASSO: A Tool
for Surng the Answer Net. In Eighth Text RE-
trieval Conference (TRE, 1999).
Thomas S. Morton. 1999. Using Coreference in
Question Answering. In Eighth Text REtrieval
Conference (TRE, 1999).
Douglas W. Oard, Jianqiang Wang, Dekang Lin,
and Ian Soboro. 1999. TREC-8 Experiments
at Maryland: CLIR, QA and Routing. In
Eighth Text REtrieval Conference (TRE, 1999).
John Prager, Dragomir Radev, Eric Brown, Anni
Coden, and Valerie Samn. 1999. The Use of
Predictive Annotation for Question Answering.
In Eighth Text REtrieval Conference (TRE,
1999).
Gerard A. Salton. 1989. Automatic Text Process-
ing: The Transformation, Analysis, and Re-
trieval of Information by Computer. Addison
Wesley, New York.
Amit Singhal, Steve Abney, Michiel Bacchiani,
Michael Collins, Donald Hindle, and Fernando
Pereira. 1999. ATT at TREC-8. In Eighth Text
REtrieval Conference (TRE, 1999).
Toru Takaki. 1999. NTT DATA: Overview of sys-
tem approach at TREC-8 ad-hoc and question
answering. In Eighth Text REtrieval Confer-
ence (TRE, 1999).
TREC-8. 1999. Eighth Text REtrieval Confer-
ence.
Splitting Complex Temporal Questions for Question Answering systems  
E. Saquete, P. Mart??nez-Barco, R. Mun?oz, J.L. Vicedo
Grupo de investigacio?n del Procesamiento del Lenguaje y Sistemas de Informacio?n.
Departamento de Lenguajes y Sistemas Informa?ticos. Universidad de Alicante.
Alicante, Spain

stela,patricio,rafael,vicedo  @dlsi.ua.es
Abstract
This paper presents a multi-layered Question An-
swering (Q.A.) architecture suitable for enhanc-
ing current Q.A. capabilities with the possibility of
processing complex questions. That is, questions
whose answer needs to be gathered from pieces
of factual information scattered in different docu-
ments. Specifically, we have designed a layer ori-
ented to process the different types of temporal
questions. Complex temporal questions are first de-
composed into simpler ones, according to the tem-
poral relationships expressed in the original ques-
tion.
In the same way, the answers of each simple ques-
tion are re-composed, fulfilling the temporal restric-
tions of the original complex question.
Using this architecture, a Temporal Q.A. system
has been developed.
In this paper, we focus on explaining the first part
of the process: the decomposition of the complex
questions. Furthermore, it has been evaluated with
the TERQAS question corpus of 112 temporal ques-
tions. For the task of question splitting our system
has performed, in terms of precision and recall, 85%
and 71%, respectively.
1 Introduction
Question Answering could be defined as the pro-
cess of computer-answering to precise or arbitrary
questions formulated by users. Q.A. systems are es-
pecially useful to obtain a specific piece of informa-
tion without the need of manually going through all
the available documentation related to the topic.
Research in Question Answering mainly focuses
on the treatment of factual questions. These require
as an answer very specific items of data, such as
dates, names of entities or quantities, e.g., ?What is
the capital of Brazil??.

This paper has been supported by the Spanish government,
projects FIT-150500-2002-244, FIT-150500-2002-416, TIC-
2003-07158-C04-01 and TIC2000-0664-C02-02.
Temporal Q.A. is not a trivial task due to the com-
plexity temporal questions may reach. Current op-
erational Q.A. systems can deal with simple factual
temporal questions. That is, questions requiring to
be answered with a date, e.g. ?When did Bob Mar-
ley die??. or questions that include simple temporal
expressions in their formulation, e.g., ?Who won the
U.S. Open in 1999??. Processing this sort of ques-
tions is usually performed by identifying explicit
temporal expressions in questions and relevant doc-
uments, in order to gather the necessary information
to answer the queries.
Even though, it seems necessary to emphasize
that the system described in (Breck et al, 2000) is
the only one also using implicit temporal expression
recognition for Q.A. purposes. It does so by apply-
ing the temporal tagger developed by Mani and Wil-
son (2000).
However, issues like addressing the temporal
properties or the ordering of events in questions, re-
main beyond the scope of current Q.A. systems:

?Who was spokesman of the Soviet Embassy
in Baghdad during the invasion of Kuwait??

?Is Bill Clinton currently the President of the
United States??
This work presents a Question Answering system
capable of answering complex temporal questions.
This approach tries to imitate human behavior when
responding this type of questions. For example, a
human that wants to answer the question: ?Who
was spokesman of the Soviet Embassy in Baghdad
during the invasion of Kuwait?? would follow this
process:
1. First, he would decompose this question into
two simpler ones: ?Who was spokesman of the
Soviet Embassy in Baghdad?? and ?When did
the invasion of Kuwait occur??.
2. He would look for all the possible answers
to the first simple question: ?Who was
spokesman of the Soviet Embassy in Bagh-
dad??.
3. After that, he would look for the answer to the
second simple question: ?When did the inva-
sion of Kuwait occur??
4. Finally, he would give as a final answer one
of the answers to the first question (if there is
any), whose associated date stays within the
period of dates implied by the answer to the
second question. That is, he would obtain
the final answer by discarding all answers to
the simple questions which do not accomplish
the restrictions imposed by the temporal signal
provided by the original question (during).
Therefore, the treatment of complex question is
based on the decomposition of these questions into
simpler ones, to be resolved using conventional
Question Answering systems. Answers to simple
questions are used to build the answer to the origi-
nal question.
This paper has been structured in the following
fashion: first of all, section 2 presents our proposal
of a taxonomy for temporal questions. Section 3
describes the general architecture of our temporal
Q.A. system. Section 4 deepens into the first part
of the system: the decomposition unit. Finally, the
evaluation of the decomposition unit and some con-
clusions are shown.
2 Proposal of a Temporal Questions
Taxonomy
Before explaining how to answer temporal ques-
tions, it is necessary to classify them, since the
way to solve them will be different in each case.
Our classification distinguishes first between simple
questions and complex questions. We will consider
as simple those questions that can be solved directly
by a current General Purpose Question Answering
system, since they are formed by a single event. On
the other hand, we will consider as complex those
questions that are formed by more than one event
related by a temporal signal which establishes an
order relation between these events.
Simple Temporal Questions:
Type 1: Single event temporal questions without
temporal expression (TE). This kind of questions
are formed by a single event and can be directly
resolved by a Q.A. System, without pre- or post-
processing them. There are not temporal expres-
sions in the question. Example: ?When did Jordan
close the port of Aqaba to Kuwait??
Type 2: Single event temporal questions with tem-
poral expression. There is a single event in the ques-
tion, but there are one or more temporal expressions
that need to be recognized, resolved and annotated.
Each piece of temporal information could help to
search for an answer. Example: ?Who won the 1988
New Hampshire republican primary??. TE: 1988
Complex Temporal Questions:
Type 3: Multiple events temporal questions with
temporal expression. Questions that contain two or
more events, related by a temporal signal. This sig-
nal establishes the order between the events in the
question. Moreover, there are one or more tempo-
ral expressions in the question. These temporal ex-
pressions need to be recognized, resolved and an-
notated, and they introduce temporal constraints to
the answers of the question. Example: ?What did
George Bush do after the U.N. Security Council or-
dered a global embargo on trade with Iraq in August
90?? In this example, the temporal signal is after
and the temporal constraint is ?between 8/1/1990
and 8/31/1990?. This question can be divided into
the following ones:
 Q1: What did George Bush do?
 Q2: When the U.N. Security Council ordered
a global embargo on trade with Iraq?
Type 4: Multiple events temporal questions with-
out temporal expression. Questions that consist
of two or more events, related by a temporal sig-
nal. This signal establishes the order between the
events in the question. Example: ?What happened
to world oil prices after the Iraqi annexation of
Kuwait??. In this example, the temporal signal is
after and the question would be decomposed into:
 Q1: What happened to world oil prices?
 Q2: When did the Iraqi ?annexation? of
Kuwait occur?
How to process each type will be explained in de-
tail in the following sections.
3 Multi-layered Question-Answering
System Architecture
Current Question Answering system architectures
do not allow to process complex questions. That is,
questions whose answer needs to be gathered from
pieces of factual information that is scattered in a
document or through different documents. In or-
der to be able to process these complex questions,
we propose a multi-layered architecture. This ar-
chitecture increases the functionality of the current
Question-Answering systems, allowing us to solve
any type of temporal questions. Moreover, this sys-
tem could be easily augmented with new layers to
cope with questions that need complex processing
and are not temporal oriented.
Some examples of complex questions are:
 Temporal questions like ?Where did Michael
Milken study before going to the University of
Pennsylvania??. This kind of questions needs
to use temporal information and event ordering
to obtain the right answer.
 Script questions like ?How do I assemble a bi-
cycle??. In these questions, the final answer is
a set of ordered answers.
 Template-based questions like ?Which are the
main biographical data of Nelson Mandela??.
This question should be divided in a number of
factual questions asking for different aspects of
Nelson Mandela?s biography. Gathering their
respective answers will make it possible to an-
swer the original question.
These three types of question have in common
the necessity of an additional processing in order
to be solved. Our proposal to deal with them is
to superpose an additional processing layer, one by
each type, to a current General Purpose Question
Answering system, as it is shown in Figure 1. This
layer will perform the following steps:
 Decomposition of the question into simple
events to generate simple questions (sub-
questions) and the ordering of the sub-
questions.
 Sending simple questions to a current General
Purpose Question Answering system.
 Receiving the answers to the simple questions
from the current General Purpose Question
Answering system.
 Filtering and comparison between sub-answers
to build the final complex answer.
	
	













 
 
Passage Selection to Improve Question Answering 
 
Fernando LLopis 
Departamento de Lenguajes y 
Sistemas Inform?ticos 
Alicante (Spain) 03800 
llopis@dlsi.ua.es 
Jos? Luis Vicedo 
 Departamento de Lenguajes y 
Sistemas Inform?ticos 
Alicante (Spain) 03800 
vicedo@dlsi.ua.es 
Antonio Ferr?ndez 
Departamento de Lenguajes y 
Sistemas Inform?ticos 
Alicante (Spain) 03800 
antonio@dlsi.ua.es 
 
Abstract  
Open-Domain Question Answering systems 
(QA) performs the task of detecting text 
fragments in a collection of documents that 
contain the response to user?s queries. 
These systems use high complexity tools that 
reduce its applicability to the treatment of 
small amounts of text. Consequently, when 
working on large document collections, QA 
systems apply Information Retrieval (IR) 
techniques to reduce drastically text 
collections to a tractable quantity of 
relevant text. In this paper, we propose a 
novel Passage Retrieval (PR) model that 
performs this task with better performance 
for QA purposes than current best IR 
systems 
1 Introduction 
Information Retrieval (IR) systems receive as 
input a user?s query, and they have to return a 
set of documents sorted by their relevance to the 
query. There are different techniques to carry 
out the document extraction process, but most of 
them are based on pattern matching modules that 
depend on the number of times that a query term 
appear in each document, as well as the 
importance or discrimination value of each term 
in the document collection. Question Answering 
(QA) systems try to improve the output 
generated by IR systems by means of returning 
just small pieces of text that are supposed to 
contain the response. Usually, QA systems 
combine IR and Natural Language Processing 
(NLP) techniques to perform their task. This 
combination allows text understanding until a 
minimum level that permits a precise answer 
detection and extraction. Nevertheless, since 
NLP techniques are computationally expensive, 
QA systems need to reduce the amount of text 
where these techniques have to be applied. In 
this way, they usually work on the output of IR  
systems [10] that select the most relevant 
documents to the query by supposing that they 
will contain the answer required. Most applied 
IR systems are mainly based on three models: 
the cosine model [15], the pivoted cosine model1 
[17], and the probabilistic model (OKAPI [18]). 
Moreover, IR systems usually employ query 
expansion techniques that frequently improve 
their precision. These techniques can be based 
on thesaurus [21] or on the incorporation of the 
most frequent terms in the top M relevant 
documents [7]. 
Currently, several Passage Retrieval (PR) 
systems have also been proposed for this task 
[2][5][8][9]. PR systems deal with fragments of 
text in order to determine the relevance of a 
document to a query, as well as to detect 
document extracts that are likely to contain the 
expected answer (instead of full documents). 
Although PR systems apply IR-based techniques 
to perform their work, they have revealed to be 
more effective than IR systems for QA tasks. 
In this paper, we are analysing the importance of 
the IR-n PR system for QA n [11] as it was used 
in last TREC-10 Conference [19]. The following 
section briefly presents the backgrounds in IR, 
PR and QA. Section 3 shows the architecture of 
IR-n. Section 4 presents the evaluation 
accomplished and finally, section 5 details 
conclusions and work in progress.  
                                                   
1 It is a modification of the cosine model. It tries to 
reduce the problem of the preference for bigger 
documents. 
2 Backgrounds in Question Answering and 
Passage Retrieval 
2.1 Information Retrieval and Passage Retrieval  
Given a question, an IR system sorts the 
documents by its relevance to the query. It 
computes the similarity between each document 
and the question by taking into account the 
frequency of each query term in the document. 
This fact usually produces that bigger 
documents are preferred. A possible alternative 
to IR models is based on obtaining the similarity 
in accordance with the relevance of the passages 
contained in the document. This new approach, 
called Passage Retrieval (PR), has several 
advantages. When used for document retrieval, 
as the relevance of a document will depend on 
the relevance of the passages it contains, this 
measure will not be affected by the length of the 
full document. Moreover, these techniques allow 
to detect high relevant information embedded in 
a long document obtaining, this way, better 
performance than IR approaches [2][9]. On the 
other hand, when applied for QA tasks, PR 
systems allow reducing the amount of text to be 
processed with costly NLP tools by returning 
passages instead of whole documents. 
Two classifications can be accomplished in PR. 
The first one is in accordance with the way of 
dividing the documents into passages. The 
second one is in accordance with the moment in 
which the passage segmentation is carried out. 
With reference to the first one, PR community 
generally agrees with the classification proposed 
in [2], where the author distinguishes between 
discourse models, semantic models, and window 
models. The first one uses the structural 
properties of the documents, such as sentences 
or paragraphs [13][16] in order to define the 
passages. The second one divides each 
document into semantic pieces according to the 
different topics in the document [5]. The last one 
uses windows of a fixed size (usually a number 
of terms) to determine passage boundaries [2] 
[8]. 
At first glance, we could think that discourse-
based models would be the most effective, in 
retrieval terms, since they use the structure of 
the document itself. However, this model 
greatest problem relies on detecting passage 
boundaries since it depends on the writing style 
of the author of each document. On the other 
hand, window models have as main advantage 
that they are simpler to accomplish, since the 
passages have a previously known size, whereas 
the remaining models have to bear in mind the 
variable size of each passage. Nevertheless, 
discourse-based and semantic models have the 
main advantage that they return full information 
units of the document, which is quite important 
if these units are used as input by other 
applications such as QA. 
According to the second classification, we can 
distinguish between approaches that segment 
documents into passages for indexing purposes, 
and those that perform segmentation after the 
query is posed. The first one allows a quicker 
calculation; nevertheless, the second one allows 
different segmentation models in accordance 
with the kind of query. 
The passage extraction model that we propose 
(IR-n) allows us to benefit from the advantages 
of discourse-based models since self-contained 
information units of text, such as sentences, are 
used for building passages. Moreover, another 
novel proposal in our PR system is the relevance 
measure which, unlike other discourse-based 
models, is not based on the number of passage 
terms, but on a fixed number of passage 
sentences. This fact allows a simpler calculation 
of this measure unlike other discourse-based or 
semantic models. Although each passage is 
made up by a fixed number of sentences, we 
consider that our proposal differs from the 
window models since our passages do not have a 
fixed size (i.e. a fixed number of words) since 
we use sentences with a variable size. 
Furthermore, IR-n document segmentation into 
passages is accomplished after the query is 
posed, which allows us to determine the number 
of sentences to be considered in accordance with 
the kind of the query. 
2.2 Question Answering 
Open domain QA systems are defined as tools 
capable of extracting the answer to user queries 
directly from unrestricted domain documents. Or 
at least, systems that can extract text snippets 
from texts, from whose content it are possible to 
infer the answer to a specific question. In both 
cases, these systems try to reduce the amount of 
time users spend to locate a concrete 
information. 
Interest in QA systems is quite recent. We had 
little information about this kind of systems until 
the ?First Question Answering Track? was held 
in TREC-8 Conference. This track tries to 
benefit from large-scale evaluation that was 
previously carried out on IR systems, in 
previous TREC conferences.  
If a QA system wants to successfully obtain a 
user?s request, it needs to understand both texts 
and questions to a minimum level. From a 
linguistic perspective, ?understanding? means to 
carry out many of the typical steps on natural 
language analysis: lexical, syntactic and 
semantic. This analysis takes much more time 
than the statistical analysis that is usually carried 
out in IR. Besides, as QA systems have to 
manage with as much text as done for IR tasks, 
and the user needs the answer in a limited 
interval of time, it is nearly mandatory that first, 
an IR system processes the query and second, 
the QA process continues with its output. In this 
way, the time of analysis is highly decreased. 
The analysis of current best systems [3] [4] [14] 
[6] allows identifying main QA sub-components 
where document retrieval is accomplished by 
using IR technology: 
? Question Analysis.  
? Document Retrieval.  
? Passage Selection. 
? Answer Extraction. 
3 IR-n overview  
In this section, we describe the architecture of 
the proposed PR system, namely IR-n, focusing 
on its three main modules: indexing, passage 
retrieval and query expansion.  
3.1 Indexing module 
The main aim of this module is to generate the 
dictionaries that contain all the required 
information for the passage retrieval module. It 
requires the following information for each 
term: 
? The number of documents that contain 
this term. 
? For each document: 
? The number of times this term 
appears in the document. 
? The position of each term in the 
document represented as the number 
of sentence it appears in. 
 
As term, we consider the stem produced by the 
Porter stemmer on those words that do not 
appear in a list of stop-words, list that is similar 
to those generally used for IR. On the other 
hand, query terms are also extracted in the same 
way, that is to say, we only consider the stems of 
query words that do not appear in the stop-words 
list.  
3.2 Passage retrieval module 
This module extracts the passages according to 
its similarity with the user?s query. The scheme 
in this process is the following:  
1. Query terms are sorted according to the 
number of documents they appear in. Terms that 
appear in fewer documents are processed firstly. 
2. The documents that contain any query term 
are selected.  
3. The following similarity measure is calculated 
for each passage p (contained in the selected 
documents) with the query q: 
 
Similarity_measure(p, q) = ? ?? qpt tq,tp, W?W  
Wp,t = loge( fp,t + 1). 
Wq, t= loge( fq,t + 1) ? idf 
idf  = loge( N / ft + 1) 
 
Where fp,t  is the number of times that the term t 
appears in the passage p. fq,t  represents the 
number of times that the term t appears in the 
query q. N is the number of documents in the 
collection and ft is refers to the number of 
documents that contain the term t. 
4. Only the most relevant passage of each 
document is selected for retrieval. 
5. The selected passages are sorted by their 
similarity measure. 
6. Passages are associated with the document 
they pertain and they are presented in a ranked 
list form.   
 
As we can notice, the similarity measure is 
similar to the cosine measure presented in [15]. 
The only difference is that the size of each 
passage (the number of terms) is not used to 
normalise the results. This proposal performs 
normalization according to the fixed number of 
sentences per passage. This difference makes the 
calculation simpler than other discourse-based 
PR or IR systems. Another important detail to 
remark is that we are using N as the number of 
documents in the collection, instead of the 
number of passages according to the 
considerations presented in [9]. 
As it has been commented, our PR system uses 
variable-sized passages that are based on a fixed 
number of sentences (with different number of 
terms per passage). The passages overlap each 
other, that is to say, if a passage contains N 
sentences, the first passage will be formed by the 
sentences from 1 to N, the second one from 2 to 
N+1, and so on. We decided to overlap just one 
sentence according to the experiments and 
results presented in [12]. This work studied the 
optimum number of overlapping sentences in 
each passage for retrieval purposes concluding, 
that best results were obtained when only one 
overlapping sentence was used. Regarding to the 
optimum number (N) of sentences per passage 
considered in this paper, it will be 
experimentally obtained. 
4 Evaluation 
This section presents the experiments developed 
for training and evaluating our approach. The 
experiments have been run on the TREC-9 QA 
Track question set and document collections.  
4.1 Data collection 
TREC-9 question test set is made up by 682 
questions with answers included in the 
document collection. The document set consists 
of 978,952 documents from the TIPSTER and 
TREC following collections: AP Newswire, 
Wall Street Journal, San Jose Mercury News, 
Financial Times, Los Angeles Times, Foreign 
Broadcast Information Service. 
4.2 Training 
Training experiments had two objectives. They 
were designed (1) to calculate the optimum 
number of sentences (N) that define passage 
length and (2) to test two different possible ways 
of applying our method.  
First training experiment consists of working on 
the output of one of the current best performing 
IR systems (the ATT system). This experiment 
re-sorts its output (the first 1,000 ranked 
documents) by using IR-n. Second experiment 
consists of using our proposal as the main IR 
system, that is, indexing the whole collections 
by means of IR-n. For each experiment, a 
different number of sentences per passage were 
tested: 5, 10, 15 and 20 sentences. The relevance 
of each returned document was measured by 
means of the tool provided by TREC 
organization that allows us to determine if a 
passage contains the right answer. The two 
experiments are summed up in Figure 1. 
 
ATTIR-system
Documents
Questions
IR-n system
QA system
1000 more
relevant
documents
200 morerelevant
passages
Documents
IR-n system
200 more relevant
passages
Answers
  
Figure 1. Training Experiments 
These experiments were performed using only 
the first 100 questions included in the data 
collection. Table 1 shows training results for 
passages of 5, 10, 15 and 20 sentences using 
both approaches. This results measure the 
number of questions whose correct answer was 
included into the top n retrieved passages (or 
documents) for the training question set. The 
first experiment (IR-n Ref) uses IR-n on the 
1,000 documents returned by ATT system while 
the second one (IR-n) applies passage retrieval 
overall collections. 
As we can see, IR-n Ref and IR-n test obtain 
similar results although using our approach to 
re-rank the output of a good IR system presents 
a slight better performance than applying IR-n 
overall document collection. Regarding to the 
number of sentences to be taken into account to 
define passage length, we can observe that best 
results are obtained with passages of 20 
sentences. In this case, both tests improve 
significantly the performance of ATT-system. It 
ranges from 12 (IR-n Ref) and 10 (IR-n) points 
on a passage length of 20 sentences (for only the 
first 5 documents retrieved) to 8 and 7 points 
when the first 200 documents are taken into 
account respectively. 
 
Answer  
included 
At 
5 
docs 
At 
10 
docs 
At 
20 
docs 
At 
30 
docs 
At 
50 
docs 
At 
100 
docs 
At 
200 
docs
IR-n Ref. 
5 Sent 57 66 78 83 85 88 93 
10 Sent 63 76 80 89 93 96 97 
15 Sent 70 78 83 89 94 95 96 
20 Sent 74 83 87 91 93 96 97 
IR-n  
5 Sent 55 63 75 80 84 89 90 
10 Sent 60 73 78 87 92 95 97 
15 Sent 70 76 82 87 93 95 95 
20 Sent 72 80 86 90 92 96 96 
ATT system 
 62 69 77 82 83 87 89 
 
Table 1. Number of questions rightly answered 
(training set of 100 questions). 
4.3 Experiment 
In order to evaluate our proposal we decided to 
compare the quality of the information retrieved 
by our approaches with the ranked list retrieved 
by the ATT IR system. For this evaluation, the 
682 questions included in the data collection 
were processed and the number N of sentences 
per passage was set to 20. Table 2 shows the 
results of this evaluation experiment. This table 
shows the percentage of questions whose answer 
can be found into the first n documents returned 
by the ATT IR system and the best n passages 
returned by IR-n Ref and IR-n respectively. 
These results are also presented in Figure 2 
 
These data confirm training results. In this case, 
both approaches perform better than ATT 
system and improvements range form 6 to 12 
points for 20 sentences passage length. 
 
 
Answer  
Included 
ATT 
system IR-n Ref IR-n 
At 5 docs 64.90% 74.59% 72.21% 
At 10 docs 70.33% 82.73% 80.37% 
At 20 docs 75.91% 87.37% 86.35% 
At 30 docs 79.14% 89.96% 89.31% 
At 50 docs 83.70% 91.62% 91.52% 
At 100 docs 87.37% 94.56% 95.55% 
At 200 docs 90.01% 96.03% 95.92% 
Table 2. ATT-system versus IR-n systems. 
60
65
70
75
80
85
90
95
100
5 10 20 30 50 100 200
Number of documents
%
Qu
es
tio
ns
IR-n Ref IR-n ATT system
Figure 2.  Comparative of ATT-system and 
experiments with IR-n (Passages of 20 sentences) 
5 Conclusions and future works 
In this paper, we have analysed the improvement 
obtained by our passage retrieval system, called 
IR-n, with reference to a high-performance IR 
system (ATT) regarding to is application for QA 
tasks. This improvement has been evaluated on 
the TREC-9 QA track data set. The achieved 
improvements are twofold: First, our approach 
obtains a better precision by retrieving more 
passages that contain the answer to users? 
queries than ATT system does. Second, since 
our approach returns passages (instead of 
documents), it significantly reduces the amount 
of text to be processed with costly techniques by 
the QA system. The related experiments show 
that the optimal passage length for this task is 20 
when passages are made up by a fixed number 
of sentences. Moreover, we have tested two 
different ways of applying our model. As we 
have seen, IR-n presents similar results when it 
works on the output of an IR system, than when 
it works on the whole collections. Nevertheless, 
in both cases, benefits range from 6 to 12 points 
with reference to ATT system depending on the 
number of first documents or passages retrieved 
to be processed for QA tasks.  
As future work, in order to improve our system 
precision, we intend to obtain the optimum size 
of passages in accordance with the kind of 
question. Besides, we need to investigate the 
effects of query expansion techniques on IR-n 
system. Furthermore, we are also trying to 
improve the relationship between IR-n and the 
following QA system, in order to detect the 
minimum number of passages to extract for each 
query without affecting QA performance. 
References  
[1] Bertoldi, N and Federico, M. ITC-irst at CLEF-2001 , 
Working Notes for the Clef 2001 Darmstdt, Germany , 
pp  41-44 
[2] Callan, J. Passage-Level Evidence in Document 
Retrieval. In Proceedings of the 17 th Annual ACM 
SIGIR Conference on Research and Development in 
Information Retrieval, Dublin, Ireland   1994, pp. 302-
310. 
[3] Clarke, C.; Cormack, g, Kisman, D and Lynam, T. 
Question Answering by Passage Selection(Multitext 
Experiments for TREC-9) Proceedings of the Tenth 
Text REtrieval Conference, TREC-9. Gaithersburg , 
USA 2000, pp 673-683 
[4] Harabagiu, S.; Moldovan, D.; Pasca, M.; Mihalcea, R.; 
Surdeanu, M.; Bunescu, R.; G?rju, R.; Rus, V. and 
Morarescu, P. FALCON: Boosting Knowledge for 
Answer Engines. In Nineth Text REtrieval Conference, 
Gaithersburg  USA 2000.pp 479- 
[5] Hearst, M. and Plaunt, C. Subtopic structuring for full-
length document access. Proceedings of the Sixteenth 
Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, 
Pittsburgh, PA USA 1993 , pp 59-68 
[6] Ittycheriah, A.; Franz, M.; Zu, W. and Ratnaparkhi, A. 
IBM's Statistical Question Answering System. In 
Nineth Text REtrieval Conference, Gaithersburg  USA  
2000., pp 231-236 
[7] J. Xu and W. Croft.  Query expansion using local and 
global document analysis. In Proceedings of the 19th 
Annual International ACM SIGIR, Zurich, 
Switzerland,  1996 pp 4?11, 18?22. 
[8] Kaskiel, M. and  Zobel, J. Passage Retrieval Revisited 
SIGIR '97: Proceedings of the 20th Annual 
International ACM  Philadelphia, PA 1997, USA, pp 
27-31 
[9] KaszKiel, M. and  Zobel, J. Effective Ranking with 
Arbitrary Passages. Journal of the American Society 
for Information Science, Vol 52, No. 4, February 
2001, pp 344-364. 
[10] Litkowski, k, Syntactic Clues and Lexical Resources 
in Question-Answering In Nineth Text REtrieval 
Conference,  Gaithersburg  USA  2000  pp177-188 
[11] Llopis,  F. and  Vicedo, J.  Ir-n system, a passage 
retrieval system  at CLEF 2001  Working Notes for 
the Clef 2001 Darmstdt, Germany  2001, pp  115-120 . 
To appear in Lecture Notes in Computer Science 
[12] Llopis,  F.; Ferr?ndez, and  Vicedo, J.   Text 
Segmentation for efficient Information Retrieval Third 
International Conference on 
Intelligent Text Processing and 
Computational Linguistics. Mexico 2002 To appear in 
Lecture Notes in Computer Science 
[13] Namba, I Fujitsu Laboratories TREC9 Report. 
Proceedings of the Nineth Text REtrieval Conference, 
TREC-9. Gaithersburg,USA.2000, pp 203-208 
[14] Prager, J.; Brown, E.; Radev, D. and Czuba, K. One 
Search Engine or Two for QuestionAnswering. In 
Nineth Text REtrieval Conference, 
Gaithersburg,USA. 2000.   
[15] Salton G.  Automatic Text Processing: The 
Transformation, Analysis, and Retrieval of 
Information by Computer, Addison Wesley 
Publishing, New York. 1989 
[16] Salton, G.;  Allan, J. Buckley Approaches to passage 
retrieval in full text information systems. In R 
Korfhage, E Rasmussen & P Willet (Eds.) 
Prodeedings of the 16 th annual international ACM-
SIGIR conference on research and development in 
information retrieval. Pittsburgh PA USA , pp 49-58  
[17] Singhal, A.; Buckley, C. and  Mitra, M. Pivoted 
document length normalization. Proceedings of the 
19th annual international ACM- 1996. 
[18] Venner, G. and Walker, S. Okapi '84: `Best match' 
system. Microcomputer networking in libraries II. 
Vine, 48,1983, pp 22-26. 
[19] Vicedo, J.; Ferrandez, A and Llopis, F. University of 
Alicante al TREC-10. In Tenth Text REtrieval 
Conference, Gaithersburg,USA. 2001 
[20] Vicedo, J.; Ferrandez, A; A semantic approach to 
Question Answering systems. In Nineth Text REtrieval 
Conference, 2000  pp 440-444. 
[21] Y. Jing and W. B. Croft. An association thesaurus for 
information retrieval. In RIAO 94 Conference 
Proceedings, , New York, 1994. pp 146--160 
  
Special Section on Restricted-Domain
Question Answering
Question Answering in Restricted Domains:
An Overview
Diego Molla??
Macquarie University, Australia
Jose? Luis Vicedo?
University of Alicante, Spain
Automated question answering has been a topic of research and development since the earliest AI
applications. Computing power has increased since the first such systems were developed, and
the general methodology has changed from the use of hand-encoded knowledge bases about simple
domains to the use of text collections as the main knowledge source over more complex domains.
Still, many research issues remain. The focus of this article is on the use of restricted domains
for automated question answering. The article contains a historical perspective on question
answering over restricted domains and an overview of the current methods and applications
used in restricted domains. A main characteristic of question answering in restricted domains is
the integration of domain-specific information that is either developed for question answering or
that has been developed for other purposes. We explore the main methods developed to leverage
this domain-specific information.
1. Introduction
There has been an interest in representing knowledge and automatically processing
it from the time of the first generation of computers. This interest has increased from
the end of the 1980s to become an urgent necessity. Decisive factors in this increase of
interest are an unprecedented growth in the amount of digital information available, an
explosion of growth in the use of computers for communications, and the increasing
number of users that have access to all this information.
These circumstances have fostered research into information systems that can facil-
itate the localization, retrieval, and manipulation of these enormous quantities of data.
Question Answering (QA) is one of these research fields.
In this article, QA is defined as the task whereby an automated machine (such as
a computer) answers arbitrary questions formulated in natural language. QA systems
are especially useful in situations in which a user needs to know a very specific piece of
information and does not have the time?or just does not want?to read all the available
documentation related to the search topic in order to solve the problem at hand.
? Division of Information and Communication Sciences, Macquarie University, New South Wales 2109,
Australia. E-mail: diego@ics.mq.edu.au.
? Departamento de Lenguajes y Sistemas Informa?ticos, Universidad de Alicante, Campus de San Vicente
del Raspeig, Apdo. 99. Alicante, Spain. E-mail: vicedo@dlsi.ua.es.
Submission received: 2 June 2006; revised submission received: 15 October 2006; accepted for publication:
23 October 2006.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 1
Research in QA has been developed from two different scientific perspectives,
artificial intelligence (AI) and information retrieval (IR).
Work in QA since the early stages of AI has led to systems that respond to questions
using the knowledge encoded in databases as an information source. Obviously, these
systems can only provide answers concerning the information previously encoded in
the database. The benefit of this approach is that having a conceptual model of the
application domain represented in the database structure allows the use of advanced
techniques such as theorem proving and deep reasoning in order to address complex
information needs.
Currently we are witnessing a surge of activity in the area from the perspective of
IR, initiated by the Question Answering track of TREC1 in 1999 (Voorhees 2001). Since
then, increasingly powerful systems have participated in TREC and other evaluation
fora such as CLEF2 (Vallin et al 2005) and NTCIR3 (Kando 2005). From this perspective,
question answering focuses on finding text excerpts that contain the answer within
large collections of documents. The tasks set in these conferences have molded a specific
kind of question answering that is easy to evaluate and that focuses on the use of fast
and shallow methods that are generally independent of the application domain. In
other words, current research focuses on text-based, open-domain question answering.
Both trends have developed in parallel and represent the opposite ends of a spec-
trum connecting what we might label as structured knowledge-based and free text-
based question answering. Whereas structured knowledge-based QA systems are well
adapted to applications managing complex queries in a very structured information
environment, the kind of research developed in TREC, CLEF, and NTCIR is probably
better suited to broad-purpose generic applications dealing with simple factual ques-
tions such as World Wide Web?based question answering.
However, both approaches have serious disadvantages when they attempt to tackle
important real applications that handle complex questions by combining domain-
specific information typically expressed in different sources (structured, semistructured,
unstructured, etc.) using reasoning techniques. Examples of such applications are:
Interfaces to machine-readable technical manuals: Many software applications are
very complex and they are accompanied by extensive documentation. A QA sys-
tem that finds specific answers to a user?s question based on such documentation
would be very useful.
Front-ends to knowledge sources: Many disciplines and areas of human activity have
their own specific knowledge sources. An example is the medical domain, which,
as we shall see in this article, contains a wealth of technical information and
resources that can be used for a QA system targeting this kind of information.
Help desk systems in large organizations: Help desk staff in large organizations need
to quickly satisfy the customer?s need for information. Although many such
requests for information will be found in FAQs available to the help desk staff,
there will always be requests that are unique and that require staff to have access
to fast methods to find the relevant information. End systems tailored to such staff
(who can be trained) are different from QA systems designed for the end user, but
they still need to leverage the organization domain.
1 Text REtrieval Conference (http://trec.nist.gov/).
2 Cross Language Evaluation Forum (http://clef.iei.pi.cnr.it/).
3 NII-NACSIS Test Collection for IR Systems (http://research.nii.ac.jp/ntcir/).
42
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
It might be argued that focusing research on restricted domains is limiting because
the results are too specific and not open to generalization. This may have been the case
with early work in natural language processing (NLP), which focused on restricted
domains simply because of limitations in computing power and in theoretical cov-
erage. This is not the case nowadays. The availability of comprehensive and reliable
resources in complex domains enables interesting and fruitful research to be carried out
in restricted-domain natural language processing.
In short, research in restricted-domain question answering (RDQA) addresses
problems related to the incorporation of domain-specific information into current
state-of-the-art QA technology with the hope of achieving deep reasoning capabili-
ties and reliable accuracy performance in real world applications. In fact, as a not-
too-long-term vision, we are convinced that research in restricted domains will drive
the convergence between structured knowledge-based and free text-based question
answering.
In this article we survey past and current work on question answering in restricted
domains. In the process we will highlight the advantages of developing systems based
on restricted domains. Section 2 provides a historical note on question answering,
with an emphasis on restricted domains, and focusing mainly on early work. Sec-
tion 3 presents desirable characteristics of restricted domains for the development of
NLP research in general, and question answering in particular. Section 4 comments
on some of the main factors that distinguish question answering in an open domain
from question answering in a restricted domain. Section 5 focuses on the use of
domain-specific resources for question answering. Section 6 outlines current restricted-
domain question answering methods. Section 7 notes the main aspects to consider
when building a restricted-domain question answering system. Section 8 introduces
the articles in this special section of the journal, and finally Section 9 presents some
conclusions.
2. Early Work
Two examples of early question-answering systems are BASEBALL and LUNAR. BASE-
BALL answered questions about baseball games played in the American league over
one season (Green et al 1961), and LUNAR answered questions about the analysis
of rock samples from the Apollo moon missions (Woods 1997). Both systems were
very successful in their chosen domains. In particular, LUNAR was demonstrated at
a lunar science convention in 1971, where it was able to answer 90% of questions
posed by geologists without prior instructions with regard to the allowable phrasing
(Hirschman and Gaizauskas 2001). Both LUNAR and BASEBALL are examples of what
have been described as natural language interfaces to databases, that is, their source
of information was a database that contained the relevant information about the topic.
The user?s question was converted into a database query, and the database output was
given as the answer. The very specific nature of the domains enabled the construction of
appropriately comprehensive databases, and a domain-specific question analysis that
enabled a mapping from the meaning of the user?s question onto the corresponding
database query.
LUNAR and BASEBALL are only two of the most salient examples of early work on
question answering, but there were many other such systems. Simmons?s (1965) survey
described a variety of early QA systems. Most of these focused on restricted domains by
developing a database of knowledge and providing a natural language interface. Still,
43
Computational Linguistics Volume 33, Number 1
many of these early systems (including LUNAR and BASEBALL) were no more than
?toy systems? that focused on very limited domains. Those early systems that used a
corpus of text as the inherent information source typically processed small volumes of
text and would rely on a human to disambiguate the corpus sentences or convert them
to a simplified version of English.
During the 1970s and 1980s there was intensive research on the development of the-
oretical bases for computational linguistics. This research prompted the development of
QA systems on domains that were more complex than those of the earlier systems. The
main goal of this research was to use QA as an application framework within which
general NLP theories could be tested. This work culminated in large and ambitious
projects such as the Berkeley Unix Consultant (Wilensky et al 1994).
The Berkeley Unix Consultant project (UC) used the domain of the UNIX operat-
ing system to develop a help system that combined research in planning, reasoning,
natural language processing, and knowledge representation. The user?s question was
analyzed and a meaning representation corresponding to the question was encoded in
a knowledge representation formalism. Then, UC hypothesized the actual information
needs of the user by consulting the user model and applying goal analysis. The answer
was tailored to the user?s expertise and goals. The sample dialogues provided were
certainly impressive. However, no transcripts of real-world dialogues were provided
and therefore it cannot be determined whether the methods and theories developed in
UC were robust enough for practical use.
Most of the early work attempted to implement QA systems from the early per-
spective of AI or computational linguistics. As noted earlier, due to the limitations
of the time, question answering focused on restricted domains. A turning point was
reached in 1999, with the introduction of the QA track in the TREC (Voorhees 1999). The
popularity of the QA track in TREC has enabled research on QA from an IR perspective.
From the IR community?s point of view, the task of question answering is reduced
to the task of finding the text that contains the answer to the question and extracting
the answer. Text documents are viewed as a source of unstructured information that
is structured by indexing it. Indexing the documents makes it feasible to locate the
fragments that are closely related to the question terms by applying term-matching
techniques.
A consequence of this new perspective is the application of domain-independent
methods, allowing what has been called open-domain question-answering. This ap-
proach is largely used in current QA systems. It is beyond the scope of this article to
survey the techniques and systems used in open-domain QA; the interested reader is
referred to the proceedings of TREC, which are available on-line.4 Instead, in the sub-
sequent sections we will review current approaches to question answering in restricted
domains. But before that, let us analyze what a restricted domain is.
3. Characteristics of Restricted Domains
The nature of a particular restricted domain affects the kinds of questions asked and
answers that can be expected. Consequently, different restricted domains benefit from
different QA techniques. Some domains are particularly appropriate for the develop-
ment of question answering systems. Minock (2005) lists three desiderata for a restricted
4 http://trec.nist.gov.
44
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
domain within the context of World Wide Web?based question answering?that is,
question answering that relies on documents taken from the World Wide Web as the
main source for finding answers. According to Minock, a restricted domain must meet
the following desiderata:
1. It should be circumscribed.
2. It should be complex.
3. It should be practical.
The same desiderata apply, with some modifications, to restricted domains on question
answering that is not World Wide Web?based.
3.1 Circumscription
Minock?s original description of a circumscribed domain is motivated by the user?s need
to know what to expect of the World Wide Web?based QA system at hand and to know
what questions are appropriate to the domain at hand. An example of a domain that
would fare low in this desideratum is that of current events, because the user might
have difficulty in ascertaining what questions can be asked. An example of a good
domain according to this desideratum would be a science domain such as astronomy or
chemistry.
If the QA system is not World Wide Web?based and, furthermore, is intended for
use within a corporation, however, users do not face the problem of wondering what
questions are appropriate. Rather, a more important motivation for a circumscribed
domain is the need for clearly defined knowledge sources. The range of techniques used
in a restricted domain should not need to use extensive knowledge from outside the
chosen domain. Rather, a domain that has authoritative and comprehensive resources is
to be preferred. Examples of resources include actual databases containing the required
information.
It is natural to assume that the more restricted the domain is and the more circum-
scribed it is, the more possible it is to obtain such comprehensive databases. For more
complex domains, useful resources are terminology databases and domain ontologies.
An added value is the existence of well-accepted terminology and ontology standards.
3.2 Complexity
A domain should be complex enough to warrant the use of a QA system. This may
seem an obvious statement, but it is important to bear in mind that, in a desire to find
a domain that is fully circumscribed, one might attempt to develop a QA system in a
domain where a simple list of facts or a FAQ would be sufficient to satisfy the user?s
need for information. There is no need for a QA system in such domains.
Developing a system for a simple domain does not advance research in any signif-
icant area. Such domains are to be left to those who are more interested in capitalizing
on current research advances to develop practical applications, rather than extending
current research boundaries. In general, the more complex a domain is, the more inter-
esting it becomes for the researcher and the more useful it presumably is to the user.
There is a balance to be achieved between the need for a complex domain and
that of a circumscribed domain, because these two desiderata are in conflict. At some
point, if a domain is complex enough, it becomes difficult to manage and there is a
45
Computational Linguistics Volume 33, Number 1
higher probability of requiring resources belonging to other domains; in other words,
the domain becomes less circumscribed.
3.3 Practicality
Practicality is an important desideratum to consider when developing a QA system.
The domain should be of use to a relatively large group of people. Otherwise one risks
wasting effort on a system that nobody would use, such as for an artificially constructed
toy domain. The choice of domain affects the kind of users to target. Therefore, for each
domain it is important to determine the kinds of questions asked in the specific domain
(question style and terminology used are two important factors to consider), the sort of
information that is most commonly requested, and the level of detail expected in the
answers.
4. Open-Domain versus Restricted-Domain Question Answering
There are various factors that determine the best techniques to use in restricted-domain
question answering, and whether techniques used in open-domain question answering
would be effective in restricted-domain question answering. In this section we will
briefly introduce some of these factors.
4.1 The Size of the Data
A well-known method used in open-domain QA is derived from redundancy-based
techniques. These techniques were first discussed by Brill et al (2001), who observed
that, as the size of the text corpus increases, it becomes more likely that the answer
to a specific question can be found with data-intensive methods that do not require a
complex language model. Thus, if the question is Who killed Abraham Lincoln?, it is easier
to find the answer in John Wilkes Booth killed Abraham Lincoln than in John Wilkes Booth
is perhaps America?s most infamous assassin. He is best known for having fired the bullet that
ended Abraham Lincoln?s life. Redundancy-based techniques are likely to have a weaker
impact in restricted-domain QA, especially in the case of domains with relatively small
corpora.
Domains with relatively small corpora will naturally have relatively fewer sen-
tences that contain the answer. In those domains it becomes important to use so-
phisticated language processing techniques, including the resolution of inferences, if
necessary, to find the answer. The haystack of a restricted domain is relatively small,
but it also has fewer needles.
Note that, if the size of the corpus is relatively small, it becomes possible to apply
complex NLP techniques to the complete corpus off-line. Nowadays it is possible to
parse the entire corpus used in the QA track of TREC and to extract all its named entities.
It is therefore feasible to parse and extract the named entities of corpora of restricted
domains.
4.2 Domain Context
The actual domain provides a specific context to the question-answering process. Con-
sequently the set of senses available to words is typically a subset of all the available
senses. The impact of word-sense disambiguation is possibly reduced in RDQA, though
46
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
it should be noted that some words would still have several senses available and
therefore word-sense disambiguation still plays a role. We are not aware of any studies
on the impact of word-sense disambiguation on restricted domains and certainly this
area is worth exploring.
The kinds of questions asked in a restricted domain are naturally different from
those asked in an open domain. Users of a restricted domain, and especially users who
are experts in the domain, will use specific terminology and will pose rather technical
questions that require very specific answers. Questions asked by such users are much
more complex than those of casual users of open-domain QA systems. This is certainly
the case in the medical domain, as the articles included in this special section show
convincingly. The challenges related to solving those questions are certainly worth the
effort in pursuing research in RDQA.
4.3 Resources
An important difference between open-domain and restricted-domain QA is the ex-
istence of specific resources for restricted domains that can be used. In the following
sections we will comment on these resources.
5. Use of Domain-Specific Resources
Intuitively, a good method for answering questions in a restricted domain needs to
leverage any information available about the domain in order to be able to address
users? information needs with the specificity and depth required.
The type of information available for a particular domain is intrinsically related
to the complexity of the domain and the particular needs of the domain users. Hence,
domain knowledge representation can range from simple lists of specialized entities
and terms to high-level ontologies where all the domain knowledge is unambiguously
represented.
Within computer science, an ontology is usually defined as a formal explicit de-
scription of concepts in the domain of discourse, together with their attributes, roles,
restrictions, and other defining features (Noy and McGuinness 2001). The relations
between the concepts are also expressed formally. The two most common relations
shown in an ontology are subclass (?is a subtype of?) and instance (?is an instance of?),
but other relations can be included, such as meronymy (?part of?) and, in the case of
WordNet (Fellbaum 1998), entailment.
For the purposes of this article, we will refer to all the possible domain knowledge
representations as ontological resources.
Generally, domains that are complex, circumscribed, and practical are likely to
have available ontological resources that can be used to quick-start QA research and
development. These resources are typically developed for the domain users to help
them categorize the domain knowledge and agree on notational standards, and to help
them retrieve information using conventional information retrieval applications.
5.1 Open-Domain Ontologies
There are ontologies that are designed without a specific domain in mind. These are
referred here as open-domain ontologies. A widely used open-domain ontology is
WordNet (Fellbaum 1998). WordNet contains a large list of open-class words grouped
47
Computational Linguistics Volume 33, Number 1
into synonym sets (the ?synsets?). A range of synset relations is encoded, such as hyper-
nymy/hyponymy, meronymy, and entailment. WordNet alo includes word relations,
such as antonymy. A departure from ontologies like Cyc (Lenat and Guha 1990) is that
WordNet does not include formal definitions of the features of the objects. Still, for the
purposes of this article, WordNet is an ontology. This view is supported by its use in
many systems, including open-domain question answering systems (Moldovan and
Novischi 2002).
Open-domain ontologies like WordNet, however, are of limited use for QA in re-
stricted domains. This is so because the information is unlikely to be well balanced with
respect to the chosen domain. For example, parts of open-domain ontologies are too
coarse-grained for specific restricted domains, whereas other parts are too fine-grained.
And worse, open-domain ontologies may contain information that is not appropriate
for specific restricted domains.
Open-domain ontologies are too coarse-grained. Restricted domains, and especially tech-
nical domains, abound in terms that are specific to the domain and largely unknown
in other domains. Open-domain ontologies typically do not include these specific
terms. In some domains, however, these terms may be used widely. Consequently,
open-domain ontologies will need to be complemented with terminology lists or local
ontologies.
Open-domain ontologies are too fine-grained. Open-domain ontologies that map words to
concepts, as is the case with WordNet, face the problem of polysemous words, that is,
words with multiple meanings. However, those ambiguous words are usually unam-
biguous in restricted domains. Take the noun file. WordNet 1.7.1 lists four meanings,
shown in Table 1. Of the four meanings, only the first one (?a set of related records kept
together?) is relevant within domains related to software development. Open-domain
ontologies therefore risk overloading the system with concepts that are rarely, if ever,
used within the chosen restricted domain.
Open-domain ontologies may have information that is not appropriate for the domain. The
most damaging property of open-domain ontologies is that they may contain informa-
tion that is misleading in certain restricted domains. Restricted domains notoriously
overload some terms commonly used outside their domain. For example, the usual
meaning of the verb print is to render something into printed matter. However, within
the domain of computer programming, the verb print usually means to display on
the computer monitor. Consequently, a system that uses an open-domain ontology
would possibly misinterpret the meaning of print in the question Which C++ instruc-
Table 1
Four senses of the noun file in WordNet 1.7.1.
Sense Gloss
1. A set of related records (either written or electronic) kept together
2. A line of persons or things ranged one behind the other
3. A container for keeping papers in order
4. A steel hand tool with small sharp teeth on some or all of its surfaces; used for
smoothing wood or metal
48
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
tion prints words onto the screen? This sense of print is not available in WordNet5 and
therefore it is not possible to apply word-sense disambiguation techniques to find the
appropriate sense.
5.2 Uses of Ontological Resources
Ontological resources define a common vocabulary for accessing information in a do-
main and this makes it easier to manage domain information as regards the following
(Noy and McGuinness 2001):
 sharing common understanding of the structure of information among
people or software agents
 enabling reuse of domain knowledge
 making domain assumptions explicit
 separating domain knowledge from the operational knowledge
 making possible different analysis of the domain knowledge
Among theses concerns, enabling the separation of domain knowledge and operational
knowledge is probably the most valuable characteristic for QA purposes. This fact al-
lows the separation of the process of representing the concepts expressed in a document
from the use of the relations between concepts for deduction or reasoning processes.
On the other hand, formalisms, theories, and algorithms either designed for domain
document representation or reasoning may be made independent from the chosen
domain ontology and can also be applied to different domains, thus enhancing system
portability between domains.
Research on using ontologies for QA has benefited from the following:
 The increasing availability of ontologies encoding different kinds of
knowledge. We can find ontologies ranging from general world
knowledge resources, such as WordNet (Fellbaum 1998), EuroWordNet
(Vossen 1998), Cyc (Lenat and Guha 1990), and FrameNet (Johnson and
Fillmore 2000, to very specific domain knowledge, such as the medical
domain (Lindberg, Humphreys, and McCray 1993) or the chemistry
domain (Barker et al 2004).
 Steady achievements in knowledge representation and reasoning (KR&R)
techniques, which enable precise representation of both domain-related
information and domain-related reasoning and deduction mechanisms
(Barker et al 2004).
 Advances in the development of modular and robust natural language
processing systems (Abney 1996; Hobbs et al 1997; Basili and Zanzotto
2002) in the context of the use of ontological resources for both textual
interpretation and representation (Ait-Mokhtar and Chanod 1997) and
database access (Popescu, Etzioni, and Kautz 2003).
5 This was the case for the on-line version of WordNet 2.1 (http://wordnet.princeton.edu/) on
8 October 2006.
49
Computational Linguistics Volume 33, Number 1
 Increasing success in the development of ontology-based QA frameworks
where answers are derived from reasoning processes over questions and
document ontological representations (Zajac 2001).
Ontology-based question answering systems attack the answer-retrieval problem
by means of an internal unambiguous knowledge representation. Both questions and
knowledge are represented using specific knowledge models based on ontological en-
tities, concepts, and relations. The answering of questions is performed by applying
different reasoning and proof techniques that allow the detection of textual entail-
ment, which is useful in determining whether a given sentence answers a particular
question.
6. The State of the Art in RDQA
Current work on QA in restricted domains tends to exploit the characteristics of the
domain in order to improve the accuracy and practicability of the system. This is
done largely by determining the types of information needs in the chosen domain, by
studying the format of questions asked, and by leveraging the ontological information
available in the domain.
Some domains are complex domains that have a history of users attempting to
streamline the process to find specific information. An example of such a domain is that
of medicine. It is important for a doctor to quickly diagnose the illness of a patient, and
to determine if a patient is developing a new variation of an illness that has occurred
before. Given the importance of finding the correct diagnosis and treatment, the domain
of medicine has developed trusted resources that can be used for question answering in
this domain. Zweigenbaum (2003) provides examples of resources for terminology and
corpora of authoritative material.
Demner-Fushman and Lin (2005) operationalize knowledge extraction for populat-
ing a database with PICO (Population, Intervention, Comparison, and Outcome) ele-
ments from medical abstracts obtained from MEDLINE. PICO structures are the frames
used for evidence-based medicine. Sang, Bouma, and de Rijke (2005a) describe several
strategies for populating a database with medical information related to diseases, symp-
toms, and treatments, which is automatically extracted from medical texts. This struc-
tured information is used for answering medical-related questions. Niu and Hirst (2004)
describe a method for identifying semantic classes and the relations between them in
medical texts. This approach is able to build an ontology for the domain automatically.
Yu, Sable, and Zhu (2005) present an algorithm to classify medical questions based
on a well-known hierarchical evidence taxonomy (Ely et al 2002). Rinaldi, Dowdall,
and Schneider (2004) describe the difficulties in adapting an existing RDQA system
developed for assisting questions on UNIX technical manuals (Molla? et al 2000) to the
Genomics domain.
Benamara (2004) reports in detail on one of the currently most advanced RDQA
systems. WEBCOOP is a logic-based system that integrates knowledge representation
and advanced reasoning procedures to generate responses to natural queries. This
system has been developed for the tourism domain.
As for any knowledge intensive application, using ontologies for QA has as a
limitation the restrictions imposed by the underlying knowledge representation models.
Thus, in the following subsections we will focus on the efforts that are being employed
from both historical trends in QA research (structured knowledge?based and free-text?
50
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
based perspectives) to provide systems with deep reasoning capabilities supported by
ontological domain information. We introduce several works that aim at combining the
use of various ontologies and we also describe current attempts to separate the process-
ing of domain-dependent information from generic domain-independent information
with the goal of increasing portability across domains.
6.1 Ontologies and Structured Knowledge?Based QA
As noted earlier, the first QA systems focused on the development of natural language
interfaces to databases (NLIDBs). This is a natural approach to follow in circumscribed
domains that are not very complex. The idea is to produce a structured information
resource containing comprehensive information on the contents of the domain. This
information resource is produced before any question is asked and is queried over when
the user asks a question.
There is a wealth of research in the area of NLIDBs and it is not within the scope
of this article to survey this important area of research. Rather, we refer the reader to
Androutsopoulos, Ritchie, and Thanisch (1995). Work in NLIDBs assumes an existing
database that is queried over. If the database does not exist, it is created by using meth-
ods based on information-extraction technology. The aim is to extract all the information
that might be used as an answer. A clear candidate is the use of named entities, but the
creation of templates has also been tried in open domains (Srihari and Li 2000) and
restricted domains (Weischedel, Xu, and Licuanan 2004).
There are other systems that support this kind of knowledge-based question-
answering, including some dealing with questions unanticipated at the time of system
construction. These include the AP Chemistry question-answering system (Barker et al
2004), Cyc (Lenat and Guha 1990), the Botany Knowledge Base system (Porter et al
1988), the two systems developed for DARPA?s High Performance Knowledge Base
(HPKB) project (Cohen et al 1988), and the two systems developed for DARPA?s Rapid
Knowledge Formation (RKF) project (Schrag et al 2002).
6.2 Ontologies and Free-Text?Based QA
In this approach, users pose questions in natural language to knowledge bases made
up of documents also written in natural language. In this case ontologies are used to
define a language in which questions and documents can be represented and exploited
to obtain the required answers. The translation from natural language to the internal
representation is automatic; this presupposes fully unambiguous representations that
are currently beyond our capabilities.
The main characteristic of these approaches is the intensive use of an ontology
in the different parts of the question answering system. For instance, the ontology is
used in the representation of the question and the documents, in the refinement of
the initial query, in the reasoning processes carried out over the classes and subclasses
from the ontology, and in the similarity algorithms employed for answer retrieval and
extraction.
Zajac (2001) presents an ontology-based semantic framework for question answer-
ing where both questions and source texts are parsed into underspecified semantic
expressions where names of the semantic atoms and predicates are defined in an in-
terlingual ontology. Answer retrieval is done using subsumption and unification, and
queries are expanded using ontological rules.
51
Computational Linguistics Volume 33, Number 1
6.3 Integrating Heterogeneous Sources of Information
More interesting than using a single database is the combination of databases with
semistructured information (such as text with some XML markup) or even unstruc-
tured information (i.e., plain text). This has been proposed for World Wide Web?based
question answering (Lin 2002), given the availability of pockets of information stored
in databases on the World Wide Web. The idea is to analyze the question and find the
relevant database among a preselected list if this is possible. If there are no suitable data-
bases or it is not possible to determine the appropriate database query, then standard
question-answering techniques are applied using the World Wide Web as a resource.
The same strategy can be applied to question answering over restricted domains by
keeping a set of relevant databases and a corpus of documents to query over in case the
question is not covered in the databases.
There are two main issues that need to be handled by a QA system that relies on
heterogeneous sources:
Interface: The resources in each domain will have their own formats and interfaces,
which must be unified by the QA system.
Selection: The QA system needs to determine the actual resource within which to look
for the answer.
Given that the actual domain-specific resources range from simple word lists to struc-
tured databases, interfacing to them is by no means simple. Two approaches are envis-
aged (Lin 2002):
Slurp: Extract all the information from the multiple sources and create a database
containing all the information. By having all the information in a unified database,
the interfacing problem is easily solved and it is even possible to handle queries
that the original databases were unable to handle (such as queries that rely on
knowledge from various domains). This method is practical if the actual databases
are available locally and their format is known. However, some databases are
available as on-line resources only and any attempt to slurp all their information
through methodical queries may be frowned upon by the database owners.
Wrap: Provide an application program interface (API) to the individual databases. The
set of databases can be seen as a federated database system. The choice of pro-
viding an API has the obvious disadvantage that it may not be possible to devise
a unified API that makes the best of what is available in the domain resources.
The compromise would be a set of APIs that may or may not be able to query the
resource with the full power of the original resource interface.
A step beyond portable QA systems is to build a meta-domain QA system. A meta-
domain QA system specializes in several restricted domains by acting as a knowledge
broker to specialized domain modules. An example of such a system is START (Katz
1997), which currently is a World Wide Web?based QA system that uses a wide range
of structured data available on the Internet.
MOSES (Basili et al 2004) is an ontology-based QA system in which users pose
questions in natural language to knowledge bases of facts extracted from a federation
of Web sites and organized in topic map repositories. This approach uses an ontology-
based methodology to search, create, maintain, and adapt semantically structured
World Wide Web content according to the vision of the Semantic Web in a domain
related to university World Wide Web sites.
52
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
AQUA (Vargas-Vera, Motta, and Domingue 2003) combines knowledge encoded
in a database with domain-related documents through an ontology that describes aca-
demic life. AQUA tries to answer a question using its knowledge base. If a query cannot
be satisfied via the database, it tries to find an answer on domain-related World Wide
Web pages.
The L&C system (Ceusters, Smith, and Van Mol 2003) is one of the most ambitious
works in the medical domain; it tries to combine authoritative medical knowledge
with information about patients. The information needed by physicians is of two sorts.
First, there is information concerning patients, such as the changes in Mr. X?s blood
pressure over the past three days, or the substances to which Ms. Y is allergic. Second,
there is what can be defined as medical knowledge, that is, the information found in
textbooks, journal articles, clinical studies, and so on. The final objective of this work
is to combine these two types of information so that the QA system, when asked,
for example, whether it is safe to give the patient an additional shot of a hypoten-
sive agent in order to reduce bleeding, would respond with: Can you please wait for
45 seconds because the patient?s blood pressure has been dropping slightly already for the last
2 minutes?
6.4 Porting to Other Domains
Developing a system in a specific domain could be time-consuming. It is natural to think
of ways to reuse technologies (or even code) in QA systems from other domains or from
open-domain QA systems. A topic that is intimately related is that of portability to other
domains.
Some question-answering systems are designed with the goals of re-usability and
portability in mind. These are generic systems that can be localized to specific domains.
For example, JAVELIN (Nyberg et al 2005) is an open-domain QA system that can be
extended to focus on restricted domains. Special care was taken to leverage ontologies
specific to the chosen domain by developing a Java API. The specific ontological in-
formation extracted is the type hierarchy and sets of synonyms (AKA, or ?also known
as? extraction). Another example that demonstrates efforts in adapting an open-domain
QA system to a specialized geographical environment can be seen in the work by Ferre?s
and Rodr??guez (2006).
Another approach, developed by Frank et al (2005), is based on the use of struc-
tured knowledge sources. This approach applies deep linguistic analysis to the question
and transforms it into an internal representation based on conceptual and semantic
characteristics. This representation is domain-independent and provides a natural in-
terface to the underlying knowledge databases. This approach has been implemented
as a prototype for two application domains: the domain of Nobel prize winners and the
domain of language technology.
Another issue is that of localizing an open-domain QA system to a restricted do-
main. Nyberg et al (2005) provides a case study that describes the problems in adapting
an existing open-domain QA system to be able to deal with knowledge from existing
domain ontologies.
7. Building a Restricted-Domain QA System: Main Considerations
It is difficult to imagine a general methodology for the development of an RDQA
system. On the one hand, current systems are overly influenced by the specific
53
Computational Linguistics Volume 33, Number 1
characteristics and requirements of the domains, from the different types of questions
to be answered to the heterogeneity of the knowledge available for the domain. On the
other hand, the known methodological proposals (Minock 2005) are so general that they
could be used to design any kind of information system.
Rather than propose a design methodology, we want to emphasize the main points
to be taken into consideration when designing a QA system for a specific domain. These
points are related to the analysis and modeling of the domain information and the
selection of the appropriate technology required by the QA system. They can be listed
as follows:
 domain query system analysis
 domain knowledge selection
 domain knowledge acquisition and representation
 system interface design
 technological requirements selection
Domain query system analysis: Knowing in detail all the different ways users ask for
information is a prerequisite for being effective in a restricted-domain scenario.
Questions need to be analyzed, classified, and associated with the different types
of information the users request. The kinds of questions in a restricted domain
may vary from general open-domain factoid and definition questions to very
special kinds of questions that depend on the selected domain.
Domain knowledge selection: The amount and type of authoritative knowledge
available for computational treatment is especially variable across different
domains. For instance, there are plenty of resources for biomedical (Zweigenbaum
2003) or technical related domains, whereas, on the other hand, less popular
domains (such as the legal domain) have minimal elaborated knowledge but
have the advantage of enormous quantities of raw text. Domain information
can be represented in different formats: from unstructured plain text documents
to semi-structured (e.g., templates, SGML annotated text) or highly structured
knowledge encoded in large databases and authoritative ontologies. Selecting the
appropriate domain knowledge resources in each particular case is an important
aspect in the design of an RDQA.
Domain knowledge acquisition and representation: Using the domain knowledge
for QA purposes requires the definition of an internal representation model
that allows the integration or combination of the different information sources
available for the domain. The complexity of the representation model used will
be proportional to the complexity of the information sources needed for encoding
domain knowledge. The model selected for domain knowledge representation
will also determine the kind of operational processes and reasoning techniques
allowed in the domain.
System interface design: In order to obtain a natural mode of communication between
users and the system, the interaction needs to be tailored according to the
domain characteristics and the user requirements. Usually, natural language
(NL) interfaces are preferred because they allow fluent communication between
the users and the system. Nevertheless, as current natural language processing
54
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
technologies do not allow the automatic translation of natural language text into
a fully unambiguous content representation, NL interfaces may be sometimes
substituted by template-like interfaces or unambiguous formal outputs (only
useful for expert users) when exact knowledge understanding and representation
is required.
Technological requirements selection: The abilities we expect from an RDQA system
depend explicitly on the different aspects of the domain analysis that we have
presented before. Decisions on the specific technology and methods to use will be
taken according to the type of questions to be solved, the availability of specialized
resources, and the representational model used for encoding the domain knowl-
edge. As discussed in previous sections (see Sections 4 and 6), QA in restricted
domains usually requires techniques that differ substantially from the techniques
used in open-domain systems. Restricted domains enable the possibility of using
comprehensive ontological knowledge, thus making it possible to perform more
complex inferences than in open-domain QA and therefore leveraging the possi-
bility of answering more complex questions. From this perspective taking accurate
design decisions customized to the task requirements and the domain resources is
essential.
8. Introduction to the Articles in this Special Section
Demner-Fushman and Lin?s article (Answering clinical questions with knowledge-based
and statistical techniques) extends previous work by the authors (Demner-Fushman and
Lin 2005) on a QA system in the medical domain. The system is designed to satisfy
information needs within the framework of evidence-based medicine (EBM) whereby
a doctor needs to gather the current best evidence, namely, high-quality patient-
centered clinical research. The data source used by the system is the set of MEDLINE
abstracts, a large bibliographic database that is accessed on-line via PubMed. Input
questions in this domain are highly specific and complex. Following practice in the
domain, the input questions are formulated as PICO-based frames representing the
major elements of a query in EBM: Problem/Population, Intervention, Comparison,
and Outcome. A central task of the system is the automatic identification of PICO
elements in the MEDLINE abstracts and their matching with the input query frame.
In the process the system uses the Unified Medical Language System (UMLS), an
extensive ontology specialized on this domain. This system is a clear example of the
adaptation of the task of question answering to a specific and highly practical domain
using specialized resources in order to satisfy information needs formulated as complex,
structured questions.
Hallett, Scott, and Power?s article (Composing questions through conceptual authoring)
focuses on the stage of question formulation. Questions in a QA system over a spe-
cialized domain where the users are domain experts are typically complex in nature.
This results in a problem both for the user, who needs to provide all the specific
information in the question, and to the system that needs to analyze the question.
The solution proposed in this article is to facilitate question formulation by means of
Conceptual Authoring, whereby the user edits a formal representation of the query and
receives feedback from an automatically generated natural language representation of
that query. The article describes this method within the context of a QA system for a
database of electronic health records. An analysis of the question model in this domain
55
Computational Linguistics Volume 33, Number 1
is presented, together with an evaluation of the usability of the method. This article
presents a concept of complex query formulation that can potentially be ported to other
specialized domains.
9. Conclusions
In this article we have presented an overview of methods used in QA in restricted
domains and we have argued for developing research in this area. To conclude we
would like to comment on two reasons for developing question answering in restricted
domains:
Development of vertical systems: Restricted domains allow the development of sys-
tems that can provide the full range of processing levels and achieve a com-
plete, end-to-end application. It therefore becomes possible to develop complete
systems that can be used without the need for any time-consuming training on
the methods required to formulate questions or to interpret the system results.
Furthermore, restricted domains can provide a focus for the research and develop-
ment of generic theories on complex question answering in particular and natural
language processing in general. A clear example is the UC project developed in
the 1980s. By reducing the research space it becomes possible to focus on solving
complex problems that would not be attempted if the main drive was to produce
a system that works in an open-domain fashion.
Applicability to current needs: General and broad scope systems are not effective in
domains restricted to the interests of different kind of users: from employees in
institutions and companies trying to find information in manuals and procedures,
to professionals in specialized domains like law, medicine, biology, mechanics,
programming, and so on. Notice that professionals in each of these areas re-
quire different types of information in their daily activities (e.g., there is a con-
siderable difference between looking for general information on the Internet as
opposed to looking for the empty weight of a wing of the Airbus A319 in a
technical manual).
A major difference between open-domain question answering and restricted-domain
question answering is the existence of domain-dependent information that can be used
to improve the accuracy of the system. Much of the focus of this article has been on
forms of tapping information from these resources.
Some domains are more appropriate for developing question answering systems
than others. A domain must be circumscribed enough so that a comprehensive on-
tological resource can be built for the domain. A domain must be complex enough
so that it presents challenging research problems in the area of natural language
processing. Finally, a domain must be practical enough so that the end product is
useful to a significant segment of the population. Domains (such as, for example,
biomedicine) that meet al these properties are naturally more popular for researchers
and developers. Consequently they have some of the best ontological information and
large corpora of texts and questions that can be used for the development of such
QA systems.
Question answering on restricted domains requires the processing of complex ques-
tions and offers the opportunity to carry out complex analysis of the text sources and
the questions. Restricted domains also provide comprehensive ontologies and domain
56
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
resources that can help in the task of processing complex questions and finding the
answers. The challenges and opportunities are there for us to take.
References
Abney, S. 1996. Part-of-speech Tagging and
Partial Parsing. Corpus-Based Methods in
Language and Speech. Kluwer Academic
Publishers, Dordrecht.
Ait-Mokhtar, Salah and Jean-Pierre Chanod.
1997. Incremental finite-state parsing. In
Fifth Conference on Applied Natural Language
Processing (ANLP 97), pages 72?79,
Washington, DC.
Androutsopoulos, I., G. D. Ritchie, and
P. Thanisch. 1995. Natural language
interfaces to databases?an introduction.
Natural Language Engineering, 1(1):29?81.
Barker, Ken, Vinay K. Chaudhri, Shaw Yi
Chaw, Peter Clark, James Fan, David
Israel, Sunil Mishra, Bruce W. Porter,
Pedro Romero, Dan Tecuci, and Peter Z.
Yeh. 2004. A Question-answering system
for AP chemistry: Assessing KR&R
technologies. In Principles of Knowledge
Representation and Reasoning: Proceedings
of the Ninth International Conference
(KR2004), pages 488?497, Whistler,
Canada.
Basili, Roberto, Dorte H. Hansen, Patrizia
Paggio, Maria Teresa Pazienza, and
Fabio Massimo Zanzotto. 2004.
Ontological resources and question
answering. In Workshop on Pragmatics of
Question Answering, held jointly with
NAACL 2004, Boston, Massachusetts.
Basili, Roberto and Fabio Massimo Zanzotto.
2002. Parsing engineering and empirical
robustness. Natural Language Engineering,
8(2/3): 97?120.
Benamara, Farah. 2004. Cooperative question
answering in restricted domains: The
WEBCOOP Experiments. In Workshop on
Question Answering in Restricted Domains.
42nd Annual Meeting of the Association for
Computational Linguistics (ACL-2004),
pages 31?38, Barcelona, Spain.
Brill, Eric, Jimmy Lin, Michele Banko,
Susan Dumais, and Andrew Ng. 2001.
Data-intensive question answering. In
Proceedings TREC 2001, number 500?250
in NIST Special Publications. NIST,
pages 393?400, Gaithersberg, MD.
Ceusters, Werner, Barry Smith, and
Maarten Van Mol. 2003. Using
ontology in query answering systems:
Scenarios, requirements and challenges.
In 2nd CoLogNET-ElsNET Symposium.
Questions and Answers: Theoretical and
Applied Perspectives, Amsterdam.
Chung, Hoojung, Young-In Song,
Kyoung-Soo Han, Do-Sang Yoon,
Joo-Young Lee, and Hae-Chang Rim.
2004. A Practical QA System in Restricted
Domains. In Workshop on Question
Answering in Restricted Domains. 42nd
Annual Meeting of the Association for
Computational Linguistics (ACL-2004),
pages 39?45, Barcelona, Spain.
Cohen, P., R. Schrag, E. Jones, A. Pease,
A. Lin, B. Starr, D. Easter, D. Gunning,
and M. Burke. 1988. The DARPA high
performance knowledge bases project.
AI Magazine, 19(4):25?49.
Demner-Fushman, Dina and Jimmy Lin.
2005. Knowledge extraction for clinical
question answering: Preliminary results.
In Workshop on Question Answering in
Restricted Domains. 20th National Conference
on Artificial Intelligence (AAAI-05),
pages 1?9, Pittsburgh, PA.
Diekema, Anne R., Ozgur Yilmazel, and
Elizabeth D. Liddy. 2004. Evaluation of
restricted domain question-answering
systems. In Workshop on Question
Answering in Restricted Domains. 42nd
Annual Meeting of the Association for
Computational Linguistics (ACL-2004),
pages 2?7, Barcelona, Spain.
Doan-Nguyen, Hai and Leila Kosseim.
2004. The problem of precision in
restricted-domain question-answering.
Some proposed methods of improvement.
In Workshop on Question Answering in
Restricted Domains. 42nd Annual Meeting
of the Association for Computational
Linguistics (ACL-2004), pages 8?15,
Barcelona, Spain.
Ely, J., J. Osheroff, M. Ebell, M. Chambliss,
D. Vinson, J. Stevermer, and E. Pifer.
2002. Obstacles to answering doctors?
questions about patient care with
evidence: Qualitative study. British
Medical Journal, 324:710?713.
Fellbaum, Christiane. 1998. WordNet:
Introduction. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database, Language, Speech, and
Communication. MIT Press, Cambrige,
MA, pages 1?19.
Ferre?s, Daniel and Horacio Rodr??guez.
2006. Experiments adapting an
open-domain question answering
system to the geographical domain
using scope-based resources. In 11th
Conference of the European Chapter of the
57
Computational Linguistics Volume 33, Number 1
Association of Computational Linguistics.
Workshop on Multilingual Question
Answering - MLQA?06, Trento, Italy.
Frank, Anette, Hans-Ulrich Krieger, Feiyu
Xu, Hans Uszkoreit, Berthold Crysmann,
Brigitte Jo?rg, and Ulrich Scha?fer. 2006.
Question answering from structured
knowledge sources. Journal of Applied
Logic, Special Issue on Questions and
Answers: Theoretical and Applied
Perspectives, 1:29.
Frank, Anette, Hans-Ulrich Krieger, Feiyu
Xu, Hans Uszkoreit, Berthold Crysmann,
Brigitte Jo?rg, and Ulrich Scha?fer. 2005.
Querying structured knowledge sources.
In Workshop on Question Answering in
Restricted Domains. 20th National Conference
on Artificial Intelligence (AAAI-05),
pages 10?19, Pittsburgh, PA.
Gabbay, Igal. 2004. Retrieving Definitions from
Scientific Text in the Salmon Fish Domain by
Lexical Pattern Matching. Ph.D. thesis,
University of Limerick.
Galitsky, Boris. 2001a. A natural language
question answering system for human
genome domain. In Proceedings of the
2nd IEEE International Symposium on
Bioinformatics and Bioengineering,
Bethesda, MD.
Galitsky, Boris. 2001b. Semi-structured
knowledge representation for the
automated financial advisor. In Proceedings
of the Fourteenth International Conference on
Industrial and Engineering Applications of
Artificial Intelligence and Expert Systems,
pages 874?879, Budapest, Hungary.
Green, B. F., A. K. Wolf, C. Chomsky, and
K. Laughery. 1961. Baseball: An automatic
question answerer. In Proceedings Western
Computing Conference, volume 19,
pages 219?224.
Hejazi, Mahmoud R., Maryam S. Mirian,
Kourosh Neshatian, Bahador R. Ofoghi,
and Ehsan Darudi. 2004. An
ontology-based question answering
system with auto extraction and
categorization capabilities in the
domain of telecommunications.
The CSI Journal on Computer Science
and Engineering, 2(1).
Hirschman, Lynette and Rob Gaizauskas.
2001. Natural language question
answering: The view from here. Natural
Language Engineering, 7(4):275?300.
Hobbs, Jerry R., Douglas Appelt, John Bear,
David Israel, Megumi Kameyama,
Mark Stickel, and Mabry Tyson. 1997.
FASTUS: A Cascaded Finite-state
Transducer for Extracting Information from
Natural-Language Text. MIT Press,
Cambridge, MA.
Johnson, Christopher and Charles J.
Fillmore. 2000. The FrameNet tagset for
frame-semantic and syntactic coding of
predicate-argument structure. In Janyce
Wiebe, editor, Proceedings of the 1st Meeting
of the North American Chapter of the
Association for Computational Linguistics,
Seattle, WA.
Kacmarcik, Gary. 2005. Question answering
in role-playing games. In Workshop
on Question Answering in Restricted
Domains. 20th National Conference on
Artificial Intelligence (AAAI-05),
pages 51?55. Pittsburgh, PA.
Kando, Noriko. 2005. Overview of the
fifth NTCIR workshop. In Proceedings
of the Fifth NTCIR Workshop Meeting on
Evaluation of Information Access Technologies:
Information Retrieval, Question Answering
and Cross-Lingual Information Access,
Tokyo, Japan.
Katz, Boris. 1997. From sentence processing
to information access on the World Wide
Web. In AAAI Spring Symposium on Natural
Language Processing for the World Wide Web,
pages 77?94, Stanford, CA.
Katz, Boris, Sue Felshin, Deniz Yuret, Ali
Ibrahim, Jimmy Lin, Gregory Marton,
Alton Jerome McFarland, and Baris
Temelkuran. 2002. Omnibase: Uniform
access to heterogeneous data for question
answering. In Proceedings of the 6th
International Conference on Applications of
Natural Language to Information Systems,
pages 230?234, Stockholm, Sweden.
Katz, Boris, Jimmy J. Lin, and Sue Felshin.
2002. The START multimedia information
system: Current technology and future
directions. In Proceedings of the International
Workshop on Multimedia Information
Systems, Tempe, AZ.
Kim, Soo-Min and Eduard Hovy. 2005.
Identifying opinion holders for
question answering in opinion texts.
In Workshop on Question Answering
in Restricted Domains. 20th National
Conference on Artificial Intelligence
(AAAI-05), pages 20?26, Pittsburgh, PA.
Lenat, D. and R. V. Guha. 1990. Building Large
Knowledge-Based Systems: Representation and
Inference in the Cyc Project. Addison-Wesley.
Lin, Jimmy. 2002. The Web as a resource
for question answering: Perspectives
and challenges. In Proceedings of the Third
International Conference on Language
Resources and Evaluation, pages 2120?2127,
Las Palmas, Spain.
58
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
Lindberg, D. A., B. L. Humphreys, and
A. T. McCray. 1993. The unified medical
language system. Methods of Information in
Medicine, 32(4):281?291.
Minock, Michael. 2005. Where are the ?killer
applications? of restricted domain question
answering? In Proceedings of the IJCAI
Workshop on Knowledge Reasoning in
Question Answering, page 4, Edinburgh,
Scotland.
Moldovan, Dan and Adrian Novischi. 2002.
Lexical chains for question answering.
In Proceedings of the 19th International
Conference on Computational Linguistics,
Taipei, Taiwan.
Molla?, Diego, Rolf Schwitter, Michael Hess,
and Rachel Fournier. 2000. Extrans, an
answer extraction system. Traitement
Automatique des Langues, 41(2):495?522.
Molla?, Diego and Jose? Luis Vicedo, editors.
2004. Workshop on Question Answering in
Restricted Domains. 42th Annual Meeting of
the Association for Computational Linguistics
(ACL-2004), Barcelona, Spain.
Molla?, Diego and Jose? Luis Vicedo, editors.
2005. Workshop on Question Answering in
Restricted Domains. Twentieth National
Conference on Artificial Intelligence
(AAAI-05), Pittsburgh, Pennsylvania, USA.
Niu, Yun and Graeme Hirst. 2004. Analysis
of semantic classes in medical text
for question answering. In Workshop
on Question Answering in Restricted
Domains. 42nd Annual Meeting of
the Association for Computational
Linguistics (ACL-2004), pages 54?61,
Barcelona, Spain.
Noy, N. F. and D. L. McGuinness. 2001.
Ontology development 101: A guide to
creating your first ontology. Technical
Report KSL-01-05, Knowledge Systems
Laboratory.
Nyberg, Eric, Teruko Mitamura, Robert
Frederking, Vasco Pedro, Matthew W.
Bilotti, Andrew Schlaikjer, and Kerry
Hannan. 2005. Extending the JAVELIN
system with domain semantics.
In Question Answering in Restricted
Domains: Papers from the AAAI Workshop,
pages 36?40, Pittsburgh, PA.
Popescu, Ana-Maria, Oren Etzioni,
and Henry Kautz. 2003. Towards a
theory of natural language interfaces to
databases. In Proceedings of the 2003
International Conference on Intelligent
User Interfaces (IUI-03), pages 149?157,
New York.
Porter, B., J. Lester, K. Murray, K. Pittman,
A. Souther, L. Acker, and T. Jones.
1988. AI research in the context of a
multifunctional knowledge base:
The botany knowledge base project.
Technical Report, AI-88-88, Department
of Computer Sciences, University of
Texas at Austin.
Rinaldi, Fabio, James Dowdall, and Gerold
Schneider. 2004. Answering questions
in the genomics domain. In Proceedings of
the ACL04 Workshop on Question Answering
in Restricted Domains, pages 46?53,
Barcelona, Spain.
Rinaldi, Fabio, Michael Hess, James
Dowdall, Diego Molla?, and Rolf Schwitter.
2004. Question answering in
terminology-rich technical domains. In
Mark T. Maybury, editor, New Directions in
Question Answering. AAAI Press/MIT
Press, Cambridge, MA, pages 71?82.
Rotaru, Mihai and Diane J. Litman. 2005.
Improving question answering for
reading comprehension tests by
combining multiple systems. In Workshop
on Question Answering in Restricted
Domains. 20th National Conference
on Artificial Intelligence (AAAI-05),
pages 46?50, Pittsburgh, PA.
Sang, Erik Tjong Kim, Gosse Bouma, and
Maarten de Rijke. 2005a. Developing
offline strategies for answering medical
questions. In Workshop on Question
Answering in Restricted Domains.
20th National Conference on Artificial
Intelligence (AAAI-05), pages 41?45,
Pittsburgh, PA.
Schrag, R., M. Pool, V. Chaudhri, R. C.
Kahlert, J. Powers, P. Cohen,
J. Fitzgerald, and S. Mishra. 2002.
Experimental evaluation of subject
matter expert-oriented knowledge base
authoring tools. In Proceedings of the
2002 PerMIS Workshop: Measuring the
Performance and Intelligence of Systems,
pages 272?279, Gaithersburg, MD.
Simmons, R. F. 1965. Answering English
questions by computer: A survey.
Communications of the ACM, 8(1):53?70.
Srihari, Rohini and Wei Li. 2000. Information
extraction supported question answering.
In Proceedings of TREC 8 (1999),
pages 185?196, Gaithersburg, MD.
Tsur, Oren, Maarten de Rijke, and Khalil
Sima?an. 2004. BioGrapher: Biography
questions as a restricted domain question
answering task. In Workshop on Question
Answering in Restricted Domains. 42nd
Annual Meeting of the Association for
Computational Linguistics (ACL-2004),
pages 23?30, Barcelona, Spain.
59
Computational Linguistics Volume 33, Number 1
Vallin, Alessandro, Bernardo Magnini,
Danilo Giampiccolo, Lili Aunimo,
Christelle Ayache, Petya Osenova,
Anselmo Pe nas, Maarten de Rijke,
Bogdan Sacaleanu, Diana Santos, and
Richard Sutcliffe. 2005. Overview of
the CLEF 2005 multilingual question
answering track. In Proceedings of
CLEF 2005, Vienna, Austria.
Vargas-Vera, Maria and Enrico Motta. 2004.
AQUA: A question answering system for
heterogeneous sources. Technical Report
KMI-04-20, KMI.
Vargas-Vera, Maria, Enrico Motta, and
John Domingue. 2003. AQUA: An
ontology-driven question answering
system. In Mark T. Maybury, editor, New
Directions in Question Answering, Papers
from 2003 AAAI Spring Symposium,
Stanford University, pages 53?57.
Stanford, CA.
Voorhees, Ellen M. 1999. The TREC-8
question answering track report. In
Proceedings of TREC-8, pages 77?82,
Gaithersburg, MD.
Voorhees, Ellen M. 2001. The TREC question
answering track. Natural Language
Engineering, 7(4):361?378.
Vossen, Piek, editor. 1998. Euro WordNet:
A Multilingual Database with Lexical
Semantic Networks. Kluwer Academic
Publishers, Dordrecht, Holland.
Weischedel, Ralph, Jinxi Xu, and
Ana Licuanan. 2004. A hybrid
approach to answering biographical
questions. In Mark T. Maybury,
editor, New Directions in Question
Answering. AAAI Press/MIT
Press, Cambridge, MA, chapter 5,
pages 59?69.
Wilensky, Robert, David N. Chin, Marc
Luria, James Martin, James Mayfield,
and Dekai Wu. 1994. The Berkeley
Unix Consultant project. Computational
Linguistics, 14(4):35?84.
Woods, William A. 1997. Conceptual
indexing: A better way to organize
knowledge. Technical Report SMLI
TR-97-61, Sun Microsystems, Inc.
Yu, Hong, Carl Sable, and Hai Ran Zhu.
2005. Classifying medical questions
based on an Evidence Taxonomy. In
Workshop on Question Answering in
Restricted Domains. 20th National
Conference on Artificial Intelligence
(AAAI-05), pages 27?35, Pittsburgh, PA.
Zajac, Re?mi. 2001. Towards ontological
question answering. In Proceedings of
ACL2001, Workshop on Open Domain
QA, Toulouse.
Zweigenbaum, Pierre. 2003. Question
answering in biomedicine. In Proceedings
of EACL2003, Workshop on NLP for
Question Answering, Budapest.
Appendix A: List of QA Systems in Restricted Domains
The following list is by no means exhaustive. Our purpose in presenting this list is to
show the breadth of current research and applications in RDQA. We welcome updates
and additions to the list, which will be maintained at http://www.ics.mq.edu.au/
?diego/answerfinder/rdqa/.
1. Generic systems
 JAVELIN (Nyberg et al 2005)
? http://www.cs.cmu.edu/?ehn/JAVELIN/
 QUETAL (Frank et al 2005, 2006)
? http://www.dfki.de/pas/f2w.cgi?ltp/quetal-e
 AQUA (Vargas-Vera and Motta 2004; Vargas-Vera, Motta, and
Domingue 2003)
? http://kmi.open.ac.uk/projects/akt/aqua/
? http://kmi.open.ac.uk/projects/akt/publications.cfm
 START (Katz 1997; Katz et al 2002; Katz, Lin, and Felshin 2002)
? http://start.csail.mit.edu/
2. Collaborative learning for engineering education
 KAAS (Diekema, Yilmazel, and Liddy 2004)
60
Molla? and Vicedo Question Answering in Restricted Domains: An Overview
3. Services provided by a large company
 Concordia University system (Doan-Nguyen and Kosseim 2004)
4. Salmon fish biology
 SOK-I (Gabbay 2004)
5. Biography information
 BioGrapher (Tsur, de Rijke, and Sima?an 2004)
 BBN Technologies (Weischedel, Xu, and Licuanan 2004)
6. Tourism
 WEBCOOP (Benamara 2004)
7. Weather forecasts
 System by Korea University and Sangmyung University
(Chung et al 2004)
8. Technical domains
 ExtrAns (Rinaldi et al 2004)
? http://www.ifi.unizh.ch/cl/extrans/
 TeLQAS (Hejazi et al 2004)
? http://www.neshatian.org/projects/telqas/
9. Genomics
 ExtrAns (Rinaldi, Dowdall, and Schneider 2004)
 System by KnowledgeTrail (Galitsky 2001a)
10. Financial
 System by KnowledgeTrail (Galitsky 2001b)
11. Medical domain
 EpoCare (Niu and Hirst 2004)
 system by University of Maryland (Demner-Fushman and
Lin 2005)
 question classification by Columbia University and Cooper Union
(Yu, Sable, and Zhu 2005)
 IMIX
12. Geographic domain
 System by UPC (Ferra?s and Rodr??guez 2006)
13. Nobel prizes
 System by DFKI (Frank et al 2005)
14. Language technology
 System by DFKI (Frank et al 2005)
15. Opinion texts
 System by University of Southern California (Kim and Hovy 2005)
16. Reading comprehension texts
 RC QA (Rotaru and Litman 2005)
17. Role-playing games
 System by Microsoft Research (Kacmarcik 2005)
61

