A Divide-and-Conquer Strategy for Shallow Parsing of German 
Free Texts 
Gi in ter  Neumann*  Chr i s t ian  Braun t Jakub  P i skorsk i  ~ 
Abst ract  
We present a divide-and-conquer st ategy based on 
finite state technology for shallow parsing of real- 
world German texts. In a first phase only the topo- 
logical structure of a sentence (i.e., verb groups, 
subclauses) are determined. In a second phase the 
phrasal grammars are applied to the contents of the 
different fields of the main and sub-clauses. Shallow 
parsing is supported by suitably configured prepro- 
cessing, including: morphological and on-line com- 
pound analysis, efficient POS-filtering, and named 
entity recognition. The whole approach proved to 
be very useful for processing of free word order lan- 
guages like German. Especially for the divide-and- 
conquer parsing strategy we obtained an f-measure 
of 87.14% on unseen data. 
1 In t roduct ion  
Current information extraction (IE) systems are 
quite successful in efficient processing of large free 
text collections due to the fact that they can provide 
a partial understanding of specific types of text with 
a certain degree of partial accuracy using fast and ro- 
bust language processing strategies (basically finite 
state technology). They have been "made sensitive" 
to certain key pieces of information and thereby pro- 
vide an easy means to skip text without deep anal- 
ysis. The majority of existing IE systems are ap- 
plied to English text, but there are now a number of 
systems which process other languages as well (e.g., 
German (Neumann et al, 1997), Italian (Ciravegna 
et al, 1999) or Japanese (Sekine and Nobata, 1998)). 
The majority of current systems perform a partial 
parsing approach using only very few general syntac- 
tic knowledge for the identification of nominal and 
prepositional phrases and verb groups. The combi- 
nation of such units is then performed by means of 
domain-specific templates. Usually, these templates 
* DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, 
Germany, neumann@dfki, de 
t DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, 
Germany, cbratm@dfki, de 
DFKI GmbH, Stuhlsatzenhausweg 3, 66123 Saarbriicken, 
Germany, piskorsk@dfki, de 
are triggered by domain-specific predicates attached 
only to a relevant subset of verbs which express 
domain-specific selectional restrictions for possible 
argument fillers. 
In most of the well-known shallow text process- 
ing systems (cf. (Sundheim, 1995) and (SAIC, 
1998)) cascaded chunk parsers are used which per- 
form clause recognition after fragment recognition 
following a bottom-up style as described in (Abne), 
1996). We have also developed a similar bottom- 
up strategy for the processing of German texts, cf. 
(Neumann et al, 1997). However, the main prob- 
lem we experienced using the bottom-up strategy 
was insufficient robustness: because the parser de- 
pends on the lower phrasal recognizers, its perfor- 
mance is heavily influenced by their respective per- 
formance. As a consequence, the parser frequently 
wasn't able to process tructurally simple sentences, 
because they contained, for example, highly complex 
nominal phrases, as in the following example: 
"\[Die vom Bundesgerichtshof und den 
Wettbewerbshfitern als Verstofi gegen 
das Kartellverbot gegeiflelte zentrale TV- 
Vermarktung\] ist g~ngige Praxis." 
Central television raarketing, censured by the 
German Federal High Court and the guards 
against unfair competition as an infringement 
of anti-cartel legislation, is common practice. 
During free text processing it might be not possible 
(or even desirable) to recognize such a phrase com- 
pletely. However, if we assume that domain-specific 
templates are associated with certain verbs or verb 
groups which trigger template filling, then it will be 
very difficult to find the appropriate fillers without 
knowing the correct clause structure. Furthermore 
in a sole bottom-up approach some ambiguities - for 
example relative pronouns - can't be resolved with- 
out introducing much underspecification into the in- 
termediate structures. 
Therefore we propose the following divide-and- 
conquer parsing strategy: In a first phase only 
the verb groups and the topological structure of 
a sentence according to the linguistic field the- 
239 
"\[CooraS \[sse,,* Diese Angaben konnte der Bundes- 
grenzschutz aber nicht best~itigen\], [ssent Kinkel 
sprach von Horrorzahlen, \[relct denen er keinen 
Glauben schenke\]\]\]." 
This information couldn't be verified by the Border 
Police, Kinkel spoke of horrible figures that he didn't 
believe. 
Figure 1: An example of a topological structure. 
PREPROCESSOR 
-TokeNlatl~ 
, v .  
! 
DC-PARSER 
TOPOt.OGICAL P ~  
v , ,~ ,~,~ .. 
FRAGMENT RECOGNIZER 
Underspeclfied dependency trees 
Figure 2: Overview of the system's architecture. 
ory (cf. (Engel, 1988)) are determined omain- 
independently. In a second phase, general (as well 
as domain-specific) phrasal grammars (nominal and 
prepositional phrases) are applied to the contents of 
the different fields of the main and sub-clauses ( ee 
fig. 1) 
This approach offers several advantages: 
? improved robustness, because parsing of the 
sentence topology is based only on simple in- 
dicators like verbgroups and conjunctions and 
their interplay, 
? the resolution of some ambiguities, including 
relative pronouns vs. determiner, sub junction 
vs. preposition and sentence coordination vs. 
NP coordination, and 
? a high degree of modularity (easy integration of 
domain-dependent subcomponents). 
The shallow divide-and-conquer parser (DC- 
PARSER) is supported by means of powerful mor- 
phological processing (including on-line compound 
analysis), efficient POS-filtering and named entity 
recognition. Thus the architecture of the complete 
shallow text processing approach consists basically 
of two main components: the preprocessor and the 
DC-PARSER itself (see fig. 2). 
2 Preprocessor  
The DC-PARSER relies on a suitably configured pre- 
processing strategy in order to achieve the desired 
simplicity and performance. It consists of the fol- 
lowing main steps: 
Tokenization The tokenizer maps sequences of 
consecutive characters into larger units called tokens 
and identifies their types. Currently we use more 
than 50 domain-independent token classes including 
generic classes for semantically ambiguous tokens 
(e.g., "10:15" could be a time expression or volley- 
ball result, hence we classify this token as number- 
dot compound) and complex classes like abbrevia- 
tions or complex compounds (e.g., "AT&T-Chief"). 
It proved that such variety of token classes impli- 
fies the processing of subsequent submodules signif- 
icantly. 
Morphology Each token identified as a potential 
wordform is submitted to the morphological nalysis 
including on-line recognition of compounds (which is 
crucial since compounding is a very productive pro- 
cess of the German language) and hyphen coordina- 
tion (e.g., in "An- und Verkauf" (purchase and sale) 
"An-" is resolved to "Ankauf" (purchase)). Each 
token recognized as a valid word form is associated 
with the list of its possible readings, characterized 
by stem, inflection information and part of speech 
category. 
POS-fi ltering Since a high amount of German 
word forms is ambiguous, especially word forms with 
a verb reading 1 and due to the fact that the qual- 
ity of the results of the DC-PARSER relies essen- 
tially on the proper recognition of verb groups, ef- 
ficient disambiguation strategies are needed. Using 
case-sensitive rules is straightforward since generally 
only nouns (and proper names) are written in stan- 
dard German with a capitalized initial letter (e.g., 
"das Unternehmen" - the enterprise vs. "wir un- 
ternehmen" - we undertake). However for disam- 
biguation of word forms appearing at the beginning 
of the sentence local contextual filtering rules are 
applied. For instance, the rule which forbids the 
verb written with a capitalized initial letter to be 
followed by a finite verb would filter out the verb 
reading of the word "unternehmen" i  the sentence 
130% of the wordforms in the test corpus 
"Wirtschaftswoche" (business news journal) ,  which have a 
verb reading, turned to have at least one other non-verb 
reading. 
240 
"Unternehmen sind an Gewinnmaximierung intere- 
siert." (Enterprises are interested in maximizing 
their profits). A major subclass of ambiguous word- 
forms are those which have an adjective or attribu- 
tivly used participle reading beside the verb reading. 
For instance, in the sentence "Sie bekannten, die 
bekannten Bilder gestohlen zu haben." (They con- 
fessed they have stolen the famous paintings.) the 
wordform "bekannten" is firstly used as a verb (con- 
fessed) and secondly as an adjective (famous). Since 
adjectives and attributively used participles are in 
most cases part of a nominal phrase a convenient 
rule would reject the verb reading if the previous 
word form is a determiner or the next word form is 
a noun. It is important o notice that such rules 
are based on some regularities, but they may yield 
false results, like for instance the rule for filtering 
out the verb reading of some word forms extremely 
rarely used as verbs (e.g., "recht" - right, to rake 
(3rd person,sg)). All rules are compiled into a sin- 
gle finite-state transducer according to the approach 
described in (Roche and Schabes, 1995). 2 
Named ent i ty  f inder  Named entities such as or- 
ganizations, persons, locations and time expressions 
are identified using finite-state grammars. Since 
some named entities (e.g. company names) may ap- 
pear in the text either with or without a designator, 
we use a dynamic lexicon to store recognized named 
entities without their designators (e.g., "Braun AG" 
vs. "Braun") in order to identify subsequent occur- 
rences correctly. However a named entity, consisting 
solely of one word, may be also a valid word form 
(e.g., "Braun" - brown). Hence we classify such 
words as candidates for named entities since gen- 
erally such ambiguities cannot be resolved at this 
level. Recognition of named entities could be post- 
poned and integrated into the fragment recognizer, 
but performing this task at this stage of processing 
seems to be more appropriate. Firstly because the 
results of POS-filtering could be partially verified 
and improved and secondly the amount of the word 
forms to be processed by subsequent modules could 
be considerably reduced. For instance the verb read- 
ing of the word form "achten" (watch vs. eight) in 
the time expression "am achten Oktober 1995" (at 
the eight of the October 1995) could be filtered out 
if not done yet. 
3 A Sha l low D iv ide -and-Conquer  
S t ra tegy  
The DC-PARSER consists of two major domain- 
independent modules based on finite state technol- 
2The manually constructed rules proved to be a useful 
means for disambiguation, however not sufficient enough to 
filter out all unplausible readings. Hence supplementary rules 
determined by Brill's tagger were used in order to achieve 
broader coverage. 
ogy: 1) construction of the topological sentence 
structure, and 2) application of phrasal grammars 
on each determined subclause (see also fig. 3). In 
this paper we will concentrate on the first step, be- 
cause it is the more novel part of the DC-PARSER, 
and will only briefly describe the second step in sec- 
tion 3.2. 
3.1 Topo log ica l  s t ruc ture  
The DC-PARSER applies cascades of finite-state 
grammars to the stream of tokens and named en- 
titles delivered by the preprocessor in order to de- 
termine the topological structure of the sentence ac- 
cording to the linguistic field theory (Engel, 1988). 3
Based on the fact that in German a verb group 
(like "h~tte fiberredet werden mfissen" - -  *have con- 
vinced been should meaning should have been con- 
vinced) can be split into a left and a right verb part 
("h?tte" and "fiberredet werden miissen") these 
parts (abbreviated as LVP and RVP) are used for the 
segmentation of a main sentence into several parts: 
the front field (VF), the left verb part, middle field 
(MF), right verb part, and rest field (RF). Subclauses 
can also be expressed in that way such that the left" 
verb part is either empty or occupied by a relative 
pronoun or a sub junction element, and the complete 
verb group is placed in the right verb part, cf. figure 
3. Note that each separated field can be arbitrarily 
complex with very few restrictions on the ordering 
of the phrases inside a field. 
Recognition of the topological structure of a sen- 
tence can be described by the following four phases 
realized as cascade of finite state grammars (see also 
fig. 2; fig. 4 shows the different steps in action). 4
Initially, the stream of tokens and named entities is 
separated into a list of sentences based on punctua- 
tion signs. 5 
Verb  groups  A verb grammar ecognizes all sin- 
gle occurrences of verbforms (in most cases corre- 
sponding to LVP) and all closed verbgroups (i.e., se- 
quences of verbforms, corresponding to RVP). The 
parts of discontinuous verb groups (e.g., separated 
LvP and RVP or separated verbs and verb-prefixes) 
cannot be put together at that step of processing 
because one needs contextual information which will 
only be available in the next steps. The major prob- 
lem at this phase is not a structural one but the 
3Details concerning the implementation f the topological 
parsing strategy can be found in (Braun, 1999). Details con- 
cerning the representation a d compilation of the used finite 
state machinery can be found in (Neumann et al, 1997) 
4In this paper we can give only a brief overview of the 
current coverage of the individual steps. An exhaustive de- 
scription of the covered phenomena can be found in (Braun, 
1999). 
5 Performing this step after preprocessing has the advan- 
tage that the tokenizer and named entity finder already have 
determined abbreviation signs, so that this sort of disam- 
biguation is resolved. 
241 
'$KERH~TE$ 
SENT~ERI  " . "  
VERBI IMODUERB$ 
FORrl~ "lK~s~e uer-k~ut'en" 
flORPHO-INFOt rRCR: ~toPphix-FUek~or$\] 
GE}I~: IRK 
EHSE .. :FI~'S 
FORM: "ve~kau~em" 
P~RPHO-IHFO= R(~R= ~rZ%~'l~iX-i:V~kto~* 
FIHIT| =INF 
G~HUS: ~ 
TI~SE = :PRES 
LST~= "~=ue"  j 
STEM: "w JeSS" 
XSU~J-CL/~ 
CONTENT~ 'ZSPANNSAT L~ 
I1FI "~REL-CL$ 
CONTKHT| Z.SF'FIHHSAT ~
L ~,  (-~,') J h 
='/\[z: / 
\[~: m~-phlx-FVel~to~* J ,?RB: *VERB* 
F01~I.. ? leb%" 
~INFO."  rp~,Rl :l~to~phix-FVek*Oe:l: 
L~ lHI Ts T GEHUSt IR~ ENSIEI IPRIE 
STEM: =leb  ~ 
;~EL-P1RI~4: (" die') 
"RB, \[*~* 1 FORM: "er l i t~en ha%~?e" 
EHSE~ IPERF 
LsTErh "er le id"  J 
) 
Figure 3: The result of the DC-PARSER for the sentence "Weil die Siemens GmbH, die vom Export lebt, 
Verluste erlitten hat, musste sie Aktien verkaufen." (Because the Siemens GmbH which strongly depends on 
exports suffered from losses they had to sell some of the shares.) abbreviated where convenient. It shows 
the separation of a sentence into the front field (vF), the verb group (VERB), and the middle field (MF). The 
elements of different fields have been computed by means of fragment recognition which takes place after 
the (possibly recursive) topological structure has been computed. Note that the front field consists only of 
one but complex subclause which itself has an internal field structure. 
Well die Siemens GmbH, die vom Export lebt, Verluste erlitt, musste sie Aktien verkaufen. 
Well die Siemens GmbH, die ...\[Verb-Fin\], Verl. \[Verb-Fin\], \[Modv-Fin\] sie Akt. \[FV-Inf\]. 
Weil die Siemens GmbH \[ReI-CI\], Verluste \[Verb-Fin\], \[Modv-Fin\] sie Aktien \[FV-Inf\]. 
\[Subconj-CL\], \[Modv-Fin\] sie Aktien \[FV-Inf\]. 
\[Subconj-CL\], \[Modv-Fin\] sie Aktien \[FV-Inf\]. 
\[clause\] 
Figure 4: The different steps of the DC-PARSER for the sentence of figure 3. 
massive morphosyntactic ambiguity of verbs (for ex- 
ample, most plural verb forms can also be non-finite 
or imperative forms). This kind of ambiguity can- 
not be resolved without taking into account a wider 
context. Therefore these verb forms are assigned dis- 
junctive types, similar to the underspecified chunk 
categories proposed by (Federici et al, 1996). These 
types, like for example Fin-Inf-PP or Fin-PP, re- 
flect the different readings of the verbform and en- 
able following modules to use these verb fonns ac- 
cording to the wider context, thereby removing the 
ambiguity. In addition to a type each recognized 
verb form is assigned a set of features which rep- 
resent various properties of the form like tense and 
mode information. (cf. figure 5). 
Base clauses (BC)  are subclauses of type sub- 
junctive and subordinate. Although they are embed- 
ded into a larger structure they can independently 
9~L9 242
I Type VG-f inal 1 Subtype Mod-Perf-Ak Modal-stem kSnn Stem lob Form nicht gelobt haben kann Neg T Agr ... 
Figure 5: The structure of the verb fragment "nicht 
gelobt haben kann" - *not praised have could-been 
meaning could not have been praised 
and simply be recognized on the basis of commas, 
initial elements (like complementizer, interrogative 
or relative item - see also fig. 4, where SUBCONJ- 
CL and REL-CL are tags for subclauses) and verb 
fragments. The different ypes of subclauses are de- 
scribed very compactly as finite state expressions. 
Figure 6 shows a (simplified) BC-structure in fea- 
ture matrix notation. 
"Type Subj-Cl 
Subj wenn 
-Type Spannsatz 
Verb J 
"Type stelltenJ Verb .For m 
MF die Arbeitgeber Forderungen) 
Cont \['Type Iohfi\[l 1~ \] \[Type   mp e-Io: I II \[Verb rType Ve.b 1!!I L~.o. rm .u 'eb"enJl|| 
L MF (als Gegenleistung / / /  neue Stellen) j j /  
Figure 6: Simplified feature matrix of the base clause 
"... ,  wenn die Arbeitgeber Forderungen steUten, 
ohne als Gegenleistung neue Stellen zu schaffen." ... 
if the employers make new demands, without compensat- 
ing by creating new jobs. 
Clause combination It is very often the case that 
base clauses are recursively embedded as in the fol- 
lowing example: 
. . .  well der Hund den Braten gefressen 
hatte, den die Frau, nachdem sie ihn zu- 
bereitet hatte, auf die Fensterbank gestellt 
hatte. 
Because the dog ate the beef which was put on 
the window sill after it had been prepared by 
the woman. 
Two sorts of recursion can be distinguished: 1) 
middle field (MF) recursion, where the embedded 
base clause is framed by the left and right verb parts 
of the embedding sentence, and 2) the rest field (RF) 
recursion, where the embedded clause follows the 
right verb part of the embedding sentence. In order 
to express and handle this sort of recursion using 
a finite state approach, both recursions are treated 
as iterations uch that they destructively substitute 
recognized embedded base clauses with their type. 
Hence, the complexity of the recognized structure 
of the sentence is reduced successively. However, 
because subclauses of MF-recursion may have their 
own embedded RF-recursion the CLAUSE COMBINA- 
TION (CC) is used for bundling subsequent base 
clauses before they would be combined with sub- 
clauses identified by the outer MF-recursion. The 
BC and CC module are called until no more base 
clauses can be reduced. If the CC module would not 
be used, then the following incorrect segmentation 
could not be avoided: 
. . .  *\[daft das Gliick \[, das Jochen 
Kroehne ernpfunden haben sollte Rel-C1\] 
\[, als ihm jiingst sein Groflaktion/ir die 
Ubertragungsrechte bescherte Sub j -e l f ,  
nicht mehr so recht erwKrmt Sub j-C1\] 
In the correct reading the second subclause "... als 
ihm jiingst sein .. ." is embedded into the first one 
".. .  das Jochen Kroehne .. .".  
Ma in  c lauses (MC)  Finally the MC module 
builds the complete topological structure of the in- 
put sentence on the basis of the recognized (remain- 
ing) verb groups and base clauses, as well as on the 
word form information ot yet consumed. The latter 
includes basically punctuations and coordinations. 
The following figure schematically describes the cur- 
rent coverage of the implemented MC-module (see 
figure 1 for an example structure): 
CSent 
SSent 
CoordS 
AsyndSent 
CmpCSent 
AsyndCond 
:: . . . .  LVP .. .  \[RVP\] . . .  
::= LVP ...\[RVP\] . . .  
::= CSent ( , CSent)* Coord CSent \] 
::= CSent (, SSent)* Coord SSent 
::= CSent , CSent 
::= CSent , SSent I CSent , CSent 
::= SSent , SSent 
3.2 Phrase recognition 
After the topological structure of a sentence has been 
identified, each substring is passed to the FRAG- 
MENT RECOGNIZER in order to determine the in- 
ternal phrasal structure. Note that processing of 
a substring might still be partial in the sense that 
no complete structure need be found (e.g., if we 
cannot combine sequences of phrases to one larger 
unit). The FRAGMENT RECOGNIZER uses finite state 
grammars in order to extract nominal and preposi- 
tional phrases, where the named entities recognized 
by the preprocessor are integrated into appropriate 
places (unplausibte phrases are rejected by agree- 
ment checking; see (Neumann et al, 1997) for more 
243 
details)). The phrasal recognizer currently only con- 
siders processing of simple, non-recursive structures 
(see fig. 3; here, *NP* and *PP* are used for de- 
noting phrasal types). Note that because of the 
high degree of modularity of our shallow parsing 
architecture, it is very easy to exchange the cur- 
rently domain-independent fragment recognizer with 
a domain-specific one, without effecting the domain- 
independent DC-PARSER. 
The final output of the parser for a sentence is an 
underspecified dependence structure UDS. An UDS 
is a flat dependency-based structure of a sentence, 
where only upper bounds for attachment and scop- 
ing of modifiers are expressed. This is achieved by 
collecting all NPs and PPs of a clause into sepa- 
rate sets as long as they are not part of some sub- 
clauses. This means that although the exact attach- 
ment point of each individual PP is not known it 
is guaranteed that a PP can only be attached to 
phrases which are dominated by the main verb of the 
sentence (which is the root node of the clause's tree). 
However, the exact point of attachment is a matter 
of domain-specific knowledge and hence should be 
defined as part of the domain knowledge of an ap- 
plication. 
4 Evaluat ion 
Due to the limited space, we concentrate on the 
evaluation of the topological structure. An eval- 
uation of the other components (based on a sub- 
set of 20.000 tokens of the mentioned corpus from 
the "Wirtschaftswoche", see below) yields: From 
the 93,89% of the tokens which were identified by 
the morphological component as valid word forms, 
95,23% got a unique POS-assignment with an ac- 
curacy of 97,9%. An initial evaluation on the same 
subset yielded a precision of 95.77% and a recall of 
85% (90.1% F-measure) for our current named en- 
tity finder. Evaluation of the compound analysis 
of nouns, i.e. how often a morphosyntactical cor- 
rect segmentation was found yield: Based on the 
20.000 tokens, 1427 compounds are found, where 
1417 have the correct segmentation (0.9929% preci- 
sion). On a smaller subset of 1000 tokens containing 
102 compounds, 101 correct segmentations where 
found (0.9901% recall), which is a quite promising 
result. An evaluation of simple NPs yielded a recall 
of 0.7611% and precision of 0.9194%. The low recall 
was mainly because of unknown words. 
During the 2nd and 5th of July 1999 a test cor- 
pus of 43 messages from different press releases (viz. 
DEUTSCHE PREESSEAGENTUR (dpa), ASSOCIATED 
PRESS (ap) and REUTERS) and different domains 
(equal distribution of politics, business, sensations) 
was collected. 6 The corpus contains 400 sentences 
6This data collection and evaluation was carried out by 
(Braun, 1999). 
with a total of 6306 words. Note that it also was 
created after the DC-PARSER and all grammars were 
finally implemented. Table 1 shows the result of 
the evaluations (the F-measure was computed with 
/3=1). We used the correctness criteria as defined in 
figure 7. 
The evaluation of each component was measured 
on the basis of the result of all previous components. 
For the BC and MC module we also measured the 
performance by manually correcting the errors of the 
previous components (denoted as "isolated evalua- 
tion"). In most cases the difference between the pre- 
cision and recall values is quite small, meaning that 
the modules keep a good balance between coverage 
and correctness. Only in the case of the MC-module 
the difference is about 5%. However, the result for 
the isolated evaluation of the MC-module suggests 
that this is mainly due to errors caused by previous 
components. 
A more detailed analysis showed that the major- 
ity of errors were caused by mistakes in the prepro- 
cessing phase. For example ten errors were caused 
by an ambiguity between different verb stems (only 
the first reading is chosen) and ten errors because 
of wrong POS-filtering. Seven errors were caused by 
unknown verb forms, and in eight cases the parser 
failed because it could not properly handle the ambi- 
guities of some word forms being either a separated 
verb prefix or adverb. 
The evaluation has been performed with the 
Lisp-based version of SMES (cf. (Neumann et al, 
1997)) by replacing the original bidirectional shal- 
low buttom-up arsing module with the DC-PARSER. 
The average run-time per sentence (average length 
26 words) is 0.57 sec. A C++-version is nearly 
finished missing only the re-implementation f the 
base and main clause recognition phases, cf. (Pisko- 
rski and Neumann, 2000). The run-time behavior 
is already encouraging: processing of a German text 
document (a collection of business news articles from 
the "Wirtschaftswoche") of 197118 tokens (1.26 MB) 
needs 45 seconds on a PentiumII, 266 MHz, 128 
RAM, which corresponds to 4380 tokens per second. 
Since this is an increase in speed-up by a factor > 20 
compared to the Lisp-version, we expect o be able 
to process 75-100 sentences per second. 
5 Re lated  Work 
To our knowledge, there are only very few other 
systems described which process free German texts. 
The new shallow text processor is a direct succes- 
sor of the one used in the SMES-system, an IE-core 
system for real world German text processing (Neu- 
mann et al, 1997). Here, a bidirectional verb-driven 
bottom-up arser was used, where the problems de- 
scribed in this paper concerning parsing of longer 
sentences were encountered. Another similar divide- 
9,a~ 244
Cr i ter ium Match ing of annotated  ata  and results Used by module 
Borders 
Type 
Partial 
Top 
Structl 
Struct2 
start  and end points 
s tar t  and end points, type 
s tar t  or end point, type 
s tar t  and end points, type 
for the largest tag 
see Top, plus test of substructures  
using Par t ia l  
see Top, plus test of substructures  
using Type 
verbforms, BC 
verbforms, BC, MC 
BC 
MC 
MC 
MC 
Figure 7: Correctness criteria used during evaluation. 
Verb-Modu le  
correctness Verb fragments Recall 
criterium total found correct in % 
Borders 897 894 883 98.43 
Type 897 894 880 98.10 
Base-C lause-Modu le  
correctness B C- Fragments Recall 
cmterium total found correct in% 
Type 130 129 121 93.08 
Par t ia l  130 129 125 96.15 
Precision F-measure 
i n% in% 
98.77 98.59 
98.43 98.26 
Precision F-measure 
in % in % 
93180 93.43 
96.89 96.51 
Base-C lause-Modu le  (isolated evaluation) 
correctness 
criterium 
Type 
Partial 
Base-Clauses Recall 
total found correct in % 
130 131 123 94.61 
130 131 127 97.69 
Ma in -C lause-Modu le  
correctness Main-Clauses Recall 
cmtemum total found correct in% 
Top 400 377 361 90.25 
St ruct l  400 377 361 90.25 
Struct2 400 377 356 89.00 
Precision F-measure 
in % in % 
93.89 94.24 
96.94 97.31 
Precision F-measure 
in % in % 
95.75 92.91 
95.75 92.91 
94.42 91.62 
Precision F-measure 
in % in ,% 
96.65 95.30 
96.65 95.30 
95.62 94.29 
Ma ln -C lause-Modu le  (isolated evaluation) 
correctness Main- Clauses Recall 
criterium total found correct in % 
Top 400 389 376 94.00 
St ruct l  400 389 376 94.00 
Struct2 400 389 372 93.00 
complete  analys is  
correctness all components Recall Precision F-measure 
criterium total \[ found \[ correct in% in% in% 
Struct2 400 \[ \ ]377  339 84.75 89.68 87.14 
Table 1: Results of the evaluation of the topological structure 
and-conquer approach using a chart-based parser 
for analysis of German text documents was pre- 
sented by (Wauschkuhn, 1996). Nevertheless, com- 
paring its performance with our approach seems to 
be rather difficult since he only measures for an un- 
annotated test corpus how often his parser finds at 
least one result (where he reports 85.7% "coverage" 
of a test corpus of 72.000 sentences) disregarding to 
measure the accuracy of the parser. In this sense, 
our parser achieved a "coverage" of 94.25% (com- 
puting faund/total), ahnost certainly because we 
use more advanced lexical and phrasal components, 
245 
e.g., pos-filter, compound and named entity process- 
ing. (Peh and Ting, 1996) also describe a divide- 
and-conquer approach based on statistical methods, 
where the segmentation f the sentence is done by 
identifying so called link words (solely punctuations, 
conjunctions and prepositions) and disambiguating 
their specific role in the sentence. On an annotated 
test corpus of 600 English sentences they report an 
accuracy of 85.1% based on the correct recognition of 
part-of-speech, comma nd conjunction disambigua- 
tion, and exact noun phrase recognition. 
6 Conc lus ion and future work 
We have presented a divide-and-conquer st ategy 
for shallow analysis of German texts which is sup- 
ported by means of powerful morphological process- 
ing, efficient POS-filtering and named entity recog- 
nition. Especially for the divide-and-conquer pars- 
ing strategy we obtained an F-measure of 87.14% 
on unseen data. Our shallow parsing strategy has 
a high degree of modularity which allows the inte- 
gration of the domain-independent sentence recog- 
nition part with arbitrary domain-dependent sub- 
components (e.g., specific named entity finders and 
fragment recognizers). 
Considered from an application-oriented point of 
view, our main experience is that even if we are only 
interested in some parts of a text (e.g., only in those 
linguistic entities which verbalize certain aspects of 
a domain-concept) wehave to unfold the structural 
relationship between all elements of a large enough 
area (a paragraph or more) up to a certain level 
of depth in which the relevant information is em- 
bedded. Beside continuing the improvement of the 
whole approach we also started investigations to- 
wards the integration of deep processing into the 
DC-PARSER. The core idea is to call a deep parser 
only to the separated field elements which contain 
sequences of simple NPs and PPs (already deter- 
mined by the shallow parser). Thus seen the shallow 
parser is used as an efficient preprocessor for divid- 
ing a sentence into syntactically valid smaller units, 
where the deep parser's task would be to identify the 
exact constituent s ructure only on demand. 
Acknowledgments 
The research underlying this paper was supported 
by a research grant from the German Bundesmin- 
isterium fiir Bildung, Wissenschaft, Forschung 
und Technologie (BMBF) to the DFKI project 
PARADIME, FKZ ITW 9704. Many thanks to 
Thierry Declerck and Milena Valkova for their sup- 
port during the evaluation of the system. 
References 
S. Abney. 1996. Partial parsing via finite-state cas- 
cades. Proceedings ofthe ESSLLI 96 Robust Pars- 
ing Workshop. 
C. Braun. 1999. Flaches und robustes Parsen 
Deutscher Satzgeffige. Master's thesis, University 
of the Saarland. 
F. Ciravegna, A. Lavelli, N. Mana, L. Gilardoni, 
S. Mazza, M. Ferraro, J. Matiasek, W. Black, 
F. Rinaldi, and D. Mowatt. 1999. Facile: Clas- 
sifying texts integrating pattern matching and in- 
formation extraction. In Proceedings of IJCAI-99, 
Stockholm. 
Ulrich Engel. 1988. Deutsche Grammatik. Julius 
Groos Verlag, Heidelberg, 2., improved edition. 
S. Federici, S. Monyemagni, and V. Pirrelli. 1996. 
Shallow parsing and text chunking: A view on um 
derspecification in syntax. In Workshop on Robust 
Parsing, 8th ESSLLI, pages 35-44. 
G. Neumann, R. Backofen, J. Baur, M. Becker, 
and C. Braun. 1997. An information extraction 
core system for real world german text processing. 
In 5th International Conference of Applied Natu- 
ral Language, pages 208-215, Washington, USA, 
March. 
L. Peh and Christopher H. Ting. 1996. A divide- 
and-conquer strategy for parsing. In Proceedings 
of the ACL/SIGPARSE 5th International Work- 
shop on Parsing Technologies, pages 57-66. 
J. Piskorski and G. Neumann. 2000. An intelligent 
text extraction and navigation system. In 6th In- 
ternational Conference on Computer-Assisted In- 
formation Retrieval (RIAO-2000). Paris, April. 
18 pages. 
E. Roche and Y. Schabes. 1995. Deterministic part- 
of-speech tagging with finite state transducers. 
Computational Linguistics, 21(2):227-253. 
SAIC, editor. 1998. Seventh Message 
Understanding Conference (MUC- 7), 
http://www.muc.saic.com/. SAIC Information 
Extraction. 
S. Sekine and C. Nobata. 1998. An infor- 
mation extraction system and a customization 
tool. In Proceedings of Hitachi workshop-98, 
http://cs.nyu.edu/cs/projects/proteus/sekine/. 
B. Sundheim, editor. 1995. Sixth Message Un- 
derstanding Conference (MUC-6), Washington. 
Distributed by Morgan Kaufmann Publishers, 
Inc.,San Mateo, California. 
O. Wauschkuhn. 1996. Ein Werkzeug zur par- 
tiellen syntaktischen Analyse deutscher Textko- 
rpora. In Dafydd Gibbon, editor, Natural Lan- 
guage Processing and Speech Technology. Results 
of the Third KONVENS Conference, pages 356- 
368. Mouton de Gruyter, Berlin. 
246 
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
An Integrated Architecture for Shallow and Deep Processing
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,
Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker and Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg 3
Saarbru?cken, Germany
whiteboard@dfki.de
Abstract
We present an architecture for the integra-
tion of shallow and deep NLP components
which is aimed at flexible combination
of different language technologies for a
range of practical current and future appli-
cations. In particular, we describe the inte-
gration of a high-level HPSG parsing sys-
tem with different high-performance shal-
low components, ranging from named en-
tity recognition to chunk parsing and shal-
low clause recognition. The NLP com-
ponents enrich a representation of natu-
ral language text with layers of new XML
meta-information using a single shared
data structure, called the text chart. We de-
scribe details of the integration methods,
and show how information extraction and
language checking applications for real-
world German text benefit from a deep
grammatical analysis.
1 Introduction
Over the last ten years or so, the trend in application-
oriented natural language processing (e.g., in the
area of term, information, and answer extraction)
has been to argue that for many purposes, shallow
natural language processing (SNLP) of texts can
provide sufficient information for highly accurate
and useful tasks to be carried out. Since the emer-
gence of shallow techniques and the proof of their
utility, the focus has been to exploit these technolo-
gies to the maximum, often ignoring certain com-
plex issues, e.g. those which are typically well han-
dled by deep NLP systems. Up to now, deep natural
language processing (DNLP) has not played a sig-
nificant role in the area of industrial NLP applica-
tions, since this technology often suffers from insuf-
ficient robustness and throughput, when confronted
with large quantities of unrestricted text.
Current information extractions (IE) systems
therefore do not attempt an exhaustive DNLP analy-
sis of all aspects of a text, but rather try to analyse or
?understand? only those text passages that contain
relevant information, thereby warranting speed and
robustness wrt. unrestricted NL text. What exactly
counts as relevant is explicitly defined by means
of highly detailed domain-specific lexical entries
and/or rules, which perform the required mappings
from NL utterances to corresponding domain knowl-
edge. However, this ?fine-tuning? wrt. a particular
application appears to be the major obstacle when
adapting a given shallow IE system to another do-
main or when dealing with the extraction of com-
plex ?scenario-based? relational structures. In fact,
(Appelt and Israel, 1997) have shown that the cur-
rent IE technology seems to have an upper perfor-
mance level of less than 60% in such cases. It seems
reasonable to assume that if a more accurate analy-
sis of structural linguistic relationships could be pro-
vided (e.g., grammatical functions, referential rela-
tionships), this barrier might be overcome. Actually,
the growing market needs in the wide area of intel-
ligent information management systems seem to re-
quest such a break-through.
In this paper we will argue that the quality of cur-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448.
                         Proceedings of the 40th Annual Meeting of the Association for
rent SNLP-based applications can be improved by
integrating DNLP on demand in a focussed manner,
and we will present a system that combines the fine-
grained anaysis provided by HPSG parsing with a
high-performance SNLP system into a generic and
flexible NLP architecture.
1.1 Integration Scenarios
Owing to the fact that deep and shallow technologies
are complementary in nature, integration is a non-
trivial task: while SNLP shows its strength in the
areas of efficiency and robustness, these aspects are
problematic for DNLP systems. On the other hand,
DNLP can deliver highly precise and fine-grained
linguistic analyses. The challenge for integration is
to combine these two paradigms according to their
virtues.
Probably the most straightforward way to inte-
grate the two is an architecture in which shallow and
deep components run in parallel, using the results of
DNLP, whenever available. While this kind of ap-
proach is certainly feasible for a real-time applica-
tion such as Verbmobil, it is not ideal for processing
large quantities of text: due to the difference in pro-
cessing speed, shallow and deep NLP soon run out
of sync. To compensate, one can imagine two possi-
ble remedies: either to optimize for precision, or for
speed. The drawback of the former strategy is that
the overall speed will equal the speed of the slow-
est component, whereas in case of the latter, DNLP
will almost always time out, such that overall preci-
sion will hardly be distinguishable from a shallow-
only system. What is thus called for is an integrated,
flexible architecture where components can play at
their strengths. Partial analyses from SNLP can be
used to identify relevant candidates for the focussed
use of DNLP, based on task or domain-specific crite-
ria. Furthermore, such an integrated approach opens
up the possibility to address the issue of robustness
by using shallow analyses (e.g., term recognition)
to increase the coverage of the deep parser, thereby
avoiding a duplication of efforts. Likewise, integra-
tion at the phrasal level can be used to guide the
deep parser towards the most likely syntactic anal-
ysis, leading, as it is hoped, to a considerable speed-
up.
shallow
NLP
components
NLP
deep
components internal repr.
layer
multi
chart
annot.
XML
external repr.
generic OOP
component
interface
WHAM
application
specification
input and
result
Figure 1: The WHITEBOARD architecture.
2 Architecture
The WHITEBOARD architecture defines a platform
that integrates the different NLP components by en-
riching an input document through XML annota-
tions. XML is used as a uniform way of represent-
ing and keeping all results of the various processing
components and to support a transparent software
infrastructure for LT-based applications. It is known
that interesting linguistic information ?especially
when considering DNLP? cannot efficiently be
represented within the basic XML markup frame-
work (?typed parentheses structure?), e.g., linguistic
phenomena like coreferences, ambiguous readings,
and discontinuous constituents. The WHITEBOARD
architecture employs a distributed multi-level repre-
sentation of different annotations. Instead of trans-
lating all complex structures into one XML docu-
ment, they are stored in different annotation layers
(possibly non-XML, e.g. feature structures). Hyper-
links and ?span? information together support effi-
cient access between layers. Linguistic information
of common interest (e.g. constituent structure ex-
tracted from HPSG feature structures) is available in
XML format with hyperlinks to full feature struc-
ture representations externally stored in correspond-
ing data files.
Fig. 1 gives an overview of the architecture of
the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and
a specification describing the components and con-
figuration options requested. The core WHAM en-
gine has an XML markup storage (external ?offline?
representation), and an internal ?online? multi-level
annotation chart (index-sequential access). Follow-
ing the trichotomy of NLP data representation mod-
els in (Cunningham et al, 1997), the XML markup
contains additive information, while the multi-level
chart contains positional and abstraction-based in-
formation, e.g., feature structures representing NLP
entities in a uniform, linguistically motivated form.
Applications and the integrated components ac-
cess the WHAM results through an object-oriented
programming (OOP) interface which is designed
as general as possible in order to abstract from
component-specific details (but preserving shallow
and deep paradigms). The interfaces of the actu-
ally integrated components form subclasses of the
generic interface. New components can be inte-
grated by implementing this interface and specifying
DTDs and/or transformation rules for the chart.
The OOP interface consists of iterators that walk
through the different annotation levels (e.g., token
spans, sentences), reference and seek operators that
allow to switch to corresponding annotations on a
different level (e.g., give all tokens of the current
sentence, or move to next named entity starting
from a given token position), and accessor meth-
ods that return the linguistic information contained
in the chart. Similarily, general methods support
navigating the type system and feature structures of
the DNLP components. The resulting output of the
WHAM can be accessed via the OOP interface or as
XML markup.
The WHAM interface operations are not only
used to implement NLP component-based applica-
tions, but also for the integration of deep and shallow
processing components itself.
2.1 Components
2.1.1 Shallow NL component
Shallow analysis is performed by SPPC, a rule-
based system which consists of a cascade of
weighted finite?state components responsible for
performing subsequent steps of the linguistic anal-
ysis, including: fine-grained tokenization, lexico-
morphological analysis, part-of-speech filtering,
named entity (NE) recognition, sentence bound-
ary detection, chunk and subclause recognition,
see (Piskorski and Neumann, 2000; Neumann and
Piskorski, 2002) for details. SPPC is capable of pro-
cessing vast amounts of textual data robustly and ef-
ficiently (ca. 30,000 words per second in standard
PC environment). We will briefly describe the SPPC
components which are currently integrated with the
deep components.
Each token identified by a tokenizer as a poten-
tial word form is morphologically analyzed. For
each token, its lexical information (list of valid read-
ings including stem, part-of-speech and inflection
information) is computed using a fullform lexicon
of about 700,000 entries that has been compiled out
from a stem lexicon of about 120,000 lemmas. Af-
ter morphological processing, POS disambiguation
rules are applied which compute a preferred read-
ing for each token, while the deep components can
back off to all readings. NE recognition is based on
simple pattern matching techniques. Proper names
(organizations, persons, locations), temporal expres-
sions and quantities can be recognized with an av-
erage precision of almost 96% and recall of 85%.
Furthermore, a NE?specific reference resolution is
performed through the use of a dynamic lexicon
which stores abbreviated variants of previously rec-
ognized named entities. Finally, the system splits
the text into sentences by applying only few, but
highly accurate contextual rules for filtering implau-
sible punctuation signs. These rules benefit directly
from NE recognition which already performs re-
stricted punctuation disambiguation.
2.1.2 Deep NL component
The HPSG Grammar is based on a large?scale
grammar for German (Mu?ller, 1999), which was
further developed in the VERBMOBIL project for
translation of spoken language (Mu?ller and Kasper,
2000). After VERBMOBIL the grammar was adapted
to the requirements of the LKB/PET system (Copes-
take, 1999), and to written text, i.e., extended with
constructions like free relative clauses that were ir-
relevant in the VERBMOBIL scenario.
The grammar consists of a rich hierarchy of
5,069 lexical and phrasal types. The core grammar
contains 23 rule schemata, 7 special verb move-
ment rules, and 17 domain specific rules. All rule
schemata are unary or binary branching. The lexicon
contains 38,549 stem entries, from which more than
70% were semi-automatically acquired from the an-
notated NEGRA corpus (Brants et al, 1999).
The grammar parses full sentences, but also other
kinds of maximal projections. In cases where no full
analysis of the input can be provided, analyses of
fragments are handed over to subsequent modules.
Such fragments consist of maximal projections or
single words.
The HPSG analysis system currently integrated
in the WHITEBOARD system is PET (Callmeier,
2000). Initially, PET was built to experiment
with different techniques and strategies to process
unification-based grammars. The resulting sys-
tem provides efficient implementations of the best
known techniques for unification and parsing.
As an experimental system, the original design
lacked open interfaces for flexible integration with
external components. For instance, in the beginning
of the WHITEBOARD project the system only ac-
cepted fullform lexica and string input. In collabora-
tion with Ulrich Callmeier the system was extended.
Instead of single word input, input items can now
be complex, overlapping and ambiguous, i.e. essen-
tially word graphs. We added dynamic creation of
atomic type symbols, e.g., to be able to add arbitrary
symbols to feature structures. With these enhance-
ments, it is possible to build flexible interfaces to
external components like morphology, tokenization,
named entity recognition, etc.
3 Integration
Morphology and POS The coupling between the
morphology delivered by SPPC and the input needed
for the German HPSG was easily established. The
morphological classes of German are mapped onto
HPSG types which expand to small feature struc-
tures representing the morphological information in
a compact way. A mapping to the output of SPPC
was automatically created by identifying the corre-
sponding output classes.
Currently, POS tagging is used in two ways. First,
lexicon entries that are marked as preferred by the
shallow component are assigned higher priority than
the rest. Thus, the probability of finding the cor-
rect reading early should increase without excluding
any reading. Second, if for an input item no entry is
found in the HPSG lexicon, we automatically create
a default entry, based on the part?of?speech of the
preferred reading. This increases robustness, while
avoiding increase in ambiguity.
Named Entity Recognition Writing HPSG gram-
mars for the whole range of NE expressions etc. is
a tedious and not very promising task. They typi-
cally vary across text sorts and domains, and would
require modularized subgrammars that can be easily
exchanged without interfering with the general core.
This can only be realized by using a type interface
where a class of named entities is encoded by a gen-
eral HPSG type which expands to a feature structure
used in parsing. We exploit such a type interface for
coupling shallow and deep processing. The classes
of named entities delivered by shallow processing
are mapped to HPSG types. However, some fine-
tuning is required whenever deep and shallow pro-
cessing differ in the amount of input material they
assign to a named entity.
An alternative strategy is used for complex syn-
tactic phrases containing NEs, e.g., PPs describ-
ing time spans etc. It is based on ideas from
Explanation?based Learning (EBL, see (Tadepalli
and Natarajan, 1996)) for natural language analy-
sis, where analysis trees are retrieved on the basis
of the surface string. In our case, the part-of-speech
sequence of NEs recognised by shallow analysis is
used to retrieve pre-built feature structures. These
structures are produced by extracting NEs from a
corpus and processing them directly by the deep
component. If a correct analysis is delivered, the
lexical parts of the analysis, which are specific for
the input item, are deleted. We obtain a sceletal
analysis which is underspecified with respect to the
concrete input items. The part-of-speech sequence
of the original input forms the access key for this
structure. In the application phase, the underspeci-
fied feature structure is retrieved and the empty slots
for the input items are filled on the basis of the con-
crete input.
The advantage of this approach lies in the more
elaborate semantics of the resulting feature struc-
tures for DNLP, while avoiding the necessity of
adding each and every single name to the HPSG lex-
icon. Instead, good coverage and high precision can
be achieved using prototypical entries.
Lexical Semantics When first applying the origi-
nal VERBMOBIL HPSG grammar to business news
articles, the result was that 78.49% of the miss-
ing lexical items were nouns (ignoring NEs). In
the integrated system, unknown nouns and NEs can
be recognized by SPPC, which determines morpho-
syntactic information. It is essential for the deep sys-
tem to associate nouns with their semantic sorts both
for semantics construction, and for providing se-
mantically based selectional restrictions to help con-
straining the search space during deep parsing. Ger-
maNet (Hamp and Feldweg, 1997) is a large lexical
database, where words are associated with POS in-
formation and semantic sorts, which are organized in
a fine-grained hierarchy. The HPSG lexicon, on the
other hand, is comparatively small and has a more
coarse-grained semantic classification.
To provide the missing sort information when re-
covering unknown noun entries via SPPC, a map-
ping from the GermaNet semantic classification to
the HPSG semantic classification (Siegel et al,
2001) is applied which has been automatically ac-
quired. The training material for this learning pro-
cess are those words that are both annotated with se-
mantic sorts in the HPSG lexicon and with synsets
of GermaNet. The learning algorithm computes a
mapping relevance measure for associating seman-
tic concepts in GermaNet with semantic sorts in the
HPSG lexicon. For evaluation, we examined a cor-
pus of 4664 nouns extracted from business news
that were not contained in the HPSG lexicon. 2312
of these were known in GermaNet, where they are
assigned 2811 senses. With the learned mapping,
the GermaNet senses were automatically mapped to
HPSG semantic sorts. The evaluation of the map-
ping accuracy yields promising results: In 76.52%
of the cases the computed sort with the highest rel-
evance probability was correct. In the remaining
20.70% of the cases, the correct sort was among the
first three sorts.
3.1 Integration on Phrasal Level
In the previous paragraphs we described strategies
for integration of shallow and deep processing where
the focus is on improving DNLP in the domain of
lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for
the integration of shallow and deep analysis at the
length cover- complete LP LR 0CB   2CB
age match
  40 100 80.4 93.4 92.9 92.1 98.9
 40 99.8 78.6 92.4 92.2 90.7 98.5
Training: 16,000 NEGRA sentences
Testing: 1,058 NEGRA sentences
Figure 2: Stochastic topological parsing: results
level of phrasal syntax by guiding the deep syntac-
tic parser towards a partial pre-partitioning of com-
plex sentences provided by shallow analysis sys-
tems. This strategy can reduce the search space, and
enhance parsing efficiency of DNLP.
Stochastic Topological Parsing The traditional
syntactic model of topological fields divides basic
clauses into distinct fields: so-called pre-, middle-
and post-fields, delimited by verbal or senten-
tial markers. This topological model of German
clause structure is underspecified or partial as to
non-sentential constituent boundaries, but provides
a linguistically well-motivated, and theory-neutral
macrostructure for complex sentences. Due to its
linguistic underpinning the topological model pro-
vides a pre-partitioning of complex sentences that is
(i) highly compatible with deep syntactic structures
and (ii) maximally effective to increase parsing ef-
ficiency. At the same time (iii) partiality regarding
the constituency of non-sentential material ensures
the important aspects of robustness, coverage, and
processing efficiency.
In (Becker and Frank, 2002) we present a corpus-
driven stochastic topological parser for German,
based on a topological restructuring of the NEGRA
corpus (Brants et al, 1999). For topological tree-
bank conversion we build on methods and results
in (Frank, 2001). The stochastic topological parser
follows the probabilistic model of non-lexicalised
PCFGs (Charniak, 1996). Due to abstraction from
constituency decisions at the sub-sentential level,
and the essentially POS-driven nature of topologi-
cal structure, this rather simple probabilistic model
yields surprisingly high figures of accuracy and cov-
erage (see Fig.2 and (Becker and Frank, 2002) for
more detail), while context-free parsing guarantees
efficient processing.
The next step is to elaborate a (partial) map-
ping of shallow topological and deep syntactic struc-
tures that is maximally effective for preference-gui-
Topological Structure:
CL-V2
VF-TOPIC LK-FIN MF RK-t
NN VVFIN ADV NN PREP NN VVFIN
[ 	 [ 
	 Peter] [ 
 i?t] [ 
 gerne Wu?rstchen mit Kartoffelsalat] [ Cross-Cutting Aspects  
of 
Cross-Language Question Answering Systems 
 
Bogdan Sacaleanu 
Language Technology Group 
DFKI GmbH 
Saarbr?cken, Germany 
bogdan@dfki.de 
G?nter Neumann 
Language Technology Group 
DFKI GmbH 
Saarbr?cken, Germany 
neumann@dfki.de 
 
 
Abstract 
We describe on-going work in the devel-
opment of a cross-language question-
answering framework for the open do-
main. An overview of the framework is 
being provided, some details on the im-
portant concepts of a flexible framework 
are presented and two cross-cutting as-
pects (cross-linguality and credibility) for 
question-answering systems are up for 
discussion. 
1 Introduction 
Different projects, different evaluation forums, 
different tasks, different languages, different 
document collections, different question types, 
different answer processing strategies ? Anyone 
familiar with all these concepts knows the com-
plexity and what a daunting prospect of develop-
ing a QA-System easily adaptable to ever chang-
ing requirements this might be. We have started 
off with a ?prototype-and-go? approach, trying to 
keep pace with the emergence of new tasks and 
managing the scarcity of your time and of your 
resources, to realize later on that what we had is 
a bunch of prototypes very tuned to their task 
requirements. Trying to adapt them to new re-
quirements seemed often more difficult then 
starting off with a new one. Therefore we started 
looking for an alternative, which should be more 
flexible and should allow us to cover much more 
requirements? variations; in other words we were 
considering putting together a Question Answer-
ing framework. 
In the rest of the paper we will shortly over-
view the components of such a framework and 
will describe the relevant aspects of the solution 
offered for each of them, aspects that should ac-
count for a large variety of question types, 
document collections and answer processing 
techniques, as well as for several languages. We 
will continue with a discussion of two issues that 
cut across several components of the framework, 
namely: cross-linguality and answer credibility, 
and will conclude by shortly naming the domains 
of usage for the framework and future work. 
2 Framework Overview 
Based on an existing set of cross-language 
Question Answering prototypes developed for 
different requirements, we began by looking for 
the commonalities among them. Following is a 
list of reusable components that might be 
considered as a starting point in defining a QA 
framework (see Figure 1). 
Several components along the work-flow of a 
typical QA system were identified: a Unit 
Alignment component in cross-language envi-
ronments and a Query Expansion compo-
nent for the Question Analysis task; a Unit 
Processor and a Query Generator com-
ponent for the Information Retrieval task; a 
Mention Chain component for the Answer 
Extraction task and a Scoring Strategy for 
the Answer Selection task. (see Figure 1) 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
15
Beside the reusability aspect in context of a 
QA framework, extensibility is a further 
important issue. We have approached it by con-
centrating on a flexible representation format of 
the data being passed around in the framework, 
namely the representation of the Question-
Object, the InformationUnit and the An-
swerObject. 
2.1 Reusability 
?Reusability is the likelihood a segment of 
structured code can be used again to add new 
functionalities with slight or no modification?.  
(Wikipedia) 
 
In a cross-language setting of a QA system, 
which is crossing the language barrier at the user 
end rather then at the document end, there are 
two approaches for getting the formalized user 
information need (QuestionObject) to the 
documents? language: either creating it based on 
the question translation or analyzing the original 
question and translating the formalized result. 
This last approach is convenient when machine 
readable dictionaries, part-of-speech taggers and 
other bilingual lexical resources (i.e. WordNets) 
for both languages are available. For this purpose 
a Unit Alignment Component was designed 
that produces an alignment of simple (words) 
and complex (phrases) syntactic units from the 
source to target language. 
Query Expansion is another component 
present among the support structures of our 
framework. Backed by lexico-semantic resources 
as EuroWordNet [V98] it can be used for all lan-
guages supported by these resource. For a given 
input word, it can return synonyms, hyponyms 
and hypernyms according to the following algo-
rithm: 
if (trgtWord_is_not_ambig) 
 return Information; 
else if (trgtWord_is_ambig) 
{ 
TRANS: 
1. translate Question 
2. do Unit Alignment 
3. if (transTrgtWord_is_EWN_aligned) 
a. if (alignment_is_not_ambig) 
return Information; 
b. else if (alignment_is_ambig) 
save Alignments; 
goto TRANS; 
} 
 
intersection = intersect_saved_alignments(); 
if (intersection.size == N)         // strict N=1 
 return Information_for_intersection; 
return NULL; 
An example for the above algorithm up to the 
stage 3.a is given in Section 3.1. This represents 
the ideal case, when our input can be 
disambiguated using the alignments of a question 
translation. But more often it is the case of 
advancing over this stage to 3.b, when the 
ambiguous EuroWordNet algnments are being 
saved and a new translation of the question 
through other online translation services is 
attempted. The idea behind this expansion 
method is that lexical diversity of different 
translations could narrow down the ambiguity of 
a word to a desired level (N). 
Figure 1: Framework Architecture 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
16
To select an initial set of information units 
relevant to the question, traditional search en-
gines are being used to scale down the search 
space. Two important aspects in this process are: 
the level of detail at which the indexation units 
are chosen and the granularity of the information 
units (be it document, paragraph, sentence or 
phrase). Two subcomponents are important at 
this stage: the Unit Processor and the 
Query Generator. 
The Unit Processor covers the above-
mentioned aspects: it takes as input an 
InformationUnit (i.e., a raw text document) 
and it either
 
reduces it to a set of new units (i.e., 
sentences), or it annotates the unit at different 
levels (i.e., named entities, grammatical rela-
tions). This way, by chaining different Unit 
Processors, you can both reduce the informa-
tion unit size and generate new indexing units.  
The Query Generator relies on an ab-
stract description of the processing method to 
accordingly generate the IRQuery to make use 
of the advanced indexation units. For example, 
when named entities were annotated during the 
Unit Processor chaining and used as index-
ing units, the Query Generator will adapt 
the IRQuery so as to search for an additional 
field (neType) that might reflect the expected 
answer type for the question considered. We 
consider the Query Generator as being the 
mediator between the question analysis result 
QuestionObject (answer type, constraints) 
and the search engine serving the retrieval com-
ponent with information units (documents, sen-
tences). Even more, the Query Generator 
relies on an abstract description of the search 
engine, too, and can adapt the IRQuery accord-
ing to either a boolean or a ranked search engine. 
A Mention Chain component in the An-
swer Extraction task provides an ease of burden 
for the Selection task by computing answer ref-
erence chains. This is very helpful for factoid 
questions on the Web, and not only, where re-
dundancy of an answer candidate is a good hint 
for its potential selection and credibility. A men-
tion chain contains all answers sharing a com-
mon normalized representation, determined ei-
ther through the string similarity of the answers 
only or by additionally employing context en-
tailment measures.  
The Scoring Strategy component builds 
on the mathematical graph theory and reduces 
the answer candidate scoring issue to a shortest 
path problem on lexical graphs. In most of the 
cases the answer ?suitability? could be scaled 
down to computing a distance metric for the an-
swer and some information in the question (i.e., 
keywords, focus). Both a simple textual distance 
measure and another one based on dependency 
structures were implemented on these graph 
structures, with slight variations making use of 
weight and cost properties for graph edges. 
Based on available web search API (i.e., 
Google, Yahoo) the Answer Validation 
component computes a total frequency count of 
co-occurrence for pairs of question and answer, 
assuming that the right answer shares more con-
texts with the question than any other candidate 
and that the considered answers are semantic 
independent and insensitive with respect to the 
timeline preferred by the search engines. 
2.2 Extensibility 
 ?Extensibility is a system design principle where 
the implementation takes into consideration fu-
ture growth. ? The central theme is to provide 
for change while minimizing impact to existing 
system functions.? (Wikipedia) 
Extensibility can be approached by two meth-
ods during framework design: through software 
design patterns and through a common extensible 
data representation format. While we have used 
some patterns through the development of reus-
able components like the chaining aspect in the 
Unit Processor, the normalization function in the 
Mention Chain and the graph design in the Scor-
ing Strategy, we have concentrated more on an 
extensible representation format for data being 
passed around through the framework: the 
QuestionObject, the InformationUnit 
and the AnswerObject. For this purpose we 
have used XML, XML Schema and data binding 
methods (JAXB) to guarantee component life on 
an evolving data format. The primary benefit of 
allowing extensibility in a format is that it en-
ables a format to evolve without requiring central 
control of the format. A secondary benefit is that 
it allows the format to stay focused and simple 
by pushing specialized cases and complex solu-
tions into optionally supported extensions. W3C 
XML Schema provides two features that promote 
extensibility in XML vocabularies: the wildcards 
xs:any and xs:anyAttribute are used to 
allow the occurrence of elements and attributes 
from specified namespaces into a given format, 
and the xsi:type attribute that can be placed 
on an element in an XML instance document to 
change its type to a more refined subtype. That 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
17
is, according to user?s need, the data exchanged 
among the framework?s components can be ex-
tended, without changing the framework func-
tionality. 
3 QA Cross-Cutting Aspects 
In a question answering framework there are as-
pects that does not directly relate to the core con-
cerns, but are needed for a proper design. More-
over, they can hardly be pinned down to a com-
ponent, as they cross several core components of 
the system.  We are talking about concepts like 
cross-linguality and credibility ? system credibil-
ity reflected in answer credibility. 
3.1 Cross-Linguality 
There are three traditional approaches that count 
for cross?linguality in context of information 
management systems: 
? translating the queries into the target 
language, 
? translating the document collection into 
the source language or 
? translating the queries and the docu-
ments into an intermediate representation 
(inter?lingua). 
Two types of translation services are well 
known within this context which are based on  
? lexical resources (e.g., dictionaries, 
aligned wordnets), or 
? machine translation (e.g., example?based 
translation). 
The only feasible approach when dealing with 
huge amounts of data, as is the case for question 
answering systems, is translating the question 
into the language of the document collection and 
the related issue of back-translating the answers 
into the language of the user. 
We are using two different methods for re-
sponding questions asked in a language different 
from the one of the answer-bearing documents. 
Both employ online translation services (Alta-
vista, FreeTranslation, etc.) for crossing the lan-
guage barrier, but at different processing steps: 
before and after formalizing the user information 
need into a QuestionObject. 
The before?method translates the question 
string in an earlier step, resulting in several 
automatic translated strings, of which the best 
one is analyzed by the Question Analysis com-
ponent and passed on to the Information Re-
trieval component. This is the strategy we use in 
an English?German cross-lingual setting. To be 
more precise: the English source question is 
translated into several alternative German ques-
tions using online MT services. Each German 
question is then parsed with SMES [NP02], our 
German parser. The resulting query object is then 
weighted according to its linguistic well?
formedness and its completeness wrt. query in-
formation (question type, question focus, an-
swer?type). 
The assumption behind this weighting scheme 
is that ?a translated string is of greater utility for 
subsequent processes than another one, if its lin-
guistic analysis is more complete or appropri-
ate.? 
The after?method translates the formal-
ized result of the Query Analysis Component by 
using the question translations, a language mod-
eling tool and a word alignment tool for creating 
a mapping of the formal information need from 
the source language into the target language. We 
illustrate this strategy in a German?English set-
ting along two lines (using the following German 
question as example: In welchem Jahr-
zehnt investierten japanische 
Autohersteller sehr stark?): 
- translations as returned by the on-line MT 
systems are being ranked according to a 
language model 
In which decade did Japanese 
automakers invest very 
strongly? (0.7) 
In which decade did Japanese 
car manufacturers invest 
very strongly? (0.8) 
- translations with a satisfactory degree of 
resemblance to a natural language utterance (i.e. 
linguistically well-formedness), given by a 
threshold on the language model ranking, are 
aligned based on several filters: back-
propagation dictionary filter - based on MRD 
(machine readable dictionaries), PoS filter - 
based on statistical part-of-speech taggers, and 
cognates filter - based on string similarity 
measures (dice coefficient and LCSR (lowest 
common substring ratio)). 
In: [in:1] 
welchem: [which:0.5] 
Jahrzehnt: [decade:1] 
investierten: [invest:1] 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
18
japanische: [Japanese:0.5] 
Autohersteller:  
[car manufacturers:0.8,  
auto makers:0.1] 
sehr: [very:1] 
stark: [strongly:0.5] 
The evaluation gives evidence that both 
strategies are comparable in results, whereby the 
last one is slightly better, due to the fact of not 
being forced to choose a best translation, but 
working with and combining all the translations 
available. That is, considering and combining 
several, possible different, translations of the 
same question, the chance of detecting a 
translation error in an earlier phase of the work?
flow becomes higher and avoids error 
propagations through the whole system. 
The related issue of back-translating is ex-
plored by looking for parallel data to the an-
swer?s context or metadata, and extracting trans-
lation candidates based on their context and 
string surface similarity. For example, in a CLEF 
setting, for a German question having as English 
answer  ?Yasser Arafat? we have extracted the 
time-stamp of the answer?s context (19.07.1994), 
collected all the data with a time-stamp of 
07.1994 in the source language, extracted the 
named entities of type PERSON and then aligned 
?Jassir Arafat? based on its string surface simi-
larity to the initial answer. 
The translations and their alignment to the 
original question, according to the above-
mentioned after-method, have also a posi-
tive side-effect, namely: some of the aligned 
words may have several ranked translations. As 
it is the case of the ?Autohersteller?, a word 
might consider the best ranked alignment (?car 
manufacturers?) as its direct translation and the 
remaining ones as its expanded words. As such, 
given a reliable alignment method, cross-
linguality can prove supportive even for Query 
Expansion. Moreover, another method of us-
age can confirm the added value of cross-
linguality for Query Expansion, as de-
scribed below. 
For this task we are using the German and the 
English wordnets aligned within the EuroWord-
Net [V98] lexical resource. Our goal is to extend 
the formalized information need Question 
Object with synonyms for the words that are 
present in the wordnet. 
Considering the ambiguity of words, a WSD 
module is required as part of the expansion task. 
For this purpose we are using both the original 
question and its translations, leveraging the re-
duction in ambiguity gained through translation. 
Our devised pseudo-WSD algorithm works 
as following: 
1. look up every word from the word-
translation alignment (see example above) in the 
lexical resource; 
2. if the word is not ambiguous (which is, for 
example, the case for Japanese) then extend 
the Question Object with its synonyms (e.g., 
[Japanese, Nipponese]); 
3. if the word is ambiguous (e.g., invest) then  
(3a) for every possible reading of it, get its 
aligned German correspondent reading (if it 
exists) and look up that reading in the German 
original question , e.g., 
1351398: adorn-clothe-invest (EN) 
1351223: invest-vest (EN) 
 1400771: empower-endow-endue-gift-
indue-invest (EN) 
1350325: induct-invest-seat (EN) 
1293271:  
? commit-invest-place-put (EN) 
? anlegen-investieren (DE) 
(3b) if an aligned reading is found (e.g., Read-
ing-1293271) retain it and add the English 
synonyms of it to the Question Object, i.e., 
expand it with: 
 commit, place, put 
Following the question expansion task, the 
Question Object has been enriched with new 
words that are synonyms of the un?ambiguous 
English words and by synonyms of those 
ambiguous words, whose meaning(s) have been 
found in the original German question. Thus our 
expanded example has gained several more 
expanded words as follows: 
{Nipponese,commit,place,put} 
3.2 Answer Credibility 
In the ideal case, a Questions Answering System 
(QAS) will deliver correct answers and knows 
that they are correct, i.e., it can deliver a proof of 
the correctness of the answers. However, at least 
for the case of open-domain textual QA 
applications, this is out of reach with current 
technology. Thus, current QAS can only deliver 
answers with certain trustworthyness. Since, a 
receiver of an answer usually assumes, that a 
QAS tries to identify the best answer possible (at 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
19
least for cooperative language games), the QAS 
should assign a credibility measure to each 
selected answer. The underlying decisions made, 
can then also be used for explaining, how the 
answer?s credibility was determined.  
We view answer credibility as an additional 
process to answer extraction and selection that 
determines the quality of identified answer 
candidates by checking the plausibility of the 
answer sources and context on basis of meta 
information. For example, useful document and 
web page information might be:1 
? The name of the author of this activity; 
? Textual fingerprints of authority, e.g., 
?official web page of US government?; 
? E-mail address of the contact person for 
this activity; 
? When was this webpage last updated and 
links were checked (also: is there an regu-
lar update); 
? The name of the host school or organiza-
tion; 
? The link structure of the document, e.g., 
link IN/OUT density, links to relevant 
people, other authorities, clusters / hierar-
chies of authorities;  
? Text structure, e.g., textual coherence or 
style; 
Another important source of meta information 
relates to ontological knowledge, e.g., the consis-
tency of contextual information with respect to a 
domain ontology that defines the scope of the 
answer candidates. By this we mean the follow-
ing:  
? Given an answer candidate A of a ques-
tion Q (e.g., an instance of a concept in 
question) determined by a web-based 
QAS. 
? Check the textual context of A concern-
ing the mentioning of other relevant 
facts/concepts, that can be determined via 
access to an external (domain) ontology 
using relevant terms from Q and A.  
                                                
1
 For information on the topic of Web Credibility, cf. 
http://credibility.stanford.edu/ . This URL links to the 
Web Credibility Project of the Stanford Persuasive 
Technology Lab. Although they do not consider 
credibility under a strict QA perspective as we do, 
their work and results are a rich source of inspiration. 
For example, if the following request is sent to a 
web-based QAS: Name IE-systems that are 
based on statistical methods. Assume that for 
this question, the QAS identifies a list of names 
of IE-systems from textual sources as answer 
candidates. Assume further, that the QAS has 
access to an ontology-based meta-store about 
Language Technology terms, cf. [JU05]. Then, 
answer credibility checking can use the query 
terms IE-system and statistical method for ex-
tracting relevant facts from this LT-store, e.g., 
names of statistical methods, IE experts, IE sub-
tasks or properties of the concept information 
extraction. Next, for each answer candidate, 
check the textual context of the answer for the 
mentioning of these terms and their mutual rela-
tionship. Then a possible credibility heuristics 
might decide that the more relevant domain-
knowledge can be identified in the context of the 
answer the higher is the credibility of this an-
swer. 
These examples demonstrate that a wide vari-
ety of metadata from different levels can be and 
should be exploited for determining the credibil-
ity of answers. Since answer credibility is a new 
research area, it is still unclear which level of 
information is best suited for which kind of ques-
tions and answers. In order to be able to investi-
gate many possible credibility scenarios, we con-
sider answer credibility as a complex abstract 
data type or QAS credibility model, QAS-CM 
for short. The QAS-CM allows the definition of 
different kinds of credibility parameters, which 
are related to corresponding meta data and are 
orthogonal to the component-oriented view of 
the QAS. The values of the parameters might be 
computed and evaluated by different compo-
nents, even in parallel. Thus, the credibility of an 
answer is a complex value determined through 
composition of component-related credibility 
values. We are distinguishing two types of meta-
data: 
? Static metadata: are available directly 
through the textual source of an answer 
candidate and are represented in form of 
annotations, e.g. HTML/XML tags. 
? Dynamic metadata: are computed 
online via the components of the QAS, 
e.g., linguistic entities, semantic relations, 
textual entailment, text structure and co-
herence, topic linkage.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
20
Following this perspective, we propose the 
following structure for the QAS-CM (see Figure 
2). We distinguish two major subsystems: the 
QAS-CM itself and the QAS runtime instance. 
The QAS-CM is further subdivided into three 
modules: 
1. CM-description 
2. Data-Model 
3. Process-Model 
The CM-description module defines the 
credibility properties in a declarative way 
independently from the specific QAS instance 
used. It defines the name space of the credibility 
properties that are to be implemented and the 
way they are composed. We assume that each 
individual credibility property returns a real 
number, i.e., we consider it as a utility function, 
cf. [RN03]. Thus, the composition rules define 
how a complex value is computed, e.g., by 
summation or multiplication. The CM-
description also contains specification about the 
actual QAS instance, e.g., the type of document 
source or natural language to be processed. The 
CM-description serves as a blueprint for the 
other two modules, the Data-Model and the 
Process-Model. 
The Data-Model implements credibility prop-
erties as decision rules over instantiated meta 
information taking into account either static or 
dynamic meta data. For example a rule like  
entails(<SOURCE>,?official 
web page of <LOCATION>?)  
authority:=?I 
assigns the weight ?I to the credibility property 
with name authority if the document source 
of the answer candidate has a tag <source> 
which contains a substring ?official web 
page <LOCATION>?, where <LOCATION> 
contains a location name which corresponds to 
one mentioned in the Wh-question. For a 
question like ?Name all German chancellors after 
World War II.?, then an answer with a web page 
source, that contains something like <SOURCE> 
? official web page of Germany ? 
</SOURCE> will receive a credibility value ?I. 
The Process-Model is related to the compo-
nents of the QAS that have been used for the 
processing of a question/answer pair. We assume 
that each component can assign a utility measure 
for its determined output. In doing so, a compo-
nent should also take into account the size and 
utility measure of its input and the ambiguity 
degree of its output etc. For example, assume 
that the question analysis component (cf. sec. 3) 
can recognize the syntactic quality of a parse tree 
of a Wh-question, as being complete, par-
tial or null, cf. [NS05a]. Then a possible 
decision rule for the Process-Model might be 
syntactic_quality(<QUESTION> 
, COMPLETE)  QueryAna-
lyser:=?I=1 
in case, the parser of the question analysis 
component was able to completely parse the Wh-
question <QUESTION>. In a similar way, the 
web-validator mentioned in sec. 2.1 can also be 
integrated as a credibility function into our QAS-
CM using the following decision rule: 
 
 
Figure 2. Credibility Model 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
21
validate(Web, <QUESTION>, 
<CurrAnsw>)  WebValua-
tor:=?I 
 
Note that in case statistical components are used 
in QAS, e.g., a stochastic NE-recognizer, then 
the probabilities of the those components can be 
used for defining the utility values for the 
representative decision rules. Furthermore note, 
that each decision rule of the Process-Model 
corresponds to a single component. The 
compositon of several such rules are defined in 
the CM-description module. 
We are currently implementing QAS-CM 
following the same object-oriented framework as 
described in sec. 3, which eases integration of the 
components of the QAS. The data and the 
process model are implemented as a set of 
production rules using the RuleML language, cf. 
http://www.ruleml.org/. Currently, we define the 
values of ?I manually. This is more easier to 
implement but more tricky to maintain because 
of potential mutual interdependencies between 
individual values. Therefore, in the next 
development cycle of our QAS-CM, we will use 
a statistical approach for automatically acquiring 
optimal parameter settings. Starting point will be 
a question/answer/document corpus. This corpus 
can directly be used for training the Data-Model. 
In the case of the Process-Model, a Bayesian 
Network will be dynamically trained following 
ideas from IE-research, cf. [PP03]. Since in this 
case we also need output from all major QAS 
components, we are integrating an exhaustive 
tracing mechanism into our QA framework as a 
basis for automatically determining initial 
training material. 
4 Results 
The presented QA framework has been used 
both in mono-lingual and cross-lingual scenarios 
for closed document collections (CLEF collec-
tion) and open document collections (World 
Wide Web). In terms of reusability and extensi-
bility, the framework allowed for up to two 
weeks of work for building fully functional QA 
systems for different use scenarios. In terms of 
time performance for systems build upon the 
framework, the following figures apply: for sys-
tems used to query the Web a response time of 
up to 20 seconds in mono-lingual settings could 
be measured; those querying the CLEF document 
collection in a mono-lingual setting (German 
only) registered a latency of up to 3 seconds and 
for a cross-lingual setting of up to 15 seconds. 
The qualitative performance has been measured 
only on closed document collection and the best 
results for 200 questions of the CLEF 2005 
evaluation campaign in different use scenarios, 
according to [NS05b], were as follows: 
 
 Right Wrong Inexact 
DeDe 87 43.5% 100 13 
DeEn 51 25.5% 141 8 
EnDe 46 23% 141 12 
Reference 
[JU05] B. J?rg and  H. Uszkoreit. The Ontology-
based Architecture of LT World, a Comprehensive 
Web Information System for a Science and Tech-
nology Discipline. Leitbild Informations-
kompetenz: Positionen - Praxis - Perspektiven im 
europ?ischen Wissensmarkt. 27. Online Tagung, 
2005. 
[NP02]
 G. Neumann and J. Piskorski. A shallow text 
processing core engine. Computational Intelli-
gence, 18(3):451?476, 2002. 
[NS05a]
 G. Neumann and S. Sacaleanu. Experiments 
on robust NL-question interpretation and multi-
layered document annotation for a cross-language 
question/answering system. In Clef 2004, volume 
3491. Springer-Verlag LNCS, 2005. 
[NS05b] G. Neumann and B. Sacaleanu. DFKI's LT-
lab at the CLEF 2005 Multiple Language Question 
AnsweringTrack. In Working Notes for the CLEF 
2005 Workshop, 21-23 September, Vienna, Aus-
tria, 2005. 
[PP03] L. Peshkin and A. Pfefer. Bayesian Informa-
tion Extraction Network. In proceedings of IJCAI, 
2003. 
[RN03] S. Russell and P. Norvig. Artificial Intelli-
gence: A Modern Approach. Prentice-Hall, Engle-
wood Cliffs, NJ, 2nd edition, 2003. 
Vossen, P. (eds) 1998 EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks, 
Kluwer Academic Publishers, Dordrecht.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
22
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 36?41,
Prague, June 2007. c?2007 Association for Computational Linguistics
Recognizing Textual Entailment Using Sentence Similarity based on 
Dependency Tree Skeletons 
 Rui Wang and G?nter Neumann 
LT-lab, DFKI 
Stuhlsatzenhausweg 3, 66123 Saarbr?cken, Germany 
{wang.rui,Neumann}@dfki.de 
 
 
Abstract 
We present a novel approach to RTE that 
exploits a structure-oriented sentence rep-
resentation followed by a similarity func-
tion. The structural features are automati-
cally acquired from tree skeletons that are 
extracted and generalized from dependency 
trees. Our method makes use of a limited 
size of training data without any external 
knowledge bases (e.g. WordNet) or hand-
crafted inference rules. We have achieved 
an accuracy of 71.1% on the RTE-3 devel-
opment set performing a 10-fold cross 
validation and 66.9% on the RTE-3 test 
data. 
1 Introduction 
Textual entailment has been introduced as a rela-
tion between text expressions, capturing the fact 
that the meaning of one expression can be inferred 
from the other (Dagan and Glickman, 2004). More 
precisely, textual entailment is defined as ?? a 
relationship between a coherent text T and a lan-
guage expression, which is considered as a hy-
pothesis, H. We say that T entails H (H is a conse-
quent of T), denoted by T ? H, if the meaning of 
H, as interpreted in the context of T, can be in-
ferred from the meaning of T.?  
Table 1 displays several examples from the 
RTE-3 development set. For the third pair (id=410) 
the key knowledge needed to decide whether the 
entailment relation holds is that ?[PN1]?s wife, 
[PN2]? entails ?The name of [PN1]?s wife is 
[PN2]?, although T contains much more (irrelevant) 
information. On the other hand, the first pair (id=1) 
requires an understanding of concepts with oppo-
site meanings (i.e. ?buy? and ?sell?), which is a 
case of semantic entailment. 
The different sources of possible entailments 
motivated us to consider the development of spe-
cialized entailment strategies for different NLP 
tasks. In particular, we want to find out the poten-
tial connections between entailment relations be-
longing to different linguistic layers for different 
applications. 
In this paper, we propose a novel approach to-
wards structure-oriented entailment based on our 
empirical discoveries from the RTE corpora: 1) H 
is usually textually shorter than T; 2) not all infor-
mation in T is relevant to make decisions for the 
entailment; 3) the dissimilarity of relations among 
the same topics between T and H are of great im-
portance.  
Based on the observations, our primary method 
starts from H to T (i.e. in the opposite direction of 
the entailment relation) so as to exclude irrelevant 
information from T. Then corresponding key top-
ics and predicates of both elements are extracted. 
We then represent the structural differences be-
tween T and H by means of a set of Closed-Class 
Symbols. Finally, these acquired representations 
(named Entailment Patterns - EPs) are classified by 
means of a subsequence kernel. 
The Structure Similarity Function is combined 
with two robust backup strategies, which are re-
sponsible for cases that are not handled by the EPs. 
One is a Triple Similarity Function applied on top 
of the local dependency relations of T and H; the 
other is a simple Bag-of-Words (BoW) approach 
that calculates the overlapping ratio of H and T. 
Together, these three methods deal with different 
entailment cases in practice. 
36
2 Related Work 
Conventional methods for RTE define measures for 
the similarity between T and H either by assuming 
an independence between words (Corley and Mi-
halcea, 2005) in a BoW fashion or by exploiting 
syntactic interpretations. (Kouylekov and Magnini, 
2006) explore a syntactic tree editing distance to 
detect entailment relations. Since they calculate the 
similarity between the two dependency trees of T 
and H directly, the noisy information may decrease 
accuracy. This observation actually motivated us to 
start from H towards the most relevant information 
in T. 
Logic rules (as proposed by (Bos and Markert, 
2005)) or sequences of allowed rewrite rules (as in 
(de Salvo Braz et al, 2005)) are another fashion of 
tackling RTE. One the best two teams in RTE-2 
(Tatu et al, 2006) proposed a knowledge represen-
tation model which achieved about 10% better per-
formance than the third (Zanzotto and Moschitti, 
2006) based on their logic prover. The other best 
team in RTE-2 (Hickl et al, 2006) automatically 
acquired extra training data, enabling them to 
achieve about 10% better accuracy than the third as 
well. Consequently, obtaining more training data 
and embedding deeper knowledge were expected 
to be the two main directions pointed out for future 
research in the RTE-2 summary statement. How-
ever, except for the positive cases of SUM, T-H 
pairs are normally not very easy to collect auto-
matically. Multi-annotator agreement is difficult to 
reach on most of the cases as well. The knowledge-
based approach also has its caveats since logical 
rules are usually implemented manually and there-
fore require a high amount of specialized human 
expertise in different NLP areas. 
 Another group (Zanzotto and Moschitti, 2006) 
utilized a tree kernel method for cross-pair similar-
ity, which showed an improvement, and this has 
motivated us to investigate kernel-based methods. 
The main difference in our method is that we apply 
subsequence kernels on patterns extracted from the 
dependency trees of T and H, instead of applying 
tree kernels on complete parsing trees. On the one 
hand, this allows us to discover essential parts in-
dicating an entailment relationship, and on the 
other hand, computational complexity is reduced. 
3 An Overview of RTE 
Figure 1 shows the different processing techniques 
and depths applied to the RTE task. Our work fo-
cuses on constructing a similarity function operat-
ing between sentences. In detail, it consists of sev-
eral similarity scores with different domains of 
locality on top of the dependency structure. Figure 
2 gives out the workflow of our system. The main 
part of the sentence similarity function is the Struc-
ture Similarity Function; two other similarity 
scores are calculated by our backup strategies. The 
first backup strategy is a straightforward BoW 
method that we will not present in this paper (see 
more details in (Corley and Mihalcea, 2005)); 
Id Ta s k Te x t H y po th e s i s En ta i l s ?
1 IE
Th e sa le wa s m a d e to  p a y Yu ko s' U S $  2 7 .5  b illio n  ta x b ill, Yu g a n skn efteg a z 
wa s o rig in a lly so ld  fo r U S $  9 .4  b illio n  to  a  little  kn o wn  co m p a n y 
B a ika lfin a n sg ro u p  wh ich  wa s la ter b o ught  b y th e R u ssia n  sta te-o wn ed  o il 
B a ika lfin a n sg ro u p  
wa s so ld  to  
R o sn eft.
Y E S
3 9 0 IR
T yp ho o n  X a n g sa n e la shed  th e P hilip p ine  ca p ita l o n  Th u rsd a y, 
g ro u n d in g  flig h ts, h a ltin g  vessels a n d  clo sin g  sch o o ls a n d  m a rkets a fter 
trig g erin g  fa ta l fla sh  flo o d s in  th e cen tre o f th e  co u n try.
A  typ ho o n  b a tters 
th e  P hilip p ines . Y E S
4 1 0 Q A
(S en ten ce 1  ...) . A lo n g  with  th e first la d y's m o th er, Jen n a  Welch , th e 
weeken d  g a th erin g  in clu d es th e p resid en t's p a ren ts, fo rm er P resid en t 
G eo rge H .W . Bush a nd  his wife, Ba rb a ra ;  h is sister D o ro  Ko ch  a n d  h er 
h u sb a n d , B o b b y;  a n d  h is b ro th er, M a rvin , a n d  h is wife , M a rg a ret.
Th e n a m e o f 
G eo rg e H .W. 
B u sh 's wife is 
B a rb a ra .
Y E S
7 3 9 SU M
Th e FD A  wo u ld  n o t sa y in  wh ich  sta tes  th e p ills  h a d  b een  so ld , b u t 
in stea d  reco m m en d ed  th a t cu sto m ers d eterm in e wh eth er p ro d u cts th ey 
b o u g h t a re b ein g  reca lled  b y ch eckin g  th e sto re list  o n  th e  FD A  Web  site , 
a n d  th e b a tch  list .  Th e b a tch  n u m b ers a p p ea r o n  th e co n ta in er's la b el.
Th e FD A 
p ro vid ed  a  list  o f 
sta tes  in  wh ich  th e 
p ills  h a ve b een  
N O
Table 1 Examples from RTE-3 
Figure 1 Overview of RTE 
37
while the second one is based on a triple set repre-
sentation of sentences that expresses the local de-
pendency relations found by a parser1. 
A dependency structure consists of a set of triple 
relations (TRs). A TR is of the form <node1, rela-
tion, node2>, where node1 represents the head, 
node2 the modifier and relation the dependency 
relation. Chief requirements for the backup system 
are robustness and simplicity. Accordingly, we 
construct a similarity function, the Triple Similar-
ity Function (TSF), which operates on two triple 
sets and determines how many triples of H2 are 
contained in T. The core assumption here is that 
the higher the number of matching triple elements, 
the more similar both sets are, and the more likely 
it is that T entails H. 
TSF uses an approximate matching function. 
Different cases (i.e. ignoring either the parent node 
or the child node, or the relation between nodes) 
might provide different indications for the similar-
ity of T and H. In all cases, a successful match be-
tween two nodes means that they have the same 
lemma and POS. We then sum them up using dif-
ferent weights and divide the result by the cardinal-
ity of H for normalization. The different weights 
learned from the corpus indicate that the ?amount 
of missing linguistic information? affect entailment 
decisions differently. 
4 Workflow of the Main Approach 
Our Structure Similarity Function is based on the 
hypothesis that some particular differences be-
tween T and H will block or change the entailment 
relationship. Initially we assume when judging the 
entailment relation that it holds for each T-H pair 
                                                 
1
 We are using Minipar (Lin, 1998) and Stanford Parser (Klein 
and Manning, 2003) as preprocessors, see also sec. 5.2. 
2
 Note that henceforth T and H will represent either the origi-
nal texts or the dependency structures. 
(using the default value ?YES?). The major steps 
are as follows (see also Figure 2): 
4.1 Tree Skeleton Extractor 
Since we assume that H indicates how to extract 
relevant parts in T for the entailment relation, we 
start from the Tree Skeleton of H (TSH). First, we 
construct a set of keyword pairs using all the nouns 
that appear in both T and H. In order to increase 
the hits of keyword pairs, we have applied a partial 
search using stemming and some word variation 
techniques on the substring level. For instance, the 
pair (id=390) in Table 1 has the following list of 
keyword pairs, 
<Typhoon_Xangsane ## typhoon, 
Philippine ## Philippines> 
Then we mark the keywords in the dependency 
trees of T and H and extract the sub-trees by ignor-
ing the inner yields. Usually, the Root Node of H 
(RNH) is the main verb; all the keywords are con-
tained in the two spines of TSH (see Figure 3). 
Note that in the Tree Skeleton of T (TST), 1) the 
Root Node (RNT) can either be a verb, a noun or 
even a dependency relation, and 2) if the two Foot 
Nodes (FNs) belong to two sentences, a dummy 
node is created that connects the two spines. 
Thus, the prerequisite for this algorithm is that 
TSH has two spines containing all keywords in H, 
and T satisfies this as well. For the RTE-3 devel-
opment set, we successfully extracted tree skele-
Figure 3 Example of a Tree Skeleton 
Figure 2 Workflow of the System 
38
?
=
?
=
+?
=
?
=
=
><><
||
1
|'|
1'
)
'
,(
||
1
|'|
1'
)
'
,(
)',',,(
H
j
H
j j
CCSjCCSCCSK
T
i
T
i i
CCSiCCSCCSK
HTHT
esubsequencK
tons from 254 pairs, i.e., 32% of the data is cov-
ered by this step, see also sec. 5.2. 
Next, we collapse some of the dependency rela-
tion names from the parsers to more generalized 
tag names, e.g., collapsing <OBJ2> and <DESC> 
to <OBJ>. We group together all nodes that have 
relation labels like <CONJ> or <NN>, since they 
are assumed to refer to the same entity or belong to 
one class of entities sharing some common charac-
teristics. Lemmas are removed except for the key-
words. Finally, we add all the tags to the CCS set. 
Since a tree skeleton TS consists of spines con-
nected via the same root node, TS can be trans-
formed into a sequence. Figure 4 displays an ex-
ample corresponding to the second pair (id=390) of 
Table 1. Thus, the general form of a sequential rep-
resentation of a tree skeleton is: 
LSP #RN# RSP 
where LSP represents the Left Spine, RSP repre-
sents the Right Spine, and RN is the Root Node. 
On basis of this representation, a comparison of the 
two tree skeletons is straightforward: 1) merge the 
two LSPs by excluding the longest common prefix, 
and 2) merge the two RSPs by excluding the long-
est common suffix. Then the Spine Difference (SD) 
is defined as the remaining infixes, which consists 
of two parts, SDT and SDH. Each part can be either 
empty (i.e. ?) or a CCS sequence. For instance, the 
two SDs of the example in Figure 4 (id=390) are 
(LSD ? Left SD; RSD ? Right SD; ## is a separa-
tor sign): 
LSDT(N) ## LSDH(?) 
RSDT(?) ## RSDH(?) 
We have observed that two neighboring depend-
ency relations of the root node of a tree skeleton 
(<SUBJ> or <OBJ>) can play important roles in 
predicting the entailment relation as well. There-
fore, we assign them two extra features named 
Verb Consistence (VC) and Verb Relation Con-
sistence (VRC). The former indicates whether two 
root nodes have a similar meaning, and the latter 
indicates whether the relations are contradictive 
(e.g. <SUBJ> and <OBJ> are contradictive). 
We represent the differences between TST and 
TSH by means of an Entailment Pattern (EP), 
which is a quadruple <LSD, RSD, VC, VRC>. VC 
is either true or false, meaning that the two RNs 
are either consistent or not. VRC has ternary value, 
whereby 1 means that both relations are consistent, 
-1 means at least one pair of corresponding rela-
tions is inconsistent, and 0 means RNT is not a 
verb.3 The set of EPs defines the feature space for 
the subsequence kernels in our Structure Similarity 
Function. 
4.2 Structure Similarity Function 
We define the function by constructing two basic 
kernels to process the LSD and RSD part of an EP, 
and two trivial kernels for VC and VRC. The four 
kernels are combined linearly by a composite ker-
nel that performs binary classification on them. 
Since all spine differences SDs are either empty 
or CCS sequences, we can utilize subsequence 
kernel methods to represent features implicitly, cf. 
(Bunescu and Mooney, 2006). Our subsequence 
kernel function is: 
whereby T and H refers to all spine differences 
SDs from T and H, and |T| and |H| represent the 
cardinalities of SDs. The function KCCS(CCS,CCS?) 
checks whether its arguments are equal. 
Since the RTE task checks the relationship be-
tween T and H, we need to consider collocations 
of some CCS subsequences between T and H as 
well. Essentially, this kernel evaluates the similar-
ity of T and H by means of those CCS subse-
quences appearing in both elements. The kernel 
function is as follows: 
On top of the two simple kernels, KVC, and KVRC, 
we use a composite kernel to combine them line-
arly with different weights: 
VRCVCncollocatioesubsequenccomposite KKKKK ???? +++= , 
                                                 
3
 Note that RNH is guaranteed to be a verb, because otherwise 
the pair would have been delegated to the backup strategies. 
????
= = = =
?=
><><
||
1
'||
1'
||
1
'||
1'
''
),(),(
)',',,(
T
i
T
i
H
j
H
j
jjCCSiiCCS
ncollocatio
CCSCCSKCCSCCSK
HTHTK
Figure 4 Spine Merging 
39
where ? and ? are learned from the training corpus; 
?=?=1. 
5 Evaluation 
We have evaluated four methods: the two backup 
systems as baselines (BoW and TSM, the Triple 
Set Matcher) and the kernel method combined with 
the backup strategies using different parsers, Mini-
par (Mi+SK+BS) and the Stanford Parser 
(SP+SK+BS). The experiments are based on RTE-
3 Data 4 . For the kernel-based classification, we 
used the classifier SMO from the WEKA toolkit 
(Witten and Frank, 1999).  
5.1 Experiment Results 
RTE-3 data include the Dev Data (800 T-H pairs, 
each task has 200 pairs) and the Test Data (same 
size). Experiment A performs a 10-fold cross-
validation on Dev Data; Experiment B uses Dev 
Data for training and Test Data for testing cf. Table 
2 (the numbers denote accuracies): 
Systems\Tasks IE IR QA SUM All 
Exp A: 10-fold Cross Validation on RTE-3 Dev Data 
BoW 54.5 70 76.5 68.5 67.4 
TSM 53.5 60 68 62.5 61.0 
Mi+SK+BS 63 74 79 68.5 71.1 
SP+SK+BS 60.5 70 81.5 68.5 70.1 
Exp B: Train: Dev Data; Test: Test Data 
BoW 54.5 66.5 76.5 56 63.4 
TSM 54.5 62.5 66 54.5 59.4 
Mi+SP+SK+BS 58.5 70.5 79.5 59 66.9 
Table 2 Results on RTE-3 Data 
For the IE task, Mi+SK+BS obtained the highest 
improvement over the baseline systems, suggesting 
that the kernel method seems to be more appropri-
ate if the underlying task conveys a more ?rela-
tional nature.? Improvements in the other tasks are 
less convincing as compared to the baselines. Nev-
ertheless, the overall result obtained in experiment 
B would have been among the top 3 of the RTE-2 
challenge. We utilize the system description table 
of (Bar-Haim et al, 2006) to compare our system 
with the best two systems of RTE-2 in Table 35: 
                                                 
4
 See (Wang and Neumann, 2007) for details concerning the 
experiments of our method on RTE-2 data. 
5
 Following the notation in  (Bar-Haim et al, 2006): Lx: Lexi-
cal Relation DB; Ng: N-Gram / Subsequence overlap; Sy: 
Syntactic Matching / Alignment; Se: Semantic Role Labeling; 
LI: Logical Inference; C: Corpus/Web; M: ML Classification; 
B: Paraphrase Technology / Background Knowledge; L: Ac-
quisition of Entailment Corpora. 
Systems Lx Ng Sy Se LI C M B L 
Hickl et al X X X X  X X  X 
Tatu et al X    X   X  
Ours  X X    X   
Table 3 Comparison with the top 2 systems in 
RTE-2. 
Note that the best system (Hickl et al, 2006) ap-
plies both shallow and deep techniques, especially 
in acquiring extra entailment corpora. The second 
best system (Tatu et al, 2006) contains many 
manually designed logical inference rules and 
background knowledge. On the contrary, we ex-
ploit no additional knowledge sources besides the 
dependency trees computed by the parsers, nor any 
extra training corpora. 
5.2 Discussions 
Table 4 shows how our method performs for the 
task-specific pairs matched by our patterns: 
Tasks IE IR QA SUM ALL 
ExpA:Matched 53 19 23.5 31.5 31.8 
ExpA:Accuracy 67.9 78.9 91.5 71.4 74.8 
ExpB:Matched 58.5 16 27.5 42 36 
ExpB:Accuracy 57.2 81.5 90.9 65.5 68.8 
Table 4 Performances of our method 
For IE pairs, we find good coverage, whereas 
for IR and QA pairs the coverage is low, though it 
achieves good accuracy. According to the experi-
ments, BoW has already achieved the best per-
formance for SUM pairs cf. Table 2. 
As a whole, developing task specific entailment 
operators is a promising direction. As we men-
tioned in the first section, the RTE task is neither a 
one-level nor a one-case task. The experimental 
results uncovered differences among pairs of dif-
ferent tasks with respect to accuracy and coverage. 
On the one hand, our method works successfully 
on structure-oriented T-H pairs, most of which are 
from IE. If both TST and TSH can be transformed 
into CCS sequences, the comparison performs well, 
as in the case of the last example (id=410) in Table 
1. Here, the relation between ?wife?, ?name?, and 
?Barbara? is conveyed by the punctuation ?,?, the 
verb ?is?, and the preposition ?of?. Other cases like 
the ?work for? relation of a person and a company 
or the ?is located in? relation between two location 
names are normally conveyed by the preposition 
?of?. Based on these findings, taking into account 
more carefully the lexical semantics based on in-
ference rules of functional words might be helpful 
in improving RTE. 
40
On the other hand, accuracy varies with T-H 
pairs from different tasks. Since our method is 
mainly structure-oriented, differences in modifiers 
may change the results and would not be caught 
under the current version of our tree skeleton. For 
instance, ?a commercial company? will not entail 
?a military company?, even though they are struc-
turally equivalent. 
Most IE pairs are constructed from a binary rela-
tion, and so meet the prerequisite of our algorithm 
(see sec. 4.1). However, our method still has rather 
low coverage. T-H pairs from other tasks, for ex-
ample like IR and SUM, usually contain more in-
formation, i.e. more nouns, the dependency trees of 
which are more complex. For instance, the pair 
(id=739) in Table 1 contains four keyword pairs 
which we cannot handle by our current method. 
This is one reason why we have constructed extra 
T-H pairs from MUC, TREC, and news articles 
following the methods of (Bar-Haim et al, 2006). 
Still, the overall performance does not improve. 
All extra training data only serves to improve the 
matched pairs (about 32% of the data set) for 
which we already have high accuracy (see Table 4). 
Thus, extending coverage by machine learning 
methods for lexical semantics will be the main fo-
cus of our future work. 
6 Conclusions and Future Work 
Applying different RTE strategies for different 
NLP tasks is a reasonable solution. We have util-
ized a structure similarity function to deal with the 
structure-oriented pairs, and applied backup strate-
gies for the rest. The results show the advantage of 
our method and direct our future work as well. In 
particular, we will extend the tree skeleton extrac-
tion by integrating lexical semantics based on in-
ference rules for functional words in order to get 
larger domains of locality. 
Acknowledgements 
The work presented here was partially supported 
by a research grant from BMBF to the DFKI pro-
ject HyLaP (FKZ: 01 IW F02) and the EC-funded 
project QALL-ME. 
References 
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampic-
colo, D., Magnini, B. and Szpektor, I. 2006. The Sec-
ond PASCAL Recognising Textual Entailment Chal-
lenge. In Proc. of the PASCAL RTE-2 Challenge. 
Bos, J. and Markert, K. 2005. Combining Shallow and 
Deep NLP Methods for Recognizing Textual Entail-
ment. In Proc. of the PASCAL RTE Challenge. 
Bunescu, R. and Mooney, R. 2006. Subsequence Ker-
nels for Relation Extraction. In Advances in Neural 
Information Processing Systems 18. MIT Press. 
Corley, C. and Mihalcea, R. 2005. Measuring the Se-
mantic Similarity of Texts. In Proc. of the ACL 
Workshop on Empirical Modeling of Semantic 
Equivalence and Entailment. 
Dagan, R., Glickman, O. 2004. Probabilistic textual 
entailment: Generic applied modelling of language 
variability. In PASCAL Workshop on Text Under-
standing and Mining. 
de Salvo Braz, R., Girju, R., Punyaka-nok, V., Roth, D., 
and Sammons, M. 2005. An Inference Model for Se-
mantic Entailment in Natural Language. In Proc. of 
the PASCAL RTE Challenge. 
Hickl, A., Williams, J., Bensley, J., Roberts, K., Rink, 
B. and Shi, Y. 2006. Recognizing Textual Entailment 
with LCC?s GROUNDHOG System. In Proc. of the 
PASCAL RTE-2 Challenge. 
Klein, D. and Manning, C. 2003. Accurate Unlexical-
ized Parsing. In Proc. of ACL 2003. 
Kouylekov, M. and Magnini, B. 2006. Tree Edit Dis-
tance for Recognizing Textual Entailment: Estimat-
ing the Cost of Insertion. In Proc. of the PASCAL 
RTE-2 Challenge. 
Lin, D. 1998. Dependency-based Evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Sys-
tems. 
Tatu, M., Iles, B., Slavik, J., Novischi, A. and Moldo-
van, D. 2006. COGEX at the Second Recognizing 
Textual Entailment Challenge. In Proc. of the PAS-
CAL RTE-2 Challenge. 
Wang, R. and Neumann, G. 2007. Recognizing Textual 
Entailment Using a Subsequence Kernel Method. In 
Proc. of AAAI 2007. 
Witten, I. H. and Frank, E. Weka: Practical Machine 
Learning Tools and Techniques with Java Implemen-
tations. Morgan Kaufmann, 1999. 
Zanzotto, F.M. and Moschitti, A. 2006. Automatic 
Learning of Textual Entailments with Cross-pair 
Similarities. In Proc. of ACL 2006. 
41
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 137?140,
Prague, June 2007. c?2007 Association for Computational Linguistics
DFKI2: An Information Extraction Based Approach to People
Disambiguation
Andrea Heyl
German Research Center for
Artificial Intelligence - DFKI,
Saarbru?cken, Germany
andrea.heyl@dfki.de
Gu?nter Neumann
German Research Center for
Artificial Intelligence - DFKI,
Saarbru?cken, Germany
guenter.neumann@dfki.de
Abstract
We propose an IE based approach to people
disambiguation. We assume the mentioning
of NEs and the relational context of a per-
son in the text to be important discriminat-
ing features in order to distinguish different
people sharing a name.
1 Introduction
In this paper, we propose a system with a linguis-
tic view on people disambiguation that exploits the
relational and NE context of a person name as dis-
criminating features.
Texts about different people differ from each
other by the names of persons, places and organiza-
tions connected to these people and by the relations
in which a person?s name is connected to other enti-
ties. Therefore we had the hypothesis that the NEs in
the documents for a person name should be a main
distinctive criterion for disambiguating people.
Furthermore, the relational context of a person
name should also be able to give good clues for dis-
ambiguation. Sentence patterns related to a name,
i.e. patterns that contain the name as subject or
object like ?be(Person X, lawyer)? often convey
uniquely identifying information about a person.
Our system was not built specifically for the web
people search task WePS (Artiles et al, 2007), but
is an early version of an IE system that has the more
general goal to discover relations between NEs. We
see the WePS task as a specific instance of the set of
tasks our system should be able to handle. There-
fore, we only adapted it slightly to work with the
WePS data, but did not make any further customiza-
tion w.r.t. the special requirements of people disam-
biguation. As our system was built to handle pure
texts rather than structured web pages, we relied
completely on linguistic information and did not ex-
ploit the html structure of the documents provided.
2 Related Work
Our system was inspired by the preemptive and on-
demand IE approaches by Sekine and Shinyama
(Sekine, 2006; Shinyama, 2006) that cluster news-
paper articles into classes of articles that talk about
the same type of event. They proposed a system to
discover in advance all possible relations and to re-
turn them in form of tables.
We took the idea of distinctive personal attributes
as a criterion for disambiguation from the work of
Bollegala et al (2006). They propose an unsu-
pervised learning approach to extract phrases that
uniquely identify a person from the web and use
these discriminative features for clustering.
3 System Overview
The goal of the WePS task is to cluster the top 100
web pages returned by a web search engine for a
certain name as search query and classify them w.r.t.
the underlying different people they refer to.
The problem of clustering documents about peo-
ple into different entities can be seen as two sub-
problems: The determination of the correct num-
ber of clusters and the clustering of the given doc-
uments into this number of entities. These problems
could either be solved consecutively by first estimat-
ing the number of classes and then produce this pre-
137
html ? text

coreference
resolution
 ''PP
P
NE-tagging

// semantic
parsing
vvmmm
m
feature vectors // clustering
Figure 1: System Overview
set number of clusters or by determining the number
of classes dynamically during the clustering process.
Figure 1 gives an overview of our system, that
clusters web documents into a pre-defined number
of classes, thereby being only concerned with the
second problem and neglecting the estimation of dif-
ferent namesakes for now.
Every web page in the WePS training data is rep-
resented by the set of its files. As our system works
on plain text only, we first needed to separate the
textual parts of all files. Therefore, we extracted the
text from the html pages. We merged the texts from
all different html pages belonging to a single web-
site into one document so that we obtained for every
person?s name 100 text files as the basis for further
clustering.
These text files were processed by a coreference
resolution tool. On the resulting texts, we ran both
an NE tagger and an NLP tool for semantic parsing.
This tool represents sentences containing the respec-
tive person name as predicate argument structures.
We constructed two feature vectors for each file
based on the counts of the NEs and predicate ar-
gument structures that contain the specific person
name. Those feature vectors were our basis for the
clustering process.
The clustering unit of the system consecutively
merged clusters, that at first contained a single file
each, until the pre-set number of classes was reached
and returned the clustering as an xml file.
4 System Components
4.1 Estimating the Number of Classes
In principle, the number of different people that are
represented in the data cannot be known in advance.
However, for the clustering process, either the num-
ber of classes has to be fixed before clustering, or
some kind of termination criterion has to be found
that tells the algorithm when to stop clustering.
A good estimation of the number of different en-
tities is a necessary prerequisite for successful clus-
tering. Clustering into too many classes would mean
assigning documents to classes that have actually no
own entity they refer to. Clustering into too few
classes means merging two entities into one class.
Our initial intuition was to distinguish people by
normally unique properties, like phone numbers or
email addresses. So we assumed that the number of
different email addresses and phone numbers occur-
ring in all documents for one name would be a good
means to estimate the number of different persons
sharing this name, but we could not find any corre-
lation between these features and the class number.
Therefore, we decided to estimate the average
number of classes from the training data. The aver-
age number of different people for one name in the
training data was about 18. Based on the observa-
tion that an underestimated number of classes leads
to better results than assuming too many classes, we
decided to guess 12 different persons for each name.
4.2 Preprocessing
For the extraction of plain text information from the
web pages, we used the html2text 1 converter. In
case that a web page consisted of more than one html
document, we put all the output from the converter
into one single file. By omitting any wrapping of
the html pages, we obviously lost useful structural
information but got the textual information for our
linguistic analysis.
Afterward, we applied several linguistic prepro-
cessing tools. We used coreference resolution to re-
place pronouns referring to a person, and variations
of a name (like ?Mr. Smith? after a mention of ?John
Smith? earlier in the text) with the person?s name in
the form of its first mention in the text.
For NE-tagging, we used the three NE types PER-
SON, LOCATION and ORGANIZATION. For both
NE tagging and coreference resolution, we used the
LingPipe toolkit 2. We counted the occurrences of
every NE in every file and replaced all instances
by their specific NE type combined with a uniquely
1http://www.mbayer.de/html2text/index.shtml
2http://www.alias-i.com/lingpipe/
138
identifying number, e.g. we replaced all occurrences
of ?Paris? with ?LOCATION27?, in order to ensure
that the predicate argument parser could work cor-
rectly and would not split up multi-word NEs into
two or more arguments.
We passed all sentences with NEs that con-
tained the specified persons family name (e.g.
?Mr. Cooper? for the name ?Alvin Cooper?) to
MontyLingua 3, that returns a semantic represen-
tation of the sentence like (?live? ?PERSON2?
?in LOCATION3?). These representations abstract
from the actual surface form of a sentence as they
represent every sentence in its underlying semantic
form (?predicate? ?semantic subject? ?semantic ob-
ject1?...) rather than just determining the syntactic
subject and objects of a sentence. We called these
structures ?patterns? and kept only those that actu-
ally contained the respective NE.
4.3 Clustering
We decided on building two vectors for every text
file, one for the NEs and one for sentence patterns
connected to a person?s name in order to give to the
NEs a weight different from that for the patterns.
After tagging the documents for NEs, we counted
the frequency of the different occurring NEs for one
name. We built a first feature vector for each docu-
ment that contained as entries the counts of the oc-
curring NEs in this document. We set a threshold n
to use only the n best NEs in the vectors, counted
over all documents for one name. We then built for
every document a second feature vector containing
the counts of the MontyLingua patterns for the doc-
ument.
For the actual clustering process, we used hierar-
chical clustering. We started with every file, rep-
resented by a pair of normalized feature vectors,
constituting a single cluster. As distance measure-
ment we used the weighted sum of the absolute dis-
tances between the centers of two clusters with re-
gard to both feature vectors, respectively, i.e. we
chose distance = w?distanceNEs+distancepatterns.
In every step, we made a pairwise comparison of all
clusters and merged those with the lowest distance.
The clustering terminated when the algorithm came
down to the pre-set number of 12 clusters. So far
3http://web.media.mit.edu/ hugo/montylingua/
we have not made any further use of the binary tree
structure within each cluster.
We assigned every file to exactly one cluster. We
had neither a ?discarded? category nor did we handle
the possibility that a page refers to more than one
person and would hence belong to different clusters.
5 Experiments
5.1 Training of Parameters
We evaluated the system on the provided WePS
training data to estimate the following parameters:
number of classes, number of best NEs to be consid-
ered and weight of the NE vector compared to the
pattern vector.
The relevant evaluation score is the F-measure
(? = 0.5) as the harmonic mean of purity and in-
verse purity as described by Hotho et al (2003).
As our attempt to use distinctive features for the
estimation of class numbers failed, we examined the
influence of a wrongly estimated number of classes
on the clustering results. Table 1 shows exemplarily
for 2 person names how the F-measure varies if the
correct number of classes is incorrectly assumed as a
higher or lower value. We concluded that it is better
to estimate the class number too low than too high.
name A. Macomb E. Fox
correct number of classes 21 16
10 classes assumed 0.76 0.80
12 classes assumed 0.75 0.75
14 classes assumed 0.72 0.76
16 classes assumed 0.69 0.60
18 classes assumed 0.60 0.58
20 classes assumed 0.48 0.72
22 classes assumed 0.56 0.55
24 classes assumed 0.59 0.58
26 classes assumed 0,52 0.56
Table 1: F-measure for different numbers of as-
sumed classes
Primarily meant as a means to reduce computa-
tion time, we gave our system the possibility not to
use all occurring NEs for clustering, but only a cer-
tain number of entities with maximal frequencies.
Test runs did not confirm our hypothesis that con-
sidering a higher number of NEs leads to better re-
sults (cf. table 2). For both training of the number of
NEs and the NE weight we assumed that we already
knew the correct class number.
As the F-measure did not increase for more con-
sidered NEs, we believe that the most important NEs
139
are already covered within the best 100 and that
adding more NEs rather adds coincidental informa-
tion than any new important facts. Usually, the best
100 NEs already cover most of those which occur
more than once in a text.
NEs average. F-measure
100 0.66
200 0.68
500 0.68
1000 0.67
w average F-measure
0.5 0.66
1.0 0.68
2.0 0.68
4.0 0.67
Table 2: varying the number of considered entities
and weight of the feature vectors
The third parameter to estimate was the weight
w given to the NE feature vector compared to the
feature vector for sentence patterns. During training,
this weight also appeared to have little influence on
the clustering results (cf. 2). We have the hypothesis
that sentence pattern detection is not very successful
for the often unstructured web page texts.
5.2 Results for WePS Test Data
In the WePS evaluation, our system scored with a
purity of 0.39, an inverse purity of 0.83 and a result-
ing overall F-measure (? = 0.5) of 0.5.
One main reason for our test results to be worse
than our training results is the fact that the test data
had a much higher average number of classes (about
46 classes). Our F-measure was best for those names
with the fewest number of referents. We had an av-
erage F-Measure (? = 0.5) of 0.66 for those names
with less than 30 instances compared to an overall
average of 0.50. These numbers show the impor-
tance of a correct estimation of the assumed number
of referents for a name.
Our purity was much lower than the inverse pu-
rity, i.e. there is too much noise in our clustering
compared to the real partition, whereas the real clus-
ters are well covered by our clustering. This is due
to a too low estimation of the number of referents.
6 Conclusions and Future Work
One obvious improvement , that would accommo-
date the general relation extraction idea of our sys-
tem, is to include the use of structural information
from the html documents in addition to our purely
linguistic view on web pages. Additionally, we
should weight our NEs using e.g. a TF/IDF formula.
A promising direction for further research in peo-
ple search will certainly include a better control of
the number of classes. This could be done either
by estimating this number in advance, or by setting
the number of classes dynamically during cluster-
ing. The latter could include comparing the size of
the current clusters to the overall feature space of all
clusters or an approach of counting occurrences of
uniquely identifying attributes within a cluster.
This second approach could match the original
purpose of our system, namely to build tables that
represent the most salient relations in a set of docu-
ments in the way Sekine and Shinyama did. If such
a table, that represents the slots of a relation in its
columns and every article in a row, is built for all
documents in a cluster, we would expect the table to
contain roughly the same information in every row.
One could define a consistency measure for the re-
sulting tables and stop clustering as soon as the ta-
bles are no longer consistent enough, i.e. when they
contain too much contradictory information.
Acknowledgment
The work presented here was supported by a re-
search grant from the Investitionsbank Berlin to the
DFKI project IDEX (Interactive Dynamic IE).
References
Javier Artiles, Julio Gonzalo and Satoshi Sekine. 2007.
The SemEval-2007 WePS Evaluation: Establishing a
Benchmark for the Web People Search Task. Proceed-
ings of Semeval 2007, ACL.
Danushka Bollegala, Yutaka Matsuo and Mitsuru
Ishizuka. 2006. Extracting Key Phrases to Disam-
biguate Personal Name Queries in Web Search. Pro-
ceedings of the Workshop on How Can Computational
Linguistics Improve Information Retrieval, p. 17?24.
Andreas Hotho, Steffen Staab and Gerd Stumme. 2003.
Wordnet Improves Text Document Clustering. Pro-
ceedings of the Semantic Web Workshop at SIGIR-
2003, 26th Annual International ACM SIGIR Confer-
ence, Toronto, Canada.
Satoshi Sekine. 2006. On-Demand IE. International
Committee on Comp. Ling. and the ACL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive Information Extraction using Unrestricted Rela-
tion Discovery. Human Language Technology con-
ference - North American chapter of the ACL annual
meeting; New York City.
140
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 346?350,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatic Detection and Correction of  Errors in Dependency Tree-
banks
Alexander Volokh
DFKI
Stuhlsatzenhausweg 3
66123 Saarbr?cken, Germany
alexander.volokh@dfki.de
G?nter Neumann
DFKI
Stuhlsatzenhausweg 3
66123 Saarbr?cken, Germany
neumann@dfki.de
Abstract
Annotated corpora are essential for almost all 
NLP applications. Whereas they are expected 
to be of a very high quality because of their 
importance  for  the  followup  developments, 
they still contain a considerable number of er-
rors. With this work we want to draw attention 
to this fact.  Additionally, we try to estimate 
the amount of errors and propose a method for 
their  automatic  correction.  Whereas  our  ap-
proach is able to find only a portion of the er-
rors that we suppose are contained in almost 
any annotated corpus due to the nature of the 
process of its creation, it has a very high pre-
cision, and thus is in any case beneficial for 
the quality of the corpus it  is  applied to. At 
last, we compare it to a different method for 
error detection in treebanks and find out that 
the errors that we are able to detect are mostly 
different and that our approaches are comple-
mentary.
1 Introduction
Treebanks and other annotated corpora  have be-
come essential for almost all NLP applications. Pa-
pers about corpora like the Penn Treebank [1] have 
thousands of citations, since most of the algorithms 
profit from annotated data during the development 
and testing and thus are widely used in the field. 
Treebanks are therefore expected to be of a very 
high  quality  in  order  to  guarantee  reliability  for 
their theoretical and practical uses. The construc-
tion of an annotated corpus involves a lot of work 
performed by large groups. However, despite the 
fact that a lot of human post-editing and automatic 
quality  assurance  is  done,  errors  can  not  be 
avoided completely [5]. 
In this paper we propose an approach for find-
ing and correcting errors in dependency treebanks. 
We apply our method to the English dependency 
corpus ? conversion of the Penn Treebank to the 
dependency format done by Richard Johansson and 
Mihai  Surdeanu [2]  for  the  CoNLL shared tasks 
[3].  This  is  probably  the  most  used  dependency 
corpus, since English is the most popular language 
among the researchers. Still we are able to find a 
considerable amount of errors in it. Additionally, 
we  compare  our  method  with  an  interesting  ap-
proach developed by a different group of research-
ers (see section 2). They are able to find a similar 
number of errors in different corpora, however, as 
our investigation shows, the overlap between our 
results is quite small and the approaches are rather 
complementary.
2 Related Work
Surprisingly, we were not able to find a lot of work 
on the topic of error detection in treebanks. Some 
organisers of shared tasks usually try to guarantee 
a certain quality of the used data, but the quality 
control is usually performed manually. E.g. in the 
already mentioned CoNLL task the organisers ana-
lysed a large amount of dependency treebanks for 
different  languages  [4],  described  problems  they 
have encountered and forwarded them to the de-
velopers  of  the  corresponding corpora.  The  only 
work,  that  we were able to find,  which involved 
automatic quality control, was done by the already 
mentioned  group  around  Detmar  Meurers.  This 
work  includes  numerous  publications  concerning 
finding errors in phrase structures [5] as well as in 
dependency treebanks [6]. The approach is based 
on the concept of ?variation detection?, first intro-
duced  in  [7].  Additionally,  [5]  presents  a  good 
346
method  for  evaluating the automatic  error  detec-
tion. We will perform a similar evaluation for the 
precision of our approach. 
3 Variation Detection
We will  compare  our  outcomes  with  the  results 
that can be found with the approach of ?variation 
detection? proposed by Meurers  et  al.  For  space 
reasons, we will not be able to elaborately present 
this method and advise to read the referred work, 
However, we think that we should at least briefly 
explain its idea.
The idea behind ?variation detection? is to find 
strings, which occur multiple times in the corpus, 
but which have varying annotations. This can obvi-
ously have only two reasons: either the strings are 
ambiguous and can have different  structures,  de-
pending on the meaning, or the annotation is erro-
neous in at least one of the cases. The idea can be 
adapted to dependency structures as well, by ana-
lysing the possible dependency relations between 
same words. Again different dependencies can be 
either the result of ambiguity or errors. 
4 Automatic Detection of Errors
We propose a different approach. We take the Eng-
lish  dependency  treebank  and  train  models  with 
two different  state  of  the  art  parsers:  the  graph-
based  MSTParser  [9]  and  the  transition-based 
MaltParser [10]. We then parse the data, which we 
have used for training, with both parsers. The idea 
behind this step is that we basically try to repro-
duce the gold standard, since parsing the data seen 
during the training is very easy (a similar idea in 
the area of POS tagging is very broadly described 
in  [8]).  Indeed  both  parsers  achieve  accuracies 
between 98% and 99% UAS (Unlabeled Attach-
ment Score), which is defined as the proportion of 
correctly identified dependency relations. The reas-
on why the parsers are not able to achieve 100% is 
on the one hand the fact that some of the phenom-
ena are too rare and are not captured by their mod-
els. On the other hand, in many other cases parsers 
do make correct predictions, but the gold standard 
they are evaluated against is wrong.
We  have  investigated  the  latter  case,  namely 
when both parsers  predict  dependencies  different 
from the gold standard (we do not consider the cor-
rectness of the dependency label). Since MSTPars-
er and MaltParser are based on completely differ-
ent parsing approaches they also tend to make dif-
ferent mistakes [11]. Additionally, considering the 
accuracies of 98-99% the chance that both parsers, 
which  have  different  foundations,  make  an  erro-
neous  decision  simultaneously is  very small  and 
therefore these cases are the most likely candidates 
when looking for errors.
5 Automatic Correction of Errors
In this section we propose our algorithm for auto-
matic  correction of  errors,  which consists  out  of 
the following steps:
1. Automatic  detection  of  error  candidates, 
i.e. cases where two parsers deliver results 
different to gold-standard.
2. Substitution of the annotation of the error 
candidates by the annotation proposed by 
one  of  the  parsers  (in  our  case 
MSTParser).
3. Parse of the modified corpus with a third 
parser (MDParser).
4. Evaluation of the results.
5. The modifications are only kept for those 
cases  when  the  modified  annotation  is 
identical  with  the  one  predicted  by  the 
third parser and undone in other cases. 
For the English dependency treebank we have 
identified  6743  error  candidates,  which  is  about 
0.7% of all tokens in the corpus.
The third dependency parser, which is used is 
MDParser1 - a fast transition-based parser. We sub-
situte  the  gold  standard  by  MSTParser  and  not 
MaltParser in order not to give an advantage to a 
parser  with  similar  basics  (both  MDParser  and 
MDParser are transition-based). 
During this experiment we have found out that 
the result of MDParser significantly improves: it is 
able to correctly recgonize 3535 more dependen-
cies than before the substitution of the gold stand-
ard. 2077 annotations remain wrong independently 
of the changes in the gold standard. 1131 of the re-
lations  become  wrong  with  the  changed  gold 
standard,  whereas they were correct  with the old 
unchanged version. We then undo the changes to 
the gold standard when the wrong cases remained 
wrong and when the correct cases became wrong. 
We suggest that the 3535 dependencies which be-
came correct after the change in gold standard are 
1 http://mdparser.sb.dfki.de/  
347
errors, since a) two state of the art parsers deliver a 
result which differs from the gold standard and b) a 
third parser confirms that by delivering exactly the 
same result as the proposed change. However, the 
exact  precision of  the  approach can probably be 
computed only by manual investigation of all cor-
rected dependencies.
6 Estimating the Overall Number Of Er-
rors
The previous section tries to evaluate the precision 
of the approach for the identified error candidates. 
However, it remains unclear how many of the er-
rors are found and how many errors can be still ex-
pected in the corpus. Therefore in this section we 
will describe our attempt to evaluate the recall of 
the proposed method.
In  order  to  estimate  the  percentage  of  errors, 
which can be found with our method, we have de-
signed the following experiment.  We have taken 
sentences of different lengths from the corpus and 
provided them with a ?gold standard? annotation 
which  was  completely  (=100%)  erroneous.  We 
have achieved that by substituting the original an-
notation by the annotation of a different sentence 
of the same length from the corpus, which did not 
contain  dependency  edges  which  would  overlap 
with the original annotation. E.g consider the fol-
lowing sentence in the (slightly simplified) CoNLL 
format:
1 Not RB 6 SBJ
2 all PDT 1 NMOD
3 those DT 1 NMOD
4 who WP 5 SBJ
5 wrote VBD 1 NMOD
6 oppose VBP 0 ROOT
7 the DT 8 NMOD
8 changes NNS 6 OBJ
9 . . 6 P
We would substitute its annotation by an annota-
tion chosen from a different sentence of the same 
length:
1 Not RB 3 SBJ
2 all PDT 3 NMOD
3 those DT 0 NMOD
4 who WP 3 SBJ
5 wrote VBD 4 NMOD
6 oppose VBP 5 ROOT
7 the DT 6 NMOD
8 changes NNS 7 OBJ
9 . . 3 P
This way we know that we have introduced a 
well-formed dependency tree (since its annotation 
belonged to a different tree before) to the corpus 
and  the  exact  number  of  errors  (since  randomly 
correct  dependencies  are  impossible).  In  case  of 
our example 9 errors are introduced to the corpus.
In  our  experiment  we  have  introduced  sen-
tences  of  different  lengths  with  overall  1350 
tokens.  We  have  then  retrained  the  models  for 
MSTParser and MaltParser and have applied our 
methodology  to  the  data  with  these  errors.  We 
have then counted how many of these 1350 errors 
could  be  found.  Our  result  is  that  619  tokens 
(45.9%)  were different  from the  erroneous gold-
standard. That means that despite the fact that the 
training data contained some incorrectly annotated 
tokens, the parsers were able to annotate them dif-
ferently. Therefore we suggest that the recall of our 
method is close to the value of 0.459. However, of 
course we do not know whether the randomly in-
troduced errors  in  our  experiment  are  similar  to 
those which occur in real treebanks.
7 Comparison with Variation Detection
The interesting question which naturally arises at 
this  point  is  whether  the  errors  we  find  are  the 
same as those found by the method of variation de-
tection. Therefore we have performed the follow-
ing experiment: We have counted the numbers of 
occurrences  for   the  dependencies  B? A (the 
word B is the head of the word A) and C? A
(the  word  C is  the  head  of  the  word  A),  where 
B? A is the dependency proposed by the pars-
ers and  C? A is the dependency proposed by 
the gold standard. In order for variation detection 
to be applicable the frequency counts for both rela-
tions must be available and the counts for the de-
pendency proposed by the parsers should ideally 
greatly outweigh the frequency of the gold stand-
ard, which would be a great indication of an error. 
For the 3535 dependencies that we classify as er-
rors the variation detection method works only 934 
times (39.5%). These are the cases when the gold 
standard is obviously wrong and occurs only few 
times, most often - once, whereas the parsers pro-
348
pose much more frequent dependencies. In all oth-
er cases the counts suggest that the variation detec-
tion would not work, since both dependencies have 
frequent counts or the correct dependency is even 
outweighed by the incorrect one.
8 Examples 
We will provide some of the example errors, which 
we are able to find with our approach. Therefore 
we  will  provide  the  sentence  strings  and briefly 
compare the gold standard dependency annotation 
of a certain dependency within these sentences.
Together, the two stocks wreaked havoc among  
takeover stock traders, and caused a 7.3% drop in  
the DOW Jones Transportation Average, second in  
size  only  to  the  stock-market  crash of  Oct.  19  
1987.
In this sentence the gold standard suggests the 
dependency  relation  market? the ,  whereas 
the  parsers  correctly  recognise  the  dependency 
crash? the .  Both  dependencies  have  very 
high counts  and therefore  the  variation detection 
would not work well in this scenario.
Actually, it  was down only a few points at the 
time.
In  this  sentence  the  gold  standard  suggests 
points?at ,  whereas  the  parsers  predict 
was? at . The gold standard suggestion occurs 
only  once  whereas  the  temporal  dependency 
was? at occurs 11 times in the corpus. This is 
an example of an error which could be found with 
the variation detection as well.
Last October, Mr. Paul paid out $12 million of  
CenTrust's cash ? plus a $1.2 million commission 
? for ?Portrait of a Man as Mars?.
In this sentence the gold standard suggests the 
dependency relation $? a , whereas the parsers 
correctly  recognise  the  dependency 
commission?a .  The  interesting  fact  is  that 
the  relation  $? a is  actually  much  more  fre-
quent than commission?a , e.g. as in the sen-
tence he cought up an additional $1 billion or so. 
( $? an )  So  the  variation  detection  alone 
would not suffice in this case.
9 Conclusion
The quality of treebanks is of an extreme import-
ance for the community.  Nevertheless, errors can 
be found even in the most popular and widely-used 
resources. In this paper we have presented an ap-
proach for  automatic  detection and correction  of 
errors and compared it to the only other work we 
have found in this field. Our results show that both 
approaches are rather complementary and find dif-
ferent types of errors. 
We have only analysed the errors in the head-
modifier annotation of the dependency relations in 
the  English  dependency  treebank.  However,  the 
same methodology can easily be applied to detect 
irregularities in any kind of annotations, e.g. labels, 
POS tags etc. In fact, in the area of POS tagging a 
similar strategy of using the same data for training 
and testing in order to detect  inconsistencies has 
proven to be very efficient [8]. However, the meth-
od lacked means  for  automatic  correction of  the 
possibly inconsistent annotations. Additionally, the 
method off course can as well be applied to differ-
ent corpora in different languages. 
Our  method  has  a  very  high  precision,  even 
though  we  could  not  compute  the  exact  value, 
since it  would require an expert  to go through a 
large number of cases. It is even more difficult to 
estimate the recall of our method, since the overall 
number of errors in a corpus is unknown. We have 
described an experiment  which to  our  mind is  a 
good  attempt  to  evaluate  the  recall  of  our  ap-
proach.  On  the  one  hand  the  recall  we  have 
achieved in this experiment is rather low (0.459), 
which means that our method would definitely not 
guarantee to find all errors in a corpus. On the oth-
er hand it has a very high precision and thus is in 
any case beneficial,  since the quality of the tree-
banks increases with the removal of errors. Addi-
tionally, the low recall suggests that treebanks con-
tain an even larger number of errors, which could 
not  be found.  The overall  number  of errors  thus 
seems to be over 1% of the total size of a corpus, 
which is expected to be of a very high quality. A 
fact that one has to be aware of when working with 
annotated resources and which we would like to 
emphasize with our paper.
10 Acknowledgements
The presented work was partially supported by a 
grant from the German Federal  Ministry of Eco-
nomics  and  Technology  (BMWi)  to  the  DFKI 
Theseus  project  TechWatch?Ordo  (FKZ:  01M-
Q07016). 
349
References
[1]  Mitchell  P.  Marcus,  Beatrice  Santorini  and  Mary 
Ann Marcinkiewicz , 1993. Building a Large Annot-
ated Corpus of English: The Penn Treebank. In Com-
putational Lingustics, vol. 19, pp. 313-330.
[2] Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis Marquez and Joakim Nivre.  The CoNLL-2008 
Shared Task on Joint  Parsing of Syntactic and Se-
mantic  Dependencies.  In  Proceedings  of  the  12th 
Conference  on  Computational  Natural  Language 
Learning (CoNLL-2008), 2008 
[3] Sabine Buchholz and Erwin Marsi, 2006. CoNLL-X 
shared task on multilingual dependency parsing.  In 
Proceedings  of  CONLL-X,  pages  149?164,  New 
York.
[4] Sabine Buchholz and Darren Green, 2006.  Quality 
control  of  treebanks:  documenting,  converting,  
patching. In LREC 2006 workshop on Quality assur-
ance  and  quality  measurement  for  language  and 
speech resources.
[5] Markus Dickinson and W. Detmar Meurers,  2005. 
Prune  Diseased  Branches  to  Get  Healthy  Trees!  
How to Find Erroneous Local Trees in a Treebank  
and Why  It  Matters.  In  Proceedings  of  the  Fourth 
Workshop on Treebanks and Linguistic Theories, pp. 
41?52
[6] Adriane Boyd, Markus Dickinson and Detmar Meur-
ers, 2008. On Detecting Errors in Dependency Tree-
banks.  In  Research on Language and Computation, 
vol. 6, pp. 113-137.
[7] Markus Dickinson and Detmar Meurers, 2003.  De-
tecting inconsistencies in treebanks.  In  Proceedings 
of TLT 2003
[8] van Halteren, H. (2000). The detection of inconsist-
ency  in  manually  tagged  text.  In  A.  Abeill?,  T. 
Brants, and H. Uszkoreit (Eds.), Proceedings of the 
Second Workshop on Linguistically Interpreted Cor-
pora (LINC-00), Luxembourg. 
[9 R. McDonald, F. Pereira, K. Ribarov, and J. Haji?c . 
2005. Non-projective  Dependency  Parsing  using  
Spanning Tree Algorithms. In Proc. of HLT/EMNLP 
2005.
[10]  Joakim Nivre,  Johan  Hall,  Jens  Nilsson,  Atanas 
Chanev,  Gulsen  Eryigit,  Sandra  Kubler,  Svetoslav 
Marinov  and  Erwin  Marsi.  2007.   MaltParser:  A  
Language-Independent  System for Data-Driven De-
pendency  Parsing,  Natural  Language  Engineering 
Journal, 13, pp. 99-135.
[11] Joakim Nivre and Ryan McDonald, 2008. Integrat-
ing GraphBased and Transition-Based Dependency  
Parsers. In Proceedings of the 46th Annual Meeting 
of  the  Association  for  Computational  Linguistics: 
Human Language Technologies.
350
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 20?25,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
A Mobile Touchable Application for Online Topic Graph Extraction and
Exploration of Web Content
Gu?nter Neumann and Sven Schmeier
Language Technology Lab, DFKI GmbH
Stuhlsatzenhausweg 3, D-66123 Saarbru?cken
{neumann|schmeier}@dfki.de
Abstract
We present a mobile touchable application for
online topic graph extraction and exploration
of web content. The system has been imple-
mented for operation on an iPad. The topic
graph is constructed from N web snippets
which are determined by a standard search en-
gine. We consider the extraction of a topic
graph as a specific empirical collocation ex-
traction task where collocations are extracted
between chunks. Our measure of association
strength is based on the pointwise mutual in-
formation between chunk pairs which explic-
itly takes their distance into account. An ini-
tial user evaluation shows that this system is
especially helpful for finding new interesting
information on topics about which the user has
only a vague idea or even no idea at all.
1 Introduction
Today?s Web search is still dominated by a docu-
ment perspective: a user enters one or more key-
words that represent the information of interest and
receives a ranked list of documents. This technology
has been shown to be very successful when used on
an ordinary computer, because it very often delivers
concrete documents or web pages that contain the
information the user is interested in. The following
aspects are important in this context: 1) Users basi-
cally have to know what they are looking for. 2) The
documents serve as answers to user queries. 3) Each
document in the ranked list is considered indepen-
dently.
If the user only has a vague idea of the informa-
tion in question or just wants to explore the infor-
mation space, the current search engine paradigm
does not provide enough assistance for these kind
of searches. The user has to read through the docu-
ments and then eventually reformulate the query in
order to find new information. This can be a tedious
task especially on mobile devices. Seen in this con-
text, current search engines seem to be best suited
for ?one-shot search? and do not support content-
oriented interaction.
In order to overcome this restricted document per-
spective, and to provide a mobile device searches to
?find out about something?, we want to help users
with the web content exploration process in two
ways:
1. We consider a user query as a specification of
a topic that the user wants to know and learn
more about. Hence, the search result is basi-
cally a graphical structure of the topic and as-
sociated topics that are found.
2. The user can interactively explore this topic
graph using a simple and intuitive touchable
user interface in order to either learn more
about the content of a topic or to interactively
expand a topic with newly computed related
topics.
In the first step, the topic graph is computed on
the fly from the a set of web snippets that has been
collected by a standard search engine using the ini-
tial user query. Rather than considering each snip-
pet in isolation, all snippets are collected into one
document from which the topic graph is computed.
We consider each topic as an entity, and the edges
20
between topics are considered as a kind of (hidden)
relationship between the connected topics. The con-
tent of a topic are the set of snippets it has been ex-
tracted from, and the documents retrievable via the
snippets? web links.
A topic graph is then displayed on a mobile de-
vice (in our case an iPad) as a touch-sensitive graph.
By just touching on a node, the user can either in-
spect the content of a topic (i.e, the snippets or web
pages) or activate the expansion of the graph through
an on the fly computation of new related topics for
the selected node.
In a second step, we provide additional back-
ground knowledge on the topic which consists of ex-
plicit relationships that are generated from an online
Encyclopedia (in our case Wikipedia). The relevant
background relation graph is also represented as a
touchable graph in the same way as a topic graph.
The major difference is that the edges are actually
labeled with the specific relation that exists between
the nodes.
In this way the user can explore in an uniform way
both new information nuggets and validated back-
ground information nuggets interactively. Fig. 1
summarizes the main components and the informa-
tion flow.
Figure 1: Blueprint of the proposed system.
2 Touchable User Interface: Examples
The following screenshots show some results for the
search query ?Justin Bieber? running on the cur-
rent iPad demo?app. At the bottom of the iPad
screen, the user can select whether to perform text
exploration from the Web (via button labeled ?i?
GNSSMM?) or via Wikipedia (touching button ?i?
MILREX?). The Figures 2, 3, 4, 5 show results for
the ?i?GNSSMM? mode, and Fig. 6 for the ?i-
MILREX? mode. General settings of the iPad demo-
app can easily be changed. Current settings allow
e.g., language selection (so far, English and German
are supported) or selection of the maximum number
of snippets to be retrieved for each query. The other
parameters mainly affect the display structure of the
topic graph.
Figure 2: The topic graph computed from the snippets for
the query ?Justin Bieber?. The user can double touch on
a node to display the associated snippets and web pages.
Since a topic graph can be very large, not all nodes are
displayed. Nodes, which can be expanded are marked by
the number of hidden immediate nodes. A single touch
on such a node expands it, as shown in Fig. 3. A single
touch on a node that cannot be expanded adds its label to
the initial user query and triggers a new search with that
expanded query.
21
Figure 3: The topic graph from Fig. 2 has been expanded
by a single touch on the node labeled ?selena gomez?.
Double touching on that node triggers the display of as-
sociated web snippets (Fig. 4) and the web pages (Fig.
5).
3 Topic Graph Extraction
We consider the extraction of a topic graph as a spe-
cific empirical collocation extraction task. How-
ever, instead of extracting collations between words,
which is still the dominating approach in collocation
extraction research, e.g., (Baroni and Evert, 2008),
we are extracting collocations between chunks, i.e.,
word sequences. Furthermore, our measure of asso-
ciation strength takes into account the distance be-
tween chunks and combines it with the PMI (point-
wise mutual information) approach (Turney, 2001).
The core idea is to compute a set of chunk?
pair?distance elements for the N first web snip-
pets returned by a search engine for the topic Q,
and to compute the topic graph from these ele-
ments.1 In general for two chunks, a single chunk?
pair?distance element stores the distance between
1For the remainder of the paper N=1000. We are using Bing
(http://www.bing.com/) for Web search.
Figure 4: The snippets that are associated with the node
label ?selena gomez? of the topic graph from Fig. 3.In or-
der to go back to the topic graph, the user simply touches
the button labeled i-GNSSMM on the left upper corner of
the iPad screen.
the chunks by counting the number of chunks in?
between them. We distinguish elements which have
the same words in the same order, but have different
distances. For example, (Peter, Mary, 3) is different
from (Peter, Mary, 5) and (Mary, Peter, 3).
We begin by creating a document S from the
N -first web snippets so that each line of S con-
tains a complete snippet. Each textline of S is
then tagged with Part?of?Speech using the SVM-
Tagger (Gime?nez and Ma`rquez, 2004) and chun-
ked in the next step. The chunker recognizes two
types of word chains. Each chain consists of longest
matching sequences of words with the same PoS
class, namely noun chains or verb chains, where
an element of a noun chain belongs to one of
the extended noun tags2, and elements of a verb
2Concerning the English PoS tags, ?word/PoS? expressions
that match the following regular expression are considered as
extended noun tag: ?/(N(N|P))|/VB(N|G)|/IN|/DT?. The En-
22
Figure 5: The web page associated with the first snippet
of Fig. 4. A single touch on that snippet triggers a call
to the iPad browser in order to display the corresponding
web page. The left upper corner button labeled ?Snip-
pets? has to be touched in order to go back to the snippets
page.
chain only contains verb tags. We finally ap-
ply a kind of ?phrasal head test? on each iden-
tified chunk to guarantee that the right?most ele-
ment only belongs to a proper noun or verb tag.
For example, the chunk ?a/DT british/NNP for-
mula/NNP one/NN racing/VBG driver/NN from/IN
scotland/NNP? would be accepted as proper NP
chunk, where ?compelling/VBG power/NN of/IN?
is not.
Performing this sort of shallow chunking is based
on the assumptions: 1) noun groups can represent
the arguments of a relation, a verb group the relation
itself, and 2) web snippet chunking needs highly ro-
bust NL technologies. In general, chunking crucially
depends on the quality of the embedded PoS?tagger.
However, it is known that PoS?tagging performance
of even the best taggers decreases substantially when
glish Verbs are those whose PoS tag start with VB. We are us-
ing the tag sets from the Penn treebank (English) and the Negra
treebank (German).
Figure 6: If mode ?i?MILREX? is chosen then text ex-
ploration is performed based on relations computed from
the info?boxes extracted from Wikipedia. The central
node corresponds to the query. The outer nodes repre-
sent the arguments and the inner nodes the predicate of a
info?box relation. The center of the graph corresponds to
the search query.
applied on web pages (Giesbrecht and Evert, 2009).
Web snippets are even harder to process because
they are not necessary contiguous pieces of texts,
and usually are not syntactically well-formed para-
graphs due to some intentionally introduced breaks
(e.g., denoted by . . . betweens text fragments). On
the other hand, we want to benefit from PoS tag-
ging during chunk recognition in order to be able to
identify, on the fly, a shallow phrase structure in web
snippets with minimal efforts.
The chunk?pair?distance model is computed
from the list of chunks. This is done by traversing
the chunks from left to right. For each chunk ci, a
set is computed by considering all remaining chunks
and their distance to ci, i.e., (ci, ci+1, disti(i+1)),
(ci, ci+2, disti(i+2)), etc. We do this for each chunk
list computed for each web snippet. The distance
distij of two chunks ci and cj is computed directly
from the chunk list, i.e., we do not count the position
23
of ignored words lying between two chunks.
The motivation for using chunk?pair?distance
statistics is the assumption that the strength of hid-
den relationships between chunks can be covered by
means of their collocation degree and the frequency
of their relative positions in sentences extracted from
web snippets; cf. (Figueroa and Neumann, 2006)
who demonstrated the effectiveness of this hypothe-
sis for web?based question answering.
Finally, we compute the frequencies of each
chunk, each chunk pair, and each chunk pair dis-
tance. The set of all these frequencies establishes
the chunk?pair?distance model CPDM . It is used
for constructing the topic graph in the final step. For-
mally, a topic graph TG = (V,E,A) consists of a
set V of nodes, a set E of edges, and a set A of node
actions. Each node v ? V represents a chunk and
is labeled with the corresponding PoS?tagged word
group. Node actions are used to trigger additional
processing, e.g., displaying the snippets, expanding
the graph etc.
The nodes and edges are computed from the
chunk?pair?distance elements. Since, the number
of these elements is quite large (up to several
thousands), the elements are ranked according to
a weighting scheme which takes into account the
frequency information of the chunks and their collo-
cations. More precisely, the weight of a chunk?pair?
distance element cpd = (ci, cj , Dij), with Di,j =
{(freq1, dist1), (freq2, dist2), ..., (freqn, distn)},
is computed based on PMI as follows:
PMI(cpd) = log2((p(ci, cj)/(p(ci) ? p(cj)))
= log2(p(ci, cj))? log2(p(ci) ? p(cj))
where relative frequency is used for approximating
the probabilities p(ci) and p(cj). For log2(p(ci, cj))
we took the (unsigned) polynomials of the corre-
sponding Taylor series3 using (freqk, distk) in the
k-th Taylor polynomial and adding them up:
PMI(cpd) = (
n?
k=1
(xk)
k
k
)? log2(p(ci) ? p(cj))
, where xk =
freqk
?n
k=1 freqk
3In fact we used the polynomials of the Taylor series for
ln(1 + x). Note also that k is actually restricted by the number
of chunks in a snippet.
The visualized topic graph TG is then computed
from a subset CPD?M ? CPDM using the m high-
est ranked cpd for fixed ci. In other words, we re-
strict the complexity of a TG by restricting the num-
ber of edges connected to a node.
4 Wikipedia?s Infoboxes
In order to provide query specific background
knowledge we make use of Wikipedia?s infoboxes.
These infoboxes contain facts and important rela-
tionships related to articles. We also tested DB-
pedia as a background source (Bizer et al, 2009).
However, it turned out that currently it contains
too much and redundant information. For exam-
ple, the Wikipedia infobox for Justin Bieber contains
eleven basic relations whereas DBpedia has fifty re-
lations containing lots of redundancies. In our cur-
rent prototype, we followed a straightforward ap-
proach for extracting infobox relations: We down-
loaded a snapshot of the whole English Wikipedia
database (images excluded), extracted the infoboxes
for all articles if available and built a Lucene Index
running on our server. We ended up with 1.124.076
infoboxes representing more than 2 million differ-
ent searchable titles. The average access time is
about 0.5 seconds. Currently, we only support ex-
act matches between the user?s query and an infobox
title in order to avoid ambiguities. We plan to ex-
tend our user interface so that the user may choose
different options. Furthermore we need to find tech-
niques to cope with undesired or redundant informa-
tion (see above). This extension is not only needed
for partial matches but also when opening the sys-
tem to other knowledgesources like DBpedia, new-
sticker, stock information and more.
5 Evaluation
For an initial evaluation we had 20 testers: 7 came
from our lab and 13 from non?computer science re-
lated fields. 15 persons had never used an iPad be-
fore. After a brief introduction to our system (and
the iPad), the testers were asked to perform three
different searches (using Google, i?GNSSMM and
i?MILREX) by choosing the queries from a set of
ten themes. The queries covered definition ques-
tions like EEUU and NLF, questions about persons
like Justin Bieber, David Beckham, Pete Best, Clark
24
Kent, and Wendy Carlos , and general themes like
Brisbane, Balancity, and Adidas. The task was
not only to get answers on questions like ?Who is
. . . ? or ?What is . . . ? but also to acquire knowledge
about background facts, news, rumors (gossip) and
more interesting facts that come into mind during
the search. Half of the testers were asked to first
use Google and then our system in order to compare
the results and the usage on the mobile device. We
hoped to get feedback concerning the usability of
our approach compared to the well known internet
search paradigm. The second half of the participants
used only our system. Here our research focus was
to get information on user satisfaction of the search
results. After each task, both testers had to rate sev-
eral statements on a Likert scale and a general ques-
tionnaire had to be filled out after completing the
entire test. Table 1 and 2 show the overall result.
Table 1: Google
#Question v.good good avg. poor
results first sight 55% 40% 15% -
query answered 71% 29% - -
interesting facts 33% 33% 33% -
suprising facts 33% - - 66%
overall feeling 33% 50% 17% 4%
Table 2: i-GNSSMM
#Question v.good good avg. poor
results first sight 43% 38% 20% -
query answered 65% 20% 15% -
interesting facts 62% 24% 10% 4%
suprising facts 66% 15% 13% 6%
overall feeling 54% 28% 14% 4%
The results show that people in general prefer
the result representation and accuracy in the Google
style. Especially for the general themes the presen-
tation of web snippets is more convenient and more
easy to understand. However when it comes to in-
teresting and suprising facts users enjoyed exploring
the results using the topic graph. The overall feeling
was in favor of our system which might also be due
to the fact that it is new and somewhat more playful.
The replies to the final questions: How success-
ful were you from your point of view? What did you
like most/least? What could be improved? were in-
formative and contained positive feedback. Users
felt they had been successful using the system. They
liked the paradigm of the explorative search on the
iPad and preferred touching the graph instead of re-
formulating their queries. The presentation of back-
ground facts in i?MILREX was highly appreciated.
However some users complained that the topic graph
became confusing after expanding more than three
nodes. As a result, in future versions of our system,
we will automatically collapse nodes with higher
distances from the node in focus. Although all of our
test persons make use of standard search engines,
most of them can imagine to using our system at
least in combination with a search engine even on
their own personal computers.
6 Acknowledgments
The presented work was partially supported by
grants from the German Federal Ministry of Eco-
nomics and Technology (BMWi) to the DFKI The-
seus projects (FKZ: 01MQ07016) TechWatch?Ordo
and Alexandria4Media.
References
Marco Baroni and Stefan Evert. 2008. Statistical meth-
ods for corpus exploitation. In A. Lu?deling and
M. Kyto? (eds.), Corpus Linguistics. An International
Handbook, Mouton de Gruyter, Berlin.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, Soren
Auer, Christian Becker, Richard Cyganiak, Sebastian
Hellmann. 2009. DBpedia - A crystallization point for
the Web of Data. Web Semantics: Science, Services
and Agents on the World Wide Web 7 (3): 154165.
Alejandro Figueroa and Gu?nter Neumann. 2006. Lan-
guage Independent Answer Prediction from the Web.
In proceedings of the 5th FinTAL, Finland.
Eugenie Giesbrecht and Stefan Evert. 2009. Part-of-
speech tagging - a solved task? An evaluation of PoS
taggers for the Web as corpus. In proceedings of the
5th Web as Corpus Workshop, San Sebastian, Spain.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general PoS tagger generator based on Support Vector
Machines. In proceedings of LREC?04, Lisbon, Por-
tugal.
Peter Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In proceedings of the 12th
ECML, Freiburg, Germany.
25
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 150?153,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
DFKI KeyWE: Ranking keyphrases extracted from scientific articles
Kathrin Eichler
DFKI - Language Technology
Berlin, Germany
kathrin.eichler@dfki.de
G?unter Neumann
DFKI - Language Technology
Saarbr?ucken, Germany
neumann@dfki.de
Abstract
A central issue for making the content
of a scientific document quickly acces-
sible to a potential reader is the extrac-
tion of keyphrases, which capture the main
topic of the document. Keyphrases can
be extracted automatically by generating a
list of keyphrase candidates, ranking these
candidates, and selecting the top-ranked
candidates as keyphrases. We present the
KeyWE system, which uses an adapted
nominal group chunker for candidate ex-
traction and a supervised ranking algo-
rithm based on support vector machines
for ranking the extracted candidates. The
system was evaluated on data provided
for the SemEval 2010 Shared Task on
Keyphrase Extraction.
1 Introduction
Keyphrases capture the main topic of the docu-
ment in which they appear and can be useful for
making the content of a document quickly ac-
cessible to a potential reader. They can be pre-
sented to the reader directly, in order to provide
a short overview of the document, but can also
be processed further, e.g. for text summarization,
document clustering, question-answering or rela-
tion extraction. The task of extracting keyphrases
automatically can be performed by generating a
list of keyphrase candidates, ranking these can-
didates, and selecting the top-ranked candidates
as keyphrases. In the KeyWE system, candidates
are generated based on an adapted nominal group
chunker described in section 3 and ranked using
the SVM
rank
algorithm (Joachims, 2006), as de-
scribed in section 4. The used features are spec-
ified in section 5. In section 6, we present the
results achieved on the test data provided for the
SemEval 2010 Shared Task on Keyphrase Extrac-
tion
1
by selecting as keyphrases the top 5, 10, and
15 top-ranked candidates, respectively.
2 Related work
The task of keyphrase extraction came up in the
1990s and was first treated as a supervised learn-
ing problem in the GenEx system (Turney, 1999).
Since then, the task has evolved and various new
approaches have been proposed. The task is usu-
ally performed in two steps: 1. candidate ex-
traction (or generation) and 2. keyphrase selec-
tion. The most common approach towards can-
didate extraction is to generate all n-grams up to
a particular length and filter them using stopword
lists. Lately, more sophisticated candidate extrac-
tion methods, usually based on additional linguis-
tic information (e.g. POS tags), have been pro-
posed and shown to produce better results (e.g.
Hulth (2004)). Liu et al (2009) restrict their can-
didate list to verb, noun and adjective words. Kim
and Kan (2009) generate regular expression rules
to extract simplex nouns and nominal phrases. As
the majority of technical terms is in nominal group
positions
2
, we assume that the same holds true for
keyphrases and apply an adapted nominal group
chunker to extract keyphrase candidates.
The selection process is usually based on some
supervised learning algorithm, e.g. Naive Bayes
(Frank et al, 1999), genetic algorithms (Turney,
1999), neural networks (Wang et al, 2005) or de-
cision trees (Medelyan et al, 2009). Unsuper-
vised approaches have also been proposed, e.g. by
Mihalcea and Tarau (2004) and Liu et al (2009).
However, as for the shared task, annotated train-
ing data was available, we opted for an approach
based on supervised learning.
1
http://semeval2.fbk.eu/semeval2.php?location=tasks#T6
2
Experiments on 100 manually annotated scientific ab-
stracts from the biology domain showed that 94% of technical
terms are in nominal group position (Eichler et al, 2009).
150
3 Candidate extraction
Rather than extracting candidates from the full text
of the article, we restrict our search for candidates
to the first 2000 characters starting with the ab-
stract
3
. We also extract title and general terms
for use in the feature construction process. From
the reduced input text, we extract keyphrase candi-
dates based on the output of a nominal group chun-
ker.
This approach is inspired by findings from cog-
nitive linguistics. Talmy (2000) divides the con-
cepts expressed in language into two subsystems:
the grammatical subsystem and the lexical sub-
system. Concepts associated with the grammati-
cal subsystem provide a structuring function and
are expressed using so-called closed-class forms
(function words, such as conjunctions, determin-
ers, pronouns, and prepositions, but also suf-
fixes such as plural markers and tense markers).
Closed-class elements (CCEs) provide a scaffold-
ing, across which concepts associated with the lex-
ical subsystem (i.e. nouns, verbs, adjectives and
adverbs) can be draped (Evans and Pourcel, 2009).
Spurk (2006) developed a nominal group (NG)
chunker that makes use of this grammatical sub-
system. Using a finite list of CCEs and learned
word class models for identifying verbs and ad-
verbs, a small set of linguistically motivated ex-
traction patterns is stated to extract NGs. The rules
are based on the following four types of occur-
rences of NGs in English: 1. at the sentence be-
ginning, 2. within a determiner phrase, 3. follow-
ing a preposition and 4. following a verb. Not
being trained on a particular corpus, the chunker
works in a domain-independent way. In addition,
it scales well to large amounts of textual data.
In order to use the chunker for keyphrase extrac-
tion, we manually analysed annotated keyphrases
in scientific texts, and, based on the outcome of the
evaluation, made some adaptations to the chun-
ker, which take care of the fact that the boundaries
of a keyphrase do not always coincide with the
boundaries of a NG. In particular, we remove de-
terminers, split NGs on conjunctions, and process
text within parentheses separately from the main
text. An evaluation on the provided training data
showed that the adapted chunker extracts 80% of
the reader-annotated keyphrases found in the text.
3
This usually covers the introductory part of the article
and is assumed to contain most of the keyphrases. Partial
sentences at the end of this input are cut off.
4 Candidate ranking
The problem of ranking keyphrase candidates can
be formalized as follows: For a document d and
a collection of n keyword candidates C = c
1
...c
n
,
the goal is to compute a ranking r that orders
the candidates in C according to their degree of
keyphraseness in d.
The problem can be transformed into an ordinal
regression problem. In ordinal regression, the la-
bel assigned to an example indicates a rank (rather
than a nominal class, as in classification prob-
lems). The ranking algorithm we use is SVM
rank
,
developed by Joachims (2006). This algorithm
learns a linear ranking function and has shown to
outperform classification algorithms in keyphrase
extraction (Jiang et al, 2009).
The target (i.e. rank) value defines the order of
the examples (i.e. keyphrase candidates). Dur-
ing training, the target values are used to gener-
ate pairwise preference constraints. A preference
constraint is included for all pairs of examples in
the training file, for which the target value differs.
Two examples are considered for a pairwise pref-
erence constraint only if they appear within the
same document.
The model that is learned from the training data
is then used to make predictions on the test ex-
amples. For each line in the test data, the model
predicts a ranking score, from which the ranking
of the test examples can be recovered via sorting.
For ranking the candidates, they are transformed
into vectors based on the features described in sec-
tion 5.
During training, the set of candidates is made up
of the annotated reader and author keywords as
well as all NG chunks extracted from the text.
These candidates are mapped to three different
ranking values: All annotated keywords are given
a ranking value of 2; all extracted NG chunks
that were annotated somewhere else in the train-
ing data are given a ranking value of 1; all other
NG chunks are assigned a ranking value of 0.
Giving a special ranking value to chunks an-
notated somewhere else in the corpus is a way
of exploiting domain-specific information about
keyphrases. Even though not annotated in this par-
ticular document, a candidate that has been anno-
tated in some other document of the domain, is
more likely to be a keyphrase than a candidate that
has never been annotated before (cf. Frank et al
(1999)).
151
5 Features
We used two types of features: term-specific
features and document-specific features. Term-
specific features cover properties of the candidate
term itself (e.g. term length). Document-specific
features relate properties of the candidate to the
text, in which it appears (e.g. frequency of the
term in the document). Our term-specific features
concern the following properties:
? Term length refers to the length of a can-
didate in number of tokens. We express
this property in terms of five boolean fea-
tures: has1token, has2tokens, has3tokens,
has4tokens, has5orMoreTokens. The advan-
tage over expressing term length as a nu-
meric value is that using binary features, we
allow the algorithm to learn that candidates
of medium lengths are more likely to be
keyphrases than very short or very long can-
didates.
? The MSN score of a candidate refers to the
number of results retrieved when querying
the candidate string using the MSN search
engine
4
. The usefulness of MSN scores for
technical term extraction has been shown by
Eichler et al (2009). We normalize the MSN
scores based on the number of digits of the
score and store the normalized value in the
feature normalizedMsn. We also use a binary
feature isZeroMsn expressing whether query-
ing the candidate returns no results at all.
? Special characters can indicate whether a
candidate is (un)likely to be a keyphrase. We
use two features concerning special charac-
ters: containsDigit and containsHyphen.
? Wikipedia has shown to be a valuable source
for extracting keywords (Medelyan et al,
2009). We use a feature isWikipediaTerm,
expressing whether the term candidate corre-
sponds to an entry in Wikipedia.
In addition, we use the following document-
specific features:
? TFIDF, a commonly used feature introduced
by Salton and McGill (1983), relates the fre-
quency of a candidate in a document to its
frequency in other documents of the corpus.
4
http://de.msn.com/
? Term position relates the position of the first
appearance of the candidate in the document
to the length of the document. In addition,
our feature appearsInTitle covers the fact that
candidates appearing in the document title
are very likely to be keyphrases.
? Average token count measures the average
occurrence of the individual (lemmatized) to-
kens of the term in the document. Our
assumption is that candidates with a high
average token count are more likely to be
keyphrases.
? Point-wise mutual information (PMI,
Church and Hanks (1989)) is used to capture
the semantic relatedness of the candidate to
the topic of the document. A similar feature
is introduced by Turney (2003), who, in
a first pass, ranks the candidates based on
a base feature set, and then reranks them
by calculating the statistical association
between the given candidate and the top K
candidates from the first pass. To avoid the
two-pass method, rather than calculating
inter-candidate association, we calculate the
association of each candidate to the terms
specified in the General Terms section of
the paper. Like Turney, we calculate PMI
based on web search results (in our case,
using MSN). The feature maxPmi captures
the maximum PMI score achieved with the
lemmatized candidate and any of the general
terms.
6 Results and critical evaluation
Table 1 presents the results achieved by applying
the KeyWE system on the data set of scientific
articles provided by the organizers of the shared
task along with two sets of manually assigned
keyphrases for each article (reader-assigned and
author-assigned keyphrases). Our model was
trained on the trial and training data (144 articles)
and evaluated on the test data set (100 articles).
The evaluation is based on stemmed keyphrases,
where stemming is performed using the Porter
stemmer (Porter, 1980).
Since SVM
rank
learns a linear function, one can
analyze the individual features by studying the
learned weights. Roughly speaking, a high pos-
itive (negative) weight indicates that candidates
with this feature should be higher (lower) in the
152
Top Set P R F
5
reader 24.40% 10.13% 14.32%
combined 29.20% 9.96% 14.85%
10
reader 19.80% 16.45% 17.97%
combined 23.30% 15.89% 18.89%
15
reader 17.40% 21.68% 19.31%
combined 20.27% 20.74% 20.50%
Table 1: Results on the two keyword sets:
reader (reader-assigned keyphrases) and combined
(reader- and author-assigned keyphrases)
ranking. In our learned model, the four most im-
portant features (i.e. those with the highest ab-
solute weight) were containsDigit (-1.17), isZe-
roMsn (-1.12), normalizedMsn (-1.00), and avgTo-
kenCount (+0.97). This result confirms that web
frequencies can be used as a valuable source for
ranking keyphrases. It also validates our assump-
tion that a high average token count indicates a
good keyphrase candidate. The maxPMI feature
turned out to be of minor importance (-0.16). This
may be due to the fact that we used the terms from
the General Terms section of the paper to calculate
the association scores, which may be too general
for this purpose.
Acknowledgments
We thank Angela Schneider for her adaptations to
the chunker and helpful evaluations. The research
project DiLiA is co-funded by the European Re-
gional Development Fund (ERDF) in the context
of Investitionsbank Berlins ProFIT program under
grant number 10140159. We gratefully acknowl-
edge this support.
References
K. W. Church and P. Hanks. 1989. Word associa-
tion norms, mutual information and lexicography. In
Proceedings of the 27th Annual Conference of the
Association of Computational Linguistics.
K. Eichler, H. Hemsen, and G. Neumann. 2009. Un-
supervised and domain-independent extraction of
technical terms from scientifc articles in digital li-
braries. In Proceedings of the LWA Information Re-
trieval Workshop, TU Darmstadt, Germany.
V. Evans and S. Pourcel. 2009. New Directions in Cog-
nitive Linguistics. John Benjamins Publishing Com-
pany.
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin,
and C. G. Nevill-Manning. 1999. Domain-specific
keyphrase extraction. In Proceedings of the 16th
International Joint Conference on Artificial Intelli-
gence.
A. Hulth. 2004. Combining Machine Learning and
Natural Language Processing for Automatic Key-
word Extraction. Ph.D. thesis, Department of Com-
puter and Systems Sciences, Stockholm University.
X. Jiang, Y. Hu, and H. Li. 2009. A ranking ap-
proach to keyphrase extraction. In Proceedings of
the 32nd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval.
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of the ACM Conference on Knowl-
edge Discovery and Data Mining.
S. N. Kim and M. Y. Kan. 2009. Re-examining auto-
matic keyphrase extraction approaches in scientific
articles. In Proceedings of the ACL/IJCNLP Multi-
word Expressions Workshop.
F. Liu, D. Pennell, F. Liu, and Y. Liu. 2009. Unsu-
pervised approaches for automatic keyword extrac-
tion using meeting transcripts. In Proceedings of the
Conference of the NAACL, HLT.
O. Medelyan, E. Frank, and I.H. Witten. 2009.
Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the Interna-
tional Conference of Empirical Methods in Natural
Language Processing (EMNLP).
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of the EMNLP.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
G. Salton and M. J. McGill. 1983. Introduction to
modern information retrieval. McGraw-Hill.
C. Spurk. 2006. Ein minimal ?uberwachtes Verfahren
zur Erkennung generischer Eigennamen in freien
Texten. Diplomarbeit, Saarland University, Ger-
many.
L. Talmy. 2000. Towards a cognitive semantics. MIT
Press, Cambridge, MA.
P. D. Turney. 1999. Learning to extract keyphrases
from text. Technical report, National Research
Council, Institute for Information Technology.
P. D. Turney. 2003. Coherent keyphrase extraction via
web mining. In Proceedings of the Eighteenth Inter-
national Joint Conference on Artificial Intelligence.
J.-B. Wang, H. Peng, and J.-S. Hu. 2005. Automatic
keyphrases extraction from document using back-
propagation. In Proceedings of 2005 international
conference on Machine Learning and Cybernetics.
153
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 308?312,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
372:Comparing the Benefit of Different Dependency Parsers for Textu-
al Entailment Using Syntactic Constraints Only 
Alexander Volokh G?nter Neumann
alexander.volokh@dfki.de neumann@dfki.de
DFKI DFKI
Stuhlsatzenhausweg 3 Stuhlsatzenhausweg 3 
66123 Saarbr?cken, Germany 66123 Saarbr?cken, Germany 
Abstract
We compare several  state of the art dependency 
parsers  with  our  own  parser  based  on  a  linear 
classification  technique.  Our  primary  goal  is 
therefore to use syntactic information only, in or-
der to keep the comparison of the parsers as fair 
as possible. We demonstrate, that despite the in-
ferior result using the standard evaluation metrics 
for  parsers  like  UAS  or  LAS  on  standard  test 
data,  our  system  achieves  comparable  results 
when used in an application, such as the SemEv-
al-2 #12 evaluation exercise PETE. Our submis-
sion achieved the 4th position out of 19 participat-
ing systems. However, since it only uses a linear 
classifier  it  works 17-20 times faster  than other 
state of the parsers, as for instance MaltParser or 
Stanford Parser.
1 Introduction
Parsing is the process of mapping sentences to 
their syntactic representations. These representa-
tions  can be used by computers  for  performing 
many  interesting  natural  language  processing 
tasks, such as question answering or information 
extraction.  In recent years  a lot of  parsers have 
been developed for this purpose.
A very interesting and important  issue is  the 
comparison between a large number of such pars-
ing systems.  The most  widespread method is to 
evaluate the number of correctly recognized units 
according to a certain gold standard. For depend-
ency-based units unlabeled or labeled attachment 
scores (percentage of correctly classified depend-
ency relations, either with or without the depend-
ency relation type) are usually used (cf. Buchholz 
and Marsi, 2006).
However, parsing is very rarely a goal in itself. 
In most cases it is a necessary preprocessing step 
for  a certain application.  Therefore it  is  usually 
not the best option to decide which parser suits 
one's goals best by purely looking on its perform-
ance on some standard test data set.  It is rather 
more  sensible  to  analyse  whether  the  parser  is 
able  to  recognise  those  syntactic  units  or  rela-
tions, which are most relevant for one's applica-
tion.
The  shared  task  #12  PETE in  the  SemEval-
2010  Evaluation  Exercises  on  Semantic  Evalu-
ation (Yuret, Han and Turgut, 2010) involved re-
cognizing  textual  entailments  (RTE).  RTE  is  a 
binary  classification  task,  whose  goal  is  to  de-
termine, whether for a pair of texts T and H the 
meaning  of  H is  contained  in  T  (Dagan et  al., 
2006). This task can be very complex depending 
on the properties of these texts. However, for the 
data, released by the organisers of PETE, only the 
syntactic information should be sufficient to reli-
ably perform this task. Thus it offers an ideal set-
ting for  evaluating the performance of different 
parsers.
To our mind evaluation of parsers via RTE is a 
very good additional possibility, besides the usual 
evaluation metrics, since in most cases the main 
thing in real-word applications is to recognize the 
primary units, such as the subject, the predicate, 
308
the objects, as well as their modifiers, rather than 
the other subordinate relations.
We have been developing our own a multilin-
gual dependency parser (called MDParser), which 
is  based  on  linear  classification1.  Whereas  the 
system is quite fast because the classification is 
linear,  it  usually achieves inferior  results  (using 
UAS/LAS evaluation metrics)  in  comparison  to 
other parsers, which for example use kernel-based 
classification  or  other  more  sophisticated  meth-
ods. 
Therefore the PETE shared task was a perfect 
opportunity for us to investigate whether the in-
ferior result of our parser is also relevant for its 
applicability  in  a  concrete  task.  We have  com-
pared our system with three state of the art pars-
ers made available on the PETE web page: Malt-
Parser,  MiniPar  and  StandfordParser.  We  have 
achieved the total score of 0.6545 (200/301 cor-
rect  answers  on  the  test  data),  which  is  the  4th 
rank out of 19 submissions. 
2 MDParser
MDParser stands for multilingual dependency 
parser and is a data-driven system, which can be 
used  to  parse  text  of  an  arbitrary  language  for 
which training data is available. It is a transition-
based parser and uses a deterministic version of 
the Covington's algorithm (Covington, 2000).
The models of the system are based on various 
features, which are extracted from the words of 
the  sentence,  including word forms  and part  of 
speech tags. No additional morphological features 
or lemmas are currently used in our models, even 
if they are available in the training data, since the 
system is especially designed for processing plain 
text in different languages, and such components 
are not available for every language.
The  preprocessing  components  of  MDParser 
include a.)  a  sentence splitter2,  since  the  parser 
constructs a dependency structure for  individual 
sentences,  b.)  a  tokenizer,  in order to recognise 
the elements between which the dependency rela-
tions will be built3, and c.) a part of speech tagger, 
1http://www.dfki.de/~avolokh/mdparser.pdf
2http://morphadorner.northwestern.edu/morphadorner/sen-
tencesplitter/
3http://morphadorner.northwestern.edu/morphadorner/word-
tokenizer/
in  order  to  determine  the  part  of  speech  tags, 
which are intensively used in the feature models4.
MDParser is an especially fast system because 
it  uses  a  linear  classification  algorithm  L1R-
LR(L1  regularised  logistic  regression)  from the 
machine learning package LibLinear (Lin et al, 
2008) for constructing its dependency structures 
and  therefore  it  is  particularly  suitable  for  pro-
cessing very large amounts of data. Thus it can be 
used as a part of larger applications in which de-
pendency structures are desired. 
Additionally,  significant efforts were made in 
order to make the gap between our linear classi-
fication and more advanced methods as small as 
possible,  e.g.  by  introducing  features  conjunc-
tions, which are complex features built out of or-
dinary features, as well as methods for automatic-
ally measuring feature usefulness in order to auto-
mate and optimise feature engineering.
3 Triple Representation
Every parser  usually produces  its  own some-
how special  representation  of  the  sentence.  We 
have created such a representation, which we will 
call  triple representation and have implemented 
an  automatic  transformation  of  the  results  of 
Minipar,  MaltParser,  Stanford  Parser  and  of 
course MDParser into it (cf. Wang and Neumann, 
2007).
The triple representation of a sentence is a set 
of  triple  elements  of  the  form  <parent,  label, 
child>, where child and parent elements stand for 
the head and the modifier words and their parts of 
speech, and label stands for the relation between 
them.  E.g.  <have:VBZ,  SBJ,  Somebody:NN>. 
This information is extractable from the results of 
any dependency parser.
4 Predicting Entailment
Whereas the first part of the PETE shared task 
was to construct syntactic representations for all 
T-H-pairs,  the  second important  subtask was  to 
determine whether the structure of H is entailed 
by the structure of T. The PETE guide5 states that 
the following three phenomena were particularly 
important to recognise the entailment relation:
4The part of speech tagger was trained with the SVMTool 
http://www.lsi.upc.edu/~nlp/SVMTool/
5http://pete.yuret.com/guide
309
1. subject-verb  dependency  (John  kissed 
Mary. ? John kissed somebody.)
2. verb-object  dependency  (John  kissed 
Mary ? Mary was kissed.)
3. noun-modifier  dependency (The big red  
boat sank. ? The boat was big.)
Thus we have manually formulated the follow-
ing generic decision rule for determining the en-
tailment relation between T and H:
1. identify  the  root  triple  of  H  <null:null, 
ROOT, x>
2. check whether the subject and the com-
plements(objects, verb complements) of the root 
word in H are present in T. Formally: all triples of 
H of the form <x, z, y>  should be contained in 
T(x in 1 and 2 is thus the same word).
3. if 2 returns false we have to check wheth-
er H is a structure in passive and T contains the 
same content in active voice(a) or the other way 
around(b). Formally:
3a. For triples of the form <be:VBZ, SBJ, s> 
and <be:VBZ, VC, t> in H check whether there is 
a  triple of the form <s, NMOD, t> in T.
3b. For triples of the form <u, OBJ,v> in H 
check whether there is  a triple  of  the  form <v, 
NMOD, u> in T.
It turned out that few additional modifications 
to  the  base  rule  were  necessary  for  some  sen-
tences: 1.) For sentences containing conjunctions: 
If we were looking for a subject of a certain verb 
and could not find it, we investigated whether this 
verb is connected via a conjunction with another 
one. If true, we compared the subject in H with 
the subject of the conjunct verb. 2.) For sentences 
containing special verbs, e.g. modal verbs may or 
can or auxiliary verbs like to have it turned out to 
be important to go one level deeper into the de-
pendency structure  and to  check whether  all  of 
their  arguments  in  H are  also present  in T,  the 
same way as in 3.
A triple <x,z,y> is contained in a set of triples 
S, when there exists at least one of the triples in S 
<u,w,v>, such that x=u, w=z and y=v. This is also 
true  if  the  words  somebody,  someone or  some-
thing are  used  on  one  of  the  equation  sides. 
Moreover, we use an English lemmatizer for all 
word forms, so when checking the equality of two 
words we actually check their lemmas, e.g., is and 
are are also treated equally.
5 Results
We have parsed the 66 pairs  of  the develop-
ment  data  with  4  parsers:6 MiniPar,  Stanford 
Parser, MaltParser and MDParser. After applying 
our rule we have achieved the following result:
Accuracy Parsing Speed
MiniPar 45/66 1233 ms
Stanford Parser 50/66 32889 ms
MaltParser 51/66 37149 ms
MDParser 50/66 1785 ms
We used  the  latest  versions  of  MiniPar7 and 
Stanford Parser8. We did not re-test the perform-
ance of these parsers on standard data, since we 
were sure that these versions provide the best pos-
sible results of these systems. 
As far as the MaltParser is concerned we had to 
train our own model. We have trained the model 
with the following LibSVM options: ?-s_0_-t_1_-
d_2_-g_0.18_-c_0.4_-r_0.4_-e_1.0?.  We  were 
able  to  achieve  a  result  of  83.86%  LAS  and 
87.25% UAS on the standard CoNLL English test 
data,  a  result  which is  only slightly worse  than 
those reported in the literature, where the options 
are probably better tuned for the data. The train-
ing  data  used  for  training  was  the  same  as  for 
MDParser. 
The application of our rule for MDParser and 
MaltParser  was fully automated,  since both use 
the  same  training  data  and  thus  work  over  the 
same tag sets. For MiniPar and Stanford Parser, 
which  construct  different  dependency structures 
with  different  relation  types,  we  had  to  go 
through all pairs manually in order to investigate 
how the rule should be adopted to their tag sets 
and structures. However, since we have already 
counted the  number  of  structures,  for  which an 
adoptation of the rule would work during this in-
vestigation, we did not implement it in the end. 
Therefore  these  results  might  be  taken  with  a 
pinch of salt, despite the fact that we have tried to 
stay as fair as possible and treated some pairs as 
correct, even if a quite large modification of the 
6For all results reported in this section a desktop PC with 
an Intel Core 2 Duo E8400 3.00 GHz processor and 4.00 GB 
RAM was used.
7http://webdocs.cs.ualberta.ca/~lindek/minipar  
8http://nlp.stanford.edu/downloads/lex-parser.shtml  
310
rule was necessary in order to adopt it to the dif-
ferent tag set and/or dependency structure.
For test  data we were only able to apply our 
rule for the results of MDParser and MaltParser, 
since for such a large number of pairs (301) only 
the fully automated version of our mechanism for 
predicting entailment could be applied. For Mini-
Par and Stanford Parser it was too tedious to ap-
ply it to them manually or to develop a mapping 
between  their  dependency  annotations  and  the 
ones used in MDParser or MaltParser.  Here are 
the official results of our submissions for Malt-
Parser and MDParser:
Accuracy Parsing Speed
MDParser 197/301 8704 ms
MaltParser 196/301 147938 ms
6 Discussion
We were able to show that our parser based on 
a linear classification technique is especially fast 
compared to other state of the art parsers. Further-
more, despite the fact, that it achieves an inferior 
result,  when using usual  evaluation metrics like 
UAS or LAS, it is absolutely suitable for being 
used in applications, since the most important de-
pendency relations are recognized correctly even 
with  a  less  sophisticated  linear  classifier  as  the 
one being used in MDParser.
As  far  as  the  overall  score  is  concerned  we 
think a much better result  could be achieved, if 
we would put more effort into our mechanism for 
recognizing  entailment  using  triple  representa-
tions. However, many of the pairs required more 
than only syntactical information. In many cases 
one would need to extend one's mechanism with 
logic,  semantics  and  the  possibility  to  resolve 
anaphoric  expressions,  which  to  our  mind  goes 
beyond the idea behind the PETE task. Since we 
were  primarly  interested  in  the  comparison 
between MaltParser and MDParser, we have not 
tried to include solutions for such cases. Here are 
some of the pairs we think require more than only 
syntax:
(4069  entailment="YES")  <t>Mr.  Sherwood 
speculated  that  the  leeway  that  Sea  Containers 
has means that Temple would have to "substan-
tially  increase  their  bid  if  they're  going  to  top 
us."</t>
<h>Someone  would  have  to  increase  the 
bid.</h>
(7003 entailment="YES") <t>After all, if  you 
were going to set up a workshop you had to have 
the proper equipment and that was that.</t>
<h>Somebody  had  to  have  the  equip-
ment.</h>
(3132.N entailment="YES")  <t>The first  was 
that America had become -- or was in danger of 
becoming  --  a  second-rate  military  power.</t>
<h>America was in danger.</h>
? 4069,  7003 and 3132.N are  examples  for 
sentences were beyond syntactical information lo-
gic  is  required.  Moreover  we are  surprised that 
sentences of the form ?if A, then B? entail B and 
a sentence of the form ?A or  B? entails  B, since 
?or? in this case means uncertainty.
(4071.N  entailment="NO")  <t>Interpublic 
Group said its television programming operations 
-- which it expanded earlier this year -- agreed to 
supply  more  than  4,000  hours  of  original  pro-
gramming across Europe in 1990.</t>
<h>Interpublic Group expanded.</h>
(6034  entailment="YES")  <t>"Oh,"  said  the 
woman, "I've seen that picture already."</t>
<h>The woman has seen something.</h>
? In 4071.N one has to resolve ?it? in ?it ex-
panded? to Interpublic Group. In 6034 one has to 
resolve ?I? in ?I've seen? to ?the woman?. Both 
cases are examples for the necessity of anaphora 
resolution, which goes beyond syntax as well.
(2055) <t>The Big Board also added computer 
capacity  to  handle  huge  surges  in  trading 
volume.</t>
<h>Surges were handled.</h>
? If something is added in order to do some-
thing it does not entail that this something is thus 
automatically done. Anyways pure syntax is not 
sufficient,  since  the  entailment  depends  on  the 
verb used in such a construction.
(3151.N) <t>Most of them are Democrats and 
nearly all consider themselves, and are viewed as, 
liberals.</t>
<h>Some consider themselves liberal.</h>
? One has to know that the semantics of ?con-
sider themselves as liberals? and ?consider them-
selves liberal? is the same.
Acknowledgements
311
The work presented here was partially suppor-
ted by a research grant from the German Federal 
Ministry of Economics and Technology (BMWi) 
to  the  DFKI  project  Theseus  Ordo  TechWatch 
(FKZ: 01MQ07016). We thank Joakim Nivre and 
Johan Hall for their support and tips when train-
ing models with MaltParser. Additionally, we are 
very grateful to Sven Schmeier for providing us 
with a trained part of speech tagger for English 
and for his support when using this tool. 
References 
Michael  A.  Covington,  2000.  A  Fundamental  Al-
gorithm for  Dependency  Parsing.  In  Proceedings  of 
the 39th Annual ACM Southeast Conference. 
Dan Klein and Christopher D. Manning, 2003. Accur-
ate  Unlexicalized  Parsing.  Proceedings  of  the  41st 
Meeting  of  the  Association  for  Computational  Lin-
guistics, pp. 423-430. 
Lin D, 2003.  Dependency-Based Evaluation Of Mini-
par. In  Building and using Parsed Corpora Edited by: 
Abeill? A. Dordrecht: Kluwer; 2003.
Sabine  Buchholz  and  Erwin  Marsi.  2006.  CoNLL-X 
shared  task  on  multilingual  dependency  parsing.  In 
Proceedings of CONLL-X, pages 149?164, New York.
Ido  Dagan,  Oren  Glickman  and  Bernardo  Magnini. 
The  PASCAL Recognising  Textual  Entailment  Chal-
lenge.  In  Quinonero-Candela, J.;  Dagan, I.;  Magnini, 
B.;  d'Alche-Buc,  F.  (Eds.),  Machine  Learning  Chal-
lenges. Lecture Notes in Computer Science, Vol. 3944, 
pp. 177-190, Springer, 2006.
Nivre, J., J. Hall and J. Nilsson, 2006. MaltParser: A 
Data-Driven Parser-Generator for Dependency Pars-
ing.  In  Proceedings  of  the  fifth  international  confer-
ence  on  Language  Resources  and  Evaluation 
(LREC2006),  May  24-26,  2006,  Genoa,  Italy,  pp. 
2216-2219. 
Rui  Wang and G?nter  Neumann,  2007.  Recognizing 
Textual  Entailment  Using  a  Subsequence  Kernel  
Method. In Proceedings of AAAI 2007. 
R.  Fan,  K.  Chang,  C.  Hsieh,  X.  Wang,  and C.  Lin, 
2008. LIBLINEAR: A Library for Large Linear Classi-
fication.  Journal of Machine Learning Research, 9(4): 
1871?1874.
Deniz Yuret,  Ayd?n Han and Zehra Turgut, 2010. Se-
mEval-2010 Task 12: Parser Evaluation using Textual  
Entailments. In  Proceedings  of  the  SemEval-2010 
Evaluation Exercises on Semantic Evaluation. 
The  Stanford  Parser:  A  Statistical  Parser.  
http://nlp.stanford.edu/downloads/lex-parser.shtml
Maltparser. http://maltparser.org/
Minipar. http://webdocs.cs.ualberta.ca/~lindek/mini-
par.htm
MDParser:  Multilingual  Dependency  Parser. 
http://mdparser.sb.dfki.de/
312
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 213?217
Manchester, August 2008
A Puristic Approach for Joint Dependency Parsing and Semantic 
Role Labeling 
Alexander Volokh 
LT-lab, DFKI 
66123 Saarbr?cken, Germany 
Alexander.Volokh@dfki.de 
G?nter Neumann 
LT-lab, DFKI 
66123 Saarbr?cken, Germany 
neumann@dfki.de 
 
Abstract 
We present a puristic approach for com-
bining dependency parsing and semantic 
role labeling. In a first step, a data-driven 
strict incremental deterministic parser is 
used to compute a single syntactic de-
pendency structure using a MEM trained 
on the syntactic part of the CoNLL 2008 
training corpus. In a second step, a cas-
cade of MEMs is used to identify predi-
cates, and, for each found predicate, to 
identify its arguments and their types. All 
the MEMs used here are trained only 
with labeled data from the CoNLL 2008 
corpus. We participated in the closed 
challenge, and obtained a labeled macro 
F1 for WSJ+Brown of 19.93 (20.13 on 
WSJ only, 18.14 on Brown). For the syn-
tactic dependencies we got similar bad 
results (WSJ+Brown=16.25, WSJ= 16.22, 
Brown=16.47), as well as for the seman-
tic dependencies (WSJ+Brown=22.36, 
WSJ=22.86, Brown=17.94). The current 
results of the experiments suggest that 
our risky puristic approach of following a 
strict incremental parsing approach to-
gether with the closed data-driven per-
spective of a joined syntactic and seman-
tic labeling was actually too optimistic 
and eventually too puristic. 
The CoNLL 2008 shared task on joint parsing of 
syntactic and semantic dependencies (cf. Sur-
deanu, 2008) offered to us an opportunity to ini-
tiate, implement and test new ideas on large-
scale data-driven incremental dependency pars-
ing. The topic and papers of the ACL-2004 
workshop ?Incremental Parsing: Bringing Engi-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
neering and Cognition Together? (accessible at 
http://aclweb.org/anthology-new/W/W04/#0300) 
present a good recent overview into the field of 
incremental processing from both an engineering 
and cognitive point of view. 
Our particular interest is the exploration and 
development of strict incremental deterministic 
strategies as a means for fast data-driven depend-
ency parsing of large-scale online natural lan-
guage processing. By strict incremental process-
ing we mean, that the parser receives a stream of 
words w1 to wn word by word in left to right or-
der, and that the parser only has information 
about the current word wi, and the previous 
words w1 to wi-1.1 By deterministic processing we 
mean that the parser has to decide immediately 
and uniquely whether and how to integrate the 
newly observed word wi with the already con-
structed (partial) dependency structure without 
the possibility of revising its decision at later 
stages. The strategy is data-driven in the sense 
that the parsing decisions are made on basis of a 
statistical language model, which is trained on 
the syntactic part of the CoNLL 2008 training 
corpus. The whole parsing strategy is based on 
Nivre (2007), but modifies it in several ways, see 
sec. 2 for details. 
Note that there are other approaches of incre-
mental deterministic dependency parsing that 
assume that the complete input string of a sen-
tence is already given before parsing starts and 
that this additional right contextual information 
is also used as a feature source for language 
modeling, e.g., Nivre (2007). 
In light of the CoNLL 2008 shared task, this 
actually means that, e.g., part-of-speech tagging 
and lemmatization has already been performed 
                                                 
1
 Note that in a truly strict incremental processing 
regime the input to the NLP system is actually a 
stream of signals where even the sentence segmenta-
tion is not known in advance. Since in our current 
system, the parser receives a sentence as given input, 
we are less strict as we could be. 
213
for the complete sentence before incremental 
parsing starts, so that this richer source of infor-
mation is available for defining the feature space. 
Since, important word-based information espe-
cially for a dependency analysis is already 
known for the whole sentence before parsing 
starts, and actually heavily used during parsing, 
one might wonder, what the benefit of such a 
weak incremental parsing approach is compared 
to a non-incremental approach. Since, we 
thought that such an incremental processing per-
spective is a bit too wide (especially when con-
sidering the rich input of the CoNLL 2008 shared 
task), we wanted to explore a strict incremental 
strategy. 
Semantic role labeling is considered as a post-
process that is applied on the output of the syn-
tactic parser. Following Hacioglu (2004), we 
consider the labeling of semantic roles as a clas-
sification problem of dependency relations into 
one of several semantic roles. However, instead 
of post-processing a dependency tree firstly into 
a sequence of relations, as done by Hacioglu 
(2004), we apply a cascade of statistical models 
on the unmodified dependency tree in order to 
identify predicates, and, for each found predicate, 
to identify its arguments and their types. All the 
language models used here are trained only with 
labeled data from the CoNLL 2008 corpus; cf. 
sec. 3 for more details. 
Both, the syntactic parser and the semantic 
classifier are language independent in the sense 
that only information contained in the given 
training corpus is used (e.g., PoS tags, depend-
ency labels, information about direction etc.), but 
no language specific features, e.g., no PropBank 
frames nor any other external language and 
knowledge specific sources. 
The complete system has been designed and 
implemented from scratch after the announce-
ment of the CoNLL 2008 shared task. The main 
goal of our participation was therefore actually 
on being able to create some initial software im-
plementation and baseline experimentations as a 
starting point for further research in the area of 
data-driven incremental deterministic parsing. 
In the rest of this brief report, we will describe 
some more details of the syntactic and semantic 
component in the next two sections, followed by 
a description and discussion of the achieved re-
sults. 
1 Syntactic Parsing 
Our syntactic dependency parser is a variant of 
the incremental non-projective dependency 
parser described in Nivre (2007). Nivres? parser 
is incremental in the sense, that although the 
complete list of words of a sentence is known, 
construction of the dependency tree is performed 
strictly from left to right. It uses Treebank-
induced classifiers to deterministically predict 
the actions of the parser. The classifiers are 
trained using support vector machines (SVM). A 
further interesting property of the parser is its 
capability to derive (a subset of) non-projective 
structures directly. The core idea here is to ex-
ploit a function permissible(i, j, d) that returns 
true if and only if the dependency links i ? j and 
j ? i have a degree less than or equal to d given 
the dependency graph built so far. A degree d=0 
gives strictly projective parsing, while setting 
d=? gives unrestricted non-projective parsing; cf. 
Nivre (2007) for more details. The goal of this 
function is to restrict the call of a function link(i, 
j) which is a nondeterministic operation that adds 
the arc i ? j, the arc j ? i, or does nothing at all. 
Thus the smaller the value of d is the fewer links 
can be drawn. 
The function link(i, j) is directed by a trained 
SVM classifier that takes as input the feature rep-
resentation of the dependency tree built so far 
and the (complete) input x = w1, ?, wn and out-
puts a decision for choosing exactly one of the 
three possible operations.  
We have modified Nivres algorithm as follows: 
1. Instead of using classifiers learned by 
SVM, we are using classifiers based on 
Maximum Entropy Models (MEMs), cf. 
(Manning and Sch?tze, 1999).2 
2. Instead of using the complete input x, we 
only use the prefix from w1 up to the cur-
rent word wi. In this way, we are able to 
model a stricter incremental processing 
regime. 
3. We are using a subset of feature set de-
scribed in Nivre (2007). 3  In particular, 
we had to discard all features from 
Nivre?s set that refer to a word right to 
the current word in order to retain our 
                                                 
2
 We are using the opennlp.maxent package available 
via http://maxent.sourceforge.net/. 
3
 We mean here all features that are explicitly de-
scribed in Nivre (2007). He also mentions the use of 
some additional language specific features, but they 
are not further described, and, hence not known to us. 
214
strict incremental behavior. Additionally, 
we added the following features: 
a. Has j more children in the current 
dependency graph compared with 
the average number of children of 
element of same POS. 
b. Analogously for node i 
c. Distance between i and j 
Although some results ? for example Wang et 
al. (2006) ? suggest that SVMs are actually more 
suitable for deterministic parsing strategies than 
MEMs, we used MEMs instead of SVM basi-
cally for practical reasons: 1) we already had 
hands-on experience with MEMs, 2) training 
time was much faster than SVM, and 3) the theo-
retical basis of MEMs should give us enough 
flexibility for testing with different sets of fea-
tures. 
Initial experiments applied on the same cor-
pora as used by Nivre (2007), soon showed that 
our initial prototype is certainly not competitive 
in its current form. For example, our best result 
on the TIGER Treebank of German (Brants et al, 
2002) is 53.6% (labeled accuracy), where Nivre 
reports 85.90%; cf. Volokh (2008) and sec. 4 for 
more details 
Anyway, we decided to use it as a basis for the 
CoNLL 2008 shared task and to combine it with 
a component for semantic role labeling at least to 
get some indication of ?what went wrong?. 
2 Semantic Role Labeling 
On the one hand, it is clear that we should expect 
that our current version of the strict incremental 
deterministic parsing regime still returns too er-
roneous dependency analysis. On the other hand, 
we decided to apply semantic role labeling on the 
parser?s output. Hence, the focus was set towards 
a robust strictly data-driven approach. 
Semantic role labeling is modeled as a se-
quence of classifiers that follow the structure of 
predicates, i.e., firstly candidate predicates are 
identified and then the arguments are looked up. 
Predicate and argument identification both 
proceed in two steps: first determine whether a 
word can be a predicate or argument (or not), and 
then, each found predicate (argument) is typed. 
More precisely, semantic role labeling receives 
the output of the syntactic parser and performs 
the following steps in that order: 
1. Classify each word as being a predicate 
or not using a MEM-based classifier. 
2. Assign to each predicate its reading. Cur-
rently, this is done on basis of the fre-
quency readings as determined from the 
corpus (for unknown words, we simply 
assign the reading .01 to the lemma if the 
whole word was classified as a predicate). 
3. For each predicate identified in a sen-
tence, classify each word as argument for 
this predicate or not using a MEM-based 
classifier. 
4. For each argument identified for each 
predicate, assign its semantic role using a 
MEM-based classifier. 
For step 1 the following features are used for 
word wi: 1) word form, 2) word lemma, 3) POS, 
4) dependency type, 5) number of dependent 
elements in subtree of wi, 6) POS of parent, 7) 
dependency type of parent, 8) children or parent 
of word belong to prepositions, and 9) parent is 
predicate. 
For step 3 the same features are used as in step 
1, but 5) (for arguments the number of children is 
not important) and two additional features are 
used: 10) left/right of predicate (arguments are 
often to the right of its predicate), and 11) dis-
tance to predicate (arguments are not far from the 
predicate). Finally, for step 4 the same features 
are used as in step 1, but 5) and 9). 
3 Experiments 
As mentioned above, we started the develop-
ment of the system from scratch with a very 
small team (actually only one programmer). 
Therefore we wanted to focus on certain aspects, 
totally abandoning our claims for achieving de-
cent results for the others. One of our major 
goals was the construction of correct syntactic 
trees and the recognition of the predicate-
argument structure - a subtask which mainly cor-
responds to the unlabeled accuracy. For that rea-
son we reduced the scale of our experiments 
concerning such steps as dependency relation 
labeling, determining the correct reading for the 
predicates or the proper type of the arguments. 
Unfortunately only the labeled accuracy was 
evaluated at this year?s task, which was very 
frustrating in the end. 
3.1 Syntactic Dependencies 
For testing the strict incremental dependency 
parser we used the CoNLL 2008 shared task 
training and development set. Our best syntactic 
score that we could achieve on the development 
data was merely unlabeled attachment score 
(UAL) of 45.31%. However, as mentioned in sec. 
2, we used a set of features proposed by Nivre, 
215
which contains 5 features relying on the depend-
ency types. Since we couldn?t develop a good 
working module for this part of the task due to 
the lack of time, we couldn?t exploit these fea-
tures.  
Note that for this experiment and all others re-
ported below, we used the default settings of the 
opennlp MEM trainer. In particular this means 
that 100 iterations were used in all training runs 
and that for all experiments no tuning of parame-
ters and smoothing was done, basically because 
we had no time left to exploit it in a sensible way. 
These parts will surely be revised and improved 
in the future. 
3.2 Semantic Dependencies 
As we describe in the sec. 3 our semantic module 
consists of 4 steps. For the first step we achieve 
the F-score of 76.9%. Whereas the verb predi-
cates are recognized very well (average score for 
every verb category is almost 90%), we do badly 
with the noun categories. Since our semantic 
module depends on the input produced by the 
syntactic parser, and is influenced by its errors, 
we also did a test assuming a 100% correct parse. 
In this scenario we could achieve the F-score of 
79.4%. 
We have completely neglected the second step 
of the semantic task. We didn?t even try to do the 
feature engineering and to train a model for this 
assignment, basically because of time con-
straints. Neither did we try to include some in-
formation about the predicate-argument structure 
in order to do better on this part of the task. The 
simple assignment of the statistically most fre-
quent reading for each predicate reduced the ac-
curacy from 76.9% down to 69.3%. In case of 
perfect syntactic parse the result went down from 
79.4% to 71.5%. 
Unfortunately the evaluation software doesn?t 
provide the differentiation between the unlabeled 
and labeled argument recognition, which corre-
sponds to our third and fourth steps respectively. 
Whereas we put some effort on identifying the 
arguments, we didn?t focus on their classifica-
tion. Therefore the overall best labeled attach-
ment score for our system is 29.38%, whereas 
the unlabeled score is 50.74%. Assuming the 
perfect parse the labeled score is 32.67% and the 
unlabeled score is 66.73%. In our further work 
we will try to reduce this great deviation between 
both results. 
3.3 Runtime performance 
One of the main strong sides of the strict incre-
mental approach is its runtime performance. 
Since we are restricted in our feature selection 
to the already seen space to the left of the current 
word, both the training and the application of our 
strategy are done fast.  
The training of our MEMs for the syntactic 
part requires 62 minutes. The training of the 
models for our semantic components needs 31 
minutes. The test run of our system for the test 
data from the Brown corpus (425 sentences with 
7207 tokens) lasted 1 minute and 18 seconds. 
The application on the WSJ test data (2399 sen-
tences with 57676 tokens) took 20 minutes and 
42 seconds. The experiments have been per-
formed on a computer with one Intel Pentium 
1,86 Ghz processor and 1GB memory. 
4 Results and Discussion 
The results of running our current version on the 
CoNLL 2008 shared task test data were actually 
a knockdown blow. We participated in the closed 
challenge, and obtained for the complete problem 
a labeled macro F1 for WSJ+Brown of 19.93 
(20.13 on WSJ only, 18.14 on Brown). For the 
syntactic dependencies we got similar bad results 
(WSJ+Brown = 16.25, WSJ = 16.22, Brown = 
16.47), as well as for the semantic dependencies 
(WSJ+Brown = 22.36, WSJ = 22.86, Brown = 
17.94).  
We see at least the following two reasons for 
this disastrous result: On the one hand we fo-
cused on the construction of correct syntactic 
trees and the recognition of the predicate-
argument structure which were only parts of the 
task. On the other hand we stuck to our strict in-
cremental approach, which greatly restricted the 
scope of development of our system. 
Whereas the labeling part, which was so far 
considerably neglected, will surely be improved 
in the future, the strict incremental strategy in its 
current form will probably have to be revised. 
4.1 Post-evaluation experiments 
We have already started beginning the im-
provement of our parsing system, and we briefly 
discuss our current findings. On the technical 
level we already found a software bug that at 
least partially might explain the unexpected high 
difference in performance between the results 
obtained for the development set and the test set. 
Correcting this error now yields an UAL of 
53.45% and an LAL of 26.95% on the syntactic 
216
part of the Brown test data which is a LAL-
improvement of about 10%. 
On the methodological level we are studying 
the effects of relaxing some of the assumptions 
of our strict incremental parsing strategy. In or-
der to do so, we developed a separate model for 
predicting the unlabeled edges and a separate 
model for labeling them. In both cases we used 
the same features as described in sec. 2, but 
added features that used a right-context in order 
to take into account the PoS-tag of the N-next 
words viz. N=5 for the syntactic parser and N=3 
for the labeling case. Using both models during 
parsing interleaved, we obtained UAL=65.17% 
and LAL=28.47% on the development set.  
We assumed that the low LAL might have 
been caused by a too narrow syntactic context. In 
order to test this assumption, we decoupled the 
prediction of the unlabeled edges and their label-
ing, such that the determination of the edge la-
bels is performed after the complete unlabeled 
dependency tree is computed. Labeling of the 
dependency edges is then simply performed by 
running through the constructed parse trees as-
signing each edge the most probable dependency 
type. This two-phase strategy achieved an LAL 
of 60.44% on the development set, which means 
an improvement of about 43%. Applying the 
two-phase parser on the WSJ test data resulted in 
UAL=65.22% and LAL=62.83%; applying it on 
the Brown test data resulted in UAL=66.50% and 
LAL=61.11%, respectively. 
Of course, these results are far from being op-
timal. Thus, beside testing and improving our 
parser on the technical level, we will run further 
experiments for different context sizes, exploit-
ing different settings of parameters of the classi-
fier and feature values, and eventually testing 
other ML approaches. The focus here will be on 
the development of unlabeled edge models, be-
cause it seems that an improvement here is sub-
stantial for an overall improvement. For exam-
ple, applying the decoupled edge labeling model 
directly on the given unlabeled dependency trees 
of the development set (i.e. we assume an UAL 
of 100%) gave as an LAL of 92.88%. 
Beside this, we will also re-investigate inter-
leaved strategies of unlabeled edge and edge la-
beling prediction as a basis for (mildly-) strict 
incremental parsing. Here, it might be useful to 
relax the strict linear control regime by exploring 
beam search strategies, e.g. along the lines of 
Collins and Roark (2004). 
5 Conclusion 
We have presented a puristic approach for 
joint dependency parsing and semantic role la-
beling. Since, the development of our approach 
has been started from scratch, we didn?t manage 
to deal with all problems. Our focus was on set-
ting up a workable backbone, and then on trying 
to do as much feature engineering as possible. 
Our bad results on the CoNLL 2008 suggest that 
our current strategy was a bit too optimistic and 
risky, and that the strict incremental deterministic 
parsing regime seemed to have failed in its cur-
rent form. We are now in the process of analysis 
of ?what went wrong?, and have already indi-
cated some issues in the paper. 
Acknowledgement 
The work presented here was partially supported 
by a research grant from the German Federal 
Ministry of Education, Science, Research and 
Technology (BMBF) to the DFKI project HyLaP, 
(FKZ: 01 IW F02). We thank the developers of 
the Opennlp.maxent software package. 
References 
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER 
Treebank in Proceedings of the Workshop on 
Treebanks and Linguistic Theories Sozopol.  
Collins, Michael, and Brian Roark. (2004). Incre-
mental Parsing with the Perceptron Algorithm. 
ACL 2004. 
Hacioglu, Kadri. 2004. Semantic Role Labeling Using 
Dependency Trees. Coling 2004. 
Nivre, Joakim. 2007. Incremental Non-Projective De-
pendency Parsing. NAACL-HLT 200). 
Manning, Christopher, and Hinrich Schutze. 1999. 
Foundations of statistical natural language process-
ing. Cambridge, Mass.: MIT Press. 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings 
of the 12th Conference on Computational Natural 
Language Learning (CoNLL-2008). 
Volokh, Alexander. 2008. Datenbasiertes De-
pendenzparsing. Bachelor Thesis, Saarland Uni-
versity. 
Wang, Mengqui, Kenji Sagae, and Teruko Mitamura. 
2006. A Fast, Accurate Deterministic Parser for 
Chinese. ACL 2006. 
217
