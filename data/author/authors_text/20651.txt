Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 93?96,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
MMAX2 for coreference annotation
Mateusz Kope
?
c
Institute of Computer Science, Polish Academy of Sciences,
Jana Kazimierza 5, 01-248 Warsaw, Poland
m.kopec@ipipan.waw.pl
Abstract
This article presents major modifications
in the MMAX2 manual annotation tool,
which were implemented for the corefer-
ence annotation of Polish texts. Among
other, a new feature of adjudication is de-
scribed, as well as some general insight
into the manual annotation tool selection
process for the natural language process-
ing tasks.
1 Introduction
Recently published Polish Coreference Corpus
(PCC) (Ogrodniczuk et al., 2013) contains a large
number of Polish texts annotated manually with
coreference. During the initial stage of this project
in 2011, a tool had to be selected for the manual
text annotation with coreference.
First issue considered during the selection was
the alternative of desktop versus online annota-
tion tool. Recently, online annotation tools are
becoming increasingly popular (see for example
BRAT (Stenetorp et al., 2012)), with their advan-
tages such as the possibility to monitor the current
state of annotation and make changes to the anno-
tation tool easily, without the need to communi-
cate with the annotators. However, in our opinion
the choice should be made mainly based on the
annotators? preferences, as their work efficiency is
crucial for the cost of the task.
10 linguists (which were annotators in previ-
ous projects conducted by the authors of this pa-
per) were asked in anonymous survey to choose
one of following three options: 1) that they pre-
fer an online annotation tool (not requiring any
installation), 2) a desktop tool with the possibil-
ity to work without constant internet access, 3)
that they do not have preference. Only one per-
son chose online tool and one person chose the
third option, leaving no choice for the annota-
tion task organizers other than to prepare a desk-
top application. The drawback of this approach
was the need to manage text distribution among
the annotators, as all the work was done on lo-
cal computers. Distribution was controlled by
the DistSys application, available at the webpage
zil.ipipan.waw.pl/DistSys.
After some analysis of the features required
by the project?s scope, the choice was narrowed
to only two tools: MMAX2 (M?ller and Strube,
2006) and Palinka (Or
?
asan, 2003). The problem
with the latter was that is was not error-prone, and
lack of publicly available sources did not allow
to make project-specific corrections. Therefore
MMAX2 environment was chosen for the man-
ual coreference annotation. It is a general purpose
cross-platform desktop annotation tool (written in
Java), created to be configurable for many natural
language annotation efforts. It?s source is publicly
available at the webpage mmax2.net. MMAX2
interface consists of the main window with the
text being annotated (see for example figure 5) and
several smaller windows facilitating various tasks
(all the other figures).
2 Annotation scope
The annotation scope of the Polish Coreference
Corpus project consisted of several subtasks for
an annotator to perform for each text. Because the
automatic preannotation was used, annotator?s job
consisted not only of addition of new markables,
but also removal and correction of existing ones.
Subtasks to perform were to:
? mark all mentions,
? mark each mention?s semantic head (choose
one word the mention consists of),
? cluster coreferent mentions,
? mark dominant expression of each cluster,
93
? mark quasi-identity links.
MMAX2 was easy to configure for most of the
subtasks, except the dominant expressions (there
is no possibility to define attributes of clusters,
only markables) and the choice of semantic head
(available types of attributes did not include a pos-
sibility to define a mention-dependent attribute).
Because an estimate of inter-annotator agree-
ment had to be calculated for the corpus, some
texts were annotated independently by two anno-
tators. Agreement measures were then calculated,
but as single gold standard annotation was needed
for the corpus, they also had to be merged into sin-
gle version by the adjudicator. This feature was
also not present in MMAX2.
3 New features
Even with it?s great number of features, there
was no possibility to use MMAX2 without any
changes in it?s source code. Some changes were
the result of project?s scope requirements, some
were added in response to annotator requests. New
implemented features include:
1. Internationalization ? the interface of the tool
is available in English and Polish and can be
easily translated to other languages. Polish
version was used by the annotators, but for
international articles about the tool (such as
this one) the English interface was used.
2. Semantic head selection ? a dropdown list
allows to choose one of the tokens mention
consists of as it?s semantic head. This head is
also underlined in markable browser.
3. Storing user setting ? which windows are
opened, where are they located, what is the
font and it?s size ? these and other user set-
tings are saved and automatically restored
when the application is reopened.
4. Dominant expressions ? clusters can have
their attribute: a dominant expression, which
can be selected as one of the mentions from
the cluster or any other expression entered by
the user.
5. Undo button, reverting last action ? very use-
ful feature to revert the last change, regard-
less of it?s nature.
Figure 1: Adjudication of mentions
Figure 2: Adjudication of clustering
6. Merge two mentions ? user can merge two
mentions into one with a single click, sum-
ming their words and merging their clusters.
Very useful feature when for example one is
correcting automatic annotation, which failed
to recognize a long named entity name and
instead created two entities, each in it?s sepa-
rate cluster.
7. Improved browser operability ? browsers al-
low to operate on mentions, links and clus-
ters, not only to view them.
8. Adjudication feature ? it will be covered in
detail in the next section.
4 Adjudication feature
Adjudication feature of the new Superannotation
plugin allows to compare two versions of anno-
tation of the same text and merge them into one,
adjudicated version. The design is based on the
original MMAX2 Diff plugin, which allowed to
see the differences between two annotations, yet
it was not possible to merge them into one. The
readability of the differences was also limited and
it was improved in our tool.
The adjudication process starts with opening
one annotation in standard way and then the other
via the menu in Superannotation plugin and con-
sist of several steps, each merging particular layer:
94
1. Mentions ? first we need to merge mention
annotations. Differences between the two an-
notations are shown in the figure 1. First col-
umn shows the mention content (and this is
constant in all steps of adjudication), second
shows if that mention is in the first annota-
tion, third column shows if it is in the second
annotation ("+" if yes, "-" if not). Single click
at "+" or the first column highlights given
span in the main window. Double click at one
of the last two columns selects the clicked
version as the proper one and changes the an-
notation in the other file to match the clicked
version. After such double click, the differ-
ence disappears and that row vanishes. After
all rows from that step are gone, mention an-
notations in both versions are the same and
we can proceed to the next step.
2. Comments ? this time first column again
shows each mention, for which there is a
difference in comments in both annotations.
Double clicking at 2nd or 3rd column re-
solves the difference in given row.
3. Heads ? similar to comments, by double-
clicking we can adjudicate differences in
head annotation.
4. Links ? analogously as with heads, we merge
near-identity links annotations.
5. Clusters ? this is the most complex adjudi-
cation task. At this point we surely have the
same set of mentions in both annotations, but
they may be clustered differently. Figure 2
presents how differences in clustering are vi-
sualized. Mentions with the same color in
one column are in the same cluster (they have
also the same cluster number). For example,
two occurrences of mention gorzka? czekolade?
are in the same cluster according to the first
annotation, and are singletons according to
the second annotation. Single click on any of
these options will show it in the main applica-
tion window, while double click will choose
the clicked version as the gold one and update
the other to match it.
6. Dominating expressions ? as the clusters are
now the same, the only annotation left con-
siders cluster attributes: dominating expres-
sions.
Figure 3: Mention attributes ? original MMAX2
Figure 4: Mention attributes ? simplified
Key point of the adjudication procedure is to
merge all differences at a given level before pro-
ceeding to the next one. This way, after we resolve
all differences in the dominating expressions, we
are certain that our annotations are fully merged
and in fact the same.
5 Removed features
Because MMAX2 is a generic tool, the first im-
pression is that it is very complicated. Numer-
ous options, many windows and menus are over-
whelming and could increase the time of creating
a manual for the annotators (often writing a lot of
text only to inform which options should not be
changed). Therefore we removed many options
and simplified the interface to leave only the fea-
tures required by our annotation task. Compare for
example the mention attribute window from the
original MMAX2 in figure 4 and in our version
in figure 3. Removed features included:
? Distinction of multiple annotation levels ?
scope of the project considers only one level,
and the need to explicitly select it in many
places (for example in markable browser) is
unnecessary.
? Possibility to edit the base text ? as we per-
95
Figure 5: Unnecessary arcs
formed the inter-annotator agreement analy-
sis, the base text could not be changed.
? Arcs between coreferent mentions in cluster
(see figure 5 for original visualization) ? from
our experience, they decrease the readability
of the cluster annotation. As the mentions
in cluster are already highlighted, there is no
need to show the arcs connecting them (the
arcs are not clickable as in BRAT).
? MMAX Query Language ? MMAX2 facili-
tates a query language to search for the an-
notations fulfilling given properties. In our
opinion this feature seems more appropriate
for an analyst, not an annotator. Moreover,
results of such querying would be more in-
formative for a collection of texts, not a sin-
gle document.
? Kappa statistic and coincidence matrix calcu-
lation for multiple annotations of a single text
? again, this feature seems more appropriate
for an analyst and for the whole corpus, not a
single text.
6 Conclusion
Every unnecessary action, which has to be re-
peated numerous times by a human annotator, has
a significant cost in terms of time and money.
We claim that annotation efforts are more effi-
cient when there is a step of tool customization
(or even design and implementation from scratch)
beforehand and also during the process, based on
the feedback from the annotators. Using general-
purpose tools has a clear benefit of cheap and
fast initialization of the project, but also there
are major drawbacks: a compromise between the
project needs and the tool capabilities. As we have
seen, even a tool with great customization options
such as MMAX2 doesn?t have all the features one
would need.
Experience of the PCC project shows, that in-
stead of trying to provide a general, configurable
annotation tool (which is very complex due to its
wide application possibilities), another way to pro-
ceed is to create simple, well designed tool fo-
cused on specific task. Such tool can be then
customized or extended by qualified programmers
without much effort and then provide great effi-
ciency of the annotation process.
Presented version of MMAX2 with its source
code is available at http://zil.ipipan.
waw.pl/MMAX4CORE webpage. We encourage
it?s use and modification for other coreference an-
notation projects.
Acknowledgments
The work reported here was cofounded by the
Computer-based methods for coreference resolu-
tion in Polish texts project financed by the Pol-
ish National Science Centre (contract number
6505/B/T02/2011/40) and by the European Union
from resources of the European Social Fund.
Project PO KL ?Information technologies: Re-
search and their interdisciplinary applications?.
References
Christoph M?ller and Michael Strube. 2006. Multi-
level annotation of linguistic data with mmax2. In
Sabine Braun, Kurt Kohn, and Joybrato Mukher-
jee, editors, Corpus Technology and Language Ped-
agogy: New Resources, New Tools, New Methods,
pages 197?214. Peter Lang, Frankfurt a.M., Ger-
many.
Maciej Ogrodniczuk, Katarzyna G?owi
?
nska, Mateusz
Kope
?
c, Agata Savary, and Magdalena Zawis?awska.
2013. Polish coreference corpus. In Zygmunt Ve-
tulani, editor, Proceedings of the 6th Language &
Technology Conference: Human Language Tech-
nologies as a Challenge for Computer Science
and Linguistics, pages 494?498, Pozna?n, Poland.
Wydawnictwo Pozna?nskie, Fundacja Uniwersytetu
im. Adama Mickiewicza.
Constantin Or?asan. 2003. PALinkA: a highly cus-
tomizable tool for discourse annotation. In Proceed-
ings of the 4th SIGdial Workshop on Discourse and
Dialog, pages 39 ? 43, Sapporo, Japan, July, 5 -6.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for nlp-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102?107, Avignon, France, April. Association
for Computational Linguistics.
96
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 221?225,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Zero subject detection for Polish
Mateusz Kope
?
c
Institute of Computer Science, Polish Academy of Sciences,
Jana Kazimierza 5, 01-248 Warsaw, Poland
m.kopec@ipipan.waw.pl
Abstract
This article reports on the first machine
learning experiments on detection of null
subjects in Polish. It emphasizes the role
of zero subject detection as the part of
mention detection ? the initial step of end-
to-end coreference resolution. Anaphora
resolution is not studied in this article.
1 Introduction
Zero subject detection is an important issue for
anaphora and coreference resolution for the null-
subject languages, including all Balto-Slavic lan-
guages and most Romance languages. Their dis-
tinctive feature is the possibility for an indepen-
dent clause to lack an explicit subject. Person,
number, and/or gender agreement with the refer-
ent is indicated by the morphology of the verb:
(1) Maria wr?ci?a ju
?
z z Francji. ?Spe?dzi?a tam
miesia?c.
?Maria came back from France. ?Had
singular:feminine
spent
a month there.?
The recently created Polish Coreference Cor-
pus
1
(PCC) (Ogrodniczuk et al., 2013) contains
zero subject annotation. A markable representing
the null subject is the verbal form following the
position where the argument would have been ex-
pected. As tested on the development part of the
corpus (described in detail later), omitting a per-
sonal pronoun is a frequent issue in the Polish lan-
guage ? about 30% of verbs do not have explicit
subjects. Russo et al. (2012) reports similar fig-
ures for Italian (30.42%) and Spanish (41.17%).
Moreover, these null subjects are often part of
large coreference clusters ? the average size of a
non-singleton coreference cluster in the develop-
ment subcorpus was 3.56 mentions. At the same
1
Publicly available at http://zil.ipipan.waw.
pl/PolishCoreferenceCorpus.
time, the non-singleton coreference cluster con-
taining at least one zero subject had on average
5.89 mentions.
A mention detection module heavily influences
the final coreference resolution score of an end-
to-end coreference resolution system. In Ogrod-
niczuk and Kope
?
c (2011a) the system working on
gold mentions achieved 82.90% F1 BLANC (Re-
casens and Hovy, 2011), whereas on system men-
tions the result dropped to 38.13% (the zero sub-
ject detection module was not implemented).
The aim of this paper is to find a method of au-
tomatic zero subject detection to improve the ac-
curacy of mention detection as the initial step of
coreference resolution.
2 Related Work
We present some of the most recent articles about
machine learning zero subject detection.
Rello et al. (2012b) describes a Brazilian Por-
tuguese corpus with 5665 finite verbs total, out
of which 77% have an explicit subject, 21% a
zero pronoun and 2% are impersonal construc-
tions. They extract various verb, clause and neigh-
boring token features for each verb occurrence and
classify it into one of these 3 classes, achieving
83.04% accuracy of a decision tree learning classi-
fier, better than the baseline result of the Palavras
parser. A very similar study is conducted also
for Spanish (Rello et al., 2012a), with the best
result of the lazy learning classifier K
?
(Cleary
and Trigg, 1995) of 87.6% accuracy, outperform-
ing the baseline of Connexor parser.
Chinese zero pronoun detection and resolution
is presented by Zhao and Ng (2007). Features for
zero pronoun identification consider mainly the
gold standard parse tree structure. Their training
corpus contained only 343 zero pronouns, as com-
pared to 10098 verbs with explicit subjects ? for
Chinese, the phenomenon is much less frequent
than for Polish or Spanish. Therefore they weigh
221
positive and negative examples to get the balance
between precision and recall ? the best result of
50.9% F
1
measure for positive to negative exam-
ple weight ratio of 8:1 is reported.
A study for the Romanian language (Mihaila et
al., 2011) describes a corpus consisting of 2741
sentences and 997 zero pronouns. Class imbalance
is solved by training machine learning algorithms
on all positive examples (zero pronouns) and the
same number of negative examples (sampled from
the corpus). Features used consider morphosyn-
tactic information about the verb, precedence of
the reflective pronoun ?se? and the number of
verbs in the sentence. Their best ensemble clas-
sifier scored 74.5% accuracy.
Only a few studies (for example (Broda et al.,
2012; Ogrodniczuk and Kope
?
c, 2011b; Kope
?
c and
Ogrodniczuk, 2012)) consider the problem of rule-
based or machine learning coreference resolution
for the Polish language, however these attempts
leave zero subject detection as a non-trivial task
for further study.
3 Problem statement
Table 1 presents part of speech definitions as-
sumed in this article, based on the book about the
National Corpus of Polish (Przepi?rkowski et al.,
2012). Coarse-grained POS indicates whether a
word with a given part of speech may be a subject
(Noun) or a verb (Verb) in a sentence. The last four
columns present which morphosyntactic informa-
tion is available for each part of speech. There are
few differences in this definition with respect to
the original approach in the book:
? We treat numerals, gerunds and pronouns as
Nouns ? because they are frequently sub-
jects of the sentence and have the same
morphosyntactic information as ?standard?
nouns.
? We do not consider siebie (?self?, tradition-
ally treated as pronoun) as a Noun, as it can-
not be a subject.
? Tags: impt, imps, inf, pcon, pant, pact, ppas,
pred, which are traditionally considered verb
tags, are not treated by us as Verbs, because
they cannot have a subject.
With such a definition of parts of speech, our
task may be stated as follows: given a clause with
a Verb, decide whether the clause contains a Noun
Coarse-
-grained
POS
POS Tag
N
u
m
b
e
r
C
a
s
e
G
e
n
d
e
r
P
e
r
s
o
n
Noun
Noun subst + + +
Depreciative form depr + + +
Main numeral num + + +
Collective numeral numcol + + +
Gerund ger + + +
Personal pronoun ? 1st, 2nd person ppron12 + + + +
Personal pronoun ? 3rd person ppron3 + + + +
Verb
Non-past form fin + +
Future byc? bedzie + +
Agglutinate byc? aglt + +
L-participle praet + +
winien-like verb winien + +
Table 1: Parts of speech
which is the Verb?s explicit subject. From now on
in this paper, the words ?noun? and ?verb? have
the meaning of Noun and Verb, respectively. In
this study, we do not try to handle the cases of
subjects not being nouns, as judging from our ob-
servations, it is very infrequent. We do take into
account in our solution the cases of the subject not
in the nominative case, as in the example:
(2) Pienie?dzy
noun:genitive
nie starczy dla wszys-
tkich.
?There wouldn?t be enough money for everyone.?
It is worth noting that Polish is a free-word-
order language, therefore there are many possible
places for the subject to appear, with respect to the
position of the verb.
As the corpus has only automatic morphosyn-
tactic information available (provided by the PAN-
TERA tagger (Aceda
?
nski, 2010)), not corrected by
the coreference annotators, the only verbs consid-
ered in this study are the ones found by the tag-
ger. If such a verb was marked as a mention by
the coreference annotator (verb mention in table
2), it is a positive example for our machine learn-
ing study, otherwise a negative one. Sentence and
clause segmentation in the corpus was also auto-
matic. We are aware that the corpus used for the
study was not perfectly suited for the task ? verbs
with a zero subject are not marked there explicitly,
but can only be found based on automatic tagging.
However the tagging error of detecting verbs is re-
ported as not higher than 0.04% (for the fin tag,
see (Aceda
?
nski, 2010) for details), so we consider
the resource sufficiently correct.
4 Development and evaluation data
Each text of the Polish Coreference Corpus is a
250-350 word sample, consisting of full, subse-
quent paragraphs extracted from a larger text. Text
genres balance correspond to the National Corpus
222
Corpus # texts # sentences # tokens # verbs # mentions # verb mentions
Development 390 6481 110379 10801 37250 3104
Evaluation 389 6737 110474 11000 37167 3106
Total 779 13218 220853 21801 74417 6210
Table 2: Zero subject study data statistics
of Polish (Przepi?rkowski et al., 2012). At the
time this study started, 779 out of 1773 texts (ran-
domly chosen) of the Polish Coreference Corpus
were already manually annotated. Annotated texts
were randomly split into two equal-sized subcor-
pora for development and evaluation. Their de-
tailed statistics are presented in Table 2.
4.1 Inter-annotator agreement
210 texts of the Polish Coreference Corpus were
annotated independently by two annotators. This
part was analyzed for the inter-annotator agree-
ment of deciding if a verb has a zero subject or
not. In the data there were 5879 verbs total,
for which observed agreement yielded 92.57%.
Agreement expected by chance (assuming a per
annotator chance annotation probability distribu-
tion) equalled 57.52%, therefore chance-corrected
Cohen?s ? for the task equalled 82.51%.
4.2 Results of full dependency parsing
The first Polish dependency parser was recently
developed and described by Wr?blewska (2012).
The author reports 71% LAS
2
and 75.2% UAS
3
performance of this parser. This parser was used
to detect null subjects ? every verb lacking the
dependency relation of the subject type (subj)
was marked as missing the subject. This base-
line method achieved accuracy of 67.23%, preci-
sion of 46.53%, recall of 90.47% and F
1
equal to
61.45%. These results are worse than a simple ma-
jority baseline classifier, therefore current state-of-
the-art Polish dependency parsing is not a satisfac-
tory solution to the task stated in this article.
5 Features
Based on a number of experiments on the develop-
ment corpus, we chose a number of features pre-
sented in table 3.
Subject candidate existence features from the
bottom of the table 3 use variables: c
1
, c
2
and w.
Separate feature was generated for each combi-
nation of these three variables. The variable w
2
Labeled attachment score ? the percentage of tokens that
are assigned a correct head and a correct dependency type.
3
Unlabeled attachment score ? the percentage of tokens
that are assigned a correct head.
represents the window around the verb, with fol-
lowing values: the clause containing the verb, the
sentence containing the verb, windows of 1 to 5
tokens before the verb, windows of 1 to 5 tokens
after the verb, windows of 1 to 5 tokens both be-
fore and after the verb. Variable c
1
represents
compatibility of noun and verb, with values be-
ing any nonempty subset of the set of following
conditions: case of the noun equal to nominative
(NOM), number agreement with the verb (NUM),
person or gender agreement (POG), depending on
which was available to check, see Table 1. Vari-
able c
2
is similar to c
1
, with the following values:
{NOM}, {POG}, {NOM, POG}.
Feature Type
Verb features
number of the verb ? to help with cases of plural verbs having two
or more singular nouns as subject
nominal
tag of the verb ? as it may happen, that some parts of speech behave
differently
boolean
is the verb on the pseudo-verbs list extracted from (
?
Swidzi?nski,
1994) ? i.e. may not require a subject
boolean
Neighboring token features
tag of the next token nominal
tag of the previous token nominal
is the previous tag equal to praet ? a redundant feature to the pre-
vious one, but it should help with the cases like:
. . . by?a
praet
m
aglt:pri
. . . ". . . (I) was . . . "
when we split a word into a L-participle and agglutinate. Annota-
tion guidelines were to only mark the agglutinate as a mention,
when the verb does not have an explicit subject
boolean
does one of the previous two tokens have the pred tag ? should al-
low detecting examples similar to:
Mo
?
zna
pred
sie? by?o
praet
tego spodziewac?.
". . . It could have been expected. . . . "
Trzeba
pred
by?o
praet
my?slec? wcze?sniej.
"(One) should have thought before."
when by?o ("have") cannot have subject, as it is part of an imper-
sonal construction
boolean
is the next tag inf ? similar role to the previous feature, as in:
Wtedy nale
?
zy
fin
poprosic?
inf
. "(One) should then ask for it."
when nale
?
zy ("one should") cannot have a subject
boolean
is the previous token a comma boolean
Length features
number of tokens in the sentence (following the hypothesis, that the
shorter the sentence/clause, the less likely for the subject to appear)
numerical
number of tokens in the clause with the verb numerical
Subject candidate existence features
existence of a noun not preceded by jak/jako ("as") in window w
fulfilling conditions from set c
1
boolean
existence of at least two nouns not preceded by jak/jako ("as") in
window w both fulfilling conditions from set c
2
boolean
Table 3: Features
6 Evaluation
Presented features were used to train a machine
learning algorithm. We chose the JRip imple-
mentation of RIPPER (Cohen, 1995) from WEKA
(Hall et al., 2009) for the possibility to interpret the
rules, which is outside of the scope of this paper.
6.1 Accuracy on the development corpus
A baseline model which always predicts that a
verb has an explicit subject achieves 71.13% ac-
223
True values
null subject explicit subject
Predictions
null subject 2093 815
explicit subject 1013 7079
Table 4: Confusion matrix
curacy on the development data. The upper bound
of the ITA (as stated earlier) is around 92.57% ac-
curacy.
We used 10-fold cross-validation which was re-
peated 10 times with different random seeds for
training and train/test splits. The average from the
total of 100 trials (each cross-validation split sep-
arately) was equal to 82.74%, with standard devi-
ation of 1.27%. As the Shapiro-Wilk (1965) test
for normality for this data gives p-value of 0.38, it
may be assumed that it follows the normal distri-
bution. In that case, the 95% confidence interval
for the accuracy is equal to [82.49%, 82.99%].
6.2 Accuracy on the evaluation corpus
The evaluation corpus was used only for two ex-
periments presented below: to calculate accuracy
and learning curve of the developed solution.
We used the model learnt on the development
corpus and tested it on the evaluation corpus,
achieving 83.38% accuracy. A majority classifier
would achieve 71.76% accuracy on this corpus.
The confusion matrix is depicted in Table 4. For
finding the null subjects, recall of 67.39% and pre-
cision of 71.97% gives F
1
measure of 69.60%.
6.3 Learning curve
To test how the number of training examples in-
fluences the quality of the trained classifier, we
used subsets of the development corpus of various
sizes as training sets. The test set was the same
in all cases (the evaluation corpus). Proportions
of the examples used ranged from 5% to 100%
of the development corpus, each proportion was
tested 10 times to provide an estimation of vari-
ance. For example, to evaluate the efficiency of
the classifier trained on 5% of the training exam-
ples, we randomly sampled 5% of the examples,
trained the classifier and tested it on the full evalu-
ation corpus. Then we repeated it another 9 times,
randomly choosing a different 5% portion of the
examples for training.
Again the Shapiro-Wilk test was taken to assess
the normality of results for each proportion, out
of 19 proportions tested (the proportion of 1 was
of course not tested for normality), only 3 had p-
0.2 0.4 0.6 0.8 1.00.
79
0.82
Proportion of training set usedAc
cura
cy o
n ev
alua
tion
 cor
pus
Figure 1: Learning curve
value less than 0.1, therefore we assumed that the
data is distributed approximately normally. The
95% confidence intervals of the classifiers trained
on a given proportion of the development corpus
are shown in the Figure 1. The algorithm clearly
benefits from having more training examples. We
observe that the curve is generally of the desired
shape, yet it flattens when approaching the full
training set used. It may suggest that the devel-
oped solution would not be able to significantly
exceed 84%, even given more training examples.
7 Conclusions and future work
This article presented an efficient zero subject de-
tection module for Polish. We highlighted some
difficult examples to take into account and pro-
posed a solution for the Polish language.
The achieved accuracy of 83.38% significantly
exceeds the baseline of majority tagging, equal to
71.76%, but there is still room for improvement,
as the upper bound of 92.57% was computed. The
achieved result for the task of null subject detec-
tion looks promising for the application in mention
detection for coreference resolution.
The invented solution needs to be incorporated
in a complete coreference resolver for Polish and
evaluated for the extent to which using such an ad-
vanced separate classifier for zero subject detec-
tion improves the mention detection and, further-
more, end-to-end coreference resolution accuracy.
Acknowledgements
The work reported here was cofounded by the
Computer-based methods for coreference resolu-
tion in Polish texts project financed by the Pol-
ish National Science Centre (contract number
6505/B/T02/2011/40) and by the European Union
from resources of the European Social Fund.
Project PO KL ?Information technologies: Re-
search and their interdisciplinary applications?.
224
References
Szymon Aceda?nski. 2010. A Morphosyntactic Brill
Tagger for Inflectional Languages. In Hrafn Lofts-
son, Eir?kur R?gnvaldsson, and Sigr?n Helgad?ttir,
editors, Advances in Natural Language Processing,
volume 6233 of Lecture Notes in Computer Science,
pages 3?14. Springer.
Bartosz Broda, ?ukasz Burdka, and Marek Maziarz.
2012. IKAR: An Improved Kit for Anaphora Res-
olution for Polish. In COLING (Demos), pages 25?
32.
John G. Cleary and Leonard E. Trigg. 1995. K*:
An instance-based learner using an entropic distance
measure. In In Proceedings of the 12th International
Conference on Machine Learning, pages 108?114.
Morgan Kaufmann.
William W. Cohen. 1995. Fast effective rule induc-
tion. In In Proceedings of the Twelfth International
Conference on Machine Learning, pages 115?123.
Morgan Kaufmann.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Mateusz Kope
?
c and Maciej Ogrodniczuk. 2012. Cre-
ating a Coreference Resolution System for Pol-
ish. In Proceedings of the Eighth International
Conference on Language Resources and Evalua-
tion, LREC 2012, pages 192?195, Istanbul, Turkey.
ELRA.
Claudiu Mihaila, Iustina Ilisei, and Diana Inkpen.
2011. Zero Pronominal Anaphora Resolution for the
Romanian Language. Research Journal on Com-
puter Science and Computer Engineering with Ap-
plications? POLIBITS, 42.
Maciej Ogrodniczuk and Mateusz Kope
?
c. 2011a. End-
to-end coreference resolution baseline system for
Polish. In Zygmunt Vetulani, editor, Proceedings of
the 5th Language & Technology Conference: Hu-
man Language Technologies as a Challenge for
Computer Science and Linguistics, pages 167?171,
Pozna?n, Poland.
Maciej Ogrodniczuk and Mateusz Kope
?
c. 2011b.
Rule-based coreference resolution module for Pol-
ish. In Proceedings of the 8th Discourse Anaphora
and Anaphor Resolution Colloquium (DAARC
2011), pages 191?200, Faro, Portugal.
Maciej Ogrodniczuk, Katarzyna G?owi
?
nska, Mateusz
Kope
?
c, Agata Savary, and Magdalena Zawis?awska.
2013. Polish coreference corpus. pages 494?498.
Adam Przepi?rkowski, Miros?aw Ba
?
nko, Rafa? L.
G?rski, and Barbara Lewandowska-Tomaszczyk,
editors. 2012. Narodowy Korpus Je?zyka Polskiego
[Eng.: National Corpus of Polish]. Wydawnictwo
Naukowe PWN, Warsaw.
Marta Recasens and E. Hovy. 2011. BLANC: Imple-
menting the Rand index for coreference evaluation.
pages 485?510.
Luz Rello, Ricardo Baeza-Yates, and Ruslan Mitkov.
2012a. Elliphant: Improved Automatic Detection
of Zero Subjects and Impersonal Constructions in
Spanish. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 706?715, Avignon,
France, April. Association for Computational Lin-
guistics.
Luz Rello, Gabriela Ferraro, and Iria Gayo. 2012b.
A First Approach to the Automatic Detection of
Zero Subjects and Impersonal Constructions in Por-
tuguese. Procesamiento del Lenguaje Natural,
49:163?170.
Lorenza Russo, Sharid Lo?iciga, and Asheesh Gulati.
2012. Improving machine translation of null sub-
jects in Italian and Spanish. In Proceedings of
the Student Research Workshop at the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 81?89, Avignon,
France, April. Association for Computational Lin-
guistics.
S. S. Shapiro and M. B. Wilk. 1965. An analysis
of variance test for normality (complete samples).
Biometrika, 52(3/4):591?611, Dec.
Marek
?
Swidzi?nski. 1994. Syntactic dictionary of pol-
ish verbs.
Alina Wr?blewska. 2012. Polish dependency bank.
Linguistic Issues in Language Technology, 7(1).
Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and Resolution of Chinese Zero Pronouns: A
Machine Learning Approach. In EMNLP-CoNLL,
pages 541?550. ACL.
225
