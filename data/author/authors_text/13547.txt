Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 103?109,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The RALI Machine Translation System for WMT 2010
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry and Philippe Langlais
RALI - Universite? de Montre?al
C.P. 6128, succursale Centre-ville
H3C 3J7, Montre?al, Que?bec, Canada
{huetstep,bourdaij,patryale,felipe}@iro.umontreal.ca
Abstract
We describe our system for the translation
task of WMT 2010. This system, devel-
oped for the English-French and French-
English directions, is based on Moses and
was trained using only the resources sup-
plied for the workshop. We report exper-
iments to enhance it with out-of-domain
parallel corpora sub-sampling, N-best list
post-processing and a French grammatical
checker.
1 Introduction
This paper presents the phrase-based machine
translation system developed at RALI in order
to participate in both the French-English and
English-French translation tasks. In these two
tasks, we used all the corpora supplied for the con-
straint data condition apart from the LDC Giga-
word corpora.
We describe its different components in Sec-
tion 2. Section 3 reports our experiments to sub-
sample the available out-of-domain corpora in or-
der to adapt the translation models to the news
domain. Section 4, dedicated to post-processing,
presents how N-best lists are reranked and how the
French 1-best output is corrected by a grammatical
checker. Section 5 studies how the original source
language of news acts upon translation quality. We
conclude in Section 6.
2 System Architecture
2.1 Pre-processing
The available corpora were pre-processed using
an in-house script that normalizes quotes, dashes,
spaces and ligatures. We also reaccentuated
French words starting with a capital letter. We
significantly cleaned up the parallel Giga word
corpus (noted as gw hereafter), keeping 18.1 M
of the original 22.5 M sentence pairs. For exam-
ple, sentence pairs with numerous numbers, non-
alphanumeric characters or words starting with
capital letters were removed.
Moreover, training material was tokenized with
the tool provided for the workshop and truecased,
meaning that the words occuring after a strong
punctuation mark were lowercased when they be-
longed to a dictionary of common all-lowercased
forms; the others were left unchanged. In order
to reduce the number of words unknown to the
translation models, all numbers were serialized,
i.e. mapped to a special unique token. The origi-
nal numbers are then placed back in the translation
in the same order as they appear in the source sen-
tence. Since translations are mostly monotonic be-
tween French and English, this simple algorithm
works well most of the time.
2.2 Language Models
We trained Kneser-Ney discounted 5-gram lan-
guage models (LMs) on each available corpus us-
ing the SRILM toolkit (Stolcke, 2002). These
LMs were combined through linear interpola-
tion: first, an out-of-domain LM was built from
Europarl, UN and gw; then, this model was
combined with the two in-domain LMs trained
on news-commentary and news.shuffled, which
will be referred to as nc and ns in the remainder
of the article. Weights were fixed by optimizing
the perplexity of a development corpus made of
news-test2008 and news-syscomb2009 texts.
In order to reduce the size of the LMs, we
limited the vocabulary of our models to 1 M
words for English and French. The words of
these vocabularies were selected from the com-
putation of the number of their occurences us-
ing the method proposed by Venkataraman and
Wang (2003). The out-of-vocabulary rate mea-
sured on news-test2009 and news-test2010
with a so-built vocabulary varies between 0.6 %
103
and 0.8 % for both English and French, while it
was between 0.4 % and 0.7 % before the vocabu-
lary was pruned.
To train the LM on the 48 M-sentence English
ns corpus, 32 Gb RAM were required and up to
16 Gb RAM, for the other corpora. To reduce the
memory needs during decoding, LMs were pruned
using the SRILM prune option.
2.3 Alignment and Translation Models
All parallel corpora were aligned with
Giza++ (Och and Ney, 2003). Our transla-
tion models are phrase-based models (PBMs)
built with Moses (Koehn et al, 2007) with the
following non-default settings:
? maximum sentence length of 80 words,
? limit on the number of phrase translations
loaded for each phrase fixed to 30.
Weights of LM, phrase table and lexicalized
reordering model scores were optimized on the
development corpus thanks to the MERT algo-
rithm (Och, 2003).
2.4 Experiments
This section reports experiments done on the
news-test2009 corpus for testing various config-
urations. In these first experiments, we trained
LMs and translation models on the Europarl cor-
pus.
Case We tested two methods to handle case. The
first one lowercases all training data and docu-
ments to translate, while the second one normal-
izes all training data and documents into their nat-
ural case. These two methods require a post-
processing recapitalization but this last step is
more basic for the truecase method. Training mod-
els on lowercased material led to a 23.15 % case-
insensitive BLEU and a 21.61 % case-sensitive
BLEU; from truecased corpora, we obtained a
23.24 % case-insensitive BLEU and a 22.13 %
case-sensitive BLEU. As truecasing induces an in-
crease of the two metrics, we built all our mod-
els in truecase. The results shown in the remain-
der of this paper are reported in terms of case-
insensitive BLEU which showed last year a bet-
ter correlation with human judgments than case-
sensitive BLEU for the two languages we con-
sider (Callison-Burch et al, 2009).
Tokenization Two tokenizers were tested: one
provided for the workshop and another we devel-
oped. They differ mainly in the processing of com-
pound words: our in-house tokenizer splits these
words (e.g. percentage-wise is turned into percent-
age - wise), which improves the lexical coverage of
the models trained on the corpus. This feature
does not exist in the WMT tool. However, us-
ing the WMT tokenizer, we measured a 23.24 %
BLEU, while our in-house tokenizer yielded a
lower BLEU of 22.85 %. Follow these results
prompted us to use the WMT tokenizer.
Serialization In order to test the effect of se-
rialization, i.e. the mapping of all numbers to
a special unique token, we measured the BLEU
score obtained by a PBM trained on Europarl for
English-French, when numbers are left unchanged
(Table 1, line 1) or serialized (line 2). These
results exhibit a slight decrease of BLEU when
serialization is performed. Moreover, if BLEU
is computed using a serialized reference (line 3),
which is equivalent to ignoring deserialization er-
rors, a minor gain of BLEU is observed, which
validates our recovering method. Since resorting
to serialization/deserialization yields comparable
performance to a system not using it, while reduc-
ing the model?s size, we chose to use it.
BLEU
no serialization 23.24
corpus serialization 23.13
corpus and reference serialization 23.27
Table 1: BLEU measured for English-French on
news-test2009 when training on Europarl.
LM Table 2 reports the perplexity measured on
news-test2009 for French (column 1) and En-
glish (column 3) LMs learned on different cor-
pora and interpolated using the development cor-
pus. We also provide the BLEU score (column 2)
for English-French obtained from translation mod-
els trained on Europarl and nc. As expected, us-
ing in-domain corpora (line 2) for English-French
led to better results than using out-of-domain data
(line 3). The best perplexities and BLEU score
are obtained when LMs trained on all the available
corpora are combined (line 4). The last three lines
exhibit how LMs perform when they are trained on
in-domain corpora without pruning them. While
the gzipped 5-gram LM (last line) obtained in
104
such a manner occupies 1.4 Gb on hard disk, the
gzipped pruned 5-gram LM (line 4) trained using
all corpora occupies 0.9 Gb and yields the same
BLEU score. This last LM was used in all the ex-
periments reported in the subsequent sections.
corpora
Fr En
ppl BLEU ppl
nc 327 22.44 454
nc + ns 125 25.69 166
Europarl + UN + Gw 156 24.91 225
all corpora 113 26.01 151
nc + ns (3g, unpruned) 138 25.32 -
nc + ns (4g, unpruned) 124 25.86 -
nc + ns (5g, unpruned) 120 26.04 -
Table 2: LMs perplexities and BLEU scores mea-
sured on news-test2009. Translation models
used here were trained on nc and Europarl.
3 Domain adaptation
As the only news parallel corpus provided for
the workshop contains 85k sentence pairs, we
must resort to other parallel out-of-domain cor-
pora in order to build reliable translation models.
If in-domain and out-of-domain LMs are usually
mixed with the well-studied interpolation tech-
niques, training translation models from data of
different domains has received less attention (Fos-
ter and Kuhn, 2007; Bertoldi and Federico, 2009).
Therefore, there is still no widely accepted tech-
nique for this last purpose.
3.1 Effects of the training data size
We investigated how increasing training data acts
upon BLEU score. Table 3 shows a high increase
of 2.7 points w.r.t. the use of nc alone (line 1)
when building the phrase table and the reordering
model from nc and either the 1.7 M-sentence-pair
Europarl (line 2) or a 1.7 M-sentence-pair cor-
pus extracted from the 3 out-of-domain corpora:
Europarl, UN and Gw (line 3). Training a PBM on
merged parallel corpora is not necessarily the best
way to combine data from different domains. We
repeated 20 times nc before adding it to Europarl
so as to have the same amount of out-of-domain
and in-domain material. This method turned out
to be less successful since it led to a minor 0.15
BLEU decrease (line 4) w.r.t. our previous system.
Following the motto ?no data is better than more
corpora En?Fr Fr?En
nc 23.29 23.23
nc + Europarl 26.01 -
nc + 1.7 M random pairs 26.02 26.68
20?nc + Europarl 25.86 -
nc + 8.7 M pairs (part 0) 26.44 27.65
nc + 8.7 M pairs (part 1) 26.68 27.46
nc + 8.7 M pairs (part 2) 26.54 27.50
3 models merged 26.86 27.56
Table 3: BLEU (in %) measured on news-
test2009 for English-French and French-English
when translations models and lexicalized reorder-
ing models are built using various amount of data
in addition to nc.
data?, a PBM was built using all the parallel cor-
pora at our disposal. Since the overall parallel sen-
tences were too numerous for our computational
resources to be simultaneously used, we randomly
split out-of-domain corpora into 3 parts of 8.7 M
sentence pairs each and then combined them with
nc. PBMs were trained on each of these parts
(lines 5 to 7), which yields respectively 0.5 and
0.8 BLEU gain for English-French and French-
English w.r.t. the use of 1.7 M out-of-domain sen-
tence pairs. The more significant improvement no-
ticed for the French-English direction is probably
explained by the fact that the French language is
morphologically richer than English. The 3 PBMs
were then combined by merging the 3 phrase ta-
bles. To do so, the 5 phrase table scores computed
by Moses were mixed using the geometric average
and a 6th score was added, which counts the num-
ber of phrase tables where the given phrase pair
occurs. We ended up with a phrase table contain-
ing 623 M entries, only 9 % and 4 % of them being
in 2 and 3 tables respectively. The resulting phrase
table led to a slight improvement of BLEU scores
(last line) w.r.t. the previous models, except for the
model trained on part 0 for French-English.
3.2 Corpus sub-sampling
Whereas using all corpora improves translation
quality, it requires a huge amount of memory and
disk space. We investigate in this section ways to
select sentence pairs among large out-of-domain
corpora.
Unknown words The main interest of adding
new training material relies on the finding of
words missing in the phrase table. According to
105
this principle, nc was extended with new sentence
pairs containing an unknown word (Table 4, line 2)
or a word that belongs to our LM vocabulary and
that occurs less than 3 times in the current cor-
pus (line 3). This resulted in adding 400 k pairs
in the first case and 950 k in the second one, with
BLEU scores close or even better than those ob-
tained with 1.7 M.
corpora En?Fr Fr?En
nc + 1.7 M random pairs 26.02 26.68
nc + 400k pairs (occ = 1) 25.67 -
nc + 950k pairs (occ = 3) 26.13 -
nc + Joshua sub-sampling 26.98 27.68
nc + IR (1-g q, w/ repet) 25.81 -
nc + IR (1-g q, no repet) 26.56 27.54
nc + IR (1,2-g q, w/ repet) 26.26 -
nc + IR (1,2-g q, no repet) 26.53 -
nc + 8.7 M pairs 26.68 27.65
+ IR score (1g q, no repet) 26.93 27.65
3 large models merged 26.86 27.56
+ IR score (1g q, no repet) 26.98 27.74
Table 4: BLEU measured on news-test2009 for
English-French and French-English using transla-
tion models trained on nc and a subset of out-of-
domain corpora.
Unknown n-grams We applied the sub-
sampling method available in the Joshua
toolkit (Li et al, 2009). This method adds a
new sentence pair when it contains new n-grams
(with 1 ? n ? 12) occurring less than 20 times in
the current corpus, which led us to add 1.5 M pairs
for English-French and 1.4 M for French-English.
A significant improvement of BLEU is observed
using this method (0.8 for English-French and
1.0 for French-English) w.r.t. the use of 1.7 M
randomly selected pairs. However, this method
has the major drawback of needing to build a new
phrase table for each document to translate.
Information retrieval Information retrieval
(IR) methods have been used in the past to sub-
sample parallel corpora (Hildebrand et al, 2005;
Lu? et al, 2007). These studies use sentences
belonging to the development and test corpora as
queries to select the k most similar source sen-
tences in an indexed parallel corpus. The retrieved
sentence pairs constitute a training corpus for
the translation models. In order to alleviate the
fact that a new PBM has to be learned for each
new test corpus, we built queries using sentences
contained in the monolingual ns corpus, leading
to the selection of sentence pairs stylistically
close to those in the news domain. The source
sentences of the three out-of-domain corpora
were indexed using Lemur.1 Two types of queries
were built from ns sentences after removing stop
words: the first one is limited to unigrams, the
second one contains both unigrams and bigrams,
with a weight for bigrams twice as high as for
unigrams. The interest of the latter query type is
based on the hypothesis that bigrams are more
domain-dependent than unigrams. Another choice
that needs to be made when using IR methods is
concerning the retention of redundant sentences
in the final corpus.
Lines 5 to 8 of Table 4 show the results obtained
when sentence pairs were gathered up to the size
of Europarl, i.e. 1.7 M pairs. 10 sentences were
retrieved per query in various configurations: with
or without bigrams inside queries, with or without
duplicate sentence pairs in the training corpus. Re-
sults demonstrate the interest of the approach since
the BLEU scores are close to those obtained us-
ing the previous tested method based on n-grams
of the test data. Taking bigrams into account does
not improve results and adding only once new sen-
tences is more relevant than duplicating them.
Since using all data led to even better perfor-
mances (see last line of Table 3), we used infor-
mation provided by the IR method in the PBMs
trained on nc + 8.7 M out-of-domain sentence
pairs or taking into account all the training ma-
terial. To this end, we included a new score in
the phrase tables which is fixed to 1 for entries
that are in the phrase table trained on sentences
retrieved with unigram queries without repetition
(see line 6 of Table 4), and 0 otherwise. Therefore,
this score aims at boosting the weight of phrases
that were found in sentences close to the news do-
main. The results reported in the 4 last lines of Ta-
ble 4 show minor but consistent gains when adding
this score. The outputs of the PBMs trained on
all the training corpus and which obtained the best
BLEU scores on news-test2009 were submitted
as contrastive runs. The two first lines of Table 5
report the results on this years?s test data, when
the score related to the retrieved corpus is incor-
porated or not. These results still exhibit a minor
improvement when adding this score.
1www.lemurproject.org
106
En?Fr Fr?En
BLEU BLEU-cased TER BLEU BLEU-cased TER
PBM 27.5 26.5 62.2 27.8 26.9 61.2
+IR score 27.7 26.6 62.1 28.0 27.0 61.0
+N-best list reranking 27.9 26.8 62.1 28.0 27.0 61.2
+grammatical checker 28.0 26.9 62.0 - - -
Table 5: Official results of our system on news-test2010.
4 Post-processing
4.1 N-best List Reranking
Our best PBM enhanced by IR methods was em-
ployed to generate 500-best lists. These lists were
reranked combining the global decoder score with
the length ratio between source and target sen-
tences, and the proportions of source sentence n-
grams that are in the news monolingual corpora
(with 1 ? n ? 5). Weights of these 7 scores are
optimized via MERT on news-test2009. Lines 2
and 3 of Table 5 provide the results obtained be-
fore and after N-best list reranking. They show a
tiny gain for all metrics for English-French, while
the results remain constant for French-English.
Nevertheless, we decided to use those translations
for the French-English task as our primary run.
4.2 Grammatical Checker
PBM outputs contain a significant number of
grammatical errors, even when LMs are trained
on large data sets. We tested the use of a gram-
matical checker for the French language: Antidote
RX distributed by Druide informatique inc.2 This
software was applied in a systematic way on the
first translation generated after N-best reranking.
Thus, as soon as the software suggests one or sev-
eral choices that it considers as more correct than
the original translation, the first proposal is kept.
The checked translation is our first run for English-
French.
Antidote RX changed at least one word in
26 % of the news-test2010 sentences. The most
frequent type of corrections are agreement errors,
like in the following example where the agreement
between the subject nombre (number) is correctly
made with the adjective coupe? (cut), thanks to the
full syntactic parsing of the French sentence.
Source: [...] the number of revaccinations could then be
cut [...]
Reranking: [...] le nombre de revaccinations pourrait
2www.druide.com
alors e?tre coupe?es [...]
+Grammatical checker: [...] le nombre de revacci-
nations pourrait alors e?tre coupe? [...]
The example below exhibits a good decision
made by the grammatical checker on the mood of
the French verb e?tre (to be).
Source: It will be a long time before anything else will be
on offer in Iraq.
Reranking: Il faudra beaucoup de temps avant que tout
le reste sera offert en Irak.
+Grammatical checker: Il faudra beaucoup de temps
avant que tout le reste soit offert en Irak.
A last interesting type of corrected errors con-
cerns negation. Antidote has indeed the capacity
to add the French particle ne when it is missing in
the expressions ne ... pas, ne ... plus, aucun ne, per-
sonne ne or rien ne. The results obtained using the
grammatical checker are reported in the last line
of Table 5. The automatic evaluation shows only a
minor improvement but we expect the changes in-
duced by this tool to be more significant for human
annotators.
5 Effects of the Original Source
Language of Articles on Translation
During our experiments, we found that translation
quality is highly variable depending on the origi-
nal source language of the news sentences. This
phenomenon is correlated to the previous work of
Kurokawa et al (2009) that showed that whether
or not a piece of text is an original or a trans-
lation has an impact on translation performance.
The main reason that explains our observations
is probably that the topics and the vocabulary of
news originally expressed in languages other than
French and English tend to differ more from those
of the training materials used to train PBM mod-
els for these two languages. In order to take into
account this phenomenon, MERT tuning was re-
peated for each original source language, using the
107
same PBM models trained on all parallel corpora
and incorporating an IR score.
Columns 1 and 3 of Table 5 display the BLEU
measured using our previous global MERT op-
timization made on 2553 sentence pairs, while
columns 2 and 4 show the results obtained when
running MERT on subsets of the development ma-
terial, made of around 700 sentence pairs each.
The BLEU measured on the whole 2010 test set
is reported in the last line. As expected, language-
dependent MERT tends to increase the LM weight
for English and French. However, an absolute
0.35 % BLEU decrease is globally observed for
English-French using this approach and a 0.21 %
improvement for French-English.
En?Fr Fr?En
MERT global lang dep global lang dep
Cz 21.95 21.45 21.84 21.85
En 30.80 29.84 33.73 35.00
Fr 37.59 36.96 31.59 32.62
De 16.60 16.73 17.41 17.76
Es 24.52 24.45 29.25 28.31
total 27.64 27.39 27.99 28.20
Table 6: BLEU scores measured on parts of
news-test2010 according to the original source
language.
6 Conclusion
This paper presented our statistical machine trans-
lation system developed for the translation task us-
ing Moses. Our submitted runs were generated
from models trained on all the corpora made avail-
able for the workshop, as this method had pro-
vided the best results in our experiments. This
system was enhanced using IR methods which
exploits news monolingual copora, N-best list
reranking and a French grammatical checker.
This was our first participation where such a
huge amount data was involved. Training models
on so many sentences is challenging from an engi-
neering point of view and requires important com-
putational resources and storage capacities. The
time spent in handling voluminous data prevented
us from testing more approaches. We suggest that
the next edition of the workshop could integrate
a task restraining the number of parameters in the
models trained.
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In 4th EACL Workshop
on Statistical Machine Translation (WMT), Athens,
Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In 4th
EACL Workshop on Statistical Machine Translation
(WMT), Athens, Greece.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In 2nd ACL Workshop
on Statistical Machine Translation (WMT), Prague,
Czech Republic.
Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel, and Alex Waibel. 2005. Adaptation of the
translation model for statistical machine translation
based on information retrieval. In 10th conference
of the European Association for Machine Transla-
tion (EAMT), Budapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In 45th Annual Meeting of the Association for
Computational Linguistics (ACL), Companion Vol-
ume, Prague, Czech Republic.
David Kurokawa, Cyril Goutte, and Pierre Isabelle.
2009. Automatic detection of translated text and
its impact on machine translation. In 12th Machine
Translation Summit, Ottawa, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In 4th
EACL Workshop on Statistical Machine Translation
(WMT), Athens, Greece.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Join Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), Prague, Czech Repub-
lic.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Sapporo, Japan.
108
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In 7th International Con-
ference on Spoken Language Processing (ICSLP),
Denver, CO, USA.
Arnand Venkataraman and Wen Wang. 2003. Tech-
niques for effective vocabulary selection. In 8th Eu-
ropean Conference on Speech Communication and
Technology (Eurospeech), Geneva, Switzerland.
109
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440?446,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The LIGA (LIG/LIA) Machine Translation System for WMT 2011
Marion Potet1, Raphae?l Rubino2, Benjamin Lecouteux1, Ste?phane Huet2,
Herve? Blanchon1, Laurent Besacier1 and Fabrice Lefe`vre2
1UJF-Grenoble1, UPMF-Grenoble2
LIG UMR 5217
Grenoble, F-38041, France
FirstName.LastName@imag.fr
2Universite? d?Avignon
LIA-CERI
Avignon, F-84911, France
FirstName.LastName@univ-avignon.fr
Abstract
We describe our system for the news com-
mentary translation task of WMT 2011. The
submitted run for the French-English direction
is a combination of two MOSES-based sys-
tems developed at LIG and LIA laboratories.
We report experiments to improve over the
standard phrase-based model using statistical
post-edition, information retrieval methods to
subsample out-of-domain parallel corpora and
ROVER to combine n-best list of hypotheses
output by different systems.
1 Introduction
This year, LIG and LIA have combined their efforts
to produce a joint submission to WMT 2011 for the
French-English translation task. Each group started
by developing its own solution whilst sharing re-
sources (corpora as provided by the organizers but
also aligned data etc) and acquired knowledge (cur-
rent parameters, effect of the size of n-grams, etc.)
with the other. Both LIG and LIA systems are stan-
dard phrase-based translation systems based on the
MOSES toolkit with appropriate carefully-tuned se-
tups. The final LIGA submission is a combination
of the two systems.
We summarize in Section 2 the resources used
and the main characteristics of the systems. Sec-
tions 3 and 4 describe the specificities and report
experiments of resp. the LIG and the LIA system.
Section 5 presents the combination of n-best lists
hypotheses generated by both systems. Finally, we
conclude in Section 6.
2 System overview
2.1 Used data
Globally, our system1 was built using all the French
and English data supplied for the workshop?s shared
translation task, apart from the Gigaword monolin-
gual corpora released by the LDC. Table 1 sums up
the used data and introduces designations that we
follow in the remainder of this paper to refer to cor-
pora. Four corpora were used to build translation
models: news-c, euro, UN and giga, while three
others are employed to train monolingual language
models (LMs). Three bilingual corpora were de-
voted to model tuning: test09 was used for the de-
velopment of the two seed systems (LIG and LIA),
whereas test08 and testcomb08 were used to tune the
weights for system combination. test10 was finally
put aside to compare internally our methods.
2.2 LIG and LIA system characteristics
Both LIG and LIA systems are phrase-based trans-
lation models. All the data were first tokenized with
the tokenizer provided for the workshop. Kneser-
Ney discounted LMs were built from monolingual
corpora using the SRILM toolkit (Stolcke, 2002),
while bilingual corpora were aligned at the word-
level using GIZA++ (Och and Ney, 2003) or its
multi-threaded version MGIZA++ (Gao and Vogel,
2008) for the large corpora UN and giga. Phrase
table and lexicalized reordering models were built
with MOSES (Koehn et al, 2007). Finally, 14 fea-
tures were used in the phrase-based models:
1When not specified otherwise ?our? system refers to the
LIGA system.
440
CORPORA DESIGNATION SIZE (SENTENCES)
English-French Bilingual training
News Commentary v6 news-c 116 k
Europarl v6 euro 1.8 M
United Nation corpus UN 12 M
109 corpus giga 23 M
English Monolingual training
News Commentary v6 mono-news-c 181 k
Shuffled News Crawl corpus (from 2007 to 2011) news-s 25 M
Europarl v6 mono-euro 1.8 M
Development
newstest2008 test08 2,051
newssyscomb2009 testcomb09 502
newstest2009 test09 2,525
Test
newstest2010 test10 2,489
Table 1: Used corpora
? 5 translation model scores,
? 1 distance-based reordering score,
? 6 lexicalized reordering score,
? 1 LM score and
? 1 word penalty score.
The score weights were optimized on the test09 cor-
pus according to the BLEU score with the MERT
method (Och, 2003). The experiments led specifi-
cally with either LIG or LIA system are respectively
described in Sections 3 and 4. Unless otherwise
indicated, all the evaluations were performed using
case-insensitive BLEU and were computed with the
mteval-v13a.pl script provided by NIST. Ta-
ble 2 summarizes the differences between the final
configuration of the systems.
3 The LIG machine translation system
LIG participated for the second time to the WMT
shared news translation task for the French-English
language pair.
3.1 Pre-processing
Training data were first lowercased with the PERL
script provided for the campaign. They were also
processed in order to normalize a special French
form (named euphonious ?t?) as described in (Potet
et al, 2010).
The baseline system was built using a 4-gram LM
trained on the monolingual corpora provided last
year and translation models trained on news-c and
euro (Table 3, System 1). A significant improve-
ment in terms of BLEU is obtained when taking into
account a third corpus, UN, to build translation mod-
els (System 2). The next section describes the LMs
that were trained using the monolingual data pro-
vided this year.
3.2 Language model training
Target LMs are standard 4-gram models trained
on the provided monolingual corpus (mono-news-c,
mono-euro and news-s). We decided to test two dif-
ferent n-gram cut-off settings. The fist set has low
cut-offs: 1-2-3-3 (respectively for 1-gram, 2-gram,
3-gram and 4-gram counts), whereas the second one
(LM2) is more aggressive: 1-5-7-7. Experiment re-
sults (Table 3, Systems 3 and 4) show that resorting
to LM2 leads to an improvement of BLEU with re-
spect to LM1. LM2 was therefore used in the sub-
sequent experiments.
441
FEATURES LIG SYSTEM LIA SYSTEM
Pre-processing Text lowercased Text truecasedNormalization of French euphonious
?t?
Reaccentuation of French words start-
ing with a capital letter
LM Training on mono-news-c, news-s and
mono-euro
Training on mono-news-c and news-s
4-gram models 5-gram models
Translation model
Training on news-c, euro and UN Training on 10 M sentence pairs se-
lected in news-c, euro, UN and giga
Phrase table filtering
Use of -monotone-at-punctuation op-
tion
Table 2: Distinct features between final configurations retained for the LIG and LIA systems
3.3 Translation model training
Translation models were trained from the parallel
corpora news-c, euro and UN. Data were aligned
at the word-level and then used to build standard
phrase-based translation models. We filtered the ob-
tained phrase table using the method described in
(Johnson et al, 2007). Since this technique drasti-
cally reduces the size of the phrase table, while not
degrading (and even slightly improving) the results
on the development and test corpora (System 6), we
decided to employ filtered phrase tables in the final
configuration of the LIG system.
3.4 Tuning
For decoding, the system uses a log-linear com-
bination of translation model scores with the LM
log-probability. We prevent phrase reordering over
punctuation using the MOSES option -monotone-at-
punctuation. As the system can be beforehand tuned
by adjusting the log-linear combination weights on
a development corpus, we used the MERT method
(System 5). Optimizing weights according to BLEU
leads to an improvement with respect to the sys-
tem with MOSES default value weights (System 5
vs System 4).
3.5 Post-processing
We also investigated the interest of a statistical
post-editor (SPE) to improve translation hypotheses.
About 9,000 sentences extracted from the news do-
main test corpora of the 2007?2009 WMT transla-
tion tasks were automatically translated by a sys-
tem very similar to that described in (Potet et al,
2010), then manually post-edited. Manual correc-
tions of translations were performed by means of the
crowd-sourcing platform AMAZON MECHANICAL
TURK2 ($0.15/sent.). These collected data make
a parallel corpus whose source part is MT output
and target part is the human post-edited version of
MT output. This are used to train a phrase-based
SMT (with Moses without the tuning step) that au-
tomatically post-edit the MT output. That aims at
learning how to correct translation hypotheses. Sys-
tem 7 obtained when post-processing MT 1-best out-
put shows a slight improvement. However, SPE was
not used in the final LIG system since we lacked
time to apply SPE on the N-best hypotheses for the
development and test corpora (the N-best being nec-
essary for combination of LIG and LIA systems).
Ths LIGA submission is thus a constrained one.
3.6 Recasing
We trained a phrase-based recaser model on the
news-s corpus using the provided MOSES scripts
and applied it to uppercase translation outputs. A
common and expected loss of around 1.5 case-
sensitive BLEU points was observed on the test cor-
pus (news10) after applying this recaser (System 7)
with respect to the score case-insensitive BLEU pre-
viously measured.
2http://www.mturk.com/mturk/welcome
442
? SYSTEM DESCRIPTION BLEU SCORE
test09 test10
1 Training: euro+news-c 24.89 26.01
2 Training: euro+news-c+UN 25.44 26.43
3 2 + LM1 24.81 27.19
4 2 + LM2 25.37 27.25
5 4 + MERT on test09 26.83 27.53
6 5 + phrase-table filtering 27.09 27.64
7 6 + SPE 27.53 27.74
8 6 + recaser 24.95 26.07
Table 3: Incremental improvement of the LIG system in
terms of case-insensitive BLEU (%), except for line 8
where case-sensitive BLEU (%) are reported
4 The LIA machine translation system
This section describes the particularities of the MT
system which was built at the LIA for its first partic-
ipation to WMT.
4.1 System description
The available corpora were pre-processed using
an in-house script that normalizes quotes, dashes,
spaces and ligatures. We also reaccentuated French
words starting with a capital letter. We significantly
cleaned up the crawled parallel giga corpus, keeping
19.3 M of the original 22.5 M sentence pairs. For ex-
ample, sentence pairs with numerous numbers, non-
alphanumeric characters or words starting with cap-
ital letters were removed. The whole training ma-
terial is truecased, meaning that the words occur-
ing after a strong punctuation mark were lowercased
when they belonged to a dictionary of common all-
lowercased forms; the others were left unchanged.
The training of a 5-gram English LM was re-
strained to the news corpora mono-news-c and news-
s that we consider large enough to ignore other data.
In order to reduce the size of the LM, we first limited
the vocabulary of our model to a 1 M word vocabu-
lary taking the most frequent words in the news cor-
pora. We also resorted to cut-offs to discard infre-
quent n-grams (2-2-3-5 thresholds on 2- to 5-gram
counts) and uses the SRILM option prune, which
allowed us to train the LM on large data with 32 Gb
RAM.
Our translation models are phrase-based models
(PBMs) built with MOSES with the following non-
default settings:
? maximum sentence length of 80 words,
? limit on the number of phrase translations
loaded for each phrase fixed to 30.
Weights of LM, phrase table and lexicalized re-
ordering model scores were optimized on the devel-
opment corpus thanks to the MERT algorithm.
Besides the size of used data, we experimented
with two advanced features made available for
MOSES. Firstly, we filtered phrase tables using the
default setting -l a+e -n 30. This dramatically
reduced phrase tables by dividing their size by a
factor of 5 but did not improve our best configu-
ration from the BLEU score perspective (Table 4,
line 1); the method was therefore not kept in the
LIA system. Secondly, we introduced reordering
constraints in order to consider quoted material as
a block. This method is particularly useful when ci-
tations included in sentences have to be translated.
Two configurations were tested: zone markups in-
clusion around quotes and wall markups inclusion
within zone markups. However, the measured gains
were finally too marginal to include the method in
the final system.
4.2 Parallel corpus subsampling
As the only news parallel corpus provided for the
workshop contains 116 k sentence pairs, we must
resort to parallel out-of-domain corpora in order to
build reliable translation models. Information re-
trieval (IR) methods have been used in the past to
subsample parallel corpora. For example, Hilde-
brand et al (2005) used sentences belonging to the
development and test corpora as queries to select the
k most similar source sentences in an indexed paral-
lel corpus. The retrieved sentence pairs constituted
a training corpus for the translation models.
The RALI submission for WMT10 proposed a
similar approach that builds queries from the mono-
lingual news corpus in order to select sentence pairs
stylistically close to the news domain (Huet et al,
2010). This method has the major interest that it
does not require to build a new training parallel
corpus for each news data set to translate. Fol-
lowing the best configuration tested in (Huet et al,
443
2010), we index the three out-of-domain corpora us-
ing LEMUR3, and build queries from English news-s
sentences where stop words are removed. The 10 top
sentence pairs retrieved per query are selected and
added to the new training corpus if they are not re-
dundant with a sentence pair already collected. The
process is repeated until the training parallel cor-
pus reaches a threshold over the number of retrieved
pairs.
Table 4 reports BLEU scores obtained with the
LIA system using the in-domain corpus news-c and
various amounts of out-of-domain data. MERT was
re-run for each set of training data. The first four
lines display results obtained with the same num-
ber of sentence pairs, which corresponds to the
size of news-c appended to euro. The experiments
show that using euro instead of the first sentences of
UN and giga significantly improves BLEU scores,
which indicates the better adequacy of euro with re-
spect to the test10 corpus. The use of the IR method
to select sentences from euro, UN and giga leads to
a similar BLEU score to the one obtained with euro.
The increase of the collected pairs up to 3 M pairs
generates a significant improvement of 0.9 BLEU
point. A further rise of the amount of collected
pairs does not introduce a major gain since retriev-
ing 10 M sentence pairs only augments BLEU from
29.1 to 29.3. This last configuration which leads to
the best BLEU was used to build the final LIA sys-
tem. Let us note that 2 M, 3 M and 15 M queries
were required to respectively obtain 3 M, 5 M and
10 M sentence pairs because of the removal of re-
dundant sentences in the increased corpus.
For a matter of comparison, a system was also
built taking into account all the training material,
i.e. 37 M sentence pairs4. This last system is out-
performed by our best system built with IR and has
finally close performance to the one obtained with
news-c+euro relatively to the quantity of used data.
5 The system combination
System combination is based on the 500-best out-
puts generated by the LIA and the LIG systems.
3www.lemurproject.org
4For this experiment, the data were split into three parts
to build independent alignment models: news-c+euro, UN and
giga, and they were joined afterwards to build translation mod-
els.
USED PARALLEL CORPORA FILTERING
without with
news-c + euro (1.77 M) 28.1 28.0
news-c + 1.77 M of UN 27.2 -
news-c + 1.77 M of giga 27.1 -
news-c + 1.77 M with IR 28.2 -
news-c + 3 M with IR 29.1 29.0
news-c + 5 M with IR 28.8 -
news-c + 10 M with IR 29.3 29.2
All data 28.9 29.0
Table 4: BLEU (%) on test10 measured with the LIA
system using different training parallel corpora
They both used the MOSES option distinct, en-
suring that the hypotheses produced for a given sen-
tence are different inside an N-best list. Each N-best
list is associated with a set of 14 scores and com-
bined in several steps.
The first step takes as input lowercased 500-best
lists, since preliminary experiments have shown a
better behavior using only lowercased output (with
cased output, combination presents some degrada-
tions). The score combination weights are opti-
mized on the development corpus, in order to max-
imize the BLEU score at the sentence level when
N-best lists are reordered according to the 14 avail-
able scores. To this end, we resorted to the SRILM
nbest-optimize tool to do a simplex-based
Amoeba search (Press et al, 1988) on the error func-
tion with multiple restarts to avoid local minima.
Once the optimized feature weights are com-
puted independently for each system, N-best lists
are turned into confusion networks (Mangu et al,
2000). The 14 features are used to compute poste-
riors relatively to all the hypotheses in the N-best
list. Confusion networks are computed for each sen-
tence and for each system. In Table 5 we present
the ROVER (Fiscus, 1997) results for the LIA and
LIG confusion networks (LIA CNC and LIG CNC).
Then, both confusion networks computed for each
sentence are merged into a single one. A ROVER
is applied on the combined confusion network and
generates a lowercased 1-best.
The final step aims at producing cased hypothe-
ses. The LIA system built from truecased corpora
achieved significantly higher performance than the
444
LIG LIA LIG CNC LIA CNC LIG+LIA
case-insensitive test10 27.6 29.3 28.1 29.4 29.7
BLEU test11 28.5 29.4 28.5 29.3 29.9
case-sensitive test10 26.1 28.4 27.0 28.4 28.7
BLEU test11 26.9 28.4 27.5 28.4 28.8
Table 5: Performance measured before and after combining systems
LIG system trained on lowercased corpora (Table 5,
two last lines). In order to get an improvement when
combining the outputs, we had to adopt the follow-
ing strategy. The 500-best truecased outputs of the
LIA system are first merged in a word graph (and
not a mesh lattice). Then, the lowercased 1-best
previously obtained with ROVER is aligned with the
graph in order to find the closest existing path, which
is equivalent to matching an oracle with the graph.
This method allows for several benefits. The new
hypothesis is based on a ?true? decoding pass gener-
ated by a truecased system and discarded marginal
hypotheses. Moreover, the selected path offers a
better BLEU score than the initial hypothesis with
and without case. This method is better than the one
which consists of applying the LIG recaser (section
3.6) on the combined (un-cased) hypothesis.
The new recased one-best hypothesis is then used
as the final submission for WMT. Our combination
approach improves on test11 the best single sys-
tem by 0.5 case-insensitive BLEU point and by 0.4
case-sensitive BLEU (Table 5). However, it also in-
troduces some mistakes by duplicating in particular
some segments. We plan to apply rules at the seg-
ment level in order to reduce these artifacts.
6 Conclusion
This paper presented two statistical machine trans-
lation systems developed at different sites using
MOSES and the combination of these systems. The
LIGA submission presented this year was ranked
among the best MT system for the French-English
direction. This campaign was the first shot for LIA
and the second for LIG. Beside following the tradi-
tional pipeline for building a phrase-based transla-
tion system, each individual system led to specific
works: LIG worked on using SPE as post-treatment,
LIA focused on extracting useful data from large-
sized corpora. And their combination implied to ad-
dress the interesting issue of matching results from
systems with different casing approaches.
WMT is a great opportunity to chase after perfor-
mance and joining our efforts has allowed to save
considerable amount of time for data preparation
and tuning choices (even when final decisions were
different among systems), yet obtaining very com-
petitive results. This year, our goal was to develop
state-of-the-art systems so as to investigate new ap-
proaches for related topics such as translation with
human-in-the-loop or multilingual interaction sys-
tems (e.g. vocal telephone information-query di-
alogue systems in multiple languages or language
portability of such systems).
References
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates:recognizer output vot-
ing error reduction (ROVER). In Proceedings of the
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347?354, Santa Barbara, CA,
USA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Proceedings of the
ACL Workshop: Software Engineering, Testing, and
Quality Assurance for Natural Language Processing,
pages 49?57, Columbus, OH, USA.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th confer-
ence of the European Association for Machine Trans-
lation (EAMT), Budapest, Hungary.
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry, and
Philippe Langlais. 2010. The RALI machine trans-
lation system for WMT 2010. In Proceedings of the
ACL Joint 5th Workshop on Statistical Machine Trans-
lation and Metrics (WMT), Uppsala, Sweden.
Howard Johnson, Joel Martin, George Foster, and Roland
445
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, jun.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), Companion Vol-
ume, pages 177?180, Prague, Czech Republic, June.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics (ACL), Sapporo, Japan.
Marion Potet, Laurent Besacier, and Herve? Blanchon.
2010. The LIG machine translation for WMT 2010.
In Proceedings of the ACL Joint 5th Workshop on Sta-
tistical Machine Translation and Metrics (WMT), Up-
psala, Sweden.
William H. Press, Brian P. Flannery, Saul A. Teukolsky,
and William T. Vetterling. 1988. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Denver, CO, USA.
446
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 72?81,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Concept Annotation using Latent Dirichlet Allocation and
Segmental Methods
Nathalie Camelin, Boris Detienne, Ste?phane Huet, Dominique Quadri and Fabrice Lefe`vre
LIA - University of Avignon, BP 91228
84911 Avignon Cedex 09, France
{nathalie.camelin,boris.detienne,stephane.huet,dominique.quadri,fabrice.lefevre}@univ-avignon.fr
Abstract
Training efficient statistical approaches for
natural language understanding generally re-
quires data with segmental semantic annota-
tions. Unfortunately, building such resources
is costly. In this paper, we propose an ap-
proach that produces annotations in an unsu-
pervised way. The first step is an implementa-
tion of latent Dirichlet alocation that produces
a set of topics with probabilities for each topic
to be associated with a word in a sentence.
This knowledge is then used as a bootstrap to
infer a segmentation of a word sentence into
topics using either integer linear optimisation
or stochastic word alignment models (IBM
models) to produce the final semantic anno-
tation. The relation between automatically-
derived topics and task-dependent concepts is
evaluated on a spoken dialogue task with an
available reference annotation.
1 Introduction
Spoken dialogue systems in the field of information
query are basically used to interface a database with
users using speech. When probabilistic models are
used in such systems, good performance can only be
reached at the price of collecting a lot of field data,
which must be transcribed and annotated at the se-
mantic level. It becomes then possible to train effi-
cient models in a supervised manner. However, the
annotation process is costly and as a consequence
represents a real difficulty hindering the widespread
development of these systems. Therefore any means
to avoid it would be profitable as portability to new
tasks, domains or languages would be greatly facili-
tated.
To give a full description of the architecture of a
dialogue system is out of the scope of this paper. In-
stead we limit ourselves to briefly recall that once
a speech recognizer has transcribed the signal it is
common (though avoidable for very simple tasks) to
use a module dedicated to extract the meaning of
the user?s queries. This meaning representation is
then conveyed to an interaction manager that decides
upon the next best action to perform considering the
current user?s input and the dialogue history. One
of the very first steps to build the spoken language
understanding (SLU) module is the identification of
literal concepts in the word sequence hypothesised
by the speech recogniser. An example of a semantic
representation in terms of literal concept is given in
Figure 1. Once the concepts are identified they can
be further composed to form the overall meaning of
the sentence, for instance by means of a tree repre-
sentation based on hierarchical semantic frames.
To address the issue of concept tagging several
techniques are available. Some of these techniques
now classical rely on probabilistic models, that can
be either discriminative or generative. Among these,
the most efficiently studied this last decade are: hid-
den Markov models, finite state transducers, max-
imum entropy Markov models, support vector ma-
chines, dynamic fields (CRF). In (Hahn et al, 2010)
it is shown that CRFs obtain the best performance on
a tourist information retrieval task in French (ME-
DIA (Bonneau-Maynard et al, 2005)), but also in
two other comparable corpora in Italian and Polish.
To be able to apply any such technique, basic con-
72
words concept normalized value
donnez-moi null
le refLink-coRef singular
tarif object payment-amount-room
puisque connectProp imply
je voudrais null
une chambre number-room 1
qui cou?te object payment-amount-room
pas plus de comparative-payment less than
cinquante payment-amount-integer-room 50
euros payment-unit euro
Figure 1: Semantic concept representation for the query ?give me the rate since I?d like a room charged not more than
fifty euros?.
cept units have to be defined by an expert. In the best
case, most of these concepts can be derived straight-
forwardly from the pieces of information lurking in
the database tables (mainly table fields but not ex-
clusively). Some others are general (dialogic units
but also generic entities such as number, dates, etc).
However, to provide an efficient and usable informa-
tion to the reasoning modules (the dialogue manager
in our case) concepts have to be fine-grained enough
and application-dependent (even general concepts
might have to be tailored to peculiar uses). To that
extent it seems out of reach to derive the concept
definitions using a fully automatic procedure. Any-
how the process can be bootstrapped, for instance
by induction of semantic classes such as in (Siu and
Meng, 1999) or (Iosif et al, 2006). Our assumption
here is that the most time-consuming parts of con-
cept inventory and data tagging could be obtained in
an unsupervised way even though a final (but hope-
fully minimal) manual procedure is still required to
tag the classes so as to manually correct automatic
annotation.
Unlike the previous attempts cited above which
developed ad hoc approaches, we investigate here
the use of broad-spectrum knowledge extraction
methods. The notion most related to that of concept
in SLU is the topic, as used in information retrieval
systems. Anyhow for a long time, the topic detec-
tion task was limited to associate a single topic to
a document and thus was not fitted to our require-
ments. The recently proposed LDA technique al-
lows to have a probabilistic representation of a doc-
ument as a mixture of topics. Then multiple topics
can co-occur inside a document and the same topic
can be repeated. From these characteristics it is pos-
sible to consider the application of LDA to unsu-
pervised concept inventory and concept tagging for
SLU. A shortcoming is that LDA does not modelize
at all the sequentiality of the data. To address this is-
sue we propose to conclude the procedure with a fi-
nal step to introduce specific constraints for a correct
segmentation of the data: the assignments of topics
proposed by LDA are modified to be more segmen-
tally coherent.
The paper is organised as follows. Principles
of automatic induction of semantic classes are pre-
sented in Section 2, followed by the presentation of
an induction system based on LDA. The additional
step of segmentation is presented in Section 3 with
two variants: stochastic word alignment (GIZA) and
integer linear programming (ILP). Then evaluations
and results are reported in Section 4 on the French
MEDIA dialogue task.
2 Automatic induction of semantic classes
2.1 Context modeling
The idea of automatic induction of semantic classes
is based on the assumption that concepts often share
the same context (syntactic or lexical). Imple-
mented systems are based on the observation of co-
occurring words according to two different ways.
The observation of consecutive words (bigrams or
trigrams) enables the generation of lexical com-
pounds supposed to follow syntactic rules. The com-
parison of right and left contexts considering pairs
of words enables to cluster words (and word com-
pounds) into semantic classes.
73
In (Siu and Meng, 1999) and (Pargellis et al,
2001), iterative systems are presented. Their im-
plementations differ in the metrics chosen to eval-
uate the similarity during the generation of syntactic
rules and semantic classes, but also in the number
of words taken into account in a word context and
the order of successive steps (which ones to gener-
ate first: syntactic rules or semantic classes?). An
iterative procedure is executed to obtain a sufficient
set of rules in order to automatically extract knowl-
edge from the data.
While there may be still room for improvement in
these techniques we decided instead to investigate
general knowledge extraction approaches in order to
evaluate their potential. For that purpose a global
strategy based on an unsupervised machine learning
technique is adopted in our work to produce seman-
tic classes.
2.2 Implementation of an automatic induction
system based on LDA
Several approaches are available for topic detection
in the context of knowledge extraction and informa-
tion retrieval. They all more or less rely on the pro-
jection of the documents of interest in a semantic
space to extract meaningful information. However,
as the considered spaces (initial document words
and latent semantics) are discrete the performance
of the proposed approaches for the topic extraction
tasks are pretty unstable, and also greatly depend on
the quantity of data available. In this work we were
motivated by the recent development of a very at-
tractive technique with major distinct features such
as the detection of multiple topics in a single docu-
ment. LDA (Blei et al, 2003) is the first principled
description of a Dirichlet-based model of mixtures
of latent variables. LDA will be used in our work
to annotate the dialogue data in terms of topics in
an unsupervised manner. Then the relation between
automatic topics and expected concepts will be ad-
dressed manually.
Basically LDA is a generative probabilistic model
for text documents. LDA follows the assumption
that a set of observations can be explained by latent
variables. More specifically documents are repre-
sented by a mixture of topics (latent variables) and
topics are characterized by distributions over words.
The LDA parameters are {?, ?}. ? represents the
Dirichlet parameters of K latent topic mixtures as
? = [?1, ?2, . . . , ?K ]. ? is a matrix representing a
multinomial distribution in the form of a conditional
probability table ?k,w = P (w|k). Based on this rep-
resentation, LDA can estimate the probability of a
new document d of N words d = [w1, w2, . . . , wN ]
using the following procedure.
A topic mixture vector ? is drawn from the Dirich-
let distribution (with parameter ?). The correspond-
ing topic sequence ? = [k1, k2, . . . , kN ] is generated
for the whole document accordingly to a multino-
mial distribution (with parameter ?). Finally each
word is generated by the word-topic multinomial
distribution (with parameter ?, that is p(wi|ki, ?)).
After this procedure, the joint probability of ?, ? and
d is then:
p(?, ?, d|?, ?) = p(?|?)
N?
i=1
p(ki|?)p(wi|ki, ?)
(1)
To obtain the marginal probability of d, a final in-
tegration over ? and a summation over all possible
topics considering a word is necessary:
p(d|?, ?) =
?
p(?|?)
?
?
N?
i=1
?
ki
p(ki|?)p(wi|ki, ?)
?
?
(2)
The framework is comparable to that of probabilis-
tic latent semantic analysis, but the topic multino-
mial distribution in LDA is assumed to be sampled
from a Dirichlet prior and is not linked to training
documents. This approach is illustrated in Figure 2.
Training of the ? and ? parameters is possible us-
ing a corpus of documents, with a fixed number of
topics to predict. A variational inference procedure
is described in (Blei et al, 2003) which alleviates
the intractability due to the coupling between ? and
? in the summation over the latent topics. Once the
parameters for the Dirichlet and multinomial distri-
butions are available, topic scores can be derived for
any given document or word sequence.
In recent years, several studies have been carried
out in language processing based on LDA. For in-
stance, (Tam and Schultz, 2006) worked on unsuper-
vised language model adaptation; (Celikyilmaz et
al., 2010) ranked candidate passages in a question-
answering system; (Phan et al, 2008) implemented
LDA to classify short and sparse web texts.
74
LATENT DIRICHLET ALLOCATION
?                                    k w?
?
M
N
Figure 1: Graphical model representation of LDA. The boxes are ?plates? representing replicates.
The outer plate represents documents, while the inner plate represents the repeated choice
of topics and words within a document.
where p(zn | ?) is simply ? i for the unique i such that zin = 1. Integrating over ? and summing over
z, we obtain the marginal distribution of a document:
p(w | ?, ?) =
?
p(? | ?)
(
N?
n=1
?
zn
p(zn | ?) p(wn |zn, ?)
)
d ?. (3)
Finally, taking the product of the marginal probabilities of single documents, we obtain the proba-
bility of a corpus:
p(D | ?, ?) = M?
d=1
?
p(? d | ?)
(
Nd?
n=1
?
zdn
p(zdn | ? d)p(wdn |zdn, ?)
)
d ? d .
The LDA model is represented as a probabilistic graphical model in Figure 1. As the figure
makes clear, there are three levels to the LDA representation. The parameters ? and ? are corpus-
level parameters, assumed to be sampled once in the process of generating a corpus. The variables
? d are document-level variables, sampled once per document. Finally, the variables zdn and wdn are
word-level variables and are sampled once for each word in each document.
It is important to distinguish LDA from a simple Dirichlet-multinomial clustering model. A
classical clustering model would involve a two-level model in which a Dirichlet is sampled once
for a corpus, a multinomial clustering variable is selected once for each document in the corpus,
and a set of words are selected for the document conditional on the cluster variable. As with many
clustering models, such a model restricts a document to being associated with a single topic. LDA,
on the other hand, involves three levels, and notably the topic node is sampled repeatedly within the
document. Under this model, documents can be associated with multiple topics.
Structures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling,
where they are referred to as hierarchical models (Gelman et al, 1995), or more precisely as con-
ditionally independent hierarchical models (Kass and Steffey, 1989). Such models are also often
referred to as parametric empirical Bayes models, a term that refers not only to a particular model
structure, but also to the methods used for estimating parameters in the model (Morris, 1983). In-
deed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters
such as ? and ? in simple implementations of LDA, but we also consider fuller Bayesian approaches
as well.
997
Figure 2: Graphical representation for LDA variables
(from (Blei et al, 2003)). The grey circle is the only ob-
servable variable.
In our work LDA is employed to annotate each
user?s utterance of a dialogue corpus with topic. Ut-
terances longer than one word are included in the
training set as its sequence of words. Once the
model has been trained, inference on data corpus as-
signs the topic with the highest probability to each
word in a document. This probability is computed
from the probability of the topic to appear in the doc-
ument and the probability of the word to be gener-
ated by th opic. As a con equence we obtain a full
topic annotati n of the utterance.
Notice that LDA considers a user utterance as a
bag of words. This implies that each topic is as-
signed to a word without any consideration for its
i m diate co text. An additional segmental process
is required if we want to introduce some context in-
formation in the topic assignment.
3 Segmental annotation
3.1 Benefits of a segmental annotation
The segmental annotation of the data is not a strict
requirement for language understanding. Up to quite
recently, most approaches for literal interpretation
were limited to lexical-concept relations; for in-
stance this is the case of the Phoenix system (Ward,
1991) based on the detection of keywords. However
in an NLP perspective, the segmental approach al-
lows to connect the various levels of sentence analy-
sis (lexical, syntactic and semantic). Even though, in
order to simplify its application, segments are gen-
erally designed specifically for the semantic anno-
tation and do not have any constraint on their rela-
tion with the actual syntactic units (chunks, phrasal
groups, etc). To get relieved of such constraints not
only simplifies the annotation process itself but as
ultimately the interpretation module is to be used in-
side a spoken dialogue system, data will be noisy
and generally bound the performance of the syn-
tactic analysers (due to highly spontaneous and un-
grammatical utterances from the users, combined
with errors from the speech recognizer).
Another interesting property of segmental ap-
proach is to offer a convenient way to dissociate the
detection of a conceptual unit from the extraction of
its associated value. The value corresponds to the
nor alisation of the surface form (see last column
in 1); for instance if the segment ?not more than?
is associated to the concept comparative-payment,
its value is ?less than?. The same value would
be associated to ?not exceeding? or ?inferior to?.
Value extraction requires a link between concepts
and words based on which the normalisation prob-
lem can be addressed by means of regular expres-
sions or concept-dependent language models (even
allowing integrated approaches such as described
in (Lefe`vre, 2007)). In the case of global approaches
(not segmental), value extraction must be dealt with
directly at the level of the conceptual unit tagging,
as in (Mairesse et al, 2009). This additional level is
very complex (as some values may not be enumer-
able, such as numbers and dates) and is only afford-
able when the number of authorised values (for the
enumerable cases) is low.
To refine the LDA output, the topic-to-word align-
ment is discarded and an automatic procedure is
used to derive the best alignment between topics and
words. While the underlying probabilistic models
are pretty comparable, the major interest of this ap-
pro ch is to eparate the tasks of detecting topics and
aligning topics with words. It is then possible to in-
troduce additional constraints (such as locality, num-
ber of segments, limits on repetitions etc) in the lat-
ter task which would otherwise hinder topic detec-
tion. Conversely the alignment is self-coherent and
able to question the associations proposed during
topic detection with respect to its own constraints
only. Two approaches were designed to this pur-
pose: one based on IBM alignment models and an-
other one based on integer linear optimisation.
75
3.2 Alignment with IBM models (GIZA)
Once topic assignments for the documents in the
corpus have been proposed by LDA, a filtering pro-
cess is done to keep only the most relevant topics
of each document. The ?max most probable top-
ics are kept according to the probability p(k|wi, d)
that topic k generated the word wi of the document
d. ?max is a value fixed empirically according to
the expected set of topics in a sentence. Then, the
obtained topic sequences are disconnected from the
words. At this point, the topic and word sequences
can be considered as a translation pair to produce
a word-topic parallel corpus. These data can be
used with classical approaches in machine transla-
tion to align source and target sentences at the word
level. Since these alignment models can align sev-
eral words with a single topic, only the first occur-
rence is kept for consecutive repetitions of the same
topic. These models are expected to correct some er-
rors made by LDA, and to assign in particular words
previously associated with discarded topics to more
likely ones.
In our experiments the statistical word alignment
toolkit GIZA++ (Och and Ney, 2003) is used to
train the so-called IBM models 1-4 as well as the
HMM model. To be able to train the most informa-
tive IBM model 4, the following training pipeline
was considered: 5 iterations of IBM1, 5 iterations
of HMM, 3 iterations of IBM3 and 3 iterations of
IBM4. The IBM4 model obtained at the last iter-
ation is finally used to align words and topics. In
order to improve alignment, IBM models are usu-
ally trained in both directions (words towards con-
cepts and vice versa) and symmetrised by combin-
ing them. For this purpose, we resorted to the default
symmetrization heuristics used by MOSES, a widely
used machine translation system toolkit (Koehn et
al., 2007).
3.3 Alignment with Integer Linear
Programming (ILP)
Another approach to the re-alignment of LDA out-
puts is based on a general optimisation technique.
ILP is a widely used tool for modelling and solv-
ing combinatorial optimisation problems. It broadly
aims at modelling a decision process as a set of equa-
tions or inequations (called constraints) which are
linear with regards to so-called decision variables.
An ILP is also composed of a linear objective func-
tion. Solving an ILP consists in assigning values to
decision variables, such that all constraints are sat-
isfied and the objective function is optimised. We
refer to (Chen et al, 2010) for an overview of appli-
cations and methods of ILP.
We provide two ILP formulations for solving the
topic assignment problem related to a given docu-
ment. They both take as input data an ordered set d
of words wi, i = 1...N , a set of K available topics
and, for each word wi ? d and topic k = 1...K,
the natural logarithm of the probability p(k|wi, d)
that k is assigned to wi in the considered document
d. Model [ILP ] simply finds the highest-probability
assignment of one topic to each word in the doc-
ument, such that at most ?max different topics are
assigned.
[ILP ] : max
N?
i=1
K?
k=1
log(p(k|wi, d)) xik (3)
?K
k=1 xik = 1 i (4)
yk ? xik ? 0 i, k (5)?K
k=1 yk ? ?max (6)
xik ? {0, 1} i, k
yk ? {0, 1} k
In this model, decision variable xik is equal to 1 if
topic k is assigned to word wi, and equal to 0 other-
wise. Constraints (4) ensure that exactly one topic is
assigned to each word. Decision variable yk is equal
to 1 if topic k is used. Constraints (5) force vari-
able yk to take a value of 1 if at least one variable
xik is not null. Moreover, Constraints (6) limit the
total number of topics used. The objective function
(3) merely states that we want to maximize the total
probability of the assignment. Through this model,
our assignment problem is identified as a p-centre
problem (see (ReVelle and Eiselt, 2005) for a survey
on such location problems).
Numerical experiments show that [ILP ] tends to
give sparse assignments: most of the time, adja-
cent words are assigned to different topics even if
the total number of topics is correct. To prevent
this unnatural behaviour, we modified [ILP ] to con-
sider groups of consecutive words instead of isolated
76
words. Model [ILP seg] partitions the document
into segments of consecutive words, and assigns one
topic to each segment, such that at most ?max seg-
ments are created. For the sake of convenience, we
denote by p?(k|wij , d) =
?j
l=i log(p(k|wl, d)) the
logarithm of the probability that topic k is assigned
to all words from i to j in the current document.
[ILP seg] : max
N?
i=1
N?
j=i
K?
k=1
p?(k|wij , d) xijk (7)
i?
j=1
N?
l=i
K?
k=1
xjlk = 1 i (8)
N?
i=1
N?
j=i
K?
k=1
xijk ? ?max (9)
xijk ? {0, 1} i, j, k
In this model, decision variable xijk is equal to 1
if topic k is assigned to all words from i to j, and
0 otherwise. Constraints (8) ensure that each word
belongs to a segment that is assigned a topic. Con-
straints (9) limit the number of segments. Due to
the small size of the instances considered in this pa-
per, both [ILP ] and [ILP seg] are well solved by a
direct application of an ILP solver.
4 Evaluation and results
4.1 MEDIA corpus
The MEDIA corpus is used to evaluate the pro-
posed approach and to compare the various con-
figurations. MEDIA is a French corpus related to
the domain of tourism information and hotel book-
ing (Bonneau-Maynard et al, 2005). 1,257 dia-
logues were recorded from 250 speakers with a wiz-
ard of Oz technique (a human agent mimics an auto-
matic system). This dataset contains 17k user utter-
ances and 123,538 words, for a total of 2,470 distinct
words.
The MEDIA data have been manually transcribed
and semantically annotated. The semantic annota-
tion uses 75 concepts (e.g. location, hotel-state,
time-month. . . ). Each concept is supported by a se-
quence of words, the concept support. The null con-
cept is used to annotate every words segment that
does not support any of the 74 other concepts (and
does not bear any information wrt the task). On aver-
age, a concept support contains 2.1 words, 3.4 con-
cepts are included in a utterance and 32% of the ut-
terances are restrained to a single word (generally
?yes? or ?no?). Table 1 gives the proportions of ut-
terances according to the number of concepts in the
utterance.
# concepts 1 2 3 [4,72]
% utterances 49.4 14.1 7.9 28.6
Table 1: Proportion of user utterances as a function of the
number of concepts in the utterance.
Notice that each utterance contains at least one
concept (the null label being considered as a con-
cept). As shown in Table 2, some concepts are sup-
ported by few segments. For example, 33 concepts
are represented by less than 100 concept supports.
Considering that, we can foresee that finding these
poorly represented concepts will be hard for LDA.
[1,100[ [100,500[ [500,1k[ [1k,9k[ [9k,15k]
33 21 6 14 1 (null)
Table 2: Number of concepts according to their occur-
rence range.
4.2 Evaluation protocol
Unlike previous studies, we chose a fully automatic
way to evaluate the systems. In (Siu and Meng,
1999), a manual process is introduced to reject in-
duced classes or rules that are not relevant to the
task and also to name the semantic classes with the
appropriate label. Thus, they were able to evaluate
their semi-supervised annotation on the ATIS cor-
pus. In (Pargellis et al, 2001), the relevance of the
generated semantic classes was manually evaluated
giving a mark to each induced semantic rule.
To evaluate the unsupervised procedure it is nec-
essary to associate each induced topic with a MEDIA
concept. To that purpose, the reference annotation
is used to align topics with MEDIA concepts at the
word level. A co-occurrence matrix is computed and
each topic is associated with its most co-occurring
concept.
As MEDIA reference concepts are very fine-
grained, we also define a high-level concept hier-
77
archy containing 18 clusters of concepts. For ex-
ample, a high-level concept payment is created from
the 4 concepts payment-meansOfPayment, payment-
currency, payment-total-amount, payment-approx-
amount; a high-level concept location corresponds
to 12 concepts (location-country, location-district,
location-street, . . . ). Thus, two levels of concepts
are considered for the evaluation: high-level and
fine-level.
The evaluation is presented in terms of the classi-
cal F-measure, defined as a combination of precision
and recall measures. Two levels are also considered
to measure topic assignment quality:
? alignment corresponds to a full evaluation
where each word is considered and associated
with one topic;
? generation corresponds to the set of topics gen-
erated for a turn (no order, no word-alignment).
4.3 System descriptions
Four systems are evaluated in our experiments.
[LDA] is the result of the unsupervised learning
of LDA models using GIBBSLDA++ tool1. It as-
signs the most probable topic to each word occur-
rence in a document as described in Section 2.2.
This approach requires prior estimation of the num-
ber of clusters that are expected to be found in the
data. To find an optimal number of clusters, we ad-
justed the number K of topics around the 75 ref-
erence concepts. 2k training iterations were made
using default values for ? and ?.
[GIZA] is the system based on the GIZA++
toolkit2 which re-aligns for each sentence the topic
sequence assigned by [LDA] to word sequence as
described in Section 3.2.
[ILP ] and [ILP seg] systems are the results of
the ILP solver IBM ILOG CPLEX3 applied to the
models described in Section 3.3.
For the three last systems, the value ?max has to
be fixed according to the desired concept annota-
tion. As on average a concept support contains 2.1
words, ?max is defined empirically according to the
number of words: with i = [[2, 4]]: ?max = i with
1http://gibbslda.sourceforge.net/
2http://code.google.com/p/giza-pp/
3http://www-01.ibm.com/software/integration/optimization/cplex-
optimizer/
 56 57 58 59 60 61 62 63 64 65 66 67
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 3: F-measure of the high-level concept generation
as a function of the number of topics.
 44 46 48 50 52 54 56
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 4: F-measure of the high-level concept alignment
as a function of the number of topics.
i = [[5, 10]] words: ?max = i? 2 and for utterances
containing more than 10 words: ?max = i/2.
For the sake of simplicity, single-word utterances
are processed separately with prior knowledge. City
names, months, days or answers (e.g. ?yes?, ?no?,
?yeah?) and numbers are identified in these one-
word utterances.
4.4 Results
Examples of topics generated by [LDA], with K =
100 topics, are shown in Table 3.
Plots comparing the different systems imple-
mented w.r.t. the different evaluation levels in terms
of F-measure are reported in Figures 3, 4, 5 and 6
(high-level vs fine-level, alignment vs generation).
The [LDA] system generates topics which are
78
Topic 0 Topic 13 Topic 18 Topic 35 Topic 33 Topic 43
information time-date sightseeing politeness location answer-yes
words prob. words prob. words prob. words prob. words prob. words prob.
d? 0.28 du 0.16 de 0.30 au 0.31 de 0.30 oui 0.62
plus 0.17 au 0.11 la 0.24 revoir 0.27 Paris 0.12 et 0.02
informations 0.16 quinze 0.08 tour 0.02 madame 0.09 la 0.06 absolument 0.008
autres 0.10 dix-huit 0.07 vue 0.02 merci 0.08 pre`s 0.06 autre 0.008
de?tails 0.03 de?cembre 0.06 Eiffel 0.02 bonne 0.01 proche 0.05 donc 0.007
obtenir 0.03 mars 0.06 sur 0.02 journe?e 0.01 Lyon 0.03 jour 0.005
alors 0.01 dix-sept 0.04 mer 0.01 villes 0.004 aux 0.02 Notre-Dame 0.004
souhaite 0.003 nuits 0.04 sauna 0.01 biento?t 0.003 gare 0.02 d?accord 0.004
Table 3: Examples of topics discovered by LDA (K = 100).
 47 48 49 50 51 52 53 54 55 56 57 58
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 5: F-measure of the fine-level concept generation
as a function of the number of topics.
correctly correlated with the high-level concepts. It
can be observed that the bag of 75 topics reaches
an F-measure of 61.5% (Fig. 3). When not enough
topics are required from [LDA], induced topics are
too wide to fit the fine-grained concept annotation of
MEDIA. On the other hand if too many topics are re-
quired, the performance of bag of high-level topics
stays the same while a substantial decrease of the
F-measure is observed in the alignment evaluation
(Fig. 4). This effect can be explained by the auto-
matic alignment method chosen to transpose topics
into reference concepts. Indeed, the increase of the
number of topics makes them co-occur with many
concepts, which often leads to assign them to the
most frequent concept null in the studied corpus.
From the high-level to fine-level concept evalua-
tions, results globally decrease by 10%. An addi-
tional global loss of 10% is also observed for both
the generation and alignment scorings. In the fine-
 34 36 38 40 42 44 46 48
 50
 100
 150
 200
Fmeasure
Numb
er of t
opics
GIZA ILP ILP_s
eg LDA
Figure 6: F-measure of the fine-level concept alignment
as a function of the number of topics.
level evaluation, a maximum F-measure of 52.2%
is observed for the generation of 75 topics (Fig. 5)
whereas the F-measure decreases to 41.5% in the
alignment evaluation (Fig. 6).
To conclude on the [LDA] system, we can see that
it generates topics having a good correlation with the
high-level concepts, seemingly the best representa-
tion level between topics and concepts. From these
results it seems obvious that an additional step is
needed to obtain a more accurate segmental annota-
tion, which is expected with the following systems.
The [GIZA] system improves the results. It is
very likely that the filtering process helps to dis-
card the irrelevant topics. Therefore, the automatic
alignment between words and the filtered topics in-
duced by [LDA] with IBM models seems more ro-
bust when more topics (a higher value for K) is re-
quired from [LDA], specifically in high-level con-
cept alignment (Fig. 4).
79
Systems based on the ILP technique perform bet-
ter than other systems whatever the evaluation. Con-
sidering [LDA] as the baseline, we can expect sig-
nificant gains of performance. For example, an F-
measure of 66% is observed for the ILP systems
considering the high-level concept generation for 75
topics (Figure 4), where the maximum for [LDA]
was 61.5%, and an F-measure of 55% is observed
(instead of 50.5% for [LDA]) considering the high-
level concept alignment.
No significant difference was finally measured be-
tween both ILP models for the concept generation
evaluations. Even though [ILP seg] seems to ob-
tain slightly better results in the alignment evalua-
tion. This could be expected since [ILP seg] intrin-
sically yields alignments with grouped topics, closer
to the reference alignment used for the evaluation.
It is worth noticing that unlike [LDA] system be-
haviour, the results of [ILP ] are not affected when
more topics are generated by [LDA]. A large num-
ber of topics enables [ILP ] to pick up the best topic
for a given segment among in a longer selection list.
As for [LDA], the same losses are observed be-
tween high-level and fine-level concepts and gener-
ation and alignment paradigms. Nevertheless, an F-
measure of 54.8% is observed at the high-level con-
cept in alignement evaluation (Figure 4) that corre-
sponds to a precision of 56.2% and a recall of 53.5%,
which is not so low considering a fully-automatic
high-level annotation system.
5 Conclusions and perspectives
In this paper an unsupervised approach for con-
cept extraction and segmental annotation has been
proposed and evaluated. Based on two steps
(topic inventory and assignment with LDA, then re-
segmentation with either IBM alignment models or
ILP) the technique has been shown to offer perfor-
mance above 50% for the retrieval of reference con-
cepts. It confirms the applicability of the technique
to practical tasks with an expected gain in data pro-
duction.
Future work will investigate the use of n-grams
to increase LDA accuracy to provide better hypothe-
ses for the following segmentation method. Besides,
other levels of data representation will be examined
(use of lemmas, a priori semantic classes like city
names. . . ) in order to better generalise on the data.
ACKNOWLEDGEMENTS
This work is supported by the ANR funded project
PORT-MEDIA (www.port-media.org) and the LIA
OptimNLP project (www.lia.univ-avignon.fr).
References
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
and D. Mostefa. 2005. Semantic annotation of the
french media dialog corpus. In Proceedings of the 9th
European Conference on Speech Communication and
Technology.
A. Celikyilmaz, D. Hakkani-Tur, and G. Tur. 2010. Lda
based similarity modeling for question answering. In
Proceedings of the NAACL HLT 2010 Workshop on Se-
mantic Search, pages 1?9. Association for Computa-
tional Linguistics.
Der-San Chen, Robert G. Batson, and Yu Dang. 2010.
Applied Integer Programming: Modeling and Solu-
tion. Wiley, January.
Stefan Hahn, Marco Dinarelli, Christian Raymond, Fab-
rice Lefvre, Patrick Lehnen, Renato De Mori, Alessan-
dro Moschitti, Hermann Ney, and Giuseppe Riccardi.
2010. Comparing stochastic approaches to spoken
language understanding in multiple languages. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, PP(99):1.
E. Iosif, A. Tegos, A. Pangos, E. Fosler-Lussier, and
A. Potamianos. 2006. Unsupervised combination of
metrics for semantic class induction. In Proceedings
of the IEEE Spoken Language Technology Workshop,
pages 86?89.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL, Companion Volume, pages 177?180,
Prague, Czech Republic.
F. Lefe`vre. 2007. Dynamic bayesian networks and
discriminative classifiers for multi-stage semantic in-
terpretation. In Proceedings of ICASSP, Honolulu,
Hawai.
F. Mairesse, M. Gas?ic?, F. Jurc???c?ek, S. Keizer, B. Thom-
son, K. Yu, and S. Young. 2009. Spoken language
80
understanding from unaligned data using discrimina-
tive classification models. In Proceedings of ICASSP,
Taipei, Taiwan.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
A. Pargellis, E. Fosler-Lussier, A. Potamianos, and C.H.
Lee. 2001. Metrics for measuring domain indepen-
dence of semantic classes. In Proceedings of the 7th
European Conference on Speech Communication and
Technology.
X.H. Phan, L.M. Nguyen, and S. Horiguchi. 2008.
Learning to classify short and sparse text & web with
hidden topics from large-scale data collections. In
Proceeding of the 17th international conference on
World Wide Web, pages 91?100. ACM.
C. S. ReVelle and H. A. Eiselt. 2005. Location analysis:
A synthesis and survey. European Journal of Opera-
tional Research, 165(1):1?19, August.
K.C. Siu and H.M. Meng. 1999. Semi-automatic acqui-
sition of domain-specific semantic structures. In Pro-
ceedings of the 6th European Conference on Speech
Communication and Technology.
Y.C. Tam and T. Schultz. 2006. Unsupervised language
model adaptation using latent semantic marginals. In
Proceedings of INTERSPEECH, pages 2206?2209.
W Ward. 1991. Understanding Spontaneous Speech.
In Proceedings of ICASSP, pages 365?368, Toronto,
Canada.
81
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 97?104,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Alignment for Segmental-based Language Understanding
St?phane Huet and Fabrice Lef?vre
Universit? d?Avignon, LIA-CERI, France
{stephane.huet,fabrice.lefevre}@univ-avignon.fr
Abstract
Recent years? most efficient approaches for
language understanding are statistical. These
approaches benefit from a segmental semantic
annotation of corpora. To reduce the produc-
tion cost of such corpora, this paper proposes
a method that is able to match first identified
concepts with word sequences in an unsuper-
vised way. This method based on automatic
alignment is used by an understanding sys-
tem based on conditional random fields and
is evaluated on a spoken dialogue task using
either manual or automatic transcripts.
1 Introduction
One of the very first step to build a spoken language
understanding (SLU) module for dialogue systems
is the extraction of literal concepts from word se-
quences hypothesised by a speech recogniser. To
address this issue of concept tagging, several tech-
niques are available. These techniques rely on mod-
els, now classic, that can be either discriminant
or generative. Among these, we can cite: hidden
Markov models, finite state transducers, maximal
entropy Markov models, support vector machines,
dynamic Bayesian networks (DBNs) or conditional
Markov random fields (CRFs) (Lafferty et al, 2001).
In (Hahn et al, 2011), it is shown that CRFs obtain
the best performance on a reference task (MEDIA) in
French (Bonneau-Maynard et al, 2005), but also on
two other comparable corpora in Italian and Polish.
Besides, the comparison of the understanding results
of manually vs automatically transcribed utterances
has shown the robustness of CRFs.
Among the approaches evaluated in (Hahn et al,
2011) was a method using log-linear models compa-
rable to those used in stochastic machine translation,
which turned out to have lower performance than
CRF. In this paper, we further exploit the idea of ap-
plying automatic translation techniques to language
understanding but limiting ourselves to the objective
of obtaining a segmental annotation of training data.
In many former approaches literal interpretation
was limited to list lexical-concept relations; for in-
stance this is the case of the PHOENIX system (Ward,
1991) based on the detection of keywords. The
segmental approach allows a finer-grained analysis
considering sentences as segment sequences during
interpretation. This characteristic enables the ap-
proach to correctly connect the various levels of
sentence analysis (lexical, syntactic and semantic).
However, in order to simplify its practical appli-
cation, segments have been designed specifically
for semantic annotation and do not integrate any
constraint in their relation with the syntactic units
(chunks, phrasal groups, etc.). Not only it simpli-
fies the annotation process itself but as the overall
objective is to use the interpretation module inside
a spoken dialogue system, transcribed speech data
are noisy and generally bound the performance of
syntactic analysers (due to highly spontaneous and
ungrammatical utterances from the users, combined
with errors from the speech recognizer).
Among other interesting proprieties, segmental
approaches offer a convenient way to dissociate the
detection of a conceptual unit from the estimation of
its associated value. The value corresponds to the
normalisation of the surface form. For instance, if
97
the segment ?no later than eleven? is associated with
the concept departure-time, its value is ?morn-
ing?; the same value is associated with the segments
?between 8 and noon? or ?in the morning?. The
value estimation requires a link between concepts
and sentence words. Then it becomes possible to
treat the normalisation problem by means of regular
expressions or concept-dependent language models
(allowing an integrated approach such as described
in (Lef?vre, 2007)). In the case of global approaches
(not segmental), value detection must be directly
incorporated in the conceptual units to identify, as
in (Mairesse et al, 2009). The additional level is a
real burden and is only affordable when the number
of authorised values is low.
Obviously a major drawback of the approach is its
cost: associating concept tags with a dialogue tran-
scription is already a tedious task and its complexity
is largely increased by the requirement for a precise
delimitation of the support (lexical segment) corre-
sponding to each tag. The SLU evaluation campaign
MEDIA has been the first opportunity to collect and
distribute a reasonably-sized corpus endowed with
segmental annotations.
Anyhow the difficulty remains unchanged each
time a corpus has to be collected for a new task.
We propose in this study a new method that reduces
the effort required to build training data for segmen-
tal annotation models. Making the assumption that
the concepts evoked in a sentence are automatically
detected beforehand or provided by an expert, we
study how to associate them with their lexical sup-
ports without prior knowledge. A conceptual seg-
mental annotation is obtained using alignment tech-
niques designed to align multilingual parallel cor-
pora in the machine translation domain. This anno-
tation can be considered as unsupervised since it is
done without a training corpus with links between
word sequences and concepts.
We present in the paper the necessary adaptations
for the application of the alignment techniques in
this new context. They have been kept to their mini-
mal so as to maintain the highest level of generality,
which in return benefits from the availability of ex-
isting software tools. Using a reference annotation,
we evaluate the alignment quality from the unsuper-
vised approach in two interesting situations depend-
ing on whether the correct order of the concepts is
known or not. Finally, the end-to-end evaluation of
the approach is made by measuring the impact of the
alignments on the CRF-based understanding system.
After a brief recall of the conceptual decoding
principles in Section 2, the principles of automatic
alignment of parallel corpora are described in Sec-
tion 3 along with the specificities due to the align-
ment of semantic concepts. Section 4 presents the
experiments and comments on the results, while
Section 5 concludes the paper.
2 Segmental conceptual decoding
If literal interpretation can be seen as the transla-
tion of natural language to the set of semantic tag
sequences, then the methods and models of machine
translation can be used. Since the number of con-
cepts is generally much lower than the vocabulary
size, this particular type of translation can also be
considered as a mere classification problem in which
the conceptual constituents represent the class to
identify. Interpretation can thus be performed by
methods and models of classification.
Discriminant approaches model the conditional
probability distribution of the semantic constituent
sequence (or concepts) c1 . . . cn considering a word
sequence w1 . . . wT : P (cn1 |w
T
1 ). In generative ap-
proaches, the joint probability P (cn1 , w
T
1 ) is mod-
elized instead and can be used to compute inferences
either for prediction/decoding or parameter training.
Generative models (such as hidden Markov mod-
els) have been first introduced to address the under-
standing problem with stochastic approaches (Levin
and Pieraccini, 1995). Recent variants offer
more degrees of freedom in modeling (see for in-
stance (He and Young, 2005) or (Lef?vre, 2007)).
Since then log-linear models have clearly shown
their superiority for tasks of sequence tagging (Hahn
et al, 2011).
Several variants of log-linear models differ in
their conditional variable independence assumptions
and use different normalisation steps. CRFs (Laf-
ferty et al, 2001) represent linear chains of random
independent variables, all conditioned over the en-
tire sequence and the normalisation is global over
the sequence.
Some generative approaches such as DBNs make
inferences in multi-level models (Lef?vre, 2007)
98
Figure 1: Example of an alignment of words with their conceptual units.
and intrinsically take into account segmentation.
For models unable to handle multi-level repre-
sentations (as CRF), it is convenient to represent
segments directly at the tag level. For this purpose
the BIO formalism can be used: B is added to tags
starting a segment, I to tags inside a segment and
O to out-of-domain tags (if these are not already
handled through a specific NULL tag). In the case
displayed in Figure 1, the concept sequence be-
comes: B-cmd-task I-cmd-task I-cmd-task
B-null I-null B-loc-town I-loc-town
I-loc-town I-loc-town I-loc-town
B-time-date I-time-date B-time-date
I-time-date I-time-date.
3 Semantic concept alignment
Automatic alignment is a major issue in machine
translation. For example, word-based alignments
are used to generate phrase tables that are core com-
ponents for many current statistical machine trans-
lation systems (Koehn et al, 2007). The alignment
task aims at finding the mapping between words of
two sentences in relation of translation. It faces sev-
eral difficulties:
? some source words are not associated with a
translated word;
? others are translated by several words;
? matched words may occur at different positions
in both sentences according to the syntactic
rules of the considered languages.
Several statistical models have been proposed to
align two sentences (Brown et al, 1993). One of
their main interests is their ability to be built in an
unsupervised way from a parallel corpus aligned at
the sentence level, but not at the word level. For-
mally, from a sentence S = s1 . . . sm expressed in a
source language and its translation T = t1 . . . tn ex-
pressed in a target language, an IBM-style alignment
A = a1 . . . am connects each source word to a tar-
get word (aj ? {1, ..., n}) or to the so-called NULL
token which accounts for untranslated target words.
IBM statistical models evaluate the translation of S
into T from the computation of P (S,A|T ); the best
alignment A? can be deduced from this criterion us-
ing the Viterbi algorithm:
A? = argmaxAP (S,A|T ) . (1)
IBM models differ according to their complexity
level. IBM1 model makes the strong assumption
that alignments are independent and can be evalu-
ated only through the transfer probabilities P (si|tj).
The HMM model, which is an improvement over
IBM2, adds a new parameter P (aj |aj?1, n) that as-
sumes a first-order dependency between alignment
variables. The next models (IBM3 to IBM5) are
mainly based on two types of parameters:
? distortion, which measures how words of T are
reordered with respect to the index of the words
from S they are aligned with,
? fertility, which measures the usual number of
words that are aligned with a target word tj .
In order to improve alignments, IBM models are
usually applied in both translation directions. These
two alignments are then symmetrized by combining
them. This last step is done via heuristic methods;
a common approach is to start with the intersection
and then iteratively add links from the union (Och et
al., 1999).
If we have at our disposal a method that can find
concepts contained in an utterance, segmental anno-
tation can be obtained by aligning words S = wT1
with the found concepts T = cn1 (Fig. 1). Con-
cepts are ideally generated in the correct order with
respect to the word segments of the analysed utter-
ance. In a more pragmatic way, concepts are likely
to be produced as bag-of-concepts rather than or-
dered sequences.
99
Statistical alignment methods used in machine
translation are relevant in our context if we consider
that the target language is the concept language.
There are nevertheless differences with genuine lan-
guage translation. First, each word is aligned to at
most one concept, while a concept is aligned with
one word or more. Consequently, it is expected that
word fertilities are one for the alignment of words
toward concepts and concept fertilities are one or
more in the reverse direction. Another consequence
is that NULL words are useless in our context. These
specificities of the alignment process raise some dif-
ficulties with regard to IBM models. Indeed, ac-
cording to the way probabilities are computed, the
alignment of concepts toward words only allows one
word to be chosen per concept, which prevents this
direction from having a sufficient number of links
between words and concepts.
Another significant difference with translation is
related to the translated token order. While word
order is not random in a natural language and fol-
lows syntactic rules, it is not the case anymore when
a word sequence have to be aligned with a bag-of-
concepts. HMM and IBM2 to IBM5 models have
parameters that assume that the index of a matched
source word or the indices of the translations of the
adjacent target words bear on the index of target
words. Therefore, the randomness of the concept
indices can disrupt performance obtained with these
models, contrary to IBM1. As shown in the next
section, it is appropriate to find ways to explicitly
re-order concept sequences than to let the distortion
parameters handle the problem alone.
4 Experiments and results
4.1 Experimental setup
The evaluation of the introduced methods was car-
ried out on the MEDIA corpus (Bonneau Maynard et
al., 2008). This corpus consists of human-machine
dialogues collected with a wizard of Oz procedure
in the domain of negotiation of tourist services. Pro-
duced for a realistic task, it is annotated with 145 se-
mantic concepts and their values (more than 2k in to-
tal for the enumerable cases). The audio data are dis-
tributed with their manual transcripts and automatic
speech recognition (ASR) hypotheses. The corpus
is divided into three parts: a training set (approxi-
matively 12k utterances), a development set (1.2k)
and a test set (3k).
The experiments led on the alignment methods
were evaluated on the development corpus using
MGIZA++ (Gao and Vogel, 2008), a multi-thread
version of GIZA++ (Och and Ney, 2003) which also
allows previously trained IBM alignments models
to be applied on the development and test corpora.1
The conceptual tagging process was evaluated on the
test corpus, using WAPITI (Lavergne et al, 2010)
to train the CRF models. Several setups have been
tested:
? manual vs ASR transcriptions,
? inclusion (or not) of values during the error
computation.
Several concept orderings (before automatic align-
ment) have also been considered:
? a first ideal one, which takes reference concept
sequences as they are, aka sequential order;
? two more realistic variants that sort concepts ei-
ther alphabetically or randomly, in order to
simulate bag-of-concepts. Alphabetical order
is introduced solely to show that a particular
order (which is not related to the natural order)
might misled the alignment process by intro-
ducing undue regularities.
To give a rough idea, these experiments required
a few minutes of computing time to train alignment
models of 12k utterances, a few hours to train CRF
models (using 8 CPUs on our cluster of Xeon CPUs)
and a few seconds to apply alignment and CRF mod-
els in order to decode the test corpus.
4.2 Experimental results for alignment
Alignment quality is estimated using the alignment
error rate (AER), a metric often employed in ma-
chine translation (Och and Ney, 2000). If H stands
for hypothesis alignments andR for reference align-
ments, AER is computed by the following relation:2
AER = 1?
2? |H ?R|
|H|+ |R|
. (2)
1With previousa, previoust, previousn, etc pa-
rameters.
2This equation is a simplification of the usually provided one
because all alignments are considered as sure in our case.
100
In our context, this metrics is evaluated by repre-
senting a link between source and target identities by
(wi, cj), instead of the usual indices (i, j). Indeed,
alignments are then used to tag words. Besides, con-
cepts to align have positions that differ from the ones
in the reference when they are reordered to simulate
bags-of-concepts.
As mentioned in the introduction, we resort to
widely used tools for alignment in order to be as gen-
eral as possible in our approach. We do not modify
the algorithms and rely on their generality to deal
with specificities of the studied domain. To train
iteratively the alignment models, we use the same
pipeline as in MOSES, a widely used machine trans-
lation system (Koehn et al, 2007):
1. 5 iterations of IBM1,
2. 5 iterations of HMM,
3. 3 iterations of IBM3 then
4. 3 iterations of IBM4.
To measure the quality of the built models, the
model obtained at the last iteration of this chain is
applied on the development corpus.
All the words of an utterance should normally
be associated with one concept, which makes the
IBM models? NULL word useless. However, in the
MEDIA corpus, a null semantic concept is associ-
ated with words that do not correspond to a concept
relevant for the tourist domain and may be omit-
ted by counting on the probability with the NULL
word included in the IBM models. Two versions
were specifically created to test this hypothesis: one
with all the reference concept sequences and another
without the null tags. The results measured when
taking into account these tags (AER of 14.2 %) are
far better than the ones obtained when they are dis-
carded (AER of 27.4 %), in the word ? concept
alignment direction.3 We decided therefore to keep
the null in all the experiments.
Table 1 presents the alignment results measured
on the development corpus according to the way
concepts are reordered with respect to the reference
and according to the considered alignment direction.
3For a fair comparison between both setups, the null con-
cept was ignored in H and R for this series of experiments.
The three first lines exhibit the results obtained with
the last IBM4 iteration. As expected, the AER mea-
sured with this model in the concept? word direc-
tion (second line), which can only associate at most
one word per concept, is clearly higher than the one
obtained in the opposite direction (first line). Quite
surprisingly, an improvement in terms of AER (third
line) over the best direction (first line) is observed
using the default MOSES heuristics (called grow-
diag-final) that symmetrizes alignments obtained in
both directions.
IBM1 models, contrary to other models, do not
take into account word index inside source and tar-
get sentences, which makes them relevant to deal
with bag-of-concepts. Therefore, we measured how
AER varies when using models previously built in
the training chain. The results obtained by applying
IBM1 and by symmetrizing alignments (last line),
show finally that these simple models lead to lower
performance than the one measured with IBM4 or
even HMM (last line), the concepts being ordered
alphabetically or randomly (two last columns).
The previous experiments have shown that align-
ment is clearly of lower quality when algorithms are
faced with bags-of-concepts instead of well-ordered
sequences. In order to reduce this phenomenon, se-
quences are reordered after a first alignmentA1 gen-
erated by the symmetrized IBM4 model. Two strate-
gies have been considered to fix the new position of
each concept ci. The first one averages the indices
of the words wi that are aligned with ci according to
A1:
pos1(cj) =
?
is.t.(i,j)?A1 i
Card({(i, j) ? A1})
. (3)
The second one weights each word index with their
transfer probabilities determined by IBM4:
pos2(cj) =
?
is.t.(i,j)?A1 i? f(wi, cj)?
is.t.(i,j)?A1 f(wi, cj)
(4)
where
f(wi, cj) = ?P (cj |wi) + (1? ?)P (wi|cj) (5)
and ? is a coefficient fixed on the development cor-
pus.
Training alignment models on the corpus re-
ordered according to pos1 (Tab. 2, second column)
101
Sequential order Alphabetic order Random order
word? concept IBM4 14.4 29.2 28.6
concept? word IBM4 40.9 51.6 49.0
symmetrized IBM4 12.8 27.3 25.7
symmetrized IBM1 33.2 33.2 33.1
symmetrized HMM 14.8 29.9 28.7
Table 1: AER (%) measured on the MEDIA development corpus with respect to the alignment model used and its
direction.
Initial 1st reordering iteration Last reordering iteration
pos1 pos2 pos2
Alphabetic order 27.3 22.2 21.0 19.4
Random order 25.7 21.9 20.2 18.5
Table 2: AER (%) measured on the MEDIA development corpus according to the strategy used to reorder concepts.
or pos2 (third column) leads to a significant im-
provement of the AER. This reordering step can be
repeated as long as performance goes on improving.
By proceeding like this until step 3 for the alphabetic
order and until step 7 for the random order, values of
AER below 20 % (last column) are finally obtained.
It is noteworthy that random reordering has better
results than alphabetic reordering. Indeed, HMM,
IBM3 and IBM4 models have probabilities that are
more biased in this latter case, where the same se-
quences occur more often although many are not in
the reference.
4.3 Experimental results for spoken language
understanding
In order to measure how spoken language un-
derstanding is disturbed by erroneous alignments,
CRFs parameters are trained under two conditions:
one where concept tagging is performed by an ex-
pert and one where corpora are obtained using au-
tomatic alignment. The performance criterion used
to evaluate the understanding task is the concept er-
ror rate (CER). CER is computed in a similar way
as word error rate (WER) used in speech recogni-
tion; it is obtained from the Levenshtein alignment
between both hypothesized and reference sequences
as the ratio of the sum of the concepts in the hy-
pothesis substituted, inserted or omitted on the total
number of concepts in the manual reference anno-
tation. The null concept is not considered during
the score computation. The CER can also take into
account the normalized values in addition to the con-
cept tags.
Starting from a state-of-the-art system (Manual
column), degradations due to various alignment con-
ditions are reported in Table 3. It can be noted that
the absolute increase in CER is at most 8.0 % (from
17.6 to 25.6 with values) when models are trained on
the corpus aligned with IBM models; the ordering
information brings it back to 3.7 % (17.6 to 21.3),
and finally with automatic transcription the impact
of the automatic alignments is smaller (resp. 5.8 %
and 2.0 %). As expected random order is preferable
to alphabetic order (slight gain of 1 %).
In Table 4, the random order alignments are used
but this time the n-best lists of alignments are con-
sidered and not only the 1-best hypotheses. Instead
of training CRFs with only one version of the align-
ment for a concept-word sequence pair, we filter
out from the n-best lists the alignments having a
probability above a given threshold. It can be ob-
served that varying this confidence threshold allows
an improvement of the SLU performance (CER can
be reduced by 0.8 % for manual transcription and
0.4 % for automatic transcription). However, this
improvement is not propagated to scores with val-
ues (CER was reduced at best by 0.1 for manual
transcription and was increased for automatic tran-
102
Automatic alignments
Manual Sequential Alphabetic order Random order
Manual transcription 13.9 (17.6) 17.7 (21.3) 22.6 (26.4) 22.0 (25.6)
ASR transcription (wer 31 %) 24.7 (29.8) 27.1 (31.8) 31.5 (36.4) 30.6 (35.6)
Table 3: CER (%) measured for concept decoding on the MEDIA test corpus with several alignment methods of the
training data. Inside parenthesis, CER for concepts and values.
scription). After closer inspection of the scoring
alignments, an explanation for this setback is that
the manually-designed rules used for value extrac-
tion are perturbed by loose segmentation. This is
particularly the case for the concept used to anno-
tate co-references, which has confusions between
the values singular and plural (e.g. ?this? is sin-
gular and ?those? plural). This issue can be solved
by an ad hoc adaptation of the rules. However, it
would infringe our objective of relying upon unsu-
pervised approaches and minimizing human exper-
tise. Therefore, a better answer would be to resort to
a probabilistic scheme also for value extraction (as
proposed in (Lef?vre, 2007)).
The optimal configuration (confidence threshold
of 0.3, 4th row of Table 4) is close to the baseline
1-best system in terms of the number of training
utterances. We also tried a slightly different setup
which adds the filtered alignments to the former cor-
pus before CRF parameter training (i.e. the 1-best
is not filtered in the n-best list). In that case perfor-
mance remains pretty stable with respect to the filter-
ing process (CER is around 21.4 % for concepts and
25.2 % for concept+value for thresholds between 0.1
and 0.7).
5 Conclusion
In this study an unsupervised approach is proposed
to the problem of conceptual unit alignment for spo-
ken language understanding. We show that unsuper-
vised statistical word alignment from the machine
translation domain can be used in this context to as-
sociate semantic concepts with word sequences. The
quality of the derived alignment, already good in the
general case (< 20 % of errors on the word-concept
associations), is improved by knowledge of the cor-
rect unit order (< 15 %). The impact of automatic
alignments on the understanding performance is an
absolute increase of +8 % in terms of CER, but is re-
duced to less than +4 % in the ordered case. When
automatic transcripts are used, these gaps decrease
to +6 % and below +3 % respectively. From these
results we do believe that the cost vs performance
ratio is in favour of the proposed method.
Acknowledgements
This work is partially supported by the ANR funded
project PORT-MEDIA.4
References
H?l?ne Bonneau-Maynard, Sophie Rosset, Christelle Ay-
ache, Anne Kuhn, and Djamel Mostefa. 2005. Seman-
tic annotation of the MEDIA corpus for spoken dialog.
In Proceedings of Eurospeech, pages 3457?3460, Lis-
boa, Portugal.
H?l?ne Bonneau Maynard, Alexandre Denis, Fr?d?ric
B?chet, Laurence Devillers, F. Lef?vre, Matthieu
Quignard, Sophie Rosset, and Jeanne Villaneau. 2008.
MEDIA : ?valuation de la compr?hension dans les
syst?mes de dialogue. In L??valuation des technolo-
gies de traitement de la langue, les campagnes Tech-
nolangue, pages 209?232. Herm?s, Lavoisier.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engineer-
ing, Testing, and Quality Assurance for Natural Lan-
guage Processing, pages 49?57, Columbus, OH, USA.
Stefan Hahn, Marco Dinarelli, Christian Raymond, Fab-
rice Lef?vre, Patrick Lehen, Renato De Mori, Alessan-
dro Moschitti, Hermann Ney, and Giuseppe Riccardi.
2011. Comparing stochastic approaches to spoken
language understanding in multiple languages. IEEE
Transactions on Audio, Speech and Language Pro-
cessing (TASLP), 19(6):1569?1583.
4www.port-media.org
103
# train utterances Manual transcription ASR transcription
(WER = 31 %)
1-best 12795 22.0 (25.6) 30.6 (35.6)
filtered 10-best (conf thres = 0.1) 18955 21.7 (25.8) 31.2 (36.9)
filtered 10-best (conf thres = 0.2) 15322 21.3 (25.5) 30.7 (36.3)
filtered 10-best (conf thres = 0.3) 13374 21.2 (25.7) 30.2 (36.0)
filtered 10-best (conf thres = 0.5) 10963 21.4 (25.7) 30.6 (36.2)
filtered 10-best (conf thres = 0.7) 9647 25.4 (29.1) 32.9 (38.2)
Table 4: CER (%) measured for concept decoding on the MEDIA test corpus with filtered n-best lists of random order
alignments of the training data. Inside parenthesis, CER for concepts and values.
Yulan He and Steve Young. 2005. Spoken language
understanding using the hidden vector state model.
Speech Communication, 48(3?4):262?275.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL, Companion Volume, pages 177?180,
Prague, Czech Republic.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of ICML, pages 282?289, Williamstown,
MA, USA.
Thomas Lavergne, Olivier Capp?, and Fran?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL, pages 504?513, Uppsala, Sweden.
Fabrice Lef?vre. 2007. Dynamic bayesian networks and
discriminative classifiers for multi-stage semantic in-
terpretation. In Proceedings of ICASSP, Honolulu,
Hawai.
Esther Levin and Roberto Pieraccini. 1995. Concept-
based spontaneous speech understanding system. In
Proceedings of Eurospeech, pages 555?558, Madrid,
Spain.
Fran?ois Mairesse, Milica Ga?ic?, Filip Jurc??c?ek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2009. Spoken language understanding from unaligned
data using discriminative classification models. In
Proceedings of ICASSP, Taipei, Taiwan.
Franz Joseph Och and Hermann Ney. 2000. A compari-
son of alignment models for statistical machine trans-
lation. In Proceedings of Coling, volume 2, pages
1086?1090, Saarbr?cken, Germany.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages 20?
28, College Park, MD, USA.
Wayne Ward. 1991. Understanding spontaneous speech:
the Phoenix system. In Proceedings of ICASSP, pages
365?368, Toronto, Canada.
104
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 154?157,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Factored Machine Translation Systems for Russian-English
Ste?phane Huet, Elena Manishina and Fabrice Lefe`vre
Universite? d?Avignon, LIA/CERI, France
FirstName.LastName@univ-avignon.fr
Abstract
We describe the LIA machine transla-
tion systems for the Russian-English and
English-Russian translation tasks. Various
factored translation systems were built us-
ing MOSES to take into account the mor-
phological complexity of Russian and we
experimented with the romanization of un-
translated Russian words.
1 Introduction
This paper presents the factored phrase-based
Machine Translation (MT) systems (Koehn and
Hoang, 2007) developed at LIA, for the Russian-
English and English-Russian translation tasks at
WMT?13. These systems use only data provided
for the evaluation campaign along with the LDC
English Gigaword corpus.
We summarize in Section 2 the resources used
and the main characteristics of the systems based
on the MOSES toolkit (Koehn et al, 2007). Sec-
tion 3 reports experiments on the use of fac-
tored translation models. Section 4 describes the
transliteration process used to improve the Russian
to English task. Finally, we conclude in Section 5.
2 System Architecture
2.1 Pre-processing
The corpora available for the workshop were pre-
processed using an in-house script that normal-
izes quotes, dashes, spaces and ligatures. Long
sentences or sentences with many numeric or
non-alphanumeric characters were also discarded.
Since the Yandex corpus is provided as lower-
cased, we decided to lowercase all the other cor-
pora. The same pipeline was applied to the LDC
Gigaword; also only the documents classified as
?story? were retained. Table 1 summarizes the
used data and introduces designations that we fol-
low in the remainder of this paper to refer to these
corpora.
Russian is a morphologically rich language with
nouns, adjectives and verbs inflected for case,
number and gender. This property requires in-
troducing morphological information inside the
MT system to handle the lack of many inflec-
tional forms inside training corpora. For this
purpose, each corpus was previously tagged with
Part-of-Speech (PoS) tags. The tagger TREE-
TAGGER (Schmid, 1995) was selected for its
good performance on several comparable tasks.
The Russian tagger associates each word (e.g.
????? (boxes)) with a complex PoS including
morphological information (e.g. ?Ncmpnn? for
?Noun Type=common Gender=masculine Num-
ber=plural Case=nominative Animate=no?) and
its lemma (e.g. ???? (box)). A description of
the Russian tagset can be found in (Sharoff et al,
2008). The English tagger provides also a lemma-
tization and outputs PoS from the Penn Treebank
tagset (Marcus et al, 1993) (e.g. ?NNS? for
?Noun plural?).
In order to simplify the comparison of differ-
ent setups, we used the tokenizer included in the
TREETAGGER tool to process all the corpora.
2.2 Language Models
Kneser-Ney discounted LMs were built
from monolingual corpora using the SRILM
toolkit (Stolcke, 2002). 5-gram LMs were trained
for words, 7-gram LMs for lemmas and PoS. A
LM was built separately on each monolingual cor-
pus: mono-news-c and news-s. Since ldc was too
large to be processed as one file, it was split into
three parts according to the original publication
year of the document. These LMs were combined
through linear interpolation. Weights were fixed
by optimizing the perplexity on a corpus made of
the WMT test sets from 2008 to 2011 for English
and on the WMT 2012 test set for Russian (the
154
CORPORA DESIGNATION SIZE (SENTENCES)
English-Russian Bilingual training
News Commentary v8 news-c 146 k
Common Crawl crawl 755 k
Yandex yandex 978 k
English Monolingual training
News Commentary v8 mono-news-c 247 k
Shuffled News Crawl corpus (from 2007 to 2012) news-s 68 M
LDC Gigaword ldc 190 M
Russian Monolingual training
News Commentary v8 mono-news-c 182 k
Shuffled News Crawl corpus (from 2008 to 2012) news-s 20 M
Development
newstest2012 test12 3,003
Table 1: Used bilingual and monolingual corpora
only available at that time).
2.3 Alignment and Translation Models
All parallel corpora were aligned using
MGIZA++ (Gao and Vogel, 2008). Our transla-
tion models are phrase-based models (PBMs) built
with MOSES using default settings. Weights of
LM, phrase table and lexicalized reordering model
scores were optimized on test12, thanks to the
MERT algorithm (Gao and Vogel, 2008). Since
only one development corpus was made available
for Russian, we used a 3-fold cross-validation
so that MERT is repeated three times for each
translation model on a 2,000-sentence subsample
of test12.
To recase the corpora, translation models were
trained using a word-to-word translation model
trained on the parallel corpora aligning lowercased
and cased sentences of the monolingual corpora
mono-news-c and news-s.
3 Experiments with Factored
Translation Models
The evaluation was performed using case-
insensitive BLEU and was computed with the
mteval-v13a.pl script provided by NIST.
The BLEU scores shown in the tables below are
all averaged on the test parts obtained from the 3-
fold cross validation process.
In the remainder of the paper, we employ the
notation proposed by Bojar et al (2012) to refer
to factored translation models. For example, tW-
W:tL-L+tP-P+gLaP-W, where ?t? and ?g? stand
for ?translation? and ?generation?, denotes a trans-
lation system with two decoding paths:
? a first one directly translates words to words
(tW-W),
? a second one is divided into three steps:
1. translation from lemmas to lemmas (tL-
L),
2. translation from PoS to PoS (tP-P) and
3. generation of target words from target
lemmas and PoS (gLaP-W).
3.1 Baseline Phrase-Based Systems
Table 2 is populated with the results of PBMs
which use words as their sole factor. When LMs
are built on mono-news-c and news-s, an improve-
ment of BLEU is observed each time a training
parallel corpus is used, both for both translation di-
rections (columns 1 and 3). We can also notice an
absolute increase of 0.4 BLEU score when the En-
glish LM is additionally trained on ldc (column 2).
3.2 Decomposition of factors
Koehn and Hoang (2007) suggested from their ex-
periments for English-Czech systems that ?it is
beneficial to carefully consider which morpholog-
ical information to be used.? We therefore tested
various decompositions of the complex Russian
PoS tagset (P) output by TREETAGGER. We con-
sidered the grammatical category alone (C), mor-
phological information restrained to case, number
155
EN? RU RU? EN
+LDC
news-c 26.52 26.82 19.89
+crawl 29.49 29.82 21.06
+yandex 31.08 31.49 22.16
Table 2: BLEU scores measured with standard
PBMs.
Tagset #tags Examples
C 17 Af, Vm, P, C
M1 95 fsg, -s-, fsa, ?
M2 380 fsg, -s-, fsa, ??? (that)
M3 580 fsg, -s-1ife, fsa3, ??? (that)
P 604 Afpfsg, Vmif1s-a-e, P-3fsa, C
Table 3: Statistics on Russian tagsets.
and gender (M1), the fields included in M1 along
with additional information (lemmas) for conjunc-
tions, particles and adpositions (M2), and finally
the information included in M2 enriched with per-
son for pronouns and person, tense and aspect for
verbs (M3). Table 3 provides the number of tags
and shows examples for each used tagset.
To speed up the training of translation models,
we experimented with various setups for factor de-
composition from news-c. The results displayed
on Table 4 show that factors with morphologi-
cal information lead to better results than a PBM
trained on word forms (line 1) but that finally the
best system is achieved when the complex PoS tag
output by TREETAGGER is used without any de-
composition (last line).
tW-W 19.89
tW-WaC 19.81
tW-WaM1 20.04
tW-WaCaM1 19.95
tW-WaM2 19.92
tW-WaCaM2 19.91
tW-WaM3 19.98
tW-WaCaM3 19.89
tW-WaP 20.30
Table 4: BLEU scores for EN?RU using news-c
as training parallel corpus.
tL-W 29.23
tW-W 31.49
tWaP-WaP 31.62
tW-W:tL-W 31.69
tW-WaP 31.80
tW-WaP:tL-WaP 31.89
Table 5: BLEU scores for RU?EN using the three
available parallel corpora.
3.3 Experimental Results for Factored
Models
The many inflections for Russian induce a hight
out-of-vocabulary rate for the PBMs, which gener-
ates many untranslated Russian words for Russian
to English. We experimented with the training of
a PMB on lemmatized Russian corpora (Table 5,
line 1) but observed a decrease in BLEU score
w.r.t. a PBM trained on words (line 2). With two
decoding paths ? one from words, one from lem-
mas (line 4) ? using the MOSES ability to manage
multiple decoding paths for factored translation
models, an absolute improvement of 0.2 BLEU
score was observed.
Another interest of factored models is disam-
biguating translated words according to their PoS.
Translating a (word, PoS) pair results in an ab-
solute increase of 0.3 BLEU (line 5), and of 0.4
BLEU when considering two decoding paths (last
line). Disambiguating source words with PoS did
not seem to help the translation process (line 3).
The Russian inflections are far more problem-
atic in the other translation direction since mor-
phological information, including case, gender
and number, has to be induced from the English
words and PoS, which are restrained for that lan-
guage to the grammatical category and knowledge
about number (singular/plural for nouns, 3rd per-
son singular or not for verbs). Disambiguating
translated Russian words with their PoS resulted
in a dramatic increase of BLEU by 1.6 points (Ta-
ble 6, last line vs line 3). The model that trans-
lates independently PoS and lemmas, before gen-
erating words, albeit appealing for its potential to
deal with data sparsity, turned out to be very dis-
appointing (first line). We additionally led ex-
periments training generation models gLaP-W on
monolingual corpora instead of the less volumi-
nous parallel corpora, but we did not observed a
gain in terms of BLEU.
156
tL-L+tP-P+gLaP-W 17.06
tW-W 22.16
tWaP-WaP 23.34
tWaP-LaP+gLaP-W 23.48
tW-LaP+gLaP-W 23.58
tW-WaP 23.72
Table 6: BLEU scores for EN?RU using the three
available parallel corpora.
BEFORE AFTER
tW-WaP 31.80 32.15
tW-WaP:tL-WaP 31.89 32.21
Table 7: BLEU scores for RU ? EN before and
after transliteration.
4 Transliteration
Words written in Cyrillic inside the English trans-
lation output were transliterated into Latin letters.
We decided to restrain the use of transliteration for
the English to Russian direction since we found
that many words, especially proper names, are in-
tentionally used in Latin letters in the Russian ref-
erence.
Transliteration was performed in two steps.
Firstly, untranslated words in Cyrillic are looked
up in the guessed-names.ru-en file provided for the
workshop and built from Wikipedia. Secondly, the
remaining words are romanized with rules of the
BGN/PCGN romanization method for Russian (on
Geographic Names, 1994). Transliterating words
in Cyrillic resulted in an absolute improvement of
0.3 BLEU for our two best factor-based system
(Table 7, last column).
The factored model with the tW-WaP:tL-
WaP translation path and a transliteration post-
processing step is the final submission for the
Russian-English workshop translation task, while
the tW-WaP is the final submission for the other
translation direction.
5 Conclusion
This paper presented experiments carried out with
factored phrase-based translation models for the
two-way Russian-English translation tasks. A mi-
nor gain was observed after romanizing Russian
words (+0.3 BLEU points for RU ? EN) and
higher improvements using word forms, PoS inte-
grating morphological information and lemma as
factors (+0.4 BLEU points for RU? EN and +1.6
for EN ? RU w.r.t. to a phrase-based restrained
to word forms). However, these improvements
were observed with setups which disambiguate
words according to their grammatical category or
morphology, while results integrating a generation
step and dealing with data sparsity were disap-
pointing. It seems that further work should be
done to fully exploit the potential of this option
inside MOSES.
References
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran. 2012.
Probes in a taxonomy of factored phrase-based mod-
els. In 7th NAACL Workshop on Statistical Machine
Translation (WMT), pages 253?260.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Proceedings of
the ACL Workshop: Software Engineering, Testing,
and Quality Assurance for Natural Language Pro-
cessing, pages 49?57.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 868?-876.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In 45th Annual Meeting of the Association for
Computational Linguistics (ACL), Companion Vol-
ume, pages 177?180.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 2:313?330.
U.S. Board on Geographic Names. 1994. Romaniza-
tion systems and roman-script spelling conventions.
Technical report, Defense Mapping Agency.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
ACL SIGDAT Workshop, pages 47?50.
Serge Sharoff, Mikhail Kopotev, Tomaz? Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing
and evaluating a russian tagset. In 6th International
Conference on Language Resources and Evaluation
(LREC), pages 279?285.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In 7th International Con-
ference on Spoken Language Processing (ICSLP).
157
