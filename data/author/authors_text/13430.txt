Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 800?807,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
On the role of context and prosody in the interpretation of ?okay?
Agust??n Gravano, Stefan Benus, He?ctor Cha?vez, Julia Hirschberg, Lauren Wilcox
Department of Computer Science
Columbia University, New York, NY, USA
{agus,sbenus,hrc2009,julia,lgw23}@cs.columbia.edu
Abstract
We examine the effect of contextual and
acoustic cues in the disambiguation of three
discourse-pragmatic functions of the word
okay. Results of a perception study show
that contextual cues are stronger predictors
of discourse function than acoustic cues.
However, acoustic features capturing the
pitch excursion at the right edge of okay fea-
ture prominently in disambiguation, whether
other contextual cues are present or not.
1 Introduction
CUE PHRASES (also known as DISCOURSE MARK-
ERS) are linguistic expressions that can be used to
convey explicit information about the structure of
a discourse or to convey a semantic contribution
(Grosz and Sidner, 1986; Reichman, 1985; Cohen,
1984). For example, the word okay can be used to
convey a ?satisfactory? evaluation of some entity in
the discourse (the movie was okay); as a backchan-
nel in a dialogue to indicate that one interlocutor
is still attending to another; to convey acknowledg-
ment or agreement; or, in its ?cue? use, to start or fin-
ish a discourse segment (Jefferson, 1972; Schegloff
and Sacks, 1973; Kowtko, 1997; Ward and Tsuka-
hara, 2000). A major question is how speakers indi-
cate and listeners interpret such variation in mean-
ing. From a practical perspective, understanding
how speakers and listeners disambiguate cue phrases
is important to spoken dialogue systems, so that sys-
tems can convey potentially ambiguous terms with
their intended meaning and can interpret user input
correctly.
There is considerable evidence that the different
uses of individual cue phrases can be distinguished
by variation in the prosody with which they are re-
alized. For example, (Hirschberg and Litman, 1993)
found that cue phrases in general could be disam-
biguated between their ?semantic? and their ?dis-
course marker? uses in terms of the type of pitch
accent borne by the cue phrase, the position of the
phrase in the intonational phrase, and the amount
of additional information in the phrase. Despite the
frequence of the word okay in natural dialogues,
relatively little attention has been paid to the rela-
tionship between its use and its prosodic realization.
(Hockey, 1993) did find that okay differs in terms of
the pitch contour speakers use in uttering it, suggest-
ing that a final rising pitch contour ?categorically
marks a turn change,? while a downstepped falling
pitch contour usually indicates a discourse segment
boundary. However, it is not clear which, if any, of
the prosodic differences identified in this study are
actually used by listeners in interpreting these po-
tentially ambiguous items.
In this study, we address the question of how hear-
ers disambiguate the interpretation of okay. Our goal
is to identify the acoustic, prosodic and phonetic fea-
tures of okay tokens for which listeners assign differ-
ent meanings. Additionally, we want to determine
the role that discourse context plays in this classi-
fication: i.e., can subjects classify okay tokens reli-
ably from the word alone or do they require addi-
tional context?
Below we describe a perception study in which
listeners were presented with a number of spoken
productions of okay, taken from a corpus of dia-
logues between subjects playing a computer game.
The tokens were presented both in isolation and in
context. Users were asked to select the meaning
800
of each token from three of the meanings that okay
can take on: ACKNOWLEDGEMENT/AGREEMENT,
BACKCHANNEL, and CUE OF AN INITIAL DIS-
COURSE SEGMENT. Subsequently, we examined the
acoustic, prosodic and phonetic correlates of these
classifications to try to infer what cues listeners used
to interpret the tokens, and how these varied by con-
text condition. Section 2 describes our corpus. Sec-
tion 3 describes the perception experiment. In Sec-
tion 4 we analyze inter-subject agreement, introduce
a novel representation of subject judgments, and ex-
amine the acoustic, prosodic, phonetic and contex-
tual correlates of subject classification of okays. In
Section 5 we discuss our results and future work.
2 Corpus
The materials for our perception study were selected
from a portion of the Columbia Games Corpus, a
collection of 12 spontaneous task-oriented dyadic
conversations elicited from speakers of Standard
American English. The corpus was collected and
annotated jointly by the Spoken Language Group
at Columbia University and the Department of Lin-
guistics at Northwestern University.
Subjects were paid to play two series of com-
puter games (the CARDS GAMES and the OBJECTS
GAMES), requiring collaboration between partners
to achieve a common goal. Participants sat in front
of laptops in a soundproof booth with a curtain be-
tween them, so that all communication would be ver-
bal. Each player played with two different partners
in two different sessions. On average, each session
took 45m 39s, totalling 9h 8m of dialogue for the
whole corpus. All interactions were recorded, digi-
tized, and downsampled to 16K.
The recordings were orthographically transcribed
and words were aligned by hand by trained annota-
tors in a ToBI (Beckman and Hirschberg, 1994) or-
thographic tier using Praat (Boersma and Weenink,
2001) to manipulate waveforms. The corpus con-
tains 2239 unique words, with 73,831 words in total.
Nearly all of the Objects Games part of the corpus
has been intonationally transcribed, using the ToBI
conventions. Pitch, energy and duration information
has been extracted for the entire corpus automati-
cally, using Praat.
In the Objects Games portion of the corpus each
player?s laptop displayed a gameboard containing 5?
7 objects (Figure 1). In each segment of the game,
both players saw the same set of objects at the same
position on each screen, except for one object (the
TARGET). For one player (the DESCRIBER), this tar-
get appeared in a random location among other ob-
jects on the screen. For the other player (the FOL-
LOWER), the target object appeared at the bottom of
the screen. The describer was instructed to describe
the position of the target object on their screen so
that the follower could move their representation of
the target to the same location on their own screen.
After the players had negotiated what they deter-
mined to be the best location, they were awarded
up to 100 points based on the actual match of the
target location on the two screens. The game pro-
ceeded in this way through 14 tasks, with describer
and follower alternating roles. On average, the Ob-
jects Games portion of each session took 21m 36s,
resulting in 4h 19m of dialogue for the twelve ses-
sions in the corpus. There are 1484 unique words in
this portion of the corpus, and 36,503 words in total.
Figure 1: Sample screen of the Objects Games.
Throughout the Objects Games, we noted that
subjects made frequent use of affirmative cue words,
such as okay, yeah, alright, which appeared to vary
in meaning. To investigate the discourse functions
of such words, we first asked three labelers to inde-
pendently classify all occurrences of alright, gotcha,
huh, mmhm, okay, right, uhhuh, yeah, yep, yes, yup
in the entire Games Corpus into one of ten cate-
gories, including acknowledgment/agreement, cue
beginning or ending discourse segment, backchan-
nel, and literal modifier. Labelers were asked to
801
choose the most appropriate category for each to-
ken, or indicate with ??? if they could not make a
decision. They were allowed to read the transcripts
and listen to the speech as they labeled.
For our perception experiment we chose materials
from the tokens of the most frequent of our labeled
affirmative words, okay, from the Objects Games,
which contained most of these tokens. Altogether,
there are 1151 instances of okay in this part of the
corpus; it is the third most frequent word, follow-
ing the, with 4565 instances, and of, with 1534.
At least two labelers agreed on the functional cat-
egory of 902 (78%) okay tokens. Of those tokens,
286 (32%) were classified as BACKCHANNEL, 255
(28%) as ACKNOWLEDGEMENT/AGREEMENT, 141
(16%) as CUE BEGINNING, 116 (13%) as PIVOT
BEGINNING (a function that combines Acknowl-
edgement/agreement and Cue beginning), and 104
(11%) as one of the other functions. We sampled
from tokens the annotators had labeled as Cue be-
ginning discourse segment, Backchannel, and Ac-
knowledgement/agreement, the most frequent cate-
gories in the corpus; we will refer to these below
simply as ?C?, ?B?, and ?A? classes, respectively.
3 Experiment
We next designed a perception experiment to ex-
amine naive subjects? perception of these tokens of
okay. To obtain good coverage both of the (labeled)
A, B, and C classes, as well as the degrees of po-
tential ambiguity among these classes, we identified
9 categories of okay tokens to include in the experi-
ment: 3 classes (A, B, C) ? 3 levels of labeler agree-
ment (UNANIMOUS, MAJORITY, NO-AGREEMENT).
?Unanimous? refers to tokens assigned to a particu-
lar class label by all 3 labelers, ?majority? to tokens
assigned to this class by 2 of the 3 labelers, and ?no-
agreement? to tokens assigned to this class by only
1 labeler. To decrease variability in the stimuli, we
selected tokens only from speakers who produced at
least one token for each of the 9 conditions. There
were 6 such speakers (3 female, 3 male), which gave
us a total of 54 tokens.
To see whether subjects? classifications of okay
were dependent upon contextual information or not,
we prepared two versions of each token. The iso-
lated versions consisted of only the word okay ex-
tracted from the waveform. For the contextualized
versions, we extracted two full speaker turns for
each okay including the full turn1 containing the tar-
get okay plus the full turn of the previous speaker. In
the following three sample contexts, pauses are indi-
cated with ?#?, and the target okays are underlined:
Speaker A: yeah # um there?s like there?s some space there?s
Speaker B: okay # I think I got it
Speaker A: but it?s gonna be below the onion
Speaker B: okay
Speaker A: okay # alright # I?ll try it # okay
Speaker B: okay the owl is blinking
The isolated okay tokens were single channel au-
dio files; the contextualized okay tokens were for-
matted so that each speaker was presented to sub-
jects on a different channel, with the speaker uttering
the target okay consistently on the same channel.
The perception study was divided into two parts.
In the first part, each subject was presented with
the 54 isolated okay tokens, in a different ran-
dom ordering for each subject. They were given
a forced choice task to classify them as A, B, or
C, with the corresponding labels (Acknowledge-
ment/agreement, Backchannel, and Cue beginning)
also presented in a random order for each token. In
the second part, the same subject was given 54 con-
textualized tokens, presented in a different random
order, and asked to make the same choice.
We recruited 20 (paid) subjects for the study, 10
female, and 10 male, all between the ages of 20 and
60. All subjects were native speakers of Standard
American English, except for one subject who was
born in Jamaica but a native speaker of English. All
subjects reported no hearing problems. Subjects per-
formed the study in a quiet lab using headphones to
listen to the tokens and indicating their classification
decisions in a GUI interface on a lab workstation.
They were given instructions on how to use the in-
terface before each of the two sections of the study.
For the study itself, for each token in the isolated
condition, subjects were shown a screen with the
three randomly ordered classes and a link to the to-
ken?s sound file. They could listen to the sound files
as many times as they wished but were instructed
not to be concerned with answering the questions
1We define a TURN as a maximal sequence of words spoken
by the same speaker during which the speaker holds the floor.
802
?correctly?, but to answer with their immediate re-
sponse if possible. However, they were allowed to
change their selection as many times as they liked
before moving to the next screen. In the contex-
tualized condition, they were also shown an ortho-
graphic transcription of part of the contextualized to-
ken, to help them identify the target okay. The mean
duration of the first part of the study was 25 minutes,
and of the second part, 27 minutes.
4 Results
4.1 Subject ratings
The distribution of class labels in each experimental
condition is shown in Table 1. While this distribu-
tion roughly mirrors our selection of equal numbers
of tokens from each previously-labeled class, in both
parts of the study more tokens were labeled as A
(acknowledgment/agreement) than as B (backchan-
nel) or C (cue to topic beginning). This supports
the hypothesis that acknowledgment/agreement may
function as the default interpretation of okay.
Isolated Contextualized
A 426 (39%) 452 (42%)
B 324 (30%) 306 (28%)
C 330 (31%) 322 (30%)
Total 1080 (100%) 1080 (100%)
Table 1: Distribution of label classes in each
study condition.
We examined inter-subject agreement using
Fleiss? ? measure of inter-rater agreement for mul-
tiple raters (Fleiss, 1971).2 Table 2 shows Fleiss? ?
calculated for each individual label vs. the other two
labels and for all three labels, in both study condi-
tions. From this table we see that, while there is very
little overall agreement among subjects about how
to classify tokens in the isolated condition, agree-
ment is higher in the contextualized condition, with
a moderate agreement for class C (? score of .497).
This suggests that context helps distinguish the cue
beginning discourse segment function more than the
other two functions of okay.
2 This measure of agreement above chance is interpreted as
follows: 0 = None, 0 - 0.2 = Small, 0.2 - 0.4 = Fair, 0.4 - 0.6 =
Moderate, 0.6 - 0.8 = Substantial, 0.8 - 1 = Almost perfect.
Isolated Contextualized
A vs. rest .089 .227
B vs. rest .118 .164
C vs. rest .157 .497
all .120 .293
Table 2: Fleiss? ? for each label class
in each study condition.
Recall from Section 3 that the okay tokens were
chosen in equal numbers from three classes accord-
ing to the level of agreement of our three original
labelers (unanimous, majority, and no-agreement),
who had the full dialogue context to use in making
their decisions. Table 3 shows Fleiss? ? measure
now grouped by amount of agreement of the orig-
inal labelers, again presented for each context con-
dition. We see here that the inter-subject agreement
Isolated Context. OL
no-agreement .085 .104 -
majority .092 .299 -
unanimous .158 .452 -
all .120 .293 .312
Table 3: Fleiss? ? in each study condition, grouped
by agreement of the three original labelers (?OL?).
also mirrors the agreement of the three original la-
belers. In both study conditions, tokens which the
original labelers agreed on also had the highest ?
scores, followed by tokens in the majority and no-
agreement classes, in that order. In all cases, tokens
which subjects heard in context showed more agree-
ment than those they heard in isolation.
The overall ? is small at .120 for the isolated con-
dition, and fair at .293 for the contextualized con-
dition. The three original labelers also achieved fair
agreement at .312.3 The similarity between the lat-
ter two ? scores suggests that the full context avail-
able to the original labelers and the limited context
presented to the experiment subjets offer compara-
ble amounts of information to disambiguate between
the three functions, although lack of any context
clearly affected subjects? decisions. We conclude
3 For the calculation of this ?, we considered four label
classes: A, B, C, and a fourth class ?other? that comprises the
remaining 7 word functions mentioned in Section 2. In conse-
quence, these ? scores should be compared with caution.
803
from these results that context is of considerable im-
portance in the interpretation of the word okay, al-
though even a very limited context appears to suf-
fice.
4.2 Representing subject judgments
In this section, we present a graphical representa-
tion of subject decisions, useful for interpreting, vi-
sualizing, and comparing the way our subjects in-
terpreted the different tokens of okay. For each in-
dividual okay in the study, we define an associated
three-dimensional VOTE VECTOR, whose compo-
nents are the proportions of subjects that classified
the token as A, B or C. For example, if a particu-
lar okay was labeled as A by 5 subjects, as B by 3,
and as C by 12, then its associated vote vector is
( 5
20 ,
3
20 ,
12
20
)
= (0.25, 0.15, 0.6). Following this def-
inition, the vectors A = (1, 0, 0), B = (0, 1, 0) and
C = (0, 0, 1) correspond to the ideal situations in
which all 20 subjects agreed on the label. We call
these vectors the UNANIMOUS-VOTE VECTORS.
Figure 2.i shows a two-dimensional representa-
tion that illustrates these definitions. The black dot
Figure 2: 2D representation of a vote vector (i)
and of the cluster centroids (ii).
represents the vote vector for our example okay,
the vertices of the triangle correspond to the three
unanimous-vote vectors (A, B and C), and the cross
in the center of the triangle represents the vote vector
of a three-way tie between the labelers (13 , 13 , 13
).
We are thus able to calculate the Euclidean dis-
tance of a vote vector to each of the unanimous-vote
vectors. The shortest of these distances corresponds
to the label assigned by the plurality4 of subjects.
Also, the smaller that distance, the higher the inter-
subject agreement for that particular token. For our
4Plurality is also known as simple majority: the candidate
who gets more votes than any other candidate is the winner.
example okay, the distances to A, B and C are 0.972,
1.070 and 0.495, respectively; its plurality label is C.
In our experiment, each okay has two associated
vote vectors, one for each context condition. To
illustrate the relationship between decisions in the
isolated and the contextualized conditions, we first
grouped each condition?s 54 vote vectors into three
clusters, according to their plurality label. Figure
2.ii shows the cluster centroids in a two-dimensional
representation of vote vectors. The filled dots corre-
spond to the cluster centroids of the isolated condi-
tion, and the empty dots, to the centroids of the con-
textualized condition. Table 4 shows the distances
in each condition from the cluster centroids (denoted
Ac, Bc, Cc) to the respective unanimous-vote vec-tors (A, B, C), and also the distance between each
pair of cluster centroids.
Isolated Contextualized
d(Ac,A) .54 .44 (?18%)
d(Bc,B) .57 .52 (?10%)
d(Cc, C) .52 .28 (?47%)
d(Ac, Bc) .41 .48 (+17%)
d(Ac, Cc) .49 .86 (+75%)
d(Bc, Cc) .54 .91 (+69%)
Table 4: Distances from the cluster centroids (Ac,
Bc, Cc) to the unanimous-vote vectors (A, B, C)and between cluster centroids, in each condition.
In the isolated condition, the three cluster cen-
troids are approximately equidistant from each other
?that is, the three word functions appear to be
equally confusable. In the contextualized condi-
tion, while Cc is further apart from the other twocentroids, the distance between Ac and Bc remainspractically the same. This suggests that, with some
context available, A and B tokens are still fairly con-
fusable, while both are more easily distinguished
from C tokens. We posit two possible explanations
for this: First, C is the only function for which
the speaker uttering the okay necessarily continues
speaking; thus the role of context in disambiguat-
ing seems quite clear. Second, both A and B have a
common element of ?acknowledgement? that might
affect inter-subject agreement.
804
4.3 Features of the okay tokens
In this section, we describe a set of acoustic,
prosodic, phonetic and contextual features which
may help to explain why subjects interpret okay dif-
ferently. Acoustic features were extracted automat-
ically using Praat. Phonetic and prosodic features
were hand-labeled by expert annotators. Contextual
features were considered only in the analysis of the
contextualized condition, since they were not avail-
able to subjects in the isolated condition.
We examined a number of phonetic features to de-
termine whether these correlated with subject clas-
sifications. We first looked at the production of the
three phonemes in the target okay (/oU/, /k/, /eI/),
noting the following possible variations:
? /oU/: [], [A], [5], [O], [OU], [m], [N], [@], [@U].
? /k/: [G], [k], [kx], [q], [x].
? /eI/: [e], [eI], [E], [e@].
We also calculated the duration of each phone and
of the velar closure. Whether the target okay was at
least partially whispered or not, and whether there
was glottalization in the target okay were also noted.
For each target okay, we also examined its du-
ration and its maximum, mean and minimum pitch
and intensity, as well as the speaker-normalized ver-
sions of these values.5 We considered its pitch slope,
intensity slope, and stylized pitch slope, calculated
over the whole target okay, its last 50, 80 and 100
milliseconds, its second half, its second syllable, and
the second half of its second syllable, as well.
We used the ToBI labeling scheme (Pitrelli et al,
1994) to label the prosody of the target okays and
their surrounding context.
? Pitch accent, if any, of the target okay (e.g., H*,
H+!H*, L*).
? Break index after the target okay (0-4).
? Phrase accent and boundary tone, if any, fol-
lowing the target okay (e.g., L-L%, !H-H%).
For contextualized tokens, we included several fea-
tures related to the exchange between the speaker
uttering the target okay (Speaker B) and the other
speaker (Speaker A).
5Speaker-normalized features were normalized by comput-
ing z-scores (z = (X ?mean)/st.dev) for the feature, where
mean and st.dev were calculated from all okays uttered by the
speaker in the session.
? Number of words uttered by Speaker A in the
context, before and after the target okay. Same
for Speaker B.
? Latency of Speaker A before Speaker B?s turn.
? Duration of silence of Speaker B before and af-
ter the target okay.
? Duration of speech by Speaker B immediately
before and after the target okay and up to a si-
lence.
4.4 Cues to interpretation
We conducted a series of Pearson?s tests to look for
correlations between the proportion of subjects that
chose each label and the numeric features described
in Section 4.3, together with two-sided t-tests to find
whether such correlations differed significantly from
zero. Tables 5 and 6 show the significant results
(two-sided t-tests, p < 0.05) for the isolated and
contextualized conditions, respectively.
Acknowledgement/agreement r
duration of realization of /k/ ?0.299
Backchannel r
stylized pitch slope over 2nd half 2nd syl. 0.752
pitch slope over 2nd half of 2nd syllable 0.409
speaker-normalized maximum intensity ?0.372
pitch slope over last 80 ms 0.349
speaker-normalized mean intensity ?0.327
duration of realization of /eI/ 0.278
word duration 0.277
Cue to discourse segment beginning r
stylized pitch slope over the whole word ?0.380
pitch slope over the whole word ?0.342
pitch slope over 2nd half of 2nd syllable ?0.319
Table 5: Features correlated to the proportion of
votes for each label. Isolated condition.
Table 5 shows that in the isolated condition, sub-
jects tended to classify tokens of okay as Acknowl-
edgment/agreement (A) which had a longer realiza-
tion of the /k/ phoneme. They tended to classify
tokens as Backchannels (B) which had a lower in-
tensity, a longer duration, a longer realization of the
/eI/ phoneme, and a final rising pitch. They tended
to classify tokens as C (cue to topic beginning) that
ended with falling pitch.
805
Acknowledgement/agreement r
latency of Spkr A before Spkr B?s turn ?0.528
duration of silence by Spkr B before okay ?0.404
number of words by Spkr B after okay ?0.277
Backchannel r
pitch slope over 2nd half of 2nd syllable 0.520
pitch slope over last 80 ms 0.455
number of words by Spkr A before okay 0.451
number of words by Spkr B after okay ?0.433
duration of speech by Spkr B after okay ?0.413
latency of Spkr A before Spkr B?s turn ?0.385
duration of silence by Spkr B before okay 0.295
intensity slope over 2nd syllable ?0.279
Cue to discourse segment beginning r
latency of Spkr A before Spkr B?s turn 0.645
number of words by Spkr B after okay 0.481
number of words by Spkr A before okay ?0.426
pitch slope over 2nd half of 2nd syllable ?0.385
pitch slope over last 80 ms ?0.377
duration of speech by Spkr B after okay 0.338
Table 6: Features correlated to the proportion of
votes for each label. Contextualized condition.
In the contextualized condition, we find very dif-
ferent correlations. Table 6 shows that nearly all of
the strong correlations in this condition involve con-
textual features, such as the latency before Speaker
B?s turn, or the number of words by each speaker be-
fore and after the target okay. Notably, only one of
the features that show strong correlations in the iso-
lated condition shows the same strong correlation in
the contextualized condition: the pitch slope at the
end of the word. In both conditions subjects tended
to label tokens with a final rising pitch contour as
B, and tokens with a final falling pitch contour as C.
This supports (Hockey, 1993)?s findings on the role
of pitch contour in disambiguating okay.
We next conducted a series of two-sided Fisher?s
exact tests to find correlations between subjects? la-
belings of okay and the nominal features described
in Section 4.3. We found significant associations be-
tween the realization of the /oU/ phoneme and the
okay function in the isolated condition (p < 0.005).
Table 7 shows that, in particular, [m] seems to be the
preferred realization for B okays, while [@] seems to
be the preferred one for A okays, and [OU] and [O]
for A and C okays.
? [A] [5] [OU] [O] [N] [@U] [@] [] [m]
A 0 0 5 6 4 0 0 8 0 0
B 2 0 4 1 0 1 0 1 1 5
C 1 1 2 3 4 0 1 3 0 0
Table 7: Realization of the /oU/ phoneme, grouped
by subject plurality label. Isolated condition only.
Notably, we did not find such significant asso-
ciations in the contextualized condition. We did
find significant correlations in both conditions, how-
ever, between okay classifications and the type of
phrase accent and boundary tone following the target
(Fisher?s Exact Test, p < 0.05 for the isolated con-
dition, p < 0.005 for the contextualized condition).
Table 8 shows that L-L% tends to be associated with
A and C classes, H-H% with B classes, and L-H%
with A and B classes. In this case, such correlations
are present in the isolated condition, and sustained
or enhanced in the contextualized condition.
H-H% H-L% L-H% L-L% other
Isolated
A 0 2 4 8 9
B 3 3 1 5 3
C 1 1 0 8 5
Context.
A 0 2 3 10 10
B 4 3 2 1 2
C 0 1 0 10 5
Table 8: Phrase accent and boundary tone, grouped
by subject plurality label.
Summing up, when subjects listened to the okay
tokens in isolation, with only their acoustic, prosodic
and phonetic properties available, a few features
seem to strongly correlate with the perception of
word function; for example, maximum intensity,
word duration, and realizing the /oU/ phoneme as
[m] tend to be associated with backchannel, while
the duration of the realization of the /k/ phoneme,
and realizing the /oU/ phoneme as [@] tend to be as-
sociated with acknowledgment/agreement.
In the second part of the study, when subjects
listened to contextualized versions of the same to-
kens of okay, most of the strong correlations of word
function with acoustic, prosodic and phonetic fea-
tures were replaced by correlations with contextual
features, like latency and turn duration. In other
words, these results suggest that contextual features
806
might override the effect of most acoustic, prosodic
and phonetic features of okay. There is nonethe-
less one notable exception: word final intonation ?
captured by the pitch slope and the ToBI labels for
phrase accent and boundary tone ? seems to play a
central role in the interpretation of both isolated and
contextualized okays.
5 Conclusion and future work
In this study, we have presented evidence of differ-
ences in the interpretation of the function of isolated
and contextualized okays. We have shown that word
final intonation strongly correlates with the subjects?
classification of okays in both conditions. Addition-
ally, the higher degree of inter-subject agreement in
the contextualized condition, along with the strong
correlations found for contextualized features, sug-
gests that context, when available, plays a central
role in the disambiguation of okay. (Note, how-
ever, that further research is needed in order to assess
whether these features are indeed, in fact, perceptu-
ally important, both individually and combined.)
We have also presented results suggesting that ac-
knowledgment/agreement acts as a default function
for both isolated an contextualized okays. Further-
more, while that function remains confusable with
backchannel in both conditions, the availability of
some context helps in distinguishing those two func-
tions from cue to topic beginning.
These results are relevant to spoken dialogue sys-
tems in suggesting how systems can convey the cue
word okay with the intended meaning and can inter-
pret users? productions of okay correctly. How these
results extend to other cue words and to other word
functions remains an open question.
As future work, we will extend this study to in-
clude the over 5800 occurrences of alright, gotcha,
huh, mmhm, okay, right, uhhuh, yeah, yep, yes, yup
in the entire Games Corpus, and all 10 discourse
functions mentioned in Section 2, as annotated by
our three original labelers. Since we have observed
considerable differences in conversation style in the
two parts of the corpus (the Objects Games elicited
more ?dynamic? conversations, with more overlaps
and interruptions than the Cards Games), we will
compare cue phrase usage in these two settings. Fi-
nally, we are also interested in examining speaker
entrainment in cue phrase usage, or how subjects
adapt their choice and production of cue phrases to
their conversation partner?s.
Acknowledgments
This work was funded in part by NSF IIS-0307905.
We thank Gregory Ward, Elisa Sneed, and Michael
Mulley for their valuable help in collecting and la-
beling the data, and the anonymous reviewers for
helpful comments and suggestions.
References
Mary E. Beckman and Julia Hirschberg. 1994. The ToBIannotation conventions. Ohio State University.
Paul Boersma and David Weenink. 2001. Praat: Doingphonetics by computer. http://www.praat.org.
Robin Cohen. 1984. A computational theory of the func-
tion of clue words in argument understanding. 22nd
Conference of the ACL, pages 251?258.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-ment among many raters. Psychological Bulletin,76(5):378?382.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse. Com-
putational Linguistics, 12(3):175?204.
Julia Hirschberg and Diane Litman. 1993. EmpiricalStudies on the Disambiguation of Cue Phrases. Com-
putational Linguistics, 19(3):501?530.
Beth Ann Hockey. 1993. Prosody and the role of okay
and uh-huh in discourse. Proceedings of the Eastern
States Conference on Linguistics, pages 128?136.
Gail Jefferson. 1972. Side sequences. Studies in social
interaction, 294:338.
Jacqueline C. Kowtko. 1997. The function of intonation
in task-oriented dialogue. Ph.D. thesis, University of
Edinburgh.
John Pitrelli, Mary Beckman, and Julia Hirschberg.1994. Evaluation of prosodic transcription labelingreliability in the ToBI framework. In ICSLP94, vol-ume 2, pages 123?126, Yokohama, Japan.
Rachel Reichman. 1985. Getting Computers to Talk Like
You and Me: Discourse Context, Focus, and Seman-
tics: (an ATN Model). MIT Press.
Emanuel A. Schegloff and Harvey Sacks. 1973. Openingup closings. Semiotica, 8(4):289?327.
Nigel Ward and Wataru Tsukahara. 2000. Prosodic fea-tures which cue back-channel responses in English and
Japanese. Journal of Pragmatics, 23:1177?1207.
807
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 169?172,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
High Frequency Word Entrainment in Spoken Dialogue
Ani Nenkova
Dept. of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
nenkova@seas.upenn.edu
Agust??n Gravano
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
agus@cs.columbia.edu
Julia Hirschberg
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
julia@cs.columbia.edu
Abstract
Cognitive theories of dialogue hold that en-
trainment, the automatic alignment between
dialogue partners at many levels of linguistic
representation, is key to facilitating both pro-
duction and comprehension in dialogue. In
this paper we examine novel types of entrain-
ment in two corpora?Switchboard and the
Columbia Games corpus. We examine en-
trainment in use of high-frequency words (the
most common words in the corpus), and its as-
sociation with dialogue naturalness and flow,
as well as with task success. Our results show
that such entrainment is predictive of the per-
ceived naturalness of dialogues and is signifi-
cantly correlated with task success; in overall
interaction flow, higher degrees of entrainment
are associated with more overlaps and fewer
interruptions.
1 Introduction
When people engage in conversation, they adapt the
way they speak to their conversational partner. For
example, they often adopt a certain way of describ-
ing something based upon the way their conversa-
tional partner describes it, negotiating a common
description, particularly for items that may be un-
familiar to them (Brennan, 1996). They also alter
their amplitude, if the person they are speaking with
speaks louder than they do (Coulston et al, 2002),
or reuse syntactic constructions employed earlier in
the conversation (Reitter et al, 2006). This phe-
nomenon is known in the literature as entrainment,
accommodation, adaptation, or alignment.
There is a considerable body of literature which
posits that entrainment may be crucial to human per-
ception of dialogue success and overall quality, as
well as to participants? evaluation of their conversa-
tional partners. Pickering and Garrod (2004) pro-
pose that the automatic alignment at many levels of
linguistic representation (lexical, syntactic and se-
mantic) is key for both production and comprehen-
sion in dialogue, and facilitates interaction. Gole-
man (2006) also claims that a key to successful com-
munication is human ability to synchronize their
communicative behavior with that of their conver-
sational partner. For example, in laboratory stud-
ies of non-verbal entrainment (mimicry of manner-
isms and facial expressions between subjects and
a confederate), Chartrand and Bargh (1999) found
not only that subjects displayed a strong uninten-
tional entrainment, but also that greater entrain-
ment/mimicry led subjects to feel that they liked the
confederate more and that the overall interaction was
progressing more smoothly. People who had a high
inclination for empathy (understanding the point of
view of the other) entrained to a greater extent than
others. Reitter et al (2007) also found that degree of
entrainment in lexical and syntactic repetitions that
occurred in only the first five minutes of each dia-
logue significantly predicted task success in studies
of the HCRC Map Task Corpus.
In this paper we examine a novel dimension of
entrainment between conversation partners: the use
of high-frequency words, the most frequent words in
the dialogue or corpus. In Section 2 we describe ex-
periments on high-frequency word entrainment and
perceived dialogue naturalness in Switchboard dia-
169
logues. The degree of high-frequency word entrain-
ment predicts naturalness with an accuracy of 67%
over a 50% baseline. In Section 3 we discuss experi-
ments on the association of high-frequency word en-
trainment with task success and turn-taking. Results
show that degree of high-frequency word entrain-
ment is positively and significantly correlated with
task success and proportion of overlaps in these di-
alogues, and negatively and significantly correlated
with proportion of interruptions.
2 Predicting perceived naturalness
2.1 The Switchboard Corpus
The Switchboard Corpus (Godfrey et al, 1992) is
a collection of recordings of spontaneous telephone
conversations between speakers of many varieties of
American English who were asked to discuss a pre-
assigned topic from a set including favorite types of
music or the new roles of women in society. The
corpus consists of 2430 conversations with an aver-
age duration of 6 minutes, for a total of 240 hours
and three million words. The corpus has been ortho-
graphically transcribed and annotated for degree of
naturalness on Likert scales from 1 (very natural) to
5 (not natural at all).
2.2 Entrainment and perceived naturalness
Previous studies (Niederhoffer and Pennebaker,
2002) have suggested that adaptation in overall word
count as well as words of particular parts of speech,
or words associated with emotion or with various
cognitive states, can predict the degree of coordi-
nation and engagement of conversational partners.
Here, we examine conversational partners? similar-
ity in high-frequency word usage in the Switchboard
corpus as a predictor of the hand-annotated natural-
ness scores for their conversation. Using entrain-
ment over the most frequent words in the entire cor-
pus has the advantage of avoiding sparsity problems;
we hypothesize that it will be more general and ro-
bust than attempting to measure lexical entrainment
over the high-frequency words that occur in a partic-
ular conversation.
Our measure of entrainment entr(w) is defined as
the negated absolute value of the difference between
the fraction of times a particular word w is used by
the two speakers S1 and S2. More formally,
entr(w) = ?
?
?
?
?
countS1(w)
ALLS1
? countS2(w)ALLS2
?
?
?
?
Here, ALLSi is the number of all words ut-
tered by speaker Si in the given conversation, and
countSi(w) is the number of times Si used word w.
The entr(w) statistic was computed for the 100
most common words in the entire Switchboard cor-
pus and feature selection was used to determine the
25 most predictive words used for later classifica-
tion: um, how, okay, go, I?ve, all, very, as, or, up, a,
no, more, something, from, this, what, too, got, can,
he, in, things, you, and.
The data for the experiments was a balanced set of
250 conversations rated ?1? (very natural) and 250
examples of problematic conversations with ratings
of 3, 4 or 5. The accuracy of predicting the binary
naturalness (ratings of 1 or 3-5) of each conversa-
tion from a logistic regression model is 63.76%, sig-
nificantly over a 50% random baseline. This result
confirms the hypothesis that entrainment in high-
frequency word usage is a good indicator of the per-
ceived naturalness of a conversation.
Some of our 25 high-frequency words are in fact
cue phrases, which are important indicators of dia-
logue structure. This suggests that a more focused
examination of this class of words might be useful.
3 Association with task success and
dialogue flow
3.1 The Columbia Games Corpus
The Columbia Games Corpus (Benus et al, 2007) is
a collection of 12 spontaneous task-oriented dyadic
conversations elicited from native speakers of Stan-
dard American English. Subjects played a series
of computer games requiring verbal communication
between partners to achieve a common goal, ei-
ther identifying matching cards appearing on each
of their screens, or moving an object on one screen
to the same location in which it appeared on the
other, where each subject could see only their own
screen. The games were designed to encourage fre-
quent and natural conversation by engaging the sub-
jects in competitive yet collaborative tasks. For ex-
ample, players could receive points in the games in a
variety of ways and had to negotiate the best strategy
170
for matching cards; in other games, they received
more points if they could place objects in exactly
the same location. Subjects were scored on each
game and their overall score determined the addi-
tional monetary compensation they would receive.
A total of 9h 8m (?73,800 words) of dialogue were
recorded. All files in the corpus were orthograph-
ically transcribed and words were hand-aligned by
trained annotators. A subset of the corpus was also
labeled for different types of turn-taking behavior.
These include (i) smooth turn exchanges?speaker
S2 takes the floor after speaker S1 has completed her
turn, with no overlap; (ii) overlaps?S2 starts his
turn before S1 has completely finished her turn, but
S1 does complete her turn; (iii) interruptions?S2
starts talking before S1 completes her turn, and as a
result S1 does not complete her utterance. We used
these annotations to study the association between
entrainment and turn-taking behavior.
3.2 Entrainment and task success
In the Columbia Games Corpus, we hypothesize that
the game score achieved by the participants is a good
measure of the effectiveness of the dialogue. To de-
termine the extent to which task success is related
to the degree of entrainment in high-frequency word
usage, we examined 48 dialogues. We computed the
correlation coefficient between the game score (nor-
malized by the highest achieved score for the game
type) and two different ways of quantifying the de-
gree of entrainment between the speakers (S1 and
S2) in several word classes. In addition to overall
high-frequency words, we looked at two subclasses
of words often used in dialogue:
25MF-G The 25 most frequent words in the game.
25MF-C The 25 most frequent words over the entire
corpus: the, a, okay, and, of, I, on, right, is, it, that, have,
yeah, like, in, left, it?s, uh, so, top, um, bottom, with, you, to.
ACW Affirmative cue words: alright, gotcha, huh,
mm-hm, okay, right, uh-huh, yeah, yep, yes, yup. There
are 5831 instances in the corpus (7.9% of all words).
FP Filled pauses: uh, um, mm. The corpus contains
1845 instances of filled pauses (2.5% of all tokens).
We generalize our measure of word entrainment
entr(w) to each of these classes of words c:
ENTR1(c) =
?
w?c
entr(w)
ENTR1 ranges from 0 to ??, with 0 meaning per-
fect match on usage of lexical items in class c. An
alternative measure of entrainment that we experi-
mented with is defined as
ENTR2(c) = ?
?
w?c
|countS1(w)? countS2(w)|
?
w?c
(countS1(w) + countS2(w))
The entrainment score defined in this way ranges
from 0 to ?1, with 0 meaning perfect match on lex-
ical usage and ?1 meaning perfect mismatch.
The correlations between the normalized game
score and these measures of entrainment are shown
in Table 1. ENTR1 for the 25 most frequent words,
both corpus-wide and game-specific, is highly and
significantly correlated with task success, with
stronger results for game-specific words. For the
ENTR1 ENTR2
Word class cor p cor p
25MF-C 0.341 0.018 0.187 0.202
25MF-G 0.376 0.008 0.260 0.074
ACW 0.230 0.116 0.372 0.009
FP ?0.080 0.591 ?0.007 0.964
Table 1: Pearson?s correlation with game score.
filled pauses class, there is essentially no correlation
between entrainment and task success, while for af-
firmative cue words there is association only under
the ENTR2 definition of entrainment. The differ-
ence in results between ENTR1 and ENTR2 sug-
gests that the two measures of entrainment capture
different aspects of dialogue coordination and that
exploring various formulations of entrainment de-
serves future attention.
3.3 Dialogue coordination
The coordination of turn-taking in dialogue is espe-
cially important for successful interaction. Speech
overlaps (O), might indicate a lively, highly coor-
dinated conversation, with participants anticipating
the end of their interlocutor?s speaking turn. Smooth
switches of turns (S) with no overlapping speech
are also characteristic of good coordination, in cases
where these are not accompanied by long pauses be-
tween turns. On the other hand, interruptions (I)
and long inter-turn latency (L)?long simultaneous
pauses by the speakers? are generally perceived as
a sign of poorly coordinated dialogues.
171
To determine the relationship between entrain-
ment and dialogue coordination, we examined the
correlation between entrainment types and the pro-
portion of interruptions, smooth switches and over-
laps, for which we have manual annotations for a
subset of 12 dialogues. We also looked at the cor-
relation of entrainment with mean latency in each
dialogue. Table 2 summarizes our major findings.
cor p
ENTR1(25MF-C) I ?0.612 0.035
ENTR1(25MF-G) I ?0.514 0.087
ENTR1(ACW) O 0.636 0.026
ENTR2(ACW) O 0.606 0.037
ENTR1(FP) O 0.750 0.005
ENTR2(25MF-G) O 0.605 0.037
ENTR2(25MF-G) S ?0.663 0.019
ENTR2(ACW) L ?0.757 0.004
ENTR2(25MF-G) L ?0.523 0.081
Table 2: Pearson?s correlation with proportion of over-
laps, interruptions, smooth switches, and mean latency.
The two measures that were significantly cor-
related with task success?ENTR1(25MF-C) and
ENTR1(25MF-G)?also correlated negatively with
the proportion of interruptions in the dialogue. This
finding could have important implications for the de-
velopment of spoken dialog systems (SDS). For ex-
ample, a measure of entrainment might be used to
anticipate the user?s propensity to interrupt the sys-
tem, signalling the need to change dialogue strategy.
It also suggests that if the system entrains to users it
might help to reduce such interruptions. While our
study is of association, not causality, this suggests
future areas of investigation.
Our other correlations reveal that turn exchanges
characterized by overlaps are reliably associated
with entrainment in usage of affirmative cue word,
filled pauses and game-specific most frequent
words. Long latency is negatively associated with
entrainment in affirmative cue words and game-
specific most frequent words. Overall, the more
entrainment, the more engaged the participants and
the better coordination there is between them, with
shorter latencies and more overlaps.
Unexpectedly, smooth switches correlate nega-
tively with entrainment in game-specific most fre-
quent words. This result might be confounded by the
presence of long latencies in some switches. While
smooth switches are desirable, especially in SDS,
long latencies between turns can indicate lack of co-
ordination.
4 Conclusion
We present a corpus study relating dialogue natural-
ness, success and coordination with speaker entrain-
ment on common words: most frequent words over-
all, most frequent words in a dialogue, filled pauses,
and affirmative cue words. We find that degree of
entrainment with respect to most frequent words can
distinguish dialogues rated most natural from those
rated less natural. Entrainment over classes of com-
mon words also strongly correlates with task success
and highly engaged and coordinated turn-taking be-
havior. Entrainment over corpus-wide most frequent
words significantly correlates with task success and
minimal interruptions?important goals of SDS. In
future work we will explore the consequences of
system entrainment to SDS users in helping systems
achieve these goals, and the use of simple measures
of entrainment to modify dialogue strategies in order
to decrease the occurrence of user interruptions.
Acknowledgments
This work was funded in part by NSF IIS-0307905.
References
S. Benus, A. Gravano, and J. Hirschberg. 2007.
The prosody of backchannels in American English.
ICPhS?07.
S.E. Brennan. 1996. Lexical entrainment in spontaneous
dialog. ISSD?96.
T. Chartrand and J. Bargh. 1999. The chameleon ef-
fect: the perception-behavior link and social interac-
tion. J. of Personality & Social Psych., 76(6):893?910.
R. Coulston, S. Oviatt, and C. Darves. 2002. Amplitude
convergence in children?s conversational speech with
animated personas. ICSLP?02.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. ICASSP?92.
Daniel Goleman. 2006. Social Intelligence. Bantam.
K. Niederhoffer and J. Pennebaker. 2002. Linguistic
style matching in social interaction.
M. J. Pickering and S. Garrod. 2004. Toward a mecha-
nistic psychology of dialogue. Behavioral and Brain
Sciences, 27:169?226.
D. Reitter and J. Moore. 2007. Predicting success in
dialogue. ACL?07.
D. Reitter, F. Keller, and J.D. Moore. 2006. Compu-
tational Modelling of Structural Priming in Dialogue.
HLT-NAACL?06.
172
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 253?261,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Turn-Yielding Cues in Task-Oriented Dialogue
Agust??n Gravano
Department of Computer Science
Columbia University
New York, NY, USA
agus@cs.columbia.edu
Julia Hirschberg
Department of Computer Science
Columbia University
New York, NY, USA
julia@cs.columbia.edu
Abstract
We examine a number of objective, au-
tomatically computable TURN-YIELDING
CUES ? distinct prosodic, acoustic and
syntactic events in a speaker?s speech that
tend to precede a smooth turn exchange ?
in the Columbia Games Corpus, a large
corpus of task-oriented dialogues. We
show that the likelihood of occurrence of
a turn-taking attempt from the interlocu-
tor increases linearly with the number of
cues conjointly displayed by the speaker.
Our results are important for improving
the coordination of speaking turns in in-
teractive voice-response systems, so that
systems can correctly estimate when the
user is willing to yield the conversational
floor, and so that they can produce their
own turn-yielding cues appropriately.
1 Introduction and Previous Research
Users of state-of-the-art interactive voice response
(IVR) systems often find interactions with these
systems to be unsatisfactory. Part of this reac-
tion is due to deficiencies in speech recognition
and synthesis technologies, but some can also be
traced to coordination problems in the exchange
of speaking turns between system and user (Ward
et al, 2005; Raux et al, 2006). Users are not sure
when the system is ready to end its turn, and sys-
tems are not sure when users are ready to relin-
quish theirs. Currently, the standard method for
determining when a user is willing to yield the
conversational floor is to wait for a silence longer
than a prespecified threshold, typically ranging
from 0.5 to 1 second (Ferrer et al, 2003). How-
ever, this strategy is rarely used by humans, who
rely instead on cues from sources such as syntax,
acoustics and prosody to anticipate turn transitions
(Yngve, 1970). If such TURN-YIELDING CUES
could be modeled and incorporated in IVR sys-
tems, it should be possible to make faster, more
accurate turn-taking decisions, thus leading to a
more fluent interaction. Additionally, a better un-
derstanding of the mechanics of turn-taking could
be used to vary the speech output of IVR systems
to (i) produce turn-yielding cues when the sys-
tem is finished speaking and the user is expected
to speak next, and (ii) avoid producing such cues
when the system has more things to say. In this
paper we examine the existence of turn-yielding
cues in a large corpus of task-oriented dialogues
in Standard American English (SAE).
The question of what types of cues humans ex-
ploit for engaging in synchronized conversation
has been addressed by several studies. Duncan
(1972, inter alia) conjectures that speakers dis-
play complex signals at turn endings, composed
of one or more discrete turn-yielding cues, such
as the completion of a grammatical clause, or any
phrase-final intonation other than a plateau. Dun-
can also hypothesizes that the likelihood of a turn-
taking attempt by the listener increases linearly
with the number of such cues conjointly displayed
by the speaker. Subsequent studies have investi-
gated some of these hypotheses (Ford and Thomp-
son, 1996; Wennerstrom and Siegel, 2003). More
recent studies have investigated how to improve
IVR system?s the turn-taking decisions by incor-
porating some of the features found to correlate
with turn endings (Ferrer et al, 2003; Atterer et
al., 2008; Raux and Eskenazi, 2008). All of these
models are shown to improve over silence-based
techniques for predicting turn endings, motivating
further research. In this paper we present results
253
of a large, corpus-based study of turn-yielding
cues in the Columbia Games Corpus which veri-
fies some of Duncan?s hypotheses and adds addi-
tional cues to turn-taking behavior.
2 Materials and Method
The materials for our study are taken from the
Columbia Games Corpus (Gravano, 2009), a col-
lection of 12 spontaneous task-oriented dyadic
conversations elicited from 13 native speakers of
SAE. In each session, two subjects were paid to
play a series of computer games requiring verbal
communication to achieve joint goals of identify-
ing and moving images on the screen, while seated
in a soundproof booth divided by a curtain to en-
sure that all communication was verbal. The sub-
jects? speech was not restricted in any way, and
the games were not timed. The corpus contains
9 hours of dialogue, which were orthographically
transcribed; words were time-aligned to the source
by hand. Around 5.4 hours have also been into-
nationally transcribed using the ToBI framework
(Beckman and Hirschberg, 1994).
We automatically extracted a number of acous-
tic features from the corpus using the Praat toolkit
(Boersma and Weenink, 2001), including pitch,
intensity and voice quality features. Pitch slopes
were computed by fitting least-squares linear re-
gression models to the F0 track extracted from
given portions of the signal. Part-of-speech
(POS) tags were labeled automatically using Rat-
naparkhi?s maxent tagger trained on a subset of the
Switchboard corpus in lower-case with all punctu-
ation removed, to simulate spoken language tran-
scripts. All speaker normalizations were calcu-
lated using z-scores: z = (x ? ?)/?, where x
is a raw measurement, and ? and ? are the mean
and standard deviation for a speaker.
For our turn-taking studies, we define an
INTER-PAUSAL UNIT (IPU) as a maximal se-
quence of words surrounded by silence longer than
50 ms.1 A TURN then is defined as a maximal se-
quence of IPUs from one speaker, such that be-
tween any two adjacent IPUs there is no speech
from the interlocutor. Boundaries of IPUs and
turns are computed automatically from the time-
aligned transcriptions. Two trained annotators
classified each turn transition in the corpus using a
labeling scheme adapted from Beattie (1982) that
identifies, inter alia, SMOOTH SWITCHES ? tran-
150 ms was identified empirically to avoid stopgaps.
sitions from speaker A to speaker B such that (i)
A manages to complete her utterance, and (ii) no
overlapping speech occurs between the two con-
versational turns. Additionally, all continuations
from one IPU to the next within the same turn
were labeled automatically as HOLD transitions.
The complete labeling scheme is shown in the Ap-
pendix.
Our general approach consists in contrasting
IPUs immediately preceding smooth switches
(S) with IPUs immediately preceding holds (H).
(Note that in this paper we consider only non-
overlapping exchanges.) We hypothesize that
turn-yielding cues are more likely to occur before
S than before H. It is important to emphasize the
optionality of all turn-taking phenomena and de-
cisions: For H, turn-yielding cues ? whatever
their nature ? may still be present; and for S, they
may sometimes be absent. However, we hypothe-
size that their likelihood of occurrence should be
much higher before S. Finally, note that we do
not make claims regarding whether speakers con-
sciously produce turn-yielding cues, or whether
listeners consciously perceive and/or use them to
aid their turn-taking decisions.
3 Individual Turn-Yielding Cues
Figures 1 and 2 show the speaker-normalized
mean of a number of objective, automatically
computed variables for IPUs preceding S and H.
In all cases, one-way ANOVA and Kruskal-Wallis
tests reveal significant differences (at p < 0.001)
between the two groups. We we discuss these re-
sults in detail below.
3.1 Intonation
The literature contains frequent mention of the
propensity of speaking turns to end in any into-
nation contour other than a plateau (a sustained
pitch level, neither rising nor falling). We first
analyze the categorical prosodic labels in the por-
tion of the Columbia Games Corpus annotated us-
ing the ToBI annotations. We tabulate the phrase
S H
H-H% 484 22.1% 513 9.1%
[!]H-L% 289 13.2% 1680 29.9%
L-H% 309 14.1% 646 11.5%
L-L% 1032 47.2% 1387 24.7%
No boundary tone 16 0.7% 1261 22.4%
Other 56 2.6% 136 2.4%
Total 2186 100% 5623 100%
Table 1: ToBI phrase accents and boundary tones.
254
Figure 1: Individual turn-yielding cues: intonation, speaking rate and IPU duration.
accent and boundary tone labels assigned to the
end of each IPU, and compare their distribution
for the S and H turn exchange types, as shown
in Table 1. A chi-square test indicates that there
is a significant departure from a random distribu-
tion (?2=1102.5, df=5, p?0). Only 13.2% of
all IPUs immediately preceding a smooth switch
(S) ? where turn-yielding cues are most likely
present ? end in a plateau ([!]H-L%); most of the
remaining ones end in either a falling pitch (L-L%)
or a high rise (H-H%). For IPUs preceding a hold
(H) the counts approximate a uniform distribution,
with the plateau contours being the most common,
supporting the hypothesis that this contour func-
tions as a TURN-HOLDING CUE (that is, a cue that
typically prevents turn-taking attempts from the
listener). The high counts for the falling contour
preceding a hold (24.7%) may be explained by the
fact that, as discussed above, taking the turn is
optional for the listener, who may choose not to
act despite hearing some turn-yielding cues. It is
not entirely clear what the role is of the low-rising
contour (L-H%), as it occurs in similar proportions
before S and before H. Finally, we note that the ab-
sence of a boundary tone works as a strong indi-
cation that the speaker has not finished speaking,
since nearly all (98%) IPUs without a boundary
tone precede a hold transition.
Next, we examine four objective acoustic ap-
proximations of this perceptual feature: the ab-
solute value of the speaker-normalized F0 slope,
both raw and stylized, computed over the final 200
and 300 ms of each IPU. The case of a plateau
corresponds to a value of F0 slope close to zero;
the other case, of either a rising or a falling pitch,
corresponds to a high absolute value of F0 slope.
As shown in Figure 1, we find that the final slope
before S is significantly higher than before H in
all four cases. These findings provide additional
support to the hypothesis that turns tend to end in
falling and high-rising final intonations, and pro-
vide automatically identifiable indicators of this
turn-yielding cue.
3.2 Speaking rate
Duncan (1972) hypothesizes a ?drawl on the fi-
nal syllable or on the stressed syllable of a termi-
nal clause? [p. 287] as a turn-yielding cue, which
would probably correspond to a noticeable de-
crease in speaking rate. We examine this hypothe-
sis in our corpus using two common definitions of
speaking rate: syllables per second and phonemes
per second. Syllable and phoneme counts were
estimated from dictionary lookup, and word dura-
tions were extracted from the manual orthographic
alignments. Figure 1 shows that both measures,
computed over either the whole IPU or its final
word, are significantly higher before S than be-
fore H, which indicates an increase in speaking
rate before turn boundaries rather than Duncan?s
hypothesized drawl.
Furthermore, the speaking rate is, in both cases
(before S and before H), significantly slower on
the final word than over the whole IPU, a finding
that is in line with phonological theories that pre-
dict a segmental lengthening near prosodic phrase
boundaries (Wightman et al, 1992). This finding
may indeed correspond to the drawl or lengthen-
ing described by Duncan before turn boundaries.
However, it seems to be the case ? at least for
our corpus ? that the final lengthening tends to
occur at all phrase final positions, not just at turn
endings. In fact, our results indicate that the fi-
nal lengthening is more prominent in turn-medial
IPUs than in turn-final ones.
255
Figure 2: Individual turn-yielding cues: intensity, pitch and voice quality.
3.3 IPU duration and acoustic cues
In the Columbia Games Corpus, we find that turn-
final IPUs tend to be significantly longer than turn-
medial ones, both when measured in seconds and
in number of words (Figure 1). This suggests that
IPU duration could function as a turn-yielding cue,
supporting similar findings in perceptual experi-
ments by Cutler and Pearson (1986).
We also find that IPUs followed by S have a
mean intensity significantly lower than those fol-
lowed by H (computed over the IPU-final 500 and
1000 ms, see Figure 2). Also, the differences in-
crease when moving towards the end of the IPU.
This suggests that speakers tend to lower their
voices when approaching potential turn bound-
aries, whereas they reach turn-internal pauses with
a higher intensity.
Phonological theories conjecture a declination
in the pitch level, which tends to decrease grad-
ually within utterances, and across utterances
within the same discourse segment, as a conse-
quence of a gradual compression of the pitch range
(Pierrehumbert and Hirschberg, 1990). For con-
versational turns, then, we would expect to find
that speakers tend to lower their pitch level as they
reach potential turn boundaries. This hypothesis
is verified by the dialogues in our corpus, where
we find that IPUs preceding S have a significantly
lower mean pitch than those preceding H (Figure
2). In consequence, pitch level may also work as a
turn-yielding cue.
Next we examine three acoustic features asso-
ciated with the perception of voice quality: jit-
ter, shimmer and noise-to-harmonics ratio (NHR)
(Bhuta et al, 2004), computed over the IPU-final
500 and 1000 ms (Figure 2). We compute jit-
ter and shimmer only over voiced frames for im-
proved robustness. For all three features, the mean
value for IPUs preceding S is significantly higher
than for IPUs preceding H, with the difference in-
creasing towards the end of the IPU. Therefore,
voice quality seems to play a clear role as a turn-
yielding cue.
3.4 Lexical cues
Stereotyped expressions such as you know or I
think have been proposed in the literature as lex-
ical turn-yielding cues. However, in the Games
Corpus we find that none of the most frequent
IPU-final unigrams and bigrams, both preceding
S and H, correspond to such expressions (see Ta-
ble A.1 in the Appendix). Instead, such unigrams
and bigrams are specific to the computer games
in which the subjects participated. For example,
the game objects tended to be spontaneously de-
scribed by subjects from top to bottom and from
left to right, as shown in the following excerpt
(pauses are indicated with #):
A: I have a blue lion on top # with a lemon
in the bottom left # and a yellow crescent
moon in- # i- # in the bottom right
B: oh okay [...]
In consequence, bigrams such as lower right and
bottom right are common before S, while on top
or bottom left are common before H. These are all
task-specific lexical constructions and do not con-
stitute stereotyped expressions in the traditional
sense.
Also very common among the most frequent
IPU-final expressions are AFFIRMATIVE CUE
WORDS ? heavily overloaded words, such as
okay or yeah, that are used both to initiate and
to end discourse segments, among other functions
(Gravano et al, 2007). The occurrence of these
words does not constitute a turn-yielding or turn-
holding cue per se; rather, additional contextual,
acoustic and prosodic information is needed to dis-
ambiguate their meaning.
256
While we do not find clear examples of lexical
turn-yielding cues in our task-oriented corpus, we
do find two lexical turn-holding cues: word frag-
ments (e.g., incompl-) and filled pauses (e.g., uh,
um). Of the 8123 IPUs preceding H, 6.7% end
in a word fragment, and 9.4% in a filled pause.
By constrast, only 0.3% of the 3246 IPUs preced-
ing S end in a word fragment, and 1% in a filled
pause. These differences suggest that, after either
a word fragment or a filled pause, the speaker is
much more likely to intend to continue holding
the floor. This notion of disfluencies functioning
as a turn-taking cue has been studied by Goodwin
(1981), who shows that they may be used to secure
the listener?s attention at turn beginnings.
3.5 Textual completion
Several authors (Duncan, 1972; Ford and Thomp-
son, 1996; Wennerstrom and Siegel, 2003) claim
that some form of syntactic or semantic comple-
tion, independent of intonation and interactional
import, functions as a turn-yielding cue. Although
some call this syntactic completion, since all au-
thors acknowledge the need for semantic and dis-
course information in judging it, we choose the
more neutral term TEXTUAL COMPLETION for
this phenomenon. We annotated a portion of
our corpus with respect to textual completion and
trained a machine learning (ML) classifier to auto-
matically label the whole corpus. From these an-
notations we then examined how textual comple-
tion labels relate to turn-taking categories in the
corpus.
3.5.1. Manual labeling: In conversation, lis-
teners judge textual completion incrementally and
without access to later material. To simulate these
conditions in the labeling task, annotators were
asked to judge the textual completion of a turn up
to a target pause from the written transcript alone,
without listening to the speech. They were al-
lowed to read the transcript of the full previous
turn by the other speaker (if any), but they were
not given access to anything after the target pause.
These are two sample tokens:
A: the lion?s left paw our front
B: yeah and it?s th- right so the
A: and then a tea kettle and then the wine
B: okay well I have the big shoe and the wine
We selected 400 tokens at random from the Games
Corpus; the target pauses were also chosen at ran-
dom. Three annotators labeled each token inde-
pendently as either complete or incomplete ac-
cording to these guidelines: Determine whether
you believe what speaker B has said up to this
point could constitute a complete response to what
speaker A has said in the previous turn/segment.
Note: If there are no words by A, then B is begin-
ning a new task, such as describing a card or the
location of an object. To avoid biasing the results,
annotators were not given the turn-taking labels of
the tokens. Inter-annotator reliability is measured
by Fleiss? ? at 0.814, which corresponds to the
?almost perfect? agreement category. The mean
pairwise agreement between the three subjects is
90.8%. For the cases in which there is disagree-
ment between the three annotators, we adopt the
MAJORITY LABEL as our gold standard; that is,
the label chosen by two annotators.
3.5.2. Automatic classification: Next, we
trained a ML model using the 400 manually anno-
tated tokens as training data to automatically clas-
sify all IPUs in the corpus as either complete or in-
complete. For each IPU we extracted a number of
lexical and syntactic features from the current turn
up to the IPU itself: lexical identity of the IPU-
final word (w); POS tags and simplified POS tags
(N, V, Adj, Adv, Other) of w and of the IPU-final
bigram; number of words in the IPU; a binary flag
indicating if w is a word fragment; size and type of
the biggest (bp) and smallest (sp) phrase that end
in w; binary flags indicating if each of bp and sp is
a major phrase (NP, VP, PP, ADJP, ADVP); binary
flags indicating if w is the head of each of bp and
sp. We chose these features in order to capture as
much lexical and syntactic information as possible
from the transcripts. The syntactic features were
computed using two different parsers: the Collins
statistical parser (Collins, 2003) and CASS, a par-
tial parser especially designed for use with noisy
text (Abney, 1996). We experimented with the
learners listed in Table 2, using the implementa-
tions provided in the WEKA ML toolkit (Witten
and Frank, 2000). Table 2 shows the accuracy of
the majority-class baseline and of each classifier,
using 10-fold cross validation on the 400 train-
ing data points, and the mean pairwise agreement
by the three human labelers. The linear-kernel
support-vector-machine (SVM) classifier achieves
the highest accuracy, significantly outperforming
the baseline, and approaching the mean agreement
of human labelers.
257
Classifier Accuracy
Majority-class (?complete?) 55.2%
C4.5 (decision trees) 55.2%
Ripper (propositional rules) 68.2%
Bayesian networks 75.7%
SVM, RBF kernel (c = 1, ? = 10?12) 78.2%
SVM, linear kernel (c = 1, ? = 10?12) 80.0%
Human labelers (mean agreement) 90.8%
Table 2: Textual completion: ML results.
3.5.3. Results: First we examine the tokens that
were manually labeled by the human annotators.
Of the 100 tokens followed by S, 91 were labeled
textually complete, a significantly higher propor-
tion than the 42% followed by H that were labeled
complete (?2=51.7, df=1, p?0). Next, we used
our highest performing classifier, the linear-kernel
SVM, to automatically label all IPUs in the cor-
pus. Of the 3246 IPUs preceding S, 2649 (81.6%)
were labeled textually complete, and about half of
all IPUs preceding H (4272/8123, or 52.6%) were
labeled complete. The difference is also signifi-
cant (?2 =818.7, df = 1, p? 0). These results
suggest that textual completion as defined above
constitutes a necessary, but not sufficient, turn-
yielding cue.
4 Combining Turn-Yielding Cues
So far, we have shown strong evidence supporting
the existence of individual acoustic, prosodic and
textual turn-yielding cues. Now we shift our atten-
tion to the manner in which they combine together
to form more complex turn-yielding signals. For
each individual cue type, we choose two or three
features shown to correlate strongly with smooth
switches, as shown in Table 3 (e.g., the speaking
rate cue is represented by two automatic features:
syllables and phonemes per second over the whole
IPU). We consider a cue c to be PRESENT on IPU
u if, for any feature f modeling c, the value of f
on u is closer to fS than to fH , where fS and fH
are the mean values of f across all IPUs preced-
ing S and H, respectively. Otherwise, we say c is
ABSENT on u. Also, we automatically annotate all
IPUs in the corpus for textual completion using the
linear-kernel SVM classifier described in Section
3.5. IPUs classified as complete are considered to
bear the textual completion turn-yielding cue.
We first analyze the frequency of occurrence
of conjoined individual turn-yielding cues. Ta-
ble 4 shows the top frequencies of complex turn-
yielding cues for IPUs immediately before smooth
Individual cues Automatic features
Intonation
Abs(F0 slope) over IPU-final 200 ms
Abs(F0 slope) over IPU-final 300 ms
Speaking rate
Syllables per second over whole IPU
Phonemes per second over whole IPU
Intensity level
Mean intensity over IPU-final 500 ms
Mean intensity over IPU-final 1000 ms
Pitch level
Mean pitch over IPU-final 500 ms
Mean pitch over IPU-final 1000 ms
IPU duration
IPU duration in ms
Number of words in IPU
Voice quality
Jitter over IPU-final 500 ms
Shimmer over IPU-final 500 ms
NHR over IPU-final 500 ms
Table 3: Features used to estimate the presence of
individual turn-yielding cues.
switches (S) and holds (H). The most frequent
cases before S correspond to all, or almost all, cues
present at once. For IPUs preceding a hold (H),
the opposite is true: those with no cues, or with
just one or two, represent the most frequent cases.
S H
Cues Count Cues Count
1234567 267 ...4... 392
.234567 226 ......7 247
1234.67 138 ....... 223
.234.67 109 ...4..7 218
.23..67 98 ...45.. 178
..34567 94 .2....7 166
123..67 93 1234.67 163
.2.4567 73 .2..5.7 157
... ...
Total 3246 Total 8123
Table 4: Top frequencies of complex turn-yielding
cues for IPUs preceding S and H. A digit indicates
the presence of a specific cue; a dot, its absence.
1: Intonation; 2: Speaking rate; 3: Intensity level;
4: Pitch level; 5: IPU duration; 6: Voice quality;
7: Textual completion.
Table 5 shows the same results, now grouping
together all IPUs with the same number of cues,
independently of the cue types. Again, we observe
that larger proportions of IPUs preceding S present
more conjoined cues than IPUs preceding H.
Next we look at how the likelihood of a turn-
taking attempt varies with respect to the number
of individual cues displayed by the speaker, a rela-
tion hypothesized to be linear by Duncan (1972).
Figure 3 shows the proportion of IPUs with 0-7
cues present that are followed by a turn-taking at-
tempt from the interlocutor.2 The dashed line cor-
2 The proportion of turn-taking attempts is computed for
each cue count as the number of S and PI divided by the num-
ber of S, PI, H and BC, according to our labeling scheme.
258
Cue count S H
0 4 0.1% 223 2.7%
1 52 1.6% 970 11.9%
2 241 7.4% 1552 19.1%
3 518 16.0% 1829 22.5%
4 740 22.8% 1666 20.5%
5 830 25.6% 1142 14.1%
6 594 18.3% 611 7.5%
7 267 8.2% 130 1.6%
Total 3246 100% 8123 100%
Table 5: Distribution of the number of turn-
yielding cues displayed in IPUs preceding smooth
switches (S) and hold transitions (H).
Figure 3: Percentage of turn-taking attempts from
the listener (either S or PI) following IPUs con-
taining 0-7 turn-yielding cues.
responds to a linear model fitted to the data (Pear-
son?s correlation test: r2 = 0.969), and the contin-
uous line, to a quadratic model (r2 = 0.995). The
high correlation coefficient of the linear model
supports Duncan?s hypothesis, that the likelihood
of a turn-taking attempt by the interlocutor in-
creases linearly with the number of individual cues
displayed by the speaker. However, an ANOVA test
reveals that the quadratic model fits the data sig-
nificantly better than the linear model (F (1, 5) =
23.01; p = 0.005), even though the curvature of
the quadratic model is only moderate, as can be
observed in the figure.
5 Speaker Variation
To investigate possible speaker dependence in our
turn-yielding cues, we examine evidence for each
cue for each of our thirteen speakers. Table 6
summarizes this data. For each speaker, a check
(
?
) indicates that there is significant evidence of
the speaker producing the corresponding individ-
ual turn-yielding cue (at p < 0.05, using the same
statistical tests described in the previous sections).
Five speakers show evidence of all seven cues,
Speaker 101 102 103 104 105 106 107 108 109 110 111 112 113
Intonation
? ? ? ? ? ? ? ? ? ? ?
Spk. rate
? ? ? ? ? ? ? ? ? ? ? ? ?
Intensity
? ? ? ? ? ? ? ? ? ? ? ?
Pitch
? ? ? ? ? ? ?
Completion
? ? ? ? ? ? ? ? ? ? ? ? ?
Voice quality
? ? ? ? ? ? ? ? ? ? ? ? ?
IPU duration
? ? ? ? ? ? ? ? ? ? ? ? ?
LM r2 .92 .93 .82 .88 .97 .96 .95 .95 .97 .91 .95 .97 .89
QM r2 .98 .95 .95 .92 .98 .98 .96 .95 .99 .94 .98 .99 .90
Table 6: Summary of results for each individual
speaker.
while the remaining eight speakers show either
five or six cues. Pitch level is the least reliable
cue, present only for seven subjects. Notably, the
cues related to speaking rate, textual completion,
voice quality, and IPU duration are present for all
thirteen speakers.
The two bottom rows in Table 6 show the cor-
relation coefficients (r2) of linear and quadratic
regressions performed on the data from each
speaker. In all cases, the coefficients are very high.
The fit of the quadratic model is significantly bet-
ter for six speakers (shown in bold typeface); for
the remaining seven speakers, both models pro-
vide statistically indistinguishable explanations of
the data.
6 Discussion
We have examined seven turn-yielding cues ?
i.e., seven measurable events that take place with
a significantly higher frequency on IPUs preced-
ing smooth turn switches than on IPUs preceding
hold transitions. These events may be summarized
as follows: (i) a falling or high-rising intonation at
the end of the IPU; (ii) an increased speaking rate;
(iii) a lower intensity level; (iv) a lower pitch level;
(v) a longer IPU duration; (vi) a higher value of
three voice quality features: jitter, shimmer, and
NHR; and (vii) a point of textual completion. We
have also shown that, when several turn-yielding
cues occur simultaneously, the likelihood of a sub-
sequent turn-taking attempt by the interlocutor in-
creases in an almost linear fashion.
We propose that these findings can be used to
improve some turn-taking decisions of state-of-
the-art IVR systems. For example, if a system
wishes to yield the floor to a user, it should in-
clude in its output as many of the described cues
as possible. Conversely, when the user is speak-
ing, the system may detect appropriate moments
to take the turn by estimating the presence of turn-
259
yielding cues at every silence. If the number of de-
tected cues is high enough, then the system should
take the turn; otherwise, it should remain silent.
Two assumptions of our study are that turn-
yielding cues are binary and all contribute equally
to the overall ?count?. In future research we
will explore alternative methods of combining and
weighting the different features ? by means of
multiple linear regression, for example ? in or-
der to experiment with more sophisticated models
of turn-yielding behavior. We also plan to exam-
ine new turn-yielding cues, paying special atten-
tion to additional voice quality features, given the
promising results obtained for jitter, shimmer and
noise-to-harmonics ratio.
7 Acknowledgements
This work was funded in part by NSF IIS-
0307905. We thank Stefan Benus, Enrique Hen-
estroza, Elisa Sneed and Gregory Ward, for valu-
able discussion and for their help in collecting and
labeling the data, and the anonymous reviewers for
helpful comments and suggestions.
References
S. Abney. 1996. Partial parsing via finite-state cas-
cades. Journal of Natural Language Engineering,
2(4):337?344.
M. Atterer, T. Baumann, and D. Schlangen. 2008. To-
wards incremental end-of-utterance detection in di-
alogue systems. In Proceedings of Coling, Manch-
ester, UK.
G. W. Beattie. 1982. Turn-taking and interruption
in political interviews: Margaret Thatcher and Jim
Callaghan compared and contrasted. Semiotica,
39(1/2):93?114.
M. E. Beckman and J. Hirschberg. 1994. The ToBI
annotation conventions. Ohio State University.
T. Bhuta, L. Patrick, and J. D. Garnett. 2004. Per-
ceptual evaluation of voice quality and its correla-
tion with acoustic measurements. Journal of Voice,
18(3):299?304.
P. Boersma and D. Weenink. 2001. Praat: Doing pho-
netics by computer. http://www.praat.org.
M. J. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?637.
E. A. Cutler and M. Pearson. 1986. On the analysis of
prosodic turn-taking cues. In C. Johns-Lewis, Ed.,
Intonation in Discourse, pp. 139?156. College-Hill.
S. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23(2):283?292.
L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A
prosody-based approach to end-of-utterance detec-
tion that does not require speech recognition. In
Proceedings of ICASSP.
C. E. Ford and S. A. Thompson. 1996. Interactional
units in conversation: Syntactic, intonational and
pragmatic resources for the management of turns. In
E. Ochs, E. A. Schegloff, and S. A. Thompson, Eds.,
Interaction and Grammar, pp. 134?184. Cambridge
University Press.
C. Goodwin. 1981. Conversational Organization:
Interaction between Speakers and Hearers. Aca-
demic Press.
A. Gravano, S. Benus, J. Hirschberg, S. Mitchell, and
I. Vovsha. 2007. Classification of discourse func-
tions of affirmative words in spoken dialogue. In
Proceedings of Interspeech.
A. Gravano. 2009. Turn-Taking and Affirmative Cue
Words in Task-Oriented Dialogue. Ph.D. thesis,
Columbia University, New York.
J. Pierrehumbert and J. Hirschberg. 1990. The mean-
ing of intonational contours in the interpretation of
discourse. In P. Cohen, J. Morgan, and M. Pol-
lack, Eds., Intentions in Communication, pp. 271?
311. MIT Pr.
A. Raux and M. Eskenazi. 2008. Optimizing endpoint-
ing thresholds using dialogue features in a spoken
dialogue system. In Proceedings of SIGdial.
A. Raux, D. Bohus, B. Langner, A. W. Black, and
M. Eskenazi. 2006. Doing research on a deployed
spoken dialogue system: One year of Let?s Go! ex-
perience. In Proceedings of Interspeech.
N. G. Ward, A. G. Rivera, K. Ward, and D. G. Novick.
2005. Root causes of lost time and user stress in
a simple dialog system. In Proceedings of Inter-
speech.
A. Wennerstrom and A. F. Siegel. 2003. Keeping the
floor in multiparty conversations: Intonation, syn-
tax, and pause. Discourse Processes, 36(2):77?107.
C. W. Wightman, S. Shattuck-Hufnagel, M. Ostendorf,
and P. J. Price. 1992. Segmental durations in the
vicinity of prosodic phrase boundaries. The Journal
of the Acoustical Society of America, 91:1707.
I. H. Witten and E. Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
V. H. Yngve. 1970. On getting a word in edgewise.
Sixth Regional Meeting of the Chicago Linguistic
Society, 6:657?677.
260
For each turn by speaker S2, where S1 is the other speaker, label S2?s turn as follows:
Is S2?s utterance in response to S1?s utterance and indicates only
?I?m still here / I hear you and please continue??(1)


no
Simultaneous speech present?


yes
S2 is successful?


yes
S1?s utterance
complete?(2)

yes
Overlap
(O)
HHH
Hj
no
Interruption
(I)
H
HHHj
no
Butting-in
(BI)
HH
HHj
no
S1?s utterance
complete?(2)


yes
Smooth
switch (S)
HH
HHj
no
Pause interruption
(PI)
HH
HHj
yes
Simultaneous speech present?


yes
Backchannel
with overlap
(BC O)
HHH
Hj
no
Backchannel
(BC)
Figure A.1: Turn-taking labeling scheme.
Appendix: Turn-Taking Labeling Scheme
We adopt a slightly modified version of Beat-
tie?s (1982) labeling scheme, depicted in Fig-
ure A.1. We incorporate backchannels (excluded
from Beattie?s study) by adding the decision
marked (1) at the root of the decision tree, for
which we use the annotations described in Gra-
vano et al (2007). For the decision marked (2), we
use Beattie?s informal definition of utterance com-
pleteness: ?Completeness [is] judged intuitively,
taking into account the intonation, syntax, and
meaning of the utterance? [p. 100]. All continu-
ations from one IPU to the next within the same
turn are labeled automatically H, for ?hold?. Also,
we identify three special cases that do not corre-
spond to actual turn exchanges:
Task beginnings: Turns beginning a new game
task are labeled X1.
Continuations after BC or BC O: If a turn t is a
continuation after a backchannel b from the other
speaker, it is labeled X2 O if t and b overlap, or
X2 if not.
Simultaneous starts: Fry (1975) reports that hu-
mans require at least 210 ms to react verbally to a
verbal stimulus.3 Thus, if two turns begin within
210 ms of each other, they are most probably con-
nected to preceding events than to one another. In
Figure A.2, A1, A2 and B1 represent turns from
speakers A and B. Most likely, A2 is simply a
continuation from A1, and B1 occurs in response
3D. B. Fry. 1975. Simple reaction-times to speech and
non-speech stimuli. Cortex, 11(4):355-60.
to A1. Thus, B1 is labeled with respect to A1 (not
A2), and A2 is labeled X3.
A1 A2x
B1y
Figure A.2: Simultaneous start (|y?x| < 210ms).
S Count H Count
okay 241 okay 402
yeah 167 on top 172
lower right 85 um 136
bottom right 74 the top 117
the right 59 of the 67
hand corner 52 blue lion 57
lower left 43 bottom left 56
the iron 37 with the 54
the onion 33 the um 54
bottom left 31 yeah 53
the ruler 30 the left 48
mm-hm 30 and 48
right 28 lower left 46
right corner 27 uh 45
the bottom 26 oh 45
the left 24 and a 45
crescent moon 23 alright 44
the lemon 22 okay um 43
the moon 20 the uh 42
tennis racket 20 the right 41
blue lion 19 the bottom 39
the whale 18 I have 39
the crescent 18 yellow lion 37
the middle 17 the middle 37
of it 17 I?ve got 34
... ...
Total 3246 Total 8123
Table A.1: 25 most frequent final bigrams preced-
ing smooth turn switches (S) and hold transitions
(H). (See Section 3.4.)
261
Affirmative Cue Words in Task-Oriented Dialogue
Agust?n Gravano?
Universidad de Buenos Aires
Julia Hirschberg??
Columbia University
?tefan Ben?u??
Constantine the Philosopher University
and Institute of Informatics,
Slovak Academy of Sciences
We present a series of studies of affirmative cue words?a family of cue words such as ?okay? or
?alright" that speakers use frequently in conversation. These words pose a challenge for spoken
dialogue systems because of their ambiguity: They may be used for agreeing with what the in-
terlocutor has said, indicating continued attention, or for cueing the start of a new topic, among
other meanings. We describe differences in the acoustic/prosodic realization of such functions in
a corpus of spontaneous, task-oriented dialogues in Standard American English. These results
are important both for interpretation and for production in spoken language applications. We
also assess the predictive power of computational methods for the automatic disambiguation of
these words. We find that contextual information and final intonation figure as the most salient
cues to automatic disambiguation.
1. Introduction
CUE PHRASES are linguistic expressions that may be used to convey explicit information
about the discourse or dialogue, or to convey a more literal, semantic contribution.
They aid speakers and writers in organizing the discourse, and listeners and readers in
processing it. In previous literature, these constructions have also been termed discourse
markers, pragmatic connectives, discourse operators, and clue words. Examples of cue
phrases include now, well, so, and, but, then, after all, furthermore, however, in consequence,
as a matter of fact, in fact, actually, okay, alright, for example, and incidentally.
The ability to correctly determine the function of cue phrases is critical for important
natural language processing tasks, including anaphora resolution (Grosz and Sidner
1986), argument understanding (Cohen 1984), plan recognition (Grosz and Sidner 1986;
Litman and Allen 1987), and discourse segmentation (Litman and Passonneau 1995).
? Departamento de Computaci?n, FCEyN, Universidad de Buenos Aires, Pabell?n I, Ciudad Universitaria,
(C1428EGA) Buenos Aires, ARGENTINA. E-mail: gravano@dc.uba.ar.
?? E-mail: julia@cs.columbia.edu.
? E-mail: sbenus@ukf.sk.
Submission received: 9 December 2009; revised submission received: 2 February 2011; accepted for
publication: 13 March 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 1
Furthermore, correctly determining the function of cue phrases using features of the
surrounding text can be used to improve the naturalness of synthetic speech in text-to-
speech systems (Hirschberg 1990).
In this study, we focus on a subclass of cue phrases that we term affirmative
cue words (hereafter, ACWs), and that include alright, mm-hm, okay, right, and
uh-huh, inter alia. These words are frequent in spontaneous conversation, especially in
task-oriented dialogue, and are heavily overloaded: Their possible discourse/pragmatic
functions include agreeing with what the interlocutor has said, displaying interest and
continued attention, and cueing the start of a new topic. Some ACWs (e.g., alright, okay)
are capable of conveying as many as ten different functions, as described in Section 3.
Whereas ACWs thus form a subset of more general classes of utterances which have
been studied in more general studies of cue words, cue phrases, discourse markers, feedback
utterances, linguistic feedback, acknowledgments, grounding acts, our focus is on this par-
ticular subset of lexical items which may convey an affirmative response?but which
may also convey many different meanings. The disambiguation of these meanings we
believe is critical to the success of spoken dialogue systems.
In the studies presented here, our goal is to extend our understanding of ACWs,
in particular by finding descriptions of the acoustic/prosodic characteristics of their
different functions, and by assessing the predictive power of computational methods for
their automatic disambiguation. This knowledge should be helpful in spoken language
generation and understanding tasks, including interactive spoken dialogue systems and
applications doing off-line analyses of conversational data, such as meeting segmenta-
tion and summarization. For example, spoken dialogue systems lacking a model of the
appropriate realization of different uses of these words are likely to have difficulty in
understanding and communicating with their users, either by producing cue phrases
in a way that does not convey the intended meaning or by misunderstanding users?
productions.
This article is organized as follows. Section 2 reviews previous literature. In Sec-
tion 3 we describe the materials used in the present study from the Columbia Games
Corpus. Section 4 presents a statistical description of the acoustic, prosodic, and con-
textual characteristics of the functions of ACWs in this corpus. In Section 5 we describe
results from a number of machine learning experiments aimed at investigating how
accurately ACWs may be automatically classified into their various functions. Finally,
in Section 6 we summarize and discuss our main findings.
2. Previous Work
Cue phrases have received extensive attention in the computational linguistics litera-
ture. Early work by Cohen (1984) presents a computational justification for the impor-
tance of cue phrases in discourse processing. Using a simple propositional framework
for analyzing discourse, Cohen claims that, in some cases, cue phrases decrease the
number of operations required by the listener to process ?coherent transmissions?; in
other cases, cue phrases are necessary to allow the recognition of ?transmissions which
would be incoherent (too complex to reconstruct) in the absence of clues? (page 251).
Reichman (1985) proposes a model of discourse structure in which discourse com-
prises a collection of basic constituents called context spaces, organized hierarchically
according to semantic and logical relations called conversational moves. In Reichman?s
model, cue phrases are portrayed as mechanisms that signal context space boundaries,
specifying the kind of conversational move about to take place. Grosz and Sidner (1986)
introduce an alternative model of discourse structure formed by three interrelated
2
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
components: a linguistic structure, an intentional structure, and an attentional state. In
this model, cue phrases play a central role, allowing the speaker to provide information
about all of the following to the listener:
1) that a change of attention is imminent; 2) whether the change returns to a previous
focus space or creates a new one; 3) how the intention is related to other intentions;
4) what precedence relationships, if any, are relevant (page 196).
In a corpus study of spontaneous conversations, Schiffrin (1987) describes cue phrases
as syntactically detachable from a sentence, commonly used in initial position within
utterances, capable of operating at both local and global levels of discourse, and having
a range of prosodic contours. As other authors, Schiffrin observes that cue phrases
provide contextual coordinates for an utterance in the discourse?that is, they indicate
the discourse segment to which an utterance belongs. However, she suggests that cue
phrases only display discourse structure relations; they do not create them. In a critique
of Schiffrin?s work, Redeker (1991) proposes defining cue phrases as phrases ?uttered
with the primary function of bringing to the listener?s attention a particular kind of
linkage of the upcoming utterance with the immediate discourse context? (page 1169).
Prior work on the automatic classification of cue phrases includes a series of studies
performed by Hirschberg and Litman (Hirschberg and Litman 1987, 1993; Litman and
Hirschberg 1990), which focus on differentiating between the discourse and sentential
senses of single-word cue phrases such as now, well, okay, say, and so in American
English. When used in a discourse sense, a cue phrase explicitly conveys information
about the discourse structure; when used in a sentential sense, a cue phrase instead
conveys semantic information. Hirschberg and Litman present two manually devel-
oped classification models, one based on prosodic features, and one based on textual
features. This line of research is further pursued by Litman (1994, 1996), who incorpo-
rates machine learning techniques to derive classification models automatically. Litman
uses different combinations of prosodic and text-based features to train decision-tree
and rule learners, and shows that machine learning constitutes a powerful tool for
developing automatic classifiers of cue phrases into their sentential and discourse uses.
Zufferey and Popescu-Belis (2004) present a similar study on the automatic classification
of like and well into discourse and sentential senses, achieving a performance close to
that of human annotators.
Besides the binary division of cue phrases into discourse vs. sentential meanings,
the Conversational Analysis (CA) literature describes items it terms linguistic feedback
or acknowledgments. These include not only the computational linguists? cue phrases
but also expressions such as I see or oh wow, which CA research describes in terms of
attention, understanding, and acceptance by the speaker of a proposition uttered by
another conversation participant (Kendon 1967; Yngve 1970; Duncan 1972; Schegloff
1982; Jefferson 1984). Such items typically occur at the second position in common
adjacency pairs and include backchannels (also referred to as continuers), which
?exhibit on the part of [their] producer an understanding that an extended unit of talk
is underway by another, and that it is not yet, or may not be (even ought not yet be)
complete; [they take] the stance that the speaker of that extended unit should continue
talking? (Schegloff 1982, page 81), and agreements, which indicate the speaker?s
agreement with a statement or opinion expressed by another speaker. Allwood,
Nivre, and Ahlsen (1992) distinguish four basic communicative functions of linguistic
feedback which enable conversational partners to exchange information: contact,
perception, understanding, and attitudinal reactions. These correspond respectively
3
Computational Linguistics Volume 38, Number 1
to whether the interlocutor is willing and able to continue the interaction, perceive the
message, understand the message, and react and respond to the message. Allwood,
Nivre, and Ahlsen posit that ?simple feedback words, like yes, [...] involve a high
degree of context dependence? (page 5), and suggest that their basic communicative
function strongly depends on the type of speech act, factual polarity, and information
status of the immediately preceding communicative act. Novick and Sutton (1994)
propose an alternative categorization of linguistic feedback in task-oriented dialogue,
which is based on the structural context of exchanges rather than on the characteristics
of the preceding utterance. The three main classes in Novick and Sutton?s catalogue
are: (i) other ? ackn, where an acknowledgment immediately follows a contribution by
other speaker; (ii) self ? other ? ackn, where self initiates an exchange, other eventually
completes it, and self utters an acknowledgment; and (iii) self + ackn, where self includes
an acknowledgment in an utterance independently of other?s previous contribution.
Substantial attention has been paid to subsets and supersets of words we include
in our class of ACWs in the psycholinguistic literature in studies of grounding?
the process by which conversants obtain and maintain a common ground of mutual
knowledge, mutual beliefs, and mutual assumptions over the course of a conversation
(Clark and Schaefer 1989; Clark and Brennan 1991). Computational work on grounding
has been pursued for a number of years by Traum and colleagues (e.g., Traum and Allen
1992; Traum 1994), who recently have described a corpus-based study of lexical and
semantic evidence supporting different degrees of grounding (Roque and Traum 2009).
Our ACWs often occur in the process of establishing such common ground.
Prosodic characteristics of the responses involved in grounding have been studied in
the Australian English Map Task corpus by Mushin et al (2003), who find that these
utterances often consist of acknowledgment contributions such as okay or yeh produced
with a ?non-final? intonational contour, and followed by speech by the same speaker
which appears to continue the intonational phrase. Studies by Walker of informa-
tionally redundant utterances (IRUs) (Walker 1992, 1996), utterances which express
?a proposition already entailed, presupposed or implicated by a previous utterance
in the same discourse situation? (Walker 1993a, page 12), also include some of our
ACWs, such as IRU prompts (e.g., uh-huh), which, according to Walker, ?add no new
propositional content to the common ground? (Walker 1993a, page 32). Walker adopts
the term ?continuer? from the Conversational Analysis school to further describe these
prompts (Walker 1993a). Walker describes some intonational contours which are used
to realize IRUs in generation in Walker (1993a) and in Walker (1993b), examining 63 IRU
tokens and finding five different types of contour used among them.
As part of a larger project on automatically detecting discourse structure for speech
recognition and understanding tasks in American English, Jurafsky et al (1998) present
a study of four particular discourse/pragmatic functions, or dialog acts (Bunt 1989;
Core and Allen 1997), closely related to ACWs: backchannel, agreement, incipient
speakership (indicating an intention to take the floor), and yes-answer (affirmative
answer to a yes?no question). The authors examine 1,155 conversations from the Switch-
board database (Godfrey, Holliman, and McDaniel 1992), and report that the vast ma-
jority of these four dialogue acts are realized with words like yeah, okay, or uh-huh. They
find that the lexical realization of the dialogue act is the strongest cue to its identity (e.g.,
backchannel is the preferred function for uh-huh and mm-hm), and report preliminary
results on some prosodic differences across dialogue acts: Backchannels are shorter in
duration, have lower pitch and intensity, and are more likely to end in a rising intonation
than agreements. Two related studies, part of the same project, address the automatic
classification of dialogue acts in conversational speech (Shriberg et al 1998; Stolcke
4
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
et al 2000). The results of their machine learning experiments, conducted on the same
subset of Switchboard used previously, indicate a high degree of confusion between
agreements and backchannels, because both classes share words such as yeah and right.
They also show that prosodic features (including duration, pause, and intensity) can aid
the automatic disambiguation between these two classes: A classifier trained using both
lexical and prosodic features slightly yet significantly outperforms one trained using
just lexical features.
There is also considerable evidence that linguistic feedback does not take place
at arbitrary locations in conversation; rather, it mostly occurs at or near transition-
relevance places for turn-taking (Sacks, Schegloff, and Jefferson 1974; Goodwin 1981).
Ward and Tsukahara (2000) describe, in both Japanese and American English, a region
of low pitch lasting at least 110 msec which may function as a prosodic cue inviting
the realization of a backchannel response from the interlocutor. In a corpus study
of Japanese dialogues, Koiso et al (1998) find that both syntax and prosody play a
central role in predicting the occurrence of backchannels. Cathcart, Carletta, and Klein
(2003) propose a method for automatically predicting the placement of backchannels
in Scottish English conversation, based on pause durations and part-of-speech tags,
that outperforms a random baseline model. Recently, Gravano and Hirschberg (2009a,
2009b, 2011) describe six distinct prosodic, acoustic, and lexical events in American
English speech that tend to precede the occurrence of a backchannel by the interlocutor.
Despite their high frequency in spontaneous conversation, the set of ACWs we
examine here have seldom, if ever, been an object of study in themselves, as a separate
subclass of cue phrases or dialogue acts. Some have attempted to model other types
of cue phrases (e.g., well, like) or cue phrases in general; others discuss discourse/
pragmatic functions that may be conveyed through ACWs, but which may also be
conveyed through other types of expressions (e.g., agreements may be communicated
by single words such as yes or longer cue phrases such as that?s correct). Subsets of ACWs
have been studied in very small corpora, with some proposals about their prosodic
and functional variations. For example, Hockey (1993) examines the prosodic variation
of two ACWs, okay and uh-huh (66 and 77 data points, respectively) produced as full
intonational phrases in two spontaneous task-oriented dialogues. She groups the F0
contours visually and auditorily, and shows that instances of okay produced with a
high-rise contour are significantly more likely to be followed by speech from the other
speaker than from the same speaker. The results of a perception experiment conducted
by Gravano et al (2007) suggest that, in task-oriented American English dialogue,
contextual information (e.g., duration of surrounding silence, number of surrounding
words) as well as word-final intonation figure as the most salient cues to disambiguation
of the function of the word okay by human listeners. Also, in a study of the function of
intonation in Scottish English task-oriented dialogue, Kowtko (1996) examines a corpus
of 273 instances of single-word utterances, including affirmative cue words such as mm-
hm, okay, right, uh-huh, and yes. Kowtko finds a significant correlation between discourse
function and intonational contour: The align function (which checks that the listener?s
understanding aligns with that of the speaker) is shown to correlate with rising in-
tonational contours; the ready function (which cues the speaker?s intention to begin
a new task) and the reply-y function (which ?has an affirmative surface and usually
indicates agreement?; Kowtko 1996, page 59) correlate with a non-rising intonation;
and the acknowledge function (which indicates having heard and understood) presents
all types of final intonation. It is important to note, however, that different dialects
and different languages have distinct ways of realizing different discourse/pragmatic
functions, so it is unclear how useful these results are for American English.
5
Computational Linguistics Volume 38, Number 1
Although broader studies focusing on the pragmatic function of cue phrases, dis-
course markers, linguistic feedback, and dialogue acts do shed light on the particular
subset of utterances we are studying, and although there is some information on par-
ticular lexical items we include here in our study, the class of ACWs itself has received
little attention. Particularly given the frequency of ACWs in dialogue, it is important
to identify reliable and automatically extractable cues to their disambiguation, so that
spoken dialogue systems can recognize the pragmatic function of ACWs in user input
and can produce ACWs that are less likely to be misinterpreted in system output.
3. Materials
The materials for all experiments in this study were taken from the Columbia Games
Corpus, a collection of 12 spontaneous task-oriented dyadic conversations elicited from
13 native speakers (6 female, 7 male) of Standard American English (SAE). A detailed
description of this corpus is given in Appendix A. In each session, two subjects were
paid to play a series of computer games requiring verbal communication to achieve
joint goals of identifying and moving images on the screen. Each subject used a separate
laptop computer; they sat facing each other in a soundproof booth, with an opaque
curtain hanging between to allow only verbal communication.
Each session contains an average of 45 minutes of dialogue, totaling roughly 9 hours
of dialogue in the corpus. Trained annotators orthographically transcribed the re-
cordings and manually aligned the words to the speech signal, yielding a total of
70,259 words and 2,037 unique words in the corpus. Additionally, self repairs and
certain non-word vocalizations were marked, including laughs, coughs, and breaths.
For roughly two thirds of the corpus, intonational patterns and other aspects of the
prosody were identified by trained annotators using the ToBI transcription framework
(Beckman and Hirschberg 1994; Pitrelli, Beckman, and Hirschberg 1994).
3.1 Affirmative Cue Words in the Games Corpus
Throughout the Games Corpus, subjects made frequent use of affirmative cue words:
The 5,456 instances of affirmative cue words alright, gotcha, huh, mm-hm, okay, right,
uh-huh, yeah, yep, yes, and yup account for 7.8% of the total words in the corpus. Because
the usage of these words seems to vary significantly in meaning, we asked three labelers
to independently classify all occurrences of these 11 words in the entire corpus into the
ten discourse/pragmatic functions listed in Table 1.
Among the distinctions we make in these pragmatic functions, we note particularly
that our categories of Agr and BC differ primarily in that Agr is defined as indicating
belief in or agreement with the interlocutor (e.g., a response to a yes?no question),
whereas BC indicates only continued attention.1
1 Our definition of BC is similar to definitions of backchannel and continuer as discussed by a number
of authors in the Conversational Analysis and spoken language processing communities (e.g.,
Stolcke et al?s [2000] ?a short utterance that plays discourse structuring roles, e.g., indicating that the
speaker should go on talking? [page 345]; and Cathcart et al?s [2003] ?utterances, with minimal content,
used to clearly signal that the speaker should continue with her current turn? [page 51]). Although some
definitions of BC also include the notion that the speaker is indicating understanding, we did not ask
annotators to make this distinction. We note further that, although it is also possible (cf. Clark and
Schaefer 1989) to signal understanding without agreement, in the process of designing the labeling
scheme we did not find instances of ACWs that seemed to us to have this function in our corpus; nor
did our labelers find such cases. Hence we did not include this distinction among our classes.
6
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Table 1
Labeled discourse/pragmatic functions of affirmative cue words.
Agr Agreement. Indicates I believe what you said, and/or I agree with what you say.
BC Backchannel. Indicates only I hear you and please continue, in response to another
speaker?s utterance.
CBeg Cue beginning discourse segment. Marks a new segment of a discourse or a new
topic.
CEnd Cue ending discourse segment. Marks the end of a current segment of a discourse
or a current topic.
PBeg Pivot beginning (Agr+CBeg). Functions both to agree and to cue a beginning
segment.
PEnd Pivot ending (Agr+CEnd). Functions both to agree and to cue the end of the
current segment.
Mod Literal modifier. Examples: I think that?s okay; to the right of the lion.
BTsk Back from a task. Indicates I?ve just finished what I was doing and I?m back.
Chk Check. Used with the meaning Is that okay?
Stl Stall. Used to stall for time while keeping the floor.
? Cannot decide.
Labelers were given examples of each category, and annotated with access to both
transcript and speech source. The guidelines used by the annotators are presented in
Appendix B. Appendix C includes some examples of each class of ACWs, as labeled
by our annotators. Inter-labeler reliability was measured by Fleiss?s ? (Fleiss 1971) as
Substantial at 0.745.2 We define the majority label of a token as the label chosen for that
token by at least two of the three labelers; we assign the ??? label to a token either when
its majority label is ???, or when it was assigned a different label by each labeler. Of the
5,456 affirmative cue words in the corpus, 5,185 (95%) have a majority label other than
??.? Table 2 shows the distribution of discourse/pragmatic functions over ACWs in the
whole corpus.
3.2 Data Downsampling
Some of the word/function pairs in Table 2 are skewed to contributions from a few
speakers. For example, for backchannel (BC) uh-huh, as many as 65 instances (44%) are
from one single speaker, and the remaining 83 are from seven other speakers. In cases
like this, using the whole sample would pose the risk of drawing false conclusions on
the usage of ACWs, possibly influenced by stylistic properties of individual speakers.
Therefore, we downsampled the tokens of ACWs in the Games Corpus to obtain a
balanced data set, with instances of each word and function coming in similar propor-
tions from as many speakers as possible. Specifically, we downsampled our data using
the following procedure: First, we discarded all word/function pairs with tokens from
fewer than four different speakers; second, for each of the remaining word/function
pairs, we discarded tokens (at random) from speakers who contributed more than 25%
of its tokens. In other words, the resulting data set meets two conditions: For each word/
2 The ? measure of agreement above chance is interpreted as follows: 0 = None, 0?0.2 = Small,
0.2?0.4 = Fair, 0.4?0.6 = Moderate, 0.6?0.8 = Substantial, 0.8?1 = Almost perfect.
7
Computational Linguistics Volume 38, Number 1
Table 2
Distribution of function over ACW. Rest = {gotcha, huh, yep, yes, yup}.
alright mm-hm okay right uh-huh yeah Rest Total
Agr 76 58 1,092 111 18 754 116 2,225
BC 6 395 120 14 148 69 5 757
CBeg 83 0 543 2 0 2 0 630
CEnd 6 0 6 0 0 0 0 12
PBeg 4 0 65 0 0 0 0 69
PEnd 11 12 218 2 0 20 15 278
Mod 5 0 18 1,069 0 0 0 1,092
BTsk 7 1 32 0 0 0 0 40
Chk 1 0 6 49 0 1 6 63
Stl 1 0 15 1 0 2 0 19
? 36 12 150 10 3 55 5 271
Total 236 478 2,265 1,258 169 903 147 5,456
function pair, (a) tokens come from at least four different speakers, and (b) no single
speaker contributes more than 25% of the tokens. The two thresholds were found via
a grid search, and were chosen as a trade-off between size and representativeness of
the data set. With this procedure we discarded 506 tokens of ACWs, or 9.3% of such
words in the corpus. Table 3 shows the resulting distribution of discourse/pragmatic
functions over ACWs in the whole corpus after downsampling the data. The ? measure
of inter-labeler reliability was practically identical for the downsampled data, at 0.751.
3.3 Feature Extraction
We extracted a number of lexical, discourse, timing, phonetic, acoustic, and prosodic
features for each target ACW, which we use in the statistical analysis and machine
learning experiments presented in the following sections. Tables 4 through 8 summarize
the full feature set. For simplicity, in those tables each line may describe one or more
features. Features that may be extracted by on-line applications are marked with letter
O; this is further explained later in this section.
Table 3
Distribution of function over ACW, after downsampling. Rest = {gotcha, huh, yep, yes, yup}.
alright mm-hm okay right uh-huh yeah Rest Total
Agr 76 58 1,092 74 16 754 87 2,157
BC 0 395 120 0 101 58 0 674
CBeg 61 0 543 0 0 0 0 604
CEnd 0 0 4 0 0 0 0 4
PBeg 0 0 64 0 0 0 0 64
PEnd 10 4 218 0 0 18 0 250
Mod 4 0 18 1,069 0 0 0 1,091
BTsk 5 0 28 0 0 0 0 33
Chk 0 0 5 49 0 0 4 58
Stl 0 0 15 0 0 0 0 15
Total 156 457 2,107 1,192 117 830 91 4,950
8
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Table 4
Lexical and discourse features. Each line may describe one or more features. Features marked O
may be available in on-line conditions.
Lexical features
O Lexical identity of the target word (w).
O Part-of-speech tag of w, original and simplified.
O Word immediately preceding w, and its original and simplified POS tags. If w is preceded
by silence, this feature takes value ?#?.
O Word immediately following w, and its original and simplified POS tags. If w is followed
by silence, this feature takes value ?#?.
Discourse features
O Number of words in w?s IPU.
O Number and proportion of words in w?s IPU before and after w.
O Number of words uttered by the other speaker during w?s IPU.
O Number of words in the previous turn by the other speaker.
Number of words in w?s turn.
Number and proportion of words and IPUs in w?s turn before and after w.
Number and proportion of turns in w?s task before and after w.
Number of words uttered by the other speaker during w?s turn.
Number of words in the following turn by the other speaker.
Number of ACWs in w?s turn other than w.
Our lexical features consist of the lexical identity and the part-of-speech (POS) tag
of the target word (w), the word immediately preceding w, and the word immediately
following w (see Table 4). POS tags were labeled automatically for the whole corpus
using Ratnaparkhi, Brill, and Church?s (1996) maxent tagger trained on a subset of the
Switchboard corpus (Charniak and Johnson 2001) in lower-case with all punctuation
removed, to simulate spoken language transcripts. Each word had an associated POS
tag from the full Penn Treebank tag set (Marcus, Marcinkiewicz, and Santorini 1993),
and one of the following simplified tags: noun, verb, adjective, adverb, contraction,
or other.
For our discourse features, listed in Table 4, we define an inter-pausal unit (IPU) as
a maximal sequence of words surrounded by silence longer than 50 msec. A turn is a
maximal sequence of IPUs from one speaker, such that between any two adjacent IPUs
there is no speech from the interlocutor.3,4 Boundaries of IPUs and turns are computed
automatically from the time-aligned transcriptions. A task in the Games Corpus cor-
responds to a simple game played by the subjects, requiring verbal communication to
achieve a joint goal of identifying and moving images on the screen (see Appendix A for
a description of these game tasks). Task boundaries are extracted from the logs collected
automatically during the sessions, and subsequently checked by hand. Our discourse
features are intended to capture discrete positional information of the target word, in
relation to its containing IPU, turn, and task.
3 Here ?between? refers strictly to the time after the end point of the former IPU and before the start point
of the latter.
4 Note that our operational definition of ?turn? here includes all speaker utterances, including
backchannels, which are typically not counted as turn-taking behaviors. We use this more inclusive
definition of ?turn? here to avoid inventing a new term to encompass ?turns and backchannels?.
9
Computational Linguistics Volume 38, Number 1
Our timing features (Table 5) are intended to capture positional information of a
temporal nature, such as the duration (in milliseconds) of w and its containing IPU
and turn, or the duration of any silence before and after w. These features also contain
information about the target word relative to the other speaker?s speech, including the
duration of any overlapping speech, and the latencies between w?s conversational turn
and the other speaker?s preceding and subsequent turns.
Prosody was annotated following the ToBI system (Beckman and Hirschberg 1994;
Pitrelli, Beckman, and Hirschberg 1994), which consists of annotations at four time-
linked levels of analysis: an orthographic tier of time-aligned words; a tonal tier describing
targets in the fundamental frequency (F0) contour; a break index tier indicating degrees
of juncture between words; and a miscellaneous tier, in which phenomena such as
disfluencies may be optionally marked. The tonal tier describes events such as pitch
accents, which make words intonationally prominent and are realized by increased F0
height, loudness, and duration of accented syllables. A given word may be accented
or not and, if accented, may bear different tones, or different degrees of prominence,
with respect to other words. Five types of pitch accent are distinguished in the ToBI
system for American English: two simple accents H* and L*, and three complex ones,
L*+H, L+H*, and H+!H*. An L indicates a low tone and an H, a high tone; the
asterisk indicates which tone of the accent is aligned with the stressable syllable of
the lexical item bearing the accent. Some pitch accents may be downstepped, such that
the pitch range of the accent is compressed in comparison to a non-downstepped accent.
Downsteps are indicated by the ?!? diacritic (e.g., !H*, L+!H*). Break indices define two
levels of phrasing: Level 3 corresponds to Pierrehumbert?s (1980) intermediate phrase
and level 4 to Pierrehumbert?s intonational phrase. Level 4 phrases consist of one or
more level 3 phrases, plus a high or low boundary tone (H% or L%) indicated in the
tonal tier at the right edge of the phrase. Level 3 phrases consist of one or more pitch
accents, aligned with the stressed syllable of lexical items, plus a phrase accent, which
also may be high (H-) or low (L-). For example, a standard declarative contour consists
Table 5
Timing features. Each line may describe one or more features. Features marked O may be
available in on-line conditions.
Timing features
O Duration (in msec) of w (raw, normalized with respect to all occurrences of the same word
by the same speaker, and normalized with respect to all words with the same number of
syllables and phonemes uttered by the same speaker).
O Flag indicating whether there was any overlapping speech from the other speaker.
O Duration of w?s IPU.
O Latency (in msec) between w?s turn and the previous turn by the other speaker.
O Duration of the silence before w (or 0 if the w is not preceded by silence), its IPU, and its
turn.
O Duration and proportion of w?s IPU elapsed before and after w.
O Duration of w?s turn before w.
O Duration of any overlapping speech from the other speaker during w?s IPU.
O Duration of the previous turn by the other speaker.
Duration of the silence after w (or 0 if w is not followed by silence), its IPU, and its turn.
Latency between w?s turn and the following turn by the other speaker.
Duration of w?s turn, as a whole and after w.
Duration of any overlapping speech from the other speaker during w?s turn.
Duration of the following turn by the other speaker.
10
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Figure 1
A standard declarative contour (left), and a standard yes?no question contour. The top panes
show the waveform and the fundamental frequency (F0) track.
of a sequence of H* pitch accents ending in a low phrase accent and low boundary
tone (L-L%); likewise, a standard yes?no question contour consists of a sequence of L*
pitch accents ending in H-H%. These are illustrated in Figure 1. In our study, prosodic
features include the ToBI labels as specified by the annotators, and also a simplified
version of the labels, considering only high and low pitch targets (i.e., H* vs. L* for
pitch accents, H- vs. L- for phrase accents, and H% vs. L% for boundary tones), and
simplified break indices (0?4). These are listed in Table 6.
All acoustic features were extracted automatically for the whole corpus using the
Praat toolkit (Boersma and Weenink 2001). These include pitch, intensity, stylized pitch,
ratio of voiced frames to total frames, jitter, shimmer, and noise-to-harmonics ratio
(NHR) (see Table 7). Pitch features capture how high the speaker?s voice sounds or how
low. Intensity is correlated with how loud the speaker sounds to a hearer. The voiced-
frames ratio roughly approximates the speaking rate. Jitter and shimmer correspond to
variability in the frequency and amplitude of vocal-fold vibration, respectively. NHR
is the energy ratio of noise to harmonic components in the voiced speech signal. Jitter,
shimmer, and NHR correlate with perceptual evaluations of voice quality, such as harsh,
whispery, creaky, and nasalized, inter alia. Pitch slope features capture elements of
the intonational contour, and were computed by fitting least-squares linear regression
models to the F0 data points extracted from given portions of the signal, such as a
full word or its last 200 msec. This procedure is illustrated in Figure 2, which shows
the pitch track of a sample utterance (blue dots) with three linear regressions, com-
puted over the whole utterance (solid black line), and over the final 300 and 200 msec
(?A? and ?B? dashed lines, respectively). We used a similar procedure to compute
Table 6
Prosodic features. In all cases, both original and simplified ToBI labels were considered. Each line
may describe one or more features. Features marked O may be available in on-line conditions.
ToBI prosodic features
? Phrase accent, boundary tone, break index, and pitch accent on w.
? Phrase accent, boundary tone, break index, and final pitch accent on the final intonational
phrase of the previous turn by the other speaker (these features are defined only when w
is turn initial).
11
Computational Linguistics Volume 38, Number 1
Table 7
Acoustic features. Each line may describe one or more features. Features marked O may be
available in on-line conditions.
Acoustic features
O w?s mean, maximum, and minimum pitch and intensity (raw and speaker normalized).
O Jitter and shimmer, computed over the whole word and over the first and second
syllables, computed over just the voiced frames (raw and speaker normalized).
O Noise-to-harmonics ratio (NHR), computed over the whole word and over the first and
second syllables (raw and speaker normalized).
O w?s ratio of voiced frames to total frames (raw and speaker normalized).
O Pitch slope, intensity slope, and stylized pitch slope, computed over the whole word, its
first and second halves, its first and second syllables, the first and second halves of each
syllable, and the word?s final 100, 200, and 300 msec (raw and normalized with respect to
all other occurrences of the same word by the same speaker).
O w?s mean, maximum, and minimum pitch and intensity, normalized with respect to three
types of context: w?s IPU, w?s immediately preceding word by the same speaker, and w?s
immediately following word by the same speaker.
O Voiced-frames ratio, jitter, and shimmer, normalized with respect to the same three types
of context.
O Mean, maximum, and minimum pitch and intensity, ratio of voiced frames, (all raw and
speaker normalized), jitter, and shimmer, calculated over the final 500, 1,000, 1,500 and
2,000 msec of the previous turn by the other speaker (only defined when w is turn initial
but not task initial).
O Pitch slope, intensity slope, and stylized pitch slope, calculated over the final 100, 200,
300, 500, 1,000, 1,500, and 2,000 msec of the previous turn by the other speaker (only
defined when w is turn initial but not task initial).
intensity slopes (which capture changes in perceived loudness) and stylized pitch
slopes (which capture more coarse-grained characteristics of the intonational contour).
Stylized pitch curves were obtained using the algorithm provided in Praat: Look up
the pitch point p that is closest to the straight line L that connects its two neighboring
points; if p is further than four semitones away from L, end; otherwise, remove p and
start over.
Figure 2
Sample pitch track with three linear regressions: computed over the whole IPU (bold line), and
over the final 300 msec (A) and 200 msec (B).
12
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
All features related to absolute (i.e., unnormalized) pitch values, such as maximum
pitch or final pitch slope, are not comparable across genders because of the different
pitch ranges of female and male speakers?roughly 75?500 kHz and 50?300 kHz, respec-
tively. Therefore, before computing those features we applied a linear transformation
to the pitch track values, thus making the pitch range of speakers of both genders
approximately equivalent. We refer to this process as gender normalization. All other
normalizations were calculated using z-scores: z = (x ? ?)/?, where x is a raw mea-
surement to be normalized (e.g., the duration of a particular word), and ? and ? are
the mean and standard deviation of a certain population (e.g., all instances of the same
word by the same speaker in the whole conversation).
For our phonetic features (listed in Table 8), we trained an automatic phone rec-
ognizer based on the Hidden Markov Model Toolkit (HTK) (Young et al 2006), using
three corpora as training data: the TIMIT Acoustic-Phonetic Continuous Speech Corpus
(Garofolo et al 1993), the Boston Directions Corpus (Hirschberg and Nakatani 1996),
and the Columbia Games Corpus. With this recognizer, we obtained automatic time-
aligned phonetic transcriptions of each instance of alright, mm-hm, okay, right, uh-huh,
and yeah in the corpus. To improve accuracy, we restricted the recognizer?s grammar to
accept only the most frequent variations of each word, as shown in Table 9. We extracted
our phonetic features, such as phone and syllable durations, from the resulting time-
aligned phonetic transcriptions. The remaining five ACWs in our corpus (gotcha, huh,
yep, yes, and yup) had too low counts to contain meaningful phonetic variation; thus, we
did not compute phonetic features for those words.
Finally, our session-specific features include the session of the Games Corpus in
which the target word was produced, along with the identity and gender of both
Table 8
Phonetic and session-specific features. Each line may describe one or more features. Features
marked O may be available in on-line conditions.
Phonetic features
O Identity of each of w?s phones.
O Absolute and relative duration of each phone.
O Absolute and relative duration of each syllable.
Session-specific features
? Session number.
? Identity and gender of both speakers.
Table 9
Restricted grammars for the automatic speech recognizer. Phones in square brackets are optional.
ACW ARPAbet Grammar
alright (aa|ao|ax) r (ay|eh) [t]
mm-hm m hh m
okay [aa|ao|ax|m|ow] k (ax|eh|ey)
right r (ay|eh) [t]
uh-huh (aa|ax) hh (aa|ax)
yeah y (aa|ae|ah|ax|ea|eh)
13
Computational Linguistics Volume 38, Number 1
speakers (Table 8). These features were solely intended for searching for speaker or
dialogue dependencies.
Also, to simulate the conditions of on-line applications, which process speech as
it is produced by the user, we distinguish a subset of features that may typically be
extracted from the speech signal only up to the IPU containing the target ACW. In
Tables 4 through 8 these features are marked with letter O (for on-line). All on-line fea-
tures can be computed automatically in real time by state-of-the-art speech processing
applications, although it should be noted that all of our lexical and discourse features
strongly rely on a speech recognizer output, which typically has a high error rate for
spontaneous productions. All on-line features are also available in off-line conditions;
the remaining features (those not tagged O in Tables 4 through 8) are normally available
only in offline conditions. We distinguish online features for the machine learning exper-
iments described in Section 5, in which we assess, among other things, the usefulness
of information contained in different feature sets, simulating the conditions of actual
on-line and off-line applications.
In the following sections, we use the features described here in several ways. We
first perform a series of statistical tests to find differences across the various func-
tions of ACWs. Subsequently, we experiment with machine learning techniques for the
automatic classification of the function of ACWs, training the models with different
combinations of features.
4. Characterizing Affirmative Cue Words
In this section we present results of a series of statistical tests aimed at identifying con-
textual, acoustic, and prosodic differences in the production of the various discourse/
pragmatic functions of affirmative cue words. This kind of characterization is important
both for interpretation and for production in spoken language applications: If we can
find reliable features that effectively distinguish the various uses of these words, we can
hope to interpret them automatically and generate them appropriately.
4.1 Position in IPU and Turn
We begin this analysis by looking at the discourse position of the various discourse/
pragmatic functions of ACWs. Because these words help shape, or at least reflect,
the structure of conversations, we expect to find positional differences between their
functions. Figure 3 shows the distribution of the six most frequent ACWs in the corpus
(alright, okay, yeah, mm-hm, uh-huh, and right) with respect to their position in their IPU.
An IPU-initial word is one that occurs in the first position in its corresponding IPU; that
is, it is preceded by at least 50 msec of silence and followed by another word. An IPU-
final word occurs last in its IPU. An IPU-medial word is both immediately preceded
and followed by other words. Lastly, a single-word IPU is an individual word both
preceded and followed by silence. Figure 3 also depicts the distribution of discourse/
pragmatic functions within each of these four categories. For example, roughly 40% of
all tokens of alright in the corpus occur as IPU initial; of those, about half are agreements
(Agr), half are cues to beginning discourse segments (CBeg), and a marginal number
convey other functions.
Similarly, Figure 4 shows the distribution of the same six ACWs with respect to their
position in the corresponding conversational turn. Turn-initial, turn-medial, and turn-
final words, and single-word turns are defined analogously to the four IPU-related
categories defined previously, but considering conversational turns instead of IPUs.
14
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Figure 3
Position of the target word in its IPU.
Figure 4
Position of the target word in its turn.
From these figures we observe several interesting aspects of the discourse position
of ACWs in the Games Corpus. Only a minority of these words occur as IPU medial
or IPU final. The only exception appears to be right, for which a high proportion of
instances do occur in such positions?mainly tokens with the literal modifier (Mod)
meaning, but also tokens used to check with the interlocutor (Chk), which take place at
the end of a turn (and thus, of an IPU).
The default function of ACWs, agreement (Agr), occurs for alright, okay, yeah, and
right in all possible positions within the IPU and the turn; for mm-hm and uh-huh,
agreements occur mostly as full conversational turns. Nearly all backchannels (BC)
occur as separate turns, with only a handful of exceptions: In four cases, the backchannel
is followed by a pause in which the interlocutor chooses not to continue speaking, and
the utterer of the backchannel takes the turn; in two other cases, two backchannels are
uttered in fast repetition (e.g., uh-huh uh-huh).
From the six lexical items analyzed in Tables 3 and 4, two pairs of words seem to
pattern similarly. The first such pair consists of mm-hm and uh-huh, which show very
similar distributions and are realized almost always as single-word turns, as either Agr
or BC. The second pair of words with comparable patterns of IPU and turn position
are alright and okay. These are precisely the only two ACWs used to convey all ten
discourse/pragmatic functions in the Games Corpus (recall from Table 2). This result
suggests that the lexical items in these two pairs may be used interchangeably in
conversation. The word yeah presents a pattern analogous to that of alright and okay,
albeit with fewer meanings.
In all, these findings confirm the existence of large differences in the discourse
position of ACWs between their functional types, as well as between their lexical
types. We will revisit this topic in Section 5, where we discuss the predictive power
15
Computational Linguistics Volume 38, Number 1
Figure 5
ToBI phrase accents and boundary tones. The ?other? category consists of cases with no phrase
accent and/or boundary tone present at the target word.
of discourse features in the automatic classification of the function of ACWs. Given the
observed positional differences, we expect these features to play a prominent role in
such a task.
4.2 Word-Final Intonation
Shifting our attention to acoustic/prosodic characteristics of ACWs, we examine next
the manner in which word-final intonation varies across ACW functions. First we look
at two categorical variables in the ToBI framework which capture the final pitch incur-
sion: phrase accent and boundary tone. Figure 5 shows the distribution of ToBI labels for
each of the six most frequent ACWs and their corresponding functions (see Section 3.3
for a description of the ToBI labeling conventions). The distributions for alright, okay,
right, and yeah depart significantly from random (alright: Fisher?s Exact test, p = 0.0483;
okay: Pearson?s Chi-squared test, ?2(24) = 261, p ? 0; right: Pearson, ?2(8) = 220, p ? 0;
yeah: Fisher, p ? 0).5,6 For right, considering just its discourse/pragmatic functions (i.e.,
excluding its Mod instances), the distribution also significantly differs from random
(Fisher, p ? 0). On the other hand, the distributions for mm-hm and uh-huh do not depart
significantly from random.
5 Fisher?s Exact test was used whenever the accuracy of Pearson?s Chi-squared test was compromised by
data sparsity.
6 We performed statistical tests for approximately 35 variables on the same data set. Applying the
Bonferroni correction, the alpha value should be lowered from the standard 0.05 to 0.05/35 ? 0.0014 to
maintain the familywise error rate. Thus, a result would be significant when p < 0.0014. According to
this, most tests are still significant in the current section; however, the Tukey post hoc tests following our
ANOVA tests are not: most of these have a confidence level of 95%, and significant differences begin to
disappear when considering a confidence level of 99%.
16
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
The first clear pattern we find is that the backchannel function (BC) shows a marked
preference for a high-rising (H-H% in the ToBI conventions) or low-rising (L-H%) pitch
contour towards the end of the word. Those two contours account for more than 60%
of the backchannel instances of mm-hm, okay, uh-huh, and yeah. For the other ACWs
there are not enough instances labeled BC in the corpus for statistical comparison.
The predominance of H% found for backchannels is consistent with the openness that
such boundary tone has been hypothesized to indicate (Hobbs 1990; Pierrehumbert
and Hirschberg 1990). The utterer of a backchannel understands that (i) there is
more to be said, and (ii) it is the speaker holding the conversational turn who must
say it.
The default function of ACWs, agreement (Agr) is produced most often with falling
(L-L%) or plateau final intonation ([!]H-L%) in the case of alright, okay, right, and yeah.
The L% boundary tone is believed to indicate the opposite of H%, a sense of closure,
separating the current phrase from a subsequent one (Pierrehumbert and Hirschberg
1990). In our case, by agreeing with what the speaker has said, the listener indicates that
enough information has been provided and that any subsequent phrases may refer to a
different topic. In other words, such closure might mean that the proposition preceding
the ACW has been added to the current context space (Reichman 1985), or that a new
focus space is about to be created (Grosz and Sidner 1986).
Notably, Agr instances of mm-hm and uh-huh present a very different behavior from
the other lexical items, with a distribution of final intonations that closely resembles
that of backchannels. In particular, over 60% of the Agr tokens of mm-hm and uh-huh
are produced with final rising intonation (either L-H% or H-H%). As we will see in the
following sections, the realization of mm-hm and uh-huh as Agr or BC seems to be very
similar along several dimensions besides intonation.
Alright and okay are the only two ACWs in the corpus that are used to cue the
beginning of a new discourse segment, either combined with an agreement function
(PBeg) or in its pure form (CBeg). These two functions typically have a falling (L-L%) or
sustained ([!]H-L%) final pitch contour. Additionally, the instances of okay and yeah used
to cue a discourse segment ending (PEnd) tend to be produced with a L-L% contour, and
also with [!]H-L% in the case of okay. This predominance of L% for ACWs conveying a
discourse boundary function is consistent with the previously mentioned closure that
such boundary tone is believed to indicate.
Lastly, the only ACW used frequently in the corpus for checking with the interlocu-
tor (the Chk function) is right, as illustrated in the following exchange:
A: and the top?s not either, right?
B: no
A: okay
Such instances of right in the corpus normally end in a high-rising pitch contour,
or H-H%. This fact is probably explained by the close semantic resemblance of
this construction to yes?no questions, which typically end in the same contour type
(Pierrehumbert and Hirschberg 1990).
In addition to the categorical prosodic variables described previously, word final
intonation may also be studied by measuring the slope of the word-final pitch track
(see Section 3.3 for a description of how pitch slopes are calculated). A high positive
value of pitch slope corresponds to a rising intonation; a value close to zero, to a flat
intonation; a high negative value, to a falling intonation. Final pitch slope has the ad-
vantage of being automatically computable; ToBI labels, on the other hand, still must be
17
Computational Linguistics Volume 38, Number 1
Figure 6
Final pitch slope, computed over the second half and over the final 100 and 200 msec of the
target word. In all cases, the vertical axis represents the change in Hertz per second. Significant
differences: For okay: BC>all; CBeg<Agr, BC, PEnd. For right: Chk>Agr. For yeah: BC>Agr.
manually annotated?although ongoing research may change this fact in the near future
(Rosenberg and Hirschberg 2009; Rosenberg 2010a, 2010b). Therefore, it is important to
verify that the results obtained using ToBI labels?if they are to be of practical use?
are also observable when considering numeric measures such as pitch slope. Figure 6
shows, for the same ACWs and functions discussed earlier,7 the mean pitch slope
computed over the second half of the word and over its final 100 and 200 msec, and
gender-normalized as described in Section 3.
The comparison of these numeric acoustic features across discourse/pragmatic
functions confirms that the observations made previously for categorical prosodic fea-
tures also hold when considering numeric features such as pitch slope, thus making the
likelihood that such observations will be of practical use in actual systems. For okay,
the three measures of word-final pitch slope are significantly higher for backchannels
(BC) than for all other functions, and significantly lower for CBeg than for Agr, BC,
and PEnd (RMANOVA for each of the three variables: between-subjects p > 0.3, within
subjects p ? 0; Tukey test confidence: 95%).8 BC tokens of yeah are also significantly
higher than Agr, with similar p-values. Figure 6 shows that BC instances of mm-hm
7 For PEnd instances of yeah and Agr instances of uh-huh, the number of tokens with no errors in the pitch
track and pitch slope computations is too low for statistical consideration.
8 Repeated-measures analysis of variance (RMANOVA) tests estimate the existence of both within-subjects
effects (i.e., differences between discourse/pragmatic functions) and between-subjects effects (i.e.,
differences between speakers). When the between-subjects effects are negligible, we may safely draw
conclusions across multiple speakers in the corpus, with low risk of a bias from the behavior of a
particular subset of speakers.
18
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
and uh-huh also have comparably high final pitch slopes. Again, for mm-hm we find no
significant difference in final pitch slope between agreements and backchannels.
Although Figure 6 shows that Chk tokens of right tend to end in a steeply rising
pitch, the RMANOVA tests yield between-subjects p-values of 0.01 or lower, indicating
substantial speaker effects. In other words, even though the general tendency for these
tokens, as indicated by both the numeric and categorical variables, seems to be to end
in a high-rising intonation, there is evidence of different behavior for some individual
speakers, which keeps us from drawing general conclusions about this pragmatic func-
tion of right.
4.3 Intensity
The next feature we find to vary significantly with the discourse/pragmatic function
of ACWs is word intensity. Commonly referred to as loudness or volume, intensity
generally functions to make words more salient or prominent. Figure 7 shows the
maximum and mean intensity for the most frequent ACWs and functions, computed
over the whole word and speaker normalized using z-scores.
The two types of differences we find are related to the discourse functions of
ACWs. For okay and yeah, both maximum and mean intensity are significantly lower
for instances cueing the end of a discourse segment (PEnd) than instances of all other
functions (for both variables and both words, RMANOVA tests report between-subjects
p > 0.4 and within-subjects p ? 0; Tukey 95%). For ACWs cueing a beginning discourse
segment, the opposite is true. Instances of alright and okay labeled CBeg or PBeg have
a maximum and mean intensity significantly higher than all other functions (for alright,
Figure 7
Word maximum and mean intensity, speaker normalized using z-scores. All vertical axes
represent z-scores. Significant differences: For alright: Agr<CBeg. For okay: PEnd<all;
Agr<CBeg, PBeg, BC; BC<CBeg. For yeah: PEnd<Agr, BC.
19
Computational Linguistics Volume 38, Number 1
a RMANOVA test reports between-subjects p > 0.12 and within-subjects p ? 0). These
results are consistent with previous studies of prosodic variation relative to discourse
structure, which find intensity to increase at the start of a new topic and decrease at the
end (Brown, Currie, and Kenworthy 1980; Hirschberg and Nakatani 1996). Because by
definition CBeg/PBeg ACWs begin a new topic and CEnd/PEnd end one, it is then not
surprising to find that the former tend to be produced with higher intensity, and the
latter with lower.
Finally, for mm-hm and uh-huh we find no significant differences in intensity be-
tween their unique functions, agreement (Agr) and backchannel (BC). Recall from the
previous section that we find no differences in final intonation either. This further
suggests that these two lexical types tend to be produced with similar acoustic/prosodic
features, independently of their function.
4.4 Other Features
For the remaining acoustic/prosodic features analyzed, we find a small number of sig-
nificant or approaching-significance differences between the functions of ACWs. These
differences are related to duration, mean pitch, and voice quality. The first set of findings
corresponds to the duration of ACWs, normalized with respect to all words with the
same number of syllables and phonemes uttered by the same speaker. For alright and
okay, instances cueing a beginning (CBeg) tend to be shorter than the other functions
(for both words, RMANOVA: between-subjects p > 0.5, within-subjects p < 0.05, Tukey
95%). We also find tokens of right used to check with the interlocutor (Chk) to be
on average shorter than the other two functions of right (RMANOVA, between-subjects
p > 0.7, within-subjects p = 0.001; Tukey 95%). Note that these two functions are rel-
atively simple: CBeg calls for the listener?s attention, and is frequently conveyed with
a filled pause (uh, um); Chk asks the interlocutor for confirmation, which may alter-
natively be achieved via a high-rising intonation. Thus, it is not surprising that these
functions take less time to be realized than other more pragmatically loaded functions,
such as agreement.
Speaker-normalized mean pitch over the whole word also presents significant
differences for okay and yeah. Instances labeled PEnd (agreement and cue ending
discourse segment) present a higher mean pitch than the other functions (for both
words, RMANOVA: between-subjects p > 0.6, within-subjects p < 0.01; Tukey 95%). This
is rather unexpected, because as noted in Section 4.2 around 70% of PEnd ACWs in the
corpus end in a L% boundary tone, and thus they would plausibly be uttered with a
low pitch level. What our data indicate, however, is that speakers tend to reset and raise
their pitch range when producing PEnd instances of ACWs.
Finally, we find some evidence of differences in voice quality. Both alright and
okay show a lower shimmer over voiced portions when starting a new segment (CBeg)
(RMANOVA: between-subjects p > 0.9 for alright, p = 0.09 for okay; within-subjects
p < 0.001 for both words). Also, okay and yeah present a lower noise-to-harmonics ratio
(NHR) for backchannels (RMANOVA: between-subjects p > 0.3 for okay, p = 0.04 for
yeah; within-subjects p < 0.005 for both words). A lower value of shimmer and NHR
has been associated with the perception of a better voice quality (Eskenazi, Childers,
and Hicks 1990; Bhuta, Patrick, and Garnett 2004). Our results suggest, then, that voice
quality may constitute another dimension along which speakers vary their productions
to convey the intended discourse/pragmatic meaning. Notice though that for these two
variables some of the between-subjects p-values are low enough to suggest significant
20
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
speaker effects. Therefore, our results related to differences in voice quality should be
considered preliminary.
5. Automatic Classification of Affirmative Cue Words
In this section we present results from machine learning (ML) experiments aimed at
investigating how accurately affirmative cue words may be classified automatically
into their various discourse/pragmatic functions. If spoken dialogue systems are to
interpret and generate ACWs reliably, we must identify reliable cues. With this goal
in mind, we explore several dimensions of the problem: We consider three classification
tasks, simulating the conditions of different speech applications, and study the perfor-
mance of different ML algorithms and feature sets on each task. We note that previous
studies have attempted to disambiguate between the sentential and discourse uses of
cue phrases such as now, well, and like, in corpora containing comparable numbers of
instances of each class. For ACWs in the Games Corpus dialogues, sentential uses are
rare, with the sole exception of right. Therefore, disambiguating between discourse and
sentential uses appears to be less useful than distinguishing among different discourse
functions.
The first ML task we consider consists in the general classification of any ACW
(alright, gotcha, huh, mm-hm, okay, right, uh-huh, yeah, yep, yes, yup) into any function
(Agr, BC, CBeg, PBeg, CEnd, PEnd, Mod, BTsk, Chk, Stl; see Table 1), a critical task
for spoken dialogue systems seeking to interpret user input in general. The second task
involves identifying instances of these words used to signal the beginning (CBeg, PBeg
in our labeling scheme) or ending (CEnd, PEnd) of a discourse segment, which is im-
portant for applications that must segment speech into coherent units, such as meeting
browsing systems and turn-taking components of spoken dialogue systems. The third
task consists in identifying tokens conveying some degree of acknowledgment: (Agr,
BC, PBeg, and PEnd), a function especially important to spoken dialogue systems, for
which it is critical to know that a user has heard the system?s output.
Speech processing applications operate in disparate conditions. On-line applica-
tions such as spoken dialogue systems process information as it is generated, having
access to a limited amount of context, normally up to the last IPU uttered by the user.
On the other hand, off-line applications, such as meeting transcription and browsing
systems, have the whole audio file available for processing. We simulate these two
conditions in our experiments, assessing how the limitations of online systems af-
fect performance. We also group the features described in Section 3.3 into five sets?
lexical (LX), discourse (DS), timing (TM), acoustic (AC), and phonetic (PH); see Tables 4
through 8?to determine the relative importance of each feature set in the various
classification tasks. For example, this approach allows us to simulate the conditions
of the understanding component of a spoken dialogue system, which can use only the
information up through the current IPU to detect the function of a user?s ACW. Such
a system may have access only to ASR transcription or it may have access to acoustic
and prosodic information; we note that our analysis does not take into account the pos-
sibility that transcriptions are likely to contain some errors. Our approach also allows
us to simulate a text-to-speech (TTS) system which might be used to produce a spoken
version of an on-line chat room. In order to choose the appropriate acoustic/prosodic
realization of each ACW, the TTS system will first need to determine its function based
on features extracted solely from the input text (in our taxonomy, LX and DS).
We conduct our ML experiments using three well-known algorithms with very
different characteristics: the decision tree learner C4.5 (Quinlan 1993), the propositional
21
Computational Linguistics Volume 38, Number 1
Table 10
Error rate of each classifier on the general task using different feature sets; F-measures of
the SVM classifier; and error rate and F-measures of two baselines and human labelers.
For the classifier error rates: ? Significantly different from full model. ? Significantly different
from SVM. (Wilcoxon signed rank sum test, p < 0.05.) Significance was not tested for the
classifier F-measures.
Error Rate SVM F-Measure
Feature Set C4.5 Ripper SVM Agr BC CBeg PEnd Mod Chk
LX DS TM AC PH 16.6% ? 16.3% ? 14.3% .86 .81 .89 .50 .97 .39
DS TM AC PH 21.3% ?? 17.2% ? 16.5% ? .84 .82 .87 .44 .94 0
LX TM AC PH 20.3% ?? 20.1% ? 17.0% ? .84 .80 .83 .16 .97 .21
LX DS AC PH 17.1% ? 18.1% ?? 14.8% ? .86 .81 .89 .38 .97 .35
LX DS TM PH 15.2% ? 16.3% 16.2% ? .85 .80 .86 .16 .97 .33
LX DS TM AC 17.0% ? 16.9% ? 14.7% .86 .80 .89 .48 .97 .35
LX 23.7% ?? 22.7% ? 22.3% ? .79 .80 .65 0 .96 .03
DS 22.8% ?? 24.0% ?? 25.3% ? .76 .67 .82 0 .87 0
TM 29.5% ?? 27.3% ?? 36.2% ? .70 0 .57 0 .83 0
AC 44.8% ?? 29.8% ?? 41.3% ? .67 .66 .14 0 .58 0
PH 56.4% ?? 26.5% ?? 45.4% ? .65 .08 .13 0 .64 0
Majority class baseline ER 56.4% .61 0 0 0 0 0
Word-based baseline ER 27.7% .75 .79 0 0 .94 .13
Human labelers ER (estimate 1) 9.3% .92 .91 .94 .51 .99 .67
Human labelers ER (estimate 2) 11.0% .90 .89 .93 ? .99 ?
rule learner RIPPER (Cohen 1995), and support vector machines (SVM) (Cortes and
Vapnik 1995; Vapnik 1995). We use the implementation of these algorithms provided
in the WEKA machine learning toolkit (Witten and Frank 2000), known respectively as
J48, JRIP, and SMO. We also use 10-fold cross-validation in all experiments.9
5.1 Classifiers and Feature Types
To assess the predictive power of the five feature types (LX, DS, TM, AC, and PH) we
exclude one type at a time and compare the performance of the resulting set to that
of the full model. Table 10 displays the error rate of each ML classifier on the general
task, classifying any ACW into any of the most frequent discourse/pragmatic functions
(Agr, BC, CBeg, PEnd, Mod, Chk). Table 11 shows the same results for the other two
tasks: the detection of a discourse boundary function?cue beginning (CBeg PBeg),
cue ending (CEnd, PEnd), or no-boundary (all other labels); and the detection of an
acknowledgment function?Agr, BC, PBeg, or PEnd, vs. all other labels.10
9 In the case of SVM, prior to the actual tests we experimented with two kernel types: polynomial
(K(x, y) = (x + y)d) and Gaussian radial basis function (RBF) (K(x, y) = exp(??||x ? y||2) for ? > 0). We
performed a grid search for the optimal arguments for either kernel using the data portion left out after
downsampling the corpus (see Section 3.2). The best results were obtained using a polynomial kernel
with exponent d = 1.0 (i.e., a linear kernel) and model complexity C = 1.0.
10 We note that performance on new data may be somewhat worse than the results reported here, because
we did exclude approximately 5% of tokens in our corpus due to lack of annotator agreement on labels.
22
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Table 11
Error rate of each classifier on the detection of discourse boundary functions and
acknowledgment functions, using different feature sets. ? Significantly different from
full model. ? Significantly different from SVM. (Wilcoxon signed rank sum test, p < 0.05.)
Disc. Boundary Acknowledgment
{CBeg, PBeg} vs. {CEnd, PEnd} vs. Rest {Agr, BC, PBeg, PEnd} vs. Rest
Feature Set C4.5 Ripper SVM C4.5 Ripper SVM
LX DS TM AC PH 6.9% 8.1% ? 6.9% 5.8% 5.9% ? 4.5%
DS TM AC PH 7.6% ? 8.0% 7.6% ? 8.5% ?? 5.5% ? 6.4% ?
LX TM AC PH 10.4% ? 10.1% ? 9.5% ? 8.7% ?? 8.7% ?? 6.5% ?
LX DS AC PH 8.0% ? 8.7% ? 7.5% ? 5.3% 5.7% ? 4.9%
LX DS TM PH 6.6% ? 7.9% 8.9% ? 5.4% 5.4% 5.1%
LX DS TM AC 7.1% 8.3% ? 7.0% 5.8% ? 5.6% ? 4.6%
LX 14.2% ? 14.5% ?? 13.9% ? 11.4% ? 11.4% ? 11.7% ?
DS 7.8% ? 8.6% ? 10.9% ? 8.4% ?? 8.9% ? 9.4% ?
TM 12.2% ?? 11.2% ?? 14.7% ? 12.8% ?? 13.5% ? 14.5% ?
AC 17.3% ?? 14.3% ?? 18.5% ? 26.7% ? 16.6% ?? 28.4% ?
PH 18.6% ? 17.6% ? 18.6% ? 36.5% ?? 14.1% ?? 25.4% ?
Majority class baseline ER 18.6% 36.5%
Word-based baseline ER 18.6% 15.3%
Human labelers ER (est. 1) 5.3% 2.9%
Human labelers ER (est. 2) 5.6% 3.0%
In both tables, the first line corresponds to the full model, with all five feature
types. The subsequent five lines show the performance of models with just four fea-
ture types, excluding one feature type at a time, and the following five lines show
the performance of models with exactly one feature type?these are two methods for
assessing the predictive power of each feature set. For the error rates of our classifiers,
the ? symbol indicates that the given classifier performs significantly worse when
trained on a particular feature set than when trained on the full set.11 The ? symbol
indicates that the difference between SVM and the given classifier, either C4.5 or Ripper,
is significant. For example, the second line (DS TM AC PH) in Table 10 indicates that,
for the general classification task, the three models trained on all but lexical features
perform significantly worse than the respective full models; also, the performance of
C4.5 is significantly worse than SVM, and the difference between Ripper and SVM is
not significant.
The bottom parts of Tables 10 and 11 show the error rate of two baselines, as
well as two estimates of the error rate of human labelers. We consider two types of
baseline: one a majority-class baseline, and one that employs a simple rule based
11 All accuracy comparisons discussed in this section are tested for significance with the Wilcoxon signed
rank sum test (a non-parametric alternative to Student?s t-test) at the p < 0.05 level, computed over
the error rates of the classifiers on the ten cross-validation folds. These tests provide evidence that
the observed differences in mean accuracy over cross-validation folds across two models are not
attributable to chance.
23
Computational Linguistics Volume 38, Number 1
on word identity. In the general classification task, the majority class is Agr, and the
best performing word-based rule is huh?Chk, mm-hm?Mod, uh-huh?BC, right?
Mod, others?Agr. For the identification of a discourse boundary function, the
majority class is no-boundary, and the word-based rule also assigns no-boundary to
all tokens. For the detection of an acknowledgment function, the majority class
is acknowledgment, and the word-based rule is right, huh?no-acknowledgment;
others?acknowledgment.
The error rates of human labelers are estimated using two different approaches.
Our first estimate compares the labels assigned by each labeler and the majority labels
as defined in Section 3.1. Because each labeler?s labels are used for calculating both the
error rate and the gold standard, this estimate is likely to be over-optimistic. Our second
estimate considers the subset of cases in which two annotators agree, and compares
those labels with the third labeler?s. Tables 10 and 11 show that these two estimates
yield similar results; for PEnd and Chk, there are not enough counts for computing the
F-measure of estimate 2.
The right half of Table 10 shows the F-measure of the SVM classifier for each
individual ACW function, for the general task. The highest F-measures correspond to
Agr, BC, CBeg, and Mod, precisely the four functions with the highest counts in the
Games Corpus. For PBeg and Chk the F-measures are much lower (and equal to zero
for the four remaining functions, not included in the table) due very likely to their low
counts, which prevent a better generalization during the learning stage. Future research
could investigate incorporating boosting and bootstrapping techniques to reduce the
negative effect on classification of low counts for some of the discourse/pragmatic
functions of ACWs.
For the three classification tasks, SVM outperforms, or performs at least comparably
to, the other two classifiers whenever acoustic features (AC) are taken into account
together with other feature types. When used alone, though, acoustic features per-
form poorly in all three tasks. Moreover, when acoustic features are excluded, SVM?s
accuracy is comparable to, or worse than, C4.5 and Ripper. This is probably due to
the fact that SVM?s mathematical model is better suited to exploit larger amounts of
continuous numerical variables, and thus makes a difference when including acoustic
features.
For the first two tasks, the SVM classifier seems to take advantage of all but one
feature type, as shown by the significantly lower performance resulting from removing
any of the feature types from the full model?the sole exception is the phonetic type
(PH), whose removal in no case negatively affects the accuracy of any classifier. C4.5
and Ripper, on the other hand, appear to take more advantage of some feature types
than others. For the third task, lexical (LX) and discourse (DS) features apparently have
more predictive power for both C4.5 and SVM than the other types. Note also that
for the second and third tasks, the error rates of our full-model SVM classifiers closely
approximate the estimated error rates of human labelers.
For the general task of classifying any ACW into any discourse/pragmatic function,
our full-model SVM classifier achieves the best overall results. To take a closer look at
the performance of this model, we compute its F-measure for the discourse/pragmatic
functions of each individual lexical item, as shown in Table 12. We observe that the
classifier achieves better results for word?function pairs with higher counts in the
Games Corpus, such as yeah-Agr or right-Mod (cf. Table 2). Again, the low counts
for the remaining word?function pairs may prevent a better generalization during the
learning stage, a problem that could be attenuated in future work with boosting and
bootstrapping techniques.
24
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
5.2 Session-Specific and ToBI Prosodic Features
When including session-specific features in the full model, such as identity and gender
of both speakers (see Table 8), the error rate of the SVM classifier is significantly reduced
for the general task (13.3%) and for the discourse boundary function identification task
(6.4%) (Wilcoxon, p < 0.05). For the detection of an acknowledgment function, the error
rate is not modified when including those features (4.5%). This suggests the existence
of speaker differences in the production of at least some functions of ACWs that may be
exploited by ML classifiers. Finally, the inclusion of categorical prosodic features based
on the ToBI framework, such as type of pitch accent and break index on the target word
(see Table 5), does not improve the performance of the SVM-based full models in any of
the classification tasks.
5.3 Individual Features
To estimate the importance of individual features in our classification tasks, we rank
them according to an information-gain metric. We find that for the three tasks, lexical
(LX), discourse (DS), and timing (TM) features dominate. The highest ranked features are
the ones capturing the position of the target word in its IPU and in its turn. Lexical
identity and POS tags of the previous, target, and following words, and duration of
the target word, are also ranked high. Acoustic features appear lower in the ranking;
the best performing ones are word intensity (range, mean, and standard deviation),
pitch (maximum and mean), pitch slope over the final part of the word (200 msec and
second half), voiced-frames ratio, and noise-to-harmonics ratio. All phonetic features
are ranked very low. Note that, whereas durational features at the word level are
ranked high, durational features at the phonetic level are not, because the latter only
capture the duration of each phone relative to the word duration?apparently not an
informative attribute for these classification tasks. These results confirm the existence of
large positional differences across functions of ACWs, as seen in Section 4. Additionally,
whereas several acoustic/prosodic features extracted from the target word contain use-
ful information for the automatic disambiguation of ACWs, it is positional information
that provides the most predictive power.
5.4 Online and Offline Tasks
To simulate the conditions of online applications, which process speech as it is produced
by the user, we consider a subset of features that may typically be extracted from the
Table 12
F-measure achieved by our full-model SVM classifier for the different discourse/pragmatic
functions of each lexical item.
Agr BC CBeg PBeg PEnd Mod BTsk Chk Stl
alright .88 ? .93 ? .33 ? ? ? ?
mm-hm .35 .94 ? ? ? ? ? ? ?
okay .82 .51 .88 .27 .63 .53 0 ? .18
right .84 ? ? ? ? .98 ? .53 ?
uh-huh .35 .93 ? ? ? ? ? ? ?
yeah .96 .54 ? ? .17 ? ? ? ?
25
Computational Linguistics Volume 38, Number 1
speech signal only up to the IPU containing the target ACW. These features are marked
in Tables 4 through 8 with letter O. With these features, we train and evaluate an SVM
classifier for the three tasks described previously. Table 13 shows the results, comparing
the performance of each classifier to that of the models trained on the full feature set,
which simulate the conditions of off-line applications. In all three cases the on-line
model performs significantly worse than its offline correlate, but also significantly better
than the baseline (Wilcoxon, p < 0.05).
Table 13 also shows the error rates of on-line and off-line classifiers trained using
solely text-based features?that is, only features of lexical (LX) or discourse (DS) types.
Text-based models simulate the conditions of spoken dialogue systems with no access
to acoustic and prosodic information, or generation systems attempting to realize text-
based exchanges in speech. They reflect the importance of text information alone in
training such systems to recognize the function of ACWs on-line and off-line and to
produce appropriate realizations from limited or full transcription.
Our on-line and off-line text-based models perform significantly worse than the
corresponding models that use the whole feature set, but they still outperform the
baseline models in all cases (Wilcoxon, p < 0.05). Finally, the off-line text-based models
also outperform their on-line correlates in all three tasks (Wilcoxon, p < 0.05). These
results indicate the important role that other classes of cues play in recognition, while
indicating the level of performance we can expect from TTS systems which have only
text available.
5.5 Backchannel Detection
The correct identification of backchannels is a desirable capability for speech processing
systems, as it would allow us to distinguish between two quite distinct speaker inten-
tions: the intention to take the conversational floor, and the intention to backchannel.
We first consider an off-line binary classification task?namely, classifying all
ACWs in the corpus into backchannels vs. the rest, using information from the whole
conversation. In such a task, an SVM classifier achieves a 4.91% error rate, slightly
yet significantly outperforming a word-based baseline (mm-hm, uh-huh?BC; others?
no-BC), with 5.17% (Wilcoxon, p < 0.05).
On-line applications such as spoken dialogue systems need to classify every new
speaker contribution immediately after (or even while) it is uttered, and certainly
without access to any subsequent context. The Games Corpus contains approximately
6,700 turns following speech from the other speaker, all of which begin as potential
backchannels and need to be disambiguated by the listener. Most of these candidates
can be trivially discarded using a simple observation about backchannels: By definition
Table 13
Error rate of the SVM classifier on online and offline tasks.
All Functions Disc. Boundary Acknowledgment
Feature Set Online Offline Online Offline Online Offline
LX DS TM AC PH (Full model) 17.4% 14.3% 10.1% 6.9% 6.7% 4.5%
LX DS (Text-based) 21.4% 16.8% 13.5% 9.1% 10.0% 5.9%
Word-based baseline 27.7% 18.6% 15.3%
26
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
they are short, isolated utterances, and consist normally in just one ACW. Of the 6,700
candidate turns in the corpus, only 2,351 (35%) begin with an isolated ACW, including
753 of the 757 backchannels in the corpus.12 Thus, an on-line classification procedure
would only need to identify backchannels in those 2,351 turns. At this point, we ex-
plore using a binary classifier for this task. The same word-based majority baseline
described earlier achieves an error rate of 11.56%. An SVM classifier trained on features
extracted from up to the current IPU (to simulate the on-line condition of a spoken
dialogue system) fails to improve over this baseline, achieving an error rate of 11.51%,
not significantly different from the baseline. A possible explanation for this might be
that backchannels seem to be difficult to distinguish from agreements in many cases,
leading to an increase in the error rate. Recall, from the statistical analyses in the
previous section, the positional and acoustic/prosodic similarities of tokens with these
two functions for mm-hm and uh-huh, for example. Shriberg et al (1998) report the same
difficulty in distinguishing these two word functions. We conclude that further research
is needed to develop novel approaches to this crucial problem of spoken dialogue
systems.
5.6 Comparison with Previous Work
In an effort to provide a general frame of reference for our results, we discuss here
what we believe to be the most relevant results from related studies. Note, however,
that comparing these results directly to the results of our classification experiments
is difficult because the type of corpora, definitions used, features examined, and/or
methodology employed vary greatly among the studies. The current study focuses
exclusively on the discourse/pragmatic functions of ACWs whereas other studies have
either a broader or narrower scope.
Among the cue words tested in Litman (1996) is okay, one of the ACWs we also
investigate. Litman describes the automatic classification of cue words in general (in-
cluding, e.g., now, well, say, and so), classifying these into discourse and sentential uses
using a corpus of monologue. In this classification task, which is not performed in our
study, the best results are reached by decision-tree learners trained on prosodic and
text-based features, with an error rate of 13.8%.
The most relevant study to ours is that of Stolcke et al (2000), which presents
experiments on the automatic disambiguation of dialogue acts (DA) on 1,155 sponta-
neous telephone conversations from the Switchboard corpus, labeled using the DAMSL
(Dialogue Act Markup in Several Layers) annotation scheme (Core and Allen 1997). For
the subtask of identifying the Agreement and Backchannels tags collapsed together, the
authors report an error rate of 27.1% when using prosodic features, 19.0% when using
features extracted from the text, and 15.3% when using all features. Other DA classi-
fications also include some of the functions of ACWs discussed in our current study.
For instance, Reithinger and Klesen (1997) employ a Bayesian approach for classifying
18 classes of DAs in transcripts of 437 German dialogues from the VERBMOBIL corpus
(Jekat et al 1995). The DA tags examined include Accept, Confirm, and Feedback, all
of which are related to the functions of ACWs discussed here. For the Accept DA tag,
the authors report an F-measure of 0.69; for Feedback, 0.48; and for Confirm, 0.40. These
12 The four remaining backchannels correspond to a rare phenomenon in which the speaker overlaps the
interlocutor?s last phrase with a short agreement, followed by an optional short pause and a backchannel.
Example: A: but it doesn?t overlap *them. B: right* yeah yeah # okay.
27
Computational Linguistics Volume 38, Number 1
experiments are repeated on transcripts of 163 English dialogues from the same corpus,
yielding an F-measure of 0.78 for the Accept DA tag, and 0 for the other two tags due to
data sparsity.
As part of a study aimed at assessing the effectiveness of machine learning for this
type of task, Core (1998) experiments with hand-coded decision trees for classifying
five high-level dialogue act classes, including AGREEMENT and UNDERSTANDING,
following the DAMSL annotation scheme. On 19 dialogues from the TRAINs corpus
(discussions related to solving transportation problems), Core reports an accuracy of
70% for both the Agreement and the Understanding DA classes, using only the previous
utterance?s DAMSL tag as a feature in the decision trees. This use of DA context in
classifying ACWs would appear to be promising, assuming an accurate automatic
classification of all DAs in the corpus.
Finally, Lampert, Dale, and Paris (2006) describe a statistical classifier trained on
text-based features for automatically predicting eight different speech acts derived from
a taxonomy called Verbal Response Modes (VRM). The experiments are conducted
on transcripts of 1,368 utterances from 14 dialogues in English. For the Acknowledg-
ment speech act (which ?conveys receipt of or receptiveness to other?s communication;
simple acceptance, salutations; e.g., yes? [page 37]), the classifier yields an F-measure
of 0.75.
Again, all of these studies differ significantly from our own, in their task definition,
in their methodology, and in the domain they examine. However, we expect this brief
summary to serve as a general frame of reference for our own classification results.
6. Discussion
In this work we have undertaken a comprehensive study of affirmative cue words, a
subset of cue phrases such as okay, yeah, or alright that may be utilized to convey as
many as ten different discourse/pragmatic functions, such as indicating continued
attention to the interlocutor or cueing the beginning of a new topic. Considering
the high frequency of ACWs in task-oriented dialogue, it is critical for some spoken
language processing applications such as spoken dialogue systems to model the usage
of these words correctly, from both an understanding and a generation perspective.
Section 4 presents statistical evidence of a number of differences in the production
of the various discourse/pragmatic functions of ACWs. The most notable contrasts in
acoustic/prosodic features relate to word final intonation and word intensity. Backchan-
nels typically end in a rising intonation; agreements and cue beginnings, in a falling
intonation. Cue beginnings tend to be produced with a high intensity, and cue endings
with a very low one. Other acoustic/prosodic features?duration, mean pitch, and
voice quality?also seem to vary with the word usage. Our findings related to final
intonation are consistent with previous results obtained by Hockey (1993) and Jurafsky
et al (1998) for American English. For Scottish English, Kowtko (1996) reports a non-
rising intonation for cue beginnings and for her ?reply-y? function, a subclass of our
agreement function. Kowtko also reports observing all types of final intonation in her
?acknowledge? function, whose definition overlaps both our agreements and backchan-
nels. Thus, we find no apparent contradictions between Kowtko?s results for Scottish
English and ours for American English.
The word okay is the most heavily overloaded ACW in our corpus. Our corpus
includes instances conveying each of the ten identified meanings, and this item shows
the highest degree of variation along the acoustic/prosodic features we have examined.
28
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
We speculate from this finding that the more ambiguous an ACW, the more a speaker
needs to vary acoustic/prosodic features to differentiate its meaning.
Our statistical analysis of ACWs also shows that these words display substantial
positional differences across functions, such as the position of the word in its con-
versational turn, or whether the word is preceded and/or followed by silence. Such
large differences bring support to Novick and Sutton?s (1994) claim that the discourse/
pragmatic role of these expressions strongly depends on their basic structural context.
For example, in Novick and Sutton?s words, an ACW in turn-initial position is ?clearly
not serving as a prompt for the other speaker to continue? (page 97).
Previous studies on the automatic disambiguation of other types of cue words, such
as now, well, or like, present the problem as a binary classification task: Each cue word has
either a discourse or a sentential sense (e.g., Litman 1996; Zufferey and Popescu-Belis
2004). In the study of automatic classification of ACWs presented in Section 5 we show
that for spoken task-oriented dialogue, the simple discourse/sentential distinction is
insufficient. In consequence, we define two new classification tasks besides the general
task of classifying any ACW into any function. Our first task, the detection of an
acknowledgment function, has important implications for the language management
component in spoken dialogue systems, which must keep track of which material has
reached mutual belief in a conversation (Bunt, Morante, and Keizer 2007; Roque and
Traum 2009). Our second task, the detection of a discourse segment boundary func-
tion, should help in discourse segmentation and meeting processing tasks (Litman and
Passonneau 1995). Our SVM models based on lexical, discourse, timing, and acoustic
features approach the error rate of trained human labelers in all tasks, while our auto-
matically computed phonetic features offer no improvement. Previous studies indicate
that the pragmatic function of ambiguous expressions may be effectively predicted by
models that combine information extracted from various sources, including lexical and
prosodic (e.g., Litman 1996; Stolcke et al 2000). Our results support this, and extend the
list of useful information sources to include discourse and timing features that may be
easily extracted from the time-aligned transcripts.
Additionally, our machine learning study includes experiments with several combi-
nations of feature sets, in an attempt to simulate the conditions of different applications.
Models that are trained using features extracted only from the speech signal up to the
IPU containing the target word simulate on-line applications such as spoken dialogue
systems with access to acoustic/prosodic features. Although such models perform
worse than ?off-line? models, which make use of left and right context, they still sig-
nificantly outperform our baseline classifiers. Models that simulate the conditions of
current spoken dialogue systems with access only to lexical features (although perhaps
errorful) and TTS systems synthesizing spoken conversations, which have access only
to features extracted from the input text, also significantly outperform our baseline
classifiers.
Interactions between state-of-the-art spoken dialogue systems and their users ap-
pear to contain very few instances of backchannel responses from either conversational
partner. On the system?s side, the absence of this important element of spoken com-
munication may be due to the difficulty of detecting appropriate moments where a
backchannel response would be welcome by the user. Recent advances on that research
topic (Ward and Tsukahara 2000; Cathcart, Carletta, and Klein 2003; Gravano and
Hirschberg 2009a) have encouraged research on ways to equip systems with the ability
to signal to the user that the system is still listening (Maatman, Gratch, and Marsella
2005; Bevacqua, Mancini, and Pelachaud 2008; Morency, de Kok, and Gratch 2008)?
for example, when the user is asked to enter large amounts of information. On the
29
Computational Linguistics Volume 38, Number 1
user?s side, an important reason for not backchanneling may lie in the unnaturalness
of such systems, often described as ?confusing? or even ?intimidating? by users, as
well as their inability to recognize backchannels as such. Nonetheless, recent Wizard-
of-Oz experiments conducted by Hjalmarsson (2009, 2011) show that humans appear
to react to turn-management cues produced by a synthetic voice in the same way that
they react to cues produced by another human. This important finding suggests that
users of spoken dialogue systems could be cued to produce backchannel responses, for
example to determine if they are still paying attention. In that case, it will be crucial for
systems to be able to distinguish backchannels from other pragmatic functions (Shriberg
et al 1998). In Section 5.5 we present results on the task of automatically identifying
backchannel ACWs from the other possible functions. Our models improve over the
baseline in an off-line condition (e.g., for meeting processing tasks), but fail to do so
in an on-line setting (e.g., for spoken dialogue systems). Practically all of the confusion
of this on-line model comes from misclassifying agreements (Agr) as backchannels (BC)
and vice versa. The reliability of our human labelers for distinguishing these two classes
was measured by Fleiss?s ? at 0.570, a level considerably lower than the 0.745 achieved
for the general labeling task, which indicates that the backchannel identification task is
difficult for humans as well, at least when they are not engaged in the conversation itself
but only listening to it after the fact. Although we asked our annotators to distinguish
the agreement function of ACWs from ?continued attention,? there are clearly cases
where people disagree about whether speakers are indicating agreement or not. In
future research we will investigate this issue in more detail, given the relevance of
on-line identification of backchannels in spoken dialogue systems.
In summary, in this study we have identified a number of characterizations of
affirmative cue words in a large corpus of SAE task-oriented dialogue. The corpus
on which our experiments were conducted, rich in ACWs conveying a wide range
of discourse/pragmatic functions, has allowed us to systematically investigate many
dimensions of these words, including their production and automatic disambiguation.
Besides the value of our findings from a linguistic modeling perspective, we believe
that incorporating these results into the production and understanding components of
spoken dialogue systems should improve their performance and increase user satisfac-
tion levels accordingly, getting us one step closer to the long-term goal of effectively
emulating human behavior in dialogue systems.
Appendix A: The COLUMBIA GAMES CORPUS
The COLUMBIA GAMES CORPUS is a collection of 12 spontaneous task-oriented dyadic
conversations elicited from native speakers of Standard American English. The cor-
pus was collected and annotated jointly by the Spoken Language Group at Columbia
University and the Department of Linguistics at Northwestern University. In each of
the 12 sessions, two subjects were paid to play a series of computer games requiring
verbal communication to achieve joint goals of identifying and moving images on the
screen. Each subject used a separate laptop computer and could not see the screen of the
other subject. They sat facing each other in a soundproof booth, with an opaque curtain
hanging between them, so that all communication was verbal. The subjects? speech was
not restricted in any way, and it was emphasized at the session beginning that the game
was not timed. Subjects were told that their goal was to accumulate as many points as
possible over the entire session, since they would be paid additional money for each
point they earned.
30
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
A.1 Game Tasks
Subjects were first asked to play three instances of the CARDS game, where they were
shown cards with one to four images on them. Images were of two sizes (small or large)
and various colors, and were selected to contain primarily voiced consonants, which
facilitates pitch track computation (e.g., yellow lion, blue mermaid). There were two parts
to each Cards game, designed to vary genre from primarily monologue to dialogue.
In the first part of the Cards game, each player?s screen displayed a stack of 9 or
10 cards (Figure A1a). Player A was asked to describe the top card on her pile, while
Player B was asked to search through his pile to find the same card, clicking a button
when he found it. This process was repeated until all cards in Player A?s deck were
matched. In all cases, Player B?s deck contained one additional card that had no match
in Player A?s deck, to ensure that she would need to describe all cards.
In the second part of the Cards game, each player saw a board of 12 cards on the
screen (Figure A1b), all initially face down. As the game began, the first card on one
player?s (the DESCRIBER?s) board was automatically turned face up. The Describer
was told to describe this card to the other player (the SEARCHER), who was to find a
matching card from the cards on his board. If the Searcher could not find a card exactly
matching the Describer?s card, but could find a card depicting one or more of the objects
on that card, the players could decide whether to declare a partial match and receive
points proportional to the numbers of objects matched on the cards. At most three cards
were visible to each player at any time, with cards seen earlier being automatically
turned face down as the game progressed. Players switched roles after each card was
described and the process continued until all cards had been described. The players
were given additional opportunities to earn points, based on other characteristics of the
matched cards, to make the game more interesting and to encourage discussion.
After completing all three instances of the Cards game, subjects were asked to play
a final game, the OBJECTS game. As in the Cards game, all images were selected to
have likely descriptions which were as voiced and sonorant as possible. In the Objects
game, each player?s laptop displayed a game board with 5 to 7 objects (Figure A1c).
Both players saw the same set of objects at the same position on the screen, except for
one (the TARGET). For the DESCRIBER, the target object appeared in a random location
among other objects on the screen; for the FOLLOWER, the target object appeared at the
bottom of the screen. The Describer was instructed to describe the position of the target
object on her screen so that the Follower could move his representation to the same
location on his own screen. After players negotiated what they believed to be their best
Figure A1
Sample screens from the Cards games (a, b) and Objects games (c).
31
Computational Linguistics Volume 38, Number 1
location match, they were awarded 1 to 100 points based on how well the Follower?s
target location matched the Describer?s.
The Objects game proceeded through 14 tasks. In the initial four tasks, one of
the subjects always acted as the Describer, and the other one as the Follower. In the
following four tasks their roles were inverted: The subject who played the Describer
role in the initial four tasks was now the Follower, and vice versa. In the final six tasks,
they alternated the roles with each new task.
A.2 Subjects and Sessions
Thirteen subjects (six women, seven men) participated in the study, which took place in
October 2004 in the Speech Lab at Columbia University. Eleven of the subjects partici-
pated in two sessions on different days, each time with a different partner. All subjects
reported being native speakers of Standard American English and having no hearing
impairments. Their ages ranged from 20 to 50 years (mean, 30.0; standard deviation,
10.9), and all subjects lived in the New York City area at the time of the study. They
were contacted through the classified advertisements Web site craigslist.org.
We recorded twelve sessions, each containing an average of 45 minutes of dialogue,
totaling roughly 9 hours of dialogue in the corpus. Of those, 70 minutes correspond to
the first part of the Cards game, 207 minutes to the second part of the Cards game, and
258 minutes to the Objects game. Each subject was recorded on a separate channel of
a DAT recorder, at a sample rate of 48 kHz with 16-bit precision, using a Crown head-
mounted close-talking microphone. Each session was later downsampled to 16 kHz,
16-bit precision, and saved as one stereo .wav file with one player per channel, and also
as two separate mono .wav files, one for each player.
Trained annotators orthographically transcribed the recordings of the Games Cor-
pus and manually aligned the words to the speech signal, yielding a total of 70,259
words and 2,037 unique words in the corpus. Additionally, self repairs and certain non-
word vocalizations were marked, including laughs, coughs, and breaths. Intonational
patterns and other aspects of the prosody were identified using the ToBI transcription
framework (Beckman and Hirschberg 1994; Pitrelli, Beckman, and Hirschberg 1994):
Trained annotators intonationally transcribed all of the Objects portion of the corpus
(258 minutes of dialogue) and roughly one third of the Cards portion (90 minutes).
Appendix B: ACW Labeling Guidelines
These guidelines for labeling the discourse/pragmatic functions of affirmative cue
words were developed by Julia Hirschberg, ?tefan Ben?u?, Agust?n Gravano, and
Michael Mulley at Columbia University.
Classification Scheme
Most of the labels are defined using okay, but the definitions hold for all of these words:
alright, gotcha, huh, mm-hm, okay, right, uh-huh, yeah, yep, yes, yup. If you really have no
clue about the function of a word, label it as ?.
[Mod] Literal Modifiers: In this case the words are used as modifiers. Examples:
?I think that?s okay.?
?It?s right between the mermaid and the car.?
?Yeah, that?s right.?
32
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
[Agr] Acknowledge/Agreement: The function of okay that indicates ?I believe what you
said?, and/or ?I agree with what you say.? This label should also be used for okay after
another okay or after an evaluative comment like ?Great? or ?Fine? in its role as an
acknowledgment.13 Examples:
A: Do you have a blue moon?
B: Yeah.
A: Then move it to the left of the yellow mermaid.
B: Okay, gotcha. Let?s see... (Here, both okay and gotcha are labeled Agr.)
[CBeg] Cue Beginning: The function of okay that marks a new segment of a discourse
or a new topic. Test: could this use of okay be replaced by ?Now??
[PBeg] Pivot Beginning: (Agr+CBeg) When okay functions as both a cue word and as an
Acknowledge/Agreement. Test: Can okay be replaced by ?Okay now? with the same
pragmatic meaning?
[CEnd] Cue Ending: The function of okay that marks the end of a current segment of a
discourse or a current topic. Example: ?So that?s done. Okay.?
[PEnd] Pivot Ending: (Agr+CEnd) When okay functions as both a cue word and as an
Acknowledge/Agreement, but ends a discourse segment.
[BC] Backchannel: The function of okay in response to another speaker?s utterance that
indicates only ?I?m still here / I hear you and please continue.?
[Stl] Stall: Okay used to stall for time while keeping the floor. Test: Can okay be replaced
by an elongated ?Um? or ?Uh? with the same pragmatic meaning? ?So I yeah I think
we should go together.?
[Chk] Check: Okay used with the meaning ?Is that okay?? or ?Is everything okay?? For
example, ?I?m stopping now, okay??
[BTsk] Back from a task: ?I?ve just finished what I was doing and I?m back.? Typical
case: One subject spends some time thinking, and then signals s/he is ready to continue
the discourse.
Special Cases
(1) ?okay so? / ?okay now? / ?okay then? / and so forth, where both words are uttered
together, okay seems to convey Agr, and so / now / then seems to convey CBeg. Because
we do not label words like so, now, or then, we label okay as PBeg.
(2) If you encounter a rapid sequence of the same word several times in a row, all of them
uttered in one ?burst? of breath, mark only the first one the corresponding label, and
label the others with ???. Example: ?okay yeah yeah yeah? should be labeled as: ?okay:Agr
yeah:Agr yeah:? yeah:??.
13 Throughout this article we have used the term ?agreement? to avoid confusion with other definitions of
?acknowledgment? found in the literature.
33
Computational Linguistics Volume 38, Number 1
Appendix C: ACW Labeling Examples
This appendix lists a number of examples of each type of ACWs from the Columbia
Games Corpus, as labeled by our annotators. Each ACW is highlighted and annotated
with its majority label. Overlapping speech segments are embraced by square brackets,
and additional notes are given in parentheses.
A: it?s aligned to the f- to the foot of the M&M guy like to the bottom of the iron
B: okayAgr lines up
A: yeahAgr it?s it?s almost it?s just barely like over
B: okayAgr
A: the tail
B: mm-hmBC
A: of the iron
B: mm-hmBC
A: is past the it?s a little bit past the mermaid?s body
A: when you look at the lower left corner of the iron
B: [okayBC]
A: [where] the turquoise stuff is [and you]
B: [mm-hmBC]
A: know the bottom point out to the farthest left for that region
A: the blinking image is a lawnmower
B: okayBC
A: and it?s gonna go below the yellow lion and above the bl- blue lion
B: mm-hmBC
A: the bottom black part is almost aligned to the white feet of the M&M guy
B: [okayAgr]
A: [yeahPEnd] (end-of-task)
A: okayCBeg um the blinking image is the iron
A: okayCBeg it?s uh the l- I guess the lime that?s blinking
A: nothing lined up real well
B: yeahAgr that?s rightMod
A: that was good okayCEnd
A: that?s awesome
B: you?re still the ace alrightCEnd
A: his beak?s kinda orange rightChk
B: uh-huhAgr
A: you can?t see any of that
34
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
A: that?s like a smaller amount than it is on the rightMod side to the ear [rightChk]
B: [rightAgr]
A: okayAgr
A: the lower rightMod corner
B: yeahAgr the lower rightMod corner
A: let?s start over
B: okayAgr
A: okayPBeg so you have your crescent moon
A: but not any of the yellow [part]
B: [okayPBeg] so would the top of the ear be aligned to like where
A: the like head of the lion to like the where the grass shoots out there?s that?s a significant
difference
B: okayPBeg so there?s definitely a bigger space from the blue lion to the lawnmower than there
is from the handle to the feet of the yellow
A: alright? I?ll try it (7.81 sec) okayBTsk
B: okayCBeg the owl is blinking
A: that thing is gonna be like (0.99 sec) okayStl (0.61 sec) one pixel to the rightMod of the edge
Acknowledgments
This work was supported in part by
NSF IIS-0307905, NSF IIS-0803148,
ANPCYT PICT-2009-0026, UBACYT
20020090300087, CONICET, and the Slovak
Agency for Science and Research Support
(APVV-0369-07). We thank Fadi Biadsy,
H?ctor Ch?vez, Enrique Henestroza,
Jackson Liscombe, Shira Mitchell, Michael
Mulley, Andrew Rosenberg, Elisa Sneed
German, Ilia Vovsha, Gregory Ward, and
Lauren Wilcox for valuable discussions
and for their help in collecting, labeling,
and processing the data.
References
Allwood, J., J. Nivre, and E. Ahlsen. 1992. On
the semantics and pragmatics of linguistic
feedback. Journal of Semantics, 9(1):1?30.
Beckman, Mary E. and Julia Hirschberg.
1994. The ToBI annotation conventions.
Available on-line at http://www.ling.
ohio-state.edu/?tobi/ame_tobi/
annotation_conventions.html.
Bevacqua, E., M. Mancini, and C. Pelachaud.
2008. A listening agent exhibiting variable
behavior. In B. H. Prendinger, J. Lester, and
M. Ishizuka, editors, Intelligent Virtual
Agents, pages 262?269. Springer, Berlin.
Bhuta, T., L. Patrick, and J. D. Garnett.
2004. Perceptual evaluation of voice
quality and its correlation with
acoustic measurements. Journal of
Voice, 18(3):299?304.
Boersma, Paul and David Weenink. 2001.
Praat: Doing phonetics by computer.
Available at http://www.praat.org.
Brown, G., K. L. Currie, and J. Kenworthy.
1980. Questions of Intonation. University
Park Press, Baltimore, MD.
Bunt, H. C. 1989. Information dialogues as
communicative actions in relation to user
modelling and information processing. In
M. M. Taylor, F. Neel, and D. G. Bouwhuis,
editors, The Structure of Multimodal
Dialogue, pages 47?73. Elsevier,
Amsterdam.
Bunt, H. C., R. Morante, and S. Keizer. 2007.
An empirically based computational
model of grounding in dialogue. In
35
Computational Linguistics Volume 38, Number 1
Proceedings of the 8th SIGdial Workshop on
Discourse and Dialogue, pages 283?290,
Antwerp.
Cathcart, N., J. Carletta, and E. Klein. 2003. A
shallow model of backchannel continuers
in spoken dialogue. In Proceedings of the
10th Conference of the European Chapter of the
Association for Computational Linguistics
(EACL), pages 51?58, Budapest.
Charniak, Eugene and Mark Johnson. 2001.
Edit detection and parsing for transcribed
speech. In Proceedings of the 2nd Meeting of
the North American Chapter of the Association
for Computational Linguistics (NAACL),
pages 118?126, Pittsburgh, PA.
Clark, H. H. and Susan Brennan. 1991.
Grounding in communication. In L.
Resnick, J. Levine, and S. Teasley, editors,
Perspectives on Socially Shared Cognition,
pages 127?149. American Psychological
Association (APA), Hyattsville, MD.
Clark, H. H. and E. F. Schaefer. 1989.
Contributing to discourse. Cognitive
Science, 13:259?294.
Cohen, Robin. 1984. A computational theory
of the function of clue words in argument
understanding. In Proceedings of the
22nd Annual Meeting Association for
Computational Linguistics (ACL),
pages 251?258, Stanford, CA.
Cohen, William C. 1995. Fast effective rule
induction. In Proceedings of the 12th
International Conference on Machine
Learning, pages 115?123, Tahoe City, CA.
Core, Mark G. 1998. Analyzing and
predicting patterns of DAMSL utterance
tags. In Working Notes of the AAAI Spring
Symposium on Applying Machine Learning
to Discourse Processing, pages 18?24,
Stanford, CA.
Core, M. G. and J. Allen. 1997. Coding
dialogs with the damsl annotation scheme.
In Proceedings of the AAAI Fall Symposium
on Communicative Action in Humans and
Machines, pages 28?35, Cambridge, MA.
Cortes, Corinna and Vladimir Vapnik. 1995.
Support vector networks. Machine
Learning, 20(3):273?297.
Duncan, Starkey. 1972. Some signals and
rules for taking speaking turns in
conversations. Journal of Personality
and Social Psychology, 23(2):283?292.
Eskenazi, L., D. G. Childers, and D. M.
Hicks. 1990. Acoustic correlates of vocal
quality. Journal of Speech, Language and
Hearing Research, 33(2):298?306.
Fleiss, J. L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Garofolo, John S., Lori F. Lamel, William M.
Fisher, Jonathan G. Fiscus, David S. Pallett,
Nancy L. Dahlgren, and Victor Zue.
1993. Ldc93s1: Timit acoustic-phonetic
continuous speech corpus. Linguistic Data
Consortium, University of Pennsylvania,
Philadelphia.
Godfrey, J. J., E. C. Holliman, and
J. McDaniel. 1992. SWITCHBOARD:
Telephone speech corpus for research and
development. In Proceedings of the IEEE
International Conference on Acoustics, Speech,
and Signal Processing, pages 517?520,
San Francisco, CA.
Goodwin, C. 1981. Conversational
Organization: Interaction Between Speakers
and Hearers. Academic Press, New York.
Gravano, Agust?n, Stefan Benus, H?ctor
Ch?vez, Julia Hirschberg, and Lauren
Wilcox. 2007. On the role of context
and prosody in the interpretation of
?okay?. In Proceedings of the 45th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 800?807,
Prague.
Gravano, Agust?n and Julia Hirschberg.
2009a. Backchannel-inviting cues in
task-oriented dialogue. In Proceedings of
Interspeech, pages 1019?1022, Brighton.
Gravano, Agust?n and Julia Hirschberg.
2009b. Turn-yielding cues in task-oriented
dialogue. In Proceedings of the 10th SIGdial
Workshop on Discourse and Dialogue,
pages 253?261, London.
Gravano, Agust?n and Julia Hirschberg.
2011. Turn-taking cues in task-oriented
dialogue. Computer Speech and Language,
25(3):601?634.
Grosz, Barbara and Candace Sidner. 1986.
Attention, intention, and the structure
of discourse. Computational Linguistics,
12(3):175?204.
Hirschberg, J. 1990. Accent and discourse
context: Assigning pitch accent in
synthetic speech. In Proceedings of the 8th
National Conference on Artificial Intelligence,
volume 2, pages 952?957, Boston, MA.
Hirschberg, Julia and Diane Litman. 1987.
Now let?s talk about now: Identifying
cue phrases intonationally. In Proceedings
of the 25th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 163?171, Stanford, CA.
Hirschberg, Julia and Diane Litman. 1993.
Empirical studies on the disambiguation
of cue phrases. Computational Linguistics,
19(3):501?530.
Hirschberg, Julia and Christine Nakatani.
1996. A prosodic analysis of discourse
36
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
segments in direction-giving monologues.
In Proceedings of the 34th Annual Meeting
Association for Computational Linguistics
(ACL), pages 286?293, Santa Cruz, CA.
Hjalmarsson, Anna. 2009. On cue?Additive
effects of turn-regulating phenomena in
dialogue. In Proceedings of Diaholmia?
13th Workshop on the Semantics and
Pragmatics of Dialogue, pages 27?34,
Stockholm.
Hjalmarsson, Anna. 2011. The additive
effect of turn-taking cues in human and
synthetic voice. Speech Communication,
53(1):23?35.
Hobbs, Jerry R. 1990. The
Pierrehumbert-Hirschberg theory of
intonational meaning made simple:
Comments on Pierrehumbert and
Hirschberg. In P. R. Cohen, J. Morgan,
and M. E. Pollack, editors, Intentions in
Communication. MIT Press, Cambridge,
MA, pages 313?323.
Hockey, B. A. 1993. Prosody and the role
of ?okay? and ?uh-huh? in discourse.
In Proceedings of the Eastern States
Conference on Linguistics, pages 128?136,
Columbus, OH.
Jefferson, G. 1984. Notes on a systematic
deployment of the acknowledgement
tokens ?yeah?; and ?mm hm?. Research
on Language & Social Interaction,
17(2):197?216.
Jekat, S., A. Klein, E. Maier, I. Maleck,
M. Mast, and J. J. Quantz. 1995. Dialogue
acts in VERBMOBIL. Technical report
Verbmobil-Report 65, Universitaet
Erlangen, Berlin.
Jurafsky, Daniel, Elizabeth Shriberg,
Barbara Fox, and Traci Curl. 1998. Lexical,
prosodic, and syntactic cues for dialog
acts. In Proceedings of ACL/COLING,
Workshop on Discourse Relations and
Discourse Markers, pages 114?120,
Montreal.
Kendon, A. 1967. Some functions of
gaze-direction in social interaction.
Acta Psychologica, 26:22?63.
Koiso, H., Y. Horiuchi, S. Tutiya, A. Ichikawa,
and Y. Den. 1998. An analysis of
turn-taking and backchannels based on
prosodic and syntactic features in Japanese
Map Task dialogs. Language and Speech:
Special Issue on Prosody and Conversation,
41(3-4):295?321.
Kowtko, Jacqueline C. 1996. The Function
of Intonation in Task-Oriented Dialogue.
Ph.D. thesis, University of Edinburgh.
Lampert, A., R. Dale, and C. Paris. 2006.
Classifying speech acts using verbal
response modes. In Proceedings of the
Australasian Language Technology
Workshop, pages 34?41, Sydney.
Litman, Diane. 1994. Classifying cue
phrases in text and speech using machine
learning. In Proceedings of the 12th National
Conference on Artificial Intelligence - AAAI,
pages 806?813, Seattle, WA.
Litman, Diane. 1996. Cue phrase
classification using machine learning.
Journal of Artificial Intelligence, 5:53?94.
Litman, Diane and Julia Hirschberg. 1990.
Disambiguating cue phrases in text and
speech. In Proceedings of the 13th
International Conference on Computational
Linguistics, pages 251?256, Helsinki.
Litman, D. J. and J. F. Allen. 1987. A plan
recognition model for subdialogues
in conversations. Cognitive Science,
11(2):163?200.
Litman, D. J. and R. J. Passonneau. 1995.
Combining multiple knowledge
sources for discourse segmentation.
In Proceedings of the 33rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 108?115,
Cambridge, MA.
Maatman, R. M., J. Gratch, and S. Marsella.
2005. Natural behavior of a listening
agent. In 5th International Conference on
Intelligent Virtual Agents, pages 25?36, Kos.
Marcus, M. P., M. A. Marcinkiewicz, and
B. Santorini. 1993. Building a large
annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19(2):313?330.
Morency, L. P., I. de Kok, and J. Gratch.
2008. Predicting listener backchannels:
A probabilistic multimodal approach.
In Proceedings of the 8th International
Conference on Intelligent Virtual Agents,
pages 176?190, Tokyo.
Mushin, I., L. Stirling, J. Fletcher, and
R. Wales. 2003. Discourse structure,
grounding, and prosody in task-oriented
dialogue. Discourse Processes, 35(1):1?31.
Novick, D. G. and S. Sutton. 1994. An
empirical model of acknowledgment for
spoken-language systems. In Proceedings
of the 32nd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 96?101, Morristown, NJ.
Pierrehumbert, Janet and Julia Hirschberg.
1990. The meaning of intonational
contours in the interpretation of
discourse. In P. R. Cohen, J. Morgan,
and M. E. Pollack, editors, Intentions in
Communication. MIT Press, Cambridge,
MA, pages 271?311.
37
Computational Linguistics Volume 38, Number 1
Pierrehumbert, J. B. 1980. The Phonology and
Phonetics of English Intonation. Ph.D. thesis,
Massachusetts Institute of Technology,
Cambridge, MA.
Pitrelli, John F., Mary E. Beckman, and Julia
Hirschberg. 1994. Evaluation of prosodic
transcription labeling reliability in the
ToBI framework. In Proceedings of the
International Conference of Spoken Language
Processing (ICSLP), pages 123?126,
Yokohama.
Quinlan, John Ross. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann,
Waltham, MA.
Ratnaparkhi, A., E. Brill, and K. Church.
1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 133?142,
Philadelphia, PA.
Redeker, G. 1991. Review article: Linguistic
markers of linguistic structure. Linguistics,
29(6):1139?1172.
Reichman, Rachel. 1985. Getting Computers
to Talk like You and Me. MIT Press,
Cambridge, MA.
Reithinger, N. and M. Klesen. 1997. Dialogue
act classification using language models.
In Proceedings of the 5th European Conference
on Speech Communication and Technology,
pages 2235?2238, Rhodes.
Roque, A. and D. Traum. 2009. Improving a
virtual human using a model of degrees
of grounding. In Proceedings of the 21st
International Joint Conferences on Artificial
Intelligence (IJCAI), pages 1537?1542,
Pasadena, CA.
Rosenberg, A. and J. Hirschberg. 2009.
Detecting pitch accent at the word,
syllable and vowel level. In Proceedings
of the North American Chapter of
the Association for Computational
Linguistics ? Human Language
Technologies (NAACL-HLT) Conference,
pages 81?84, Boulder, CO.
Rosenberg, Andrew. 2010a. AuToBI ? A
tool for automatic ToBI annotation. In
Proceedings of Interspeech, pages 146?149,
Makuhari.
Rosenberg, Andrew. 2010b. Classification
of prosodic events using quantized
contour modeling. In Proceedings of the
North American Chapter of the Association
for Computational Linguistics ? Human
Language Technologies (NAACL-
HLT) Conference, pages 721?724,
Los Angeles, CA.
Sacks, H., E. A. Schegloff, and G. Jefferson.
1974. A simplest systematics for the
organization of turn-taking for
conversation. Language, 50:696?735.
Schegloff, E. A. 1982. Discourse as an
interactional achievement: Some
uses of ?uh huh? and other things
that come between sentences. In
Tannen D, editor, Analyzing Discourse:
Text and Talk, pages 71?93. APA,
Hyattsville, MD.
Schiffrin, Deborah. 1987. Discourse
Markers. Cambridge University Press,
Cambridge, UK.
Shriberg, E., R. Bates, A. Stolcke, P. Taylor,
D. Jurafsky, K. Ries, N. Coccaro, R. Martin,
M. Meteer, and C. Van Ess-Dykema.
1998. Can prosody aid the automatic
classification of dialog acts in
conversational speech? Language
and Speech, 41(3-4):443?492.
Stolcke, A., K. Ries, N. Coccaro, E. Shriberg,
R. Bates, D. Jurafsky, P. Taylor, R. Martin,
C. V. Ess-Dykema, and M. Meteer. 2000.
Dialogue act modeling for automatic
tagging and recognition of conversational
speech. Computational Linguistics,
26(3):339?373.
Traum, David. 1994. A Computational
Theory of Grounding in Natural Language
Conversation. Ph.D. thesis, Rochester
University, Rochester, NY.
Traum, David and James Allen. 1992.
A speech acts approach to grounding
in conversation. In Proceedings of the
International Conference on Spoken
Language Processing (ICSLP),
pages 137?140, Banff.
Vapnik, Vladimir N. 1995. The Nature of
Statistical Learning Theory. Springer-Verlag,
New York.
Walker, M. A. 1992. Redundancy in
collaborative dialogue. In Proceedings
of the 14th Conference on Computational
Linguistics, pages 345?351, Morristown, NJ.
Walker, M. A. 1993a. Informational
Redundancy and Resource Bounds in
Dialogue. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Walker, M. A. 1993b. When given
information is accented: Repetition,
paraphrase and inference in dialogue.
In LSA Annual Meeting, pages 231?240,
Los Angeles, CA.
Walker, M. A. 1996. Inferring acceptance and
rejection in dialogue. Language and Speech,
39(2-3).
Ward, N. and W. Tsukahara. 2000. Prosodic
features which cue back-channel responses
in English and Japanese. Journal of
Pragmatics, 32(8):1177?1207.
38
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Witten, I. H. and E. Frank. 2000. Data Mining:
Practical Machine Learning Tools and
Techniques with Java Implementations.
Morgan Kaufmann, Waltham, MA.
Yngve, V. H. 1970. On getting a word in
edgewise. In Proceedings of the 6th Regional
Meeting of the Chicago Linguistic Society,
volume 6, pages 657?677, Chicago, IL.
Young, S., G. Evermann, M. Gales,
D. Kershaw, G. Moore, J. Odell,
D. Ollason, D. Povey, V. Valtchev, and
P. Woodland. 2006. The HTK Book,
version 3.4. Available on-line at
http://htk.eng.cam.ac.uk.
Zufferey, S. and A. Popescu-Belis. 2004.
Towards automatic identification of
discourse markers in dialogs: The case of
?like?. In Proceedings of the 5th SIGdial
Workshop on Discourse and Dialogue,
pages 63?71, Boston, MA.
39
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 11?19,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Acoustic-Prosodic Entrainment and Social Behavior
Rivka Levitan1, Agust??n Gravano2, Laura Willson1,
S?tefan Ben?us?3, Julia Hirschberg1, Ani Nenkova4
1 Dept. of Computer Science, Columbia University, New York, NY 10027, USA
2 Departamento de Computacio?n (FCEyN), Universidad de Buenos Aires, Argentina
3 Constantine the Philosopher University & Institute of Informatics, Slovak Academy of Sciences, Slovakia
4 Dept. of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, USA
rlevitan@cs.columbia.edu, gravano@dc.uba.ar, law2142@barnard.edu,
sbenus@ukf.sk, julia@cs.columbia.edu, nenkova@seas.upenn.edu
Abstract
In conversation, speakers have been shown
to entrain, or become more similar to each
other, in various ways. We measure entrain-
ment on eight acoustic features extracted from
the speech of subjects playing a cooperative
computer game and associate the degree of en-
trainment with a number of manually-labeled
social variables acquired using Amazon Me-
chanical Turk, as well as objective measures
of dialogue success. We find that male-female
pairs entrain on all features, while male-male
pairs entrain only on particular acoustic fea-
tures (intensity mean, intensity maximum and
syllables per second). We further determine
that entrainment is more important to the per-
ception of female-male social behavior than it
is for same-gender pairs, and it is more impor-
tant to the smoothness and flow of male-male
dialogue than it is for female-female or mixed-
gender pairs. Finally, we find that entrainment
is more pronounced when intensity or speak-
ing rate is especially high or low.
1 Introduction
Entrainment, also termed alignment, adaptation,
priming or coordination, is the phenomenon of
conversational partners becoming more similar to
each other in what they say, how they say it,
and other behavioral phenomena. Entrainment has
been shown to occur for numerous aspects of spo-
ken language, including speakers? choice of re-
ferring expressions (Brennan & Clark, 1996); lin-
guistic style (Niederhoffer & Pennebaker, 2002;
Danescu-Niculescu-Mizil et al, 2011); syntactic
structure (Reitter et al, 2006); speaking rate (Lev-
itan & Hirschberg, 2011); acoustic/prosodic fea-
tures such as fundamental frequency, intensity, voice
quality (Levitan & Hirschberg, 2011); and phonet-
ics (Pardo, 2006).
Entrainment in many of these dimensions has also
been associated with different measures of dialogue
success. For example, Chartrand and Bargh (1999)
demonstrated that mimicry of posture and behavior
led to increased liking between the dialogue par-
ticipants as well as a smoother interaction. They
also found that naturally empathetic individuals ex-
hibited a greater degree of mimicry than did oth-
ers. Nenkova et al (2008) found that entrainment
on high-frequency words was correlated with nat-
uralness, task success, and coordinated turn-taking
behavior. Natale (1975) showed that an individ-
ual?s social desirability, or ?propensity to act in
a social manner,? can predict the degree to which
that individual will match her partner?s vocal inten-
sity. Levitan et al (2011) showed that entrainment
on backchannel-preceding cues is correlated with
shorter latency between turns, fewer interruptions,
and a higher degree of task success. In a study of
married couples discussing problems in their rela-
tionships, Lee et al (2010) found that entrainment
measures derived from pitch features were signifi-
cantly higher in positive interactions than in nega-
tive interactions and were predictive of the polarity
of the participants? attitudes.
These studies have been motivated by theoreti-
cal models such as Giles? Communication Accom-
modation Theory (Giles & Coupland, 1991), which
proposes that speakers promote social approval or
11
efficient communication by adapting to their inter-
locutors? communicative behavior. Another theory
informing the association of entrainment and dia-
logue success is the coordination-rapport hypoth-
esis (Tickle-Degnen & Rosenthal, 1990), which
posits that the degree of liking between conversa-
tional partners should be correlated with the degree
of nonverbal coordination between them.
Motivated by such theoretical proposals and em-
pirical findings, we hypothesized that entrainment
on acoustic/prosodic dimensions such as pitch, in-
tensity, voice quality and speaking rate might also
be correlated with positive aspects of perceived
social behaviors as well as other perceived char-
acteristics of efficient, well-coordinated conversa-
tions. In this paper we describe a series of ex-
periments investigating the relationship between ob-
jective acoustic/prosodic dimensions of entrainment
and manually-annotated perception of a set of so-
cial variables designed to capture important as-
pects of conversational partners? social behaviors.
Since prior research on other dimensions of entrain-
ment has sometimes observed differences in degree
of entrainment between female-female, male-male
and mixed gender groups (Bilous & Krauss, 1988;
Pardo, 2006; Namy et al, 2002), we also exam-
ined our data for variation by gender pair, consid-
ering female-female, male-male, and female-male
pairs of speakers separately. If previous findings
extend to acoustic/prosodic entrainment, we would
expect female-female pairs to entrain to a greater
degree than male-male pairs and female partners in
mixed gender pairs to entrain more than their male
counterparts. Since prior findings posit that entrain-
ment leads to smoother and more natural conversa-
tions, we would also expect degree of entrainment
to correlate with perception of other characteristics
descriptive of such conversations.
Below we describe the corpus and annotations
used in this study and how our social annotations
were obtained in Sections 2 and 3. We next discuss
our method and results for the prevalence of entrain-
ment among different gender groups (Section 4). In
Sections 5 and 6, we present the results of correlat-
ing acoustic entrainment with social variables and
objective success measures, respectively. Finally, in
Section 7, we explore entrainment in cases of outlier
feature values.
2 The Columbia Games Corpus
The Columbia Games Corpus (Gravano & Hirsch-
berg, 2011) consists of approximately nine hours
of spontaneous dialogue between pairs of subjects
playing a series of computer games. Six females and
seven males participated in the collection of the cor-
pus; eleven of the subjects returned on a different
day for another session with a new partner.
During the course of each session, a pair of speak-
ers played three Cards games and one Objects game.
The work described here was carried out on the Ob-
jects games. This section of each session took 7m
12s on average. We have a total of 4h 19m of Ob-
jects game speech in the corpus.
For each task in an Objects game, the players
saw identical collections of objects on their screens.
However, one player (the Describer) had an addi-
tional target object positioned among the other ob-
jects, while the other (the Follower) had the same
object at the bottom of her screen. The Describer
was instructed to describe the position of the target
object so that the Follower could place it in exactly
the same location on her screen. Points (up to 100)
were awarded based on how well the Follower?s tar-
get location matched the describers. Each pair of
partners completed 14 such tasks, alternating roles
with each task. The partners were separated by a
curtain to ensure that all communication was oral.
The entire corpus has been orthographically tran-
scribed and words aligned with the speech source. It
has also been ToBI-labeled (Silverman et al, 1992)
for prosodic events, as well as labeled for turn-
taking behaviors.
3 Annotation of Social Variables
In order to study how entrainment in various dimen-
sions correlated with perceived social behaviors of
our subjects, we asked Amazon Mechanical Turk1
annotators to label the 168 Objects games in our cor-
pus for an array of social behaviors perceived for
each of the speakers, which we term here ?social
variables.?
Each Human Intelligence Task (HIT) presented to
the AMT workers for annotation consisted of a sin-
gle Objects game task. To be eligible for our HITs,
1http://www.mturk.com
12
annotators had to have a 95% success rate on pre-
vious AMT HITs and to be located in the United
States. They also had to complete a survey estab-
lishing that they were native English speakers with
no hearing impairments. The annotators were paid
$0.30 for each HIT they completed. Over half of the
annotators completed fewer than five hits, and only
four completed more than twenty.
The annotators listened to an audio clip of the
task, which was accompanied by an animation that
displayed a blue square or a green circle depending
on which speaker was currently talking. They were
then asked to answer a series of questions about each
speaker: Does Person A/B believe s/he is better than
his/her partner? Make it difficult for his/her partner
to speak? Seem engaged in the game? Seem to dis-
like his/her partner? Is s/he bored with the game?
Directing the conversation? Frustrated with his/her
partner? Encouraging his/her partner? Making
him/herself clear? Planning what s/he is going to
say? Polite? Trying to be liked? Trying to domi-
nate the conversation? They were also asked ques-
tions about the dialogue as a whole: Does it flow
naturally? Are the participants having trouble un-
derstanding each other? Which person do you like
more? Who would you rather have as a partner?
A series of check questions with objectively de-
terminable answers (e.g. ?Which speaker is the De-
scriber??) were included among the target questions
to ensure that the annotators were completing the
task with integrity. HITs for which the annotator
failed to answer the check questions correctly were
disqualified.
Each task was rated by five unique annotators who
answered ?yes? or ?no? to each question, yielding
a score ranging from 0 to 5 for each social vari-
able, representing the number of annotators who an-
swered ?yes.? A fuller description of the annotation
for social variables can be found in (Gravano et al,
2011).
In this study, we focus our analysis on annotations
of four social variables:
? Is the speaker trying to be liked?
? Is the speaker trying to dominate the conversa-
tion?
? Is the speaker giving encouragement to his/her
partner?
? Is the conversation awkward?
We correlated annotations of these variables with
an array of acoustic/prosodic features.
4 Acoustic entrainment
We examined entrainment in this study in eight
acoustic/prosodic features:
? Intensity mean
? Intensity max
? Pitch mean
? Pitch max
? Jitter
? Shimmer
? Noise-to-harmonics ratio (NHR)
? Syllables per second
Intensity is an acoustic measure correlated with
perceived loudness. Jitter, shimmer, and noise-to-
harmonics ratios are three measures of voice quality.
Jitter describes varying pitch in the voice, which is
perceived as a rough sound. Shimmer describes fluc-
tuation of loudness in the voice. Noise-to-harmonics
ratio is associated with perceived hoarseness. All
features were speaker-normalized using z-scores.
For each task, we define entrainment between
partners on each feature f as
ENTp = ?|speaker1f ? speaker2f |
where speaker[1,2]f represents the corresponding
speaker?s mean for that feature over the task.
We say that the corpus shows evidence of en-
trainment on feature f if ENTp, the similarities be-
tween partners, are significantly greater than ENTx,
the similarities between non-partners:
ENTx = ?
?
i |speaker1f ?Xi,f |
|X|
where X is the set of speakers of same gender and
role as the speaker?s partner who are not paired with
the speaker in any session. We restrict the compar-
isons to speakers of the same gender and role as the
speaker?s partner to control for the fact that differ-
ences may simply be due to differences in gender or
role. The results of a series of paired t-tests compar-
ing ENTp and ENTx for each feature are summarized
in Table 1.
13
Feature FF MM FM
Intensity mean X X X
Intensity max X X X
Pitch mean X
Pitch max X
Jitter X X
Shimmer X X
NHR X
Syllables per sec X X X
Table 1: Evidence of entrainment for gender pairs. A tick
indicates that the data shows evidence of entrainment on
that row?s feature for that column?s gender pair.
We find that female-female pairs in our corpus
entrain on, in descending order of significance, jitter,
intensity max, intensity mean, syllables per second
and shimmer. They do not entrain on pitch mean
or max or NHR. Male-male pairs show the least
evidence of entrainment, entraining only on inten-
sity mean, intensity max, and syllables per second,
supporting the hypothesis that entrainment is less
prevalent among males. Female-male pairs entrain
on, again in descending order of significance, inten-
sity mean, intensity max, jitter, syllables per second,
pitch mean, NHR, shimmer, and pitch max ? in fact,
on every feature we examine, with significance val-
ues in each case of p<0.01.
To look more closely at the entrainment behavior
of males and females in mixed-gender pairs, we de-
fine ENT2p as follows:
ENT2p = ?
?
i |Pi,f ? Ti,f |
|T|
where T is the set of the pause-free chunks of speech
that begin a speaker?s turns, and P is the correspond-
ing set of pause-free chunks that end the interlocu-
tor?s preceding turns. Unlike ENTp, this measure is
asymmetric, allowing us to consider each member
of a pair separately.
We compare ENT2p for each feature for males and
females of mixed gender pairs. Contrary to our hy-
pothesis that females in mixed-gender pairs would
entrain more, we found no significant differences
in partner gender. Females in mixed-gender pairs
do not match their interlocutor?s previous turn any
more than do males. This may be due to the fact
Feature FM MM F p
Intensity mean ? ? 3.83 0.02
Intensity max ? ? 4.01 0.02
Syllables per sec ? ? 2.56 0.08
Table 2: Effects of gender pair on entrainment. An arrow
pointing up indicates that the group?s normalized entrain-
ment for that feature is greater than that of female-female
pairs; an arrow pointing down indicates that it is smaller.
that, as shown in Table 1, the overall differences be-
tween partners in mixed-gender pairs are quite low,
and so neither partner may be doing much turn-by-
turn matching.
However, as we expected, entrainment is least
prevalent among male-male pairs. Although we ex-
pected female-female pairs to exhibit the highest
prevalence of entrainment, they do not show evi-
dence of entrainment on pitch mean, pitch max or
NHR, while female-male pairs entrain on every fea-
ture. In fact, although ENTp for these features is not
significantly smaller between female-female pairs
than between female-male pairs, ENTx, the overall
similarity among non-partners for these features, is
significantly larger between females than between
females and males. The degree of similarity between
female-female partners is therefore attributable to
the overall similarity between females rather than
the effect of entrainment.
All three types of pairs exhibit entrainment on in-
tensity mean, intensity max, and syllables per sec-
ond. We look more closely into the gender-based
differences in entrainment behavior with an ANOVA
with the ratio of ENTp to ENTx as the dependent
variable and gender pair as the independent variable.
Normalizing ENTp by ENTx allows us to compare
the degree of entrainment across gender pairs. Re-
sults are shown in Table 2. Male-male pairs have
lower entrainment than female-female pairs for ev-
ery feature; female-male pairs have higher entrain-
ment than female-female pairs for intensity mean
and max and lower for syllables per second (p <
0.1). These results are consistent with the general
finding that male-male pairs entrain the least and
female-male pairs entrain the most.
14
5 Entrainment and social behavior
We next correlate each of the social variables de-
scribed in Section 3 with ENTp for our eight acous-
tic features. Based on Communication Accommo-
dation Theory, we would expect gives encourage-
ment, a variable representing a desirable social char-
acteristic, to be positively correlated with entrain-
ment. Conversely, conversation awkward should be
negatively correlated with entrainment. We note that
Trying to be liked is negatively correlated with the
like more variable in our data ? that is, annotators
were less likely to prefer speakers whom they per-
ceived as trying to be liked. This reflects the in-
tuition that someone overly eager to be liked may
be perceived as annoying and socially inept. How-
ever, similarity-attraction theory states that similar-
ity promotes attraction, and someone might there-
fore entrain in order to obtain his partner?s social
approval. This idea is supported by Natale?s find-
ing that the need for social approval is predictive
of the degree of a speaker?s convergence on inten-
sity (Natale, 1975). We can therefore expect trying
to be liked to positively correlate with entrainment.
Speakers who are perceived as trying to dominate
may be overly entraining to their interlocutors in
what is sometimes called ?dependency overaccom-
modation.? Dependency overaccommodation causes
the interlocutor to appear dependent on the speaker
and gives the impression that the speaker is control-
ling the conversation (West & Turner, 2009).
The results of our correlations of social vari-
ables with acoustic/prosodic entrainment are gen-
erally consonant with these intuitions. Although it
is not straightforward to compare correlation coeffi-
cients of groups for which we have varying amounts
of data, for purposes of assessing trends, we will
consider a correlation strong if it is significant at the
p < 0.00001 level, moderate at the p < 0.01 level,
and weak at the p < 0.05 level. The results are sum-
marized in Table 3; we present only the significant
results for space considerations.
For female-female pairs, giving encouragement
is weakly correlated with entrainment on intensity
max and shimmer. Conversation awkward is weakly
correlated with entrainment on jitter. For male-male
pairs, trying to be liked is moderately correlated
with entrainment on intensity mean and weakly cor-
related with entrainment on jitter and NHR. Giv-
ing encouragement is moderately correlated with
entrainment on intensity mean, intensity max, and
NHR. For female-male pairs, trying to be liked
is moderately correlated with entrainment on pitch
mean. Giving encouragement is strongly corre-
lated with entrainment on intensity mean and max
and moderately correlated with entrainment on pitch
mean and shimmer. However, it is negatively cor-
related with entrainment on jitter, although the cor-
relation is weak. Conversation awkward is weakly
correlated with entrainment on jitter.
As we expected, giving encouragement is corre-
lated with entrainment for all three gender groups,
and trying to be liked is correlated with entrainment
for male-male and female-male groups. However,
trying to dominate is not correlated with entrainment
on any feature, and conversation awkward is actu-
ally positively correlated with entrainment on jitter.
Entrainment on jitter is a clear outlier here, with
all of its correlations contrary to our hypotheses. In
addition to being positively correlated with conver-
sation awkward, it is the only feature to be nega-
tively correlated with giving encouragement.
Entrainment is correlated with the most social
variables for female-male pairs; these correlations
are also the strongest. We therefore conclude that
acoustic entrainment is not only most prevalent for
mixed-gender pairs, it is also more important to the
perception of female-male social behavior than it is
for same-gender pairs.
6 Entrainment and objective measures of
dialogue success
We now examine acoustic/prosodic entrainment in
our corpus according to four objective measures of
dialogue success: the mean latency between turns,
the percentage of turns that are interruptions, the
percentage of turns that are overlaps, and the number
of turns in a task.
High latency between turns can be considered a
sign of an unsuccessful conversation, with poor turn-
taking behavior indicating a possible lack of rapport
and difficulty in communication between the part-
ners. A high percentage of interruptions, another ex-
ample of poor turn-taking behavior, may be a symp-
tom of or a reason for hostility or awkwardness be-
15
Social Acoustic df r p
Female-Female
Giving Int. max -0.24 0.03
enc. Shimmer -0.24 0.03
Conv. Jitter -0.23 0.03
awkward
Male-Male
Trying to Int. mean -0.30 0.006
be liked Jitter -0.27 0.01
NHR -0.23 0.03
Giving Int. mean -0.39 0.0003
enc. Int. max -0.31 0.005
NHR -0.30 0.005
Female-Male
Trying to Pitch mean -0.26 0.001
be liked
Giving Int. mean -0.36 2.8e-06
enc. Int. max -0.31 7.7e-05
Pitch mean -0.23 0.003
Jitter 0.19 0.02
Shimmer -0.16 0.04
Conv. Jitter -0.17 0.04
awkward
Table 3: Correlations between entrainment and social
variables.
tween partners. We expect these measures to be neg-
atively correlated with entrainment. Conversely, a
high percentage of overlaps may be a symptom of
a well-coordinated conversation that is flowing eas-
ily. In the guidelines for the turn-taking annotation
of the Games Corpus (Gravano, 2009), overlaps are
defined as cases in which Speaker 2 takes the floor,
overlapping with the completion of Speaker 1?s ut-
terance. Overlaps require the successful reading of
turn-taking cues and by definition preclude awkward
pauses. We expect a high percentage of overlaps to
correlate positively with entrainment.
The number of turns in a task can be interpreted
either positively or negatively. A high number is
negative in that it is the sign of an inefficient dia-
logue, one which takes many turn exchanges to ac-
complish the objective. However, it may also be
the sign of easy, flowing dialogue between the part-
ners. In our domain, it may also be a sign of a high-
achieving pair who are placing the object meticu-
Objective Acoustic df r p
Female-Female
Latency Int. mean 0.22 0.04
Int. max 0.31 0.005
Pitch mean 0.24 0.02
Jitter 0.29 0.007
Shimmer 0.33 0.002
Syllables/sec 0.39 0.0002
# Turns Int. max -0.30 0.006
Shimmer -0.34 0.002
NHR -0.24 0.03
Syllables/sec -0.28 0.01
% Overlaps Int. max -0.23 0.04
Shimmer -0.30 0.005
% Interruptions Shimmer -0.33 0.005
Male-Male
Latency Int. mean 0.57 8.8e-08
Int. max 0.43 0.0001
Pitch mean 0.52 2.4e-06
Pitch max 0.61 5.7e-09
Jitter 0.65 4.5e-10
NHR 0.40 0.0004
# Turns Int. mean -0.29 0.0002
Pitch mean -0.32 0.003
Pitch max -0.29 0.007
NHR -0.47 7.9e-06
Syllables/sec -0.25 0.02
% Overlaps Int. mean -0.39 0.0002
Int. max -0.39 0.0002
% Interruptions NHR -0.33 0.002
Female-Male
# Turns Int. mean -0.24 0.003
Int. max -0.19 0.02
Shimmer -0.16 0.04
% Overlaps Shimmer -0.26 0.001
Table 4: Correlations between entrainment and objective
variables.
lously in order to secure every single point. We
therefore expect the number of turns to be positively
correlated with entrainment. As before, we con-
sider a correlation strong if it is significant at the
p < 0.00001 level, moderate at the p < 0.01 level,
and weak at the p < 0.05 level. The significant cor-
relations are presented in Table 4.
For female-female pairs, mean latency between
16
turns is negatively correlated with entrainment on all
variables except pitch max and NHR. The correla-
tions are weak for intensity mean and pitch mean
and moderate for intensity max, jitter, shimmer, and
syllables per second. The number of turns is moder-
ately correlated with entrainment on intensity max
and shimmer and weakly correlated with entrain-
ment on syllables per second. Contrary to our expec-
tations, the percentage of interruptions is positively
(though moderately) correlated with entrainment on
shimmer; the percentage of overlaps is moderately
correlated with entrainment on shimmer and weakly
correlated with entrainment on intensity max.
Male-male pairs show the most correlations be-
tween entrainment and objective measures of dia-
logue success. The latency between turns is neg-
atively correlated with entrainment on all variables
except shimmer and syllables per second; the corre-
lations are moderate for intensity max and NHR and
strong for the rest. The number of turns in a task
is positively correlated with entrainment on every
variable except intensity mean, jitter and shimmer:
strongly for NHR; moderately for intensity mean,
pitch mean, and pitch max; and weakly for syllables
per second.. The percentage of overlaps is moder-
ately correlated with entrainment on intensity mean
and max. The percentage of interruptions is moder-
ately correlated with entrainment on NHR.
For female-male pairs, the number of turns is
moderately correlated with entrainment on intensity
mean and weakly correlated with entrainment on in-
tensity max and shimmer. The percentage of over-
laps is moderately correlated with entrainment on
shimmer.
For the most part, the directions of the correla-
tions we have found are in accordance with our hy-
potheses. Latency is negatively correlated with en-
trainment and overlaps and the number of turns are
positively correlated. A puzzling exception is the
percentage of interruptions, which is positively cor-
related with entrainment on shimmer (for female-
female pairs) and NHR (for male-male pairs).
While the strongest correlations were for mixed-
gender pairs for the social variables, we find that
the strongest correlations for objective variables are
for male-male pairs, which also have the great-
est number of correlations. It therefore seems that
while entrainment is more important to the percep-
tion of social behavior for mixed-gender pairs than
it is for same-gender pairs, it is more important to
the smoothness and flow of dialogue for male-male
pairs than it is for female-female or female-male
pairs.
7 Entrainment in outliers
Since acoustic entrainment is generally considered
an unconscious phenomenon, it is interesting to con-
sider tasks in which a particular feature of a person?s
speech is particularly salient. This will occur when a
feature differs significantly from the norm ? for ex-
ample, when a person?s voice is unusually loud or
soft. Chartrand and Bargh (1999) suggest that the
psychological mechanism behind the entrainment is
the perception-behavior link, the finding that the act
of observing another?s behavior increases the like-
lihood of the observer?s engaging in that behavior.
Based on this finding, we hypothesize that a part-
ner pair containing one ?outlier? speaker will exhibit
more entrainment on the salient feature, since that
feature is more likely to be observed and therefore
imitated.
We consider values in the 10th or 90th percentile
for a feature ?outliers.? We can consider ENTx, the
similarity between a speaker and the speakers of her
partner?s role and gender with whom she is never
paired, the ?baseline? value for the similarity be-
tween a speaker and her interlocutor when no en-
trainment occurs. ENTp ? ENTx, the difference be-
tween the similarity existing between partners and
the baseline similarity, is then a measure of how
much entrainment exists relative to baseline.
We compare ENTp ? ENTx for ?normal? versus
?outlier? speakers. ENTp should be smaller for out-
lier speakers, since their interlocutors are not likely
to be similarly unusual. However, ENTx should also
be lower for outlier speakers, since by definition they
diverge from the norm, while the normal speakers
by definition represent the norm. It is therefore rea-
sonable to expect ENTp ? ENTx to be the same for
outlier speakers and normal speakers.
If ENTp ? ENTx is higher for outlier speakers,
that means that ENTp is higher than we expect, and
entrainment is greater relative to baseline for pairs
containing an outlier speaker. If ENTp ? ENTx is
lower for outlier speakers, that means that ENTp is
17
Acoustic t df p
Intensity mean 5.66 94.26 1.7e-07
Intensity max 8.29 152.05 5.5e-14
Pitch mean -1.20 76.82 N.S.
Pitch max -0.84 76.76 N.S.
Jitter 0.36 70.23 N.S.
Shimmer 2.64 102.23 0.02
NHR -0.92 137.34 N.S.
Syllables per sec 2.41 72.60 0.02
Table 5: T-tests for relative entrainment for outlier vs.
normal speakers.
lower than we expect, and pairs containing an outlier
speaker entrain less than do pairs of normal speak-
ers, even allowing for the fact that their usual values
should be further apart to begin with.
The results for t-tests comparing ENTp ? ENTx
for ?normal? versus ?outlier? speakers are shown
in Table 5. Outlier pairs have higher relative en-
trainment than do normal pairs for intensity mean
and max, shimmer, and syllables per second. This
means that speakers confronted with an interlocutor
who diverges widely from the norm for those four
features make a larger adjustment to their speech in
order to converge to that interlocutor.
An ANOVA shows that relative entrainment on
intensity max is higher in outlier cases for male-
male pairs than for female-female pairs and even
higher for female-male pairs (F=11.33, p=5.3e-05).
Relative entrainment on NHR in these cases is lower
for male-male pairs than for female-female pairs
and higher for female-male pairs (F=11.41, p=6.5e-
05). Relative entrainment on syllables per second
is lower for male-male pairs and higher for female-
male pairs (F=5.73, p=0.005). These results differ
slightly from the results in Table 2 for differences
in entrainment in the general case among gender
pairs, reinforcing the idea that cases in which fea-
ture values diverge widely from the norm are unique
in terms of entrainment behavior.
8 Conclusion
Our study of entrainment on acoustic/prosodic vari-
ables yields new findings about entrainment be-
havior for female-female, male-male, and mixed-
gender dyads, as well as the association of entrain-
ment with perceived social characteristics and ob-
jective measures of dialogue smoothness and effi-
ciency. We find that entrainment is the most preva-
lent for mixed-gender pairs, followed by female-
female pairs, with male-male pairs entraining the
least. Entrainment is the most important to the per-
ception of social behavior of mixed-gender pairs,
and it is the most important to the efficiency and flow
of male-male dialogues.
For the most part, the directions of the correla-
tions of entrainment with success variables accord
with hypotheses motivated by the relevant literature.
Giving encouragement and trying to be liked are
positively correlated with entrainment, as are per-
centage of overlaps and number of turns. Mean la-
tency, a symptom of a poorly-run conversation, is
negatively associated with entrainment. However,
several exceptions suggest that the associations are
not straightforward and further research must be
done to fully understand the relationship between
entrainment, social characteristics and dialogue suc-
cess. In particular, the explanation behind the as-
sociations of entrainment on certain variables with
certain social and objective measures is an interest-
ing direction for future work.
Finally, we find that in ?outlier? cases where a
particular speaker diverges widely from the norm for
intensity mean, intensity max, or syllables per sec-
ond, entrainment is more pronounced. This supports
the theory that the perception-behavior link is the
mechanism behind entrainment and provides a pos-
sible direction for research into why speakers entrain
on certain features and not others. In future work we
will explore this direction and go more thoroughly
into individual differences in entrainment behavior.
Acknowledgments
This material is based upon work supported in
part by NSF IIS-0307905, NSF IIS-0803148,
UBACYT 20020090300087, ANPCYT PICT-2009-
0026, CONICET, VEGA No. 2/0202/11; and the
EUSF (ITMS 26240220060).
References
Amazon Mechanical Turk, http://www.mturk.com.
Frances R. Bilous and Robert M. Krauss 1988. Dom-
inance and accommodation in the conversational be-
18
haviours of same- and mixed-gender dyads. Language
and Communication, 8(3/4):183?194.
Susan E. Brennan and Herbert H. Clark. 1996. Concep-
tual Pacts and Lexical Choice in Conversation. Jour-
nal of Experimental Psychology: Learning, Memory
and Cognition, 22(6):1482?1493.
Tanya L. Chartrand and John A. Bargh. 1999. The
Chameleon Effect: The Perception-Behavior Link and
Social Interaction. Journal of Personality and Social
Psychology, 76(6):893?910.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark My Words! Linguistic
Style Accommodation in Social Media. Proceedings
of WWW 2011.
H. Giles and N. Coupland. 1991. Language: Contexts
and Consequences. Pacific Grove, CA: Brooks/Cole.
Agust??n Gravano. 2009. Turn-Taking and Affirmative
Cue Words in Task-Oriented Dialogue. Ph.D. thesis,
Columbia University, New York.
Agust??n Gravano and Julia Hirschberg. 2011. Turn-
taking cues in task-oriented dialogue. Computer
Speech and Language, 25(3):601?634.
Agust??n Gravano, Rivka Levitan, Laura Willson, S?tefan
Ben?us?, Julia Hirschberg, Ani Nenkova. 2011. Acous-
tic and Prosodic Correlates of Social Behavior. Inter-
speech 2011.
Chi-Chun Lee, Matthew Black, Athanasios Katsama-
nis, Adam Lammert, Brian Baucom, Andrew Chris-
tensen, Panayiotis G. Georgiou, Shrikanth Narayanan.
2010. Quantification of Prosodic Entrainment in Af-
fective Spontaneous Spoken Interactions of Married
Couples. Eleventh Annual Conference of the Interna-
tional Speech Communication Association.
Rivka Levitan, Agust??n Gravano, and Julia Hirschberg.
2011. Entrainment in Speech Preceding Backchan-
nels. Proceedings of ACL/HLT 2011.
Rivka Levitan and Julia Hirschberg. 2011. Measuring
acoustic-prosodic entrainment with respect to multi-
ple levels and dimensions. Proceedings of Interspeech
2011.
Laura L. Namy, Lynne C. Nygaard, Denise Sauerteig.
2002. Gender differences in vocal accommodation:
the role of perception. Journal of Language and So-
cial Psychology, 21(4):422?432.
Michael Natale. 1975. Convergence of Mean Vocal In-
tensity in Dyadic Communication as a Function of So-
cial Desirability. Journal of Personality and Social
Psychology, 32(5):790?804.
Ani Nenkova, Agust??n Gravano, and Julia Hirschberg.
2008. High-frequency word entrainment in spoken di-
alogue. Proceedings of ACL/HLT 2008.
Kate G. Niederhoffer and James W. Pennebaker. 2002.
Linguistic style matching in social interaction. Jour-
nal of Language and Social Psychology, 21(4):337?
360.
Jennifer S. Pardo. 2006. On phonetic convergence dur-
ing conversational interaction. Journal of the Acousti-
cal Society of America, 119(4):2382?2393.
David Reitter, Johanna D. Moore, and Frank Keller.
1996. Priming of Syntactic Rules in Task-Oriented Di-
alogue and Spontaneous Conversation. Proceedings of
the 28th Annual Conference of the Cognitive Science
Society.
Kim Silverman, Mary Beckman, John Pitrelli, Mori Os-
tendorf, Colin Wightman, Patti Price, Janet Pierrehum-
bert, Julia Hirschberg. 1992. TOBI: A Standard for
Labeling English Prosody. ICSLP-1992, 867-870.
Linda Tickle-Degnen and Robert Rosenthal. 1990. The
Nature of Rapport and its Nonverbal Correlates. Psy-
chological Inquiry, 1(4):285?293.
Richard West & Lynn Turner. 2009. Introducing
Communication Theory: Analysis and Application.
McGraw-Hill Humanities/Social Sciences/Languages,
4th edition.
19
Proceedings of NAACL-HLT 2013, pages 502?506,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improving speech synthesis quality by reducing pitch peaks
in the source recordings
Luisina Violante, Pablo Rodr??guez Zivic and Agust??n Gravano
Departamento de Computacio?n, FCEyN
Universidad de Buenos Aires, Argentina
{lviolante,prodriguez,gravano}@dc.uba.ar
Abstract
We present a method for improving the perceived nat-
uralness of corpus-based speech synthesizers. It con-
sists in removing pronounced pitch peaks in the origi-
nal recordings, which typically lead to noticeable dis-
continuities in the synthesized speech. We perceptu-
ally evaluated this method using two concatenative and
two HMM-based synthesis systems, and found that us-
ing it on the source recordings managed to improve
the naturalness of the synthesizers and had no effect
on their intelligibility.
1 Introduction
By definition, corpus-based speech synthesizers,
such as concatenative and HMM-based systems,
rely heavily on the quality of the speech corpus used
for building the systems. Creating speech corpora
for this purpose is expensive and time consuming, so
when the synthesized speech obtained is not as good
as expected, it may be desirable to modify or correct
the corpus rather than record a new one. Common
corrections are limited to discarding mispronounced
words or noisy units. In this work we describe a sim-
ple method for attenuating pronounced pitch peaks,
a frequent problem in recordings made by profes-
sional speakers, and evaluate it using four different
corpus-based systems. Sections 2 and 3 describe the
speech synthesis systems and corpus employed in
this work. In Section 4 we present the method for
reducing pitch peaks. In Section 5 we describe how
we evaluated the effect of our method on intelligibil-
ity and naturalness of the synthesizers.
2 Synthesis systems
Festival1 is a general framework for building speech
synthesis systems, written in C++ and developed by
the Center of Speech Technology Research at the
University of Edinburgh (Black et al, 2001). It
provides an implementation of concatenative speech
synthesis as well as synthesis based on Hidden
Markov Models (HMM). In this work we used a Fes-
tival module called Clunits unit selection engine to
build concatenative synthesizers. The unit size is the
phone, although since a percentage of the previous
unit is included in the acoustic distance measure, the
unit size is rather ?phone plus previous phone?, thus
similar to a diphone (Black and Lenzo, 2007). Ad-
ditionally, we used a second Festival module called
Clustergen parametric synthesis engine for building
HMM-based speech synthesizers.
MARY TTS2 is an open-source synthesis plat-
form written in Java, originally jointly developed
by the Language Technology Lab at the German
Research Center for Artificial Intelligence (DFKI)
and the Institute of Phonetics at Saarland Univer-
sity, and currently maintained by DFKI. Like Fes-
tival, MARY provides toolkits for building unit se-
lection and HMM-based synthesis voices (Schro?der
and Trouvain, 2003).
3 Corpus
For building our systems we used the SECYT cor-
pus, created by the Laboratorio de Investigacio-
nes Sensoriales (Universidad de Buenos Aires) for
1http://festvox.org/festival
2http://mary.dfki.de
502
studying the prosody of Argentine Spanish (Torres
and Gurlekian, 2004). It consists of 741 declarative
sentences recorded by a female professional speaker
(pitch range: 130-380Hz). On average, sentences
are 7 words and 3.9 seconds long. The entire corpus
has manual phonetic transcriptions and time align-
ments, following a version of the Speech Assessment
Methods Phonetic Alphabet (SAMPA) adapted for
Argentine Spanish (Gurlekian et al, 2001).
A priori, this corpus is a very good candidate for
building a synthesis system ? its 741 sentences are
phonetically balanced, the audio quality is excellent,
and it has precise time-aligned phonetic transcrip-
tions. We thus built two concatenation systems us-
ing this corpus: Festival?s diphone-like and MARY?s
diphone systems. The results were not satisfactory.
The new voices presented clearly noticeable discon-
tinuities, both in intensity and pitch, which affected
their naturalness ? as judged impressionistically by
the authors and non-expert colleagues.
In an attempt to attenuate these problems, we lev-
eled the intensity of all recordings to a mean of 72dB
using linear interpolation. Specifically, each sound
was multiplied by a number such that its new aver-
age RMS intensity was 72dB; so that all sentences
in the corpus ended up with the same average inten-
sity. After this conversion, we rebuilt the systems.
The resulting voices sounded somewhat better, but
their most noticeable problem, severe pitch discon-
tinuities, persisted.
Further analysis of the corpus recordings revealed
that this issue was likely due to the speaking style
employed by the professional speaker. It contains
frequent pronounced pitch peaks, a verbal stylistic
device acquired by the speaker as part of her pro-
fessional training. These events produced units with
very different pitch levels and slopes, thus leading to
the discontinuities mentioned above.
4 Reduction of pitch peaks
We searched for ways to reduce the magnitude of
these pitch peaks by manipulating the pitch track
of the recordings using the Time-Domain Pitch-
Synchronous OverLap-and-Add (TD-PSOLA) sig-
nal processing technique (Moulines and Charpen-
tier, 1990). We used the implementation of TD-
PSOLA included in the Praat toolkit (Boersma and
Weenink, 2012).
We tried several formulas for TD-PSOLA and
ended up choosing the one that appeared to yield the
best results, evaluated perceptually by the authors:
f(x) =
{
(x? T ) ? s + T if x > T
x otherwise.
This formula linearly scales the pitch track by a scal-
ing factor s above a threshold T , and leaves it intact
below T . When 0 < s < 1, the pitch track gets com-
pressed above the threshold. We experimented with
several values for the two constants, and selected
T = 200Hz and s = 0.4 as the ones producing the
best results. Figure 1 illustrates the pitch peak re-
duction method. The black solid line corresponds to
1.0 1.5 2.0 2.5Time (s)
150
200
250
300
Hz
OriginalModified
Figure 1: Reduction of pitch peaks. The original pitch
track (in black) is scaled down 40% above 200Hz.
the pitch track of the original audio; the red dotted
line, to the pitch track of the modified audio. Note
that the modified pitch track is scaled down above
200Hz, but identical to the original below it.
5 Evaluation of the method
Next we proceeded to evaluate the effect on synthe-
sizer quality of reducing pitch peaks in the train-
ing corpus. For this purpose we prepared two ver-
sions of the SECYT corpus ? with and without ap-
plying our pitch-peak reduction technique. We refer
to these two as the original and modified recordings,
respectively. In both cases, the intensity level of all
audios was first leveled to a mean of 72dB using
linear interpolation, to compensate for differences
across recordings.
503
Subsequently, we built 8 speech synthesizers,
consisting in all combinations of: Festival and
MARY frameworks, concatenative and HMM-based
synthesis, and original and modified recordings. We
refer to these systems using the following nota-
tion: {fest, mary} {conc, hmm} {orig, mod}; e.g.,
mary conc mod is a concatenative system built us-
ing the MARY framework with the modified corpus.
We evaluated these systems along two dimen-
sions: intelligibility and naturalness. Our goal was
to compare four system pairs: systems built using
the original recordings vs. those built using the mod-
ified recordings. The null hypothesis was that there
was no difference between ?orig? and ?mod? sys-
tems; and the alternative hypothesis was that ?mod?
systems were better than ?orig? ones.
5.1 Intelligibility
To evaluate intelligibility we used the Semantically
Unpredictable Sentences (SUS) method (Nye and
Gaitenby, 1974), which consists in asking partici-
pants to listen to and transcribe sentences with cor-
rect syntax but no semantic sense, for later measur-
ing and comparing the number of transcription er-
rors. We used a set of 50 such sentences, each 6-10
words long, created by Gurlekian et al (2012) for
evaluating Spanish speech synthesizers. A sample
sentence is, El viento dulce armo? un libro de pan-
queques (The sweet wind made a book of pancakes).
For each participant, 40 sentences were selected
at random and synthesized with the 8 systems (5 sen-
tences per system, with no repetitions). Participants
were given the following instructions,
La primera tarea consiste en escuchar varios audios, y
transcribir para cada audio la oracio?n que escuches.
Presta? atencio?n, porque pode?s escuchar cada audio
una sola vez.
(The first task consists in listening to several audios,
and transcribing for each audio the sentence you hear.
Pay attention, because you can only listen to each au-
dio once.)
5.2 Naturalness
To evaluate naturalness we used the Mean Opin-
ion Score (MOS) method, in which participants
are asked to rate the overall quality of synthe-
sized speech on a 10-point scale (Viswanathan and
Viswanathan, 2005).
We used a set of 20 sentences, each 5-20 words
long, created by Gurlekian et al (2012), plus 20 ad-
ditional sentences created for this study. A sample
sentence is, El sector de informa?tica es el nuevo
generador de empleo del pa??s (The information
technology sector is the country?s new job creator).
Again, for each participant, 40 sentences were se-
lected at random and synthesized with the 8 systems
(5 sentences per system). Participants were given
the following instructions,
La segunda (y u?ltima) tarea consiste en escuchar otros
audios, y puntuar la naturalidad de cada uno. Usar
una escala de 1 a 10, donde 1 significa ?no suena nat-
ural en lo absoluto? y 10 significa ?suena completa-
mente natural?. En este caso, pode?s escuchar cada
audio una o ma?s veces.
(The second (and last) task consists in listening to
other audios, and score the naturalness of each. Use
a scale from 1 to 10, where 1 means ?it does not sound
natural at all? and 10 means ?it sounds completely nat-
ural?. In this case, you may listen to each audio one or
more times.)
5.3 Results
SUS and MOS tests were administered on a com-
puter interface in a silent laboratory using regular
headphones. 14 graduate and undergraduate stu-
dents (11 male, 3 female; mean age: 27.6) com-
pleted both tests ? first SUS, followed by MOS.
The transcriptions of the SUS tests were manually
corrected for obvious typos and spelling errors that
did not form a valid Spanish word. Suspected typos
and spelling errors that formed a valid word were not
corrected. For example, peliculas was corrected to
pel??culas, and precion to presio?n; but canto was not
corrected to canto?, since it is a valid word. Subse-
quently, we computed the Levenshtein distance be-
tween each transcription and the corresponding sen-
tence. Figure 2 shows the distribution of Leven-
shtein distances for each of our eight systems. We
observe that all systems had a low error count, with
a median of 0 or 1 errors per sentence. Two-tail
Wilcoxon signed-rank tests revealed no significant
differences between the systems built with the origi-
nal and modified recordings (p=0.70 for fest conc,
p = 0.40 for fest hmm, p = 0.69 for mary conc,
p=0.40 for mary hmm, and p=0.41 for all systems
together). These results indicate that the intelligibil-
ity of all four system types was not affected by the
504
festconcmod
festconcorig
festhmmmod
festhmmorig
maryconcmod
maryconcorig
maryhmmmod
maryhmmorig
1
0
1
2
3
4
5
6
7
Lev
ens
hte
in d
ista
nce
Figure 2: Intelligibility (SUS) results.
modifications performed on the corpus for reducing
pitch peaks.
To account for the different interpretations of the
10-point scale, we normalized all MOS test scores
by participant using z-scores.3 Figure 3 shows the
distribution of values for each system.
festconcorig
festconcmod
festhmmorig
festhmmmod
maryconcorig
maryconcmod
maryhmmorig
maryhmmmod
3
2
1
0
1
2
3
z-sc
ore
Figure 3: Naturalness (MOS) results.
We performed a series of Wilcoxon signed-rank
tests to assess the statistical significance of the ob-
served differences. The null hypothesis was that
there was no difference between ?orig? and ?mod?
systems; and the alternative hypothesis was that
?mod? systems were perceived as more natural than
?orig? ones. Table 5.3 summarizes these results.
For mary conc and mary hmm (concatenative
and HMM-based systems built using the MARY
3z = (x? x)/s, where x and s are estimates of the partici-
pant?s mean and standard deviation, respectively.
W p-value
fest conc 2485 0.559
fest hmm 2175 0.126
mary conc 1933 0.016
mary hmm 1680.5 0.001
All systems 34064.5 0.004
Table 1: Results of Wilcoxon tests comparing systems
using the original and modified audios.
framework) the perceived naturalness was signifi-
cantly higher for systems built using the modified
recordings (i.e., after reducing pitch peaks) than
for systems built with the original recordings. For
fest conc (concatenative system built with Festival)
we found no evidence of such differences. Finally,
for fest hmm (Festival HMM-based) the difference
approaches significance at 0.126.
6 Conclusions
In this paper we presented a method for improving
the perceived naturalness of corpus-based speech
synthesizers. It consists in removing pronounced
pitch peaks in the original recordings, which typ-
ically produce discontinuities in the synthesized
speech. We evaluated this method using two com-
mon technologies (concatenative and HMM-based
synthesis) and two different implementations (Festi-
val and MARY), aiming at a good coverage of state-
of-the-art speech synthesizers, and obtained clear re-
sults. First, its utilization on the source recordings
had no effect (negative or positive) on the intelligi-
bility of any of the systems. Second, the natural-
ness of the concatenative and HMM-based systems
built with the MARY framework improved signif-
icantly; the HMM-based system built with Festival
showed an improved naturalness at a level approach-
ing significance; and the Festival concatenative sys-
tem showed no improvement. In summary, the pre-
sented method did not harm the intelligibility of the
systems, and in some cases managed to improve
their naturalness. Therefore, since the impact of the
proposed modifications on all four systems was pos-
itive to neutral, developers may find this methodol-
ogy beneficial.
505
Acknowledgments
This work was funded in part by CONICET, ANPCYT
PICT 2009-0026, and UBACYT 20020090300087. The
authors thank Jorge A. Gurlekian, Humberto M. Torres
and Christian G. Cossio-Mercado from LIS (INIGEM,
CONICET-UBA) for kindly sharing the SECYT corpus
and other materials for the present study, as well as for
valuable suggestions and comments.
References
Alan W. Black and Kevin A. Lenzo. 2007. Building
Synthetic Voices. Language Technologies Institute,
Carnegie Mellon University, http://festvox.org/bsv.
A. Black, P. Taylor, R. Caley, R. Clark, K. Richmond,
S. King, V. Strom, and H. Zen. 2001. The festival
speech synthesis system.
Paul Boersma and David Weenink. 2012. Praat: doing
phonetics by computer. http://www.praat.org/.
J. Gurlekian, L. Colantoni, and H. Torres. 2001. El al-
fabeto fone?tico SAMPA y el disen?o de corpora fone?-
ticamente balanceados. Fonoaudiolo?gica, 47:58?69.
J. A. Gurlekian, C. Cossio-Mercado, H. Torres, and M. E.
Vaccari. 2012. Subjective evaluation of a high quality
text-to-speech system for Argentine Spanish. In Pro-
ceedings of Iberspeech, Madrid, Spain.
E. Moulines and F. Charpentier. 1990. Pitch-syn-
chronous waveform processing techniques for text-to-
speech synthesis using diphones. Speech communica-
tion, 9(5):453?467.
P. W. Nye and J. H. Gaitenby. 1974. The intelligibility
of synthetic monosyllabic words in short, syntactically
normal sentences. Haskins Laboratories Status Report
on Speech Research, 37(38):169?190.
M. Schro?der and J. Trouvain. 2003. The German text-to-
speech synthesis system MARY: A tool for research,
development and teaching. International Journal of
Speech Technology, 6(4):365?377.
H. M. Torres and J. A. Gurlekian. 2004. Automatic de-
termination of phrase breaks for Argentine Spanish. In
Speech Prosody 2004, International Conference.
Mahesh Viswanathan and Madhubalan Viswanathan.
2005. Measuring speech quality for text-to-speech
systems: Development and assessment of a modified
mean opinion score (MOS) scale. Computer Speech
& Language, 19(1):55?83.
506
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 113?117,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Entrainment in Speech Preceding Backchannels
Rivka Levitan
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
rlevitan@cs.columbia.edu
Agust??n Gravano
DC-FCEyN & LIS
Universidad de Buenos Aires
Buenos Aires, Argentina
gravano@dc.uba.ar
Julia Hirschberg
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
julia@cs.columbia.edu
Abstract
In conversation, when speech is followed by
a backchannel, evidence of continued engage-
ment by one?s dialogue partner, that speech
displays a combination of cues that appear to
signal to one?s interlocutor that a backchan-
nel is appropriate. We term these cues back-
channel-preceding cues (BPC)s, and examine
the Columbia Games Corpus for evidence of
entrainment on such cues. Entrainment, the
phenomenon of dialogue partners becoming
more similar to each other, is widely believed
to be crucial to conversation quality and suc-
cess. Our results show that speaking partners
entrain on BPCs; that is, they tend to use simi-
lar sets of BPCs; this similarity increases over
the course of a dialogue; and this similarity is
associated with measures of dialogue coordi-
nation and task success.
1 Introduction
In conversation, dialogue partners often become
more similar to each other. This phenomenon,
known in the literature as entrainment, alignment,
accommodation, or adaptation has been found to
occur along many acoustic, prosodic, syntactic and
lexical dimensions in both human-human interac-
tions (Brennan and Clark, 1996; Coulston et al,
2002; Reitter et al, 2006; Ward and Litman,
2007; Niederhoffer and Pennebaker, 2002; Ward and
Mamidipally, 2008; Buder et al, 2010) and human-
computer interactions (Brennan, 1996; Bell et al,
2000; Stoyanchev and Stent, 2009; Bell et al, 2003)
and has been associated with dialogue success and
naturalness (Pickering and Garrod, 2004; Goleman,
2006; Nenkova et al, 2008). That is, interlocutors
who entrain achieve better communication. How-
ever, the question of how best to measure this phe-
nomenon has not been well established. Most re-
search has examined similarity of behavior over a
conversation, or has compared similarity in early
and later phases of a conversation; more recent work
has proposed new metrics of synchrony and conver-
gence (Edlund et al, 2009) and measures of similar-
ity at a more local level (Heldner et al, 2010).
While a number of dimensions of potential en-
trainment have been studied in the literature, en-
trainment in turn-taking behaviors has received lit-
tle attention. In this paper we examine entrainment
in a novel turn-taking dimension: backchannel-
preceding cues (BPC)s.1 Backchannels are short
segments of speech uttered to signal continued in-
terest and understanding without taking the floor
(Schegloff, 1982). In a study of the Columbia
Games Corpus, Gravano and Hirschberg (2009;
2011) identify five speech phenomena that are
significantly correlated with speech followed by
backchannels. However, they also note that indi-
vidual speakers produced different combinations of
these cues and varied the way cues were expressed.
In our work, we look for evidence that speaker pairs
negotiate the choice of such cues and their realiza-
tions in a conversation ? that is, they entrain to one
another in their choice and production of such cues.
We test for evidence both at the global and at the
local level.
1Prior studies termed cues that precede backchannels, back-
channel-inviting cues. To avoid suggesting that such cues are a
speaker?s conscious decision, we adopt a more neutral term.
113
In Section 2, we describe the Columbia Games
Corpus, on which the current analysis was con-
ducted. In Section 3, we present three measures of
BPC entrainment. In Section 4, we further show that
two of these measures also correlate with dialogue
coordination and task success.
2 The Columbia Games Corpus
The Columbia Games Corpus is a collection of 12
spontaneous dyadic conversations elicited from na-
tive speakers of Standard American English. 13 peo-
ple participated in the collection of the corpus. 11
participated in two sessions, each time with a dif-
ferent partner. Subjects were separated by a curtain
to ensure that all communication was verbal. They
played a series of computer games requiring collab-
oration in order to achieve a high score.
The corpus consists of 9h 8m of speech. It is
orthographically transcribed and annotated for var-
ious types of turn-taking behavior, including smooth
switches (cases in which one speaker completes her
turn and another speaker takes the floor), interrup-
tions (cases in which one speaker breaks in, leaving
the interlocutor?s turn incomplete), and backchan-
nels. There are 5641 exchanges in the corpus; of
these, approximately 58% are smooth switches, 2%
are interruptions, and 11% are backchannels. Other
turn types include overlaps and pause interruptions;
a full description of the Columbia Games Corpus?
annotation for turn-taking behavior can be found in
(Gravano and Hirschberg, 2011).
3 Evidence of entrainment
Gravano and Hirschberg (2009; 2011) identify five
cues that tend to be present in speech preceding
backchannels. These cues, and the features that
model them, are listed in Table 1. The likelihood
that a segment of speech will be followed by a
backchannel increases quadratically with the num-
ber of cues present in the speech. However, they
note that individual speakers may display different
combinations of cues. Furthermore, the realization
of a cue may differ from speaker to speaker. We hy-
pothesize that speaker pairs adopt a common set of
cues to which each will respond with a backchan-
nel. We look for evidence for this hypothesis us-
ing three different measures of entrainment. Two of
Cue Feature
Intonation pitch slope over the IPU-
final 200 and 300 ms
Pitch mean pitch over the final
500 and 1000 ms
Intensity mean intensity over the
final 500 and 1000 ms
Duration IPU duration in seconds
and word count
Voice quality NHR over the final 500
and 1000 ms
Table 1: Features modeling each of the five cues.
these measures capture entrainment globally, over
the course of an entire dialogue, while the third
looks at entrainment on a local level. The unit of
analysis we employ for each experiment is an inter-
pausal unit (IPU), defined as a pause-free segment
of speech from a single speaker, where pause is de-
fined as a silence of 50ms or more from the same
speaker. We term consecutive pairs of IPUs from
a single speaker holds, and contrast hold-preceding
IPUs with backchannel-preceding IPUs to isolate
cues that are significant in preceding backchannels.
That is, when a speaker pauses without giving up
the turn, which IPUs are followed by backchannels
and which are not? We consider a speaker to use
a certain BPC if, for any of the features model-
ing that cue, the difference between backchannel-
preceding IPUs and hold-preceding IPUs is signif-
icant (ANOVA, p < 0.05).
3.1 Entrainment measure 1: Common cues
For our first entrainment metric, we measure the
similarity of two speakers? cue sets by simply count-
ing the number of cues that they have in common
over the entire conversation. We hypothesize that
speaker pairs will use similar sets of cues.
The speakers in our corpus each displayed 0 to 5
of the BPCs described in Table 1 (mean = 2.17). The
number of cues speaker pairs had in common ranged
from 0 to 4 (out of a maximum of 5). Let S1 and S2
be two speakers in a given dialogue, and n1,2 the
number of BPCs they had in common. Let alo n1,?
and n?,2 be the mean number of cues S1 and S2 had
in common with all other speakers in the corpus not
partnered with them in any session. For all 12 dia-
114
logues in the corpus, we pair n1,2 both with n1,? and
with n?,2, and run a paired t-test. The results indi-
cate that, on average, the speakers had significantly
more cues in common with their interlocutors than
with other speakers in the corpus (t = 2.1, df = 23,
p < 0.05).
These findings support our hypothesis that speak-
er pairs negotiate common sets of cues, and suggest
that, like other aspects of conversation, speaker vari-
ation in use of BPCs is not simply an expression of
personal behavior, but is at least partially the result
of coordination with a conversational partner.
3.2 Entrainment measure 2: BPC realization
With our second measure, we look for evidence that
the speakers? actual values for the cue features are
similar: that not only do they alter their production
of similar feature sets when preceding a backchan-
nel, they also alter their productions in similar ways.
We measure how similarly two speakers S1 and
S2 in a conversation realize a BPC as follows:
First, we compute the difference (df1,2) between both
speakers for the mean value of a feature f over
all backchannel-preceding IPUs. Second, we com-
pute the same difference between each of S1 and S2
and the averaged values of all other speakers in the
corpus who are not partnered with that speaker in
any session (df1,? and df?,2). Finally, if for any fea-
ture f modeling a given cue, it holds that df1,2 <
min(df1,?, d
f
?,2), we say that that session exhibits
mutual entrainment on that cue.
Eleven out of 12 sessions exhibit mutual entrain-
ment on pitch and intensity, 9 exhibit mutual entrain-
ment on voice quality, 8 on intonation, and 7 on du-
ration. Interestingly, the only session not entrain-
ing on intensity is the only session not entraining
on pitch, but the relationships between the different
types of entrainment is not readily observable.
For each of the 10 features associated with
backchannel invitation, we compare the differences
between conversational partners (df1,2) and the aver-
aged differences between each speaker and the other
speakers in the corpus (df1,? and df?,2). Paired t-tests
(Table 2) show that the differences in intensity, pitch
and voice quality in backchannel-preceding IPUs
are smaller between conversational partners than be-
tween speakers and their non-partners in the corpus.
Feature t df p-value Sig.
Intensity 500 -4.73 23 9.09e-05 *
Intensity 1000 -2.80 23 0.01 *
Pitch 500 -3.38 23 0.002 *
Pitch 1000 -3.28 23 0.003 *
Pitch slope 200 -1.77 23 0.09 .
Pitch slope 300 -0.93 23 N.S.
Duration 0.50 23 N.S.
# Words 1.39 23 N.S.
NHR 500 -2.00 23 0.06 .
NHR 1000 -2.30 23 0.03 *
Table 2: T -tests between partners and their non-partners
in the corpus.
The differences between interlocutor and their
non-partners in features modeling pitch show that
there is no single ?optimal? value for a pitch level
that precedes a backchannel; this value is coordi-
nated between partners on a pair-by-pair basis. Sim-
ilarly, while varying intensity or voice quality may
be considered a universal cue for a backchannel, the
specific values of the production appear to be a mat-
ter of coordination between individual speaker pairs.
While some views of entrainment hold that coor-
dination takes place at the very beginning of a dia-
logue, others hypothesize that coordination contin-
ues to improve over the course of the conversation.
T -tests for difference of means show that indeed
the differences between conversational partners in
mean pitch and intensity in the final 1000 millisec-
onds of backchannel-preceding IPUs are smaller in
the second half of the conversation than in the first
(t = 3.44, 2.17; df = 23; p < 0.05, 0.01), indicat-
ing that entrainment in this dimension is an ongoing
process that results in closer alignment after the in-
terlocutors have been speaking for some time.
3.3 Measure 3: Local BPC entrainment
Measures 1 and 2 capture global entrainment and
can be used to characterize an entire dialogue with
respect to entrainment. We now look for evidence
to support the hypothesis that a speaker?s realization
of BPCs influences how her interlocutor produces
BPCs. To capture this, we compile a list of pairs
of backchannel-preceding IPUs, in which the second
member of each pair follows the first in the conver-
115
sation and is produced by a different speaker. For
each feature, we calculate the Pearson?s correlation
between acoustic variables extracted from the first
element of each pair and the second.
The correlations for mean pitch and intensity are
significant (r = 0.3, two-sided t-test: p < 0.05, in
both cases). Other correlations are not significant.
These results suggest that entrainment on pitch and
intensity at least is a localized phenomenon. Spoken
dialogue systems may exploit this information, mod-
ifying their output to invite a backchannel similar to
the user?s own previous backchannel invitation.
4 Correlation with dialogue coordination
and task success
Entrainment is widely believed to be crucial to dia-
logue coordination. In the specific case of BPC en-
trainment, it seems intuitive that some consensus on
BPCs should be integral to the successful coordina-
tion of a conversation. Long latencies (periods of si-
lence) before backchannels can be considered a sign
of poor coordination, as when a speaker is waiting
for an indication that his partner is still attending,
and the partner is slow to realize this. Similarly,
interruptions signal poor coordination, as when a
speaker has not finished what he has to say, but his
partner thinks it is her turn to speak. We thus use
mean backchannel latency and proportion of inter-
ruptions as measures of coordination of whole ses-
sions. We use the combined score of the games the
subjects played as a measure of task success. We
correlate all three with our two global entrainment
scores and report correlation coefficients in Table 3.
Entrain. Success/coord. r p-value
measure measure
1 Latency -0.33 0.06
Interruptions -0.50 0.01
Score 0.22 N.S.
2 Latency -0.61 0.002
Interruptions -0.22 N.S.
Score 0.72 6.9e-05
Table 3: Correlations with success and coordination.
Our first metric for identifying entrainment, Mea-
sure 1, the number of cues the speaker pair has in
common, is negatively correlated with mean latency
and proportion of interruptions, our two measures of
poor coordination. Its correlation with score, though
not significant, is positive. So, more entrainment in
BPCs under Measure 1 means smaller latency before
backchannels and fewer interruptions, while there
is a tendency for such entrainment to be associated
with higher scores.
Our second entrainment metric, Measure 2, cap-
tures the similarities between speaker means of the
10 features associated with BPCs. To test correla-
tions of this measure with task success, we collapse
the ten features into a single measure by taking the
negated Euclidean distance between each speaker
pair?s 2 vectors of means; this measure tells us how
close these speakers are across all features exam-
ined. Under this analysis, we find that Measure 2
is negatively correlated with mean latency and pos-
itively correlated with score. Both correlations are
strong and highly significant. Again, the correlation
with interruptions is negative, although not signifi-
cant. Thus, more entrainment defined by this metric
means shorter latency between turns, fewer interrup-
tions, and again and more strongly, higher scores.
We thus find that, the more entrainment at the
global level, the better the coordination between the
partners and the better their performance on their
joint task. These results provide evidence of the im-
portance of BPC entrainment to dialogue.
5 Conclusion
In this paper we discuss the role of entrainment
in turn-taking behavior and its impact on conversa-
tional coordination and task success in the Columbia
Games Corpus. We examine a novel form of en-
trainment, entrainment in BPCs ? characteristics of
speech segments that are followed by backchannels
from the interlocutor. We employ three measures
of entrainment ? two global and one local ? and
find evidence of entrainment in all three. We also
find correlations between our two global entrain-
ment measures and conversational coordination and
task success. In future, we will extend this analysis
to the complementary turn-taking category of turn-
yielding cues and explore how a spoken dialogue
system may take advantage of information about en-
trainment to improve dialogue coordination and the
user experience.
116
6 Acknowledgments
This material is based on work supported in
part by the National Science Foundation under
Grant No. IIS-0803148 and by UBACYT No.
20020090300087.
References
L. Bell, J. Boye, J. Gustafson, and M. Wiren. 2000.
Modality convergence in a multimodal dialogue sys-
tem. In Proceedings of 4th Workshop on the Semantics
and Pragmatics of Dialogue (GOTALOG).
L. Bell, J. Gustafson, and M. Heldner. 2003. Prosodic
adaptation in human-computer interaction. In Pro-
ceedings of the 15th International Congress of Pho-
netic Sciences (ICPhS).
S.E. Brennan and H.H. Clark. 1996. Conceptual pacts
and lexical choice in conversation. Journal of Exper-
imental Psychology: Learning, Memory, and Cogni-
tion, 22(6):1482?1493.
S.E. Brennan. 1996. Lexical entrainment in spontaneous
dialog. In Proceedings of the International Sympo-
sium on Spoken Dialog (ISSD).
E.H. Buder, A.S. Warlaumont, D.K. Oller, and L.B.
Chorna. 2010. Dynamic indicators of Mother-Infant
Prosodic and Illocutionary Coordination. In Proceed-
ings of the 5th International Conference on Speech
Prosody.
R. Coulston, S. Oviatt, and C. Darves. 2002. Amplitude
convergence in children?s conversational speech with
animated personas. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP).
J. Edlund, M. Heldner, and J. Hirschberg. 2009. Pause
and gap length in face-to-face interaction. In Proceed-
ings of Interspeech.
D. Goleman. 2006. Social Intelligence: The New Sci-
ence of Human Relationships. Bantam.
A. Gravano and J. Hirschberg. 2009. Backchannel-
inviting cues in task-oriented dialogue. In Proceedings
of SigDial.
A. Gravano and J. Hirschberg. 2011. Turn-taking cues
in task-oriented dialogue. Computer Speech and Lan-
guage, 25(33):601?634.
M. Heldner, J. Edlund, and J. Hirschberg. 2010. Pitch
similarity in the vicinity of backchannels. In Proceed-
ings of Interspeech.
A. Nenkova, A. Gravano, and J. Hirschberg. 2008. High
frequency word entrainment in spoken dialogue. In
Proceedings of ACL/HLT.
K. Niederhoffer and J. Pennebaker. 2002. Linguistic
style matching in social interaction. Journal of Lan-
guage and Social Psychology, 21(4):337?360.
M. J. Pickering and S. Garrod. 2004. Toward a mecha-
nistic psychology of dialogue. Behavioral and Brain
Sciences, 27:169?226.
D. Reitter, F. Keller, and J.D. Moore. 2006. Computa-
tional modelling of structural priming in dialogue. In
Proceedings of HLT/NAACL.
E. Schegloff. 1982. Discourse as an interactional
achievement: Some uses of ?uh huh? and other things
that come between sentences. In D. Tannen, editor,
Analyzing Discourse: Text and Talk, pages 71?93.
Georgetown University Press.
S. Stoyanchev and A. Stent. 2009. Lexical and syntactic
priming and their impact in deployed spoken dialogue
systems. In Proceedings of NAACL.
A. Ward and D. Litman. 2007. Automatically measuring
lexical and acoustic/prosodic convergence in tutorial
dialog corpora. In Proceedings of the SLaTE Work-
shop on Speech and Language Technology in Educa-
tion.
N.G. Ward and S.K. Mamidipally. 2008. Factors Affect-
ing Speaking-Rate Adaptation in Task-Oriented Di-
alogs. In Proceedings of the 4th International Con-
ference on Speech Prosody.
117
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 21?28,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Spanish DAL: A Spanish Dictionary of Affect in Language
Mat??as G. Dell? Amerlina R??os and Agust??n Gravano
Departamento de Computacio?n, FCEyN
Universidad de Buenos Aires, Argentina
{mamerlin,gravano}@dc.uba.ar
Abstract
The topic of sentiment analysis in text has
been extensively studied in English for the
past 30 years. An early, influential work by
Cynthia Whissell, the Dictionary of Affect in
Language (DAL), allows rating words along
three dimensions: pleasantness, activation and
imagery. Given the lack of such tools in Span-
ish, we decided to replicate Whissell?s work in
that language. This paper describes the Span-
ish DAL, a knowledge base formed by more
than 2500 words manually rated by humans
along the same three dimensions. We evalu-
ated its usefulness on two sentiment analysis
tasks, which showed that the knowledge base
managed to capture relevant information re-
garding the three affective dimensions.
1 Introduction
In an attempt to quantify emotional meaning in writ-
ten language, Whissell developed the Dictionary of
Affect in Language (DAL), a tool for rating words
and texts in English along three dimensions ? pleas-
antness, activation and imagery (Whissell et al,
1986; Whissell, 1989, inter alia). DAL works by
looking up individual words in a knowledge base
containing 8742 words. All words in this lexicon
were originally rated by 200 na??ve volunteers along
the same three dimensions.
Whissell?s DAL has subsequently been used in di-
verse research fields, for example as a keystone for
sentiment analysis in written text (Yi et al, 2003,
e.g.) and emotion recognition in spoken language
(Cowie et al, 2001). DAL has also been used to aid
the selection of emotionally balanced word stimuli
for Neuroscience and Psycholinguistics experiments
(Gray et al, 2002). Given the widespread impact of
DAL for the English language, it would be desirable
to create similar lexicons for other languages.
In recent years, there have been efforts to build
cross-lingual resources, such as using sentiment
analysis tools in English to score Spanish texts af-
ter performing machine translation (Brooke et al,
2009) or to automatically derive sentiment lexicons
in Spanish (Pe?rez-Rosas et al, 2012). The purpose
of the present work is to create a manually anno-
tated lexicon for the Spanish language, replicating
Whissell?s DAL, aiming at alleviating the scarcity
of resources for the Spanish language, and at deter-
mining if the lexicon-based approach would work
in Spanish as well as it does in English. We leave
for future work the comparison of the different ap-
proaches mentioned here. This paper describes the
three steps performed to accomplish that goal: i)
creating a knowledge base which is likely to have
a good word coverage on arbitrary texts from any
topic and genre (Section 2); ii) having a number of
volunteers annotate each word for the three affective
dimensions under study (Section 3); and iii) evaluat-
ing the usefulness of our knowledge base on simple
tasks (Section 4).
2 Word selection
The first step in building a Spanish DAL consists in
selecting a list of content words that is representa-
tive of the Spanish language, in the sense that it will
have a good coverage of the words in arbitrary input
texts from potentially any topic or genre. To accom-
plish this we decided to use texts downloaded from
Wikipedia in Spanish1 and from an online collection
of short stories called Los Cuentos.2 Articles from
Wikipedia cover a wide range of topics and are gen-
1http://es.wikipedia.org
2http://www.loscuentos.net
21
erally written in encyclopedia style. We downloaded
the complete set of articles in March, 2012, consist-
ing of 834,460 articles in total. Short stories from
Los Cuentos were written by hundreds of different
authors, both popular and amateur, on various gen-
res, including tales, essays and poems. We down-
loaded the complete collection from Los Cuentos in
April, 2012, consisting of 216,060 short stories.
2.1 Filtering and lemmatizing words
We extracted all words from these texts, sorted them
by frequency, and filtered out several word classes
that we considered convey no affect by themselves
(and thus it would be unnecessary to have them rated
by the volunteers). Prepositions, determinants, pos-
sessives, interjections, conjunctions, numbers, dates
and hours were tagged and removed automatically
using the morphological analysis function included
in the Freeling toolkit (Padro? et al, 2010).3 We
also excluded the following adverb subclasses for
the same reason: place, time, mode, doubt (e.g.,
quiza?s, maybe), negation, affirmation and amount.
Nouns and verbs were lemmatized using Freel-
ing as well, except for augmentative and diminu-
tive terminations, which were left intact due to their
potential effect on a word?s meaning and/or affect
(e.g., burrito is either a small donkey, burro, or a
type of Mexican food). Additionally, proper nouns
were excluded. Names of cities, regions, countries
and nationalities were marked and removed using
GeoWorldMap,4 a freely-available list of location
names from around the world. Names of people
were also filtered out. Proper names were manu-
ally inspected to avoid removing those with a lexical
meaning, a common phenomenon in Spanish (e.g.,
Victoria). Other manually removed words include
words in foreign languages (mainly in English), ro-
man numbers (e.g., XIX) and numbers in textual
form, such as seis (six), sexto (sixth), etc. Words
with one or two characters were removed automat-
ically, since we noticed that they practically always
corresponded to noise in the downloaded texts.
2.2 Counting ?word, word-class? pairs
We implemented a small refinement over Whissell?s
work, which consisted in considering ?word, word-
3http://nlp.lsi.upc.edu/freeling/
4http://www.geobytes.com/FreeServices.htm
class? pairs, rather than single words, since in Span-
ish the same lexical form may have different senses.
Thus, to each word (in its lemmatized form) we at-
tached one of four possible word classes ? noun,
verb, adjective or adverb. For example, bajoprep (un-
der) or bajonoun (bass guitar).
For each input word w, Freeling?s morphological
analysis returns a sequence of tuples ?lemma, POS-
tag, probability?, which correspond to the possible
lemmas and part-of-speech tags for w, together with
their prior probability. For example, the analysis
for the word bajo returns four tuples: ?bajo, SPS00
(i.e, preposition), 0.879?, ?bajo, AQ0MS0 (adjec-
tive), 0.077?, ?bajo, NCMS000 (noun), 0.040?,
and ?bajar, VMIP1S0 (verb), 0.004?. This means
that bajo, considered without context, has 87.9%
chances of being a noun, or 0.04% of being a verb.
Using this information, we computed the counts
of all ?word, word-class? pairs, taking into account
their prior probabilities. For example, assuming the
word bajo appeared 1000 times in the texts, it would
contribute with 1000?0.879 = 879 to the frequency
of bajoprep (i.e., bajo as a preposition), 77 to bajoadj,
40 to bajonoun, and 4 to bajarverb.
2.3 Merging Wikipedia and Los Cuentos
This process yielded 163,071 ?word, word-class?
pairs from the Wikipedia texts, and 30,544 from Los
Cuentos. To improve readability, hereafter we will
refer to ?word, word-class? pairs simply as words.
Figure 1 shows the frequency of each word count
in our two corpora. We note that both graphics are
practically identical, with a majority of low-count
words and a long tail with few high-count words.
To create our final word list to be rated by vol-
unteers, we needed to merge our two corpora from
Wikipedia and Los Cuentos. To accomplish this, we
Figure 1: Frequency of word counts in texts taken from
Wikipedia and Los Cuentos.
22
normalized all word counts for corpus size (normal-
ized count(w) = count(w) / corpus size), combined
both lists and sorted the resulting list by the normal-
ized word count (for the words that appeared in both
lists, we used its average count instead). The result-
ing list contained 175,413 words in total.
The top 10 words from Wikipedia were ma?sadv,
an?onoun, ciudadnoun, poblacio?nnoun, estadonoun, nom-
brenoun, veznoun, municipionoun, gruponoun and his-
torianoun (more, year, city, population, state, name,
time, as in ?first time?, municipality, group and his-
tory, respectively). The 10 words most common
from Los Cuentos were ma?sadv, veznoun, vidanoun,
d??anoun, tanadv, tiemponoun, ojonoun, manonoun,
amornoun and nochenoun (more, time, life, day, so,
time, eye, hand, love and night).
2.4 Assessing word coverage
Next we studied the coverage of the top k words
from our list on texts from a third corpus formed
by 3603 news stories downloaded from Wikinews in
Spanish in April, 2012.5 We chose news stories for
this task because we wanted a different genre for
studying the evolution of coverage.
Formally, let L be a word list, T any text, and
W (T ) the set of words occurring at least once in T .
We define the coverage of L on T as the percentage
of words in W (T ) that appear in L. Figure 2 shows
the evolution of the mean coverage on Wikinews ar-
ticles of the top k words from our word list. In
this figure we can observe that the mean coverage
grows rapidly, until it reaches a plateau at around
Figure 2: Mean coverage of the top k words from our list
on Wikinews articles.
5http://es.wikinews.org
80%. This suggests that even a low number of words
may achieve a relatively high coverage on new texts.
The 20% that remains uncovered, independently of
the size of the word list, may be explained by the
function words and proper names that were removed
from our word list. Note that news articles normally
contain many proper names, days, places and other
words that we intentionally discarded.
3 Word rating
After selecting the words, the next step consisted in
having them rated by a group of volunteers. For this
purpose we created a web interface, so that volun-
teers could complete this task remotely.
3.1 Web interface
On the first page of the web interface, volunteers
were asked to enter their month and year of birth,
their education level and their native language, and
was asked to complete a reCAPTCHA6 to avoid
bots. Subsequently, volunteers were taken to a page
with instructions for the rating task. They were
asked to rate each word along the three dimensions
shown in Table 1. These are the same three dimen-
Pleasantness Activation Imagery
1 Desagradable Pasivo Dif??cil de imaginar
(Unpleasant) (Passive) (Hard to imagine)
2 Ni agradable Ni activo Ni dif??cil ni fa?cil
ni desagradable ni pasivo de imaginar
(In between) (In between) (In between)
3 Agradable Activo Fa?cil de imaginar
(Pleasant) (Active) (Easy to imagine)
Table 1: Possible values for each of the three dimensions.
sions used in Whissell?s work. Importantly, these
concepts were not defined, to avoid biasing the judg-
ments. Volunteers were also encouraged to follow
their first impression, and told that there were no
?correct? answers. Appendix A shows the actual lo-
gin and instructions pages used in the study.
After reading the instructions, volunteers pro-
ceeded to judge two practice words, intended to help
them get used to the task and the interface, followed
by 20 target words. Words were presented one per
page. Figure 3 shows a screenshot of the page for
rating the word navegarverb. Note that the word class
6http://www.recaptcha.net
23
Figure 3: Screenshot of the web page for rating a word.
(verb in this example) is indicated right below the
word. After completing the first batch of 20 words,
volunteers were asked if they wanted to finish the
study or do a second batch, and then a third, a fourth,
and so on. This way, they were given the chance to
do as many words as they felt comfortable with. If
a volunteer left before completing a batch, his/her
ratings so far were also recorded.
3.2 Volunteers
662 volunteers participated in the study, with a mean
age of 33.3 (SD = 11.2). As to their level of educa-
tion, 76% had completed a university degree, 23%
had finished only secondary school, and 1% had
completed only primary school. Only volunteers
whose native language was Spanish were allowed
to participate in the study. Each volunteer was as-
signed 20 words following this procedure: (1) The
175,413 words in the corpus were sorted by word
count. (2) Words that had already received 5 or more
ratings were excluded. (3) Words that had already
been rated by a volunteer with the same month and
year of birth were excluded, to prevent the same vol-
unteer from rating twice the same word. (4) The top
20 words were selected.
Each volunteer rated 52.3 words on average (SD
= 34.0). Roughly 30% completed 20 words or
fewer; 24% completed 21-40 words; 18%, 41-60
words; and the remaining 28%, more than 60 words.
3.3 Descriptive statistics
A total of 2566 words were rated by at least 5 volun-
teers. Words with fewer annotations were excluded
from the study. We assigned each rating a numeric
value from 1 to 3, as shown in Table 1. Table 2
shows some basic statistics for each of the three di-
mensions.
Mean SD Skewness Kurtosis
Pleasantness 2.23 0.47 ?0.47 ?0.06
Activation 2.33 0.48 ?0.28 ?0.84
Imagery 2.55 0.42 ?0.90 0.18
Table 2: Descriptive statistics for the three dimensions.
The five most pleasant words, according to the
volunteers, were jugarverb, besonoun, sonrisanoun,
compan???anoun and reirverb (play, kiss, smile, com-
pany and laugh, respectively). The least pleas-
ant ones were asesinatonoun, caroadj, ahogarverb,
heridanoun and cigarronoun (murder, expensive,
drown, wound and cigar).
Among the most active words appear ideanoun,
publicarverb, violentoadj, sexualadj and talentonoun
(idea, publish, violent, sexual and talent). Among
the least active, we found yacerverb, espiritualadj,
quietoadj, esperarverb and cada?veradj (lay, spiritual,
still, wait and corpse).
The easiest to imagine include sucioadj, silen-
cionoun, darverb, peznoun and pensarverb (dirty, si-
lence, give, fish and think). Finally, the hardest
to imagine include consistirverb, constarverb, mor-
folog??anoun, piedadnoun and tendencianoun (consist,
consist, morphology, compassion and tendency).
We conducted Pearson?s correlation tests between
the different dimensions. Table 3 shows the correla-
tion matrix. Correlations among rating dimensions
were very weak, which supports the assumption that
pleasantness, activation and imagery are three inde-
pendent affective dimensions. These numbers are
very similar to the ones reported in Whissell?s work.
Pleasantness Activation Imagery
Pleasantness 1.00 0.14 0.10
Activation 0.14 1.00 0.11
Imagery 0.10 0.11 1.00
Table 3: Correlation between the different dimensions
Next, we computed Cohen?s ? to measure the de-
gree of agreement above chance between volunteers
(Cohen, 1968).7 Given that we used a three-point
scale for rating each affective dimension, we used
7This measure of agreement above chance is interpreted as
follows: 0 = None, 0 - 0.2 = Small, 0.2 - 0.4 = Fair, 0.4 - 0.6 =
Moderate, 0.6 - 0.8 = Substantial, 0.8 - 1 = Almost perfect.
24
a weighted version of ?, thus taking into account
the distance on that scale between disagreements.
For example, the distance between pleasant and un-
pleasant was 2, and the distance between pleasant
and in-between was 1. We obtained a weighted ?
measure of 0.42 for pleasantness, 0.30 for activation,
and 0.14 for imagery. Considering that these were
highly subjective rating tasks, the agreement lev-
els for pleasantness and activation were quite high.
The imagery task seemed somewhat more difficult,
although we still observed some agreement above
chance. These results indicate that our knowledge
base managed to, at least partially, capture informa-
tion regarding the three affective dimensions.
4 Evaluation
Next we proceeded to evaluate the usefulness of our
knowledge base. For this purpose, we developed a
simple system for estimating affect along our three
affective dimensions, and evaluated it on two differ-
ent sentiment-analysis tasks. The first task consisted
in a set of texts labeled by humans, and served to
compare the judgments of human labelers with the
predictions of our system. The second task consisted
in classifying a set of user product reviews into ?pos-
itive? or ?negative? opinions, a common application
for online stores.
4.1 Simple system for estimating affect
We created a simple computer program for automat-
ically estimating the degree of pleasantness, acti-
vation and imagery of an input text, based on the
knowledge base described in the previous sections.
For each word in the knowledge base, we cal-
culated its mean rating for each dimension. Sub-
sequently, for an input text T we used Freeling to
generate a full syntactic parsing, from which we ex-
tracted all ?word, word-class? pairs in T . The system
calculates the value for affective dimension d using
the following procedure:
score? 0
count? 0
for each word w in T (counting repetitions):
if w is included in KB:
score? score+KBd(w)
count? count+ 1
return score/count
where KB is our knowledge base, and KBd(w) is
the value for w in KB for dimension d.
For example, given the sentence ?Mi amiga espe-
raba terminar las pruebas a tiempo? (?My female-
friend was hoping to finish the tests on time?), and
assuming our knowledge base contains the numbers
shown in Table 4, the three values are computed as
follows. First, all words are lemmatized (i.e., mi
amigo esperar terminar el prueba a tiempo). Sec-
ond, the mean of each dimension is calculated with
the described procedure, yielding a pleasantness of
2.17, activation of 2.27 and imagery of 2.53.
word word-class mean P mean A mean I
amigo noun 3.0 2.4 3
esperar verb 1.2 1 2.8
poder verb 2.8 2.8 2.2
terminar verb 2.2 3 2.8
prueba noun 1.8 2.4 2.2
tiempo noun 2 2 2.2
mean: 2.17 2.27 2.53
Table 4: Knowledge base for the example text (P = pleas-
antness; A = activation; I = imagery).
It is important to mention that this system is just a
proof of concept, motivated by the need to evaluate
the effectiveness of our knowledge base. It could be
used as a baseline system against which to compare
more complex affect estimation systems. Also, if
results are good enough with such a simple system,
this would indicate that the information contained
in the knowledge base is useful, and in the future it
could help create more complex systems.
4.2 Evaluation #1: Emotion estimation
The first evaluation task consisted in comparing pre-
dictions made by our simple system against rat-
ings assigned by humans (our gold standard), on a
number of sentences and paragraphs extracted from
Wikipedia and Los Cuentos.
4.2.1 Gold standard
From each corpus we randomly selected 15 sen-
tences with 10 or more words, and 5 paragraphs with
at least 50 words and two sentences ? i.e. 30 sen-
tences and 10 paragraphs in total. These texts were
subsequently rated by 5 volunteers (2 male, 3 fe-
male), who were instructed to rate each entire text
(sentence or paragraph) for pleasantness, activation
25
and imagery using the same three-point scale shown
in Table 1. The weighted ?measure for these ratings
was 0.17 for pleasantness, 0.17 for activation and
0.22 for imagery. Consistent with the subjectivity
of these tasks, the degree of inter-labeler agreement
was rather low, yet still above chance level. Note
also that for pleasantness and activation the agree-
ment level was lower for texts than for individual
words, while the opposite was true for imagery.
4.2.2 Results
To evaluate the performance of our system, we
conducted Pearson?s correlation test for each affec-
tive dimension, in order to find the degree of cor-
relation between the system?s predictions for the 40
texts and their corresponding mean human ratings.
Table 5 shows the resulting ? coefficients.
System \ GS Pleasantness Activation Imagery
Pleasantness 0.59 * 0.15 * ?0.18 *
Activation 0.13 * 0.40 * 0.14 *
Imagery 0.16 0.19 0.07
Table 5: Correlations between gold standard and system?s
predictions. Statistically significant results are marked
with ?*? (t-tests, p < 0.05).
The coefficient for pleasantness presented a high
value at 0.59, which indicates that the system?s esti-
mation of pleasantness was rather similar to the rat-
ings given by humans. For activation the correlation
was weaker, although still significant. On the other
hand, for imagery this simple system did not seem
able to successfully emulate human judgments.
These results suggest that, at least for pleasant-
ness and activation, our knowledge base success-
fully captured useful information regarding how hu-
mans perceive those affective dimensions. For im-
agery, it is not clear whether the information base
did not capture useful information, or the estimation
system was too simplistic.
4.2.3 Effect of word count on performance
Next we studied the evolution of performance as
a function of the knowledge base size, aiming at as-
sessing the potential impact of increasing the num-
ber of words annotated by humans. Figure 4 sum-
marizes the results of a simulation, in which succes-
sive systems were built and evaluated using the top
250, 350, 450, ..., 2350, 2450 and 2566 words in our
knowledge base.
The green line (triangles) represents the mean
coverage of the system?s knowledge base on the gold
standard texts; the corresponding scale is shown on
the right axis. Similarly to Figure 2, the coverage
grew rapidly, starting at 18% when using 250 words
to 44% when using all 2566 words.
The blue (circles), red (squares) and purple (di-
amonds) lines correspond to the correlations of the
system?s predictions and the gold standard ratings
for pleasantness, activation and imagery, respec-
tively; the corresponding scale is shown on the left
axis. The black lines are a logarithmic function fit to
each of the three curves (?2 = 0.90, 0.72 and 0.68,
respectively).
Figure 4: Evolution of the correlation between system
predictions and Gold Standard, with respect to the knowl-
edge base size.
These results indicate that the system perfor-
mance (measured as the correlation with human
judgments) grew logarithmically with the number of
words in the knowledge base. Interestingly, the per-
formance grew at a slower pace than word cover-
age. In other words, an increase in the proportion
of words in a text that were known by the system
did not lead to a similar increase in the accuracy of
the predictions. An explanation may be that, once
an emotion had been established based on a percent-
age of words in the text, the addition of a few extra
words did not significantly change the outcome.
In consequence, if we wanted to do a substantial
improvement to our baseline system, it would prob-
ably not be a good idea to simply annotate more
26
words. Instead, it may be more effective to work
on how the system uses the information contained in
the knowledge base.
4.3 Evaluation #2: Classification of reviews
The second evaluation task consisted in using our
baseline system for classifying user product reviews
into positive or negative opinions.
4.3.1 Corpus
For this task we used a corpus of 400 user reviews
of products such as cars, hotels, dishwashers, books,
cellphones, music, computers and movies, extracted
from the Spanish website Ciao.es.8 This is the same
corpus used by Brooke (2009), who employed senti-
ment analysis tools in English to score Spanish texts
after performing machine translation.
On Ciao.es, users may enter their written reviews
and associate a numeric score to them, ranging from
1 to 5 stars. For this evaluation task, we made the
assumption that there was a strong relation between
the written reviews and their corresponding numeric
scores. Following this assumption, we tagged re-
views with 1 or 2 stars as ?negative? opinions, and
reviews with 4 or 5 stars as ?positive?. Reviews with
3 stars were considered neutral, and ignored.
4.3.2 Results
We used our system in a very simple way for pre-
dicting the polarity of opinions. First we computed
M , the mean pleasantness score on 80% of the re-
views. Subsequently, for each review in the remain-
ing 20%, if its pleasantness score was greater than
M , then it was classified as ?positive?; otherwise, it
was classified as ?negative?.
After repeating this procedure five times using
5-fold cross validation, the overall accuracy was
62.33%. Figure 5 shows the evolution of the sys-
tem?s accuracy with respect to the number of words
in the knowledge base. The green line (triangles)
represents the mean coverage of the system?s knowl-
edge base on user review texts; the corresponding
scale is shown on the right axis. The blue line (cir-
cles) corresponds to the classification accuracy; the
corresponding scale is shown on the left axis. The
black line is a logarithmic function fit to this curve
(?2 = 0.80).
8http://ciao.es
Figure 5: Evolution of the classification accuracy with
respect to the size of the knowledge base.
Notably, with as few as 500 words the accuracy
is already significantly above chance level, which is
50% for this task. This indicates that our knowl-
edge base managed to capture information on pleas-
antness that may aid the automatic classification of
positive and negative user reviews.
Also, similarly to our first evaluation task, we
observe that the accuracy increased as more words
were added to the knowledge base. However, it did
so at a logarithmic pace slower than the growth of
the word coverage on the user reviews. This sug-
gests that adding more words labeled by humans to
the knowledge base would only have a limited im-
pact on the performance of this simple system.
5 Conclusion
In this work we presented a knowledge base of Span-
ish words labeled by human volunteers for three
affective dimensions ? pleasantness, activation and
imagery, inspired by the English DAL created by
Whissell (1986; 1989). The annotations of these
three dimensions were weakly intercorrelated, indi-
cating a high level of independence of each other.
Additionally, the agreement between volunteers was
quite high, especially for pleasantness and activa-
tion, given the subjectivity of the labeling task.
To evaluate the usefulness of our lexicon, we built
a simple emotion prediction system. When used for
predicting the same three dimensions on new texts,
its output significantly correlated with human judg-
ments for pleasantness and activation, but the results
27
for imagery were not satisfactory. Also, when used
for classifying the opinion polarity of user product
reviews, the system managed to achieve an accuracy
better than random. These results suggest that our
knowledge base successfully captured useful infor-
mation of human perception of, at least, pleasant-
ness and activation. For imagery, either it failed to
capture any significant information, or the system
we created was too simple to exploit it accordingly.
Regarding the evolution of the system?s perfor-
mance as a function of the size of the lexicon, the
results were clear. When more words were included,
the system performance increased only at a loga-
rithmic pace. Thus, working on more complex sys-
tems seems to be more promising than adding more
human-annotated words.
In summary, this work presented a knowledge
base that may come handy to researchers and de-
velopers of sentiment analysis tools in Spanish. Ad-
ditionally, it may be useful for disciplines that need
to select emotionally balanced word stimuli, such as
Neuroscience or Psycholinguistics. In future work
we will compare the usefulness of our manually
annotated lexicon and cross-linguistic approaches
(Brooke et al, 2009; Pe?rez-Rosas et al, 2012).
Acknowledgments
This work was funded in part by ANPCYT PICT-2009-
0026 and CONICET. The authors thank Carlos ?Greg?
Diuk and Esteban Mocskos for valuable suggestions and
comments, and Julian Brooke, Milan Tofiloski and Maite
Taboada for kindly sharing the Ciao corpus.
References
J. Brooke, M. Tofiloski, and M. Taboada. 2009. Cross-
linguistic sentiment analysis: From English to Span-
ish. In International Conference on Recent Advances
in NLP, Borovets, Bulgaria, pages 50?54.
J. Cohen. 1968. Weighted kappa: Nominal scale
agreement provision for scaled disagreement or partial
credit. Psychological bulletin, 70(4):213.
R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis,
S. Kollias, W. Fellenz, and J.G. Taylor. 2001. Emo-
tion recognition in human-computer interaction. Sig-
nal Processing Magazine, IEEE, 18(1):32?80.
J.R. Gray, T.S. Braver, and M.E. Raichle. 2002. Integra-
tion of emotion and cognition in the lateral prefrontal
cortex. Proceedings of the National Academy of Sci-
ences, 99(6):4115.
L. Padro?, M. Collado, S. Reese, M. Lloberes, and
I. Castello?n. 2010. Freeling 2.1: Five years of open-
source language processing tools. In International
Conf. on Language Resources and Evaluation (LREC).
V. Pe?rez-Rosas, C. Banea, and R. Mihalcea. 2012.
Learning sentiment lexicons in spanish. In Int. Conf.
on Language Resources and Evaluation (LREC).
C. Whissell, M. Fournier, R. Pelland, D. Weir, and
K. Makarec. 1986. A dictionary of affect in language:
Iv. reliability, validity, and applications. Perceptual
and Motor Skills, 62(3):875?888.
Cynthia Whissell. 1989. The dictionary of affect in lan-
guage. Emotion: Theory, research, and experience,
4:113?131.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using NLP techniques. In 3rd IEEE Int.
Conf. on Data Mining, pages 427?434. IEEE.
A Login and instructions pages
Figures 6 and 7 show the screenshots of the login and
instructions pages of our web interface for rating words.
Figure 6: Screenshot of the login page.
Figure 7: Screenshot of the instructions page.
28
