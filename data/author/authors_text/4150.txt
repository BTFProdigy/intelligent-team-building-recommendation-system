Statistical Sentence Condensation using Ambiguity Packing and Stochastic
Disambiguation Methods for Lexical-Functional Grammar
Stefan Riezler and Tracy H. King and Richard Crouch and Annie Zaenen
Palo Alto Research Center, 3333 Coyote Hill Rd., Palo Alto, CA 94304
{riezler|thking|crouch|zaenen}@parc.com
Abstract
We present an application of ambiguity pack-
ing and stochastic disambiguation techniques
for Lexical-Functional Grammars (LFG) to the
domain of sentence condensation. Our system
incorporates a linguistic parser/generator for
LFG, a transfer component for parse reduc-
tion operating on packed parse forests, and a
maximum-entropy model for stochastic output
selection. Furthermore, we propose the use of
standard parser evaluation methods for auto-
matically evaluating the summarization qual-
ity of sentence condensation systems. An ex-
perimental evaluation of summarization qual-
ity shows a close correlation between the au-
tomatic parse-based evaluation and a manual
evaluation of generated strings. Overall sum-
marization quality of the proposed system is
state-of-the-art, with guaranteed grammatical-
ity of the system output due to the use of a
constraint-based parser/generator.
1 Introduction
Recent work in statistical text summarization has put for-
ward systems that do not merely extract and concate-
nate sentences, but learn how to generate new sentences
from ?Summary, Text? tuples. Depending on the cho-
sen task, such systems either generate single-sentence
?headlines? for multi-sentence text (Witbrock and Mittal,
1999), or they provide a sentence condensation module
designed for combination with sentence extraction sys-
tems (Knight and Marcu, 2000; Jing, 2000). The chal-
lenge for such systems is to guarantee the grammatical-
ity and summarization quality of the system output, i.e.
the generated sentences need to be syntactically well-
formed and need to retain the most salient information of
the original document. For example a sentence extraction
system might choose a sentence like:
The UNIX operating system, with implementations
from Apples to Crays, appears to have the advan-
tage.
from a document, which could be condensed as:
UNIX appears to have the advantage.
In the approach of Witbrock and Mittal (1999), selec-
tion and ordering of summary terms is based on bag-
of-words models and n-grams. Such models may well
produce summaries that are indicative of the original?s
content; however, n-gram models seem to be insufficient
to guarantee grammatical well-formedness of the system
output. To overcome this problem, linguistic parsing and
generation systems are used in the sentence condensation
approaches of Knight and Marcu (2000) and Jing (2000).
In these approaches, decisions about which material to in-
clude/delete in the sentence summaries do not rely on rel-
ative frequency information on words, but rather on prob-
ability models of subtree deletions that are learned from
a corpus of parses for sentences and their summaries.
A related area where linguistic parsing systems
have been applied successfully is sentence simplifica-
tion. Grefenstette (1998) presented a sentence reduction
method that is based on finite-state technology for lin-
guistic markup and selection, and Carroll et al (1998)
present a sentence simplification system based on linguis-
tic parsing. However, these approaches do not employ
statistical learning techniques to disambiguate simplifi-
cation decisions, but iteratively apply symbolic reduction
rules, producing a single output for each sentence.
The goal of our approach is to apply the fine-grained
tools for stochastic Lexical-Functional Grammar (LFG)
parsing to the task of sentence condensation. The system
presented in this paper is conceptualized as a tool that can
be used as a standalone system for sentence condensation
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 118-125
                                                         Proceedings of HLT-NAACL 2003
or simplification, or in combination with sentence extrac-
tion for text-summarization beyond the sentence-level. In
our system, to produce a condensed version of a sen-
tence, the sentence is first parsed using a broad-coverage
LFG grammar for English. The parser produces a set of
functional (f )-structures for an ambiguous sentence in a
packed format. It presents these to the transfer compo-
nent in a single packed data structure that represents in
one place the substructures shared by several different in-
terpretations. The transfer component operates on these
packed representations and modifies the parser output to
produce reduced f -structures. The reduced f -structures
are then filtered by the generator to determine syntac-
tic well-formedness. A stochastic disambiguator using a
maximum entropy model is trained on parsed and manu-
ally disambiguated f -structures for pairs of sentences and
their condensations. Using the disambiguator, the string
generated from the most probable reduced f -structure
produced by the transfer system is chosen. In contrast
to the approaches mentioned above, our system guaran-
tees the grammaticality of generated strings through the
use of a constraint-based generator for LFG which uses
a slightly tighter version of the grammar than is used by
the parser. As shown in an experimental evaluation, sum-
marization quality of our system is high, due to the com-
bination of linguistically fine-grained analysis tools and
expressive stochastic disambiguation models.
A second goal of our approach is to apply the standard
evaluation methods for parsing to an automatic evaluation
of summarization quality for sentence condensation sys-
tems. Instead of deploying costly and non-reusable hu-
man evaluation, or using automatic evaluation methods
based on word error rate or n-gram match, summariza-
tion quality can be evaluated directly and automatically
by matching the reduced f -structures that were produced
by the system against manually selected f -structures that
were produced by parsing a set of manually created con-
densations. Such an evaluation only requires human labor
for the construction and manual structural disambigua-
tion of a reusable gold standard test set. Matching against
the test set can be done automatically and rapidly, and
is repeatable for development purposes and system com-
parison. As shown in an experimental evaluation, a close
correspondence can be established for rankings produced
by the f -structure based automatic evaluation and a man-
ual evaluation of generated strings.
2 Statistical Sentence Condensation in the
LFG Framework
In this section, each of the system components will be
described in more detail.
2.1 Parsing and Transfer
In this project, a broad-coverage LFG gram-
mar and parser for English was employed (see
Riezler et al (2002)). The parser produces a set of
context-free constituent (c-)structures and associated
functional (f -)structures for each input sentence, repre-
sented in packed form (see Maxwell and Kaplan (1989)).
For sentence condensation we are only interested in the
predicate-argument structures encoded in f -structures.
For example, Fig. 1 shows an f -structure manually
selected out of the 40 f -structures for the sentence:
A prototype is ready for testing, and Leary hopes to
set requirements for a full system by the end of the
year.
The transfer component for the sentence condensation
system is based on a component previously used in a ma-
chine translation system (see Frank (1999)). It consists
of an ordered set of rules that rewrite one f -structure
into another. Structures are broken down into flat lists
of facts, and rules may add, delete, or change individ-
ual facts. Rules may be optional or obligatory. In the case
of optional rules, transfer of a single input structure may
lead to multiple alternate output structures. The transfer
component is designed to operate on packed input from
the parser and can also produce packed representations
of the condensation alternatives, using methods adapted
from parse packing.1
An example rule that (optionally) removes an adjunct
is shown below:
+adjunct(X,Y), in-set(Z,Y) ?=>
delete-node(Z,r1), rule-trace(r1,del(Z,X)).
This rule eliminates an adjunct, Z, by deleting the fact that
Z is contained within the set of adjuncts, Y, associated
with the expression X. The + before the adjunct(X,Y)
fact marks this fact as one that needs to be present for the
rule to be applied, but which is left unaltered by the rule
application. The in-set(Z,Y) fact is deleted. Two
new facts are added. delete-node(Z,r1) indicates
that the structure rooted at node Z is to be deleted, and
rule-trace(r1,del(Z,X)) adds a trace of this
rule to an accumulating history of rule applications. This
history records the relation of transferred f -structures to
the original f -structure and is available for stochastic dis-
ambiguation.
Rules used in the sentence condensation transfer sys-
tem include the optional deletion of all intersective ad-
juncts (e.g., He slept in the bed. can become He slept.,
but He did not sleep. cannot become He did sleep. or He
1The packing feature of the transfer component could not
be employed in these experiments since the current interface
to the generator and stochastic disambiguation component still
requires unpacked representations.
"A prototype is ready for testing , and Leary hopes to set requirements for a full system by the end of the year."
?be<[93:ready]>[30:prototype]?PRED
?prototype?PRED
countGRAINNTYPE
?a?PREDDET?FORM a, DET?TYPE indefDETSPEC
CASE nom, NUM sg, PERS 330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJADEGREE positive, ATYPE predicative93XCOMP
?for<[141:test]>?PRED
?test?PRED
gerundGRAINNTYPE
CASE acc, NUM sg, PERS 3, PFORM for, VTYPE main141
OBJ
ADV?TYPE vpadv, PSEM unspecified, PTYPE sem125
ADJUNCT
MOOD indicative, PERF ?_, PROG ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular[252:hope]>s73
?hope<[235:Leary], [280:set]>?PRED
?Leary?PRED
properGRAIN
namePROPERNSEMNTYPE
ANIM +, CASE nom, NUM sg, PERS 3235
SUBJ
?set<[235:Leary], [336:requirement], [355:for]>?PRED [235:Leary]SUBJ
?requirement?PRED
unspecifiedGRAINNTYPE
CASE acc, NUM pl, PERS 3336
OBJ
?for<[391:system]>?PRED
?system?PRED
?full?PREDADEGREE positive, ADJUNCT?TYPE nominal, ATYPE attributive398ADJUNCT
unspecifiedGRAINNTYPE
?a?PREDDET?FORM a, DET?TYPE indefDETSPEC
CASE acc, NUM sg, PERS 3, PFORM for391
OBJ
PSEM unspecified, PTYPE sem355
OBL
?by<[469:end]>?PRED
?end?PRED
?of<[519:year]>?PRED
?year?PRED
countGRAINNTYPE
?the?PREDDET?FORM the, DET?TYPE defDETSPEC
CASE acc, NUM sg, PERS 3, PFORM of519
OBJ
ADJUNCT?TYPE nominal, PSEM unspecified, PTYPE sem512
ADJUNCT
countGRAINNTYPE
?the?PREDDET?FORM the, DET?TYPE defDETSPEC
CASE acc, NUM sg, PERS 3, PFORM by469
OBJ
ADV?TYPE vpadv, PSEM unspecified, PTYPE sem451
ADJUNCT
PERF ?_, PROG ?_TNS?ASP
INF?FORM to, PASSIVE ?, VTYPE main280
XCOMP
MOOD indicative, PERF ?_, PROG ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE main252
COORD +_, COORD?FORM and, COORD?LEVEL ROOT197
Figure 1: F -structure for non-condensed sentence.
slept.), the optional deletion of parts of coordinate struc-
tures (e.g., They laughed and giggled. can become They
giggled.), and certain simplifications (e.g. It is clear that
the earth is round. can become The earth is round. but
It seems that he is asleep. cannot become He is asleep.).
For example, one possible post-transfer output of the sen-
tence in Fig. 1 is shown in Fig. 2.
2.2 Stochastic Selection and Generation
The transfer rules are independent of the grammar and are
not constrained to preserve the grammaticality or well-
formedness of the reduced f-structures. Some of the re-
duced structures therefore may not correspond to any En-
glish sentence, and these are eliminated from future con-
sideration by using the generator as a filter. The filter-
ing is done by running each transferred structure through
the generator to see whether it produces an output string.
If it does not, the structure is rejected. For example, for
the f -structure in Fig. 1, the transfer system proposed
32 possible reductions. After filtering these structures by
generation, 16 reduced f -structures comprising possible
"A prototype is ready for testing."
?be  <[93:ready]>[30:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PREDDET?FORM  a, DET?TYPE  indefDETSPEC
CASE nom, NUM sg, PERS 330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJADEGREE  positive , ATYPE  predicative93XCOMP
?for<[141:test]>?PRED
?test ?PRED
gerundGRAINNTYPE
CASE acc, NUM sg, PERS 3, PFORM for, VTYPE main141
OBJ
ADV?TYPE  vpadv , PSEM  unspecified , PTYPE  sem125
ADJUNCT
MOOD	  indicative, PERF  ?_, PROG  ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular73
Figure 2: Gold standard f -structure reduction.
condensations of the input sentence survive. The 16 well-
formed structures correspond to the following strings that
were outputted by the generator (note that a single struc-
ture may correspond to more than one string and a given
string may correspond to more than one structure):
A prototype is ready.
A prototype is ready for testing.
Leary hopes to set requirements for a full system.
A prototype is ready and Leary hopes to set require-
ments for a full system.
A prototype is ready for testing and Leary hopes to
set requirements for a full system.
Leary hopes to set requirements for a full system by
the end of the year.
A prototype is ready and Leary hopes to set require-
ments for a full system by the end of the year.
A prototype is ready for testing and Leary hopes to
set requirements for a full system by the end of the
year.
In order to guarantee non-empty output for the over-
all condensation system, the generation component has
to be fault-tolerant in cases where the transfer system op-
erates on a fragmentary parse, or produces non-valid f -
structures from valid input f -structures. Robustness tech-
niques currently applied to the generator include insertion
and deletion of features in order to match invalid transfer-
output to the grammar rules and lexicon. Furthermore,
repair mechanisms such as repairing subject-verb agree-
ment from the subject?s number value are employed. As
a last resort, a fall-back mechanism to the original un-
condensed f -structure is used. These techniques guaran-
tee that a non-empty set of reduced f -structures yielding
grammatical strings in generation is passed on to the next
system component. In case of fragmentary input to the
transfer component, grammaticaliy of the output is guar-
anteed for the separate fragments. In other words, strings
generated from a reduced fragmentary f -structure will be
as grammatical as the string that was fed into the parsing
component.
After filtering by the generator, the remaining f -
structures were weighted by the stochastic disambigua-
tion component. Similar to stochastic disambiguation for
constraint-based parsing (Johnson et al, 1999; Riezler et
al., 2002), an exponential (a.k.a. log-linear or maximum-
entropy) probability model on transferred structures is es-
timated from a set of training data. The data for estima-
tion consists of pairs of original sentences y and gold-
standard summarized f -structures s which were manu-
ally selected from the transfer output for each sentence.
For training data {(sj , yj)}mj=1 and a set of possible sum-
marized structures S(y) for each sentence y, the objective
was to maximize a discriminative criterion, namely the
conditional likelihood L(?) of a summarized f -structure
given the sentence. Optimization of the function shown
below was performed using a conjugate gradient opti-
mization routine:
L(?) = log
m?
j=1
e??f(sj)
?
s?S(yj)
e??f(s)
.
At the core of the exponential probability model is a vec-
tor of property-functions f to be weighted by parameters
?. For the application of sentence condensation, 13,000
property-functions of roughly three categories were used:
? Property-functions indicating attributes, attribute-
combinations, or attribute-value pairs for f -structure
attributes (? 1,000 properties)
? Property-functions indicating co-occurences of verb
stems and subcategorization frames (? 12,000 prop-
erties)
? Property-functions indicating transfer rules used to
arrive at the reduced f - structures (? 60 properties).
A trained probability model is applied to unseen data
by selecting the most probable transferred f -structure,
yielding the string generated from the selected struc-
ture as the target condensation. The transfered f -structure
chosen for our current example is shown in Fig. 3.
"A prototype is ready."
?be  <[93:ready]>[30:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PRED
DET?FORM a, DET?TYPE indefDETSPEC
CASE nom, NUM  sg, PERS  330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJ
ADEGREE positive , ATYPE predicative93XCOMP

MOOD indicative, PERF ?_, PROG ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular73
Figure 3: Transferred f -structure chosen by system.
This structure was produced by the following set of
transfer rules, where var refers to the indices in the rep-
resentation of the f -structure:
rtrace(r13,keep(var(98),of)),
rtrace(r161,keep(system,var(85))),
rtrace(r1,del(var(91),set,by)),
rtrace(r1,del(var(53),be,for)),
rtrace(r20,equal(var(1),and)),
rtrace(r20,equal(var(2),and)),
rtrace(r2,del(var(1),hope,and)),
rtrace(r22,delb(var(0),and)).
These rules delete the adjunct of the first conjunct (for
testing), the adjunct of the second conjunct (by the end
of the year), the rest of the second conjunct (Leary hopes
to set requirements for a full system), and the conjunction
itself (and).
3 A Method for Automatic Evaluation of
Sentence Summarization
Evaluation of quality of sentence condensation systems,
and of text summarization and simplification systems in
general, has mostly been conducted as intrinsic evalua-
tion by human experts. Recently, Papineni et al?s (2001)
proposal for an automatic evaluation of translation sys-
tems by measuring n-gram matches of the system out-
put against reference examples has become popular for
evaluation of summarization systems. In addition, an au-
tomatic evaluation method based on context-free deletion
decisions has been proposed by Jing (2000). However, for
summarization systems that employ a linguistic parser as
an integral system component, it is possible to employ
the standard evaluation techniques for parsing directly
to an evaluation of summarization quality. A parsing-
based evaluation allows us to measure the semantic as-
pects of summarization quality in terms of grammatical-
functional information provided by deep parsers. Further-
more, human expertise was necessary only for the cre-
ation of condensed versions of sentences, and for the
manual disambiguation of parses assigned to those sen-
tences. Given such a gold standard, summarization qual-
ity of a system can be evaluated automatically and re-
peatedly by matching the structures of the system out-
put against the gold standard structures. The standard
metrics of precision, recall, and F-score from statisti-
cal parsing can be used as evaluation metrics for mea-
suring matching quality: Precision measures the number
of matching structural items in the parses of the sys-
tem output and the gold standard, out of all structural
items in the system output?s parse; recall measures the
number of matches, out of all items in the gold stan-
dard?s parse. F-score balances precision and recall as
(2 ? precision ? recall)/(precision + recall).
For the sentence condensation system presented above,
the structural items to be matched consist of rela-
tion(predicate, argument) triples. For example, the gold-
standard f -structure of Fig. 2 corresponds to 23 depen-
dency relations, the first 14 of which are shared with the
reduced f -structure chosen by the stochastic disambigua-
tion system:
tense(be:0, pres),
mood(be:0, indicative),
subj(be:0, prototype:2),
xcomp(be:0, ready:1),
stmt_type(be:0, declarative),
vtype(be:0, copular),
subj(ready:1, prototype:2),
adegree(ready:1, positive),
atype(ready:1, predicative),
det(prototype:2, a:7),
num(prototype:2, sg),
pers(prototype:2, 3),
det_form(a:7, a),
det_type(a:7, indef),
adjunct(be:0, for:12),
obj(for:12, test:14),
adv_type(for:12, vpadv),
psem(for:12, unspecified),
ptype(for:12, semantic),
num(test:14, sg),
pers(test:14, 3),
pform(test:14, for),
vtype(test:14, main).
Matching these f -structures against each other corre-
sponds to a precision of 1, recall of .61, and F-score of
.76.
The fact that our method does not rely on a compar-
ison of the characteristics of surface strings is a clear
advantage. Such comparisons are bad at handling exam-
ples which are similar in meaning but differ in word or-
der or vary structurally, such as in passivization or nom-
inalization. Our method handles such examples straight-
forwardly. Fig. 4 shows two serialization variants of the
condensed sentence of Fig. 2. The f -structures for these
examples are similar to the f -structure assigned to the
gold standard condensation shown in Fig. 2 (except for
the relations ADJUNT-TYPE:parenthetical ver-
sus ADV-TYPE:vpadv versus ADV-TYPE:sadv). An
evaluation of summarization quality that is based on
matching f -structures will treat these examples equally,
whereas an evaluation based on string matching will yield
different quality scores for different serializations.
"A prototype, for testing, is ready."
?be  <[221:ready]>[30:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PREDDET?FORM  a, DET?TYPE  indefDETSPEC
CASE nom, NUM sg, PERS 330
SUBJ
?ready<[30:prototype]>?PRED [30:prototype]SUBJADEGREE  positive , ATYPE  predicative221XCOMP
?for<[117:test]>?PRED
?test ?PRED
gerundGRAINNTYPECASE acc, NUM sg, PERS 3, PFORM for, VTYPE main117OBJADJUNCT?TYPE  parenthetical , PSEM  unspecified , PTYPE  sem73
ADJUNCT
MOOD  indicative, PERF  ?_, PROG  ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular201
"For testing, a prototype is ready."
?be  <[177:ready]>[131:prototype]?PRED
?prototype ?PRED
countGRAINNTYPE
?a?PREDDET?FORM  a, DET?TYPE  indefDETSPEC
CASE nom, NUM sg, PERS 3131
SUBJ
?ready<[131:prototype]>?PRED [131:prototype]SUBJADEGREE  positive , ATYPE  predicative177XCOMP
?for<[27:test]>?PRED
?test ?PRED
gerundGRAINNTYPECASE acc, NUM sg, PERS 3, PFORM for, VTYPE main27OBJADV?TYPE  sadv, PSEM  unspecified , PTYPE  sem11
ADJUNCT
MOOD  indicative, PERF  ?_, PROG  ?_, TENSE presTNS?ASP
PASSIVE ?, STMT?TYPE decl, VTYPE copular83
Figure 4: F -structure for word-order variants of gold
standard condensation.
In the next section, we present experimental results
of an automatic evaluation of the sentence condensation
system described above. These results show a close cor-
respondence between automatically produced evaluation
results and human judgments on the quality of generated
condensed strings.
4 Experimental Evaluation
The sentences and condensations we used are taken from
data for the experiments of Knight and Marcu (2000),
which were provided to us by Daniel Marcu. These data
consist of pairs of sentences and their condensed versions
that have been extracted from computer-news articles and
abstracts of the Ziff-Davis corpus. Out of these data, we
parsed and manually disambiguated 500 sentence pairs.
These included a set of 32 sentence pairs that were used
for testing purposes in Knight and Marcu (2000). In or-
der to control for the small corpus size of this test set, we
randomly extracted an additional 32 sentence pairs from
the 500 parsed and disambiguated examples as a second
test set. The rest of the 436 randomly selected sentence
pairs were used to create training data. For the purpose
of discriminative training, a gold-standard of transferred
f -structures was created from the transfer output and the
manually selected f -structures for the condensed strings.
This was done automatically by selecting for each exam-
ple the transferred f -structure that best matched the f -
structure annotated for the condensed string.
In the automatic evaluation of f -structure match, three
different system variants were compared. Firstly, ran-
domly chosen transferred f -structures were matched
against the manually selected f -structures for the man-
ually created condensations. This evaluation constitutes
a lower bound on the F-score against the given gold
standard. Secondly, matching results for transferred f -
structures yielding the maximal F-score against the gold
standard were recorded, giving an upper bound for the
system. Thirdly, the performance of the stochastic model
within the range of the lower bound and upper bound was
measured by recording the F-score for the f -structure that
received highest probability according to the learned dis-
tribution on transferred structures.
In order to make our results comparable to the re-
sults of Knight and Marcu (2000) and also to investigate
the correspondence between the automatic evaluation and
human judgments, a manual evaluation of the strings gen-
erated by these system variants was conducted. Two hu-
man judges were presented with the uncondensed sur-
face string and five condensed strings that were displayed
in random order for each test example. The five con-
densed strings presented to the human judges contained
(1) strings generated from three randomly selected f -
structures, (2) the strings generated from the f -structures
which were selected by the stochastic model, and (3) the
manually created gold-standard condensations extracted
from the Ziff-Davis abstracts. The judges were asked
to judge summarization quality on a scale of increasing
quality from 1 to 5 by assessing how well the generated
strings retained the most salient information of the orig-
inal uncondensed sentences. Grammaticality of the sys-
tem output is optimal and not reported separately. Results
for both evaluations are reported for two test corpora of
32 examples each. Testset I contains the sentences and
condensations used to evaluate the system described in
Knight and Marcu (2000). Testset II consists of another
randomly extracted 32 sentence pairs from the same do-
main, prepared in the same way.
Fig. 5 shows evaluation results for a sentence conden-
sation run that uses manually selected f -structures for
the original sentences as input to the transfer component.
These results demonstrate how the condenstation system
performs under the optimal circumstances when the parse
chosen as input is the best available. Fig. 6 applies the
same evaluation data and metrics to a sentence conden-
sation experiment that performs transfer from packed f -
structures, i.e. transfer is performed on all parses for an
ambiguous sentence instead of on a single manually se-
lected parse. Alternatively, a single input parse could be
selected by stochastic models such as the one described
in Riezler et al (2002). A separate phase of parse disam-
biguation, and perhaps the effects of any errors that this
might introduce, can be avoided by transferring from all
parses for an ambiguous sentence. This approach is com-
putationally feasible, however, only if condensation can
be carried all the way through without unpacking. Our
technology is not yet able to do this (in particular, as men-
tioned earlier, we have not yet implemented a method for
stochastic disambiguation on packed f -structures). How-
ever, we conducted a preliminary assessment of this pos-
sibility by unpacking and enumerating the transferred f -
structures. For many sentences this resulted in more can-
didates than we could operate on in the available time
and space, and in those cases we arbitrarily set a cut-off
on the number of transferred f -structures we considered.
Since transferred f -structures are produced according to
the number of rules applied to transfer them, in this setup
the transfer system produces smaller f -structures first,
and cuts off less condensed output. The result of this ex-
periment, shown in Fig. 6, thus provides a conservative
estimate on the quality of the condensations we might
achieve with a full-packing implementation.
In Figs. 5 and 6, the first row shows F-scores for a
random selection, the system selection, and the best pos-
sible selection from the transfer output against the gold
standard. The second rows show summarization quality
scores for generations from a random selection and the
system selection, and for the human-written condensa-
tion. The third rows report compression ratios. As can
testset I lowerbound
system
selection
upper
bound
F-score 58% 67.3% 77.2 %
sum-quality 2.0 3.5 4.4
compr. 50.2% 60.4% 54.9%
testset II lowerbound
system
selection
upper
bound
F-score 59% 65.4% 83.3%
sum-quality 2.1 3.4 4.6
compr. 52.7% 65.9% 56.8%
Figure 5: Sentence condensation from manually selected
f -structure for original uncondensed sentences.
be seen from these tables, the ranking of system variants
produced by the automatic and manual evaluation con-
firm a close correlation between the automatic evaluation
and human judgments. A comparison of evaluation re-
sults across colums, i.e. across selection variants, shows
that a stochastic selection of transferred f -structures is
indeed important. Even if all f -structures are transferred
from the same linguistically rich source, and all gener-
ated strings are grammatical, a reduction in error rate of
around 50% relative to the upper bound can be achieved
by stochastic selection. In contrast, a comparison be-
tween transfer runs with and without perfect disambigua-
tion of the original string shows a decrease of about 5% in
F-score, and of only .1 points for summarization quality
when transferring from packed parses instead of from the
manually selected parse. This shows that it is more im-
portant to learn what a good transferred f -structure looks
like than to have a perfect f -structure to transfer from.
The compression rates associated with the systems that
used stochastic selection is around 60%, which is accept-
able, but not as aggressive as human-written condensa-
tions. Note that in our current implementation, in some
cases the transfer component was unable to operate on
the packed representation. In those cases a parse was cho-
sen at random as a conservative estimate of transfer from
all parses. This fall-back mechanism explains the drop in
F-score for the upper bound in comparing Figs. 5 and 6.
5 Conclusion
We presented an approach to sentence condensation
that employs linguistically rich LFG grammars in a
parsing/generation-based stochastic sentence condensa-
tion system. Fine-grained dependency structures are out-
put by the parser, then modified by a highly expressive
transfer system, and filtered by a constraint-based gener-
ator. Stochastic selection of generation-filtered reduced
structures uses a powerful Maximum-Entropy model.
As shown in an experimental evaluation, summarization
testset I lowerbound
system
selection
upper
bound
F-score 55.2% 63.0% 72.0%
sum-quality 2.1 3.4 4.4
compres. 46.5% 61.6% 54.9%
testset II lowerbound
system
selection
upper
bound
F-score 54% 59.7% 76.0 %
sum-quality 1.9 3.3 4.6
compres. 50.9% 60.0% 56.8%
Figure 6: Sentence condensation from packed f -
structures for original uncondensed sentences.
quality of the system output is state-of-the-art, and gram-
maticality of condensed strings is guaranteed. Robustness
techniques for parsing and generation guarantee that the
system produces non-empty output for unseen input.
Overall, the summarization quality achieved by
our system is similar to the results reported in
Knight and Marcu (2000). This might seem disappoint-
ing considering the more complex machinery employed
in our approach. It has to be noted that these re-
sults are partially due to the somewhat artificial na-
ture of the data that were used in the experiments of
Knight and Marcu (2000) and therefore in our experi-
ments: The human-written condensations in the data set
extracted from the Ziff-Davis corpus show the same
word order as the original sentences and do not exhibit
any structural modification that are common in human-
written summaries. For example, humans tend to make
use of structural modifications such as nominalization
and verb alternations such as active/passive or transi-
tive/intransitive alternations in condensation. Such alter-
nations can easily be expressed in our transfer-based
approach, whereas they impose severe problems to ap-
proaches that operate only on phrase structure trees. In
the given test set, however, the condensation task re-
stricted to the operation of deletion. A creation of addi-
tional condensations for the original sentences other than
the condensed versions extracted from the human-written
abstracts would provide a more diverse test set, and fur-
thermore make it possible to match each system output
against any number of independent human-written con-
densations of the same original sentence. This idea of
computing matching scores to multiple reference exam-
ples was proposed by Alshawi et al (1998), and later by
Papineni et al (2001) for evaluation of machine transla-
tion systems. Similar to these proposals, an evaluation
of condensation quality could consider multiple reference
condensations and record the matching score against the
most similar example.
Another desideratum for future work is to carry
condensation all the way through without unpacking
at any stage. Work on employing packing techniques
not only for parsing and transfer, but also for genera-
tion and stochastic selection is currently underway (see
Geman and Johnson (2002)). This will eventually lead to
a system whose components work on packed represen-
tations of all or n-best solutions, but completely avoid
costly unpacking of representations.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
1998. Automatic acquisition of hierarchical trans-
duction models for machine translation. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics (ACL?98), Montreal, Que-
bec, Canada.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
Proceedings of the AAAI Workshop on Integrating Arti-
ficial Intelligence and Assistive Technology, Madison,
WI.
Anette Frank. 1999. From parallel grammar develop-
ment towards machine translation. In Proceedings of
the MT Summit VII. MT in the Great Translation Era,
pages 134?142. Kent Ridge Digital Labs, Singapore.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochas-
tic unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?02), Philadelphia, PA.
Gregory Grefenstette. 1998. Producing intelligent tele-
graphic text reduction to provide an audio scanning
service for the blind. In Proceedings of the AAAI
Spring Workshop on Intelligent Text Summarization,
Stanford, CA.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP?00),
Seattle, WA.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?99), College Park, MD.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization?step one: Sentence compression. In
Proceedings of the 17th National Conference on Arti-
ficial Intelligence (AAAI-2000), Austin, TX.
John Maxwell and Ronald M. Kaplan. 1989. An
overview of disjunctive constraint satisfaction. In Pro-
ceedings of the International Workshop on Parsing
Technologies, Pittsburgh, PA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. Technical Report IBM Re-
search Division Technical Report, RC22176 (W0190-
022), Yorktown Heights, N.Y.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization: A statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
Berkeley, CA.
Speed and Accuracy in Shallow and Deep Stochastic Parsing
Ronald M. Kaplan , Stefan Riezler , Tracy Holloway King
John T. Maxwell III, Alexander Vasserman and Richard Crouch
Palo Alto Research Center, 3333 Coyote Hill Rd., Palo Alto, CA 94304
{kaplan|riezler|king|maxwell|avasserm|crouch}@parc.com
Abstract
This paper reports some experiments that com-
pare the accuracy and performance of two
stochastic parsing systems. The currently pop-
ular Collins parser is a shallow parser whose
output contains more detailed semantically-
relevant information than other such parsers.
The XLE parser is a deep-parsing system that
couples a Lexical Functional Grammar to a log-
linear disambiguation component and provides
much richer representations theory. We mea-
sured the accuracy of both systems against a
gold standard of the PARC 700 dependency
bank, and also measured their processing times.
We found the deep-parsing system to be more
accurate than the Collins parser with only a
slight reduction in parsing speed.1
1 Introduction
In applications that are sensitive to the meanings ex-
pressed by natural language sentences, it has become
common in recent years simply to incorporate publicly
available statistical parsers. A state-of-the-art statistical
parsing system that enjoys great popularity in research
systems is the parser described in Collins (1999) (hence-
forth ?the Collins parser?). This system not only is fre-
quently used for off-line data preprocessing, but also
is included as a black-box component for applications
such as document summarization (Daume and Marcu,
2002), information extraction (Miller et al, 2000), ma-
chine translation (Yamada and Knight, 2001), and ques-
tion answering (Harabagiu et al, 2001). This is be-
1This research has been funded in part by contract #
MDA904-03-C-0404 awarded from the Advanced Research and
Development Activity, Novel Intelligence from Massive Data
program. We would like to thank Chris Culy whose original ex-
periments inspired this research.
cause the Collins parser shares the property of robustness
with other statistical parsers, but more than other such
parsers, the categories of its parse-trees make grammati-
cal distinctions that presumably are useful for meaning-
sensitive applications. For example, the categories of
the Model 3 Collins parser distinguish between heads,
arguments, and adjuncts and they mark some long-
distance dependency paths; these distinctions can guide
application-specific postprocessors in extracting impor-
tant semantic relations.
In contrast, state-of-the-art parsing systems based on
deep grammars mark explicitly and in much more de-
tail a wider variety of syntactic and semantic dependen-
cies and should therefore provide even better support for
meaning-sensitive applications. But common wisdom has
it that parsing systems based on deep linguistic grammars
are too difficult to produce, lack coverage and robustness,
and also have poor run-time performance. The Collins
parser is thought to be accurate and fast and thus to repre-
sent a reasonable trade-off between ?good-enough? out-
put, speed, and robustness.
This paper reports on some experiments that put this
conventional wisdom to an empirical test. We investi-
gated the accuracy of recovering semantically-relevant
grammatical dependencies from the tree-structures pro-
duced by the Collins parser, comparing these dependen-
cies to gold-standard dependencies which are available
for a subset of 700 sentences randomly drawn from sec-
tion 23 of the Wall Street Journal (see King et al (2003)).
We compared the output of the XLE system, a
deep-grammar-based parsing system using the English
Lexical-Functional Grammar previously constructed as
part of the Pargram project (Butt et al, 2002), to the
same gold standard. This system incorporates sophisti-
cated ambiguity-management technology so that all pos-
sible syntactic analyses of a sentence are computed in
an efficient, packed representation (Maxwell and Ka-
plan, 1993). In accordance with LFG theory, the output
includes not only standard context-free phrase-structure
trees but also attribute-value matrices (LFG?s f(unctional)
structures) that explicitly encode predicate-argument re-
lations and other meaningful properties. XLE selects the
most probable analysis from the potentially large candi-
date set by means of a stochastic disambiguation com-
ponent based on a log-linear (a.k.a. maximum-entropy)
probability model (Riezler et al, 2002). The stochas-
tic component is also ?ambiguity-enabled? in the sense
that the computations for statistical estimation and selec-
tion of the most probable analyses are done efficiently
by dynamic programming, avoiding the need to unpack
the parse forests and enumerate individual analyses. The
underlying parsing system also has built-in robustness
mechanisms that allow it to parse strings that are outside
the scope of the grammar as a shortest sequence of well-
formed ?fragments?. Furthermore, performance parame-
ters that bound parsing and disambiguation work can be
tuned for efficient but accurate operation.
As part of our assessment, we also measured the pars-
ing speed of the two systems, taking into account all
stages of processing that each system requires to produce
its output. For example, since the Collins parser depends
on a prior part-of-speech tagger (Ratnaparkhi, 1996), we
included the time for POS tagging in our Collins mea-
surements. XLE incorporates a sophisticated finite-state
morphology and dictionary lookup component, and its
time is part of the measure of XLE performance.
Performance parameters of both the Collins parser and
the XLE system were adjusted on a heldout set consist-
ing of a random selection of 1/5 of the PARC 700 depen-
dency bank; experimental results were then based on the
other 560 sentences. For Model 3 of the Collins parser, a
beam size of 1000, and not the recommended beam size
of 10000, was found to optimize parsing speed at little
loss in accuracy. On the same heldout set, parameters of
the stochastic disambiguation system and parameters for
parsing performance were adjusted for a Core and a Com-
plete version of the XLE system, differing in the size of
the constraint-set of the underlying grammar.
For both XLE and the Collins parser we wrote con-
version programs to transform the normal (tree or f-
structure) output into the corresponding relations of
the dependency bank. This conversion was relatively
straightforward for LFG structures (King et al, 2003).
However, a certain amount of skill and intuition was
required to provide a fair conversion of the Collins
trees: we did not want to penalize configurations in the
Collins trees that encoded alternative but equally legit-
imate representations of the same linguistic properties
(e.g. whether auxiliaries are encoded as main verbs or
aspect features), but we also did not want to build into
the conversion program transformations that compensate
for information that Collins cannot provide without ap-
pealing to additional linguistic resources (such as identi-
fying the subjects of infinitival complements). We did not
include the time for dependency conversion in our mea-
sures of performance.
The experimental results show that stochastic parsing
with the Core LFG grammar achieves a better F-score
than the Collins parser at a roughly comparable parsing
speed. The XLE system achieves 12% reduction in error
rate over the Collins parser, that is 77.6% F-score for the
XLE system versus 74.6% for the Collins parser, at a cost
in parsing time of a factor of 1.49.
2 Stochastic Parsing with LFG
2.1 Parsing with Lexical-Functional Grammar
The grammar used for this experiment was developed in
the ParGram project (Butt et al, 2002). It uses LFG as a
formalism, producing c(onstituent)-structures (trees) and
f(unctional)-structures (attribute value matrices) as out-
put. The c-structures encode constituency and linear or-
der. F-structures encode predicate-argument relations and
other grammatical information, e.g., number, tense, state-
ment type. The XLE parser was used to produce packed
representations, specifying all possible grammar analyses
of the input.
In our system, tokenization and morphological analy-
sis are performed by finite-state transductions arranged in
a compositional cascade. Both the tokenizer and the mor-
phological analyzer can produce multiple outputs. For ex-
ample, the tokenizer will optionaly lowercase sentence
initial words, and the morphological analyzer will pro-
duce walk +Verb +Pres +3sg and walk +Noun +Pl for
the input form walks. The resulting tokenized and mor-
phologically analyzed strings are presented to the sym-
bolic LFG grammar.
The grammar can parse input that has XML de-
limited named entity markup: <company>Columbia
Savings</company> is a major holder of so-called junk
bonds. To allow the grammar to parse this markup,
the tokenizer includes an additional tokenization of the
strings whereby the material between the XML markup
is treated as a single token with a special morphologi-
cal tag (+NamedEntity). As a fall back, the tokenization
that the string would have received without that markup
is also produced. The named entities have a single mul-
tiword predicate. This helps in parsing both because it
means that no internal structure has to be built for the
predicate and because predicates that would otherwise be
unrecognized by the grammar can be parsed (e.g., Cie.
Financiere de Paribas). As described in section 5, it was
also important to use named entity markup in these ex-
periments to more fairly match the analyses in the PARC
700 dependency bank.
To increase robustness, the standard grammar is aug-
mented with a FRAGMENT grammar. This allows sen-
tences to be parsed as well-formed chunks specified by
the grammar, in particular as Ss, NPs, PPs, and VPs, with
unparsable tokens possibly interspersed. These chunks
have both c-structures and f-structures corresponding to
them. The grammar has a fewest-chunk method for de-
termining the correct parse.
The grammar incorporates a version of Optimality
Theory that allows certain (sub)rules in the grammar to be
prefered or disprefered based on OT marks triggered by
the (sub)rule (Frank et al, 1998). The Complete version
of the grammar uses all of the (sub)rules in a multi-pass
system that depends on the ranking of the OT marks in
the rules. For example, topicalization is disprefered, but
the topicalization rule will be triggered if no other parse
can be built. A one-line rewrite of the Complete grammar
creates a Core version of the grammar that moves the ma-
jority of the OT marks into the NOGOOD space. This ef-
fectively removes the (sub)rules that they mark from the
grammar. So, for example, in the Core grammar there is
no topicalization rule, and sentences with topics will re-
ceive a FRAGMENT parse. This single-pass Core grammar
is smaller than the Complete grammar and hence is faster.
The XLE parser also allows the user to adjust per-
formance parameters bounding the amount of work that
is done in parsing for efficient but accurate operation.
XLE?s ambiguity management technology takes advan-
tage of the fact that relatively few f-structure constraints
apply to constituents that are far apart in the c-structure,
so that sentences are typically parsed in polynomial time
even though LFG parsing is known to be an NP-complete
problem. But the worst-case exponential behavior does
begin to appear for some constructions in some sentences,
and the computational effort is limited by a SKIMMING
mode whose onset is controlled by a user-specified pa-
rameter. When skimming, XLE will stop processing the
subtree of a constituent whenever the amount of work ex-
ceeds that user-specified limit. The subtree is discarded,
and the parser will move on to another subtree. This guar-
antees that parsing will be finished within reasonable lim-
its of time and memory but at a cost of possibly lower
accuracy if it causes the best analysis of a constituent
to be discarded. As a separate parameter, XLE also lets
the user limit the length of medial constituents, i.e., con-
stituents that do not appear at the beginning or the end
of a sentence (ignoring punctuation). The rationale be-
hind this heuristic is to limit the weight of constituents in
the middle of the sentence but still to allow sentence-final
heavy constituents. This discards constituents in a some-
what more principled way as it tries to capture the psy-
cholinguistic tendency to avoid deep center-embedding.
When limiting the length of medial constituents, cubic-
time parsing is possible for sentences up to that length,
even with a deep, non-context-free grammar, and linear
parsing time is possible for sentences beyond that length.
The Complete grammar achieved 100% coverage of
section 23 as unseen unlabeled data: 79% as full parses,
21% FRAGMENT and/or SKIMMED parses.
2.2 Dynamic Programming for Estimation and
Stochastic Disambiguation
The stochastic disambiguation model we employ defines
an exponential (a.k.a. log-linear or maximum-entropy)
probability model over the parses of the LFG grammar.
The advantage of this family of probability distributions
is that it allows the user to encode arbitrary properties
of the parse trees as feature-functions of the probability
model, without the feature-functions needing to be inde-
pendent and non-overlapping. The general form of con-
ditional exponential models is as follows:
p?(x|y) = Z?(y)
?1e??f(x)
where Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant over the set X(y) of parses for sentence y, ? is
a vector of log-parameters, f is a vector of feature-
values, and ? ? f(x) is a vector dot product denoting the
(log-)weight of parse x.
Dynamic-programming algorithms that allow the ef-
ficient estimation and searching of log-linear mod-
els from a packed parse representation without enu-
merating an exponential number of parses have
been recently presented by Miyao and Tsujii (2002)
and Geman and Johnson (2002). These algorithms can
be readily applied to the packed and/or-forests of
Maxwell and Kaplan (1993), provided that each conjunc-
tive node is annotated with feature-values of the log-
linear model. In the notation of Miyao and Tsujii (2002),
such a feature forest ? is defined as a tuple ?C,D, r, ?, ??
where C is a set of conjunctive nodes, D is a set of dis-
junctive nodes, r ? C is the root node, ? : D ? 2C is
a conjunctive daughter function, and ? : C ? 2D is a
disjunctive daughter function.
A dynamic-programming solution to the problem of
finding most probable parses is to compute the weight
?d of each disjunctive node as the maximum weight of
its conjunctive daugher nodes, i.e.,
?d = max
c??(d)
?c (1)
and to recursively define the weight ?c of a conjunctive
node as the product of the weights of all its descendant
disjunctive nodes and of its own weight:
?c =
?
d??(c)
?d e
??f(c) (2)
Keeping a trace of the maximally weighted choices in a
computaton of the weight ?r of the root conjunctive node
r allows us to efficiently recover the most probable parse
of a sentence from the packed representation of its parses.
The same formulae can be employed for an effi-
cient calculation of probabilistic expectations of feature-
functions for the statistical estimation of the parameters
?. Replacing the maximization in equation 1 by a sum-
mation defines the inside weight of disjunctive node. Cor-
respondingly, equation 2 denotes the inside weight of a
conjunctive node. The outside weight ?c of a conjunctive
node is defined as the outside weight of its disjunctive
mother node(s):
?c =
?
{d|c??(d)}
?d (3)
The outside weight of a disjunctive node is the sum of
the product of the outside weight(s) of its conjunctive
mother(s), the weight(s) of its mother(s), and the inside
weight(s) of its disjunctive sister(s):
?d =
?
{c|d??(c)}
{?c e
??f(c)
?
{d?|d???(c),d? 6=d}
?d?} (4)
From these formulae, the conditional expectation of a
feature-function fi can be computed from a chart with
root node r for a sentence y in the following way:
?
x?X(y)
e??f(x)fi(x)
Z?(y)
=
?
c?C
?c?cfi(c)
?r
(5)
Formula 5 is used in our system to compute expectations
for discriminative Bayesian estimation from partially la-
beled data using a first-order conjugate-gradient routine.
For a more detailed description of the optimization prob-
lem and the feature-functions we use for stochastic LFG
parsing see Riezler et al (2002). We also employed a
combined `1 regularization and feature selection tech-
nique described in Riezler and Vasserman (2004) that
considerably speeds up estimation and guarantees small
feature sets for stochastic disambiguation. In the experi-
ments reported in this paper, however, dynamic program-
ming is crucial for efficient stochastic disambiguation,
i.e. to efficiently find the most probable parse from a
packed parse forest that is annotated with feature-values.
There are two operations involved in stochastic disam-
biguation, namely calculating feature-values from a parse
forest and calculating node weights from a feature forest.
Clearly, the first one is more expensive, especially for
the extraction of values for non-local feature-functions
over large charts. To control the cost of this compu-
tation, our stochastic disambiguation system includes
a user-specified parameter for bounding the amount of
work that is done in calculating feature-values. When the
user-specified threshold for feature-value calculation is
reached, this computation is discontinued, and the dy-
namic programming calculation for most-probable-parse
search is computed from the current feature-value anno-
tation of the parse forest. Since feature-value computa-
tion proceeds incrementally over the feature forest, i.e.
for each node that is visited all feature-functions that ap-
ply to it are evaluated, a complete feature annotation can
be guaranteed for the part of the and/or-forest that is vis-
ited until discontinuation. As discussed below, these pa-
rameters were set on a held-out portion of the PARC700
which was also used to set the Collins parameters.
In the experiments reported in this paper, we used a
threshold on feature-extraction that allowed us to cut off
feature-extraction in 3% of the cases at no loss in accu-
racy. Overall, feature extraction and weight calculation
accounted for 5% of the computation time in combined
parsing and stochastic selection.
3 The Gold-Standard Dependency Bank
We used the PARC 700 Dependency Bank (DEPBANK)
as the gold standard in our experiments. The DEPBANK
consists of dependency annotations for 700 sentences that
were randomly extracted from section 23 of the UPenn
Wall Street Journal (WSJ) treebank. As described by
(King et al, 2003), the annotations were boot-strapped
by parsing the sentences with a LFG grammar and trans-
forming the resulting f-structures to a collection of depen-
dency triples in the DEPBANK format. To prepare a true
gold standard of dependencies, the tentative set of depen-
dencies produced by the robust parser was then corrected
and extended by human validators2. In this format each
triple specifies that a particular relation holds between a
head and either another head or a feature value, for ex-
ample, that the SUBJ relation holds between the heads
run and dog in the sentence The dog ran. Average sen-
tence length of sentences in DEPBANK is 19.8 words, and
the average number of dependencies per sentence is 65.4.
The corpus is freely available for research and evaluation,
as are documentation and tools for displaying and prun-
ing structures.3
In our experiments we used a Reduced version of the
DEPBANK, including just the minimum set of dependen-
cies necessary for reading out the central semantic rela-
tions and properties of a sentence. We tested against this
Reduced gold standard to establish accuracy on a lower
bound of the information that a meaning-sensitive appli-
cation would require. The Reduced version contained all
the argument and adjunct dependencies shown in Fig.
1, and a few selected semantically-relevant features, as
shown in Fig. 2. The features in Fig. 2 were chosen be-
2The resulting test set is thus unseen to the grammar and
stochastic disambiguation system used in our experiments. This
is indicated by the fact that the upperbound of F-score for the
best matching parses for the experiment grammar is in the range
of 85%, not 100%.
3http://www2.parc.com/istl/groups/nltt/fsbank/
Function Meaning
adjunct adjuncts
aquant adjectival quantifiers (many, etc.)
comp complement clauses (that, whether)
conj conjuncts in coordinate structures
focus int fronted element in interrogatives
mod noun-noun modifiers
number numbers modifying nouns
obj objects
obj theta secondary objects
obl oblique
obl ag demoted subject of a passive
obl compar comparative than/as clauses
poss possessives (John?s book)
pron int interrogative pronouns
pron rel relative pronouns
quant quantifiers (all, etc.)
subj subjects
topic rel fronted element in relative clauses
xcomp non-finite complements
verbal and small clauses
Figure 1: Grammatical functions in DEPBANK.
cause it was felt that they were fundamental to the mean-
ing of the sentences, and in fact they are required by the
semantic interpreter we have used in a knowledge-based
application (Crouch et al, 2002).
Feature Meaning
adegree degree of adjectives and adverbs
(positive, comparative, superlative)
coord form form of a coordinating
conjunction (e.g., and, or)
det form form of a determiner (e.g., the, a)
num number of nouns (sg, pl)
number type cardinals vs. ordinals
passive passive verb (e.g., It was eaten.)
perf perfective verb (e.g., have eaten)
precoord form either, neither
prog progressive verb (e.g., were eating)
pron form form of a pronoun (he, she, etc.)
prt form particle in a particle verb
(e.g., They threw it out.)
stmt type statement type (declarative,
interrogative, etc.)
subord form subordinating conjunction (e.g. that)
tense tense of the verb (past, present, etc.)
Figure 2: Selected features for Reduced DEPBANK
.
As a concrete example, the dependency list in Fig. 3 is
the Reduced set corresponding to the following sentence:
He reiterated his opposition to such funding,
but expressed hope of a compromise.
An additional feature of the DEPBANK that is relevant
to our comparisons is that dependency heads are rep-
resented by their standard citation forms (e.g. the verb
swam in a sentence appears as swim in its dependencies).
We believe that most applications will require a conver-
sion to canonical citation forms so that semantic relations
can be mapped into application-specific databases or on-
tologies. The predicates of LFG f-structures are already
represented as citation forms; for a fair comparison we
ran the leaves of the Collins tree through the same stem-
mer modules as part of the tree-to-dependency transla-
tion. We also note that proper names appear in the DEP-
BANK as single multi-word expressions without any in-
ternal structure. That is, there are no dependencies hold-
ing among the parts of people names (A. Boyd Simpson),
company names (Goldman, Sachs & Co), and organiza-
tion names (Federal Reserve). This multiword analysis
was chosen because many applications do not require
the internal structure of names, and the identification of
named entities is now typically carried out by a separate
non-syntactic pre-processing module. This was captured
for the LFG parser by using named entity markup and for
the Collins parser by creating complex word forms with
a single POS tag (section 5).
conj(coord?0, express?3)
conj(coord?0, reiterate?1)
coord form(coord?0, but)
stmt type(coord?0, declarative)
obj(reiterate?1, opposition?6)
subj(reiterate?1, pro?7)
tense(reiterate?1, past)
obj(express?3, hope?15)
subj(express?3, pro?7)
tense(express?3, past)
adjunct(opposition?6, to?11)
num(opposition?6, sg)
poss(opposition?6, pro?19)
num(pro?7, sg)
pron form(pro?7, he)
obj(to?11, funding?13)
adjunct(funding?13, such?45)
num(funding?13, sg)
adjunct(hope?15, of?46)
num(hope?15, sg)
num(pro?19, sg)
pron form(pro?19, he)
adegree(such?45, positive)
obj(of?46, compromise?54)
det form(compromise?54, a)
num(compromise?54, sg)
Figure 3: Reduced dependency relations for He reiterated
his opposition to such funding, but expressed hope of a
compromise.
4 Conversion to Dependency Bank Format
A conversion routine was required for each system to
transform its output so that it could be compared to the
DEPBANK dependencies. While it is relatively straightfor-
ward to convert LFG f-structures to the dependency bank
format because the f-structure is effectively a dependency
format, it is more difficult to transform the output trees of
the Model 3 Collins parser in a way that fairly allocates
both credits and penalties.
LFG Conversion We discarded the LFG tree structures
and used a general rewriting system previously developed
for machine translation to rewrite the relevant f-structure
attributes as dependencies (see King et al (2003)). The
rewritings involved some deletions of irrelevant features,
some systematic manipulations of the analyses, and some
trivial respellings. The deletions involved features pro-
duced by the grammar but not included in the PARC 700
such as negative values of PASS, PERF, and PROG and
the feature MEASURE used to mark measure phrases. The
manipulations are more interesting and are necessary to
map systematic differences between the analyses in the
grammar and those in the dependency bank. For example,
coordination is treated as a set by the LFG grammar but as
a single COORD dependency with several CONJ relations
in the dependency bank. Finally, the trivial rewritings
were used to, for example, change STMT-TYPE decl in
the grammar to STMT-TYPE declarative in the de-
pendency bank. For the Reduced version of the PARC
700 substantially more features were deleted.
Collins Model 3 Conversion An abbreviated represen-
tation of the Collins tree for the example above is shown
in Fig. 4. In this display we have eliminated the head lex-
ical items that appear redundantly at all the nonterminals
in a head chain, instead indicating by a single number
which daughter is the head. Thus, S?2 indicates that the
head of the main clause is its second daughter, the VP,
and its head is its first VP daughter. Indirectly, then, the
lexical head of the S is the first verb reiterated.
(TOP?1
(S?2 (NP-A?1 (NPB?1 He/PRP))
(VP?1 (VP?1 reiterated/VBD
(NP-A?1 (NPB?2 his/PRP$
opposition/NN)
(PP?1 to/TO
(NPB?2 such/JJ
funding/NN))))
but/CC
(VP?1 expressed/VBD
(NP-A?1 (NPB?1 hope/NN)
(PP?1 of/IN
(NP-A?1 (NPB?2 a/DT
compromise/NN))))))))
Figure 4: Collins Model 3 tree for He reiterated his op-
position to such funding, but expressed hope of a compro-
mise.
The Model 3 output in this example includes standard
phrase structure categories, indications of the heads, and
the additional -A marker to distinguish arguments from
adjuncts. The terminal nodes of this tree are inflected
forms, and the first phase of our conversion replaces them
with their citation forms (the verbs reiterate and express,
and the decapitalized and standardized he for He and his).
We also adjust for systematic differences in the choice of
heads. The first conjunct tends to be marked as the head
of a coordination in Model 3 output, whereas the depen-
dency bank has a more symmetric representation: it in-
troduces a new COORD head and connects that up to the
conjunction, and it uses a separate CONJ relation for each
of the coordinated items. Similarly, Model 3 identifies
the syntactic markers to and that as the heads of com-
plements, whereas the dependency bank treats these as
selectional features and marks the main predicate of the
complements as the head. These adjustments are carried
out without penalty. We also compensate for the differ-
ences in the representation of auxiliaries: Model 3 treats
these as main verbs with embedded complements instead
of the PERF, PROG, and PASSIVE features of the DEP-
BANK, and our conversion flattens the trees so that the
features can be read off.
The dependencies are read off after these and a few
other adjustments are made. NPs under VPs are read off
either as objects or adjuncts, depending on whether or
not the NP is annotated with the argument indicator (-A)
as in this example; the -A presumably would be miss-
ing in a sentence like John arrived Friday, and Friday
would be treated as an ADJUNCT. Similarly, NP-As un-
der S are read off as subject. In this example, however,
this principle of conversion does not lead to a match with
the dependency bank: in the DEPBANK grammatical rela-
tions that are factored out of conjoined structures are dis-
tributed back into those structures, to establish the correct
semantic dependencies (in this case, that he is the subject
of both reiterate and express and not of the introduced
coord). We avoided the temptation of building coordinate
distribution into the conversion routine because, first, it is
not always obvious from the Model 3 output when dis-
tribution should take place, and second, that would be
a first step towards building into the conversion routine
the deep lexical and syntactic knowledge (essentially the
functional component of our LFG grammar) that the shal-
low approach explicitly discounts4.
For the same reasons our conversion routine does not
identify the subjects of infinitival complements with par-
ticular arguments of matrix verbs. The Model 3 trees pro-
vide no indication of how this is to be done, and in many
cases the proper assignment depends on lexical informa-
tion about specific predicates (to capture, for example, the
well-known contrast between promise and persuade).
Model 3 trees also provide information about certain
4However, we did explore a few of these additional transfor-
mations and found only marginal F-score increases.
long-distance dependencies, by marking with -g annota-
tions the path between a filler and a gap and marking the
gap by an explicit TRACE in the terminal string. The filler
itself is not clearly identified, but our conversion treats
all WH categories under SBAR as potential fillers and
attempts to propagate them down the gap-chain to link
them up to appropriate traces.
In sum, it is not a trivial matter to convert a Model 3
tree to an appropriate set of dependency relations, and the
process requires a certain amount of intuition and skill.
For our experiments we tried to define a conversion that
gives appropriate credit to the dependencies that can be
read from the trees without relying on an undue amount
of sophisticated linguistic knowledge5.
5 Experiments
We conducted our experiments by preparing versions of
the test sentences in the form appropriate to each sys-
tem. We used a configuration of the XLE parser that ex-
pects sentences conforming to ordinary text conventions
to appear in a file separated by double line-feeds. A cer-
tain amount of effort was required to remove the part-of-
speech tags and labeled brackets of the WSJ corpus in a
way that restored the sentences to a standard English for-
mat (for example, to remove the space between wo and n?t
that remains when the POS tags are removed). Since the
PARC 700 treats proper names as multiword expressions,
we then augmented the input strings with XML markup
of the named entities. These are parsed by the grammar
as described in section 2. We used manual named entity
markup for this experiment because our intent is to mea-
sure parsing technology independent of either the time
or errors of an automatic named-entity extractor. How-
ever, in other experiments with an automatic finite-state
extractor, we have found that the time for named-entity
recognition is negligible (on the order of seconds across
the entire corpus) and makes relatively few errors, so that
the results reported here are good approximations of what
might be expected in more realistic situations.
As input to the Collins parser, we used the part-of-
speech tagged version of section 23 that was provided
with the parser. From this we extracted the 700 sentences
in the PARC 700. We then modified them to produce
named entity input so that the parses would match the
PARC 700. This was done by putting underscores be-
tween the parts of the named entity and changing the final
part of speech tag to the appropriate one (usually NNP)
if necessary. (The number of words indicated at the be-
ginning of the input string was also reduced accordingly.)
An example is shown in (1).
5The results of this conversion are available at
http://www2.parc.com/istl/groups/nltt/fsbank/
(1) Sen. NNP Christopher NNP Dodd NNP ??
Sen. Christopher Dodd NNP
After parsing, the underscores were converted to spaces
to match the PARC 700 predicates.
Before the final evaluation, 1/5 of the PARC 700 de-
pendency bank was randomly extracted as a heldout set.
This set was used to adjust the performance parameters of
the XLE system and the Collins parser so as to optimize
parsing speed without losing accuracy. For example, the
limit on the length of medial phrases was set to 20 words
for the XLE system (see Sec. 2), and a regularizer penalty
of 10 was found optimal for the `1 prior used in stochas-
tic disambiguation. For the Collins parser, a beam size
of 1000 was found to improve speed considerably at lit-
tle cost in accuracy. Furthermore, the np-bracketing flag
(npbflag) was set to 0 to produce an extended set of NP
levels for improved argument/adjunct distinction6. The fi-
nal evaluation was done on the remaining 560 examples.
Timing results are reported in seconds of CPU time7. POS
tagging of the input to the Collins parser took 6 seconds
and this was added to the timing result of the Collins
parser. Time spent for finite-state morphology and dictio-
nary lookup for XLE is part of the measure of its timing
performance. We did not include the time for dependency
extraction or stemming the Collins output.
Table 1 shows timing and accuracy results for the Re-
duced dependency set. The parser settings compared are
Model 3 of the Collins parser adjusted to beam size 1000,
and the Core and Complete versions of the XLE sys-
tem, differing in the size of the grammar?s constraint-
set. Clearly, both versions of the XLE system achieve a
significant reduction in error rate over the Collins parser
(12% for the core XLE system and 20% for the complete
system) at an increase in parsing time of a factor of only
1.49 for the core XLE system. The complete version gives
an overall improvement in F-score of 5% over the Collins
parser at a cost of a factor of 5 in parsing time.
Table 1: Timing and accuracy results for Collins parser
and Complete and Core versions of XLE system on Re-
duced version of PARC 700 dependency bank.
time prec. rec. F-score
LFG core 298.88 79.1 76.2 77.6
LFG complete 985.3 79.4 79.8 79.6
Collins 1000 199.6 78.3 71.2 74.6
6A beam size of 10000 as used in Collins (1999) improved
the F-score on the heldout set only by .1% at an increase of pars-
ing time by a factor of 3. Beam sizes lower than 1000 decreased
the heldout F-score significantly.
7All experiments were run on one CPU of a dual proces-
sor AMD Opteron 244 with 1.8 GHz and 4GB main memory.
Loading times are included in CPU times.
6 Conclusion
We presented some experiments that compare the accu-
racy and performance of two stochastic parsing systems,
the shallow Collins parser and the deep-grammar-based
XLE system. We measured the accuracy of both systems
against a gold standard derived from the PARC 700 de-
pendency bank, and also measured their processing times.
Contrary to conventional wisdom, we found that the shal-
low system was not substantially faster than the deep
parser operating on a core grammar, while the deep sys-
tem was significantly more accurate. Furthermore, ex-
tending the grammar base of the deep system results in
much better accuracy at a cost of a factor of 5 in speed.
Our experiment is comparable to recent work on read-
ing off Propbank-style (Kingsbury and Palmer, 2002)
predicate-argument relations from gold-standard tree-
bank trees and automatic parses of the Collins parser.
Gildea and Palmer (2002) report F-score results in the
55% range for argument and boundary recognition based
on automatic parses. From this perspective, the nearly
75% F-score that is achieved for our deterministic rewrit-
ing of Collins? trees into dependencies is remarkable,
even if the results are not directly comparable. Our scores
and Gildea and Palmer?s are both substantially lower than
the 90% typically cited for evaluations based on labeled
or unlabeled bracketing, suggesting that extracting se-
mantically relevant dependencies is a more difficult, but
we think more valuable, task.
References
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In Proceedings of COL-
ING2002, Workshop on Grammar Engineering and
Evaluation, pages 1?7.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
D. Crouch, C. Condoravdi, R. Stolle, T.H. King,
V. de Paiva, J. Everett, and D. Bobrow. 2002. Scal-
ability of redundancy detection in focused document
collections. In Proceedings of Scalable Natural Lan-
guage Understanding, Heidelberg.
Hal Daume and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), Philadelphia, PA.
Anette Frank, Tracy H. King, Jonas Kuhn, and John
Maxwell. 1998. Optimality theory style constraint
ranking in large-scale LFG grammars. In Proceedings
of the Third LFG Conference.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochatic
unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?02), Philadelphia, PA.
Dan Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL?02), Philadelphia.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Ra?zvan Bunescu, Roxana
G??rju, Vasile Rus, and Paul Mora?rescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of the 39th
Annual Meeting and 10th Conference of the European
Chapter of the Asssociation for Computational Lin-
guistics (ACL?01), Toulouse, France.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the Work-
shop on ?Linguistically Interpreted Corpora? at the
10th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (LINC?03), Bu-
dapest, Hungary.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?02), Las Palmas, Spain.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
the 1st Conference of the North American Chapter of
the Association for Computational Linguistics (ANLP-
NAACL 2000), Seattle, WA.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the Human Language Technology Conference
(HLT?02), San Diego, CA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP-
1.
Stefan Riezler and Alexander Vasserman. 2004. Gradi-
ent feature testing and `1 regularization for maximum
entropy parsing. Submitted for publication.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting and 10th Conference of the Eu-
ropean Chapter of the Asssociation for Computational
Linguistics (ACL?01), Toulouse, France.
Parsing the Wall Street Journal using a Lexical-Functional Grammar and
Discriminative Estimation Techniques
Stefan Riezler Tracy H. King Ronald M. Kaplan
Palo Alto Research Center Palo Alto Research Center Palo Alto Research Center
Palo Alto, CA 94304 Palo Alto, CA 94304 Palo Alto, CA 94304
riezler@parc.com thking@parc.com kaplan@parc.com
Richard Crouch John T. Maxwell III Mark Johnson
Palo Alto Research Center Palo Alto Research Center Brown University
Palo Alto, CA 94304 Palo Alto, CA 94304 Providence, RI 02912
crouch@parc.com maxwell@parc.com mj@cs.brown.edu
Abstract
We present a stochastic parsing system
consisting of a Lexical-Functional Gram-
mar (LFG), a constraint-based parser and
a stochastic disambiguation model. We re-
port on the results of applying this sys-
tem to parsing the UPenn Wall Street
Journal (WSJ) treebank. The model com-
bines full and partial parsing techniques
to reach full grammar coverage on unseen
data. The treebank annotations are used
to provide partially labeled data for dis-
criminative statistical estimation using ex-
ponential models. Disambiguation perfor-
mance is evaluated by measuring matches
of predicate-argument relations on two
distinct test sets. On a gold standard of
manually annotated f-structures for a sub-
set of the WSJ treebank, this evaluation
reaches 79% F-score. An evaluation on a
gold standard of dependency relations for
Brown corpus data achieves 76% F-score.
1 Introduction
Statistical parsing using combined systems of hand-
coded linguistically fine-grained grammars and
stochastic disambiguation components has seen con-
siderable progress in recent years. However, such at-
tempts have so far been confined to a relatively small
scale for various reasons. Firstly, the rudimentary
character of functional annotations in standard tree-
banks has hindered the direct use of such data for
statistical estimation of linguistically fine-grained
statistical parsing systems. Rather, parameter esti-
mation for such models had to resort to unsupervised
techniques (Bouma et al, 2000; Riezler et al, 2000),
or training corpora tailored to the specific grammars
had to be created by parsing and manual disam-
biguation, resulting in relatively small training sets
of around 1,000 sentences (Johnson et al, 1999).
Furthermore, the effort involved in coding broad-
coverage grammars by hand has often led to the spe-
cialization of grammars to relatively small domains,
thus sacrificing grammar coverage (i.e. the percent-
age of sentences for which at least one analysis is
found) on free text. The approach presented in this
paper is a first attempt to scale up stochastic parsing
systems based on linguistically fine-grained hand-
coded grammars to the UPenn Wall Street Journal
(henceforth WSJ) treebank (Marcus et al, 1994).
The problem of grammar coverage, i.e. the fact
that not all sentences receive an analysis, is tack-
led in our approach by an extension of a full-
fledged Lexical-Functional Grammar (LFG) and a
constraint-based parser with partial parsing tech-
niques. In the absence of a complete parse, a so-
called ?FRAGMENT grammar? allows the input to be
analyzed as a sequence of well-formed chunks. The
set of fragment parses is then chosen on the basis
of a fewest-chunk method. With this combination of
full and partial parsing techniques we achieve 100%
grammar coverage on unseen data.
Another goal of this work is the best possible ex-
ploitation of the WSJ treebank for discriminative es-
timation of an exponential model on LFG parses. We
define discriminative or conditional criteria with re-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 271-278.
                         Proceedings of the 40th Annual Meeting of the Association for
CS 1: FRAGMENTS
Sadj[fin]
S[fin]
NP
D 
the
NPadj
AP[attr]
A
golden
NPzero
N
share
VPall[fin]
VP[pass,fin]
AUX[pass,fin]
was
VPv[pass]
V[pass]
scheduled
VPinf
VPinf?pos
PARTinf
to
VPall[base]
VPv[base]
V[base]
expire
PPcl
PP
P
at
NP
D
the
NPadj
NPzero
N
beginning
FRAGMENTS
TOKEN
of
"The golden share was scheduled to expire at the beginning of"
?schedule<NULL, [132:expire]>[11:share]?PRED
?share?PRED 
?golden<[11:share]>?PRED  [11:share]SUBJADEGREE positive , ADJUNCT?TYPE nominal, ATYPE attributive23ADJUNCT
unspecifiedGRAINNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE nom , NUM  sg, PERS   311
SUBJ
?expire<[11:share]>?PRED  [11:share]SUBJ
?at<[170:beginning]>?PRED
?beginning ?PRED 
GERUND +, GRAIN unspecifiedNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE acc, NUM  sg, PCASE   at, PERS   3170
OBJ
ADV?TYPE	  vpadv
 , PSEM   locative, PTYPE   sem164
ADJUNCT	
INF?FORM to , PASSIVE   ?, VTYPE  main132
XCOMP
MOOD indicative, TENSE pastTNS?ASP
PASSIVE +, STMT?TYPE decl, VTYPE main67
FIRST
ofTOKEN229FIRST3218REST3188
Figure 1: FRAGMENT c-/f-structure for The golden share was scheduled to expire at the beginning of
spect to the set of grammar parses consistent with
the treebank annotations. Such data can be gathered
by applying labels and brackets taken from the tree-
bank annotation to the parser input. The rudimen-
tary treebank annotations are thus used to provide
partially labeled data for discriminative estimation
of a probability model on linguistically fine-grained
parses.
Concerning empirical evaluation of disambigua-
tion performance, we feel that an evaluation measur-
ing matches of predicate-argument relations is more
appropriate for assessing the quality of our LFG-
based system than the standard measure of match-
ing labeled bracketing on section 23 of the WSJ
treebank. The first evaluation we present measures
matches of predicate-argument relations in LFG f-
structures (henceforth the LFG annotation scheme)
to a gold standard of manually annotated f-structures
for a representative subset of the WSJ treebank. The
evaluation measure counts the number of predicate-
argument relations in the f-structure of the parse
selected by the stochastic model that match those
in the gold standard annotation. Our parser plus
stochastic disambiguator achieves 79% F-score un-
der this evaluation regime.
Furthermore, we employ another metric which
maps predicate-argument relations in LFG f-
structures to the dependency relations (henceforth
the DR annotation scheme) proposed by Carroll et
al. (1999). Evaluation with this metric measures the
matches of dependency relations to Carroll et al?s
gold standard corpus. For a direct comparison of our
results with Carroll et al?s system, we computed an
F-score that does not distinguish different types of
dependency relations. Under this measure we obtain
76% F-score.
This paper is organized as follows. Section 2
describes the Lexical-Functional Grammar, the
constraint-based parser, and the robustness tech-
niques employed in this work. In section 3 we
present the details of the exponential model on LFG
parses and the discriminative statistical estimation
technique. Experimental results are reported in sec-
tion 4. A discussion of results is in section 5.
2 Robust Parsing using LFG
2.1 A Broad-Coverage LFG
The grammar used for this project was developed in
the ParGram project (Butt et al, 1999). It uses LFG
as a formalism, producing c(onstituent)-structures
(trees) and f(unctional)-structures (attribute value
matrices) as output. The c-structures encode con-
stituency. F-structures encode predicate-argument
relations and other grammatical information, e.g.,
number, tense. The XLE parser (Maxwell and Ka-
plan, 1993) was used to produce packed represen-
tations, specifying all possible grammar analyses of
the input.
The grammar has 314 rules with regular expres-
sion right-hand sides which compile into a collec-
tion of finite-state machines with a total of 8,759
states and 19,695 arcs. The grammar uses several
lexicons and two guessers: one guesser for words
recognized by the morphological analyzer but not
in the lexicons and one for those not recognized.
As such, most nouns, adjectives, and adverbs have
no explicit lexical entry. The main verb lexicon con-
tains 9,652 verb stems and 23,525 subcategorization
frame-verb stem entries; there are also lexicons for
adjectives and nouns with subcategorization frames
and for closed class items.
For estimation purposes using the WSJ treebank,
the grammar was modified to parse part of speech
tags and labeled bracketing. A stripped down ver-
sion of the WSJ treebank was created that used
only those POS tags and labeled brackets relevant
for determining grammatical relations. The WSJ la-
beled brackets are given LFG lexical entries which
constrain both the c-structure and the f-structure of
the parse. For example, the WSJ?s ADJP-PRD la-
bel must correspond to an AP in the c-structure and
an XCOMP in the f-structure. In this version of the
corpus, all WSJ labels with -SBJ are retained and
are restricted to phrases corresponding to SUBJ in
the LFG grammar; in addition, it contains NP under
VP (OBJ and OBJth in the LFG grammar), all -LGS
tags (OBL-AG), all -PRD tags (XCOMP), VP under
VP (XCOMP), SBAR- (COMP), and verb POS tags
under VP (V in the c-structure). For example, our
labeled bracketing of wsj 1305.mrg is [NP-SBJ His
credibility] is/VBZ also [PP-PRD on the line] in the
investment community.
Some mismatches between the WSJ labeled
bracketing and the LFG grammar remain. These
often arise when a given constituent fills a gram-
matical role in more than one clause. For exam-
ple, in wsj 1303.mrg Japan?s Daiwa Securities Co.
named Masahiro Dozen president., the noun phrase
Masahiro Dozen is labeled as an NP-SBJ. However,
the LFG grammar treats it as the OBJ of the ma-
trix clause. As a result, the labeled bracketed version
of this sentence does not receive a full parse, even
though its unlabeled, string-only counterpart is well-
formed. Some other bracketing mismatches remain,
usually the result of adjunct attachment. Such mis-
matches occur in part because, besides minor mod-
ifications to match the bracketing for special con-
structions, e.g., negated infinitives, the grammar was
not altered to mirror the idiosyncrasies of the WSJ
bracketing.
2.2 Robustness Techniques
To increase robustness, the standard grammar has
been augmented with a FRAGMENT grammar. This
grammar parses the sentence as well-formed chunks
specified by the grammar, in particular as Ss, NPs,
PPs, and VPs. These chunks have both c-structures
and f-structures corresponding to them. Any token
that cannot be parsed as one of these chunks is
parsed as a TOKEN chunk. The TOKENs are also
recorded in the c- and f-structures. The grammar has
a fewest-chunk method for determining the correct
parse. For example, if a string can be parsed as two
NPs and a VP or as one NP and an S, the NP-S
option is chosen. A sample FRAGMENT c-structure
and f-structure are shown in Fig. 1 for wsj 0231.mrg
(The golden share was scheduled to expire at the
beginning of), an incomplete sentence; the parser
builds one S chunk and then one TOKEN for the
stranded preposition.
A final capability of XLE that increases cov-
erage of the standard-plus-fragment grammar is a
SKIMMING technique. Skimming is used to avoid
timeouts and memory problems. When the amount
of time or memory spent on a sentence exceeds
a threshhold, XLE goes into skimming mode for
the constituents whose processing has not been
completed. When XLE skims these remaining con-
stituents, it does a bounded amount of work per sub-
tree. This guarantees that XLE finishes processing
a sentence in a polynomial amount of time. In pars-
ing section 23, 7.2% of the sentences were skimmed;
26.1% of these resulted in full parses, while 73.9%
were FRAGMENT parses.
The grammar coverage achieved 100% of section
23 as unseen unlabeled data: 74.7% as full parses,
25.3% FRAGMENT and/or SKIMMED parses.
3 Discriminative Statistical Estimation
from Partially Labeled Data
3.1 Exponential Models on LFG Parses
We employed the well-known family of exponential
models for stochastic disambiguation. In this paper
we are concerned with conditional exponential mod-
els of the form:
p?(x|y) = Z?(y)
?1e??f(x)
where X(y) is the set of parses for sentence y,
Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant, ? = (?1, . . . , ?n) ? IRn is a vector of
log-parameters, f = (f1, . . . , fn) is a vector of
property-functions fi : X ? IR for i = 1, . . . , n
on the set of parses X , and ? ? f(x) is the vector dot
product
?n
i=1 ?ifi(x).
In our experiments, we used around 1000
complex property-functions comprising information
about c-structure, f-structure, and lexical elements
in parses, similar to the properties used in Johnson
et al (1999). For example, there are property func-
tions for c-structure nodes and c-structure subtrees,
indicating attachment preferences. High versus low
attachment is indicated by property functions count-
ing the number of recursively embedded phrases.
Other property functions are designed to refer to
f-structure attributes, which correspond to gram-
matical functions in LFG, or to atomic attribute-
value pairs in f-structures. More complex property
functions are designed to indicate, for example, the
branching behaviour of c-structures and the (non)-
parallelism of coordinations on both c-structure and
f-structure levels. Furthermore, properties refering
to lexical elements based on an auxiliary distribution
approach as presented in Riezler et al (2000) are
included in the model. Here tuples of head words,
argument words, and grammatical relations are ex-
tracted from the training sections of the WSJ, and
fed into a finite mixture model for clustering gram-
matical relations. The clustering model itself is then
used to yield smoothed probabilities as values for
property functions on head-argument-relation tuples
of LFG parses.
3.2 Discriminative Estimation
Discriminative estimation techniques have recently
received great attention in the statistical machine
learning community and have already been applied
to statistical parsing (Johnson et al, 1999; Collins,
2000; Collins and Duffy, 2001). In discriminative es-
timation, only the conditional relation of an analysis
given an example is considered relevant, whereas in
maximum likelihood estimation the joint probability
of the training data to best describe observations is
maximized. Since the discriminative task is kept in
mind during estimation, discriminative methods can
yield improved performance. In our case, discrimi-
native criteria cannot be defined directly with respect
to ?correct labels? or ?gold standard? parses since
the WSJ annotations are not sufficient to disam-
biguate the more complex LFG parses. However, in-
stead of retreating to unsupervised estimation tech-
niques or creating small LFG treebanks by hand, we
use the labeled bracketing of the WSJ training sec-
tions to guide discriminative estimation. That is, dis-
criminative criteria are defined with respect to the set
of parses consistent with the WSJ annotations.1
The objective function in our approach, denoted
by P (?), is the joint of the negative log-likelihood
?L(?) and a Gaussian regularization term ?G(?)
on the parameters ?. Let {(yj , zj)}mj=1 be a set of
training data, consisting of pairs of sentences y and
partial annotations z, let X(y, z) be the set of parses
for sentence y consistent with annotation z, and let
X(y) be the set of all parses produced by the gram-
mar for sentence y. Furthermore, let p[f ] denote the
expectation of function f under distribution p. Then
P (?) can be defined for a conditional exponential
model p?(z|y) as:
P (?) = ?L(?)?G(?)
= ? log
m?
j=1
p?(zj |yj) +
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
?
X(yj)
e??f(x)
+
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
+
m?
j=1
log
?
X(yj)
e??f(x) +
n?
i=1
?2i
2?2i
.
Intuitively, the goal of estimation is to find model pa-
1An earlier approach using partially labeled data for estimat-
ing stochastics parsers is Pereira and Schabes?s (1992) work on
training PCFG from partially bracketed data. Their approach
differs from the one we use here in that Pereira and Schabes
take an EM-based approach maximizing the joint likelihood of
the parses and strings of their training data, while we maximize
the conditional likelihood of the sets of parses given the corre-
sponding strings in a discriminative estimation setting.
rameters which make the two expectations in the last
equation equal, i.e. which adjust the model param-
eters to put all the weight on the parses consistent
with the annotations, modulo a penalty term from
the Gaussian prior for too large or too small weights.
Since a closed form solution for such parame-
ters is not available, numerical optimization meth-
ods have to be used. In our experiments, we applied
a conjugate gradient routine, yielding a fast converg-
ing optimization algorithm where at each iteration
the negative log-likelihood P (?) and the gradient
vector have to be evaluated.2 For our task the gra-
dient takes the form:
?P (?) =
?
?P (?)
??1
,
?P (?)
??2
, . . . ,
?P (?)
??n
?
, and
?P (?)
??i
= ?
m?
j=1
(
?
x?X(yj ,zj)
e??f(x)fi(x)
?
x?X(yj ,zj)
e??f(x)
?
?
x?X(yj)
e??f(x)fi(x)
?
x?X(yj)
e??f(x)
) +
?i
?2i
.
The derivatives in the gradient vector intuitively are
again just a difference of two expectations
?
m?
j=1
p?[fi|yj , zj ] +
m?
j=1
p?[fi|yj ] +
?i
?2i
.
Note also that this expression shares many common
terms with the likelihood function, suggesting an ef-
ficient implementation of the optimization routine.
4 Experimental Evaluation
4.1 Training
The basic training data for our experiments are sec-
tions 02-21 of the WSJ treebank. As a first step, all
sections were parsed, and the packed parse forests
unpacked and stored. For discriminative estimation,
this data set was restricted to sentences which re-
ceive a full parse (in contrast to a FRAGMENT or
SKIMMED parse) for both its partially labeled and
its unlabeled variant. Furthermore, only sentences
2An alternative numerical method would be a combination
of iterative scaling techniques with a conditional EM algorithm
(Jebara and Pentland, 1998). However, it has been shown exper-
imentally that conjugate gradient techniques can outperform it-
erative scaling techniques by far in running time (Minka, 2001).
which received at most 1,000 parses were used.
From this set, sentences of which a discriminative
learner cannot possibly take advantage, i.e. sen-
tences where the set of parses assigned to the par-
tially labeled string was not a proper subset of the
parses assigned the unlabeled string, were removed.
These successive selection steps resulted in a fi-
nal training set consisting of 10,000 sentences, each
with parses for partially labeled and unlabeled ver-
sions. Altogether there were 150,000 parses for par-
tially labeled input and 500,000 for unlabeled input.
For estimation, a simple property selection pro-
cedure was applied to the full set of around 1000
properties. This procedure is based on a frequency
cutoff on instantiations of properties for the parses
in the labeled training set. The result of this proce-
dure is a reduction of the property vector to about
half its size. Furthermore, a held-out data set was
created from section 24 of the WSJ treebank for ex-
perimental selection of the variance parameter of the
prior distribution. This set consists of 120 sentences
which received only full parses, out of which the
most plausible one was selected manually.
4.2 Testing
Two different sets of test data were used: (i) 700 sen-
tences randomly extracted from section 23 of the
WSJ treebank and given gold-standard f-structure
annotations according to our LFG scheme, and (ii)
500 sentences from the Brown corpus given gold
standard annotations by Carroll et al (1999) accord-
ing to their dependency relations (DR) scheme.3
Annotating the WSJ test set was bootstrapped
by parsing the test sentences using the LFG gram-
mar and also checking for consistency with the
Penn Treebank annotation. Starting from the (some-
times fragmentary) parser analyses and the Tree-
bank annotations, gold standard parses were created
by manual corrections and extensions of the LFG
parses. Manual corrections were necessary in about
half of the cases. The average sentence length of
the WSJ f-structure bank is 19.8 words; the average
number of predicate-argument relations in the gold-
standard f-structures is 31.2.
Performance on the LFG-annotated WSJ test set
3Both corpora are available online. The WSJ f-structure
bank at www.parc.com/istl/groups/nltt/fsbank/, and Carroll et
al.?s corpus at www.cogs.susx.ac.uk/lab/nlp/carroll/greval.html.
was measured using both the LFG and DR metrics,
thanks to an f-structure-to-DR annotation mapping.
Performance on the DR-annotated Brown test set
was only measured using the DR metric.
The LFG evaluation metric is based on the com-
parison of full f-structures, represented as triples
relation(predicate, argument). The predicate-
argument relations of the f-structure for one parse of
the sentence Meridian will pay a premium of $30.5
million to assume $2 billion in deposits. are shown
in Fig. 2.
number($:9, billion:17) number($:24, million:4)
detform(premium:3, a) mood(pay:0, indicative)
tense(pay:0, fut) adjunct(million:4, ?30.5?:28)
adjunct(premium:3, of:23) adjunct(billion:17, ?2?:19)
adjunct($:9, in:11) adjunct(pay:0, assume:7)
obj(pay:0, premium:3) stmttype(pay:0, decl)
subj(pay:0, ?Meridian?:5) obj(assume:7, $:9)
obj(of:23, $:24) subj(assume:7, pro:8)
obj(in:11, deposit:12) prontype(pro:8, null)
stmttype(assume:7, purpose)
Figure 2: LFG predicate-argument relation represen-
tation
The DR annotation for our example sentence, ob-
tained via a mapping from f-structures to Carroll et
al?s annotation scheme, is shown in Fig. 3.
(aux pay will) (subj pay Meridian )
(detmod premium a) (mod million 30.5)
(mod $ million) (mod of premium $)
(dobj pay premium ) (mod billion 2)
(mod $ billion) (mod in $ deposit)
(dobj assume $ ) (mod to pay assume)
Figure 3: Mapping to Carroll et al?s dependency-
relation representation
Superficially, the LFG and DR representations are
very similar. One difference between the annotation
schemes is that the LFG representation in general
specifies more relation tuples than the DR represen-
tation. Also, multiple occurences of the same lex-
ical item are indicated explicitly in the LFG rep-
resentation but not in the DR representation. The
main conceptual difference between the two an-
notation schemes is the fact that the DR scheme
crucially refers to phrase-structure properties and
word order as well as to grammatical relations in
the definition of dependency relations, whereas the
LFG scheme abstracts away from serialization and
phrase-structure. Facts like this can make a correct
mapping of LFG f-structures to DR relations prob-
lematic. Indeed, we believe that we still underesti-
mate by a few points because of DR mapping diffi-
culties. 4
4.3 Results
In our evaluation, we report F-scores for both types
of annotation, LFG and DR, and for three types
of parse selection, (i) lower bound: random choice
of a parse from the set of analyses (averaged over
10 runs), (ii) upper bound: selection of the parse
with the best F-score according to the annotation
scheme used, and (iii) stochastic: the parse selected
by the stochastic disambiguator. The error reduc-
tion row lists the reduction in error rate relative to
the upper and lower bounds obtained by the stochas-
tic disambiguation model. F-score is defined as 2 ?
precision? recall/(precision+ recall).
Table 1 gives results for 700 examples randomly
selected from section 23 of the WSJ treebank, using
both LFG and DR measures.
Table 1: Disambiguation results for 700 randomly
selected examples from section 23 of the WSJ tree-
bank using LFG and DR measures.
LFG DR
upper bound 84.1 80.7
stochastic 78.6 73.0
lower bound 75.5 68.8
error reduction 36 35
The effect of the quality of the parses on disam-
biguation performance can be illustrated by break-
ing down the F-scores according to whether the
parser yields full parses, FRAGMENT, SKIMMED, or
SKIMMED+FRAGMENT parses for the test sentences.
The percentages of test examples which belong to
the respective classes of quality are listed in the first
row of Table 2. F-scores broken down according to
classes of parse quality are recorded in the follow-
4See Carroll et al (1999) for more detail on the DR an-
notation scheme, and see Crouch et al (2002) for more de-
tail on the differences between the DR and the LFG annotation
schemes, as well as on the difficulties of the mapping from LFG
f-structures to DR annotations.
ing rows. The first column shows F-scores for all
parses in the test set, as in Table 1. The second col-
umn shows the best F-scores when restricting atten-
tion to examples which receive only full parses. The
third column reports F-scores for examples which
receive only non-full parses, i.e. FRAGMENT or
SKIMMED parses or SKIMMED+FRAGMENT parses.
Columns 4-6 break down non-full parses according
to examples which receive only FRAGMENT, only
SKIMMED, or only SKIMMED+FRAGMENT parses.
Results of the evaluation on Carroll et al?s Brown
test set are given in Table 3. Evaluation results for
the DR measure applied to the Brown corpus test set
broken down according to parse-quality are shown
in Table 2.
In Table 3 we show the DR measure along with an
evaluation measure which facilitates a direct com-
parison of our results to those of Carroll et al
(1999). Following Carroll et al (1999), we count
a dependency relation as correct if the gold stan-
dard has a relation with the same governor and de-
pendent but perhaps with a different relation-type.
This dependency-only (DO) measure thus does not
reflect mismatches between arguments and modi-
fiers in a small number of cases. Note that since
for the evaluation on the Brown corpus, no heldout
data were available to adjust the variance parame-
ter of a Bayesian model, we used a plain maximum-
likelihood model for disambiguation on this test set.
Table 3: Disambiguation results on 500 Brown cor-
pus examples using DO measure and DR measures.
DO DR
Carroll et al (1999) 75.1 -
upper bound 82.0 80.0
stochastic 76.1 74.0
lower bound 73.3 71.7
error reduction 32 33
5 Discussion
We have presented a first attempt at scaling up a
stochastic parsing system combining a hand-coded
linguistically fine-grained grammar and a stochas-
tic disambiguation model to the WSJ treebank.
Full grammar coverage is achieved by combining
specialized constraint-based parsing techniques for
LFG grammars with partial parsing techniques. Fur-
thermore, a maximal exploitation of treebank anno-
tations for estimating a distribution on fine-grained
LFG parses is achieved by letting grammar analyses
which are consistent with the WSJ labeled bracket-
ing define a gold standard set for discriminative es-
timation. The combined system trained on WSJ data
achieves full grammar coverage and disambiguation
performance of 79% F-score on WSJ data, and 76%
F-score on the Brown corpus test set.
While disambiguation performance of around
79% F-score on WSJ data seems promising, from
one perspective it only offers a 3% absolute im-
provement over a lower bound random baseline.
We think that the high lower bound measure high-
lights an important aspect of symbolic constraint-
based grammars (in contrast to treebank gram-
mars): the symbolic grammar already significantly
restricts/disambiguates the range of possible analy-
ses, giving the disambiguator a much narrower win-
dow in which to operate. As such, it is more appro-
priate to assess the disambiguator in terms of reduc-
tion in error rate (36% relative to the upper bound)
than in terms of absolute F-score. Both the DR and
LFG annotations broadly agree in their measure of
error reduction.
The lower reduction in error rate relative to the
upper bound for DR evaluation on the Brown corpus
can be attributed to a corpus effect that has also been
observed by Gildea (2001) for training and testing
PCFGs on the WSJ and Brown corpora.5
Breaking down results according to parse quality
shows that irrespective of evaluation measure and
corpus, around 4% overall performance is lost due
to non-full parses, i.e. FRAGMENT, or SKIMMED, or
SKIMMED+FRAGMENT parses.
Due to the lack of standard evaluation measures
and gold standards for predicate-argument match-
ing, a comparison of our results to other stochastic
parsing systems is difficult. To our knowledge, so
far the only direct point of comparison is the parser
of Carroll et al (1999) which is also evaluated on
Carroll et al?s test corpus. They report an F-score
5Gildea reports a decrease from 86.1%/86.6% re-
call/precision on labeled bracketing to 80.3%/81% when
going from training and testing on the WSJ to training on the
WSJ and testing on the Brown corpus.
Table 2: LFG F-scores for the 700 WSJ test examples and DR F-scores for the 500 Brown test examples
broken down according to parse quality.
WSJ-LFG all full non-full fragments skimmed skimmed+fragments
% of test set 100 74.7 25.3 20.4 1.4 3.4
upper bound 84.1 88.5 73.4 76.7 70.3 61.3
stochastic 78.6 82.5 69.0 72.4 66.6 56.2
lower bound 75.5 78.4 67.7 71.0 63.0 55.9
Brown-DR all full non-full fragments skimmed skimmed+fragments
% of test set 100 79.6 20.4 20.0 2.0 1.6
upper bound 80.0 84.5 65.4 65.4 56.0 53.5
stochastic 74.0 77.9 61.5 61.5 52.8 50.0
lower bound 71.1 74.8 59.2 59.1 51.2 48.9
of 75.1% for a DO evaluation that ignores predicate
labels, counting only dependencies. Under this mea-
sure, our system achieves 76.1% F-score.
References
Gosse Bouma, Gertjan von Noord, and Robert Malouf.
2000. Alpino: Wide-coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands, Amsterdam, Netherlands.
Miriam Butt, Tracy King, Maria-Eugenia Nin?o, and
Fre?de?rique Segond. 1999. A Grammar Writer?s Cook-
book. Number 95 in CSLI Lecture Notes. CSLI Publi-
cations, Stanford, CA.
John Carroll, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In Proceed-
ings of the EACL workshop on Linguistically Inter-
preted Corpora (LINC), Bergen, Norway.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14(NIPS?01), Van-
couver.
Michael Collins. 2000. Discriminative reranking for nat-
ural language processing. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML?00), Stanford, CA.
Richard Crouch, Ronald M. Kaplan, Tracy H. King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad-coverage stochastic parser. In Pro-
ceedings of the ?Beyond PARSEVAL? Workshop at the
3rd International Conference on Language Resources
and Evaluation (LREC?02), Las Palmas, Spain.
Dan Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Pittsburgh, PA.
Tony Jebara and Alex Pentland. 1998. Maximum con-
ditional likelihood via bound maximization and the
CEM algorithm. In Advances in Neural Information
Processing Systems 11 (NIPS?98).
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?99), College Park, MD.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Thomas Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Department of Statis-
tics, Carnegie Mellon University.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics (ACL?92),
Newark, Delaware.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized Stochastic Modeling of
Constraint-Based Grammars using Log-Linear Mea-
sures and EM Training. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?00), Hong Kong.
Urdu and the Parallel Grammar Project
Miriam Butt
Cent. for Computational Linguistics
UMIST
PO Box 88
Manchester M60 1QD GB
mutt@csli.stanford.edu
Tracy Holloway King
Palo Alto Research Center
3333 Coyote Hill Rd.
Palo Alto, CA 94304 USA
thking@parc.com
Abstract
We report on the role of the Urdu grammar in the
Parallel Grammar (ParGram) project (Butt et al,
1999; Butt et al, 2002).1 The ParGram project was
designed to use a single grammar development plat-
form and a unified methodology of grammar writ-
ing to develop large-scale grammars for typologi-
cally different languages. At the beginning of the
project, three typologically similar European gram-
mars were implemented. The addition of two Asian
languages, Urdu and Japanese, has shown that the
basic analysis decisions made for the European lan-
guages can be applied to typologically distinct lan-
guages. However, the Asian languages required the
addition of a small number of new standard analy-
ses to cover constructions and analysis techniques
not found in the European languages. With these ad-
ditional standards, the ParGram project can now be
applied to other typologically distinct languages.
1 Introduction
In this paper, we report on the role of the Urdu
grammar in the Parallel Grammar (ParGram) project
(Butt et al, 1999; Butt et al, 2002). The ParGram
project originally focused on three closely related
European languages: English, French, and German.
Once grammars for these languages were estab-
lished, two Asian languages were added: Japanese
and Urdu.2 Both grammars have been successfully
integrated into the project. Here we discuss the Urdu
grammar and what special challenges it brought to
the ParGram project. We are pleased to report that
creating an Urdu grammar within the ParGram stan-
dards has been possible and has led to typologically
useful extensions to the project.
The ParGram project uses the XLE parser
1We would like to thank Mary Dalrymple, Ron Kaplan, Hi-
roshi Masuichi, and Tomoko Ohkuma for their comments.
2Norwegian was also added at this time.
and grammar development platform (Maxwell
and Kaplan, 1993) to develop deep grammars
for six languages. All of the grammars use the
Lexical-Functional Grammar (LFG) formalism
which produces c(onstituent)-structures (trees)
and f(unctional)-structures (AVMs) as syntactic
analyses.
LFG assumes a version of Chomsky?s Universal
Grammar hypothesis, namely that all languages are
governed by similar underlying structures. Within
LFG, f-structures encode a language universal level
of analysis, allowing for cross-linguistic parallelism.
The ParGram project aims to test the LFG formal-
ism for its universality and coverage limitations and
to see how far parallelism can be maintained across
languages. Where possible, the analyses produced
for similar constructions in each language are paral-
lel. This parallelism requires a standard for linguistic
analysis. In addition, the LFG theory itself limits the
set of possible analyses, thus restricting the possible
analyses to choose from. The standardization of the
analyses has the computational advantage that the
grammars can be used in similar applications, and
it can simplify cross-language applications (Frank,
1999).
The conventions developed within the ParGram
grammars are extensive. The ParGram project dic-
tates not only the form of the features used in the
grammars, but also the types of analyses that are
chosen for constructions. In addition, the XLE plat-
form necessarily restricts how the grammars can be
written. In all cases, the Urdu grammar has success-
fully, and straightforwardly, incorporated the stan-
dards that were originally designed for the European
languages. In addition, it has contributed to the for-
mulation of new standards of analysis. Below we
discuss several aspects of this: morphology, lexicon,
and grammar development for the Urdu grammar
within the ParGram project.
2 Morphology
The grammars in the ParGram project depend on
finite-state morphologies as input (Beesley and
Karttunen, 2002). Without this type of resource, it
is difficult to build large-scale grammars, especially
for languages with substantial morphology. For
the original three languages, such morphologies
were readily available. As they had been developed
for information extraction applications instead of
deep grammar applications, there were some minor
problems, but the coverage of these morphologies
is excellent. An efficient, broad-coverage mor-
phology was also available for Japanese (Asahara
and Matsumoto, 2000) and was integrated into the
grammar. This has aided in the Japanese grammar
rapidly achieving broad coverage. It has also helped
control ambiguity because in the case of Japanese,
the morphology determines the part of speech of
each word in the string with very little ambiguity.
While some morphological analyzers al-
ready exist for Hindi,3 e.g., as part of the
tools developed at the Language Technolo-
gies Research Centre (LTRC), IIT Hyderabad
(http://www.iiit.net/ltrc/index.html), they are not
immediately compatible with the XLE grammar
development platform, nor is it clear that the
morphological analyses they produce conform to
the standards and methods developed within the
ParGram project. As such, part of the Urdu project
is to build a finite-state morphology that will serve
as a resource to the Urdu grammar and could be
used in other applications.
The development of the Urdu morphology in-
volves a two step process. The first step is to de-
termine the morphological class of words and their
subtypes in Urdu. Here we hope to use existing re-
sources and lexicons. The morphological paradigms
which yield the most efficient generalizations from
an LFG perspective must be determined. Once the
basic paradigms and morphological classes have
been identified, the second step is to enter all words
in the language with their class and subtype informa-
tion. These steps are described below. Currently we
are working on the first step; grant money is being
sought for further development.
The finite-state morphologies used in the Par-
Gram project associate surface forms of words with
a canonical form (a lemma) and a series of morpho-
logical tags that provide grammatical information
3An on-line morphological analyzer is available at:
http://ccat.sas.upenn.edu/plc/tamilweb/hindi.html
about that form. An example for English is shown
in (1) and for Urdu in (2).
(1) pushes: push +Verb +Pres +3sg
push +Noun +Pl
(2) bOlA bOl +Verb +Perf +Masc +Sg
(1) states the English surface form pushes can either
be the third singular form of the verb push or the plu-
ral of the noun push. (2) states that the Urdu surface
form bOlA is the perfect masculine singular form of
the verb bOl.
The first step of writing a finite-state morphology
for Urdu involves determining which tags are as-
sociated with which surface forms. As can be seen
from the above examples, determining the part of
speech (e.g., verb, noun, adjective) is not enough for
writing deep grammars. For verbs, tense, aspect, and
agreement features are needed. For nouns, number
and gender information is needed, as well as infor-
mation as to whether it is a common or proper noun.
Furthermore, for a number of problematic morpho-
logical phenomena such as oblique inflection on
nominal forms or default agreement on verbs, the
most efficient method of analyzing this part of the
morphology-syntax interface must be found (Butt
and Kaplan, 2002).
After having determined the tag ontology, the pat-
terns of how the surface forms map to the stem-tag
sets must be determined. For example, in English the
stem-tag set dog +Noun +Pl corresponds to the sur-
face form dogs in which an s is added to the stem,
while box +Noun +Pl corresponds to boxes in which
an es is added. At this point in time, the basic tag set
for Urdu has been established. However, the mor-
phological paradigms that correspond to these tag
combinations have not been fully explored.
Once the basic patterns are determined, the sec-
ond stage of the process begins. This stage involves
greatly increasing the coverage of the morphology
by adding in all the stems in Urdu and marking them
for which set of tags and surface forms they appear
with. This is a very large task. However, by using
frequency lists for the language and existing lexi-
cons,4 the most common words can be added first to
obtain a major gain in coverage.
In addition, a guesser can be added to guess words
that the morphology does not yet recognize (Chanod
4A web search on Hindi dictionary results in several
promising sites.
and Tapanainen, 1995). This guessing is based on
the morphological form of the surface form. For ex-
ample, if a form ending in A is encountered and not
recognized, it could be considered a perfect mascu-
line singular form, similar to bOlA in (2).
3 Lexicon
One advantage of the fact that the XLE system in-
corporates large finite-state morphologies is that the
lexicons for the languages can then be relatively
small. This is because lexicons are not needed for
words whose syntactic lexical entry can be deter-
mined based on their morphological analysis. This is
particularly true for nouns, adjectives, and adverbs.
Consider the case of nouns. The Urdu morphol-
ogy provides the following analysis for the proper
noun nAdyA.
(3) nAdyA +Noun +Name +Fem
The tags provide the information that it is a noun, in
particular a type of proper noun (Name), and is fem-
inine. The lexical entries for the tags can then pro-
vide the grammar with all of the features that it needs
to construct the analysis of nAdyA; this resulting f-
structure analysis is seen in Figures 2 and 4. Thus,
nAdyA itself need not be in the lexicon of the gram-
mar because it is already known to the morphologi-
cal analyzer.
Items whose lexical entry cannot be predicted
based on the morphological tags need explicit lex-
ical entries. This is the case for items whose subcat-
egorization frames are not predictable, primarily for
verbs. Currently, the Urdu verb lexicon is hand con-
structed and only contains a few verbs, generally one
for each subcategorization frame for use in grammar
testing. To build a broad-coverage Urdu grammar, a
more complete verb lexicon will be needed. To pro-
vide some idea of scale, the current English verb lex-
icon contains entries for 9,652 verbs; each of these
has an average of 2.4 subcategorization frames; as
such, there are 23,560 verb-subcategorization frame
pairs. However, given that Urdu employs produc-
tive syntactic complex predicate formation for much
of its verbal predication, the verb lexicon for Urdu
will be smaller than its English counterpart. On the
other hand, writing grammar rules for the productive
combinatorial possibilities between adjectives and
verbs (e.g., sAf karnA ?clean do?=?clean?), nouns and
verbs (e.g., yAd karnA ?memory do?=?remember?)
and verbs and verbs (e.g., kHA lEnA ?eat take?=?eat
up?) is anticipated to require significant effort.
There are a number of ways to obtain a broad-
coverage verb lexicon. One is to extract the informa-
tion from an electronic dictionary. This does not ex-
ist for Urdu, as far as we are aware. Another is to ex-
tract it from Urdu corpora. Again, these would have
to be either collected or created as part of the gram-
mar development project. A final way is to enter the
information by hand, depending on native speaker
knowledge and print dictionaries; this option is very
labor intensive. Fortunately, work is being done on
verb subcategorization frames in Hindi.5 We plan to
incorporate this information into the Urdu grammar
verb lexicon.
4 Grammar
The current Urdu grammar is relatively small, com-
prising 25 rules (left-hand side categories) which
compile into a collection of finite-state machines
with 106 states and 169 arcs. The size of the other
grammars in the ParGram project are shown in (4)
for comparison.
(4)
Language Rules States Arcs
German 444 4883 15870
English 310 4935 13268
French 132 1116 2674
Japanese 50 333 1193
Norwegian 46 255 798
Urdu 25 106 169
It is our intent to drastically expand the Urdu gram-
mar to provide broad-coverage on standard (gram-
matical, written) texts. The current size of the Urdu
grammar is not a reflection of the difficulty of the
language, but rather of the time put into it. Like the
Japanese and Norwegian grammars, it is less than
two years in development, compared with seven
years6 for the English, French, and German gram-
mars. However, unlike the Japanese and Norwe-
gian grammars, there has been no full-time gram-
mar writer on the Urdu grammar. Below we discuss
the Urdu grammar analyses and how they fit into the
ParGram project standardization requirements.
Even within a linguistic formalism, LFG for Par-
Gram, there is often more than one way to ana-
5One significant effort is the Hindi Verb Project run by Prof.
Alice Davison at the University of Iowa; further information is
available via their web site.
6Much of the effort in the initial years went into developing
the XLE platform and the ParGram standards. Due to these ini-
tial efforts, new grammars can be developed more quickly.
lyze a construction. Moreover, the same theoreti-
cal analysis may have different possible implemen-
tations in XLE. These solutions often differ in ef-
ficiency or conceptual simplicity. Whenever possi-
ble, the ParGram grammars choose the same anal-
ysis and the same technical solution for equivalent
constructions. This was done, for example, with im-
peratives. Imperatives are assigned a null pronomi-
nal subject within the f-structure and a feature indi-
cating that they are imperatives.
Parallelism, however, is not maintained at the cost
of misrepresenting the language. Situations arise in
which what seems to be the same construction in
different languages cannot have the same analysis.
An example of this is predicate adjectives (e.g., It
is red.). In English, the copular verb is considered
the syntactic head of the clause, with the pronoun
being the subject and the predicate adjective be-
ing an XCOMP. However, in Japanese, the adjective
is the main predicate, with the pronoun being the
subject. As such, these constructions receive non-
parallel analyses.
Urdu contains several syntactic constructions
which find no direct correlate in the European
languages of the ParGram project. Examples are
correlative clauses (these are an old Indo-European
feature which most modern European languages
have lost), extensive use of complex predication,
and rampant pro-drop. The ability to drop argu-
ments is not correlated with agreement or case
features in Urdu, as has been postulated for Italian,
for example. Rather, pro-drop in Urdu correlates
with discourse strategies: continuing topics and
known background information tend to be dropped.
Although the grammars do not encode discourse
information, the Japanese grammar analyzes pro-
drop effectively via technical tools made available
by the grammar development platform XLE. The
Urdu grammar therefore anticipates no problems
with pro-drop phenomena.
In addition, many constructions which are stal-
warts of English syntax do not exist in Asian lan-
guages. Raising constructions with seem, for exam-
ple, find no clear correlate in Urdu: the construction
is translated via a psych verb in combination with
a that-clause. This type of non-correspondence be-
tween European and South Asian languages raises
challenges of how to determine parallelism across
analyses. A similar example is the use of expletives
(e.g., There is a unicorn in the garden.) which do not
exist in Urdu.
4.1 Existing Analysis Standards
While Urdu contains syntactic constructions which
are not mirrored in the European languages, it shares
many basic constructions, such as sentential com-
plementation, control constructions, adjective-noun
agreement, genitive specifiers, etc. The basic analy-
sis of these constructions was determined in the ini-
tial stage of the ParGram project in writing the En-
glish, French, and German grammars. These analy-
sis decisions have not been radically changed with
the addition of two typologically distinct Asian lan-
guages, Urdu and Japanese.
The parallelism in the ParGram project is pri-
marily across the f-structure analyses which encode
predicate-argument structure and other features that
are relevant to syntactic analysis, such as tense and
number.7 A sample analysis for the sentence in (5)
is shown in Figures 1 and 2.
(5) nAdyA kA kuttA AyA
Nadya Gen.M.Sg dog.Nom come-Perf.M.Sg
?Nadya?s dog came.?
The Urdu f-structure analysis of (5) is similar to that
of its English equivalent. Both have a PRED for the
verb which takes a SUBJ argument at the top level
f-structure. This top level structure also has TNS-
ASP features encoding tense and aspect information,
as well as information about the type of sentence
(STMT-TYPE) and verb (VTYPE); these same fea-
tures are found in the English structure. The analy-
sis of the subject is also the same, with the posses-
sive being in the SPEC POSS and with features such
as NTYPE, NUM, and PERS. The sentence in (5) in-
volves an intransitive verb and a noun phrase with a
possessive; these are both basic constructions whose
analysis was determined before the Urdu gram-
mar was written. Yet, despite the extensive differ-
ences between Urdu and the European languages?
indeed, the agreement relations between the genitive
and the head noun are complex in Urdu but not in
English?there was no problem using the standard
analysis for the Urdu construction.
4.2 New Analysis Standards
Analyses of new constructions have been added for
constructions found in the new project languages.
7The c-structures are less parallel in that the languages differ
significantly in their word orders. Japanese and Urdu are SOV
while English is SVO. However, the standards for naming the
nodes in the trees and the types of constituents formed in the
trees, such as NPs, are similar.
CS 1: ROOT
S
KP
NP
KPposs
NP
N
nAdyA
Kposs
kA
N
kuttA
VCmain
Vmain
V
AyA
Figure 1: C-structure tree for (5)
"nAdyA kA kuttA AyA"
?A<[14:kutt]>?PRED
?kutt?PRED
massGRAINNTYPE
?Nadya?PRED
namePROPERNTYPE
+SPECIFIC
CASE gen, GEND fem, NMORPH nom, NUM sg, PERS 30
POSSSPEC
CASE nom, GEND masc, NUM sg, PERS 314
SUBJ
perfASPECT
inflMTYPEVMORPH
PASSIVE ,  decl, VFORM perf, VTYPE unacc34
Figure 2: F-structure AVM for (5)
These analyses have not only established new stan-
dards within the ParGram project, but have also
guided the development of the XLE grammar de-
velopment platform. Consider the analysis of case
in Urdu. Although the features used in the analysis
of case were sufficient for Urdu, there was a prob-
lem with implementing it. In Urdu, the case mark-
ers constrain the environments in which they occur
(Butt and King, to appear). For example, the ergative
marker ne only occurs on subjects. However, not all
subjects are ergative. To the contrary, subjects can
occur in the ergative, nominative, dative, genitive,
and instrumental cases. Similarly, direct objects can
be marked with (at least) an accusative or nomina-
tive, depending on the semantics of the clause. Min-
imal pairs such as in (6) for subjects and (7) for ob-
jects suggest a constructive (Nordlinger, 1998) ap-
proach to case.
(6) a. rAm kH ?As-A
Ram.Nom cough-Perf.M.Sg
?Ram coughed.?
b. rAm nE kH ?As-A
Ram=Erg cough-Perf.M.Sg
?Ram coughed (purposefully).?
(7) a. nAdyA nE gArI calAyI
Nadya=Erg car.Nom drive-Perf.F.Sg
hai
be.Pres.3.Sg
?Nadya has driven a car.?
b. nAdyA nE gArI kO calAyA
Nadya=Erg car=Acc drive-Perf.M.Sg
hai
be.Pres.3.Sg
?Nadya has driven the car.?
We therefore designed the lexical entries for the case
markers so that they specify information about what
grammatical relations they attach to and what se-
mantic information is needed in the clausal analysis.
The lexical entry for the ergative case, for example,
states that it applies to a subject.
These statements require inside-out functional
uncertainty (Kaplan, 1988) which had not been used
in the other grammars. Inside-out functional uncer-
tainty allows statements about the f-structure that
contains an item. The lexical entry for nE is shown
in (8).
(8) nE K @(CASE erg) line 1
(SUBJ ($) ? ) line 2
@VOLITION line 3
In (8), the K refers to the part of speech (a case
clitic). Line 1 calls a template that assigns the CASE
feature the value erg; this is how case is done in
the other languages. Line 2 provides the inside-out
functional uncertainty statement; it states that the f-
structure of the ergative noun phrase, referred to as
?, is inside a SUBJ. Finally, line 3 calls a template
that assigns the volitionality features associated with
ergative noun phrases. The analysis for (9) is shown
in Figures 3 and 4.
(9) nAdyA nE yassin ko mArA
Nadya=Erg Yassin=Acc hit-Perf.M.Sg
?Nadya hit Yassin.?
CS 1: ROOT
S
KP
NP
N
nAdyA
K
nE
KP
NP
N
yassin
K
kO
VCmain
Vmain
V
mArA
Figure 3: C-structure tree for (9)
"nAdyA nE yassin kO mArA"
?hit<[0:Nadya], [16:Yassin]>?PRED
?Nadya?PRED
namePROPERNTYPE
+SPECIFIC
CASE erg, GEND fem, NUM sg, PERS 30
SUBJ
?Yassin?PRED
namePROPERNTYPE
+SPECIFIC
CASE acc, GEND masc, NUM sg, PERS 316
OBJ
perfASPECT
inflMTYPEVMORPH
GEND masc, NUM sg, PASSIVE ,  decl, VFORM perf, VTYPE agentive32
Figure 4: F-structure AVM for (9)
There are two intesting points about this analy-
sis of case in Urdu. The first is that although the
Urdu grammar processes case differently than the
other grammars, the resulting f-structure in Figure
4 is similar to its counterparts in English, German,
etc. English would have CASE nom on the subject in-
stead of erg, but the remaining structure is the same:
the only indication of case is the CASE feature. The
second point is that Urdu tested the application of
inside-out functional uncertainty to case both theo-
retically and computationally. In both respects, the
use of inside-out functional uncertainty has proven a
success: not only is it theoretically desirable for lan-
guages like Urdu, but it is also implementationally
feasible, efficiently providing the desired output.
Another interesting example of how Urdu has ex-
tended the standards of the ParGram project comes
from complex predicates. The English, French, and
German grammars do not need a complex predicate
analysis. However, as complex predicates form an
essential and pervasive part of Urdu grammar, it is
necessary to analyze them in the project. At first, we
attempted to analyze complex predicates using the
existing XLE tools. However, this proved to be im-
possible to do productively because XLE did not al-
low for the manipulation of PRED values outside of
the lexicon. Given that complex predicates in Urdu
are formed in the syntax and not the lexicon (Butt,
1995), this poses a significant problem. The syntac-
tic nature of Urdu complex predicate formation is il-
lustrated by (10), in which the two parts of the com-
plex predicate l?kh ?write? and diya ?gave? can be
separated.
(10) a. [anjum nE] [saddaf kO] [ciTTHI]
Anjum.F=Erg Saddaf.F=Dat note.F.Nom
[likHnE dI]
write-Inf.Obl give-Perf.F.Sg
?Anjum let Saddaf write a note.?
b. anjum nE dI saddaf kO [ciTTHI likHnE]
c. anjum nE [ciTTHI likHnE] saddaf kO dI
The manipulation of predicational structures in the
lexicon via lexical rules (as is done for the English
passive, for example), is therefore inadequate for
complex predication. Based on the needs of the Urdu
grammar, XLE has been modified to allow the anal-
ysis of complex predicates via the restriction oper-
ator (Kaplan and Wedekind, 1993) in conjunction
with predicate composition in the syntax. These new
tools are currently being tested by the implementa-
tion of the new complex predicates analysis.
5 Script
One issue that has not been dealt with in the Urdu
grammar is the different script systems used for
Urdu and Hindi. As seen in the previous discussions
and the Figures, transcription into Latin ASCII is
currently used by the Urdu grammar. This is not a
limitation of the XLE system: the Japanese grammar
has successfully integrated Japanese Kana and Kanji
into their grammar.
The approach taken by the Urdu grammar is dif-
ferent from that of the Japanese, largely because two
scripts are involved. The Urdu grammar uses the
ASCII transcription in the finite-state morphologies
and the grammar. At a future date, a component will
be built onto the grammar system that takes Urdu
(Arabic) and Hindi (Devanagari) scripts and tran-
scribes them for use in the grammar. This compo-
nent will be written using finite-state technology and
hence will be compatible with the finite-state mor-
phology. The use of ASCII in the morphology al-
lows the same basic morphology to be used for both
Urdu and Hindi. Samples of the scripts are seen in
(11) for Urdu and (12) for Hindi.
(11)
(12)
6 Conclusion
The ParGram project was designed to use a single
grammar development platform and a unified
methodology of grammar writing to develop
large-scale grammars for typologically different
languages. At the beginning of the project, three
typologically similar European grammars were
used to test this idea. The addition of two Asian
languages, has shown that the basic analysis de-
cisions made for the European languages can be
applied to typologically distinct languages. How-
ever, the Asian languages required the addition of a
few new standard analyses to the project to cover
constructions and analysis techniques not found
in the European languages. With this new set of
standards, the ParGram project can now be applied
to other typologically distinct languages.
The parallelism between the grammars in the Par-
Gram project can be exploited in applications using
the grammars: the fewer the differences, the simpler
a multi-lingual application can be. For example, a
translation system that uses the f-structures as input
and output can take advantage of the fact that similar
constructions have the same analysis (Frank, 1999).
The standardization also aids further grammar de-
velopment efforts. Many of the basic decisions about
analyses and formalism have already been made in
the project. Thus, the grammar writer for a new lan-
guage can use existing technology to bootstrap a
grammar for the new language and can parse equiv-
alent constructions in the existing languages to see
how to analyze a construction. This allows the gram-
mar writer to focus on more difficult constructions
not yet encountered in the existing grammars.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Ex-
tended models and tools for high-performance
part-of-speech tagger. In Proceedings of COL-
ING.
Kenneth Beesley and Lauri Karttunen. 2002.
Finite-State Morphology: Xerox Tools and
Techniques. Cambridge University Press. To
Appear.
Miriam Butt and Ron Kaplan. 2002. The mor-
phology syntax interface in LFG. Presented at
LFG02, Athens, Greece; to appear in the proceed-
ings (CSLI Publications).
Miriam Butt and Tracy Holloway King. to appear.
The status of case. In Veneeta Dayal and Anoop
Mahajan, editors, Clause Structure in South Asian
Languages. Kluwer.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nin?o, and Fre?de?rique Segond. 1999. A Grammar
Writer?s Cookbook. CSLI Publications.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
COLING 2002. Workshop on Grammar Engi-
neering and Evaluation.
Miriam Butt. 1995. The Structure of Complex Pred-
icates in Urdu. CSLI Publications.
Jean-Pierrre Chanod and Pasi Tapanainen. 1995.
Creating a tagset, lexicon, and guesser for a
French tagger. In Proceedings of the ACL SIG-
DAT Workshop: From Texts To Tags. Issues in
Multilingual Language Analysis, pages 58?64.
Anette Frank. 1999. From parallel grammar devel-
opment towards machine translation. In Proceed-
ings of MT Summit VII, pages 134?142.
Ron Kaplan and Ju?rgen Wedekind. 1993. Restric-
tion and correspondence-based translation. In
Proceedings of the Sixth European Conference
of the Association for Computational Linguistics,
pages 193?202.
Ron Kaplan. 1988. Correspondences and their in-
verses. Presented at the Titisee Workshop on Uni-
fication Formalisms: Syntax, Semantics, and Im-
plementation, Titisee, Germany.
John T. Maxwell, III and Ron Kaplan. 1993. The
interface between phrasal and functional con-
straints. Computational Lingusitics, 19:571?589.
Rachel Nordlinger. 1998. Constructive Case: Evi-
dence from Australian Languages. CSLI Publica-
tions.
The Parallel Grammar Project
Miriam Butt
Cent. for Computational Linguistics
UMIST
Manchester M60 1QD GB
mutt@csli.stanford.edu
Helge Dyvik
Dept. of Linguistics
University of Bergen
N5007 Bergen NORWAY
helge.dyvik@lili.uib.no
Tracy Holloway King
Palo Alto Research Center
Palo Alto, CA 94304 USA
thking@parc.com
Hiroshi Masuichi
Corporate Research Center
Fuji Xerox Co., Ltd.
Kanagawa 259-0157, JAPAN
hiroshi.masuichi@fujixerox.co.jp
Christian Rohrer
IMS Universita?t Stuttgart
D-70174 Stuttgart GERMANY
rohrer@ims.uni-stuttgart.de
Abstract
We report on the Parallel Grammar (ParGram)
project which uses the XLE parser and grammar
development platform for six languages: English,
French, German, Japanese, Norwegian, and Urdu.1
1 Introduction
Large-scale grammar development platforms are ex-
pensive and time consuming to produce. As such, a
desideratum for the platforms is a broad utilization
scope. A grammar development platform should be
able to be used to write grammars for a wide variety
of languages and a broad range of purposes. In this
paper, we report on the Parallel Grammar (ParGram)
project (Butt et al, 1999) which uses the XLE parser
and grammar development platform (Maxwell and
Kaplan, 1993) for six languages: English, French,
German, Japanese, Norwegian, and Urdu. All of
the grammars use the Lexical-Functional Gram-
mar (LFG) formalism which produces c(onstituent)-
structures (trees) and f(unctional)-structures (AVMs)
as the syntactic analysis.
LFG assumes a version of Chomsky?s Universal
Grammar hypothesis, namely that all languages are
structured by similar underlying principles. Within
LFG, f-structures are meant to encode a language
universal level of analysis, allowing for cross-
linguistic parallelism at this level of abstraction. Al-
though the construction of c-structures is governed
1We would like to thank Emily Bender, Mary Dalrymple,
and Ron Kaplan for help with this paper. In addition, we would
like to acknowledge the other grammar writers in the Par-
Gram project, both current: Stefanie Dipper, Jean-Philippe Mar-
cotte, Tomoko Ohkuma, and Victoria Rose?n; and past: Caroline
Brun, Christian Fortmann, Anette Frank, Jonas Kuhn, Veronica
Lux, Yukiko Morimoto, Mar??a-Eugenia Nin?o, and Fre?de?rique
Segond.
by general wellformedness principles, this level of
analysis encodes language particular differences in
linear word order, surface morphological vs. syntac-
tic structures, and constituency.
The ParGram project aims to test the LFG formal-
ism for its universality and coverage limitations and
to see how far parallelism can be maintained across
languages. Where possible, the analyses produced
by the grammars for similar constructions in each
language are parallel. This has the computational
advantage that the grammars can be used in simi-
lar applications and that machine translation (Frank,
1999) can be simplified.
The results of the project to date are encouraging.
Despite differences between the languages involved
and the aims and backgrounds of the project groups,
the ParGram grammars achieve a high level of paral-
lelism. This parallelism applies to the syntactic anal-
yses produced, as well as to grammar development
itself: the sharing of templates and feature decla-
rations, the utilization of common techniques, and
the transfer of knowledge and technology from one
grammar to another. The ability to bundle grammar
writing techniques, such as templates, into transfer-
able technology means that new grammars can be
bootstrapped in a relatively short amount of time.
There are a number of other large-scale gram-
mar projects in existence which we mention briefly
here. The LS-GRAM project (Schmidt et al, 1996),
funded by the EU-Commission under LRE (Lin-
guistic Research and Engineering), was concerned
with the development of grammatical resources for
nine European languages: Danish, Dutch, English,
French, German, Greek, Italian, Portuguese, and
Spanish. The project started in January 1994 and
ended in July 1996. Development of grammatical
resources was carried out in the framework of the
Advanced Language Engineering Platform (ALEP).
The coverage of the grammars implemented in LS-
GRAM was, however, much smaller than the cov-
erage of the English (Riezler et al, 2002) or Ger-
man grammar in ParGram. An effort which is closer
in spirit to ParGram is the implemention of gram-
mar development platforms for HPSG. In the Verb-
mobil project (Wahlster, 2000), HPSG grammars for
English, German, and Japanese were developed on
two platforms: LKB (Copestake, 2002) and PAGE.
The PAGE system, developed and maintained in the
Language Technology Lab of the German National
Research Center on Artificial Intelligence DFKI
GmbH, is an advanced NLP core engine that facili-
tates the development of grammatical and lexical re-
sources, building on typed feature logics. To evalu-
ate the HPSG platforms and to compare their mer-
its with those of XLE and the ParGram projects, one
would have to organize a special workshop, partic-
ularly as the HPSG grammars in Verbmobil were
written for spoken language, characterized by short
utterances, whereas the LFG grammars were devel-
oped for parsing technical manuals and/or newspa-
per texts. There are some indications that the Ger-
man and English grammars in ParGram exceed the
HPSG grammars in coverage (see (Crysmann et al,
2002) on the German HPSG grammar).
This paper is organized as follows. We first pro-
vide a history of the project. Then, we discuss how
parallelism is maintained in the project. Finally, we
provide a summary and discussion.
2 Project History
The ParGram project began in 1994 with three lan-
guages: English, French, and German. The gram-
mar writers worked closely together to solidify the
grammatical analyses and conventions. In addition,
as XLE was still in development, its abilities grew
as the size of the grammars and their needs grew.
After the initial stage of the project, more lan-
guages were added. Because Japanese is typolog-
ically very different from the initial three Euro-
pean languages of the project, it represented a chal-
lenging case. Despite this typological challenge, the
Japanese grammar has achieved broad coverage and
high performance within a year and a half. The
South Asian language Urdu also provides a widely
spoken, typologically distinct language. Although it
is of Indo-European origin, it shares many character-
istics with Japanese such as verb-finality, relatively
free word order, complex predicates, and the abil-
ity to drop any argument (rampant pro-drop). Nor-
wegian assumes a typological middle position be-
tween German and English, sharing different prop-
erties with each of them. Both the Urdu and the Nor-
wegian grammars are still relatively small.
Each grammar project has different goals, and
each site employs grammar writers with different
backgrounds and skills. The English, German, and
Japanese projects have pursued the goal of hav-
ing broad coverage, industrial grammars. The Nor-
wegian and Urdu grammars are smaller scale but
are experimenting with incorporating different kinds
of information into the grammar. The Norwegian
grammar includes a semantic projection; their anal-
yses produce not only c- and f-structures, but also
semantic structures. The Urdu grammar has imple-
mented a level of argument structure and is test-
ing various theoretical linguistic ideas. However,
even when the grammars are used for different pur-
poses and have different additional features, they
have maintained their basic parallelism in analysis
and have profited from the shared grammar writing
techniques and technology.
Table (1) shows the size of the grammars. The first
figure is the number of left-hand side categories in
phrase-structure rules which compile into a collec-
tion of finite-state machines with the listed number
of states and arcs.
(1)
Language Rules States Arcs
German 444 4883 15870
English 310 4935 13268
French 132 1116 2674
Japanese 50 333 1193
Norwegian 46 255 798
Urdu 25 106 169
3 Parallelism
Maintaining parallelism in grammars being devel-
oped at different sites on typologically distinct lan-
guages by grammar writers from different linguis-
tic traditions has proven successful. At project meet-
ings held twice a year, analyses of sample sentences
are compared and any differences are discussed; the
goal is to determine whether the differences are jus-
tified or whether the analyses should be changed
to maintain parallelism. In addition, all of the f-
structure features and their values are compared; this
not only ensures that trivial differences in naming
conventions do not arise, but also gives an overview
of the constructions each language covers and how
they are analyzed. All changes are implemented be-
fore the next project meeting. Each meeting also in-
volves discussion of constructions whose analysis
has not yet been settled on, e.g., the analysis of parti-
tives or proper names. If an analysis is agreed upon,
all the grammars implement it; if only a tentative
analysis is found, one grammar implements it and
reports on its success. For extremely complicated or
fundamental issues, e.g., how to represent predicate
alternations, subcommittees examine the issue and
report on it at the next meeting. The discussion of
such issues may be reopened at successive meetings
until a concensus is reached.
Even within a given linguistic formalism, LFG for
ParGram, there is usually more than one way to an-
alyze a construction. Moreover, the same theoreti-
cal analysis may have different possible implemen-
tations in XLE. These solutions often differ in effi-
ciency or conceptual simplicity and one of the tasks
within the ParGram project is to make design deci-
sions which favor one theoretical analysis and con-
comitant implementation over another.
3.1 Parallel Analyses
Whenever possible, the ParGram grammars choose
the same analysis and the same technical solution
for equivalent constructions. This was done, for
example, with imperatives. Imperatives are always
assigned a null pronominal subject within the f-
structure and a feature indicating that they are im-
peratives, as in (2).
(2) a. Jump! Saute! (French)
Spring! (German) Tobe! (Japanese)
Hopp! (Norwegian) kuudoo! (Urdu)
b. PRED jump SUBJ
SUBJ PRED pro
STMT-TYPE imp
Another example of this type comes from the
analysis of specifiers. Specifiers include many dif-
ferent types of information and hence can be ana-
lyzed in a number of ways. In the ParGram analysis,
the c-structure analysis is left relatively free accord-
ing to language particular needs and slightly vary-
ing theoretical assumptions. For instance, the Nor-
wegian grammar, unlike the other grammars, im-
plements the principles in (Bresnan, 2001) concern-
ing the relationship between an X -based c-structure
and the f-structure. This allows Norwegian speci-
fiers to be analyzed as functional heads of DPs etc.,
whereas they are constituents of NPs in the other
grammars. However, at the level of f-structure, this
information is part of a complex SPEC feature in
all the grammars. Thus parallelism is maintained
at the level of f-structure even across different the-
oretical preferences. An example is shown in (3)
for Norwegian and English in which the SPEC con-
sists of a QUANT(ifier) and a POSS(essive) (SPEC
can also contain information about DETerminers and
DEMONstratives).
(3) a. alle mine hester (Norwegian)
all my horses
?all my horses?
b. PRED horse
SPEC
QUANT PRED all
POSS
PRED pro
PERS 1
NUM sg
Interrogatives provide an interesting example be-
cause they differ significantly in the c-structures of
the languages, but have the same basic f-structure.
This contrast can be seen between the German ex-
ample in (4) and the Urdu one in (5). In German,
the interrogative word is in first position with the
finite verb second; English and Norwegian pattern
like German. In Urdu the verb is usually in final po-
sition, but the interrogative can appear in a number
of positions, including following the verb (5c).
(4) Was hat John Maria gegeben? (German)
what has John Maria give.PerfP
?What did John give to Mary??
(5) a. jon=nee marii=koo kyaa diiyaa? (Urdu)
John=Erg Mary=Dat what gave
?What did John give to Mary?
b. jon=nee kyaa marii=koo diiyaa?
c. jon=nee marii=ko diiyaa kyaa?
Despite these differences in word order and hence in
c-structure, the f-structures are parallel, with the in-
terrogative being in a FOCUS-INT and the sentence
having an interrogative STMT-TYPE, as in (6).
(6) PRED give SUBJ,OBJ,OBL
FOCUS-INT
PRED pro
PRON-TYPE int
SUBJ PRED John
OBJ [ ]
OBL PRED Mary
STMT-TYPE int
In the project grammars, many basic construc-
tions are of this type. However, as we will see in
the next section, there are times when parallelism is
not possible and not desirable. Even in these cases,
though, the grammars which can be parallel are;
so, three of the languages might have one analysis,
while three have another.
3.2 Justified Differences
Parallelism is not maintained at the cost of misrepre-
senting the language. This is reflected by the fact that
the c-structures are not parallel because word order
varies widely from language to language, although
there are naming conventions for the nodes. Instead,
the bulk of the parallelism is in the f-structure. How-
ever, even in the f-structure, situations arise in which
what seems to be the same construction in different
languages do not have the same analysis. An exam-
ple of this is predicate adjectives, as in (7).
(7) a. It is red.
b. Sore wa akai. (Japanese)
it TOP red
?It is red.?
In English, the copular verb is considered the syn-
tactic head of the clause, with the pronoun being the
subject and the predicate adjective being an XCOMP.
However, in Japanese, the adjective is the main pred-
icate, with the pronoun being the subject. As such,
these receive the non-parallel analyses seen in (8a)
for Japanese and (8b) for English.
(8) a. PRED red SUBJ
SUBJ PRED pro
b. PRED be XCOMP SUBJ
SUBJ PRED pro
XCOMP
PRED red SUBJ
SUBJ [ ]
Another situation that arises is when a feature
or construction is syntactically encoded in one lan-
guage, but not another. In such cases, the informa-
tion is only encoded in the languages that need it.
The equivalence captured by parallel analyses is not,
for example, translational equivalence. Rather, par-
allelism involves equivalence with respect to gram-
matical properties, e.g. construction types. One con-
sequence of this is that a typologically consistent
use of grammatical terms, embodied in the feature
names, is enforced. For example, even though there
is a tradition for referring to the distinction between
the pronouns he and she as a gender distinction in
English, this is a different distinction from the one
called gender in languages like German, French,
Urdu, and Norwegian, where gender refers to nom-
inal agreement classes. Parallelism leads to the sit-
uation where the feature GEND occurs in German,
French, Urdu, and Norwegian, but not in English
and Japanese. That is, parallelism does not mean
finding the same features in all languages, but rather
using the same features in the same way in all lan-
guages, to the extent that they are justified there. A
French example of grammatical gender is shown in
(9); note that determiner, adjective, and participle
agreement is dependent on the gender of the noun.
The f-structure for the nouns crayon and plume are
as in (10) with an overt GEND feature.
(9) a. Le petit crayon est casse?. (French)
the-M little-M pencil-M is broken-M.
?The little pencil is broken.?
b. La petite plume est casse?e. (French)
the-F little-F pen-F is broken-F.
?The little pen is broken.?
(10)
PRED crayon
GEND masc
PERS 3
PRED plume
GEND fem
PERS 3
F-structures for the equivalent words in English and
Japanese will not have a GEND feature.
A similar example comes from Japanese dis-
course particles. It is well-known that Japanese has
syntactic encodings for information such as honori-
fication. The verb in the Japanese sentence (11a)
encodes information that the subject is respected,
while the verb in (11b) shows politeness from the
writer (speaker) to the reader (hearer) of the sen-
tence. The f-structures for the verbs in (11) are as in
(12) with RESPECT and POLITE features within the
ADDRESS feature.
(11) a. sensei ga hon wo oyomininaru.
teacher Nom book Acc read-Respect
?The teacher read the book.? (Japanese)
b. seito ga hon wo yomimasu.
student Nom book Acc read-Polite
?The student reads the book.? (Japanese)
(12) a. PRED yomu SUBJ,OBJ
ADDRESS RESPECT +
b. PRED yomu SUBJ,OBJ
ADDRESS POLITE +
A final example comes from English progres-
sives, as in (13). In order to distinguish these two
forms, the English grammar uses a PROG feature
within the tense/aspect system. (13b) shows the f-
structure for (13a.ii).
(13) a. John hit Bill. i. He cried.
ii. He was crying.
b. PRED cry SUBJ
SUBJ PRED pro
TNS-ASP
TENSE past
PROG +
However, this distinction is not found in the other
languages. For example, (14a) is used to express
both (13a.i) and (13a.ii) in German.
(14) a. Er weinte. (German)
he cried
?He cried.?
b. PRED weinen SUBJ
SUBJ PRED pro
TNS-ASP TENSE past
As seen in (14b), the German f-structure is left un-
derspecified for PROG because there is no syntactic
reflex of it. If such a feature were posited, rampant
ambiguity would be introduced for all past tense
forms in German. Instead, the semantics will deter-
mine whether such forms are progressive.
Thus, there are a number of situations where hav-
ing parallel analyses would result in an incorrect
analysis for one of the languages.
3.3 One Language Shows the Way
Another type of situation arises when one language
provides evidence for a certain feature space or type
of analysis that is neither explicitly mirrored nor
explicitly contradicted by another language. In the-
oretical linguistics, it is commonly acknowledged
that what one language codes overtly may be harder
to detect for another language. This situation has
arisen in the ParGram project. Case features fall un-
der this topic. German, Japanese, and Urdu mark
NPs with overt case morphology. In comparison,
English, French, and Norwegian make relatively lit-
tle use of case except as part of the pronominal sys-
tem. Nevertheless, the f-structure analyses for all the
languages contain a case feature in the specification
of noun phrases.
This ?overspecification? of information expresses
deeper linguistic generalizations and keeps the f-
structural analyses as parallel as possible. In addi-
tion, the features can be put to use for the isolated
phenomena in which they do play a role. For exam-
ple, English does not mark animacy grammatically
in most situations. However, providing a ANIM +
feature to known animates, such as people?s names
and pronouns, allows the grammar to encode infor-
mation that is relevant for interpretation. Consider
the relative pronoun who in (15).
(15) a. the girl[ANIM +] who[ANIM +] left
b. the box[ANIM +] who[ANIM +] left
The relative pronoun has a ANIM + feature that is as-
signed to the noun it modifies by the relative clause
rules. As such, a noun modified by a relative clause
headed by who is interpreted as animate. In the case
of canonical inanimates, as in (15b), this will result
in a pragmatically odd interpretation, which is en-
coded in the f-structure.
Teasing apart these different phenomena crosslin-
guistically poses a challenge that the ParGram mem-
bers are continually engaged in. As such, we have
developed several methods to help maintain paral-
lelism.
3.4 Mechanics of Maintaining Parallelism
The parallelism among the grammars is maintained
in a number of ways. Most of the work is done dur-
ing two week-long project meetings held each year.
Three main activities occur during these meetings:
comparison of sample f-structures, comparison of
features and their values, and discussions of new or
problematic constructions.
A month before each meeting, the host site
chooses around fifteen sentences whose analysis is
to be compared at the meeting. These can be a ran-
dom selection or be thematic, e.g., all dealing with
predicatives or with interrogatives. The sentences
are then parsed by each grammar and the output is
compared. For the more recent grammars, this may
mean adding the relevant rules to the grammars, re-
sulting in growth of the grammar; for the older gram-
mars, this may mean updating a construction that has
not been examined in many years. Another approach
that was taken at the beginning of the project was to
have a common corpus of about 1,000 sentences that
all of the grammars were to parse. For the English,
French, and German grammars, this was an aligned
tractor manual. The corpus sentences were used for
the initial f-structure comparisons. Having a com-
mon corpus ensured that the grammars would have
roughly the same coverage. For example, they all
parsed declarative and imperative sentences. How-
ever, the nature of the corpus can leave major gaps
in coverage; in this case, the manual contained no in-
terrogatives.
The XLE platform requires that a grammar de-
clare all the features it uses and their possible val-
ues. Part of the Urdu feature table is shown in (16)
(the notation has been simplified for expository pur-
poses). As seen in (16) for QUANT, attributes which
take other attributes as their values must also be de-
clared. An example of such a feature was seen in
(3b) for SPEC which takes QUANT and POSS fea-
tures, among others, as its values.
(16) PRON-TYPE: pers poss null .
PROPER: date location name title .
PSEM: locational directional .
PTYPE: sem nosem .
QUANT: PRED QUANT-TYPE
QUANT-FORM .
The feature declarations of all of the languages are
compared feature by feature to ensure parallelism.
The most obvious use of this is to ensure that the
grammars encode the same features in the same way.
For example, at a basic level, one feature declaration
might have specified GEN for gender while the oth-
ers had chosen the name GEND; this divergence in
naming is regularized. More interesting cases arise
when one language uses a feature and another does
not for analyzing the same phenomena. When this is
noticed via the feature-table comparison, it is deter-
mined why one grammar needs the feature and the
other does not, and thus it may be possible to elim-
inate the feature in one grammar or to add it to an-
other.
On a deeper level, the feature comparison is use-
ful for conducting a survey of what constructions
each grammar has and how they are implemented.
For example, if a language does not have an ADE-
GREE (adjective degree) feature, the question will
arise as to whether the grammar analyzes compar-
ative and superlative adjectives. If they do not, then
they should be added and should use the ADEGREE
feature; if they do, then the question arises as to why
they do not have this feature as part of their analysis.
Finally, there is the discussion of problematic
constructions. These may be constructions that al-
ready have analyses which had been agreed upon in
the past but which are not working properly now that
more data has been considered. More frequently,
they are new constructions that one of the grammars
is considering adding. Possible analyses for the con-
struction are discussed and then one of the gram-
mars will incorporate the analysis to see whether it
works. If the analysis works, then the other gram-
mars will incorporate the analysis. Constructions
that have been discussed in past ParGram meet-
ings include predicative adjectives, quantifiers, par-
titives, and clefts. Even if not all of the languages
have the construction in question, as was the case
with clefts, the grammar writers for that language
may have interesting ideas on how to analyze it.
These group discussions have proven particularly
useful in extending grammar coverage in a parallel
fashion.
Once a consensus is reached, it is the responsi-
bility of each grammar to make sure that its anal-
yses match the new standard. As such, after each
meeting, the grammar writers will rename features,
change analyses, and implement new constructions
into their grammars. Most of the basic work has now
been accomplished. However, as the grammars ex-
pand coverage, more constructions need to be inte-
grated into the grammars, and these constructions
tend to be ones for which there is no standard analy-
sis in the linguistic literature; so, differences can eas-
ily arise in these areas.
4 Conclusion
The experiences of the ParGram grammar writers
has shown that the parallelism of analysis and imple-
mentation in the ParGram project aids further gram-
mar development efforts. Many of the basic deci-
sions about analyses and formalism have already
been made in the project. Thus, the grammar writer
for a new language can use existing technology to
bootstrap a grammar for the new language and can
parse equivalent constructions in the existing lan-
guages to see how to analyze a construction. This
allows the grammar writer to focus on more diffi-
cult constructions not yet encountered in the existing
grammars.
Consider first the Japanese grammar which was
started in the beginning of 2001. At the initial stage,
the work of grammar development involved imple-
menting the basic constructions already analyzed in
the other grammars. It was found that the grammar
writing techniques and guidelines to maintain par-
allelism shared in the ParGram project could be ef-
ficiently applied to the Japanese grammar. During
the next stage, LFG rules needed for grammatical is-
sues specific to Japanese have been gradually incor-
porated, and at the same time, the biannual ParGram
meetings have helped significantly to keep the gram-
mars parallel. Given this system, in a year and a half,
using two grammar writers, the Japanese grammar
has attained coverage of 99% for 500 sentences of a
copier manual and 95% for 10,000 sentences of an
eCRM (Voice-of-Customer) corpus.
Next consider the Norwegian grammar which
joined the ParGram group in 1999 and also empha-
sized slightly different goals from the other groups.
Rather than prioritizing large textual coverage from
the outset, the Norwegian group gave priority to the
development of a core grammar covering all major
construction types in a principled way based on the
proposals in (Bresnan, 2001) and the inclusion of a
semantic projection in addition to the f-structure. In
addition, time was spent on improving existing lexi-
cal resources ( 80,000 lemmas) and adapting them
to the XLE format. Roughly two man-years has been
spent on the grammar itself. The ParGram cooper-
ation on parallelism has ensured that the derived f-
structures are interesting in a multilingual context,
and the grammar will now serve as a basis for gram-
mar development in other closely related Scandina-
vian languages.
Thus, the ParGram project has shown that it is
possible to use a single grammar development plat-
form and a unified methodology of grammar writing
to develop large-scale grammars for typologically
different languages. The grammars? analyses show a
large degree of parallelism, despite being developed
at different sites. This is achieved by intensive meet-
ings twice a year. The parallelism can be exploited in
applications using the grammars: the fewer the dif-
ferences, the simpler a multilingual application can
be (see (Frank, 1999) on a machine-translation pro-
totype using ParGram).
References
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nin?o, and Fre?de?rique Segond. 1999. A Grammar
Writer?s Cookbook. CSLI Publications.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications.
Berthold Crysmann, Anette Frank, Bernd Keifer, St.
Mu?ller, Gu?nter Neumann, Jakub Piskorski, Ulrich
Scha?fer, Melanie Siegel, Hans Uszkoreit, Feiyu
Xu, Markus Becker, and Hans-Ulrich Krieger.
2002. An integrated architecture for shallow and
deep parsing. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics, University of Pennsylvania.
Anette Frank. 1999. From parallel grammar devel-
opment towards machine translation. In Proceed-
ings of MT Summit VII, pages 134?142.
John T. Maxwell, III and Ron Kaplan. 1993. The
interface between phrasal and functional con-
straints. Computational Lingusitics, 19:571?589.
Stefan Riezler, Tracy Holloway King, Ronald Ka-
plan, Dick Crouch, John T. Maxwell, III, and
Mark Johnson. 2002. Parsing the wall street jour-
nal using a lexical-functional grammar and dis-
criminative estimation techniques. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics, University of Penn-
sylvania.
Paul Schmidt, Sibylle Rieder, Axel Theofilidis, and
Thierry Declerck. 1996. Lean formalisms, lin-
guistic theory, and applications: Grammar devel-
opment in alep. In Proceedings of COLING.
Wolfgang Wahlster, editor. 2000. Verbmobil:
Foundations of Speech-to-Speech Translation.
Springer.
Adapting Existing Grammars: The XLE Experience
Ronald M. Kaplan and Tracy Holloway King and John T. Maxwell III
Palo Alto Research Center
Palo Alto, CA 94304 USA
kaplan, thking, maxwell @parc.com
Abstract
We report on the XLE parser and grammar develop-
ment platform (Maxwell and Kaplan, 1993) and de-
scribe how a basic Lexical Functional Grammar for
English has been adapted to two different corpora
(newspaper text and copier repair tips).
1 Introduction
Large-scale grammar development platforms should
be able to be used to develop grammars for a wide
variety of purposes. In this paper, we report on the
the XLE system (Maxwell and Kaplan, 1993), a
parser and grammar development platform for Lex-
ical Functional Grammars. We describe some of the
strategies and notational devices that enable the ba-
sic English grammar developed for the ParGram
project (Butt et al, 1999; Butt et al, 2002) to be
adapted to two corpora with different properties.
1.1 The Corpora
The STANDARD Pargram English grammar covers
the core phenomena of English (e.g., main and sub-
ordinate clauses, noun phrases, adjectives and ad-
verbs, prepositional phrases, coordination; see (Butt
et al, 1999)). We have built two different specialized
grammars on top of this: the EUREKA grammar and
the WSJ grammar.
The EUREKA grammar parses the Eureka cor-
pus of copier repair tips, a collection of documents
offering suggestions for how to diagnose and fix
particular copier malfunctions. These informal and
unedited documents were contributed by copier re-
pair technicians, and the corpus is characterized by
a significant amount of ungrammatical input (e.g.,
typos, incorrect punctuation, telegraphic sentences)
and much technical terminology (1). The goal of
parsing this corpus is to provide input to a semantics
and world-knowledge reasoning application (Ev-
erett et al, 2001).
(1) a. (SOLUTION 27032 70) If exhibiting 10-
132 faults replace the pre-fuser transport
sensor (Q10-130).
b. (SOLUTION 27240 80) 4. Enter into the
machine log, the changes that have been
made.
The WSJ grammar covers the UPenn Wall Street
Journal (WSJ) treebank sentences (Marcus et al,
1994). This corpus is characterized by long sen-
tences with many direct quotes and proper names,
(2a). In addition, for evaluation and training pur-
poses we also parsed a version of this corpus marked
up with labeled brackets and part-of-speech tags, as
in (2b). Riezler et al (2002) report on our WSJ pars-
ing experiments.
(2) a. But since 1981, Kirk Horse Insurance Inc.
of Lexington, Ky. has grabbed a 20% stake
of the market.
b. But since 1981, [NP-SBJ Kirk Horse In-
surance Inc. of Lexington, Ky.] has/VBZ
grabbed/VBN [NP a 20% stake of the mar-
ket].
2 Priority-based Grammar Specialization
The XLE system is designed so that the grammar
writer can build specialized grammars by both ex-
tending and restricting another grammar (in our case
the base grammar is the STANDARD Pargram En-
glish grammar). An LFG grammar is presented to
the XLE system in a priority-ordered sequence of
files containing phrase-structure rules, lexical en-
tries, abbreviatory macros and templates, feature
declarations, and finite-state transducers for tok-
enization and morphological analysis. XLE is ap-
plied to a single root file holding a CONFIGURA-
TION that identifies all the other files containing rel-
evant linguistic specifications, that indicates how
those components are to be assembled into a com-
plete grammar, and that specifies certain parameters
that control how that grammar is to be interpreted.
A key idea is that there can be only one definition
of an item of a given type with a particular name
(e.g., there can be only one NP rule although that sin-
gle rule can have many alternative expansions), and
items in a higher priority file override lower priority
items of the same type with the same name. This set
up is similar to the priority-override scheme of the
earlier LFG Grammar Writer?s Workbench (Kaplan
and Maxwell, 1996).
This arrangement makes it relatively easy to con-
struct a specialized grammar from a pre-existing
standard. The specialized grammar is defined by
a CONFIGURATION in its own root file that speci-
fies the relevant STANDARD grammar files as well
as the new files for the specialized grammar. The
files for the specialized grammar can also contain
items of different types (phrase-structure rules, lex-
ical entries, templates, etc.), and they are ordered
with higher priority than the STANDARD files.
Consider the configuration for the EUREKA gram-
mar. It specifies all of the STANDARD grammar files
as well as its own rule, template, lexicon, and mor-
phology files. A part of this configuration is shown
in (3) (the notationtemplates.lfg are shared by all the
languages? grammars, not just English).
(3) FILES ../standard/english-lexicons.lfg
../standard/english-rules.lfg
../standard/english-templates.lfg
../../common/notationtemplates.lfg
english-eureka-morphconfig
eureka-lexicons.lfg
eureka-rules.lfg
eureka-templates.lfg
This configuration specifies that the EUREKA rules,
templates, and lexical entries are given priority
over the STANDARD items by putting the spe-
cial EUREKA files at the end of the list. Thus, if
the ../standard/english-rules.lfg and eureka-rules.lfg
files both contain a rule expanding the NP category,
the one from the STANDARD file will be discarded in
favor of the EUREKA rule.
In the following subsections, we provide several
illustrations of how simple overriding has been used
for the EUREKA and WSJ grammar extensions.
2.1 Rules
The override convention makes it possible to: add
rules (e.g., for new or idiosyncratic constructions);
delete rules (e.g., to block constructions not found in
the new corpus); and modify rules to allow different
daughter sequences.
Rules may need to be added to allow for corpus-
specific constructions. This is illustrated in the EU-
REKA corpus by the identifier information that pre-
cedes each sentence, as in (1). In order to parse this
substring, a new category (FIELD) was defined with
an expansion that covers the identifier information
followed by the usual ROOT category of the STAN-
DARD grammar. The top-level category is one of
the parameters of a configuration, and the EUREKA
CONFIGURATION specifies that FIELD instead of the
STANDARD ROOT is the start-symbol of the gram-
mar. Thus the EUREKA grammar produces the tree
in (4) and functional-structure in (5) for (1a).
(4) FIELD
LP EURHEAD ID SUB-ID RP ROOT
( SOLUTION 27032 70 )
(5) PRED replace SUBJ, OBJ
SUBJ [ ]
OBJ [ ]
FIELD solution
TIP-ID 27032
SUB-TIP-ID 70
It is unusual in practice to need to delete a rule,
i.e., to eliminate completely the possibility of ex-
panding a given category of the STANDARD gram-
mar. This is generally only motivated when the spe-
cialized grammar applies to a domain where certain
constructions are rarely encountered, if at all. Al-
though there has been no need to delete rules for the
EUREKA and WSJ corpora, the override convention
also provides a natural way of achieving this effect.
For example, topicalization is extremely rare in the
the Eureka corpus and the STANDARD topicalization
rule sometimes introduces parsing inefficiency. This
can be avoided by having the high priority EUREKA
file replace the STANDARD rule with the one in (6).
(6) CPtop .
This vacuous rule expands the CPtop category to the
empty language, the language containing no strings;
so, this category is effectively removed from the
grammar.
Perhaps the most common change is to make
modifications to the behavior of existing rules. The
most direct way of doing this is simply to define a
new, higher priority expansion of the same left-hand
category. Since XLE only allows a single rule for a
given category, the old rule is discarded and the new
one comes into play. The new rule can be arbitrar-
ily different from the STANDARD one, but this is not
typically the case. It is much more common that the
specialized version incorporates most of the behav-
ior of the original, with minor extensions or restric-
tions. One way of producing the modified behavior
is to create a new rule that includes a copy of some
or all of the STANDARD rule?s right side along with
new material, and to give the new definition higher
priority than the old. For example, plurals in the Eu-
reka corpus can be formed by the addition of ?s in-
stead of the usual s, as in (7).
(7) (CAUSE 27416 10) A 7mfd inverter motor ca-
pacitor was installed on an unknown number of
UDH?s.
In order to allow for this, the N rule was rewritten to
allow a PL marker to optionally occur after any N,
as in (8).
(8) N copy of STANDARD N rule
(PL)
As a result of this rule modification, UDH?s in (7)
will have the tree and functional-structure in (9).
(9) a. N
PART PL
UDH ?s
b.
PRED UDH
NUM pl
Copying material from one version to another is
perhaps reasonable for relatively stable and simple
rules, like the N rule, but this can cause maintainabil-
ity problems with complicated rules in the STAN-
DARD grammar that are updated frequently. An al-
ternative strategy is to move the body of the STAN-
DARD N rule to a different rule, e.g., Nbody, which
in turn is called by the N rule in both the STANDARD
and EUREKA grammars. The Nbody category can be
supressed in the tree structure by invoking this rule
as a macro (notationally indicated as @Nbody).
(10) N @Nbody (PL).
Often the necessary modification can be made
simply by redefining a macro that existing rules al-
ready invoke. Consider the ROOT rule, in (11).
(11) ROOT @DECL-BODY @DECL-PUNCT
@INT-BODY @INT-PUNCT
@HEADER .
In the STANDARD grammar, the DECL-PUNCT
macro is defined as in (12a). However, this must
be modified in the EUREKA grammar because the
punctuation is much sloppier and often does not
occur at all; the EUREKA version is shown in (12b).
(12) a. DECL-PUNCT = PERIOD
EXCL-POINT .
b. DECL-PUNCT = ( PERIOD
EXCL-POINT
COLON
SEMI-COLON ).
The modular specifications that macros and tem-
plates provide allow rule behavior to be modified
without having to copy the parts of the rule that do
not change.
XLE also has a mechanism for systemati-
cally modifying the behavior of all rules: the
METARULEMACRO. For example, in order to
parse labeled bracketed input, as in (2b), the WSJ
grammar was altered so that constituents could
optionally be surrounded by the appropriately
labeled brackets. The METARULEMACRO is applied
to each rule in the grammar and produces as output
a modified version of that rule. This is used in
the STANDARD grammar for coordination and to
allow quote marks to surround any constituent. The
METARULEMACRO is redefined for the WSJ to add
the labeled bracketing possibilities for each rule, as
shown in (13).
(13) METARULEMACRO( CAT BASECAT RHS) =
LSB LABEL[ BASECAT] CAT RSB
copy of STANDARD coordination
copy of STANDARD surrounding quote .
The CAT, BASECAT, and RHS are arguments to
the METARULEMACRO that are instantiated to dif-
ferent values for each rule. RHS is instantiated to
the right-hand side of the rule, i.e., the rule expan-
sion. CAT and BASECAT are two ways of repre-
senting the left-hand side of the rule. For simple cat-
egories the CAT and BASECAT are the same (e.g.
NP for the NP rule). XLE also allows for complex
category symbols to specialize the expansion of par-
ticular categories in particular contexts. For exam-
ple, the VP rule is parameterized for the form of its
complement and its own form, so that VP[perf,fin]
is one of the complex VP categories. When the
METARULEMACRO applies to rules with complex
left-side categories, CAT refers to the category in-
cluding the parameters and the BASECAT refers to
the category without the parameters. For the VP ex-
ample, CAT is VP[perf,fin] and BASECAT is VP.
In the definition in (13), LSB and RSB parse the
brackets themselves, while the LABEL[ BASECAT]
parses the label in the bracketing and matches it to
the label in the tree (NP in (2b)); the consituent itself
is the CAT. Thus, a label-bracketed NP is assigned
the structure in (14).
(14) NP
LSB LABEL[NP] NP RSB
[ NP-SBJ Kirk Horse ]
These examples illustrate how the prioritized re-
definition of rules and macros has enabled us to in-
corporate the STANDARD rules in grammars that are
tuned to the special properties of the EUREKA and
WSJ corpora.
2.2 Lexical Entries
Just as for rules, XLE?s override conventions make
it possible to: add new lexical items or new part-of-
speech subentries for existing lexical items; delete
lexical items; and modify lexical items. In addition
to the basic priority overrides, XLE provides for
?edit lexical entries? (Kaplan and Newman, 1997)
that give finer control over the construction of the
lexicon. Edit entries were introduced as a way of rec-
onciling information from lexical databases of vary-
ing degrees of quality, but they are also helpful in
tailoring a STANDARD lexicon to a specialized cor-
pus. When working on specialized corpora, such as
the Eureka corpus, modifications to the lexicon are
extremely important for correctly handling techni-
cal terminology and eliminating word senses that are
not appropriate for the domain.
Higher-priority edit lexical entries provide for op-
erators that modify the definitions found in lower-
priority entries. The operators can: add a subentry
(+); delete a subentry ( ); replace a subentry (!);
or retain existing subentries (=). For example, the
STANDARD grammar might have an entry for button
as in (15).
(15) button !V @(V-SUBJ-OBJ %stem);
!N @(NOUN %stem);
ETC.
However, the EUREKA grammar might not need the
V entry but might require a special partname N en-
try. Assuming that the EUREKA lexicons are given
priority over the STANDARD lexicons, the entry in
(16) would accomplish this.
(16) button V ;
+N @(PARTNAME %stem);
ETC.
Note that the lexical entries in (15) and (16) end with
ETC. This is also part of the edit lexical entry sys-
tem. It indicates that other lower-priority definitions
of that lexical item will be retained in addition to
the new entries. For example, if in another EUREKA
lexicon there was an adjective entry for button with
ETC, the V, N, and A entries would all be used. The
alternative to ETC is ONLY which indicates that only
the new entry is to be used. In our button example, if
an adjective entry was added with ONLY, the V and
N entries would be removed, assuming that the ad-
jective entry occurred in the highest priority lexicon.
This machinery provides a powerful tool for build-
ing specialized lexicons without having to alter the
STANDARD lexicons.
The EUREKA corpus contains a large number of
names of copier parts. Due to their particular syn-
tax and to post-syntactic processing requirements, a
special lexical entry is added for each part name. In
addition, the regular noun parse of these entries is
deleted because whenever they occur in the corpus
they are part names. A sample lexical is shown in
(17); the ? is the escape character for the space.
(17) separator? finger
!PART-NAME @(PART-NAME %stem);
N;
ETC.
The first line in (17) states that separator finger can
be a PART NAME and when it is, it calls a template
PART-NAME that provides relevant information for
the functional-structure. The second line removes
the N entry, if any, as signalled by the before the
category name.
Because of the non-context free nature of Lexical
Functional Grammar, it sometimes happens that ex-
tensions in one part of the grammar require a cor-
responding adjustment in other rules or lexical en-
tries. Consider again the EUREKA ?s plurals. The
part-name UDH is singular when it appears with-
out the ?s and thus the morphological tag +Sg is ap-
pended to it. In the STANDARD grammar, the tag +Sg
has a lexical entry as in (18a) which states that +Sg is
of category NNUM and assigns sg to its NUM. How-
ever, if this is used in the EUREKA grammar, the sg
NUM specification will clash with the pl NUM spec-
ification when UDH appears with ?s, as seen in (7).
Thus, a new entry for +Sg is needed which has sg
as a default value, as in (18b). The first line of (18b)
states that NUM must exist but does not specify a
value, while the second line optionally supplies a sg
value to NUM; when the ?s is used, this option does
not apply since the form already has a pl NUM value.
(18) a. +Sg NNUM ( NUM)=sg
b. +Sg NNUM ( NUM)
(( NUM)=sg)
3 Tokenizing and Morphological Analysis
Tokenization and morphological analysis in XLE
are carried out by means of finite state transductions.
The STANDARD tokenizing transducer encodes the
punctuation conventions of normal English text,
which is adequate for many applications. However,
the Eureka and WSJ corpora include strings that must
be tokenized in non-standard ways. The Eureka part
identifiers have internal punctuation that would nor-
mally cause a string to be broken up (e.g. the hyphen
in PL1-B7), and the WSJ corpus is marked up with
labeled brackets and part-of-speech tags that must
also receive special treatment. An example of the
WSJ mark-up is seen in (19).
(19) [NP-SBJ Lloyd?s, once a pillar of the world
insurance market,] is/VBZ being/VBG
shaken/VBN to its very foundation.
Part-of-speech tags appear in a distinctive format,
beginning with a / and ending with a , with the in-
tervening material indicating the content of the tag
(VBZ for finite 3rd singular verb, VBG for a progres-
sive, VBN for a passive, etc.). The tokenizing trans-
ducer must recognize this pattern and split the tags
off as separate tokens. The tag-tokens must be avail-
able to filter the output of the morphological ana-
lyzer so that only verbal forms are compatible with
the tags in this example and the adjectival reading of
shaken is therefore blocked.
XLE tokenizing transducers are compiled from
specifications expressed in the sophisticated Xerox
finite state calculus (Beesley and Karttunen, 2002).
The Xerox calculus includes the composition, ig-
nore, and substitution operator discussed by Kaplan
and Kay (1994) and the priority-union operator of
Kaplan and Newman (1997). The specialized tok-
enizers are constructed by using these operators to
combine the STANDARD specification with expres-
sions that extend or restrict the standard behavior.
For example, the ignore operator is applied to allow
the part-of-speech information to be passed through
to the morphology without interrupting the standard
patterns of English punctuation.
XLE also allows separately compiled transduc-
ers to be combined at run-time by the operations
of priority-union, composition, and union. Priority-
union was used to supplement the standard morphol-
ogy with specialized ?guessing? transducers that ap-
ply only to tokens that would otherwise be unrec-
ognized. Thus, a finite-state guesser was added to
identify Eureka fault numbers (09-425), adjustment
numbers (12-23), part numbers (606K2100), part list
numbers (PL1-B7), repair numbers (2.4), tag num-
bers (P-102), and diagnostic code numbers (dC131).
Composition was used to apply the part-of-speech
filtering transducer to the output of the morpholog-
ical analyzer, and union provided an easy way of
adding new, corpus-specific terminology.
4 Optimality Marks
XLE supports a version of Optimality Theory (OT)
(Prince and Smolensky, 1993) which is used to rank
an analysis relative to other possible analyses (Frank
et al, 2001). In general, this is used within a specific
grammar to prefer or disprefer a construction. How-
ever, it can also be used in grammar extensions to
delete or include rules or parts of rules.
The XLE implementation of OT works as fol-
lows.1 OT marks are placed in the grammar and are
associated with particular rules, parts of rules, or
lexical entries. These marks are then ranked in the
grammar CONFIGURATION. In addition to a simple
ranking of constraints which states that a construc-
tion with a given OT mark is (dis)prefered to one
1The actual XLE OT implementation is more complicated
than this, allowing for UNGRAMMATICAL and STOPPOINT
marks as well. Only OT marks that are associated with NO-
GOOD are of interest here. For a full description, see (Frank et
al., 2001).
without it, XLE allows the marks to be specified as
NOGOOD. A rule or rule disjunct which has a NO-
GOOD OT mark associated with it will be ignored
by XLE. This can be used for grammar extensions
in that it allows a standard grammar to anticipate the
variations required by special corpora without using
them in normal circumstances.
Consider the example of the EUREKA ?s plurals
discussed in section 2.1. Instead of rewriting the N
rule in the EUREKA grammar, it would be possible
to modify it in the STANDARD grammar and include
an OT mark, as in (20).
(20) N original STANDARD N rules
(PL: @(OT-MARK EUR-PLURAL)).
The CONFIGURATION files of the STANDARD and
EUREKA grammars would differ in that the STAN-
DARD grammar would rank the EUR-PLURAL OT
mark as NOGOOD, as in (21a), while the EUREKA
grammar would simply not rank the mark, as in
(21b).
(21) a. STANDARD optimality order:
EUR-PLURAL NOGOOD
b. EUREKA optimality order:
NOGOOD
Given the OT marks, it would be possible to have
one large grammar that is specialized by different
OT rankings to produce the STANDARD, EUREKA,
and WSJ variants. However, from a grammar writ-
ing perspective this is not a desirable solution be-
cause it becomes difficult to keep track of which
constructions belong to standard English and are
shared among all the specializations and which are
corpus-specific. In addition, it does not distinguish a
core set of slowly changing linguistic specifications
for the basic patterns of the language, and thus does
not provide a stable foundation that the writers of
more specialized grammars can rely on.
5 Maintenance with Grammar Extensions
Maintenance is a serious issue for any large-scale
grammar development activity, and the maintenance
problems are compounded when multiple versions
are being created perhaps by several different gram-
mar writers. Our STANDARD grammar is now quite
mature and covers all the linguistically significant
constructions and most other constructions that we
have encountered in previous corpus analysis. How-
ever, every now and then, a new corpus, even a spe-
cialized one, will evidence a standard construction
that has not previously been accounted for. If spe-
cialized grammars were written by copying all the
STANDARD files and then modifying them, the im-
plementation of new standard constructions would
tend to appear only in the specialized grammar. Our
techniques for minimizing the amount of copying
encourages us to implement new constructions in the
STANDARD grammar and this makes them available
to all other specializations.
If a new version of a rule for a specialized gram-
mar is created by copying the corresponding STAN-
DARD rule, changes later made to the special rule
will not automatically be reflected in the STANDARD
grammar, and vice versa. This is the desired behav-
ior when adding unusual, corpus-specific construc-
tions. However, if the non-corpus specific parts of
the new rule are modified, these modifications will
not migrate to the STANDARD grammar. To avoid
this problem, the smallest rule possible should be
modified in the specialized grammar, e.g., modify-
ing the N head rule instead of the entire NP. For
this reason, having highly modularized rules and us-
ing macros and templates helps in grammar mainte-
nance both within a grammar and across specialized
grammar extensions.
As seen above, the XLE grammar development
platform provides a number of mechanisms to allow
for grammar extensions without altering the core
(STANDARD) grammar. However, there are still ar-
eas that could use improvement. For example, as
mentioned in section 2, the CONFIGURATION file
states which other files the grammar includes and
how they are prioritized. The CONFIGURATION con-
tains other information such as declarations of the
governable grammatical functions, the distributive
features, etc. As this information rarely changes
with grammar extensions, it would be helpful for
an extension configuration to incorporate by refer-
ence such additional parameters of the STANDARD
configuration. Currently these declarations must be
copied into each CONFIGURATION.
6 Discussion and Conclusion
As a result of the strategies and notational devices
outlined above, our specialized grammars share
substantial portions of the pre-existing STANDARD
grammar. The statistics in table (22) give an indica-
tion of the size of the STANDARD grammar and of
the additional material required for the EUREKA and
WSJ specializations. As can be seen from this table,
the specialized grammars require a relatively small
number of rules compared to the rules in the STAN-
DARD grammar. The number of lines that the rules
and lexical entries take up also provides a measure of
the relative size of the specifications. The WSJ lexi-
cons include many titles and proper nouns that may
ultimately be moved to the STANDARD files. The ta-
ble also shows the number of files called by the CON-
FIGURATION, as another indication of the size of the
specifications. This number is somewhat arbitrary as
separate files can be combined into a single multi-
sectioned file, although this is likely to reduce main-
tainability and readability.
(22)
STANDARD EUREKA WSJ
rules 310 32 14
lines:
rules 6,539 425 894
lexicons 44,879 5,565 15,135
files 14 5 8
The grammars compile into a collection of finite-
state machines with the number of states and arcs
listed in table (23). The WSJ grammar compiles into
the largest data structures, mainly because of its abil-
ity to parse labeled bracketed strings and part-of-
speech tags, (2b). This size increase is the result of
adding one disjunct in the METARULEMACRO and
hence reflects only a minor grammar change.
(23)
STANDARD EUREKA WSJ
states 4,935 5,132 8,759
arcs 13,268 13,639 19,695
In sum, the grammar specialization system used
in XLE has been quite sucessful in developing cor-
pus specific grammars using the STANDARD English
grammar as a basis. A significant benefit comes from
being able to distinguish truly unusual constructions
that exist only in the specialized grammar from those
that are (or should be) in the STANDARD grammar.
This allows idiosyncratic information to remain in a
specialized grammar while all the specialized gram-
mars benefit from and contribute to the continuing
development of the STANDARD grammar.
References
K. Beesley and L. Karttunen. 2002. Finite-State
Morphology: Xerox Tools and Techniques. Cam-
bridge University Press. To Appear.
M. Butt, T.H. King, M.-E. Nin?o, and F. Segond.
1999. A Grammar Writer?s Cookbook. CSLI
Publications, Stanford, CA.
M. Butt, H. Dyvik, T.H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project.
In Proceedings of COLING 2002. Workshop on
Grammar Engineering and Evaluation.
J. Everett, D. Bobrow, R. Stolle, R. Crouch,
V. de Paiva, C. Condoravdi, M. van den Berg,
and L. Polanyi. 2001. Making ontologies work
for resolving redundancies across documents.
Communications of the ACM, 45:55?60.
A. Frank, T. H. King, J. Kuhn, and J. T. Maxwell III.
2001. Optimality theory style constraint rank-
ing in large-scale LFG grammars. In Peter Sells,
editor, Formal and Empirical Issues in Optimal-
ity Theoretic Syntax. CSLI Publications, Stanford,
CA.
R. Kaplan and M. Kay. 1994. Regular models of
phonological rule systems. Computational Lin-
guistics, 20:331?378.
R. Kaplan and J. Maxwell. 1996. LFG Gram-
mar Writer?s Workbench. System documentation
manual; available on-line at PARC.
R. Kaplan and P. Newman. 1997. Lexical resource
conciliation in the Xerox Linguistic Environment.
In Proceedings of the ACL Workshop on Com-
putational Environments for Grammar Develop-
ment and Engineering.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. Mac-
Intyre, A. Bies, M. Ferguson, K. Katz, and
B. Schasberger. 1994. The Penn treebank: An-
notative predicate argument structure. In ARPA
Human Language Technology Workshop.
J. Maxwell and R. Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Compu-
tational Lingusitics, 19:571?589.
A. Prince and P. Smolensky. 1993. Optimality the-
ory: Constraint interaction in generative gram-
mar. RuCCS Technical Report #2, Rutgers Uni-
versity.
S. Riezler, T.H. King, R. Kaplan, D. Crouch, J. T.
Maxwell, III, and M. Johnson. 2002. Parsing
the Wall Street Journal using a lexical-functional
grammar and discriminative estimation tech-
niques. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics,
University of Pennsylvania.
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 65?72,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Pruning the Search Space of a Hand-Crafted Parsing System with a
Probabilistic Parser
Aoife Cahill
Dublin City University
acahill@computing.dcu.ie
Tracy Holloway King
PARC
thking@parc.com
John T. Maxwell III
PARC
maxwell@parc.com
Abstract
The demand for deep linguistic analysis
for huge volumes of data means that it is
increasingly important that the time taken
to parse such data is minimized. In the
XLE parsing model which is a hand-crafted,
unification-based parsing system, most of
the time is spent on unification, searching
for valid f-structures (dependency attribute-
value matrices) within the space of the many
valid c-structures (phrase structure trees).
We carried out an experiment to determine
whether pruning the search space at an ear-
lier stage of the parsing process results in
an improvement in the overall time taken to
parse, while maintaining the quality of the
f-structures produced. We retrained a state-
of-the-art probabilistic parser and used it to
pre-bracket input to the XLE, constraining
the valid c-structure space for each sentence.
We evaluated against the PARC 700 Depen-
dency Bank and show that it is possible to
decrease the time taken to parse by ?18%
while maintaining accuracy.
1 Introduction
When deep linguistic analysis of massive data is re-
quired (e.g. processing Wikipedia), it is crucial that
the parsing time be minimized. The XLE English
parsing system is a large-scale, hand-crafted, deep,
unification-based system that processes raw text
and produces both constituent-structures (phrase
structure trees) and feature-structures (dependency
attribute-value matrices). A typical breakdown of
parsing time of XLE components is Morphology
(1.6%), Chart (5.8%) and Unifier (92.6%).
The unification process is the bottleneck in the
XLE parsing system. The grammar generates many
valid c-structure trees for a particular sentence: the
Unifier then processes all of these trees (as packed
structures), and a log-linear disambiguation module
can choose the most probable f-structure from the
resulting valid f-structures. For example, the sen-
tence ?Growth is slower.? has 84 valid c-structure
trees according to the current English grammar;1
however once the Unifier has processed all of these
trees (in a packed form), only one c-structure and
f-structure pair is valid (see Figure 1). In this in-
stance, the log-linear disambiguation does not need
to choose the most probable result.
The research question we pose is whether the
search space can be pruned earlier before unifi-
cation takes place. Bangalore and Joshi (1999),
Clark and Curran (2004) and Matsuzaki et al (2007)
show that by using a super tagger before (CCG and
HPSG) parsing, the space required for discrimini-
tive training is drastically reduced. Supertagging
is not widely used within the LFG framework, al-
though there has been some work on using hypertags
(Kinyon, 2000). Ninomiya et al (2006) propose a
method for faster HPSG parsing while maintaining
accuracy by only using the probabilities of lexical
entry selections (i.e. the supertags) in their discrim-
initive model. In the work presented here, we con-
1For example, is can be a copula, a progressive auxiliary or
a passive auxiliary, while slower can either be an adjective or an
adverb.
65
centrate on reducing the number of c-structure trees
that the Unifier has to process, ideally to one tree.
The hope was that this would speed up the parsing
process, but how would it affect the quality of the f-
structures? This is similar to the approach taken by
Cahill et al (2005) who do not use a hand-crafted
complete unification system (rather an automatically
acquired probabilistic approximation). They parse
raw text into LFG f-structures by first parsing with a
probabilistic CFG parser to choose the most proba-
ble c-structure. This is then passed to an automatic
f-structure annotation algorithm which deterministi-
cally generates one f-structure for that tree.
The most compact way of doing this would be to
integrate a statistical component to the parser that
could rank the c-structure trees and only pass the
most likely forward to the unification process. How-
ever, this would require a large rewrite of the sys-
tem. So, we first wanted to investigate a ?cheaper?
alternative to determine the viability of the pruning
strategy; this is the experiment reported in this pa-
per. This is implemented by stipulating constituent
boundaries in the input string, so that any c-structure
that is incompatible with these constraints is invalid
and will not be processed by the Unifier. This was
done to some extent in Riezler et al (2002) to au-
tomatically generate training data for the log-linear
disambiguation component of XLE. Previous work
obtained the constituent constraints (i.e. brackets)
from the gold-standard trees in the Penn-II Tree-
bank. However, to parse novel text, gold-standard
trees are unavailable.
We used a state-of-the-art probabilistic parser to
provide the bracketing constraints to XLE. These
parsers are accurate (achieving accuracy of over
90% on Section 23 WSJ text), fast, and robust.
The idea is that pre-parsing of the input text by a
fast and accurate parser can prune the c-structure
search space, reducing the amount of work done by
the Unifier, speed up parsing and maintain the high
quality of the f-structures produced.
The structure of this paper is as follows: Section
2 introduces the XLE parsing system. Section 3 de-
scribes a baseline experiment and based on the re-
sults suggests retraining the Bikel parser to improve
results (Section 4). Section 5 describes experiments
on the development set, from which we evaluate the
most successful system against the PARC 700 test
CS 1: ROOT
Sadj[fin]
S[fin]
NP
NPadj
NPzero
N
^ growth
VPall[fin]
VPcop[fin]
Vcop[fin]
is
AP[pred]
A
slower
PERIOD
.
"Growth is slower."
'be<[68:slow]>[23:growth]'PRED
'growth'PRED23SUBJ
'slow<[23:growth]>'PRED
[23:growth]SUBJ
'more'PRED-1ADJUNCT68
XCOMP
47
Figure 1: C- and F-Structure for ?Growth is slower.?
set (Section 6). Finally, Section 7 concludes.
2 Background
In this section we introduce Lexical Functional
Grammar, the grammar formalism underlying the
XLE, and briefly describe the XLE parsing system.
2.1 Lexical Functional Grammar
Lexical Functional Grammar (LFG) (Kaplan and
Bresnan, 1982) is a constraint-based theory of gram-
mar. It (minimally) posits two levels of repre-
sentation, c(onstituent)-structure and f(unctional)-
structure. C-structure is represented by context-
free phrase-structure trees, and captures surface
grammatical configurations such as word order.
The nodes in the trees are annotated with func-
tional equations (attribute-value structure con-
straints) which are resolved to produce an f-
structure. F-structures are recursive attribute-value
matrices, representing abstract syntactic functions.
F-structures approximate basic predicate-argument-
adjunct structures or dependency relations. Fig-
ure 1 shows the c- and f-structure for the sentence
?Growth is slower.?.
66
Parser Output: (S1 (S (NP (NN Growth)) (VP (AUX is) (ADJP (JJR slower))) (. .)))
Labeled: \[S1 \[S Growth \[VP is \[ADJP slower\] \].\] \]
Unlabeled:\[ \[ Growth \[ is \[ slower\] \].\] \]
Figure 2: Example of retained brackets from parser output to constrain the XLE parser
2.2 The XLE Parsing System
The XLE parsing system is a deep-grammar-based
parsing system. The experiments reported in this
paper use the English LFG grammar constructed
as part of the ParGram project (Butt et al, 2002).
This system incorporates sophisticated ambiguity-
management technology so that all possible syn-
tactic analyses of a sentence are computed in
an efficient, packed representation (Maxwell and
Kaplan, 1993). In accordance with LFG the-
ory, the output includes not only standard context-
free phrase-structure trees (c-structures) but also
attribute-value matrices (f-structures) that explic-
itly encode predicate-argument relations and other
meaningful properties. The f-structures can be de-
terministically mapped to dependency triples with-
out any loss of information, using the built-in or-
dered rewrite system (Crouch et al, 2002). XLE se-
lects the most probable analysis from the potentially
large candidate set by means of a stochastic disam-
biguation component based on a log-linear proba-
bility model (Riezler et al, 2002) that works on the
packed representations. The underlying parsing sys-
tem also has built-in robustness mechanisms that al-
low it to parse strings that are outside the scope of
the grammar as a list of fewest well-formed ?frag-
ments?. Furthermore, performance parameters that
bound parsing and disambiguation can be tuned for
efficient but accurate operation. These parameters
include at which point to timeout and return an error,
the amount of stack memory to allocate, the num-
ber of new edges to add to the chart and at which
point to start skimming (a process that guarantees
XLE will finish processing a sentence in polynomial
time by only carrying out a bounded amount of work
on each remaining constituent after a time threshold
has passed). For the experiments reported here, we
did not fine-tune these parameters due to time con-
straints; so default values were arbitrarily set and the
same values used for all parsing experiments.
3 Baseline experiments
We carried out a baseline experiment with two
state-of-the-art parsers to establish what effect pre-
bracketing the input to the XLE system has on the
quality and number of the solutions produced. We
used the Bikel () multi-threaded, head-driven chart-
parsing engine developed at the University of Penn-
sylvania. The second parser is that described in
Charniak and Johnson (2005). This parser uses a
discriminative reranker that selects the most proba-
ble parse from the 50-best parses returned by a gen-
erative parser based on Charniak (2000).
We evaluated against the PARC 700 Dependency
Bank (King et al, 2003) which provides gold-
standard analyses for 700 sentences chosen at ran-
dom from Section 23 of the Penn-II Treebank. The
Dependency Bank was bootstrapped by parsing the
700 sentences with the XLE English grammar, and
then manually correcting the output. The data is di-
vided into two sets, a 140-sentence development set
and a test set of 560 sentences (Kaplan et al, 2004).
We took the raw strings from the 140-sentence
development set and parsed them with each of the
state-of-the-art probabilistic parsers. As an upper
bound for the baseline experiment, we use the brack-
ets in the original Penn-II treebank trees for the 140
development set.
We then used the brackets from each parser out-
put (or original treebank trees) to constrain the XLE
parser. If the input to the XLE parser is bracketed,
the parser will only generate c-structures that respect
these brackets (i.e., only c-structures with brackets
that are compatible with the input brackets are con-
sidered during the unification stage). Figure 2 gives
an example of retained brackets from the parser out-
put. We do not retain brackets around PRN (paren-
thetical phrase) or NP nodes as their structure often
differed too much from XLE analyses of the same
phrases. We passed pre-bracketed strings to the XLE
and evaluated the output f-structures in terms of de-
pendency triples against the 140-sentence subset of
67
Non-Fragment Fragment
Penn-XLE Penn-XLE Penn-XLE Penn-XLE
(lab.) (unlab.) (lab.) (unlab.)
Total XLE parses (/140) 0 89 140 140
F-Score of subset 0 84.11 53.92 74.87
Overall F-Score 0 58.91 53.92 74.87
Table 1: Upper-bound results for original Penn-II trees
Non-Fragment Fragment
XLE Bikel-XLE Bikel-XLE XLE Bikel-XLE Bikel-XLE
(lab.) (unlab.) (lab.) (unlab.)
Total XLE Parses (/140) 119 0 84 135 140 140
F-Score of Subset 81.57 0 84.23 78.72 54.37 73.71
Overall F-Score 72.01 0 55.06 76.13 54.37 *73.71
XLE CJ-XLE CJ-XLE XLE CJ-XLE CJ-XLE
(lab.) (unlab.) (lab.) (unlab.)
Total XLE Parses (/140) 119 0 86 135 139 139
F-Score of Subset 81.57 0 86.57 78.72 53.96 75.64
Overall F-Score 72.01 0 58.04 76.13 53.48 *74.98
Table 2: Bikel (2002) and Charniak and Johnson (2005) out-of-the-box baseline results
the PARC 700 Dependency Bank.
The results of the baseline experiments are given
in Tables 1 and 2. Table 1 gives the upper bound
results if we use the gold standard Penn treebank
to bracket the input to XLE. Table 2 compares the
XLE (fragment and non-fragment) grammar to the
system where the input is pre-parsed by each parser.
XLE fragment grammars provide a back-off when
parsing fails: the grammar is relaxed and the parser
builds a fragment parse of the well-formed chunks.
We compare the parsers in terms of total number
of parses (out of 140) and the f-score of the sub-
set of sentences successfully parsed. We also com-
bine these scores to give an overall f-score, where
the system scores 0 for each sentence it could not
parse. When testing for statistical significance be-
tween systems, we compare the overall f-score val-
ues. Figures marked with an asterisk are not statisti-
cally significantly different at the 95% level.2
The results show that using unlabeled brackets
achieves reasonable f-scores with the non-fragment
grammar. Using the labeled bracketing from the out-
put of both parsers causes XLE to always fail when
parsing. This is because the labels in the output of
parsers trained on the Penn-II treebank differ con-
siderably from the labels on c-structure trees pro-
2We use the approximate randomization test (Noreen, 1989)
to test for significance.
duced by XLE. Interestingly, the f-scores for both
the CJ-XLE and Bikel-XLE systems are very sim-
ilar to the upper bounds. The gold standard upper
bound is not as high as expected because the Penn
trees used to produce the gold bracketed input are
not always compatible with the XLE-style trees. As
a simple example, the tree in Figure 1 differs from
the parse tree for the same sentence in the Penn
Treebank (Figure 3). The most obvious difference
is the labels on the nodes. However, even in this
small example, there are structural differences, e.g.
the position of the period. In general, the larger the
tree, the greater the difference in both labeling and
structure between the Penn trees and the XLE-style
trees. Therefore, the next step was to retrain a parser
to produce trees with structures the same as XLE-
style trees and with XLE English grammar labels on
the nodes. For this experiment we use the Bikel ()
parser, as it is more suited to being retrained on a
new treebank annotation scheme.
4 Retraining the Bikel parser
We retrained the Bikel parser so that it produces
trees like those outputted by the XLE parsing sys-
tem (e.g. Figure 1). To do this, we first created a
training corpus, and then modified the parser to deal
with this new data.
Since there is no manually-created treebank of
68
SNP
NN
Growth
VP
VBZ
is
ADJP-PRD
JJR
slower
?
?
Figure 3: Penn Treebank tree for ?Growth is slower.?
XLE-style trees, we created one automatically from
sections 02-21 of the Penn-II Treebank. We took the
raw strings from those sections and marked up NP
and SBAR constituents using the brackets from the
gold standard Penn treebank. The NP constituents
are labeled, and the SBAR unlabeled (i.e. the SBAR
constituents are forced to exist in the XLE parse, but
the label on them is not constrained to be SBAR).
We also tagged verbs, adjectives and nouns, based
on the gold standard POS tags.
We parsed the 39,832 marked-up sentences in the
standard training corpus and used the XLE disam-
biguation module to choose the most probable c-
and f-structure pair for each sentence. Ideally we
would have had an expert choose these. We au-
tomatically extracted the c-structure trees produced
by the XLE and performed some automatic post-
processing.3 This resulted in an automatically cre-
ated training corpus of 27,873 XLE-style trees. The
11,959 missing trees were mainly due to the XLE
parses not being compatible with the bracketed in-
put, but sometimes due to time and memory con-
straints.
Using the automatically-created training corpus
of XLE-style trees, we retrained the Bikel parser on
this data. This required adding a new language mod-
ule (?XLE-English?) to the Bikel parser, and regen-
erating head-finding rules for the XLE-style trees.
5 Experiments
Once we had a retrained version of the Bikel parser
that parses novel text into XLE-style trees, we car-
ried out a number of experiments on our develop-
ment set in order to establish the optimum settings
3The postprocessing included removing morphological in-
formation and the brackets from the original markup.
All Sentences
XLE Bikel-XLE
Non-fragment grammar
Labeled brackets
Total Parsing Time 964 336
Total XLE Parses (/140) 119 77
F-Score of Subset 81.57 86.11
Overall F-Score 72.01 52.84
Non-fragment grammar
Unlabeled brackets
Total Parsing Time 964 380
Total XLE Parses (/140) 119 89
F-Score of Subset 81.57 85.62
Overall F-Score 72.01 59.34
Fragment grammar
Labeled brackets
Total Parsing Time 1143 390
Total XLE Parses (/140) 135 140
F-Score of Subset 78.72 71.86
Overall F-Score 76.13 71.86
Fragment grammar
Unlabeled brackets
Total Parsing Time 1143 423
Total XLE Parses (/140) 135 140
F-Score of Subset 78.72 74.51
Overall F-Score 76.13 *74.51
Table 3: Bikel-XLE Initial Experiments
for the evaluation against the PARC 700 test set.
5.1 Pre-bracketing
We automatically pre-processed the raw strings from
the 140-sentence development set. This made sys-
tematic changes to the tokens so that the retrained
Bikel parser can parse them. The changes included
removing quotes, converting a and an to a, con-
verting n?t to not, etc. We parsed the pre-processed
strings with the new Bikel parser.
We carried out four initial experiments, experi-
menting with both labeled and unlabeled brackets
and XLE fragment and non-fragment grammars. Ta-
ble 3 gives the results for these experiments. We
compare the parsers in terms of time, total number
of parses (out of 140), the f-score of the subset of
sentences successfully parsed and the overall f-score
if the system achieves a score of 0 for all sentences
it does not parse. The time taken for the Bikel-XLE
system includes the time taken for the Bikel parser
to parse the sentences, as well as the time taken for
XLE to process the bracketed input.
Table 3 shows that using the non-fragment gram-
mar, the Bikel-XLE system performs better on the
69
subset of sentences parsed than XLE system alone,
though the results are not statistically significantly
better overall, since the coverage is much lower. The
number of bracketed sentences that can be parsed
by XLE increases if the brackets are unlabeled.
The table also shows that the XLE system performs
much better than Bikel-XLE when using the frag-
ment grammars. Although the Bikel-XLE system is
quite a bit faster, there is a drop in f-score; however
this is not statistically significant when the brackets
are unlabeled.
5.2 Pre-tagging
We performed some error analysis on the output of
the Bikel-XLE system and noticed that a consider-
able number of errors were due to mis-tagging. So,
we pre-tagged the input to the Bikel parser using the
MXPOST tagger (Ratnaparkhi, 1996). The results
for the non-fragment grammars are presented in Ta-
ble 4. Pre-tagging with MXPOST, however, does
not result in a statistically significantly higher re-
sult than parsing untagged input, although more sen-
tences can be parsed by both systems. Pre-tagging
also adds an extra time overhead cost.
No pretags MXPOST tags
XLE Bikel-XLE Bikel-XLE
Unlabeled
Total Parsing Time 964 380 493
# XLE Parses (/140) 119 89 92
F-Score of Subset 81.57 85.62 84.98
Overall F-Score 72.01 59.34 *61.11
Labeled
Total Parsing Time 964 336 407
# XLE Parses (/140) 119 77 80
F-Score of Subset 81.57 86.11 85.87
Overall F-Score 72.01 52.84 *54.91
Table 4: MXPOST pre-tagged, Non-fragment gram-
mar
5.3 Pruning
The Bikel parser can be customized to allow differ-
ent levels of pruning. The above experiments were
carried out using the default level. We carried out
experiments with three levels of pruning.4 The re-
4The default level of pruning starts at 3.5, has a maximum of
4 and relaxes constraints when parsing fails. Level 1 pruning is
the same as the default except the constraints are never relaxed.
Level 2 pruning has a start value of 3.5 and a maximum value
of 3.5. Level 3 pruning has a start and maximum value of 3.
sults are given in Table 5 for the experiment with
labeled brackets and the non-fragment XLE gram-
mar. More pruning generally results in fewer and
lower-quality parses. The biggest gain is with prun-
ing level 1, where the number and quality of brack-
eted sentences that can be parsed with XLE remains
the same as with the default level. This is because
Bikel with pruning level 1 does not relax the con-
straints when parsing fails and does not waste time
parsing sentences that cannot be parsed in bracketed
form by XLE.
Default L1 L2 L3
Total Parsing Time 336 137 137 106
# XLE Parses (/140) 77 77 76 75
F-Score of Subset 86.11 86.11 86.04 85.87
Overall F-Score 52.84 *52.84 *52.43 *52.36
Table 5: Pruning with Non-fragment grammar, La-
beled brackets, Levels default-3
5.4 Hybrid systems
Although pre-parsing with Bikel results in faster
XLE parsing time and high-quality f-structures
(when examining only the quality of the sentences
that can be parsed by the Bikel-XLE system), the
coverage of this system remains poor, therefore the
overall f-score remains poor. One solution is to build
a hybrid two-pass system. During the first pass all
sentences are pre-parsed by Bikel and the bracketed
output is parsed by the XLE non-fragment gram-
mar. In the second pass, the sentences that were
not parsed during the first pass are parsed with the
XLE fragment grammar. We carried out a number
of experiments with hybrid systems and the results
are given in Table 6.
The results show that again labeled brackets re-
sult in a statistically significant increase in f-score,
although the time taken is almost the same as the
XLE fragment grammar alone. Coverage increases
by 1 sentence. Using unlabeled brackets results in
3 additional sentences receiving parses, and parsing
time is improved by ?12%; however the increase in
f-score is not statistically significant.
Table 7 gives the results for hybrid systems with
pruning using labeled brackets. The more pruning
that the Bikel parser does, the faster the system,
but the quality of the f-structures begins to deteri-
70
XLE Bikel-XLE hybrid Bikel-XLE hybrid
(frag) (labeled) (unlabeled)
Total Parsing Time 1143 1121 1001
Total XLE Parses (/140) 135 136 138
F-Score of Subset 78.72 79.85 79.51
Overall F-Score 76.13 77.61 *78.28
Table 6: Hybrid systems compared to the XLE fragment grammar alone
XLE Bikel-XLE hybrid Bikel-XLE hybrid Bikel-XLE hybrid
(frag) (level 1) (level 2) (level 3)
Total Parsing Time 1143 918 920 885
Total XLE Parses (/140) 135 136 136 136
F-Score of Subset 78.72 79.85 79.79 79.76
Overall F-Score 76.13 77.61 77.55 77.53
Table 7: Hybrid systems with pruning compared to the XLE fragment grammar alone
orate. The best system is the Bikel-XLE hybrid sys-
tem with labeled brackets and pruning level 1. This
system achieves a statistically significant increase in
f-score over the XLE fragment grammar alone, de-
creases the time taken to parse by almost 20% and
increases coverage by 1 sentence. Therefore, we
chose this system to perform our final evaluation
against the PARC 700 Dependency Bank.
6 Evaluation against the PARC 700
We evaluated the system that performs best on the
development set against the 560-sentence test set of
the PARC 700 Dependency Bank. The results are
given in Table 8. The hybrid system achieves an
18% decrease in parsing time, a slight improvement
in coverage of 0.9%, and a 1.12% improvement in
overall f-structure quality.
XLE Bikel-XLE hybrid
(frag) (labeled, prune 1)
Total Parsing Time 4967 4077
Total XLE Parses (/560) 537 542
F-Score of Subset 80.13 80.63
Overall F-Score 77.04 78.16
Table 8: PARC 700 evaluation of the Hybrid system
compared to the XLE fragment grammar alone
7 Conclusions
We successfully used a state-of-the-art probabilistic
parser in combination with a hand-crafted system to
improve parsing time while maintaining the quality
of the output produced. Our hybrid system consists
of two phases. During phase one, pre-processed, to-
kenized text is parsed with a retrained Bikel parser.
We use the labeled brackets in the output to constrain
the c-structures generated by the XLE parsing sys-
tem. In the second phase, we use the XLE fragment
grammar to parse any remaining sentences that have
not received a parse in the first phase.
Given the slight increase in overall f-score per-
formance, the speed up in parsing time (?18%) can
justify more complicated processing architecture for
some applications.5 The main disadvantage of the
current system is that the input to the Bikel parser
needs to be tokenized, whereas XLE processes raw
text. One solution to this is to use a state-of-the-art
probabilistic parser that accepts untokenized input
(such as Charniak and Johnson, 2005) and retrain it
as described in Section 4.
Kaplan et al (2004) compared time and accuracy
of a version of the Collins parser tuned to maximize
speed and accuracy to an earlier version of the XLE
parser. Although the XLE parser was more accu-
rate, the parsing time was a factor of 1.49 slower
(time converting Collins trees to dependencies was
not counted in the parse time; time to produce f-
structures from c-structures was counted in the XLE
parse time). The hybrid system here narrows the
speed gap while maintaining greater accuracy.
The original hope behind using the brackets to
constrain the XLE c-structure generation was that
5For example, in massive data applications, if the parsing
task takes 30 days, reducing this by 18% saves more than 5
days.
71
the brackets would force the XLE to choose only
one tree. However, the brackets were sometimes
ambiguous, and sometimes more than one valid tree
was found. In the final evaluation against the PARC
700 test set, the average number of optimal solutions
was 4.05; so the log-linear disambiguation mod-
ule still had to chose the most probable f-structure.
However, this is considerably less to choose from
than the average of 341 optimal solutions produced
by the XLE fragment grammar for the same sen-
tences when unbracketed.
Based on the results of this experiment we have
integrated a statistical component into the XLE
parser itself. With this architecture the packed c-
structure trees are pruned before unification with-
out needing to preprocess the input text. The XLE
c-structure pruning results in a ?30% reduction in
parse time on the Wikipedia with little loss in preci-
sion. We hope to report on this in the near future.
Acknowledgments
The research in this paper was partly funded by Sci-
ence Foundation Ireland grant 04/BR/CS0370.
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to alsmost parsing. Com-
putational Linguistics, 25(2):237?265.
Dan Bikel. Design of a Multi-lingual, Parallel-processing
Statistical Parsing Engine. In Proceedings of HLT,
YEAR = 2002, pages = 24?27, address = San Diego,
CA,.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The Par-
allel Grammar Project. In Proceedings of Workshop
on Grammar Engineering and Evaluation, pages 1?7,
Taiwan.
Aoife Cahill, Martin Forst, Michael Burke, Mairead Mc-
Carthy, Ruth O?Donovan, Christian Rohrer, Josef van
Genabith, and Andy Way. 2005. Treebank-based
acquisition of multilingual unification grammar re-
sources. Journal of Research on Language and Com-
putation, pages 247?279.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL, pages 173?180,
Ann Arbor, Michigan.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139,
Seattle, WA.
Stephen Clark and James R. Curran. 2004. The Impor-
tance of Supertagging for Wide-Coverage CCG Pars-
ing . In Proceedings of COLING, pages 282?288,
Geneva, Switzerland, Aug 23?Aug 27. COLING.
Richard Crouch, Ron Kaplan, Tracy Holloway King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad coverage parser. In Proceedings of
the LREC Workshop: Beyond PARSEVAL, pages 67?
74, Las Palmas, Canary Islands, Spain.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar, a Formal System for Grammatical Repre-
sentation. In Joan Bresnan, editor, The Mental Repre-
sentation of Grammatical Relations, pages 173?281.
MIT Press, Cambridge, MA.
Ron Kaplan, Stefan Riezler, Tracy Holloway King,
John T. Maxwell, Alexander Vasserman, and Richard
Crouch. 2004. Speed and Accuracy in Shallow and
Deep Stochastic Parsing. In Proceedings of HLT-
NAACL, pages 97?104, Boston, MA.
Tracy Holloway King, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ron Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of LINC, pages
1?8, Budapest, Hungary.
Alexandra Kinyon. 2000. Hypertags. In Proceedings of
COLING, pages 446?452, Saarbru?cken.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient HPSG Parsing with Supertagging and
CFG-filtering. In Proceedings of IJCAI, pages 1671?
1676, India.
John T. Maxwell and Ronald M. Kaplan. 1993. The
interface between phrasal and functional constraints.
Computational Linguistics, 19(4):571?590.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely Lexicalized Models for Accurate and Fast
HPSG Parsing. In Proceedings of EMNLP, pages
155?163, Australia.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses: An Introduction. Wiley, New
York.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
Of-Speech Tagger. In Proceedings of EMNLP, pages
133?142, Philadelphia, PA.
Stefan Riezler, Tracy King, Ronald Kaplan, Richard
Crouch, John T. Maxwell, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-
Functional Grammar and Discriminative Estimation
Techniques. In Proceedings of ACL, pages 271?278,
Philadelphia, PA.
72
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 16?21,
Prague, June 2007. c?2007 Association for Computational Linguistics
Precision-focused Textual Inference
D. G. Bobrow, C. Condoravdi, R. Crouch, V. de Paiva, L. Karttunen, T. H. King, R. Nairn, L. Price, A. Zaenen
Palo Alto Research Center
Abstract
This paper describes our system as used in
the RTE3 task. The system maps premise and
hypothesis pairs into an abstract knowledge
representation (AKR) and then performs en-
tailment and contradiction detection (ECD)
on the resulting AKRs. Two versions of ECD
were used in RTE3, one with strict ECD and
one with looser ECD.
1 Introduction
In the RTE textual entailment challenge, one is given
a source text T and a hypothesis H, and the task is to
decide whether H can be inferred from T. Our sys-
tem interprets inference in a strict way. Given the
knowledge of the language embedded in the system,
does the hypothesis logically follow from the infor-
mation embedded in the text? Thus we are empha-
sizing precision, particularly in question-answering.
This was reflected in our results in the RTE3 chal-
lenge. We responded correctly with YES to relatively
few of the examples, but on the QA-type examples,
we achieved 90-95% average precision.
The methodology employed is to use the linguis-
tic information to map T and H onto a logical form in
AKR, our Abstract Knowledge Representation. The
AKR is designed to capture the propositions the au-
thor of a statement is committed to. For the sake of
ECD, the representation of T may include elements
that are not directly expressed in the text. For ex-
ample, in the AKR of John bought a car includes the
fact that the car was sold. The AKR of John forgot to
buy milk includes the fact that John did not buy milk.
Our reasoning algorithm tries to determine whether
the AKR of H is subsumed by the AKR of T and detect
cases when they are in conflict.
The Entailment and Contradiction Detection
(ECD) algorithm makes a distinction that is not part
of the basic RTE challenge. If T entails the negation
of H, we answer NO (Contradiction). On the other
Process Output
Text-Breaking Delimited sentences
Named-entity recognition Type-marked Entities
Morphological Analysis Word stems plus features
LFG Parsing Functional Structure
Semantic processing Scope, Predicate-
argument structure
AKR rules Conceptual, Contextual,
Temporal Structure
Figure 1: The processing pipeline: processes with
their ambiguity-enabled packed outputs
hand, if there is no direct entailment we answer UN-
KNOWN. We do not try to construct a likely scenario
that would link T and H. Nor have we tried to col-
lect data on phrases that would tend to indicate such
likely associations between T and H. That approach
is clearly very useful (e.g. (Hickl et al, 2006)), and
could be used as a backup strategy with our more
formal entailment approach. We have chosen to fo-
cus on strict structural and lexical entailments.
This paper describes the processing pipeline for
mapping to AKR, the ECD algorithm, the challenges
we faced in processing the RTE data and a summary
of our results on RTE3.
2 Process Pipeline
Figure 1 shows the processing pipeline for mapping
texts to AKR. The input is a text of one or more
sentences.
All components of the system are ?ambiguity en-
abled? (Maxwell and Kaplan, 1991). This allows
each component to accept ambiguous input in a
?packed? format, process it without unpacking the
ambiguities, and then pass packed input to the next
stage. The syntactic component, LFG Parsing, also
has a stochastic disambiguation system which al-
lows us to pass the n-best on to the semantics (Rie-
zler et al, 2002); for the RTE3 challenge, we used
16
n=50.
The parser takes the output of the morphology
(i.e. a series of lemmata with their tags) and pro-
duces a tree (constituent-structure) and a depen-
dency structure (functional-structure) represented as
an attribute-value matrix. The functional-structure
is of primary importance for the semantics and
AKR. In particular, it encodes predicate-argument
relations, including long-distance dependencies, and
provides other syntactic features (e.g. number, tense,
noun type).
The output of the syntax is input for the seman-
tics that is produced by an ambiguity enabled packed
rewriting system. The semantics is described in de-
tail in (Crouch and King, 2006). Semantic process-
ing assigns scope to scope-bearing elements such as
negation and normalizes the output of the syntax.
This normalization includes reformulating syntactic
passives as actives (e.g. The cake was eaten by Mary.
/ Mary ate the cake.), resolving many null pronouns
(e.g. Laughing, John entered the room / Johni laugh-
ing, Johni entered the room.), and canonicalizing
measure phrases, comparatives, and dates. More
complex normalizations involve converting nominal
deverbals into the equivalent verbal form, identify-
ing arguments of the verb from the arguments of
the nominal (Gurevich et al, 2006). For example,
the semantic representation of Iraq?s destruction of
its WMD is similar to the representation of Iraq de-
stroyed its WMD.
The final main task of the semantics rules is to
convert words into concepts and syntactic grammat-
ical functions into roles. The mapping onto concepts
uses WordNet (Fellbaum, 1998) to map words into
lists of synsets. The named entity types provided by
the morphology and syntax are used to create more
accurate mapping of proper nouns since these are
not systematically represented in WordNet. The se-
mantic rules use the grammatical function subcat-
egorization information from the verb and the role
information found in extended VerbNet (Kipper et
al., 2000) to map syntactic subjects, objects, and
obliques into more abstract thematic roles such as
Agent, Theme, and Goal (Crouch and King, 2005).
This mapping into thematic-style roles allows the
system to correctly align the arguments in pairs like
(1) and (2), something which is impossible using just
syntactic functions. In the first, the object and sub-
ject have a common thematic role in the alternation
between transitive and intransitive; while in the sec-
ond, the common role is shared by the subjects.
(1) John broke the vasesyn:object,sem:patient.
The vasesyn:subject,sem:patient broke.
(2) Johnsyn:subject,sem:agent ate the cake.
Johnsyn:subject,sem:agent ate.
The goal of these semantic normalizations is to
abstract away from the syntactic representation so
that sentences with similar meaning have similar se-
mantic representations. However, the semantics is
still fundamentally a linguistic level of representa-
tion; further abstraction towards the meaning is done
in the mapping from semantics to AKR. The AKR
is the level of representation that is used to deter-
mine entailment and contradiction in our RTE3 sys-
tem. A preliminary description of its logic was pro-
vided in (Bobrow et al, 2005). The AKR mapping
converts grammatical tense and temporal modifiers
into temporal relations, identifies anaphoric refer-
ents and makes explicit the implied relation between
complement clauses and the main verb (e.g. for
manage, fail) (Nairn et al, 2006). AKR also deals
with standard phrases that are equivalent to simple
vocabulary terms. For example, take a flight to New
York is equivalent to fly to New York. These uses
of ?light? verbs (e.g. take, give) are not included
in synonyms found in WordNet. Another class of
phrasal synonyms involve inchoatives (e.g. take a
turn for the worse/worsen). We included a special
set of transformation rules for phrasal synonyms:
some of the rules are part of the mapping from se-
mantics to AKR while others are part of the ECD
module. The mapping to AKR is done using the same
ambiguity-enabled ordered rewriting system that the
semantics uses, allowing the AKR mapping system
to efficiently process the packed output of the se-
mantics.
The AKR for a sentence like Bush claimed that
Iraq possessed WMDs in Figure 2 introduces two
contexts: a top level context t, representing the com-
mitments of the speaker of sentence, and an embed-
ded context claim cx:37 representing the state of af-
fairs according to Bush?s claim. The two contexts
are related via the Topic role of the claim event.
The representation contains terms like claim:37 or
17
Conceptual Structure
subconcept(claim:37,[claim-1,. . .,claim-5])
role(Topic,claim:37,claim cx:37)
role(Agent,claim:37,Bush:1)
subconcept(Bush:1,[person-1])
alias(Bush:1,[Bush])
role(cardinality restriction,Bush:1,sg)
subconcept(possess:24,[possess-1,own-1,possess-3])
role(Destination,possess:24,wmd:34)
role(Agent,possess:24,Iraq:19)
subconcept(Iraq:19,[location-1,location-4])
alias(Iraq:19,[Iraq])
role(cardinality restriction,Iraq:19,sg)
subconcept(wmd:34,
[weapon of mass destruction-1])
role(cardinality restriction,wmd:34,pl)
Contextual Structure
context(t)
context(claim cx:37)
context relation(t,claim cx:37,crel(Topic,claim:37))
instantiable(Bush:1,t)
instantiable(Iraq:19,t)
instantiable(claim:37,t)
instantiable(Iraq:19,claim cx:37)
instantiable(possess:24,claim cx:37)
instantiable(wmd:34,claim cx:37)
Temporal Structure
temporalRel(After,Now,claim:37)
temporalRel(After,claim:37,possess:24)
Figure 2: AKR for Bush claimed that Iraq possessed
WMDs.
Bush:1 which refer to the kinds of object that the
sentence is talking about. The subconcept facts ex-
plicitly link these terms to their concepts in Word-
Net. Thus claim:37 is stated to be some subkind
of the type claim-1, etc., and wmd:34 to be some
subkind of the type weapon of mass destruction-
1. Terms like claim:37 and wmd:34 do not refer
to individuals, but to concepts (or types or kinds).
Saying that there is some subconcept of the kind
weapon of mass destruction-1, where this subcon-
cept is further restricted to be a kind of WMD pos-
sessed by Iraq, does not commit you to saying that
there are any instances of this subconcept.
The instantiable assertions capture the commit-
ments about the existence of the kinds of object de-
scribed. In the top-level context t, there is a com-
mitment to an instance of Bush and of a claim:37
event made by him. However, there is no top-level
commitment to any instances of wmd:34 possessed
by Iraq:19. These commitments are only made in
the embedded claim cx:37 context. It is left open
whether these embedded commitments correspond,
or not, to the beliefs of the speaker. Two distinct
levels of structure can thus be discerned in AKR: a
conceptual structure and a contextual structure. The
conceptual structure, through use of subconcept and
role assertions, indicates the subject matter. The
contextual structure indicates commitments as to the
existence of the subject matter via instantiability as-
sertions linking concepts to contexts, and via context
relations linking contexts to contexts. In addition,
there is a temporal structure that situates the events
described with respect to the time of utterance and
temporally relates them to one another.
3 Entailment and Contradiction Detection
ECD is implemented as another set of rewrite rules,
running on the same packed rewrite system used to
generate the AKR representations. The rules (i) align
concept and context terms in text (T) and hypoth-
esis (H) AKRs, (ii) calculate concept subsumption
orderings between aligned T and H terms, and (iii)
check instantiability and uninstantiability claims in
the light of subsumption orderings to determine
whether T entails H, T contradicts H, or T neither
entails not contradicts H. For the purposes of RTE3,
both contradiction and neither contradiction nor en-
tailment are collapsed into a NO (does not follow)
judgment.
One of the novel features of this approach is that
T and H representations do not need to be disam-
biguated before checking for entailment or contra-
diction. The approach is able to detect if there is one
reading of T that entails (or contradicts) one reading
of H. The T and H passages can in effect mutually
disambiguate one another through the ECD. For ex-
ample, although plane and level both have multiple
readings, they can both refer to a horizontal surface,
and in that sense The plane is dry entails The level is
dry, and vice versa.
The first phase of ECD aligns concepts and con-
text terms in the T and H AKRs. Concepts are repre-
18
sented as lists of WordNet hypernym lists, in Word-
Net sense order. Two concept terms can be aligned
if a sense synset of one term (i.e. the first element
of one of the term?s hypernym lists) is contained in
a hypernym list of the other term. The alignment
can be weighted according to word sense; so a con-
cept overlap on the first senses of a T and H term
counts for more than a concept overlap on the n and
mth senses. However, no weightings were used in
RTE3. For named entities, alignment demands not
only a concept overlap, but also an intersection in
the ?alias? forms of the proper nouns. For exam-
ple,?George Bush? may be aligned with ?George?
or with ?Bush?. Context alignment relies on associ-
ating each context with an indexing concept, usually
the concept for the main verb in the clause heading
the context. Contexts are then aligned on the basis
of these concept indices.
Typically, an H term can align with more than one
T term. In such cases all possible alignments are
proposed, but the alignment rules put the alternative
alignments in different parts of the choice space.
Having aligned T and H terms, rules are applied to
determine concept specificity and subsumption rela-
tions between aligned terms. Preliminary judgments
of specificity are made by looking for hypernym in-
clusion. For example, an H term denoting the con-
cept ?person? is less specific than a T term denot-
ing ?woman?. These preliminary judgments need to
be revised in the light of role restrictions modifying
the terms: a ?tall person? is neither more nor less
specific than a ?woman?. Revisions to specificity
judgments also take into account cardinality modi-
fiers: while ?person? is less specific than ?woman?,
?all persons? is judged to be more specific than ?all
women?.
With judgments of concept specificity in place,
it is possible to determine entailment relations on
the basis of (un)instantiability claims in the T and
H AKRs. For example, suppose the T and H AKRs
contain the facts in (3).
(3) T: instantiable(C T, Ctx T)
H: instantiable(C H, Ctx H)
where concept C T is aligned with C H, C T is
judged to be more specific than C H, and context
Ctx T is aligned with context Ctx H. In this case,
the hypothesis instantiability claim is entailed by
the text instantiability claim (existence of something
more specific entails existence of something more
general). This being so, the H instantiability claim
can be deleted without loss of information.
If instead we had the (un)instantiability claims in
(4) for the same alignments and specificity relations,
(4) T: instantiable(C T, Ctx T)
H: uninstantiable(C H, Ctx H)
we would have a contradiction: the text says that
there is something of the more specific type C T,
whereas the hypothesis says there are no things of
the more general type C H. In this case, the rules
explicitly flag a contradiction.
Once all (un)instantiability claims have been
compared, it is possible to judge whether the text en-
tails or contradicts the hypothesis. Entailed hypothe-
sis (un)instantiability assertions are deleted from the
representation. Consequently, if there is one T and H
AKR readings and one set of alignments under which
all the H (un)instantiability assertions have been re-
moved, then there is an entailment of H by T. If
there is a pair of readings and a set of alignments
under which a contradiction is flagged, then there
is a contradiction. If there is no pair of readings or
set of alignments under which there is either an en-
tailment or a contradiction, then T and H are merely
consistent with one another. There are exceptional
cases such as (5) where one reading of T entails H
and another reading contradicts it.
(5) T: John did not wait to call for help.
H: John called for help.
Our ECD rules detect such cases.
WordNet often misses synonyms needed for the
alignment in the ECD. In particular, the hierarchy
and synsets for verbs are one of WordNet?s least de-
veloped parts. To test the impact of the missing syn-
onyms, we developed a variation on the ECD algo-
rithm that allows loose matching.
First, in concept alignment, if a verb concept in H
does not align with any verb concept in T, then we
permit it to (separately) align with all the text verb
concepts. We do not permit the same loose align-
ment for noun concepts, since we judge WordNet
information to be more reliable for nouns. This free
alignment of verbs might sound risky, but in gen-
eral these alignments will not lead to useful concept
19
specificity judgments unless the T and H verbs have
very similar arguments / role restrictions.
When such a loose verb alignment is made, we
explicitly record this fact in a justification term in-
cluded in the alignment fact. Similarly, when judg-
ing concept specificity, each rule that applies adds a
term to a list of justifications recorded as part of the
fact indicating the specificity relation. This means
that when the final specificity judgments are deter-
mined, each judgment has a record of the sequence
of decisions made to reach it.
(Un)instantiability comparisons are made as in
strict matching. However, the criteria for detect-
ing an entailment are selectively loosened. If no
contradiction is flagged, and there is a pairing of
readings and alignments under which just a single
H instantiability assertion is left standing, then this
is allowed through as a loose entailment. However,
further rules are applied to block those loose entail-
ments that are deemed inappropriate. These block-
ing rules look at the form of the justification terms
gathered based on specificity judgments.
These blocking rules are manually selected. First,
a loose matching run is made without any block-
ing rules. Results are dumped for each T-H pair,
recording the expected logical relation and the jus-
tifications collected. Blocking rules are created by
detecting patterns of justification that are associated
with labeled non-entailments. One such blocking
rule says that if you have just a single H instantia-
bility left, but the specificity justifications leading to
this have been shown to be reliable on training data,
then the instantiability should not be eliminated as a
loose entailment.
4 Challenges in Processing the RTE Data
The RTE3 data set contains inconsistencies in
spelling and punctuation between the text and the
hypothesis. To handle these, we did an automatic
prepass where we compared the strings in the pas-
sage text to those in the hypothesis. Some of the
special cases that we handled include:
? Normalize capitalization and spacing
? Identify acronyms and shorten names
? Title identification
? Spelling correction
Role names in VerbNet are in part intended to cap-
ture the relation of the argument to the event be-
ing described by the verb. For example, an object
playing an Agent role is causally involved in the
event, while an object playing a Theme or Patient
role is only supposed to be affected. This allows
participants in an action to be identified regardless
of the syntactic frame chosen to represent the verb;
this was seen in (1) and (2). Sometimes the roles
from VerbNet are not assigned in such a way as to
allow such transparent identification across frames
or related verbs. Consider an example. In Ed trav-
els/goes to Boston VerbNet identifies Ed as playing a
Theme role. However, in Ed flies to Boston VerbNet
assigns Ed an Agent role; this difference can make
determining contradiction and entailment between T
and H difficult. We have tried to compensate in our
ECD, by using a backoff strategy where fewer role
names are used (by projecting down role names to
the smaller set). As we develop the system further,
we continue to experiment with which set of roles
works best for which tasks.
Another open issue involves identifying alterna-
tive ways vague relations among objects appear in
text. We do not match the expression the Boston
team with the team from Boston. To improve our re-
call, we are considering loose matching techniques.
5 Summary of our results on RTE3
We participated in the RTE challenge as a way to
understand what our particular techniques could do
with respect to a more general version of textual en-
tailment. The overall experiment was quite enlight-
ening. Tables 1 and 2 summarize how we did on the
RTE3 challenge. System 1 is our standard system
with strict ECD. System 2 used the looser set of ECD
rules.
Gold Sys Cor- R P F
YES YES rect
IE 105 6 5 0.048 0.83 0.20
IR 87 4 4 0.046 1.00 0.21
QA 106 10 9 0.085 0.90 0.28
SUM 112 11 7 0.063 0.64 0.20
Total 410 31 25 0.060 0.84 0.22
Table 1: System 1 with Strict ECD
20
Gold Sys Cor- R P F
YES YES rect
IE 105 15 10 0.095 0.67 0.25
IR 87 6 4 0.046 0.67 0.18
QA 106 14 13 0.12 0.93 0.34
SUM 112 17 10 0.089 0.59 0.23
Total 410 52 37 0.088 0.71 0.25
Table 2: System 2 with Loose ECD
As can be seen, we answered very few of the ques-
tions; only 31 of the possible 410 with a YES answer.
However, for those we did answer (requiring only
linguistic, and not world knowledge), we achieved
high precision: up to 90% on QA. However, we were
not perfect even from this perspective. Here are sim-
plified versions of the errors where our system an-
swered YES, and the answer should be NO with an
analysis of what is needed in the system to correct
the error.
The wrong result in (6) is due to our incomplete
coverage of intensional verbs (seek, want, look for,
need, etc.).
(6) T: The US sought the release of hostages.
H: Hostages were released.
The object of an intensional verb cannot be assumed
to exist or to occur. Intensional verbs need to be
marked systematically in our lexicon.
The problem with (7) lies in the lack of treatment
for generic sentences.
(7) T: Girls and boys are segregated in high school
during sex education class.
H: Girls and boys are segregated in high school.
The natural interpretation of H is that girls and boys
are segregated in high school ALL THE TIME. Be-
cause we do not yet handle generic sentences prop-
erly, our algorithm for calculating specificity pro-
duces the wrong result here. It judges segregation in
H to be less specific than in T whereas the opposite
is in fact the case. Adding the word ?sometimes? to
H would make our YES the correct answer.
The distinction between generic and episodic
readings is difficult to make but crucial for the in-
terpretation of bare plural noun phrases such as girls
and boys. For example, the most likely interpreta-
tion of Counselors are available is episodic: SOME
counselors are available. But Experts are highly
paid is weighted towards a generic reading: MOST
IF NOT ALL experts get a good salary.
These examples are indicative of the subtlety of
analysis necessary for high precision textual infer-
ence.
References
Danny Bobrow, Cleo Condoravdi, Richard Crouch,
Ronald Kaplan, Lauri Karttunen, Tracy Holloway
King, Valeria de Paiva, and Annie Zaenen. 2005. A
basic logic for textual inference. In Proceedings of the
AAAI Workshop on Inference for Textual Question An-
swering.
Dick Crouch and Tracy Holloway King. 2005. Unify-
ing lexical resources. In Proceedings of the Interdisci-
plinary Workshop on the Identification and Represen-
tation of Verb Features and Verb Classes.
Dick Crouch and Tracy Holloway King. 2006. Se-
mantics via F-structure rewriting. In Proceedings of
LFG06. CSLI On-line Publications.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2007. XLE docu-
mentation. Available on-line.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Olga Gurevich, Richard Crouch, Tracy Holloway King,
and Valeria de Paiva. 2006. Deverbal nouns in knowl-
edge representation. In Proceedings of FLAIRS 2006.
Andres Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing textual entailment with LCC?s GROUNDHOG
system. In The Second PASCAL Recognising Textual
Entailment Challenge.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon. In
AAAI-2000 17th National Conference on Artificial In-
telligence.
John Maxwell and Ron Kaplan. 1991. A method for
disjunctive constraint satisfaction. Current Issues in
Parsing Technologies.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5.
Stefan Riezler, Tracy Holloway King, Ron Kaplan, Dick
Crouch, John Maxwell, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-
Functional Grammar and discriminative estimation
techniques. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
21
Proceedings of the 10th Conference on Parsing Technologies, pages 36?38,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
The Impact of Deep Linguistic Processing on Parsing Technology
Timothy Baldwin
University of Melbourne
tim@csse.unimelb.edu.au
Mark Dras
Macquarie University
madras@ics.mq.edu.au
Julia Hockenmaier
University of Pennsylvania
juliahr@cis.upenn.edu
Tracy Holloway King
PARC
thking@parc.com
Gertjan van Noord
University of Groningen
vannoord@let.rug.nl
Abstract
As the organizers of the ACL 2007 Deep
Linguistic Processing workshop (Baldwin et
al., 2007), we were asked to discuss our per-
spectives on the role of current trends in
deep linguistic processing for parsing tech-
nology. We are particularly interested in
the ways in which efficient, broad coverage
parsing systems for linguistically expressive
grammars can be built and integrated into
applications which require richer syntactic
structures than shallow approaches can pro-
vide. This often requires hybrid technolo-
gies which use shallow or statistical methods
for pre- or post-processing, to extend cover-
age, or to disambiguate the output.
1 Introduction
Our talk will provide a view on the relevance of deep
linguistic processing for parsing technologies from
the perspective of the organizers of the ACL 2007
Workshop on Deep Linguistic Processing (Baldwin
et al, 2007). The workshop was conceived with the
broader aim of bringing together the different com-
putational linguistic sub-communities which model
language predominantly by way of theoretical syn-
tax, either in the form of a particular theory (e.g.
CCG, HPSG, LFG, TAG, the Prague School) or a
more general framework which draws on theoretical
and descriptive linguistics. These ?deep linguistic
processing? approaches differ from shallower meth-
ods in that they yield richer, more expressive, struc-
tural representations which capture long-distance
dependencies or the underlying predicate-argument
structure directly.
Aspects of this research have often had their own
separate fora, such as the ACL 2005 workshop on
deep lexical acquisition (Baldwin et al, 2005), as
well as the TAG+ (Kallmeyer and Becker, 2006),
Alpino (van der Beek et al, 2005), ParGram (Butt
et al, 2002) and DELPH-IN (Oepen et al, 2002)
projects and meetings. However, the fundamental
approaches to building a linguistically-founded sys-
tem and many of the techniques used to engineer
efficient systems are common across these projects
and independent of the specific grammar formal-
ism chosen. As such, we felt the need for a com-
mon meeting in which experiences could be shared
among a wider community, similar to the role played
by recent meetings on grammar engineering (Wint-
ner, 2006; Bender and King, 2007).
2 The promise of deep parsing
Deep linguistic processing has traditionally been
concerned with grammar development (for use in
both parsing and generation). However, the linguis-
tic precision and complexity of the grammars meant
that they had to be manually developed and main-
tained, and were computationally expensive to run.
In recent years, machine learning approaches
have fundamentally altered the field of natural lan-
guage processing. The availability of large, manu-
ally annotated, treebanks (which typically take years
of prior linguistic groundwork to produce) enabled
the rapid creation of robust, wide-coverage parsers.
However, the standard evaluation metrics for which
such parsers have been optimized generally ignore
36
much of the rich linguistic information in the orig-
inal treebanks. It is therefore perhaps only natural
that deep processing methods, which often require
substantial amounts of manual labor, have received
considerably less attention during this period.
But even if further work is required for deep
processing techniques to fully mature, we believe
that applications that require natural language under-
standing or inference, among others, will ultimately
need detailed syntactic representations (capturing,
e.g., bounded and unbounded long-range dependen-
cies) from which semantic interpretations can eas-
ily be built. There is already some evidence that
our current deep techniques can, in some cases, out-
perform shallow approaches. There has been work
demonstrating this in question answering, targeted
information extraction and the recent textual entail-
ment recognition task, and perhaps most notably in
machine translation: in this latter field, after a period
of little use of linguistic knowledge, deeper tech-
niques are beginning to lead to better performance,
e.g. by redefining phrases by syntactic ?treelets?
rather than contiguous word sequences, or by explic-
itly including a syntactic component in the probabil-
ity model, or by syntactic preprocessing of the data.
3 Closing the divide
In the past few years, the divide between ?deep?,
rule-based, methods and ?shallow?, statistical, ap-
proaches, has begun to close from both sides. Re-
cent advances in using the same treebanks that have
advanced shallow techniques to extract more expres-
sive grammars or to train statistical disambiguators
for them, and in developing framework-specific tree-
banks, have made it possible to obtain similar cov-
erage, robustness, and disambiguation accuracy for
parsers that use richer structural representations. As
witnessed by many of the papers in our workshop
(Baldwin et al, 2007), a large proportion of current
deep systems have statistical components to them,
e.g., as pre- or post-processing to control ambigu-
ity, as means of acquiring and extending lexical re-
sources, or even use machine learning techniques
to acquire deep grammars automatically. From the
other side of the divide, many of the purely statistical
approaches are using progressively richer linguistic
features and are taking advantage of these more ex-
pressive features to tackle problems that were tradi-
tionally thought to require deep systems, such as the
recovery of traces or semantic roles.
4 The continued need for research on deep
processing
Although statistical techniques are becoming com-
monplace even for systems built around hand-
written grammars, there is still a need for further
linguistic research and manual grammar develop-
ment. For example, supervised machine-learning
approaches rely on large amounts of manually anno-
tated data. Where such data are available, develop-
ers of deep parsers and grammars can exploit them
to determine frequency of certain constructions, to
bootstrap gold standards for their systems, and to
provide training data for the statistical components
of their systems such as parse disambiguators. But
for the majority of the world?s languages, and even
for many languages with large numbers of speakers,
such corpora are unavailable. Under these circum-
stances, manual grammar development is unavoid-
able, and recent progress has allowed the underlying
systems to become increasingly better engineered,
allowing for more rapid development of any given
grammar, as well as for overlay grammars that adapt
to particular domains and applications and for port-
ing of grammars from one language to another.
Despite recent work on (mostly dependency
grammar-based) multilingual parsing, it is still the
case that most research on statistical parsing is done
on English, a fixed word-order language where sim-
ple context-free approximations are often sufficient.
It is unclear whether our current models and al-
gorithms carry over to morphologically richer lan-
guages with more flexible word order, and it is possi-
ble that the more complex structural representations
allowed by expressive formalisms will cease to re-
main a luxury.
Further research is required on all aspects of
deep linguistic processing, including novel linguis-
tic analyses and implementations for different lan-
guages, formal comparisons of different frame-
works, efficient parse and learning algorithms, better
statistical models, innovative uses of existing data
resources, and new evaluation tools and methodolo-
gies. We were fortunate to receive so many high-
37
quality submissions on all of these topics for our
workshop.
5 Conclusion and outlook
Deep linguistic processing brings together a range of
perspectives. It covers current approaches to gram-
mar development and issues of theoretical linguis-
tic and algorithmic properties, as well as the appli-
cation of deep linguistic techniques to large-scale
applications such as question answering and dialog
systems. Having industrial-scale, efficient parsers
and generators opens up new application domains
for natural language processing, as well as inter-
esting new ways in which to approach existing ap-
plications, e.g., by combining statistical and deep
processing techniques in a triage process to pro-
cess massive data quickly and accurately at a fine
level of detail. Notably, several of the papers ad-
dressed the relationship of deep linguistic process-
ing to topical statistical approaches, in particular in
the area of parsing. There is an increasing inter-
est in deep linguistic processing, an interest which
is buoyed by the realization that new, often hybrid,
techniques combined with highly engineered parsers
and generators and state-of-the-art machines opens
the way towards practical, real-world application of
this research. We look forward to further opportu-
nities for the different computational linguistic sub-
communities who took part in this workshop, and
others, to continue to come together in the future.
References
Timothy Baldwin, Anna Korhonen, and Aline Villavicen-
cio, editors. 2005. Proceedings of the ACL-SIGLEX
Workshop on Deep Lexical Acquisition. Ann Arbor,
USA.
Timothy Baldwin, Mark Dras, Julia Hockenmaier,
Tracy Holloway King, and Gertjan van Noord, editors.
2007. Proceedings of the ACL Workshop on Deep Lin-
guistic Processing, Prague, Czech Republic.
Emily Bender and Tracy Holloway King, editors. 2007.
Grammar Engineering Across Frameworks, Stanford
University. CSLI On-line Publications. to appear.
Miriam Butt, Helge Dyvik, T. H. King, Hiroshi Masuichi,
and Christian Rohrer. 2002. The parallel grammar
project. In COLING Workshop on Grammar Engi-
neering and Evaluation, Taipei, Taiwan.
Laura Kallmeyer and Tilman Becker, editors. 2006. Pro-
ceedings of the Eighth International Workshop on Tree
Adjoining Grammar and Related Formalisms (TAG+),
Sydney, Australia.
Stephan Oepen, Dan Flickinger, J. Tsujii, and Hand
Uszkoreit, editors. 2002. Collaborative Language En-
gineering: A Case Study in Efficient Grammar-based
Processing. CSLI Publications.
Leonoor van der Beek, Gosse Bouma, Jan Daciuk, Tanja
Gaustad, Robert Malouf, Mark-Jan Nederhof, Gert-
jan van Noord, Robbert Prins, and Bego na Vil-
lada Moiro?n. 2005. Algorithms for linguistic pro-
cessing. NWO Pionier final report. Technical report,
University of Groningen.
Shuly Wintner. 2006. Large-scale grammar development
and grammar engineering. Research workshop of the
Israel Science Foundation.
38
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 3?4,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Type-checking in Formally non-typed Systems
Dick Crouch
Powerset, Inc.
San Francisco, USA
crouch@powerset.com
Tracy Holloway King
Palo Alto Research Center
Palo Alto, USA
thking@parc.com
Abstract
Type checking defines and constrains system
output and intermediate representations. We
report on the advantages of introducing multi-
ple levels of type checking in deep parsing sys-
tems, even with untyped formalisms.
1 Introduction
Some formalisms have type checking as an inherent
part of their theory (Copestake (2002)). However,
many formalisms do not require type checking. We
report on our experiences with a broad-coverage sys-
tem for mapping English text into semantic repre-
sentations for search applications. This system uses
the XLE LFG parser for converting from text to syn-
tactic structures and the XLE ordered-rewriting sys-
tem to convert from syntax to semantic structures.
Neither component formally requires type checking.
However, type checking was introduced into the syn-
tactic parser and at multiple levels in the semantics in
response to the engineering requirements on a large-
scale, multi-developer, multi-site system.
2 Syntactic Typing
The syntactic parser outputs a tree and an attribute
value matrix (f(unctional)-structure). Meaning-
sensitive applications use the f-structure which
contains predicate argument relations and other
semantically relevant dependencies.
A feature declaration (FD) requires every f-
structure attribute to be declared with its possible
values. These values are typed as to whether they
are atomic or are embedded f-structures. (1) shows
the FD for NUM(ber) and SPEC(ifier). NUM takes
an atomic value, while SPEC takes an f-structure
containing the features ADJUNCT, AQUANT, etc.
(1) a. NUM: - $ pl sg .
b. SPEC: - [ADJUNCT AQUANT DET
NUMBER POSS QUANT SPEC-TYPE].
XLE supports overlay grammars where a gram-
mar for an application uses another grammar as its
base. The FDs form part of the overlay system. For
example, there is an FD used by the Parallel Gram-
mar project (Butt et al (2003)); the standard English
FD adds and modifies features; then domain specific
FDs overlay this. (2) gives the number of features in
the ParGram FD and the standard English overlay.
(2) atomic f-structure
English 76 33
ParGram 34 11
The grammar cannot be loaded if there is a feature
or value that is not licensed by the FD (to type check
the lexicon, the generator is loaded). The command
print-unused-feature-declarations
can be used after a large parse run to determine
which features never surfaced in the analysis of the
corpus and hence might be candidates to be removed
from the grammar.
As LFG does not have type checking as part of its
theory (Dalrymple et al (2004)), XLE originally did
not implement it. However, in grammar engineering,
type checking over features speeds up the develop-
ment process and informs later processes and appli-
cations what features to expect since the FD serves
as an overview of the output of the grammar.
3
3 Semantic Typing
The syntactic output is the input to several sets of
ordered rewriting rules that produce semantic struc-
tures (Crouch and King (2006)). The nature of or-
dered rewriting systems, which consume input facts
to create novel output facts, makes type checking ex-
tremely important for determining well formedness.
When these representations are used in applications,
type declarations can document changes so that the
subsequent processing can take them into account.
The semantic typing is done by declaring ev-
ery fact that can appear in the structure, its arity,
and the type of its arguments. A field is available
for comments and examples. (3) shows the licens-
ing of nominal modifiers in noun-noun compounds
(nn element), where skolem and integer are argu-
ment types.
(3) - type(proposition,
nn element(%%Element:skolem,
%%Head:skolem,
%%Nth:integer),
comment([ %%Element is the %%Nth
term in the compound noun %%Head
Example NP: the hinge oil bottle
in context(t,nn element(hinge:10,bottle:1,2)) ])).
The xfr semantics is developed by multiple users.
By breaking the rules into modules, type checking
can occur at several stages in the processing pipeline.
The current system provides for type checking at
word-prime semantics, the final semantics, and ab-
stract knowledge representation. (4) shows the num-
ber of (sub)features licensed at each level.1
(4) word prime 91
lexical semantics 102
akr 45
In addition to aiding the developers of the seman-
tics rules, the type declarations serve as documenta-
tion for the next steps in the process, e.g. creating the
semantic search index and query reformulation.
4 Additional Engineering Support
The semantic type checking is a set of ordered
rewrite rules, using the same mechanism as the se-
1A stripped-down XML version of the semantics uses an
xschema which checks that only the reduced feature set is used
and that the XML is well-formed.
mantics rules. As such, the notation and applica-
tion are familiar to the grammar engineers and hence
more accessible. Since the type checking involves
additional processing time, it is not part of run-time
processing. Instead, it is run within a larger regres-
sion testing regime (Chatzichrisafis et al (2007)).
Grammar engineers run a core set of regression tests
before checking in any changes to the svn repository.
Larger nightly runs check performance as well as
typing at all levels of analysis and help ensure com-
patibility of changes from multiple developers.
The syntactic grammar cannot be loaded with fea-
ture type violations. However, the nature of an or-
dered rewriting system makes it so that loading the
rules does not give the full feature type space of
the resulting output. To force compliance with type
checking requirements, check-ins require regression
tests before committing changes. The output of these
tests is type checked and, if unlicensed features are
found, the commit is blocked. The grammar engi-
neer can then update the type checking rules or mod-
ify the semantic rules to produce only licensed fea-
tures. The regression testing is then rerun and, if the
type checking passes, the commit proceeds.
In sum, introducing type checking at multiple lev-
els provides a better development environment for
grammar engineers as well as documentation for the
developers and for applications.
References
Butt, M., Forst, M., King, T.H. and Kuhn, J. 2003.
The Feature Space in Parallel Grammar Writing.
In ESSLLI Workshop on Ideas and Strategies for
Multilingual Grammar Development.
Chatzichrisafis, N., Crouch, D., King, T.H., Nairn,
R., Rayner, M. and Santaholma, M. 2007. Re-
gression Testing for Grammar-based Systems. In
Grammar Engineering Across Frameworks.
Copestake, A. 2002. Implementing Typed Feature
Structure Grammars. CSLI.
Crouch, D. and King, T.H. 2006. Semantics via F-
Structure Rewriting. In Proceedings of LFG06.
Dalrymple, M., Kaplan, R. and King, T.H. 2004.
Linguistic Generalizations over Descriptions. In
Proceedings of LFG04.
4
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 63?70,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Using Large-scale Parser Output to Guide Grammar Development
Ascander Dost
Powerset, a Microsoft company
adost@microsoft.com
Tracy Holloway King
Powerset, a Microsoft company
Tracy.King@microsoft.com
Abstract
This paper reports on guiding parser de-
velopment by extracting information from
output of a large-scale parser applied to
Wikipedia documents. Data-driven parser
improvement is especially important for
applications where the corpus may differ
from that originally used to develop the
core grammar and where efficiency con-
cerns affect whether a new construction
should be added, or existing analyses mod-
ified. The large size of the corpus in ques-
tion also brings scalability concerns to the
foreground.
1 Introduction
Initial development of rule-based parsers1 is often
guided by the grammar writer?s knowledge of the
language and test suites that cover the ?core? lin-
guistic phenomena of the language (Nerbonne et
al., 1988; Cooper et al, 1996; Lehmann et al,
1996). Once the basic grammar is implemented,
including an appropriate lexicon, the direction of
grammar development becomes less clear. Inte-
gration of a grammar in a particular application
and the use of a particular corpus can guide gram-
mar development: the corpus and application will
require the implementation of specific construc-
tions and lexical items, as well as the reevalua-
tion of existing analyses. To streamline this sort
of output-driven development, tools to examine
parser output over large corpora are necessary, and
as corpus size increases, the efficiency and scal-
ability of those tools become crucial concerns.
Some immediate relevant questions for the gram-
mar writer include:
1The techniques discussed here may also be relevant to
purely machine-learned parsers and are certainly applicable
to hybrid parsers.
? What constructions and lexical items need to
be added for the application and corpus in
question?
? For any potential new construction or lexical
item, is it worth adding, or would it be better
to fall back to robust techniques?
? For existing analyses, are they applying cor-
rectly, or do they need to be restricted, or even
removed?
In the remainder of this section, we briefly dis-
cuss some existing techniques for guiding large-
scale grammar development and then introduce
the grammar being developed and the tool we use
in examining the grammar?s output. The remain-
der of the paper discusses development of lexical
resources and grammar rules, how overall progress
is tracked, and how analysis of the grammar output
can help development in other natural language
components.
1.1 Current Techniques
There are several techniques currently being used
by grammar engineers to guide large-scale gram-
mar development, including error mining to de-
tect gaps in grammar coverage, querying tools for
gold standard treebanks to determine frequency
of linguistic phenomena, and tools for querying
parser output to determine how linguistic phenom-
ena were analyzed in practice.
An error mining technique presented by van
Noord (2004) (henceforth: the van Noord Tool)
can reveal gaps in grammar coverage by compar-
ing the frequency of arbitrary n-grams of words
in unsuccessfully parsed sentences with the same
n-grams in unproblematic sentences, for large
unannotated corpora.2 A parser can be run over
new text, and a comparison of the in-domain and
2The suffix array error mining software is available at:
http://www.let.rug.nl/?vannoord/SuffixArrays.tgz
63
out-of-domain sentences can determine, for in-
stance, that the grammar cannot parse adjective-
noun hyphenation correctly (e.g. an electrical-
switch cover). A different technique for error
mining that uses discriminative treebanking is de-
scribed in (Baldwin et al, 2005). This tech-
nique aims at determining issues with lexical cov-
erage, grammatical (rule) coverage, ungrammati-
cality within the corpus (e.g. misspelled words),
and extragrammaticality within the corpus (e.g.
bulleted lists).
A second approach involves querying gold-
standard treebanks such as the Penn Treebank
(Marcus et al, 1994) and Tiger Treebank (Brants
et al, 2004) to determine the frequency of cer-
tain phenomena. For example, Tiger Search (Lez-
ius, 2002) can be used to list and frequency-
sort stacked prepositions (e.g. up to the door) or
temporal noun/adverbs after prepositions (e.g. by
now). The search tools over these treebanks al-
low for complex searches involving specification
of lexical items, parts of speech, and tree config-
urations (see (M??rovsky?, 2008) for discussion of
query requirements for searching tree and depen-
dency banks).
The third approach we discuss here differs from
querying gold-standard treebanks in that corpora
of actual parser output are queried to examine
how constructions are analyzed by the grammar.
For example, Bouma and Kloosterman (2002) use
XQuery (an XML query language) to mine parse
results stored as XML data.3 It is this sort of ex-
amination of parser output that is the focus of the
present paper, and specific examples of our expe-
riences follow in Section 2.2.
Use of such tools has proven vital to the devel-
opment of large-scale grammars. Based on our
experiences with them, we began extensively us-
ing a tool called Oceanography (Waterman, 2009)
to search parser output for very large (approxi-
mately 125 million sentence) parse runs stored on
a distributed file system. Oceanography queries
the parser output and returns counts of specific
constructions or properties, as well as the exam-
ple sentences they were extracted from. In the
subsequent sections we discuss how this tool (in
conjunction with existing ones like the van No-
ord Tool and Tiger Search) has enhanced grammar
development for an English-language Lexical-
3See also (Bouma and Kloosterman, 2007) for further dis-
cussion of this technique.
Functional Grammar used for a semantic search
application over Wikipedia.
1.2 The Grammar and its Role
The grammar being developed is a Lexical-
Functional Grammar (LFG (Dalrymple, 2001))
that is part of the ParGram parallel grammar
project (Butt et al, 1999; Butt et al, 2002). It runs
on the XLE system (Crouch et al, 2009) and pro-
duces c(onstituent)-structures which are trees and
f(unctional)-structures which are attribute value
matrices recording grammatical functions and
other syntactic features such as tense and number,
as well as debugging features such as the source
of lexical items (e.g. from a named entity finder,
the morphology, or the guesser). There is a base
grammar which covers the constructions found in
standard written English, as well as three overlay
grammars: one for parsing Wikipedia sentences,
one for parsing Wikipedia headers, and one for
parsing queries (sentential, phrasal, and keyword).
The grammar is being used by Powerset (a Mi-
crosoft company) in a semantic consumer-search
reference vertical which allows people to search
Wikipedia using natural language queries as well
as traditional keyword queries. The system uses a
pipeline architecture which includes: text extrac-
tion, sentence breaking, named entity detection,
parsing (tokenization, morphological analysis, c-
structure, f-structure, ranking), semantic analysis,
and indexing of selected semantic facts (see Fig-
ure 1). A similar pipeline is used on the query
side except that the resulting semantic analysis is
turned into a query execution language which is
used to query the index.
text extraction script
sentence breaker finite state
named entity detection MaxEnt model
LFG grammars
tokenizer finite state
morphology finite state
grammar XLE: parser
ranking MaxEnt model
semantics XLE: XFR
Figure 1: NL Pipeline Components
The core idea behind using a deep parser in the
pipeline in conjunction with the semantic rules is
to localize role information as to who did what to
whom (i.e. undo long-distance dependencies and
64
locate heads of arguments), to abstract away from
choice of particular lexical items (i.e. lemmatiza-
tion and detection of synonyms), and generally
provide a more normalized representation of the
natural language string to improve both precision
and recall.
1.3 Oceanography
As a byproduct of the indexing pipeline, all of
the syntactic and semantic structures are stored
for later inspection as part of failure analysis.4
The files containing these structures are distributed
over several machines since ?125 million sen-
tences are parsed for the analysis of Wikipedia.
For any given syntactic or semantic structure,
the XLE ordered rewrite system (XFR; (Crouch et
al., 2009)) can be used to extract information that
is of interest to the grammar engineer, by way of
?rules? or statements in the XFR language. As the
XFR ordered rewrite system is also used for the
semantics rules that turn f-structures into seman-
tic representations, the notation is familiar to the
grammar writers and is already designed for ma-
nipulating the syntactic f-structures.
However, the mechanics of accessing each file
on each machine and then assembling the results is
prohibitively complicated without a tool that pro-
vides a simple interface to the system. Oceanogra-
phy was designed to take a single specification file
stating:
? which data to examine (which corpus ver-
sion; full Wikipedia build or fixed 10,000
document set);
? the XFR rules to be applied;
? what extracted data to count and report back.
Many concrete examples of Oceanography runs
will be discussed below. The basic idea is to
use the XFR rules to specify searches over lexi-
cal items, features, and constructions in a way that
is similar to that of Tiger Search and other facili-
ties. The Oceanography machinery enables these
searches over massive data and helps in compil-
ing the results for the grammar engineer to inspect.
We believe that similar approaches would be fea-
sible to implement in other grammar development
environments and, in fact, for some grammar out-
puts and applications, existing tools such as Tiger
4The index is self-contained and does not need to refer-
ence the semantic, much less the earlier syntactic, structures
as part of the search application.
Search would be sufficient. By providing exam-
ples where such searches have aided our grammar
development, we hope to encourage other gram-
mar engineers to similarly extend their efforts to
use easy access to massive data to drive their work.
2 Grammar Development
The ParGram English LFG grammar has been de-
veloped over many years. However, the focus of
development was on newspaper text and technical
manuals, although some adaptation was done for
new domains (King and Maxwell, 2007). When
moving to the Wikipedia domain, many new con-
structions and lexical items were encountered (see
(Baldwin et al, 2005) for a similar experience
with the BNC) and, at the same time, the require-
ments on parsing efficiency increased.
2.1 Lexical Development
When first parsing a new corpus, the grammar en-
counters new words that were previously unknown
to the morphology. The morphology falls back to a
guesser that uses regular expressions to guess the
part of speech and other features associated with
an unknown form. For example, a novel word end-
ing in s might be a plural noun. The grammar
records a feature LEX-SOURCE with the value
guesser for all guessed words. Oceanography was
used to extract all guessed forms and their parts
of speech. In many cases, the guesser had cor-
rectly identified the word?s part of speech. How-
ever, words that occurred frequently were added to
the morphology to avoid the possibility that they
would be incorrectly guessed as a different part of
speech. The fact that Oceanography was able to
identify not just the word, but its posited part of
speech and frequency in the corpus greatly sped
lexical development.
Incorrect guessing of verbs was of particular
concern to the grammar writers, as misidentifica-
tion of verbs was almost always accompanied by
a bad parse. In addition, subcategorization frames
for guessed verbs were guessed as either transi-
tive or intransitive, which often proved to be in-
correct. As such, the guessed verbs extracted us-
ing Oceanography were hand curated: true verbs
were added to the morphology and their subcate-
gorization frames to the lexicon. Due to the high
rate of error with guessed verbs, once the correctly
guessed verbs were added to the morphology, this
65
option was removed from the guesser.5
Overall, ?4200 new stems were added to the
already substantial morphology, with correct in-
flection. Approximately ?1300 of these were
verbs. The decision to eliminate verbs as possi-
ble guessed parts of speech was directly motivated
by data extracted using Oceanography.
Since the guesser works with regular expres-
sions (e.g. lowercase letters + s form plural
nouns), it is possible to encounter forms in
the corpus that neither the morphology nor the
guesser recognize. The grammar will fragment on
these sentences, creating well-formed f-structure
chunks but no single spanning parse, and the un-
recognized forms will be recorded as TOKENs
(Riezler et al, 2002). An Oceanography run ex-
tracting all TOKENs resulted in the addition of sev-
eral new patterns to the guesser as well as the addi-
tion of some of the frequent forms to the morphol-
ogy. For example, sequences of all upper case let-
ters followed by a hyphen and then by a sequence
of digits were added for forms like AK-47, F-22,
and V-1.
The guesser and TOKENs Oceanography runs
look for general problems with the morphology
and lexicon, and can be run for every new cor-
pus. More specific jobs are run when evaluating
whether to implement a new analysis, or when
evaluating whether a current analysis is function-
ing properly. For example, use of the van No-
ord tool indicated that the grammar had problems
with certain less common multiword prepositions
(e.g. pursuant to, in contrast with). Once these
multiword prepositions were added, the question
then arose as to whether more common preposi-
tions should be multiwords when stacked (e.g. up
to, along with). An Oceanography run was per-
formed to extract all occurrences of stacked prepo-
sitions from the corpus. Their frequency was tal-
lied in both the stacked formations and when used
as simple prepositions. With this information, we
determined which stacked configurations to add to
the lexicon as multiword prepositions, while main-
taining preposition stacking for less common com-
binations.
2.2 Grammar Rule Development
In addition to using Oceanography to help develop
the morphology and lexicon, it has also proven ex-
5It is simple to turn the guessed verbs back on in order to
run the same Oceanography experiment with a new corpus.
tremely useful in grammar rule development. In
general, the issue is not in finding constructions
which the grammar does not cover correctly: a
quick investigation of sentences which fragment
can provide these and issues are identified and re-
ported by the semantics which uses the syntax out-
put as its input. Furthermore, the van Noord tool
can be used to effectively identify gaps in gram-
mar rule coverage.
Rather, the more pressing issues include
whether it is worthwhile adding a construction,
which possible solution to pick (when it is worth-
while), and whether an existing solution is ap-
plying correctly and efficiently. Being able to
look at the occurrence of a construction over large
amounts of data can help with all of these issues,
especially when combined with searching over
gold standard treebanks such as the Penn Tree-
bank.
Determining which constructions to examine
using Oceanography is often the result of failure
analysis findings on components outside the gram-
mar itself, but that build on the grammar?s output
later in the natural language processing pipeline.
The point we wish to emphasize here is that the
grammar engineer?s effectiveness can greatly ben-
efit from being able to take a set of problematic
data gathered from massive parser output and de-
termine from it that a particular construction mer-
its closer scrutiny.
2.2.1 When relative/subordinate clauses
An observation that subordinate clauses contain-
ing when (e.g. Mary laughed when Ed tripped.)
were sometimes misanalyzed as relative clauses
attaching to a noun (e.g. the time when Ed tripped)
prompted a more directed analysis of whether
when relative clauses should be allowed to at-
tach to nouns that were not time-related expres-
sions (e.g. time, year, day). An Oceanography run
was performed to extract all when relative clauses,
the modified nominal head, and the sentence con-
taining the construction. A frequency-sorted list
of nouns taking when relative clause modifiers
helped to direct hand-examination ofwhen relative
clauses for accuracy of the analysis. This yielded
some correct analyses:
(1) There are times [when a Bigfoot sighting or
footprint is a hoax].
More importantly, however, the search revealed
many incorrect analyses of when subordinate
66
clauses as relative clauses:
(2) He gets the last laugh [when he tows away
his boss? car as well as everyone else?s].
By extracting all when relative clauses, and their
head nouns, it was determined that the construc-
tion was generally only correct for a small class of
time expression nominals. Comparatively, when
relative clause modification of other nominals was
rarely correct. The grammar was modified to dis-
prefer relative clause analyses of when clauses un-
less the head noun was an expression of time. As
a result, the overall quality of parses for all sen-
tences containing when subordinate clauses was
improved.
2.2.2 Relative clauses modifying gerunds
Another example of an issue with the accuracy of a
grammatical analysis concerns gerund nouns mod-
ified by relative clauses without an overt relative
pronoun (e.g. the singing we liked). It was ob-
served that many strings were incorrectly analyzed
as a gerund and reduced relative clause modifier:
(3) She lost all of her powers, including [her
sonic screams].
Again, a frequency sorted list of gerunds modi-
fied by reduced relative clauses helped to guide
hand inspection of the instances of this construc-
tion. By extracting all of the gerunds with re-
duced relative clause modifiers, it was possible
to see which gerunds were appearing in this con-
struction (e.g. including occurred alarmingly fre-
quently) and how rarely the overall analysis was
correct. As a result of the data analysis, such rel-
ative clause modifiers are now dispreferred in the
grammar and certain verbs (e.g. include) are addi-
tionally dispreferred as gerunds in general. Note
that this type of failure analysis is not possible
with a tool (such as the van Noord tool) that only
points out gaps in grammar coverage.
2.2.3 Noun-noun compounds
As part of the semantic search application,
argument-relation triples are extracted from the
corpus and presented to the user as a form of sum-
mary over what Wikipedia knows about a partic-
ular entity. These are referred to as Factz. For
example, a search on Noam Chomsky will find
Factz triples as in Figure 2. Such an application
highlights parse problems, since the predicate-
argument relations displayed are ultimately ex-
tracted from the syntactic parses themselves.
One class of problem arises when forms which
are ambiguous between nominal and verbal analy-
ses are erroneously analyzed as verbs and hence
show up as Factz relations. This is particularly
troublesome when the putative verb is part of a
noun-noun compound (e.g. ice cream, hot dog)
and the verb form is comparatively rare. A list
of potentially problematic noun-noun compounds
was extracted by using an independent part of
speech tagger over the sentences that generated the
Factz triples. If the relation in the triple was tagged
as a noun and was not a deverbal noun (e.g. de-
struction, writing), then the first argument of the
triple and the relation were tagged as potentially
problematic noun-noun compounds. Oceanogra-
phy was then used to determine the relative fre-
quency of whether the word pairs were analyzed as
noun-noun compounds, verb-argument relations,
or independent nouns and verbs.
This distributional information, in conjunction
with information about known noun-noun com-
pounds in WordNet (Fellbaum, 1998), is being
used to extract a set of ?100,000 noun-noun com-
pounds whose analysis is extremely strongly pre-
ferred by the grammar. Currently, these are con-
strained via c-structure optimality marks6 but they
may eventually be allowed only as noun-noun
compounds if the list proves reliable enough.
3 Tracking Grammar Progress
The grammar is used as part of a larger applica-
tion which is actively being developed and which
is regularly updated. As such, new versions of
the grammar are regularly released. Each release
includes a detailed list of improvements and bug
fixes, as well as requirements on other compo-
nents of the system (e.g. the grammar may require
a specific version of the XLE parser or of the mor-
phology). It is extremely important to be able to
confirm that the changes to the grammar are in
place and are functioning as expected when used
in the pipeline. Some changes can be confirmed
by browsing documents, finding a sentence likely
to contain the relevant lexical item or construction,
and then inspecting the syntactic structures for that
6See (Frank et al, 2001) and (Crouch et al, 2009) on the
use of Optimality Theory marks within XLE. C-structure op-
timality marks apply preferences to the context free backbone
before any constraints supplied by the f-structure annotations
are applied. This means that the noun-noun compounds will
be the only analysis possible if any tree can be constructed
with them.
67
Figure 2: Example Factz
document.
3.1 Confirming Grammar Changes
However, some changes are more complicated to
confirm either because it is hard to determine from
a sentence whether the grammar change would ap-
ply or because the change is more frequency re-
lated. For these types of changes, Oceanogra-
phy runs can detect whether a rare change oc-
curred at all, alleviating the need to search through
documents by hand. For example, to determine
whether the currency symbols are being correctly
treated by the grammar, especially the ones that
are not standard ASCII (e.g. the euro and yen sym-
bols), two simple XFR rules can be written: one
that looks for the relevant c-structure leaf node and
counts up which symbols occur under this node
and one that looks for the known list of currency
symbols in the f-structure and counts up what part-
of-speech they were analyzed as.
To detect whether frequency related changes
to the grammar are behaving as expected, two
Oceanography runs can be compared, one with
the older grammar and one with the newer one.
For example, to determine whether relative clauses
headed by when were dispreferred relative to
subordinate clauses, the number of such relative
clauses and such subordinate clauses were counted
in two successive runs; the relative occurrence of
the types confirmed that the preference mecha-
nism was working correctly. In addition, a quick
examination of sentences containing each type
showed that the change was not over-applying
(e.g. incorrectly analyzing when relative clauses as
subordinate clauses).
3.2 General Grammar Checking
In addition to Oceanography runs done to check
on specific changes to the grammar, a core set of
XFR rules extracts all of the features from the f-
structure and counts them. The resulting statistics
of features and counts are computed for each ma-
jor release and compared to that of the previous
release. This provides a list of new features which
subsequent components must be alerted to (e.g. a
feature added to indicate what type of punctua-
tion surrounded a parenthetical). It also provides a
quick check of whether some feature is no longer
occurring with the same frequency. In some cases
this is expected; once many guessed forms were
added to the lexicon, the feature indicating that
the guesser had applied dropped sharply. How-
ever, unexpected steep variations from previous
runs can be investigated to make sure that rules
were not inadvertently removed from the gram-
mar, and that rules added to the grammar are func-
tioning correctly.
4 Using Grammar Output to Develop
Other Components
In addition to being used in development of the
grammar itself, examination of the grammar out-
put can be useful for engineering efforts on other
components. In addition to the examples cited
above concerning the development of the mor-
phology used by the grammar, we discuss one sim-
ple example here. The sentence breaker used in
the pipeline is designed for high precision; it only
breaks sentences when it is sure that there is a sen-
tence break. To make up for breaks that may have
been missed, the grammar contains a rule that al-
lows multiple sentences to be parsed as a single
string. The resulting f-structure has the final sen-
tence?s f-structure as the value of a feature, LAST,
and the remainder as the value of a feature, REST.
The grammar iteratively parses multiple sentences
into these LAST-REST structures. Because the fea-
ture LAST is only instantiated when parsing mul-
tiple sentences, input strings whose parses con-
tained a LAST component could be extracted to
determine whether the sentence breaker?s behavior
should be changed. An example of two sentences
which were not broken is:
68
(4) The current air staff includes former CNN
Headline News gal Holly Firfer in the morn-
ings with co-host Orff. Mid-days is Mara
Davis, who does a theme lunch hour.
The relatively short unknown wordOrff before the
period makes it unclear whether this is an abbrevi-
ation or not. Based on the Oceanography analysis,
the number of unbroken sentences which received
analyses was roughly halved and one bug concern-
ing footnote markers was discovered and fixed.
5 Conclusion
Large-scale grammars are increasingly being used
in applications. In order to maximize their effec-
tiveness in terms of coverage, accuracy, and effi-
ciency for a given application, it is increasingly
important to examine the behavior of the grammar
on the relevant corpus and in the relevant applica-
tion.
Having good tools makes the grammar engi-
neer?s task of massive data driven grammar de-
velopment significantly easier. In this paper we
have discussed how such a tool, which can ap-
ply search patterns over the syntactic (and seman-
tic) representations of Wikipedia, is being used in
a semantic search research vertical. When used
in conjunction with existing tools for detecting
gaps in parser coverage (e.g. the van Noord tool),
Oceanography greatly aids in the evaluation of ex-
isting linguistic analyses from the parser. In ad-
dition, oceanography provides vital information to
determining whether or not to implement coverage
for a particular construction, based on efficiency
requirements. Thus, the grammar writer has a
suite of tools available to address the questions
raised in the introduction of this paper: what gaps
exist in parser coverage, how to best address those
gaps, and whether existing analyses are function-
ing appropriately. We hope that our experiences
encourage other grammar engineers to use similar
techniques in their grammar development efforts.
Acknowledgments
We would like to thank Scott Waterman for creat-
ing Oceanography and adapting it to our needs.
References
Timothy Baldwin, John Beavers, Emily M. Bender,
Dan Flickinger, Ara Kim, and Stephan Oepen.
2005. Beauty and the beast: What running a
broad-coverage precision grammar over thee bnc
taught us about the grammar ? and the corpus.
In Stephan Kepser and Marga Reis, editors, Lin-
guistic Evidence: Empirical, Theoretical, and Com-
putational Perspectives, pages 49?70. Mouton de
Gruyter, Berlin.
Gosse Bouma and Geert Kloosterman. 2002. Query-
ing dependency treebanks in XML. In Proceedings
of the Third international conference on Language
Resources and Evaluation (LREC), Gran Canaria.
Gosse Bouma and Geert Kloosterman. 2007. Mining
syntactically annotated corpora using XQuery. In
Proceedings of the Linguistic Annotation Workshop,
Prague, June. ACL.
Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-
via Hansen, Esther Ko?nig, Wolfgang Lezius, Chris-
tian Rohrer, George Smith, and Hans Uszkoreit.
2004. TIGER: Linguistic interpretation of a German
corpus. Research on Language and Computation,
2:597?620.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nin?o, and Fre?de?rique Segond. 1999. A Grammar
Writer?s Cookbook. CSLI Publications.
Miram Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project. In COLING2002 Work-
shop on Grammar Engineering and Evaluation,
pages 1?7.
Robin Cooper, Dick Crouch, Jan van Eijck, Chris
Fox, Josef van Genabith, Jan Jaspars, Hans Kamp,
David Milward, Manfred Pinkal, Massimo Poesio,
and Steve Pulman. 1996. Using the framework.
FraCas: A Framework for Computational Semantics
(LRE 62-051).
Dick Crouch, Mary Dalrymple, Ronald Kaplan,
Tracy Holloway King, John T. Maxwell III, and
Paula Newman. 2009. XLE Documentation. On-
line.
Mary Dalrymple. 2001. Lexical Functional Grammar.
Syntax and Semantics. Academic Press.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell III. 2001. Optimality theory style
constraint ranking in large-scale LFG grammars. In
Peter Sells, editor, Formal and Empirical Issues in
Optimality Theoretic Syntax, pages 367?397. CSLI
Publications.
Tracy Holloway King and John T. Maxwell, III. 2007.
Overlay mechanisms for multi-level deep processing
applications. In Proceedings of the Grammar En-
gineering Across Frameworks (GEAF07) Workshop.
CSLI Publications.
69
Sabine Lehmann, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herve? Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP
? Test Suites for Natural Language Processing. In
Proceedings of COLING 1996.
Wolfgang Lezius. 2002. Ein Suchwerkzeug fu?r syn-
taktisch annotierte Textkorpora (in German). Ph.D.
thesis, IMS, University of Stuttgart Arbeitspapiere
des Instituts fu?r Maschinelle Sprachverarbeitung
(AIMS). volume 8, number 4.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn treebank: Annotating
predicate argument structure. In ARPA Human
Language Technology Workshop.
Jir??? M??rovsky?. 2008. PDT 2.0 requirements on a query
language. In Proceedings of ACL-08: HLT, pages
37?45. Association for Computational Linguistics.
John Nerbonne, Dan Flickinger, and Tom Wasow.
1988. The HP Labs natural language evaluation
tool. In Proceedings of the Workshop on Evaluation
of Natural Language Processing Systems.
Stefan Riezler, Tracy Holloway King, Ronald Kaplan,
Dick Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
lexical-functional grammar and discriminative esti-
mation techniques. In Proceedings of the ACL.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of
ACL.
Scott A. Waterman. 2009. Distributed parse mining.
In Proceedings of the NAACL Workshop on Soft-
ware Engineering, Testing, and Quality Assurance
for Natural Language Processing.
70
Book Reviews
Unification Grammars
? 2012 Association for Computational Linguistics
Nissim Francez? and Shuly Wintner?
(?Technion?Israel Institute of Technology and ?University of Haifa)
Cambridge University Press, 2012, xii+312 pp; hardbound, ISBN 978-1-107-01417-6,
$95.00
Reviewed by
Tracy Holloway King
eBay Inc.
Francez andWintner?s textbook on unification grammars is aimed at students interested
in computational linguistics who are at the advanced undergraduate or introductory
graduate level. As stated by the authors, the book assumes a solid introductory course
in syntax and a year of undergraduate mathematics, but no programming experience;
I agree with this assessment. The book will also be of interest to anyone working
with unification grammars?for example, an HPSG or LFG theoretician or grammar
engineer, who wants to understand more about the mathematical underpinnings of the
systems they are working on; even without a strong mathematics background, large
portions of the book will be accessible for motivated readers with previous unification
grammar experience.
Although at first glance some readers may be intimidated by the technical depth
of the book, the material is made accessible by its presentation style. Concepts are
introduced in stages, with frequent references to earlier concepts and sections. Each
concept is introduced in prose, in relevant proofs, and by examples tied to linguistic
issues, thereby reinforcing the material. Exercises are provided throughout each chapter
so that readers can check their understanding as they work through the material; many
of the exercises have answers provided in the back of the book. Another strength of the
book is the detailed further-reading section at the end of each chapter: These provide
historical background as well as an introduction to more-advanced topics.
The book comprises seven chapters, three appendices, the bibliography, and an
index.
Chapter 1: ?Introduction.? The introduction overviews several major issues in the
syntax of natural languages (e.g., parts of speech, subcategorization, control, long-
distance dependencies, and coordination). There is also an overview of formal lan-
guages and context-free grammars. These are then linked by a discussion of some of the
arguments against natural languages being context-free. Building on this discussion,
mildly context-sensitive languages are introduced. This chapter will serve as a review
of relevant concepts for most readers.
Chapter 2: ?Feature Structures.? Feature structures are introduced as a way of ex-
tending context-free grammars to express linguistic information, using examples from
agreement as motivation. The connection between feature graphs, feature structures,
abstract feature structures, and attribute-value matrices is presented in detail. Special
attention is paid to reentrancies and cycles, two key issues in the formal and practical
understanding of feature structures. Numerous graphical examples illustrate the formal
proofs and help to provide the intuition behind the concepts and how they relate to one
another.
Computational Linguistics Volume 38, Number 2
Chapter 3: ?Unification.? Using the structures presented in Chapter 2, unification is
introduced as the mechanism to combine the information in two compatible feature
structures. Again, graphical examples provide an intuitive view into the formalisms
introduced. A simple, destructive algorithm for unification is introduced and linked to
the formal definitions (computational aspects of unification grammars are discussed in
detail in Chapter 6). Generalization is briefly discussed as the dual of unification.
Chapter 4: ?Unification Grammars.? In order to capture natural language phenomena,
feature structures are extended to multirooted feature structures and then combined
with unification to form unification grammars. Grammar rules and derivations, along
with the lexicon, are defined. Comparisons to context-free grammars and their limita-
tions are provided to further exemplify the formal power of unification grammars.
Chapter 5: ?Linguistic Applications.? The formalisms introduced in Chapters 2?4
are then used to account for a variety of linguistic phenomena, including traditional
?movement? phenomena, by starting with a simple unification grammar for a fragment
of English and gradually extending it. Examples are provided of where the grammar
engineer must choose among different ways to formulate the grammar rules within
the unification grammar formalism and how these different choices can reflect different
linguistic generalizations as well as have different computational costs.
Chapter 6: ?Computational Aspects of Unification Grammars.? This chapter provides a
solid overview of computational complexity and then discusses how unification gram-
mars fit into the picture. Issues with recognition and parsing are discussed. Examples
are worked out in detail (e.g., showing relevant dotted rules) for context-free grammars
and are then extended to unification grammars. This chapter could be skipped by
those focused on theoretical and formal aspects, but it provides a practical view of
the repercussions of the formal issues introduced earlier and is directly relevant for
computational linguists.
There is a short concluding chapter. The three appendices comprise a list of symbols
used in the book, a summary of preliminary mathematical notions, and solutions to
selected exercises.
In sum, this book will be an excellent textbook for computational linguistics classes,
especially in programs that have a grammar engineering track or that want to build on
a strong formal language program. I also particularly recommend it for those working
with unification grammars, especially with implementations of such grammars.
Tracy Holloway King is a principal product manager with eBay?s search science team. Previously
she focused on LFG grammar engineering and applications at The Palo Alto Research Center. Her
address is eBay Inc., 2065 Hamilton Ave, San Jose, CA 95125, USA; e-mail: tracyking@ebay.com.
442
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 550?560,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ParGramBank: The ParGram Parallel Treebank
Sebastian Sulger and Miriam Butt
University of Konstanz, Germany
{sebastian.sulger|miriam.butt}@uni-konstanz.de
Tracy Holloway King
eBay Inc., USA
tracyking@ebay.com
Paul Meurer
Uni Research AS, Norway
paul.meurer@uni.no
Tibor Laczko? and Gyo?rgy Ra?kosi
University of Debrecen, Hungary
{laczko.tibor|rakosi.gyorgy}@arts.unideb.hu
Cheikh Bamba Dione and Helge Dyvik and Victoria Rose?n and Koenraad De Smedt
University of Bergen, Norway
dione.bamba@lle.uib.no, {dyvik|victoria|desmedt}@uib.no
Agnieszka Patejuk
Polish Academy of Sciences
aep@ipipan.waw.pl
O?zlem C?etinog?lu
University of Stuttgart, Germany
ozlem@ims.uni-stuttgart.de
I Wayan Arka* and Meladel Mistica+
*Australian National University and Udayana University, Indonesia
+Australian National University
wayan.arka@anu.edu.au, meladel.mistica@gmail.com
Abstract
This paper discusses the construction of
a parallel treebank currently involving ten
languages from six language families. The
treebank is based on deep LFG (Lexical-
Functional Grammar) grammars that were
developed within the framework of the
ParGram (Parallel Grammar) effort. The
grammars produce output that is maxi-
mally parallelized across languages and
language families. This output forms the
basis of a parallel treebank covering a
diverse set of phenomena. The treebank
is publicly available via the INESS tree-
banking environment, which also allows
for the alignment of language pairs. We
thus present a unique, multilayered paral-
lel treebank that represents more and dif-
ferent types of languages than are avail-
able in other treebanks, that represents
deep linguistic knowledge and that allows
for the alignment of sentences at sev-
eral levels: dependency structures, con-
stituency structures and POS information.
1 Introduction
This paper discusses the construction of a parallel
treebank currently involving ten languages that
represent several different language families, in-
cluding non-Indo-European. The treebank is based
on the output of individual deep LFG (Lexical-
Functional Grammar) grammars that were deve-
loped independently at different sites but within
the overall framework of ParGram (the Parallel
Grammar project) (Butt et al, 1999a; Butt et al,
2002). The aim of ParGram is to produce deep,
wide coverage grammars for a variety of lan-
guages. Deep grammars provide detailed syntactic
analysis, encode grammatical functions as well as
550
other grammatical features such as tense or aspect,
and are linguistically well-motivated. The Par-
Gram grammars are couched within the linguis-
tic framework of LFG (Bresnan, 2001; Dalrymple,
2001) and are constructed with a set of grammati-
cal features that have been commonly agreed upon
within the ParGram group. ParGram grammars are
implemented using XLE, an efficient, industrial-
strength grammar development platform that in-
cludes a parser, a generator and a transfer sys-
tem (Crouch et al, 2012). XLE has been devel-
oped in close collaboration with the ParGram
project. Over the years, ParGram has continu-
ously grown and includes grammars for Ara-
bic, Chinese, English, French, German, Georgian,
Hungarian, Indonesian, Irish, Japanese, Mala-
gasy, Murrinh-Patha, Norwegian, Polish, Spanish,
Tigrinya, Turkish, Urdu, Welsh and Wolof.
ParGram grammars produce output that has
been parallelized maximally across languages ac-
cording to a set of commonly agreed upon uni-
versal proto-type analyses and feature values. This
output forms the basis of the ParGramBank paral-
lel treebank discussed here. ParGramBank is con-
structed using an innovative alignment methodol-
ogy developed in the XPAR project (Dyvik et al,
2009) in which grammar parallelism is presup-
posed to propagate alignment across different pro-
jections (section 6). This methodology has been
implemented with a drag-and-drop interface as
part of the LFG Parsebanker in the INESS infras-
tructure (Rose?n et al, 2012; Rose?n et al, 2009).
ParGramBank has been constructed in INESS and
is accessible in this infrastructure, which also of-
fers powerful search and visualization.
In recent years, parallel treebanking1 has gained
in importance within NLP. An obvious applica-
tion for parallel treebanking is machine transla-
tion, where treebank size is a deciding factor for
whether a particular treebank can support a par-
ticular kind of research project. When conduct-
ing in-depth linguistic studies of typological fea-
tures, other factors such as the number of in-
cluded languages, the number of covered phe-
nomena, and the depth of linguistic analysis be-
come more important. The treebanking effort re-
ported on in this paper supports work of the lat-
ter focus, including efforts at multilingual depen-
dency parsing (Naseem et al, 2012). We have
1Throughout this paper ?treebank? refers to both phrase-
structure resources and their natural extensions to depen-
dency and other deep annotation banks.
created a parallel treebank whose prototype in-
cludes ten typologically diverse languages and re-
flects a diverse set of phenomena. We thus present
a unique, multilayered parallel treebank that rep-
resents more languages than are currently avail-
able in other treebanks, and different types of lan-
guages as well. It contains deep linguistic knowl-
edge and allows for the parallel and simultane-
ous alignment of sentences at several levels. LFG?s
f(unctional)-structure encodes dependency struc-
tures as well as information that is equivalent to
Quasi-Logical Forms (van Genabith and Crouch,
1996). LFG?s c(onstituent)-structure provides in-
formation about constituency, hierarchical rela-
tions and part-of-speech. Currently, ParGramBank
includes structures for the following languages
(with the ISO 639-3 code and language fam-
ily): English (eng, Indo-European), Georgian (kat,
Kartvelian), German (deu, Indo-European), Hun-
garian (hun, Uralic), Indonesian (ind, Austrone-
sian), Norwegian (Bokma?l) (nob, Indo-European),
Polish (pol, Indo-European), Turkish (tur, Altaic),
Urdu (urd, Indo-European) and Wolof (wol, Niger-
Congo). It is freely available for download under
the CC-BY 3.0 license via the INESS treebanking
environment and comes in two formats: a Prolog
format and an XML format.2
This paper is structured as follows. Section
2 discusses related work in parallel treebanking.
Section 3 presents ParGram and its approach to
parallel treebanking. Section 4 focuses on the tree-
bank design and its construction. Section 5 con-
tains examples from the treebank, focusing on ty-
pological aspects and challenges for parallelism.
Section 6 elaborates on the mechanisms for paral-
lel alignment of the treebank.
2 Related Work
There have been several efforts in parallel tree-
banking across theories and annotation schemes.
Kuhn and Jellinghaus (2006) take a mini-
mal approach towards multilingual parallel tree-
banking. They bootstrap phrasal alignments over
a sentence-aligned parallel corpus of English,
French, German and Spanish and report concrete
treebank annotation work on a sample of sen-
tences from the Europarl corpus. Their annotation
2http://iness.uib.no. The treebank is in the
public domain (CC-BY 3.0). The use of the INESS platform
itself is not subject to any licensing. To access the treebank,
click on ?Treebank selection? and choose the ParGram collec-
tion.
551
scheme is the ?leanest? possible scheme in that it
consists solely of a bracketing for a sentence in
a language (where only those units that play the
role of a semantic argument or modifier in a larger
unit are bracketed) and a correspondence relation
of the constituents across languages.
Klyueva and Marec?ek (2010) present a small
parallel treebank using data and tools from two
existing treebanks. They take a syntactically an-
notated gold standard text for one language and
run an automated annotation on the parallel text
for the other language. Manually annotated Rus-
sian data are taken from the SynTagRus treebank
(Nivre et al, 2008), while tools for parsing the cor-
responding text in Czech are taken from the Tec-
toMT framework (Popel and Z?abokrtsky?, 2010).
The SMULTRON project is concerned with con-
structing a parallel treebank of English, German
and Swedish. The sentences have been POS-tagged
and annotated with phrase structure trees. These
trees have been aligned on the sentence, phrase
and word level. Additionally, the German and
Swedish monolingual treebanks contain lemma in-
formation. The treebank is distributed in TIGER-
XML format (Volk et al, 2010).
Megyesi et al (2010) discuss a parallel English-
Swedish-Turkish treebank. The sentences in each
language are annotated morphologically and syn-
tactically with automatic tools, aligned on the
sentence and the word level and partially hand-
corrected.3
A further parallel treebanking effort is Par-
TUT, a parallel treebank (Sanguinetti and Bosco,
2011; Bosco et al, 2012) which provides depen-
dency structures for Italian, English and French
and which can be converted to a CCG (Combina-
tory Categorial Grammar) format.
Closest to our work is the ParDeepBank, which
is engaged in the creation of a highly paral-
lel treebank of English, Portuguese and Bulgar-
ian. ParDeepBank is couched within the linguistic
framework of HPSG (Head-Driven Phrase Struc-
ture Grammar) and uses parallel automatic HPSG
grammars, employing the same tools and imple-
mentation strategies across languages (Flickinger
et al, 2012). The parallel treebank is aligned on
the sentence, phrase and word level.
In sum, parallel treebanks have so far fo-
cused exclusively on Indo-European languages
3The paper mentions Hindi as the fourth language, but
this is not yet available: http://stp.lingfil.uu.
se/?bea/turkiska/home-en.html.
(with Turkish providing the one exception) and
generally do not extend beyond three or four
languages. In contrast, our ParGramBank tree-
bank currently includes ten typologically differ-
ent languages from six different language families
(Altaic, Austronesian, Indo-European, Kartvelian,
Niger-Congo, Uralic).
A further point of comparison with ParDeep-
Bank is that it relies on dynamic treebanks, which
means that structures are subject to change dur-
ing the further development of the resource gram-
mars. In ParDeepBank, additional machinery is
needed to ensure correct alignment on the phrase
and word level (Flickinger et al, 2012, p. 105).
ParGramBank contains finalized analyses, struc-
tures and features that were designed collabora-
tively over more than a decade, thus guaranteeing
a high degree of stable parallelism. However, with
the methodology developed within XPAR, align-
ments can easily be recomputed from f-structure
alignments in case of grammar or feature changes,
so that we also have the flexible capability of
allowing ParGramBank to include dynamic tree-
banks.
3 ParGram and its Feature Space
The ParGram grammars use the LFG formalism
which produces c(onstituent)-structures (trees)
and f(unctional)-structures as the syntactic anal-
ysis. LFG assumes a version of Chomsky?s Uni-
versal Grammar hypothesis, namely that all lan-
guages are structured by similar underlying prin-
ciples (Chomsky, 1988; Chomsky, 1995). Within
LFG, f-structures encode a language universal
level of syntactic analysis, allowing for crosslin-
guistic parallelism at this level of abstraction. In
contrast, c-structures encode language particular
differences in linear word order, surface morpho-
logical vs. syntactic structures, and constituency
(Dalrymple, 2001). Thus, while the Chomskyan
framework is derivational in nature, LFG departs
from this view by embracing a strictly representa-
tional approach to syntax.
ParGram tests the LFG formalism for its uni-
versality and coverage limitations to see how far
parallelism can be maintained across languages.
Where possible, analyses produced by the gram-
mars for similar constructions in each language are
parallel, with the computational advantage that the
grammars can be used in similar applications and
that machine translation can be simplified.
552
The ParGram project regulates the features and
values used in its grammars. Since its inception
in 1996, ParGram has included a ?feature com-
mittee?, which collaboratively determines norms
for the use and definition of a common multilin-
gual feature and analysis space. Adherence to fea-
ture committee decisions is supported technically
by a routine that checks the grammars for com-
patibility with a feature declaration (King et al,
2005); the feature space for each grammar is in-
cluded in ParGramBank. ParGram also conducts
regular meetings to discuss constructions, analy-
ses and features.
For example, Figure 1 shows the c-structure
of the Urdu sentence in (1) and the c-structure
of its English translation. Figure 2 shows the f-
structures for the same sentences. The left/upper
c- and f-structures show the parse from the En-
glish ParGram grammar, the right/lower ones from
Urdu ParGram grammar.4,5 The c-structures en-
code linear word order and constituency and thus
look very different; e.g., the English structure is
rather hierarchical while the Urdu structure is flat
(Urdu is a free word-order language with no evi-
dence for a VP; Butt (1995)). The f-structures, in
contrast, are parallel aside from grammar-specific
characteristics such as the absence of grammati-
cal gender marking in English and the absence of
articles in Urdu.6
(1) ? Aj J
K. Q?K
QK A 	JK @ ?

	
G
	
?A??
kisAn=nE apnA
farmer.M.Sg=Erg self.M.Sg
TrEkTar bEc-A
tractor.M.Sg sell-Perf.M.Sg
?Did the farmer sell his tractor??
With parallel analyses and parallel features, maxi-
mal parallelism across typologically different lan-
guages is maintained. As a result, during the con-
struction of the treebank, post-processing and con-
version efforts are kept to a minimum.
4The Urdu ParGram grammar makes use of a translitera-
tion scheme that abstracts away from the Arabic-based script;
the transliteration scheme is detailed in Malik et al (2010).
5In the c-structures, dotted lines indicate distinct func-
tional domains; e.g., in Figure 1, the NP the farmer and the
VP sell his tractor belong to different f-structures: the former
maps onto the SUBJ f-structure, while the latter maps onto the
topmost f-structure (Dyvik et al, 2009). Section 6 elaborates
on functional domains.
6The CASE feature also varies: since English does not
distinguish between accusative, dative, and other oblique
cases, the OBJ is marked with a more general obl CASE.
Figure 1: English and Urdu c-structures
We emphasize the fact that ParGramBank is
characterized by a maximally reliable, human-
controlled and linguistically deep parallelism
across aligned sentences. Generally, the result of
automatic sentence alignment procedures are par-
allel corpora where the corresponding sentences
normally have the same purported meaning as
intended by the translator, but they do not nec-
essarily match in terms of structural expression.
In building ParGramBank, conscious attention is
paid to maintaining semantic and constructional
parallelism as much as possible. This design fea-
ture renders our treebank reliable in cases when
the constructional parallelism is reduced even at f-
structure. For example, typological variation in the
presence or absence of finite passive constructions
represents a case of potential mismatch. Hungar-
ian, one of the treebank languages, has no produc-
tive finite passives. The most common strategy in
translation is to use an active construction with a
topicalized object, with no overt subject and with
3PL verb agreement:
(2) A fa?-t ki-va?g-t-a?k.
the tree-ACC out-cut-PAST-3PL
?The tree was cut down.?
In this case, a topicalized object in Hungarian has
to be aligned with a (topical) subject in English.
Given that both the sentence level and the phrase
level alignments are human-controlled in the tree-
bank (see sections 4 and 6), the greatest possible
parallelism is reliably captured even in such cases
of relative grammatical divergence.
553
Figure 2: Parallel English and Urdu f-structures
4 Treebank Design and Construction
For the initial seeding of the treebank, we focused
on 50 sentences which were constructed manu-
ally to cover a diverse range of phenomena (tran-
sitivity, voice alternations, interrogatives, embed-
ded clauses, copula constructions, control/raising
verbs, etc.). We followed Lehmann et al (1996)
and Bender et al (2011) in using coverage of
grammatical constructions as a key component for
grammar development. (3) lists the first 16 sen-
tences of the treebank. An expansion to 100 sen-
tences is scheduled for next year.
(3) a. Declaratives:
1. The driver starts the tractor.
2. The tractor is red.
b. Interrogatives:
3. What did the farmer see?
4. Did the farmer sell his tractor?
c. Imperatives:
5. Push the button.
6. Don?t push the button.
d. Transitivity:
7. The farmer gave his neighbor an old
tractor.
8. The farmer cut the tree down.
9. The farmer groaned.
e. Passives and traditional voice:
10. My neighbor was given an old tractor
by the farmer.
11. The tree was cut down yesterday.
12. The tree had been cut down.
13. The tractor starts with a shudder.
f. Unaccusative:
14. The tractor appeared.
g. Subcategorized declaratives:
15. The boy knows the tractor is red.
16. The child thinks he started the tractor.
The sentences were translated from English
into the other treebank languages. Currently, these
languages are: English, Georgian, German, Hun-
garian, Indonesian, Norwegian (Bokma?l), Polish,
Turkish, Urdu and Wolof. The translations were
done by ParGram grammar developers (i.e., expert
linguists and native speakers).
The sentences were automatically parsed with
ParGram grammars using XLE. Since the pars-
ing was performed sentence by sentence, our re-
sulting treebank is automatically aligned at the
sentence level. The resulting c- and f-structures
were banked in a database using the LFG Parse-
banker (Rose?n et al, 2009). The structures were
disambiguated either prior to banking using XLE
or during banking with the LFG Parsebanker and
its discriminant-based disambiguation technique.
The banked analyses can be exported and down-
loaded in a Prolog format using the LFG Parse-
banker interface. Within XLE, we automatically
convert the structures to a simple XML format and
make these available via ParGramBank as well.
The Prolog format is used with applications
which use XLE to manipulate the structures, e.g.
for further semantic processing (Crouch and King,
2006) or for sentence condensation (Crouch et al,
2004).
554
5 Challenges for Parallelism
We detail some challenges in maintaining paral-
lelism across typologically distinct languages.
5.1 Complex Predicates
Some languages in ParGramBank make extensive
use of complex predicates. For example, Urdu uses
a combination of predicates to express concepts
that in languages like English are expressed with
a single verb, e.g., ?memory do? = ?remember?,
?fear come? = ?fear?. In addition, verb+verb com-
binations are used to express permissive or as-
pectual relations. The strategy within ParGram is
to abstract away from the particular surface mor-
phosyntactic expression and aim at parallelism
at the level of f-structure. That is, monoclausal
predications are analyzed via a simple f-structure
whether they consist of periphrastically formed
complex predicates (Urdu, Figure 3), a simple
verb (English, Figure 4), or a morphologically de-
rived form (Turkish, Figure 5).
In Urdu and in Turkish, the top-level PRED
is complex, indicating a composed predicate. In
Urdu, this reflects the noun-verb complex predi-
cate sTArT kar ?start do?, in Turkish it reflects a
morphological causative. Despite this morphosyn-
tactic complexity, the overall dependency struc-
ture corresponds to that of the English simple verb.
(4) ?


?
f
A

KQ ? HPA

J ? ? ? Q

 ? K
Q

K P?

J K
 @P

X
DrAIvar TrEkTar=kO
driver.M.Sg.Nom tractor.M.Sg=Acc
sTArT kartA hE
start.M.Sg do.Impf.M.Sg be.Pres.3Sg
?The driver starts the tractor.?
(5) su?ru?cu? trakto?r-u? c?al?s?-t?r-?yor
driver.Nom tractor-Acc work-Caus-Prog.3Sg
?The driver starts the tractor.?
The f-structure analysis of complex predicates
is thus similar to that of languages which do not
use complex predicates, resulting in a strong syn-
tactic parallelism at this level, even across typo-
logically diverse languages.
5.2 Negation
Negation also has varying morphosyntactic sur-
face realizations. The languages in ParGramBank
differ with respect to their negation strategies.
Languages such as English and German use inde-
pendent negation: they negate using words such as
Figure 3: Complex predicate: Urdu analysis of (4)
Figure 4: Simple predicate: English analysis of (4)
adverbs (English not, German nicht) or verbs (En-
glish do-support). Other languages employ non-
independent, morphological negation techniques;
Turkish, for instance, uses an affix on the verb, as
in (6).
555
Figure 5: Causative: Turkish analysis of (5)
(6) du?g?me-ye bas-ma
button-Dat push-Neg.Imp
?Don?t push the button.?
Within ParGram we have not abstracted away
from this surface difference. The English not in
(6) functions as an adverbial adjunct that modifies
the main verb (see top part of Figure 6) and infor-
mation would be lost if this were not represented
at f-structure. However, the same cannot be said of
the negative affix in Turkish ? the morphological
affix is not an adverbial adjunct. We have there-
fore currently analyzed morphological negation as
adding a feature to the f-structure which marks the
clause as negative, see bottom half of Figure 6.
5.3 Copula Constructions
Another challenge to parallelism comes from co-
pula constructions. An approach advocating a uni-
form treatment of copulas crosslinguistically was
advocated in the early years of ParGram (Butt et
al., 1999b), but this analysis could not do justice to
the typological variation found with copulas. Par-
GramBank reflects the typological difference with
three different analyses, with each language mak-
ing a language-specific choice among the three
possibilities that have been identified (Dalrymple
et al, 2004; Nordlinger and Sadler, 2007; Attia,
2008; Sulger, 2011; Laczko?, 2012).
The possible analyses are demonstrated here
with respect to the sentence The tractor is red.
The English grammar (Figure 7) uses a raising ap-
proach that reflects the earliest treatments of cop-
ulas in LFG (Bresnan, 1982). The copula takes
a non-finite complement whose subject is raised
to the matrix clause as a non-thematic subject of
the copula. In contrast, in Urdu (Figure 8), the
Figure 6: Different f-structural analyses for nega-
tion (English vs. Turkish)
copula is a two-place predicate, assigning SUBJ
and PREDLINK functions. The PREDLINK function
is interpreted as predicating something about the
subject. Finally, in languages like Indonesian (Fig-
ure 9), there is no overt copula and the adjective is
the main predicational element of the clause.
Figure 7: English copula example
556
Figure 8: Urdu copula example
Figure 9: Indonesian copula example
5.4 Summary
This section discussed some challenges for main-
taining parallel analyses across typologically di-
verse languages. Another challenge we face is
when no corresponding construction exists in a
language, e.g. with impersonals as in the English
It is raining. In this case, we provide a translation
and an analysis of the structure of the correspond-
ing translation, but note that the phenomenon be-
ing exemplified does not actually exist in the lan-
guage. A further extension to the capabilities of
the treebank could be the addition of pointers from
the alternative structure used in the translation to
the parallel aligned set of sentences that corre-
spond to this alternative structure.
6 Linguistically Motivated Alignment
The treebank is automatically aligned on the sen-
tence level, the top level of alignment within Par-
GramBank. For phrase-level alignments, we use
the drag-and-drop alignment tool in the LFG Parse-
banker (Dyvik et al, 2009). The tool allows the
alignment of f-structures by dragging the index
of a subsidiary source f-structure onto the index
of the corresponding target f-structure. Two f-
structures correspond if they have translationally
matching predicates, and the arguments of each
predicate correspond to an argument or adjunct in
the other f-structure. The tool automatically com-
putes the alignment of c-structure nodes on the
basis of the manually aligned corresponding f-
structures.7
7Currently we have not measured inter-annotator agree-
ment (IAA) for the f-structure alignments. The f-structure
alignments were done by only one person per language pair.
We anticipate that multiple annotators will be needed for this
This method is possible because the c-structure
to f-structure correspondence (the ? relation) is
encoded in the ParGramBank structures, allow-
ing the LFG Parsebanker tool to compute which c-
structure nodes contributed to a given f-structure
via the inverse (??1) mapping. A set of nodes
mapping to the same f-structure is called a ?func-
tional domain?. Within a source and a target
functional domain, two nodes are automatically
aligned only if they dominate corresponding word
forms. In Figure 10 the nodes in each func-
tional domain in the trees are connected by whole
lines while dotted lines connect different func-
tional domains. Within a functional domain, thick
whole lines connect the nodes that share align-
ment; for simplicity the alignment is only indi-
cated for the top nodes. The automatically com-
puted c-structural alignments are shown by the
curved lines. The alignment information is stored
as an additional layer and can be used to ex-
plore alignments at the string (word), phrase (c-
)structure, and functional (f-)structure levels.
We have so far aligned the treebank pairs
English-Urdu, English-German, English-Polish
and Norwegian-Georgian. As Figure 10 illustrates
for (7) in an English-Urdu pairing, the English ob-
ject neighbor is aligned with the Urdu indirect ob-
ject (OBJ-GO) hamsAyA ?neighbor?, while the En-
glish indirect object (OBJ-TH) tractor is aligned
with the Urdu object TrEkTar ?tractor?. The c-
structure correspondences were computed auto-
matically from the f-structure alignments.
(7) AK
X Q?K
QK A 	K @QK ?? ?
G
A???f ?

	
?K @ ?

	
G
	
?A??
kisAn=nE apnE
farmer.M.Sg=Erg self.Obl
hamsAyE=kO purAnA
neighbor.M.Sg.Obl=Acc old.M.Sg
TrEkTar di-yA
tractor.M.Sg give-Perf.M.Sg
?The farmer gave his neighbor an old tractor.?
The INESS platform additionally allows for the
highlighting of connected nodes via a mouse-over
technique. It thus provides a powerful and flexible
tool for the semi-automatic alignment and subse-
task in the future, in which case we will measure IAA for this
step.
557
Figure 10: Phrase-aligned treebank example English-Urdu: The farmer gave his neighbor an old tractor.
quent inspection of parallel treebanks which con-
tain highly complex linguistic structures.8
7 Discussion and Future Work
We have discussed the construction of ParGram-
Bank, a parallel treebank for ten typologically
different languages. The analyses in ParGram-
Bank are the output of computational LFG Par-
Gram grammars. As a result of ParGram?s cen-
trally agreed upon feature sets and prototypical
analyses, the representations are not only deep
in nature, but maximally parallel. The representa-
tions offer information about dependency relations
as well as word order, constituency and part-of-
speech.
In future ParGramBank releases, we will pro-
vide more theory-neutral dependencies along with
the LFG representations. This will take the form of
triples (King et al, 2003). We also plan to provide
a POS-tagged and a named entity marked up ver-
sion of the sentences; these will be of use for more
general NLP applications and for systems which
use such markup as input to deeper processing.
8One reviewer inquires about possibilities of linking
(semi-)automatically between languages, for example using
lexical resources such as WordNets or Panlex. We agree that
this would be desirable, but unrealizable, since many of the
languages included in ParGramBank do not have a WordNet
resource and are not likely to achieve an adequate one soon.
Third, the treebank will be expanded to include
100 more sentences within the next year. We also
plan to include more languages as other ParGram
groups contribute structures to ParGramBank.
ParGramBank, including its multilingual sen-
tences and all annotations, is made freely avail-
able for research and commercial use under the
CC-BY 3.0 license via the INESS platform, which
supports alignment methodology developed in the
XPAR project and provides search and visualiza-
tion methods for parallel treebanks. We encourage
the computational linguistics community to con-
tribute further layers of annotation, including se-
mantic (Crouch and King, 2006), abstract knowl-
edge representational (Bobrow et al, 2007), Prop-
Bank (Palmer et al, 2005), or TimeBank (Mani
and Pustejovsky, 2004) annotations.
References
Mohammed Attia. 2008. A Unified Analysis of Cop-
ula Constructions. In Proceedings of the LFG ?08
Conference, pages 89?108. CSLI Publications.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2011. Grammar Engineering and Linguistic Hy-
pothesis Testing: Computational Support for Com-
plexity in Syntactic Analysis. In Emily M. Bender
and Jennifer E. Arnold, editors, Languages from a
Cognitive Perspective: Grammar, Usage and Pro-
cessing, pages 5?30. CSLI Publications.
558
Daniel G. Bobrow, Cleo Condoravdi, Dick Crouch,
Valeria de Paiva, Lauri Karttunen, Tracy Holloway
King, Rowan Nairn, Lottie Price, and Annie Zaenen.
2007. Precision-focused Textual Inference. In Pro-
ceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Cristina Bosco, Manuela Sanguinetti, and Leonardo
Lesmo. 2012. The Parallel-TUT: a multilingual and
multiformat treebank. In Proceedings of the Eighth
International Conference on Language Resources
and Evaluation (LREC-2012), pages 1932?1938, Is-
tanbul, Turkey. European Language Resources As-
sociation (ELRA).
Joan Bresnan. 1982. The Passive in Lexical Theory. In
Joan Bresnan, editor, The Mental Representation of
Grammatical Relations, pages 3?86. The MIT Press.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell Publishing.
Miriam Butt, Stefanie Dipper, Anette Frank, and
Tracy Holloway King. 1999a. Writing Large-
Scale Parallel Grammars for English, French and
German. In Proceedings of the LFG99 Conference.
CSLI Publications.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nin?o, and Fre?de?rique Segond. 1999b. A Grammar
Writer?s Cookbook. CSLI Publications.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project. In Proceedings of the
COLING-2002 Workshop on Grammar Engineering
and Evaluation, pages 1?7.
Miriam Butt. 1995. The Structure of Complex Predi-
cates in Urdu. CSLI Publications.
Noam Chomsky. 1988. Lectures on Government and
Binding: The Pisa Lectures. Foris Publications.
Noam Chomsky. 1995. The Minimalist Program. MIT
Press.
Dick Crouch and Tracy Holloway King. 2006. Seman-
tics via F-structure Rewriting. In Proceedings of the
LFG06 Conference, pages 145?165. CSLI Publica-
tions.
Dick Crouch, Tracy Holloway King, John T. Maxwell
III, Stefan Riezler, and Annie Zaenen. 2004. Ex-
ploiting F-structure Input for Sentence Condensa-
tion. In Proceedings of the LFG04 Conference,
pages 167?187. CSLI Publications.
Dick Crouch, Mary Dalrymple, Ronald M. Kaplan,
Tracy Holloway King, John T. Maxwell III, and
Paula Newman, 2012. XLE Documentation. Palo
Alto Research Center.
Mary Dalrymple, Helge Dyvik, and Tracy Holloway
King. 2004. Copular Complements: Closed or
Open? In Proceedings of the LFG ?04 Conference,
pages 188?198. CSLI Publications.
Mary Dalrymple. 2001. Lexical Functional Gram-
mar, volume 34 of Syntax and Semantics. Academic
Press.
Helge Dyvik, Paul Meurer, Victoria Rose?n, and Koen-
raad De Smedt. 2009. Linguistically Motivated Par-
allel Parsebanks. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8), pages 71?82, Milan, Italy. EDU-
Catt.
Dan Flickinger, Valia Kordoni, Yi Zhang, Anto?nio
Branco, Kiril Simov, Petya Osenova, Catarina Car-
valheiro, Francisco Costa, and Se?rgio Castro. 2012.
ParDeepBank: Multiple Parallel Deep Treebank-
ing. In Proceedings of the 11th International Work-
shop on Treebanks and Linguistic Theories (TLT11),
pages 97?107, Lisbon. Edic?o?es Colibri.
Tracy Holloway King, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ronald Kaplan. 2003. The
PARC700 Dependency Bank. In Proceedings of the
EACL03: 4th International Workshop on Linguisti-
cally Interpreted Corpora (LINC-03).
Tracy Holloway King, Martin Forst, Jonas Kuhn, and
Miriam Butt. 2005. The Feature Space in Paral-
lel Grammar Writing. In Emily M. Bender, Dan
Flickinger, Frederik Fouvry, and Melanie Siegel, ed-
itors, Research on Language and Computation: Spe-
cial Issue on Shared Representation in Multilingual
Grammar Engineering, volume 3, pages 139?163.
Springer.
Natalia Klyueva and David Marec?ek. 2010. To-
wards a Parallel Czech-Russian Dependency Tree-
bank. In Proceedings of the Workshop on Anno-
tation and Exploitation of Parallel Corpora, Tartu.
Northern European Association for Language Tech-
nology (NEALT).
Jonas Kuhn and Michael Jellinghaus. 2006. Multilin-
gual Parallel Treebanking: A Lean and Flexible Ap-
proach. In Proceedings of the LREC 2006, Genoa,
Italy. ELRA/ELDA.
Tibor Laczko?. 2012. On the (Un)Bearable Lightness
of Being an LFG Style Copula in Hungarian. In Pro-
ceedings of the LFG12 Conference, pages 341?361.
CSLI Publications.
Sabine Lehmann, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herve? Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP ?
Test Suites for Natural Language Processing. In
Proceedings of COLING, pages 711 ? 716.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian
Sulger, Tina Bo?gel, Atif Gulzar, Ghulam Raza, Sar-
mad Hussain, and Miriam Butt. 2010. Transliter-
ating Urdu for a Broad-Coverage Urdu/Hindi LFG
Grammar. In Proceedings of the Seventh Con-
ference on International Language Resources and
Evaluation (LREC 2010), Valletta, Malta.
559
Inderjeet Mani and James Pustejovsky. 2004. Tem-
poral Discourse Models for Narrative Structure. In
Proceedings of the 2004 ACL Workshop on Dis-
course Annotation, pages 57?64.
Bea?ta Megyesi, Bengt Dahlqvist, E?va A?. Csato?, and
Joakim Nivre. 2010. The English-Swedish-Turkish
Parallel Treebank. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC?10), Valletta, Malta. Euro-
pean Language Resources Association (ELRA).
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective Sharing for Multilingual Depen-
dency Parsing. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 629?637,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Joakim Nivre, Igor Boguslavsky, and Leonid Iomdin.
2008. Parsing the SynTagRus Treebank. In Pro-
ceedings of COLING08, pages 641?648.
Rachel Nordlinger and Louisa Sadler. 2007. Verb-
less Clauses: Revealing the Structure within. In An-
nie Zaenen, Jane Simpson, Tracy Holloway King,
Jane Grimshaw, Joan Maling, and Chris Manning,
editors, Architectures, Rules and Preferences: A
Festschrift for Joan Bresnan, pages 139?160. CSLI
Publications.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: Modular NLP Framework. In Proceedings
of the 7th International Conference on Advances in
Natural Language Processing (IceTAL 2010), pages
293?304.
Victoria Rose?n, Paul Meurer, and Koenraad de Smedt.
2009. LFG Parsebanker: A Toolkit for Building and
Searching a Treebank as a Parsed Corpus. In Pro-
ceedings of the 7th International Workshop on Tree-
banks and Linguistic Theories (TLT7), pages 127?
133, Utrecht. LOT.
Victoria Rose?n, Koenraad De Smedt, Paul Meurer, and
Helge Dyvik. 2012. An Open Infrastructure for Ad-
vanced Treebanking. In META-RESEARCH Work-
shop on Advanced Treebanking at LREC2012, pages
22?29, Istanbul, Turkey.
Manuela Sanguinetti and Cristina Bosco. 2011. Build-
ing the Multilingual TUT Parallel Treebank. In Pro-
ceedings of Recent Advances in Natural Language
Processing, pages 19?28.
Sebastian Sulger. 2011. A Parallel Analysis of have-
Type Copular Constructions in have-Less Indo-
European Languages. In Proceedings of the LFG
?11 Conference. CSLI Publications.
Josef van Genabith and Dick Crouch. 1996. Direct and
Underspecified Interpretations of LFG f-structures.
In Proceedings of the 16th International Conference
on Computational Linguistics (COLING-96), vol-
ume 1, pages 262?267, Copenhagen, Denmark.
Martin Volk, Anne Go?hring, Torsten Marek,
and Yvonne Samuelsson. 2010. SMUL-
TRON (version 3.0) ? The Stock-
holm MULtilingual parallel TReebank.
http://www.cl.uzh.ch/research/paralleltreebanks en.
html.
560
Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 9?16
Manchester, August 2008
Context Inducing Nouns
Charlotte Price
Palo Alto Research Center
3333 Coyote Hill Rd.
Palo Alto, CA 94304 USA
lprice@parc.com
Valeria de Paiva
Palo Alto Research Center
3333 Coyote Hill Rd.
Palo Alto, CA 94304 USA
valeria.paiva@gmail.com
Tracy Holloway King
Palo Alto Research Center
3333 Coyote Hill Rd.
Palo Alto, CA 94304 USA
thking@parc.com
Abstract
It is important to identify complement-
taking nouns in order to properly analyze
the grammatical and implicative structure
of the sentence. This paper examines the
ways in which these nouns were identified
and classified for addition to the BRIDGE
natural language understanding system.
1 Introduction
One of the goals of computational linguistics is to
draw inferences from a text: that is, for the sys-
tem to be able to process a text, and then to con-
clude, based on the text, whether some other state-
ment is true.1 Clausal complements confound the
process because, despite their surface similarity to
adjuncts, they generate very different inferences.
In this paper we examine complement-taking
nouns: how to identify them and how to incorpo-
rate them into an inferencing system. We first dis-
cuss what we mean by complement-taking nouns
(section 2) and how to identify a list of such
nouns (section 3). We then describe the question-
answering system that uses the complement-taking
nouns as part of its inferencing (section 4), how the
nouns are added to the system (section 5), and how
the coverage is tested (section 6). Finally, we dis-
cuss several avenues for future work (section 7),
including automating the search process, identify-
ing other context-inducing forms, and taking ad-
vantage of cross-linguistic data.
c 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1We would like to thank the Natural Language Theory and
Technology group at PARC, Dick Crouch, and the three re-
viewers for their input.
2 What is a complement-taking noun?
Identifying complement-taking nouns is somewhat
involved. It is important to identify the clause, to
ensure that the clause is indeed a complement and
not an adjunct (e.g. a relative clause or a purpose
infinitive), and to figure out what is licensing the
complement, as it is not only nouns that license
complements.
2.1 Verbal vs. nominal complements
A clause is a portion of a sentence that includes a
predicate and its arguments. Clauses come in a va-
riety of forms, a subset of which is shown in (1)
for verbs taking complements. The italicized part
is the complement, and the part in bold is what li-
censes it. The surface form of the clause can vary
significantly depending on the licensing verb.
(1) a. Mary knows that Bob is happy.
b. John wants (Mary) to leave right now.
c. John likes fixing his bike.
d. John let Mary fix his bike.
For this paper, we touch briefly on nouns taking
to clauses, as in (2b), but the main focus is on that
clauses, as in (2a).
(2) a. the fact that Mary hopped
b. the courage to hop
Both types of complements pose problems in
mining corpora for lexicon development. The that
clauses can superficially resemble relative clauses,
as in (3), and the to clauses can resemble purpose
infinitives, as in (4).
(3) a. COMPLEMENT-TAKING NOUN: John
liked the idea that Mary sang last
evening.
9
b. RELATIVE CLAUSE: John liked the song
that Mary sang last evening.
(4) a. COMPLEMENT-TAKING NOUN: John had
a chance to sing that song.
b. PURPOSE INFINITIVE: John had a song
book (in order) to sing that song.
As discussed in section 3, this superficial re-
semblance makes the automatic identification of
complement-taking nouns very difficult: simple
string-based searches would return large numbers
of incorrect candidates which would have to be vet-
ted before incorporating the new nouns into the
system.
2.2 Contexts introduced by nominals
Complements and relative clause adjuncts allow
very different inferences. Whereas the speaker?s
beliefs about adjuncts take on the truth value of
the clause they are embedded in, the truth value of
clausal complements is also affected by the licens-
ing noun. Compare the sentences below. The itali-
cized clause in (5) is a complement, while in (6) it
is an adjunct.
(5) The lie that Mary was ill paralyzed Bob.
Mary was not ill.
(6) The situation that she had gotten herself into
paralyzed Bob. She had gotten herself
into a situation.
To explain how this is possible, we introduce the
notion of implicative contexts (Nairn et al, 2006),
and claim that complement-taking nouns introduce
a context for the complement, whereas no such
context is created for the adjuncts. Perhaps the eas-
iest way to think of a context is to imagine em-
bedding the complement in an extra layer, with the
layer adding information about how to adjust the
truth-value of its contents.2 This allows us to con-
clude in (5) that the speaker believes that Mary and
Bob exist, as does the event of Bob?s paralysis, but
the event Mary was ill does not. These are ref-
ered to as the (un)instantiability of the components
in the sentence. Contexts can be embedded within
each other recursively, as in (7). Note that these se-
mantic contexts often, but not always, correspond
to syntactic embedding.
2In the semantic representations, the contexts are flattened,
or projected, onto the leaf nodes of the parse tree, so that every
leaf has access to information locally.
(7) Paul believes [that John?s lie [that Mary wor-
ries [that fish can fly]] surprised us].
Contexts may have an implication signature
(Nairn et al, 2006) attached to them, specifying,
for example, that the clause is something that the
speaker presupposes to be true or that the speaker
believes the truth value of the clause should be re-
versed. The default for a context is to allow no
implications to be drawn, as in (1b), where the
speaker has not committed to whether or not Mary
is leaving.
Below is a more detailed example showing how
the context introduced by a noun changes the im-
plications of the sentence, and how it would behave
differently from a relative clause adjunct to a noun.
Consider the pair of sentences in (8).
(8) a. The lie that Mary had won surprised John.
Mary did not win.
b. The bonus that Mary had won surprised
John. Mary won a bonus.
In (8), that John was surprised is in the speaker?s
top context, which is what the author commits to as
truth. In (8a), lie is within the context of surprised.
Surprised does not change the implications of ele-
ments within its context.3 Therefore, lie gets a true
value: that a lie was told is considered true. That
Mary won, however, is within the context of lie,
which reverses the polarity of implications within
its scope or context. If that Mary won were only
within the context of surprised instead of within
lie, which would be the case if lie did not create
a context, then that Mary won would fall within
the context of surprised. The implication signa-
ture of surprised would determine the veridicality
of the embedded clause instead of the signature of
lie: this would incorrectly allow the conclusion that
Mary won.
The content of the relative clause in (8b) is in the
same context as surprise since no additional con-
text is introduced by bonus. As such, we can con-
clude that Mary did win a bonus.
2.3 Complements introduced by to
The previous subsection focused on finite comple-
ments introduced by that. From the perspective
3We say surprise has the implication signature ++/--: el-
ements within its context have a positive implication in a pos-
itive context and negative in a negative context. See (Nairn et
al., 2006) for detailed discussion of possible implication sig-
natures and how to propagate them through contexts.
10
of aiding inferencing in the BRIDGE system, the
nouns that take to complements that are not dever-
bal nouns (see section 2.4 for discussion of dever-
bals) seem to fall into three main classes:4 ability,
bravery, and chance. Examples are shown in (9).
(9) a. John has the ability to sing.
b. John has the guts to sing out loud.
c. John?s chance to sing came quickly.
These all have an implication signature that
gives a (negative) implication only in a negative
context, as in (10); in a positive context as in (9),
no implication can be drawn.
(10) John didn?t have the opportunity to sing.
John didn?t sing.
Note also that the implication only applies when
the verb is have. Other light verbs, such as take in
(11) change the implications.
(11) John took the opportunity to sing.
John sang.
For this reason, these nouns are treated differ-
ently than those with that complements. They are
marked in the grammar as taking a complement in
the same way that that complements are (section 5),
but the mechanism which attaches an implication
signature takes the governing verb into account.
2.4 Deverbal nouns
A large number of complement-taking nouns are
related to verbs that take complements. These
nouns are analyzed differently than non-deverbal
nouns. They are linked to their related verb and
classified according to how the arguments of the
noun and the sentence relate to the arguments for
the verb (e.g. -ee, -er).5 The BRIDGE system uses
this linking to map these nouns to their verbal coun-
terparts and to draw conclusions of implicativity as
if they were verbs, as explained in (Gurevich et al,
2006). Consider (12) where the paraphrases using
fear as a verb or a noun are clearly related.
(12) a. The fear that Mary was ill paralyzed Bob.
b. Bob feared that Mary was ill; this fear par-
alyzed Bob.
4The work described in this section was done by Lauri
Karttunen and Karl Pichotta (Pichotta, 2008).
5NOMLEX (Macleod et al, 1998) is an excellent source of
these deverbal nouns.
Deverbal nouns can take that complements or, as
in (13), to complements. Most often, the context
introduced by a deverbal noun does not add an im-
plication signature, as in (11), which results in the
answer UNKNOWN to the question Was Mary ill?.
(13) a. John?s promise to go swimming surprised
us.
b. John?s persuasion of Mary to sing at the
party surprised us.
Gerunds, being even more verb-like, are treated as
verbs in our system and hence inherit the implica-
tive properties from the corresponding verb.
(14) Knowing that Mary had sung upset John.
Mary sang.
Gerunds and deverbal nouns are discussed in de-
tail in (Gurevich et al, 2006) and are outside of the
scope of this paper.
3 Finding complement-taking nouns
In order for the system to draw the inferences dis-
cussed above, the complement-taking nouns must
first be identified and then classified and incorpo-
rated into the BRIDGE system (section 4). First,
the gerunds are removed since these are mapped by
the syntax into their verbal counterparts. Then the
non-gerund deverbal nouns (section 2.4) are linked
to their verbal counterpart so that they can be ana-
lyzed by the system as events. These two classes
represent a significant number of the nouns that
take that complements.
3.1 Syntactic classification
However, there are many complement-taking
nouns that are not deverbal. To expand our
lexicon of these nouns, we started with a seed
set garnered from the Penn Treebank (Marcus et
al., 1994), which uses distinctive tree structures
for complement-taking nouns, and a small list
of linguistically prominent nouns. For each of
these lexical items, we extracted words in the
same semantic class from WordNet. Classes
include words like fact, which direct attention
to the clausal complement, as in (15), and nouns
expressing emotion, as in (16).
(15) It?s a fact that Mary came.
(16) Bob?s joy that Mary had returned reduced him
to tears.
11
These semantic classes provided a starting point
for discovering more of these nouns: the class of
emotion nouns, for example, has more than a hun-
dred hyponyms.
Identifying the class is not enough, as not all
members take clausal complements. Compare joy
in (16) and warmheartedness in (17) from the emo-
tion class. The sentence containing joy is much
more natural than that in (17).
(17) #Bob?s warmheartedness that Mary had re-
turned reduced him to tears.
From the candidate list, the deverbal nouns are
added to the lexicon of deverbal noun mappings.
The remaining list is checked word-by-word. To
ease the process, test sentences that take a range of
meanings are created for each class of nouns, as in
(18).
(18) Bob?s that Mary visited her mother re-
duced him to tears.
If the noun does not fit the test sentences, a
web search is done on ?X that? to extract po-
tential complement-bearing sentences. These are
checked to eliminate sentences with adjuncts, or
where some other feature licenses the clause, such
as in (19) where the bold faced structure is licens-
ing the italicized clause.
(19) a. John is so warmhearted that he took her in
without question.
b. They had such a good friendship that she
could tell him anything.
Using these methods, from a seed set of 13
nouns, 170 non-deverbal complement-taking
nouns were identified, most in the emotion and
feeling classes. The same techniques were then
applied to the state and information classes. Once
the Penn Treebank seeds were incorporated, the
same process was applied to the complement-
taking nouns from NOMLEX (Macleod et al,
1998).
3.2 Determining implications
As examples (8a) and (8b) showed, whether a word
takes a complement is lexically determined; so is
the type of implication signature introduced by the
word. Compare the implications in (20).
(20) a. The fact that Mary had returned surprised
John. Mary had returned.
b. The falsehood that Mary had returned sur-
prised John. Mary had not returned.
c. The possibility that Mary had returned
surprised John. ? Mary had returned.
These nouns have different implication signa-
tures: facts imply truth; lies imply falsehood; and
possibilities do not allow truth or falsehood to be
established. The default for complements is that no
implications can be drawn, as in (20c), which in the
BRIDGE system is expressed as the noun having no
implication signature.6
Once identified and its implication signature de-
termined, adding the complement-taking noun to
the BRIDGE system and deriving the correct infer-
ences is straightforward. This process is described
in section 5.
4 The BRIDGE system
The BRIDGE system (Bobrow et al, 2007) includes
a syntactic grammar, a semantics rule set (Crouch
and King, 2006), an abstract knowledge represen-
tation (AKR) rule set, and an entailment and con-
tradiction detection (ECD) system. The syntax, se-
mantics, and AKR all depend on lexicons.
The BRIDGE grammar defines syntactic proper-
ties of words, such as predicate-argument structure,
tense, number, and nominal specifiers. The gram-
mar produces a packed representation of the sen-
tence which allows ambiguity to be dealt with effi-
ciently (Maxwell and Kaplan, 1991).
The parses are passed to the semantic rules
which also work on packed structures (Crouch,
2005). The semantic layer looks up words in a
Unified Lexicon (UL), connects surface arguments
of verbs to their roles, and determines the context
within which a word occurs in the sentence. Nega-
tion introduces a context, as do the complement-
taking nouns discussed here (Bobrow et al, 2005).
The UL combines several sources of information
(Crouch and King, 2005). Much of the information
comes from the syntactic lexicon, VerbNet (Kipper
et al, 2000), and WordNet (Fellbaum, 1998), but
there are also handcoded entries that add semanti-
cally relevant information such as its implication
signature. A sample UL entry is given in Figure 1.
The current number of complement-taking
nouns in the system is shown in (21). Only a
6A context is still generated for these. Adjuncts, having no
context of their own, inherit the implication signature of the
clause containing them (section 2.2).
12
(cat(N), word(fact), subcat(NOUN-EXTRA),
concept(%1),
source(hand annotated data), source(xle),
xfr:concept for(%1,fact),
xfr:lex class(%1,impl pp nn),
xfr:wordnet classes(%1,[])).
Figure 1: One entry for the word fact in the Uni-
fied Lexicon. NOUN-EXTRA states that this use of
fact fits in structures such as it is a fact that The
WordNet meaning is found by looking up the con-
cept for fact in the WordNet database. The implica-
tion signature of the word is impl pp nn or ++/--
as seen in (22). Lastly, the sources for this informa-
tion are noted.
fifth of the nouns have implication signatures.
However, all of the nouns introduce contexts; the
default implication for contexts is to allow neither
true nor false to be concluded, as in (20c).
(21)
Complement-taking Nouns
that complements 411
to complements 173
with implication signatures 107
The output of the semantics level is fed into
the AKR. At this level, contexts are used to deter-
mine (un)instantiability based on the relationship
between contexts.7 An entity?s (un)instantiability
encodes whether it exists in some context. In (8a),
for example, we can conclude that the speaker be-
lieves that Mary exists, but that the event Mary won
is uninstantiated: the speaker believes it did not
happen.
The final layer is the ECD, which uses the struc-
tures built by the AKR to reason about a given
passage-query pair to determine whether or not the
query is inferred by the passage, answering with
YES, NO, UNKNOWN, or AMBIGUOUS. For more
details, see (Bobrow et al, 2005).
5 Adding complement-taking nouns to
the system
Adding complement-taking nouns to the BRIDGE
system is straightforward. A syntactic entry is
added indicating that the noun takes a complement.
The syntactic classes are defined by templates, and
the relevant template is called in the lexical en-
try for that word. For example, the template call
7See (Bobrow et al, 2007; Bobrow et al, 2005) for other
information contained in the AKR.
@(NOUN-EXTRA %stem) is added to the entry for
fact.
If there is an implication signature for the com-
plement, this is added to the noun?s entry in the
file for hand-annotated data used to build the UL.
The fifth line in Figure 1 is an example. The AKR
and ECD rules that calculate the context and im-
plications on verbs and deverbal nouns general-
ize to handle implications on complement-taking
nouns and so do not need to be altered as new
complement-taking nouns are found.
As described in section 3, deciding which nouns
take complements is currently hand curated, as it is
quite difficult to distinguish them entirely automat-
ically.
6 Testing
To ensure that complement-taking nouns are work-
ing properly in the system, for each noun, a
passage-query-correct answer triplet such as:
(22) PASSAGE: The fact that Mary had returned
surprised John.
QUERY: Had Mary returned?
ANSWER: YES
is added to a testsuite. The testsuites are run and
the results reported as part of the daily regres-
sion testing (Chatzichrisafis et al, 2007). Both
naturally occurring and hand-crafted examples are
used to ensure that the correct implications are
being drawn. Natural examples test interactions
between phenomena such as noun complementa-
tion and copular constructions, while hand-crafted
examples allow isolation of the phenomenon and
show that all cases are being tested (Cohen et al,
2008), e.g., that the correct entailments emerge un-
der negation as well as in the positive case.
Our current testsuites contain about 180 hand-
crafted examples. The number of natural exam-
ples is harder to count as they occur somewhat
rarely in the mixed-phenomena testsuites. One
of our natural example files, which is based on
newswire extracts from the PASCAL Recognizing
Textual Entailment Challenge (Dagan et al, 2005),
shows an approximate breakdown of the uses of the
word that is as shown in (23). This sample, which
is somewhat biased towards verbal complements
since it contains many examples that can be para-
phrased as said that, nonetheless shows the relative
scarcity of noun complements in the wild and un-
derscores the importance of hand-crafted examples
13
for testing purposes. It it is clear that these noun
complements were being analyzed incorrectly be-
fore; what is unclear is how much of an impact
the misanalysis would have caused. Perhaps some
other domain would demonstrate a significantly
higher presence of non-deverbal nouns that take
complements and would be more significantly im-
pacted by their misanalysis.
(23)
Uses of the word that in RTE 2007
verbal complements 68
adjuncts 50
deverbal complements 14
noun complements 3
other 8 19
7 Future work
The detection and incorporation of noun comple-
ments for use in the BRIDGE system can be ex-
panded in several directions, such as automat-
ing the search process, identifying and classifying
other parts of speech that take complements, and
exploring transferability to other languages.
7.1 Automating the search
Testing whether a clause is an adjunct or a noun
complement or is licensed by something else is cur-
rently done by hand. Automating the testing would
allow many more nouns to be tested. However, this
is non-trivial. As (8a) and (8b) demonstrated, the
surface structure can appear very similar; it is only
when we try to figure out the implications of the ex-
amples that the differences emerge.
The Penn Treebank (Marcus et al, 1994) was
initially used to extract complement-taking nouns.
As more tree and dependency banks, as well as lex-
ical resources (Macleod et al, 1998), are available,
further lexical items can be extracted in this way.
However, such resources are costly to build and
so are only slowly added to the available NLP re-
sources.
Rather than trying to identify all potential noun
complement clauses, a simpler approach would be
to reduce the search space for the human judge. For
example, some adjuncts (perhaps three quarters of
them) could be eliminated from natural examples
by using a part-of-speech tagger to identify occur-
rences where a conjugated verb immediately fol-
8This includes demonstrative uses, uses licensed by other
parts of speech such as so, and clauses which are the subject
of a sentence or the object of a prepositional phrase.
lows the word that, as in (24). These commonly
identify adjuncts.
(24) The shark that bit the swimmer appears to
have left.
By eliminating these adjuncts and by removing
those sentences where it is known that the clause
is a complement of the verb based on the syntac-
tic classification of that verb (the syntactic lexicon
contains 2500 verbs with various clausal comple-
ments), as in (25), the search space could be signif-
icantly reduced.
(25) The judge announced that the defendant was
guilty.
7.2 Other parts of speech that introduce
contexts
Verbs, adjectives, and adverbs can also license
complements and hence contexts with implication
signatures. Examples in (26) show different parts
of speech that introduce contexts.9
(26) a. Verb: John said that Paul had arrived.
b. Adjective: It is possible that someone ate
the last piece of cake.
c. Adjective: John was available to see
Mary.
d. Adverb: John falsely reported that Mary
saw Bill.
Many classes of verbs have already been iden-
tified and are incorporated into the system (Nairn
et al, 2006): verbs relating to speech (e.g., say,
report, etc.), implicative verbs such as manage
and fail (Karttunen, 2007), and factive verbs (e.g.
agree, realize, consider) (Vendler, 1967; Kiparsky
and Kiparsky, 1971), to name a few. Many adjec-
tives have also been added to the system, includ-
ing ones taking to and that complements.10 As with
the complement-taking nouns, a significant part of
the effort in incorporating the complement-taking
adjectives into the system was identifying which
adjectives license complements. The adverbs have
not been explored in as much depth.
9From a syntactic perspective, the adverb falsely does not
take a complement. However, it does introduce a context in
the semantics and hence requires a lexical entry similar to
those discussed for the complement-taking nouns.
10This work was largely done by Hannah Copperman dur-
ing her internship at PARC.
14
7.3 Other languages
The fact that it has been productive to search
for complement-taking nouns through synonyms
and WordNet classes suggests that other languages
could benefit from the work done in English. It
would be interesting to see to what extent the im-
plicative signatures from one language carry over
into another, and to what extent they differ. Strong
similarities could, for example, suggest some com-
mon mechanism at work in these nouns that we
have been unable to identify by studying only one
language. Searching in other languages could also
potentially turn up classes or candidates that were
missed in English.11
8 Conclusions
It is important to identify complement-taking
nouns in order to properly analyze the grammati-
cal and implicative structure of the sentence. Here
we described a bootstrapping approach whereby
annotated corpora and existing lexical resources
were used to identify complement-taking nouns.
WordNet was used to find semantically similar
nouns. These were then tested in closed examples
and in Web searches in order to determine whether
they licensed complements and what the implica-
tive signature of the complement was. Although
identifying the complete set of these nouns is
non-trivial, the context mechanism for dealing
with implicatives makes adding them to the
BRIDGE system to derive the correct implications
straightforward.
9 Appendix: Complement-taking nouns
This appendix contains sample complement-taking
nouns and their classification in the BRIDGE sys-
tem.
9.1 Noun that take to clauses
Ability nouns (impl nn with verb have): ability,
choice, energy, flexibility, freedom, heart, means,
way, wherewithal
Asset nouns (impl nn with verb have): money, op-
tion, time
Bravery nouns (impl nn with verb have): au-
dacity, ball, cajones, cheek, chutzpah, cojones,
11Thanks to Martin Forst (p.c.) for suggesting this direc-
tion.
courage, decency, foresight, gall, gumption, gut,
impudence, nerve, strength, temerity
Chance nouns (impl nn with verb have): chance,
occasion, opportunity
Effort nouns (impl nn with verb have): initiative,
liberty, trouble
Other nouns (no implicativity or not yet classi-
fied): accord, action, agreement, aim, ambition,
appetite, application, appointment, approval, at-
tempt, attitude, audition, authority, authorization,
battle, bid, blessing, campaign, capacity, clear-
ance, commission, commitment, concession, con-
fidence, consent, consideration, conspiracy, con-
tract, cost, decision, demand, desire, determina-
tion, directive, drive, duty, eagerness, effort, ev-
idence, expectation, failure, fear, fight, figure,
franchise, help, honor, hunger, hurry, idea, im-
pertinence, inability, incentive, inclination, indi-
cation, information, intent, intention, invitation,
itch, job, journey, justification, keenness, legisla-
tion, license, luck, mandate, moment, motion, mo-
tive, move, movement, need, note, notice, notifi-
cation, notion, obligation, offer, order, pact, pat-
tern, permission, plan, pledge, ploy, police, posi-
tion, potential, power, pressure, principle, process,
program, promise, propensity, proposal, proposi-
tion, provision, push, readiness, reason, recom-
mendation, refusal, reluctance, reminder, removal,
request, requirement, responsibility, right, rush,
scheme, scramble, sense, sentiment, shame, sign,
signal, stake, stampede, strategy, study, support,
task, temptation, tendency, threat, understanding,
undertaking, unwillingness, urge, venture, vote,
willingness, wish, word, work
9.2 Nouns that take that clauses
Nouns with impl pp nn: abomination, angriness,
angst, animosity, anxiousness, apprehensiveness,
ardor, awe, bereavement, bitterness, case, choler,
consequence, consternation, covetousness, discon-
certion, disconcertment, disquiet, disquietude, ec-
stasy, edginess, enmity, enviousness, event, fact,
fearfulness, felicity, fright, frustration, fury, gall,
gloom, gloominess, grudge, happiness, hesitancy,
hostility, huffiness, huffishness, inquietude, in-
security, ire, jealousy, jitteriness, joy, joyous-
ness, jubilance, jumpiness, lovingness, poignance,
poignancy, premonition, presentiment, problem,
qualm, rancor, rapture, sadness, shyness, situa-
15
tion, somberness, sorrow, sorrowfulness, suspense,
terror, trepidation, truth, uneasiness, unhappiness,
wrath
Nouns with fact p: absurdity, accident, hypocrisy,
idiocy, irony, miracle
Nouns with impl pn np: falsehood, lie
Other nouns (no implicativity or not yet classi-
fied): avowal, axiom, conjecture, conviction, cri-
tique, effort, fear, feeling, hunch, hysteria, idea,
impudence, inability, incentive, likelihood, news,
notion, opinion, optimism, option, outrage, pact,
ploy, point, police, possibility, potential, power,
precedent, premise, principle, problem, prospect,
proviso, reluctance, responsibility, right, rumor,
scramble, sentiment, showing, sign, skepticism,
stake, stand, story, strategy, tendency, unwilling-
ness, viewpoint, vision, willingness, word
References
Bobrow, Daniel G., Cleo Condoravdi, Richard Crouch,
Ron Kaplan, Lauri Karttunen, Tracy Holloway King,
Valeria de Paiva, and Annie Zaenen. 2005. A ba-
sic logic for textual inference. In Proceedings of
the AAAI Workshop on Inference for Textual Question
Answering.
Bobrow, Daniel G., Bob Cheslow, Cleo Condoravdi,
Lauri Karttunen, Tracy Holloway King, Rowan
Nairn, Valeria de Paiva, Charlotte Price, and Annie
Zaenen. 2007. PARC?s Bridge and question answer-
ing system. In Grammar Engineering Across Frame-
works, pages 46?66. CSLI Publications.
Chatzichrisafis, Nikos, Dick Crouch, Tracy Holloway
King, Rowan Nairn, Manny Rayner, and Marianne
Santaholma. 2007. Regression testing for grammar-
based systems. In Grammar Engineering Across
Frameworks, pages 28?143. CSLI Publications.
Cohen, K. Bretonnel, William A. Baumgartner Jr., and
Lawrence Hunter. 2008. Software testing and the
naturally occurring data assumption in natural lan-
guage processing. In Software Engineering, Testing,
and Quality Assurance for Natural Language Pro-
cessing, pages 23?30. Association for Computational
Linguistics.
Crouch, Dick and Tracy Holloway King. 2005. Unify-
ing lexical resources. In Proceedings of the Interdis-
ciplinary Workshop on the Identification and Repre-
sentation of Verb Features and Verb Classes.
Crouch, Dick and Tracy Holloway King. 2006. Seman-
tics via f-structure rewriting. In LFG06 Proceedings.
CSLI Publications.
Crouch, Dick. 2005. Packed rewriting for mapping se-
mantics to KR. In Proceedings of the International
Workshop on Computational Semantics.
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognizing textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognizing Textual Entailment,
Southampton, U.K.
Fellbaum, Christiane, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Gurevich, Olga, Richard Crouch, Tracy Holloway
King, and Valeria de Paiva. 2006. Deverbal nouns
in knowledge representation. In Proceedings of the
19th International Florida AI Research Society Con-
ference (FLAIRS ?06), pages 670?675.
Karttunen, Lauri. 2007. Word play. Computational
Linguistics, 33:443?467.
Kiparsky, Paul and Carol Kiparsky. 1971. Fact. In
Steinberg, D. and L. Jakobovits, editors, Semantics.
An Inderdisciplinary Reader, pages 345?369. Cam-
bridge University Press.
Kipper, Karin, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In AAAI-2000 17th National Conference on Artificial
Intelligence.
Macleod, Catherine, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In EURALEX?98.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies adn
Mark Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn treebank: Annotative predicate
argument structure. In ARPA Human Language
Technology Workshop.
Maxwell, John and Ron Kaplan. 1991. A method for
disjunctive constraint satisfaction. Current Issues in
Parsing Technologies.
Nairn, Rowan, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual in-
ference. In Inference in Computational Semantics
(ICoS-5).
Pichotta, Karl. 2008. Processing paraphrases
and phrasal implicatives in the Bridge question-
answering system. Stanford University, Symbolic
Systems undergraduate honors thesis.
Vendler, Zeno. 1967. Linguistics and Philosophy. Cor-
nell University Press.
16
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 49?56
Manchester, August 2008
Designing Testsuites for Grammar-based Systems in Applications
Valeria de Paiva
Palo Alto Research Center
3333 Coyote Hill Rd.
Palo Alto, CA 94304 USA
valeria.paiva@gmail.com
Tracy Holloway King
Palo Alto Research Center
3333 Coyote Hill Rd.
Palo Alto, CA 94304 USA
thking@parc.com
Abstract
In complex grammar-based systems, even
small changes may have an unforeseeable
impact on overall system performance. Re-
gression testing of the system and its com-
ponents becomes crucial for the grammar
engineers developing the system. As part
of this regression testing, the testsuites
themselves must be designed to accurately
assess coverage and progress and to help
rapidly identify problems. We describe
a system of passage-query pairs divided
into three types of phenomenon-based test-
suites (sanity, query, basic correct). These
allow for rapid development and for spe-
cific coverage assessment. In addition,
real-world testsuites allow for overall per-
formance and coverage assessment. These
testsuites are used in conjunction with the
more traditional representation-based re-
gression testsuites used by grammar engi-
neers.
1 Introduction
In complex grammar-based systems, even small
changes may have an unforeseeable impact on
overall system performance.1 Systematic regres-
sion testing helps grammar engineers to track
progress, and to recognize and correct shortcom-
ings in linguistic rule sets. It is also an essential tool
c 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1We would like to thank Rowan Nairn for his design and
implementation of the regression platform that runs these test-
suites. We would also like to thank the PARC Natural Lan-
guage Theory and Technology group for their work with these
testsuites and their comments on this paper.
for assessing overall system status in terms of task
and runtime performance.
As discussed in (Chatzichrisafis et al, 2007),
regression testing for grammar-based systems in-
volves two phases. The first includes systematic
testing of the grammar rule sets during their de-
velopment. This is the part of regression testing
that grammar engineers are generally most familiar
with. The second phase involves the deployment
of the grammar in a system and the regression test-
ing of the grammar as a part of the whole system.
This allows the grammar engineer to see whether
changes have any effect on the system, positive or
negative. In addition, the results of regression test-
ing in the system allow a level of abstraction away
from the details of the grammar output, which can
ease maintenance of the regression testsuites so
that the grammar engineers do not need to change
the gold standard annotation every time an interme-
diate level of representation changes.
In this paper, we focus on the design of testsuites
for grammar-based systems, using a question-
answering system as a model. In particular, we are
interested in what types of testsuites allow for rapid
development and efficient debugging.
1.1 The Question-Answering System
To anchor the discussion, we focus on regression
testsuites designed for a grammar-based question-
answering system (Bobrow et al, 2007). The
Bridge system uses the XLE (Crouch et al, 2008)
parser to produce syntactic structures and then the
XLE ordered rewrite system to produce linguistic
semantics (Crouch and King, 2006) and abstract
knowledge representations. Abstract knowledge
representations for passages and queries are pro-
cessed by an entailment and contradiction detec-
tion system which determines whether the query is
49
entailed, contradicted, or neither by the passage.
Entailment and contradiction detection between
passages and queries is a task well suited to regres-
sion testing. There are generally only two or three
possible answers given a passage and a query: en-
tails, contradicts or neither (or in the looser case:
relevant or irrelevant). Wh-questions (section 5.1)
receive a YES answer if an alignment is found be-
tween the wh-word in the query and an appropriate
part of the passage representation; in this case, the
proposed alignment is returned as well as the YES
answer. This is particularly important for who and
what questions where more than one entity in the
passage might align with the wh-word.
From the standpoint of regression testing, two
important aspects of the question-answering appli-
cation are:
(1) The correct answer for a given pair is indepen-
dent of the representations used by the system
and even of which system is used.
(2) The passage-query pairs with answers can be
constructed by someone who does not know
the details of the system.
The first aspect means that even drastic changes in
representation will not result in having to update
the regression suites. This contrasts sharply with
regressions run against representative output which
require either that the gold standard be updated or
that the mapping from the output to that standard be
updated. The second aspect means that externally
developed testsuites (e.g. FraCaS (Cooper et al,
1996), Pascal RTE (Sekine et al, 2007)) can eas-
ily be incorporated into the regression testing and
that grammar engineers can rapidly add new test-
suites, even if they do not have experience with the
internal structure of the system. These aspects also
mean that such passage-query pairs can be used
for cross-system comparisons of coverage (Bos,
2008).
1.2 Testsuite Types
In the regression testsuites designed for the
question-answering system, the passage-query
pair testsuites are divided into two main types:
those that focus on single phenomena (section 2)
and those that use real-world passages (section
3). The phenomenon-based testsuites allow the
grammar engineer to track the behavior of the
system with respect to a given construction, such
as implicativity, noun-noun compounds, temporal
expressions, or comparatives. In contrast, the
real-world passages allow the grammar engineer
to see how the system will behave when applied
to real data, including data which the system will
encounter in applications. Such sentences tend to
stress the system in terms of basic performance
(e.g. efficiency and memory requirements for
processing of long sentences) and in terms of
interactions of different phenomena (e.g. coordi-
nation ambiguity interacting with implicativity).
In addition to the passage-query pairs, the sys-
tem includes regression over representations at
several levels of analysis (section 4). These are
limited in number, focus only on core phenomena,
and are not gold standard representations but in-
stead the best structure of the ones produced. These
are used to detect whether unintentional changes
were introduced to the representations (e.g. new
features were accidentally created).
2 Phenomenon Sets
Real-world sentences involve analysis of multiple
interacting phenomena. Longer sentences tend to
have more diverse sets of phenomena and hence
a higher chance of containing a construction that
the system does not handle well. This can lead to
frustration for grammar engineers trying to track
progress; fixing a major piece of the system can
have little or no effect on a testsuite of real-world
examples. To alleviate this frustration, we have
extensive sets of hand-crafted test examples that
are focused as much as possible on single phe-
nomenon. These include externally developed test-
suites such as the FraCaS (Cooper et al, 1996) and
HP testsuites (Nerbonne et al, 1988). Focused test-
suites are also good for quickly diagnosing prob-
lems. If all the broken examples are in the deverbal
testsuite, for example, it gives grammar engineers
a good idea of where to look for bugs.
The majority of the testsuites are organized by
syntactic and semantic phenomena and are de-
signed to test all known variants of that phe-
nomenon (see (Cohen et al, 2008) on the need
to use testsuites designed to test system coverage
as well as real-world corpora). For the question-
answering system, these include topics such as
anaphora, appositives, copulars, negation, dever-
bal nouns and adjectives, implicatives and factives,
temporals, cardinality and quantifiers, compara-
tives, possessives, context introducing nouns, and
pertainyms. These categories align with many of
50
those cited by (Bos, 2008) in his discussion of se-
mantic parser coverage. Some example passage-
query pairs for deverbal nouns are shown in (3).
(3) a. P: Ed?s abdication of the throne was wel-
come.
Q: Ed abdicated the throne.
A: YES
b. P: Ed?s abdication was welcome.
Q: Ed abdicated.
A: YES
c. P: Ed is an abdicator.
Q: Ed abdicated.
A: YES
Each of the phenomena has three sets of test-
suites associated with it. Sanity sets (section 2.1)
match a passage against itself. The motivation be-
hind this is that a passage should generally entail
itself and that if the system cannot capture this en-
tailment, something is wrong. Query sets (sec-
tion 2.2) match the passage against query versions
of the passage. The simplest form of this is to
have a polarity question formed from the passage.
More complex versions involve negative polarity
questions, questions with different adjuncts or ar-
gument structures, and questions with synonyms or
antonyms. Basic correct sets (section 2.3) are se-
lected passage-query pairs in which the system is
known to obtain the correct answer for the correct
reason. The idea behind these sets is that they can
be run immediately by the grammar engineer af-
ter making any changes and the results should be
100% correct: any mistakes indicates a problem in-
troduced by the grammar engineer?s changes.
2.1 Sanity Sets
The entailment and contradiction detection part
of the system is tested in isolation by matching
queries against themselves. Some example sanity
pairs from the copula testsuite are shown in (4).
(4) a. P: A boy is tall.
Q: A boy is tall.
A: YES
b. P: A girl was the hero.
Q: A girl was the hero.
A: YES
c. P: The boy is in the garden.
Q: The boy is in the garden.
A: YES
d. P: The boy is not in the garden.
Q: The boy is not in the garden.
A: YES
Note that queries in the question-answering sys-
tem do not have to be syntactically interrogative.
This allows the sanity pairs to be processed by
the same mechanism that processes passage-query
pairs with syntactically interrogative queries.
The sanity check testsuites are largely composed
of simple, hand-crafted examples of all the syntac-
tic and semantic patterns that the system is known
to cover. This minimal check ensures that at least
identical representations trigger an entailment.
2.2 Query Sets
The query sets form the bulk of the regression
sets. The query sets comprise passages of the types
found in the sanity sets, but with more complex
queries. The simplest form of these is to form the
polarity question from the passage, as in (5). More
complex queries can be formed by switching the
polarity from the passage to the query, as in (6).
(5) a. P: A boy is tall.
Q: Is a boy tall?
A: YES
b. P: A girl was the hero.
Q: Was a girl the hero?
A: YES
(6) P: The boy is not in the garden.
Q: Is the boy in the garden?
A: NO
To form more complex pairs, adjuncts and ar-
gument structure can be altered from the passage
to the query. These have to be checked carefully
to ensure that the correct answer is coded for the
pair since entailment relations are highly sensitive
to such changes. Some examples are shown in
(7). Alternations such as those in (7c) are crucial
for testing implicativity, which plays a key role in
question answering.
(7) a. P: An older man hopped.
Q: A man hopped.
A: YES
b. P: John broke the box.
Q: The box broke.
A: YES
51
c. P: Ed admitted that Mary arrived.
Q: Mary arrived.
A: YES
A similar type of alteration of the query is to
substitute synonyms for items in the passage, as in
(8). This is currently done less systematically in the
testsuites but helps determine lexical coverage.
(8) a. P: Some governments ignore historical
facts.
Q: Some governments ignore the facts of
history.
A: YES
b. P: The boys bought some candy.
Q: The boys purchased some candy.
A: YES
In addition to the testsuites created by the
question-answering system developers, the query
sets include externally developed pairs, such as
those created for FraCaS (Cooper et al, 1996).
These testsuites also involve handcrafted passage-
query pairs, but the fact that they were developed
outside of the system helps to detect gaps in sys-
tem coverage. In addition, some of the FraCaS
pairs involve multi-sentence passages. Since the
sentences in these passages are very short, they are
appropriate for inclusion in the phenomenon-based
testsuites. Some externally developed testsuites
such as the HP testsuite (Nerbonne et al, 1988) do
not involve passage-query pairs but the same tech-
niques used by the grammar engineers to create the
sanity and the query sets are applied to these test-
suites as well.
2.3 Basic Correct Sets
A subset of the query sets described above are used
to form a core set of basic correct testsuites. These
testsuites contain passage-query pairs that the de-
velopers have determined the system is answering
correctly for the correct reason.
Since these testsuites are run each time the gram-
mar engineer makes a change to the system be-
fore checking the changes into the version control
repository, it is essential that the basic correct test-
suites can be run quickly. Each pair is processed
rapidly because the query sets are composed of
simple passages that focus on a given phenomenon.
In addition, only one or two representatives of any
given construction is included in the basic correct
set; that is, the sanity sets and query sets may con-
tain many pairs testing copular constructions with
adjectival complements, but only a small subset of
these are included in the basic correct set. In the
question-answering system, 375 passage-query
pairs are in the basic correct sets; it takes less than
six minutes to run the full set on standard machines.
In addition, since the basic correct sets are divided
by phenomena, developers can first run those test-
suites which relate directly to the phenomena they
have been working on.
Examining the basic correct sets gives an
overview of the expected base coverage of the
system. In addition, since all of the pairs are
working for the correct reason when they are
added to the basic correct set, any breakage is a
sign that an error has been introduced into the
system. It is important to fix these immediately so
that grammar engineers working on other parts of
the system can use the basic correct sets to assess
the impact of their changes on the system.
3 Real-world Sets
The ultimate goal of the system is to work on real-
world texts used in the application. So, tests of
those texts are important for assessing progress on
naturally occurring data. These testsuites are cre-
ated by extracting sentences from the corpora ex-
pected to be used in the run-time system, e.g. news-
paper text or the Wikipedia.2 Queries are then cre-
ated by hand for these sentences. Once the system
is being used by non-developers, queries posed by
those users can be incorporated into the testsuites to
ensure that the real-world sets have an appropriate
range of queries. Currently, the system uses a com-
bination of hand-crafted queries and queries from
the RTE data which were hand-crafted, but not by
the question-answering system developers. Some
examples are shown in (9).
(9) a. P: The interest of the automotive industry
increases and the first amplifier project, a
four-channel output module for the Ger-
man car manufacturer, Porsche, is fin-
ished.
Q: Porsche is a German car manufacturer.
A: YES
b. P: The Royal Navy servicemen being held
captive by Iran are expected to be freed to-
2If the application involves corpora containing ungram-
matical input (e.g. email messages), it is important to include
both real-world and phenomenon sets for such data.
52
day.
Q: British servicemen detained
A: YES
c. P: ?I guess you have to expect this in
a growing community,? said Mardelle
Kean, who lives across the street from
John Joseph Famalaro, charged in the
death of Denise A. Huber, who was 23
when she disappeared in 1991.
Q: John J. Famalaro is accused of having
killed Denise A. Huber.
A: YES
These real-world passages are not generally use-
ful for debugging during the development cycle.
However, they serve to track progress over time,
to see where remaining gaps may be, and to pro-
vide an indication of system performance in appli-
cations. For example, the passage-query pairs can
be roughly divided as to those using just linguis-
tic meaning, those using logical reasoning, those
requiring plausible reasoning, and finally those re-
quiring world knowledge. Although the bound-
aries between these are not always clear (Sekine et
al., 2007), having a rough division helps in guiding
development.
4 Regression on Representations
There has been significant work on regression test-
ing of a system?s output representations (Nerbonne
et al, 1988; Cooper et al, 1996; Lehmann et al,
1996; Oepen et al, 1998; Oepen et al, 2002): de-
signing of the testsuites, running and maintaining
them, and tracking the results over time. As men-
tioned in the previous discussion, for a complex
system such as a question-answering system, hav-
ing regression testing that depends on the perfor-
mance of the system rather than on details of the
representations has significant advantages for de-
velopment because the regression testsuites do not
have to be redone whenever there is a change to the
system and because the gold standard items (i.e.,
the passage-query pairs with answers) can be cre-
ated by those less familiar with the details of the
system.
However, having a small but representative set
of banked representations at each major level of
system output has proven useful for detecting un-
intended changes that may not immediately disturb
the passage-query pairs.3 This is especially the case
3In addition to running regression tests against representa-
with the sanity sets and the most basic query sets:
with these the query is identical to or very closely
resembles the passage so that changes to the repre-
sentation on the passage side will also be in the rep-
resentation on the query side and hence may not be
detected as erroneous by the entailment and contra-
diction detection.
For the question-answering system, 1200 sen-
tences covering basic syntactic and semantic types
form a testsuite for representations. The best rep-
resentation currently produced by the system is
stored for the syntax, the linguistic semantics, and
the abstract knowledge representation levels. To
allow for greater stability over time and less sen-
sitivity to minor feature changes in the rule sets, it
is possible to bank only the most important features
in the representations may, e.g. the core predicate-
argument structure. The banked representations
are then compared with the output of the system
after any changes are made. Any differences are
examined to see whether they are intentional. If
they were intended, then new representations need
to be banked for the ones that have changed (see
(Rose?n et al, 2005) for ways to speed up this pro-
cess by use of discrimants). If the differences were
not intended, then the developer knows which con-
structions were affected by their changes and can
more easily determine where in the system the er-
ror might have been introduced.
5 Discussion and Conclusions
The testsuites discussed above are continually un-
der development. We believe that the basic ideas
behind these testsuites should be applicable to
other grammar-based systems used in applications.
The passage-query pairs are most applicable to
question-answering and search/retrieval systems,
but aspects of the approach can apply to other sys-
tems.
Some issues that remain for the testsuites dis-
cussed above are extending the use of wh-questions
in passage-query pairs, the division between devel-
opment and test sets, and the incorporation of con-
text into the testing.
5.1 Wh-questions
The testsuites as described have not yet been sys-
tematically extended to wh-questions. The query
tions, the syntax, semantics, and abstract knowledge represen-
tation have type declarations (Crouch and King, 2008) which
help to detect malformed representations.
53
sets can be easily extended to involve some substi-
tution of wh-phrases for arguments and adjuncts in
the passage, as in (10).
(10) a. P: John broke the box.
Q: Who broke the box?
b. P: John broke the box.
Q: What did John break?
c. P: John broke the box.
Q: What broke?
d. P: John broke the box.
Q: What did John do?
e. P: We went to John?s party last night.
Q: Who went to John?s party?
There is a long-standing issue as to how to eval-
uate responses to wh-questions (see (Voorhees and
Tice, 2000a; Voorhees and Tice, 2000b) and the
TREC question-answering task web pages for dis-
cussion and data). For example, in (10a) most peo-
ple would agree that the answer should be John, al-
though there may be less agreement as to whether
John broke the box. is an appropriate answer. In
(10b) and (10c) there is an issue as to whether the
answer should be box or the box and how to assess
partial answers. This becomes more of an issue as
the passages become more complicated, e.g. with
heavily modified nominals that serve as potential
answers. While for (10d) the passage is a good an-
swer to the question, for (10e) presumably the an-
swer should be a list of names, not simply ?we?.
Obtaining such lists and deciding how complete
and appropriate they are is challenging. Since most
question-answering systems are not constrained to
polarity questions, it is important to assess per-
formance on wh-questions as the system develops.
Other, even more complicated questions, for exam-
ple how to questions are also currently out of the
scope of our testsuites.
5.2 Development vs. Testing
For development and evaluation of systems, test-
suites are usually divided into development sets,
which the system developers examine in detail, and
test sets, which represent data unseen by the de-
velopers.4 To a limited extent, the real-world sets
4The usual division is between training, development, and
test sets, with the training set generally being much larger than
the development and test sets. For rule based systems, the
training/development distinction is often irrelevant, and so a
serve as a form of test set since they reflect the per-
formance of the system on real data and are of-
ten not examined in detail for why any given pair
fails to parse. However, the testsuites described
above are all treated as development sets. There are
no reserved phenomenon-based testsuites for blind
testing of the system?s performance on each phe-
nomenon, although there are real-world testsuites
reserved as test sets.
If a given testsuite was created all at once, a ran-
dom sampling of it could be held out as a test set.
However, since there are often only a few pairs per
construction or lexical item, it is unclear whether
this approach would give a fair view of system cov-
erage. In addition, for rule-based systems such
as the syntax and semantics used in the question-
answering system, the pairs are often constructed
based on the rules and lexicons as they were being
developed. As such, they more closely match the
coverage of the system than if it were possible to
randomly select such pairs from external sources.
As a system is used in an application, a test set of
unseen, application-specific data becomes increas-
ingly necessary. Such sets can be created from the
use of the application: for example, queries and
returned answers with judgments as to correctness
can provide seeds for test sets, as well as for ex-
tending the phenomenon-based and real-world de-
velopment testsuites.
5.3 Context
The real sentences that a question-answering sys-
tem would use to answer questions appear in a
larger textual and metadata context. This context
provides information as to the resolution of pro-
nouns, temporal expressions such as today and this
morning, ellipsis, etc. The passage-query pairs in
the testsuites do not accurately reflect how well the
system handles the integration of context. Small
two sentence passages can be used to, for example,
test anaphora resolution, as shown in (11).
(11) P: Mary hopped. Then, she skipped.
Q: Did Mary skip?
A: YES
Even in this isolated example, the answer can be
construed as being UNKNOWN since it is possible,
although unlikely, that she resolves to some other
entity. This type of problem is pervasive in using
distinction is made between those sets used in the development
of the system and those unseen sets used to test and evaluate
the system?s performance.
54
simple passage-query pairs for system regression
testing.
A further issue with testing phenomena linked to
context, such as anaphora resolution, is that they
are usually very complex and can result in signifi-
cant ambiguity. When used on real-world texts, ef-
ficiency can be a serious issue which this type of
more isolated testing does not systematically ex-
plore. As a result of this, the anaphora testsuites
must be more carefully constructed to take advan-
tage of isolated, simpler pairs when possible but
to also contain progressively more complicated ex-
amples that eventually become real-world pairs.
5.4 Summary Conclusions
In complex grammar-based systems, even small
changes may have an unforeseeable impact on sys-
tem performance. Regression testing of the system
and its components becomes crucial for the gram-
mar engineers developing the system.
A key part of regression testing is the testsuites
themselves, which must be designed to accurately
assess coverage and progress and to help to rapidly
identify problems. For broad-coverage grammars,
such as those used in open domain applications like
consumer search and question answering, testsuite
design is particularly important to ensure adequate
coverage of basic linguistic (e.g. syntactic and se-
mantic) phenomena as well as application specific
phenomena (e.g. interpretation of markup, incor-
poration of metadata).
We described a system of passage-query pairs
divided into three types of phenomenon-based test-
suites (sanity, query, basic correct). These allow
for rapid development and specific coverage as-
sessment. In addition, real-world testsuites allow
for overall performance and coverage assessment.
More work is needed to find a systematic way to
provide ?stepping stones? in terms of complexity
between phenomenon-based and real-world test-
suites.
These testsuites are used in conjunction with the
more traditional representation-based regression
testsuites used by grammar engineers. These
representation-based testsuites use the same
phenomenon-based approach in order to assess
coverage and pinpoint problems as efficiently as
possible.
References
Bobrow, Daniel G., Bob Cheslow, Cleo Condoravdi,
Lauri Karttunen, Tracy Holloway King, Rowan
Nairn, Valeria de Paiva, Charlotte Price, and An-
nie Zaenen. 2007. PARC?s bridge and ques-
tion answering system. In King, Tracy Holloway
and Emily M. Bender, editors, Grammar Engineer-
ing Across Frameworks, pages 46?66. CSLI Publica-
tions.
Bos, Johan. 2008. Let?s not argue about semantics. In
Proceedings of LREC.
Chatzichrisafis, Nikos, Dick Crouch, Tracy Holloway
King, Rowan Nairn, Manny Rayner, and Mari-
anne Santaholma. 2007. Regression testing
for grammar-based systems. In King, Tracy Hol-
loway and Emily M. Bender, editors, Proceedings
of the Grammar Engineering Across Frameworks
(GEAF07) Workshop, pages 128?143. CSLI Publica-
tions.
Cohen, K. Bretonnel, William A. Baumgartner Jr., and
Lawrence Hunter. 2008. Software testing and the
naturally occurring data assumption in natural lan-
guage processing. In Software Engineering, Testing,
and Quality Assurance for Natural Language Pro-
cessing, pages 23?30. Association for Computational
Linguistics.
Cooper, Robin, Dick Crouch, Jan van Eijck, Chris
Fox, Josef van Genabith, Jan Jaspars, Hans Kamp,
David Milward, Manfred Pinkal, Massimo Poesio,
and Steve Pulman. 1996. Using the framework.
FraCas: A Framework for Computational Semantics
(LRE 62-051).
Crouch, Dick and Tracy Holloway King. 2006. Seman-
tics via f-structure rewriting. In Butt, Miriam and
Tracy Holloway King, editors, LFG06 Proceedings,
pages 145?165. CSLI Publications.
Crouch, Dick and Tracy Holloway King. 2008. Type-
checking in formally non-typed systems. In Software
Engineering, Testing, and Quality Assurance for Nat-
ural Language Processing, pages 3?4. Association
for Computational Linguistics.
Crouch, Dick, Mary Dalrymple, Ron Ka-
plan, Tracy King, John Maxwell, and Paula
Newman. 2008. XLE documentation.
http://www2.parc.com/isl/groups/nltt/xle/doc/.
Lehmann, Sabine, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herve? Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP ?
Test Suites for Natural Language Processing. In Pro-
ceedings of COLING 1996.
Nerbonne, John, Dan Flickinger, and Tom Wasow.
1988. The HP Labs natural language evaluation
tool. In Proceedings of the Workshop on Evaluation
of Natural Language Processing Systems.
55
Oepen, Stephan, Klaus Netter, and Judith Klein. 1998.
TSNLP ? Test Suites for Natural Language Process-
ing. In Nerbonne, John, editor, Linguistic Databases,
pages 13?36. CSLI.
Oepen, Stephan, Dan Flickinger, Kristina Toutanova,
and Chris D. Manning. 2002. LinGO Redwoods. a
rich and dynamic treebank for HPSG. In Proceed-
ings of The First Workshop on Treebanks and Lin-
guistic Theories, pages 139?149.
Rose?n, Victoria, Koenraad de Smedt, Helge Dyvik, and
Paul Meurer. 2005. TREPIL: Developing methods
and tools for multilevel treebank construction. In
Proceedings of The Fourth Workshop on Treebanks
and Linguistic Theories.
Sekine, Satoshi, Kentaro Inui, Ido Dagan, Bill Dolan,
Danilo Giampiccolo, and Bernardo Magnini, editors.
2007. Proceedings of the ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing. Association
for Computational Linguistics, Prague, June.
Voorhees, Ellen and Dawn Tice. 2000a. Building a
question answering test collection. In Proceedings
of SIGIR-2000, pages 200?207.
Voorhees, Ellen and Dawn Tice. 2000b. The TREC-8
question answering track evaluation. In Proceedings
8th Text REtrieval Conference (TREC-8), pages 83?
105.
56
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 62?66,
Gothenburg, Sweden, 26-27 April 2014.
c?2014 Association for Computational Linguistics
Mickey Mouse is not a Phrase: Improving Relevance in E-Commerce with
Multiword Expressions
Prathyusha Senthil Kumar, Vamsi Salaka, Tracy Holloway King, and Brian Johnson
Search Science
eBay, Inc.
San Jose, CA, USA
{ prathykumar, vsalaka, tracyking, bjohnson } @ebay.com
Abstract
We describe a method for detecting
phrases in e-commerce queries. The key
insight is that previous buyer purchasing
behavior as well as the general distribu-
tion of phrases in item titles must be used
to select phrases. Many multiword ex-
pression (mwe) phrases which might be
useful in other situations are not suitable
for buyer query phrases because relevant
items, as measured by purchases, do not
contain these terms as phrases.
1 Phrase MWE in e-Commerce Search
Processing buyers? queries is key for successful
e-commerce. As with web search queries, e-
commerce queries are shorter and have different
syntactic patterns than standard written language.
For a given query, the system must provide suffi-
cient recall (i.e. return all items relevant to the buy-
ers? query, regardless of the tokens used) and suffi-
cient precision (i.e. exclude items which are token
matches but not relevant for the query). This paper
looks at how identifying phrases in buyer queries
can help with recall and precision in e-commerce
at eBay. We focus primarily on precision, which
is the harder problem to solve.
Phrases are a sub-type of mwe: one where
the tokens of the mwe appear strictly adjacent to
one another and in a specified order ((Sag et al.,
2002)?s words with spaces).
The eBay product search engine takes buyer
queries and retrieves items relevant to the buyer?s
purchasing intent. The items are listed in cate-
gories (e.g. women?s dresses) and each item has a
title provided by the seller. The buyer can choose
to sort the items by most relevant (e.g. similar
to web search ranking) or deterministically (e.g.
price low to high). There are versions of the e-
commerce site for different countries such as US,
UK, Germany, France, Poland, etc. and so the
query processing is language-specific according to
site. Here we report on incorporating phrases into
English for the US and German for Germany.
2 Controlling Retrieval via Query
Phrases
The query processing system has three core capa-
bilities
1
which expand tokens in the buyer?s query
into other forms. Both single and multiple to-
kens can be expanded. Token-to-token expan-
sions (Jammalamadaka and Salaka, 2012) include
acronyms, abbreviations, inflectional variants (e.g.
hats to hat), and space synonyms (e.g. ray ban to
rayban). Category expansions expand tokens to
all items in a given category (e.g. womens shoes
retrieves all items in the Womens? Shoes cate-
gory). Finally, attribute expansions map tokens
to structured data (e.g. red retrieves any item with
Color=Reds in its structured data). These expan-
sions are used to increase the number of relevant
items brought back for a specific buyer query.
Precision issues occur when a buyer?s query re-
turns an item that is a spurious match. For exam-
ple, the query diamond ring size 10 matches all
the tokens in the title ?10 kt gold, size 7 diamond
ring? even though it is not a size 10 ring.
Recall issues occur when relevant items are not
returned for a buyer?s query. The core capabilities
of token-to-token mappings, category mappings,
and attribute mapping largely address this. How-
ever, some query tokens are not covered by these
capabilities. For example, the query used cars for
sale contains the tokens for sale which rarely oc-
cur in e-commerce item titles.
1
Here we ignore tokenization, although the quality of the
tokenizer affects the quality of all remaining components
(Manning et al., 2008).
62
2.1 Hypothesis: Phrasing within Queries
To address these precision and recall issues, we
provide special treatment for phrases in queries.
To address the precision issue where spurious
items are returned, we require certain token se-
quences to be treated as phrases. For example, size
10 will be phrased and hence only match items
whose titles have those tokens in that order. To
address the recall issue, we identify queries which
contain phrases that can be dropped. For exam-
ple, in the query used cars for sale the tokens for
sale can be dropped; similarly for German kaufen
(buy) in the query waschtrockner kaufen (washer-
dryer buy). For the remainder of the paper we will
use the terminology:
? REQUIRED PHRASES: Token sequences re-
quired to be phrases when used in queries
(e.g. apple tv)
? DROPPED PHRASES: Phrases which allow
sub-phrase deletion (e.g. used cars for sale)
The required-phrases approach must be high
confidence since it will block items from being re-
turned for the buyer?s query.
We first mined candidate phrases for required
phrases and for dropped phrases in queries. From
this large set of candidates, we then used past
buyer behavior to determine whether the candi-
date was viable for application to queries (see
(Ramisch et al., 2008) on mwe candidate evalu-
ation in general). As we will see, many phrases
which seem to be intuitively well-formed mwe
cannot be used as e-commerce query phrases be-
cause they would block relevant inventory from
being returned (see (Diab et al., 2010) on mwe in
NLP applications).
The phrases which pass candidate selection
are then incorporated into the existing query ex-
pansions (i.e. token-to-token mappings, category
mappings, attribute mappings). The phrases are a
new type of token-to-token mapping which require
the query tokens to appear in order and adjacent,
i.e. as a mwe phrase, or to be dropped.
2.2 Phrase Candidate Selection
The first stage of the algorithm is candidate selec-
tion: from all the possible buyer query n-grams we
determine which are potential mwe phrase candi-
dates. We use a straight-forward selection tech-
nique in order to gather a large candidate set; at
this stage we are concerned with recall, not preci-
sion, of the phrases.
First consider required phrases. For a given
site (US and Germany here), we consider all the
bi- and tri-grams seen in buyer queries. Since
e-commerce queries are relatively short, even
shorter than web queries, we do not consider
longer n-grams. The most frequent of these are
then considered candidates. Manual inspection
of the candidate set shows a variety of mwe se-
mantic types. As expected in the e-commerce do-
main, these contain primarily nominal mwe: brand
names, product types, and measure phrases (see
(
?
O S?eaghdha and Copestake, 2007) on identifying
nominal mwe). Multiword verbs are non-existent
in buyer queries and relatively few adjectives are
candidates (e.g. navy blue, brand new).
Next consider dropped phrases. These are
stop words specialized to the e-commerce domain.
They are mined from behavioral logs by looking
at query-to-query transitions. We consider query
transitions where buyers drop a word or phrase in
the transition and show increased engagement af-
ter the transition. For example, buyers issue the
query used cars for sale followed by the query
used cars and subsequently engage with the search
results (e.g. view or purchase items). The most fre-
quent n-grams identified by this approach are can-
didates for dropped phrases and are contextually
dropped, i.e. they are dropped when they are parts
of specific larger phrases. Query context is impor-
tant because for sale should not be dropped when
part of the larger phrase plastic for sale signs.
2.3 Phrase Selection: Sorry Mickey
Once we have candidate phrases, we use buyer
behavioral data (Carterette et al., 2012) to deter-
mine which phrases to require in buyer queries.
For each query which contains a given phrase (e.g.
for the candidate phrase apple tv consider queries
such as apple tv, new apple tv, apple tv remote)
we see which items were purchased. Item titles
from purchased items which contain the phrase
are referred to as ?phrase bought? while item ti-
tles shown in searches are ?phrase impressed?. We
are interested only in high confidence phrases and
so focus on purchase behavior: this signal is rela-
tively sparse but is the strongest indicator of buyer
interest. To determine the candidates, we want to
compute the conditional probability of an item be-
ing bought (B(ought)) given a phrase (Ph(rase)).
P (B|Ph) =
P (Ph|B) ? P (B)
P (Ph)
(1)
63
However, this is computationally intensive in that
all items retrieved for a query must be considered.
In equation 1, P(Ph|B) is easy to compute since
only bought items are considered; P(Ph) can be ap-
proximated by the ratio of phrases to non-phrases
for bought items; P(B) is a constant and hence can
be ignored. So, we use the following two metrics
based on these probabilities:
? SALE EFFICIENCY: Probability of phrases in
bought items, P(Ph|B) > 95%. Ensures qual-
ity and acts as an upper bound for the ex-
pected loss (equation 2).
? LIFT: Ensures phrasing has a positive rev-
enue impact and handles presentation bias
(equation 3).
First consider sale efficiency:
P (Ph|B) =
P (Ph
?
B)
P (B)
=
n(ph bought)
n(bought)
(2)
One drawback of sale efficiency P(Ph|B) is data
sparsity. There is a high false positive rate in
identifying phrases when the frequency of bought
items is low since it is hard to distinguish sig-
nal from noise with a strict threshold. We used
Beta-Binomial smoothing to avoid this (Schuck-
ers, 2003; Agarwal et al., 2009). Conceptually,
by incorporating Beta-Binomial smoothing, we
model the number of phrases bought as a binomial
process and use the Beta distribution, which is its
conjugate prior, for smoothing the sale efficiency.
However the sale efficiency as captured by the
conditional probability of being bought as a phrase
(equation 2) does not take into account the dis-
tribution of the phrases in the retrieved set. For
example for the phrase apple tv, 80% of the im-
pressed items contained the phrase while 99%
of the bought items contained the phrase, which
makes it an excellent phrase. However, for mount
rushmore 99% of the impressed items contained
the phrase while only 97% of the bought items
contained the phrase. This implies that the proba-
bility of being bought as a phrase for mount rush-
more is high because of presentation bias (i.e. the
vast majority of token matches contain phrases)
and not because the phrase itself is an indicator
of relevance. To address the issue of presentation
bias in P(Ph|B), we use the following lift metric:
P (Ph|B)? P (Ph)
P (Ph)
> 0 (3)
Lift (equation 3) measures the buyers? tendency
to purchase phrase items. For a good phrase this
value should be high. For example, for apple tv
this value is +23.13% while for mount rushmore it
is ?1.8%. We only consider phrases that have a
positive lift.
Examples of English phrases for buyer queries
include apple tv, bubble wrap, playstation 3, 4 x
4, tank top, nexus 4, rose gold, 1 gb, hot pack, 20
v, kindle fire, hard rock and new balance and Ger-
man phrases include geflochtene schnur (braided
line) and energiespar regler (energy-saving con-
troller). These form a disparate semantic set in-
cluding brand names (new balance), product types
(bubble wrap), and units of measure (1 gb).
Consider the phrases which were not selected
because a significant percentage of the buyer de-
mand was for items where the tokens appeared
either in a different order or not adjacent. These
include golf balls, hard drive and mickey mouse.
You might ask, what could possibly be a stronger
phrase in American English than mickey mouse?
Closer examination of the buyer behavioral data
shows that many buyers are using queries with the
tokens mickey mouse to find and purchase mickey
and minnie mouse items. The introduction of and
minnie in the item titles breaks the query phrase.
3 Experiment Results
We selected phrase candidates for two sites: The
US and Germany. These sites were selected be-
cause there was significant query and purchasing
data which alleviates data sparsity issues and be-
cause the language differences allowed us to test
the general applicability of the approach.
2
We created query assets which contained the
existing production assets and modified them to
include the required phrases and the dropped
phrases. The relative query frequency of required
phrases (blue) vs. dropped phrases (red) in each
experiment is shown in Figure 2.
US Germany
Figure 2: Impacted Query Frequency: red=drop-
ped; blue=required
For US and Germany, 10% of users were ex-
2
English and German are closely related languages. We
plan to apply mwe phrases to Russian and French.
64
Figure 1: US Phrase Query Impressions: Head-vs.-tail queries
posed to the new phrase assets, while a 10% con-
trol
3
were exposed to the existing production as-
sets. The test was run for two weeks. We mea-
sured the number of items bought in test vs. con-
trol, the revenue, and the behavior of new users.
Bought items and revenue are both measured to
determine whether changes in purchases are com-
ing from better deals (e.g. bought items might in-
crease while revenue is constant) or improved dis-
covery (e.g. more items are bought at the same
price). New user success is measured because new
users are generally sensitive to irrelevant items be-
ing returned for their queries; the required phrase
mwe in this experiment target this use case.
As a result of the phrase experiment, in the
US, revenue, bought items, and new user engage-
ment increased statistically significantly (p<0.1).
The German test showed directionally similar re-
sults but was only statistically significant for new
buyers. We cannot show proprietary business re-
sults, but both experiences are now in production
in place of the previous query processing. The
graph in Figure 1 shows the distribution of head-
vs.-tail queries for the US with some sample af-
fected head queries.
4 Discussion and Conclusion
We described a relatively straight-forward method
for detecting phrases in buyer queries. The key
insight is that previous buyer purchasing behavior
as well as the distribution of phrases in item titles
must be used to select which candidate phrases
to keep in the final analysis. Many mwe phrases
which might be useful in other situations (e.g.
3
Technically there were two 5% controls which were
compared to determine variability within the control group.
our friend mickey mouse (?2.3)) are not suitable
for buyer queries because many relevant items, as
measured by purchases, do not contain these to-
kens phrases (e.g. mickey and minnie mouse).
Among the rejected candidate phrases, the
higher confidence ones are likely to be suitable for
ranking of the results even though they could not
be used to filter out results. This is an area of ac-
tive research: what mwe phrases can improve the
ranking of e-commerce results, especially given
the presence of the phrase in the buyer query?
Another method to increase phrase coverage is to
consider contextualized phrases, whereby token
sequences may be a phrase in one query but not
in another.
The experiments here were conducted on two
of our largest sites, thereby avoiding data spar-
sity issues. We have used the same algorithm
on smaller sites such as Australia: the resulting
required phrases and dropped phrases look rea-
sonable but have not been tested experimentally.
An interesting question is whether phrases from
same-language sites (e.g. UK, Australia, Canada,
US) can be combined or whether a site with more
behavioral data can be used to learn phrases for
smaller sites. The later has been done for Canada
using US data.
In sum, mwe phrases improved eBay e-
commerce, but it was important to use domain-
specific data in choosing the relevant phrases. This
suggests that the utility of universal vs. domain
specific mwe is an area requiring investigation.
65
References
Deepak Agarwal, Bee-Chung Chen, and Pradheep
Elango. 2009. Spatio-temporal models for esti-
mating click-through rate. In Proceedings of the
18th International Conference on World Wide Web.
ACM.
Ben Carterette, Evangelos Kanoulas, Paul Clough, and
Mark Sanderson, editors. 2012. Information Re-
trieval Over Query Sessions. Springer Lecture
Notes in Computer Science.
Mona Diab, Valia Kordoni, and Hans Uszkoreit. 2010.
Multiword expressions: From theory to applica-
tions. Panel at MWE2010.
Ravi Chandra Jammalamadaka and Vamsi Salaka.
2012. Synonym mining and usage in e-commerce.
Presented at ECIR.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Diarmuid
?
O S?eaghdha and Ann Copestake. 2007. Co-
occurrence contexts for noun compound interpreta-
tion. In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 57?64.
Association for Computational Linguistics.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and
Aline Villavicencio. 2008. An evaluation of meth-
ods for the extraction of multiword expressions. In
Towards a Shared Task for Multiword Expressions,
pages 50?53.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the Third International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, CICLing ?02, pages 1?15. Springer-Verlag.
Michael E. Schuckers. 2003. Using the beta-binomial
distribution to assess performance of a biometric
identification device. International Journal of Im-
age and Graphics, pages 523?529.
66
