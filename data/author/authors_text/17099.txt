Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 124?129,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Parsing and Disfluency Detection in Linear Time
Mohammad Sadegh Rasooli?
Department of Computer Science
Columbia University, New York, NY
rasooli@cs.columbia.edu
Joel Tetreault
Nuance Communications, Inc.
Sunnyvale, CA
joel.tetreault@nuance.com
Abstract
We introduce a novel method to jointly parse
and detect disfluencies in spoken utterances.
Our model can use arbitrary features for pars-
ing sentences and adapt itself with out-of-
domain data. We show that our method, based
on transition-based parsing, performs at a high
level of accuracy for both the parsing and
disfluency detection tasks. Additionally, our
method is the fastest for the joint task, running
in linear time.
1 Introduction
Detecting disfluencies in spontaneous speech has
been widely studied by researchers in different com-
munities including natural language processing (e.g.
Qian and Liu (2013)), speech processing (e.g. Wang
et al (2013)) and psycholinguistics (e.g. Finlayson
and Corley (2012)). While the percentage of spo-
ken words which are disfluent is typically not more
than ten percent (Bortfeld et al, 2001), this addi-
tional ?noise? makes it much harder for spoken lan-
guage systems to predict the correct structure of the
sentence.
Disfluencies can be filled pauses (e.g. ?uh?, ?um?,
?huh?), discourse markers (e.g. ?you know?, ?I
mean?) or edited words which are repeated or cor-
rected by the speaker. For example, in the follow-
ing sentence, an edited phrase or reparandum inter-
val (?to Boston?) occurs with its repair (?to Den-
ver?), a filled pause (?uh?) and discourse marker (?I
? The first author worked on this project while he was a
research intern in CoreNL research group, NLU lab, Nuance
Communications, Sunnyvale, CA.
mean?).1
I want a flight to Boston? ?? ?
Reparandum
Interregnum
? ?? ?
uh????
FP
I mean? ?? ?
DM
to Denver? ?? ?
Repair
Filled pauses and discourse markers are to some
extent a fixed and closed set. The main challenge
in finding disfluencies is the case where the edited
phrase is neither a rough copy of its repair or has any
repair phrase (i.e. discarded edited phrase). Hence,
in previous work, researchers report their method
performance on detecting edited phrases (reparan-
dum) (Johnson and Charniak, 2004).
In contrast to most previous work which focuses
solely on either detection or on parsing, we intro-
duce a novel framework for jointly parsing sentences
with disfluencies. To our knowledge, our work is
the first model that is based on joint dependency and
disfluency detection. We show that our model is ro-
bust enough to detect disfluencies with high accu-
racy, while still maintaining a high level of depen-
dency parsing accuracy that approaches the upper
bound. Additionally, our model outperforms prior
work on joint parsing and disfluency detection on
the disfluency detection task, and improves upon this
prior work by running in linear time complexity.
The remainder of this paper is as follows. In ?2,
we overview some the previous work on disfluency
detection. ?3 describes our model. Experiments are
described in ?4 and Conclusions are made in ?5.
1In the literature, edited words are also known as ?reparan-
dum?, and the fillers are known as ?interregnum?. Filled pauses
are also called ?Interjections?.
124
2 Related Work
Disfluency detection approaches can be divided into
two different groups: text-first and speech first
(Nakatani and Hirschberg, 1993). In the first ap-
proach, all prosodic and acoustic cues are ignored
while in the second approach both grammatical and
acoustic features are considered. For this paper, we
focus on developing a text-first approach but our
model is easily flexible with speech-first features be-
cause there is no restriction on the number and types
of features in our model.
Among text-first approaches, the work is split
between developing systems which focus specifi-
cally on disfluency detection and those which couple
disfluency detection with parsing. For the former,
Charniak and Johnson (2001) employ a linear clas-
sifier to predict the edited phrases in Switchboard
corpus (Godfrey et al, 1992). Johnson and Char-
niak (2004) use a TAG-based noisy channel model
to detect disfluencies while parsing with getting n-
best parses from each sentence and re-ranking with
a language model. The original TAG parser is not
used for parsing itself and it is used just to find
rough copies in the sentence. Their method achieves
promising results on detecting edited words but at
the expense of speed (the parser has a complexity of
O(N5). Kahn et al (2005) use the same TAG model
and add semi-automatically extracted prosodic fea-
tures. Zwarts and Johnson (2011) improve the per-
formance of TAG model by adding external lan-
guage modeling information from data sets such as
Gigaword in addition to using minimal expected F-
loss in n-best re-ranking.
Georgila (2009) uses integer linear programming
combined with CRF for learning disfluencies. That
work shows that ILP can learn local and global con-
straints to improve the performance significantly.
Qian and Liu (2013) achieve the best performance
on the Switchboard corpus (Godfrey et al, 1992)
without any additional data. They use three steps for
detecting disfluencies using weighted Max-Margin
Markov (M3) network: detecting fillers, detecting
edited words, and refining errors in previous steps.
Some text-first approaches treat parsing and dis-
fluency detection jointly, though the models differ
in the type of parse formalism employed. Lease and
Johnson (2006) use a PCFG-based parser to parse
sentences along with finding edited phrases. Miller
and Schuler (2008) use a right-corner transform of
binary branching structures on bracketed sentences
but their results are much worse than (Johnson and
Charniak, 2004). To date, none of the prior joint ap-
proaches have used a dependency formalism.
3 Joint Parsing Model
We model the problem using a deterministic
transition-based parser (Nivre, 2008). These parsers
have the advantage of being very accurate while be-
ing able to parse a sentence in linear time. An ad-
ditional advantage is that they can use as many non-
local and local features as needed.
Arc-Eager Algorithm We use the arc-eager algo-
rithm (Nivre, 2004) which is a bottom-up parsing
strategy that is used in greedy and k-beam transition-
based parsers. One advantage of this strategy is that
the words can get a head from their left side, before
getting right dependents. This is particularly bene-
ficial for our task, since we know that reparanda are
similar to their repairs. Hence, a reparandum may
get its head but whenever the parser faces a repair, it
removes the reparandum from the sentence and con-
tinues its actions.
The actions in an arc-eager parsing algorithm are:
? Left-arc (LA): The first word in the buffer be-
comes the head of the top word in the stack.
The top word is popped after this action.
? Right-arc (RA): The top word in the stack be-
comes the head of the first word in the buffer.
? Reduce (R): The top word in the stack is
popped.
? Shift (SH): The first word in the buffer goes to
the top of the stack.
Joint Parsing and Disfluency Detection We first
extend the arc-eager algorithm by augmenting the
action space with three new actions:
? Reparandum (Rp[i:j]): treat a phrase (words i
to j) outside the look-ahead buffer as a reparan-
dum. Remove them from the sentence and clear
their dependencies.
? Discourse Marker (Prn[i]): treat a phrase in
the look-ahead buffer (first i words) as a dis-
course marker and remove them from the sen-
tence.
125
Stack Buffer Act.
flight to Boston uh I mean ... RA
flight to Boston uh I mean to ... RA
flight to Boston uh I mean to Denver Intj[1]
flight to Boston I mean to Denver Prn[1]
flight to Boston to Denver RP[2:3]
flight to Denver RA
flight to Denver RA
flight to Denver R
flight to R
flight R
Figure 1: A sample transition sequence for the sentence
?flight to Boston uh I mean to Denver?. In the third col-
umn, only the underlined parse actions are learned by the
parser (second classifier). The first classifier uses all in-
stances for training (learns fluent words with ?regular?
label).
? Interjection (Intj[i]): treat a phrase in the
look-ahead buffer (first i words) as a filled
pause and remove them from the sentence.2
Our model has two classifiers. The first classi-
fier decides between four possible actions and pos-
sible candidates in the current configuration of the
sentence. These actions are the three new ones
from above and a new action Regular (Reg): which
means do one of the original arc-eager parser ac-
tions.
At each configuration, there might be several can-
didates for being a prn, intj or reparandum, and
one regular candidate. The candidates for being
a reparandum are a set of words outside the look-
ahead buffer and the candidates for being an intj or
prn are a set of words beginning from the head of
the look-ahead buffer. If the parser decides regular
as the correct action, the second classifier predicts
the best parsing transition, based on arc-eager pars-
ing (Nivre, 2004).
For example, in the 4th state in Figure 1, there are
multiple candidates for the first classifier: regular,
?I? as prn[1] or intj[1], ?I mean? as prn[2] or intj[2],
?I mean to? as prn[3] or intj[3], ?I mean to Denver?
as prn[4] or intj[4], ?Boston? as rp[3:3], ?to Boston?
as rp[2:3], and ?flight to Boston? as rp[1:3].
2In the bracketed version of Switchboard corpus, reparan-
dum is tagged with EDITED and discourse markers and paused
fillers are tagged as PRN and INTJ respectively.
Training A transition-based parser action (our
second-level classifier) is sensitive to the words in
the buffer and stack. The problem is that we do not
have gold dependencies for edited words in our data.
Therefore, we need a parser to remove reparandum
words from the buffer and push them into the stack.
Since our parser cannot be trained on disfluent sen-
tences from scratch, the first step is to train it on
clean treebank data.
In the second step, we adapt parser weights by
training it on disfluent sentences. Our assumption
is that we do not know the correct dependencies be-
tween disfluent words and other words in the sen-
tence. At each configuration, the parser updates it-
self with new instances by traversing all configura-
tions in the sentences. In this case, if at the head of
the buffer there is an intj or prn tag, the parser allows
them to be removed from the buffer. If a reparan-
dum word is not completely outside the buffer (the
first two states in Figure 1), the parser decides be-
tween the four regular arc-eager actions (i.e. left-
arc, right-arc, shift, and reduce). If the last word
pushed into the stack is a reparandum and the first
word in the buffer is a regular word, the parser re-
moves all reparanda at the same level (in the case of
nested edited words), removes their dependencies to
other words and push their dependents into the stack.
Otherwise, the parser performs the oracle action and
adds that action as its new instance.3
With an adapted parser which is our second-level
classifier, we can train our first-level classifier. The
same procedure repeats, except that instances for
disfluency detection are used for updating param-
eter weights for the first classifier for deciding the
actions. In Figure 1, only the oracle actions (under-
lined) are added to the instances for updating parser
weights but all first-level actions are learned by the
first level classifier.
4 Experiments and Evaluation
For our experiments, we use the Switchboard corpus
(Godfrey et al, 1992) with the same train/dev/test
split as Johnson and Charniak (2004). As in that
3The reason that we use a parser instead of expanding all
possible transitions for an edited word is that, the number of reg-
ular actions will increase and the other actions become sparser
than natural.
126
work, incomplete words and punctuations are re-
moved from data (except that we do not remove in-
complete words that are not disfluent4) and all words
are turned into lower-case. The main difference with
previous work is that we use Switchboard mrg files
for training and testing our model (since they con-
tain parse trees) instead of the more commonly used
Swithboard dps text files. Mrg files are a subset of
dps files with about more than half of their size.
Unfortunately, the disfluencies marked in the dps
files are not exactly the same as those marked in
the corresponding mrg files. Hence, our result is not
completely comparable to previous work except for
(Kahn et al, 2005; Lease and Johnson, 2006; Miller
and Schuler, 2008).
We use Tsurgeon (Levy and Andrew, 2006) for
extracting sentences from mrg files and use the
Penn2Malt tool5 to convert them to dependencies.
Afterwards, we provide dependency trees with dis-
fluent words being the dependent of nothing.
Learning For the first classifier, we use averaged
structured Perceptron (AP) (Collins, 2002) with a
minor modification. Since the first classifier data is
heavily biased towards the ?regular label?, we mod-
ify the weight updates in the original algorithm to 2
(original is 1) for the cases where a ?reparandum?
is wrongly recognized as another label. We call
the modified version ?weighted averaged Perceptron
(WAP)?. We see that this simple modification im-
proves the model accuracy.6 For the second classi-
fier (parser), we use the original averaged structured
Perceptron algorithm. We report results on both AP
and WAP versions of the parser.
Features Since for every state in the parser config-
uration, there are many candidates for being disflu-
ent; we use local features as well as global features
for the first classifier. Global features are mostly
useful for discriminating between the four actions
and local features are mostly useful for choosing a
phrase as a candidate for being a disfluent phrase.
The features are described in Figure 2. For the sec-
ond classifier, we use the same features as (Zhang
and Nivre, 2011, Table 1) except that we train our
4E.g. I want t- go to school.
5http://stp.lingfil.uu.se/?nivre/
research/Penn2Malt.html
6This is similar to WM3N in (Qian and Liu, 2013).
Global Features
First n words inside/outside buffer (n=1:4)
First n POS i/o buffer (n=1:6)
Are n words i/o buffer equal? (n=1:4)
Are n POS i/o buffer equal? (n=1:4)
n last FG transitions (n=1:5)
n last transitions (n=1:5)
n last FG transitions + first POS in the buffer (n=1:5)
n last transitions + first POS in the buffer (n=1:5)
(n+m)-gram of m/n POS i/o buffer (n,m=1:4)
Refined (n+m)-gram of m/n POS i/o buffer (n,m=1:4)
Are n first words of i/o buffer equal? (n=1:4)
Are n first POS of i/o buffer equal? (n=1:4)
Number of common words i/o buffer words (n=1:6)
Local Features
First n words of the candidate phrase (n=1:4)
First n POS of the candidate phrase (n=1:6)
Distance between the candidate and first word in the buffer
Figure 2: Features used for learning the first classifier.
Refined n-gram is the n-gram without considering words
that are recognized as disfluent. Fine-grained (FG) tran-
sitions are enriched with parse actions (e.g. ?regular:left-
arc?).
parser in a similar manner as the MaltParser (Nivre
et al, 2007) without k-beam training.
Parser Evaluation We evaluate our parser with
both unlabeled attachment accuracy of correct words
and precision and recall of finding the dependencies
of correct words.7 The second classifier is trained
with 3 iterations in the first step and 3 iterations in
the second step. We use the attachment accuracy
of the parse tree of the correct sentences (without
disfluencies) as the upper-bound attachment score
and parsed tree of the disfluent sentences (without
disfluency detection) as our lower-bound attachment
score. As we can see in Table 1, WAP does a slightly
better job parsing sentences. The upper-bound pars-
ing accuracy shows that we do not lose too much in-
formation while jointly detecting disfluencies. Our
parser is not comparable to (Johnson and Charniak,
2004) and (Miller and Schuler, 2008), since we use
dependency relations for evaluation instead of con-
stituencies.
Disfluency Detection Evaluation We evaluate
our model on detecting edited words in the sentences
7The parser is actually trained to do labeled attachment and
labeled accuracy is about 1-1.5% lower than UAS.
127
UAS LB UB Pr. Rec. F2
AP 88.6 70.7 90.2 86.8 88.0 87.4
WAP 88.1 70.7 90.2 87.2 88.0 87.6
Table 1: Parsing results. UB = upperbound (parsing clean
sentences), LB = lowerbound (parsing disfluent sentences
without disfluency correction). UAS is unlabeled attach-
ment score (accuracy), Pr. is precision, Rec. is recall and
F1 is f-score.
Pr. Rec. F1
AP 92.9 71.6 80.9
WAP 85.1 77.9 81.4
KL (2005) ? ? 78.2
LJ (2006) ? ? 62.4
MS (2008) ? ? 30.6
QL (2013) ? Default ? ? 81.7
QL (2013) ? Optimized ? ? 82.1
Table 2: Disfluency results. Pr. is precision, Rec. is recall
and F1 is f-score. KL = (Kahn et al, 2005), LJ = (Lease
and Johnson, 2006), MS = (Miller and Schuler, 2008) and
QL = (Qian and Liu, 2013).
(words with ?EDITED? tag in mrg files). As we
see in Table 2, WAP works better than the original
method. As mentioned before, the numbers are not
completely comparable to others except for (Kahn
et al, 2005; Lease and Johnson, 2006; Miller and
Schuler, 2008) which we outperform. For the sake
of comparing to the state of the art, the best result
for this task (Qian and Liu, 2013) is replicated from
their available software8 on the portion of dps files
that have corresponding mrg files. For a fairer com-
parison, we also optimized the number of training
iterations of (Qian and Liu, 2013) for the mrg set
based on dev data (10 iterations instead of 30 iter-
ations). As shown in the results, our model accu-
racy is slightly less than the state-of-the-art (which
focuses solely on the disfluency detection task and
does no parsing), but we believe that the perfor-
mance can be improved through better features and
by changing the model. Another characteristic of
our model is that it operates at a very high precision,
though at the expense of some recall.
8We use the second version of the code: http://code.
google.com/p/disfluency-detection/. Results
from the first version are 81.4 and 82.1 for the default and opti-
mized settings.
5 Conclusion
In this paper, we have developed a fast, yet accurate,
joint dependency parsing and disfluency detection
model. Such a parser is useful for spoken dialogue
systems which typically encounter disfluent speech
and require accurate syntactic structures. The model
is completely flexible with adding other features (ei-
ther text or speech features).
There are still many ways of improving this
framework such as using k-beam training and decod-
ing, using prosodic and acoustic features, using out
of domain data for improving the language and pars-
ing models, and merging the two classifiers into one
through better feature engineering. It is worth noting
that we put the dummy root word in the first position
of the sentence. Ballesteros and Nivre (2013) show
that parser accuracy can improve by changing that
position for English.
One of the main challenges in this problem is
that most of the training instances are not disflu-
ent and thus the sample space is very sparse. As
seen in the experiments, we can get further improve-
ments by modifying the weight updates in the Per-
ceptron learner. In future work, we will explore
different learning algorithms which can help us ad-
dress the sparsity problem and improve the model
accuracy. Another challenge is related to the parser
speed, since the number of candidates and features
are much greater than the number used in classical
dependency parsers.
Acknowledgements We would like to thank
anonymous reviewers for their helpful comments
on the paper. Additionally, we were aided by re-
searchers by their prompt responses to our many
questions: Mark Core, Luciana Ferrer, Kallirroi
Georgila, Mark Johnson, Jeremy Kahn, Yang Liu,
Xian Qian, Kenji Sagae, and Wen Wang. Finally,
this work was conducted during the first author?s
summer internship at the Nuance Sunnyvale Re-
search Lab. We would like to thank the researchers
in the group for the helpful discussions and assis-
tance on different aspects of the problem. In particu-
lar, we would like to thank Chris Brew, Ron Kaplan,
Deepak Ramachandran and Adwait Ratnaparkhi.
128
References
Miguel Ballesteros and Joakim Nivre. 2013. Going to
the roots of dependency parsing. Computational Lin-
guistics, 39(1):5?13.
Heather Bortfeld, Silvia D. Leon, Jonathan E. Bloom,
Michael F. Schober, and Susan E. Brennan. 2001.
Disfluency rates in conversation: Effects of age, re-
lationship, topic, role, and gender. Language and
Speech, 44(2):123?147.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In NAACL-
HLT, pages 1?9.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In ACL, pages 1?8.
Ian R. Finlayson and Martin Corley. 2012. Disfluency
in dialogue: an intentional signal from the speaker?
Psychonomic bulletin & review, 19(5):921?928.
Kallirroi Georgila. 2009. Using integer linear program-
ming for detecting speech disfluencies. In NAACL-
HLT, pages 109?112.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech corpus
for research and development. In ICASSP, volume 1,
pages 517?520.
Mark Johnson and Eugene Charniak. 2004. A tag-based
noisy channel model of speech repairs. In ACL, pages
33?39.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
EMNLP, pages 233?240.
Matthew Lease and Mark Johnson. 2006. Early dele-
tion of fillers in processing conversational speech. In
NAACL-HLT, pages 73?76.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In LREC, pages 2231?2234.
Tim Miller and William Schuler. 2008. A unified syn-
tactic model for parsing fluent and disfluent speech. In
ACL-HLT, pages 105?108.
Christine Nakatani and Julia Hirschberg. 1993. A
speech-first model for repair detection and correction.
In ACL, pages 46?53.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In the Workshop on Incremen-
tal Parsing: Bringing Engineering and Cognition To-
gether, pages 50?57.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Xian Qian and Yang Liu. 2013. Disfluency detection
using multi-step stacked learning. In NAACL-HLT,
pages 820?825.
Wen Wang, Andreas Stolcke, Jiahong Yuan, and Mark
Liberman. 2013. A cross-language study on auto-
matic speech disfluency detection. In NAACL-HLT,
pages 703?708.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
ACL (Short Papers), pages 188?193.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disflu-
ency detection. In ACL, pages 703?711.
129
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 48?53,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Non-Monotonic Parsing of Fluent umm I Mean Disfluent Sentences
Mohammad Sadegh Rasooli
Department of Computer Science
Columbia University, New York, NY, USA
rasooli@cs.columbia.edu
Joel Tetreault
Yahoo Labs
New York, NY, USA
tetreaul@yahoo-inc.com
Abstract
Parsing disfluent sentences is a challeng-
ing task which involves detecting disflu-
encies as well as identifying the syntactic
structure of the sentence. While there have
been several studies recently into solely
detecting disfluencies at a high perfor-
mance level, there has been relatively lit-
tle work into joint parsing and disfluency
detection that has reached that state-of-
the-art performance in disfluency detec-
tion. We improve upon recent work in this
joint task through the use of novel features
and learning cascades to produce a model
which performs at 82.6 F-score. It outper-
forms the previous best in disfluency de-
tection on two different evaluations.
1 Introduction
Disfluencies in speech occur for several reasons:
hesitations, unintentional mistakes or problems in
recalling a new object (Arnold et al., 2003; Merlo
and Mansur, 2004). Disfluencies are often de-
composed into three types: filled pauses (IJ) such
as ?uh? or ?huh?, discourse markers (DM) such
as ?you know? and ?I mean? and edited words
(reparandum) which are repeated or corrected by
the speaker (repair). The following sentence illus-
trates the three types:
I want a flight to Boston
? ?? ?
Reparandum
uh
????
IJ
I mean
? ?? ?
DM
to Denver
? ?? ?
Repair
To date, there have been many studies on disflu-
ency detection (Hough and Purver, 2013; Rasooli
and Tetreault, 2013; Qian and Liu, 2013; Wang et
al., 2013) such as those based on TAGs and the
noisy channel model (e.g. Johnson and Charniak
(2004), Zhang et al. (2006), Georgila (2009), and
Zwarts and Johnson (2011)). High performance
disfluency detection methods can greatly enhance
the linguistic processing pipeline of a spoken dia-
logue system by first ?cleaning? the speaker?s ut-
terance, making it easier for a parser to process
correctly. A joint parsing and disfluency detection
model can also speed up processing by merging
the disfluency and parsing steps into one. How-
ever, joint parsing and disfluency detection mod-
els, such as Lease and Johnson (2006), based
on these approaches have only achieved moder-
ate performance in the disfluency detection task.
Our aim in this paper is to show that a high perfor-
mance joint approach is viable.
We build on our previous work (Rasooli and
Tetreault, 2013) (henceforth RT13) to jointly
detect disfluencies while producing dependency
parses. While this model produces parses at a
very high accuracy, it does not perform as well as
the state-of-the-art in disfluency detection (Qian
and Liu, 2013) (henceforth QL13). In this pa-
per, we extend RT13 in two important ways: 1)
we show that by adding a set of novel features se-
lected specifically for disfluency detection we can
outperform the current state of the art in disfluency
detection in two evaluations
1
and 2) we show that
by extending the architecture from two to six clas-
sifiers, we can drastically increase the speed and
reduce the memory usage of the model without a
loss in performance.
2 Non-monotonic Disfluency Parsing
In transition-based dependency parsing, a syntac-
tic tree is constructed by a set of stack and buffer
actions where the parser greedily selects an action
at each step until it reaches the end of the sentence
with an empty buffer and stack (Nivre, 2008). A
state in a transition-based system has a stack of
words, a buffer of unprocessed words and a set of
arcs that have been produced in the parser history.
The parser consists of a state (or a configuration)
1
Honnibal and Johnson (2014) have a forthcoming paper
based on a similar idea but with a higher performance.
48
which is manipulated by a set of actions. When an
action is made, the parser goes to a new state.
The arc-eager algorithm (Nivre, 2004) is a
transition-based algorithm for dependency pars-
ing. In the initial state of the algorithm, the buffer
contains all words in the order in which they ap-
pear in the sentence and the stack contains the arti-
ficial root token. The actions in arc-eager parsing
are left-arc (LA), right-arc (RA), reduce (R) and
shift (SH). LA removes the top word in the stack
by making it the dependent of the first word in the
buffer; RA shifts the first word in the buffer to the
stack by making it the dependent of the top stack
word; R pops the top stack word and SH pushes
the first buffer word into the stack.
The arc-eager algorithm is a monotonic parsing
algorithm, i.e. once an action is performed, subse-
quent actions should be consistent with it (Honni-
bal et al., 2013). In monotonic parsing, if a word
becomes a dependent of another word or acquires
a dependent, other actions shall not change those
dependencies that have been constructed for that
word in the action history. Disfluency removal is
an issue for monotonic parsing in that if an ac-
tion creates a dependency relation, the other ac-
tions cannot repair that dependency relation. The
main idea proposed by RT13 is to change the
original arc-eager algorithm to a non-monotonic
one so it is possible to repair a dependency tree
while detecting disfluencies by incorporating three
new actions (one for each disfluency type) into a
two-tiered classification process. The structure is
shown in Figure 1(a). In short, at each state the
parser first decides between the three new actions
and a parse action (C1). If the latter is selected, an-
other classifier (C2) is used to select the best parse
action as in normal arc eager parsing.
The three additional actions to the arc-eager al-
gorithm to facilitate disfluency detection are as
follows: 1) RP[i:j]: From the words outside the
buffer, remove words i to j from the sentence and
tag them as reparandum, delete all of their depen-
dencies and push all of their dependents onto the
stack. 2) IJ[i]: Remove the first i words from
the buffer (without adding any dependencies to
them) and tag them as interjection. 3) DM[i]:
Remove the first i words from the buffer (with-
out adding any dependencies) and tag them as dis-
course marker.
State
C1
Parse
RP[i:j]
IJ[i]DM[i]
C2
LA RA RSH
(a) A structure with two classifiers.
IJ[i]
C3
DM[i]
Parse
C5
C2
IJDM
C1 Other
C4 RP
C6
RLARA SH
RP[i:j]
State
(b) A structure with six classifiers.
Figure 1: Two kinds of cascades for disfluency
learning. Circles are classifiers and light-colored
blocks show the final decision by the system.
3 Model Improvements
To improve upon RT13, we first tried to learn all
actions jointly. Essentially, we added the three
new actions to the original arc-eager action set.
However, this method (henceforth M1) performed
poorly on the disfluency detection task. We be-
lieve this stems from a feature mismatch, i.e. some
of the features, such as rough copies, are only use-
ful for reparanda while some others are useful for
other actions. Speed is an additional issue. Since
for each state, there are many candidates for each
of the actions, the space of possible candidates
makes the parsing time potentially squared.
Learning Cascades One possible solution for
reducing the complexity of the inference is to for-
mulate and develop learning cascades where each
cascade is in charge of a subset of predictions with
its specific features. For this task, it is not es-
sential to always search for all possible phrases
because only a minority of cases in speech texts
are disfluent (Bortfeld et al., 2001). For address-
ing this problem, we propose M6, a new structure
for learning cascades, shown in Figure 1(b) with
a more complex structure while more efficient in
terms of speed and memory. In the new structure,
we do not always search for all possible phrases
which will lead to an expected linear time com-
plexity. The main processing overhead here is the
number of decisions to make by classifiers but this
is not as time-intensive as finding all candidate
phrases in all states.
Feature Templates RT13 use different feature
sets for the two classifiers: C2 uses the parse fea-
49
tures promoted in Zhang and Nivre (2011, Table
1) and C1 uses features which are shown with
regular font in Figure 2. We show that one can
improve RT13 by adding new features to the C1
classifier which are more appropriate for detecting
reparanda (shown in bold in Figure 2). We call
this new model M2E, ?E? for extended. In Figure
3, the features for each classifier in RT13, M2E,
M6 and M1 are described.
We introduce the following new features: LIC
looks at the number of common words between the
reparandum candidate and words in the buffer; e.g.
if the candidate is ?to Boston? and the words in the
buffer are ?to Denver?, LIC[1] is one and LIC[2]
is also one. In other words, LIC is an indicator
of a rough copy. The GPNG (post n-gram fea-
ture) allows us to model the fluency of the result-
ing sentence after an action is performed, without
explicitly going into it. It is the count of possible
n-grams around the buffer after performing the ac-
tion; e.g. if the candidate is a reparandum action,
this feature introduces the n-grams which will ap-
pear after this action. For example, if the sentence
is ?I want a flight to Boston | to Denver? (where
| is the buffer boundary) and the candidate is ?to
Boston? as reparandum, the sentence will look like
?I want a flight | to Denver? and then we can count
all possible n-grams (both lexicalized and unlexi-
calized) in the range i and j inside and outside the
buffer. GBPF is a collection of baseline parse fea-
tures from (Zhang and Nivre, 2011, Table 1).
The need for classifier specific features be-
comes more apparent in the M6 model. Each of
the classifiers uses a different set of features to op-
timize performance. For example, LIC features
are only useful for the sixth classifier while post
n-gram features are useful for C2, C3 and C6. For
the joint model we use the C1 features from M2B
and the C1 features from M6.
4 Experiments and Evaluation
We evaluate our new models, M2E and M6,
against prior work on two different test conditions.
In the first evaluation (Eval 1), we use the parsed
section of the Switchboard corpus (Godfrey et al.,
1992) with the train/dev/test splits from Johnson
and Charniak (2004) (JC04). All experimental set-
tings are the same as RT13. We compare our new
models against this prior work in terms of disflu-
ency detection performance and parsing accuracy.
In the second evaluation (Eval 2), we compare our
Abbr. Description
GS[i/j] First n Ws/POS outside ? (n=1:i/j)
GB[i/j] First n Ws/POS inside ? (n=1:i/j)
GL[i/j] Are n Ws/POS i/o ? equal? (n=1:i/j)
GT[i] n last FGT; e.g. parse:la (n=1:i)
GTP[i] n last FGT e.g. parse (n=1:i)
GGT[i] n last FGT + POS of ?
0
(n=1:i)
GGTP[i] n last CGT + POS of ?
0
(n=1:i)
GN[i] (n+m)-gram of m/n POS i/o ? (n,m=1:i)
GIC[i] # common Ws i/o ? (n=1:i)
GNR[i] Rf. (n+m)-gram of m/n POS i/o ? (n,m=1:i)
GPNG[i/j] PNGs from n/m Ws/POS i/o ? (m,n:1:i/j)
GBPF Parse features (Zhang and Nivre, 2011)
LN[i,j] First n Ws/POS of the cand. (n=1:i/j)
LD Distance between the cand. and s
0
LL[i,j] first n Ws/POS of rp and ? equal? (n=1:i/j)
LIC[i] # common Ws for rp/repair (n=1:i)
Figure 2: Feature templates used in this paper and
their abbreviations. ?: buffer, ?
0
: first word in
the buffer, s
0
: top stack word, Ws: words, rp:
reparadnum, cand.: candidate phrase, PNGs: post
n-grams, FGT: fine-grained transitions and CGT:
coarse-grained transitions. Rf. n-gram: n-gram
from unremoved words in the state.
Classifier Features
M2 Features
C1 (RT13) GS[4/4], GB[4/4], GL[4/6], GT[5], GTP[5]
GGT[5], GGTP[5], GN[4], GNR[4], GIC[6]
LL[4/6], LD
C1 (M2E) RT13 ? (LIC[6], GBPF, GPNG[4/4]) - LD
C2 GBPF
M6 Features
C1 GBPF, GB[4/4], GL[4/6], GT[5], GTP[5]
GGT[5], GGTP[5], GN[4], GNR[4], GIC[6]
C2 GB[4/4], GT[5], GTP[5], GGT[5], GGTP[5]
GN[4], GNR[4], GPNG[4/4], LD, LN[24/24]
C3 GB[4/4], GT[5], GTP[5], GGT[5], GGTP[5]
GN[4], GNR[4], GPNG[4/4], LD, LN[12/12]
C4 GBPF, GS[4/6], GT[5], GTP[5], GGT[5]
GGTP[5], GN[4], GNR[4], GIC[13]
C5 GBPF
C6 GBPF, LL[4/6], GPNG[4/4]
LN[6/6], LD, LIC[13]
M1 Features: RT13 C1 features ? C2 features
Figure 3: Features for each model. M2E is the
same as RT13 with extended features (bold fea-
tures in Figure 2). M6 is the structure with six
classifiers. Other abbreviations are described in
Figure 2.
work against the current best disfluency detection
method (QL13) on the JC04 split as well as on a
10 fold cross-validation of the parsed section of
the Switchboard. We use gold POS tags for all
evaluations.
For all of the joint parsing models we use the
weighted averaged Perceptron which is the same
as averaged Perceptron (Collins, 2002) but with a
50
loss weight of two for reparandum candidates as
done in prior work. The standard arc-eager parser
is first trained on a ?cleaned? Switchboard corpus
(i.e. after removing disfluent words) with 3 train-
ing iterations. Next, it is updated by training it on
the real corpus with 3 additional iterations. For
the other classifiers, we use the same number of
iterations determined from the development set.
Eval 1 The disfluency detection and parse re-
sults on the test set are shown in Table 1 for the
four systems (M1, RT13, M2E and M6). The joint
model performs poorly on the disfluency detection
task, with an F-score of 41.5, and the prior work
performance which serves as our baseline (RT13)
has a performance of 81.4. The extended version
of this model (M2E) raises performance substan-
tially to 82.2. This shows the utility of training the
C1 classifier with additional features. Finally, the
M6 classifier is the top performing model at 82.6.
Disfluency Parse
Model Pr. Rec. F1 UAS F1
M1 27.4 85.8 41.5 60.2 64.6
RT13 85.1 77.9 81.4 88.1 87.6
M2E 88.1 77.0 82.2 88.1 87.6
M6 87.7 78.1 82.6 88.4 87.7
Table 1: Comparison of joint parsing and disflu-
ency detection methods. UAS is the unlabeled
parse accuracy score.
The upperbound for the parser attachment ac-
curacy (UAS) is 90.2 which basically means that
if we have gold standard disfluencies and remove
disfluent words from the sentence and then parse
the sentence with a regular parser, the UAS will
be 90.2. If we had used the regular parser to parse
the disfluent sentences, the UAS for correct words
would be 70.7. As seen in Table 1, the best parser
UAS is 88.4 (M6) which is very close to the up-
perbound, however RT13, M2E and M6 are nearly
indistinguishable in terms of parser performance.
Eval 2 To compare against QL13, we use the
second version of the publicly provided code and
modify it so it uses gold POS tags and retrain and
optimize it for the parsed section of the Switch-
board corpus (these are known as mrg files, and
are a subset of the section of the Switchboard cor-
pus used in QL13, known as dps files). Since their
system has parameters tuned for the dps Switch-
board corpus we retrained it for a fair comparison.
As in the reimplementation of RT13, we have eval-
uated the QL13 system with optimal number of
training iterations (10 iterations). As seen in Table
2, although the annotation in the mrg files is less
precise than in the dps files, M6 outperforms all
models on the JC04 split thus showing the power
of the new features and new classifier structure.
Model JC04 split xval
RT13 81.4 81.6
QL13 (optimized) 82.5 82.2
M2E 82.2 82.8
M6 82.6 82.7
Table 2: Disfluency detection results (F1 score) on
JC04 split and with cross-validation (xval)
To test for robustness of our model, we per-
form 10-fold cross validation after clustering files
based on their name alphabetic order and creating
10 data splits. As seen in Table 2, the top model
is actually M2E, nudging out M6 by 0.1. More
noticeable is the difference in performance over
QL13 which is now 0.6.
Speed and memory usage Based on our Java
implementation on a 64-bit 3GHz Intel CPU with
68GB of memory, the speed for M6 (36 ms/sent)
is 3.5 times faster than M2E (128 ms/sent) and 5.2
times faster than M1 (184 ms/sent) and it requires
half of the nonzero features overall compared to
M2E and one-ninth compared to M1.
5 Conclusion and Future Directions
In this paper, we build on our prior work by in-
troducing rich and novel features to better handle
the detection of reparandum and by introducing an
improved classifier structure to decrease the uncer-
tainty in decision-making and to improve parser
speed and accuracy. We could use early updating
(Collins and Roark, 2004) for learning the greedy
parser which is shown to be useful in greedy pars-
ing (Huang and Sagae, 2010). K-beam parsing is a
way to improve the model though at the expense of
speed. The main problem with k-beam parsers is
that it is complicated to combine classifier scores
from different classifiers. One possible solution
is to modify the three actions to work on just one
word per action, thus the system will run in com-
pletely linear time with one classifier and k-beam
parsing can be done by choosing better features
for the joint parser. A model similar to this idea is
designed by Honnibal and Johnson (2014).
51
Acknowledgement We would like to thank the
reviewers for their comments and useful insights.
The bulk of this research was conducted while
both authors were working at Nuance Commu-
nication, Inc.?s Laboratory for Natural Language
Understanding in Sunnyvale, CA.
References
Jennifer E. Arnold, Maria Fagnano, and Michael K.
Tanenhaus. 2003. Disfluencies signal theee, um,
new information. Journal of Psycholinguistic Re-
search, 32(1):25?36.
Heather Bortfeld, Silvia D. Leon, Jonathan E. Bloom,
Michael F. Schober, and Susan E. Brennan. 2001.
Disfluency rates in conversation: Effects of age, re-
lationship, topic, role, and gender. Language and
Speech, 44(2):123?147.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain. Association for
Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics.
Kallirroi Georgila. 2009. Using integer linear pro-
gramming for detecting speech disfluencies. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Companion Volume: Short Papers, pages
109?112, Boulder, Colorado. Association for Com-
putational Linguistics.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP-92), volume 1, pages 517?520.
Matthew Honnibal and Mark Johnson. 2014. Joint in-
cremental disuency detection and dependency pars-
ing. Transactions of the Association for Computa-
tional Linguistics (TACL), to appear.
Matthew Honnibal, Yoav Goldberg, and Mark John-
son. 2013. A non-monotonic arc-eager transition
system for dependency parsing. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning, pages 163?172, Sofia, Bul-
garia. Association for Computational Linguistics.
Julian Hough and Matthew Purver. 2013. Modelling
expectation in the self-repair processing of annotat-,
um, listeners. In The 17th Workshop on the Seman-
tics and Pragmatics of Dialogue.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077?
1086, Uppsala, Sweden. Association for Computa-
tional Linguistics.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 33?39, Barcelona, Spain.
Matthew Lease and Mark Johnson. 2006. Early dele-
tion of fillers in processing conversational speech.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 73?76, New York City, USA.
Association for Computational Linguistics.
Sandra Merlo and Let?cia Lessa Mansur. 2004.
Descriptive discourse: topic familiarity and dis-
fluencies. Journal of Communication Disorders,
37(6):489?503.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50?57. Association
for Computational Linguistics.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Xian Qian and Yang Liu. 2013. Disfluency detection
using multi-step stacked learning. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 820?825, At-
lanta, Georgia. Association for Computational Lin-
guistics.
Mohammad Sadegh Rasooli and Joel Tetreault. 2013.
Joint parsing and disfluency detection in linear time.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
124?129, Seattle, Washington, USA. Association
for Computational Linguistics.
Wen Wang, Andreas Stolcke, Jiahong Yuan, and Mark
Liberman. 2013. A cross-language study on au-
tomatic speech disfluency detection. In Proceed-
ings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
703?708, Atlanta, Georgia. Association for Compu-
tational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
52
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA. Association for Computational Linguis-
tics.
Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A pro-
gressive feature selection algorithm for ultra large
feature spaces. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 561?568, Sydney, Aus-
tralia. Association for Computational Linguistics.
Simon Zwarts and Mark Johnson. 2011. The impact
of language models and loss functions on repair dis-
fluency detection. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
703?711, Portland, Oregon, USA. Association for
Computational Linguistics.
53
Proceedings of NAACL-HLT 2013, pages 306?314,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Development of a Persian Syntactic Dependency Treebank
Mohammad Sadegh Rasooli
Department of Computer Science
Columbia University
New York, NY
rasooli@cs.columbia.edu
Manouchehr Kouhestani
Department of Linguistics
Tarbiat Modares University
Tehran, Iran
m.kouhestani@modares.ac.ir
Amirsaeid Moloodi
Department of Linguistics
University of Tehran
Tehran, Iran
a.moloodi@ut.ac.ir
Abstract
This paper describes the annotation process
and linguistic properties of the Persian syn-
tactic dependency treebank. The treebank
consists of approximately 30,000 sentences
annotated with syntactic roles in addition to
morpho-syntactic features. One of the unique
features of this treebank is that there are al-
most 4800 distinct verb lemmas in its sen-
tences making it a valuable resource for ed-
ucational goals. The treebank is constructed
with a bootstrapping approach by means of
available tagging and parsing tools and man-
ually correcting the annotations. The data is
splitted into standard train, development and
test set in the CoNLL dependency format and
is freely available to researchers.
1 Introduction1
The process of manually annotating linguistic data
from a huge amount of naturally occuring texts is a
very expensive and time consuming task. Due to the
recent success of machine learning methods and the
rapid growth of available electronic texts, language
processing tasks have been facilitated greatly. Con-
sidering the value of annotated data, a great deal of
budget has been allotted to creating such data.
Among all linguistic datasets, treebanks play an
important role in the natural language processing
tasks especially in parsing because of its applica-
1This research is done while working in Dadegan Research
Group, Supreme Council of Information and Communications
Technology (SCICT), Tehran, Iran. The project is fully funded
by SCICT.
tions in tasks such as machine translation. Depen-
dency treebanks are collections of sentences with
their corresponding dependency trees. In the last
decade, many dependency treebanks have been de-
veloped for a large number of languages. There are
at least 29 languages for which at least one depen-
dency treebank is available (Zeman et al, 2012).
Dependency trees are much more similar to the hu-
man understanding of language and can easily rep-
resent the free word-order nature of syntactic roles
in sentences (Ku?bler et al, 2009).
Persian is a language with about 110 million
speakers all over the world (Windfuhr, 2009), yet in
terms of the availability of teaching materials and
annotated data for text processing, it is undoubt-
edly a low-resourced language. The need for more
language teaching materials together with an ever-
increasing need for Persian-language data process-
ing has been the incentive for the inception of our
project which has defined the development of the
syntactic treebank of Persian as its ultimate aim. In
this paper, we review the process of creating the Per-
sian syntactic treebank based on dependency gram-
mar. In this treebank, approximately 30,000 sen-
tences from contemporary Persian-language texts
are manually tokenized and annotated at morpholog-
ical and syntactic levels. One valuable aspect of the
treebank is its containment of near 5000 distinct verb
lemmas in its sentences making it a good resource
for educational goals. The dataset is developed af-
ter the creation of the syntactic valency lexicon of
Persian verbs (Rasooli et al, 2011c). This treebank
is developed with a bootstrapping approach by au-
tomatically building dependency trees based on the
306
I?@
	
?

@ QK. ?
	
?

JJ.?
	?K
@ root
P?st PAn b?r mobt?ni Pin
is that on based it
V PR PP ADJ PR
root
SBJ
MOS
AJPPPOSDEP
(a) A simple projective dependency
tree for a Persian sentence: ?It is based
on that??.
	
?

@ QK.
I?@ ?
	
?

JJ.?
	?K
@ root
PAn b?r P?st mobt?ni Pin
that on is based it
PR PP V ADJ PR
root
SBJ
MOS
AJPP
POSDEP
(b) A simple non-projective depen-
dency tree for a Persian sentence: ?It
is based on that?.
Figure 1: Examples of Persian sentences with the
dependency-based syntactic trees. 1(a) and 1(b) are ex-
amples of a projective and a non-projective dependency
tree, respectively. The first lines show the original words
in Persian. The pronunciation and their meanings are
shown in the second line and the third line respectively. In
the fourth line, the part of speech (POS) tags of the words
are presented. Note that the words are written from right
to left (the direction of Perso-Arabic script). The depen-
dency relations are described in Table 2. The relation is
shown with an arc pointing from the head to the depen-
dent.
previous annotated trees. In the next step, automatic
annotation is corrected manually.
The remainder of this paper is as follows. In Sec-
tion 2, we briefly review the challenges in Persian
language processing. In Sections 3 and 4, the de-
tails about the annotation process, linguistic and sta-
tistical information about the data and the annotator
agreement are reported. In Section 5, the conclusion
and suggestions for future research are presented.
2 Persian Language Processing Challenges
Persian is an Indo-European language that is writ-
ten in Arabic script. There are lots of problems
in its orthography such as encoding problems, hid-
den diacritics and writing standards (Kashefi et al,
2010). A number of challenges such as the free or-
Raw Sentence
Encoding and
Spell Correction
Tokenization and
POS Tagging
Verb Analysis
Dependency
Parsing Parsing Model
Manual Error
Correction
(Treebank
Annotation)
Dependency
Treebank
Need to
Update the
Parsing
Model?
Retrain the
Parser
Insert
Add to the Treebank
Yes
Update Model
Figure 2: Diagram of bootstrapping approach in the de-
velopment of the dependency treebank.
der of words, the existence of colloquial texts, the
pro-drop nature of the Persian language and its com-
plex inflections (Shamsfard, 2011) in addition to the
lack of efficient annotated linguistic data have made
the processing of Persian texts very difficult; e.g.
there are more than 100 conjugates and 2800 de-
clensions for some word forms in Persian (Rasooli
et al, 2011b), some words in the Persian language
do not have a clear word category (i.e. the lexical
category ?mismatch?) (Karimi-Doostan, 2011a) and
many compound verbs (complex predicates) can be
separable (i.e. the non-verbal element may be sepa-
rated from the verbal element by one or more other
words) (Karimi-Doostan, 2011b).
After the development of the Bijankhan corpus
(Bijankhan, 2004) with the annotation of word cat-
egories, other kinds of datasets have been created
to address the need for Persian language process-
ing. Among them, a Persian parser based on link
grammar (Dehdari and Lonsdale, 2008), a compu-
tational grammar based on GPSG (Bahrani et al,
2011), syntactic treebank based on HPSG (Ghay-
oomi, 2012) and Uppsala dependency treebank (Ser-
aji et al, 2012) are the efforts to satisfy the need for
307
syntactic processing in the Persian language.
?XQ? ?

K AK. ?XAK

	P ?A? IJ. m
? root
k?rd?m to bA zijAdi sohb?thAje
did (1st, sing) you with a lot speaking(s)
V PR PP ADJ N
root
NVE
NPOSTMOD
NPP
POSDEP
(a) A simple dependency tree with compound verb
for a Persian sentence: ?I spoke with you a lot?.
The NVE is a relation between a light verb and its
nonverbal element. As shown in the tree, not only
the nonverbal element is not near the light verb, but
also it is inflected for plurality (i.e. speakings).
??P?? ?
	
KA
	
g 	?K
@
	P@ ?P@X root
mir?v?m xAne Pin P?z dAr?m
go (pres.cont., 1st sing.) house this from have (pres., 1st sing.)
V N PREM PP V
root
PROG
VPP
POSDEP
NPREMOD
(b) A simple dependency tree for a Persian sentence with a pro-
gressive auxiliary: ?I am going from this house?. The PROG is a
relation between a verb and its progressive auxiliary.
I ????@?
	
mProceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1349?1359,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Unsupervised Morphology-Based Vocabulary Expansion
Mohammad Sadegh Rasooli, Thomas Lippincott, Nizar Habash and Owen Rambow
Center for Computational Learning Systems
Columbia University, New York, NY, USA
{rasooli,tom,habash,rambow}@ccls.columbia.edu
Abstract
We present a novel way of generating un-
seen words, which is useful for certain ap-
plications such as automatic speech recog-
nition or optical character recognition in
low-resource languages. We test our vo-
cabulary generator on seven low-resource
languages by measuring the decrease in
out-of-vocabulary word rate on a held-out
test set. The languages we study have
very different morphological properties;
we show how our results differ depend-
ing on the morphological complexity of
the language. In our best result (on As-
samese), our approach can predict 29% of
the token-based out-of-vocabulary with a
small amount of unlabeled training data.
1 Introduction
In many applications in human language technolo-
gies (HLT), the goal is to generate text in a target
language, using its standard orthography. Typical
examples include automatic speech recognition
(ASR, also known as STT or speech-to-text), opti-
cal character recognition (OCR), or machine trans-
lation (MT) into a target language. We will call
such HLT applications ?target-language genera-
tion technologies? (TLGT). The best-performing
systems for these applications today rely on train-
ing on large amounts of data: in the case of ASR,
the data is aligned audio and transcription, plus
large unannotated data for the language model-
ing; in the case of OCR, it is transcribed optical
data; in the case of MT, it is aligned bitexts. More
data provides for better results. For languages with
rich resources, such as English, more data is often
the best solution, since the required data is readily
available (including bitexts), and the cost of anno-
tating (e.g., transcribing) data is outweighed by the
potential significance of the systems that the data
will enable. Thus, in HLT, improvements in qual-
ity are often brought about by using larger data
sets (Banko and Brill, 2001).
When we move to low-resource languages, the
solution of simply using more data becomes less
appealing. Unannotated data is less readily avail-
able: for example, at the time of publishing this
paper, 55% of all websites are in English, the top
10 languages collectively account for 90% of web
presence, and the top 36 languages have a web
presence that covers at least 0.1% of web sites.
1
All other languages (and all languages considered
in this paper except Persian) have a web presence
of less than 0.1%. Considering Wikipedia, another
resource often used in HLT, English has 4.4 mil-
lion articles, while only 48 other languages have
more than 100,000.
2
As attention turns to de-
veloping HLT for more languages, including low-
resource languages, alternatives to ?more-data?
approaches become important.
At the same time, it is often not possible to use
knowledge-rich approaches. For low-resource lan-
guages, resources such as morphological analyz-
ers are not usually available, and even good schol-
arly descriptions of the morphology (from which
a tool could be built) are often not available. The
challenge is therefore to use data, but to make do
with a small amount of data, and thus to use data
better. This paper is a contribution to this goal.
Specifically, we address TLGTs, i.e., the types
of HLT mentioned above that generate target lan-
guage text. We propose a new approach to gener-
ating unseen words of the target language which
have not been seen in the training data. Our ap-
proach is entirely unsupervised. It assumes that
word-units are specified, typically by whitespace
and punctuation.
1
http://en.wikipedia.org/wiki/
Languages_used_on_the_Internet
2
http://meta.wikimedia.org/wiki/List_
of_Wikipedias
1349
Expanding the vocabulary of the target lan-
guage can be useful for TLGTs in different ways.
For ASR and OCR, which can compose words
from smaller units (phones or graphically recog-
nized letters), an expanded target language vocab-
ulary can be directly exploited without the need
for changing the technology at all: the new words
need to be inserted into the relevant resources (lex-
icon, language model) etc, with appropriately es-
timated probabilities. In the case of MT into mor-
phologically rich low-resource languages, mor-
phological segmentation is typically used in devel-
oping the translation models to reduce sparsity, but
this does not guarantee against generating wrong
word combinations. The expanded word combi-
nations can be used to extend the language models
used for MT to bias against incoherent hypothe-
sized new sequences of segmented words.
Our approach relies on unsupervised morpho-
logical segmentation. We do not in this paper con-
tribute to research in unsupervised morphological
segmentation; we only use it. The contribution
of this paper lies in proposing how to use the re-
sults of unsupervised morphological segmentation
in order to generate unseen words of the language.
We investigate several ways of doing so, and we
test them on seven low-resource languages. These
languages have very different morphological prop-
erties, and we show how our results differ depend-
ing on the morphological complexity of the lan-
guage. In our best result (on Assamese), we show
that our approach can predict 29% of the token-
based out-of-vocabulary with a small amount of
unlabeled training data.
The paper is structured as follows. We first dis-
cuss related work in Section 2. We then present
our method in Section 3, and present experimental
results in Section 4. We conclude with a discus-
sion of future work in Section 5.
2 Related Work
Approaches to Morphological Modeling
Computational morphology is a very active area
of research with a multitude of approaches that
vary in the degree of manual annotation needed,
and the amount of machine learning used. At one
extreme, we find systems that are painstakingly
and carefully designed by hand (Koskenniemi,
1983; Buckwalter, 2004; Habash and Rambow,
2006; D?etrez and Ranta, 2012). Next on the
continuum, we find work that focuses on defining
morphological models with limited lexica that
are then extended using raw text (Cl?ement et al,
2004; Forsberg et al, 2006). In the middle of
this continuum, we find efforts to learn complete
paradigms using fully supervised methods relying
on completely annotated data points with rich
morphological information (Durrett and DeNero,
2013; Eskander et al, 2013). Next, there is
work on minimally supervised methods that use
available resources such as dictionaries, bitexts,
and other additional morphological annotations
(Yarowsky and Wicentowski, 2000; Cucerzan and
Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder
and Barzilay, 2008). At the other extreme, we
find unsupervised methods that learn morphology
models from unannotated data (Creutz and Lagus,
2007; Monson et al, 2008; Dreyer and Eisner,
2011; Sirts and Goldwater, 2013).
The work we present in this paper makes no
use of any morphological annotations whatsoever,
yet we are quite distinct from the approaches cited
above. We compare our work to two efforts specif-
ically. First, consider work in automatic mor-
phological segmentation learning from unanno-
tated data (Creutz and Lagus, 2007; Monson et
al., 2008). Unlike these approaches which provide
segmentations for training data and produce mod-
els that can be used to segment unseen words, our
approach can generate words that have not been
seen in the training data. The focus of efforts is
rather complementary: we actually use an off-the-
shelf unsupervised segmentation system (Creutz
and Lagus, 2007) as part of our approach. Second,
consider paradigm completion methods such as
the work of Dreyer and Eisner (2011). This effort
is closely related to our work although unlike it,
we make no assumptions about the data and do not
introduce any restrictions along the lines of deriva-
tion/inflectional morphology: Dreyer and Eisner
(2011) limited their work to verbal paradigms and
used annotated training data in addition to basic
assumptions about the problem such as the size
of the paradigms. In our approach, we have zero
annotated information and we do not distinguish
between inflectional and derivational morphology,
nor do we limit ourselves to a specific part-of-
speech (POS).
Vocabulary Expansion in HLT There have
been diverse approaches towards dealing with out-
of-vocabulary (OOV) words in ASR. In some
models, the approach is to expand the lexicon by
1350
adding new words or pronunciations. Ohtsuki et
al. (2005) propose a two-run model where in the
first run, the input speech is recognized by the
reference vocabulary and relevant words are ex-
tracted from the vocabulary database and added
thereafter to the reference vocabulary to build an
expanded lexicon. Word recognition is done in the
second run based on the lexicon. Lei et al (2009)
expanded the pronunciation lexicon via generat-
ing all possible pronunciations for a word be-
fore lattice generation and indexation. There are
also other methods for generating abbreviations in
voice search systems such as Yang et al (2012).
While all of these approaches involve lexicon ex-
pansion, they do not employ any morphological
information.
In the context of MT, several researchers have
addressed the problem of OOV words by relating
them to known in-vocabulary (INV) words. Yang
and Kirchhoff (2006) anticipated OOV words
that are potentially morphologically related using
phrase-based backoff models. Habash (2008) con-
sidered different techniques for vocabulary expan-
sion online. One of their techniques learned mod-
els of morphological mapping between morpho-
logically rich source words in Arabic that pro-
duce the same English translation. This was used
to relate an OOV word to a morphologically re-
lated INV word. Another technique expanded
the MT phrase tables with possible transliterations
and spelling alternatives.
3 Morphology-based Vocabulary
Expansion
3.1 Approach
Our approach to morphology-based vocabulary
expansion consists of three steps (Figure 1). We
start with a ?training? corpus of (unannotated)
words and generate a list of new (unseen) words
that expands the vocabulary of the training corpus.
1. Unsupervised Morphology Segmentation
The first step is to segment each word in the
training corpus into sequences of prefixes,
stem and suffixes, where the prefixes and suf-
fixes are optional.
3
2. FST-based Morphology Expansion We
then construct new word models using the
3
In this paper, we use an off-the-shelf system for this step
but plan to explore new methods in the future, such as joint
segmentation and expansion.
segmented stems and affixes. We explore two
different techniques for morphology-based
vocabulary expansion that we discuss below.
The output of these models is represented as
a weighted finite state machine (WFST).
3. Reranking Models Given that the size of the
expanded vocabulary can be quite large and
it may include a lot of over-generation, we
rerank the expanded set of words before tak-
ing the top n words to use in downstream
processes. We consider four reranking con-
ditions which we describe below.
Training Transcripts
Unsupervised
Morphology
Segmentation
Segmented Words
FST-based
Expansion Model
Expanded List
Reranking
Reranked Expansion
Figure 1: The flowchart of the lexicon expansion
system.
3.2 Morphology Expansion Techniques
As stated above, the input to the morphology ex-
pansion step is a list of words segmented into mor-
phemes: zero or more prefixes, one stem, and zero
or more suffixes. Figure 2a presents an example of
such input using English words (for clarity).
We use two different models of morphology ex-
pansion in this paper: Fixed Affix model and Bi-
gram Affix model.
3.2.1 Fixed Affix Expansion Model
In the Fixed Affix model, we construct a set of
fused prefixes from all the unique prefix sequences
in the training data; and we similarly construct a
1351
re+ pro+ duc +e
func +tion +al
re+ duc +e
re+ duc +tion +s
in
pro+ duct
concept +u +al + ly
(a) Training data with morpheme boundaries. Prefixes end with and suffixes start with ?+? signs.
30 1
repro
<epsilon>
re
pro
2
duc
func
in
concept
duct
e
tional
tions
utually
<epsilon>
(b) FST for the Fixed Affix expansion model
30 4re
<epsilon>
1pro
<epsilon>
2
duc
func
in
concept
duct
e
<epsilon>
5
tion
u
7
tion
6al
s
ly
<epsilon>
(c) FST for the Bigram Affix expansion model
Figure 2: Two models of word generation from morphologically annotated data. In our experiments, we
used weighted finite state machine. We use character-based WFST in the implementation to facilitate
analyzing inputs as well as word generation.
set of fused suffixes from all the unique suffix se-
quences in the training data. In other words, we
simply pick characters from beginning of the word
up to the first stem as the prefix and characters
from the first suffix to the end of the word as the
suffix. Everything in the middle is the stem. In
this model, each word has one single prefix and
one single suffix (each of which can be empty in-
dependently). The Fixed Affix model is simply
the concatenation of the disjunction of all prefixes
with the disjunction of all stems and the disjunc-
tion of all suffixes into one FST:
prefix? stem? suffix
The morpheme paths in the FST are weighted to
reflect their probability in the training corpus.
4
Figure 2b exemplifies a Fixed Affix model derived
from the example training data in Figure 2a.
4
We convert the probability into a cost by taking the neg-
ative of the log of the probability.
3.2.2 Bigram Affix Expansion Model
In the Bigram Affix model, we do the same for the
stem as in the Fixed Affix model, but for prefixes
and suffixes, we create a bigram language model
in the finite state machine. The advantage of this
technique is that unseen compound affixes can be
generated by our model. For example, the Fixed
Affix model in Figure 2b cannot generate the word
func+tion+al+ly since the suffix +tionally is not
seen in the training data. However, this word can
be generated in the Bigram Affix model as shown
in Figure 2c: there is a path passing 0? 4? 1?
2 ? 5 ? 6 ? 3 in the FST that can produce this
word. We expect this model to have better recall
for generating new words in the language because
of its affixation flexibility.
3.3 Reranking Techniques
The expanded models allow for a large number of
words to be generated. We limit the number of vo-
cabulary expansion using different thresholds af-
ter reranking or reweighing the WFSTs generated
1352
above. We consider four reranking conditions.
3.3.1 No Reranking (NRR)
The baseline reranking option is no reranking
(NRR). In this approach we use the weights in
the WFST, which are based on the independent
prefix/stem/suffix probabilities, to determine the
ranking of the expanded vocabulary.
3.3.2 Trigraph-based Reweighting (W?Tr)
We reweight the weights in the WFST model
(Fixed or Bigram) by composing it with a letter
trigraph language model (W?Tr). A letter tri-
graph LM is itself a WFST where each trigraph (a
sequence of three consequent letters) has an asso-
ciated weight equal to its negative log-likelihood
in the training data. This reweighting allows us
to model preferences of sequences of word letters
seen more in the training data. For example, in a
word like producttions, the trigraphs ctt and tti are
very rare and thus decrease its probability.
3.3.3 Trigraph-based Reranking (TRR)
When we compose our initial WFST with the tri-
graph FST, the probability of each generated word
from the new FST is equal to the product of the
probability of its morphemes and the probabilities
of each trigraph in that word. This basically makes
the model prefer shorter words and may degrade
the effect of morphology information. Instead of
reweighting the WFST, we get the n-best list of
generated words and rerank them using their tri-
graph probabilities. We will refer to this technique
as TRR.
3.3.4 Reranking Morpheme Boundaries
(BRR)
The last reranking technique reranks the n-best
generated word list with trigraphs that are incident
on the morpheme boundaries (in case of Bigram
Affix model, the last prefix and first suffix). The
intuition is that we already know that any mor-
pheme that is generated from the morphology FST
is already seen in the training data but the bound-
ary for different morphemes are not guaranteed to
be seen in the training data. For example, for the
word producttions, we only take into account the
trigraphs rod, odu, ctt and tti instead of all possible
trigraphs. We will refer to this technique as BRR.
4 Evaluation
4.1 Evaluation Data and Tools
Evaluation Data The IARPA Babel program is
a research program for developing rapid spoken
detection systems for under-resourced languages
(Harper, 2013). We use the IARPA Babel pro-
gram limited language pack data which consists
of 20 hours of telephone speech with transcrip-
tion. We use six languages which are known
to have rich morphology: Assamese (IARPA-
babel102b-v0.5a), Bengali (IARPA-babel103b-
v0.4b), Pashto (IARPA-babel104b-v0.4bY), Taga-
log (IARPA-babel106-v0.2g), Turkish (IARPA-
babel105b-v0.4) and Zulu (IARPA-babel206b-
v0.1e). Speech annotation such as silences and
hesitations are removed from transcription and all
words are turned into lower-case (for languages
using the Roman script ? Tagalog, Turkish and
Zulu). Moreover, in order to be able to perform a
manual error analysis, we include a language that
has rich morphology and of which the first author
is a native speaker: Persian. We sampled data from
the training and development set of the Persian de-
pendency treebank (Rasooli et al, 2013) to create
a comparable seventh dataset in Persian. Statis-
tics about the datasets are shown in Table 1. We
also conduct further experiments on just verbs and
nouns in the data set for Persian (Persian-N and
Persian V). As shown in Table 1, the training data
is very small and the OOV rate is high especially
in terms of types. For some languages that have
richer morphology such as Turkish and Zulu, the
OOV rate is much higher than other languages.
Word Generation Tools and Settings For un-
supervised learning of morphology, we use Mor-
fessor CAT-MAP (v. 0.9.2) which was shown to be
a very accurate morphological analyzer for mor-
phologically rich languages (Creutz and Lagus,
2007). In order to be able to analyze Unicode-
based data, we convert each character in our
dataset to some conventional ASCII character and
then train Morfessor on the mapped dataset; after
finishing the training, we map the data back to the
original character set. We use the default setting
in Morfessor for unsupervised learning.
For preparing the WFST, we use OpenFST (Ri-
ley et al, 2009). We get the top one million short-
est paths (i.e., least costly paths of words) and ap-
ply our reranking models on them. It is worth
pointing out that our WFSTs are character-based
1353
Language
Training Data Development Data
Type Token Type Token Type OOV% Token OOV%
Assamese 8694 73151 7253 66184 49.57 8.28
Bengali 9460 81476 7794 70633 50.65 8.47
Pashto 6968 115069 6135 108137 44.89 4.25
Persian 14047 71527 10479 42939 44.16 12.78
Tagalog 6213 69577 5480 64334 54.95 7.81
Turkish 11985 77128 9852 67042 56.84 12.34
Zulu 15868 65655 13756 57141 68.72 21.76
Persian-N 9204 31369 7502 18816 46.36 22.11
Persian-V 2653 11409 1332 7318 41.07 9.01
Table 1: Statistics of training and development data for morphology-based unsupervised word generation
experiments.
and thus we also have a morphological analyzer
that can give all possible segmentations for a given
word. By running the morphological analyzer on
the OOVs, we can have the potential upper bound
of OOV reduction by the system (labeled ??? in
Tables 2 and 3).
4.2 Lexicon Expansion Results
The results for lexicon expansion are shown in Ta-
ble 2 for types and Table 3 for tokens.
We use the trigraph WFST as our baseline
model. This model does not use any morphologi-
cal information. In this case, words are generated
according to the likelihood of their trigraphs, with-
out using any information from the morphologi-
cal segmentation. We call this model the trigraph
WFST (Tr. WFST). We consistently have better
numbers than this baseline in all of our models
except for Pashto when measured by tokens. ?
is the upper-bound OOV reduction for our expan-
sion model: for each word in the development set,
we ask if our model, without any vocabulary size
restriction at all, could generate it.
The best results (again, except for Pashto) are
achieved using one of the three reranking methods
(reranking by trigraph probabilities or morpheme
boundaries) as opposed to doing no reranking. To
our surprise, the Fixed Affix model does a slightly
better job in reducing out of vocabulary than the
Bigram Affix model. We can also see from the
results that reranking in general is very effective.
We also compare our models with the case that
there is much more training data and we do not do
vocabulary expansion at all. In Table 2 and Ta-
ble 3, ?FP? indicates the full language pack for
the Babel project data which is approximately six
to eight times larger than the limited pack training
data, and the full training data for Persian which
is approximately five times larger. We see that
the larger training data outperforms our methods
in all languages. However, from the results of?,
which is the upper-bound OOV reduction by our
expansion model, for some languages such as As-
samese, our numbers are close to the FP results
and for Zulu it is even better than FP.
We also study how OOV reduction is affected
by the size of the generated vocabulary. The
trends for different sizes of the lexicon expansion
by Fixed Affix model that is reranked by trigraph
probabilities is shown in Figure 3. As seen in the
results, for languages that have richer morphol-
ogy, it is harder to achieve results near to the up-
per bound. As an outlier, morphology does not
help for Pashto. One possible reason might be that
based on the results in Table 4, Morfessor does not
explore morphology in Pashto as well as other lan-
guages.
Morphological Complexity As for further anal-
ysis, we can study the correlation between mor-
phological complexity and hardness of reducing
OOVs. Much work has been done in linguis-
tics to classify languages (Sapir, 1921; Greenberg,
1960). The common wisdom is that languages
are not either agglutinative or fusional, but are
on a spectrum; however, no work to our knowl-
edge places all languages (or at least the ones we
worked on) on such a spectrum. We propose sev-
eral metrics. First, we can consider the number
of unique affixival morphemes in each language,
as determined by Morfessor. As shown in Table 4
(|pr| + |sf |), Zulu has the most morphemes and
Pashto the fewest. A second possible metric of the
1354
Language
Tr. Fixed Affix Model Bigram Affix Model FP
WFST NRR W?Tr TRR BRR ? NRR W?Tr TRR BRR ?
Assamese 15.94 24.03 28.46 28.15 27.15 48.07 23.50 28.15 27.84 26.59 51.02 50.96
Bengali 15.68 20.09 24.75 24.49 22.54 40.98 21.78 24.65 24.67 23.51 42.55 48.83
Pashto 18.70 19.03 19.28 19.24 18.63 25.13 19.43 18.81 18.92 18.77 25.24 64.96
Persian 12.83 18.95 18.39 19.30 19.99 50.11 18.58 18.09 18.65 18.84 53.13 58.45
Tagalog 11.39 14.61 16.51 16.21 16.81 35.64 14.45 16.01 15.81 16.74 38.72 53.64
Turkish 07.75 09.11 14.79 14.79 14.71 55.48 09.04 13.63 14.34 13.52 66.54 53.54
Zulu 07.63 11.87 12.96 13.87 13.68 66.73 12.04 12.35 13.69 13.75 82.38 35.62
Average 12.85 16.81 19.31 19.31 19.07 46.02 17.02 18.81 19.13 18.81 51.37 52.29
Persian-N 14.86 24.67 22.74 22.83 24.15 37.32 23.78 21.68 22.51 23.32 38.38 -
Persian-V 54.84 68.19 72.39 73.49 71.12 80.44 67.28 71.48 72.58 70.02 80.62 -
Table 2: Type-based expansion results for the 50k-best list for different models. Tr. WFST stands for
trigraph WFST, NRR for no reranking, W?Tr for trigraph reweighting, TRR for trigraph-based rereank-
ing, BRR for reranking morpheme boundary, and? for the upper bound of OOV reduction via lexicon
expansion if we produce all words. FP (full-pack data) shows the effect of using bigger data with the size
of about seven times larger than our data set, instead of using our unsupervised approach.
Language
Tr. Fixed Affix Model Bigram Affix Model FP
WFST NRR W?Tr TRR BRR ? NRR W?Tr TRR BRR ?
Assamese 18.07 25.70 29.43 29.12 28.13 47.88 25.34 29.06 28.82 27.64 50.31 58.03
Bengali 17.79 20.91 25.61 25.27 23.65 40.60 22.58 25.20 25.41 24.77 42.22 55.92
Pashto 21.27 19.40 19.94 19.92 18.59 25.45 19.68 19.40 19.29 18.72 25.58 71.46
Persian 14.78 20.77 20.32 21.30 22.03 51.00 20.63 19.72 20.61 20.95 54.01 63.10
Tagalog 12.88 14.55 16.88 16.36 16.60 33.95 14.37 16.12 16.12 16.38 37.07 61.53
Turkish 09.97 11.42 17.82 17.67 17.23 56.54 11.05 16.82 17.41 15.98 66.54 59.68
Zulu 08.85 13.70 14.72 15.62 15.67 68.07 13.70 14.07 15.47 15.60 87.90 41.27
Average 14.80 18.06 20.67 20.75 20.27 44.78 18.19 20.48 20.45 20.01 51.95 58.71
Persian-N 16.82 26.46 24.42 24.56 25.71 38.40 25.69 23.50 24.20 25.04 39.41 ?
Persian-V 60.09 71.47 75.57 76.48 73.60 82.55 70.56 74.81 75.72 72.53 82.70 ?
Table 3: Token-based expansion results for the 50k-best list for different models. Abbreviations are the
same as Table 2.
complexity of the morphology is by calculating
the average number of unique prefix-suffix pairs
in the training data after morpheme segmentation
which is shown as |If | in Table 4. Finally, a third
possible metric is the number of all possible words
that can be generated (|L|). These three metrics
correlate fairly well across the languages.
The metrics we propose also correlate with
commonly accepted classifications: e.g., Zulu and
Turkish (highly agglutinative) have higher scores
in terms of our |pr| + |sf |, |If | and |L| metrics in
Table 4 than other languages. The results from full
language packs in Table 3 also show that there is
a reverse interaction of morphological complexity
and the effect of blindly adding more data. Thus
for morphologically rich languages, adding more
data is less effective than for languages with poor
morphology.
The size of the languages (|L|) suggests that we
are suffering from vast overgeneration; we over-
generate because in our model any affix can at-
tach to any stem, which is not in general true.
Thus there is a lack of linguistic knowledge such
as paradigm information (Stump, 2001) for each
word category in our model. In other words, all
morphemes are treated the same in our model
which is not true in natural languages. One way
to tackle this problem is through an unsupervised
POS tagger. The challenge here is that fully unsu-
pervised POS taggers (without any tag dictionary)
are not very accurate (Christodoulopoulos et al,
2010). Another way is through using joint mor-
1355
Figure 3: Trends for token-based OOV reduction with different sizes for the Fixed Affix model with
trigraph reranking.
Language |pr| |stm| |sf| |L| |If|
Assamese 4 4791 564 10.8M 1.8
Bengali 3 6496 378 7.4M 1.5
Pashto 1 5395 271 1.5M 1.3
Persian 49 6998 538 184M 2.0
Tagalog 179 4259 299 228M 1.5
Turkish 45 5266 1801 427M 2.3
Zulu 2254 5680 427 5.5B 2.8
Persian-N 3 6121 268 4.9M 1.5
Persian-V 43 788 44 1.5M 3.4
Table 4: Information about the number of unique
morphemes in the Fixed Affix model for each
dataset including empty affixes. |L| shows the
upper bound of the number of possible unique
words that can be generated from the word gener-
ation model. |If | is the average number of unique
prefix-suffix pairs (including empty pairs) for each
stem.
phology and tagging models such as Frank et al
(2013).
Error Analysis on Turkish Unfortunately for
most languages we could not find an available
rule-based or supervised morphological analyzer
to verify the words generated by our model. The
only available tool for us is a Turkish finite-state
morphological analyzer (Oflazer, 1996) imple-
mented with the Xerox FST toolkit (Beesley and
Karttunen, 2003). As we can see in Table 5, the
system with the largest proportion of correct gen-
erated words reranks the expansion with trigraph
probabilities using a Fixed Affix model. Results
also show that we are overgenerating many non-
sense words that we ought to be pruning from our
results. Another observation is that the recognition
percentage of the morphological analyzer on INV
words is much higher than on OOVs, which shows
that OOVs in Turkish dataset are much harder to
analyze.
1356
Model Precision
Tr. WFST 17.19
Fixed Affix Model
NRR 13.36
W?Tr 25.66
TRR 26.30
BRR 25.14
Bigram Affix Model
NRR 12.94
W?Tr 24.21
TRR 25.39
BRR 23.45
Development
words 89.30
INVs 95.44
OOVs 84.64
Table 5: Results from running a hand-crafted
Turkish morphological analyzer (Oflazer, 1996)
on different expansions and on the development
set. Precision refers to the percentage of the words
are recognized by the analyzer. The results on de-
velopment are also separated into INV and OOV.
Error Analysis on Persian From the best 50k
word result for Persian (Fixed Affix Model:BRR),
we randomly picked 200 words and manually an-
alyzed them. 89 words are correct (45.5%) where
55.0% of these words are from noun affixation,
23.6% from verb clitics, 9.0% from verb inflec-
tions, 5.6% from incorrect affixations that acci-
dentally resulted in possible words, 4.5% from un-
inflected stems, and a few from adjective affixa-
tion. Among incorrectly generated words, 65.8%
are from combining a stem of one POS with af-
fixes from another POS (e.g., attaching a noun af-
fix to a verb stem), 14.4% from combining a stem
with affixes which are compatible with POS but
not allowed for that particular stem (e.g., there is
a noun suffix that can only attach to a subset of
noun stems), 9.0% are from wrong affixes pro-
duced by Morfessor and others are from incorrect
vowel harmony or double affixation.
In order to study the effect of vocabulary ex-
pansion more deeply, we trained a subset of all
nouns and verbs in the same dataset (also shown
in Table 1). Verbs in Persian have rich but more
or less regular morphology, while nouns, which
have many irregular cases, have rich morphol-
ogy but not as rich as verbs. The results in Ta-
ble 4 show that Morfessor captures these phenom-
ena. Furthermore, our results in Table 2 and Ta-
ble 3 show that our performance on OOV reduc-
tion with verbs is far superior to our performance
with nouns. We also randomly picked 200 words
from each of the experiments (noun and verbs)
to study the degree of correctness of generated
forms. For nouns, 94 words are correct and for
verbs only 71 words are correct. Most verb errors
are due to incorrect morpheme extraction by Mor-
fessor. In contrast, most noun errors result from
affixes that are only compatible with a subset of
all possible noun stems. This suggests that if we
conduct experiments using more accurate unsu-
pervised morphology and also have a more fine-
grained paradigm completion model, we might
improve our performance.
5 Conclusion and Future Work
We have presented an approach to generating new
words. This approach is useful for low-resource,
morphologically rich languages. It provides words
that can be used in HLT applications that require
target-language generation in this language, such
as ASR, OCR, and MT. An implementation of our
approach, named BabelGUM (Babel General Un-
supervised Morphology), will be publicly avail-
able. Please contact the authors for more infor-
mation.
In future work we will explore the possibil-
ity of jointly performing unsupervised morpho-
logical segmentation with clustering of words
into classes with similar morphological behavior.
These classes will extend POS classes. We will
tune the system for our purposes, namely OOV re-
duction.
Acknowledgements
We thank Anahita Bhiwandiwalla, Brian Kings-
bury, Lidia Mangu, Michael Picheny, Beno??t
Sagot, Murat Saraclar, and G?eraldine Walther for
helpful discussions. The project is supported by
the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Department of Defense U.S.
Army Research Laboratory (DoD/ARL) contract
number W911NF-12-C-0012. The U.S. Govern-
ment is authorized to reproduce and distribute
reprints for Governmental purposes notwithstand-
ing any copyright annotation thereon. Disclaimer:
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, DoD/ARL, or the U.S. Government.
1357
References
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, ACL
?01, pages 26?33, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Kenneth R Beesley and Lauri Karttunen. 2003. Finite-
state morphology: Xerox tools and techniques.
CSLI, Stanford.
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsu-
pervised pos induction: How far have we come?
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
575?584. Association for Computational Linguis-
tics.
Lionel Cl?ement, Beno??t Sagot, and Bernard Lang.
2004. Morphology based automatic acquisition of
large-coverage lexica. In Proceedings of the Fourth
International Conference on Language Resources
and Evaluation (LREC?04). European Language Re-
sources Association (ELRA).
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In The 6th Conference on Natural Lan-
guage Learning (CoNLL-2002), pages 1?7.
Gr?egoire D?etrez and Aarne Ranta. 2012. Smart
paradigms and the predictability and complexity of
inflectional morphology. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 645?653.
Association for Computational Linguistics.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using a
dirichlet process mixture model. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 616?627. Associ-
ation for Computational Linguistics.
Greg Durrett and John DeNero. 2013. Supervised
learning of complete morphological paradigms. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 1185?1195. Association for Computa-
tional Linguistics.
Ramy Eskander, Nizar Habash, and Owen Rambow.
2013. Automatic extraction of morphological lex-
icons from morphologically annotated corpora. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1032?1043, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Markus Forsberg, Harald Hammarstr?om, and Aarne
Ranta. 2006. Morphological lexicon extraction
from raw text data. Advances in Natural Language
Processing, pages 488?499.
Stella Frank, Frank Keller, and Sharon Goldwater.
2013. Exploring the utility of joint morphological
and syntactic learning from child-directed speech.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
30?41. Association for Computational Linguistics.
Joseph H Greenberg. 1960. A quantitative approach to
the morphological typology of language. Interna-
tional journal of American linguistics, pages 178?
194.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
A morphological analyzer and generator for the Ara-
bic dialects. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 681?688, Sydney, Aus-
tralia.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in Arabic-English
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 57?60. Association for
Computational Linguistics.
Mary Harper. 2013. The babel program and low
resource speech technology. In Automatic Speech
Recognition and Understanding Workshop (ASRU)
Invited talk.
Kimmo Koskenniemi. 1983. Two-Level Model for
Morphological Analysis. In Proceedings of the 8th
International Joint Conference on Artificial Intelli-
gence, pages 683?685.
Xin Lei, Wen Wang, and Andreas Stolcke. 2009.
Data-driven lexicon expansion for Mandarin broad-
cast news and conversation speech recognition. In
International conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 4329?4332.
Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. Paramor: Finding paradigms
across morphology. Advances in Multilingual and
Multimodal Information Retrieval, pages 900?907.
Sylvain Neuvel and Sean A Fulop. 2002. Unsuper-
vised learning of morphology without morphemes.
In Proceedings of the ACL-02 workshop on Morpho-
logical and phonological learning-Volume 6, pages
31?40. Association for Computational Linguistics.
1358
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22(1):73?89.
Katsutoshi Ohtsuki, Nobuaki Hiroshima, Masahiro
Oku, and Akihiro Imamura. 2005. Unsupervised
vocabulary expansion for automatic transcription of
broadcast news. In International conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 1021?1024.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of
a Persian syntactic dependency treebank. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
306?314. Association for Computational Linguis-
tics.
Michael Riley, Cyril Allauzen, and Martin Jansche.
2009. Openfst: An open-source, weighted finite-
state transducer library and its applications to speech
and language. In Human Language Technologies
Tutorials: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 9?10.
Edward Sapir. 1921. Language: An introduction to
the study of speech. Harcourt, Brace and company
(New York).
Kairit Sirts and Sharon Goldwater. 2013. Minimally-
supervised morphological segmentation using adap-
tor grammars. Transactions for the ACL, 1:255?266.
Benjamin Snyder and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphologi-
cal segmentation. In Proceedings of the 46th an-
nual meeting of the association for computational
linguistics: Human language Technologies (ACL-
HLT), pages 737?745. Association for Computa-
tional Linguistics.
Gregory T. Stump. 2001. A theory of paradigm struc-
ture. Cambridge.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 41?48, Trento,
Italy.
Dong Yang, Yi-Cheng Pan, and Sadaoki Furui. 2012.
Vocabulary expansion through automatic abbrevia-
tion generation for Chinese voice search. Computer
Speech & Language, 26(5):321?335.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of the 38th
Annual Meeting on Association for Computational
Linguistics, pages 207?216.
1359
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?9,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Fast Unsupervised Dependency Parsing with Arc-Standard Transitions
Mohammad Sadegh Rasooli
Department of Computer Engineering
Iran University of Science and Technology
Narmak, Tehran, Iran
rasooli@comp.iust.ac.ir
rasooli.ms@gmail.com
Heshaam Faili
School of Electrical
and Computer Engineering
University of Tehran
Amir-Abaad, Tehran, Iran
hfaili@ut.ac.ir
Abstract
Unsupervised dependency parsing is one of
the most challenging tasks in natural lan-
guages processing. The task involves find-
ing the best possible dependency trees from
raw sentences without getting any aid from
annotated data. In this paper, we illus-
trate that by applying a supervised incre-
mental parsing model to unsupervised pars-
ing; parsing with a linear time complex-
ity will be faster than the other methods.
With only 15 training iterations with linear
time complexity, we gain results compara-
ble to those of other state of the art methods.
By employing two simple universal linguis-
tic rules inspired from the classical depen-
dency grammar, we improve the results in
some languages and get the state of the art
results. We also test our model on a part of
the ongoing Persian dependency treebank.
This work is the first work done on the Per-
sian language.
1 Introduction
Unsupervised learning of grammars has achieved
considerable focus in recent years. The lack
of sufficient manually tagged linguistic data and
the considerable successes of unsupervised ap-
proaches on some languages have motivated re-
searchers to test different models of unsupervised
learning on different linguistic representations.
Since the introduction of the dependency model
with valence (DMV) proposed by Klein and Man-
ning (2004), dependency grammar induction has
received great attention by researchers. DMV
was the first model to outperform the right attach-
ment accuracy in English. Since this achievement,
the model has been used by many researchers
(e.g. (Cohen and Smith, 2010); (Gillenwater et al,
2011); (Headden III et al, 2009); and (Spitkovsky
et al, 2011b)).
The main task of unsupervised dependency
parsing is to obtain the most likely dependency
tree of a sentence without using any annotated
training data. In dependency trees, each word has
only one head and the head of the sentence is a de-
pendent of an artificial root word. Problems such
as data sparsity and a large search space that in-
creases the ambiguity have made the task difficult.
Even deciding the direction of the link between
two words in a dependency relation has made the
task more difficult than finding phrase structures
themselves (Klein and Manning, 2004).
In this paper, we propose a model based on
Arc-Standard Transition System of Nivre (2004),
which is known as an incremental greedy projec-
tive parsing model that parses sentences in lin-
ear time. To the best of our knowledge, the only
incremental unsupervised dependency parsing is
the model of Daume? III (2009) with Shift-Reduce
parsing model (Nivre, 2003).1
Our model is not lexicalized, has a simple fea-
ture space and converges in 15 iterations with
a linear (O(n)) parsing and training time, while
other methods based on DMV in the best case
work inO(n3) time complexity withO(n3) mem-
ory use for sentences with of length n. We be-
lieve that the output of this model can also im-
prove DMV.2 In addition, we use punctuation
clues (Spitkovsky et al, 2011c), tying feature sim-
ilarity in the transition system configuration, and
1The other study is in Seginer (2007) that is for con-
stituency parsing (phrase structure extraction).
2For the effect of model initialization in unsupervised de-
pendency parsing, see Gimpel and Smith (2011).
1
?baby steps? notion (Spitkovsky et al, 2009) to
improve the model accuracy.
We test our model on 9 CoNLL 2006 and 2007
shared task data sets (Buchholz and Marsi, 2006;
Nivre et al, 2007) and WSJ part of Penn treebank
and show that in some languages our model is bet-
ter than the recent models. We also test our model
on a part of an ongoing first Persian dependency
corpus (Rasooli et al, 2011). Our study may be
the first work to test dependency parsing on the
Persian language.
The remainder of this paper is organized as fol-
lows. In Section 2, related work on unsupervised
dependency parsing is reviewed. In Section 3, we
describe our dependency parsing model. In Sec-
tion 4 and Section 5, after the reporting experi-
mental results on several languages, the conclu-
sion is made.
2 Related Work
The first considerable work on unsupervised de-
pendency parsing which outperforms the base-
line (right attachment) accuracy in English was
proposed by Klein and Manning (2004). The
model is called dependency model with valence
(DMV). In the DMV, each word can be the
head of the sentence with the probability of
P (root|X). Each word X , decides to get a
child Y from a direction (right or left), with
the probability PCHOOSE(X|Y, dir, adj), where
adj is a Boolean value indicating whether the
word has gotten a child in the direction dir or
not. The other probability used in the DMV
is PSTOP (X|Y, dir, adj) that means whether to
stop getting dependents from the direction with
adjacency value or not. All the probabilities in the
model are assumed to be independent and the de-
pendency tree likelihood is a product of all prob-
abilities. Only part of speech (POS) tags are used
as features and the probabilities are multinomial.
The model uses the inside-outside algorithm to
find all possible subtrees efficiently in Expecta-
tion Maximization (EM) algorithm.
Several researchers have tried to improve and
modify the DMV. In Headden III et al (2009), by
using the lexical values with the frequency more
than 100 and defining tied probabilistic context
free grammar (PCFG) and Dirichlet priors, the ac-
curacy is improved. In Smith and Eisner (2005),
by producing artificial neighbors of the feature
space via actions such as deletion of one word,
substitution of adjacent words and adding a word,
the likelihood of the true feature space in all
neighbors is calculated. That method is known
as contrastive estimation (CE).
In Spitkovsky et al (2009), the idea of learning
the initial parameters of the model from shorter
sentences leads to a method named ?baby steps?.
In ?baby steps?, the model prior of each training
set with the sentence length less than or equal to
N , is achieved by training DMV on the training
set with the sentence length less than or equal to
N ? 1. The other method used in the mentioned
work, is ?less is more? which hypothesize that
training on a subset of all data (with the length
of less than or equal to 15) in batch mode is more
useful than training on all data. In Spitkovsky et
al. (2010a), a combination of ?baby steps? and
?less is more?, named ?leapfrog? is applied to
the DMV. In Spitkovsky et al (2011b), a mixture
of EMs is used to improve the DMV by trying
to escape from local maxima; i.e., changing the
EM policy in some iterations in order to escape
from local maxima. The model is termed ?lateen?
EM. In Spitkovsky et al (2010b), HTML hyper-
text tags are used as indicators of phrases in or-
der to localize the search space of the dependency
model. In Spitkovsky et al (2011c), punctuation
marks are used as indicators of local dependencies
of the words in the sentence.
In Cohen and Smith (2010), shared logistic nor-
mal distribution is used to tie grammatical roles
that are not assumed to be independent from each
other. In the study, the bilingual similarity of each
POS tag probability in the dependency model is
applied to the probability model. In Blunsom and
Cohn (2010), Pitman-Yor priors (PYP) are ap-
plied to the DMV. Furthermore, tree substitution
grammar (TSG) is used as an intermediate repre-
sentation of the tree. In Gillenwater et al (2011),
a mathematical model is employed to overcome
the posterior sparsity in the DMV, by defining
constraints on the probability model.
There are also some models different from
DMV. In Daume? III (2009), based on a stochas-
tic search method, Shift-Reduce transition pars-
ing model of Nivre (2003) is applied. The model
is greedy and selects an action stochastically ac-
cording to each action probability at the time. The
advantage of the model lies on its parsing and
training speed. In Naseem and Barzilay (2011),
sparse semantic annotations in texts are used as
2
Initialization ?nil,W,??
Termination ?S, nil, A?
Left-Reduce ?wiwj |S, I, A? ? ?wj |S, I, A ? ?wj , wi??
Right-Reduce ?wiwj |S, I, A? ? ?wi|S, I, A ? ?wi, wj??
Shift ?S,wi|I, A? ? ?wi|S, I, A?
Figure 1: Actions in Arc-Standard Transition System (Nivre, 2004)
clues to unsupervised parsing. In Marec?ek and
Z?abokrtsky? (2011), by applying Gibbs sampling
method to count the data occurrences, a simple
probability model (the fraction of each depen-
dency relation divided by the number of head POS
tags) is used. In that model, non-projective depen-
dency trees are allowed and all noun-root depen-
dency probabilities are multiplied by a small num-
ber, to decrease the chance of choosing a noun-
root dependency. There are also some studies in
which labeled data in one language is employed
to guide unsupervised parsing in the others (Co-
hen et al, 2011).
3 Fast Unsupervised Parsing
In this section, after a brief description of the Arc-
Standard parsing model, our probability model,
and the unsupervised search-based structure pre-
diction (Daume? III, 2009) are reviewed. After
these descriptions, we go through ?baby steps,?
the use of curricula in unsupervised learning (Tu
and Honavar, 2011), and the use of punctuation
in unsupervised parsing. Finally, we describe our
tied feature model that tries to overcome the data
sparsity. In this paper, a mixture of ?baby steps?
and punctuation clues along with search-based
structure prediction is applied to the Arc-Standard
model.
3.1 Arc-Standard Transition Model
The parser in this model has a configuration repre-
sented by ?S, I, A?, where S is a stack of words,
I is a buffer of input words which are not pro-
cessed yet andA is the list of all arcs that are made
until now. The parser initializes with ?nil,W, ??
in which W is a string of all words in the sen-
tence, nil shows a stack with a root word and ?
shows an empty set. The termination configura-
tion is shown as ?S, nil, A?, where S shows an
empty stack with only root word, nil shows an
empty buffer and A is the full arc set. An arc in
which wj is the head of wi is shown by wj ? wi
or (wj , wi).
As shown in Figure 1, there are three actions
in this model. In the shift action, the top-most
input word goes to the top of the stack. In the left-
reduce action, the top-most stack word becomes
the head of the second item in the stack and the
second item is removed from the stack. On the
other hand, in the right-reduce action, the second
word in the stack becomes the head of the top item
in the stack and the top item is removed from the
stack.
3.2 Feature Space and Probability Model
The feature space that we use in this model is
a tuple of three POS tags; i.e., the first item in
the buffer, the top-most and the second item in
the stack. The probability of each action is in-
spired from Chelba and Jelinek (2000) as in equa-
tion (1). In each step in the configuration, the
parser chooses an action based on the probability
in equation (1), where feat is the feature value
and act is an action.
P (act, feat) = P (act) ? P (feat|act) (1)
The action selection in the training phase is
done stochastically. In other words, in every step
there is a maximum of 3 actions and a minimum
of one action.3 After calculating all probabili-
ties, a roulette wheel is made to do multinomial
sampling. The sampling is done with stochas-
tic EM (Celeux and Diebolt, 1985) in a roulette
wheel selection model.
The probabilities are initialized equally (except
that P (shift) = 0.5 and P (right ? reduce) =
P (left? reduce) = 0.25). After sampling from
the data, we update the model as in equations 2?
4, where ? is a smoothing variable and Nf is the
number of all possible unique features in the data
set. C(?) is a function that counts the data from
3For example, in the first state only shift is possible and
in the last state only right-reduce is possible.
3
samples. In equations 3 and 4, sh, r ? r and
l?r are shift, right-arc and left-arc actions respec-
tively. C(Action, Feature) is obtained from the
samples drawn in the training phase. For exam-
ple, if the right-reduce action is selected, we add
its probability to C(right? reduce, feature).
P (feat|act) =
C(act, feat) + ?
C(act) +Nf?
(2)
P (sh) = 0.5 (3)
P (act) =
C(act) + ?
C(r ? r) + C(l ? r) + 2?
;
act 6= Shift
(4)
3.3 Unsupervised Search-Based Structure
Prediction
Since there are 32n+1 possible actions for a sen-
tence with the length of n it seems impractical
to track the search space for even middle-length
sentences. Accordingly, in Daume? III (2009) a
stochastic search model is designed to improve
the model accuracy based on random actions. In
that work, with each configuration step, the trainer
selects one of the actions according to the proba-
bility of each action stochastically. By choosing
actions stochastically, a set of samples is drawn
from the data and the model parameters are up-
dated based on the pseudo-code in Figure 2. In
Figure 2, pi is known as the policy of the prob-
ability model and ? is a constant number which
changes in each iteration based on the iteration
number (? = 1iteration#3 ). We employ this model
in our work to learn probability values in equa-
tion 1. The learning from samples is done via
equations (2?4).
3.4 ?Baby Steps? Incremental Parsing
In Spitkovsky et al(2009), the idea that shorter
sentences are less ambiguous, hence more in-
formative, is applied to the task. Spitkovsky et
al. (2009) emphasize that starting from sentences
with a length of 1 and iterating on sentences with
the length ? N from the probabilities gained
from the sentences with the length ? N ? 1,
leads to better results.
Initialize pi = pi?
while not converge
Take samples stochastically
h? learn from samples
pi = ?pi + (1? ?)h
end while
return pi
Figure 2: Pseudo-code of the search-based structure
prediction model in Daume? III (2009)
We also use ?baby steps? on our incremental
model. For the sentences having length 1 through
5, we only iterate once in each sentence length.
At those sentence lengths, the full search space is
explored (all trees are made by doing all possi-
ble actions in each state of all possible configura-
tions), while for sentence length 6 towards 15, we
iterate at each step 3 times, only choosing one ac-
tion stochastically at each state. The procedure is
done similarly for all languages with the same pa-
rameters. In fact, the greedy nature of the model
encourages us to bail out of each sentence length
quickly. In other words, we want to jump out of
early local maxima, as in early-terminating lateen
EM (Spitkovsky et al, 2011b).
In curricula (Tu and Honavar, 2011), smooth-
ing variable for shorter sentences is larger than
smoothing variable for longer sentences. With re-
gards to the idea, we start with smoothing variable
equal to 1 and multiply it on each sentence length
by a constant value equal to e?1.
3.5 Punctuation Clues
Spitkovsky et al (2011c) show that about 74.0%
of words in English texts occurring between two
punctuation marks have only one word linking
with other words of the sentence. This character-
istic is known as ?loose?. We apply this restriction
on our model to improve the parsing accuracy and
decrease the total search space. We show that this
clue not only does not improve the dependency
parsing accuracy, but also decreases it in some oc-
casions.
3.6 Tying Probabilities with Feature
Similarity Measure
We assume that the most important features in
the feature set for right-reduce and left-reduce ac-
tions are the two top words in the stack. On the
other hand, for the shift action, the most impor-
4
tant words are first buffer and top stack words. In
order to solve the sparsity problem, we modify
the probability of each action based on equation
(5). In this equation, neigh(act, feat) is gained
via searching over all features with the same top
and second stack item for left-reduce and right-
reduce, and all features with the same top stack
and first buffer item for the shift action.
P ?(feat|act) =
P (feat|act) +
?
f ??neigh(act,feat) P (f
?|act)
C(neigh(act,feat))
2
(5)
3.7 Universal Linguistic Heuristics to
Improve Parsing Accuracy
Based on the nature of dependency grammar, we
apply two heuristics. In the first heuristic, we mul-
tiply the probability of the last verb reduction by
10?10 in order to keep verbocentricity of the de-
pendency grammar. The last verb reduction oc-
curs when there is neither a verb in the buffer
nor in the stack except the one that is going to
be reduced by one of the right-arc or left-arc ac-
tions. In other words, the last verb remaining
on the stack should be less likely to be removed
than the other actions in the current configura-
tion.4 In the second heuristic, in addition to the
first heuristic, we multiply each noun ? verb,
adjective ? verb, and adjective ? noun by
0.1 in order to keep the nature of dependency
grammar in which nouns and adjective in most
cases are not able to be the head of a verb and an
adjective is not able to be the head of a noun.5 We
show in the experiments that, in most languages,
considering this nature will help improve the pars-
ing accuracy.
We have tested our model on 9 CoNLL data
sets (Buchholz and Marsi, 2006; Nivre et al,
2007). The data sets include Arabic, Czech, Bul-
garian, Danish, Dutch, Portuguese, Slovenian,
Spanish, and Swedish. We have also tested our
model on a part of the ongoing project of Persian
dependency treebank. The data set includes 2,113
4It is important to note that the only reason that we
choose a very small number is to decrease the chance of
verb-reduction among three possible actions. Using other
values? 0.01 does not change results significantly.
5The are some exceptions too. For example, in the sen-
tence: ?I am certain your work is good.? Because of that, we
do not choose a very small number.
train and 235 test sentences.6
As shown in Figure 3, we use the same proce-
dure as in Daume? III (2009), except that we re-
strict ? to not be less than 0.005 in order to in-
crease the chance of finding new search spaces
stochastically. As in previous works, e.g., Smith
and Eisner (2005), punctuation is removed for
evaluation.
iteration# = 0
for i=1 to 15 do
Train-set=all sentences-length?i
max-iter=3
if(i? 5)
max-iter=1
end-if
for j=1 to max-iter do
? = max( 1iteration#3 , 0.005)
iteration#? iteration# + 1
if(i ? 5)
samples? find all subtrees
end-if
else
samples? sample instances stochastically
end-else
h? learn from samples
pi = ?pi + (1? ?)h
end-for
? = ? ? e?1
end-for
Figure 3: Pseudo-code of the unsupervised Arc-
Standard training model
4 Evaluation Results
Although training is done on sentences of length
less than 16, the test was done on all sentences in
the test data without dropping any sentences form
the test data. Results are shown in Table 1 on 9
languages. In Table 1, ?h1? and ?h2? refer to the
two linguistic heuristics that are used in this pa-
per. We also compare our work with Spitkovsky
et al (2011b) and Marec?ek and Z?abokrtsky? (2011)
6This dataset is obtained via contacting with the project
team at http://www.dadegan.ir/en/. Recently an official
pre-version of the dataset is released, consisting more
than 12,000 annotated sentences (Dadegan Research Group,
2012). We wish to report results on the dataset in our future
publications.
5
Baselines Using Heuristic 1 and 2 Using Heuristic 1 Without any heuristic
Language Rand LA RA fs+punc fs punc fs+punc punc punc+fs simp.
Arabic?07 3.90 59.00 06.00 52.05 52.05 52.05 54.55 54.55 55.64 55.64
Bulgarian 8.00 38.80 17.90 52.48 53.86 46.36 42.75 37.35 35.99 35.99
Czech?07 7.40 29.60 24.20 42.37 42.40 39.31 30.21 27.94 25.17 25.17
Danish 6.70 47.80 13.10 52.14 53.11 52.14 51.10 51.70 46.01 46.01
Dutch 7.50 24.50 28.00 48.14 48.80 48.20 28.30 28.36 23.47 23.45
Persian 9.50 03.90 29.16 51.65 51.37 50.99 49.78 50.99 26.87 26.87
Portuguese 5.80 31.20 25.80 54.86 55.84 46.82 33.84 33.62 28.83 28.83
Slovenian 7.90 26.60 24.30 22.44 22.44 22.43 21.31 21.30 19.47 19.45
Spanish 4.30 29.80 24.70 30.88 31.16 30.88 32.33 32.33 29.63 29.69
Swedish 7.80 27.80 25.90 32.74 34.33 33.52 28.48 28.48 25.74 25.74
Turkish 6.40 01.50 65.40 33.83 27.39 38.13 61.27 47.92 30.56 34.52
Average 6.84 29.14 25.86 43.05 42.98 41.89 39.45 37.69 31.58 31.94
Table 1: Results tested on CoNLL data sets and the Persian data set. ?Rand?, ?LA? and ?RA? stand for random,
left-attach and right-attach, respectively; ?punc? refers to punctuation clues and fs refers to feature similarity cue;
?all? refers to using both heuristics h1 and h2; and ?simp.? refers to the simple model.
in Table 2. As shown in Table 2, our model out-
performs the accuracy in 7 out of 9 languages.
The Effect of Feature Similarity
As shown in Table 1, feature similarity cannot
have any effect on the simple model. When we
add linguistic information to the model, this fea-
ture similarity measure keeps the trainer from
diverging. In other words, the greedy nature
of the model becomes endangered when incom-
plete knowledge (as in our linguistic heuristics)
is used. Incomplete knowledge may cause early
divergence. In other words, the greedy algo-
rithm tracks the knowledge which it has and does
not consider other probable search areas. This
phenomenon may cause early divergence in the
model. By using feature similarity, we try to es-
cape from this event.
The Effect of Punctuation Clues
As shown in Table 1, in most languages punc-
tuation clues do not improve the accuracy. This
maybe arises out of the fact that ?loose? is not a
good clue for incremental parsing. The other clue
is ?sprawl? in which the external link restriction
is lifted. This restriction is in 92.9% of fragments
in English texts (Spitkovsky et al, 2011c), but it
is not implemented and tested in this paper.
4.1 Evaluation on English
We also test our data on Penn Treebank but we do
not gain better results than state of the art meth-
ods. We use the same train and test set as in
Model WSJ?10 WSJ ??
h1+fs 45.16 31.97
h1+fs+punc 44.17 30.17
Stoch. EM(1-5) 40.86 33.65
Stoch. EM(1-5)+h1 52.70 42.85
Stoch. EM(1-5)+h1+h2 50.30 41.37
A1+fs+h1 49.9 43.3
Klein and Manning (2004) 43.2 -
Daume? III (2009) 45.4 -
Blunsom and Cohn (2010) 67.7 55.7
Spitkovsky et al (2011a) - 59.1
Table 3: Results of our model on WSJ, compared
to its counterpart Daume? III (2009) and other DMV-
based models. Since in Blunsom and Cohn (2010) and
Spitkovsky et al (2011b), other results are reported,
we only limit our report to some of the results on WSJ.
In the Table, ?h1? shows heuristic 1 and ?fs? shows
the use of feature similarity. Stochastic EM(1-5) is
one test that have done only by applying baby steps
on sentences with the length 1 to 5 without using un-
supervised search-based model. A1 refers to a change
in the model in which smoothing variable in steps 1 to
5 is multiplied by 10.
Spitkovsky et al (2009). We convert the Penn
treebank data via automatic ?head-percolation?
rules (Collins, 1999). We have also tested our
model via simple stochastic EM (without using
unsupervised structure prediction) and show that
the main problem with this method in English is
its fast divergence when jumping from sentence
length 5 to 6. In the model settings tested for
English, the model with heuristic 1 with the fea-
ture similarity is the best setting that we find. By
testing with a smoothing variable ten times bigger
6
``````````````Language
Method Name
MZ-NR MZ Spi5 Spi6 Our Best
Arabic?07 24.8 25.0 22.0 49.5 55.64
Bulgarian 51.4 25.4 44.3 43.9 53.86
Czech?07 33.3 24.3 31.4 28.4 42.40
Danish 38.6 30.2 44.0 38.3 53.11
Dutch 43.4 32.2 32.5 27.8 48.80
Persian - - - - 51.65
Portuguese 41.8 43.2 34.4 36.7 55.84
Slovenian 34.6 25.4 33.6 32.2 22.44
Spanish 54.6 53.0 33.3 50.6 32.33
Swedish 26.9 23.3 42.5 50.0 34.33
Turkish 32.1 32.2 33.4 35.9 61.27
Average (except Persian) 38.1 31.4 35.1 39.3 46.00
Table 2: Comparison of our work to those of Table 5 (?Spi5?) and Table 6 (?Spi6?) in Spitkovsky et al (2011b)
and Marec?ek and Z?abokrtsky? (2011) with Noun-Root constraint (?MZ-NR?) and no constraint (?MZ?). The
comparison results are from Table 4 in Marec?ek and Z?abokrtsky? (2011). ?Our Best? refers to the bold scores in
Table 1.
in the first 5 steps, we have seen that the results
change significantly. The results are shown in Ta-
ble 3.
One main problem of the converted depen-
dencies in English is their conversion errors like
multi-root trees.7 There are many trees in the
corpus that have wrong multi-root dependencies.
Such problems lead us to believe that we should
not rely too much on the results on WSJ part of
the Penn treebank.
5 Analysis and Conclusion
One main aspect of incremental methods is their
similarity to the way that humans read and learn
sentences in language. The interesting character-
istic of incremental parsing lies on its speed and
low memory use. In this paper, we use one new
incremental parsing model on unsupervised pars-
ing for the first time. The simple mathematical
model and its linear training order has made the
model flexible to be used for bigger data sets.
In addition to testing recently used heuristics in
unsupervised parsing, by inspiring basic depen-
dency theory from linguistics, parsing accuracy
has been increased in some languages. We see
that this model is capable of detecting many true
dependencies in many languages.
Some observations show that choosing inap-
7We assume to find only trees that are projective and
single-rooted.
propriate parameters for the model may lead to
unwanted divergence in the model. The diver-
gence is mostly seen in English, where we see
a significant accuracy decrease at the last step in
comparison to step 5 instead of seeing an increase
in the accuracy. With one setting in English, we
reach the accuracy equal to 43% (13% more than
the accuracy of the model reported in this paper).
In some languages like Slovenian, we see that
even with a good undirected accuracy, the model
does not succeed in finding the dependency di-
rection with heuristics. While in Czech, Dutch,
and Bulgarian the second heuristic works well,
it does not change accuracy a lot in other lan-
guages (in languages like Turkish and English this
heuristic decreases accuracy). We believe that
choosing better linguistic knowledge like the ones
in Naseem et al (2010), tying grammatical rules
from other languages similar to the work in Cohen
and Smith (2010), and choosing better probability
models that can be enriched with lexical features
and broad context consideration (like the works in
supervised incremental dependency parsing) will
help the model perform better on different lan-
guages.
Despite the fact that our study is the first work
done on Persian, we believe that the results that
we achieve for Persian is very considerable, re-
garding the free-word order nature of the Persian
language.
7
Acknowledgments
The paper is funded by Computer Research Cen-
ter of Islamic Sciences (CRCIS). We would like
to appreciate Valentin Spitkovsky and Shay Co-
hen for their technical comments on the re-
search and paper draft. We would also thank
Maryam Faal-Hamedanchi, Manouchehr Kouhes-
tani, Amirsaeid Moloodi, Hamid Reza Ghader,
Maryam Aminian and anonymous reviewers for
their comments on the draft version. We would
also like to thank Jason Eisner, Mark Johnson,
Noah Smith and Joakim Nivre for their help in
answering our questions and Saleh Ziaeinejad and
Younes Sangsefidi for their help on the project.
References
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2010), pages 1204?1213.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceeding of the Tenth Conforence on Computa-
tional Natural Language Learning (CoNLL).
Gilles Celeux and Jean Diebolt. 1985. The SEM al-
gorithm: A probabilistic teacher algorithm derived
from the em algorithm for the mixture problem.
Computational Statistics Quarterly, 2:73?82.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech & Lan-
guage, 14(4):283?332.
Shay B. Cohen and Noah A. Smith. 2010. Co-
variance in unsupervised learning of probabilistic
grammars. Journal of Machine Learning Research
(JMLR), 11:3117?3151.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011. Unsupervised structure prediction with Non-
Parallel multilingual guidance. In Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP 2011).
Michael Collins. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
Dadegan Research Group. 2012. Persian Dependency
Treebank Version 0.1, Annotation Manual and User
Guide. http://dadegan.ir/en/.
Hal Daume? III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning, 75(3):297?325.
Hal Daume? III. 2009. Unsupervised search-based
structured prediction. In 26th International Confer-
ence on Machine Learning (ICML), pages 209?216.
ACM.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a,
Fernando Pereira, and Ben Taskar. 2011. Poste-
rior sparsity in unsupervised dependency parsing.
Journal of Machine Learning Research (JMLR),
12:455?490.
Kevin Gimpel and Noah A. Smith. 2011. Concav-
ity and initialization for unsupervised dependency
grammar induction. Technical report.
William P. Headden III, Mark Johnson, and David
McClosky. 2009. Improving unsupervised depen-
dency parsing with richer contexts and smoothing.
In Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the ACL, pages 101?109.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure:
Models of dependency and constituency. In Asso-
ciation for Computational Linguistics (ACL).
David Marec?ek and Zdene?k Z?abokrtsky?. 2011. Gibbs
sampling with treeness constraint in unsupervised
dependency parsing. In RANLP Workshop on Ro-
bust Unsupervised and Semisupervised Methods in
Natural Language Processing.
Tahira Naseem and Regina Barzilay. 2011. Using se-
mantic cues to learn syntax. In 25th Conference on
Artificial Intelligence (AAAI-11).
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2010).
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceeding of CoNLL 2007.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In International Work-
shop on Parsing Technologies, pages 149?160.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Workshop on Incremental
Parsing: Bringing Engineering and Cognition To-
gether, pages 50?57.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
Persian verbs: The first steps towards Persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231.
Yoav Seginer. 2007. Fast unsupervised incremen-
tal parsing. In 45th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL), pages
384?391.
Noah A. Smith and Jason Eisner. 2005. Guiding un-
supervised grammar induction using contrastive es-
timation. In IJCAI Workshop on Grammatical In-
ference Applications, pages 73?82.
8
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2009. Baby steps: How ?Less is more? in
unsupervised dependency parsing. In NIPS 2009
Workshop on Grammar Induction, Representation
of Language and Language Learning.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010a. From baby steps to leapfrog: How
?Less is more? in unsupervised dependency pars-
ing. In Human Language Technologies: The 11th
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010).
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010b. Profiting from Mark-Up: Hyper-
Text annotations for guided parsing. In 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2010).
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X
Chang, and Daniel Jurafsky. 2011a. Unsupervised
dependency parsing without gold Part-of-Speech
tags. In 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2011b. Lateen EM: unsupervised train-
ing with multiple objectives, applied to dependency
grammar induction. In 2011 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel
Jurafsky. 2011c. Punctuation: Making a point
in unsupervised dependency parsing. In Fifteenth
Conference on Computational Natural Language
Learning (CoNLL-2011).
Kewei Tu and Vasant Honavar. 2011. On the utility of
curricula in unsupervised learning of probabilistic
grammars. In 22nd International Joint Conference
on Artificial Intelligence (IJCAI 2011).
9
