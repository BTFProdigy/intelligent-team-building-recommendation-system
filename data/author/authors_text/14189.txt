Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 190?197,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Model Adaptation using Information-Theoretic Criterion
Ariya Rastrow1, Frederick Jelinek1, Abhinav Sethy2 and Bhuvana Ramabhadran2
1Human Language Technology Center of Excellence, and
Center for Language and Speech Processing, Johns Hopkins University
{ariya, jelinek}@jhu.edu
2IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
{asethy, bhuvana}@us.ibm.com
Abstract
In this paper we propose a novel general
framework for unsupervised model adapta-
tion. Our method is based on entropy which
has been used previously as a regularizer in
semi-supervised learning. This technique in-
cludes another term which measures the sta-
bility of posteriors w.r.t model parameters, in
addition to conditional entropy. The idea is to
use parameters which result in both low con-
ditional entropy and also stable decision rules.
As an application, we demonstrate how this
framework can be used for adjusting language
model interpolation weight for speech recog-
nition task to adapt from Broadcast news data
to MIT lecture data. We show how the new
technique can obtain comparable performance
to completely supervised estimation of inter-
polation parameters.
1 Introduction
All statistical and machine learning techniques for
classification, in principle, work under the assump-
tion that
1. A reasonable amount of training data is avail-
able.
2. Training data and test data are drawn from the
same underlying distribution.
In fact, the success of statistical models is cru-
cially dependent on training data. Unfortunately,
the latter assumption is not fulfilled in many appli-
cations. Therefore, model adaptation is necessary
when training data is not matched (not drawn from
same distribution) with test data. It is often the case
where we have plenty of labeled data for one specific
domain/genre (source domain) and little amount of
labeled data (or no labeled data at all) for the de-
sired domain/genre (target domain). Model adapta-
tion techniques are commonly used to address this
problem. Model adaptation starts with trained mod-
els (trained on source domain with rich amount of la-
beled data) and then modify them using the available
labeled data from target domain (or instead unla-
beled data). A survey on different methods of model
adaptation can be found in (Jiang, 2008).
Information regularization framework has been
previously proposed in literature to control the la-
bel conditional probabilities via input distribution
(Szummer and Jaakkola, 2003). The idea is that la-
bels should not change too much in dense regions
of the input distribution. The authors use the mu-
tual information between input features and labels as
a measure of label complexity. Another framework
previously suggested is to use label entropy (condi-
tional entropy) on unlabeled data as a regularizer to
Maximum Likelihood (ML) training on labeled data
(Grandvalet and Bengio, 2004).
Availability of resources for the target domain cat-
egorizes these techniques into either supervised or
unsupervised. In this paper we propose a general
framework for unsupervised adaptation using Shan-
non entropy and stability of entropy. The assump-
tion is that in-domain and out-of-domain distribu-
tions are not too different such that one can improve
the performance of initial models on in-domain data
by little adjustment of initial decision boundaries
(learned on out-of-domain data).
190
2 Conditional Entropy based Adaptation
In this section, conditional entropy and its relation
to classifier performance are first described. Next,
we introduce our proposed objective function for do-
main adaptation.
2.1 Conditional Entropy
Considering the classification problem whereX and
Y are the input features and the corresponding class
labels respectively, the conditional entropy is a mea-
sure of the class overlap and is calculated as follows
H(Y|X) = EX[H(Y|X = x)] =
?
?
p(x)
(
?
y
p(y|x) log p(y|x)
)
dx (1)
Through Fano?s Inequality theorem, one can see
how conditional entropy is related to classification
performance.
Theorem 1 (Fano?s Inequality) Suppose
Pe = P{Y? 6= Y} where Y? = g(X) are the
assigned labels for the data points, based on the
classification rule. Then
Pe ?
H(Y|X)? 1
log(|Y| ? 1)
where Y is the number of possible classes and
H(Y |X) is the conditional entropy with respect to
true distibution.
The proof to this theorem can be found in (Cover and
Thomas, 2006). This inequality indicates thatY can
be estimated with low probability of error only if the
conditional entropy H(Y|X) is small.
Although the above theorem is useful in a sense
that it connects the classification problem to Shan-
non entropy, the true distributions are almost never
known to us1. In most classification methods, a spe-
cific model structure for the distributions is assumed
and the task is to estimate the model parameters
within the assumed model space. Given the model
1In fact, Theorem 1 shows how relevant the input features
are for the classification task by putting a lower bound on the
best possible classifier performance. As the overlap between
features from different classes increases, conditional entropy in-
creases as well, thus lowering the performance of the best pos-
sible classifier.
structure and parameters, one can modify Fano?s In-
equality as follows,
Corollary 1
Pe(?) = P{Y? 6= Y |?} ? H?(Y|X)? 1log(|Y| ? 1)
(2)
where Pe(?) is the classifier probability of error
given model parameters, ? and
H?(Y|X) =
?
?
p(x)
(
?
y
p?(y|x) log p?(y|x)
)
dx
Here, H?(Y|X) is the conditional entropy imposed
by model parameters.
Eqn. 2 indicates the fact that models with low
conditional entropy are preferable. However, a low
entropy model does not necessarily have good per-
formance (this will be reviewed later on) 2
2.2 Objective Function
Minimization of conditional entropy as a framework
in the classification task is not a new concept and
has been tried by researchers. In fact, (Grandvalet
and Bengio, 2004) use this along with the maxi-
mum likelihood criterion in a semi-supervised set
up such that parameters with both maximum like-
lihood on labeled data and minimum conditional en-
tropy on unlabeled data are chosen. By minimiz-
ing the entropy, the method assumes a prior which
prefers minimal class overlap. Entropy minimiza-
tion is used in (Li et al, 2004) as an unsupervised
non-parametric clustering method and is shown to
result in significant improvement over k-mean, hier-
archical clustering and etc.
These methods are all based on the fact that mod-
els with low conditional entropy have their decision
boundaries passing through low-density regions of
the input distribution, P (X). This is consistent with
the assumption that classes are well separated so that
one can expect to take advantage of unlabeled exam-
ples (Grandvalet and Bengio, 2004).
In many cases shifting from one domain to an-
other domain, initial trained decision boundaries (on
2Imagine a model which classifies any input as class 1.
Clearly for this model H?(Y|X) = 0.
191
out-of-domain data) result in high conditional en-
tropy for the new domain, due to mismatch be-
tween distributions. Therefore, there is a need to
adjust model parameters such that decision bound-
aries goes through low-density regions of the distri-
bution. This motivates the idea of using minimum
conditional entropy criterion for adapting to a new
domain. At the same time, two domains are often
close enough that one would expect that the optimal
parameters for the new domain should not deviate
too much from initial parameters. In order to formu-
late the technique mentioned in the above paragraph,
let us define ?init to be the initial model parame-
ters estimated on out-of-domain data (using labeled
data). Assuming the availability of enough amount
of unlabeled data for in-domain task, we try to min-
imize the following objective function w.r.t the pa-
rameters,
?new = argmin
?
H?(Y|X) + ? ||? ? ?init||p
(3)
where ||? ? ?init||p is an Lp regularizer and tries to
prevent parameters from deviating too much from
their initial values3.
Once again the idea here is to adjust the param-
eters (using unlabeled data) such that low-density
separation between the classes is achieved. In the
following section we will discuss the drawback of
this objective function for adaptation in realistic sce-
narios.
3 Issues with Minimum Entropy Criterion
It is discussed in Section 2.2 that the model param-
eters are adapted such that a minimum conditional
entropy is achieved. It was also discussed how this is
related to finding decision boundaries through low-
density regions of input distribution. However, the
obvious assumption here is that the classes are well
separated and there in fact exists low-density regions
between classes which can be treated as boundaries.
Although this is a suitable/ideal assumption for clas-
sification, in most practical problems this assump-
tion is not satisfied and often classes overlap. There-
fore, we can not expect the conditional entropy to be
3The other reason for using a regularizer is to prevent trivial
solutions of minimum entropy criterion
convex in this situation and to achieve minimization
w.r.t parameters (other than the trivial solutions).
Let us clarify this through an example. Consider
X to be generated by mixture of two 2-D Gaus-
sians (each with a particular mean and covariance
matrix) where each Gaussian corresponds to a par-
ticular class ( binary class situation) . Also in order
to have linear decision boundaries, let the Gaussians
have same covariance matrix and let the parameter
being estimated be the prior for class 1, P (Y = 1).
Fig. 1 shows two different situations with over-
lapping classes and non-overlapping classes. The
left panel shows a distribution in which classes are
well separated whereas the right panel corresponds
to the situation where there is considerable overlap
between classes. Clearly, in the later case there is
no low-density region separating the classes. There-
fore, as we change the parameter (here, the prior on
the class Y = 1), there will not be any well defined
point with minimum entropy. This can be seen from
Fig. 2 where model conditional entropy is plotted
vs. class prior parameter for both cases. In the case
of no-overlap between classes, entropy is a convex
function w.r.t the parameter (excluding trivial solu-
tions which happens at P (Y = 1) = 0, 1) and is
minimum at P (Y = 1) = 0.7 which is the true prior
with which the data was generated.
We summarize issues with minimum entropy cri-
terion and our proposed solutions as follows:
? Trivial solution: this happens when we put de-
cision boundaries such that both classes are
considered as one class (this can be avoided us-
ing the regularizer in Eqn. 3 and the assump-
tion that initial models have a reasonable solu-
tion, e.g. close to the optimal solution for new
domain )
? Overlapped Classes: As it was discussed in
this section, if the overlap is considerable then
the entropy will not be convex w.r.t to model
parameters. We will address this issue in
the next section by introducing the entropy-
stability concept.
4 Entropy-Stability
It was discussed in the previous section that a mini-
mum entropy criterion can not be used (by itself) in
192
?3 ?2 ?1 0 1 2 3 4 5 6 7?4
?2
0
2
4
6
8
10
X1
X 2
?3 ?2 ?1 0 1 2 3 4 5 6 7?3
?2
?1
0
1
2
3
4
5
6
7
X1
X 2
Figure 1: Mixture of two Gaussians and the corresponding Bayes decision boundary: (left) with no class overlap
(right) with class overlap
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0	 ?
0.005	 ?
0.01	 ?
0.015	 ?
0.02	 ?
0.025	 ?
0.03	 ?
0.035	 ?
0	 ? 0.1	 ? 0.2	 ? 0.3	 ? 0.4	 ? 0.5	 ? 0.6	 ? 0.7	 ? 0.8	 ? 0.9	 ? 1	 ?
Con
di?n
al	 ?E
ntro
py	 ?
P(Y=1)	 ?
without	 ?overlap	 ?
with	 ?overlap	 ?
Figure 2: Condtional entropy vs. prior parameter, P (Y =
1)
situations where there is a considerable amount of
overlap among classes. Assuming that class bound-
aries happen in the regions close to the tail of class
distributions, we introduce the concept of Entropy-
Stability and show how it can be used to detect
boundary regions. Define Entropy-Stability to be the
reciprocal of the following
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p
=
?
?
?
?
?
?
?
?
?
?
?
?
?
p(x)
?
(?
y p?(y|x) log p?(y|x)
)
?? dx
?
?
?
?
?
?
?
?
?
?
?
?
p
(4)
Recall: since ? is a vector of parameters, ?H?(Y|X)??
will be a vector and by using Lp norm Entropy-
stability will be a scalar.
The introduced concept basically measures the
stability of label entropies w.r.t the model parame-
ters. The idea is that we prefer models which not
only have low-conditional entropy but also have sta-
ble decision rules imposed by the model. Next, we
show through the following theorem how Entropy-
Stability measures the stability over posterior prob-
abilities (decision rules) of the model.
Theorem 2
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p
=
?
?
?
?
?
?
?
?
?
?
?
p(x)
(
?
y
?p?(y|x)
?? log p?(y|x)
)
dx
?
?
?
?
?
?
?
?
?
?
p
where the term inside the parenthesis is the weighted
sum (by log-likelihood) over the gradient of poste-
rior probabilities of labels for a given sample x
Proof The proof is straight forward and uses the fact
that
? ?p?(y|x)
?? = ?(
P
p?(y|x))
?? = 0 .
Using Theorem 2 and Eqn. 4, it should be clear
how Entropy-Stability measures the expected sta-
bility over the posterior probabilities of the model.
A high value of
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
p
implies models with
less stable decision rules. In order to explain how
this is used for detecting boundaries (overlapped
193
regions) we once again refer back to our mixture
of Gaussians? example. As the decision boundary
moves from class specific regions to overlapped re-
gions (by changing the parameter which is here class
prior probability) we expect the entropy to continu-
ously decrease (due to the assumption that the over-
laps occur at the tail of class distributions). How-
ever, as we get close to the overlapping regions the
added data points from other class(es) will resist
changes in the entropy. resulting in stability over the
entropy until we enter the regions specific to other
class(es).
In the following subsection we use this idea to
propose a new objective function which can be used
as an unsupervised adaptation method even for the
case of input distribution with overlapping classes.
4.1 Better Objective Function
The idea here is to use the Entropy-Stability con-
cept to accept only regions which are close to the
overlapped parts of the distribution (based on our
assumption, these are valid regions for decision
boundaries) and then using the minimum entropy
criterion we find optimum solutions for our parame-
ters inside these regions. Therefore, we modify Eqn.
3 such that it also includes the Entropy-Stability
term
?new = argmin
?
(
H?(Y|X) + ?
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p?
+ ? ||? ? ?init||p
)
(5)
The parameter ? and ? can be tuned using small
amount of labeled data (Dev set).
5 Speech Recognition Task
In this section we will discuss how the proposed
framework can be used in a speech recognition task.
In the speech recognition task, Y is the sequence
of words and X is the input speech signal. For a
given speech signal, almost every word sequence is
a possible output and therefore there is a need for
a compact representation of output labels (words).
For this, word graphs (Lattices) are generated dur-
ing the recognition process. In fact, each lattice is
an acyclic directed graph whose nodes correspond
to particular instants of time, and arcs (edges con-
necting nodes) represent possible word hypotheses.
Associated with each arc is an acoustic likelihood
and language model likelihood scores. Fig. 3 shows
an example of recognition lattice 4 (for the purpose
of demonstration likelihood scores are not shown).L. Mangu et al: Finding Consensus in Speech Recognition 6
(a) Input lattice (?SIL? marks pauses)
SIL
SIL
SIL
SIL
SIL
SIL
VEAL
VERY
HAVE
MOVE
HAVE
HAVE
IT
MOVE
HAVE IT
VERY
VERY
VERY
VERY
OFTEN
OFTEN
FINE
FINE
FAST
I
I
I
(b) Multiple alignment (?-? marks deletions)
- -
I
MOVE
HAVE IT VEAL 
VERY
FINE
OFTEN
FAST
Figure 1: Sample recognition lattice and corresponding multiple alignment represented as
confusion network.
alignment (which gives rise to the standard string edit distance WE (W,R)) with
a modified, multiple string alignment. The new approach incorporates all lattice
hypotheses7 into a single alignment, and word error between any two hypotheses
is then computed according to that one alignment. The multiple alignment thus
defines a new string edit distance, which we will call MWE (W,R). While the
new alignment may in some cases overestimate the word error between two
hypotheses, as we will show in Section 5 it gives very similar results in practice.
The main benefit of the multiple alignment is that it allows us to extract
the hypothesis with the smallest expected (modified) word error very efficiently.
To see this, consider an example. Figure 1 shows a word lattice and the corre-
sponding hypothesis alignment. Each word hypothesis is mapped to a position
in the alignment (with deletions marked by ?-?). The alignment also supports
the computation of word posterior probabilities. The posterior probability of a
word hypothesis is the sum of the posterior probabilities of all lattice paths of
which the word is a part. Given an alignment and posterior probabilities, it is
easy to see that the hypothesis with the lowest expected word error is obtained
by picking the word with the highest posterior at each position in the alignment.
We call this the consensus hypothesis.
7In practice we apply some pruning of the lattice to remove low probability word hypotheses
(see Section 3.4).
Figure 3: Lattice Example
Since lattices contain all the likely hypotheses
(unlikely hypotheses are pruned during recognition
and will not be included in the lattice), conditional
entropy for any given input speech signal, x, can be
approximated by the conditional entropy of the lat-
tice. That is,
H?(Y|X = xi) = H?(Y|Li)
whereLi is the corresponding decoded lattice (given
speech recognizer parameters) of utterance xi.
For the calculation of entropy we need to
know the distribution of X because H?(Y|X) =
EX [H?(Y|X = x)] and since this distribution is not
known to us, we use Law of Large Numbers to ap-
proximate it by the empirical average
H?(Y|X) ? ? 1N
N?
i=1
?
y?Li
p?(y|Li) log p?(y|Li) (6)
Here N indicates the number of unlabeled utter-
ances for which we calculate the empirical value of
conditional entropy. Similarly, expectation w.r.t in-
put distribution in entropy-stability term is also ap-
proximated by the empirical average of samples.
Since the number of paths (hypotheses) in the lat-
tice is very large, it would be computationally infea-
sibl to c ute the conditi nal entropy y enumer-
ating all possible paths in the lattice and calculating
4The figure is adopted from (Mangu et al, 1999)
194
Element ?p, r?
?p1, r1?? ?p2, r2? ?p1p2, p1r2 + p2r1?
?p1, r1?? ?p2, r2? ?p1 + p2, r1 + r2?
0 ?0, 0?
1 ?1, 0?
Table 1: First-Order (Expectation) semiring: Defining
multiplication and sum operations for first-order semir-
ings.
their corresponding posterior probabilities. Instead
we use Finite-State Transducers (FST) to represent
the hypothesis space (lattice). To calculate entropy
and the gradient of entropy, the weights for the FST
are defined to be First- and Second-Order semirings
(Li and Eisner, 2009). The idea is to use semirings
and their corresponding operations along with the
forward-backward algorithm to calculate first- and
second-order statistics to compute entropy and the
gradient of entropy respectively. Assume we are in-
terested in calculating the entropy of the lattice,
H(p) = ??
d?Li
p(d)
Z log(
p(d)
Z )
= logZ ? 1Z
?
d?Li
p(d) log p(d)
= logZ ? r?Z (7)
where Z is the total probability of all the paths in
the lattice (normalization factor). In order to do so,
we need to compute ?Z, r?? on the lattice. It can
be proved that if we define the first-order semir-
ing ?pe, pe log pe? (pe is the non-normalized score of
each arc in the lattice) as our FST weights and define
semiring operations as in Table. 1, then applying the
forward algorithm will result in the calculation of
?Z, r?? as the weight (semiring weight) for the final
node.
The details for using Second-Order semirings for
calculating the gradient of entropy can be found
in (Li and Eisner, 2009). The same paper de-
scribes how to use the forward-backward algorithm
to speed-up the this procedure.
6 Language Model Adaptation
Language Model Adaptation is crucial when the
training data does not match the test data being de-
coded. This is a frequent scenario for all Automatic
Speech Recognition (ASR) systems. The applica-
tion domain very often contains named entities and
N-gram sequences that are unique to the domain of
interest. For example, conversational speech has
a very different structure than class-room lectures.
Linear Interpolation based methods are most com-
monly used to adapt LMs to a new domain. As
explained in (Bacchiani et al, 2003), linear inter-
polation is a special case of Maximum A Posterior
(MAP) estimation, where an N-gram LM is built on
the adaptation data from the new domain and the two
LMs are combined using:
p(wi|h) = ?pB(wi|h) + (1? ?)pA(wi|h)
0 ? ? ? 1
where pB refers to out-of-domain (background)
models and pA is the adaptation (in-domain) mod-
els. Here ? is the interpolation weight.
Conventionally, ? is calculated by optimizing per-
plexity (PPL) or Word Error Rate (WER) on some
held-out data from target domain. Instead using
our proposed framework, we estimate ? on enough
amount of unlabeled data from target domain. The
idea is that resources on the new domain have al-
ready been used to build domain specific models
and it does not make sense to again use in-domain
resources for estimating the interpolation weight.
Since we are trying to just estimate one parameter
and the performance of the interpolated model is
bound by in-domain/out-of-domain models, there is
no need to include a regularization term in Eqn. 5.
Also
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
p
= |?H?(Y|X)?? | because we only
have one parameter. Therefore, interpolation weight
will be chosen by the following criterion
?? = argmin
0???1
H?(Y|X) + ?|?H?(Y|X)?? | (8)
For the purpose of estimating one parameter ?, we
use ? = 1 in the above equation
7 Experimental Setup
The large vocabulary continuous speech recognition
(LVCSR) system used throughout this paper is based
on the 2007 IBM Speech transcription system for
GALE Distillation Go/No-go Evaluation (Chen et
al., 2006). The acoustic models used in this system
195
are state-of-the-art discriminatively trained models
and are the same ones used for all experiments pre-
sented in this paper.
For LM adaptation experiments, the out-of-
domain LM (pB , Broadcast News LM) training
text consists of 335M words from the follow-
ing broadcast news (BN) data sources (Chen et
al., 2006): 1996 CSR Hub4 Language Model
data, EARS BN03 closed captions, GALE Phase
2 Distillation GNG Evaluation Supplemental Mul-
tilingual data, Hub4 acoustic model training tran-
scripts, TDT4 closed captions, TDT4 newswire, and
GALE Broadcast Conversations and GALE Broad-
cast News. This language model is of order 4-gram
with Kneser-Ney smoothing and contains 4.6M n-
grams based on a lexicon size of 84K.
The second source of data is the MIT lectures data
set (J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D.
Huynh, and R. Barzilay, 2007) . This serves as the
target domain (in-domain) set for language model
adaptation experiments. This set is split into 8 hours
for in-domain LM building, another 8 hours served
as unlabeled data for interpolation weight estimation
using criterion in Eqn. 8 (we refer to this as unsuper-
vised training data) and finally 2.5 hours Dev set for
estimating the interpolation weight w.r.t WER (su-
pervised tuning) . The lattice entropy and gradient
of entropy w.r.t ? are calculated on the unsupervised
training data set. The results are discussed in the
next section.
8 Results
In order to optimize the interpolation weight ? based
on criterion in Eqn. 8, we devide [0, 1] to 20 differ-
ent points and evaluate the objective function (Eqn.
8) on those points. For this, we need to calculate
entropy and gradient of the entropy on the decoded
lattices of the ASR system on 8 hours of MIT lecture
set which is used as an unlabeled training data. Fig.
4 shows the value of the objective function against
different values of model parameters (interpolation
weight ?). As it can be seen from this figure just
considering the conditional entropy will result in a
non-convex objective function whereas adding the
entropy-stability term will make the objective func-
tion convex. For the purpose of the evaluation, we
show the results for estimating ? directly on the tran-
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Model Entropy
Model Entropy+Entropy-Stability
BN-LM MIT-LM?
Figure 4: Objective function with and without including
Entropy-Stability term vs. interpolation weight ? on 8
hours MIT lecture unlabeled data
scription of the 8 hour MIT lecture data and compare
it to estimated value using our framework. The re-
sults are shown in Fig. 5. Using ? = 0 and ? = 1
the WERs are 24.7% and 21.1% respectively. Us-
ing the new proposed objective function, the optimal
? is estimated to be 0.6 with WER of 20.1% (Red
circle on the figure). Estimating ? w.r.t 8 hour train-
ing data transcription (supervised adaptation) will
result in ? = 0.7 (green circle) andWER of 20.0%.
Instead ? = 0.8 will be chosen by tuning the inter-
polation weight on 2.5 hour Dev set with compara-
ble WER of 20.1%. Also it is clear from the figure
that the new objective function can be used to pre-
dict the WER trend w.r.t the interpolation weight
parameter.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Model Entropy + Entropy Stability
WER
24.7%
20.0%
21.1%
supervised tuning
?
Figure 5: Estimating ? based on WER vs. the
information-theoretic criterion
Therefore, it can be seen that the new unsuper-
196
vised method results in the same performance as su-
pervised adaptation in speech recognition task.
9 Conclusion and Future Work
In this paper we introduced the notion of entropy
stability and presented a new criterion for unsu-
pervised adaptation which combines conditional en-
tropy minimization with entropy stability. The en-
tropy stability criterion helps in selecting parameter
settings which correspond to stable decision bound-
aries. Entropy minimization on the other hand tends
to push decision boundaries into sparse regions of
the input distributions. We show that combining
the two criterion helps to improve unsupervised pa-
rameter adaptation in real world scenario where
class conditional distributions show significant over-
lap. Although conditional entropy has been previ-
ously proposed as a regularizer, to our knowledge,
the gradient of entropy (entropy-stability) has not
been used previously in the literature. We presented
experimental results where the proposed criterion
clearly outperforms entropy minimization. For the
speech recognition task presented in this paper, the
proposed unsupervised scheme results in the same
performance as the supervised technique.
As a future work, we plan to use the proposed
criterion for adapting log-linear models used in
Machine Translation, Conditional Random Fields
(CRF) and other applications. We also plan to ex-
pand linear interpolation Language Model scheme
to include history specific (context dependent)
weights.
Acknowledgments
The Authors want to thank Markus Dreyer and
Zhifei Li for their insightful discussions and sugges-
tions.
References
M. Bacchiani, B. Roark, and M. Saraclar. 2003. Un-
supervised language model adaptation. In Proc.
ICASSP, pages 224?227.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
Thomas M. Cover and Joy A. Thomas. 2006. Elements
of information theory. Wiley-Interscience, 3rd edition.
Yves Grandvalet and Yoshua Bengio. 2004. Semi-
supervised learning by entropy minimization. In
Advances in neural information processing systems
(NIPS), volume 17, pages 529?536.
J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D. Huynh,
and R. Barzilay. 2007. Recent progress in MIT spo-
ken lecture processing project. In Proc. Interspeech.
Jing Jiang. 2008. A literature survey on domain adapta-
tion of statistical classifiers, March.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP.
Haifeng Li, Keshu Zhang, and Tao Jiang. 2004. Min-
imum entropy clustering and applications to gene ex-
pression analysis. In Proceedings of IEEE Computa-
tional Systems Bioinformatics Conference, pages 142?
151.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 1999.
Finding consensus among words: Lattice-based word
error minimization. In Sixth European Conference on
Speech Communication and Technology.
M. Szummer and T. Jaakkola. 2003. Information regu-
larization with partially labeled data. In Advances in
Neural Information Processing Systems, pages 1049?
1056.
197
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 712?721,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Sub-Word Units for Open Vocabulary Speech Recognition
Carolina Parada1, Mark Dredze1, Abhinav Sethy2, and Ariya Rastrow1
1Human Language Technology Center of Excellence, Johns Hopkins University
3400 N Charles Street, Baltimore, MD, USA
carolinap@jhu.edu, mdredze@cs.jhu.edu, ariya@jhu.edu
2IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
asethy@us.ibm.com
Abstract
Large vocabulary speech recognition systems
fail to recognize words beyond their vocab-
ulary, many of which are information rich
terms, like named entities or foreign words.
Hybrid word/sub-word systems solve this
problem by adding sub-word units to large vo-
cabulary word based systems; new words can
then be represented by combinations of sub-
word units. Previous work heuristically cre-
ated the sub-word lexicon from phonetic rep-
resentations of text using simple statistics to
select common phone sequences. We pro-
pose a probabilistic model to learn the sub-
word lexicon optimized for a given task. We
consider the task of out of vocabulary (OOV)
word detection, which relies on output from
a hybrid model. A hybrid model with our
learned sub-word lexicon reduces error by
6.3% and 7.6% (absolute) at a 5% false alarm
rate on an English Broadcast News and MIT
Lectures task respectively.
1 Introduction
Most automatic speech recognition systems operate
with a large but limited vocabulary, finding the most
likely words in the vocabulary for the given acoustic
signal. While large vocabulary continuous speech
recognition (LVCSR) systems produce high quality
transcripts, they fail to recognize out of vocabulary
(OOV) words. Unfortunately, OOVs are often infor-
mation rich nouns, such as named entities and for-
eign words, and mis-recognizing them can have a
disproportionate impact on transcript coherence.
Hybrid word/sub-word recognizers can produce a
sequence of sub-word units in place of OOV words.
Ideally, the recognizer outputs a complete word for
in-vocabulary (IV) utterances, and sub-word units
for OOVs. Consider the word ?Slobodan?, the given
name of the former president of Serbia. As an un-
common English word, it is unlikely to be in the vo-
cabulary of an English recognizer. While a LVCSR
system would output the closest known words (e.x.
?slow it dawn?), a hybrid system could output a
sequence of multi-phoneme units: s l ow, b ax,
d ae n. The latter is more useful for automatically
recovering the word?s orthographic form, identify-
ing that an OOV was spoken, or improving perfor-
mance of a spoken term detection system with OOV
queries. In fact, hybrid systems have improved OOV
spoken term detection (Mamou et al, 2007; Parada
et al, 2009), achieved better phone error rates, espe-
cially in OOV regions (Rastrow et al, 2009b), and
obtained state-of-the-art performance for OOV de-
tection (Parada et al, 2010).
Hybrid recognizers vary in a number of ways:
sub-word unit type: variable-length phoneme
units (Rastrow et al, 2009a; Bazzi and Glass, 2001)
or joint letter sound sub-words (Bisani and Ney,
2005); unit creation: data-driven or linguistically
motivated (Choueiter, 2009); and how they are in-
corporated in LVCSR systems: hierarchical (Bazzi,
2002) or flat models (Bisani and Ney, 2005).
In this work, we consider how to optimally cre-
ate sub-word units for a hybrid system. These units
are variable-length phoneme sequences, although in
principle our work can be use for other unit types.
Previous methods for creating the sub-word lexi-
712
con have relied on simple statistics computed from
the phonetic representation of text (Rastrow et al,
2009a). These units typically represent the most fre-
quent phoneme sequences in English words. How-
ever, it isn?t clear why these units would produce the
best hybrid output. Instead, we introduce a prob-
abilistic model for learning the optimal units for a
given task. Our model learns a segmentation of a
text corpus given some side information: a mapping
between the vocabulary and a label set; learned units
are predictive of class labels.
In this paper, we learn sub-word units optimized
for OOV detection. OOV detection aims to identify
regions in the LVCSR output where OOVs were ut-
tered. Towards this goal, we are interested in select-
ing units such that the recognizer outputs them only
for OOV regions while prefering to output a com-
plete word for in-vocabulary regions. Our approach
yields improvements over state-of-the-art results.
We begin by presenting our log-linear model for
learning sub-word units with a simple but effective
inference procedure. After reviewing existing OOV
detection approaches, we detail how the learned
units are integrated into a hybrid speech recognition
system. We show improvements in OOV detection,
and evaluate impact on phone error rates.
2 Learning Sub-Word Units
Given raw text, our objective is to produce a lexicon
of sub-word units that can be used by a hybrid sys-
tem for open vocabulary speech recognition. Rather
than relying on the text alone, we also utilize side
information: a mapping of words to classes so we
can optimize learning for a specific task.
The provided mapping assigns labels Y to the cor-
pus. We maximize the probability of the observed
labeling sequence Y given the text W : P (Y |W ).
We assume there is a latent segmentation S of this
corpus which impacts Y . The complete data likeli-
hood becomes: P (Y |W ) =
?
S P (Y, S|W ) during
training. Since we are maximizing the observed Y ,
segmentation S must discriminate between different
possible labels.
We learn variable-length multi-phone units by
segmenting the phonetic representation of each word
in the corpus. Resulting segments form the sub-
word lexicon.1 Learning input includes a list of
words to segment taken from raw text, a mapping
between words and classes (side information indi-
cating whether token is IV or OOV), a pronuncia-
tion dictionaryD, and a letter to sound model (L2S),
such as the one described in Chen (2003). The cor-
pus W is the list of types (unique words) in the raw
text input. This forces each word to have a unique
segmentation, shared by all common tokens. Words
are converted into phonetic representations accord-
ing to their most likely dictionary pronunciation;
non-dictionary words use the L2S model.2
2.1 Model
Inspired by the morphological segmentation model
of Poon et al (2009), we assume P (Y, S|W ) is a
log-linear model parameterized by ?:
P?(Y, S|W ) =
1
Z(W )
u?(Y, S,W ) (1)
where u?(Y, S,W ) defines the score of the pro-
posed segmentation S for words W and labels Y
according to model parameters ?. Sub-word units
? compose S, where each ? is a phone sequence, in-
cluding the full pronunciation for vocabulary words;
the collection of ?s form the lexicon. Each unit
? is present in a segmentation with some context
c = (?l, ?r) of the form ?l??r. Features based on
the context and the unit itself parameterize u?.
In addition to scoring a segmentation based on
features, we include two priors inspired by the Min-
imum Description Length (MDL) principle sug-
gested by Poon et al (2009). The lexicon prior
favors smaller lexicons by placing an exponential
prior with negative weight on the length of the lex-
icon
?
? |?|, where |?| is the length of the unit ?
in number of phones. Minimizing the lexicon prior
favors a trivial lexicon of only the phones. The
corpus prior counters this effect, an exponential
prior with negative weight on the number of units
in each word?s segmentation, where |si| is the seg-
mentation length and |wi| is the length of the word
in phones. Learning strikes a balance between the
two priors. Using these definitions, the segmenta-
tion score u?(Y, S,W ) is given as:
1Since sub-word units can expand full-words, we refer to
both words and sub-words simply as units.
2The model can also take multiple pronunciations (?3.1).
713
s l ow b ax d ae n
s l ow
(#,#, , b, ax)
b ax
(l,ow, , d, ae)
d ae n
(b,ax, , #, #)
Figure 1: Units and bigram phone context (in parenthesis)
for an example segmentation of the word ?slobodan?.
u?(Y, S,W ) = exp
(
?
?,y
??,yf?,y(S, Y )
+
?
c,y
?c,yfc,y(S, Y )
+ ? ?
?
??S
|?|
+ ? ?
?
i?W
|si|/|wi|
)
(2)
f?,y(S, Y ) are the co-occurrence counts of the pair
(?, y) where ? is a unit under segmentation S and y
is the label. fc,y(S, Y ) are the co-occurrence counts
for the context c and label y under S. The model
parameters are ? = {??,y, ?c,y : ??, c, y}. The neg-
ative weights for the lexicon (?) and corpus priors
(?) are tuned on development data. The normalizer
Z sums over all possible segmentations and labels:
Z(W ) =
?
S?
?
Y ?
u?(Y
?, S?,W ) (3)
Consider the example segmentation for the word
?slobodan? with pronunciation s,l,ow,b,ax,d,ae,n
(Figure 1). The bigram phone context as a four-tuple
appears below each unit; the first two entries corre-
spond to the left context, and last two the right con-
text. The example corpus (Figure 2) demonstrates
how unit features f?,y and context features fc,y are
computed.
3 Model Training
Learning maximizes the log likelihood of the ob-
served labels Y ? given the words W :
`(Y ?|W ) = log
?
S
1
Z(W )
u?(Y
?, S,W ) (4)
We use the Expectation-Maximization algorithm,
where the expectation step predicts segmentations S
Labeled corpus: president/y = 0 milosevic/y = 1
Segmented corpus: p r eh z ih d ih n t/0 m ih/1 l aa/1
s ax/1 v ih ch/1
Unit-feature:Value p r eh z ih d ih n t/0:1 m ih/1:1
l aa/1:1 s ax/1:1 v ih ch/1:1
Context-feature:Value
(#/0,#/0, ,l/1,aa/1):1,
(m/1,ih/1, ,s/1,ax/1):1,
(l/1,aa/1, ,v/1,ih/1):1,
(s/1,ax/1, ,#/0,#/0):1,
(#/0,#/0, ,#/0,#/0):1
Figure 2: A small example corpus with segmentations
and corresponding features. The notation m ih/1:1
represents unit/label:feature-value. Overlapping context
features capture rich segmentation regularities associated
with each class.
given the model?s current parameters ? (?3.1), and
the maximization step updates these parameters us-
ing gradient ascent. The partial derivatives of the
objective (4) with respect to each parameter ?i are:
?`(Y ?|W )
??i
= ES|Y ?,W [fi]? ES,Y |W [fi] (5)
The gradient takes the usual form, where we en-
courage the expected segmentation from the current
model given the correct labels to equal the expected
segmentation and expected labels. The next section
discusses computing these expectations.
3.1 Inference
Inference is challenging since the lexicon prior ren-
ders all word segmentations interdependent. Con-
sider a simple two word corpus: cesar (s,iy,z,er),
and cesium (s,iy,z,iy,ax,m). Numerous segmen-
tations are possible; each word has 2N?1 possible
segmentations, where N is the number of phones in
its pronunciation (i.e., 23 ? 25 = 256). However,
if we decide to segment the first word as: {s iy,
z er}, then the segmentation for ?cesium?:{s iy,
z iy ax m} will incur a lexicon prior penalty for
including the new segment z iy ax m. If instead
we segment ?cesar? as {s iy z, er}, the segmen-
tation {s iy, z iy ax m} incurs double penalty
for the lexicon prior (since we are including two new
units in the lexicon: s iy and z iy ax m). This
dependency requires joint segmentation of the entire
corpus, which is intractable. Hence, we resort to ap-
proximations of the expectations in Eq. (5).
One approach is to use Gibbs Sampling: it-
erating through each word, sampling a new seg-
714
mentation conditioned on the segmentation of all
other words. The sampling distribution requires
enumerating all possible segmentations for each
word (2N?1) and computing the conditional prob-
abilities for each segmentation: P (S|Y ?,W ) =
P (Y ?, S|W )/P (Y ?|W ) (the features are extracted
from the remaining words in the corpus). Using M
sampled segmentations S1, S2, . . . Sm we compute
ES|Y ?,W [fi] as follows:
ES|Y ?,W [fi] ?
1
M
?
j
fi[Sj ]
Similarly, to compute ES,Y |W we sample a seg-
mentation and a label for each word. We com-
pute the joint probability of P (Y, S|W ) for each
segmentation-label pair using Eq. (1). A sampled
segmentation can introduce new units, which may
have higher probability than existing ones.
Using these approximations in Eq. (5), we update
the parameters using gradient ascent:
??new = ??old + ??`??(Y
?|W )
where ? > 0 is the learning rate.
To obtain the best segmentation, we use determin-
istic annealing. Sampling operates as usual, except
that the parameters are divided by a value, which
starts large and gradually drops to zero. To make
burn in faster for sampling, the sampler is initialized
with the most likely segmentation from the previous
iteration. To initialize the sampler the first time, we
set al the parameters to zero (only the priors have
non-zero values) and run deterministic annealing to
obtain the first segmentation of the corpus.
3.2 Efficient Sampling
Sampling a segmentation for the corpus requires
computing the normalization constant (3), which
contains a summation over all possible corpus seg-
mentations. Instead, we approximate this constant
by sampling words independently, keeping fixed all
other segmentations. Still, even sampling a single
word?s segmentation requires enumerating probabil-
ities for all possible segmentations.
We sample a segmentation efficiently using dy-
namic programming. We can represent all possible
segmentations for a word as a finite state machine
(FSM) (Figure 3), where arcs weights arise from
scoring the segmentation?s features. This weight is
the negative log probability of the resulting model
after adding the corresponding features and priors.
However, the lexicon prior poses a problem for
this construction since the penalty incurred by a new
unit in the segmentation depends on whether that
unit is present elsewhere in that segmentation. For
example, consider the segmentation for the word
ANJANI: AA N, JH, AA N, IY. If none of these units
are in the lexicon, this segmentation yields the low-
est prior penalty since it repeats the unit AA N. 3 This
global dependency means paths must encode the full
unit history, making computing forward-backward
probabilities inefficient.
Our solution is to use the Metropolis-Hastings al-
gorithm, which samples from the true distribution
P (Y, S|W ) by first sampling a new label and seg-
mentation (y?, s?) from a simpler proposal distribu-
tion Q(Y, S|W ). The new assignment (y?, s?) is ac-
cepted with probability:
?(Y ?, S?|Y, S,W )=min
?
1,
P (Y ?, S?|W )Q(Y, S|Y ?, S?,W )
P (Y, S|W )Q(Y ?, S?|Y, S,W )
?
We choose the proposal distribution Q(Y, S|W )
as Eq. (1) omitting the lexicon prior, removing the
challenge for efficient computation. The probability
of accepting a sample becomes:
?(Y ?, S?|Y, S,W )=min
?
1,
P
??S? |?|P
??S |?|
?
(6)
We sample a path from the FSM by running the
forward-backward algorithm, where the backward
computations are carried out explicitly, and the for-
ward pass is done through sampling, i.e. we traverse
the machine only computing forward probabilities
for arcs leaving the sampled state.4 Once we sample
a segmentation (and label) we accept it according to
Eq. (6) or keep the previous segmentation if rejected.
Alg. 1 shows our full sub-word learning proce-
dure, where sampleSL (Alg. 2) samples a segmen-
tation and label sequence for the entire corpus from
P (Y, S|W ), and sampleS samples a segmentation
from P (S|Y ?,W ).
3Splitting at phone boundaries yields the same lexicon prior
but a higher corpus prior.
4We use OpenFst?s RandGen operation with a costumed arc-
selector (http://www.openfst.org/).
715
0 1AA
5
AA_N_JH_AA_N
4
AA_N_JH_AA
3
AA_N_JH 2
AA_N
N_JH_AA_N
N_JH_AA
N_JH
N
6
N_JH_AA_N_IY
IY
N
AA_NAA
AA_N_IY
JH_AA_N
JH_AA
JH
JH_AA_N_IY
Figure 3: FSM representing all segmentations for the word ANJANI with pronunciation: AA,N,JH,AA,N,IY
Algorithm 1 Training
Input: Lexicon L from training text W , Dictionary D,
Mapping M , L2S pronunciations, Annealing temp T .
Initialization:
Assign label y?m = M [wm]. ??0 = 0?
S0 = random segmentation for each word in L.
for i = 1 to K do
/* E-Step */
Si = bestSegmentation(T, ?i?1, Si?1).
for k = 1 to NumSamples do
(S?k, Y
?
k) = sampleSL(P (Y, Si|W ),Q(Y, Si|W ))
S?k = sampleS(P (Si|Y ?,W ),Q(Si|Y ?,W ))
end for
/* M-Step */
ES,Y |W [fi] =
1
NumSamples
?
k f?,l[S
?
k, Y
?
k]
ES|Y ?,W [f?,l] =
1
NumSamples
?
k f?,l[S?k, Y
?]
??i = ??i?1 + ??L??(Y
?|W )
end for
S = bestSegmentation(T, ?K , S0)
Output: Lexicon Lo from S
4 OOV Detection Using Hybrid Models
To evaluate our model for learning sub-word units,
we consider the task of out-of-vocabulary (OOV)
word detection. OOV detection for ASR output can
be categorized into two broad groups: 1) hybrid
(filler) models: which explicitly model OOVs us-
ing either filler, sub-words, or generic word mod-
els (Bazzi, 2002; Schaaf, 2001; Bisani and Ney,
2005; Klakow et al, 1999; Wang, 2009); and
2) confidence-based approaches: which label un-
reliable regions as OOVs based on different con-
fidence scores, such as acoustic scores, language
models, and lattice scores (Lin et al, 2007; Burget
et al, 2008; Sun et al, 2001; Wessel et al, 2001).
In the next section we detail the OOV detection
approach we employ, which combines hybrid and
Algorithm 2 sampleSL(P (S, Y |W ), Q(S, Y |W ))
for m = 1 to M (NumWords) do
(s?m, y
?
m) = Sample segmentation/label pair for
word wm according to Q(S, Y |W )
Y ? = {y1 . . . ym?1y?mym+1 . . . yM}
S? = {s1 . . . sm?1s?msm+1 . . . sM}
?=min
(
1,
P
??S? |?|P
??S |?|
)
with prob ? : ym,k = y?m, sm,k = s
?
m
with prob (1? ?) : ym,k = ym, sm,k = sm
end for
return (S?k, Y
?
k) = [(s1,k, y1,k) . . . (sM,k, yM,k)]
confidence-based models, achieving state-of-the art
performance for this task.
4.1 OOV Detection Approach
We use the state-of-the-art OOV detection model of
Parada et al (2010), a second order CRF with fea-
tures based on the output of a hybrid recognizer.
This detector processes hybrid recognizer output, so
we can evaluate different sub-word unit lexicons for
the hybrid recognizer and measure the change in
OOV detection accuracy.
Our model (?2.1) can be applied to this task by
using a dictionary D to label words as IV (yi = 0 if
wi ? D) and OOV (yi = 1 if wi /? D). This results
in a labeled corpus, where the labeling sequence Y
indicates the presence of out-of-vocabulary words
(OOVs). For comparison we evaluate a baseline
method (Rastrow et al, 2009b) for selecting units.
Given a sub-word lexicon, the word and sub-
words are combined to form a hybrid language
model (LM) to be used by the LVCSR system. This
hybrid LM captures dependencies between word and
sub-words. In the LM training data, all OOVs are
represented by the smallest number of sub-words
which corresponds to their pronunciation. Pronun-
ciations for all OOVs are obtained using grapheme
716
to phone models (Chen, 2003).
Since sub-words represent OOVs while building
the hybrid LM, the existence of sub-words in ASR
output indicate an OOV region. A simple solution to
the OOV detection problem would then be reduced
to a search for the sub-words in the output of the
ASR system. The search can be on the one-best
transcripts, lattices or confusion networks. While
lattices contain more information, they are harder
to process; confusion networks offer a trade-off be-
tween richness (posterior probabilities are already
computed) and compactness (Mangu et al, 1999).
Two effective indications of OOVs are the exis-
tence of sub-words (Eq. 7) and high entropy in a
network region (Eq. 8), both of which are used as
features in the model of Parada et al (2010).
Sub-word Posterior =
?
??tj
p(?|tj) (7)
Word-Entropy =?
?
w?tj
p(w|tj) log p(w|tj) (8)
tj is the current bin in the confusion network and
? is a sub-word in the hybrid dictionary. Improving
the sub-word unit lexicon, improves the quality of
the confusion networks for OOV detection.
5 Experimental Setup
We used the data set constructed by Can et al
(2009) (OOVCORP) for the evaluation of Spoken
Term Detection of OOVs since it focuses on the
OOV problem. The corpus contains 100 hours of
transcribed Broadcast News English speech. There
are 1290 unique OOVs in the corpus, which were
selected with a minimum of 5 acoustic instances per
word and short OOVs inappropriate for STD (less
than 4 phones) were explicitly excluded. Example
OOVs include: NATALIE, PUTIN, QAEDA,
HOLLOWAY, COROLLARIES, HYPERLINKED,
etc. This resulted in roughly 24K (2%) OOV tokens.
For LVCSR, we used the IBM Speech Recogni-
tion Toolkit (Soltau et al, 2005)5 to obtain a tran-
script of the audio. Acoustic models were trained
on 300 hours of HUB4 data (Fiscus et al, 1998)
and utterances containing OOV words as marked in
OOVCORP were excluded. The language model was
trained on 400M words from various text sources
5The IBM system used speaker adaptive training based on
maximum likelihood with no discriminative training.
with a 83K word vocabulary. The LVCSR system?s
WER on the standard RT04 BN test set was 19.4%.
Excluded utterances amount to 100hrs. These were
divided into 5 hours of training for the OOV detec-
tor and 95 hours of test. Note that the OOV detector
training set is different from the LVCSR training set.
We also use a hybrid LVCSR system, combin-
ing word and sub-word units obtained from ei-
ther our approach or a state-of-the-art baseline ap-
proach (Rastrow et al, 2009a) (?5.2). Our hybrid
system?s lexicon has 83K words and 5K or 10K
sub-words. Note that the word vocabulary is com-
mon to both systems and only the sub-words are se-
lected using either approach. The word vocabulary
used is close to most modern LVCSR system vo-
cabularies for English Broadcast News; the result-
ing OOVs are more challenging but more realistic
(i.e. mostly named entities and technical terms). The
1290 words are OOVs to both the word and hybrid
systems.
In addition we report OOV detection results on a
MIT lectures data set (Glass et al, 2010) consisting
of 3 Hrs from two speakers with a 1.5% OOV rate.
These were divided into 1 Hr for training the OOV
detector and 2 Hrs for testing. Note that the LVCSR
system is trained on Broadcast News data. This out-
of-domain test-set help us evaluate the cross-domain
performance of the proposed and baseline hybrid
systems. OOVs in this data set correspond mainly to
technical terms in computer science and math. e.g.
ALGORITHM, DEBUG, COMPILER, LISP.
5.1 Learning parameters
For learning the sub-words we randomly selected
from training 5,000 words which belong to the 83K
vocabulary and 5,000 OOVs6. For development we
selected an additional 1,000 IV and 1,000 OOVs.
This was used to tune our model hyper parameters
(set to ? = ?1, ? = ?20). There is no overlap
of OOVs in training, development and test sets. All
feature weights were initialized to zero and had a
Gaussian prior with variance ? = 100. Each of the
words in training and development was converted to
their most-likely pronunciation using the dictionary
6This was used to obtain the 5K hybrid system. To learn sub-
words for the 10K hybrid system we used 10K in-vocabulary
words and 10K OOVs. All words were randomly selected from
the LM training text.
717
for IV words or the L2S model for OOVs.7
The learning rate was ?k =
?
(k+1+A)? , where k is
the iteration,A is the stability constant (set to 0.1K),
? = 0.4, and ? = 0.6. We used K = 40 itera-
tions for learning and 200 samples to compute the
expectations in Eq. 5. The sampler was initialized
by sampling for 500 iterations with deterministic an-
nealing for a temperature varying from 10 to 0 at 0.1
intervals. Final segmentations were obtained using
10, 000 samples and the same temperature schedule.
We limit segmentations to those including units of at
most 5 phones to speed sampling with no significant
degradation in performance. We observed improved
performance by dis-allowing whole word units.
5.2 Baseline Unit Selection
We used Rastrow et al (2009a) as our baseline
unit selection method, a data driven approach where
the language model training text is converted into
phones using the dictionary (or a letter-to-sound
model for OOVs), and a N-gram phone LM is es-
timated on this data and pruned using a relative en-
tropy based method. The hybrid lexicon includes
resulting sub-words ? ranging from unigrams to 5-
gram phones, and the 83K word lexicon.
5.3 Evaluation
We obtain confusion networks from both the word
and hybrid LVCSR systems. We align the LVCSR
transcripts with the reference transcripts and tag
each confusion region as either IV or OOV. The
OOV detector classifies each region in the confusion
network as IV/OOV. We report OOV detection accu-
racy using standard detection error tradeoff (DET)
curves (Martin et al, 1997). DET curves measure
tradeoffs between false alarms (x-axis) and misses
(y-axis), and are useful for determining the optimal
operating point for an application; lower curves are
better. Following Parada et al (2010) we separately
evaluate unobserved OOVs.8
7In this work we ignore pronunciation variability and sim-
ply consider the most likely pronunciation for each word. It
is straightforward to extend to multiple pronunciations by first
sampling a pronunciation for each word and then sampling a
segmentation for that pronunciation.
8Once an OOV word has been observed in the OOV detector
training data, even if it was not in the LVCSR training data, it is
no longer truly OOV.
6 Results
We compare the performance of a hybrid sys-
tem with baseline units9 (?5.2) and one with units
learned by our model on OOV detection and phone
error rate. We present results using a hybrid system
with 5k and 10k sub-words.
We evaluate the CRF OOV detector with two dif-
ferent feature sets. The first uses only Word En-
tropy and Sub-word Posterior (Eqs. 7 and 8) (Fig-
ure 4)10. The second (context) uses the extended
context features of Parada et al (2010) (Figure 5).
Specifically, we include all trigrams obtained from
the best hypothesis of the recognizer (a window of 5
words around current confusion bin). Predictions at
different FA rates are obtained by varying a proba-
bility threshold.
At a 5% FA rate, our system (This Paper 5k) re-
duces the miss OOV rate by 6.3% absolute over the
baseline (Baseline 5k) when evaluating all OOVs.
For unobserved OOVs, it achieves 3.6% absolute
improvement. A larger lexicon (Baseline 10k and
This Paper 10k ) shows similar relative improve-
ments. Note that the features used so far do not nec-
essarily provide an advantage for unobserved ver-
sus observed OOVs, since they ignore the decoded
word/sub-word sequence. In fact, the performance
on un-observed OOVs is better.
OOV detection improvements can be attributed to
increased coverage of OOV regions by the learned
sub-words compared to the baseline. Table 1 shows
the percent of Hits: sub-word units predicted in
OOV regions, and False Alarms: sub-word units
predicted for in-vocabulary words. We can see
that the proposed system increases the Hits by over
8% absolute, while increasing the False Alarms by
0.3%. Interestingly, the average sub-word length
for the proposed units exceeded that of the baseline
units by 0.3 phones (Baseline 5K average length
was 2.92, while that of This Paper 5K was 3.2).
9Our baseline results differ from Parada et al (2010). When
implementing the lexicon baseline, we discovered that their hy-
brid units were mistakenly derived from text containing test
OOVs. Once excluded, the relative improvements of previous
work remain, but the absolute error rates are higher.
10All real-valued features were normalized and quantized us-
ing the uniform-occupancy partitioning described in White et
al. (2007). We used 50 partitions with a minimum of 100 train-
ing values per partition.
718
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(a)
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(b)
Figure 4: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed dis-
criminative hybrid system on OOVCORP data set. Evaluation on un-observed OOVs (a) and all OOVs (b).
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(a)
0 5 10 15 20%FA10
20
30
40
50
60
70
80
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(b)
Figure 5: Effect of adding context features to baseline and discriminative hybrid systems on OOVCORP data set.
Evaluation on un-observed OOVs (a) and all OOVs (b).
Consistent with previously published results, in-
cluding context achieves large improvement in per-
formance. The proposed hybrid system (This Pa-
per 10k + context-features) still improves over the
baseline (Baseline 10k + context-features), however
the relative gain is reduced. In this case, we ob-
tain larger gains for un-observed OOVs which ben-
efit less from the context clues learned in training.
Lastly, we report OOV detection performance on
MIT Lectures. Both the sub-word lexicon and the
LVCSR models were trained on Broadcast News
data, helping us evaluate the robustness of learned
sub-words across domains. Note that the OOVs
in these domains are quite different: MIT Lec-
tures? OOVs correspond to technical computer sci-
Hybrid System Hits FAs
Baseline (5k) 18.25 1.49
This Paper (5k) 26.78 1.78
Baseline (10k) 24.26 1.82
This Paper (10k) 28.96 1.92
Table 1: Coverage of OOV regions by baseline and pro-
posed sub-words in OOVCORP.
ence and math terms, while in Broadcast News they
are mainly named-entities.
Figure 6 and 7 show the OOV detection results in
the MIT Lectures data set. For un-observed OOVs,
the proposed system (This Paper 10k) reduces the
miss OOV rate by 7.6% with respect to the base-
line (Baseline 10k) at a 5% FA rate. Similar to
Broadcast News results, we found that the learned
sub-words provide larger coverage of OOV regions
in MIT Lectures domain. These results suggest that
the proposed sub-words are not simply modeling the
training OOVs (named-entities) better than the base-
line sub-words, but also describe better novel unex-
pected words. Furthermore, including context fea-
tures does not seem as helpful. We conjecture that
this is due to the higher WER11 and the less struc-
tured nature of the domain: i.e. ungrammatical sen-
tences, disfluencies, incomplete sentences, making
it more difficult to predict OOVs based on context.
11WER = 32.7% since the LVCSR system was trained on
Broadcast News data as described in Section 5.
719
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(a)
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(b)
Figure 6: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed dis-
criminative hybrid system on MIT Lectures data set. Evaluation on un-observed OOVs (a) and all OOVs (b).
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(a)
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(b)
Figure 7: Effect of adding context features to baseline and discriminative hybrid systems on MIT Lectures data set.
Evaluation on un-observed OOVs (a) and all OOVs (b).
6.1 Improved Phonetic Transcription
We consider the hybrid lexicon?s impact on Phone
Error Rate (PER) with respect to the reference tran-
scription. The reference phone sequence is obtained
by doing forced alignment of the audio stream to the
reference transcripts using acoustic models. This
provides an alignment of the pronunciation variant
of each word in the reference and the recognizer?s
one-best output. The aligned words are converted to
the phonetic representation using the dictionary.
Table 2 presents PERs for the word and differ-
ent hybrid systems. As previously reported (Ras-
trow et al, 2009b), the hybrid systems achieve bet-
ter PER, specially in OOV regions since they pre-
dict sub-word units for OOVs. Our method achieves
modest improvements in PER compared to the hy-
brid baseline. No statistically significant improve-
ments in PER were observed on MIT Lectures.
7 Conclusions
Our probabilistic model learns sub-word units for
hybrid speech recognizers by segmenting a text cor-
pus while exploiting side information. Applying our
System OOV IV All
Word 1.62 6.42 8.04
Hybrid: Baseline (5k) 1.56 6.44 8.01
Hybrid: Baseline (10k) 1.51 6.41 7.92
Hybrid: This Paper (5k) 1.52 6.42 7.94
Hybrid: This Paper (10k) 1.45 6.39 7.85
Table 2: Phone Error Rate for OOVCORP.
method to the task of OOV detection, we obtain an
absolute error reduction of 6.3% and 7.6% at a 5%
false alarm rate on an English Broadcast News and
MIT Lectures task respectively, when compared to a
baseline system. Furthermore, we have confirmed
previous work that hybrid systems achieve better
phone accuracy, and our model makes modest im-
provements over a baseline with a similarly sized
sub-word lexicon. We plan to further explore our
new lexicon?s performance for other languages and
tasks, such as OOV spoken term detection.
Acknowledgments
We gratefully acknowledge Bhuvaha Ramabhadran
for many insightful discussions and the anonymous
reviewers for their helpful comments. This work
was funded by a Google PhD Fellowship.
720
References
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
eling. In EuroSpeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
M. Bisani and H. Ney. 2005. Open vocabulary speech
recognition with flat hybrid models. In INTER-
SPEECH.
L. Burget, P. Schwarz, P. Matejka, M. Hannemann,
A. Rastrow, C. White, S. Khudanpur, H. Hermansky,
and J. Cernocky. 2008. Combination of strongly and
weakly constrained recognizers for reliable detection
of OOVS. In ICASSP.
D. Can, E. Cooper, A. Sethy, M. Saraclar, and C. White.
2009. Effect of pronounciations on OOV queries in
spoken term detection. Proceedings of ICASSP.
Stanley F. Chen. 2003. Conditional and joint models
for grapheme-to-phoneme conversion. In Eurospeech,
pages 2033?2036.
G. Choueiter. 2009. Linguistically-motivated sub-
word modeling with applications to speech recogni-
tion. Ph.D. thesis, Massachusetts Institute of Technol-
ogy.
Jonathan Fiscus, John Garofolo, Mark Przybocki,
William Fisher, and David Pallett, 1998. 1997 En-
glish Broadcast News Speech (HUB4). Linguistic
Data Consortium, Philadelphia.
James Glass, Timothy Hazen, Lee Hetherington, and
Chao Wang. 2010. Analysis and processing of lec-
ture audio data: Preliminary investigations. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Dietrich Klakow, Georg Rose, and Xavier Aubert. 1999.
OOV-detection in large vocabulary system using au-
tomatically defined word-fragments as fillers. In Eu-
rospeech.
Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff. 2007.
OOV detection by joint word/phone lattice alignment.
In ASRU, pages 478?483, Dec.
Jonathan Mamou, Bhuvana Ramabhadran, and Olivier
Siohan. 2007. Vocabulary independent spoken term
detection. In Proceedings of SIGIR.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding con-
sensus among words. In Eurospeech.
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocky. 1997. The det curve in assessment of
detection task performance. In Eurospeech.
Carolina Parada, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009. Query-by-example spoken term detec-
tion for oov terms. In ASRU.
Carolina Parada, Mark Dredze, Denis Filimonov, and
Fred Jelinek. 2010. Contextual information improves
oov detection in speech. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsu-
pervised morphological segmentation with log-linear
models. In ACL.
Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009a. A new method for OOV detection
using hybrid word/fragment system. Proceedings of
ICASSP.
Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran,
and Fred Jelinek. 2009b. Towards using hybrid,
word, and fragment units for vocabulary independent
LVCSR systems. INTERSPEECH.
T. Schaaf. 2001. Detection of OOV words using gen-
eralized word models and a semantic class language
model. In Eurospeech.
H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
and G. Zweig. 2005. The ibm 2004 conversational
telephony system for rich transcription. In ICASSP.
H. Sun, G. Zhang, f. Zheng, and M. Xu. 2001. Using
word confidence measure for OOV words detection in
a spontaneous spoken dialog system. In Eurospeech.
Stanley Wang. 2009. Using graphone models in au-
tomatic speech recognition. Master?s thesis, Mas-
sachusetts Institute of Technology.
F. Wessel, R. Schluter, K. Macherey, and H. Ney. 2001.
Confidence measures for large vocabulary continuous
speech recognition. IEEE Transactions on Speech and
Audio Processing, 9(3).
Christopher White, Jasha Droppo, Alex Acero, and Ju-
lian Odell. 2007. Maximum entropy confidence esti-
mation for speech recognition. In ICASSP.
721
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 175?183,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fast Syntactic Analysis for Statistical Language Modeling
via Substructure Sharing and Uptraining
Ariya Rastrow, Mark Dredze, Sanjeev Khudanpur
Human Language Technology Center of Excellence
Center for Language and Speech Processing, Johns Hopkins University
Baltimore, MD USA
{ariya,mdredze,khudanpur}@jhu.edu
Abstract
Long-span features, such as syntax, can im-
prove language models for tasks such as
speech recognition and machine translation.
However, these language models can be dif-
ficult to use in practice because of the time
required to generate features for rescoring a
large hypothesis set. In this work, we pro-
pose substructure sharing, which saves dupli-
cate work in processing hypothesis sets with
redundant hypothesis structures. We apply
substructure sharing to a dependency parser
and part of speech tagger to obtain significant
speedups, and further improve the accuracy
of these tools through up-training. When us-
ing these improved tools in a language model
for speech recognition, we obtain significant
speed improvements with bothN -best and hill
climbing rescoring, and show that up-training
leads to WER reduction.
1 Introduction
Language models (LM) are crucial components in
tasks that require the generation of coherent natu-
ral language text, such as automatic speech recog-
nition (ASR) and machine translation (MT). While
traditional LMs use word n-grams, where the n ? 1
previous words predict the next word, newer mod-
els integrate long-span information in making deci-
sions. For example, incorporating long-distance de-
pendencies and syntactic structure can help the LM
better predict words by complementing the predic-
tive power of n-grams (Chelba and Jelinek, 2000;
Collins et al, 2005; Filimonov and Harper, 2009;
Kuo et al, 2009).
The long-distance dependencies can be modeled
in either a generative or a discriminative framework.
Discriminative models, which directly distinguish
correct from incorrect hypothesis, are particularly
attractive because they allow the inclusion of arbi-
trary features (Kuo et al, 2002; Roark et al, 2007;
Collins et al, 2005); these models with syntactic in-
formation have obtained state of the art results.
However, both generative and discriminative LMs
with long-span dependencies can be slow, for they
often cannot work directly with lattices and require
rescoring large N -best lists (Khudanpur and Wu,
2000; Collins et al, 2005; Kuo et al, 2009). For dis-
criminative models, this limitation applies to train-
ing as well. Moreover, the non-local features used in
rescoring are usually extracted via auxiliary tools ?
which in the case of syntactic features include part of
speech taggers and parsers ? from a set of ASR sys-
tem hypotheses. Separately applying auxiliary tools
to each N -best list hypothesis leads to major ineffi-
ciencies as many hypotheses differ only slightly.
Recent work on hill climbing algorithms for ASR
lattice rescoring iteratively searches for a higher-
scoring hypothesis in a local neighborhood of the
current-best hypothesis, leading to a much more ef-
ficient algorithm in terms of the number, N , of hy-
potheses evaluated (Rastrow et al, 2011b); the idea
also leads to a discriminative hill climbing train-
ing algorithm (Rastrow et al, 2011a). Even so, the
reliance on auxiliary tools slow LM application to
the point of being impractical for real time systems.
While faster auxiliary tools are an option, they are
usually less accurate.
In this paper, we propose a general modifica-
175
tion to the decoders used in auxiliary tools to uti-
lize the commonalities among the set of generated
hypotheses. The key idea is to share substructure
states in transition based structured prediction al-
gorithms, i.e. algorithms where final structures are
composed of a sequence of multiple individual deci-
sions. We demonstrate our approach on a local Per-
ceptron based part of speech tagger (Tsuruoka et al,
2011) and a shift reduce dependency parser (Sagae
and Tsujii, 2007), yielding significantly faster tag-
ging and parsing of ASR hypotheses. While these
simpler structured prediction models are faster, we
compensate for the model?s simplicity through up-
training (Petrov et al, 2010), yielding auxiliary tools
that are both fast and accurate. The result is signif-
icant speed improvements and a reduction in word
error rate (WER) for both N -best list and the al-
ready fast hill climbing rescoring. The net result
is arguably the first syntactic LM fast enough to be
used in a real time ASR system.
2 Syntactic Language Models
There have been several approaches to include syn-
tactic information in both generative and discrimi-
native language models.
For generative LMs, the syntactic information
must be part of the generative process. Structured
language modeling incorporates syntactic parse
trees to identify the head words in a hypothesis for
modeling dependencies beyond n-grams. Chelba
and Jelinek (2000) extract the two previous exposed
head words at each position in a hypothesis, along
with their non-terminal tags, and use them as con-
text for computing the probability of the current po-
sition. Khudanpur and Wu (2000) exploit such syn-
tactic head word dependencies as features in a maxi-
mum entropy framework. Kuo et al (2009) integrate
syntactic features into a neural network LM for Ara-
bic speech recognition.
Discriminative models are more flexible since
they can include arbitrary features, allowing for
a wider range of long-span syntactic dependen-
cies. Additionally, discriminative models are di-
rectly trained to resolve the acoustic confusion in the
decoded hypotheses of an ASR system. This flexi-
bility and training regime translate into better perfor-
mance. Collins et al (2005) uses the Perceptron al-
gorithm to train a global linear discriminative model
which incorporates long-span features, such as head-
to-head dependencies and part of speech tags.
Our Language Model. We work with a discrimi-
native LM with long-span dependencies. We use a
global linear model with Perceptron training. We
rescore the hypotheses (lattices) generated by the
ASR decoder?in a framework most similar to that
of Rastrow et al (2011a).
The LM score S(w,a) for each hypothesis w of
a speech utterance with acoustic sequence a is based
on the baseline ASR system score b(w,a) (initial n-
gram LM score and the acoustic score) and ?0, the
weight assigned to the baseline score.1 The score is
defined as:
S(w,a) = ?0 ? b(w,a) + F (w, s1, . . . , sm)
= ?0 ? b(w,a) +
d?
i=1
?i ? ?i(w, s1, . . . , sm)
where F is the discriminative LM?s score for the
hypothesis w, and s1, . . . , sm are candidate syntac-
tic structures associated with w, as discussed be-
low. Since we use a linear model, the score is a
weighted linear combination of the count of acti-
vated features of the word sequence w and its as-
sociated structures: ?i(w, s1, . . . , sm). Perceptron
training learns the parameters ?. The baseline score
b(w,a) can be a feature, yielding the dot product
notation: S(w,a) = ??,?(a,w, s1, . . . , sm)? Our
LM uses features from the dependency tree and part
of speech (POS) tag sequence. We use the method
described in Kuo et al (2009) to identify the two
previous exposed head words, h?2, h?1, at each po-
sition i in the input hypothesis and include the fol-
lowing syntactic based features into our LM:
1. (h?2.w ? h?1.w ? wi) , (h?1.w ? wi) , (wi)
2. (h?2.t ? h?1.t ? ti) , (h?1.t ? ti) , (ti) , (tiwi)
where h.w and h.t denote the word identity and the
POS tag of the corresponding exposed head word.
2.1 Hill Climbing Rescoring
We adopt the so called hill climbing framework of
Rastrow et al (2011b) to improve both training and
rescoring time as much as possible by reducing the
1We tune ?0 on development data (Collins et al, 2005).
176
number N of explored hypotheses. We summarize
it below for completeness.
Given a speech utterance?s lattice L from a first
pass ASR decoder, the neighborhood N (w, i) of a
hypothesis w = w1w2 . . . wn at position i is de-
fined as the set of all paths in the lattice that may
be obtained by editing wi: deleting it, substituting
it, or inserting a word to its left. In other words,
it is the ?distance-1-at-position i? neighborhood of
w. Given a position i in a word sequence w, all
hypotheses in N (w, i) are rescored using the long-
span model and the hypothesis w??(i) with the high-
est score becomes the new w. The process is re-
peated with a new position ? scanned left to right
? until w = w??(1) = . . . = w??(n), i.e. when w
itself is the highest scoring hypothesis in all its 1-
neighborhoods, and can not be furthered improved
using the model. Incorporating this into training
yields a discriminative hill climbing algorithm (Ras-
trow et al, 2011a).
3 Incorporating Syntactic Structures
Long-span models ? generative or discriminative,
N -best or hill climbing ? rely on auxiliary tools,
such as a POS tagger or a parser, for extracting
features for each hypothesis during rescoring, and
during training for discriminative models. The top-
m candidate structures associated with the ith hy-
pothesis, which we denote as s1i , . . . , smi , are gener-
ated by these tools and used to score the hypothesis:
F (wi, s1i , . . . , smi ). For example, s
j
i can be a part of
speech tag or a syntactic dependency. We formally
define this sequential processing as:
w1
tool(s)????? s11, . . . , sm1
LM??? F (w1, s11, . . . , sm1 )
w2
tool(s)????? s12, . . . , sm2
LM??? F (w2, s12, . . . , sm2 )
...
wk
tool(s)????? s1k, . . . , smk
LM??? F (wk, s1k, . . . , smk )
Here, {w1, . . . ,wk} represents a set of ASR output
hypotheses that need to be rescored. For each hy-
pothesis, we apply an external tool (e.g. parser) to
generate associated structures s1i , . . . , smi (e.g. de-
pendencies.) These are then passed to the language
model along with the word sequence for scoring.
3.1 Substructure Sharing
While long-span LMs have been empirically shown
to improve WER over n-gram LMs, the computa-
tional burden prohibits long-span LMs in practice,
particularly in real-time systems. A major complex-
ity factor is due to processing 100s or 1000s of hy-
potheses for each speech utterance, even during hill
climbing, each of which must be POS tagged and
parsed. However, the candidate hypotheses of an
utterance share equivalent substructures, especially
in hill climbing methods due to the locality present
in the neighborhood generation. Figure 1 demon-
strates such repetition in an N -best list (N=10) and
a hill climbing neighborhood hypothesis set for a
speech utterance from broadcast news. For exam-
ple, the word ?ENDORSE? occurs within the same
local context in all hypotheses and should receive
the same part of speech tag in each case. Processing
each hypothesis separately wastes time.
We propose a general algorithmic approach to re-
duce the complexity of processing a hypothesis set
by sharing common substructures among the hy-
potheses. Critically, unlike many lattice parsing al-
gorithms, our approach is general and produces ex-
act output. We first present our approach and then
demonstrate its generality by applying it to a depen-
dency parser and part of speech tagger.
We work with structured prediction models that
produce output from a series of local decisions: a
transition model. We begin in initial state pi0 and
terminate in a possible final state pif . All states
along the way are chosen from the possible states
?. A transition (or action) ? ? ? advances the
decoder from state to state, where the transition ?i
changes the state from pii to pii+1. The sequence
of states {pi0 . . . pii, pii+1 . . . pif} can be mapped to
an output (the model?s prediction.) The choice of
action ? is given by a learning algorithm, such as
a maximum-entropy classifier, support vector ma-
chine or Perceptron, trained on labeled data. Given
the previous k actions up to pii, the classifier g :
? ? ?k ? R|?| assigns a score to each possi-
ble action, which we can interpret as a probability:
pg(?i|pii, ?i?1?i?2 . . . ?i?k). These actions are ap-
plied to transition to new states pii+1. We note that
state definitions can encode the k previous actions,
which simplifies the probability to pg(?i|pii). The
177
N -best list Hill climbing neighborhood
(1) AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(2) TO AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(3) AL GORE HAS PROMISE THAT HE WOULD ENDORSE A CANDIDATE
(4) SO AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE (1) YEAH FIFTY CENT GALLON NOMINATION WHICH WAS GREAT
(5) IT?S AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE (2) YEAH FIFTY CENT A GALLON NOMINATION WHICH WAS GREAT
(6) AL GORE HAS PROMISED HE WOULD ENDORSE A CANDIDATE (3) YEAH FIFTY CENT GOT A NOMINATION WHICH WAS GREAT
(7) AL GORE HAS PROMISED THAT HE WOULD ENDORSE THE CANDIDATE
(8) SAID AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(9) AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE FOR
(10) AL GORE HIS PROMISE THAT HE WOULD ENDORSE A CANDIDATE
Figure 1: Example of repeated substructures in candidate hypotheses.
score of the new state is then
p(pii+1) = pg(?i|pii) ? p(pii) (1)
Classification decisions require a feature represen-
tation of pii, which is provided by feature functions
f : ?? Y , that map states to features. Features are
conjoined with actions for multi-class classification,
so pg(?i|pii) = pg(f(pi) ? ?i), where ? is a conjunc-
tion operation. In this way, states can be summarized
by features.
Equivalent states are defined as two states pi and
pi? with an identical feature representation:
pi ? pi? iff f(pi) = f(pi?)
If two states are equivalent, then g imposes the same
distribution over actions. We can benefit from this
substructure redundancy, both within and between
hypotheses, by saving these distributions in mem-
ory, sharing a distribution computed just once across
equivalent states. A similar idea of equivalent states
is used by Huang and Sagae (2010), except they use
equivalence to facilitate dynamic programming for
shift-reduce parsing, whereas we generalize it for
improving the processing time of similar hypotheses
in general models. Following Huang and Sagae, we
define kernel features as the smallest set of atomic
features f?(pi) such that,
f?(pi) = f?(pi?) ? pi ? pi?. (2)
Equivalent distributions are stored in a hash table
H : ?? ??R; the hash keys are the states and the
values are distributions2 over actions: {?, pg(?|pi)}.
2For pure greedy search (deterministic search) we need only
retain the best action, since the distribution is only used in prob-
abilistic search, such as beam search or best-first algorithms.
H caches equivalent states in a hypothesis set and re-
sets for each new utterance. For each state, we first
check H for equivalent states before computing the
action distribution; each cache hit reduces decod-
ing time. Distributing hypotheses wi across differ-
ent CPU threads is another way to obtain speedups,
and we can still benefit from substructure sharing by
storing H in shared memory.
We use h(pi) = ?|f?(pi)|i=1 int(f?i(pi)) as the hash
function, where int(f?i(pi)) is an integer mapping of
the ith kernel feature. For integer typed features
the mapping is trivial, for string typed features (e.g.
a POS tag identity) we use a mapping of the cor-
responding vocabulary to integers. We empirically
found that this hash function is very effective and
yielded very few collisions.
To apply substructure sharing to a transition based
model, we need only define the set of states ? (in-
cluding pi0 and pif ), actions ? and kernel feature
functions f? . The resulting speedup depends on the
amount of substructure duplication among the hy-
potheses, which we will show is significant for ASR
lattice rescoring. Note that our algorithm is not an
approximation; we obtain the same output {sji} as
we would without any sharing. We now apply this
algorithm to dependency parsing and POS tagging.
3.2 Dependency Parsing
We use the best-first probabilistic shift-reduce de-
pendency parser of Sagae and Tsujii (2007), a
transition-based parser (Ku?bler et al, 2009) with a
MaxEnt classifier. Dependency trees are built by
processing the words left-to-right and the classifier
assigns a distribution over the actions at each step.
States are defined as pi = {S,Q}: S is a stack of
178
Kernel features f?(pi) for state pi = {S,Q}
S = s0, s1, . . . & Q = q0, q1, . . .
(1) s0.w s0.t s0.r (5) ts0?1
s0.lch.t s0.lch.r ts1+1
s0.rch.t s0.rch.r
(2) s1.w s1.t s1.r (6) dist(s0, s1)
s1.lch.t s1.lch.r dist(q0, s0)
s1.rch.t s1.rch.r
(3) s2.w s2.t s2.r
(4) q0.w q0.t (7) s0.nch
q1.w q1.t s1.nch
q2.w
Table 1: Kernel features for defining parser states. si.w
denotes the head-word in a subtree and t its POS tag.
si.lch and si.rch are the leftmost and rightmost children
of a subtree. si.r is the dependency label that relates a
subtree head-word to its dependent. si.nch is the number
of children of a subtree. qi.w and qi.t are the word and
its POS tag in the queue. dist(s0,s1) is the linear distance
between the head-words of s0 and s1.
subtrees s0, s1, . . . (s0 is the top tree) and Q are
words in the input word sequence. The initial state is
pi0 = {?, {w0, w1, . . .}}, and final states occur when
Q is empty and S contains a single tree (the output).
? is determined by the set of dependency labels
r ? R and one of three transition types:
? Shift: remove the head of Q (wj) and place it on
the top of S as a singleton tree (only wj .)
? Reduce-Leftr: replace the top two trees in S (s0
and s1) with a tree formed by making the root of
s1 a dependent of the root of s0 with label r.
? Reduce-Rightr: same as Reduce-Leftr except re-
verses s0 and s1.
Table 1 shows the kernel features used in our de-
pendency parser. See Sagae and Tsujii (2007) for a
complete list of features.
Goldberg and Elhadad (2010) observed that pars-
ing time is dominated by feature extraction and
score calculation. Substructure sharing reduces
these steps for equivalent states, which are persis-
tent throughout a candidate set. Note that there are
far fewer kernel features than total features, hence
the hash function calculation is very fast.
We summarize substructure sharing for depen-
dency parsing in Algorithm 1. We extend the def-
inition of states to be {S,Q, p} where p denotes the
score of the state: the probability of the action se-
quence that resulted in the current state. Also, fol-
Algorithm 1 Best-first shift-reduce dependency parsing
w ? input hypothesis
S0 = ?, Q0 = w, p0 = 1
pi0 ? {S0, Q0, p0} [initial state]
H ?Hash table (?? ?? R)
Heap? Heap for prioritizing states and performing best-first search
Heap.push(pi0) [initialize the heap]
while Heap 6= ? do
picurrent ?Heap.pop() [the best state so far]
if picurrent = pif [if final state]
return picurrent [terminate if final state]
else ifH.find(picurrent)
ActList? H[picurrent] [retrieve action list from the hash table]
else [need to construct action list]
for all ? ? ? [for all actions]
p? ? pg(?|picurrent) [action score]
ActList.insert({?, p?})
H.insert(picurrent,ActList) [Store the action list into hash table]
end if
for all {?, p?} ? ActList [compute new states]
pinew ? picurrent ? ?
Heap.push(pinew) [push to the heap]
end while
lowing Sagae and Tsujii (2007) a heap is used to
maintain states prioritized by their scores, for apply-
ing the best-first strategy. For each step, a state from
the top of the heap is considered and all actions (and
scores) are either retrieved from H or computed us-
ing g.3 We use pinew ? picurrent ? ? to denote the
operation of extending a state by an action ? ? ?4.
3.3 Part of Speech Tagging
We use the part of speech (POS) tagger of Tsuruoka
et al (2011), a transition based model with a Per-
ceptron and a lookahead heuristic process. The tag-
ger processes w left to right. States are defined as
pii = {ci,w}: a sequence of assigned tags up to wi
(ci = t1t2 . . . ti?1) and the word sequence w. ? is
defined simply as the set of possible POS tags (T )
that can be applied. The final state is reached once
all the positions are tagged. For f we use the features
of Tsuruoka et al (2011). The kernel features are
f?(pii) = {ti?2, ti?1, wi?2, wi?1, wi, wi+1, wi+2}.
While the tagger extracts prefix and suffix features,
it suffices to look at wi for determining state equiv-
alence. The tagger is deterministic (greedy) in that
it only considers the best tag at each step, so we do
not store scores. However, this tagger uses a depth-
3 Sagae and Tsujii (2007) use a beam strategy to increase
speed. Search space pruning is achieved by filtering heap states
for probability greater than 1b the probability of the most likely
state in the heap with the same number of actions. We use b =
100 for our experiments.
4We note that while we have demonstrated substructure
sharing for dependency parsing, the same improvements can
be made to a shift-reduce constituent parser (Sagae and Lavie,
2006).
179
t2t1 ti 2 ti 1
t1i
t2i
t|T |i t|T |i+1
t1i+1
t2i+1
w1 w2 wi 1wi 2 wi wi+1 wi+2 wi+3? ? ?
? ? ?
lookahead search
Figure 2: POS tagger with lookahead search of d=1. At
wi the search considers the current state and next state.
first search lookahead procedure to select the best
action at each step, which considers future decisions
up to depth d5. An example for d = 1 is shown
in Figure 2. Using d = 1 for the lookahead search
strategy, we modify the kernel features since the de-
cision forwi is affected by the state pii+1. The kernel
features in position i should be f?(pii) ? f?(pii+1):
f?(pii) =
{ti?2, ti?1, wi?2, wi?1, wi, wi+1, wi+2, wi+3}
4 Up-Training
While we have fast decoding algorithms for the pars-
ing and tagging, the simpler underlying models can
lead to worse performance. Using more complex
models with higher accuracy is impractical because
they are slow. Instead, we seek to improve the accu-
racy of our fast tools.
To achieve this goal we use up-training, in which
a more complex model is used to improve the accu-
racy of a simpler model. We are given two mod-
els, M1 and M2, as well as a large collection of
unlabeled text. Model M1 is slow but very accu-
rate while M2 is fast but obtains lower accuracy.
Up-training applies M1 to tag the unlabeled data,
which is then used as training data for M2. Like
self-training, a model is retrained on automatic out-
put, but here the output comes form a more accurate
model. Petrov et al (2010) used up-training as a
domain adaptation technique: a constituent parser ?
which is more robust to domain changes ? was used
to label a new domain, and a fast dependency parser
5 Tsuruoka et al (2011) shows that the lookahead search
improves the performance of the local ?history-based? models
for different NLP tasks
was trained on the automatically labeled data. We
use a similar idea where our goal is to recover the
accuracy lost from using simpler models. Note that
while up-training uses two models, it differs from
co-training since we care about improving only one
model (M2). Additionally, the models can vary in
different ways. For example, they could be the same
algorithm with different pruning methods, which
can lead to faster but less accurate models.
We apply up-training to improve the accuracy of
both our fast POS tagger and dependency parser. We
parse a large corpus of text with a very accurate but
very slow constituent parser and use the resulting
data to up-train our tools. We will demonstrate em-
pirically that up-training improves these fast models
to yield better WER results.
5 Related Work
The idea of efficiently processing a hypothesis set is
similar to ?lattice-parsing?, in which a parser con-
sider an entire lattice at once (Hall, 2005; Chep-
palier et al, 1999). These methods typically con-
strain the parsing space using heuristics, which are
often model specific. In other words, they search in
the joint space of word sequences present in the lat-
tice and their syntactic analyses; they are not guaran-
teed to produce a syntactic analysis for all hypothe-
ses. In contrast, substructure sharing is a general
purpose method that we have applied to two differ-
ent algorithms. The output is identical to processing
each hypothesis separately and output is generated
for each hypothesis. Hall (Hall, 2005) uses a lattice
parsing strategy which aims to compute the marginal
probabilities of all word sequences in the lattice by
summing over syntactic analyses of each word se-
quence. The parser sums over multiple parses of a
word sequence implicitly. The lattice parser there-
fore, is itself a language model. In contrast, our
tools are completely separated from the ASR sys-
tem, which allows the system to create whatever fea-
tures are needed. This independence means our tools
are useful for other tasks, such as machine transla-
tion. These differences make substructure sharing a
more attractive option for efficient algorithms.
While Huang and Sagae (2010) use the notion of
?equivalent states?, they do so for dynamic program-
ming in a shift-reduce parser to broaden the search
space. In contrast, we use the idea to identify sub-
180
structures across inputs, where our goal is efficient
parsing in general. Additionally, we extend the defi-
nition of equivalent states to general transition based
structured prediction models, and demonstrate ap-
plications beyond parsing as well as the novel setting
of hypothesis set parsing.
6 Experiments
Our ASR system is based on the 2007 IBM
Speech transcription system for the GALE Distilla-
tion Go/No-go Evaluation (Chen et al, 2006) with
state of the art discriminative acoustic models. See
Table 2 for a data summary. We use a modi-
fied Kneser-Ney (KN) backoff 4-gram baseline LM.
Word-lattices for discriminative training and rescor-
ing come from this baseline ASR system.6 The long-
span discriminative LM?s baseline feature weight
(?0) is tuned on dev data and hill climbing (Rastrow
et al, 2011a) is used for training and rescoring. The
dependency parser and POS tagger are trained on su-
pervised data and up-trained on data labeled by the
CKY-style bottom-up constituent parser of Huang et
al. (2010), a state of the art broadcast news (BN)
parser, with phrase structures converted to labeled
dependencies by the Stanford converter.
While accurate, the parser has a huge grammar
(32GB) from using products of latent variable gram-
mars and requires O(l3) time to parse a sentence of
length l. Therefore, we could not use the constituent
parser for ASR rescoring since utterances can be
very long, although the shorter up-training text data
was not a problem.7 We evaluate both unlabeled
(UAS) and labeled dependency accuracy (LAS).
6.1 Results
Before we demonstrate the speed of our models, we
show that up-training can produce accurate and fast
models. Figure 3 shows improvements to parser ac-
curacy through up-training for different amount of
(randomly selected) data, where the last column in-
dicates constituent parser score (91.4% UAS). We
use the POS tagger to generate tags for depen-
dency training to match the test setting. While
there is a large difference between the constituent
and dependency parser without up-training (91.4%
6For training a 3-gram LM is used to increase confusions.
7Speech utterances are longer as they are not as effectively
sentence segmented as text.
84.0	 ?
85.0	 ?
86.0	 ?
87.0	 ?
88.0	 ?
89.0	 ?
90.0	 ?
91.0	 ?
92.0	 ?
0M	 ? 2.5M	 ? 5M	 ? 10M	 ? 20M	 ? 40M	 ? Cons?tuent	 ?Parser	 ?
Accu
racy
	 ?(%)
	 ?
Amount	 ?of	 ?Added	 ?Uptraining	 ?Data	 ?
Unlabeled	 ?A?achment	 ?Score	 ?
Labeled	 ?A?achment	 ?Score	 ?
Figure 3: Up-training results for dependency parsing for
varying amounts of data (number of words.) The first
column is the dependency parser with supervised training
only and the last column is the constituent parser (after
converting to dependency trees.)
vs. 86.2% UAS), up-training can cut the differ-
ence by 44% to 88.5%, and improvements saturate
around 40m words (about 2m sentences.)8 The de-
pendency parser remains much smaller and faster;
the up-trained dependency model is 700MB with
6m features compared with 32GB for constituency
model. Up-training improves the POS tagger?s accu-
racy from 95.9% to 97%, when trained on the POS
tags produced by the constituent parser, which has a
tagging accuracy of 97.2% on BN.
We train the syntactic discriminative LM, with
head-word and POS tag features, using the faster
parser and tagger and then rescore the ASR hypothe-
ses. Table 3 shows the decoding speedups as well as
the WER reductions compared to the baseline LM.
Note that up-training improvements lead to WER re-
ductions. Detailed speedups on substructure sharing
are shown in Table 4; the POS tagger achieves a 5.3
times speedup, and the parser a 5.7 speedup with-
out changing the output. We also observed speedups
during training (not shown due to space.)
The above results are for the already fast hill
climbing decoding, but substructure sharing can also
be used for N -best list rescoring. Figure 4 (logarith-
mic scale) illustrates the time for the parser and tag-
ger to processN -best lists of varying size, with more
substantial speedups for larger lists. For example,
for N=100 (a typical setting) the parsing time re-
8Better performance is due to the exact CKY-style ? com-
pared with best-first and beam? search and that the constituent
parser uses the product of huge self-trained grammars.
181
Usage Data Size
Acoustic model training Hub4 acoustic train 153k uttr, 400 hrs
Baseline LM training: modified KN 4-gram TDT4 closed captions+EARS BN03 closed caption 193m words
Disc. LM training: long-span w/hill climbing Hub4 (length <50) 115k uttr, 2.6m words
Baseline feature (?0) tuning dev04f BN data 2.5 hrs
Supervised training: dep. parser, POS tagger Ontonotes BN treebank+ WSJ Penn treebank 1.3m words, 59k sent.
Supervised training: constituent parser Ontonotes BN treebank + WSJ Penn treebank 1.3m words, 59k sent.
Up-training: dependency parser, POS tagger TDT4 closed captions+EARS BN03 closed caption 193m words available
Evaluation: up-training BN treebank test (following Huang et al (2010)) 20k words, 1.1k sent.
Evaluation: ASR transcription rt04 BN evaluation 4 hrs, 45k words
Table 2: A summary of the data for training and evaluation. The Ontonotes corpus is from Weischedel et al (2008).
10	 ?
100	 ?
1000	 ?
10000	 ?
100000	 ?
1000000	 ?
1	 ? 10	 ? 100	 ? 1000	 ?
Elap
sed
	 ?Tim
e	 ?(s
ec)	 ?
N-??best	 ?Size	 ?(N)	 ?
No	 ?Sharing	 ?
Substructure	 ?Sharing	 ?
(a)
1	 ?
10	 ?
100	 ?
1000	 ?
10000	 ?
1	 ? 10	 ? 100	 ? 1000	 ?
Elap
sed
	 ?Tim
e	 ?(s
ec)	 ?
N-??best	 ?Size	 ?(N)	 ?
No	 ?Sharing	 ?
Substructure	 ?Sharing	 ?
(b)
Figure 4: Elapsed time for (a) parsing and (b) POS tagging the N -best lists with and without substructure sharing.
Substr. Share (sec)
LM WER No Yes
Baseline 4-gram 15.1 - -
Syntactic LM 14.8
8,658 1,648
+ up-train 14.6
Table 3: Speedups and WER for hill climbing rescor-
ing. Substructure sharing yields a 5.3 times speedup. The
times for with and without up-training are nearly identi-
cal, so we include only one set for clarity. Time spent
is dominated by the parser, so the faster parser accounts
for much of the overall speedup. Timing information in-
cludes neighborhood generation and LM rescoring, so it
is more than the sum of the times in Table 4.
duces from about 20,000 seconds to 2,700 seconds,
about 7.4 times as fast.
7 Conclusion
The computational complexity of accurate syntac-
tic processing can make structured language models
impractical for applications such as ASR that require
scoring hundreds of hypotheses per input. We have
Substr. Share Speedup
No Yes
Parser 8,237.2 1,439.5 5.7
POS tagger 213.3 40.1 5.3
Table 4: Time in seconds for the parser and POS tagger
to process hypotheses during hill climbing rescoring.
presented substructure sharing, a general framework
that greatly improves the speed of syntactic tools
that process candidate hypotheses. Furthermore, we
achieve improved performance through up-training.
The result is a large speedup in rescoring time, even
on top of the already fast hill climbing framework,
and reductions in WER from up-training. Our re-
sults make long-span syntactic LMs practical for
real-time ASR, and can potentially impact machine
translation decoding as well.
Acknowledgments
Thanks to Kenji Sagae for sharing his shift-reduce
dependency parser and the anonymous reviewers for
helpful comments.
182
References
C. Chelba and F. Jelinek. 2000. Structured lan-
guage modeling. Computer Speech and Language,
14(4):283?332.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
J. Cheppalier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Sixth Conference sur le Traitement Automatique du
Langage Naturel (TANL?99).
M Collins, B Roark, and M Saraclar. 2005. Discrimina-
tive syntactic language modeling for speech recogni-
tion. In ACL.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Proc. HLT-NAACL, number
June, pages 742?750.
Keith B Hall. 2005. Best-first word-lattice parsing:
techniques for integrated syntactic language modeling.
Ph.D. thesis, Brown University.
L. Huang and K. Sagae. 2010. Dynamic Programming
for Linear-Time Incremental Parsing. In Proceedings
of ACL.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with Products of Latent Variable Gram-
mars. In Proc. EMNLP, number October, pages 12?
22.
S. Khudanpur and J. Wu. 2000. Maximum entropy tech-
niques for exploiting syntactic, semantic and colloca-
tional dependencies in language modeling. Computer
Speech and Language, pages 355?372.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies, 2(1):1?127.
Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang,
and Chin-Hui Lee. 2002. Discriminative training of
language models for speech recognition. In ICASSP.
H. K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and
L. Young-Suk. 2009. Syntactic features for Arabic
speech recognition. In Proc. ASRU.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705?713, Cambridge, MA,
October. Association for Computational Linguistics.
Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur.
2011a. Efficient discrimnative training of long-span
language models. In IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU).
Ariya Rastrow, Markus Dreyer, Abhinav Sethy, San-
jeev Khudanpur, Bhuvana Ramabhadran, and Mark
Dredze. 2011b. Hill climbing on speech lattices : A
new rescoring framework. In ICASSP.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech & Language, 21(2).
K. Sagae and A. Lavie. 2006. A best-first probabilis-
tic shift-reduce parser. In Proc. ACL, pages 691?698.
Association for Computational Linguistics.
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. EMNLP-CoNLL, volume 7, pages
1044?1050.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with Lookahead :
Can History-Based Models Rival Globally Optimized
Models ? In Proc. CoNLL, number June, pages 238?
246.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
183
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 50?58,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Revisiting the Case for Explicit Syntactic Information in Language Models
Ariya Rastrow, Sanjeev Khudanpur, Mark Dredze
Human Language Technology Center of Excellence,
Center for Language and Speech Processing, Johns Hopkins University
Baltimore, MD USA
{ariya,khudanpur,mdredze}@jhu.edu
Abstract
Statistical language models used in deployed
systems for speech recognition, machine
translation and other human language tech-
nologies are almost exclusively n-gram mod-
els. They are regarded as linguistically na??ve,
but estimating them from any amount of text,
large or small, is straightforward. Further-
more, they have doggedly matched or out-
performed numerous competing proposals for
syntactically well-motivated models. This un-
usual resilience of n-grams, as well as their
weaknesses, are examined here. It is demon-
strated that n-grams are good word-predictors,
even linguistically speaking, in a large major-
ity of word-positions, and it is suggested that
to improve over n-grams, one must explore
syntax-aware (or other) language models that
focus on positions where n-grams are weak.
1 Introduction
Language models (LM) are crucial components in
tasks that require the generation of coherent natu-
ral language text, such as automatic speech recog-
nition (ASR) and machine translation (MT). Most
language models rely on simple n-gram statistics
and a wide range of smoothing and backoff tech-
niques (Chen and Goodman, 1998). State-of-the-art
ASR systems use (n ? 1)-gram equivalence classi-
fication for the language model (which result in an
n-gram language model).
While simple and efficient, it is widely believed
that limiting the context to only the (n ? 1) most
recent words ignores the structure of language, and
several statistical frameworks have been proposed
to incorporate the ?syntactic structure of language
back into language modeling.? Yet despite consider-
able effort on including longer-dependency features,
such as syntax (Chelba and Jelinek, 2000; Khudan-
pur and Wu, 2000; Collins et al, 2005; Emami
and Jelinek, 2005; Kuo et al, 2009; Filimonov and
Harper, 2009), n-gram language models remain the
dominant technique in automatic speech recognition
and machine translation (MT) systems.
While intuition suggests syntax is important, the
continued dominance of n-gram models could in-
dicate otherwise. While no one would dispute that
syntax informs word choice, perhaps sufficient in-
formation aggregated across a large corpus is avail-
able in the local context for n-gram models to per-
form well even without syntax. To clearly demon-
strate the utility of syntactic information and the de-
ficiency of n-gram models, we empirically show that
n-gram LMs lose significant predictive power in po-
sitions where the syntactic relation spans beyond the
n-gram context. This clearly shows a performance
gap in n-gram LMs that could be bridged by syntax.
As a candidate syntactic LM we consider the
Structured Language Model (SLM) (Chelba and Je-
linek, 2000), one of the first successful attempts to
build a statistical language model based on syntac-
tic information. The SLM assigns a joint probabil-
ity P (W,T ) to every word sequence W and every
possible binary parse tree T , where T ?s terminals
are words W with part-of-speech (POS) tags, and
its internal nodes comprise non-terminal labels and
lexical ?heads? of phrases. Other approaches in-
clude using the exposed headwords in a maximum-
entropy based LM (Khudanpur and Wu, 2000), us-
50
ing exposed headwords from full-sentence parse tree
in a neural network based LM (Kuo et al, 2009),
and the use of syntactic features in discriminative
training (Rastrow et al, 2011). We show that the
long-dependencies modeled by SLM, significantly
improves the predictive power of the LM, specially
in positions where the syntactic relation is beyond
the reach of regular n-gram models.
2 Weaknesses of n-gram LMs
Consider the following sentence, which demon-
strates why the (n? 1)-gram equivalence classifica-
tion of history in n-gram language models may be
insufficient:
<s> i asked the vice president for
his endorsement </s>
In an n-gram LM, the word for would be modeled
based on a 3-gram or 4-gram history, such as <vice
president> or <the vice president>.
Given the syntactic relation between the preposition
for and the verb asked (which together make a
compound verb), the strongest evidence in the his-
tory (and hence the best classification of the history)
for word for should be <asked president>,
which is beyond the 4-gram LM. Clearly, the
syntactic relation between a word position and the
corresponding words in the history spans beyond
the limited (n ? 1)-gram equivalence classification
of the history.
This is but one of many examples used for moti-
vating syntactic features (Chelba and Jelinek, 2000;
Kuo et al, 2009) in language modeling. How-
ever, it is legitimate to ask if this deficiency could
be overcome through sufficient data, that is, accu-
rate statistics could somehow be gathered for the n-
grams even without including syntactic information.
We empirically show that (n? 1)-gram equivalence
classification of history is not adequate to predict
these cases. Specifically, n-gram LMs lose predic-
tive power in the positions where the headword rela-
tion, exposed by the syntactic structure, goes beyond
(n? 1) previous words (in the history.)
We postulate the following three hypotheses:
Hypothesis 1 There is a substantial difference in
the predictive power of n-gram LMs at positions
within a sentence where syntactic dependencies
reach further back than the n-gram context versus
positions where syntactic dependencies are local.
Hypothesis 2 This difference does not diminish by
increasing training data by an order of magnitude.
Hypothesis 3 LMs that specifically target positions
with syntactically distant dependencies will comple-
ment or improve over n-gram LMs for these posi-
tions.
In the following section (Section 3), we present a set
of experiments to support the hypotheses 1 and 2.
Section 4 introduces a SLM which uses dependency
structures followed by experiments in Section 5.
3 Experimental Evidence
In this section, we explain our experimental evi-
dence for supporting the hypotheses stated above.
First, Section 3.1 presents our experimental design
where we use a statistical constituent parser to iden-
tify two types of word positions in a test data,
namely positions where the headword syntactic re-
lation spans beyond recent words in the history and
positions where the headword syntactic relation is
within the n-gram window. The performance of
an n-gram LM is measured on both types of posi-
tions to show substantial difference in the predictive
power of the LM in those positions. Section 3.3 de-
scribes the results and analysis of our experiments
which supports our hypotheses.
Throughout the rest of the paper, we refer to
a position where the headword syntactic relation
reaches further back than the n-gram context as a
syntactically-distant position and other type of posi-
tions is referred to as a syntactically-local position.
3.1 Design
Our experimental design is based on the idea of
comparing the performance of n-gram LMs for
syntactically-distant vs. syntactically-local . To this
end, we first parse each sentence in the test set us-
ing a constituent parser, as illustrated by the exam-
ple in Figure 1. For each word wi in each sentence,
we then check if the ?syntactic heads? of the preced-
ing constituents in the parse ofw1, w2, ? ? ? , wi?1 are
within an (n? 1) window of wi. In this manner, we
split the test data into two disjoint sets, M and N ,
51
!"!#$%&'!!!!!()&!*"+&!,-&$"'&.(!!!!!!/0-!!!!!!!)"$!&.'0-$&1&.(!
232!
42!
5!
67!
62!
89! 442! 442!
42!
:4! 232;! 44!
442!
22!
#$%&'!
#$%&'!
/0-!
&.'0-$&1&.(!,-&$"'&.(!"!
Figure 1: Example of a syntactically distant position in
a sentence: the exposed headwords preceding for are
h.w?2 =asked and h.w?1 = president, while the
two preceding words are wi?2 = vice and wi?1 =
president.
as follows,
M = {j|positions s.t h.w?1, h.w?2 = wj?1, wj?2}
N = {j|positions s.t h.w?1, h.w?2 6= wj?1, wj?2}
Here, h?1 and h?2 correspond, respectively, to the
two previous exposed headwords at position i, based
on the syntactic structure. Therefore, M corre-
sponds to the positions in the test data for which two
previous exposed heads match exactly the two previ-
ous words. Whereas, N corresponds to the position
where at least on of the exposed heads is further back
in the history than the two previous words, possibly
both.
To extract the exposed headwords at each posi-
tion, we use a constituent parser to obtain the syn-
tactic structure of a sentence followed by headword
percolation procedure to get the headwords of cor-
responding syntactic phrases in the parse tree. The
following method, described in (Kuo et al, 2009),
is then used to extract exposed headwords from the
history of position i from the full-sentence parse
trees:
1. Start at the leaf corresponding to the word posi-
tion (wi) and the leaf corresponding to the pre-
vious context word (wi?1).
2. From each leaf, go up the tree until the two
paths meet at the lowest common ancestor
(LCA).
3. Cut the link between the LCA and the child that
is along the path from the context word wi?1.
The head word of the the LCA child, the one
that is cut, is chosen as previous exposed head-
word h.w?1.
These steps may be illustrated using the parse tree
shown in Figure 1. Let us show the procedure for
our example from Section 2. Figure 1 shows the cor-
responding parse tree of our example. Considering
word position wi=for and wi?1=president and
applying the above procedure, the LCA is the node
VPasked. Now, by cutting the link from VPasked to
NPpresident the word president is obtained as
the first exposed headword (h.w?1).
After the first previous exposed headword has
been extracted, the second exposed headword also
can be obtained using the same procedure, with
the constraint that the node corresponding the sec-
ond headword is different from the first (Kuo et al,
2009). More precisely,
1. set k = 2
2. Apply the above headword extraction method
between wi and wi?k.
3. if the extracted headword has previously been
chosen, set k = k + 1 and go to step (2).
4. Otherwise, return the headword as h.w?2.
Continuing with the example of Figure 1, after
president is chosen as h.w?1, asked is cho-
sen as h.w?2 of position for by applying the pro-
cedure above. Therefore, in this example the po-
sition corresponding to word for belongs to the
set N as the two extracted exposed headwords
(asked,president) are different from the two
previous context words (vice,president).
After identifying sets N andM in our test data,
we measure perplexity of n-gram LMs on N , M
and N ?M separately. That is,
PPLN?M = exp
[
?
?
i?N?M log p(wi|W i?1i?n+1)
|N ?M|
]
PPLN = exp
[
?
?
i?N
log p(wi|W i?1i?n+1)
|N |
]
PPLM = exp
[
?
?
i?M
log p(wi|W i?1i?n+1)
|M|
]
,
52
where p(wi|wi?1wi?2 ? ? ?wi?n+1) is the condi-
tional probability calculated by an n-gram LM at
position i and |.| is the size (in number of words)
of the corresponding portion of the test.
In addition, to show the performance of n-gram
LMs as a function of training data size, we train
different n-gram LMs on 10%,20%,? ? ? ,100% of a
large corpus of text and report the PPL numbers us-
ing each trained LM with different training data size.
For all sizes less than 100%, we select 10 random
subset of the training corpus of the required size, and
report the average perplexity of 10 n-gram models.
This will enable us to observe the improvement of
the n-gram LMs on as we increase the training data
size. The idea is to test the hypothesis that not only
is there significant gap between predictive power of
the n-gram LMs on setsN andM, but also that this
difference does not diminish by adding more train-
ing data. In other words, we want to show that the
problem is not due to lack of robust estimation of
the model parameters but due to the fact that the in-
cluded features in the model (n-grams) are not in-
formative enough for the positions N .
3.2 Setup
The n-gram LMs are built on 400M words from
various Broadcast News (BN) data sources includ-
ing (Chen et al, 2006): 1996 CSR Hub4 Language
Model data, EARS BN03 closed captions, GALE
Phase 2 Distillation GNG Evaluation Supplemen-
tal Multilingual data, Hub4 acoustic model training
scripts (corresponding to the 300 Hrs), TDT4 closed
captions, TDT4 newswire, GALE Broadcast Con-
versations, and GALE Broadcast News. All the LMs
are trained using modified Kneser-Ney smoothing.
To build the LMs, we sample from each source and
build a source specific LM on the sampled data. The
final LMs are then built by interpolating those LMs.
Also, we do not apply any pruning to the trained
LMs, a step that is often necessary for speech recog-
nition but not so for perplexity measurement. The
test set consists of the NIST rt04 evaluation data set,
dev04f evaluation set, and rt03 evaluation set. The
test data includes about 70K words.
We use the parser of (Huang and Harper, 2009),
which achieves state-of-the-art performance on
broadcast news data, to identify the word poisons
that belong to N and M, as was described in Sec-
tion 3.1. The parser is trained on the Broadcast News
treebank from Ontonotes (Weischedel et al, 2008)
and the WSJ Penn Treebank (Marcus et al, 1993)
along with self-training on 1996 Hub4 CSR (Garo-
folo et al, 1996) utterances.
3.3 Analysis
We found that |N ||N?M| ? 0.25 in our test data. In
other words, two previous exposed headwords go
beyond 2-gram history for about 25% of the test
data.
!"#
$%#
$"#
&%#
&"#
'%%#
'%# (%# )%# *%# "%# +%# !%# $%# &%# '%%#
,-./
01-#
223#
456#
78/9:9:;#</=/#>9?-#456#
@AB# @# B#
(a)
!"#
$%#
$"#
&%#
&"#
'%#
'"#
(%%#
(%# )%# *%# +%# "%# !%# $%# &%# '%# (%%#
,-./
01-#
223#4
56#
78/9:9:;#</=/#>9?-#456#
@AB# @# B#
(b)
Figure 2: Reduction in perplexity with increasing training
data size on the entire test setN +M, on its syntactically
local subset M, and the syntactically distant subset N .
The figure shows relative perplexity instead of absolute
perplexity ? 100% being the perplexity for the smallest
training set size ? so that (a) 3-gram and (b) 4-gram LMs
may be directly compared.
We train 3-gram and 4-gram LMs on
10%,20%,? ? ? ,100% of the BN training data,
where each 10% increase corresponds to about
40M words of training text data. Figure 2 shows
reduction in perplexity with increasing training data
size on the entire test setN+M, on its syntactically
local subsetM, and the syntactically distant subset
N . The figure basically shows relative perplexity
instead of absolute perplexity ? 100% being the
53
Position Training Data Size
in 40M words 400M words
Test Set 3-gram 4-gram 3-gram 4-gram
M 166 153 126 107
N 228 217 191 171
N +M 183 170 143 123
PPLN
PPLM
138% 142% 151% 161%
Table 1: Perplexity of 3-gram and 4-gram LMs on syntac-
tically local (M) and syntactically distant (N ) positions
in the test set for different training data sizes, showing the
sustained higher perplexity in distant v/s local positions.
perplexity for the smallest training set size ? so the
rate of improvement for 3-grams and 4-gram LMs
can be compared. As can be seen from Figure 2,
there is a substantial gap between the improvement
rate of perplexity in syntactically distant positions
compared to that in syntactically local positions
(with 400M woods of training data, this gap is about
10% for both 3-gram and 4-gram LMs). In other
words, increasing the training data size has much
more effect on improving the predictive power of
the model for the positions included inM. Also, by
comparing Figure 2(a) to 2(b) one can observe that
the gap is not overcome by increasing the context
length (using 4-gram features).
Also, to better illustrate the performance of the n-
gram LMs for different portions of our test data, we
report the absolute values of PPL results in Table 1.
It can be seen that there exits a significant difference
between perplexity of sets N and M and that the
difference gets larger as we increase the training data
size.
4 Dependency Language Models
To overcome the lack of predictive power of n-gram
LMs in syntactically-distant positions, we use the
SLM framework to build a long-span LM. Our hope
is to show not only that long range syntactic depen-
dencies improve over n-gram features, but also that
the improvement is largely due to better predictive
power in the syntactically distant positions N .
Syntactic information may be encoded in terms
of headwords and headtags of phrases, which may
be extracted from a syntactic analysis of a sen-
tence (Chelba and Jelinek, 2000; Kuo et al, 2009),
such as a dependency structure. A dependency in
a sentence holds between a dependent (or modifier)
word and a head (or governor) word: the dependent
depends on the head. These relations are encoded in
a dependency tree (Figure 3), a directed graph where
each edge (arc) encodes a head-dependent relation.
The specific parser used to obtain the syntactic
structure is not important to our investigation. What
is crucial, however, is that the parser proceeds left-
to-right, and only hypothesized structures based on
w1, . . . , wi?1 are used by the SLM to predict wi.
Similarly, the specific features used by the parser
are also not important: more noteworthy is that the
SLM uses (h.w?3, h.w?2, h.w?1) and their POS
tags to predict wi. The question is whether this
yields lower perplexity than predicting wi from
(wi?3, wi?2, wi?1).
For the sake of completeness, we next describe
the parser and SLM in some detail, but either may
be skipped without loss of continuity.
The Parser: We use the shift-reduce incremen-
tal dependency parser of (Sagae and Tsujii, 2007),
which constructs a tree from a transition sequence
governed by a maximum-entropy classifier. Shift-
reduce parsing places input words into a queue Q
and partially built structures are organized by a stack
S. Shift and reduce actions consume the queue and
build the output parse on the stack. The classi-
fier g assigns probabilities to each action, and the
probability of a state pg(pi) can be computed as the
product of the probabilities of a sequence of ac-
tions that resulted in the state. The parser therefore
provides (multiple) syntactic analyses of the history
w1, . . . , wi?1 at each word position wi.
The Dependency Language Model: Parser states
at position wi, called history-states, are denoted
??i = {pi0?i, pi1?i ? ? ? , piKi?i }, where Ki is the total
number of such states. Given ??i, the probability
assignment for wi is given by
p(wi|W?i) =
|??i|?
j=1
p
(
wi|f(pij?i)
)
pg(pij?i|W?i) (1)
where, W?i is the word history w1, . . . , wi?1 for
wi, pij?i is the jth history-state of position i,
pg(pij?i|W?i) is the probability assigned to pi
j
?i by
54
step
action stack queue
i asked the vice president ...-0
asked the vice president ...shift1 i
the vice president for ...shift2 i asked
the vice president for ...left-reduce3 asked
i
for his endorsement ...shift6 asked the vice president
i
for his endorsement ...left-reduce7 asked the president
i vice
<s>   i   asked   the vice president   for    his  endorsement
Thursday, March 29, 12
for his endorse ent ...left-reduce8 asked president
i
vicethe
for his endorsement ...right-reduce9 asked
i
vicethe
president
Thursday, March 29, 12
step
action stack queue
i asked the vice president ...-0
asked the vice president ...shift1 i
the vice president for ...shift2 i asked
the vice president for ...left-reduce3 asked
i
for his endorsement ...shift6 asked the vice president
i
for his endorsement ...left-reduce7 asked the president
i vice
<s>   i   asked   the vice president   for    his  endorsement
Thursday, March 29, 12
Tuesday, April 3, 12
Figure 3: Actions of a shift-reduce parser to produce
the dependency structure (up to the word president)
shown above.
the parser, and f(pij?i) denotes an equivalence clas-
sification of the parser history-state, capturing fea-
tures from pij?i that are useful for predicting wi.
We restrict f(pi) to be based on only the heads of
the partial trees {s0 s1 ? ? ? } in the stack. For exam-
ple, in Figure 3, one possible parser state for pre-
dicting the word for is the entire stack shown after
step 8, but we restrict f(?) to depend only on the
headwords asked/VB and president/NNP.
Given a choice of f(?), the parameters of the
model p(wi|f(pij?i)) are estimated to maximize the
log-likelihood of the training data T using the
Baum-Welch algorithm (Baum, 1972), and the re-
sulting estimate is denoted pML(wi|f(pij?i)).
The estimate pML(w|f(?)) must be smoothed to
handle unseen events, which we do using the method
of Jelinek and Mercer (1980). We use a fine-to-
coarse hierarchy of features of the history-state as
illustrated in Figure 4. With
fM (pi?i) ? fM?1(pi?i) ? . . . ? f1(pi?i)
denoting the set of M increasingly coarser equiv-
alence classifications of the history-state pi?i,
we linearly interpolate the higher order esti-
mates pML
(
w|fm(pi?i)
)
with lower order estimates
pML
(
w|fm?1(pi?i)
)
as
pJM(wi|fm(pi?i))
= ?fmpML(wi|fm(pi?i))
+(1? ?fm)pJM(wi|fm?1(pi?i)),
for 1 ? m ? M , where the 0-th order model
pJM(wi|f0(pi?i)) is a uniform distribution.
HW+HT :
(h.w0h.t0, h.w 1h.t 1, h.w 2h.t 2)
(h.w0h.t0)
()
(h.w0, h.t0, h.w 1, h.t 1, h.t 2)
(h.w0, h.t0, h.t 1)
(h.t0)
Saturday, April 14, 12
Figure 4: The hierarchal scheme of fine-to-coarse con-
texts used for Jelinek-Mercer smoothing in the SLM.
The coefficients ?fm(pi?i) are estimated on a held-
out set using the bucketing algorithm suggested by
Bahl (1983), which ties ?fm(pi?i)?s based on the
count of fm(pi?i)?s in the training data. We use the
expected count of the features from the last iteration
of EM training, since the pi?i are latent states.
We perform the bucketing algorithm for each level
f1, f2, ? ? ? , fM of equivalence classification sepa-
rately, and estimate the bucketed ?c(fm) using the
Baum-Welch algorithm (Baum, 1972) to maximize
the likelihood of held out data, where the word prob-
ability assignment in Eq. 1 is replaced with:
p(wi|W?i) =
|?i|?
j=1
pJM
(
wi|fM (pij?i)
)
pg(pij?i|W?i).
The hierarchy shown in Figure 4 is used1 for obtain-
ing a smooth estimate pJM(?|?) at each level.
5 SLM Experiments
We train a dependency SLM for two different tasks,
namely Broadcast News (BN) and Wall Street Jour-
nal (WSJ). Unlike Section 3.2, where we swept
through multiple training sets of multiple sizes,
1The original SLM hierarchical interpolation scheme is ag-
gressive in that it drops both the tag and headword from the
history. However, in many cases the headword?s tag alone is
sufficient, suggesting a more gradual interpolation. Keeping the
headtag adds more specific information and at the same time
is less sparse. A similar idea is found, e.g., in the back-off hi-
erarchical class n-gram language model (Zitouni, 2007) where
instead of backing off from the n-gram right to the (n ? 1)-
gram a more gradual backoff ? by considering a hierarchy of
fine-to-coarse classes for the last word in the history? is used.
55
training the SLM is computationally intensive. Yet,
useful insights may be gained from the 40M word
case. So we choose the source of text most suitable
for each task, and proceed as follows.
5.1 Setup
The following summarizes the setup for each
task:
? BN setup : EARS BN03 corpus, which has
about 42M words serves as our training text.
We also use rt04 (45K words) as our evaluation
data. Finally, to interpolate our structured lan-
guage models with the baseline 4-gram model,
we use rt03+dev04f (about 40K words) data sets
to serve as our development set. The vocabulary
we use in BN experiments has about 84K words.
? WSJ setup : The training text consists of about
37M words. We use eval92+eval93 (10K
words) as our evaluation set and dev93 (9K
words) serves as our development set for inter-
polating SLMs with the baseline 4-gram model.
In both cases, we sample about 20K sentences from
the training text (we exclude them from training
data) to serve as our heldout data for applying the
bucketing algorithm and estimating ??s. To apply
the dependency parser, all the data sets are first
converted to Treebank-style tokenization and POS-
tagged using the tagger of (Tsuruoka et al, 2011)2.
Both the POS-tagger and the shift-reduce depen-
dency parser are trained on the Broadcast News tree-
bank from Ontonotes (Weischedel et al, 2008) and
the WSJ Penn Treebank (after converting them to
dependency trees) which consists of about 1.2M to-
kens. Finally, we train a modified kneser-ney 4-gram
LM on the tokenized training text to serve as our
baseline LM, for both experiments.
5.2 Results and Analysis
Table 2 shows the perplexity results for BN and WSJ
experiments, respectively. It is evident that the 4-
gram baseline for BN is stronger than the 40M case
of Table 1. Yet, the interpolated SLM significantly
improves over the 4-gram LM, as it does for WSJ.
2To make sure we have a proper LM, the POS-tagger and
dependency parser only use features from history to tag a word
position and produce the dependency structure. All lookahead
features used in (Tsuruoka et al, 2011) and (Sagae and Tsujii,
Language Model Dev Eval
BN
Kneser-Ney 4-gram 165 158
SLM 168 159
KN+SLM Interpolation 147 142
WSJ
Kneser-Ney 4-gram 144 121
SLM 149 125
KN+SLM Interpolation 132 110
Table 2: Test set perplexities for different LMs on the BN
and WSJ tasks.
Also, to show that, in fact, the syntactic depen-
dencies modeled through the SLM parameterization
is enhancing predictive power of the LM in the prob-
lematic regions, i.e. syntactically-distant positions,
we calculate the following (log) probability ratio for
each position in the test data,
log pKN+SLM(wi|W?i)pKN(wi|W?i)
, (2)
where pKN+SLM is the word probability assign-
ment of the interpolated SLM at each position, and
pKN(wi) is the probability assigned by the baseline
4-gram model. The quantity above measures the im-
provement (or degradation) gained as a result of us-
ing the SLM parameterization3.
Figures 5(a) and 5(b) illustrate the histogram of
the above probability ratio for all the word positions
in evaluation data of BN and WSJ tasks, respectively.
In these figures the histograms for syntactically-
distant and syntactically-local are shown separately
to measure the effect of the SLM for either of the
position types. It can be observed in the figures
that for both tasks the percentage of positions with
log pKN+SLM(wi|W?i)pKN(wi|W?i) around zero is much higher for
syntactically-local (blue bars) than the syntactically-
distant (red bars). To confirm this, we calculate
the average log pKN+SLM(wi|W?i)pKN(wi|W?i) ?this is the aver-
age log-likelihood improvement, which is directly
2007) are excluded.
3If log pKN+SLM(wi|W?i)pKN(wi|W?i) is greater than zero, then the SLM
has a better predictive power for word position wi. This is a
meaningful comparison due to the fact that the probability as-
signment using both SLM and n-gram is a proper probability
(which sums to one over all words at each position).
56
?1 ?0.5 0 0.5 1 1.5 2 2.5 3 3.5 40
2
4
6
8
10
12
14
16
Probability Ratio (Log)
Percent
age Pos
itions (%)
 
 Syntactically?local positions    (mean=0.1372)Syntactically?distant postions  (mean=0.2351)
(a) BN
?1 ?0.5 0 0.5 1 1.5 2 2.5 3 3.5 402
46
810
1214
1618
2022
Probability Ratio (Log)
Percent
age Pos
itions (%)
 
 Syntactically?local positions    (mean=0.0984)Syntactically?distant postions  (mean=0.2124)
(b) WSJ
Figure 5: Probability ratio histogram of SLM to 4-gram
model for (a) BN task (b) WSJ task.
related to perplexity improvement? for each posi-
tion type in the figures.
Table 3, reports the perplexity performance of
each LM (baseline 4-gram, SLM and interpolated
SLM) on different positions of the evaluation data
for BN and WSJ tasks. As it can be observed from
this table, the use of long-span dependencies in the
SLM partially fills the gap between the performance
of the baseline 4-gram LM on syntactically-distant
positionsN versus syntactically-local positionsM.
In addition, it can be seen that the SLM by itself
fills the gap substantially, however, due to its under-
lying parameterization which is based on Jelinek-
Mercer smoothing it has a worse performance on
regular syntactically-local positions (which account
for the majority of the positions) compared to the
Kneser-Ney smoothed LM4. Therefore, to improve
the overall performance, the interpolated SLM takes
advantage of both the better modeling performance
of Kneser-Ney for syntactically-local positions and
4This is merely due to the superior modeling power and
better smoothing of the Kneser-Ney LM (Chen and Goodman,
1998).
Test Set 4-gram SLM 4-gram + SLM
Position BN
M 146 152 132
N 201 182 171
N +M 158 159 142
PPLN
PPLM
138% 120% 129%
WSJ
M 114 120 105
N 152 141 131
N +M 121 125 110
PPLN
PPLM
133% 117% 125%
Table 3: Perplexity on the BN and WSJ evaluation sets for
the 4-gram LM, SLM and their interpolation. The SLM
has lower perplexity than the 4-gram in syntactically dis-
tant positions N , and has a smaller discrepancy PPLNPPLM
between preplexity on the distant and local predictions,
complementing the 4-gram model.
the better features included in the SLM for improv-
ing predictive power on syntactically-distant posi-
tions.
6 Conclusion
The results of Table 1 and Figure 2 suggest that
predicting the next word is about 50% more diffi-
cult when its syntactic dependence on the history
reaches beyond n-gram range. They also suggest
that this difficulty does not diminish with increas-
ing training data size. If anything, the relative diffi-
culty of word positions with nonlocal dependencies
relative to those with local dependencies appears to
increase with increasing training data and n-gram
order. Finally, it appears that language models that
exploit long-distance syntactic dependencies explic-
itly at positions where the n-gram is least effective
are beneficial as complementary models.
Tables 2 and 3 demonstrates that a particular,
recently-proposed SLM with such properties im-
proves a 4-gram LM trained on a large corpus.
Acknowledgments
Thanks to Kenji Sagae for sharing his shift-reduce
dependency parser and the anonymous reviewers for
helpful comments.
57
References
LR Bahl. 1983. A maximum likelihood approach to
continuous speech recognition. IEEE Transactions
on Pattern Analysis and Machine Inteligence (PAMI),
5(2):179?190.
L. E. Baum. 1972. An equality and associated maxi-
mization technique in statistical estimation for proba-
bilistic functions of Markov processes. Inequalities,
3:1?8.
C. Chelba and F. Jelinek. 2000. Structured lan-
guage modeling. Computer Speech and Language,
14(4):283?332.
SF Chen and J Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report, Computer Science Group, Harvard Univer-
sity.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
M Collins, B Roark, and M Saraclar. 2005. Discrimina-
tive syntactic language modeling for speech recogni-
tion. In ACL.
Ahmad Emami and Frederick Jelinek. 2005. A Neu-
ral Syntactic Language Model. Machine learning,
60:195?227.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.
John Garofolo, Jonathan Fiscus, William Fisher, and
David Pallett, 1996. CSR-IV HUB4. Linguistic Data
Consortium, Philadelphia.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In EMNLP.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381?397.
S. Khudanpur and J. Wu. 2000. Maximum entropy tech-
niques for exploiting syntactic, semantic and colloca-
tional dependencies in language modeling. Computer
Speech and Language, pages 355?372.
H. K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and
L. Young-Suk. 2009. Syntactic features for Arabic
speech recognition. In Proc. ASRU.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):330.
Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur.
2011. Efficient discrimnative training of long-span
language models. In IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU).
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. EMNLP-CoNLL, volume 7, pages
1044?1050.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with Lookahead :
Can History-Based Models Rival Globally Optimized
Models ? In Proc. CoNLL, number June, pages 238?
246.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
Imed Zitouni. 2007. Backoff hierarchical class n-
gram language models: effectiveness to model unseen
events in speech recognition. Computer Speech &
Language, 21(1):88?104.
58
