Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2052?2063, Dublin, Ireland, August 23-29 2014.
Quality Estimation of English-French Machine Translation:
A Detailed Study of the Role of Syntax
Rasoul Kaljahi
??
, Jennifer Foster
?
, Raphael Rubino
??
, Johann Roturier
?
?
NCLT, School of Computing, Dublin City University, Ireland
{rkaljahi, jfoster, rrubino}@computing.dcu.ie
?
Symantec Research Labs, Dublin, Ireland
johann roturier@symantec.com
Abstract
We investigate the usefulness of syntactic knowledge in estimating the quality of English-French
translations. We find that dependency and constituency tree kernels perform well but the error
rate can be further reduced when these are combined with hand-crafted syntactic features. Both
types of syntactic features provide information which is complementary to tried-and-tested non-
syntactic features. We then compare source and target syntax and find that the use of parse trees
of machine translated sentences does not affect the performance of quality estimation nor does
the intrinsic accuracy of the parser itself. However, the relatively flat structure of the French
Treebank does appear to have an adverse effect, and this is significantly improved by simple
transformations of the French trees. Finally, we provide further evidence of the usefulness of
these transformations by applying them in a separate task ? parser accuracy prediction.
1 Introduction
Quality Estimation (QE) for Machine Translation (MT) involves judging the correctness of the output
of an MT system given an input and no reference translation (Blatz et al., 2003; Ueffing et al., 2003;
Specia et al., 2009). An accurate QE-for-MT system would mean that reliable decisions could be made
regarding whether to publish a machine translation as is or to re-direct it to a translator, either for post-
editing or to be translated from scratch. The scores produced by a QE system can also be used to choose
between translations, in a system combination framework or in n-best list reranking. The work presented
here takes place in the context of a wider study, the aim of which is to develop an English-French QE
system so that technical support material that is produced on a daily basis by a company?s English-
speaking customers can be translated automatically into French and made available with confidence to
the company?s French-speaking customer base.
It is reasonable to assume that syntactic features are useful in QE for MT as a way of capturing
the syntactic complexity of the source sentence, the grammaticality of the target translation and the
syntactic symmetry between the source sentence and its translation. This assumption has been borne out
by previous research which has demonstrated the usefulness of syntactic features for English-Spanish
QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role
of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002;
Moschitti, 2006), and by teasing apart the contribution of target and source syntax.
We find that both tree kernels and manually engineered features produce statistically significantly
better results than a strong set of non-syntactic features provided as a baseline by the organisers of the
2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic
features can be combined fruitfully with this baseline. Furthermore, we show that it is worthwhile to
combine tree kernels with hand-crafted features. Our tree kernel features are the complete set of tree
fragments of both the constituency and dependency trees of the source and target sentences. Our hand-
crafted feature set consists of an initial set of 489 constituency and dependency features which are then
reduced to a set of 144 with no significant loss in performance.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2052
We then show that source (English) constituency trees significantly outperform target (French) transla-
tion constituency trees in this task. We hypothesise that this is happening because a) the French parser has
a lower accuracy compared to the English, or b) the target trees sentences are harder to parse, represent-
ing, as they do, potentially ill-formed machine translations which may result in noisier parse trees which
are harder to learn from. If the first hypothesis were true, we would expect to see a drop in the accuracy
of our QE system when we use lower-accuracy parses. We do not observe this. If the second hypothesis
were true, we would expect to observe that the target trees were also less useful than the source trees in
the opposite translation direction (French-English). Instead, we find that the target (English) constituency
trees significantly outperform the source (French) constituency trees, suggesting that the difference be-
tween source and target that we observe in the original English-French experiment is related neither to
intrinsic parser accuracy nor to translation direction but rather to the languages/treebanks.
We explore the extent to which the difference between French and English constituency trees is due
to the relatively flatter structure of the French treebank. We use simple transformation heuristics to
introduce more nodes into the French trees and significantly improve the performance. We also apply
these heuristics in a second task, parser accuracy prediction. This task is similar to QE for MT except
we are predicting the quality of a parse tree in the absence of a reference parse tree. We also find here
that the modified trees also outperform the original trees, suggesting that one must proceed with caution
when using French Treebank tree fragments in a machine-learning task.
The paper?s novel contributions are as follows:
1. Evidence that syntactic information is useful in English-French QE for MT and further evidence
that it is useful in QE for MT in general
2. A comparison of two methods of representing syntactic information in QE
3. A more comprehensive set of syntactic features than has been previously been used in QE for MT
4. A comparison of the role of source and target syntax in English-French QE for MT
5. A set of heuristics that can be applied to French Treebank trees resulting in performance improve-
ments in the tasks of both QE for MT and parser accuracy prediction
The rest of this paper is organised as follows: we discuss related work in using syntax in QE in
Section 2, we describe the data in Section 3, and we then go on to describe the QE framework and the
systems built in Section 4. We follow this with an investigation of the role of source and target syntax in
Section 5 before presenting our heuristics to modify the French constituency trees in Section 6.
2 Related Work
Features extracted from parser output have been used before in training QE for MT systems. Quirk
(2004) uses a single syntax-based feature which indicates whether a full parse for the source sentence
could be found. Hardmeier et al. (2012) employ tree kernels to predict the 1-to-5 post-editing cost of a
machine-translated sentence. They use tree kernels derived from syntactic constituency and dependency
trees of the source side (English) and only dependency trees of the translation side (Spanish). The tree
kernels are used both alone and combined with non-syntactic features. The combined setting ranked
second in the 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore
a variety of syntactic features extracted from the output of both a hand-crafted broad-coverage gram-
mar/parser and a statistical constituency parser on the WMT 2012 data set. They find that the syntactic
features make an important contribution to the overall system. In a framework for combining QE and
automatic metrics to evaluate MT output, Specia and Gim?enez (2010) use part-of-speech (POS) tag lan-
guage model probabilities of the MT output 3-grams as features for QE and features built upon syntactic
chunks, dependencies and constituent structure to build automatic MT evaluation metrics. Avramidis
(2012) builds a series of models for estimating post-editing effort using syntactic features such as parse
probabilities and syntactic label frequency. In a similar vein, Gamon et al. (2005) use POS tag trigrams,
CFG rules and features derived from a semantic analysis of the MT output to classify it as fluent or
disfluent.
2053
In this work, we compare the use of tree kernels and hand-crafted features extracted from the con-
stituency and dependency trees of the source and target sides of a translation pair, as well as comparing
the role of source and target syntax. In addition, we conduct a more in-depth analysis of these approaches
and compare the utility of syntactic information extracted from the source side and target sides of the
translation.
3 Data
While there is evidence to suggest that predicting human evaluation scores is superior to predicting
automatic metrics in QE for ME (Quirk, 2004), it has also been shown that human judgements are not
necessarily consistent (Snover et al., 2006). A more practical consideration is that human evaluation
exists for just a few language pairs and domains. To the best of our knowledge, the only available
English-to-French data set which contains human judgements of translation quality are as follows:
? CESTA (Hamon et al., 2007), which is selected from the Official Journal of the European Commis-
sion and also from the health domain. In addition to the domain (and style) difference to newswire
(the domain on which our parsers are trained), a major stumbling block which prevents us from
using this data set is its small size: only 1135 segments have been evaluated manually.
? WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each
with approx. 5 translations) only half of which is in the news domain.
? FAUST
1
, which is out-of-domain and difficult to apply to our setting as the evaluations and post-
edits are user feedbacks, often in the form of phrases/fragments.
Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel
text for our language pair and domain. We use BLEU
2
(Papineni et al., 2002), TER
3
(Snover et al., 2006)
and METEOR
4
(Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics.
All metrics are applied at the segment level.
5
We randomly select 4500 parallel segments from the News development data sets released for the
WMT13 translation task (Bojar et al., 2013). In order to be independent of any one translation system,
we translate the data set with the following three systems and randomly choose 1500 distinct segments
from each:
? ACCEPT
6
: a phrase-based Moses system trained on training sets of WMT12 releases of Europarl
and News Commentary plus data from Translators Without Borders (TWB)
? SYSTRAN: a proprietary rule-based system
? Bing
7
: an online translation system
The data set is randomly split into 3000 training, 500 development and 1000 test segments. We use the
development set for tuning model parameters and building hand-crafted feature sets, and the test set for
testing model performance and analyses purposes.
4 Syntax-based QE
One way to employ syntactic information in a machine-learning task is to manually compile a set of
features that can be extracted automatically from a parse tree. An example of one such feature is the
label of the root of the tree. Another method is to directly use these trees in a tree kernel (Collins and
Duffy, 2002; Moschitti, 2006). This approach allows exponentially-sized feature spaces (e.g. all subtrees
1
http://www.faust-fp7.eu/faust/Main/DataReleases
2
Version 13a of MTEval script was used at the segment level.
3
TER COMpute 0.7.25: http://www.cs.umd.edu/
?
snover/tercom/
4
METEOR 1.4: http://www.cs.cmu.edu/
?
alavie/METEOR/
5
We present 1-TER to be more easily comparable to BLEU and METEOR. There is no upper bound for TER scores unlike
the other two metrics. Scores higher than 1 occur when the number of errors is higher than the segment length. To avoid this,
scores higher than 1 are cut-off to 1 before being converted to 1-TER.
6
http://www.accept.unige.ch/Products/D_4_1_Baseline_MT_systems.pdf
7
http://www.bing.com/translator
2054
of a tree) to be efficiently modelled using dynamic programming and has shown to be effective in many
natural language processing tasks including parsing and named entity recognition (Collins and Duffy,
2002), semantic role labelling (Moschitti, 2006), sentiment analysis (Wiegand and Klakow, 2010) and
QE for MT (Hardmeier et al., 2012). Although there can be overlap between the information captured by
the two approaches, each can capture information that the other one cannot. In addition, while tree ker-
nels involve minimal feature engineering, hand-crafted features offer more flexibility. Moschitti (2006)
shows that combining the two is beneficial. We use both hand-crafted features and tree kernels, applied
separately and combined together.
For parsing the English and French data into their constituency structures, a PCFG-LA parser
8
is
used. We train the English parser on the training section of the Wall Street Journal (WSJ) section of the
Penn Treebank (PTB) (Marcus et al., 1993). The French parser is trained on the training section of the
French Treebank (FTB) (Abeill?e et al., 2003). We obtain dependency parses by converting the English
constituency parses using the Stanford converter (de Marneffe and Manning, 2008) and the French
parses using Const2Dep (Candito et al., 2010). We evaluate the performance of the QE models using
Root Mean Square Error (RMSE) and Pearson correlation coefficient (r). To compute the statistical
significance of the performance differences between QE models, we use paired bootstrap resampling
following Koehn (2004). We randomly resample (with replacement) a set of N instances from the
predictions of each of the two given systems, where N is the size of the test set. We repeat this sampling
N times and count the number of times each of the two settings is better in terms of each measure (RMSE
and Pearson r). If a setting is better more than 95% of the time, we consider it statistically significant
at p < 0.05.
In the following sections, we first describe our baseline systems and then the quality estimation sys-
tems build using tree kernels, hand-crafted features and a combination of both.
4.1 Baseline QE Systems
In order to verify the usefulness of syntax-based QE, we build two baselines. The first baseline (BM) uses
the mean of the segment-level evaluation scores in the training set for all instances. In the second baseline
(BW), the 17 baseline features of the WMT12 QE Shared Task are used. BW is considered a strong baseline
as the system that used only these features was ranked higher than many of the participating systems.
We use support vector regression implemented in the SVMLight toolkit
9
to build BW. The Radial Basis
Function (RBF) kernel is used. The results for both baselines are presented in the first two rows of
Table 1. Since BW is a stronger baseline than BM, we will compare all syntax-based systems to BW only.
4.2 Syntax-based QE with Tree Kernels
Tree kernels are kernel functions that compute the similarity between two instances of data represented
as trees based on the number of common fragments between them. Therefore, the need for explicitly en-
coding an instance in terms of manually-designed and extracted features is eliminated, while benefitting
from a very high-dimensional feature space. Moschitti (2006) introduces an efficient implementation
of tree kernels within a support vector machine framework. Instead of extracting all possible tree frag-
ments, the algorithm compares only tree fragments rooted in two similar nodes. This algorithm is made
available through SVMLight-TK software
10
, which is used in this work.
In order to extract tree kernels from dependency trees, the labels on the arcs must be removed. Fol-
lowing Tu et al. (2012), the nodes in the resulting tree representation are word forms and dependency
relations, omitting POS tag information. An example is shown in Figure 1. A word is a child of its
dependency relation to its head. The dependency relation in turn is the child of the head word. This
continues until the root of the tree.
Based on preliminary experiments on our development set, we use subset tree kernels, where the tree
fragments are subtrees rooted at any node in the tree so that no production rule expanding a node in the
8
https://github.com/CNGLdlab/LORG-Release. The Lorg parser is very similar to the Berkeley parser (Petrov
et al., 2006), the main difference being its unknown word handling mechanism (Attia et al., 2010).
9
http://svmlight.joachims.org/
10
http://disi.unitn.it/moschitti/Tree-Kernel.htm
2055
BLEU 1-TER METEOR
RMSE r RMSE r RMSE r
BM 0.1626 0 0.1965 0 0.1657 0
BW 0.1601 0.1766 0.1949 0.1565 0.1625 0.2047
TK 0.1581 0.2437 0.1888 0.2774 0.1595 0.2715
BW+TK 0.1570 0.2696 0.1879 0.2939 0.1576 0.3111
HC 0.1603 0.1998 0.1913 0.2365 0.1610 0.2516
BW+HC 0.1587 0.2418 0.1899 0.2611 0.1585 0.2964
SyQE 0.1577 0.2535 0.1887 0.2797 0.1594 0.2743
BW+SyQE 0.1568 0.2802 0.1879 0.2937 0.1576 0.3127
Table 1: QE performances measured by RMSE and Pearson r; BM: Mean baseline, BW: WMT 17 base-
line features, TK: tree kernels, HC: hand-crafted features, SyQE: full syntax-based systems (TK+HC).
Statistically significantly better scores compared to their counterpart (upper row in the row block) are in
bold.
rootcamecc      advmod      nsubj      punctAnd       then           era            .                      det      amod                        the    American 
Figure 1: Tree Kernel Representation of Dependency Structure for And then the American era came.
subtree is split. Unlike subtree kernels, subset tree kernels allow tree fragments with non-terminals as
leaves. We tune the C parameter for Pearson r on the development set, with all other parameters left as
default.
We build a system with all four parse trees for every training instance, which includes the constituency
and dependency trees of the source and target side of the translation. The third row of Table 1 shows
the performance of this system which is named TK. The results achieved using this system represent a
statistically significant improvement over the BW baseline results. In order to examine their complemen-
tarity, we combine these tree kernels and the baseline features (BW+TK) in the fourth row of Table 1.
This combined system performs better than the two individual systems.
While BLEU prediction is the most accurate (lowest RMSE), METEOR prediction appears to be the
easiest to learn (highest Pearson r). TER prediction seems to be more difficult than BLEU and METEOR
prediction, especially in terms of prediction error. This is probably related to the distribution of each of
these metric scores in our data set. The standard deviations (?) of BLEU, TER and METEOR scores are
0.1620, 0.1943 and 0.1652 respectively. The substantially higher ? of TER scores makes them harder to
predict accurately leading to higher prediction error.
4.3 Syntax-based QE with Hand-crafted Features
We design a set of constituency and dependency feature types, some of which have previously been used
by the works described in Section 2 and some introduced here. Each feature type contains at least two
features, one extracted from the source and the other from the translation. Numerical feature types can
be further instantiated by extracting the ratio and differences between the source and target side feature
values. Some feature types are parametric meaning that they can be varied by changing the value of a pa-
rameter. For example, the non-terminal label is a parameter for the non-terminal-label-count
2056
Constituency
?1 Label of the root node of the constituency tree
2 Height of the constituency tree which is the number of edges from root node to the farthest terminal (leaf) node
?3 Number of nodes in the constituency tree
4 Log probability of the constituency parse assigned by the parser
?5 Parseval F
1
score of the tree with respect to a tree produced by the Stanford parser (Klein and Manning, 2003)
?6 Right hand side of the CFG production rule expanding the root node
7 All non-lexical and lexical CFG production rules expanding the tree nodes
?8 Average arity of the non-lexical CFG production rules expanding the constituency tree nodes
9 Counts of each non-terminal label in the tree
?10 POS unigrams, 3-grams and 5-grams
11 POS n-gram scores against language models trained on the POS tags of the respective treebanks using the SRILM
toolkit (http://www.speech.sri.com/projects/srilm/) with Witten-Bell smoothing
?12 Counts of each 12 universal POS tags (Petrov et al., 2012)
?13 Location of the first verb in the sentence in terms of the token distance from the beginning
?14 Average number of POS n-grams in each n-gram frequency quartile of the POS corpora of the respective treebanks
Dependency
?1 POS tag of the top node (dependent of the dummy root node) of the dependency tree
?2 Number of dependents of the top node
?3 Sequence of all dependency relations which modify the top node
?4 Sequence of the POS tags of the dependents of the top node
?5 Average number of dependents per node
?6 Height of the tree computed in the same way for the constituency tree
?7 3- and 5-gram sequences of dependency relations of the tokens to their head
?8 Number of most frequent dependency relations in our News training set
?9 Dependency relation n-gram scores against language models trained on the respective treebanks for each language
?10 Average number of dependency relation n-grams in each n-gram frequency quartile of the respective treebanks
?11 Pairs of tokens and their dependency relations to their head
Table 2: Constituency and dependency feature types
feature type. Therefore, it instantiates as several features, one for each non-terminal-label.
As in BW, we use support vector machines (SVM) to build the QE systems using these hand-crafted
features. We keep only those features which fire for more than a threshold which is set empirically on
the development set. Table 2 lists our syntax-based feature types and their descriptions. Those that have,
to the best of our knowledge, not been used in QE for MT before are marked with an asterisk.
The total number of feature-value pairs in the full feature set is 489. Since this feature set is large
and contains many sparse features, we attempt to reduce it through ablation experiments in which we
directly compare the effect of leaving out features that we suspect may be redundant. For example, we
investigate whether either the ratio or difference of the source and target numerical features or both of
them are redundant by building three systems, one without ratio features, one without difference features
and one with neither. This process is also carried out for log probability and perplexity features, original
and universal POS-tag-based features, n-gram and language model score features, lexical and non-lexical
CFG rules, and n-gram orders (i.e. 3-gram vs. 5-gram features). This process proved useful: we found,
for example, that either 3- or 5-grams worked better than both together and features based on universal
POS tags better than those based on original POS tags.
The final reduced feature set contains 144 features-value pairs. We build one QE system with all 489
features HC-all and one with the reduced set of 144 features HC . Table 3 compares the performance on
the development and test set. The system with the reduced feature set performs consistently better than
the HC-all system on the development set, mostly with statistically significant differences. However,
on the test set, the performance degrades albeit not statistically significantly. Considering a more than
70% reduction in feature set size, this relatively small degradation is tolerable. We use the reduced
feature set as our hand-crafted feature set for the rest of the work.
Compared to TK in Table 1 (third and fourth versus fifth and sixth rows), the performances are lower
for all MT metrics, though not statistically significantly. It is worth noting that we observed an opposite
2057
BLEU 1-TER METEOR
RMSE r RMSE r RMSE r
Development Set
HC-all 0.1567 0.3026 0.1851 0.2746 0.1575 0.2996
HC 0.1540 0.3398 0.1819 0.3263 0.1547 0.3452
Test Set
HC-all 0.1603 0.2108 0.1902 0.2510 0.1607 0.2493
HC 0.1603 0.1998 0.1913 0.2365 0.1610 0.2516
Table 3: QE performance with all hand-crafted syntactic features HC-all and the reduced feature set
HC. Statistically significantly better scores compared to their counterpart (upper row) are in bold.
RMSE r
TK-CD-ST 0.1581 0.2437
TK-CD-S 0.1584 0.2294
TK-CD-T 0.1597 0.2101
TK-C-S 0.1583 0.2312
TK-C-T 0.1608 0.1479
TK-D-S 0.1598 0.1869
TK-D-T 0.1598 0.2102
Table 4: BLEU prediction performances with tree kernels of only source S or translation T side trees.
The scores in bold are statistically better than their counterparts in the same row block. The original
result with source and target combined is provided for reference in the first row.
behaviour on the development set, where hand-crafted features largely outperform tree kernels. This
suggests that the tree kernels are more generalisable. We also combine these features with the WMT
17 baseline features (BW+HC). This combination also improves over both syntax-based and baseline
systems, confirming again the usefulness of syntactic information in addition to surface features.
We combine tree kernels and hand-crafted features to build a full syntax-based QE system (SyQE),
which improves over both TK and HC (Table 1) . The improvements for TER and METEOR prediction
are slight but statistically significant for BLEU prediction. This system is also combined with BW in
BW+SyQE (the last row of Table 1), resulting in statistically significant gains for all metrics.
5 Source and Target Syntax in Syntax-based QE
We now turn our attention to the parts played by source and target syntax in QE for MT. To save space,
we present only the BLEU scores for the tree kernel systems. Table 4 shows the results achieved by
systems built using either the source or target side of the translations.
At a glance, it can be seen that the source side constituency tree kernels outperform the target side
ones, while the opposite is the case for dependency tree kernels. The differences for constituency trees
are however substantially bigger. When both constituency and dependency trees are combined, the source
side trees perform better (TK-CD-S vs. TK-CD-T).
The following three hypotheses could explain this difference between TK-C-S and TK-C-T:
1. The Role of Parser Accuracy: The fact that French parsing models do not reach the high Parseval
F1s achieved by English parsing models could explain the difference in usefulness between the
French and English consistuency trees. On the standard parsing test sets, the English parsing model
achieves an F1 of 89.6 and the French an F
1
of 83.4.
2. Parsing Machine Translation Output: The difference between the source and target could be
happening because the target side is machine translation output and (presumably) represents a lower
2058
Sombre  Matter  Affects  de  vol  Probes   spatiale
NP
NP
NP
PP
SENT
NC
ET ET P
ET ET ET
Figure 2: Parse tree of the machine translation of Dark Matter Affects Flight of Space Probes to French
quality set of sentences than the source (see Figure 2 for an example of a parse tree for a poor
translation).
3. Differences in Annotation Strategies: The difference between the source and target could be due
to the idiosyncrasies of the underlying treebanks which is not carried over via the conversion tools
to the dependency structure.
Hypotheses 1 and 2 relate the usefulness of parse trees in QE to the intrinsic quality of the parse trees.
French constituency trees are less accurate than English ones, either because the French parsing model
is not as accurate as the English one (Hypothesis 1) or because the possibly ungrammatical nature of the
French parsing input adversely affects the quality of the parse tree (Hypothesis 2). Although this low
quality would be expected to affect the dependency trees in the same way since they are directly derived
from the consistency trees, this is not the case and it appears that the problematic aspects of the French
parses are abstracted away from the dependency trees.
To test the first hypothesis, we investigate the role of parser accuracy in QE. For both languages, we
substitute the standard parsing models used in all our prior experiments with ?lower-accuracy? mod-
els trained using only a fraction of the training data (following Quirk and Corston-Oliver (2006)). The
English parsing model achieves an F
1
of 72.5 and the French an F
1
of 66.5, representing drops of ap-
proximately 17 points from the original models. The RMSE and Pearson r of the new QE model are
0.1583 and 0.2350 compared to 0.1581 and 0.2437 of the one trained with original trees (see also the
third row of Table 1). These results show that the use of these lower-accuracy models has only a minimal
and statistically insignificant effect on QE performance, suggesting that intrinsic parser accuracy is not
the reason why the target constituency trees are less useful than the source constituency trees.
11
To investigate the second hypothesis, we switch the translation direction to French-to-English. There-
fore, we now parse the well-formed French input sentences and the machine-translated English segments.
If the second hypothesis were true, the target side parse trees in this direction would still underperform
the source side ones. The results are shown in Table 5. All the systems using target trees outperform
those using source trees. The difference between source and target in the models that use constituency
trees is especially substantial and statistically significant. Thus, it is apparent that the suspected lower
quality of constituency parse trees of MT output is not the reason for the lower QE performance.
We now seek the answer in our third hypothesis, i.e. in the difference between the annotation schemes
of the PTB and the FTB. One major difference, noted by, for example, Schluter and van Genabith (2007),
is that the FTB has a relatively flatter structure. It lacks a verb phrase (VP) node and phrases modifying
the verb are the sibling of the verb nucleus. We investigate this further in the next section.
6 Modifying French Parse Trees
In order to test whether the annotation strategy is a reason for the lower performance of French con-
stituency tree kernels, we apply a set of three heuristics which introduce more structure to the French
parse trees (1&2) or simply make them more PTB-like (3):
? Heuristic 1 automatically adds a VP node above the verb node (VN) and at most 3 of its immediate
adjacent nodes if they are noun or prepositional phrases (NP or PP).
11
See (Kaljahi et al., 2013) for a more detailed exploration of the role of parser accuracy in QE for MT.
2059
RMSE r
TK-FE/CD-ST 0.1561 0.2334
TK-FE/CD-S 0.1574 0.1830
TK-FE/CD-T 0.1559 0.2423
TK-FE/C-S 0.1581 0.1578
TK-FE/C-T 0.1556 0.2336
TK-FE/D-S 0.1577 0.1655
TK-FE/D-T 0.1579 0.1886
Table 5: BLEU prediction performances with tree kernels for Fr-En direction (FE) (C: constituency, D:
dependency, S: source, T: translation)
RMSE r
TK-C-T 0.1608 0.1479
TK-C-T
m
0.1591 0.2143
TK-CD-ST 0.1581 0.2437
TK-CD-ST
m
0.1574 0.2609
Table 6: QE with tree kernels using original and modified French trees (
m
)
? Heuristic 2 stratifies some of the production rules in the tree by grouping together every two equal
adjacent POS tags under a new node with a tag made of the POS tag suffixed with St.
? Heuristic 3 moves coordinated nodes (the immediate left sibling of the COORD node) under COORD.
Figure 3 shows examples of the application of each of these methods. We apply these heuristics to
the parsed MT output in the English-French translation direction and rebuild the tree kernel system with
translation side constituency trees (TK-C-T) and the full tree kernel system (TK-CD-ST) with the mod-
ified trees. The results are presented in Table 6. Despite the possibility of introducing linguistic errors,
these heuristics yield a statistically significant improvement in QE performance. Unsurprisingly, the
changes are bigger for the system with only translation side constituency trees as in the full system there
are three other tree types involved. These results suggest that the structure of the French constituency
trees is a factor in the lower performance of its tree kernels in QE.
12
The gain achieved by applying these heuristics is related to the fact that there are more similar frag-
ments extracted from the modified structure which are useful for the tree kernel system. For example, in
the original top left tree in Figure 3, there is no chance that a fragment consisting only of VN and NP ?
a very common structure and thus useful in calculating tree similarity ? will be extracted by the subset
tree kernel. The reason is that this kernel type does not allow the production rule to be split (in this case
the rule expanding the S node). However, after applying Heuristic 1, the fragment equivalent to VP ->
VN NP production rule can be easily extracted. Among the three heuristics, the first one contributes the
largest part of the improvement; the other two have a very slight effect according to the results of their
individual application, though they contribute to the overall performance when all three are combined.
The success of using modified French trees in improving tree kernel performance may of course de-
pend on the data set and even the task in hand, and may not be generalisable. We next explore this
question by applying the modification to a different task and a different data set.
6.1 Parser Accuracy Prediction
The task we choose is parser accuracy prediction, the aim of which is to predict the accuracy of a parse
tree without a reference (QE for parsing). The task was previously explored for English by Ravi et al.
12
We also see a slightly smaller improvement for the hand-crafted features using the modified French trees. The combina-
tion of tree kernels and hand-crafted features with the modified trees leads to a statistically significant improvement over the
combination with the original trees.
2060
Figure 3: Application of tree modification heuristics on example French translation parse trees
RMSE r
PAP 0.1239 0.4035
PAP
m
0.1233 0.4197
Table 7: Parser Accuracy Prediction (PAP) performance with tree kernels using original and modified
French trees (
m
)
(2008). We build a tree kernel model to predict the accuracy of French parses. To train the system, we
parse the training section of FTB with our French parser and score them using F
1
. We use the FTB
development set to tune the SVM C parameter and test the model on the FTB test set. Two parser
accuracy prediction models are then built using this setting, one with the original parse trees and the
second with the modified parse trees produced using the three heuristics listed above. The results are
presented in Table 7.
Both RMSE and Pearson r improve with the modified trees, where the r improvement is statistically
significant. Although the improvement we observe is not as large as the one we observed for the QE for
MT task, the results add weight to our claim that the structure of the FTB trees should be optimised for
use in tree kernel learning.
7 Conclusion
We analysed the utility of syntactic information in QE of English-French MT and found it useful both
individually and combined with standard QE features. We found that tree kernels are a convenient and
effective way of encoding syntactic knowledge but that our hand-crafted feature set also brings additional,
useful information. As a result of comparing the role of source and target syntax, we also found that the
constituent structure in the FTB could be amended to be more useful in QE for MT and parser accuracy
prediction. Now that we have explored the role of syntax in this project, our next step is try to further
improve our QE system by adding semantic information. However, there are many other ways in which
the research in this paper could be further extended. Our focus is on the language pair English-French
and the QE task but it would certainly be interesting to perform a similar analysis on the role of syntax
in QE for other language pairs, or to investigate the impact of French tree modification on other tasks.
2061
Acknowledgments
This research has been supported by the Irish Research Council Enterprise Partnership Scheme (EP-
SPG/2011/102 and EPSPD/2011/135) and the computing infrastructure of the Centre for Next Gener-
ation Localisation at Dublin City University. We are grateful to Djam?e Seddah for useful discussions
about the French Treebank. We also thank the reviewers for their helpful comments.
References
Anne Abeill?e, Lionel Cl?ement, and Franc?ois Toussenel. 2003. Building a treebank for French. In Treebanks:
Building and Using Syntactically Annotated Corpora. Kluwer Academic Publishers.
Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi, and Josef van Genabith. 2010.
Handling unknown words in statistical latent-variable parsing models for Arabic, English and French. In Pro-
ceedings of the 1st Workshop on Statistical Parsing of Morphologically Rich Languages.
Eleftherios Avramidis. 2012. Quality estimation for machine translation output using linguistic analysis and
decoding features. In Proceedings of WMT.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2003. Confidence estimation for machine translation. In JHU/CLSP Summer Workshop Final
Report.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 workshop on statistical machine
translation. In Proceedings of the 8th WMT.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation,
pages 136?158.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh WMT.
Marie Candito, Benot Crabb, and Pascal Denis. 2010. Statistical French dependency parsing: treebank conversion
and first results. In Proceedings of LREC.
Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of the ACL.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of WMT.
Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-level MT evaluation without reference trans-
lations: beyond language modeling. In EAMT.
Olivier Hamon, Antony Hartley, Andr?ei Popescu-Belis, and Khalid Choukri. 2007. Assessing human and auto-
mated quality judgments in the french MT evaluation campaign CESTA. In Proceedings of the MT Summit.
Christian Hardmeier, Joakim Nivre, and J?org Tiedemann. 2012. Tree kernels for machine translation quality
estimation. In Proceedings of the WMT.
Rasoul Samed Zadeh Kaljahi, Jennifer Foster, Raphael Rubino, Johann Roturier, and Fred Hollowood. 2013.
Parser accuracy in quality estimation of machine translation: A tree kernel approach. In Proceedings of IJCNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the ACL.
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
English: the Penn Treebank. Computational Linguistics, 19(2):313?330.
Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In Proceedings of EACL.
2062
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of the ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact and interpretable
tree annotation. In Proceedings of the 21st COLING-ACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of
LREC.
Chris Quirk and Simon Corston-Oliver. 2006. The impact of parse quality on syntactically-informed statistical
machine translation. In Proceedings of EMNLP.
Chris Quirk. 2004. Training a sentence-level machine translation confidence measure. In Proceedings of LREC.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Automatic prediction of parser accuracy. In Proceedings of
EMNLP.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Johann Roturier, Rasoul Kaljahi, and Fred Hollowood. 2012.
DCU-Symantec submission for the WMT 2012 quality estimation task. In Proceedings of WMT.
Natalie Schluter and Josef van Genabith. 2007. Preparing, restructuring, and augmenting a french treebank:
Lexicalised parsers or coherent treebanks? In Proceedings of the 10th Conference of the Pacific Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In Proceedings of AMTA.
Lucia Specia and Jes?us Gim?enez. 2010. Combining confidence estimation and reference-based metrics for seg-
ment level mt evaluation. In Proceedings of AMTA.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc Dymetman, and Nello Cristianini. 2009. Estimating the
sentence-level quality of machine translation systems. In EAMT, pages 28?35.
Zhaopeng Tu, Yifan He, Jennifer Foster, Josef van Genabith, Qun Liu, and Shouxun Lin. 2012. Identifying high-
impact sub-structures for convolution kernels in document-level sentiment classification. In Proceedings of the
ACL.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003. Confidence measures for statistical machine transla-
tion. In Machine Translation Summit IX.
Michael Wiegand and Dietrich Klakow. 2010. Convolution kernels for opinion holder extraction. In Proceedings
of NAACL-HLT.
2063
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 138?144,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DCU-Symantec Submission for the WMT 2012 Quality Estimation Task
Raphael Rubino??, Jennifer Foster?, Joachim Wagner?,
Johann Roturier?, Rasul Samad Zadeh Kaljahi??, Fred Hollowood?
?Dublin City University, ?Symantec, Ireland
?firstname.lastname@computing.dcu.ie
?firstname lastname@symantec.com
Abstract
This paper describes the features and the ma-
chine learning methods used by Dublin City
University (DCU) and SYMANTEC for the
WMT 2012 quality estimation task. Two sets
of features are proposed: one constrained, i.e.
respecting the data limitation suggested by the
workshop organisers, and one unconstrained,
i.e. using data or tools trained on data that was
not provided by the workshop organisers. In
total, more than 300 features were extracted
and used to train classifiers in order to predict
the translation quality of unseen data. In this
paper, we focus on a subset of our feature set
that we consider to be relatively novel: fea-
tures based on a topic model built using the
Latent Dirichlet Allocation approach, and fea-
tures based on source and target language syn-
tax extracted using part-of-speech (POS) tag-
gers and parsers. We evaluate nine feature
combinations using four classification-based
and four regression-based machine learning
techniques.
1 Introduction
For the first time, the WMT organisers this year pro-
pose a Quality Estimation (QE) shared task, which
is divided into two sub-tasks: scoring and ranking
automatic translations. The aim of this workshop is
to define useful sets of features and machine learn-
ing techniques in order to predict the quality of a
machine translation (MT) output T (Spanish) given
a source segment S (English). Quality is measured
using a 5-point likert scale which is based on post-
editing effort, following the scoring scheme:
1. The MT output is incomprehensible
2. About 50-70% of the MT output needs to be
edited
3. About 25-50% of the MT output needs to be
edited
4. About 10-25% of the MT output needs to be
edited
5. The MT output is perfectly clear and intelligi-
ble
The final score is a combination of the scores as-
signed by three evaluators. The use of a 5-point scale
makes the scoring task more difficult than a binary
classification task where a translation is considered
to be either good or bad. However, if the task is
successfully carried out, the score produced is more
useful.
Dublin City University and Symantec jointly ad-
dress the scoring task. For each pair (S, T ) of source
segment S and machine translation T , we train three
classifiers and one classifier combination using the
training data provided by the organisers to predict
5-point Likert scores. In this paper, we present the
classification results on the test set alng with addi-
tional results obtained using regression techniques.
We evaluate the usefulness of two new sets of fea-
tures:
1. topic-based features using Latent Dirichlet Al-
location (LDA (Blei et al, 2003)),
2. syntax-based features using POS taggers and
parsers (Wagner et al, 2009)
The remainder of this paper is organised as fol-
lows. In Section 2, we give an overview of all the
138
features employed in our QE system. Then, in Sec-
tion 3, we describe the topic and syntax-based fea-
tures in more detail. Section 4 presents the vari-
ous classification and regression techniques we ex-
plored. Our results are presented and discussed in
Section 5. Finally, we summarise and outline our
plans in Section 6.
2 Features Overview
In this section, we describe the features used in our
QE system. In the first subsection, the features in-
cluded in our constrained system are presented. In
the second subsection, we detail the features in-
cluded in our unconstrained system. Both of these
systems include the 17 baseline features provided
for the shared task.
2.1 Constrained System
The constrained system is based only on the data
provided by the organisers. We extracted 70 fea-
tures in total (including the baseline features) and
we present them here according to the type of infor-
mation they capture.
Word and Phrase-Level Features
? Ratio of source and target segment length:
the number of source words divided by the
number of target words
? Ratio of source and target number of punc-
tuation marks: the number of source punctua-
tion marks divided by the number of target ones
? Number of phrases comprising the MT out-
put: given a phrase-table, we assume that a
sentence composed of several phrases indicates
uncertainty on the part of the MT system.
? Average length of source and target phrases:
concatenating short phrases may result in lower
fluency compared to the use of longer ones.
? Ratio of source and target averaged phrase
length
? Number of source prepositions and conjunc-
tions word: our assumption here is that seg-
ments containing a relatively high number of
prepositions and conjunctions may be more
complex and difficult to translate.
? Number of source out-of-vocabulary words
Language Model Features
All the language models (LMs) used in our work
are n-gram LMs with Kneser-Ney smoothing built
with the SRI Toolkit (Stolcke, 2002).
? Backward 2-gram and 3-gram source and
target log probabilities: as proposed by
Duchateau et al (2002)
? Log probability of target segments on
5-gram MT-output-based LM: using
MOSES (Koehn et al, 2007) trained on the
provided parallel corpus, we translated the En-
glish side of this corpus into Spanish, assuming
that the MT output contains mistakes. This
MT output is used to build a LM that models
the behavior of the MT system. We assume
that for a given MT output, a high n-gram
probability (or a low perplexity) of the LM
indicates that the MT output contains mistakes.
MT-system Features
? 15 scores provided by Moses: phrase-table,
language model, reordering model and word
penalty (weighted and unweighted)
? Number of n-bests for each source segment
? MT output back-translation: from Spanish to
English using MOSES trained on the provided
parallel corpus, scored with TER (Snover et
al., 2006), BLEU (Papineni et al, 2002) and
the Levenshtein distance (Levenshtein, 1966),
based on the source segments as a translation
reference
Topic Model Features
? Probability distribution over topics: Source
and target segment probability distribution over
topics for a 10-dimension topic model
? Cosine distance between source and target
topic vectors
More details about these two features are provided
in Section 3.1.
2.2 Unconstrained System
In addition to the features used for the constrained
system, a further 238 unconstrained features were
included in our unconstrained system.
139
MT System Features
As for our constrained system, we use MT output
back-translation from Spanish to English, but this
time using Bing Translator1 in addition to Moses.
Each back-translated segment is scored with TER,
BLEU and the Levenshtein distance, based on the
source segments as a translation reference.
Source Syntax Features
Wagner et al (2007; 2009) propose a series of
features to measure sentence grammaticality. These
features rely on a part-of-speech tagger, a probabilis-
tic parser and a precision grammar/parser. We have
at our disposal these tools for English and so we ap-
ply them to the source data. The features themselves
are described in more detail in Section 3.2.
Target Syntax Features
We use a part-of-speech tagger trained on Spanish
to extract from the target data the subset of grammat-
icality features proposed by Wagner et al (2007;
2009) that are based on POS n-grams. In addition
we extract features which reflect the prevalence of
particular POS tags in each target segment. These
are explained in more detail in Section 3.2 below.
Grammar Checker Features
LANGUAGETOOL (based on (Naber, 2003)) is an
open-source grammar and style proofreading tool
that finds errors based on pre-defined, language-
specific rules. The latest version of the tool can
be run in server mode, so individual sentences can
be checked and assigned a total number of errors
(which may or may not be true positives).2 This
number is used as a feature for each source segment
and its corresponding MT output.
3 Topic and Syntax-based Features
In this section, we focus on the set of features
that aim to capture adequacy using topic modelling
and grammaticality using POS tagging and syntactic
parsing.
1http://www.microsofttranslator.com/
2The list of English and Spanish rules is available at:
http://languagetool.org/languages.
3.1 Topic-based Features
We extract source and target features based on a
topic model built using LDA. The main idea in topic
modelling is to produce a set of thematic word clus-
ters from a collection of documents. Using the par-
allel corpus provided for the task, a bilingual corpus
is built where each line is composed of a source seg-
ment and its translation separated by a space. Each
pair of segments is considered as a bilingual docu-
ment. This corpus is used to train a bilingual topic
model after stopwords removal. The resulting model
is one set of bilingual topics z containing words w
with a probability p(wn|zn, ?) (with n equal to the
vocabulary size in the whole parallel corpus). This
model can be used to infer the probability distri-
bution of unseen source and target segments over
bilingual topics. During the test step, each source
segment and its translation are considered individu-
ally, as two monolingual documents. This method
allows us to compare the source and target topic dis-
tributions. We assume that a source segment and its
translation share topic similarities.
We propose two ways of using topic-based fea-
tures for quality estimation: keeping source and tar-
get topic vectors as two sets of k features, or com-
puting a vector distance between these two vectors
and using one feature only. To measure the prox-
imity of two vectors, we decided to used the Co-
sine distance, as it leads to the best results in terms
of classification accuracy. However, we plan to
study different metrics in further experiments, like
the Manhattan or the Euclidean distances. Some
parameters related to LDA have to be studied more
carefully too, such as the number of topics (dimen-
sions in the topic space), the number of words per
topic, the Dirichlet hyperparameter ?, etc. In our
experiments, we built a topic model composed of 10
dimensions using Gibbs sampling with 1000 itera-
tions. We assume that a higher dimensionality can
lead to a better repartitioning of the vocabulary over
the topics.
Multilingual LDA has been used before in nat-
ural language processing, e.g. polylingual topic
models (Mimno et al, 2009) or multilingual topic
models for unaligned text (Boyd-Graber and Blei,
2009). In the field of machine translation, Tam et
al. (2007) propose to adapt a translation and a lan-
140
guage model to a specific topic using Latent Se-
mantic Analysis (LSA, or Latent Semantic Index-
ing, LSI (Deerwester et al, 1990)). More recently,
some studies were conducted on the use of LDA to
adapt SMT systems to specific domains (Gong et al,
2010; Gong et al, 2011) or to extract bilingual lexi-
con from comparable corpora (Rubino and Linare`s,
2011). Extracting features from a topic model is, to
the best of our knowledge, the first attempt in ma-
chine translation quality estimation.
3.2 Syntax-based Features
Syntactic features have previously been used in MT
for confidence estimation and for building automatic
evaluation measures. Corston-Oliver et al (2001)
build a classifier using 46 parse tree features to pre-
dict whether a sentence is a human translation or MT
output. Quirk (2004) uses a single parse tree feature
in the quality estimation task with a 4-point scale,
namely whether a spanning parse can be found, in
addition to LM perplexity and sentence length. Liu
and Gildea (2005) measure the syntactic similarity
between MT output and reference translation. Al-
brecht and Hwa (2007) measure the syntactic simi-
larity between MT output and reference translation
and between MT output and a large monolingual
corpus. Gimenez and Marquez (2007) explore lexi-
cal, syntactic and shallow semantic features and fo-
cus on measuring the similarity of MT output to ref-
erence translation. Owczarzak et al (2007) use la-
belled dependencies together with WordNet to avoid
penalising valid syntactic and lexical variations in
MT evaluation. In what follows, we describe how
we make use of syntactic information in the QE task,
i.e. evaluating MT output without a reference trans-
lation.
Wagner et al (2007; 2009) use three sources
of linguistic information in order to extract features
which they use to judge the grammaticality of En-
glish sentences:
1. For each POS n-gram (with n ranging from 2 to
7), a feature is extracted which represents the
frequency of the least frequent n-gram in the
sentence according to some reference corpus.
TreeTagger (Schmidt, 1994) is used to produce
POS tags.
2. Features provided by a hand-crafted, broad-
coverage precision grammar of English (Butt
et al, 2002) and a Lexical Functional Grammar
parser (Maxwell and Kaplan, 1996). These in-
clude whether or not a sentence could be parsed
without resorting to robustness measures, the
number of analyses found and the parsing time.
3. Features extracted from the output of three
probabilistic parsers of English (Charniak and
Johnson, 2005), one trained on Wall Street
Journal trees (Marcus et al, 1993), one trained
on a distorted version of the treebank obtained
by automatically creating grammatical error
and adjusting the parse trees, and the third
trained on the union of the original and dis-
torted versions.
These features were originally designed to distin-
guish grammatical sentences from ungrammatical
ones and were tested on sentences from learner cor-
pora by Wagner et al (2009) and Wagner (2012).
In this work we extract all three sets of features
from the source side of our data and the POS-based
subset from the target side.3 We use the publicly
available pre-trained TreeTagger models for English
and Spanish4. The reference corpus used to obtain
POS n-gram frequences is the MT translation model
training data.5
In addition to the POS-based features described in
Wagner et al (2007; 2009), we also extract the fol-
lowing features from the Spanish POS-tagged data:
for each POS tag P and target segment T , we ex-
tract a feature which is the proportion of words in
T that are tagged as P . Two additional features are
extracted to represent the proportion of words in T
that are assigned more than one tag by the tagger,
3Unfortunately, due to time constraints, we were unable to
source a suitable probabilistic phrase-structure parser and a pre-
cision grammar for Spanish and were thus unable to extract
parser-based features for Spanish. We expect that these features
would be more useful on the target side than the source side.
4http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
5To aid machine learning methods that linearly combine fea-
ture values, we add binarised features derived from the raw XLE
and POS n-gram features described above, for example we add
a feature indicating whether the frequency of the least frequent
POS 5-gram is below 10. We base the choice of binary fea-
tures on (a) decision rules observed in decision trees trained for
a binary scoring task and (b) decision rules of simple classifiers
(decision trees with just one decision node and 2 leaf nodes)
that form a convex hull of optimal classifiers in ROC space.
141
and the proportion of words in T that are unknown
to the tagger.
4 Machine Learning
In this section, we describe the machine learning
methods that we experimented with. Our final sys-
tems submitted for the shared task are based on clas-
sification methods. However, we also performed
some experiments with regression methods.
We evaluate the systems on the test set using the
official evaluation script and the reference scores.
We report the evaluation results as Mean Aver-
age Error (MAE) and Root Mean Squared Error
(RMSE).
4.1 Classification
In order to apply classification algorithms to the
set of features associated with each source and tar-
get segment, we rounded the training data scores
to the closest integer. We tested several classifiers
and empirically chose three algorithms: Support
Vector Machine using sequential minimal optimiza-
tion and RBF kernel (parameters optimized by grid-
search) (Platt, 1999), Naive Bayes (John and Lang-
ley, 1995) and Random Forest (Breiman, 2001) (the
latter two techniques were applied with default pa-
rameters). We use the Weka toolkit (Hall et al,
2009) to train the classifiers and predict the scores
on the test set. Each method is evaluated individu-
ally and then combined by averaging the predicted
scores.
4.2 Regression
We applied three different regression techniques:
SVM epsilon-SVR with RBF kernel, Linear Regres-
sion and M5P (Quinlan, 1992; Wang and Witten,
1997). The two latter algorithms were used with
default parameters, whereas SVM parameters (?, c
and ) were optimized by grid-search. We also per-
formed a combination of the three algorithms by av-
eraging the predicted scores. We apply a linear func-
tion on the predicted scores S in order to keep them
in the correct range (from 1 to 5) as detailed in (1),
where S? is the rescaled sentence score, Smin is the
lowest predicted score and Smax is the highest pre-
dicted score.
S? = 1 + 4?
S ? Smin
Smax ? Smin
(1)
5 Evaluation
Table 1 shows the results obtained by our classifi-
cation approach on various feature subsets. Note
that the two submitted systems used the combined
classifier approach with the constrained and uncon-
strained feature sets. Table 2 shows the results for
the same feature combinations, this time using re-
gression rather than classification.
The results of quality estimation using classifica-
tion methods show that the baseline and the syntax-
based features with the classifier combination leads
to the best results with an MAE of 0.71 and an
RMSE of 0.87. However, these scores are substan-
tially lower than the ones obtained using regression,
where the unconstrained set of features with SVM
leads to an MAE of 0.62 and an RMSE of 0.78.
It seems that the classification methods are not
suitable for this task according to the different sets
of features studied. Furthermore, the topic-distance
feature is not correlated with the quality scores, ac-
cording to the regression results. On the other hand,
the syntax-based features appear to be the most in-
formative and lead to an MAE of 0.70.
6 Conclusion
We presented in this paper our submission for the
WMT12 Quality Estimation shared task. We also
presented further experiments using different ma-
chine learning techniques and we evaluated the im-
pact of two sets of features - one set which is based
on linguistic features extracted using POS tagging
and parsing, and a second set which is based on topic
modelling. The best results are obtained by our un-
constrained system containing all features and us-
ing an -SVR regression method with a Radial Basis
Function kernel. This setup leads to a Mean Aver-
age Error of 0.62 and a Root Mean Squared Error
of 0.78. Unfortunately, we did not submit our best
configuration for the shared task.
We plan to continue working on the task of ma-
chine translation quality estimation. Our immediate
next steps are to continue to investigate the contribu-
tion of individual features, to explore feature selec-
tion in a more detailed fashion and to apply our best
system to other types of data including sentences
taken from an online discussion forum.
142
SMO NAIVE BAYES RANDOM FOREST Combination
Features MAE RMSE MAE RMSE MAE RMSE MAE RMSE
baseline 0.74 0.89 0.85 1.10 0.84 1.06 0.71 0.88
topic distribution 0.84 1.02 1.09 1.38 0.91 1.15 0.78 0.98
topic distance 0.88 1.11 0.93 1.17 1.04 1.23 0.84 1.04
syntax 0.78 0.97 1.01 1.27 0.83 1.05 0.72 0.90
baseline + topic 0.82 1.01 1.00 1.31 0.84 1.05 0.75 0.95
baseline + syntax 0.76 0.94 1.01 1.25 0.79 0.98 0.71 0.87
baseline + topic + syntax 0.82 1.04 1.03 1.29 0.79 0.98 0.74 0.93
all constrained 0.99 1.26 1.12 1.46 0.71 0.88 0.86 ? 1.12 ?
all unconstrained 0.97 1.25 0.80 1.02 0.79 0.99 0.75 ? 0.97 ?
Table 1: MAE and RMSE results for different sets of features using three classification methods. The results with ?
and ? correspond to the DCU-SYMC constrained and the DCU-SYMC unconstrained systems respectively, submitted
for the shared task.
SVM LINEAR REG. M5P Combination
Features MAE RMSE MAE RMSE MAE RMSE MAE RMSE
baseline 0.78 0.93 0.80 0.99 0.73 0.91 0.72 0.88
topic distribution 0.78 0.95 0.79 0.96 0.80 0.96 0.79 0.95
topic distance 1.38 1.67 1.31 1.62 1.85 2.09 1.00 1.24
syntax 0.70 0.88 0.97 1.22 1.41 1.65 0.76 0.92
baseline + topic 0.78 0.96 1.06 1.31 1.16 1.42 0.88 1.10
baseline + syntax 0.67 0.82 0.90 1.12 2.17 2.38 0.98 1.22
baseline + topic + syntax 0.68 0.84 0.93 1.16 2.12 2.33 0.97 1.21
all constrained 0.83 1.02 0.94 1.18 0.78 0.99 0.71 0.88
all unconstrained 0.62 0.78 1.33 1.60 0.71 0.89 0.73 0.91
Table 2: MAE and RMSE results for different sets of features using three regression methods.
References
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level MT eval-
uation. In Proceedings of the 45th Annual Meeting of
the ACL, pages 880?887.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet Allocation. The Journal of Machine Learn-
ing Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of the 25th Conference on Uncertainty in Artificial In-
telligence, pages 75?82.
L. Breiman. 2001. Random forests. Machine learning,
45(1):5?32.
M. Butt, H. Dyvik, T. Holloway King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project. In
Proceedings of the Coling Workshop on Grammar En-
gineering and Evaluation.
E. Charniak and M. Johnson. 2005. Course-to-fine n-
best-parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 173?180, Ann Arbor.
S. Corston-Oliver, M. Gamon, and C. Brockett. 2001.
A machine learning approach to the automatic evalu-
ation of machine translation. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 148?155, Toulouse, France, July.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society for Infor-
mation Science, 41(6):391?407.
J. Duchateau, K. Demuynck, and P. Wambacq. 2002.
Confidence scoring based on backward language mod-
els. In Proceedings IEEE international confer-
ence on acoustics, speech, and signal processing,
ICASSP?2002, volume 1, pages 221?224.
J. Gime?nez and L. Ma`rquez. 2007. Linguistic features
for automatic evaluation of heterogenous MT systems.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 256?264, Prague, Czech
Republic, June.
143
Z. Gong, Y. Zhang, and G. Zhou. 2010. Statistical ma-
chine translation based on lda. In Universal Commu-
nication Symposium (IUCS), 2010 4th International,
pages 286?290.
Z. Gong, G. Zhou, and L. Li. 2011. Improve smt with
source-side ?topic-document? distributions. In MT
Summit, pages 496?501.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
G.H. John and P. Langley. 1995. Estimating continuous
distributions in bayesian classifiers. In Eleventh con-
ference on uncertainty in artificial intelligence, pages
338?345. Morgan Kaufmann Publishers Inc.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180.
V.I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. In Soviet
Physics Doklady, volume 10-8, pages 707?710.
D. Liu and D. Gildea. 2005. Syntactic features for eval-
uation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, pages 25?32, Ann Arbor, Michigan.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the penn treebank. Computational Linguistics,
19(2):313?330.
John Maxwell and Ron Kaplan. 1996. An Efficient
Parser for LFG. In Proceedings of LFG-96, Grenoble.
D. Mimno, H.M. Wallach, J. Naradowsky, D.A. Smith,
and A. McCallum. 2009. Polylingual topic models.
In Proceedings of EMNLP: Volume 2-Volume 2, pages
880?889. Association for Computational Linguistics.
D. Naber. 2003. A rule-based style and grammar
checker. Technical report, Bielefeld University Biele-
feld, Germany.
K. Owczarzak, J. van Genabith, and A. Way. 2007. La-
belled dependencies in machine translation evaluation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 104?111, Prague, Czech
Republic, June.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, pages 311?318.
J.C. Platt. 1999. Fast training of support vector machines
using sequential minimal optimization. In Advances in
kernel methods, pages 185?208. MIT Press.
R. J. Quinlan. 1992. Learning with continuous classes.
In 5th Australian Joint Conference on Artificial Intel-
ligence, pages 343?348, Singapore. World Scientific.
C. Quirk. 2004. Training a sentence-level machine trans-
lation confidence measure. In Proceedings of LREC,
Lisbon, June.
R. Rubino and G. Linare`s. 2011. A multi-view approach
for term translation spotting. Computational Linguis-
tics and Intelligent Text Processing, 6609:29?40.
H. Schmidt. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Natural Lan-
guage Processing.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of Associ-
ation for Machine Translation in the Americas, pages
223?231.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In InterSpeech, volume 2, pages 901?
904.
Y.C. Tam, I. Lane, and T. Schultz. 2007. Bilingual
lsa-based adaptation for statistical machine translation.
Machine Translation, 21(4):187?207.
J. Wagner, J. Foster, and J. van Genabith. 2007. A com-
parative evaluation of deep and shallow approaches to
the automatic detection of common grammatical er-
rors. In Proceedings of EMNLP-CoNLL, pages 112?
121, Prague, Czech Republic, June.
J. Wagner, J. Foster, and J. van Genabith. 2009. Judg-
ing grammaticality: Experiments in sentence classifi-
cation. CALICO Journal, 26(3):474?490.
J. Wagner. 2012. Detecting grammatical errors with
treebank-induced probabilistic parsers. Ph.D. thesis,
Dublin City University.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
144
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 213?218,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CNGL-DCU-Prompsit Translation Systems for WMT13
Raphael Rubino?, Antonio Toral?, Santiago Cort?s Va?llo?,
Jun Xie?, Xiaofeng Wu?, Stephen Doherty[, Qun Liu?
?NCLT, Dublin City University, Ireland
?Prompsit Language Engineering, Spain
?ICT, Chinese Academy of Sciences, China
?,[CNGL, Dublin City University, Ireland
?,?{rrubino, atoral, xfwu, qliu}@computing.dcu.ie
?santiago@prompsit.com
?junxie@ict.ac.cn
[stephen.doherty@dcu.ie
Abstract
This paper presents the experiments con-
ducted by the Machine Translation group
at DCU and Prompsit Language Engineer-
ing for the WMT13 translation task. Three
language pairs are considered: Spanish-
English and French-English in both direc-
tions and German-English in that direc-
tion. For the Spanish-English pair, the use
of linguistic information to select paral-
lel data is investigated. For the French-
English pair, the usefulness of the small in-
domain parallel corpus is evaluated, com-
pared to an out-of-domain parallel data
sub-sampling method. Finally, for the
German-English system, we describe our
work in addressing the long distance re-
ordering problem and a system combina-
tion strategy.
1 Introduction
This paper presents the experiments conducted
by the Machine Translation group at DCU1 and
Prompsit Language Engineering2 for the WMT13
translation task on three language pairs: Spanish-
English, French-English and German-English.
For these language pairs, the language and trans-
lation models are built using different approaches
and datasets, thus presented in this paper in sepa-
rate sections.
In Section 2, the systems built for the Spanish-
English pair in both directions are described. We
investigate the use of linguistic information to se-
lect parallel data. In Section 3, we present the sys-
tems built for the French-English pair in both di-
1http://www.nclt.dcu.ie/mt/
2http://www.prompsit.com/
rections. The usefulness of the small in-domain
parallel corpus is evaluated, compared to an out-
of-domain parallel data sub-sampling method. In
Section 4, for the German-English system, aiming
at exploring the long distance reordering problem,
we first describe our efforts in a dependency tree-
to-string approach, before combining different hi-
erarchical systems with a phrase-based system and
show a significant improvement over three base-
line systems.
2 Spanish-English
This section describes the experimental setup for
the Spanish-English language pair.
2.1 Setting
Our setup uses the MOSES toolkit, version
1.0 (Koehn et al, 2007). We use a pipeline
with the phrase-based decoder with standard pa-
rameters, unless noted otherwise. The decoder
uses cube pruning (-cube-pruning-pop-limit 2000
-s 2000), MBR (-mbr-size 800 -mbr-scale 1) and
monotone at punctuation reordering.
Individual language models (LMs), 5-gram and
smoothed using a simplified version of the im-
proved Kneser-Ney method (Chen and Goodman,
1996), are built for each monolingual corpus using
IRSTLM 5.80.01 (Federico et al, 2008). These
LMs are then interpolated with IRSTLM using
the test set of WMT11 as the development set. Fi-
nally, the interpolated LMs are merged into one
LM preserving the weights using SRILM (Stol-
cke, 2002).
We use all the parallel corpora available for
this language pair: Europarl (EU), News Com-
mentary (NC), United Nations (UN) and Common
Crawl (CC). Regarding monolingual corpora, we
use the freely available monolingual corpora (Eu-
213
roparl, News Commentary, News 2007?2012) as
well as the target side of several parallel corpora:
Common Crawl, United Nations and 109 French?
English corpus (only for English as target lan-
guage). Both the parallel and monolingual data
are tokenised and truecased using scripts from the
MOSES toolkit.
2.2 Data selection
The main contribution in our participation regards
the selection of parallel data. We follow the
perplexity-based approach to filter monolingual
data (Moore and Lewis, 2010) extended to filter
parallel data (Axelrod et al, 2011). In our case, we
do not measure perplexity only on word forms but
also using different types of linguistic information
(lemmas and named entities) (Toral, 2013).
We build LMs for the source and target sides
of the domain-specific corpus (in our case NC)
and for a random subset of the non-domain-
specific corpus (EU, UN and CC) of the same size
(number of sentences) of the domain-specific cor-
pus. Each parallel sentence s in the non-domain-
specific corpus is then scored according to equa-
tion 1 where PPIsl(s) is the perplexity of s in
the source side according to the domain-specific
LM and PPOsl(s) is the perplexity of s in the
source side according to the non-domain-specific
LM. PPItl(s) and PPOtl(s) contain the corre-
sponding values for the target side.
score(s) = 12 ? (PPIsl(s)? PPOsl(s))
+(PPItl(s)? PPOtl(s)) (1)
Table 1 shows the results obtained using four
models: word forms (forms), forms and named en-
tities (forms+nes), lemmas (lem) and lemmas and
named entities (lem+nes). Details on these meth-
ods can be found in Toral (2013).
For each corpus we selected two subsets (see in
bold in Table 1), the one for which one method
obtained the best perplexity (top 5% of EU us-
ing forms, 2% of UN using lemmas and 50% of
CC using forms and named entities) and a big-
ger one used to compare the performance in SMT
(top 14% of EU using lemmas and named entities
(lem+nes), top 12% of UN using forms and named
entities and the whole CC). These subsets are used
as training data in our systems.
As we can see in the table, the use of lin-
guistic information allows to obtain subsets with
lower perplexity than using solely word forms, e.g.
1057.7 (lem+nes) versus 1104.8 (forms) for 14%
of EU. The only exception to this is the subset that
comprises the top 5% of EU, where perplexity us-
ing word forms (957.9) is the lowest one.
corpus size forms forms+nes lem lem+nes
EU 5% 957.9 987.2 974.3 1005.514% 1104.8 1058.7 1111.6 1057.7
UN 2% 877.1 969.6 866.6 962.212% 1203.2 1130.9 1183.8 1131.6
CC 50% 573.0 547.2 574.5 546.4100% 560.1 560.1 560.1 560.1
Table 1: Perplexities in data selection
2.3 Results
Table 2 presents the results obtained. Note that
these were obtained during development and thus
the systems are tuned on WMT?s 2011 test set and
tested on WMT?s 2012 test set.
All the systems share the same LM. The first
system (no selection) is trained with the whole NC
and EU. The second (small) and third (big) sys-
tems use as training data the whole NC and sub-
sets of EU (5% and 14%, respectively), UN (2%
and 12%, respectively) and CC (50% and 100%,
respectively), as shown in Table 1.
System #sent. BLEU BLEUcased
no selection 2.1M 31.99 30.96
small 1.4M 33.12 32.05
big 3.8M 33.49 32.43
Table 2: Number of sentences and BLEU scores
obtained on the WMT12 test set for the different
systems on the EN?ES translation task.
The advantage of data selection is clear. The
second system, although smaller in size compared
to the first (1.4M sentence pairs versus 2.1M),
takes its training from a more varied set of data,
and its performance is over one absolute BLEU
point higher.
When comparing the two systems that rely on
data selection, one might expect the one that uses
data with lower perplexity (small) to perform bet-
ter. However, this is not the case, the third system
(big) performing around half an absolute BLEU
point higher than the second (small). This hints
at the fact that perplexity alone is not an optimal
metric for data selection, but size should also be
considered. Note that the size of system 3?s phrase
table is more than double that of system 2.
214
3 French-English
This section describe the particularities of the MT
systems built for the French-English language pair
in both directions. The goal of the experimen-
tal setup presented here is to evaluate the gain of
adding small in-domain parallel data into a trans-
lation system built on a sub-sample of the out-of-
domain parallel data.
3.1 Data Pre-processing
All the available parallel and monolingual data for
the French-English language pair, including the
last versions of LDC Gigaword corpora, are nor-
malised and special characters are escaped using
the scripts provided by the shared task organisers.
Then, the corpora are tokenised and for each lan-
guage a true-case model is built on the concatena-
tion of all the data after removing duplicated sen-
tences, using the scripts included in MOSES dis-
tribution. The corpora are then true-cased before
being used to build the language and the transla-
tion models.
3.2 Language Model
To build our final language models, we first build
LMs on each corpus individually. All the monolin-
gual corpora are considered, as well as the source
or target side of the parallel corpora if the data
are not already in the monolingual data. We build
modified Kneser-Ney discounted 5-gram LMs us-
ing the SRILM toolkit for each corpus and sepa-
rate the LMs in three groups: one in-domain (con-
taining news-commentary and news crawl cor-
pora), another out-of-domain (containing Com-
mon Crawl, Europarl, UN and 109 corpora), and
the last one with LDC Gigaword LMs (the data
are kept separated by news source, as distributed
by LDC). The LMs in each group are linearly in-
terpolated based on their perplexities obtained on
the concatenation of all the development sets from
previous WMT translation tasks. The same devel-
opment corpus is used to linearly interpolate the
in-domain and LDC LMs. We finally obtain two
LMs, one containing out-of-domain data which is
only used to filter parallel data, and another one
containing in-domain data which is used to filter
parallel data, tuning the translation model weights
and at decoding time. Details about the number of
n-grams in each language model are presented in
Table 3.
French English
out in out in
1-gram 4.0 3.3 4.2 10.7
2-gram 43.0 44.0 48.2 161.9
3-gram 54.2 61.8 63.4 256.8
4-gram 99.7 119.2 103.2 502.7
5-gram 136.4 165.0 125.4 680.7
Table 3: Number of n-grams (in millions) for the
in-domain and out-of-domain LMs in French and
English.
3.3 Translation Model
Two phrase-based translation models are built
using MGIZA++ (Gao and Vogel, 2008) and
MOSES3, with the default alignment heuris-
tic (grow-diag-final) and bidirectional reordering
models. The first translation model is in-domain,
built with the news-commentary corpus. The sec-
ond one is built on a sample of all the other paral-
lel corpora available for the French-English lan-
guage pair. Both corpora are cleaned using the
script provided with Moses, keeping the sentences
with a length below 80 words. For the second
translation model, we used the modified Moore-
Lewis method based on the four LMs (two per
language) presented in section 3.2. The sum of
the source and target perplexity difference is com-
puted for each sentence pair of the corpus. We set
an acceptance threshold to keep a limited amount
of sentence pairs. The kept sample finally con-
tains ? 3.7M sentence pairs to train the translation
model. Statistics about this data sample and the
news-commentary corpus are presented in Table 4.
The test set of WMT12 translation task is used to
optimise the weights for the two translation mod-
els with the MERT algorithm. For this tuning step,
the limit of target phrases loaded per source phrase
is set to 50. We also use a reordering constraint
around punctuation marks. The same parameters
are used during the decoding of the test set.
news-commentary sample
tokens FR 4.7M 98.6M
tokens EN 4.0M 88.0M
sentences 156.5k 3.7M
Table 4: Statistics about the two parallel corpora,
after pre-processing, used to train the translation
models.
3Moses version 1.0
215
3.4 Results
The two translation models presented in Sec-
tion 3.3 allow us to design three translation sys-
tems: one using only the in-domain model, one
using only the model built on the sub-sample of
the out-of-domain data, and one using both mod-
els by giving two decoding paths to Moses. For
this latter system, the MERT algorithm is also used
to optimise the translation model weights. Results
obtained on the WMT13 test set, measured with
the official automatic metrics, are presented in Ta-
ble 5. The submitted system is the one built on
the sub-sample of the out-of-domain parallel data.
This system was chosen during the tuning step be-
cause it reached the highest BLEU scores on the
development corpus, slightly above the combina-
tion of the two translation models.
News-Com. Sample Comb.
FR-EN
BLEUdev 26.9 30.0 29.9
BLEU 27.0 30.8 30.4
BLEUcased 26.1 29.8 29.3
TER 62.9 58.9 59.3
EN-FR
BLEUdev 27.1 29.7 29.6
BLEU 26.6 29.6 29.4
BLEUcased 25.8 28.7 28.5
TER 65.1 61.8 62.0
Table 5: BLEU and TER scores obtained by our
systems. BLEUdev is the score obtained on the
development set given by MERT, while BLEU,
BLEUcased and TER are obtained on the test set
given by the submission website.
For both FR-EN and EN-FR tasks, the best re-
sults are reached by the system built on the sub-
sample taken from the out-of-domain parallel data.
Using only News-Commentary to build a trans-
lation model leads to acceptable BLEU scores,
with regards to the size of the training corpus.
When the sub-sample of the out-of-domain par-
allel data is used to build the translation model,
adding a model built on News-Commentary does
not improve the results. The difference between
these two systems in terms of BLEU score (both
cased sensitive and insensitive) indicates that sim-
ilar results can be achieved, however it appears
that the amount of sentence pairs in the sample
is large enough to limit the impact of the small
in-domain corpus parallel. Further experiments
are still required to determine the minimum sam-
ple size needed to outperform both the in-domain
system and the combination of the two translation
models.
4 German-English
In this section we describe our work on German
to English subtask. Firstly we describe the De-
pendency tree to string method which we tried but
unfortunately failed due to short of time. Secondly
we discuss the baseline system and the preprocess-
ing we performed. Thirdly a system combination
method is described.
4.1 Dependency Tree to String Method
Our original plan was to address the long distance
reordering problem in German-English transla-
tion. We use Xie?s Dependency tree to string
method(Xie et al, 2011) which obtains good re-
sults on Chinese to English translation and ex-
hibits good performance at long distance reorder-
ing as our decoder.
We use Stanford dependency parser4 to parse
the English side of the data and Mate-Tool5 for
the German side. The first set of experiments did
not lead to encouraging results and due to insuffi-
cient time, we decide to switch to other decoders,
based on statistical phrase-based and hierarchical
approaches.
4.2 Baseline System
In this section we describe the three baseline sys-
tem we used as well as the preprocessing technolo-
gies and the experiments set up.
4.2.1 Preprocessing and Corpus
We first use the normalisation scripts provided by
WMT2013 to normalise both English and Ger-
man side. Then we escape special characters on
both sides. We use Stanford tokeniser for English
and OpenNLP tokeniser6 for German. Then we
train a true-case model using with Europarl and
News-Commentary corpora, and true-case all the
corpus we used. The parallel corpus is filtered
with the standard cleaning scripts provided with
4http://nlp.stanford.edu/software/
lex-parser.shtml
5http://code.google.com/p/mate-tools/
6http://opennlp.sourceforge.net/
models-1.5/
216
MOSES. We split the German compound words
with jWordSplitter7.
All the corpus provided for the shared task are
used for training our translation models, while
WMT2011 and WMT2012 test sets are used to
tune the models parameters. For the LM, we
use all the monolingual data provided, including
LDC Gigaword. Each LM is trained with the
SRILM toolkit, before interpolating all the LMs
according to their weights obtained by minimiz-
ing the perplexity on the tuning set (WMT2011
and WMT2012 test sets). As SRILM can only
interpolate 10 LMs, we first interpolate a LM with
Europarl, News Commentary, News Crawl (2007-
2012, each year individually, 6 separate parts),
then we interpolate a new LM with this interpo-
lated LM and LDC Gigawords (we kept the Gi-
gaword subsets separated according to the news
sources as distributed by LDC, which leads to 7
corpus).
4.2.2 Three baseline systems
We use the data set up described by the former
subsection and build up three baseline systems,
namely PB MOSES (phrase-based), Hiero MOSES
(hierarchical) and CDEC (Dyer et al, 2010). The
motivation of choosing Hierarchical Models is to
address the German-English?s long reorder prob-
lem. We want to test the performance of CDEC and
Hiero MOSES and choose the best. PB MOSES is
used as our benchmark. The three results obtained
on the development and test sets for the three base-
line system and the system combination are shown
in the Table 6.
Development Test
PB MOSES 22.0 24.0
Hiero MOSES 22.1 24.4
CDEC 22.5 24.4
Combination 23.0 24.8
Table 6: BLEU scores obtained by our systems on
the development and test sets for the German to
English translation task.
From the Table 6 we can see that on develop-
ment set, CDEC performs the best, and its much
better than MOSES?s two decoder, but on test
set, Hiero MOSES and CDEC performs as well as
each other, and they both performs better than PB
Model.
7http://www.danielnaber.de/
jwordsplitter/
4.3 System Combination
We also use a word-level combination strat-
egy (Rosti et al, 2007) to combine the three trans-
lation hypotheses. To combine these systems, we
first use the Minimum Bayes-Risk (MBR) (Kumar
and Byrne, 2004) decoder to obtain the 5 best hy-
pothesis as the alignment reference for the Con-
fusion Network (CN) (Mangu et al, 2000). We
then use IHMM (He et al, 2008) to choose the
backbone build the CN and finally search for and
generate the best translation.
We tune the system parameters on development
set with Simple-Simplex algorithm. The param-
eters for system weights are set equal. Other pa-
rameters like language model, length penalty and
combination coefficient are chosen when we see a
good improvement on development set.
5 Conclusion
This paper presented a set of experiments con-
ducted on Spanish-English, French-English and
German-English language pairs. For the Spanish-
English pair, we have explored the use of linguistic
information to select parallel data and use this as
the training for SMT. However, the comparison of
the performance obtained using this method and
the purely statistical one (i.e. perplexity on word
forms) remains to be carried out. Another open
question regards the optimal size of the selected
data. As we have seen, minimum perplexity alone
cannot be considered an optimal metric since us-
ing a larger set, even if it has higher perplexity,
allowed us to obtain notably higher BLEU scores.
The question is then how to decide the optimal size
of parallel data to select.
For the French-English language pair, we inves-
tigated the usefulness of the small in-domain par-
allel data compared to out-of-domain parallel data
sub-sampling. We show that with a sample con-
taining ? 3.7M sentence pairs extracted from the
out-of-domain parallel data, it is not necessary to
use the small domain-specific parallel data. Fur-
ther experiments are still required to determine the
minimum sample size needed to outperform both
the in-domain system and the combination of the
two translation models.
Finally, for the German-English language pair,
we presents our exploitation of long ordering
problem. We compared two hierarchical models
with one phrase-based model, and we also use a
system combination strategy to further improve
217
the translation systems performance.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme FP7/2007-2013 under
grant agreement PIAP-GA-2012-324414 (Abu-
MaTran) and through Science Foundation Ireland
as part of the CNGL (grant 07/CE/I1142).
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7?12. Association for Computational
Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In INTER-
SPEECH, pages 1618?1621. ISCA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57. Association for
Computational Linguistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 98?107. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word er-
ror minimization and other applications of confu-
sion networks. Computer Speech & Language,
14(4):373?400.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Antti-Veikko I Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple
machine translation systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 228?235.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In John H. L. Hansen and
Bryan L. Pellom, editors, INTERSPEECH. ISCA.
Antonio Toral. 2013. Hybrid Selection of Language
Model Training Data Using Linguistic Information
and Perplexity. In Proceedings of the Second Work-
shop on Hybrid Approaches to Machine Translation
(HyTra), ACL 2013.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 216?226. Association for Computational
Linguistics.
218
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 392?397,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
DCU-Symantec at the WMT 2013 Quality Estimation Shared Task
Raphael Rubino??, Joachim Wagner??, Jennifer Foster?,
Johann Roturier? Rasoul Samad Zadeh Kaljahi?? and Fred Hollowood?
?NCLT, School of Computing, Dublin City University, Ireland
?Center for Next Generation Localisation, Dublin, Ireland
?Symantec Research Labs, Dublin, Ireland
?{rrubino, jwagner, jfoster}@computing.dcu.ie
?{johann roturier, fhollowood}@symantec.com
Abstract
We describe the two systems submit-
ted by the DCU-Symantec team to Task
1.1. of the WMT 2013 Shared Task on
Quality Estimation for Machine Transla-
tion. Task 1.1 involve estimating post-
editing effort for English-Spanish trans-
lation pairs in the news domain. The
two systems use a wide variety of fea-
tures, of which the most effective are the
word-alignment, n-gram frequency, lan-
guage model, POS-tag-based and pseudo-
references ones. Both systems perform at
a similarly high level in the two tasks of
scoring and ranking translations, although
there is some evidence that the systems are
over-fitting to the training data.
1 Introduction
The WMT 2013 Quality Estimation Shared Task
involve both sentence-level and word-level qual-
ity estimation (QE). The sentence-level task con-
sist of three subtasks: scoring and ranking transla-
tions with regard to post-editing effort (Task 1.1),
selecting among several translations produced by
multiple MT systems for the same source sentence
(Task 1.2), and predicting post-editing time (Task
1.3). The DCU-Symantec team enter two systems
to Task 1.1. Given a set of source English news
sentences and their Spanish translations, the goals
are to predict the HTER score of each translation
and to produce a ranking based on HTER for the
set of translations. A set of 2,254 sentence pairs
are provided for training.
On the ranking task, our system DCU-SYMC
alltypes is second placed out of thirteen sys-
tems and our system DCU-SYMC combine is
ranked fifth, according to the Delta Average met-
ric. According to the Spearman rank correlation,
our systems are the joint-highest systems. In the
scoring task, the DCU-SYMC alltypes system
is placed sixth out of seventeen systems accord-
ing to Mean Absolute Error (MAE) and third ac-
cording to Root Mean Squared Error (RMSE). The
DCU-SYMC combine system is placed fifth ac-
cording to MAE and second according to RMSE.
In this system description paper, we describe the
features, the learning methods used, the results for
the two submitted systems and some other systems
we experiment with.
2 Features
Our starting point for the WMT13 QE shared task
was the feature set used in the system we submit-
ted to the WMT12 QE task (Rubino et al, 2012).
This feature set, comprising 308 features in to-
tal, extended the 17 baseline features provided by
the task organisers to include 6 additional sur-
face features, 6 additional language model fea-
tures, 17 additional features derived from the
MT system components and the n-best lists, 138
features obtained by part-of-speech tagging and
parsing the source sentences and 95 obtained by
part-of-speech tagging the target sentences, 21
topic model features, 2 features produced by a
grammar checker1 and 6 pseudo-source (or back-
translation) features.
We made the following modifications to this
2012 feature set:
? The pseudo-source (or back-translation) fea-
tures were removed, as they did not con-
tribute useful information to our system last
year.
? The language model and n-gram frequency
feature sets were extended in order to cover
1 to 5 gram sequences, as well as source and
target ratios for these feature values.
? The word-alignment feature set was also
extended by considering several thresholds
1http://www.languagetool.org/
392
when counting the number of target words
aligned with source words.
? We extracted 8 additional features from the
decoder log file, including the number of dis-
carded hypotheses, the total number of trans-
lation options and the number of nodes in the
decoding graph.
? The set of topic model features was reduced
in order to keep only those that were shown
to be effective on three quality estimation
datasets (the details can be found in (Rubino
et al (to appear), 2013)). These features en-
code the difference between source and target
topic distributions according to several dis-
tance/divergence metrics.
? Following Soricut et al (2012), we employed
pseudo-reference features. The source sen-
tences were translated with three different
MT systems: an in-house phrase-based SMT
system built using Moses (Koehn et al,
2007) and trained on the parallel data pro-
vided by the organisers, the rule-based sys-
tem Systran2 and the online, publicly avail-
able, Bing Translator3. The obtained trans-
lations are compared to the target sentences
using sentence-level BLEU (Papineni et al,
2002), TER (Snover et al, 2006) and the Lev-
enshtein distance (Levenshtein, 1966).
? Also following Soricut et al (2012), one-
to-one word-alignments, with and without
Part-Of-Speech (POS) agreement, were in-
cluded as features. Using the alignment in-
formation provided by the decoder, we POS
tagged the source and target sentences with
TreeTagger (Schmidt, 1994) and the publicly
available pre-trained models for English and
Spanish. We mapped the tagsets of both lan-
guages by simplifying the initial tags and ob-
tain a reduced set of 8 tags. We applied that
simplification on the tagged sentences before
checking for POS agreement.
3 Machine Learning
In this section, we describe the learning algo-
rithms and feature selection used in our experi-
ments, leading to the two submitted systems for
the shared task.
2Systran Enterprise Server version 6
3http://www.bing.com/translator
3.1 Primary Learning Method
To estimate the post-editing effort of translated
sentences, we rely on regression models built us-
ing the Support Vector Machine (SVM) algorithm
for regression -SVR, implemented in the LIB-
SVM toolkit (Chang and Lin, 2011). To build
our final regression models, we optimise SVM
hyper-parameters (C, ? and ) using a grid-search
method with 5-fold cross-validation for each pa-
rameter triplet. The parameters leading to the best
MAE, RMSE and Pearson?s correlation coefficient
(r) are kept to build the model.
3.2 Feature Selection on Feature Types
In order to reduce the feature and obtain more
compact models, we apply feature selection on
each of our 15 feature types. Examples of feature
types are language model features or topic model
features. For each feature type, we apply a feature
subset evaluation method based on the wrapper
paradigm and using the best-first search algorithm
to explore the feature space. The M5P (Wang
and Witten, 1997) regression tree algorithm im-
plemented in the Weka toolkit (Hall et al, 2009)
is used with default parameters to train and eval-
uate a regression model for each feature subset
obtained with best-first search. A 10-fold cross-
validation is performed for each subset and we
keep the features leading to the best RMSE. We
use M5P regression trees instead of -SVR be-
cause grid-search with the latter is too computa-
tionally expensive to be applied so many times.
Using feature selection in this way, we obtain 15
reduced feature sets that we combine to form the
DCU-SYMC alltypes system, containing 102
features detailed in Table 1.
3.3 Feature Binarisation
In order to aid the SVM learner, we also experi-
ment with binarising our feature set, i.e. convert-
ing our features with various feature value ranges
into features whose values are either 1 or 0. Again,
we employ regression tree learning. We train
regression trees with M5P and M5P-R4 (imple-
mented in the Weka toolkit) and create a binary
feature for each regression rule found in the trees
(ignoring the leaf nodes). For example, a binary
feature indicating whether the Bing TER score is
less than or equal to 55.685 is derived from the
4We experiment with J48 decision trees as well, but this
method did not outperform regression tree methods.
393
Backward LM
Source 1-gram perplexity.
Source & target 1-grams perplexity ratio.
Source & target 3-grams and 4-gram perplexity ratio.
Target Syntax
Frequency of tags: ADV, FS, DM, VLinf, VMinf, semicolon, VLger, NC, PDEL, VEfin, CC, CCNEG, PPx, ART, SYM,
CODE, PREP, SE and number of ambiguous tags
Frequency of least frequent POS 3-gram observed in a corpus.
Frequency of least frequent POS 4-gram and 6-gram with sentence padding (start and end of sentence tags) observed in a
corpus.
Source Syntax
Features from three probabilistic parsers. (Rubino et al, 2012).
Frequency of least frequent POS 2-gram, 4-gram and 9-gram with sentence padding observed in a corpus.
Number of analyses found and number of words, using a Lexical Functional Grammar of English as described in Rubino
et al (2012).
LM
Source unigram perplexity.
Target 3-gram and 4-gram perplexity with sentence padding.
Source & target 1-gram and 5-gram perplexity ratio.
Source & target unigram log-probability.
Decoder
Component scores during decoding.
Number of phrases in the best translation.
Number of translation options.
N -gram Frequency
Target 2-gram in second and third frequency quartiles.
Target 3-gram and 5-gram in low frequency quartiles.
Number of target 1-gram seen in a corpus.
Source & target 1-grams in highest and second highest frequency quartile.
One-to-One Word-Alignment
Count of O2O word alignment, weighted by target sentence length.
Count of O2O word alignment with POS agreement, weighted by count of O2O, by source length, by target length.
Pseudo-Reference
Moses translation TER score.
Bing translation number of words and TER score.
Systran sBLEU, number of substitutions and TER score.
Surface
Source number of punctuation marks and average words occurrence in source sentence.
Target number of punctuation marks, uppercased letters and binary value if the last character of the sentence is a punctuation
mark.
Ratio of source and target sentence lengths, average word length and number of punctuation marks over sentence lengths.
Topic Model
Cosine distance between source and target topic distributions.
Jensen-Shannon divergence between source and target topic distributions.
Word Alignment
Averaged number of source words aligned per target words with p(s|t) thresholds: 1.0, 0.75, 0.5, 0.25, 0.01
Averaged number of source words aligned per target words with p(s|t) = 0.01 weighted by target words frequency
Averaged number of target words aligned per source word with p(t|s) = 0.01 weighted by source words frequency
Ratio of source and target averaged aligned words with thresholds: 1.0 and 0.1, and with threshold: 0.75, 0.5, 0.25 weighted
by words frequency
Table 1: Features selected with the wrapper approach using best-first search and M5P. These features are
included in the submitted system alltypes.
394
Feature to which threshold t is applied t (?)
Target 1-gram backward LM log-prob. ?35.973
Target 3-gram backward LM perplexity 7144.99
Probabilistic parsing feature 3.756
Probabilistic parsing feature 57.5
Frequency of least frequent POS 6-gram 0.5
Source 3-gram LM log-prob. 65.286
Source 4-gram LM perplexity with padding 306.362
Target 2-gram LM perplexity 176.431
Target 4-gram LM perplexity 426.023
Target 4-gram LM perplexity with padding 341.801
Target 5-gram LM perplexity 112.908
Ratio src&trg 5-gram LM log-prob. 1.186
MT system component score ?50
MT system component score ?0.801
Source 2-gram frequency in low quartile 0.146
Ratio src&trg 2-gram in high freq. quartile 0.818
Ratio src&trg 3-gram in high freq. quartile 0.482
O2O word alignment 15.5
Pseudo-ref. Moses Levenshtein 19
Pseudo-ref. Moses TER 21.286
Pseudo-ref. Bing TER 16.905
Pseudo-ref. Bing TER 23.431
Pseudo-ref. Bing TER 37.394
Pseudo-ref. Bing TER 55.685
Pseudo-ref. Systran sBLEU 0.334
Pseudo-ref. Systran TER 36.399
Source average word length 4.298
Target uppercased/lowercased letters ratio 0.011
Ratio src&trg average word length 1.051
Source word align., p(s|t) > 0.75 11.374
Source word align., p(s|t) > 0.1 485.062
Source word align., p(s|t) > 0.75 weighted 0.002
Target word align., p(t|s) > 0.01 weighted 0.019
Word align. ratio p > 0.25 weighted 1.32
Table 2: Features selected with the M5P-R M50
binarisation approach. For each feature, the cor-
responding rule indicates the binary feature value.
These features are included in the submitted sys-
tem combine in addition to the features presented
in Table 1.
regression rule Bing TER score ? 55.685.
The primary motivation for using regression
tree learning in this way was to provide a quick
and convenient method for binarising our feature
set. However, we can also perform feature selec-
tion using this method by experimenting with vari-
ous minimum leaf sizes (Weka parameter M ). We
plot the performance of the M5P and M5P-R (opti-
mising towards correlation) over the parameter M
and select the best three values of M . To experi-
ment with the effect of smaller and larger feature
sets, we further include parameters of M that (a)
lead to an approximately 50% bigger feature set
and (b) to an approximately 50% smaller feature
set.
Our DCU-SYMC combine system was built
by combining the DCU-SYMC alltypes fea-
ture set, reduced using the best-first M5P wrap-
per approach as described in subsection 3.2, with
a binarised set produced using an M5P regres-
sion tree with a minimum of 50 nodes per leaf.
This latter configuration, containing 34 features
detailed in Table 2, was selected according to the
evaluation scores obtained during cross-validation
on the training set using -SVR, as described in
the next section. Finally, we run a greedy back-
ward feature selection algorithm wrapping -SVR
on both DCU-SYMC alltypes and DCU-SYMC
combine in order to optimise our feature sets for
the SVR learning algorithm, removing 6 and 2 fea-
tures respectively.
4 System Evaluation and Results
In this section, we present the results obtained with
-SVR during 5-fold cross-validation on the train-
ing set and the final results obtained on the test
set. We selected two systems to submit amongst
the different configurations based on MAE, RMSE
and r. As several systems reach the same perfor-
mance according to these metrics, we use the num-
ber of support vectors (noted SV) as an indicator
of training data over-fitting. We report the results
obtained with some of our systems in Table 3.
The results show that the submitted sys-
tems DCU-SYMC alltypes and DCU-SYMC
combine lead to the best scores on cross-
validation, but they do not outperform the system
combining the 15 feature types without feature se-
lection (15 types). This system reaches the best
scores on the test set compared to all our systems
built on reduced feature sets. This indicates that
we over-fit and fail to generalise from the training
data.
Amongst the systems built using reduced fea-
ture sets, the M5P-R M80 system, based on the
tree binarisation approach using M5P-R, yields
the best results on the test set on 3 out of 4 offi-
cial metrics. These results indicate that this sys-
tem, trained on 16 features only, tends to estimate
HTER scores more accurately on the unseen test
data. The results of the two systems based on
the M5P-R binarisation method are the best com-
pared to all the other systems presented in this
Section. This feature binarisation and selection
method leads to robust systems with few features:
31 and 16 for M5P-R M50 and M5P-R M80 re-
spectively. Even though these systems do not lead
to the best results, they outperform the two sub-
mitted systems on one metric used to evaluate the
395
Cross-Validation Test
System nb feat MAE RMSE r SV MAE RMSE DeltaAvg Spearman
15 types 442 0.106 0.138 0.604 1194.6 0.126 0.156 0.108 0.625
M5P M50 34 0.106 0.138 0.600 1417.8 0.135 0.167 0.102 0.586
M5P M130 4 0.114 0.145 0.544 750.6 0.142 0.173 0.079 0.517
M5P-R M50 31 0.106 0.137 0.610 655.4 0.135 0.166 0.100 0.591
M5P-R M80 16 0.107 0.139 0.597 570.6 0.134 0.165 0.106 0.597
alltypes? 96 0.104 0.135 0.624 1130.6 0.135 0.171 0.101 0.589
combine? 134 0.104 0.134 0.629 689.8 0.134 0.166 0.098 0.588
Table 3: Results obtained with different regression models, during cross-validation on the training set
and on the test set, depending on the feature selection method. Systems marked with ? were submitted
for the shared task.
scoring task and two metrics to evaluate the rank-
ing task.
On the systems built using reduced feature sets,
we observe a difference of approximately 0.03pt
absolute between the MAE and RMSE scores ob-
tained during cross-validation and those on the test
set. Such a difference can be related to train-
ing data over-fitting, even though the feature sets
obtained with the tree binarisation methods are
small. For instance, the system M5P M130 is
trained on 4 features only, but the difference be-
tween cross-validation and test MAE scores is
similar to the other systems. We see on the fi-
nal results that our feature selection methods is an
over-fitting factor: by selecting the features which
explain well the training set, the final model tends
to generalise less. The selected features are suited
for the specificities of the training data, but are less
accurate at predicting values on the unseen test set.
5 Discussion
Training data over-fitting is clearly shown by the
results presented in Table 3, indicated by the per-
formance drop between results obtained during
cross-validation and the ones obtained on the test
set. While this drop may be related to data over-
fitting, it may also be related to the use of differ-
ent machine learning methods for feature selec-
tion (M5P and M5P-R) and for building the fi-
nal regression models (-SVR). In order to ver-
ify this aspect, we build two regression models
using M5P, based on the feature sets alltypes
and combine. Results are presented in Table 4
and show that, for the alltypes feature set, the
RMSE, DeltaAvg and Spearman scores are im-
proved using M5P compared to SVM. For the
combine feature set, the scoring results (MAE
and RMSE) are better using SVM, while the rank-
ing results are similar for both machine learning
methods.
The performance drop between the results on
the training data (or a development set) and the
test data was also observed by the highest ranked
participants in the WMT12 QE shared task. To
compare our system without feature selection to
the winner of the previous shared task, we eval-
uate the 15 types system in Table 3 using the
WMT12 QE dataset. The results are presented in
Table 5. We can see that similar MAEs are ob-
tained with our feature set and the WMT12 QE
winner, whereas our system gets a higher RMSE
(+0.01). For the ranking scores, our system is
worse using the DeltaAvg metric while it is bet-
ter on Spearman coefficient.
6 Conclusion
We presented in this paper our experiments for the
WMT13 Quality Estimation shared task. Our ap-
proach is based on the extraction of a large ini-
tial feature set, followed by two feature selection
methods. The first one is a wrapper approach us-
ing M5P and a best-first search algorithm, while
the second one is a feature binarisation approach
using M5P and M5P-R. The final regression mod-
els were built using -SVR and we selected two
systems to submit based on cross-validation re-
sults.
We observed that our system reaching the best
scores on the test set was not a system trained on
a reduced feature set and it did not yield the best
cross-validation results. This system was trained
using 442 features, which are the combination of
15 different feature types. Amongst the systems
built on reduced sets, the best results are obtained
396
System nb feat MAE RMSE DeltaAvg Spearman
alltypes 96 0.135 0.165 0.104 0.604
combine 134 0.139 0.169 0.098 0.587
Table 4: Results obtained with the two feature sets contained in our submitted systems using M5P to
build the regression models instead of -SVR.
System nb feat MAE RMSE DeltaAvg Spearman
WMT12 winner 15 0.61 0.75 0.63 0.64
15 types 442 0.61 0.76 0.60 0.65
Table 5: Results obtained on WMT12 QE dataset with our best system (15 types) compared to WMT12
QE highest ranked team, in the Likert score prediction task.
using the feature binarisation approach M5P-R
80, which contains 16 features selected from our
initial set of features. The tree-based feature bina-
risation is a fast and flexible method which allows
us to vary the number of features by optimising the
leaf size and leads to acceptable results with a few
selected features.
Future work involves a deeper analysis of the
over-fitting effect and an investigation of other
methods in order to outperform the non-reduced
feature set. We are also interested in finding a ro-
bust way to optimise the leaf size parameter for
our tree-based feature binarisation method, with-
out using cross-validation on the training set with
an SVM algorithm.
Acknowledgements
The research reported in this paper has been
supported by the Research Ireland Enterprise
Partnership Scheme (EPSPG/2011/102 and EP-
SPD/2011/135) and Science Foundation Ireland
(Grant 12/CE/I2267) as part of the Centre for
Next Generation Localisation (www.cngl.ie)
at Dublin City University.
References
Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A Library for Support Vector Ma-
chines. ACM Transactions on Intelligent Sys-
tems and Technology, 2:27:1?27:27. Soft-
ware available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The WEKA Data Mining Software: an
Update. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. DCU-Symantec Submission for
the WMT 2012 Quality Estimation Task. In Pro-
ceedings of the Seventh WMT, pages 138?144.
Raphael Rubino et al (to appear). 2013. Topic Models
for Translation Quality Estimation for Gisting Pur-
poses. In Proceeding of MT Summit XIV.
Helmut Schmidt. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Natu-
ral Language Processing.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA, pages 223?231.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the Seventh WMT, pages 145?151.
Yong Wang and Ian H Witten. 1997. Inducing Model
Trees for Continuous Classes. In Proceedings of
ECML, pages 128?137. Prague, Czech Republic.
397
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 429?434,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
An approach using style classification features for Quality Estimation
Erwan Moreau
CNGL and Computational Linguistics Group
Centre for Computing and Language Studies
School of Computer Science and Statistics
Trinity College Dublin
Dublin 2, Ireland
moreaue@cs.tcd.ie
Raphael Rubino
NCLT
Dublin City University
Dublin 9, Ireland
rrubino@computing.dcu.ie
Abstract
In this paper we describe our participation
to the WMT13 Shared Task on Quality Es-
timation. The main originality of our ap-
proach is to include features originally de-
signed to classify text according to some
author?s style. This implies the use of ref-
erence categories, which are meant to rep-
resent the quality of the MT output.
Preamble
This paper describes the approach followed in the
two systems that we submitted to subtask 1.3 of
the WMT13 Shared Task on Quality Estimation,
identified as TCD-DCU-CNGL 1-3 SVM1 and
TCD-DCU-CNGL 1-3 SVM2. This approach
was also used by the first author in his submissions
to subtask 1.1, identified as TCD-CNGL OPEN
and TCD-CNGL RESTRICTED1. In the remain-
ing of this paper we focus on subtask 1.3, but there
is very little difference in the application of the ap-
proach to task 1.1.
1 Introduction
Quality Estimation (QE) aims to provide a quality
indicator for machine translated sentences. There
are many cases where such an indicator would be
useful in a translation process: to compare differ-
ent Machine Translation (MT) models on a given
set of sentences, to tune automatically the param-
eters of a MT model, to select the bad sentences
for human translation or post-editing, to select the
good sentences for immediate publication and try
to apply automatic post-editing to the others, or
simply to provide users who are not fluent in the
source language information about the fluency of
1The second author?s submission to subtask 1.1 is inde-
pendent from this approach and is described in a different
paper in this volume.
the translated text they are reading. As long as ma-
chine translated text cannot be of reasonably con-
sistent quality, QE is helpful in indicating linguis-
tic quality variability.2
After focusing on automatic prediction of ad-
hoc quality scores (as estimated by professional
annotators) in the previous edition (Callison-
Burch et al, 2012), the WMT Shared Task on
Quality Estimation 2013 proposes several variants
of the task. We participated in task 1.1 which aims
to predict HTER scores (edit distance between the
MT output and its manually post-edited version),
and in task 1.3 which aims to predict the expected
time needed to post-edit the MT output.
The originality of our participation lies in the
fact that we intended to test ?style classification?
features for the task of QE: the idea is to select a
set of n-grams which are particularly representa-
tive of a given level of quality. In practice we use
only two levels which simply represent low and
high quality. We explore various ways to build
these two reference categories and to select the n-
grams, as described in ?2. The goal was to see
if such features can contribute to the task of pre-
dicting quality of MT. As explained in ?3, how-
ever, various constraints forced us to somehow cut
corners in some parts of the features selection and
training process; therefore we think that the mod-
est results presented and discussed in ?4 might not
necessarily reflect the real contribution of these
features.
2 Features
2.1 Classical features
We extract a set of features inspired by the ones
provided by the shared task organisers in their 17
baseline feature set. Using the corpora provided
for the task, we extract for each source and target
2We focus on translation fluency rather than target lan-
guage faithfulness to sources.
429
segments pair:
? 24 surface features, such as the segment
length, the number of punctuation marks and
uppercased letters, words with mixed case,
etc.
? 30 language Model (LM) features, n-gram
log-probability and perplexity (with and
without start and end of sentence tags) with
n ? [1; 5].
? 30 backward LM features, n-gram log-
probability and perplexity (with and without
start and end of sentence tags) with n ?
[1; 5].
? 44 n-gram frequency features, with n ?
[1; 5], extracted from frequency quartiles.
? 24 word-alignment features according to the
alignment probability thresholds: 0.01, 0.1,
0.25, 0.5, 0.75 and 1.0, with or without words
frequency weighting.
For all these features, except the ones with binary
values, we compute the ratio between the source
and target feature values and add them to our fea-
ture set, which contains 223 classical features.
2.2 Style classification features
We call the features described below ?style
classification? features because they have been
used recently in the context of author identifica-
tion/profiling (Moreau and Vogel, 2013a; Moreau
and Vogel, 2013b) (quite sucessfully in some
cases). The idea consists in representing the n-
grams which are very specific to a given ?cate-
gory?, a category being a level of quality in the
context of QE, and more precisely we use only the
?good? and ?bad? categories here.
Thus this approach requires the following pa-
rameters:
? At least two datasets used as reference for the
categories;
? Various n-grams patterns, from which com-
parisons based on frequency can be done;
? One or several methods to compare a sen-
tence to a category.
2.2.1 Reference categories
As reference categories we use both the training
datasets provided for task 1.1 and 1.3: both are
used in each task, that is, categories are extracted
from subtasks 1.1 dataset and 1.3 dataset and used
in task 1.1 and 1.3 as well. However we use only
half of the sentences of task 1.1 in 1.1 and sim-
ilarly in 1.3, in order to keep the other half for
the classical training process. This is necessary to
avoid using (even indirectly) a sentence as both a
fixed parameter from which features are extracted
(the category data) and an actual instance on which
features are computed. In other words this simply
follows the principle of keeping the training and
test data independent, but in this case there are two
stages of training (comparing sentences to a refer-
ence category is also a supervised process).
The two datasets are used in three different
ways, leading to three distinct pairs of categories
?good/bad?:3
? The sentences for which the quality is below
the median form the ?bad? category, the one
above form the ?good? category;
? The sentences for which the quality is below
the first quartile form the ?bad? category, the
one above the third quartile form the ?good?
category;
? The complete set of MT output sentences
form the ?bad? category, their manually
post-edited counterpart form the ?good? cat-
egory.
We use these three different ways to build cate-
gories because there is no way to determine a pri-
ori the optimal choice. For instance, on the one
hand the opposite quartiles probably provide more
discriminative power than the medians, but on the
other hand the latter contains more data and there-
fore possibly more useful cases.4 In the last ver-
sion the idea is to consider that, in average, the
machine translated sentences are of poor quality
compared to the manually post-edited sentences;
in this case the categories contain more data, but it
might be a problem that (1) some of the machine-
translated sentences are actually good and (2) the
3Below we call ?quality? the value given by the HTER
score (1.1) or post-editing time (1.3), the level of quality be-
ing of course conversely proportional to these values.
4The datasets are not very big: only 803 sentences in task
1.3 and 2,254 sentences in task 1.1 (and we can only use half
of these for categories, as explained above).
430
right translation of some difficult phrases in the
post-edited sentences might never be found in MT
output. We think that the availability of differ-
ent categories built in various ways is potentially a
good thing, because it lets the learning algorithm
decide which features (based on a particular cate-
gory) are useful and which are not, thus tuning the
model automatically while possibly using several
possibilities together, rather than relying on some
predefined categories.
It is important to notice that the correspondence
between an MT output and its post-edited version
is not used5: in all categories the sentences are
only considered as an unordered set. For instance
it would be possible to use a third-party corpus as
well (provided it shares at least a common domain
with the data).
We use only the target language (Spanish) of the
translation and not the source language in order
not to generate too many categories, and because
it has been shown that there is a high correlation
between the complexity of the source sentence and
the fluency of the translation (Moreau and Vogel,
2012). However it is possible to do so for the cat-
egories based on quantiles.
2.2.2 n-grams patterns, thresholds and
distance measures
We use a large set of 30 n-grams patterns based on
tokens and POS tags. POS tagging has been per-
formed with TreeTagger (Schmid, 1995). Various
combinations of n-grams are considered, includ-
ing standard sequential n-grams, skip-grams, and
combinations of tokens and POS tags.
Since the goal is to compare a sentence to a
category, we consider the frequency in terms of
number of sentences in which the n-gram appears,
rather than the global frequency or the local fre-
quency by sentence.6
Different frequency thresholds are considered,
from 1 to 25. Additionally we can also filter out
n-grams for which the relative frequency is too
5in the categories used as reference data; but it is used in
the final features during the (supervised) training stage (see
?3).
6The frequency by sentence is actually also taken into ac-
count in the following way: instead of considering only the
n-gram, we consider a pair (n-gram, local frequency) as an
observation. This way if a particular frequency is observed
more often in a given category, it can be interpreted as a clue
in favor of this category. However in most cases (long n-
grams sequences) the frequency by sentence is almost always
one, sometimes two. Thus this is only marginally a relevant
criterion to categorize a sentence.
similar between the ?good? and ?bad? categories.
For instance it is possible to keep only the n-grams
for which 80% of the occurrencies belong to the
?bad? category, thus making it a strong marker
for low quality. Once again different thresholds
are considered, in order to tradeoff between the
amount of cases and their discriminative power.
We use only three simple distance/similarity
measures when comparing a sentence to a cate-
gory:
? Binary match: for each n-gram in the sen-
tence, count 1 if it belongs to the category, 0
otherwise, then divide by the number of n-
grams in the sentence;
? Weighted match: same as above but sum the
proportion of occurrences belonging to the
category instead of 1 (this way an n-gram
which is more discriminative is given more
weight);
? Cosine similarity.
Finally for every tuple formed by the combina-
tion of
? a category,
? a quality level (?good/bad?),
? an n-gram pattern,
? a frequency threshold,
? a threshold for the proportion of the occur-
rences in the given category,
? and a distance measure
a feature is created. For every sentence the value
of the feature is the score computed using the pa-
rameters defined in the tuple. From our set of
parameters we obtain approximately 35,000 fea-
tures.7 It is worth noticing that these features
are not meant to represent the sentence entirely,
but rather particularly noticeable parts (in terms of
quality) of the sentence.
7The number of features depends on the data in the cate-
gory, because if no n-gram at all in the category satisfies the
conditions given by the parameters (which can happen with
very high thresholds), then the feature does not exist.
431
2.3 Features specific to the dataset
In task 1.3 we are provided with a translator id
and a document id for each sentence. The distribu-
tion of the time spent to post-edit the sentence de-
pending on these parameters shows some signifi-
cant differences among translators and documents.
This is why we add several features intended to ac-
count for these parameters: the id itself, the mean
and the median for both the translator and the doc-
ument.
3 Design and training process
The main difficulty with so many features (around
35,000) is of course to select a subset of reason-
able size, in order to train a model which is not
overfitted. This requires an efficient optimization
method, since it is clearly impossible to explore
the search space exhaustively in this case.
Initially it was planned to use an ad-hoc genetic
algorithm to select an optimal subset of features.
But unfortunately the system designed in this goal
did not work as well as expected8, this is why we
had to switch to a different strategy: the two fi-
nal sets of features were obtained through several
stages of selection, mixing several different kinds
of correlation-based features selection methods.
The different steps described below were car-
ried out using the Weka Machine Learning toolkit9
(Hall et al, 2009). Since we have used half of the
training data as a reference corpus for some of the
categories (see ?2), we use the other half as train-
ing instances in the selection and learning process,
with 10 folds cross-validation for the latter.
3.1 Iterative selection of features
Because of the failure of the initial strategy, in or-
der to meet the time constraints of the Shared Task
we had to favor speed over performance in the pro-
cess of selecting features and training a model.
This probably had a negative impact on the final
results, as discussed in section ?4.
In particular the amount of features was too
big to be processed in the remaining time by a
subset selection method. This is why the fea-
tures were first ranked individually using the Re-
lief attribute estimation method (Robnik-Sikonja
8At the time of writing it is still unclear if this was due to
a design flaw or a bug in the implementation.
9Weka 3.6.9, http://www.cs.waikato.ac.nz/
ml/weka.
and Kononenko, 1997). Only the 20,00010 top fea-
tures were extracted from this ranking and used
further in the selection process.
From this initial subset of features, the follow-
ing heuristic search algorithms combined with a
correlation-based method11 to evaluate subsets of
features (Hall, 1998) are applied iteratively to a
given input set of features:
? Best-first search (forward, backward, bi-
directional);
? Hill-climbing search (forward and back-
ward);
? Genetic search with Bayes Networks.
Each of these algorithms was used with differ-
ent predefined parameters in order to trade off be-
tween time and performance. This selection pro-
cess is iterated as long as the number of features
left is (approximately) higher than 200.
3.2 Training the models
When less than 200 features are obtained, the it-
erative selection process is still applied but a 10
folds cross-validated evaluation is also performed
with the following regression algorithms:
? Support Vector Machines (SVM) (Smola and
Scho?lkopf, 2004; Shevade et al, 2000);
? Decision trees (Quinlan, 1992; Wang and
Witten, 1996);
? Pace regression (Wang and Witten, 2002).
These learning algorithms are also run with
several possible sets of parameters. Eventually
the submitted models are chosen among those
for which the set of features can not be reduced
anymore without decreasing seriously the perfor-
mance. Most of the best models were obtained
with SVM, although the decision trees regression
algorithm performed almost as well. It was not
possible to decrease the number of features below
60 for task 1.3 (80 for task 1.1) without causing a
loss in performance.
10For subtask 1.3. Only the 8,000 top features for subtask
1.1.
11Weka class
weka.attributeSelection.CfsSubsetEval.
432
4 Results and discussion
The systems are evaluated based on the Mean Av-
erage Error, and every team was allowed to submit
two systems. Our systems ranked 10th and 11th
among 14 for task 1.1, and 13th and 15th among
17 for task 1.1.
4.1 Possible causes of loss in performance
We plan to investigate why our approach does not
perform as well as others, and in particular to
study more exhaustively the different possibilities
in the features selection process.12 It is indeed
very probable that the method can perform better
with an appropriate selection of features and opti-
mization of the parameters, in particular:
? The final number of features is too large,
which can cause overfitting. Most QE system
do not need so many features (only 15 for the
best system in the WMT12 Shared Task on
QE (Soricut et al, 2012)).
? We had to perform a first selection to discard
some of the initial features based on their in-
dividual contribution. This is likely to be a
flaw, since some features can be very useful
in conjuction with other even if poorly infor-
mative by themselves.
? We also probably made a mistake in apply-
ing the selection process to the whole set of
features, including both classical features and
style classification features: it might be rel-
evant to run two independent selection pro-
cesses at first and then gather the resulting
features together only for a more fine-grained
final selection. Indeed, the final models that
we submitted include very few classical fea-
tures; we believe that this might have made
these models less reliable, since our initial
assumption was rather that the style classifi-
cation features would act as secondary clues
in a model primarily relying on the classical
features.
4.2 Selected features
The following observations can be made on the fi-
nal models obtained for task 1.3, keeping in mind
that the models might not be optimal for the rea-
sons explained above:
12Unfortunately the results of this study are not ready yet
at the time of writing.
? Only 5% of the selected features are classical
features;
? The amount of data used in the category
seems to play an important role: most fea-
tures correspond to categories built from the
1.1 dataset (which is bigger), and the pro-
portions between the different kinds of cate-
gories are: 13% for first quartile vs. fourth
quartile (smallest dataset), 25% for below
median vs. above median, and 61% for
MT output vs. postedited sentence (largest
dataset);
? It seems more interesting to identify the low
quality n-grams (i.e. errors) rather than the
high quality ones: 76% of the selected fea-
tures represent the ?bad? category;
? 81% of the selected features represent an
n-grams containing at least one POS tag,
whereas only 40% contain a token;
? Most features correspond to selecting n-
grams which are very predictive of the
?good/bad? category (high difference of the
relative proportion between the two cate-
gories), although a significant number of less
predictive n-grams are also selected;
? The cosine distance is selected about three
times more often than the two other distance
methods.
5 Conclusion and future work
In conclusion, the approach performed decently on
the Shared Task test data, but was outperformed
by most other participants systems. Thus cur-
rently it is not proved that style classification fea-
tures help assessing the quality of MT. However
the approach, and especially the contribution of
these features, have yet to be evaluated in a less
constrained environment in order to give a well-
argued answer to this question.
Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of the
Centre for Next Generation Localisation (www.
cngl.ie) funding at Trinity College, University
of Dublin.
433
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
M. A. Hall. 1998. Correlation-based Feature Subset
Selection for Machine Learning. Ph.D. thesis, Uni-
versity of Waikato, Hamilton, New Zealand.
Erwan Moreau and Carl Vogel. 2012. Quality esti-
mation: an experimental study using unsupervised
similarity measures. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
120?126, Montre?al, Canada, June. Association for
Computational Linguistics.
Erwan Moreau and Carl Vogel. 2013a. Participation
to the pan author identification task. In to appear in
the proceeding of CLEF 2013.
Erwan Moreau and Carl Vogel. 2013b. Participation
to the pan author profiling task. In to appear in the
proceeding of CLEF 2013.
J.R. Quinlan. 1992. Learning with continuous classes.
In Proceedings of the 5th Australian joint Confer-
ence on Artificial Intelligence, pages 343?348. Sin-
gapore.
Marko Robnik-Sikonja and Igor Kononenko. 1997.
An adaptation of relief for attribute estimation in
regression. In Douglas H. Fisher, editor, Four-
teenth International Conference on Machine Learn-
ing, pages 296?304. Morgan Kaufmann.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
S.K. Shevade, SS Keerthi, C. Bhattacharyya, and
K.R.K. Murthy. 2000. Improvements to the SMO
algorithm for SVM regression. Neural Networks,
IEEE Transactions on, 11(5):1188?1193.
A.J. Smola and B. Scho?lkopf. 2004. A tutorial on
support vector regression. Statistics and computing,
14(3):199?222.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver systems in the WMT12
Quality Estimation shared task. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 145?151, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Y. Wang and I.H. Witten. 1996. Induction of model
trees for predicting continuous classes.
Y. Wang and I.H. Witten. 2002. Modeling for optimal
probability prediction. In Proceedings of the Nine-
teenth International Conference on Machine Learn-
ing, pages 650?657. Morgan Kaufmann Publishers
Inc.
434
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 171?177,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Abu-MaTran at WMT 2014 Translation Task:
Two-step Data Selection and RBMT-Style Synthetic Rules
Raphael Rubino
?
, Antonio Toral
?
, Victor M. S
?
anchez-Cartagena
??
,
Jorge Ferr
?
andez-Tordera
?
, Sergio Ortiz-Rojas
?
, Gema Ram??rez-S
?
anchez
?
,
Felipe S
?
anchez-Mart??nez
?
, Andy Way
?
?
Prompsit Language Engineering, S.L., Elche, Spain
{rrubino,vmsanchez,jferrandez,sortiz,gramirez}@prompsit.com
?
NCLT, School of Computing, Dublin City University, Ireland
{atoral,away}@computing.dcu.ie
?
Dep. Llenguatges i Sistemes Inform`atics, Universitat d?Alacant, Spain
fsanchez@dlsi.ua.es
Abstract
This paper presents the machine trans-
lation systems submitted by the Abu-
MaTran project to the WMT 2014 trans-
lation task. The language pair concerned
is English?French with a focus on French
as the target language. The French to En-
glish translation direction is also consid-
ered, based on the word alignment com-
puted in the other direction. Large lan-
guage and translation models are built us-
ing all the datasets provided by the shared
task organisers, as well as the monolin-
gual data from LDC. To build the trans-
lation models, we apply a two-step data
selection method based on bilingual cross-
entropy difference and vocabulary satura-
tion, considering each parallel corpus in-
dividually. Synthetic translation rules are
extracted from the development sets and
used to train another translation model.
We then interpolate the translation mod-
els, minimising the perplexity on the de-
velopment sets, to obtain our final SMT
system. Our submission for the English to
French translation task was ranked second
amongst nine teams and a total of twenty
submissions.
1 Introduction
This paper presents the systems submitted by the
Abu-MaTran project (runs named DCU-Prompsit-
UA) to the WMT 2014 translation task for the
English?French language pair. Phrase-based sta-
tistical machine translation (SMT) systems were
submitted, considering the two translation direc-
tions, with the focus on the English to French di-
rection. Language models (LMs) and translation
models (TMs) are trained using all the data pro-
vided by the shared task organisers, as well as
the Gigaword monolingual corpora distributed by
LDC.
To train the LMs, monolingual corpora and the
target side of the parallel corpora are first used
individually to train models. Then the individ-
ual models are interpolated according to perplex-
ity minimisation on the development sets.
To train the TMs, first a baseline is built us-
ing the News Commentary parallel corpus. Sec-
ond, each remaining parallel corpus is processed
individually using bilingual cross-entropy differ-
ence (Axelrod et al., 2011) in order to sepa-
rate pseudo in-domain and out-of-domain sen-
tence pairs, and filtering the pseudo out-of-
domain instances with the vocabulary saturation
approach (Lewis and Eetemadi, 2013). Third,
synthetic translation rules are automatically ex-
tracted from the development set and used to train
another translation model following a novel ap-
proach (S?anchez-Cartagena et al., 2014). Finally,
we interpolate the four translation models (base-
line, in-domain, filtered out-of-domain and rules)
by minimising the perplexity obtained on the de-
velopment sets and investigate the best tuning and
decoding parameters.
The reminder of this paper is organised as fol-
lows: the datasets and tools used in our experi-
ments are described in Section 2. Then, details
about the LMs and TMs are given in Section 3 and
Section 4 respectively. Finally, we evaluate the
performance of the final SMT system according to
different tuning and decoding parameters in Sec-
tion 5 before presenting conclusions in Section 6.
171
2 Datasets and Tools
We use all the monolingual and parallel datasets
in English and French provided by the shared task
organisers, as well as the LDC Gigaword for the
same languages
1
. For each language, a true-case
model is trained using all the data, using the train-
truecaser.perl script included in the MOSES tool-
kit (Koehn et al., 2007).
Punctuation marks of all the monolingual and
parallel corpora are then normalised using the
script normalize-punctuation.perl provided by the
organisers, before being tokenised and true-cased
using the scripts distributed with the MOSES tool-
kit. The same pre-processing steps are applied to
the development and test sets. As development
sets, we used all the test sets from previous years
of WMT, from 2008 to 2013 (newstest2008-2013).
Finally, the training parallel corpora are cleaned
using the script clean-corpus-n.perl, keeping the
sentences longer than 1 word, shorter than 80
words, and with a length ratio between sentence
pairs lower than 4.
2
The statistics about the cor-
pora used in our experiments after pre-processing
are presented in Table 1.
For training LMs we use KENLM (Heafield et
al., 2013) and the SRILM tool-kit (Stolcke et al.,
2011). For training TMs, we use MOSES (Koehn
et al., 2007) version 2.1 with MGIZA++ (Och and
Ney, 2003; Gao and Vogel, 2008). These tools are
used with default parameters for our experiments
except when explicitly said.
The decoder used to generate translations is
MOSES using features weights optimised with
MERT (Och, 2003). As our approach relies on
training individual TMs, one for each parallel cor-
pus, our final TM is obtained by linearly interpo-
lating the individual ones. The interpolation of
TMs is performed using the script tmcombine.py,
minimising the cross-entropy between the TM
and the concatenated development sets from 2008
to 2012 (noted newstest2008-2012), as described
in Sennrich (2012). Finally, we make use of the
findings from WMT 2013 brought by the win-
ning team (Durrani et al., 2013) and decide to use
the Operation Sequence Model (OSM), based on
minimal translation units and Markov chains over
sequences of operations, implemented in MOSES
1
LDC2011T07 English Gigaword Fifth Edition,
LDC2011T10 French Gigaword Third Edition
2
This ratio was empirically chosen based on words fertil-
ity between English and French.
Corpus Sentences (k) Words (M)
Monolingual Data ? English
Europarl v7 2,218.2 59.9
News Commentary v8 304.2 7.4
News Shuffled 2007 3,782.5 90.2
News Shuffled 2008 12,954.5 308.1
News Shuffled 2009 14,680.0 347.0
News Shuffled 2010 6,797.2 157.8
News Shuffled 2011 15,437.7 358.1
News Shuffled 2012 14,869.7 345.5
News Shuffled 2013 21,688.4 495.2
LDC afp 7,184.9 869.5
LDC apw 8,829.4 1,426.7
LDC cna 618.4 45.7
LDC ltw 986.9 321.1
LDC nyt 5,327.7 1,723.9
LDC wpb 108.8 20.8
LDC xin 5,121.9 423.7
Monolingual Data ? French
Europarl v7 2,190.6 63.5
News Commentary v8 227.0 6.5
News Shuffled 2007 119.0 2.7
News Shuffled 2008 4,718.8 110.3
News Shuffled 2009 4,366.7 105.3
News Shuffled 2010 1,846.5 44.8
News Shuffled 2011 6,030.1 146.1
News Shuffled 2012 4,114.4 100.8
News Shuffled 2013 9,256.3 220.2
LDC afp 6,793.5 784.5
LDC apw 2,525.1 271.3
Parallel Data
10
9
Corpus
21,327.1
549.0 (EN)
642.5 (FR)
Common Crawl 3,168.5
76.0 (EN)
82.7 (FR)
Europarl v7 1,965.5
52.5 (EN)
56.7 (FR)
News Commentary v9 181.3
4.5 (EN)
5.3 (FR)
UN 12,354.7
313.4 (EN)
356.5 (FR)
Table 1: Data statistics after pre-processing of the
monolingual and parallel corpora used in our ex-
periments.
and introduced by Durrani et al. (2011).
3 Language Models
The LMs are trained in the same way for both
languages. First, each monolingual and parallel
corpus is considered individually (except the par-
allel version of Europarl and News Commentary)
and used to train a 5-gram LM with the modified
Kneser-Ney smoothing method. We then interpo-
late the individual LMs using the script compute-
best-mix available with the SRILM tool-kit (Stol-
cke et al., 2011), based on their perplexity scores
on the concatenation of the development sets from
2008 to 2012 (the 2013 version is held-out for the
tuning of the TMs).
172
The final LM for French contains all the word
sequences from 1 to 5-grams contained in the
training corpora without any pruning. However,
with the computing resources at our disposal, the
English LMs could not be interpolated without
pruning non-frequent n-grams. Thus, n-grams
with n ? [3; 5] with a frequency lower than 2 were
removed. Details about the final LMs are given in
Table 2.
1-gram 2-gram 3-gram 4-gram 5-gram
English 13.4 198.6 381.2 776.3 1,068.7
French 6.0 75.5 353.2 850.8 1,354.0
Table 2: Statistics, in millions of n-grams, of the
interpolated LMs.
4 Translation Models
In this Section, we describe the TMs trained for
the shared task. First, we present the two-step data
selection process which aims to (i) separate in and
out-of-domain parallel sentences and (ii) reduce
the total amount of out-of-domain data. Second,
a novel approach for the automatic extraction of
translation rules and their use to enrich the phrase
table is detailed.
4.1 Parallel Data Filtering and Vocabulary
Saturation
Amongst the parallel corpora provided by the
shared task organisers, only News Commentary
can be considered as in-domain regarding the de-
velopment and test sets. We use this training
corpus to build our baseline SMT system. The
other parallel corpora are individually filtered us-
ing bilingual cross-entropy difference (Moore and
Lewis, 2010; Axelrod et al., 2011). This data
filtering method relies on four LMs, two in the
source and two in the target language, which
aim to model particular features of in and out-of-
domain sentences.
We build the in-domain LMs using the source
and target sides of the News Commentary paral-
lel corpus. Out-of-domain LMs are trained on a
vocabulary-constrained subset of each remaining
parallel corpus individually using the SRILM tool-
kit, which leads to eight models (four in the source
language and four in the target language).
3
3
The subsets contain the same number of sentences and
the same vocabulary as News Commentary.
Then, for each out-of-domain parallel corpus,
we compute the bilingual cross-entropy difference
of each sentence pair as:
[H
in
(S
src
)?H
out
(S
src
)] + [H
in
(S
trg
)?H
out
(S
trg
)] (1)
where S
src
and S
trg
are the source and the tar-
get sides of a sentence pair, H
in
and H
out
are
the cross-entropies of the in and out-of-domain
LMs given a sentence pair. The sentence pairs are
then ranked and the lowest-scoring ones are taken
to train the pseudo in-domain TMs. However,
the cross-entropy difference threshold required to
split a corpus in two parts (pseudo in and out-of-
domain) is usually set empirically by testing sev-
eral subset sizes of the top-ranked sentence pairs.
This method is costly in our setup as it would lead
to training and evaluating multiple SMT systems
for each of the pseudo in-domain parallel corpora.
In order to save time and computing power,
we consider only pseudo in-domain sentence pairs
those with a bilingual cross-entropy difference be-
low 0, i.e. those deemed more similar to the
in-domain LMs than to the out-of-domain LMs
(H
in
< H
out
). A sample of the distribution of
scores for the out-of-domain corpora is shown in
Figure 1. The resulting pseudo in-domain corpora
are used to train individual TMs, as detailed in Ta-
ble 3.
-4
-2
 0
 2
 4
 6
 8
 10
0 2k 4k 6k 8k 10k
Bilin
gual
 Cro
ss-E
ntro
py D
iffer
ence
Sentence Pairs
Common CrawlEuroparl10^9UN
Figure 1: Sample of ranked sentence-pairs (10k)
of each of the out-of-domain parallel corpora with
bilingual cross-entropy difference
The results obtained using the pseudo in-
domain data show BLEU (Papineni et al., 2002)
scores superior or equal to the baseline score.
Only the Europarl subset is slightly lower than
the baseline, while the subset taken from the 10
9
corpus reaches the highest BLEU compared to the
other systems (30.29). This is mainly due to the
173
size of this subset which is ten times larger than
the one taken from Europarl. The last row of Ta-
ble 3 shows the BLEU score obtained after interpo-
lating the four pseudo in-domain translation mod-
els. This system outperforms the best pseudo in-
domain one by 0.5 absolute points.
Corpus Sentences (k) BLEU
dev
Baseline 181.3 27.76
Common Crawl 208.3 27.73
Europarl 142.0 27.63
10
9
Corpus 1,442.4 30.29
UN 642.4 28.91
Interpolation - 30.78
Table 3: Number of sentence pairs and BLEU
scores reported by MERT on English?French new-
stest2013 for the pseudo in-domain corpora ob-
tained by filtering the out-of-domain corpora with
bilingual cross-entropy difference. The interpola-
tion of pseudo in-domain models is evaluated in
the last row.
After evaluating the pseudo in-domain parallel
data, the remaining sentence pairs for each cor-
pora are considered out-of-domain according to
our filtering approach. However, they may still
contain useful information, thus we make use of
these corpora by building individual TMs for each
corpus (in a similar way we built the pseudo in-
domain models). The total amount of remaining
data (more than 33 million sentence pairs) makes
the training process costly in terms of time and
computing power. In order to reduce these costs,
sentence pairs with a bilingual cross-entropy dif-
ference higher than 10 were filtered out, as we no-
ticed that most of the sentences above this thresh-
old contain noise (non-alphanumeric characters,
foreign languages, etc.).
We also limit the size of the remaining data by
applying the vocabulary saturation method (Lewis
and Eetemadi, 2013). For the out-of-domain sub-
set of each corpus, we traverse the sentence pairs
in the order they are ranked by perplexity differ-
ence and filter out those sentence pairs for which
we have seen already each 1-gram at least 10
times. Each out-of-domain subset from each par-
allel corpus is then used to train a TM before inter-
polating them to create the pseudo out-of-domain
TM. The results reported by MERT obtained on
the newstest2013 development set are detailed in
Table 4.
Mainly due to the sizes of the pseudo out-of-
Corpus Sentences (k) BLEU
dev
Baseline 181.3 27.76
Common Crawl 1,598.7 29.84
Europarl 461.9 28.87
10
9
Corpus 5,153.0 30.50
UN 1,707.3 29.03
Interpolation - 31.37
Table 4: Number of sentence pairs and BLEU
scores reported by MERT on English?French
newstest2013 for the pseudo out-of-domain cor-
pora obtained by filtering the out-of-domain cor-
pora with bilingual cross-entropy difference, keep-
ing sentence pairs below an entropy score of 10
and applying vocabulary saturation. The interpo-
lation of pseudo out-of-domain models is evalu-
ated in the last row.
domain subsets, the reported BLEU scores are
higher than the baseline for the four individual
SMT systems and the interpolated one. This latter
system outperforms the baseline by 3.61 absolute
points. Compared to the results obtained with the
pseudo in-domain data, we observe a slight im-
provement of the BLEU scores using the pseudo
out-of-domain data. However, despite the com-
paratively larger sizes of the latter datasets, the
BLEU scores reached are not that higher. For in-
stance with the 10
9
corpus, the pseudo in and out-
of-domain subsets contain 1.4 and 5.1 million sen-
tence pairs respectively, and the two systems reach
30.3 and 30.5 BLEU. These scores indicate that
the pseudo in-domain SMT systems are more ef-
ficient on the English?French newstest2013 devel-
opment set.
4.2 Extraction of Translation Rules
A synthetic phrase-table based on shallow-transfer
MT rules and dictionaries is built as follows. First,
a set of shallow-transfer rules is inferred from the
concatenation of the newstest2008-2012 develop-
ment corpora exactly in the same way as in the
UA-Prompsit submission to this translation shared
task (S?anchez-Cartagena et al., 2014). In sum-
mary, rules are obtained from a set of bilingual
phrases extracted from the parallel corpus after
its morphological analysis and part-of-speech dis-
ambiguation with the tools in the Apertium rule-
based MT platform (Forcada et al., 2011).
The extraction algorithm commonly used in
phrase-based SMT is followed with some added
heuristics which ensure that the bilingual phrases
174
extracted are compatible with the bilingual dic-
tionary. Then, many different rules are generated
from each bilingual phrase; each of them encodes
a different degree of generalisation over the partic-
ular example it has been extracted from. Finally,
the minimum set of rules which correctly repro-
duces all the bilingual phrases is found based on
integer linear programming search (Garfinkel and
Nemhauser, 1972).
Once the rules have been inferred, the phrase
table is built from them and the original rule-
based MT dictionaries, following the method
by S?anchez-Cartagena et al. (2011), which was
one of winning systems
4
(together with two on-
line SMT systems) in the pairwise manual evalu-
ation of the WMT11 English?Spanish translation
task (Callison-Burch et al., 2011). This phrase-
table is then interpolated with the baseline TM and
the results are presented in Table 5. A slight im-
provement over the baseline is observed, which
motivates the use of synthetic rules in our final MT
system. This small improvement may be related
to the small coverage of the Apertium dictionar-
ies: the English?French bilingual dictionary has a
low number of entries compared to more mature
language pairs in Apertium which have around 20
times more bilingual entries.
System BLEU
dev
Baseline 27.76
Baseline+Rules 28.06
Table 5: BLEU scores reported by MERT on
English?French newstest2013 for the baseline
SMT system standalone and with automatically
extracted translation rules.
5 Tuning and Decoding
We present in this Section a short selection of our
experiments, amongst 15+ different configura-
tions, conducted on the interpolation of TMs, tun-
ing and decoding parameters. We first interpolate
the four TMs: the baseline, the pseudo in and out-
of-domain, and the translation rules, minimising
the perplexity obtained on the concatenated de-
velopment sets from 2008 to 2012 (newstest2008-
2012). We investigate the use of OSM trained on
pseudo in-domain data only or using all the paral-
lel data available. Finally, we make variations of
4
No other system was found statistically significantly bet-
ter using the sign test at p ? 0.1.
the number of n-bests used by MERT.
Results obtained on the development set new-
stest2013 are reported in Table 6. These scores
show that adding OSM to the interpolated trans-
lation models slightly degrades BLEU. However,
by increasing the number of n-bests considered by
MERT to 200-bests, the SMT system with OSM
outperforms the systems evaluated previously in
our experiments. Adding the synthetic translation
rules degrades BLEU (as indicated by the last row
in the Table), thus we decide to submit two sys-
tems to the shared task: one without and one with
synthetic rules. By submitting a system without
synthetic rules, we also ensure that our SMT sys-
tem is constrained according to the shared task
guidelines.
System BLEU
dev
Baseline 27.76
+ pseudo in + pseudo out 31.93
+ OSM 31.90
+ MERT 200-best 32.21
+ Rules 32.10
Table 6: BLEU scores reported by MERT on
English?French newstest2013 development set.
As MERT is not suitable when a large number
of features are used (our system uses 19 fetures),
we switch to the Margin Infused Relaxed Algo-
rithm (MIRA) for our submitted systems (Watan-
abe et al., 2007). The development set used is
newstest2012, as we aim to select the best decod-
ing parameters according to the scores obtained
when decoding the newstest2013 corpus, after de-
truecasing and de-tokenising using the scripts dis-
tributed with MOSES. This setup allowed us to
compare our results with the participants of the
translation shared task last year. We pick the de-
coding parameters leading to the best results in
terms of BLEU and decode the official test set of
WMT14 newstest2014. The results are reported in
Table 7. Results on newstest2013 show that the de-
coding parameters investigation leads to an over-
all improvement of 0.1 BLEU absolute. The re-
sults on newstest2014 show that adding synthetic
rules did not help improving BLEU and degraded
slightly TER (Snover et al., 2006) scores.
In addition to our English?French submission,
we submitted a French?English translation. Our
French?English MT system is built on the align-
ments obtained from the English?French direc-
tion. The training processes between the two sys-
175
System BLEU13A TER
newstest2013
Best tuning 31.02 60.77
cube-pruning (pop-limit 10000) 31.04 60.71
increased table-limit (100) 31.06 60.77
monotonic reordering 31.07 60.69
Best decoding 31.14 60.66
newstest2014
Best decoding 34.90 54.70
Best decoding + Rules 34.90 54.80
Table 7: Case sensitive results obtained with
our final English?French SMT system on new-
stest2013 when experimenting with different de-
coding parameters. The best parameters are kept
to translate the WMT14 test set (newstest2014)
and official results are reported in the last two
rows.
tems are identical, except for the synthetic rules
which are not extracted for the French?English
direction. Tuning and decoding parameters for
this latter translation direction are the best ones
obtained in our previous experiments on this
shared task. The case-sensitive scores obtained
for French?English on newstest2014 are 35.0
BLEU13A and 53.1 TER, which ranks us at the
fifth position for this translation direction.
6 Conclusion
We have presented the MT systems developed by
the Abu-MaTran project for the WMT14 trans-
lation shared task. We focused on the French?
English language pair and particularly on the
English?French direction. We have used a two-
step data selection process based on bilingual
cross-entropy difference and vocabulary satura-
tion, as well as a novel approach for the extraction
of synthetic translation rules and their use to en-
rich the phrase table. For the LMs and the TMs,
we rely on training individual models per corpus
before interpolating them by minimising perplex-
ity according to the development set. Finally, we
made use of the findings of WMT13 by including
an OSM model.
Our English?French translation system was
ranked second amongst nine teams and a total of
twenty submissions, while our French?English
submission was ranked fifth. As future work,
we plan to investigate the effect of adding to the
phrase table synthetic translation rules based on
larger dictionaries. We also would like to study the
link between OSM and the different decoding pa-
rameters implemented in MOSES, as we observed
inconsistent results in our experiments.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme FP7/2007-2013 under
grant agreement PIAP-GA-2012-324414 (Abu-
MaTran).
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain Adaptation Via Pseudo In-domain
Data Selection. In Proceedings of EMNLP, pages
355?362.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceed-
ings of WMT, pages 22?64.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of ACL/HLT,
pages 1045?1054.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of WMT, pages 112?119.
Mikel L Forcada, Mireia Ginest??-Rosell, Jacob Nord-
falk, Jim O?Regan, Sergio Ortiz-Rojas, Juan An-
tonio P?erez-Ortiz, Felipe S?anchez-Mart??nez, Gema
Ram??rez-S?anchez, and Francis M Tyers. 2011.
Apertium: A Free/Open-source Platform for Rule-
based Machine Translation. Machine Translation,
25(2):127?144.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Robert S Garfinkel and George L Nemhauser. 1972.
Integer Programming, volume 4. Wiley New York.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Mod-
ified Kneser-Ney Language Model Estimation. In
Proceedings of ACL, pages 690?696.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of ACL, Interactive Poster and Demonstra-
tion Sessions, pages 177?180.
176
William D. Lewis and Sauleh Eetemadi. 2013. Dra-
matically Reducing Training Data Size Through Vo-
cabulary Saturation. In Proceedings of WMT, pages
281?291.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In Pro-
ceedings of ACL, pages 220?224.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL, volume 1, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL, pages 311?318.
V??ctor M. S?anchez-Cartagena, Felipe S?anchez-
Mart??nez, and Juan Antonio P?erez-Ortiz. 2011. In-
tegrating Shallow-transfer Rules into Phrase-based
Statistical Machine Translation. In Proceedings of
MT Summit XIII, pages 562?569.
V??ctor M. S?anchez-Cartagena, Juan Antonio P?erez-
Ortiz, and Felipe S?anchez-Mart??nez. 2014. The
UA-Prompsit Hybrid Machine Translation System
for the 2014 Workshop on Statistical Machine
Translation. In Proceedings of WMT.
Rico Sennrich. 2012. Perplexity Minimization for
Translation Model Domain Adaptation in Statisti-
cal Machine Translation. In Proceedings of EACL,
pages 539?549.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of AMTA, pages 223?231.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at Sixteen: Update and Out-
look. In Proceedings of ASRU.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online Large-margin Train-
ing for Statistical Machine Translation. In Proceed-
ings of EMNLP.
177
