Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 88?95,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Information State-Based Dialogue Manager for Call for Fire Dialogues
Antonio Roque and David Traum
USC Institute for Creative Technologies
13274 Fiji Way, Marina Del Rey, CA 90292
roque@ict.usc.edu, traum@ict.usc.edu
Abstract
We present a dialogue manager for ?Call
for Fire? training dialogues. We describe
the training environment, the domain, the
features of its novel information state-
based dialogue manager, the system it is a
part of, and preliminary evaluation results.
1 Overview
Dialogue systems are built for many different pur-
poses, including information gathering (e.g., (Aust
et al, 1995)), performing simple transactions (e.g,
(Walker and Hirschman, 2000)), collaborative in-
teraction (e.g., (Allen et al, 1996)), tutoring (e.g.,
(Rose et al, 2003)), and training (e.g. (Traum
and Rickel, 2002)). Aspects of the purpose, as
well as features of the domain itself (e.g., train
timetables, air flight bookings, schedule mainte-
nance, physics, and platoon-level military opera-
tions) will have a profound effect on the nature of
the dialogue which a system will need to engage
in. Issues such as initiative, error correction, flex-
ibility in phrasing and dialogue structure may de-
pend crucially on these factors.
The information state approach to dialogue
managers (Larsson and Traum, 2000) has been an
attempt to cast some of these differences within
the same framework. In this approach, a theory
of dialogue is constructed by providing informa-
tion structure elements, a set of dialogue moves
that can be recognized and produced and are used
to modify the nature of these elements, a set of
update rules that govern the dynamics of how the
information is changed as dialogue moves are per-
formed, and an update strategy. Many differ-
ent dialogue systems have been built according to
this general approach (e.g., (Cooper and Larsson,
1999; Matheson et al, 2000; Lemon et al, 2001;
Johnston et al, 2002; Traum and Rickel, 2002;
Purver, 2002)).
In this paper, we present an information-state
based dialogue manager for a new domain: train-
ing call for fire dialogues. Like other dialogue sys-
tems used as role-players in training applications,
the structure of the dialogue is not completely free
for a dialogue designer to specify based on issues
of dialogue efficiency. The dialogue system must
conform as much as possible to the type of dia-
logue that a trainee would actually encounter in the
types of interaction he or she is being trained for.
In particular, for military radio dialogues, much
of the protocol for interaction is specified by con-
vention (e.g., (Army, 2001)). Still, there is a fair
amount of flexibility in how other aspects of the
dialogue progress.
This dialogue manager is part of a system we
call Radiobot-CFF. Radiobots are a general class
of dialogue systems meant to speak over the ra-
dio in military simulations. Our most extended
effort to date is the Radiobot-CFF system, which
engages in ?call for fire? dialogues to train ar-
tillery observers within a virtual reality training
simulation. Our dialogue system can operate ac-
cording to three different use cases, depending on
how much control a human operator/trainer would
like to exercise over the dialogue. There is a fully
automatic mode in which the Radiobot-CFF sys-
tem engages unassisted in dialogue with the user, a
semi-automatic mode in which the Radiobot-CFF
system fills in forms (which can be edited) and the
operator can approve or change communication
with a simulator or trainee, and a passive mode
in which the operator is engaging in the dialogue
and the Radiobot-CFF system is just observing.
In section 2, we describe the training applica-
88
tion that our dialogue system has been embedded
in as well as the system itself. In section 3, we de-
scribe some aspects of ?call for fire dialogues?, es-
pecially the differences in initiative and purposes
of different phases in the dialogue. In section 4,
we describe the information-state based dialogue
model we have developed for this domain. This in-
cludes dialogue moves, information components,
and update rules. We describe some error handling
capabilities in section 5, and evaluation results in
section 6.
2 Testbed
Our current testbed, Radiobot-CFF, has been
developed in a military training environment,
JFETS-UTM, at the U.S. Army base in in Ft. Sill,
Oklahoma. JFETS-UTM trains soldiers to make
Calls for Fire (CFFs), in which a Forward Ob-
server (FO) team locates an enemy target and re-
quests an artillery fire mission by radio from a Fire
Direction Center (FDC). The training room resem-
bles a battle-scarred apartment in a Middle East-
ern country. A window shows a virtual city dis-
played by a rear-projected computer screen, and
the soldiers use binoculars with computer displays
at their ends to search for targets.
Ordinarily, two trainers control a UTM session.
One communicates with the FO via a simulated
radio, and the other decides what the artillery fire
should be and inputs it to a GUI for the simu-
lator. It is our goal to replace those two train-
ers with one trainer focusing on assessment while
Radiobot-CFF handles the radio communications
and interfaces with the virtual world.
Radiobot-CFF is composed of several pipelined
components. A Speech Recognition component
is implemented using the SONIC speech recogni-
tion system (Pellom, 2001) with custom language
and acoustic models. An Interpreter component
tags the ASR output with its its dialogue move
and parameter labels using two separate Condi-
tional Random Field (Sha and Pereira, 2003; Mc-
Callum, 2002) taggers trained on hand-annotated
utterances. A Dialogue Manager processes the
tagged output, sending a reply to the FO (via a
template-based Generator) and, when necessary, a
message to the artillery simulator FireSim XXI1 to
make decisions on what type of fire to send. The
reply to FO and messages to simulator are medi-
ated by GUIs where the trainer can intervene if
1http://sill-www.army.mil/blab/sims/FireSimXXI.htm
need be.
3 Call for Fire Dialogues
Call for Fire procedures are specified in an Army
field manual (Army, 2001) with variations based
on a unit?s standard operating procedure. Mes-
sages are brief and followed by confirmations,
where any misunderstandings are immediately
corrected. A typical CFF is shown in Figure 1.
1 FO steel one niner this is gator niner one adjust
fire polar over
2 FDC gator nine one this is steel one nine adjust fire
polar out
3 FO direction five niner four zero distance four
eight zero over
4 FDC direction five nine four zero distance four eight
zero out
5 FO one b m p in the open i c m in effect over
6 FDC one b m p in the open i c m in effect out
7 FDC message to observer kilo alpha high explo-
sive four rounds adjust fire target number al-
pha bravo one zero zero zero over
8 FO m t o kilo alpha four rounds target number al-
pha bravo one out
9 FDC shot over
10 FO shot out
11 FDC splash over
12 FO splash out
13 FO right five zero fire for effect out over
14 FDC right five zero fire for effect out
15 FDC shot over
16 FO shot out
17 FDC rounds complete over
18 FO rounds complete out
19 FO end of mission one b m p suppressed zero ca-
sualties over
20 FDC end of mission one b m p suppressed zero ca-
sualties out
Figure 1: Example Dialogue with Radiobot-CFF
CFFs can generally be divided into three
phases. In the first phase (utterances 1-6 in Fig-
ure 1) the FOs identify themselves and important
information about the CFF, including their coor-
dinates, the kind of fire they are requesting, the
location of the target, and the kind of target. In
utterance 1 in Figure 1 the FO performs an identi-
fication, giving his own call sign and that of the
FDC he is calling, and also specifies a method
of fire (?adjust fire?) and a method of targeting
(?polar?.) Note that when speakers expect a reply,
they end their utterance with ?over? as in utter-
ance 1, otherwise with ?out? as in the confirmation
in utterance 2. In utterance 3 the FO gives target
coordinates, and in utterance 5 the FO identifies
the target as a BMP (a type of light tank) and re-
quests ICM rounds (?improved conventional mu-
nitions?.) These turns typically follow one another
89
in quick sequence.
In the second phase of a CFF, (utterances 7-12
in Figure 1), after the FDC decides what kind of
fire they will send, they inform the FO in a mes-
sage to observer (MTO) as in utterance 7. This
includes the units that will fire (?kilo alpha?), the
kind of ammunition (?high explosive?), the num-
ber of rounds and method of fire (?4 rounds ad-
just fire?), and the target number (?alpha bravo one
zero zero zero?). CFFs are requests rather than or-
ders, and they may be denied in full or in part. In
this example, the FO?s request for ICM rounds was
denied in favor of High Explosive rounds. Next
the FDC informs the FO when the fire mission has
been shot, as in utterance 9, and when the fire is
about to land, as in utterance 11. Each of these are
confirmed by the FO.
In the third phase, (utterances 13-20 in Fig-
ure 1) the FO regains dialogue initiative. Depend-
ing on the observed results, the FO may request
that the fire be repeated with an adjust in location
or method of fire. In utterance 13 the FO requests
that the shot be re-sent to a location 50 meters to
the right of the previous shot as a ?fire for effect?
all-out bombardment rather than an ?adjust fire?
targeting fire. This is followed by the abbreviated
FDC-initiated phase of utterances 15-18. In utter-
ance 19 the FO ends the mission, describing the
results and number of casualties.
Besides the behavior shown, at any turn either
participant may request or initiate an intelligence
report or request the status of a mission. Further-
more, after receiving an MTO the FO may imme-
diately begin another fire mission and thus have
multiple missions active; subsequent adjusts are
disambiguated with the target numbers assigned
during the MTOs.
4 Dialogue Manager
We have constructed an Information State-based
dialogue manager (Larsson and Traum, 2000) on
this domain consisting of a set of dialogue moves,
a set of informational components with appropri-
ate formal representations, and a set of update
rules with an update strategy. We describe each
of these in turn.
4.1 Dialogue Moves
We defined a set of dialogue moves to represent
the incoming FO utterances based on a study of
transcripts of human-controlled JFETS-UTM ses-
sions, Army manuals, and the needs of the simu-
lator. As shown in Figure 2 these are divided into
three groups: those that provide information about
the FO or the fire mission, those that confirm in-
formation that the FDC has transmitted, and those
that make requests.
Mission Information:
Observer Coordinates
Situation Report
Identification
Warning Order
Method of Control
Method of Engagement
Target Location
Target Description
End of Mission
Confirming Information:
Message to Observer
Shot
Splash
Rounds Complete
Intel Report
Other Requests:
Radio Check
Say Again
Status
Standby
Command
Figure 2: FO Dialogue Moves
The dialogue moves that provide information
include those in which the FOs transmit their Ob-
server Coordinates (grid location on a map), a
generic Situation Report, or one of the various
components of a fire mission request ranging from
call sign Identification to final End of Mission.
The dialogue moves that confirm information in-
clude those that confirm the MTO and other FDC-
initiated utterances, or a general report on scenario
Intel. The final group includes requests to check
radio functionality, to repeat the previous utter-
ance, for status of a shot, to stand by for transmis-
sion of information, and finally a set of commands
such as ?check fire? requesting cancellation of a
submitted fire mission.
Each of these dialogue moves contains informa-
tion important to the dialogue manager. This in-
formation is captured by the parameters of the di-
alogue move, which are enumerated in Figure 3.
Each parameter is listed with the dialogue move
it usually occurs with, but this assignment is not
strict. For example, ?number of enemies? param-
eters occur in Target Description as well as End of
Mission dialogue moves.
90
Identification-related:
fdc_id
fo_id
Warning Order-related:
method_of_fire
method_of_control
method_of_engagement
method_of_location
Target Location-related:
grid_location
direction
distance
attitude
left_right
left_right_adjust
add_drop
add_drop_adjust
known_point
End Of Mission-related:
target_type
target_description
number_of_enemies
disposition
Other:
command
detail_of_request
target_number
Figure 3: Dialogue Move Parameters
Figure 4 shows how the dialogue moves and pa-
rameters act to identify the components of an FO
utterance. The example is based on utterance 1 in
Figure 1; the Identification move has two param-
eters representing the call signs of the FDC and
the FO, and the Warning Order has two parame-
ters representing the method of fire and method of
location. Parameters need to be identified to con-
firm back to the FO, and in some cases to be sent
to the simulator and for use in updating the infor-
mation state. In the example in Figure 4, the fact
that the requested method of fire is an ?adjust fire?
will be sent to the simulator, and the fact that a
method of fire has been given will be updated in
the information state.
Identification: steel one nine this is gator niner one
fdc id: steel one nine
fo id: gator niner one
Warning Order: adjust fire polar
method of fire: adjust fire
method of location: polar
Figure 4: Example Dialogue Moves and Parame-
ters
4.2 Informational Components
The Radiobot-CFF dialogue manager?s informa-
tion state consists of five classes of informational
components, defined by their role in the dia-
logue and their level of accessibility to the user.
These are the Fire Mission Decision components,
the Fire Mission Value components, the Post-Fire
Value components, the Disambiguation compo-
nents, and the Update Rule Processing compo-
nents.
By dividing the components into multiple
classes we separate those that are simulator-
specific from more general aspects of the domain.
Decisions to fire are based on general con-
straints of the domain, whereas the exact com-
ponents to include in a message to simulator will
be simulator-specific. Also, the components have
been designed such that there is almost no over-
lap in the update rules that modify them (see sec-
tion 4.3). This reduces the complexity involved
in editing or adding rules; although there are over
100 rules in the information state, there are few
unanticipated side-effects when rules are altered.
The first class of components are the Fire Mis-
sion Decision components, which are used to de-
termine whether enough information has been col-
lected to send fire. These components are boolean
flags, updated by rules based on incoming dia-
logue moves and parameters. Figure 5 shows the
values of these components after utterance 3 in
Figure 1 has been processed. The FO has given a
warning order, and a target location (which can ei-
ther be given through a grid location, or through a
combination of direction and distance values, and
observer coordinates), so the appropriate compo-
nents are ?true?. After the FO gives a target de-
scription, that component will be true as well, and
an update rule will recognize that enough informa-
tion has been gathered to send a fire mission.
has warning order? true
has target location? true
has grid location? false
has polar direction? true
has polar distance? true
has polar obco? true
has target descr? false
Figure 5: Fire Mission Decision Components
The second class of information state compo-
nents is the set of Fire Mission Value components,
which track the value of various information el-
91
ements necessary for requesting a fire mission.
These are specific to the FireSim XXI simulator.
Figure 6 shows the values after utterance 3 in Fig-
ure 1. Components such as ?direction value? take
number values, and components such as ?method
of fire? take values from a finite set of possibilities.
Several of these components, such as ?attitude?
have defaults that are rarely changed. Once the
dialogue manager or human trainer decides that it
has enough information to request fire, these com-
ponents are translated into a simulator command
and sent to the simulator.
method of control: adjust fire
method of fire: adjust fire
method of engagement: none given
target type: -
grid value: -
direction value: 5940
distance value: 480
length: 0
width: 100
attitude: 0
observer coordinate value: 45603595
Figure 6: Fire Mission Value Components
Fire Mission Value components are also directly
modifiable by the trainer. Figure 7 shows the GUI
which the trainer can use to take control of the
session, edit any of the Fire Mission Value com-
ponents, and relinquish control of the session back
to Radiobot-CFF. This allows the trainer to correct
any mistakes that the Radiobot may have made or
test the trainee?s adaptability by sending the fire
to an unexpected location. The example shown in
Figure 7 is after utterance 5 of Figure 1; the sys-
tem is running in semi-automated mode and the
dialogue manager has decided that it has enough
information to send a fire. The trainer may send
the message or edit it and then send it. A second
GUI, not shown, allows the trainer to take con-
trol of the outgoing speech of the Radiobot, and,
in semi-automated mode, either confirm the send-
ing of a suggested output utterance, alter it before
sending, or author new text for the radiobot to say.
The third class of components is the Post-Fire
Value components, which are also exposed to the
trainer for modification. The example shown in
Figure 8 is from after utterance 13 in Figure 1; the
FO has requested an ?adjust fire? with an indica-
tor of ?fire for effect? and a right adjustment of 50.
At this point in the dialogue the FO could have in-
stead chosen to end the mission. If the initial fire
had been a ?fire for effect? it could have been re-
Figure 7: GUI
peated, rather than following up an initial ?adjust
fire.? The adjust fire stage does not have any de-
cision components because typically the adjust in-
formation is given in one move.
adjust fire: true
shift indicator: fire for effect
repeat FFE: false
left-right adjustment: 50
add-drop adjustment: 0
vertical adjustment: 0
end of mission: false
disposition: -
number of casualties: -
Figure 8: Post-Fire Value Components
The fourth class, Disambiguation components,
are used by many rules to disambiguate local in-
formation based on global dialogue features. The
example shown in Figure 9 is from the dialogue
in Figure 1, after utterance 1. The ?mission is
polar? component helps determine the method of
target location if speech recognition erroneously
detects both polar and grid coordinates. Target
numbers allow the FOs to handle multiple mis-
sions at the same time (e.g., starting a new call for
fire, before the previous mission has been com-
pleted). The ?missions active? component tracks
how many missions are currently being discussed.
The ?phase? refers to the state of a three-state FSA
92
that tracks which of the three subdialogue phases
(described in section 3) the dialogue is in for the
most recently-discussed mission.
An example of the use of the Disambiguation
components is to determine whether the phrase
?fire for effect? refers to an adjustment of a pre-
vious mission or the initiation of a new mission.
In utterance 13 in Figure 1, ?fire for effect? refers
to an adjustment of a CFF that began with an ?ad-
just fire? in utterance 1. However, the FO could
have started that CFF by calling for a ?fire for ef-
fect?. Furthermore the FO could have started a
second CFF in utterance 13 rather than doing an
adjust, and might have specified ?fire for effect?.
By using a rule to check the phase of the mission
the move can be disambiguated to understand that
it is referring to an adjustment, rather than the ini-
tiation of a new fire mission.
mission is polar?: true
target number: 0
missions active: 0
last method of fire: adjust
phase: Info-Gathering
Figure 9: Disambiguation Components
The last class of components, shown in Fig-
ure 10, is closely tied to the update rule processing,
and is therefore described in the following section.
current reply: gator nine one this is
steel one nine
previous reply: -
understood? true
send EOM? false
send repeat? false
send repeat adjust? false
send repeat ffe? false
Figure 10: Update Rule Processing Components
4.3 Update Rules
Update rules update the informational compo-
nents, build a message to send to the FO, build
a message to send to the simulator, and decide
whether a message should actually be sent to the
FO or simulator.
As an example of rule application, consider the
processing of utterance 1 in Figure 1. Figure 4
shows the moves and parameters for this utterance.
When the dialogue manager processes this utter-
ance, a set of rules associated with the Identifi-
cation move are applied, which starts building a
response to the FO. This response is built in the
?current reply? Update Rule Processing compo-
nent. Figure 10 shows a reply in the process of
being built: a rule has recognized that an Identifi-
cation move is being given, and has filled in slots
in a template with the necessary information and
added it to the ?current reply? component.
Next, the update rules will recognize that a
Warning Order is being given, and will identify
that it is an ?adjust fire? method of fire, and up-
date the ?has warning order? decision component,
the ?method of control? and ?method of fire? value
components, and the ?last method of fire? disam-
biguation component. As part of this, the appro-
priate fields of the GUIs will be filled in to allow
the trainer to override the FO?s request if need be.
Another rule will then fill in the slots of a template
to add ?adjust fire polar? to the current reply, and
later another rule will add ?out?, thus finishing the
reply to the FO. After the reply is finished, it will
place it in the ?previous reply? component, for ref-
erence if the FO requests a repeat of the previous
utterance.
Certain rules are specified as achieving compre-
hension ? that is, if they are applied, the ?under-
stood? variable for that turn is set. If no reply has
been built but the move has been understood, then
no reply needs to be sent. This happens, for ex-
ample, for each of utterances 8, 10, and 12 in Fig-
ure 1: because they are confirmations of utterances
that the FDC has initiated, they do not need to be
replied to. Similarly, no reply needs to be sent if
no reply has been built and the incoming message
is empty or only contains one or two words in-
dicative of an open mic and background noise. Fi-
nally, if no reply has been built and the move has
not been understood, then the FO is prompted to
repeat the message.
As described above, the Fire Mission Decision
components are used to determine whether to send
a fire mission. For other communications with the
simulator, a simpler approach is possible. The de-
cisions to send an end of mission, a repeat fire, or a
repeat fire with the ?adjust? or ?fire for effect? spec-
ification can be made with update rules acting on
a single boolean, and so these are also part of the
Update Rule Processing Components as shown in
Figure 10.
Finally, the application of rules follows a spe-
cific strategy. A given utterance may contain one
or more dialogue moves, each with a set of rules
specific to it. The dialogue manager applies the
93
appropriate rules to each dialogue move in the
utterance before applying the rules that send the
FO messages or simulator commands, as shown in
Figure 11. Rules for producing replies and simula-
tor commands are delayed until the end of process-
ing an utterance to allow for utterances that may
contain self-corrections or relevant details later in
the turn.
for each dialogue move in utterance
apply rules for that dialogue move
end for
apply rules to send reply to FO
apply rules to send simulator commands
Figure 11: Update Strategy for Rules
5 Error Handling
Radiobot-CFF is able to handle various kind of
problematic input in a number of ways. It can han-
dle partially correct information, as in Figure 12.
Speech recognition errors caused the ?three casu-
alties? information to be lost, but the update rules
were able to handle the essential part of the FO
contribution: that the mission was ended, and that
the target was neutralized. The domain is forgiv-
ing in this particular example, although a strict
trainer might want to intervene by the GUI and
insist that the FO re-submit the end of mission re-
port.
FO Said: end of mission target
neutralized estimate three
casualties over
ASR Output: in end of mission target
neutralized as the make three
catch a these over
Radiobot: end of mission target
neutralized out
Figure 12: Error Correction
In other cases, such as when giving number co-
ordinates, all information must be fully grounded.
An example of this is in Figure 13, where the num-
ber ?five? is lost by the speech recognition. In
this case, the domain-appropriate response is to
prompt for a repetition.
FO Said: right five zero over
ASR Output: right by zero over
Radiobot: say again over
Figure 13: Error Correction - Prompt
6 Evaluation
We conducted an evaluation of the Radiobot-CFF
system in fully-automated, semi-automated, and
human-controlled conditions. The system per-
formed well in a number of measures; for exam-
ple, Table 1 shows the scores for median time-to-
fire and task-completion rates. Additional mea-
sures and further details are available in (Robinson
et al, 2006).
Table 1: Example Evaluation Measures
Measure Human Semi Fully
Time To Fire 106.2 s 139.4 s 104.3 s
Task Compl. 100% 97.5% 85.9%
Of particular relevance here, we performed an
evaluation of the dialogue manager, using the eval-
uation corpus of 17 missions run on 8 sessions, a
total of 408 FO utterances. We took transcribed
recordings of the FO utterances, ran them through
the Interpreter, and corrected them. For each ses-
sion, we ran corrected Interpreter output through
the Dialogue Manager to print out the values of the
informational components at the end of every turn.
We then corrected those, and compared the cor-
rections to the uncorrected values to receive preci-
sion, accuracy, and f-scores of 0.99 each.2
7 Summary
We presented a dialogue manager which can en-
gage in Call for Fire training dialogues, and de-
scribed the environment and system in which it
works. It has an information state-based design
with several components accessible to a human
operator, and may be controlled either fully, in
part, or not at all by that human operator.
8 Acknowledgements
This work has been sponsored by the U.S. Army
Research, Development, and Engineering Com-
mand (RDECOM). Statements and opinions ex-
pressed do not necessarily reflect the position or
the policy of the United States Government, and
no official endorsement should be inferred.
2In this preliminary evaluation, the Interpreter and infor-
mational component corrections were all done by a single
coder; also, the coder was correcting the informational com-
ponent output rather than entering informational component
information from blank, thus any errors of omission on the
part of the coder would work in favor of the system perfor-
mance.
94
We would like to thank Charles Hernandez and
Janet Sutton of the Army Research Laboratory,
and Bill Millspaugh and the Depth & Simultane-
ous Attack Battle Lab in Fort Sill, Oklahoma, for
their efforts on this project. We would also like to
thank the other members of the Radiobots project.
References
James F. Allen, Bradford W. Miller, Eric K. Ringger,
and Teresa Sikorski. 1996. A robust system for nat-
ural spoken dialogue. In Proceedings of the 1996
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-96), pages 62?70.
Department of the Army. 2001. Tactics, techniques
and procedures for observed fire and fire support at
battalion task force and below. Technical Report FM
3-09.30 (6-30), Department of the Army.
H. Aust, M. Oerder, F. Siede, and V. Steinbiss. 1995. A
spoken language enquiry system for automatic train
timetable information. Philips Journal of Research,
49(4):399?418.
Robin Cooper and Staffan Larsson. 1999. Dialogue
moves and information states. In H.C. Bunt and
E. C. G. Thijsse, editors, Proceedings of the Third
International Workshop on Computational Seman-
tics.
Michael Johnston, Srinivas Bangalore, Gunaranjan
Vasireddy, Amanda Stent, Patrick Ehlen, Mari-
lyn Walker, Steve Whittaker, and Preetam Maloor.
2002. Match: An architecture for multimodal dia-
logue systems. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 376?383.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language En-
gineering, 6:323?340, September. Special Issue on
Spoken Language Dialogue System Engineering.
Oliver Lemon, Anne Bracy, Alexander Gruenstein, and
Stanley Peters. 2001. The witas mult-modal dia-
logue system i. In Proc. European Conf. on Speech
Communication and Tech- nology, pages 559?1562.
Colin Matheson, Massimo Poesio, and David Traum.
2000. Modelling grounding and discourse obliga-
tions using update rules. In Proceedings of the First
Conference of the North American Chapter of the
Association for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Bryan Pellom. 2001. Sonic: The university of col-
orado continuous speech recognizer. Technical Re-
port TR-CSLR-2001-01, University of Colorado.
Matthew Purver. 2002. Processing unknown words
in a dialogue system. In Proceedings of the 3rd
ACL SIGdial Workshop on Discourse and Dialogue,
pages 174?183. Association for Computational Lin-
guistics, July.
Susan Robinson, Antonio Roque, Ashish Vaswani, and
David Traum. 2006. Evaluation of a spoken dia-
logue system for military call for fire training. To
Appear.
C. Rose, D. Litman, D. Bhembe, K. Forbes, S. Silli-
man, R. Srivastava, and K. van Lehn. 2003. A com-
parison of tutor and student behavior in speech ver-
sus text based tutoring.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields.
David R. Traum and Jeff Rickel. 2002. Embodied
agents for multi-party dialogue in immersive virtual
worlds. In Proceedings of the first International
Joint conference on Autonomous Agents and Mul-
tiagent systems, pages 766?773.
M. Walker and L. Hirschman. 2000. Evaluation for
darpa communicator spoken dialogue systems.
95
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 54?63,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Degrees of Grounding Based on Evidence of Understanding
Antonio Roque
USC Institute for Creative Technologies
Marina del Rey, CA, USA
roque@ict.usc.edu
David Traum
USC Institute for Creative Technologies
Marina del Rey, CA, USA
traum@ict.usc.edu
Abstract
We introduce the Degrees of Grounding
model, which defines the extent to which ma-
terial being discussed in a dialogue has been
grounded. This model has been developed and
evaluated by a corpus analysis, and includes a
set of types of evidence of understanding, a set
of degrees of groundedness, a set of ground-
ing criteria, and methods for identifying each
of these. We describe how this model can be
used for dialogue management.
1 Introduction
Dialogue system researchers are active in investi-
gating ways of detecting and recovering from er-
ror, including determining when to provide confir-
mations or rejections, or how to handle cases of
complete non-understanding (Bohus and Rudnicky,
2005a; Bohus and Rudnicky, 2005b; Skantze, 2005).
Studying the strategies that humans use when
speaking amongst themselves can be helpful (Swerts
et al, 2000; Paek, 2003; Litman et al, 2006). One
approach to studying how humans manage errors of
understanding is to view conversation as a joint ac-
tivity, in which grounding, or the process of adding
material to the common ground between speakers,
plays a central role (Clark and Schaefer, 1989).
From this perspective, conversations are highly co-
ordinated efforts in which participants work together
to ensure that knowledge is properly understood by
all participants. There is a wide variety of grounding
behavior that is determined by the communication
medium, among other things (Clark and Brennan,
1991).
This approach is developed computationally by
Traum, who presents a model of grounding which
adapts Clark and Schaefer?s contributions model to
make it usable in an online dialogue system (Traum,
1994). Other computational approaches to ground-
ing use decision theory (Paek and Horvitz, 2000a)
or focus on modeling belief (Bunt et al, 2007).
Grounding models generally consider material to
be in one of three states: ungrounded, in the process
of becoming sufficiently grounded, or sufficiently
grounded. (An exception is (Paek and Horvitz,
2000b), who use a continuous model of grounded-
ness.) We are developing a model of grounding that
is attentive to a larger set of types of evidence of un-
derstanding than is typical, and use this to define a
model of Degrees of Grounding, which tracks the
extent to which material has become a part of the
common ground.
This model includes a set of types of Evidence of
Understanding that describes the kinds of cues that
the dialogue gives about the state of grounding. A
set of Degrees of Groundedness describes the ex-
tent to which material has achieved mutual belief
while being discussed. A set of Grounding Crite-
ria describes the degree to which material needs to
be grounded. Finally, the model provides algorithms
to assist dialogue management.
The next section describes the radio domain
which we used to begin developing this model. The
dialogues in this domain contain a large amount of
confirmation behavior, which make it a good testbed
for the initial development of the model. However,
because these radio dialogues are highly structured
we are not yet able to make strong claims about the
54
generality of this model.
In following sections we describe the components
of the model, annotation evaluations, and ongoing
development of the model.
2 Domain
The domain for this corpus analysis involves a radio-
based military training application. This corpus was
developed while building the Radiobot-CFF system
(Roque et al, 2006) in which soldiers are trained
to perform artillery strike requests over a simulated
radio in an immersive virtual environment.
Calls for Fire (CFFs) are coordinated artillery at-
tacks on an enemy. Several teams work together to
execute a CFF. A Forward Observer (FO) team lo-
cates an enemy target and initiates the call. The FO
team is made up of two or more soldiers, usually
with one soldier dedicated to spotting the enemy and
another soldier dedicated to operating the radio. The
FO radio operator communicates with the Fire Di-
rection Center (FDC) team, which decides whether
to execute the attack, and if so, which of the avail-
able fire assets to use. An example CFF is given in
the Appendix.
3 Evidence of Understanding
An influential description of evidence of understand-
ing was presented in (Clark and Schaefer, 1989), as
shown in Table 1. This set of types of evidence was
described as being ?graded roughly from weakest to
strongest? and was part of the acceptance phase of a
two-phase grounding process. (Clark and Brennan,
1991) further develop Clark?s notion of evidence,
describing ?the three most common forms of posi-
tive evidence? as being acknowledgments, initiation
of the relevant next turn, and continued attention.
The Degrees of Grounding model exchanges
Clark and Schaefer?s two-phase model for an ap-
proach that tracks grounding acts in a way similar
to (Traum, 1994). Also, rather than concerning it-
self with the strength of a given type of evidence, the
current model tracks the strength of material based
on its degree of groundedness, which is derived from
sequences of evidence as described in Section 4.
Evidence in the Degrees of Grounding model is
tracked per Common Ground Unit (CGU) in an in-
formation state, as in (Traum and Rickel, 2002). An
Evidence Description
Continued Attention B shows he is continuing to
attend and therefore remains
satisfied with As presentation.
Initiation of Relevant
Next Contribution
B starts in on the next contri-
bution that would be relevant
at a level as high as the current
one.
Acknowledgement B nods or says ?uh huh,?
?yeah,? or the like.
Demonstration B demonstrates all or part of
what he has understood A to
mean.
Display B displays verbatim all or part
of As presentation.
Table 1: (Clark and Schaefer, 1989)?s Evidence of Un-
derstanding between speakers A and B
example of such a CGU is given in Figure 1. Ma-
terial under discussion is disambiguated by several
identifying components of the CGU: in this domain
this is the dialogue move, the parameter, the mission
number, and the adjust number. Note that parameter
value is not used as an identifying component; this
allows for reference to the material by participants
who may not yet agree on its value.
information:
dialogue move: target location
parameter: direction
value: 5940
mission number: to be determined
adjust number: 0
evidence history:
submit-G91, repeat_back-S19
degree of groundedness: agreed-content
grounding criteria met: true
Figure 1: Example Common Ground Unit
The remainder of this section describes the kinds
of evidence of understanding found in the corpus.
Section 6 describes inter-annotator agreement stud-
ies that determine that humans can reliably identify
these types of evidence.
3.1 Submit
A Submit type of evidence is provided when ma-
terial is introduced into the common ground for the
first time. The Submit type of evidence is derived
from the Presentation phase of (Clark and Schaefer,
1989).
55
An example of a Submit is given in line 1 of Table
2: ?direction 6120? is information that had not yet
been mentioned and has no assumed values.
Line ID Utterance Evidence
1 G91 direction 6120 over Submit
2 S19 direction 6120 out Repeat Back
3 G91 correction direction
6210 over
Resubmit
Table 2: Example Dialogue
Dialogue systems that do not specifically model
grounding generally assume that material is
grounded when it is first Submitted unless there is
evidence to the contrary.
3.2 Repeat Back
A Repeat Back type of evidence is provided when
material that was Submitted by another dialogue
participant is presented back to them, often as part
of an explicit confirmation.
The Repeat Back evidence is related to the ?Dis-
play? evidence of (Clark and Schaefer, 1989) and
described in Table 1, however here it is renamed to
indicate that it pertains to verbal repetitions, rather
than general displays which may be in other modal-
ities, such as visual. In fact, there is evidence that
grounding behavior related to visual feedback is dif-
ferent from that related to auditory feedback (Clark
and Brennan, 1991; Thompson and Gergle, 2008).
An example is given in line 2 of Table 2: the
?direction 6120? information given in line 1 is Re-
peated Back as part of a confirmation.
3.3 Resubmit
A Resubmit type of evidence is provided when ma-
terial that has already been Submitted by a dialogue
participant is presented again as part of a self- or
other-correction. This is an example of what (Clark
and Brennan, 1991) call negative evidence, which
indicate a lack of mutual belief.
An example is shown in Table 2; the direction in-
formation which was Submitted in turn 1 and Re-
peated Back in turn 2 is Resubmitted in turn 3.
In this domain, follow-up presentations of mate-
rial were almost always corrections, usually of in-
formation that has been repeated back by the other
participant, or based on new occurences in the vir-
tual world (for example, the lifting of smoke that
was previously obscuring a target.) Due to the na-
ture of the task, this corpus had few instances of
non-correction follow-up behavior, where material
was presented a second time for the purposes of fur-
ther discussion. Such follow-ups are an evidence of
understanding whose behavior is probably different
from that of the Resubmit type of evidence as de-
scribed here, and will be examined in future work as
described in Section 7.
3.4 Acknowledge
An Acknowledge type of evidence is a general state-
ment of agreement that does not specifically address
the content of the material. Acknowledges are iden-
tified by semantic interpretation. Acknowledges are
a part of (Clark and Schaefer, 1989)?s set of types of
evidence of understanding.
Table 3 contains an example: in line 1 the speaker
G91 Submits information about the target?s status,
which is then Acknowledged by speaker S19 in turn
line 2.
Line ID Utterance Evidence
1 G91 end of mission target
destroyed over
Submit
2 S19 roger Acknowledge
Table 3: Example of an Acknowledgment
3.5 Request Repair
A Request Repair type of evidence is a statement
that indicates that the speaker needs to have the
material Resubmitted by the other participant. Re-
quest Repairs are identified by semantic interpreta-
tion. Request Repairs are another example of nega-
tive evidence (Clark and Brennan, 1991).
Table 4 gives an example: in line 1 G91 submits
a map grid coordinate, and in line 2 S19 asks that
the other speaker ?say again? that grid coordinate,
which is a Request for Repair.
Line ID Utterance Evidence
1 G91 grid 5843948 Submit
2 S19 say again grid over Request Repair
Table 4: Example of a Request Repair
56
3.6 Move On
A Move On type of evidence is provided when a
participant decides to proceed to the next step of the
task at hand. This requires that the given task have
a set of well-defined steps, and that the step being
Moved On from needs to be grounded before the
next step can be discussed. Move Ons are identified
based on a model of the task at hand. Move Ons are
related to (Clark and Schaefer, 1989)?s ?Initiation of
the relevant next contribution,? although Clark and
Schaefer do not specify that ?next contributions?
should be dependent on sufficiently grounding the
previous step.
A Move On provides evidence because a cooper-
ative dialogue participant would typically not move
on to the next step of the task under such condi-
tions unless they felt that the previous step was suf-
ficiently grounded.
Table 5 shows an example of a Move On. In line
1, G91 indicates that the kind of artillery fire they
want is a ?fire for effect?; this is Repeated Back in
line 2. G91 then Submits grid information related
to the target location. The task specification of Calls
for Fire indicates that fire requests should proceed in
several steps: after a Warning Order is established, a
Target Location should be given, followed by a Tar-
get Description. By moving on to the step in which
a Target Location is provided, G91 tacitly indicates
that the step in which a Warning Order is established
has been dealt with to their satisfaction.
Line ID Utterance Evidence
1 G91 fire for effect over Submit
2 S19 fire for effect out Repeat Back
3 G91 grid 45183658 Submit, Move
On
Table 5: Example of a Move On
Line ID Utterance Evidence
1 S19 message to observer
kilo 2 rounds AB0001
over
Submit
2 G91 mike tango oscar kilo
2 rounds target number
AB0001 out
Repeat Back
3 S19 shot over Submit
Table 6: Example of a non-Move On
Not all typical sequences provide Move On ev-
idence. In the example in Table 6, in line 1 S91
submits a ?message to observer? indicating the kind
of fire that is being delivered, which is followed in
line 2 by a confirmation by G91. S19 then proceeds
to the next step of the task by indicating in line 3
that the artillery has been fired. Line 3, however, is
not a Move On because although it is typically the
next step in the task, providing that information is
not dependent on fully grounding the material being
discussed in line 2 - in fact, line 3 will be provided
when the artillery has been fired, and not based on
any other decision by S19.
3.7 Use
A Use type of evidence is provided when a partici-
pant presents an utterance that indicates, through its
semantics, that a previous utterance was understood.
Uses are related to (Clark and Schaefer, 1989)?s
?Demonstration?.
In the Radiobot-CFF corpus, most Uses are
replies to a request for information, such as in Ta-
ble 7, where S19?s request for a target description in
line 1 is answered with a target description, in line
2.
Line ID Utterance Evidence
1 S19 s2 wants to know whats
the target description
over
Submit
2 G91 zsu over Submit,
Use
Table 7: Example of a Use
Another example of Use is shown in Table 8, in
which S19 is providing an intelligence report in line
1 regarding an enemy target, and line 2 replies with
a statement asking whether the target is a vehicle.
The utterance in line 2 uses information provided in
line 1.
3.8 Lack of Response
A Lack of Response type of evidence is provided
when neither participant speaks for a given length of
time. Identifying a Lack of Response type of evi-
dence involves determining how much silence will
be significant for signalling understanding or lack of
understanding.
57
Line ID Utterance Evidence
1 S19 again it should have
rather large antennas af-
fixed to it uh they are
still sending out signals
at the time
Submit
2 G91 this is some kind of Submit,
vehicle over Use
Table 8: Example of a Use
In the example shown in Table 9, G91 submits
an identifying utterance to see if S19 is available.
After 12 seconds, G91 has heard nothing back; this
is negative evidence of grounding, so in line 3 G91
resubmits the utterance.
Line ID Utterance Evidence
1 G91 S 1 9 this is G 9 1 Submit
2 (12 seconds of silence) Lack of
Response
3 G91 S 1 9 this is G 9 1 Resubmit
Table 9: Example of a Lack of Response
A Lack of Response can also be an indication of
positive grounding, as in Table 10. In line 1, G91
submits information about a target, which in line 2
is repeated back. Line 3 indicates a period of silence,
in which neither speaker took the opportunity to re-
quest a repair or otherwise indicate their disapproval
with the state of the groundedness of the material. In
that sense, the silence of line 3 is positive evidence
of understanding.
Line ID Utterance Evidence
1 G91 b m p in the open over Submit
2 S19 b m p in the open out Repeat
Back
3 (10 seconds of silence) Lack of
Response
Table 10: Example of a Lack of Response
4 Degrees of Groundedness
Degrees of groundedness are defined such that mate-
rial has a given degree before and after any sequence
of evidence given. For example, in Table 10 the tar-
get description given in line 1 has a certain degree
Degreee Pattern/Identifier
Unknown not yet introduced
Misunderstood (anything,Request Repair)
Unacknowledged (Submit, Lack of Response)
Accessible (Submit) or (anything,Resubmit)
Agreed-Signal (Submit, Acknowledgment)
Agreed-Signal+ (Submit, Acknowledgment, other)
Agreed-Content (Submit, Repeat Back)
Agreed-Content+ (Submit, Repeat Back, other)
Assumed grounded by other means
Table 11: Degrees of Groundedness
of groundedness before it is Submitted, another de-
gree after it is Submitted, another degree after it is
Repeated Back, and another degree after the Lack of
Response.
A key part of defining these degrees is to deter-
mine which of these degrees is worth modeling. For
example, Table 3 shows a CGU further grounded by
a single Acknowledgment. In this domain, for the
purposes of determining grounding criteria and dia-
logue management algorithms, it is not worth distin-
guishing between the case in which it had been fol-
lowed by one more Acknowledgment and the case
in which it had been followed by two or more Ac-
knowledgments.
Table 11 shows the significant degrees identified
during the corpus study, as well as the definition or
identifying pattern of evidence. These degrees are
shown from Unknown, which is least grounded, to
Assumed, which is grounded by other means, such
as written information given during a scenario brief-
ing. Most degrees are identified by patterns of evi-
dence. For example, a CGU is misunderstood if the
latest item of evidence provided is a Request Repair,
and CGU is Unacknowledged if it is Submitted fol-
lowed by a Lack of Response.
The degree of groundedness is used to compute
how much (if any) additional evidence is needed
to reach the grounding criterion, or ?criterion suffi-
cient for current purposes? as defined by (Clark and
Schaefer, 1989). This computation can be used in di-
alogue management to help select a next utterance.
In this domain, information such as target num-
bers have high grounding criteria, such as Agreed-
Content+; they would need to be Repeated Back,
and followed at least by a Lack of Response, giv-
ing the other participant an opportunity to correct.
58
Other information might have a grounding crite-
rion of Agreed-Signal, needing only an Acknowl-
edgment to be grounded, as in Table 3. Future work
will address the fact that grounding criteria are vari-
able: for example, in noisy conditions where errors
are more probable, the grounding criteria may in-
crease.
5 Dialogue Management
Exploiting this model of grounding for dialogue
management involves several steps. Evidence of un-
derstanding must be identified given a semantic in-
terpretation and the history of evidence provided so
far. Given an utterance?s new evidence and a CGU?s
current degree of groundedness, the CGU?s new de-
gree of groundedness must be determined.
Once a CGU?s current degree is determined, it can
be compared to its grounding criterion to determine
whether or not it has been sufficiently grounded, and
if not, a new item of evidence may be suggested to
help further ground the material.
All of these can be put together in one algorithm,
as shown in Figure 2.
for each dialogue act parameter,
identify the relevant CGU
identify evidence of understanding
compute the CGU?s degree of groundedness
for each CGU not sufficiently grounded
determine evidence to be given
compute the CGU?s degree of groundedness
if Lack of Response detected
compute the CGU?s degree of groundedness
Figure 2: Dialogue Management Algorithm
The specifics of how this algorithm is integrated
into a system and how it influences task decisions
will vary based on the system being used. To ex-
plore the domain-independence of this model, we
are currently integrating it into a dialogue manager
in a domain unrelated to the CFF task.
6 Evaluation
The validity of this model has been evaluated in sev-
eral corpus tests to measure inter-annotator agree-
ment in identifying evidence, to ensure that identify-
ing evidence can reliably be done by an algorithm,
to measure inter-annotator agreement in identifying
the increase or decrease of the degree of grounded-
ness, and to ensure that identifying the increase or
decrease of a degree of groundedness can reliably
be done by an algorithm.
Human transcribers produced transcriptions of
several sessions between two sets of humans acting
as Forward Observer and Fire Direction Center radio
operators in the training simulation. A subset of the
corpus was used for close analysis: this subset was
made up of 4 training sessions, composed of 17 fire
missions, totaling 456 utterances; this provided a to-
tal of 1222 possible indicators of evidence of under-
standing made up of 886 dialogue move parameters
and 336 period of silence.
We automatically performed a dialogue act inter-
pretation on the dialogue move parameters, which
were then manually corrected. We then manually
annotated the evidence of understanding identified
in each dialogue move parameter and period of si-
lence. An example of the data produced from this
process is given in the Appendix.
6.1 Inter-Annotator Agreement - Identifying
Evidence
An inter-annotator agreement study was performed
in which two annotators tagged a subset of the cor-
pus (318 dialogue move parameters and 74 silences)
to identify the evidence of understanding, given an
utterance and dialogue act interpretation. One anno-
tator was the first author of this paper, and the other
was a computer professional who had no previous
experience with the domain or with tagging data.
Table 12 shows the results, broken down by the
Standalone types of evidence, which could occur
by themselves (Submit, Repeat Back, Resubmit,
Acknowledge, and Request Repair), the Additional
types of evidence, which only occurred with other
types of evidence (Move On and Use), and the
Silence-Related Lack of Understanding type of ev-
idence. Each of these showed acceptable levels of
agreement, with the exception of the Kappa for the
additional evidence. The low score on the additional
evidence is probably due to the fact that Move On
judgments depend on a strong understanding of the
domain-specific task structure, as described in sec-
tion 3.6; to a lesser extent Use judgments tend to
rely on an understanding of the scenario as well.
59
Evidence Type P(A) Kappa
Standalone 0.95 0.91
Additional 0.87 0.53
Silence-Related 0.92 0.84
Table 12: Inter-Annotator Agreement - Evidence
Evidence Type P(A) Kappa
Standalone 0.88 0.81
Additional 0.98 0.92
Silence-Related 1.0 1.0
Table 13: Algorithm Agreement - Evidence
This highlights the fact that for most of the evidence
of understanding (all except for Move On and Use),
agreement can be reached with a non-expert annota-
tor.
6.2 Algorithm Agreement - Identifying
Evidence
The results of the inter-annotator agreement test
were merged into the larger 1222-markable corpus,
to create a consensus human-annotated corpus. This
was used in the next test, to identify whether an al-
gorithm can automatically identify evidence.
We authored a set of rules to identify evidence of
understanding based on the order in which CGUs
were introduced into the common ground, the iden-
tity of the speaker who introduced them, and the
semantic interpretations. The rules were then ap-
plied to the 1222-markable corpus, and the resulting
identifications were then compared to the identifica-
tions made by the human annotators. The results are
shown in Table 13. The respectable agreement and
kappa values indicate that it is possible for an algo-
rithm to reliably identify evidence.
6.3 Degree Increase/Decrease Agreements
Finally, we explored whether humans could reliably
agree on whether a given material?s groundedness
had increased or decreased after a given turn.
We studied this because we are not here claiming
that humans explicitly model degrees of grounded-
ness or perform a computation to compare a given
material with something they had grounded pre-
viously. It is more likely that humans track evi-
dence, determine whether material is more or less
grounded than it was before, and check whether it
Agreement Type P(A) Kappa
Human-Human 0.97 0.94
Human-Algorithm 0.87 0.73
Table 14: Degree Increase/Decrease Agreements
has reached a grounding criterion. A dialogue sys-
tem need not be tied to human behavior to be effec-
tive, so given these human behaviors, we are inter-
ested in whether computer algorithms can be built
to produce useful results in terms of task completion
and human-realistic behavior. For this reason we
evaluate the model of degrees of grounding based on
how human-realistic its ability to identify whether a
CGU?s degree of groundedness has increased or de-
creased, and in future work study whether a system
implementation performs acceptably in terms of task
completion and managing human-realistic ground-
ing behavior.
To perform the test of whether degree increase or
decrease could be reliable detected, we annotated a
subset of the corpus with a non-domain expert. For
a set of CGUs, we tracked the sequence of evidence
that was provided to ground that CGU. Before and
after each item of evidence, we asked the annota-
tors to determine whether the CGU was more or less
grounded than it was the turn before.
We also developed a set of rules based on the defi-
nition of the degrees of groundedness defined in sec-
tion 4 to determine after each utterance whether a
CGU?s degree of groundedness had increased or de-
creased from the utterance before. We then com-
pared the results of that set of rules with human-
consensus judgments about degree increase and de-
crease.
The results are shown in Table 14, indicating that
humans could reliably agree among themselves, and
a rule-based algorithm could reliably agree with the
human consensus judgments.
7 Discussion and Future Work
In this paper we describe the initial development of
the Degrees of Grounding model, which tracks the
extent to which material has been grounded in a di-
alogue. The Degrees of Grounding model contains
a richer variety of evidence of understanding than
most models of grounding, which allows us to de-
60
fine a full set of degrees of groundedness.
We recognize that the initial domain, although
rich in grounding behavior, is not typical of most hu-
man conversation. Besides the structured dialogues
and the domain-specific word use, the types of evi-
dence of understanding presented in Section 3 does
not cover all possible types of evidence. For ex-
ample, (Clark and Schaefer, 1989) describe ?contin-
ued attention? as another possibility, which was not
available with the radio modality used in this study.
Furthermore, it is a feature of this domain that Re-
submit evidence generally indicates lack of under-
standing; in general conversation, it is not true that
the repeated mention of material indicates that it is
not understood, so a ?Follow-Up? evidence is likely,
as are variations of ?Use.?
To explore these questions, we are extending
work to other domains, and are currently focusing
on one in which virtual humans are used for a ques-
tioning task. Also, we plan to run evaluations in im-
plemented systems, exploring performance in terms
of task completion and believable human behavior.
Acknowledgments
This work has been sponsored by the U.S. Army Re-
search, Development, and Engineering Command
(RDECOM). Statements and opinions expressed do
not necessarily reflect the position or the policy of
the United States Government, and no official en-
dorsement should be inferred.
The authors would like to thank Kevin Knight
and the anonymous reviewers for feedback about the
evaluation.
References
Dan Bohus and Alexander Rudnicky. 2005a. Error han-
dling in the RavenClaw dialog management architec-
ture. In Proceedings of HLT-EMNLP-2005.
Dan Bohus and Alexander Rudnicky. 2005b. Sorry,
I didn?t catch that! - an investigation of non-
understanding errors and recovery strategies. In Pro-
ceedings of SIGdial-2005. Lisbon, Portugal.
Harry Bunt, Roser Morante, and Simon Keizer. 2007. An
empirically based computational model of grounding
in dialogue. In Proceedings of the 8th SIGdial Work-
shop on Discourse and Dialogue.
Herbert H. Clark and Susan E. Brennan. 1991. Ground-
ing in communication. In Perspectives on Socially
Shared Cognition, pages 127?149. APA Books.
Herbert H Clark and Edward F Schaefer. 1989. Con-
tributing to discourse. Cognitive Science, 13:259?294.
Diane Litman, Julia Hirschberg, and Marc Swerts. 2006.
Characterizing and predicting corrections in spoken
dialogue systems. Computational linguistics, pages
417?438.
Tim Paek and Eric Horvitz. 2000a. Conversation as
action under uncertainty. In Proceedings of the 16th
Conference on Uncertainty in Artificial Intelligence
(UAI), pages 455?464.
Tim Paek and Eric Horvitz. 2000b. Grounding criterion:
Toward a formal theory of grounding. Technical re-
port, Microsoft Research, April. Microsoft Technical
Report, MSR-TR-2000-40.
Tim Paek. 2003. Toward a taxonomy of communica-
tion errors. In Proceedings of the ISCA Tutorial and
Research Workshop on Error Handling in Spoken Di-
alogue Systems, pages 53?58, August 28-31. Chateau
d?Oex, Vaud, Switzerland.
Antonio Roque, Anton Leuski, Vivek Rangarajan, Su-
san Robinson, Ashish Vaswani, Shri Narayanan, and
David Traum. 2006. Radiobot-CFF: A spoken dia-
logue system for military training. In 9th International
Conference on Spoken Language Processing (Inter-
speech 2006 - ICSLP), September.
Gabriel Skantze. 2005. Galatea: a discourse modeller
supporting concept-level error handling in spoken dia-
logue systems. In Proceedings of SigDial, pages 178?
189). Lisbon, Portugal.
Marc Swerts, Diane Litman, and Julia Hirschberg. 2000.
Corrections in spoken dialogue systems. In Proceed-
ings of the 6th International Conference of Spoken
Language Processing (ICSLP-2000), October.
Will Thompson and Darren Gergle. 2008. Modeling
situated conversational agents as partially observable
markov decision processes. In Proceedings of Intelli-
gent User Interfaces (IUI).
David Traum and Jeff Rickel. 2002. Embodied agents
for multi-party dialogue in immersive virtual world.
In Proceedings of the First International Joint Confer-
ence on Autonomous Agents and Multi-agent Systems
(AAMAS 2002), pages 766?773, July.
David R. Traum. 1994. A Computational Theory of
Grounding in Natural Language Conversation. Ph.D.
thesis, University of Rochester.
61
Appendix
Line ID Utterance Semantic Interpretation Evidence:
Standalone
Evidence:
Additional
1 G91 fire for effect over WO-MOF: fire for effect Submit
2 S19 fire for effect out WO-MOF: fire for effect Repeat Back
3 Silence: 0.7 seconds
ah roger ROGER Acknowledge
4 G91 grid four five four two ah three six
three eight
TL-GR: 45423638 Submit Move On
5 Silence: 2.3 seconds
6 S19 grid four five four two three six
three eight out
TL-GR: 45423638 Repeat Back
7 Silence: 0.7 seconds
ah roger ROGER Acknowledge
8 G91 b r d m TD-TYPE: b r d m Submit Move On
in the open over TD-DESC: in the open Submit
9 Silence: 1.3 seconds
10 S19 b r d m TD-TYPE: b r d m Repeat Back
in the open out TD-DESC: in the open Repeat Back
11 Silence: 9.9 seconds Lack of Re-
sponse
Comments:
This dialogue is between G91 as a Forward Observer identifying a target, and S19 as a Fire Direction
Center who will send the artillery fire when given the appropriate information.
In line 1, G19?s utterance is interpreted as a Warning Order - Method of Fire (WO-MOF), describing the
kind of artillery fire requested, whose value is ?fire for effect.? This is the first mention of a WO-MOF for
this particular CFF, so it is identified as a Submit type of evidence related to a new CGU, which now has an
Accessible degree of groundedness.
In line 2, a WO-MOF is again given. The WO-MOF is identified as referring to the CGU introduced
in line 1, and a Repeat Back type of evidence is added to that CGU?s evidence history, which gives it an
Agreed-Content degree of groundedness.
In line 3 there follows a silence that is not long enough to be a Lack of Response.
In line 4, G91 provides an Acknowledge type of evidence, and Moves On to the next task item: identifying
the Target Location - Grid (TL-GR) of the CFF. The Acknowledge and Move On, referring to the CGU
created in line 1, raise that CGU?s degree of groundedness to its grounding criterion of Agreed-Content+, at
which point it becomes grounded. At the same time, the introduction of the TL-GR information creates a
new CGU, whose degree is Accessible.
In line 6 the TL-GR CGU is Repeated Back, thereby raising its degree of groundedness to Agreed-Content.
In line 8 an Acknowledge is provided and a set of information related to the Target Description (TD-) is
given, providing a Move On, thereby grounding the TL-GR CGU. So by line 8, two CGUs (WO-MOF and
TL-GR) have been added to the common ground, and two more CGUs (TD-TYPE and TD-DESC) have
Accessible degrees and are in the process of being grounded.
In line 10 the TD CGUs are Repeated Back, raising their degree of groundedness to Agreed-Content.
In line 11 the Lack of Response raises the TD CGUs to Agreed-Content+ thereby grounding them. At this
point there is enough information in the common ground for S19 to send the artillery fire.
62
Line ID Utterance Semantic Interpretation Evidence:
Standalone
Evidence:
Additional
message to observer kilo MTO-BAT: kilo Submit
12 S19 two rounds MTO-NUM: two Submit Move On
target number alpha bravo zero
zero one over
TN: AB001 Submit
13 Silence: 3.1 seconds
a roger mike tango alpha ah alpha ROGER Acknowledge
14 G91 target number alpha bravo zero
zero zero one
TN: AB0001 Repeat Back
a kilo MTO-BAT: kilo Repeat Back
two rounds out MTO-NUM: two Repeat Back
11 Silence: 11.4 seconds Lack of Re-
sponse
16 S19 shot SHOT Submit
rounds complete over RC Submit
17 Silence: 0.8 seconds
18 G91 shot SHOT Repeat Back
rounds complete out RC Repeat Back
19 S19 splash over SPLASH Submit
20 Silence: 1.5 seconds
21 G91 splash out SPLASH Repeat Back
22 Silence: 30.4 seconds Lack of Re-
sponse
...
ah end of mission a target number
alpha bravo zero zero one
TN: AB001 Submit
23 G91 one EOM-NUM: one Submit
b r d m EOM-TYPE: b r d m Submit
destroyed over EOM-BDA: destroyed Submit
24 S19 end of mission b r d des m d cor-
rection b r d m
EOM-TYPE: b r d m Repeat Back
destroyed out EOM-BDA: destroyed Repeat Back
Comments:
In line 12, S19 provides information about the artillery fire that is going to be sent. This includes the
battery that will be firing (MTO-BAT), the number of rounds to be fired (MTO-NUM) and the target number
that will be used to refer to this particular fire mission from that point on (TN).
In line 14, G91 Repeats Back the information presented in line 12 along with an Acknowledge.
In line 16, S19 notifies that the mission has been fired; in line 18 this is confirmed. Likewese, in line 19
S19 notifies that the mission is about the land; in line 21 this is confirmed.
Between lines 22 and 23 several turns have been removed for space reasons. These turns were related to an
adjustment of the artillery fire: after the initial bombardment, the Forward Observer requested that the same
artillery be fired 100 meters to the left of the original bombardment. This was confirmed and delivered.
In line 23, G91 sends a description of the amount of damage suffered by the target: the number of enemy
affected (EOM-NUM), the type of enemy (EOM-TYPE) and the extent of the damage (EOM-BDA). These
are Repeated Back by S19, thereby ending the CFF. Note that S19 does not Repeat Back the EOM-NUM. In
this particular instance the number of enemies is implied by the EOM-TYPE being singular, but throughout
the corpus EOMs are seen to have a low grounding criteria.
63
A Hybrid Approach to Content Analysis for Automatic Essay Grading
Carolyn P. Rose?, Antonio Roque, Dumisizwe Bhembe, and Kurt VanLehn
LRDC, University of Pittsburgh, 3939 O?hara St., Pittsburgh, PA 15260
rosecp@pitt.edu
Abstract
We present CarmelTC, a novel hybrid text clas-
sification approach for automatic essay grad-
ing. Our evaluation demonstrates that the hy-
brid CarmelTC approach outperforms two ?bag
of words? approaches, namely LSA and a Naive
Bayes, as well as a purely symbolic approach.
1 Introduction
In this paper we describe CarmelTC
 
, a novel automatic
essay grading approach using a hybrid text classifica-
tion technique for analyzing essay answers to qualitative
physics questions inside the Why2 tutorial dialogue sys-
tem (VanLehn et al, 2002). In contrast to many previ-
ous approaches to automated essay grading (Burstein et
al., 1998; Foltz et al, 1998; Larkey, 1998), our goal is
not to assign a letter grade to student essays. Instead, our
purpose is to tally which set of ?correct answer aspects?
are present in student essays. Previously, tutorial dia-
logue systems such as AUTO-TUTOR (Wiemer-Hastings
et al, 1998) and Research Methods Tutor (Malatesta et al,
2002) have used LSA (Landauer et al, 1998) to perform
the same type of content analysis for student essays that
we do in Why2. While Bag of Words approaches such as
LSA have performed successfully on the content analy-
sis task in domains such as Computer Literacy (Wiemer-
Hastings et al, 1998), they have been demonstrated to
perform poorly in causal domains such as research meth-
ods (Malatesta et al, 2002) because they base their pre-
dictions only on the words included in a text and not on
the functional relationships between them. Thus, we pro-
pose CarmelTC as an alternative. CarmelTC is a rule
learning text classification approach that bases its predic-
tions both on features extracted from CARMEL?s deep

This research was supported by the ONR, Cognitive Sci-
ence Division under grant number N00014-0-1-0600 and by
NSF grant number 9720359 to CIRCLE.
syntactic functional analyses of texts (Rose?, 2000) and a
?bag of words? classification of that text obtained from
Rainbow Naive Bayes (McCallum and Nigam, 1998).
We evaluate CarmelTC in the physics domain, which is
a highly causal domain like research methods. In our
evaluation we demonstrate that CarmelTC outperforms
both Latent Semantic Analysis (LSA) (Landauer et al,
1998) and Rainbow Naive Bayes (McCallum and Nigam,
1998), as well as a purely symbolic approach similar to
(Furnkranz et al, 1998). Thus, our evaluation demon-
strates the advantage of combining predictions from sym-
bolic and ?bag of words? approaches for content analysis
aspects of automatic essay grading.
2 Student Essay Analysis
We cast the Student Essay Analysis problem as a text clas-
sification problem where we classify each sentence in the
student?s essay as an expression one of a set of ?correct
answer aspects?, or ?nothing? in the case where no ?cor-
rect answer aspect? was expressed. Essays are first seg-
mented into individual sentence units. Next, each seg-
ment is classified as corresponding to one of the set of key
points or ?nothing? if it does not include any key point.
We then take an inventory of the classifications other than
?nothing? that were assigned to at least one segment. We
performed our evaluation over essays collected from stu-
dents interacting with our tutoring system in response to
the question ?Suppose you are running in a straight line at
constant speed. You throw a pumpkin straight up. Where
will it land? Explain.?, which we refer to as the Pumpkin
Problem. Thus, there are a total of six alternative classifi-
cations for each segment:
Class 1 After the release the only force acting on the
pumpkin is the downward force of gravity.
Class 2 The pumpkin continues to have a constant hori-
zontal velocity after it is released.
Class 3 The horizontal velocity of the pumpkin contin-
ues to be equal to the horizontal velocity of the man.
Class 4 The pumpkin and runner cover the same distance
over the same time.
Class 5 The pumpkin will land on the runner.
Class 6 Sentence does not adequately express any of the
above specified key points.
Often what distinguishes sentences from one class and
another is subtle. For example, ?The pumpkin?s horizon-
tal velocity, which is equal to that of the man when he re-
leased it, will remain constant.? belongs to Class 2. How-
ever, it could easily be mistaken for Class 3 based on the
set of words included, although it does not express that
idea since it does not address the relationship between the
pumpkin?s and man?s velocity after the release. Similarly,
?So long as no other horizontal force acts upon the pump-
kin while it is in the air, this velocity will stay the same.?,
belongs to Class 2 although looks similar on the surface to
either Class 1 or 3. Nevertheless, it does not express the
required propositional content for either of those classes.
The most frequent problem is that sentences that express
most but not all of the content associated with a required
point should be classified as ?nothing? although they have
a lot of words in common with sentences from the class
that they are most similar to. Similarly, sentences like ?It
will land on the ground where the runner threw it up.?
contain all of the words required to correctly express the
idea corresponding to Class 5, although it does not ex-
press that idea, and in fact expresses a wrong idea. These
very subtle distinctions pose problems for ?bag of words?
approaches since they base their decisions only on which
words are present regardless of their order or the func-
tional relationships between them.
The hybrid CarmelTC approach induces decision trees
using features from a deep syntactic functional analysis
of an input text as well as a prediction from the Rainbow
Naive Bayes text classifier (McCallum and Nigam, 1998).
Additionally, it uses features that indicate the presence or
absence of words found in the training examples. From
these features CarmelTC builds a vector representation
for each sentence. It then uses the ID3 decision tree learn-
ing algorithm (Quinlin, 1993) to induce rules for identify-
ing sentence classes based on these feature vectors.
From CARMEL?s deep syntactic analysis of a sen-
tence, we extract individual features that encode func-
tional relationships between syntactic heads (e.g., (subj-
throw man)), tense information (e.g., (tense-throw past)),
and information about passivization and negation (e.g.,
(negation-throw +) or (passive-throw -)). Syntactic fea-
ture structures produced by the grammar factor out those
aspects of syntax that modify the surface realization of
a sentence but do not change its deep functional analy-
sis, including syntactic transformations such as passiviza-
tion and extraction. These deep functional relationships
give CarmelTC the information lacking on Bag of Words
approaches that is needed for effective content analysis
in highly causal domains, such as research methods or
physics.
3 Evaluation
We conducted an evaluation to compare the effective-
ness of CarmelTC at analyzing student essays in compar-
ison to LSA, Rainbow, and a purely symbolic approach
similar to (Furnkranz et al, 1998), which we refer to
here as CarmelTCsymb. CarmelTCsymb is identical to
CarmelTC except that it does not include in its feature set
the prediction from Rainbow. We conducted our evalua-
tion over a corpus of 126 previouslyunseen student essays
in response to the Pumpkin Problem described above,
with a total of 500 text segments, and just under 6000
words altogether. Each text segment was hand tagged
by at least two coders, and conflicts were resolved at a
consensus meeting. Pairwise Kappas between our three
coders computed over initial codings of our data was al-
ways above .75.
The LSA space used for this evaluation was trained
over three first year physics text books. The Rainbow
models used to generate the Rainbow predictions that are
part of the feature set provided to CarmelTC were trained
over a development corpus of 248 hand tagged example
sentences extracted from a corpus of human-human tu-
toring dialogues, just like those included in the 126 es-
says mentioned above. However, when we evaluated the
performance of Rainbow for comparison with CarmelTC,
LSA, and the symbolic approach, we ran a 50 fold cross
validation evaluation using the complete set of examples
in both sets (i.e., the 248 sentences used to train the Rain-
bow models used to by CarmelTC as well as the 126 es-
says) so that Rainbow would have access to the exact
same training data as CarmelTC, to make it a fair com-
parison between alternative machine learning approaches.
On each iteration, we randomly selected a subset of essays
such that the number of text segments included in the test
set were greater than 10 but less than 15 and then train-
ing Rainbow using the remaining text segments. Thus,
CarmelTC uses the same set of training data, but unlike
the other approaches, it uses its training data in two sepa-
rate parts, namely one to train the Rainbow models it uses
to produce the Rainbow prediction that is part of the vec-
tor representation it builds for each text and one to train
the decision trees. This is because for CarmelTC, the data
for training Rainbow must be separate from that used to
train the decision trees so the decision trees are trained
from a realistic distribution of assigned Rainbow classes
based on its performance on unseen data rather than on
Figure 1: This Table compares the performance of the 3 alternative approaches
Approach Precision Recall False Alarm Rate F-Score
LSA 93% 54% 3% .70
Rainbow 81% 73% 9% .77
CarmelTCsymb 88% 72% 7% .79
CarmelTC 90% 80% 8% .85
Rainbow?s training data. Thus, for CarmelTC, we also
performed a 50 fold cross validation, but this time only
over the set of 126 example essays not used to train the
Rainbow models used by CarmelTC.
Note that LSA works by using its trained LSA space
to construct a vector representation for any text based on
the set of words included therein. It can thus be used
for text classification by comparing the vector obtained
for a set of exemplar texts for each class with that ob-
tained from the text to be classified. We tested LSA using
as exemplars the same set of examples used as Rainbow
training data, but it always performed better when using a
small set of hand picked exemplars. Thus, we present re-
sults here using only those hand picked exemplars. For
every approach except LSA, we first segmented the es-
says at sentence boundaries and classified each sentence
separately. However, for LSA, rather than classify each
segment separately, we compared the LSA vector for the
entire essay to the exemplars for each class (other than
?nothing?), since LSA?s performance is better with longer
texts. We verified that LSA also performed better specif-
ically on our task under these circumstances. Thus, we
compared each essay to each exemplar, and we counted
LSA as identifying the corresponding ?correct answer as-
pect? if the cosine value obtained by comparing the two
vectors was above a threshold. We used a threshold value
of .53, which we determined experimentally to achieve
the optimal f-score result, using a beta value of 1 in order
to treat precision and recall as equally important.
Figure 1 demonstrates that CarmelTC out performs the
other approaches, achieving the highest f-score, which
combines the precision and recall scores into a single
measure. Thus, it performs better at this task than two
commonly used purely ?bag of words? approaches as well
as to an otherwise equivalent purely symbolic approach.
References
J. Burstein, K. Kukich, S. Wolff, C. Lu, M. Chodorow,
L. Braden-Harder, and M. D. Harris. 1998. Automated
scoring using a hybrid feature identification technique.
In Proceedings of COLING-ACL?98, pages 206?210.
P. W. Foltz, W. Kintsch, and T. Landauer. 1998. The
measurement of textual coherence with latent semantic
analysis. Discourse Processes, 25(2-3):285?307.
J. Furnkranz, T. Mitchell Mitchell, and E. Riloff. 1998.
A case study in using linguistic phrases for text cat-
egorization on the www. In Proceedings from the
AAAI/ICML Workshop on Learning for Text Catego-
rization.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. In-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25(2-3):259?284.
L. Larkey. 1998. Automatic essay grading using text cat-
egorization techniques. In Proceedings of SIGIR.
K. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the short answer question with research
methods tutor. In Proceedings of the Intelligent Tutor-
ing Systems Conference.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification. In
Proceedings of the AAAI-98 Workshop on Learning for
Text Classification.
J. R. Quinlin. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann Publishers: San Mateo, CA.
C. P. Rose?. 2000. A framework for robust semantic in-
terpretation. In Proceedings of the First Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 311?318.
K. VanLehn, P. Jordan, C. P. Rose?, and The Natural Lan-
guage Tutoring Group. 2002. The architecture of
why2-atlas: a coach for qualitative physics essay writ-
ing. Proceedings of the Intelligent Tutoring Systems
Conference.
P. Wiemer-Hastings, A. Graesser, D. Harter, and the Tu-
toring Res earch Group. 1998. The foundations and
architecture of autotutor. In B. Goettl, H. Halff, C. Red-
field, and V. Shute, editors, Intelligent Tutoring Sys-
tems: 4th International Conference (ITS ?98 ), pages
334?343. Springer Verlag.
A Hybrid Text Classication Approach for Analysis of Student Essays
Carolyn P. Rose?, Antonio Roque, Dumisizwe Bhembe, Kurt Vanlehn
Learning Research and Development Center, University of Pittsburgh,
3939 O?Hara St., Pittsburgh, PA 15260
rosecp,roque,bhembe,vanlehn@pitt.edu
Abstract
We present CarmelTC, a novel hybrid text clas-
sification approach for analyzing essay answers
to qualitative physics questions, which builds
upon work presented in (Rose? et al, 2002a).
CarmelTC learns to classify units of text based
on features extracted from a syntactic analysis
of that text as well as on a Naive Bayes clas-
sification of that text. We explore the trade-
offs between symbolic and ?bag of words? ap-
proaches. Our goal has been to combine the
strengths of both of these approaches while
avoiding some of the weaknesses. Our evalu-
ation demonstrates that the hybrid CarmelTC
approach outperforms two ?bag of words? ap-
proaches, namely LSA and a Naive Bayes, as
well as a purely symbolic approach.
1 Introduction
In this paper we describe CarmelTC, a novel hybrid
text classification approach for analyzing essay answers
to qualitative physics questions. In our evaluation we
demonstrate that the novel hybrid CarmelTC approach
outperforms both Latent Semantic Analysis (LSA) (Lan-
dauer et al, 1998; Laham, 1997) and Rainbow (Mc-
Callum, 1996; McCallum and Nigam, 1998), which is
a Naive Bayes approach, as well as a purely symbolic
approach similar to (Furnkranz et al, 1998). Whereas
LSA and Rainbow are pure ?bag of words? approaches,
CarmelTC is a rule learning approach where rules for
classifying units of text rely on features extracted from
a syntactic analysis of that text as well as on a ?bag
of words? classification of that text. Thus, our evalu-
ation demonstrates the advantage of combining predic-
tions from symbolic and ?bag of words? approaches for
text classification. Similar to (Furnkranz et al, 1998),
neither CarmelTC nor the purely symbolic approach re-
quire any domain specific knowledge engineering or text
annotation beyond providing a training corpus of texts
matched with appropriate classifications, which is also
necessary for Rainbow, and to a much lesser extent for
LSA.
CarmelTC was developed for use inside of the Why2-
Atlas conceptual physics tutoring system (VanLehn et al,
2002; Graesser et al, 2002) for the purpose of grad-
ing short essays written in response to questions such as
?Suppose you are running in a straight line at constant
speed. You throw a pumpkin straight up. Where will it
land? Explain.? This is an appropriate task domain for
pursuing questions about the benefits of tutorial dialogue
for learning because questions like this one are known
to elicit robust, persistent misconceptions from students,
such as ?heavier objects exert more force.? (Hake, 1998;
Halloun and Hestenes, 1985). In Why2-Atlas, a stu-
dent first types an essay answering a qualitative physics
problem. A computer tutor then engages the student in
a natural language dialogue to provide feedback, cor-
rect misconceptions, and to elicit more complete expla-
nations. The first version of Why2-Atlas was deployed
and evaluated with undergraduate students in the spring
of 2002; the system is continuing to be actively devel-
oped (Graesser et al, 2002).
In contrast to many previous approaches to automated
essay grading (Burstein et al, 1998; Foltz et al, 1998;
Larkey, 1998), our goal is not to assign a letter grade
to student essays. Instead, our purpose is to tally which
set of ?correct answer aspects? are present in student es-
says. For example, we expect satisfactory answers to the
example question above to include a detailed explana-
tion of how Newton?s first law applies to this scenario.
From Newton?s first law, the student should infer that the
pumpkin and the man will continue at the same constant
horizontal velocity that they both had before the release.
Thus, they will always have the same displacement from
the point of release. Therefore, after the pumpkin rises
and falls, it will land back in the man?s hands. Our goal
is to coach students through the process of constructing
good physics explanations. Thus, our focus is on the
physics content and not the quality of the student?s writ-
ing, in contrast to (Burstein et al, 2001).
2 Student Essay Analysis
We cast the Student Essay Analysis problem as a text
classification problem where we classify each sentence in
the student?s essay as an expression one of a set of ?cor-
rect answer aspects?, or ?nothing? in the case where no
?correct answer aspect? was expressed.
After a student attempts an initial answer to the ques-
tion, the system analyzes the student?s essay to assess
which key points are missing from the student?s argu-
ment. The system then uses its analysis of the student?s
essay to determine which help to offer that student. In
order to do an effective job at selecting appropriate inter-
ventions for helping students improve their explanations,
the system must perform a highly accurate analysis of the
student?s essay. Identifying key points as present in es-
says when they are not (i.e., false alarms), cause the sys-
tem to miss opportunities to help students improve their
essays. On the other hand, failing to identify key points
that are indeed present in student essays causes the sys-
tem to offer help where it is not needed, which can frus-
trate and even confuse students. A highly accurate inven-
tory of the content of student essays is required in order
to avoid missing opportunities to offer needed instruction
and to avoid offering inappropriate feedback, especially
as the completeness of student essays increases (Rose? et
al., 2002a; Rose? et al, 2002c).
In order to compute which set of key points, i.e., ?cor-
rect answer aspects?, are included in a student essay, we
first segment the essay at sentence boundaries. Note that
run-on sentences are broken up. Once an essay is seg-
mented, each segment is classified as corresponding to
one of the set of key points or ?nothing? if it does not
include any key point. We then take an inventory of the
classifications other than ?nothing? that were assigned to
at least one segment. Thus, our approach is similar in
spirit to that taken in the AUTO-TUTOR system (Wiemer-
Hastings et al, 1998), where Latent Semantic Analysis
(LSA) (Landauer et al, 1998; Laham, 1997) was used to
tally which subset of ?correct answer aspects? students
included in their natural language responses to short es-
say questions about computer literacy.
We performed our evaluation over essays collected
from students interacting with our tutoring system in re-
sponse to the question ?Suppose you are running in a
straight line at constant speed. You throw a pumpkin
straight up. Where will it land? Explain.?, which we refer
to as the Pumpkin Problem. Thus, there are a total of six
alternative classifications for each segment:
Class 1 Sentence expresses the idea that after the release
the only force acting on the pumpkin is the down-
ward force of gravity.
Class 2 Sentence expresses the idea that the pumpkin
continues to have a constant horizontal velocity after
it is released.
Class 3 Sentence expresses the idea that the horizontal
velocity of the pumpkin continues to be equal to the
horizontal velocity of the man.
Class 4 Sentence expresses the idea that the pumpkin
and runner cover the same distance over the same
time.
Class 5 Sentence expresses the idea that the pumpkin
will land on the runner.
Class 6 Sentence does not adequately express any of the
above specified key points.
Note that this classification task is strikingly different
from those typically used for evaluating text classifica-
tion systems. First, these classifications represent spe-
cific whole propositions rather than general topics, such
as those used for classifying web pages (Craven et al,
1998), namely ?student?, ?faculty?, ?staff?, etc. Sec-
ondly, the texts are much shorter, i.e., one sentence in
comparison with a whole web page, which is a disadvan-
tage for ?bag of words? approaches.
In some cases what distinguishes sentences from one
class and sentences from another class is very subtle.
For example, ?Thus, the pumpkin?s horizontal velocity,
which is equal to that of the man when he released it, will
remain constant.? belongs to Class 2 although it could
easily be mistaken for Class 3. Similarly, ?So long as
no other horizontal force acts upon the pumpkin while it
is in the air, this velocity will stay the same.?, belongs
to Class 2 although looks similar on the surface to ei-
ther Class 1 or 3. A related problem is that sentences
that should be classified as ?nothing? may look very sim-
ilar on the surface to sentences belonging to one or more
of the other classes. For example, ?It will land on the
ground where the runner threw it up.? contains all of the
words required to correctly express the idea correspond-
ing to Class 5, although it does not express this idea, and
in fact expresses a wrong idea. These very subtle distinc-
tions also pose problems for ?bag of words? approaches
since they base their decisions only on which words are
present regardless of their order or the functional relation-
ships between them. That might suggest that a symbolic
approach involving syntactic and semantic interpretation
might be more successful. However, while symbolic ap-
proaches can be more precise than ?bag of words? ap-
proaches, they are also more brittle. And approaches that
rely both on syntactic and semantic interpretation require
a larger knowledge engineering effort as well.
3 CarmelTC
Figure 1: This example shows the deep syntactic parse
of a sentence.
Sentence: The pumpkin moves slower because the
man is not exerting a force on it.
Deep Syntactic Analysis
((clause2
((mood *declarative)
(root move)
(tense present)
(subj
((cat dp)(root pumpkin)
(specifier ((cat detp)(def +)(root the)))
(modifier ((car adv) (root slow)))))))
(clause2
(mood *declarative)
(root exert)
(tense present)
(negation +)
(causesubj
((cat dp)(root man)(agr 3s)
(specifier
((cat detp)(def +)(root the)))))
(subj
((cat dp)(root force)
(specifier ((cat detp)(root a)))))
(obj ((cat dp)(root it))))
(connective because))
The hybrid CarmelTC approach induces decision trees
using features from both a deep syntactic functional anal-
ysis of an input text as well as a prediction from the Rain-
bow Naive Bayes text classifier (McCallum, 1996; Mc-
Callum and Nigam, 1998) to make a prediction about the
correct classification of a sentence. In addition, it uses
features that indicate the presence or absence of words
found in the training examples. Since the Naive Bayes
classification of a sentence is more informative than any
single one of the other features provided, CarmelTC can
be conceptualized as using the other features to decide
whether or not to believe the Naive Bayes classification,
and if not, what to believe instead.
From the deep syntactic analysis of a sentence, we ex-
tract individual features that encode functional relation-
Figure 2: This example shows the features extracted
from the deep syntactic parse of a sentence.
Sentence: The pumpkin moves slower because the
man is not exerting a force on it.
Extracted Features
(tense-move present)
(subj-move pumpkin)
(specifier-pumpkin the)
(modifier-move slow)
(tense-exert present)
(negation-exert +)
(causesubj-exert man)
(subj-exert force)
(obj-exert it)
(specifier-force a)
(specifier-man the)
ships between syntactic heads (e.g., (subj-throw man)),
tense information (e.g., (tense-throw past)), and infor-
mation about passivization and negation (e.g., (negation-
throw +) or (passive-throw -)). See Figures 1 and 2. Rain-
bow has been used for a wide range of text classification
tasks. With Rainbow, P(doc,Class), i.e., the probability of
a document belonging to class Class, is estimated by mul-
tiplying P(Class), i.e., the prior probability of the class,
by the product over all of the words   found in the text of

 	 
	 , i.e., the probability of the word given that
class. This product is normalized over the prior probabil-
ity of all words. Using the individual features extracted
from the deep syntactic analysis of the input as well as
the ?bag of words? Naive Bayes classification of the in-
put sentence, CarmelTC builds a vector representation
of each input sentence, with each vector position corre-
sponding to one of these features. We then use the ID3
decision tree learning algorithm (Mitchell, 1997; Quin-
lin, 1993) to induce rules for identifying sentence classes
based on these feature vectors.
The symbolic features used for the CarmelTC ap-
proach are extracted from a deep syntactic functional
analysis constructed using the CARMEL broad coverage
English syntactic parsing grammar (Rose?, 2000) and the
large scale COMLEX lexicon (Grishman et al, 1994),
containing 40,000 lexical items. For parsing we use an
incremental version of the LCFLEX robust parser (Rose?
et al, 2002b; Rose? and Lavie, 2001), which was designed
for efficient, robust interpretation. While computing a
deep syntactic analysis is more computationally expen-
sive than computing a shallow syntactic analysis, we can
do so very efficiently using the incrementalized version
of LCFLEX because it takes advantage of student typ-
ing time to reduce the time delay between when students
submit their essays and when the system is prepared to
respond.
Syntactic feature structures produced by the CARMEL
grammar factor out those aspects of syntax that modify
the surface realization of a sentence but do not change
its deep functional analysis. These aspects include tense,
negation, mood, modality, and syntactic transformations
such as passivization and extraction. In order to do this
reliably, the component of the grammar that performs the
deep syntactic analysis of verb argument functional re-
lationships was generated automatically from a feature
representation for each of COMLEX?s verb subcatego-
rization tags. It was verified that the 91 verb subcatego-
rization tags documented in the COMLEX manual were
covered by the encodings, and thus by the resulting gram-
mar rules. These tags cover a wide range of patterns of
syntactic control and predication relationships. Each tag
corresponds to one or more case frames. Each case frame
corresponds to a number of different surface realizations
due to passivization, relative clause extraction, and wh-
movement. Altogether there are 519 syntactic patterns
covered by the 91 subcategorization tags, all of which are
covered by the grammar.
There are nine syntactic functional roles assigned by
the grammar. These roles include subj (subject), caus-
esubj (causative subject), obj (object), iobj (indirect ob-
ject), pred (descriptive predicate, like an adjectival phrase
or an adverb phrase), comp (a clausal complement), mod-
ifier, and possessor. The roles pertaining to the rela-
tionship between a verb and its arguments are assigned
based on the subcat tags associated with verbs in COM-
LEX. However, in some cases, arguments that COM-
LEX assigns the role of subject get redefined as caus-
esubj (causative subject). For example, the subject in ?the
pumpkin moved? is just a subject but in ?the man moved
the pumpkin?, the subject would get the role causesubj
instead since ?move? is a causative-inchoative verb and
the obj role is filled in in the second case 1. The modifier
role is used to specify the relationship between any syn-
tactic head and its adjunct modifiers. Possessor is used
to describe the relationship between a head noun and its
genitive specifier, as in man in either ?the man?s pump-
kin? or ?the pumpkin of the man?.
With the hybrid CarmelTC approach, our goal has been
to keep as many of the advantages of both symbolic anal-
ysis as well as ?bag of words? classification approaches
as possible while avoiding some of the pitfalls of each.
Since the CarmelTC approach does not use the syntactic
analysis as a whole, it does not require that the system be
able to construct a totally complete and correct syntactic
analysis of the student?s text input. It can very effectively
1The causative-inchoative verb feature is one that we added
to verb entries in COMLEX, not one of the features provided
by the lexicon originally.
make use of partial parses. Thus, it is more robust than
purely symbolic approaches where decisions are based on
complete analyses of texts. And since it makes use only
of the syntactic analysis of a sentence, rather than also
making use of a semantic interpretation, it does not re-
quire any sort of domain specific knowledge engineering.
And yet the syntactic features provide information nor-
mally not available to ?bag of words? approaches, such
as functional relationships between syntactic heads and
scope of negation and other types of modifiers.
4 Related Work: Combining Symbolic and
Bag of Words Approaches
CarmelTC is most similar to the text classification ap-
proach described in (Furnkranz et al, 1998). In the ap-
proach described in (Furnkranz et al, 1998), features that
note the presence or absence of a word from a text as
well as extraction patterns from AUTOSLOG-TS (Riloff,
1996) form the feature set that are input to the RIPPER
(Cohen, 1995), which learns rules for classifying texts
based on these features. CarmelTC is similar in spirit
in terms of both the sorts of features used as well as the
general sort of learning approach. However, CarmelTC is
different from (Furnkranz et al, 1998) in several respects.
Where (Furnkranz et al, 1998) make use of
AUTOSLOG-TS extraction patterns, CarmelTC makes
use of features extracted from a deep syntactic analysis
of the text. Since AUTOSLOG-TS performs a surface
syntactic analysis, it would assign a different representa-
tion to all aspects of these texts where there is variation in
the surface syntax. Thus, the syntactic features extracted
from our syntactic analyses are more general. For exam-
ple, for the sentence ?The force was applied by the man
to the object?, our grammar assigns the same functional
roles as for ?The man applied the force to the object? and
also for the noun phrase ?the man that applied the force to
the object?. This would not be the case for AUTOSLOG-
TS.
Like (Furnkranz et al, 1998), we also extract word
features that indicate the presence or absence of a root
form of a word from the text. However, in contrast for
CarmelTC one of the features for each training text that is
made available to the rule learning algorithm is the clas-
sification obtained using the Rainbow Naive Bayes clas-
sifier (McCallum, 1996; McCallum and Nigam, 1998).
Because the texts classified with CarmelTC are so
much shorter than those of (Furnkranz et al, 1998), the
feature set provided to the learning algorithm was small
enough that it was not necessary to use a learning algo-
rithm as sophisticated as RIPPER (Cohen, 1995). Thus,
we used ID3 (Mitchell, 1997; Quinlin, 1993) instead with
excellent results. Note that in contrast to CarmelTC, the
(Furnkranz et al, 1998) approach is purely symbolic.
Thus, all of its features are either word level features or
surface syntactic features.
Recent work has demonstrated that combining multi-
ple predictors yields combined predictors that are supe-
rior to the individual predictors in cases where the in-
dividual predictors have complementary strengths and
weaknesses (Larkey and Croft, 1996; Larkey and Croft,
1995). We have argued that this is the case with symbolic
and ?bag of words? approaches. Thus, we have reason to
expect a hybrid approach that makes a prediction based
on a combination of these single approaches would yield
better results than either of these approaches alone. Our
results presented in Section 5 demonstrate that this is true.
Other recent work has demonstrated that symbolic and
?Bag of Words? approaches can be productively com-
bined. For example, syntactic information can be used
to modify the LSA space of a verb in order to make LSA
sensitive to different word senses (Kintsch, 2002). How-
ever, this approach has only been applied to the analysis
of mono-transitive verbs. Furthermore, it has never been
demonstrated to improve LSA?s effectiveness at classify-
ing texts.
In the alternative Structured Latent Semantic Analy-
sis (SLSA) approach, hand-coded subject-predicate in-
formation was used to improve the results obtained by
LSA for text classification (Wiemer-Hastings and Zipi-
tria, 2001), but no fully automated evaluation of this ap-
proach has been published.
In contrast to these two approaches, CarmelTC is both
fully automatic, in that the symbolic features it uses are
obtained without any hand coding whatsoever, and fully
general, in that it applies to the full range of verb subcat-
egorization frames covered by the COMLEX lexicon, not
only mono-transitive verbs. In Section 5 we demonstrate
that CarmelTC outperforms both LSA and Rainbow, two
alternative bag of words approaches, on the task of stu-
dent essay analysis.
5 Evaluation
We conducted an evaluation to compare the effective-
ness of CarmelTC at analyzing student essays in compar-
ison to LSA, Rainbow, and a purely symbolic approach
similar to (Furnkranz et al, 1998), which we refer to
here as CarmelTCsymb. CarmelTCsymb is identical to
CarmelTC except that it does not include in its feature
set the prediction from Rainbow. Thus, by comparing
CarmelTC with Rainbow and LSA, we can demonstrate
the superiority of our hybrid approach to purely ?bag of
words? approaches. And by comparing with CarmelTC-
symb, we can demonstrate the superiority of our hybrid
approach to an otherwise equivalent purely symbolic ap-
proach.
We conducted our evaluation over a corpus of 126 pre-
viously unseen student essays in response to the Pumpkin
Problem described above, with a total of 500 text seg-
ments, and just under 6000 words altogether. We first
tested to see if the text segments could be reliably tagged
by humans with the six possible Classes associated with
the problem. Note that this includes ?nothing? as a class,
i.e., Class 6. Three human coders hand classified text
segments for 20 essays. We computed a pairwise Kappa
coefficient (Cohen, 1960) to measure the agreement be-
tween coders, which was always greater than .75, thus
demonstrating good agreement according to the Krippen-
dorf scale (Krippendorf, 1980). We then selected two
coders to individually classify the remaining sentences in
the corpus. They then met to come to a consensus on
the tagging. The resulting consensus tagged corpus was
used as a gold standard for this evaluation. Using this
gold standard, we conducted a comparison of the four
approaches on the problem of tallying the set of ?correct
answer aspects? present in each student essay.
The LSA space used for this evaluation was trained
over three first year physics text books. The other three
approaches are trained over a corpus of tagged examples
using a 50 fold random sampling evaluation, similar to a
cross-validation methodology. On each iteration, we ran-
domly selected a subset of essays such that the number
of text segments included in the test set were greater than
10 but less than 15. The randomly selected essays were
then used as a test set for that iteration, and the remain-
der of the essays were used for training in addition to a
corpus of 248 hand tagged example sentences extracted
from a corpus of human-human tutoring transcripts in
our domain. The training of the three approaches dif-
fered only in terms of how the training data was parti-
tioned. Rainbow and CarmelTCsymb were trained us-
ing all of the example sentences in the corpus as a single
training set. CarmelTC, on the other hand, required parti-
tioning the training data into two subsets, one for training
the Rainbow model used for generating the value of its
Rainbow feature, and one subset for training the decision
trees. This is because for CarmelTC, the data for train-
ing Rainbow must be separate from that used to train the
decision trees so the decision trees are trained from a re-
alistic distribution of assigned Rainbow classes based on
its performance on unseen data rather than on Rainbow?s
training data.
In setting up our evaluation, we made it our goal to
present our competing approaches in the best possible
light in order to provide CarmelTC with the strongest
competitors as possible. Note that LSA works by using
its trained LSA space to construct a vector representation
for any text based on the set of words included therein. It
can thus be used for text classification by comparing the
vector obtained for a set of exemplar texts for each class
with that obtained from the text to be classified. We tested
LSA using as exemplars the same set of examples used
Figure 3: This Table compares the performance of the 4 alternative approaches in the per essay evaluation in
terms of precision, recall, false alarm rate, and f-score.
Approach Precision Recall False Alarm Rate F-Score
LSA 93% 54% 3% .70
Rainbow 81% 73% 9% .77
CarmelTCsymb 88% 72% 7% .79
CarmelTC 90% 80% 8% .85
as Rainbow training data, but it always performed better
when using a small set of hand picked exemplars. Thus,
we present results here using only those hand picked ex-
emplars. For every approach except LSA, we first seg-
mented the essays at sentence boundaries and classified
each sentence separately. However, for LSA, rather than
classify each segment separately, we compared the LSA
vector for the entire essay to the exemplars for each class
(other than ?nothing?), since LSA?s performance is better
with longer texts. We verified that LSA also performed
better specifically on our task under these circumstances.
Thus, we compared each essay to each exemplar, and we
counted LSA as identifying the corresponding ?correct
answer aspect? if the cosine value obtained by compar-
ing the two vectors was above a threshold. We tested
LSA with threshold values between .1 and .9 at incre-
ments of .1 as well as testing a threshold of .53 as is
used in the AUTO-TUTOR system (Wiemer-Hastings et
al., 1998). As expected, as the threshold increases from
.1 to .9, recall and false alarm rate both decrease together
as precision increases. We determined based on comput-
ing f-scores2 for each threshold level that .53 achieves the
best trade off between precision and recall. Thus, we used
a threshold of .53, to determine whether LSA identified
the corresponding key point in the student essay or not
for the evaluation presented here.
We evaluated the four approaches in terms of precision,
recall, false alarm rate, and f-score, which were computed
for each approach for each test essay, and then averaged
over the whole set of test essays. We computed preci-
sion by dividing the number of ?correct answer aspects?
(CAAs) correctly identified by the total number of CAAs
identified3 We computed recall by dividing the number of
CAAs correctly identified over the number of CAAs actu-
ally present in the essay4 False alarm rate was computed
by dividing the number of CAAs incorrectly identified by
the total number of CAAs that could potentially be incor-
2We computed our f-scores with a beta value of 1 in order to
treat precision and recall as equally important.
3For essays containing no CAAs, we counted precision as 1
if none were identified and 0 otherwise.
4For essays with no CAAs present, we counted recall as 1
for all approaches.
rectly identified5. F-scores were computed using 1 as the
beta value in order to treat precision and recall as equally
important.
The results presented in Figure 3 clearly demon-
strate that CarmelTC outperforms the other approaches.
In particular, CarmelTC achieves the highest f-score,
which combines the precision and recall scores into a
single measure. In comparison with CarmelTCsymb,
CarmelTC achieves a higher recall as well as a slightly
higher precision. While LSA achieves a slightly higher
precision, its recall is much lower. Thus, the difference
between the two approaches is clearly shown in the f-
score value, which strongly favors CarmelTC. Rainbow
achieves a lower score than CarmelTC in terms of preci-
sion, recall, false alarm rate, and f-score.
6 Conclusion and Current Directions
In this paper we have introduced the CarmelTC text clas-
sification approach as it is applied to the problem of stu-
dent essay analysis in the context of a conceptual physics
tutoring system. We have evaluated CarmelTC over data
collected from students interacting with our system in re-
sponse to one of its 10 implemented conceptual physics
problems. Our evaluation demonstrates that the novel
hybrid CarmelTC approach outperforms both Latent Se-
mantic Analysis (LSA) (Landauer et al, 1998; Laham,
1997) and a Naive Bayes approach (McCallum, 1996;
McCallum and Nigam, 1998) as well as a purely sym-
bolic approach similar to (Furnkranz et al, 1998). We
plan to run a larger evaluation with essays from multiple
problems to test the generality of our result. We also plan
to experiment with other rule learning approaches, such
as RIPPER (Cohen, 1995).
7 Acknowledgments
This research was supported by the Office of Naval Re-
search, Cognitive Science Division under grant number
N00014-0-1-0600 and by NSF grant number 9720359
to CIRCLE, Center for Interdisciplinary Research on
Constructive Learning Environments at the University of
Pittsburgh and Carnegie Mellon University.
5For essays containing all possible CAAs, false alarm rate
was counted as 0 for all approaches.
References
J. Burstein, K. Kukich, S. Wolff, C. Lu, M. Chodorow,
L. Braden-Harder, and M. D. Harris. 1998. Au-
tomated scoring using a hybrid feature identification
technique. In Proceedings of COLING-ACL?98, pages
206?210.
J. Burstein, D. Marcu, S. Andreyev, and M. Chodorow.
2001. Towards automatic classification of discourse
elements in essays. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics, Toulouse, France.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20(Winter):37?46.
W. W. Cohen. 1995. Fast effective rule induction. In
Proceedings of the 12th International Conference on
Machine Learning.
M. Craven, D. DiPasquio, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1998. Learn-
ing to extract symbolic knowledge from the world wide
web. In Proceedings of the 15th National Conference
on Articial Intelligence.
P. W. Foltz, W. Kintsch, and T. Landauer. 1998. The
measurement of textual coherence with latent semantic
analysis. Discourse Processes, 25(2-3):285?307.
J. Furnkranz, T. Mitchell Mitchell, and E. Riloff. 1998.
A case study in using linguistic phrases for text cat-
egorization on the www. In Proceedings from the
AAAI/ICML Workshop on Learning for Text Catego-
rization.
A. Graesser, K. Vanlehn, TRG, and NLT Group. 2002.
Why2 report: Evaluation of why/atlas, why/autotutor,
and accomplished human tutors on learning gains for
qualitative physics problems and explanations. Tech-
nical report, LRDC Tech Report, University of Pitts-
burgh.
R. Grishman, C. Macleod, and A. Meyers. 1994. COM-
LEX syntax: Building a computational lexicon. In
Proceedings of the 15th International Conference on
Computational Linguistics (COLING-94).
R. R. Hake. 1998. Interactive-engagement versus tra-
ditional methods: A six-thousand student survey of
mechanics test data for introductory physics students.
American Journal of Physics, 66(64).
I. A. Halloun and D. Hestenes. 1985. The initial knowl-
edge state of college physics students. American Jour-
nal of Physics, 53(11):1043?1055.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
K. Krippendorf. 1980. Content Analysis: An Introduc-
tion to Its Methodology. Sage Publications.
D. Laham. 1997. Latent semantic analysis approaches
to categorization. In Proceedings of the Cognitive Sci-
ence Society.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. In-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25(2-3):259?284.
L. S. Larkey and W. B. Croft. 1995. Automatic assign-
ment of icd9 codes to discharge summaries. Technical
Report IR-64, University of Massachusetts Center for
Intelligent Information Retrieval.
L. S. Larkey and W. B. Croft. 1996. Combining classi-
fiers in text categorization. In Proceedings of SIGIR.
L. Larkey. 1998. Automatic essay grading using text
categorization techniques. In Proceedings of SIGIR.
A. McCallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification. In
Proceedings of the AAAI-98 Workshop on Learning for
Text Classication.
Andrew Kachites McCallum. 1996. Bow: A toolkit
for statistical language modeling, text retrieval, clas-
sification and clustering. http://www.cs.cmu.edu/ mc-
callum/bow.
T. M. Mitchell. 1997. Machine Learning. McGraw Hill.
J. R. Quinlin. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann Publishers: San Mateo, CA.
E. Riloff. 1996. Using learned extraction patterns for text
classification. In S. Wermter, R. Riloff, and G. Scheler,
editors, Connectionist, Statistical, and Symbolic Ap-
proaches for Natural Language Processing. Springer-
Verlag.
C. P. Rose? and A. Lavie. 2001. Balancing robustness
and efficiency in unification augmented context-free
parsers for large practical applications. In J. C. Junqua
and G. Van Noord, editors, Robustness in Language
and Speech Technologies. Kluwer Academic Press.
C. P. Rose?, D. Bhembe, A. Roque, S. Siler, R. Srivas-
tava, and K. Vanlehn. 2002a. A hybrid language un-
derstanding approach for robust selection of tutoring
goals. In Proceedings of the Intelligent Tutoring Sys-
tems Conference.
C. P. Rose?, D. Bhembe, A. Roque, and K. VanLehn.
2002b. An efficient incremental architecture for ro-
bust interpretation. In Proceedings of the Human Lan-
guages Technology Conference, pages 307?312.
C. P. Rose?, P. Jordan, and K. VanLehn. 2002c. Can we
help students with high initial competency? In Pro-
ceedings of the ITS Workshop on Empirical Methods
for Tutorial Dialogue Systems.
C. P. Rose?. 2000. A framework for robust semantic in-
terpretation. In Proceedings of the First Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 311?318.
K. VanLehn, P. Jordan, C. P. Rose?, and The Natural Lan-
guag e Tutoring Group. 2002. The architecture of
why2-atlas: a coach for qualitative physics essay writ-
ing. In Proceedings of the Intelligent Tutoring Systems
Conference, pages 159?167.
P. Wiemer-Hastings and I. Zipitria. 2001. Rules for
syntax, vectors for semantics. In Proceedings of the
Twenty-third Annual Conference of the Cognitive Sci-
ence Society.
P. Wiemer-Hastings, A. Graesser, D. Harter, and the Tu-
toring Res earch Group. 1998. The foundations
and architecture of autotutor. In B. Goettl, H. Halff,
C. Redfield, and V. Shute, editors, Intelligent Tutor-
ing Systems: 4th International Conference (ITS ?98 ),
pages 334?343. Springer Verlag.
Workshop on Computational Linguistics for Literature, pages 97?104,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Towards a computational approach to literary text analysis
Antonio Roque
Computer Science Department
University of California, Los Angeles
antonio@roque-brown.net
Abstract
We consider several types of literary-theoretic ap-
proaches to literary text analysis; we describe sev-
eral concepts from Computational Linguistics and 
Artificial Intelligence that could be used to model 
and support them.
1 Problem Statement
Consider the first sentence of the novel Finnegan's  
Wake (Joyce, 1939):
riverrun, past Eve and Adam's,  from swerve of 
shore to bend of bay, brings us by a commodius 
vicus of recirculation back to Howth Castle and 
Environs.
To computationally analyze this sentence as literat-
ure, we must understand that its meaning is more 
than the combination of its semantic components. 
The  rubric  of  "who  did  what  to  whom,  when, 
where, and why" will at best lead us only to under-
stand that somewhere, probably in Ireland, a river 
is flowing.
Some  obvious  low-level  tasks  to  improve  our 
reading  include:  exploring  the  meaning  of  non-
standard  capitalization  and  spacing,  as  in 
"riverrun";  resolving allusions,  such as  "Eve and 
Adam's," and considering the significance of vari-
ations from common phrasings1;; identifying allit-
erated phrases such as "swerve of shore" and "bend 
of  bay"  and considering their  effect;  recognizing 
tone shifts such as "commodius vicus of recircula-
1 For example, the quotation-delimited phrase "Adam and 
Eve" returns over 12 million Google results but "Eve and 
Adam" only returns around 200,000 (as of March 28, 2012.)
tion," and resolving any allusions they may indic-
ate; identifying the significance of named entities 
such as "Howth Castle and Environs"2; exploring 
the effect of the line's syntax on reception, as de-
scribed by writing scholars (Tufte, 2006).
But becoming absorbed in these admittedly in-
teresting questions threatens to distract us from the 
larger  questions  that  literary  theorists  have  been 
studying for over a century.   Those questions in-
clude:
? what interpretation is  the "gold standard" 
by which others should be judged?  Is it 
the meaning intended by the author?  Is it 
the significance of the text to the readers 
(and if so, which readers?)  Or is the mean-
ing  of  a  literary  text  inherent  in  how  it 
takes part in a system and process of lan-
guage use?
? what metrics can tell us whether one inter-
pretation is better than another?
? how should we model the literary text as it 
relates to the larger body of language use, 
which includes both literary and nonliter-
ary texts as well as everyday situated lan-
guage use by intelligent agents?  What fea-
tures are necessary and sufficient to repres-
ent  the  way  meaning  (both  literary  and 
non-literary)  is  created  and  established 
among language-using populations?  How 
is this meaning tied both to an intelligent 
2 For example: do they have an appearance or other attribute 
that would commonly be brought to mind? Are there associ-
ations that would normally be suggested to members of a giv-
en community of language use?  cf. the significance of the 
Watergate office complex in American communities of polit-
ical discourse.
97
agent's  abstract  beliefs  as  well  as  that 
agent's  moment-to-moment  understanding 
of its environment?
The wording of these questions is slanted to sug-
gest their utility to computational linguistics.  First, 
we may want to know how much of the meaning of 
a literary text comes from the author as opposed to 
from our situated interpretation of the text or from 
a  language  system3.   Second,  evaluation  metrics 
would help us determine whether or not the per-
formance  of  an automated literary system is  im-
proving.  Finally,  we would benefit  from the ex-
planations of a computational model  of a literary 
text's meaning as it emerges from the situated read-
ing of an authored artifact in the context of a multi-
agent  language system;  if  nothing  else,  it  would 
tell us how to design algorithms that both consume 
and produce literary artifacts in human-like ways.
2 Approach
Computationally, the questions in Section 1 are 
likely to be answered over the course of decades 
rather than years.  Contemporary relevant research 
from the fields of Computational Linguistics (CL) 
and Artificial Intelligence (AI) includes: semantic 
analysis of narratives (Elson and McKeown, 2009, 
Finlayson,  2011);  summarizing  fiction  (Mani, 
2005; Kazantseva and Szpakowicz, 2010) and per-
forming information-extraction on fiction (Elson et 
al,  2010); modeling affect and reader-response in 
narrative  (Mani,  2010;  McQuiggan,  2010;  Mo-
hammad, 2011; Francisco et al, 2011); properties 
of narrative such as novelty (Peinado et al, 2010) 
and irony (Utsumi, 2004); models of discourse in 
narrative (Polanyi et al, 2004; Goyal et al, 2010); 
computational models of aesthetic creativity (Ger-
v?s et al, 2009); and the automatic generation of 
prose (Callaway and Lester, 2002) and poetry (Ma-
nurung, 2003; Gerv?s, 2007; Greene et al, 2010).
However,  these  disparate  research  traditions 
consider questions closer to the low-level tasks de-
scribed in Section 1 than to the theoretical ques-
tions  of  interpretation  ranking,  evaluation,  and 
computational modeling of meaningful human lan-
3 We may be interested in user modeling of the author, versus 
modeling our own interpretative techniques, versus perform-
ing sentiment analysis on a particular community of language 
use, for example.
guage use.  This is possibly because of the empiric-
al methods which have become dominant in AI/CL 
in recent  history (Cohen,  1995).   A field whose 
methods are tuned to empirical evaluation will nat-
urally shy from an area with few clear empirical 
tasks, whose humanities practitioners are likely to 
indulge  in  analyses  assuming  human  levels  of 
knowledge and language-processing capabilities.
Because of this we will turn instead for inspira-
tion  from  the  digital  humanities (Schreibman, 
2004).   With  its  roots  in  humanities  computing 
(Hockey, 2004) which constituted the earliest use 
of computers in the humanities, digital humanities 
took shape with the advent of the Internet.  Digital 
humanities researchers currently apply computers 
to research questions such as authorship attribution 
(Jockers  and  Witten,  2010),  statistical  word-use 
analysis (Burrows, 2004), and the development of 
resources for classical lexicography (Bamman and 
Crane, 2009), often collaborating with statisticians 
or computer scientists.  
Digital  humanities  has  always  had  detractors 
among  more  traditional  humanities  scholars,  but 
scholars sympathetic to the overall goals of digital 
humanities  have  also  critiqued some  of  its  prac-
tices.   Consider the technological  constraints im-
posed by projects in which texts are digitized, an-
notated,  and  statistically  analyzed.   Those  con-
straints make tacit assumptions about the objectiv-
ity of knowledge and the transparency of its trans-
mission (Drucker, 2009).  Those assumptions may 
be contrary to a literary theorist's understanding of 
how literary text analysis actually works.  
For  example,  in  the  case  of  scholar/artist  Jo-
hanna Drucker, knowledge is seen as partial, sub-
jective,  and  situated.  Subjectivity  in  this  context 
has two components: a point of view inscribed in 
the possible interpretations of a work, and "inflec-
tion, the marked presence of affect and specificity, 
registered as the trace of difference, that inheres in 
material expressions" (Drucker, 2009). To Druck-
er, subjectivity of knowledge is evident in the fact 
that  interpretation  occurs  in  modeling,  encoding, 
processing, and accessing knowledge.  
Drucker's focus is on humanities tools in digital 
contexts rather than digital tools in humanities con-
texts.  We will proceed in a similar spirit, consider-
ing the tasks and approaches of literary text analys-
is as practiced by literary theorists and considering 
what  kinds of  models  and approaches from con-
temporary AI/CL research they might find useful, 
98
rather than starting with the tasks and approaches 
that AI/CL researchers are most familiar with and 
asking how they can be applied to literary text ana-
lysis.  
As a specific goal to guide our thought, we will 
adopt a statement  from another scholar who em-
phasizes the importance of keeping the humanities 
central to computational text analysis.  In Reading 
Machines:  Toward  an  Algorithmic  Criticism, 
Stephen Ramsay develops the notion of adapting 
the constraints imposed by computation to inten-
tionally create "those heightened subjectivities ne-
cessary for critical work" (Ramsay, 2011).  While 
doing so, Ramsay states that from a humanist's per-
spective:
Tools that can adjudicate the hermeneutical para-
meters of human reading experiences - tools that 
can tell you whether an interpretation is permiss-
ible - stretch considerably beyond the most am-
bitious fantasies of artificial intelligence.
The rest of this paper will attempt to respond to 
Ramsay's  claim  by  developing  such  ambitious 
fantasies.  We will strive to consider literary text 
analysis as it is understood by literary theorists of 
recent history, and we will describe how represent-
ative processes from each of these theories could 
be modeled computationally using techniques from 
the AI/CL research communities.
3 Literary Text Analysis
3.1  Expressive Realism
Human judgments on the nature of literature and 
the way literature is  best  read have changed fre-
quently since classical times.  The last century in 
particular has provided numerous, often contradict-
ory,  notions  of  how  we  should  determine  the 
meaning of a story, leaving us with no consensus. 
Even within a school of thought there may be sig-
nificant  differences  of  opinion,  and evaluation is 
typically no more empirical  than how persuasive 
the interpretation of a given text may be.  Still, we 
may  identify  certain  key  ideas  and  use  them to 
imagine ways they could involve computation.
We may begin by considering  expressive real-
ism,  an  approach  to  literary  theory  which  de-
veloped in the late 19th and early 20th centuries, 
and is a combination of the classical Aristotelian 
notions of art as mimesis (reproducing reality) and 
the Romantic-era view of poetry as an outpouring 
of  strong  emotions  produced by an  artist  whose 
percepts  and  affective  processing  are  unusually 
well-tuned4 (Belsey, 1980).  The task of the reader 
in this formulation is  to faithfully create in their 
minds the realities being represented by the work, 
and to enrich themselves by following the thoughts 
and feelings that the artist experienced.  
Computationally, we may frame this as a know-
ledge engineering task: the writer is a subject mat-
ter  expert  in  perceiving  the  world,  and  has  de-
veloped knowledge about the world and innovative 
ways of emotionally relating to the world. The lit-
erary critic's task is to identify which writers have 
produced  knowledge  and  affective  relationships 
that are most worth adopting.  The reader's task is 
to be guided by the critics to the best writers, and 
then strive to adopt those writers' knowledge and 
affective relations as their own.  
It may seem difficult to perform such a task with 
a text such as Finnegan's Wake, which is not easy 
to  translate  into  propositions.   But  consider  a 
writer's understanding of what happens when read-
ing expressive realist fiction (Gardner, 1991):
If  we  carefully  inspect  our  experience  as  we 
read, we discover that the importance of physical 
detail is that it creates for us a kind of dream, a 
rich and vivid play in the mind.  We read a few 
words at the beginning of a book or the particu-
lar story, and suddenly we find ourselves seeing 
not  only  words  on  a  page  but  a  train  moving 
through Russia, an old Italian crying, or a farm-
house battered by rain.
Gardner  describes  fiction as  producing an im-
mersive  experience  in  which  the  reader's  sensa-
tions  are  empathically  aligned with  those  of  the 
writer.  This alignment produces an understanding 
unlike that of propositional knowledge: 
[The writer] at the very least should be sure he 
understands the common objection summed up 
in the old saw "Show, don't tell." The reason, of 
course,  is  that  set  beside  the  complex  thought 
achieved  by  drama,  explanation  is  thin  gruel, 
4 Belsey, who is critical of this approach, quotes the poet Wil-
liam Wordsworth's view of artists as  "possessed of more than 
usual organic sensibility."  In fact, Wordsworth believed a 
Poet was "endowed with more lively sensibility; more enthusi-
asm and tenderness, who has a greater knowledge of human 
nature, and a more comprehensive soul, than are supposed to 
be common among mankind..." (Wordsworth, 1802.)
99
hence boring. ... After our [reading] experience, 
which can be intense if the writer is a good one, 
we know why the character leaves when finally 
she walks out the door.  We know in a way al-
most too subtle for words...
The  subtletly  described  by  Gardner's  explains 
how a text such as  Finnegan's Wake may be read 
without recourse to a detailed exegesis producing 
propositional  content.   The reader  need only be-
come suggestible to the text, and allow themselves 
to experience the "complex thought" suggested by 
the  writer.   Of  course,  this  "intense"  experience 
may  lead  one  to  a  further  study  of  the  writer's 
mind-set, which would then create an even fuller 
understanding of that writer's approach.  
Such a  description may seem like an unlikely 
candidate  for  computational  modeling,  but  con-
sider the neurolinguistic implications of models of 
the  mirror  neuron  system  (Rizzolatti  and 
Craighero, 2004): hypothetically, a reader's neural 
structure might  literally copy that  of  the writer's, 
provided the stimulus of the text.  In this way we 
might  model  the  transmission  of  knowledge  "al-
most too subtle for words."
3.2  New Criticism
Later literary theories found expressive realism 
problematic  in  various  ways.   For  example,  the 
Anglo-American New Criticism defined the inten-
tional fallacy, which states that "the design or in-
tention of the author is neither available nor desir-
able as a standard for judging the success of a work 
of  literary  art"  (Wimsatt  and  Beardsley,  1954)5. 
Wimsatt and Beardsley proposed to avoid "author 
psychology" by focusing on the  internal evidence 
of the text, which they defined as 
public evidence which is discovered through the 
semantics and syntax of a poem, through our ha-
bitual knowledge of the language, through gram-
mars, dictionaries, and all the literature which is 
the source of dictionaries, in general through all 
that makes a language and culture...
The  language  knowledge  and  resources  were 
used to identify the "technique of art".  New Critic 
5 Note that Wimsatt and Beardsley did not not deny the schol-
arly value of "literary biography," and New Critic John Crowe 
Ransom stated "Without [historical studies] what could we 
make of Chaucer, for instance?" (Ransom, 1938)  New Critics 
merely believed that close readings of the text should take 
precedence during literary text analysis.
John Crowe Ransom provided examples  of  what 
devices should  be  used  in  analyzing  poetry 
(Ransom, 1938): 
its metric; its inversions; solecisms, lapses from 
the  prose  norm  of  language,  and  from  close 
prose logic; its tropes; its fictions, or inventions, 
by which  it  secures  'aesthetic  distance'  and  re-
moves itself from history...
However,  these  devices  were  not  studied  for 
their own sake.  Ransom continued: "the superior 
critic  is  not  content  with  the  compilation  of  the 
separate devices; the suggest to him a much more 
general  question."   The  question  in  this  case  is 
"what [the poem] is trying to represent" and why it 
does so using those particular devices.  This was 
worth understanding because the New Critics be-
lieved that "great works of literature are vessels in 
which humane values survive" (Selden and Wid-
dowson, 1993) and which reinforce those values in 
the diligent reader.
Computationally, the list of language resources 
described  by  Wimsatt  and  Beardsley  recalls  the 
corpus- and knowledge-based resources promoted 
by textbook approaches to CL (Jurafsky and Mar-
tin, 2000).  The low-level tasks in analyzing  Fin-
negan's Wake described in Section 1 align with the 
New  Critical  identification  of  literary  devices. 
Much of the CL/AI research described in Section 2 
is in this vein.
However,  to  create  a  complete  computational 
model of literary reading from this perspective we 
would also need a model of the types of "humane 
values" that New Critics revered.  Unfortunately, 
the  New  Critics  themselves  did  not  explicitly 
provide such a model, as doing so was considered 
irrelevant.  But we ourselves could adapt a compu-
tational model of culture to develop a representa-
tion  of  the  New  Critic's  cultural  values.  AI  re-
searchers develop computational model of culture 
by,  for  example,  implementing  Cultural  Schema 
Theory and Appraisal Theory in cognitive architec-
tures to describe how culture emerges from an in-
dividual's cognitive processes (Taylor et al, 2007). 
There  is  room here  to  adapt  the  system of  per-
ceived  affordances  (Gorniak  and  Roy,  2006)  in 
which language understanding is represented as the 
process  of  filtering  real-world  devices  in  a  way 
analogous  to  how the  New Critics  filter  literary 
devices.
100
3.3  Russian Formalism
The New Criticism developed independently of 
Russian  formalism,  which  similarly  focused  on 
the text and the literary devices present, rather than 
the author's intentions or the context of the text's 
production.  Because of this, most of the computa-
tional  representations  used  in  discussion  of  the 
New Critics could also be applied to the Russian 
formalists.
However,  unlike the New Critics,  the  Russian 
formalists  believed   that  art  existed  to  create  a 
sense of defamiliarization: 
art exists that one may recover the sensation of 
life; it exists to make one feel things... The tech-
nique  of  art  is  to  make objects  'unfamiliar,'  to 
make  forms  difficult,  to  increase  the  difficulty 
and length of perception because the process of 
perception is an aesthetic end in itself and must 
be prolonged.  Art is a way of experiencing the  
artfulness of an object: the object is not import-
ant.6
The defamiliarizing force of literature is easy to 
see  in  a  text  such  as  Finnegan's  Wake,  whose 
second sentence reads:
Sir  Tristram,  violer  d'amores,  fr'over  the  short 
sea, had passencore rearrived from North Armor-
ica on this side the scraggy isthmus of Europe 
Minor to wielderfight his penisolate war: nor had 
topsawyer's rocks by the stream Oconee exagger-
ated  themselse  to  Laurens  County's  gorgios 
while  they  went  doublin  their  mumper  all  the 
time:  nor  avoice  from  afire  bellowsed  mishe 
mishe to tauftauf thuartpeatrick: not yet, though 
venissoon after, had a kidscad buttended a bland 
old isaac:  not  yet,  though all's  fair  in  vanessy, 
were  sosie sesthers  wroth with twone nathand-
joe.
This is not a text that can easily be read rapidly. 
A more methodical reading is most obviously re-
warded by the portmanteaux (which are created by 
combining words in new ways) along with the oth-
er literary devices.  Computationally, as before this 
can be seen as another set of devices to be auto-
matically processed.  However it may be more pro-
ductive to see this as an example of how writers 
strive to invent new devices and combine devices 
in new ways, which may be resistant to automated 
6 First published in 1917, this translation is from (Shlovsky, 
1988).  Emphasis from the original.
analyses.  From this perspective, defamiliarization 
has its effect on the computational linguist who is 
developing the algorithms.  The perception of the 
researcher is thus shifted and prolonged, creating 
in them a recovery of the sensation for language.
3.4  Structuralism and Post-Structuralism
Linguist  Roman  Jakobson  was  central  figure  in 
both  Russian  formalism  and  structuralism,  two 
mutually influential schools of thought.  A key dif-
ference between the two is their understanding of 
the  relation between aesthetic  products  and their 
cultural context.  To Russian formalists (as well as 
to  New  Critics),  literary  text  existed  apart  from 
other  cultural  phenomena,  whereas  structuralism 
provided a formal  framework which studied sys-
tems of arbitrary signs which could be built at dif-
ferent  levels,  (Schleifer,  1993)  so  that  literary 
structures could be built with reference to cultural 
structures.
With roots in the semiotics of linguist Ferdinand 
de  Saussure  and  of  philosopher  Charles  Sanders 
Peirce,  structuralism aimed  at  systematically  un-
covering the way that meaning arises from systems 
of signs forming linguistic elements such as sen-
tences and paragraphs as well as higher levels of 
narrative discourse.
Continued scholarship on structuralism exposed 
a  number  of difficulties.   Besides  its  lack of  in-
terest  in  individual  cases  or  in  the  way systems 
change over time, the arbitrary nature of structural-
ist signs contradicted its aspirations to systematic 
representation (Schleifer,  1993).  This was lever-
aged by philosopher Jacques Derrida, who argued 
that one could not study structures from "outside," 
in the way that an objective study requires.  
Derrida  was  a  post-structuralist,  who  used 
structuralism as a starting point but did not limit 
themselves with structuralism's constraints. Anoth-
er  post-structuralist,  literary  theorist  Roland 
Barthes, used the phrase  death of the author in a 
way reminiscent of the New Critics' intentional fal-
lacy.  Barthes, however, used the the arbitrariness 
of signs to go beyond the New Critics and reject 
the existence of any "ultimate meaning" of a text. 
Barthes  saw  the  source  of  understanding  as  the 
reader:
[A]  text  consists  of  multiple  writings,  issuing 
from several cultures and entering into dialogue 
with each  other,  into parody,  into contestation; 
101
but there is one place where this multiplicity is 
collected, united, and this place is not the author, 
as we have hitherto said it was, but the reader... 
(Barthes, 1967)
To Barthes, readers are not important in terms of 
their  personal  history or  their  state  of  mind,  but 
rather that they are the one who "holds gathered 
into a single field all the paths of which the text is 
constituted." (Barthes, 1967)  In other words, the 
text's  meaning  is  dependent  on  the  structures  of 
signs  in  which  the  reader  exists.   And  because 
signs  are  arbitrary,  the  reading produced by any 
reader must likewise be arbitrary, at least in terms 
of any objective measure of quality.
Another post-structuralist, psychologist Jacques 
Lacan, maintained that humans entered systems of 
signs in which they found or were provided roles, 
such  as  child/parent  or  male/female  (Selden  and 
Widdowson, 1993).  This process is directed by the 
unconscious, and the only way it is able to take on 
comprehensible meaning is in expression through a 
system of language signs.
These are but a few of the influential structural-
ist and post-structuralist scholars, but they suffice 
to consider applicable computational techniques.
We  begin  by  considering  the  concept  of  lan-
guage as a complex adaptive system (Beckner et 
al., 2009).  This provides a model that brings to-
gether  language,  interpretation,  and  intelligent 
agents (Steels, 2007) in a way that allows experi-
ments with both sets of software agents and lan-
guage-using robots (Steels, 2006).  As in the struc-
turalist view, meaningful language use is depend-
ent on complex systems involving signification.  
But  this  complex  system  is  made  up  of  lan-
guage-using agents, who must work together to de-
termine norms as well as actually use language for 
real-world tasks and abstract reasoning.  The mod-
el must work not only at the system level, but also 
at the individual level. CL/AI research in societal 
grounding (DeVault et al, 2006), dialogue ground-
ing (Traum, 1994), semantic alignment (Pickering 
and  Garrod,  2004),  and  relational  agency (Bick-
more and Picard, 2005) provide ways of represent-
ing how populations of agents use language mean-
ingfully,  and how pairs of  human-like intelligent 
agents  coordinate  language in  situated dialogues, 
while  developing social  relationships.   As in  the 
Lacanian  subject,  these  agents  are  created  or 
trained  in  terms  of  their  difference  or  similarity 
from the other agents, adopting and being defined 
by their roles in the structured societies of agents.
When considering  Finnegan's Wake, an intelli-
gent  agent  would  bring  with  it  an  algorithm for 
identifying features in stories, as well as resources 
such as language model data and its model of the 
role it fits in its social structures.  Reading the text, 
the agent identifies literary devices that it uses as 
affordances to react with its emotions and its social 
perceptions, as well as to weigh the semantics of 
the text.  When reading the text, the agent's inter-
pretation of the story will be based on its gendered 
identity and personal history.  In this way, the liter-
ary analysis of the agent is highly dependent on its 
sense of identity, as well as the localized nature of 
its language resources.
4  Conclusions
We began by describing some of the larger ques-
tions that literary theorists have been working with 
for over a century.  We described some ideas from 
the  digital  humanities,  including  an  expressed 
skepticism  in  artificial  intelligence's  ability  to 
model human-like readings of literary texts.  In re-
sponse to that skepticism, we have described sever-
al major approaches to literary text  analysis,  and 
for each we have suggested ways in which state-of-
the-art CL/AI techniques could be applied to mod-
el or support their approach.
Of course this is by no means an exhaustive sur-
vey of either literary theoretical approaches or ap-
plicable  CL/AI  techniques.   Rather,  we  are  sug-
gesting that a great number of possibilities remain 
unexplored between the two.
References 
David  Bamman  and  Gregory  Crane.  2009.  Computa-
tional Linguistics and Classical Lexicography, Digit-
al Humanities Quarterly, Volume 3 Number 1.
Roland Barthes.  1967.  The Death of the Author.  As-
pen.  No. 5-6.
Clay Beckner, Nick C. Ellis, Richard Blythe, John Hol-
land,  Joan  Bybee,  Jinyun  Ke,  Morten  H.  Christi-
ansen,  Diane Larsen-Freeman,  William Croft,  Tom 
Schoenemann. 2009.  Language Is a Complex Adapt-
ive  System:  Position  Paper.   Language  Learning, 
59:Suppl 1, December 2009, pp 1-26.
Catherine Belsey.  1980.  Critical Practice.  Routledge. 
London, UK.
102
Timothy Bickmore  and Rosalind Picard.  2005. Estab-
lishing and maintaining long-term human-computer 
relationships.  ACM Transactions  on  Computer-Hu-
man Interaction (ToCHI).
John Burrows.  2004.  Textual Analysis. In A Compan-
ion  to  Digital  Humanities,  ed.  S.   Schreibman,  R. 
Siemens, and J. Unsworth, Oxford:  Blackwell  Pub-
lishing.
Charles B. Callaway and James C. Lester.  2002.  Nar-
rative  Prose  Generation,  Artificial  Intelligence. 
Volume 139 Issue 2, Elsevier Science Publishers Ltd. 
Essex, UK 
Paul R. Cohen.  1995.  Empirical Methods for Artificial  
Intelligence.  Bradford Books.  Cambridge, MA.
David DeVault, Iris Oved, and Matthew Stone.   2006. 
Societal Grounding is Essential to Meaningful Lan-
guage Use.  In  Proceedings of the Twenty-First Na-
tional  Conference  on Artificial  Intelligence  (AAAI-
06)
Johanna  Drucker.  2009.  SpecLab:  Digital  Aesthetics  
and  Projects  in  Speculative  Computing.  University 
Of Chicago Press.
David  K.  Elson,  Nicholas  Dames,  Kathleen  R.  McK-
eown. 2010. Extracting Social Networks from Liter-
ary Fiction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics  
(ACL 2010), Uppsala, Sweden
David K. Elson and Kathleen R. McKeown.  2009.  Ex-
tending and Evaluating a Platform for Story Under-
standing.  Papers from the 2009 AAAI Spring Sym-
posium: Intelligent Narrative Technologies II.   The 
AAAI Press, Menlo Park, California.
Mark A. Finlayson. 2011. Corpus Annotation in Service 
of Intelligent Narrative Technologies, Proceedings of  
the 4th Workshop on Intelligent Narrative Technolo-
gies, Stanford, CA.
Virginia Francisco, Raquel Herv?s,  Federico Peinado, 
and Pablo Gerv?s.  2011.  EmoTales: creating a cor-
pus of folk tales with emotional annotations.  Lan-
guage Resources & Evaluation.
John Gardner.  1991.  The Art of Fiction: Notes on Craft  
for Young Writers.  Vintage, New York, NY.
Pablo  Gerv?s.  2009.  Computational  Approaches  to 
Storytelling and Creativity.  AI Magazine, Fall, p 49-
62.
Pablo Gerv?s, Raquel Herv?s, Jason R Robinson. 2007. 
"Difficulties  and  Challenges  in  Automatic  Poem 
Generation: Five Years of Research at UCM". in Pa-
pers presented at e-poetry 2007, Universit? Paris8.
Peter Gorniak and Deb Roy.  2007.  Situated Language 
Understanding  as  Filtering  Perceived  Affordances. 
Cognitive Science, Volume 31, Issue 2, pages 197?
231.
Amit Goyal, Ellen Riloff, Hal Daum?, III.  2010.  Auto-
matically producing plot unit representations for nar-
rative text.  In  Proceedings of the 2010 Conference  
on  Empirical  Methods  in  Natural  Language  Pro-
cessing.
Erica  Greene,  Tugba  Bodrumlu,  and  Kevin  Knight. 
2010.  Automatic Analysis of Rhythmic Poetry with 
Applications to Generation and Translation.  In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 524?533.
Susan Hockey. 2004. The History of Humanities Com-
puting.  In  A  Companion  to  Digital  Humanities. 
Blackwell, Oxford, UK. 
Matthew L.  Jockers  and Daniela M. Witten, 2010 "A 
comparative study of machine learning methods for 
authorship attribution", Literary and Linguistic Com-
puting  25(2):215-223
James  Joyce.   1939.   Finnegan's  Wake.   Faber  and 
Faber, London, UK.
Daniel  Jurafsky and James  H.  Martin.   2000.  Speech  
and  Language  Processing. Pearson  Prentice  Hall. 
Upper Saddle River, New Jersey.
Anna Kazantseva and Stan Szpakowicz. 2010. Summar-
izing  Short  Stories.  In  Computational  Linguistics, 
36(1), pp. 71-109.
Scott W. McQuiggan, Jennifer L. Robison, and James 
C.  Lester.  2010.   Affective  Transitions  in  Narrat-
ive-Centered Learning Environments.  In Education-
al Technology & Society. 13 (1): 40?53.
Inderjeet Mani. 2005. Narrative Summarization. Journ-
al Traitement automatique des langues (TAL): Spe-
cial  issue  on Context:  Automatic Text  Summariza-
tion. 
Inderjeet  Mani.  2010.  Predicting  Reader  Response  in 
Narrative.  In  Proceedings of the Intelligent Narrat-
ive Technologies III Workshop.
Hisar  Maruli  Manurung.  2003.  An  Evolutionary  Al-
gorithm Approach to Poetry Generation. PhD thesis, 
University of Edinburgh. College of Science and En-
gineering. School of Informatics.
Saif Mohammad.  2011.  From Once Upon a Time to 
Happily  Ever  After:  Tracking  Emotions  in  Novels 
and  Fairy  Tales,  In  Proceedings  of  the  ACL 2011 
Workshop  on  Language  Technology  for  Cultural  
Heritage,  Social  Sciences,  and  Humanities  (LaT-
eCH), June, Portland, OR.
Federico  Peinado,  Virginia  Francisco,  Raquel  Herv?s, 
Pablo Gerv?s.  2010.  Assessing the Novelty of Com-
puter-Generated Narratives Using Empirical Metrics. 
Minds & Machines, 20:565?588.
Martin J. Pickering and Simon Garrod. 2004.  Towards 
a  mechanistic  Psychology  of  dialogue.  Behavioral  
and Brain Sciences, 27:169-22.
Livia Polanyi, Chris Culy, Martin van den Berg, Gian 
Lorenzo Thione, David Ahn, 2004.  Sentential Struc-
ture and Discourse Parsing.  In ACL2004 - Workshop  
on Discourse Annotation.
103
Stephen Ramsay.  2011.  Reading machines: Towards 
an  Algorithmic  Criticism.   University  of  Illinois 
Press, Urbana, IL
John Crowe Ransom.  1938.  Criticism, Inc.  Antholo-
gized in  The Norton Anthology of  Theory  & Criti-
cism.  2010.  WW Norton & Company, New York, 
NY.
Giacomo  Rizzolatti  and  Laila  Craighero.  2004.  The 
Mirror Neuron System.  In  Annual Review of Neur-
oscience. 27:169?92.
Ronald Schleifer.  1993.  Structuralism.  in  The Johns  
Hopkins  Guide  to  Literary  Theory  and  Criticism. 
Michael  Groden  and  Martin  Kreiswirth,  eds.   The 
Johns Hopkins University Press.  Baltimore, USA.
Susan Schreibman, Ray Siemens, John Unsworth, eds. 
2004.  A Companion to Digital Humanities.  Black-
well, Oxford, UK. 
Raman  Selden  and  Peter  Widdowson.   1993.   A 
Reader's  Guide  to  Contemporary  Literary  Theory. 
University Press of Kentucky.  Lexington, KY.
Luc Steels.  2006.  How to do experiments in artificial 
language evolution and why.  Proceedings of the 6th  
International  Conference  on  the  Evolution  of  Lan-
guage.  pp 323-332.
Luc  Steels.   2007.  Language  Originated  in  Social 
Brains.  Social Brain Matters: Stances of Neurobio-
logy  of  Social  Cognition,  pages  223-242,  Editions 
Rodopi. Amsterdam NL.
Glenn  Taylor,  Michael  Quist,  Steve  Furtwangler,  and 
Keith  Knudsen.  2007.  Toward  a  Hybrid  Cultural  
Cognitive Architecture. CogSci Workshop on Culture 
and Cognition.
David  Traum.   1994.   A  Computational  Theory  of  
Grounding  in  Natural  Language Conversation,  TR 
545 and Ph.D. Thesis, Computer Science Dept., U. 
Rochester, NY.
Virginia Tufte. 2006. Artful Sentences: Syntax as Style. 
Graphics Press, Chesire, CT.
Akira Utsumi. 2004. Stylistic and Contextual Effects in 
Irony Processing. In Proceedings of the 26th Annual  
Meeting of the Cognitive Science Society.
W.K. Wimsatt,  Jr.,  and Monroe  C.  Beardsley.   1954. 
The  Intentional  Fallacy.  From  The  Verbal  Icon:  
Studies in the Meaning of Poetry.  University of Ken-
tucky Press, Lexington, KY.
William Wordsworth. 1802.  Preface to Lyrical Ballads. 
Anthologized in The Norton Anthology of Theory & 
Criticism.  2010.   WW Norton  & Company,  New 
York, NY.
104
