JEP-TALN-RECITAL 2012, Atelier TALAf 2012: Traitement Automatique des Langues Africaines, pages 107?117,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Extraction de lexiques bilingues ? partir de Wikip?dia 
Rahma Sellami1  Fatiha Sadat2 Lamia Hadrich Belguith1 
(1) ANLP Research Group ? Laboratoire MIRACL 
Facult? des Sciences Economiques et de Gestion de Sfax 
B.P. 1088, 3018 - Sfax ? TUNISIE 
(2) Universit? du Qu?bec ? Montr?al, 201 av. President Kennedy, 
Montr?al, QC, H3X 2Y3, Canada 
Rahma.Sellami@fsegs.rnu.tn, sadat.fatiha@uqam.ca, 
l.belguith@fsegs.rnu.tn 
RESUME ____________________________________________________________________________________________________________   
Avec l'int?r?t accru de la traduction automatique, le besoin de ressources multilingues 
comme les corpus comparables et les lexiques bilingues s?est impos?. Ces ressources sont 
peu disponibles, surtout pour les paires de langues qui ne font pas intervenir l'anglais. 
Cet article pr?sente notre approche sur l'extraction de lexiques bilingues pour les paires 
de langues arabe-fran?ais et yoruba-fran?ais ? partir de l?encyclop?die en ligne 
Wikip?dia. Nous exploitons la taille gigantesque et la couverture de plusieurs domaines 
des articles pour extraire deux lexiques, qui pourront ?tre exploit?s pour d'autres 
applications en traitement automatique du langage naturel. 
ABSTRACT _________________________________________________________________________________________________________  
Bilingual lexicon extraction from Wikipedia 
With the increased interest of the machine translation, needs of multilingual resources 
such as comparable corpora and bilingual lexicon has increased. These resources are not 
available mainly for pair of languages that do not involve English. 
This paper aims to describe our approach on the extraction of bilingual lexicons for 
Arabic-French and Yoruba-French pairs of languages from the online encyclopedia, 
Wikipedia. We exploit the large scale of Wikipedia article to extract two bilingual 
lexicons that will be very useful for natural language applications.  
MOTS-CLES : Lexique bilingue, corpus comparable, Wikip?dia, arabe-fran?ais, yoruba-
fran?ais. 
KEYWORDS : Bilingual lexicon, comparable corpora, Wikipedia, Arabic-French, Yoruba-
French. 
 
107
1 Introduction 
Les ressources linguistiques multilingues sont g?n?ralement construites ? partir de corpus 
parall?les. Cependant, l'absence de ces corpus a incit? les chercheurs ? exploiter d'autres 
ressources multilingues, telles que les corpus comparables : ensembles de textes dans 
diff?rentes langues, qui ne sont pas des traductions les uns des autres (Adafre et de Rijke, 
2006), mais qui contiennent des textes partageant des caract?res communs, tel que le 
domaine, la date de publication, etc. Car moins contrains, ils sont donc plus faciles ? 
construire que les corpus parall?les.  
Les lexiques bilingues constituent une partie cruciale dans plusieurs applications telles 
que la traduction automatique (Och et Ney, 2003) et la recherche d?information 
multilingue  (Grefenstette, 1998). 
Dans cet article, nous cherchons ? exploiter l?aspect multilingue ainsi que la taille 
gigantesque de l?encyclop?die en ligne, Wikip?dia, comme un grand corpus comparable 
pour l'extraction de deux lexiques bilingues (arabe-fran?ais et yoruba-fran?ais). (Morin, 
2007) a montr? que non seulement la taille du corpus comparable mais aussi sa qualit? 
est importante pour l?extraction d?un dictionnaire bilingue. Nous proposons d'utiliser une 
m?thode simple mais efficace, il s?agit d?exploiter les liens inter-langues entre les articles 
Wikip?dia afin d'extraire des termes (simples ou compos?s) arabes et yoruba et leurs 
traductions en fran?ais, puis, utiliser une approche statistique pour aligner les mots des 
termes compos?s.  
Les lexiques extraits seront utilis?s pour l?extraction d?un corpus parall?le ? partir de 
wikip?dia. 
Le contenu de cet article se r?sume comme suit. La section 2 pr?sente un bref aper?u des 
travaux ant?rieurs sur l'extraction de lexiques bilingues. La section 3 d?crit certaines 
caract?ristiques de Wikip?dia que nous avons exploit?es pour l?extraction de nos lexiques 
bilingues. La section 4 pr?sente bri?vement les langues arabe et yoruba. Nous 
pr?sentons, dans la section 5, notre travail de construction des lexiques bilingues ? partir 
de Wikip?dia. Nous ?valuons nos lexiques, dans la section 6. La section 7 conclu cet 
article et donne des pointeurs et extensions pour le futur. 
2 Etat de l?art 
Dans un premier temps, les chercheurs construisent les lexiques bilingues ? partir des 
corpus parall?les. Mais, en raison de l'absence de ces ressources, l?exploitation des corpus 
108
comparables a attir? l?attention de plusieurs chercheurs. (Morin et Daille, 2004) 
pr?sentent une m?thode pour l'extraction de terminologie bilingue ? partir d?un corpus 
comparable du domaine technique. Ils extraient les termes compos?s dans chaque langue 
puis ils alignent ces termes au niveau mot en utilisant une m?thode statistique exploitant 
le contexte des termes. (Otero, 2007) a cr?e un lexique bilingue (anglais-espagnol), en se 
basant sur des informations syntaxiques et lexicales extraites ? partir d?un petit corpus 
parall?le. (Sadat et al, 2003) ont pr?sent? une m?thode hybride qui se base sur des 
informations statistiques (deux mod?les de traduction bidirectionnels) combin?es ? des 
informations linguistiques pour construire une terminologie anglais-japonais. (Morin et 
Prochasson, 2011) ont pr?sent? une m?thode pour l'extraction d?un lexique bilingue 
sp?cialis? ? partir d?un corpus comparable, agr?ment? d?un corpus parall?le. Ils extraient 
des phrases parall?les ? partir du corpus comparable, puis, ils alignent ces phrases au 
niveau mots pour en extraire un lexique bilingue. (Hazem et al, 2011) proposent une 
extension de l?approche par similarit? inter-langue abord?e dans les travaux pr?c?dents. 
Ils pr?sentent un mod?le inspir? des m?tamoteurs de recherche d?information. 
Dans ce qui suit, nous d?crivons les travaux ant?rieurs qui ont exploit? Wikip?dia comme 
corpus comparable pour la construction d?un lexique bilingue.  
(Adafre et de Rijke, 2006) a cr?? un lexique bilingue (anglais-n?erlandais) ? partir de 
Wikipedia dans le but de l?utiliser pour la construction d'un corpus parall?le ? partir des 
articles de Wikip?dia.  Le lexique extrait est compos? uniquement de titres des articles 
Wikip?dia reli?s par des liens inter-langues. Les auteurs ont montr? l?efficacit? de 
l?utilisation de ce lexique pour la construction d?un corpus parall?le. (Bouma et al, 2006) 
ont construit un lexique bilingue pour la cr?ation d'un syst?me de question r?ponse 
multilingue (fran?ais-n?erlandais). En outre, (Decklerck et al, 2006) ont extrait un 
lexique bilingue ? partir des liens inter-langues de Wikip?dia. Ce lexique a ?t? utilis? 
pour la traduction des labels d?une ontologie. Ces travaux sont caract?ris?s par le fait 
qu?ils exploitent uniquement les liens inter-langues de Wikip?dia. Par contre, (Erdmann 
et al, 2008) analysent non seulement les liens inter-langues de wikip?dia, mais 
exploitent aussi les redirections et les liens inter-wiki pour la construction d?un 
dictionnaire anglais-japonais. Les auteurs ont montr? l?apport de l?utilisation de 
Wikip?dia par rapport aux corpus parall?les pour l?extraction d?un dictionnaire bilingue. 
Cet apport apparait surtout au niveau de la large couverture des termes. (Sadat et 
Terrasa, 2010) proposent une approche pour l?extraction de terminologie bilingue ? 
partir de Wikip?dia. Cette approche consiste ? extraire d?abord des paires de termes et 
109
traductions ? partir des diff?rents types d?informations, des liens et des textes de 
Wikip?dia, puis, ? utiliser des informations linguistiques afin de r?ordonner les termes et 
leurs traductions pertinentes et ainsi ?liminer les termes cibles inutiles.  
3 Bref aper?u sur les langues arabe et yoruba  
3.1 La langue arabe 
L?arabe (???????) est une langue originaire de la p?ninsule Arabique. Elle est parl?e en Asie 
et en Afrique du Nord. L?Arabe est issue du groupe m?ridional des langues s?mitiques. 
Elle s??crit de droite ? gauche tout en utilisant des lettres qui prennent des formes 
diff?rentes suivant qu?elles soient isol?es, au d?but, au milieu ou ? la fin du mot.1  
La langue arabe est morphologiquement riche ce qui pose le probl?me de l?ambigu?t? au 
niveau de son traitement automatique, un mot en arabe peut encapsuler la signification 
de toute une phrase (? ? ??? ?? ? ??/est ce que vous souvenez de nous ?). 
3.2 La langue yoruba 
Le yoruba (yor?b?) est une langue tonale appartenant ? la famille des langues nig?ro-
congolaises. Le yorouba, langue maternelle d?environ 20% de la population nig?riane, est 
?galement parl? au B?nin et au Togo. Au Nig?ria, il est parl? dans la plus grande partie 
des ?tats d?Oyo, Ogun, Ondo, Osun, Kwara et Lagos, et ? l?ouest de l??tat de Kogi.  
La langue se subdivise en de nombreux dialectes. Il existe n?anmoins aussi une langue 
standard2. 
Le yoruba s'?crit au moyen de plusieurs alphabet fond?es sur l?alphabet latin muni 
d?accents pour noter les tons (dont la charge fonctionnelle est tr?s importante), et de 
points souscrits pour noter les voyelles ouvertes. 
La voyelle est le centre de la syllabe. Le ton appara?t comme une caract?ristique 
inh?rente ? la voyelle ou ? la syllabe. Il y a autant de syllabes que de tons. Le 
symbolisme se pr?sente comme suit : ton haut: (/), ton bas: (\), ton moyen: (-). 
Ces tons d?terminent le sens du mot, une forme peut avoir plusieurs sens (ex. Igba/deux 
cent, Igba/calebasse, ?gba/temps, etc)3.  
                                                          
1 http://fr.wikipedia.org/wiki/Arabe [consult? le 26/04/2012]. 
2 http://fr.wikipedia.org/wiki/Yoruba_(langue) [consult? le 18/04/2012]. 
3 http://www.africananaphora.rutgers.edu/downloads/casefiles/YorubaGS.pdf [consult? le 
24/04/2012]. 
110
La morphologie de la langue yoruba est riche, faisant, par exemple, un large emploi 
du redoublement (ex. Eso/fruit, so/donner de fruits, j?/ d?goutter , ?jo/pluie). 
4 Caract?ristiques de Wikip?dia 
Lors de l'extraction de terminologies bilingues ? partir de corpus parall?les ou 
comparables, il est difficile d'atteindre une pr?cision et une couverture suffisantes, en 
particulier pour les mots moins fr?quents tels que les terminologies sp?cifiques ? un 
domaine (Erdmann, 2008). Pour notre travail de construction de lexiques bilingues, nous 
proposons d?exploiter Wikip?dia, une ressource multilingue dont la taille est gigantesque 
et qui est en d?veloppement continu. 
Dans ce qui suit, nous d?crivons certaines caract?ristiques de Wikip?dia, ces 
caract?ristiques font de Wikip?dia une ressource pr?cieuse pour l'extraction de ressources 
bilingues. 
Actuellement, Wikip?dia contient 21 368 483 articles dont 1 221 995 articles fran?ais, 
170771 articles en langue arabe et 29 884 articles en langue yoruba4. Ces articles 
couvrent plusieurs domaines. Nous exploitons l?aspect multilingue et gigantesque de 
cette ressource afin d?extraire des lexiques bilingues de large couverture. 
La structure de Wikip?dia est tr?s dense en liens ; ces liens relient soit des articles d?une 
seule langue soit des articles r?dig?s en langues diff?rentes.  
Les liens Wikip?dia peuvent ?tre class?s en :  
- Lien inter-langue : un lien inter-langue relie deux articles en langues diff?rentes. Un 
article a au maximum un seul lien inter-langue pour chaque langue, ce lien a comme 
syntaxe [[code de la langue cible : titre de l?article en langue cible]] avec ? code de la 
langue cible ? identifie la langue de l?article cible  et ? titre de l?article en langue cible ? 
identifie son titre (ex. [[yo:J?p?t?r?]]). Puisque les titres des articles Wikip?dia sont 
uniques,  la syntaxe des liens inter-langue est suffisante pour identifier les articles en 
langues cibles.  
- Redirection : une redirection  renvoie automatiquement le visiteur sur une autre 
page. La syntaxe Wikip?dia d'une redirection est : #REDIRECTION[[page de 
destination]]. Les pages de redirection sont notamment utilis?es pour des abr?viations 
(ex. SNCF redirige vers Soci?t? Nationale des Chemins de Fer), des synonymes (ex. e-
                                                          
4 http://meta.wikimedia.org/wiki/List_of_Wikipedias [consult? le 01/03/2012]. 
111
mail, courriel, m?l et messagerie ?lectronique redirigent vers courrier ?lectronique), des 
noms alternatifs (ex. Karol Wojty?a redirige vers Jean-Paul II), etc. 
- Lien inter-wiki : c'est un lien vers une autre page de la m?me instance de Wikip?dia. 
Le texte du lien peut correspondre au titre de l'article qui constitue la cible du lien (la 
syntaxe en sera alors : [[titre de l'article]]), ou diff?rer du titre de l'article-cible (avec 
la syntaxe suivante : [[titre de l'article|texte du lien]]). 
5 Extraction des lexiques bilingues ? partir de Wikip?dia 
5.1 Extraction des termes 
Nous avons extrait deux lexiques bilingues en exploitant la syntaxe des liens inter-
langues de Wikip?dia. En effet, les liens inter-langues relient deux articles en langues 
diff?rentes dont les titres sont en traduction mutuelle. En outre, ces liens sont cr??s 
par les auteurs des articles, nous supposons que les auteurs ont correctement positionn? 
ces liens. Aussi, un article en langue source est li? ? un seul article en langue cible, donc, 
nous n?avons pas ? g?rer d??ventuels probl?mes d?ambigu?t? au niveau de l?extraction des 
paires de titres.  
Nous avons t?l?charg? la base de donn?es Wikip?dia arabe (janvier 2012)5 et yoruba 
(mars 2012)6 sous format XML et nous avons extrait 104 104 liens inter-langue arabe et 
15 345 liens inter-langue yoruba vers les articles fran?ais. Chaque lien correspond ? une 
paire de titres arabe-fran?ais et yoruba-fran?ais. Certains titres sont compos?s de termes 
simples et d?autres sont compos?s de termes compos?s de plusieurs mots. 
5.2 Alignement des mots 
Dans le but d?avoir un lexique compos? uniquement des termes simples, nous avons 
proc?der ? une ?tape d?alignement des mots. 
Cette ?tape pr?sente plusieurs difficult?s dont : Premi?rement, les alignements ne sont 
pas n?cessairement contigus : deux mots cons?cutifs dans la phrase source peuvent ?tre 
align?s avec deux mots arbitrairement distants de la phrase cible. On appelle ce 
ph?nom?ne distorsion. Deuxi?mement, un mot en langue source peut ?tre align? ? 
plusieurs mots en langue cible ; ce qui est d?fini en tant que fertilit?. 
                                                          
5 http://download.wikipedia.com/arwiki/20120114/ [consult? le 01/03/2012]. 
6 http://dumps.wikimedia.org/yowiki/20120316/ [consult? le 15/03/2012]. 
112
Nous avons proc?d? ? une ?tape d?alignement des mots des paires de titres en nous 
basant sur une approche statistique, nous avons utilis? les mod?les IBM [1-5] (Brown et 
al., 1993) combin?s avec les mod?les de Markov cach?s HMM (Vogel et al,1996) vu que 
ces mod?les standard se sont av?r?s efficaces dans les travaux d'alignement de mots. 
Les mod?les IBM sont des mod?les ? base de mots, c?est-?-dire que l?unit? de traduction 
qui appara?t dans les lois de probabilit? est le mot.  
Les cinq mod?les IBM permettent d?estimer les probabilit?s P(fr |ar) et P(fr |yo) de fa?on 
it?rative, tel que fr est un mot fran?ais, ar est un mot arabe et yo est un mot yoruba. 
Chaque mod?le s?appuie sur les param?tres estim?s par le mod?le le pr?c?dant et prend 
en compte de nouvelles caract?ristiques telles que la distorsion, la fertilit?, etc.  
Le mod?le de Markov cach? (nomm? usuellement HMM) (Vogel et al, 1996) est une 
am?lioration du mod?le IBM2. Il mod?lise explicitement la distance entre l?alignement 
du mot courant et l?alignement du mot pr?c?dent. 
Nous avons utilis? l?outil open source Giza++ (Och et Ney, 2003) qui impl?mente ces 
mod?les pour l?alignement des mots et nous avons extrait les  traductions candidates  ? 
partir d?une table de traductions cr??e par Giza++. Chaque ligne de cette table contient 
un mot en langue arabe (ar) (respectivement yoruba (yo)), une traduction  candidate (fr) 
et un score qui calcule la probabilit? de traduction P(fr|ar) (resp. yoruba P(fr|yo)). 
Apr?s l??tape d?alignement, nous avons extrait 65 049 mots arabes et 155 348 paires de 
traductions candidates en fran?ais. En ce qui concerne le lexique yoruba-fran?ais, nous 
avons extrait 11 235 mots yoruba et 20 089 paires de traductions candidates en fran?ais. 
Afin d?am?liorer la qualit? de nos lexiques, nous avons proc?d? ? une ?tape de filtrage 
qui ?limine les traductions candidates ayant un score inf?rieur ? un seuil.  
 
 
 
 
FIGURE 1 ? Extrait de la table de traduction ar-fr 
 
 
 
 
 
FIGURE 2 ? Extrait de la table de traduction yo-fr 
R?m?           Rome               0.7500 
R?m?           romaine           0.33333 
al?d?nid?     naturelles         1.00000 
?w?j?          Soci?t?             0.66666 
?w?j?          Communaut?    0.20000 
Mathim?t?k? Math?matiques 0.50000 
Copper         Cuivre              1.000 
?? ? ?      Flou               1.0000000 
?? ? ?      Diffusion        0.1666667 
????? ?      ?quipes           0.1250000 
????? ?      f?minin           0.0067568 
????? ?      masculin         0.6690141 
??? ??? ?   N?gociations   1.0000000 
??? ??????  Amazones        1.0000000 
 
113
6 Evaluation 
Puisque notre int?r?t est centr? sur les liens inter-langues de Wikip?dia, les lexiques 
extraits ne contiennent pas des verbes.  
Nous avons ?valu?, manuellement, la qualit? de notre lexique bilingue en calculant la 
mesure de pr?cision et en se r?f?rant ? un expert.  
????????? =
nombre de traductions extraites correctes 
nombre de traductions extraites
 
Nous avons calcul? la pr?cision en se basant sur les traductions candidates de 50 mots 
arabes et yoruba et nous avons fait varier le seuil de 0 ? 1 pour en identifier la valeur 
optimale en fonction de la pr?cision. 
La figure 3 pr?sente les valeurs de pr?cision des deux lexiques en variant le seuil.  
Remarquons qu?en augmentant le seuil, la pr?cision est am?lior?e. Sa valeur passe de 
0.46 (avec un seuil ?gale 0) ? 0.74 (quand le seuil ?gale ? 1) pour le lexique yoruba-
fran?ais et de 0.22 ? 0.75 pour le lexique arabe-fran?ais. 
La figure 4 montre que la couverture du lexique fran?ais-yoruba et presque stable, elle 
varie entre 14045 (quand le seuil ?gale ? 0) et 11184 (quand le seuil ?gale ? 1). Ces 
valeurs sont tr?s inf?rieures par rapport ? celles du lexique arabe-fran?ais, ceci est d? 
principalement au faible nombre des articles Wikip?dia yoruba.  
La figure 3 montre que les meilleures valeurs de pr?cision sont atteintes ? partir d?un 
seuil ?gal ? 0.6 pour le lexique arabe-fran?ais. Mais, remarquons dans la figure 4, qu?? 
partir de ce seuil, la couverture du lexique est affaiblie. Ceci est expliqu? par le fait que 
plusieurs fausses traductions ont ?t? ?limin?es ? partir de ce seuil. 
Les erreurs du lexique yoruba-fran?ais sont dues principalement au fait que certains 
titres wikip?dia sont introduits en anglais (ex. density/densit?) et aux erreurs 
d?alignements (ex. Tanaka/Giichi).  
Les erreurs de traduction du lexique arabe-fran?ais sont dues principalement au fait que 
certains titres arabes sont introduits en langue autre que l?arabe (ex. cv/cv), en majorit? 
en langue anglaise. Certaines traductions candidates sont des translit?rations et pas des 
traductions (ex. ???????/Intifada). Aussi, nous avons d?tect? des erreurs d?alignement (ex.  
?? ?? ?/diagnostique). D?autres erreurs sont dues au fait que les paires de titres des 
articles ne sont pas des traductions pr?cises mais il s?agit juste de la m?me notion  (ex. 
???/No?l). 
114
 
FIGURE 3 ?Variation de la pr?cision des lexiques yo-fr et ar-fr selon le seuil 
  
FIGURE 4 ? Variation de la couverture des lexiques yo-fr et ar-fr selon le seuil 
7 Conclusion 
L?exploitation de Wikip?dia pour la construction de ressources linguistiques multilingues 
fait l?objet de plusieurs travaux de recherches, comme la construction des corpus 
parall?les, des lexiques multilingues et des ontologies multilingues. 
Dans cet article, nous avons d?crit notre travail pr?liminaire d?extraction de lexiques 
(arabe-fran?ais et yoruba-fran?ais) ? partir de Wikip?dia. En effet, notre but majeur est 
d?exploiter Wikip?dia en tant que corpus comparable pour la traduction automatique 
statistique.  
La m?thode que nous proposons est efficace malgr? sa simplicit?. Il s?agit d?extraire les 
titres arabes, yorubas et fran?ais des articles de Wikip?dia, en se basant sur les liens 
inter-langues puis d?aligner les mots de ces titres en se basant sur une approche 
statistique. Nous avons atteint des valeurs de pr?cision et de couverture encourageantes 
qui d?passent respectivement 0.7 et 60 000 paires de traductions pour le lexique arabe-
fran?ais et 0.7 et 14 000 paires de traductions pour le lexique yoruba-fran?ais. 
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
pr?cision ar-fr pr?cision yo-fr
0
10000
20000
30000
40000
50000
60000
70000
couverture du lexique yo-fr couverture du lexique ar-fr
no
m
br
e 
de
 p
ai
re
s 
de
 tr
ad
uc
tio
ns
115
Comme travaux futurs, nous envisageons d??largir la couverture de nos lexiques en 
exploitant d?autres liens Wikip?dia comme les redirections et les liens inter-wiki. Nous 
envisageons aussi d?utiliser ces lexiques pour l?extraction des corpus parall?les (arabe- 
fran?ais et yoruba-fran?ais) ? partir de Wikip?dia. Ces corpus seront utilis?s au niveau de 
l?apprentissage des syst?mes de traduction automatique statistique arabe-fran?ais et 
yoruba-fran?ais.  
Re?fe?rences 
ADAFRE, S. F. ET DE RIJKE, M. (2006). Finding Similar Sentences across Multiple Languages 
in Wikipedia. In Proceedings of the EACL Workshop on NEW TEXT Wikis and blogs and 
other dynamic text sources, pages 62?69. 
BOUMA, G., FAHMI, I., MUR, J., G. VAN NOORD, VAN DER, L., ET TIEDEMANN, J. (2006). Using 
Syntactic Knowledge for QA. In Working Notes for the Cross Language Evaluation Forum 
Workshop. 
BROWN PETER, F., PIETRA, V. J., PIETRA, S. A., ET MERCER, R. L. (1993). The Mathematics of 
Statistical Machine Translation: Parameter Estimation. IBM T.J. Watson Research Center, 
pages 264-311. 
DECLERCK, T., PEREZ, A. G., VELA, O., , Z., ET MANZANO-MACHO, D. (2006). Multilingual 
Lexical Semantic Resources for Ontology Translation. In Proceedings of International 
Conference on Language Ressources and Evaluation (LREC), pages 1492 ? 1495. 
ERDMANN, M., NAKAYAMA, K., HARA, T. ET NISHIO, S. (2008). A bilingual dictionary 
extracted from the wikipedia link structure. In Proceedings of International Conference on 
Database Systems for Advanced Applications (DASFAA) Demonstration Track, pages 380-
392. 
ERDMANN, M. (2008). Extraction of Bilingual Terminology from the Link Structure of 
Wikipedia. MSc. Thesis, Graduate School of Information Science and Engineering, Osaka 
University. 
GREFENSTETTE, G. (1998). The Problem of Cross-language Information Retrieval. Cross-
language Information Retrieval. Kluwer Academic Publishers. 
HAZEM, A., MORIN, E. ET SEBASTIAN P. S. (2011). Bilingual Lexicon Extraction from 
Comparable Corpora as Metasearch. In Proceedings of the 4th Workshop on Building and 
116
Using Comparable Corpora, pages 35?43, 49th Annual Meeting of the Association for 
Computational Linguistics, Portland, Oregon.  
MORIN, E. (2007). Synergie des approches et des ressources d?ploy?es pur le traitement 
de l??crit. Ph.D. thesis, Habilitation ? Diriger les Recherches, Universit? de Nantes. 
MORIN, E. ET DAILLE, B. (2004). Extraction de terminologies bilingues ? partir de corpus 
comparables d?un domaine sp?cialis?. Traitement Automatique des Langues (TAL), pages 
103?122. 
MORIN, E. ET PROCHASSON E. (2011). Bilingual Lexicon Extraction from Comparable 
Corpora Enhanced with Parallel Corpora. In Proceedings of the 4th Workshop on Building 
and Using Comparable Corpora, pages 27?34. 
OCH, F.J. ET NEY, H. (2003). A systematic comparison of various statistical alignment 
models. Computational Linguistics, pages 19?51, March. 
OTERO, PABLO G. (2007). Learning bilingual lexicons from comparable english and 
spanish corpora. In Proceedings of Machine Translation Summit XI, pages 191?198. 
SADAT, F., YOSHIKAWA, M. ET UEMURA, S. 2003. Bilingual terminology acquisition from 
comparable corpora and phrasal translation to cross-language information retrieval. In 
Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume, 
pages 141?144. Association for Computational Linguistics. 
SADAT, F. ET TERRASSA, A. (2010). Exploitation de Wikip?dia pour l?Enrichissement et la 
Construction des Ressources Linguistiques. TALN 2010, Montr?al. 
VOGEL, S., NEY H. ET C. TILLMANN (1996). HMM-based word alignment in statistical 
translation. In Preceding of the Conference on Computational Linguistics, pages 836?841, 
Morristown, NJ, USA. 
117

Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 102?110,
Coling 2014, Dublin, Ireland, August 24 2014.
Collaboratively Constructed Linguistic Resources for Language Vari-ants and their Exploitation in NLP Applications ? the case of Tunisian Arabic and the Social Media 
  Fatiha Sadat University of Quebec in Mon-treal 201 President Kennedy, Mon-treal, QC, Canada sadat.fatiha@uqam.ca 
Fatma Mallek  University of Quebec in Mon-treal 201 President Kennedy, Mon-treal, QC, Canada mallek.fatma@uqam.ca 
Rahma Sellami Sfax University, Sfax, Tunisia rah-ma.sellami@fsegs.rnu.tn  Mohamed Mahdi Boudabous Sfax University, Sfax, Tunisia mehdiboudabous@gmail.com 
Atefeh Farzindar NLP Technologies Inc. 52 LeRoyer Street W, Montreal, Canada farzindar@nlptechnologies.ca  Abstract Modern Standard Arabic (MSA) is the formal language in most Arabic countries. Ara-bic Dialects (AD) or daily language differs from MSA especially in social media communication. However, most Arabic social media texts have mixed forms and many variations especially between MSA and AD. This paper aims to bridge the gap be-tween MSA and AD by providing a framework for the translation of texts of social media. More precisely, this paper focuses on the Tunisian Dialect of Arabic (TAD) with an application on automatic machine translation for a social media text into MSA and any other target language. Linguistic tools such as a bilingual TAD-MSA lexicon and a set of grammatical mapping rules are collaboratively constructed and exploited in addition to a language model to produce MSA sentences of Tunisian dialectal sen-tences. This work is a first-step towards collaboratively constructed semantic and lexi-cal resources for Arabic Social Media within the ASMAT (Arabic Social Media Anal-ysis Tools) project.  1 Introduction The explosive growth of social media has led to a wide range of new challenges for machine transla-tion and language processing. The language used in social media occupies a new space between struc-tured and unstructured media, formal and informal language, and dialect and standard usage. Yet these new platforms have given a digital voice to millions of user on the Internet, giving them the opportuni-ty to communicate on the first truly global stage ? the Internet (Colbath, 2012). Social media poses three major computational challenges, dubbed by Gartner the 3Vs of big data: volume, velocity, and variety1. Natural Language Processing (NLP) methods, in particular, face further difficulties arising from the short, noisy, and strongly contextualised nature of social media. In order to address the 3Vs of social media, new language technologies have emerged, such as the identification and definition of users' language varieties and the translation to a different language, than the source.                                                 1 http://en.wikipedia.org/wiki/Big_data  This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
102
Furthermore, language in social media is very rich with linguistic innovations, morphology and lexical changes. People are not only socially connected across the world but also emotionally and linguistical-ly (Sadat, 2013).  The importance of social media stems from the fact that the use of social networks has made every-body a potential author, which means that the language is now closer to the user than to any prescribed norms. Thus, considerable interest has recently been focused on the analysis of social media in order to create or enrich NLP tools and applications. There are, however, still many challenges to be faced depending on the used language and its variants.  This paper deal with Arabic language and its variants for the analysis of social media and the col-laborative construction of linguistic tools, such as lexical dictionaries and grammars and their exploita-tion in NLP applications, such as translation technologies.  Basically, Arabic is considered as morphologically rich and complex language, which presents sig-nificant challenges for NLP and its applications. It is the official language in 22 countries spoken by more than 350 million people around the world2. Moreover, Arabic language exists in a state of di-glossia where the standard form of the language, Modern Standard Arabic (MSA) and the regional dialects (AD) live side-by-side and are closely related (Elfardy and Diab, 2013). Arabic has more than 22 variants, refereed a as dialects; some countries share the same dialects, while many dialects may exist alongside MSA within the same Arab country. Arabic Dialects (AD) or daily language differs from MSA especially in social media communication. However, most Arabic social media texts have mixed forms and many variations especially between MSA and AD. This paper describes our efforts to create linguistic resources and translation tool for TDA to MSA. First, a bilingual TDA-MSA lexicon and a set of TDA mapping rules for the social media context are collaboratively constructed. Second, these tools are exploited in addition to a language model extract-ed from MSA corpus, to produce MSA sentences of the Tunisian dialectal sentences of social media. The rule-based translation system can be coupled with a statistical machine translation system from MSA into any language, example French or English to provide a translation from TDA to French or English of original Tunisian dialectal sentences of social media. This paper is organized as follows. In Section 2, we present some related works to this research. Section 3 discusses the Tunisian Dialect of Arabic  (TDA) and its challenges in social media context. In Section 4, we present the collaboratively construct linguistic tools for the social media. Section 5 presents some evaluations of the combined TDA-MSA rule-based translation and disambiguation sys-tem. Section 6 concludes this paper and gives some future extensions. 2 Related Work There have been several works on Arabic NLP. However, most traditional techniques have focused on MSA, since it is understood across a wide spectrum of audience in the Arab world and is widely used in the spoken and written media. Few works relate the processing of dialectal Arabic that is different from processing MSA. First, dialects leverage different subsets of MSA vocabulary, introduce different new vocabulary that are more based on the geographical location and culture, exhibit distinct grammatical rules, and adds new morphologies to the words. The gap between MSA and Arabic dia-lects has affected morphology, word order, and vocabulary (Kirchhoff  and Vergyri, 2004). Almeman and Lee (2013) have shown in their work that only 10% of words (uni-gram) share between MSA and dialects. Second, one of the challenges for Arabic NLP applications is the mixture usage of both AD and MSA within the same text in social media context.  Recently, research groups have started focusing on dialects. For instance, Columbia University provides a morphological analyzer (MAGEAD) for Levantine verbs and assumes the input is non-noisy and purely Levantine (Habash and Rambow, 2006b). Given that DA and MSA do not have much in terms of parallel corpora, rule-based methods to translate DA-to-MSA or other methods to collect word-pair lists have been explored. Abo Bakr et al. (2008) introduced a hybrid approach to translate a sentence from Egyptian Arabic into MSA. This hy-brid system consists of a statistical system for tokenizing and tagging, and a rule-based system for the construction of diacritized MSA sentences. Al-Sabbagh and Girju (2010) described an approach of                                                 2 http://en.wikipedia.org/wiki/Geographic_distribution_of_Arabic#Population 
103
mining the web to build a DA-to-MSA lexicon. Salloum and Habash (2012) developed Elissa, a dia-lectal to standard Arabic tool that employs a rule-based translation approach and relies on morphologi-cal analysis, morphological transfer rules and dictionaries in addition to language models to produce MSA paraphrases of dialectal sentences. Using closely related languages has been shown to improve MT quality when resources are limited. In the context of Arabic dialect translation, Sawaf (2010) built a hybrid MT system that uses both sta-tistical and rule-based approaches for DA-to-English MT. In his approach, DA (but not TDA) is nor-malized into MSA by performing a combination of character- and morpheme-level mappings. They then translated the normalized source to English using a hybrid MT or alternatively a Statistical MT system.  Very few researches were reported on Tunisian variant of Arabic or any other Maghrebi variant. Hamdi et al. (2013) presented a translation system between MSA TDA verbal forms. Their approach relies on modeling the translation process over the deep morphological representations of roots and patterns, commonly used to model Semitic morphology. The reported results are aat 80% recall in the TDA into MSA and 84% recall in the opposite direction. However, the translation process was highly ambiguous, and a contextual disambiguation process was therefore necessary for such a process to be of practical use. Boudjelbene et al. (2013a, 2013b) described a method for building a bilingual diction-ary using explicit knowledge about the relation between TDA and MSA and presented an automatic process for creating Tunisian Dialect corpora. However, their work focused on verbs mainly in order to adapt MAGEAD morphological analyser and generator of arabic dialect to TDA (Hamdi et al., 2013). Also, they developed a tool that generates TDA corpora and enrich semi-automatically the dic-tionaries they built. Experiments in progress showed that the integration of translated data improves lexical coverage and the perplexity of language models significantly. Their research was very pertinent for TDA but did not consider the mixture form of social media corpora.  Shaalan (2010) presented a rule-based approach for Arabic NLP and developed a transfer-based machine translation system of English noun phrase to Arabic. Their research showed that a rapid de-velopment of rule-based systems is feasible, especially in the absence of linguistic resources and the difficulties faced in adapting tools from other languages due to peculiarities an the nature of Arabic language.  In real-life practise, a company named Qordoba3 launched social media translation service for Ara-bic in general. However, no demonstration or freely available version was found online. Furthermore, a new Twitter service automatically translates tweets from some Arabic language variants to English. However, this translation tool is not 100% accurate4. 3 The Tunisian Dialect of Arabic and its Challenges in Social Media Tunisian, or Tunisian Arabic5 (TDA) is a Maghrebi dialect of the Arabic language, spoken by some 11 million people in coastal Tunisia. It is usually known by its own speakers as Derja, which means dia-lect, to distinguish it from Standard Arabic, or as Tunsi, which means Tunisian. In the interior of the country it merges, as part of a dialect continuum, into Algerian Arabic and Libyan Arabic. The morphology, syntax, pronunciation and vocabulary of Tunisian Arabic are quite different from Standard or Classical Arabic. TDA, like other Maghrebi dialects, has a vocabulary mostly Ara-bic, with significant Berber substrates, and many words and loanwords borrowed from Berber, French, Turkish, Italian and Spanish. Derja is mutually spoken and understood in the Maghreb countries, especially Morocco, Algeria and Tunisia, but hard to understand for middle east-ern Arabic speakers. It continues to evolve by integrating new French or English words, notably in technical fields, or by replacing old French and Spanish ones with Standard Arabic words within some circles. Moreover, Tunisian is also closely related to Maltese, which is not considered to be a dialect of Arabic for sociolinguistic reasons.  An exemple is the following sentences in Tunisian Dialect of Arabic (TDA) in social media, as pre-sented in Figure 1. The underlined words (also in red) cannot be analyzable by MSA morphological analyzers, and thus need their own TDA analysis. Moreover, there are some words (in blue) expressed                                                 3 http://www.wamda.com/2013/06/qordoba-launches-new-social-media-translation-service 4 http://www.neurope.eu/article/twitter-launches-arabic-translation-service 5 http://en.wikipedia.org/wiki/Tunisian_Arabic 
104
in languages other than Arabic, French in this case. At least three morphological tools are needed for this short text that is very common in social media. It is often assumed that for the country in question, users of social media will use mostly if not al-ways, the native language. However, this isn?t always the case. Languages will be mixed up with up to three languages in a single ?tweet? or blog. A Tunisian user of social media can involve the following languages or their variants: 1) native tongue (Arabic dialect), 2) MSA (example for greetings), 3) Eng-lish and 4) the Colonial country?s language, which is French in our case. In the case of Tunisian, French words/numbers may be used that sound like an Arabic word. An accurate machine translation for social media should manage this level of complexity, especially when ones add numerical charac-ters and an ever-changing Lexicon of words.  
  Figure 1. Example of Social media text including a mixture of MSA, TDA and French language 4 Collaboratively Constructed Linguistic Tools for TDA  This section describes our effort in collaboratively constructing some linguitic tools that help translate Tunisian text of social media into MSA and other target languages considering MSA as a pivot language. Among these tools, a bilingual TDA-MSA lexicon and a set of mapping rules that will be integrated in the rule-based translation system (TDA-into-MSA). Furthermore, a language Modeling of MSA will help disambiguate the many translation hypothesis and thus select the best phrasal translation in MSA.  
 Figure 2. An example from the TDA-MSA lexicon database 
105
4.1 The TDA-MSA Bilingual Lexicon  We have manually and collaboratively developed a bilingual TDA-MSA lexicon that contains around 1,600 source words in TDA and its corresponding translations in MSA, defined by a human expert. Furthermore, our research on some downloadded extracts from Tunisian blogs (around 6,000 words), showed a difference between verb morphology in TDA and that in MSA. We find that in TDA, the gender distinction is not marked. Similarly, we noticed the absence of the masculine and feminine dual in TDA.   In this phase, our aim was to build a bilingual lexicon of Tunisian nouns and verbs and their translations into MSA. Note that a term can be a noun, a verb, an adverb, etc. Furthermore, the most used imported words from other language than Arabic (Berbere, French, English, Turkish, Spanish, Maltese) and used in social media context were considered in this lexicon.  These TDA-MSA couples are stored in an XML database. Figure 2 shows a bilingual TDA-MSA extract from the lexicon database, encoded in XML. 4.2 Grammatical Mapping Rules for TDA  Our second collaboratively constructed linguistic tool, consists on a set of mapping rules that were checked by human experts. This set consists of some rules applied on verbs transformation in TDA and their corresponding translation into MSA. In final, we have defined a set of 226 mapping rules from TDA into MSA on verbs transformation. Figure 3 shows an extract of the defined rules, encoded in XML. Figure 4 shows an example of a verb in TDA and its translation ito MSA using rule number 171 of the collaboratively built set of mapping rules. 4.3 Automatic Rule-based TDA-MSA Machine Translation  We have developed a rule-based translation system that is able to translate any social media text in TDA into MSA. This rule-based translation system can be coupled with any statistical machine trans-lation system from MSA to another language to provide a translation of original Tunisian dialectal sentences of social media from TDA to that other language.  Figure 5 shows the different steps used in the translation of any social media text from TDA into MSA. First, for each word in the TDA social media text, we proceed by searching in the TDA-MSA lexicon database for the corresponding translation of the TDA word. Mostly, TDA nouns and imported words from other languages than Arabic were included in the lexicon.  Second, we proceed by search-ing in the database of mapping rules for the source verb in TDA and its corresponding MSA transla-tion, as shown in Figure 4. Last, both word-by-word translation candidates are extracted from the lexi-cons and using the set of mapping rules; thus considered as translation hypothesis.    
 Figure 3. An example of some mapping rules from TDA to MSA 
106
  Figure 4. An example of the application of rule 171 for a verb in TDA and its translation into MSA 
4.4 Language Modeling  The rule-based translation system is based on a word-by-word translation using the bilingual lexicon and the set of mapping rules. Thus, most of the time, one TDA sentence will have more than one pos-sible translation. The language modeling (LM) of the target language (MSA) combined to the previous rule-based translation system will help disambiguate and select the best translation hypothesis in MSA.  
 Figure 5. The rule-based translation approach for an automatic mapping from TDA to MSA 5 Evaluations  We have carried out some experiments and evaluations on the accuracy of the translation of TDA so-cial media texts into MSA. First, we collected manually a TDA corpus consisting of 6,000 words from some Tunisian forums and blogs. This corpus is very heterogeneous and multilingual, as many words are not in TDA but in MSA, French, English and sometimes using a certain style and form of social media, example using tweeter or SMS slangs). An extract of this corpus is presented in Figure 1. 
107
For evaluation purposes, we considered a reference set of 50 phrases in TDA, translated manually into MSA. We also considered these 50 TDA phrases as the test set. Thus, we applied the proposed rule-based approach on this test set. In order to combine adequately the rule-based translation approach to the language modeling (in MSA), we considered using the United Nation Arabic corpus to train a trigram language model. This training corpus contains around 50M words after cleaning the Latin content.  A preprocessing step is very crucial to any Arabic language processing. We considered tokenizing the MSA words using the D3 (Habash and Sadat, 2006a) scheme to overcome all problems of aggluti-nation. The D3 scheme splits off clitics as follows: the class of conjunction clitics (w+ and f+), the class of particles (l+, k+, b+ and s+), the definite article (Al+) and all pronominal enclitics. These pre-processing are applied for both the hypothesis translation sentences and the training corpus, both in MSA. In addition to this preprocessing step, manual cleaning the MSA corpus of Latin contents was required. Thus, a trigram language model was implemented using the SRILM toolkit (Stolcke, 2002) on this training MSA corpus.  Next, we extracted all possible trigrams from the preprocessed MSA hypotheses translations and we computed the probability that these trigrams extracted appear in the MSA corpus based on the lan-guage model. A probability for each hypothesis translation is computed based on a trigram language model (LM). The hypothesis translation that has the highest probability is considered as the best trans-lation. Evaluations of the best translation sentence from TDA to MSA against the reference sentence in MSA were completed using the BLEU metric for automatic machine translation (Papineni et al., 2002). Our experiment produced a score of 14.32 BLEU. This low score could be related to our rule-based translation approach that is word-based and to the high number of unknown words in our source test file in other language variants than TDA. Adopting a phrasal translation and solving the problem of unknown words should be more effective. Unfortunately, we could not found an available TDA-MSA test and reference files to conduct better evaluations in machine translation and social media context.    6 Conclusion and Future Work Social media has become a key communication tool for people around the world. Building any NLP tool for texts extracted from social media is very challenging and daunting task and always be limited by the rapid changes in the social media. Considering an Arabic social media text is much more chal-lenging because of the dominant use of English, French and other languages which intend to bring more problems to solve. This paper presents our effort to create linguistic resources such as a bilingual lexicon, a set of grammatical mapping rule and a ruel-based translation and disambiguation system for the translation of any social media text from TDA into MSA. A language modeling of MSA is used in the disambiguation phase and the selection of  the best translation phrase. As for future work, we intend to enlarge the set of words in the TDA-MSA lexicon as well as the set of mapping rules. We intend to develop more grammatical rules for not only verbs but also adjec-tives and nouns. Furthermore, it would be interesting to build a parallel or comparable TDA-MSA corpus by selecting the most pertinent sources of social media and mining the web. A phrase-based statistical machine translation can be built using this parallel/comparable corpus and coupled to the rule-based translation system.  What we presented in this draft is a research on exploiting social media corpora for Arabic in order to analyze them and exploit them for NLP applications, such as machine translation within the scope of the ASMAT project. Reference Hitham Abo-Bakr, Khaled Shaalan, and Ibrahim Ziedan. 2008. A Hybrid Approach for Converting Written Egyptian Colloquial Dialect into Discretized Arabic. Proceedings of the 6th International Conference on In-formatics and Systems 2008. Cairo University. 
108
Rania Al-Sabbagh and Roxana Girju. 2010. Mining the Web for the Induction of a Dialectical Arabic Lexicon. Proceedings of the 7th International Conference on Language Resources and Evaluation LREC 2010. Vallet-ta, Malta, May 19-21, 2010. Khalid Almeman and Mark Lee. 2013. Automatic Building of Arabic Multi Dialect Text Corpora by Bootstrap-ping Dialect Words. In Communications, Signal Processing, and their Applications ICCSPA 2013. Sharjah, UAE, Feb.12-14, 2013. Rahma Boujelbane, Mariem Ellouze Khemekhem, Siwar BenAyed, and Lamia Hadrich Belguith. 2013. Building Bilingual Lexicon to Create Dialect Tunisian Corpora and Adapt Language Model. Proceedings of the 2nd Workshop on Hybrid Approaches to Translation, ACL 2013. Sofia, Bulgaria. Rahma Boujelbane, Mariem Ellouze Khemekhem, and Lamia Hadrich Belguith. 2013. Mapping Rules for Build-ing a Tunisian Dialect Lexicon and Generating Corpora. Proceedings of the International Joint Conference on Natural Language Processing. Nagoya, Japan.   David Chiang, Mona Diab, Nizar Habash, Owen Rambow, and Safiullah Shareef. 2006. Parsing Arabic Dialects. Proceedings of the European Chapter of ACL EACL 2006. Sean Colbath. 2012. Language and Translation Challenges in Social Media. Proceedings of AMTA 2012, Gov-ernment presentations. Submitted by Raytheon BBN Technologies. Oct. 28th to Nov. 1st, 2012. San Diego, USA.  Mona Diab and Nizar Habash. 2007. Arabic Dialect Processing Tutorial. Proceedings of HLT-NAACL, Tutorial Abstracts 2007: 5-6. Heba Elfardy and Mona Diab. 2013. Sentence-Level Dialect Identification in Arabic. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, Sofia, Bulgaria. 2013. Nizar Habash and Fatiha Sadat. 2006. Arabic Pre-processing Schemes for Statistical Machine Translation. Pro-ceedings of the Human Language Technology Conference of the NAACL. Companion volume: 49?52. New York City, USA. Nizar Habash and Owen Rambow. 2006. MAGEAD: A Morphological Analyzer and Generator for the Arabic Dialects. Proceedings of the 21st International Conference on Computational Linguistics and 44st Annual Meeting of the Association for Computational Linguistics: 681?688, Sydney, Australia. Ahmed Hamdi, Rahma Boujelbane, Nizar Habash, and Alexis Nasr. 2013. The Effects of Factorizing Root and Pattern Mapping in Bidirectional Tunisian - Standard Arabic Machine Translation. Proceedings of MT Sum-mit 2013, Nice, France. Ahmed Hamdi, Rahma Boujelbane, Nizar Habash, and Alexis Nasr. 2013. Un Syst?me de Traduction de Verbes entre Arabe Standard et Arabe Dialectal par Analyse Morphologique Profonde. Proceedings of TALN 2013, Nantes, France. Hanaa Kilany, Hassan. Gadalla, Howaida Arram, Ashraf Yacoub, Alaa El-Habashi, and Cynthia McLemore. 2002. Egyptian Colloquial Arabic Lexicon. LDC catalog number LDC99L22. Katrin Kirchhoff, Jeff Bilmes, Sourin Das, Nicolae Duta, Melissa Egan, Gang Ji, Feng He, John Henderson, Daben Liu, Mohamed Noamany, Pat Schone, Richard Schwartz, and Dimitra Vergyri. 2003. Novel Ap-proaches to Arabic Speech Recognition: Report from the 2002 Johns Hopkins Summer Workshop. Proceed-ings of IEEE International Conference on Acoustics, Speech, and Signal Processing. Hong Kong, China. Katrin Kirchhoff and Dimitra Vergyri. 2004. Cross-dialectal Acoustic Data Sharing for Arabic Speech Recogni-tion. Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, 2004.  Kishore Papineni, Salim Roukos, Todd Ward, and Wei J. Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. Proceedings of the 40th Annual Meeting of the Association for Computational Lin-guistics: 311?318. Philadelphia, USA. Fatiha Sadat. 2013. Arabic social media analysis for the construction and the enrichment of NLP tools. In Cor-pus Linguistics 2013. Lancaster University, UK. Jul. 22-26, 2013. Hassan Sajjad, Kareem Darwish and Yonatan Belinkov. 2013. Translating Dialectal Arabic to English. Proceed-ings of the 51st Annual Meeting of the Association for Computational Linguistics: 1?6. Sofia, Bulgaria, Aug. 4-9 2013. 
109
Wael Salloum and Nizar Habash. 2011. Dialectal to Standard Arabic Paraphrasing to Improve Arabic-English Statistical Machine Translation. Proceedings of the First Workshop on Algorithms and Resources for Model-ling of Dialects and Language Varieties. Edinburgh, Scotland. Wael Salloum and Nizar Habash. 2012. Elissa: A Dialectal to Standard Machine Translation System. Proceed-ings of Coling 2012: Demonstration Papers: 385-392, Mumbai, India. Hassan Sawaf. 2010. Arabic Dialect Handling in Hybrid Machine Translation. Proceedings of the Conference of the Association for Machine Translation in the Americas AMTA 2010. Denver, Colorado. Khaled Shaalan. 2010. Rule-based Approach in Arabic Natural Language Processing. International Journal on Information and Communication Technologies, 3(3). Andreas Stolcke. 2002.  SRILM?An Extensible Language Modeling Toolkit. Proceedings of ICSLP, 2002. Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz, John Ma-khoul, Omar F. Zaidan, and Chris Callison-Burch. 2012. Machine translation of Arabic Dialects. Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Montreal, Canada. 
110
