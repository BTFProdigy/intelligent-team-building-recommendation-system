Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 162?165,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Simple Parser for Indian Languages in a Dependency Framework 
 
Akshar Bharati, Mridul Gupta, Vineet Yadav, Karthik Gali and Dipti Misra Sharma 
Language Technologies Research Center, 
IIIT-Hyderabad, India 
{mridulgupta,vineetyadav}@students.iiit.ac.in, 
karthikg@research.iiit.ac.in,dipti@iiit.ac.in 
 
 
Abstract 
This paper is an attempt to show that an inter-
mediary level of analysis is an effective way 
for carrying out various NLP tasks for linguis-
tically similar languages. We describe a 
process for developing a simple parser for 
doing such tasks. This parser uses a grammar 
driven approach to annotate dependency rela-
tions (both inter and intra chunk) at an inter-
mediary level. Ease in identifying a particular 
dependency relation dictates the degree of 
analysis reached by the parser. To establish ef-
ficiency of the simple parser we show the im-
provement in its results over previous gram-
mar driven dependency parsing approaches for 
Indian languages like Hindi. We also propose 
the possibility of usefulness of the simple 
parser for Indian languages that are similar in 
nature. 
1 Introduction and Related Work 
Broad coverage parsing is a challenging task. For 
languages such as the Indian languages, it be-
comes all the more difficult as these languages 
are morphologically richer and the word order 
for these languages is relatively variable and less 
bound. Although dependency grammar driven 
parsing is much better suited for such type of 
languages (Hudson, 1984; Mel?Cuk, 1988), ro-
bust broad coverage parsing (Bharati et. al, 2008) 
still involves extensive analysis. Achieving good 
results in parsing for these languages may re-
quire large amount of linguistic resources such as 
annotated corpora, verb frames, lexicon etc. On 
the other hand, pure shallow parsing techniques 
(PVS and Gali, 2007) are not enough for provid-
ing sufficient information for applications such 
as machine translation, query answering etc.  
It is here that the notion of a simple parser is 
born where the idea is to parse a sentence at a 
coarser level. One could go to a finer level of 
parse depending on the ease with which such a 
parse can be generated. The simple parser that 
we describe here is a grammar oriented model 
that makes use of linguistic features to identify 
relations. We have modeled the simple parser on 
the Paninian grammatical model (Begum et al, 
2008; Bharati et al, 1995) which provides a de-
pendency grammar framework. Paninian depen-
dency grammar works well for analyzing Indian 
languages (Bharati et al, 1993).  We have fol-
lowed karaka1 based approach for parsing. 
An effort has been previously made in gram-
mar driven parsing for Hindi by us (Gupta et al, 
2008) where the focus was not to mark relations 
in a broad coverage sense but to mark certain 
easily identifiable relations using a rule base. In 
this paper, we show improvements in results over 
our previous work by including some additional 
linguistic features which help in identifying rela-
tions better. Our previous work focused only on 
inter-chunk annotation. In this paper, however, 
we have worked on both inter as well as intra 
chunk annotation. We later show their effective-
ness and results at different levels of dependency 
annotation. We also propose how useful the sim-
ple parser is for Indian languages which are simi-
lar in nature. 
2 Paninian Dependency Annotation 
Scheme at Various Levels 
Paninian dependency scheme is based on a mod-
ifier-modified relationship (Bharati et al, 1995). 
The modified chunk (or group) is classified on 
the basis of its part of speech category. A hie-
rarchy of dependency relations is thus estab-
lished on the basis of this category. For example, 
all those relations whose parent (modified group) 
is a verb are classified under the verb modifier 
(vmod) category. Subsequent levels further clas-
sify these relations (or labels). Depth of a level in 
the hierarchy reflects the fineness of the depen-
dency relations/labels. There are five labels at the 
                                               
1 The elements modifying the verb participate in the action 
specified by the verb. These participant relations with the 
verb are called karakas. 
162
coarsest level namely, vmod, nmod (noun mod-
ifier), jjmod (adjective modifier), advmod (ad-
verbial modifier) and ccof (conjunct of). 
Athough, ccof is not strictly a dependency rela-
tion (Begum et al, 2008). Figure 1 shows the 
hierarchy of relations used in the scheme. 
 
 
Figure 1: Hierarchy of Dependency Labels. 
 
The next level comprises of varg (verb argu-
ment), vad (verb adjunct) and vmod_1 2  labels 
under vmod. Under the nmod label, nmod_adj 
(adjective), r6 (genitive) are classified. At the 
most fine grained level, varg and vad further 
branch out into labels like k1, k2, k3, k5, k7 and 
rh, rt, rd, k1s etc. The relations under varg are the 
six karakas that are the most essential partici-
pants in an action. All the other dependency la-
bels3 are non-karakas (for a more detailed expla-
nation see Begum et al (2008) and Bharati et al 
(1995)). 
Languages often have constructions that are 
ambiguous, owing to similar feature and context 
distribution. Thus, in such cases, it is appropriate 
to under-specify the relations (labels) or group 
some of them together. Also, some labels have 
very less frequency of occurrence in the corpus 
and it is thus appropriate to leave them out for 
marking by the simple parser. One can later, on 
the availability of more information, try to identi-
fy and mark such instances with appropriate la-
bels. 
The dependency tagset described in this sec-
tion is used to mark inter-chunk relations. For 
marking relations between words within a chunk 
(intra-chunk), a similar tagset has been devel-
oped. 
                                               
2 vmod_1: A dependency relation in the vmod category, that 
exists between a non-finite verb and its parent verb. It has 
been under-specified for simplicity. 
3A comprehensive list of the dependency tagset can be 
found at http://ltrc.iiit.ac.in/MachineTrans/research/tb/dep-
tagset.pdf 
3 Procedure 
Our approach is corpus based where rules have 
been crafted after studying the corpus. We used 
the Hyderabad Dependency Treebank (HyDT) 
for development and testing our rules. The tree-
bank consists of about 2100 sentences in Hindi, 
of which 1800 were part of the development set 
and 300 were used as test data. Each sentence is 
POS tagged and chunked (Bharati et al, 2006) in 
SSF format (Bharati et al, 2005). 
3.1 Approach 
The simple parser we propose here is a language 
independent engine that takes a rule file specific 
for a particular language (Gupta et. al, 2008). 
Indian languages are similar in various respects 
(Emeneau 1956; 1980). Hence, rules made for 
one language can be efficiently transferred for 
other similar languages. However, there can be 
cases where rules for one language may not work 
for another. These cases can be handled by add-
ing some new rules for that particular language. 
The relative closeness among such languages, 
determines the efficiency of transference of rules 
from one language to another. We have taken 
Hindi and Punjabi, as example languages to sup-
port our proposal. 1(a) below is in Hindi, 
 
1(a). raama  ko      mithaaii acchii    nahii 
     ?Ram - dat?      ?sweets?          ?good?     ?not? 
       lagatii. 
      ?appear? 
 
?Ram does not like sweets.? 
 
Its corresponding Punjabi sentence, 
1(b).  raama   nuu  mitthaai   changii        nii                   
        ?Ram - dat?   ?sweets?  ?good?       ?not? 
     lagadii. 
     ?appear? 
 
?Ram does not like sweets.? 
 
Now, the rules for identifying k14 and k2 in 
Hindi are similar to that of Punjabi. For instance, 
in both the cases, the noun chunk possessing a 
nominative case marker (chunks take the proper-
ties of their heads) and the TAM (tense, aspect 
and modality of the main verb) should agree in 
                                               
4
 k1 (karta) and k2 (karma) are syntactico-semantic labels 
which have some properties of both grammatical roles and 
thematic roles. k1 for example, behaves similar to subject 
and agent. Likewise, k2 behaves like object/theme (Begum 
et al, 2008) 
163
GNP for the noun to be a k2. It is easy to see 
how rules made for identifying certain relations 
in Hindi can be transferred to identify the same 
relations in Punjabi and similarly for other lan-
guages. However, not all rules can be transferred 
from one language to another. 
3.2 Intra-chunk Relations 
We also mark intra-chunk dependency relations. 
The procedure of marking intra-chunk labels is 
also rule based. Rules have been crafted using a 
common POS5 tagset for Indian languages (Bha-
rati et al, 2006). Rules can be applied to other 
languages. However, some rules may not work. 
In those cases we need to add some rules specific 
to the language. The rule format is a five-tuple 
containing the following fields, 
1. Modified word 
2. Modified constraints 
3. Modifier word 
4. Modifier constraints 
5. Dependency relation 
Rules for marking intra-chunk relations have 
been marked studying the POS tagged and 
chunked corpus. Commonly occurring linguistic 
patterns between two or more nodes are drawn 
out in the form of statistics and their figures are 
collected. Such patterns are then converted into 
robust rules. 
4 Experiments and Results 
We conducted experiments using the simple 
parser to establish its efficacy in identifying a 
particular set of relations explained in section 2. 
Experiments were conducted on gold standard 
test data derived from HyDT. The experiments 
were carried out on Hindi. 
4.1 Marking Relations at Various Levels 
We marked dependency labels at various levels 
described above using the proposed simple pars-
er. The results are shown below We report two 
measures for evaluation, labeled (L) and labeled 
attachment (LA). Table 1 shows results for mark-
ing relations at the top most level (cf. Figure 1).  
It should be noted that we have not marked re-
lations like jjmod and advmod because the fre-
quency of their occurrence in the treebank is 
quite low. The focus is only on those relations 
whose frequency of occurrence is above a bare 
minimum (>15). The frequency of labels like 
jjmod and advmod is not above that threshold 
                                               
5 POS: Part of Speech 
value (Relations like k1 and k2 occur more than 
1500 times in the treebank). 
 
Relation 
Precision 
L LA 
 
Recall 
L     LA 
 
vmod 93.7% 83.0% 76.1% 67.4% 
nmod 83.6% 79.1% 77.5% 73.3% 
ccof 92.9% 82.9% 53.5% 50.4% 
Total 91.8% 82.3% 72.9% 65.4% 
Table 1. Figures for relations at the highest level. 
 
Table 2 below depicts the figures obtained for 
the next level. 
Relation 
Precision 
L LA 
 
Recall 
L     LA 
 
varg 77.7% 69.3% 77.9% 69.4% 
vad 75.2% 66.6% 30.3% 26.9% 
vmod_1 89.6% 75.8% 46.0% 38.9% 
r6 83.2% 78.5% 90.2% 85.2% 
nmod__adj 77.8% 77.8% 10.9% 10.9% 
Total 79.1% 71.2% 64.6% 58.2% 
Table 2. Figures for level 2. 
 
In section 1, improvement in marking certain 
relations over our previous attempt (Gupta et. al, 
2008) was mentioned. We provide a comparison 
of the results for the simple parser as opposed to 
the previous results. Figures shown in table 3 
have been reproduced for comparing them 
against the results of the simple parser shown in 
this paper. 
 
Relation 
Precision 
L LA 
 
Recall 
L LA 
 
k1 66.0% 57.7% 65.1% 57.6% 
k2 31.3% 28.3% 27.8% 25.1% 
k7(p/t) 80.8% 77.2% 61.0% 58.4% 
r6 82.1% 78.7% 89.6% 85.8% 
nmod__adj 23.2% 21.9% 27.4% 25.8% 
Table 3. Figures reproduced from our previous 
work. 
 
Table 4 shows results of the simple parser. 
Note the improvement in precision values for all 
the relations.  
 
 
Relation 
Precision 
L LA 
 
Recall 
L LA 
 
k1 72.6% 68.0% 67.9% 63.5% 
k2 61.6% 54.1% 29.9% 26.2% 
k7(p/t) 84.6% 77.9% 73.5% 68.7% 
r6 83.2% 78.6% 90.2% 85.5% 
nmod__adj 77.8% 77.8% 10.9% 10.9% 
pof 89.4% 87.7% 25.7% 25.2% 
Table 4. Figures for simple parser. 
164
4.2 Intra-chunk Experiments 
We also carried out some experiments to deter-
mine the efficiency of the simple parser with re-
spect to annotating intra-chunk relations for Hin-
di. Results shown below were obtained after test-
ing the simple parser using gold standard test 
data of about 200 sentences. Table 5 shows fig-
ures for labeled accuracy as well as labeled at-
tachment accuracy. 
 
Relation 
Precision 
L LA 
 
Recall 
L LA 
 
nmod 100% 89.3% 70.0% 62.5% 
nmod__adj 100% 92.7% 85.2% 79.0% 
nmod__dem 100% 100% 100% 100% 
nmod__qf 97.0% 92.4% 80.0% 76.2% 
pof 84.5% 82.1% 94.5% 92.0% 
ccof 91.8% 80.0% 70.9% 62.0% 
jjmod__intf 100% 100% 100% 100% 
Total 96.2% 90.4% 82.6% 77.7% 
Table 5. Figures for intra-chunk annotation. 
5 Conclusion 
We introduced the notion of a simple parser for 
Indian languages which follows a grammar dri-
ven methodology. We compared its performance 
against previous similar attempts and reported its 
efficiency. We showed how by using simple yet 
robust rules one can achieve high performance in 
the identification of various levels of dependency 
relations. 
The immediate tasks for the near future would 
be to identify relative clauses in order to reduce 
labeled attachment errors and hence to come up 
with rules for better identification of clauses. We 
also intend to thoroughly test our rules for Indian 
languages that are similar in nature and hence 
evaluate the efficiency of the simple parser. 
Acknowledgements  
We sincerely thank Samar Husain, for his impor-
tant role in providing us with valuable linguistic 
inputs and ideas. The treebank (Hyderabad de-
pendency treebank, version 0.05) used, was pre-
pared at LTRC, IIIT-Hyderabad. 
References 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti 
Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 
2008. Dependency annotation scheme for Indian 
languages. In Proceedings of IJCNLP-2008. 
Akshar Bharati, Vineet Chaitanya and Rajeev Sangal. 
1995. Natural Language Processing: A Pani-
nian Perspective, Prentice-Hall of India, New 
Delhi, pp. 65-106. 
Akshar Bharati, Samar Husain, Dipti Misra Sharma, 
and Rajeev Sangal. 2008. A Two-Stage Constraint 
Based Dependency Parser for Free Word Order 
Languages. In Proc. of the COLIPS Interna-
tional Conference on Asian Language 
Processing 2008 (IALP). Chiang Mai, Thailand. 
2008. 
Akshar Bharati and Rajeev Sangal. 1993. Parsing Free 
Word Order Languages in the Paninian Frame-
work, ACL93: Proc. of Annual Meeting of As-
sociation for Computational Linguistics. 
Akshar Bharati, Rajeev Sangal and Dipti M. Sharma. 
2005. ShaktiAnalyser: SSF Representation.  
Akshar Bharati, Rajeev Sangal, Dipti Misra Sharma 
and Lakshmi Bai. 2006. AnnCorra: Annotating 
Corpora Guidelines for POS and Chunk Annota-
tion for Indian Languages. Technical Report 
(TR-LTRC-31), Language Technologies Re-
search Centre IIIT, Hyderabad 
http://ltrc.iiit.ac.in/MachineTrans/publications
/technicalReports/tr031/posguidelines.pdf 
Murray B. Emeneau. 1956. India as a linguistic area. 
Linguistics, 32:3-16. 
Murray B. Emeneau. 1980. Language and linguis-
tic area. Essays by Murray B. Emeneau. Se-
lected and introduced by Anwar S. Dil. Stan-
ford University Press. 
Mridul Gupta, Vineet Yadav, Samar Husain and Dipti 
M. Sharma. 2008. A Rule Based Approach for Au-
tomatic Annotation of a Hindi Treebank. In Proc. 
Of the 6th International Conference on Natu-
ral Language Processing (ICON-08), CDAC 
Pune, India. 
R. Hudson. 1984. Word Grammar, Basil Blackwell, 
Oxford, OX4 1JF, England. 
I. Mel'cuk . 1988. Dependency Syntax: Theory and 
Practice, State University, Press of New York. 
Avinesh PVS and Karthik Gali. 2007. Part-of-speech 
tagging and chunking using conditional random 
fields and transformation based learning. In Proc. 
Of IJCAI-07 Workshop on ?Shallow Parsing 
in South Asian Languages?, 2007. 
165
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 543?548, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Serendio: Simple and Practical lexicon based approach to Sentiment
Analysis
Prabu Palanisamy, Vineet Yadav and Harsha Elchuri
Serendio Software Pvt Ltd
Guindy, Chennai 600032, India
{prabu, vineet, harsha}@serendio.com
Abstract
This paper describes the system developed by
the Serendio team for the SemEval-2013 Task
2 competition (Task A). We use a lexicon
based approach for discovering sentiments.
Our lexicon is built from the Serendio tax-
onomy. The Serendio taxonomy consists of
positive, negative, negation, stop words and
phrases. A typical tweet contains word varia-
tions, emoticons, hashtags etc. We use prepro-
cessing steps such as stemming, emoticon de-
tection and normalization, exaggerated word
shortening and hashtag detection. After the
preprocessing, the lexicon-based system clas-
sifies the tweets as positive or negative based
on the contextual sentiment orientation of the
words. Our system yields an F-score of 0.8004
on the test dataset.
1 Introduction
Social media websites like Twitter, Facebook etc.
are a major hub for users to express their opinions
online. On these social media sites, users post com-
ments and opinions on various topics. Hence these
sites become rich sources of information to mine for
opinions and analyze user behavior and provide in-
sights for:
? User behavior
? Product feedback
? User intentions
? Lead generation
Businesses spend an enormous amount of time
and money to understand their customer opinions
about their products and services. Thus Sentiment
Analysis has become a hot research area since 2002.
Sentiment Analysis is used to determine sentiments,
emotions and attitudes of the user. The text used for
analysis can range from big document (e.g. Product
reviews from Amazon, blogs) to small status mes-
sage (e.g. Tweets, Facebook comments). In this pa-
per, we confine to Twitter data i.e classify a tweet to
have a positive, negative or neutral sentiment.
The rest of the paper is organized as follows. In
Section 2, we study relevant previous work on Sen-
timent Analysis on Twitter data. In Section 3, we
describe each processing step of our approach in de-
tail. In Section 4, we experiment with the training
and the lexicon. In Section 5, we report and evaluate
the final result obtained from the test data published
by the SemEval team. In Section 6, we present our
conclusions and outline our future work.
2 Related Work
Sentiment Analysis on raw text is a well known
problem. The Liu (2012) book covers the entire
field of Sentiment Analysis. Sentiment Analysis can
be done using Machine learning or a Lexicon-based
approach. We use our lexicon based approach in our
study. The rest of the paper is confined to Lexicon
based approach
2.1 Lexicon based approach
The lexicon based approach is based on the assump-
tion that the contextual sentiment orientation is the
543
sum of the sentiment orientation of each word or
phrase. Turney (2002) identifies sentiments based
on the semantic orientation of reviews. (Taboada et
al., 2011; Melville et al, 2011; Ding et al, 2008)
use lexicon based approach to extract sentiments.
Sentiment Analysis on microblogs is more chal-
lenging compared to longer discourses like reviews.
Major challenges for microblog sentiment analysis
are short length status message, informal words,
word shortening, spelling variation and emoticons.
Sentiment Analysis on Twitter data have been re-
searched by (Bifet and Frank, 2010; Berming-
ham and Smeaton, 2010; Pak and Paroubek, 2010).
We use our lexicon based approach to extract sen-
timents. The open lexicon such as Sentiword-
net (Esuli and Sebastiani, 2006; Baccianella et
al., 2010), Q-wordnet (Agerri and Garc??a-Serrano,
2010), WordNet-Affect (Strapparava and Valitutti,
2004) are developed for supporting Sentiment Anal-
ysis. Studies have been made on preprocessing
tweets. Han and Baldwin (2011) used a classi-
fier to detect word variation and match the related
word. Kaufmann and Kalita (2010) gives the full
preprocessing approach to convert a tweet to normal
text. Sentiment Analysis on Twitter data is not con-
fined to raw text. Analyzing Emoticons have been
an interesting study. Go et al (2009) used emoti-
cons to classify the tweets as positive or negative
and train standard classifiers such as Naive Bayes,
Maximum Entropy, and Support Vector Machines.
Hashtag may have some sentiment in it. Davidov
et al (2010) used 50 hashtags and 15 emoticons as
sentiment labels for classification to allow diverse
sentiment types for the tweet. Negation and inten-
sifier play an important role in Sentiment Analysis.
Negation word can reverse the polarity, where as in-
tensifier increases sentiment strength. Taboada et
al. (2011) studied role of the intensifier and negation
in the lexicon based Sentiment Analysis. Wiegand
et al (2010) survey the role of negation in Sentiment
Analysis.
3 Serendio Approach
Serendio sentiment engine extracts and analyzes
sentiments for a given product and feature set.
Serendio sentiment engine currently works for eight
different domains such as banking, tablets, smart-
phones, televisions, apparel, gaming, automobiles
and e-readers. In this section, we will introduce
Serendio?s Sentiment engine and the enhancements
that were made to handle the SemEval Task 2, Task
A - Contextual Polarity Disambiguation (Wilson et
al., 2013). The main steps of our approach are ex-
plained in detail in the subsections.
3.1 Creation of lexicon
The lexicon can be created either manually
(Taboada et al, 2011; Tong et al, 2001) or expand-
ing automatically from a seed of words (Kanayama
et al, 2006; Kaji and Kitsuregawa, 2007; Turney,
2002; Turney and Littman, 2003). In our study, the
lexicon is manually created. It is a one time process.
Two types of lexicons are created.
Common lexicon: This contains data that would
have the same semantic meaning or sense across dif-
ferent domains and categories.
? Common or default sentiment words. Posi-
tive and Negative sentiment words that have the
same sentiment value or sense across different
domains. For e.g. sentiment word ?good? al-
ways represents a positive sentiment and it is
independent of any category. Positive or Nega-
tive sentiment words have a sentiment score of
+1 or -1 to indicate the respective polarity.
? Negation Words. Negation words are the
words which reverse the polarity of sentiment.
For example, ?The battery life is not good? has
negative sentiment
? Blind Negation Words. In the sentence, ?The
T.V needs a better remote?, ?needs? is a blind
negation word. Blind negation words operate
at a sentence level and points out the absence
or presence of some sense that is not desired in
a product feature.
? Split words. Split words are the words used
for splitting sentences into clauses. The split
words list consists of conjunctions and punc-
tuation marks. For example the complex sen-
tence, ?Camera is good but the battery is bad?
is split into two clauses ?Camera is good? and
?Battery is bad?.
544
Category specific lexicon: Category specific lex-
icon contains the (1) Product Catalog which iden-
tifies all the products that we are interested in. (2)
Feature Catalog which is a list of attributes that the
product has. This enables the Serendio engine to do
analysis at the feature level. (3) Sentiment words
(positive and negative) that are specific to the cate-
gory. For example, for a category such as Televi-
sions, a product would be Samsung TV. The feature
would be LCD screen and the word ?glare? would
be the category specific negative sentiment word.
The semeval task 2 contains Twitter data that can-
not be pinned to any specific category. So for this
task, only the common lexicon was used.
3.2 Preprocessing
A typical tweet contains word variations, emoticons,
hashtags etc. The objective of the preprocessing step
is to normalize the text into an appropriate form to
extract the sentiments. Below are the preprocessing
steps used.
? POS Tagging. POS Tagger gives part of
speech tag associated with words. POS tagging
is done using NLTK (Bird, 2006).
? Stemming. Stemmer gives the stem word.
Serendio lexicon contains stem words only. So
non stem words are stemmed and replaced with
stem words. For example, words like ?loved?,
?loves?, ?loving?, ?love? are replaced with ?lov?.
This would aid the engine to do the word match
from the text to the lexicon. Stemming is done
using NLTK
? Exaggerated word shortening. Words which
have same letter more than two times and not
present in the lexicon are reduced to the word
with the repeating letter occurring just once
(Kouloumpis et al, 2011). For example, the
exaggerated word ?NOOOOOO? is reduced to
?NO?.
? Emoticon detection. Emoticon has some sen-
timent associated with it. Twitter NLP (Ritter
et. al , 2011; Ritter et. al , 2012) is used to
extract emoticons along with the sentiments in
the Twitter data.
? Hashtag detection. The hashtag is a topic or
a keyword that is marked with a tweet. Hash-
tag is a phrase starting with # with no space
between them. Hashtags are identified and sen-
timents are extracted from them.
3.3 Sentiment calculation
Sentiment calculation is the aggregation of the sum
of the sentiment bearing entities of the tweet. Enti-
ties can be text, emoticons and hashtags. The sen-
timent calculation algorithm is shown in Algorithm
1. The sentiment calculation is based on a set of
heuristics built on the sentiment orientation of the
words. Blind negation words are extracted from the
sentence. The presence of the blind negation words
indicate negative sentiment. If the sentence contains
a blind negation word then other steps are skipped
and sentiment is blindly assigned as negative. Next,
sentiment words are extracted. The sentiment po-
larity of the word can be changed due to negation
words that occur in proximity (2 word distance).
If a sentiment word is not present, then the senti-
ment negation word becomes additive to the neg-
ative sentiment list. The sentence ?I can not deal
it? has the negation word ?not? and it does not con-
tain a sentiment word. So the negation word just
gets added to the negative sentiment word. Senti-
ments from emoticons are extracted with the help
of Twitter NLP. Sentiment words within the hash-
tag are extracted by python regex functions. For ex-
ample, from the hashtag ?#ihateu?, the word ?hate?
is extracted as a sentiment word. The sentiment of
the tweet is aggregated as the sum of the sentiments
from all the entities.
4 Experimental Data
The training data consist of real time tweets. 9451
subjective expressions are marked from all the
tweets and are labeled as positive or negative or neu-
tral. The average number of words of the marked
subjective expression is around 2 to 3 words. The
common dictionary that is constructed is shown in
Table 2. The Serendio sentiment engine is run on
the training data set. We identify the correct senti-
ment of the the phrases which are misclassified as
neutral, we include the phrases in our lexicon with
their appropriate sentiments.
545
Algorithm 1: Sentiment Calculation
Data: Preprocessed Twitter data
Result: Output: Positive, Negative, Neutral
Find the list of sentiment words SentiList, its
position in the sentence;
Find the list of sentiment negation words
SentiNegat, its position in the sentence;
Find the list of blind negation words
BlindNegat, its position in the sentence;
if BlindNegat then
return negativity;
else
if SentiList and SentiNegat then
foreach word in the SentiList do
if word is atmost the distance of 2
from SentiNegat then
Revert the polarity of the word;
end
end
else
if SentiNegat then
Add the SentiNegat to the
negative SentiList;
end
end
end
SentiSum=0;
foreach word in the SentiList do
SentiSum=SentiSum+sentiment of
word;
end
if Hashtag is present then
Find all the sentiment words in hash tag
using regex matching and add them to
SentiList
end
if Emoticon is present then
Find sentiment of the emoticon and add
emoticon,it?s sentiment to SentiList
end
SentiType=?neutral?;
if SentiSum > 0 then
SentiType=?positive?;
end
if SentiSum < 0 then
SentiType=?negative?;
end
return SentiType;
Table 1: Training Data
Sentiment type Expression count
Positive 5865
Negative 3120
Neutral 466
Table 2: Lexicon Details
Data type Count
Blind Negation word 7
Negation 13
Positive sentiment word 1260
Negative sentiment word 1703
Split word 16
5 Result and Discussion
Our sentiment engine performed reasonably well.
Please see Table 3 for Precision and Recall measure-
ments. The recall rates are lower because of our lexi-
cons lack of coverage of all the sentiment words. In-
formal language of tweets posed another challenge
for identifying negative sentiments. For example,
swear words such as ?sh*t? and ?f**k? are generally
considered as negative sentiment words. Phrases
such as ?This sh*t is good? and ?F**king awesome?
were identified as negative sentiments when in fact
they were expressing positive sentiments.
Table 3: Results
POSITIVE NEGATIVE
PRECISION 0.9361 0.8884
RECALL 0.7132 0.7912
The Serendio lexicon that we used has sentiment
words with a sentiment attached to it. By integrat-
ing with a lexical source such as Sentiwordnet, we
feel we could get a more nuanced word sense dis-
ambiguation. For example, the word ?good? is con-
sidered to have positive polarity. According to Sen-
tiwordnet 3.0, good as an adjective has 21 different
senses with different sentiments. For example, the
sentiment word ?good? in the phrase ?A good mile
from here? gives an objective sense, not in a positive
sense.
546
6 Conclusion
In this paper we presented our system that we used
for the SemEval-2013 Task 2 for doing Sentiment
Analysis for Twitter data. We got an F-score of
0.8004 on the test data set.
We presented a lexicon based method for Senti-
ment Analysis with Twitter data. We provided prac-
tical approaches to identifying and extracting sen-
timents from emoticons and hashtags. We also pro-
vided a method to convert non-grammatical words to
grammatical words and normalize non-root to root
words to extract sentiments.
A lexicon based approach is a simple, viable and
practical approach to Sentiment Analysis of Twitter
data without a need for training. A Lexicon based
approach is as good as the lexicon it uses. To achieve
better results, word sense disambiguation should be
combined with the existing lexicon approach.
7 Acknowledgments
We would like to thank the organizers of SemEval
2013. We also would like to express our gratitude to
the various reviewers for their encouragement and
positive feedback.
References
Rodrigo Agerri and Ana Garc??a-Serrano. 2010. Q-
WordNet: Extracting polarity from WordNet senses.
Seventh Conference on International Language Re-
sources and Evaluation, Malta.
Stefano Baccianella, Andrea Esuli and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. Proceedings of the 7th conference on Interna-
tional Language Resources and Evaluation (LREC10),
Valletta, Malta, May.
Adam Bermingham and Alan F Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? Proceedings of the 19th ACM international con-
ference on Information and knowledge management
1833?1836, ACM.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. Discovery
Science 1?14, Springer.
Steven Bird. 2006. NLTK: the natural language toolkit.
Proceedings of the COLING/ACL on Interactive pre-
sentation sessions 69?72, Association for Computa-
tional Linguistics.
Kushal Dave, Steve Lawrence and David M Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. Pro-
ceedings of the 12th international conference on World
Wide Web 519?528, ACM.
Dmitry Davidov, Oren Tsur and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. Proceedings of the 23rd International
Conference on Computational Linguistics 241 ?249,
Association for Computational Linguistics.
Xiaowen Ding, Bing Liu and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
Proceedings of the international conference on Web
search and web data mining 231?240, ACM.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. Proceedings of LREC Volume 6, 417?422.
Alec Go, Richa Bhayani and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford 1?12.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a# twitter.
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies Volume 1 , 368?378.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building
lexicon for sentiment analysis from massive collection
of HTML documents. Proceedings of the joint confer-
ence on empirical methods in natural language pro-
cessing and computational natural language learning
(EMNLP-CoNLL) 1075?1083, Association for Com-
putational Linguistics.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing 355?363,Association for Computational Linguis-
tics.
Max Kaufmann and Jugal Kalita. 2010. Syntactic nor-
malization of Twitter messages. International Con-
ference on Natural Language Processing Kharagpur,
India.
Efthymios Kouloumpis, Theresa Wilson and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg. Proceedings of the Fifth Interna-
tional AAAI Conference on Weblogs and Social Media
538?541.
Bing Liu. 2008 Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
1?167,Morgan & Claypool Publishers.
Prem Melville, Wojciech Gryc and Richard D Lawrence.
2011. Sentiment analysis of blogs by combining lex-
ical knowledge with text classification. Proceedings
547
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, 1275?1284,
ACM.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
Proceedings of LREC, Volume 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval Volume 2 number 1-2, 1?135, Now
Publishers Inc.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. Proceedings of the ACL-02
conference on Empirical methods in natural language
processing Volume 10, 79?86, Association for Compu-
tational Linguistics.
Ellen Riloff, Janyce Wiebe and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. Proceedings of the seventh conference
on Natural language learning at HLT-NAACL 2003-
Volume 4 25?32, Association for Computational Lin-
guistics.
Alan Ritter, Sam Clark, Mausam and Oren Etzioni. 2011.
Named Entity Recognition in Tweets: An Experimen-
tal Study. EMNLP.
Alan Ritter, Mausam, Oren Etzioni, Sam Clark. 2012.
Open Domain Event Extraction from Twitter. KDD.
Carlo Strapparava and Alessandro Valitutti. 2004.
WordNet-Affect: an affective extension of WordNet.
Proceedings of LREC 1083?1086.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational linguistics,
volume 37, number2, 267?307, MIT Press.
Richard M Tong 2001. An operational system for detect-
ing and tracking opinions in on-line discussions. In
Working Notes of the ACM SIGIR 2001 Workshop on
Operational Text Classification 1?6,New York, NY.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems 21(4):315?346.
Peter Turney. 2002. Thumbs up or thumbs down?: se-
mantic orientation applied to unsupervised classifica-
tion of reviews. Proceedings of the 40th annual meet-
ing on association for computational linguistics, 417?
424, Association for Computational Linguistics.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, Andre?s Montoyo. 2010. A survey
on the role of negation in sentiment analysis. Pro-
ceedings of the workshop on negation and speculation
in natural language processing 60?68, Association for
Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal and Veselin Stoyonov. 2013.
SemEval-2013 Task 2: Sentiment Analysis in Twitter.
Proceedings of the 7th International Workshop on Se-
mantic Evaluation Association for Computation Lin-
guistics.
548
