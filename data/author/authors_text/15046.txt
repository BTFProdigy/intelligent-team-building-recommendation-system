Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 27?33,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A New Yardstick and Tool for Personalized Vocabulary Building
Thomas K Landauer Kirill Kireyev
Pearson Education,
 Knowledge Technologies
Charles Panaccione
{tom.landauer,kirill.kireyev,charles.panaccione}@pearson.com
Abstract 
The goal of this research is to increase the 
value of each individual student's vocabulary 
by finding words that the student doesn?t 
know, needs to, and is ready to learn. To help 
identify such words, a better model of how 
well any given word is expected to be known 
was created. This is accomplished by using a 
semantic language model, LSA, to track how 
every word changes with the addition of more 
and more text from an appropriate corpus. We 
define the ?maturity? of a word as the degree 
to which it has become similar to that after 
training on the entire corpus. 
An individual student?s average vocabu-
lary level can then be placed on the word-
maturity scale by an adaptive test. Finally, the 
words that the student did or did not know on 
the test can be used to predict what other 
words the same student knows by using mul-
tiple maturity models trained on random sam-
ples of typical educational readings. This 
detailed information can be used to generate 
highly customized vocabulary teaching and 
testing exercises, such as Cloze tests.
1 Introduction
1.1 Why ?Vocabulary First?
There are many arguments for the importance 
of more effective teaching of vocabulary. Here are 
some examples: 
(1) Baker, Simmons, & Kame'enui (1997) 
found that children who enter school with limited 
vocabulary knowledge grow much more discrepant 
over time from their peers who have rich vocabu-
lary knowledge.
(2.) Anderson & Freebody (1981) found that 
the number of words in student?s meaning vocabu-
laries was the best predictor of how well they 
comprehend text. 
(3) An unpublished 1966 study of the correla-
tion between entering scores of Stanford Students 
on the SAT found the vocabulary component to be 
the best predictor of grades in every subject, in-
cluding science.    
(4) The number of words students learn varies 
greatly, from 0.2 to 8 words per day and from 50 to 
over 3,000 per year. (Anderson & Freebody,1981)
(5) Printed materials in grades 3 to 9 on average 
contain almost 90,000, distinct word families and 
nearly 500,000 word forms (including proper 
names.) (Nagy & Anderson, 1984). 
(6) Nagy and Anderson (1984) found that on 
average not knowing more than one word in a sen-
tence prevented its tested understanding, and that 
the probability of learning the meaning of a new 
word by one encounter on average was less than 
one in ten.
(7) John B. Carroll?s (1993) meta-analysis of 
factor analyses of measured cognitive ability found 
the best predictor to be tests of vocabulary.
(8) Hart and Risley?s large randomized obser-
vational study of the language used in households 
with young children found that the number of 
words spoken within hearing of a child was associ-
ated with a three-fold difference in vocabulary by 
school entry.
1.2 The Challenge
Several published sources and inspection of the 
number of words taught in recent literacy text-
books and online tools suggest that less than 400 
words per year are directly tutored in American 
schools. Thus, the vast majority of vocabulary 
must be acquired from language exposure, espe-
cially from print because the oral vocabulary of 
daily living is usually estimated to be about 20,000 
27
words, of which most are known by early school 
years. But it would obviously be of great value to 
find a way to make the explicit teaching of vocabu-
lary more effective, and to make it multiply the 
effects of reading. These are the goals of the new 
methodologies reported here.
It is also clear that words are not learned in iso-
lation: learning the meaning of a new word re-
quires prior knowledge of many other words, and 
by most estimates it takes a (widely variable) aver-
age of ten encounters in different and separated 
contexts. (This, by the way, is what is required to 
match human adult competence in the computa-
tional language model used here. Given a text cor-
pus highly similar to that experienced by a 
language learner, the model learns at very close to 
the same rate as an average child, and it learns new 
words as much as four times faster the more old 
words it knows (Landauer & Dumais, 1997).)
An important aside here concerns a widely cir-
culated inference from the Nagy and Anderson
(1984) result that teaching words by presenting 
them in context doesn?t produce enough vocabu-
lary growth to be the answer. The problem is that 
the experiments actually show only that the in-
serted target word itself is usually not learned well 
enough to pass a test. But in the simulations, words 
are learned a little at a time; exposure to a sentence 
increases the knowledge of many other words, both 
ones in the sentence and not. Every encounter with 
any word in context percolates meaning through 
the whole current and future vocabulary. Indeed, in 
the simulator, indirect learning is three to five 
times as much as direct, and is what accounts for 
its ability to match human vocabulary growth and 
passage similarity. Put differently, the helpful thing 
that happens on encountering an unknown word is 
not guessing its meaning but its contribution to 
underlying understanding of language. 
However, a vicious negative feedback loop 
lurks in this process. Learning from reading re-
quires vocabulary knowledge. So the vocabulary-
rich get richer and the vocabulary-poor get rela-
tively poorer. Fortunately, however, in absolute 
terms there is a positive feedback loop: the more 
words you know, the faster you can learn new 
ones, generating exponential positive growth. Thus 
the problem and solution may boil down to in-
creasing the growth parameter for a given student 
enough to make natural reading do its magic better. 
Nonetheless, importantly, it is patently obvious 
that it matters greatly what words are taught how, 
when and to which students. 
The hypothesis, then, is that a set of tools that 
could determine what particular words an individ-
ual student knows and doesn?t, and which ones 
learned (and sentences understood) would most 
help other words to be learned by that student 
might have a large multiplying effect. It is such a 
toolbox that we are endeavoring to create by using 
a computational language model with demon-
strated ability to simulate human vocabulary 
growth to a reasonably close approximation. The 
principal foci are better selection and ?personaliza-
tion? of what is taught and teaching more quickly 
and with more permanence by application of opti-
mal spacing of tests and practice?into which we 
will not go here. 
1.3 Measuring vocabulary knowledge 
Currently there are three main methods for 
measuring learner vocabulary, all of which are in-
adequate for the goal. They are:
1. Corpus Frequency. Collect a large sample 
of words used in the domain of interest, for exam-
ple a collection of textbooks and readers used in 
classrooms, text from popular newspapers, a large 
dictionary or the Internet. Rank the words by fre-
quency of occurrence. Test students on a random 
subset of, say, the 1,000, 2,000 and 5,000 most 
frequent words, compute the proportion known at 
each ?level? and interpolate and extrapolate. This 
is a reasonable method, because frequently en-
countered words are the ones most frequently 
needed to be understood. 
2. Educational Materials. Sample vocabulary 
lessons and readings over classrooms at different 
school grades.
3. Expert Judgments. Obtain informed expert 
opinions about what words are important to know 
by what age for what purposes.
Some estimates combine two or more of these 
approaches, and they vary in psychometric sophis-
tication. For example, one of the most sophisti-
cated, the Lexile Framework, uses Rasch scaling
(Rasch, 1980) of a large sample of student vocabu-
lary test scores (probability right on a test, holding 
student ability constant) to create a difficulty 
measure for sentences and then infers the difficulty 
of words, in essence, from the average difficulty of 
the sentences in which they appear.
28
The problem addressed in the present project 
goal is that all of these methods measure only the 
proportion of tested words known at one or more 
frequency ranges, in chosen school grades or for
particular subsets of vocabulary (e.g. ?academic? 
words), and for a very small subset?those tested -
some of the words that the majority of a class 
knows.  What they don?t measure is exactly which 
words in the whole corpus a given student knows 
and to what extent, or which words would be most 
important for that student to learn.
A lovely analog of the problem comes from 
Ernst Rothkopf?s (1970) metaphor that everyone 
passes through highly different ?word swarms? 
each day on their way to their (still highly differen-
tiated) adult literacy. 
2 A new metric: Word Maturity
The new metric first applies Latent Semantic 
Analysis (LSA) to model how representation of
individual words changes and grows toward their 
adult meaning as more and more language is en-
countered. Once the simulation has been created, 
an adaptive testing method can be applied to place 
individual words on separate growth curves - char-
acteristic functions in psychometric terminology. 
Finally, correlations between growth curves at 
given levels can be used to estimate the achieved 
growth of other words.
2.1 How it works in more detail: LSA.
A short review of how LSA works will be use-
ful here because it is often misunderstood and a 
correct interpretation is important in what follows. 
LSA models how words combine into meaningful 
passages, the aspect of verbal meaning we take to 
be most critical to the role of words in literacy. It 
does this by assuming that the ?meaning? (please 
bear with the nickname) of a meaningful passage is 
the sum of the meanings of its words:
Meaning of passage = 
{meaning of first wd} + 
{meaning of second word} + ?  + 
{meaning of last word}
A very large and representative corpus of the 
language to be modeled is first collected and repre-
sented as a term-by-document matrix. A powerful 
matrix algebra method called Singular Value De-
composition is then used to make every paragraph 
in the corpus conform to the above objective func-
tion?word representations sum to passage repre-
sentations - up to a best least-squares 
approximation. A dimensionality-reduction step is 
performed, resulting in each word and passage 
meanings represented as a (typically) 300 element 
real number vector. Note that the property of a vec-
tor standing for a word form in this representation 
is the effect that it has on the vector standing for 
the passage. (In particular, it is only indirectly a 
reflection of how similar two words are to each 
other or how frequently they have occurred in the 
same passages.) In the result, the vector for a word 
is the average of the vectors for all the passages in 
which it occurs, and the vector for a passage is, of 
course, the average all of its words.
In many previous applications to education, in-
cluding automatic scoring of essays, the model?s
similarity to human judgments (e.g. by mutual in-
formation measures) has been found to be 80 to 
90% as high as that between two expert humans, 
and, as mentioned earlier, the rate at which it 
learns the meaning of words as assessed by various 
standardized and textbook-based tests has been 
found to closely match that of students. For more 
details, evaluations and previous educational appli-
cations, see (Landauer et al, 2007).
2.2 How it works in more detail: Word Ma-
turity.
Taking LSA to be a sufficiently good approxi-
mation of human learning of the meanings con-
veyed by printed word forms, we can use it to track 
their gradual acquisition as a function of increasing 
exposure to text representative in size and content 
of that which students at successive grade levels
read. 
Thus, to model the growth of meaning of indi-
vidual words, a series of sequentially accumulated 
LSA ?semantic spaces? (the collection of vectors 
for all of the words and passages) are created. Cu-
mulative portions of the corpus thus emulate the 
growing total amount of text that has been read by 
a student. At each step, a new LSA semantic space 
is created from a cumulatively larger subset of the 
full adult corpus. 
Several different ways of choosing the succes-
sive sets of passages to be added to the training set 
have been tried, ranging from ones based on read-
ability metrics (such as Lexiles or DRPs) to en-
29
tirely randomly selected subsets. Here, the steps 
are based on Lexiles to emulate their order of en-
counter in typical school reading. 
This process results in a separate LSA model of 
word meanings corresponding to each stage of lan-
guage learning. To determine how well a word or 
passage is known at a given stage of learning?a 
given number or proportion of passages from the 
corpus?its vector in the LSA model correspond-
ing to a particular stage is compared with the vec-
tor of the full adult model (one that has been 
trained on a corpus corresponding to a typical 
adult?s amount of language exposure). This is done 
using a linear transformation technique known as 
Procrustes Alignment to align the two spaces?
those after a given step to those based on the full 
corpus, which we call its ?adult? meaning.
Word maturity is defined as the similarity of a 
word?s vector at a given stage of training and that 
at its adult stage as measured by cosine. It is scaled 
as values ranging between 0 (least mature) and 1 
(most mature).
Figure 1 shows growth curves for an illustrative 
set of words. In this example, 17 successive cumu-
lative steps were created, each containing ~5000 
additional passages. 
Word Meaning Maturity
0.0
0.2
0.4
0.6
0.8
1.0
1 3 5 7 9 11 13 15 17
Model Level
Si
m
ila
rit
y
dog
electoral
primate
productivity
turkey
Figure 1. An illustration of meaning maturity growth of sev-
eral words as a function of language exposure.
Some words (e.g. ?dog?) are almost at their 
adult meaning very early. Others hardly get started 
until later. Some grow quickly, some slowly. Some 
grow smoothly, some in spurts. Some, like ?tur-
key,? grow rapidly, plateau, then resume growing 
again, presumably due to multiple senses 
(?Thanksgiving bird? vs. ?country?) learned at dif-
ferent periods (in LSA, multiple ?senses? are com-
bined in a word representation approximately in 
proportion to their frequency.) 
The maturity metric has several conceptual ad-
vantages over existing measures of the status of 
a word?s meaning, and in particular should be kept 
conceptually distinct from the ambiguous and often 
poorly defined term ?difficulty? and from whether 
or not students in general or at some developmen-
tal stage can properly use, define or understand its 
meaning. It is a mathematical property of a word 
that may or may not be related to what particular 
people can do with it. 
What it does is provide a detailed view of the 
course of development of a word?s changing repre-
sentation?its ?meaning?, reciprocally defined as 
its effect on the ?meaning? of passages in which it 
occurs,?as a function of the amount and nature of 
the attestedly meaningful passages in which it has 
been encountered. Its relation to ?difficulty? as 
commonly used would depend, among other 
things, on whether a human could use it for some 
purpose at some stage of development of the word. 
Thus, its relation to a student?s use of a word re-
quires a second step of aligning the student?s word 
knowledge with the metric scaling. This is analo-
gous to describing a runner?s ?performance? by 
aligning it with well-defined metrics for time and 
distance.
It is nevertheless worth noting that the word 
maturity metric is not based directly on corpus fre-
quency as some other measures of word status are 
(although its average level over all maturities is 
moderately highly correlated with total corpus fre-
quency as it should be) or on other heuristics, such 
as grade of first use or expert opinions of suitabil-
ity.
What is especially apparent in the graph above 
is that after a given amount of language exposure, 
analogous to age or school grade, there are large 
differences in the maturity of different words. In 
fact the correlation between frequency of occur-
rence in a particular one of the 17 intermediate cor-
pora and word maturity is only 0.1, measured over 
20,000 random words. According to the model--
and surely common sense--words of the same fre-
quency of encounter (or occurrence in a corpus) 
are far from equally well known. Thus, all methods 
for ?leveling? text and vocabulary instruction 
based on word frequency must hide a great range 
of differences.
To illustrate this in more detail, Table 1, shows 
computed word maturities for a set of words that 
have nearly the same frequency in the full corpus 
30
(column four) when they have been added only 
50?5 times (column two). The differences are so 
large as to suggest the choice of words to teach 
students in a given school grade would profit much 
from being based on something more discrimina-
tive than either average word frequency or word 
frequency as found in the texts being read or in the 
small sample that can be humanly judged. Even 
better, it would appear, should be to base what is 
taught to a given student on what that student does 
and doesn?t know but needs to locally and would 
most profit from generally.
Word Occurrences 
in intermedi-
ate corpus
(level 5)
Occurrences 
in adult 
corpus
Word 
maturity 
(at level 
5)
marble 54 485 0.21
sunshine 49 508 0.31
drugs 53 532 0.42
carpet 48 539 0.59
twin 48 458 0.61
earn 53 489 0.70
beam 47 452 0.76
Table 1 A sample of words with roughly the same number of 
occurrences in both intermediate (~50) and adult (~500) cor-
pus
The word maturity metric appears to perform 
well when validated by some external methods. 
For example, it reliably discriminates between 
words that were assigned to be taught in different 
school grades by (Biemiller, 2008), based on a 
combination of expert judgments and comprehen-
sion tests (p < 0.03), as shown in Table 2.
grade 2,
known 
by > 80%
grade 2,
known by 
40-80%
grade 6,
known by 
40-80%
grade 6,
known 
by < 40%
n=1034 n=606 n=1125 n=1411
4.4 6.5 8.8 9.5
Table 2 Average level for each word to reach a 0.5 maturity 
threshold, for words that are known at different levels by stu-
dents of different grades (Biemiller, 2008).
Median word maturity also tracks the differ-
ences (p < 0.01) between essays written by stu-
dents in different grades as shown in Figure 2.
Percent of "adult" words in essay
0%
1%
2%
3%
4%
5%
4 6 8 10 12
Student grade level
Figure 2 Percentage of ?adult? words used in essays written 
by students of different grade levels. ?Adult? words are de-
fined as words that reach a 0.5 word maturity threshold at or 
later than the point where half of the words in the language 
have reached 0.5 threshold.
2.3 Finding words to teach individual stu-
dents
Using the computed word maturity values, a 
sigmoid characteristic curve is generated to ap-
proximate the growth curve of every word in the 
corpus. A model similar to one used in item re-
sponse theory (Rasch, 1980) can be constructed 
from the growth curve due to its similarity in shape 
and function to an IRT characteristic curve; both 
curves represent the ability of a student.  The char-
acteristic curve for the IRT is needed to properly 
administer adaptive testing, which greatly in-
creases the precision and generalizeability of the 
exam. Words to be tested are chosen from the cor-
pus beginning at the average maturity of words at 
the approximate grade level of the student. Thirty 
to fifty word tests are used to home in on the stu-
dent?s average word maturity level. In initial trials, 
a combination of yes/no and Cloze tests are being 
used. Because our model does not treat all words 
of a given frequency as equivalent, this alone sup-
ports a more precise and personalized measure of a 
student?s vocabulary. In plan, the student level will 
be updated by the results of additional tests admin-
istered in school or by Internet delivery.
The final step is to generalize from the assessed 
knowledge of words a particular student (let?s call 
her Alice) is tested on to other words in the corpus. 
This is accomplished by first generating a large 
number of simulated students (and their word ma-
turity curves) using the method described above. 
Each simulated student is trained on one of many ~ 
12 million word corpora, size and content ap-
proximating the lifelong reading of a typical col-
lege student, that have been randomly sampled 
from a representative corpus of more than half a 
31
billion words. Some of these simulated students? 
knowledge of the words being tested will be more 
similar to Alice than others. We can then estimate 
Alice?s knowledge of any other word w in the cor-
pus by averaging the levels of knowledge of w by 
simulated students whose patterns of tested word 
knowledge are most similar hers. The method rests 
on the assumption that there are sufficiently strong 
correlations between the words that a given student 
has learned at a given stage (e.g. resulting from 
Rothkopf?s personal ?swarms?.) While simulations 
are promising, empirical evidence as to the power 
of the approach with non-simulated students is yet 
to be determined. 
3 Applying the method
On the assumption that learning words by their 
effects on passage meanings as LSA does is good, 
initial applications use Cloze items to simultane-
ously test and teach word meanings by presenting 
them in a natural linguistic context. Using the 
simulator, the context words in an item are pre-
dicted to be ones that the individual student already 
knows at a chosen level. The target words, where 
the wider pedagogy permits, are ones that are re-
lated and important to the meaning of the sentence 
or passage, as measured by LSA cosine similarity 
metric, and, ipso facto, the context tends to contex-
tually teach their meaning. They can also be cho-
sen to be those that are computationally estimated 
to be the most important for a student to know in 
order to comprehend assigned or student-chosen 
readings?because their lack has the most effect on 
passage meanings?and/or in the language in gen-
eral. Using a set of natural language processing 
algorithms (such as n-gram models, POS-tagging, 
WordNet relations and LSA) the distracter items 
for each Cloze are chosen in such a way that they 
are appropriate grammatically, but not semanti-
cally, as illustrated in the example below.
In summary, Cloze-test generation involves the 
following steps:
1. Determine the student?s overall knowledge 
level and individual word knowledge predictions 
based on previous interactions.
2. Find important words in a reading that are 
appropriate for a particular student (using metrics 
that include word maturity).
3. For each word, find a sentence in a large 
collection of natural text, such that the rest of the 
sentence semantically implies (is related to) the 
target word and is appropriate for student?s knowl-
edge level.
4.Find distracter words that are (a) level-
appropriate, (b) are sufficiently related and (c) fit 
grammatically, but (d) not semantically, into the 
sentence.
All the living and nonliving things around an ___ 
is its environment.
A. organism   B. oxygen   C. algae
Freshwater habitats can be classified according to 
the characteristic species of fish found in them, 
indicating the strong ecological relationship be-
tween an ___ and its environment.
A. adaptation   B. energy   C. organism
Table 3 Examples of auto-generated Cloze tests for the same 
word (organism) and two students of lower and higher ability, 
respectively.
4 Summary and present status
A method based on computational model-
ing of language, in particular one that makes the 
representation of the meaning of a word its effect 
on the meaning of a passage its objective, LSA, 
has been developed and used to simulate the 
growth of meaning of individual word representa-
tions towards those of literate adults. Based 
thereon, a new metric for word meaning growth 
called ?Word Maturity? is proposed. The measure 
is then applied to adaptively measuring the average 
level of an individual student?s vocabulary, pre-
sumably with greater breadth and precision than 
offered by other methods, especially those based 
on knowledge of words at different corpus fre-
quency. There are many other things the metric 
may support, for example better personalized 
measurement of text comprehensibility.
However, it must be emphasized that the 
method is very new and essentially untried except 
in simulation. And it is worth noting that while the 
proposed method is based on LSA, many or all of 
its functionalities could be obtained with some 
other computational language models, for example 
the Topics model. Comparisons with other meth-
ods will be of interest, and more and more rigorous 
evaluations are needed, as are trials with more 
various applications to assure robustness. 
32
5 References
Richard C. Anderson, Peter Freebody. 1981. Vo-
cabulary Knowledge. In J. T. Guthrie (Ed.), 
Comprehension and teaching: Research reviews
(pp. 77-117). International Reading Association, 
Newark DE.
Scott K. Baker,  Deborah C. Simmons, Edward J. 
Kameenui. 1997. Vocabulary acquisition: Re-
search bases. In Simmons, D. C. & Kameenui, 
E. J. (Eds.), What reading research tells us 
about children with diverse learning needs: 
Bases and basics. Erlbaum, Mahwah, NJ. 
Andrew Biemiller (2008).  Words Worth Teaching.  
Co-lumbus, OH:  SRA/McGraw-Hill.
John B Carroll. 1993. Cognitive Abilities: A survey 
of factor-analytic studies. Cambridge: Cam-
bridge University Press, 1993. 
Betty Hart, Todd R. Risley. 1995. Meaningful dif-
ferences in the everyday experience of young 
American children. Brookes Publishing, 1995.
Melanie R. Kuhn, Steven A. Stahl. 1998. Teaching 
children to learn word meanings from context: 
A synthesis and some questions. Journal of Lit-
eracy Research, 30(1) 119-138.
Thomas K Landauer, Susan Dumais. 1997. A solu-
tion to Plato's problem: The Latent Semantic 
Analysis theory of the Acquisition, Induction, 
and Representation of Knowledge.  Psychologi-
cal Review, 104, pp 211-240.
Thomas K Landauer, Danielle S. McNamara, 
Simon Dennis, and Walter Kintsch. 2007. Hand-
book of Latent Semantic Analysis. Lawrence 
Erlbaum.
Cleborne D. Maddux (1999). Peabody Picture Vo-
cabulary Test III (PPVT-III). Diagnostique, v24 
n1-4, p221-28, 1998-1999
William E. Nagy, Richard C. Anderson. 1984. 
How many words are there in printed school 
English? Reading Research Quarterly, 19, 304-
330.
Ernst Z. Rothkopf, Ronald D. Thurner. 1970. Ef-
fects of written instructional material on the sta-
tistical structure of test essays. Journal of 
Educacational Psychology, 61, 83-89.
George Rasch. (1980). Probabilistic models for 
some intelligence and attainment tests. (Copen-
hagen, Danish Institute for Educational Re-
search), expanded edition (1980) with foreword 
and afterword by B.D. Wright. Chicago: The 
University of Chicago Press.
33
Pasteur's Quadrant, Computational Linguistics, LSA, Education 
 
Thomas K Landauer 
Knowledge Analysis Technologies. 4940 Pearl East Circle, Boulder, CO 80301 
landauer@psych.colorado.edu 
 
Abstract 
 
This paper argues that computational cognitive 
psychology and computational linguistics have 
much to offer the science of language by 
adopting the research strategy that Donald 
Stokes called Pasteur?s quadrant--starting and 
testing success with important real world 
problems--and that education offers an ideal 
venue. Some putative examples from 
applications of Latent Semantic Analysis 
(LSA) are presented, as well as some detail on 
how LSA works, what it is and is not, and 
what it does and doesn?t do. For example, LSA 
is used successfully in automatic essay grading 
with content coverage feedback, computing 
optimal sequences of study materials, and 
partially automating metadata tagging, but is 
insufficient for scoring mathematical and short 
textual answers, for revealing reasons. It is 
explained that LSA is not construable as 
measuring co-occurrence, but rather measure 
the similarity of words in their effect on 
passage meaning, 
   
1     Credits. 
 
The research reported here has been 
supported by NSF, the Army Research Institute, the 
Air Force Office of Scientific Research, the Office of 
Naval Research, and the Institute of Educational 
Sciences. Many people contributed to the research 
including, but by no means limited to Susan Dumais, 
Peter Foltz, George Furnas, Walter Kintsch, Darrell 
Laham, Karen Lochbaum, Bob Rehder, and Lynn 
Streeter,  
 
2     Introduction 
 
In my outsider?s opinion?I?m not a linguist 
and this is my first ACL meeting?this workshop 
marks an important turn in the study of language.  
Here is why I think so. 
Donald Stokes, in Pasteur?s Quadrant 
(1997), argues that the standard view that science 
progresses from pure to applied research to 
engineering implementations is often wrong. This 
doctrine was the brainchild of Vannevar Bush, who 
was Roosevelt?s science advisor during war II. It has, 
of course, since been enshrined in the DoD?s 6.1,2,3 
funding structure, and modeled in the national 
research institutes and large industrial laboratories 
such as Bell Labs, IBM and Microsoft. Stokes shows 
that while this trajectory is sometimes followed, often 
with dramatic success, over the whole course of 
scientific advance it has been the exception rather 
than the rule, and for good reasons. Stokes 
summarized his view of the real relations in a two by 
two table much like the one in the figure, in which I 
have made a few minor additions and modifications. 
 
Pure research Pasteur?s quadrant 
(random walk research) Pragmatic engineering 
 
Table 1. Donald Stokes? (1997) illustration of his 
conception of science, slightly modified. 
 
The upper left quadrant is ?pure? research, 
driven by a desire to understand nature, its problems 
chosen by what natural phenomena are most 
pervasive, mysterious or intuitively interesting. 
Particle physics is its standard bearer. The lower right 
quadrant is empirical engineering, incremental cut 
and try, each improvement based on lessons learned 
from the successes and failures of previous attempts. 
Internal combustion engines are a type case. 
The upper right quadrant, Pasteur?s, is 
research driven by the desire to solve practical 
problems, for Pasteur preventing the spoilage of 
vinegar, beer, wine and milk, and conquering 
diseases in silkworms, sheep, chickens, cattle and 
humans. Such problems inspire and set concrete 
goals for research. To solve them it is often necessary 
to delve into empirical facts and first causes. The 
quadrant also offers an important way to evaluate 
scientific success; because failure proves a lack of 
full understanding. 
Stokes doesn?t name the lower left quadrant, 
but it might be dubbed ?random walk? science. It 
resembles theological scholasticism, where the next 
problem is chosen by flaws in the answer to the last. 
In my field, cognitive psychology, it is exemplified 
by 100 years of experiment, thousands of papers, and 
dozens of quantitative models about how people 
remember lists of words. 
Of course, these activities bleed into one 
another and sometimes evince the Bush progression. 
Even list learning has produced basic principles that 
can be used effectively in education and the treatment 
of dementia. Nonetheless, the argument is that efforts 
in Pasteur?s quadrant, because they avoid the dangers 
of excessive-abstraction, simplification and 
irrelevance, are the most productive, both of 
scientific advance and of practical value. 
I believe that the Pasteur attitude is 
especially important in psychology, because 
identifying problems that are critical for understand 
the human mind is anything but easy. Human minds 
do many unique and currently unexplainable things. 
Their first-cause mechanisms are hidden deeply in 
the intricate connections of billions neurons and 
billions of experiences. Better keys to the secrets of 
the mind are needed than hunches of the kind that 
have motivated list-learning research. To be surer 
that what we study is actually relevant to the real 
topic of interest we need to try to solve problems at 
the level of normal, representative mental functions. 
Although there are other good candidates, such as 
automobile driving and economic decision making, 
education is particularly apt. This is partly because 
cognitive psychology already knows quite a lot about 
learning, but more importantly because education is 
the primary venue in which society intentionally 
focuses on making a cognitive function happen well, 
and where success and failure can tell us what we do 
and do not know, and do so with some guarantee that 
the knowing is important to understanding the target 
phenomena. 
It seems to me that computational linguistics 
is in much the same position. Much traditional 
linguistics has concerned itself with descriptions of 
abstract properties of language whose actual role in 
the quotidian human use of language is not often 
studied, and, therefore, whose promise to explain 
how language is acquired and works for its users is 
sometimes hard to evaluate. Computational 
linguistics itself appears to have been devoted mostly 
to the upper left and lower right quadrants; on one 
hand it has spent much of its effort automating or 
supporting traditional linguistic analyses such as 
parsing, part-of-speech tagging and semantic role 
classification. On the other hand, it has developed 
practical tools, such as dictionaries, ontologies and n-
gram language models for doing practical language 
engineering tasks, such as speech-to-text conversion 
and machine translation. There has been relatively 
little effort to use the successes and failures of 
computer automations to guide, illuminate, or test 
models of how human language works. 
This workshop, represents an important step 
northeast in Stokes? map. Not only is education 
accomplished primarily through the use of language, 
it is also a critical source of advanced abilities to use 
language-- reading, writing, and thinking--and is the 
primary medium by which the fruits of education are 
made useful. Thus trying to improve education is just 
the kind of thing that the Pasteur approach exploits, 
compelling reasons to understand, a laboratory for 
exploration, and strong, broad, relevant tests of 
success. Putting this argument starkly, it is too easy 
to treat language as an isolated abstract system and 
ignore its functional role in human life, and it is too 
easy to treat education as a humanity, where abstract 
philosophical arguments, ethical principles or 
historical precedent guide practice. Attempts to 
enhance the role of language in education through 
computation, which makes exquisitely specific what 
we are doing, should lead to new understanding of 
the nature of language--and vice versa. 
Now for a few words on my own work, and 
some ways in which it has, at least in part, followed 
the Pasteur path, plus a few words on how 
computational linguistics in education might make 
use of some of its outcomes. This will be take the 
form of a review of Latent Semantic Analysis (LSA): 
its origins and history, its computationally simulated 
mental mechanisms, its applications in education, and 
some implications it may have for understanding how 
the mind does language. I?ll briefly describe where 
LSA came from, how it works, what it does and 
doesn?t do, some educational applications in which 
what it does is useful, some things that limit its 
usefulness and beg for better basic science, and some 
nitty-gritty on how and how not to apply it. 
 
3     The History and Nature of LSA 
 
In the early eighties the management of Bell 
Telephone Laboratories, where I was working, asked 
me to form a group to find out why secretaries in the 
legal department were having trouble using UNIX, an 
obvious godsend, and fix them. This led to trying to 
find out why customers sometimes couldn?t find what 
they wanted in the Yellow Pages, why service 
representatives didn?t always give correct charges 
even thought they were plainly stated in well indexed 
manuals, and why the new online databases for parts 
and circuits required so much training and yielded 
only small gains in speed and accuracy, if any. 
 We undertook a series of lab experiments 
whose details are skippable. What we discovered was 
this. In every case the words that people wanted to 
use, to give orders to computers, or to look things up, 
rarely matched the words the computer understood or 
the manuals were indexed by. Roughly, but almost 
always, the data could be summarized as: ask 100 
people by what one word something should be called 
and you will get 30 different answers. The 
distribution is such that it takes five words to cover 
half the answers. We called this the problem of 
?verbal disagreement? (Furnas et al, 1987). 
Our first solution was brute force; find all 
the words people would use for what we called an 
?information object? and index by all of them, which 
we called ?unlimited aliasing? (what do you think the 
chances are that anyone else would have named them 
that way?). Later, largely led by George Furnas 
(1985), we invented some ways to semi-automate 
that process by what he called ?adaptive indexing?, 
having the computer ask people if the words they had 
used unsuccessfully should be added as pointers to 
things they eventually found. Of course, we also 
worried about the problem of ambiguity, now known 
as ?the Google problem?, that almost every word has 
several very different meanings that will lead you 
astray. At least under some circumstances that was 
fixable by giving more context in the response, one 
version of which is Furnas? ?fisheye view?, to guide 
navigation. (Adaptive indexing also greatly reduces 
the ambiguity problem because the pointers are one 
way--from what people said to the one thing they 
actually told the systems they meant.) 
So what had we done here? We?d used the 
practical problem to lead to empirically exploration 
of how people actually used words in daily life 
(although computers were not as much of daily life 
then as now, and some of their persisting problems 
may be due to our failure to get our solutions widely 
adopted. Here I am, still trying.) The surprising 
extent and universality of verbal disagreement could 
be viewed as a baby step in language science, at least 
as we construed language science. 
But just pinning down the nature of the 
problem in the statistics of actual pragmatic word 
usage (we called the new field ?statistical semantics?, 
which didn?t catch on), was only a start. Clearly the 
problems that computers were having understanding 
what people meant is special to computers. People 
understand each other much better. (People also have 
trouble, although less, with queries of one or two 
isolated words, but they are very good at using 
baseline statistics of what people mean by a word 
(which is, of course, Google?s stock in trade, using an 
indirect version of adaptive indexing), and they 
appear to use context when available in a much more 
efficient manner (although this still needs research in 
the style of statistical semantics.) 
What was needed was a way to mimic what 
people do so well--understand all the meanings of all 
the words they know, and know just how much and 
how any word is related to any other. It is perfectly 
obvious that people learn the meanings of the words 
in their language, only slightly less so that they must 
do so primarily from experiencing the words in 
context and from how they are used in combination 
to produce emergent meanings. With these facts and 
clues in mind, the next step was to find 
computational techniques to do something similar, 
and see if it improved a computer?s understanding. 
(An apology is in order for idiosyncratic use of the 
words ?meaning?, ?understanding?, and ?semantics?. 
They are used here in special senses that differ from 
myriad usages in linguistics and philosophy, and may 
offend some readers. Because detailed definitions and 
circumlocutions would be burdensome and of little 
value, let us leave it to context.) 
The best method we hit upon was what is 
now called Latent Semantic Analysis, LSA (or, in 
information retrieval, Latent Semantic Indexing, 
LSI.) Because there have been some 
misinterpretations in the literature it may be useful to 
give a conceptual explanation of how LSA works. It 
assumes that the meaning of a passage (in practice 
typically a paragraph) can be approximated by the 
sum of the meanings of its words. That makes a large 
print corpus a huge system of simultaneous linear 
equations. To solve such systems we used the matrix 
algebraic technique of Singular Value Decomposition 
(SVD), the general method behind factor analysis and 
principal components analysis. Applied to a corpus of 
text, the result is a vector standing for every word in 
the corpus, with any passage represented by the 
vector sum of its word vectors. (At first we could 
only do that with rather small corpora, but with 
improved algorithms and hardware, size is no longer 
a barrier.) 
The first applications of LSA were to 
information retrieval, which we conceived of as a 
problem in the psychology of meaning, how to 
measure the similarity of meaning to a human of a 
query and a document given pervasive verbal 
disagreement. The method was to compute the 
similarity of corresponding vectors, typically by their 
cosine (of their angle in a very high dimensional 
?semantic space?.) The result was that, everything 
else equal (e.g. tokenizing, term-weighting, etc.), LSI 
gave about 20% better precision-for-recall results, 
largely because it could rightly judge meaning 
similarity despite differences in literal word use. It 
also does any language, and cross language retrieval 
handily because its numerical vectors don?t care 
whether the ?words? are Chinese characters or Arabic 
script. If the training corpus contains a moderate 
number of known good translations, and is processed 
correctly, it does pretty well with no other help. 
Along the way we discovered that choosing 
the right number of dimensions?the number of 
(independent) elements composing each vector--was 
critical, three hundred to five hundred being strongly 
optimal. One way of describing the value of reducing 
the number of dimensions well below the number of 
word types or passages is that it forces the system to 
induce relations between every word and every other 
rather than keeping track of the full pattern of 
empirical occurrences of each, as standard vector 
retrieval methods do. 
Because we like to think we are trying to 
model human minds as well as solve practical 
problems, we have also tested LSA on a variety of 
human tasks. For word meaning an early test was to 
give it a standardized multiple-choice vocabulary 
tests (it chooses the word with the most similar 
meaning by computing which has the highest cosine). 
Trained on text of similar volume and context to what 
an American high school senior has read, it does well 
on the Test of English as a Foreign Language 
(TOEFL), equaling successful non-native applicants 
to U.S. Colleges. It also mimics the astounding ten 
words per day vocabulary growth of middle school 
children as measured by multiple choice tests. To 
evaluate its representations of passage meaning, 
perhaps the most interesting and quantitative tests 
have been through its use in scoring the conceptual 
content of expository essays. In actual essay scoring 
systems we use a suite of analytic tools that includes 
other things. However, for the present purpose we 
need to consider how well LSA does when used 
alone.  In doing this, LSA is used to predict the score 
a human grader would give a new essay on the basis 
of its similarity to other essays on the same topic that 
have previously been humanly scored.  The LSA-
based score predicts very nearly as well as does that 
of a second independent human reader. Several other 
evidences of passage-passage success will be 
described later. 
The astute reader will be puzzled by how 
this could happen, given the very strong 
simplification of LSA?s additivity assumption, by 
which word order within passages is completely 
ignored. We will return to this matter, and to more on 
essay grading later. 
Before going on, a few more common 
misinterpretations of LSA need dealing with. First, 
LSA is not a measure of co-occurrence, at least as co-
occurrence is usually conceived. For LSA a passage 
meaning is the combination of its word meanings. 
This does not imply that the words in a passage have 
the same meaning; indeed that would not be very 
useful. Empirically, over a typical large corpus, the 
correlation between the cosine between a random pair 
of words and the number of passages in which they 
both occurred is +.35, while the correlation with how 
often they occur separately, which by the usual 
interpretation should make them dissimilar, is +.30. 
By the same token--unlike n-gram language models--
LSA estimates the probability that one word will 
follow another only indirectly and very weakly. 
(Although, surprisingly, LSA similarities have 
recently been shown to account for much of what 
goes on in recalling word lists in order, but not by 
conditional probability effects (Howard and Kahana, 
2001)). More correct interpretations are that LSA 
reflects the degree to which two words could 
substitute for one another in similar contexts, that 
they tend to appear in similar (but not necessarily 
identical) contexts, and, most precisely, that they 
have the same effects on passage meanings. 
Now what about the fact that LSA ignores 
word order and thus all syntactically conveyed 
grammatical effects on sentential meaning? First, it 
needs emphasis that LSA is very good at measuring 
the similarity of two words or two passages, 
sometimes good on sentence to sentence similarity 
and sometimes not, and least good on word to 
sentence, or word-to-passage meanings. A good and 
bad feature of its word-to-word function is that it 
merges all contextual effects (different senses) of a 
word into a frequency-weighted average.  LSA, as a 
theory of psychological meaning, proposes that a 
word is represented as a single central meaning that is 
modified by context (see Kintsch (2002) for how this 
could play out in predication and metaphor). The 
reason it does well on passage-to-passage is that 
passages are redundant and complex, and that local 
syntactic effects tend to average out. (This is true for 
humans too?e.g. they ignore misplaced nots) LSA 
should be used with all of this in mind. 
However, still, you might say, LSA?s lack of 
understanding of prediction, attachment, binding, and 
constituent structure, thus of representation of logical 
propositions--all traditional foci of linguistic 
semantics and computational linguistics-- must surely 
weaken if not cripple it. Weaken surely, but by how 
much? Here is one ?ballpark? estimate. A typical 
college educated adult understands around 100,000 
word forms, an average sentence contains around 20 
tokens. There are thus 100,00020 possible 
combinations of words in a sentence, therefore a 
maximum of log2 100,00020 = 332 bits of information 
in word choice alone. There are 20! =2.4 x 1018 
possible orders of 20 words for and additional 
maximum of 61 bits from syntactic effects. Of the 
possible information in a sentence, then, the part that 
bag-of-words LSA can use is 332/(61+ 332) = 84%.  
A substantial amount of human meaning is 
missing from LSA, but a much larger component is 
apparently captured. It turns out that, judiciously 
applied, this component can be quite useful. 
Moreover, applying it can help pin down the roles of 
what?s missing and not and thus advance our 
understanding of the nature language as used. Some 
successful and less so applications to education are 
described next, along with some implications, as well 
as some radical conjectures. 
 
4      Applications of LSA in Education 
   
First, a few more words on the use of LSA 
in information retrieval (IR) (and relevant to some 
educational applications described later) and essay 
scoring. What LSA captures in IR is the degree to 
which two documents are about the same thing, 
independent of what equivalent wording may be 
used. Thus it is useful for finding documents that talk 
about something, even though it misses details-- 
sometimes important ones--about what was said 
about the matter. What kind of computation might 
achieve a representation of the rest? 
To achieve a high degree of validity in 
representing word meaning, LSA uses only 
information on how words are used, it does not need 
to assume or identify more primitive semantic 
features. A possible hint from its success may be that 
the meaning of groups of words in their order may 
also rely entirely on how they relate to other groups 
of words in their orders. (Unpublished work of the 
psychologist Simon Dennis is pushing in this 
direction with very interesting results.) Could it be 
possible that word strings themselves actually are the 
deepest, most fundamental representation of verbal 
meaning, not some more abstract underlying 
primitive entities or structures? 
In essay grading, LSA information turns out 
to be almost, but not quite enough.  In practice we 
add a number of primarily statistical measures, for 
example n-gram model estimates of how well the 
words have been ordered relative to standard English 
statistics. The remarkable thing is that even without 
any explicit extraction or representation of the logic 
or propositions in the essays, the methods usually 
produce slightly more reliable scores than do 
humans. Is it possible that merely the joint choosing 
of a set of words and a normative order for arranging 
them (including nonlinear interactions) suffices to 
convey all that?s needed, without needing any deeper 
level of representation? Clearly, this is very doubtful, 
but perhaps worth thinking about? 
LSA?s text analysis and matching capability, 
originally devised for IR, has found several fairly 
direct applications in education. One automatically 
measures the overlap between the content of courses 
by the text in their exams--agreeing well with teacher 
judgments on samples. This is used to help 
rationalize curricula. Another relates the content of 
job tasks, training materials, and work histories, all 
by placing their verbal descriptions in the same 
semantic space, and uses the results to assign people 
to jobs and just-in-time compensatory training. A 
new application automatically matches test items and 
learning materials to state achievement standards, 
with high agreement to human experts. Another 
automatically finds best-sentence summaries and 
categories as an aid for meta-data tagging of learning 
objects. A kind of inversion of the LSA 
representation automatically generates candidate 
keywords. 
The closest relative to essay grading is 
LSA?s role in the Summary Street program. In this 
application students read 4-10 page educational 
documents, then write 100-200 word summaries. 
Using LSA, the system tells the student about how 
well the summary covers each section of the 
document, how coherent it is--by measuring the 
similarity of successive sentences--and marks 
redundant and irrelevant sentences. (Interestingly, 
experiments have shown that students learn more 
from text that is coherent, but not excessively so, and 
LSA can be used to determine the right degree, 
although no working application has yet been built 
around the capability.) 
Another version of the Summary Street and 
essay analysis technology is a web based tool that 
scores short essays written to summarize or discuss 
the content of chapters of college textbooks, 
providing feedback on what sections to re-read to 
improve coverage. 
A somewhat different manner of extending 
LSA?s text analytic properties lies behind another 
group of applications. Suppose that a student reads a 
document about the human heart, then wants to 
choose another to read that will best advance her 
knowledge. Experiments have shown that the greatest 
learning will occur if the next reading introduces 
neither too little nor too much new knowledge. We 
call this the Goldilocks principle. By LSA analysis of 
how all of a set of materials on a topic are related to 
one another it is possible to accurately place them on 
a continuum of conceptual sophistication and 
automatically choose optimum steps. For a large 
electronic maintenance model currently under 
development, the technique is being generalized to 
provide optimum paths to knowledge? in which users 
choose a starting place and a target procedure they 
want to know, and the system picks a sequence of 
sections to read that is intended to introduce the 
needed information in an effective and efficient order 
for understanding. Combined with fisheye views, 
adaptive indexing, meaning-based LSA search, 
embedded LSA-based constructed response 
assessments, and other guidance features the system 
is a sort of midway, automatically constructed, 
intelligent tutor. 
Still another application combines aspects of 
the search and essay evaluation techniques to act as a 
kind of automated mentor for a collaborative learning 
environment. Its most interesting capabilities are 
monitoring and continuously assessing the content of 
the individual and the total group contributions, 
connecting individuals with others who are have 
made comments about similar things, posting alerts 
when the discussion wanders, both on request and 
autonomously reaching out to repositories for 
materials relevant to a discussion, and measuring the 
formation of consensus. In one small experiment, the 
system?s automatic evaluation of individual content 
contributions over a semester had a correlation of .9 
with independent ratings by participating instructors. 
Still more applications are just entering the 
research stage. One set is stimulated by the widely 
perceived inadequacy of multiple choice testing; 
students need to be able to think of answers, not just 
choose someone else?s. The goal is to replace, for 
example, missing word multiple choice vocabulary 
tests with ones in which the student supplies the word 
and the system evaluates how well it fits. 
That?s enough for successes. What about 
failures and limitations, what they teach, and where 
they point research? First, it is true that many 
laboratory tasks can reveal shortcomings and errors 
in LSA. Incorrect measures of similarity occur 
especially for sentences to sentence comparisons in 
which syntax has strong effects, where broader 
contextual information or pragmatic intent is 
involved, and where word meanings have strong 
relations to perceptual sources to which LSA training 
has had no access. In some of these cases, it is 
reasonable to suppose that the basic theoretical 
foundation is sound but the training data is not 
sufficient. In other cases it is fairly obvious that more 
fundamental limitations are at fault, such as the lack 
of a purely computation process by which to 
contextually disambiguate the phenomena 
traditionally described as multiple word senses. 
But what about the lessons from trying to 
solve educational problems promised earlier? There 
are two glaring examples. One is scoring answers to 
math problems, or mathematical answers to problems 
in physics and chemistry (never mind questions 
requiring drawings or diagrams), something we are 
frequently asked to do. Syntactic expressions with 
abstract symbols, where order is critical to logic and 
bindings are arbitrary, are simply beyond the powers 
of LSA. How to get them into a fully computational 
model, one that does not use human help in the form, 
for example, of manually constructed rules that 
natural humans could not know, preferably one in 
which the system learns the capability from the same 
interaction with the world that humans do, is the 
challenge to computational cognitive psychology and 
linguistics that forcefully presents itself, and whose 
solution could not help but require important new 
scientific knowledge about language. 
 A second educational soft spot for LSA is 
its weakness on sentences. It would almost certainly 
be better to be able to treat the meaning of an essay 
as the combination of the meaning of its sentences 
and the propositional information that order, both 
within and between sentences, helps to convey. 
Moreover, simply scoring short answers, another 
frequent request is problematic. The usual LSA-based 
methods are not useless, but they fall significantly 
short of human reliabilities. There seem to be two 
issues involved. One is again the necessity of 
accounting for syntax, especially for negation, 
quantification, and binding. ?The price of cloth will 
go up and the cost of plastics down? is not handled 
by LSA.  The other is that short answer questions 
often require very specific responses in which some 
words must be literal entities and others admit of 
synonyms, circumlocutions and ambiguity. No one 
has found a way to match humans with without 
adding what we consider ad hoc methods, rules and 
triggers devised and coded by people who know the 
answer. What we want is a fully computational 
method that might be a possible model of how natural 
human minds represent knowledge and turn it into an 
answer of a few words or sentences that can be 
reliably evaluated by a human who has also learned 
the needed knowledge in a computationally realistic 
way.  Finding one is another strong challenge whose 
successful attack would almost have to reveal new 
scientific truth. 
Finally, it is worth noting that LSA has up to 
very recently relied exclusively on SVD for its 
central engine. There are certainly other possibilities 
for doing the same job, and perhaps for doing it 
better, and for doing more. For example, several new 
matrix decomposition methods (that?s what LSA is) 
have recently been devised that have interesting new 
properties, such as more interpretable representations. 
Other new approaches use entirely different 
computations, for example the model of Simon 
Dennis mentioned earlier relies on string-edit theory, 
computing what operations it takes to change one 
sentence into another. There is no room, and as yet 
no results to warrant review of these here, but it is 
clear that the exploration of innovative computational 
models of language, ones that, like LSA, are quite 
different in spirit from linguistic tradition, is being 
pushed by a desire to solve practical problems, 
featuring especially ones in education, and that the 
effort has not nearly reached its limits. 
 
 
References 
 
Simon Dennis.  Unpublished.  A memory-based  
  Theory of verbal cognition. 
 
G. W. Furnas.  1985.  Experience with an adaptive  
  indexing scheme.  In Proceedings of CHI?85, ACM, 
  New York:  16-23. 
 
G. W. Furnas, T. K. Landauer, L. M. Gomez,  
  qnd  S. T. Dumais.  1987.  The vocabulary  
  problem in human system communication.  
 Communications of the ACM, 30(11):  964-971. 
 
W. Kintsch.  2001.  Predication. Cognitive Science,  
   25: 173-202 
 
T. K. Landauer.  2002. On the computational basis of  
  learning and cognition:  Arguments from LSA. In  
  N. Ross   (Ed.), The Psychology of Learning and   
 Motivation, 41:  43-84. 
 
D. E. Stokes.  1997.  Pasteur?s Quadrant: Basic  
  Science and Technological Innovation, Brookings  
  Institution Press, Washington, DC 
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 299?308,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Word Maturity: Computational Modeling of Word Knowledge 
Kirill Kireyev   Thomas K Landauer  Pearson Education, Knowledge Technologies Boulder, CO  {kirill.kireyev, tom.landauer}@pearson.com
 
 
Abstract 
While computational estimation of difficulty of words in the lexicon is useful in many edu-cational and assessment applications, the concept of scalar word difficulty and current corpus-based methods for its estimation are inadequate. We propose a new paradigm called word meaning maturity which tracks the degree of knowledge of each word at dif-ferent stages of language learning. We pre-sent a computational algorithm for estimating word maturity, based on modeling language acquisition with Latent Semantic Analysis. We demonstrate that the resulting metric not only correlates well with external indicators, but captures deeper semantic effects in lan-guage. 1 Motivation It is no surprise that through stages of language learning, different words are learned at different times and are known to different extents. For ex-ample, a common word like ?dog? is familiar to even a first-grader, whereas a more advanced word like ?focal? does not usually enter learners? vocabulary until much later. Although individual rates of learning words may vary between high- and low-performing students, it has been observed that ?children [?] acquire word meanings in roughly the same sequence? (Biemiller, 2008). The aim of this work is to model the degree of knowledge of words at different learning stages. Such a metric would have extremely useful appli-cations in personalized educational technologies, for the purposes of accurate assessment and per-sonalized vocabulary instruction. 
2 Rethinking Word Difficulty Previously, related work in education and psy-chometrics has been concerned with measuring word difficulty or classifying words into different difficulty categories.  Examples of such approaches include creation of word lists for targeted vocabulary instruction at various grade levels that were compiled by educa-tional experts, such as Nation (1993) or Biemiller (2008). Such word difficulty assignments are also implicitly present in some readability formulas that estimate difficulty of texts, such as Lexiles (Stenner, 1996), which include a lexical difficulty component based on the frequency of occurrence of words in a representative corpus, on the as-sumption that word difficulty is inversely correlat-ed to corpus frequency. Additionally, research in psycholinguistics has attempted to outline and measure psycholinguistic dimensions of words such as age-of-acquisition and familiarity, which aim to track when certain words become known and how familiar they appear to an average per-son. Importantly, all such word difficulty measures can be thought of as functions that assign a single scalar value to each word w: ? ?????? ? ? ? ? ? (1) There are several important limitations to such metrics, regardless of whether they are derived from corpus frequency, expert judgments or other measures. First, learning each word is a continual process, one that is interdependent with the rest of the vo-cabulary. Wolter (2001) writes:  
299
[?] Knowing a word is quite often not an either-or situation; some words are known well, some not at all, and some are known to varying degrees. [?] How well a particular word is known may condition the connections made between that particular word and the other words in the mental lexicon.  Thus, instead of modeling when a particular word will become fully known, it makes more sense to model the degree to which a word is known at different levels of language exposure.  Second, word difficulty is inherently perspec-tival: the degree of word understanding depends not only on the word itself, but also on the sophis-tication of a given learner. Consider again the dif-ference between ?dog? and ?focal?: a typical first-grader will have much more difficulty understand-ing the latter word compared to the former, where-as a well-educated adult will be able to use these words with equal ease. Therefore, the degree, or maturity, of word knowledge is inherently a func-tion of two parameters -- word w and learner level l: ??????? ? ? ?, ? ? ? (2) As the level l increases (i.e. for more advanced learners), we would expect the degree of under-standing of word w to approach its full value cor-responding to perfect knowledge; this will happen at different rates for different words.  Ideally, we would obtain maturity values by testing word knowledge of learners across differ-ent levels (ages or school grades) for all the words in the lexicon. Such a procedure, however, is pro-hibitively expensive; so instead we would like to estimate word maturity by using computational models.  To summarize: our aim is to model the devel-opment of meaning of words as a function of in-creasing exposure to language, and ultimately - the degree to which the meaning of words at each stage of exposure resemble their ?adult? meaning. We therefore define word meaning maturity to be the degree to which the understanding of the word (expected for the average learner of a particular level) resembles that of an ideal mature learner. 
3 Modeling Word Meaning Acquisition with Latent Semantic Analysis  3.1 Latent Semantic Analysis (LSA) An appealing choice for quantitatively modeling word meanings and their growth over time is La-tent Semantic Analysis (LSA), an unsupervised method for representing word and document meaning in a multi-dimensional vector space.  The LSA vector representation is derived in an unsupervised manner, based on occurrence pat-terns of words in a large corpus of natural lan-guage documents. A Singular Value Decomposition on the high-dimensional matrix of word/document occurrence counts (A) in the cor-pus, followed by zeroing all but the largest r ele-ments1 of the diagonal matrix S, yields a lower-rank word vector matrix (U).  The dimensionality reduction has the effect of smoothing out inci-dental co-occurrences and preserving significant semantic relationships between words. The result-ing word vectors2 in U are positioned in such a way that semantically related words vectors point in similar directions or, equivalently, have higher cosine values between them. For more details, please refer to Landauer et al (2007) and others.  
 Figure 1. The SVD process in LSA illustrated. The original high-dimensional word-by-document matrix A is decomposed into word (U) and document (V) matrices of lower dimen-sionality. In addition to merely measuring semantic relat-edness, LSA has been shown to emulate the learn-ing of word meanings from natural language (as can be evidenced by a broad range of applications from synonym tests to automated essay grading), at rates that resemble those of human learners (Laundauer et al 1997). Landauer and Dumais (1997) have demonstrated empirically that LSA can emulate not only the rate of human language acquisition, but also more subtle phenomena, such as the effects of learning certain words on mean-ing of other words. LSA can model meaning with                                                             1 Typically the first approx. 300 dimensions are retained 2 U? is used to project word vectors into V-space 
SVD x x
Document	 ?VectorsWord	 ?VectorsOriginal	 ?Matrix
word	 ?1word	 ?2
word	 ?n
doc	 ?1 doc	 ?2 doc	 ?m. 	 ?.	 ?.
.	 ?.	 ?. r
r
r
r A U S V ? 
300
high accuracy, as attested, for example, by 90% correlation with human judgments on assessing the quality of student essay content (Landauer, 2002). 3.2 Using LSA to Compute Word Maturity In this work, the general procedure behind computationally estimating word maturity of a learner at a particular intermediate level (i.e. age or school grade level) is as follows: 1. Create an intermediate corpus for the given level. This corpus approximates the amount and sophistication of language encountered by a learner at the given level. 2. Build an LSA space on that corpus. The re-sulting LSA word vectors model the mean-ing of each word to the particular intermediate-level learner. 3. Compare the meaning representation of each word (its LSA vector) to the corresponding one in a reference model. The reference model is trained on a much larger corpus and approximates the word meanings by a mature adult learner.  We can repeat this process for each of a num-ber of levels. These levels may directly correspond to school grades, learner ages or any other arbi-trary gradations.  In summary, we estimate word maturity of a given word at a given learner level by comparing the word vector from an intermediate LSA model (trained on a corpus of size and sophistication comparable to that which a typical real student at the given level encounters) to the corresponding vector from a reference adult LSA model (trained on a larger corpus corresponding to a mature lan-guage learner). A high discrepancy between the vectors would suggest that an intermediate mod-el?s meaning of a particular word is quite different from the reference meaning, and thus the word maturity at the corresponding level is relatively low. 3.3 Procrustes Alignment (PA) Comparing vectors across different LSA spaces is less straightforward, since the individual dimen-sions in LSA do not have a meaningful interpreta-tion, and are an artifact of the content and ordering of the training corpus used. Therefore, direct com-
parisons across two different spaces, even of the same dimensionality, are meaningless, due to a mismatch in their coordinate systems.  Fortunately, we can employ a multivariate al-gebra technique known as Procrustes Alignment (or Procrustes Analysis) (PA) typically used to align two multivariate configurations of a corre-sponding set of points in two different geometric spaces.  PA has been used in conjunction with LSA, for example, in cross-language information retrieval (Littman, 1998). The basic idea behind PA is to derive a rotation matrix that allows one space to be rotated into the other. The rotation matrix is computed in such a way as to minimize the differences (namely: sum of squared distances) between corresponding points, which in the case of LSA can be common words or documents in the training set. For more details, the reader is advised to con-sult chapter 5 of (Krzanowski, 2000) or similar literature on multivariate analysis. In summary, given two matrices containing coordinates of n corresponding points X and Y (and assuming mean-centering and equal number of dimensions, as is the case in this work), we would like to min-imize the sum of squared distances between the points: 
?? = ? ? ? ?? ?????
?
???  We try to find an orthogonal rotation matrix Q, which minimizes M2  by rotating Y relative to X. That matrix can be obtained by solving the equa-tion: ?? = ???? (??? + ??? ? 2?????) It turns out that the solution to Q is given by VU?, where U?V? is the singular value decomposition of the matrix X?Y.  In our situation, where there are two spaces, adult and intermediate, the alignment points are the corresponding document vectors correspond-ing to the documents that the training corpora of the two models have in common (recall that the adult corpus is a superset of each of the intermedi-ate corpora).  The result of the Procrustes Align-ment of the two spaces is effectively a joint LSA space containing two distinct word vectors for each word (e.g. ?dog1?, ?dog2?), corresponding to the vectors from each of the original spaces. After 
301
merging using Procrustes Alignment, the compari-son of word meanings becomes a simple problem of comparing word vectors in the joint space using the standard cosine metric. 4 Implementation Details In our experiments we used passages from the MetaMetrics Inc. 2002 corpus3, largely consisting of educational and literary content representative of the reading material used in American schools at different grade levels. The average length of each passage is approximately 135 words. The first-level intermediate corpus was com-posed of 6,000 text passages, intended for school grade 1 or below. The grade level is approximated using the Coleman-Liau readability formula (Coleman, 1975), which estimates the US grade level necessary to comprehend a given text, based on its average sentence and word length statistics: ?? = 0.0588? ? 0.296? ? 15.8 (4) where L is the average number of letters per 100 words and S is the average number of sentences per 100 words.   Each subsequent intermediate corpus contains additional 6,000 new passages of the next grade level, in addition to the previous corpus. In this way, we create 14 levels. The adult corpus is twice as large, and of same grade level range (0-14) as the largest intermediate corpus. In summary, the following describes the size and makeup of the corpora used:  Corpus Size (passages) Approx. Grade Level (Coleman-Liau Index) Intermediate 1 6,000 0.0 - 1.0 Intermediate 2 12,000 0.0 - 2.0 Intermediate 3 18,000 0.0 - 3.0 Intermediate 4 24,000 0.0 - 4.0 ?   Intermediate 14 84,000 0.0 - 14.0 Adult 168,000 0.0 - 14.0 Table 1. Size and makeup of corpora. used for LSA models. The particular choice of the Coleman-Liau readability formula (CLI) is not essential; our ex-periments show that other well-known readability formulas (such as Lexiles) work equally well. All that is needed is some approximate ordering of                                                             3 We would like to acknowledge Jack Stenner and MetaMet-rics for the use of their corpus. 
passages by difficulty, in order to mimic the way typical human learners encounter progressively more difficult materials at successive school grades.  After creating the corpora, we: 1. Build LSA spaces on the adult and each of the intermediate corpora 2. Merge the intermediate space for level l with the adult space, using Procrustes Alignment. This results in a joint space with two sets of vec-tors: the versions from the intermediate space {vlw}, and adult space{vaw}. 3. Compute the cosine in the joint space be-tween the two word vectors for the given word w ? ?, ? = ??  ?( ? ? , ? ?) (5) In the cases where a word w has not been encoun-tered in a given intermediate space, or in the rare cases where the cosine value falls below 0, the word maturity value is set to 0. Hence, the range for the word maturity function falls in the closed interval [0.0, 1.0]. A higher cosine value means greater similarity in meaning between the refer-ence and intermediate spaces, which implies a more mature meaning of word w at the level l, i.e. higher word meaning maturity. The scores be-tween discrete levels are interpolated, resulting in a continuous word maturity curve for each word. Figure 1 below illustrates resulting word ma-turity curves for some of the words. 
!"
!#$"
!#%"
!#&"
!#'"
("
!" (" $" )" %" *" &" +" '" ," (!" ((" ($" ()" (%"-."
!"
#$%&
'()
#*(+
%
,-.-/%
/01"234567"846/9204":0;9<"
Figure 2. Word maturity curves for selected words. Consistent with intuition, simple words like ?dog? approach their adult meaning rather quickly, while ?focal? takes much longer to become known to any degree.  An interesting example is ?turkey?, which has a noticeable plateau in the middle. This can be explained by the fact that this word has two dis-tinct senses. Closer analysis of the corpus and the semantic near-neighbor word vectors at each in-
302
termediate space, shows that earlier meaning deal almost exclusively with the first sense (bird), while later readings with the other (country). Therefore, even though the word ?turkey? is quite prevalent in earlier readings, its full meaning is not learned until later levels. This demonstrates that our method takes into account the meaning, and not merely the frequency of occurrence. 5 Evaluation 5.1 Time-to-maturity Evaluation of the word maturity metric against external data is not always straightforward be-cause, to the best of our knowledge, data that con-tains word knowledge statistics at different learner levels does not exist. Instead, we often have to evaluate against external data consisting of scalar difficulty values (see Section 2 for discussion) for each word, such as age-of-acquisition norms de-scribed in the following subsection.  There are two ways to make such comparisons possible. One is to compute the word maturity at a particular level, obtaining a single number for each word. Another is by computing time-to-maturity: the minimum level (the value on the x-axis of the word maturity graph) at which the word maturity reaches4 a particular threshold ?:  ?? ? = min ? ?. ?. ? ?, ? > ?       (6) Intuitively, this measure corresponds to the age in a learner?s development when a given word be-comes sufficiently understood. The parameter ? can be estimated empirically (in practice ?=0.45 gives good correlations with external measures). Since the values of word maturity are interpolated, the ttm(w) can take on fractional values. It should be emphasized that such a collapsing of word maturity into a scalar value inherently results in loss of information; we only perform it in order to allow evaluation against external data sources. As a baseline for these experiments we include word frequency, namely the document frequency of words in the adult corpus. 
                                                            4 Values between discrete levels are obtained using piecewise linear interpolation 
5.2 Age-of-Acquisition Norms Age-of-Acquisition (AoA) is a psycholinguistic property of words originally reported by Carol & White (1973). Age of Acquisition approximates the age at which a word is first learned and has been proposed as a significant contributor to lan-guage and memory processes. With some excep-tions, AoA norms are collected by subjective measures, typically by asking each of a large number of participants to estimate in years the age when they have learned the word. AoA estimates have been shown to be reliable and provide a valid estimate for the objective age at which a word is acquired; see (Davis, in press) for references and discussion. In this experiment we compute Spearman cor-relations between time-to-maturity and two avail-able collections of AoA norms: Gilhooly et al, (1980) norms5, and Bristol norms6 (Stadthagen-Gonzalez et al, 2010).  Measure Gilhooly (n=1643)  Bristol (n=1402) (-) Frequency 0.59 0.59 Time-to-Maturity (?=0.45) 0.72 0.64 Table 2. Correlations with Age of Acquisition norms. 5.3 Instruction Word Lists In this experiment, we examine leveled lists of words, as created by Biemiller (2008) in the book entitled ?Words Worth Teaching: Closing the Vo-cabulary Gap?. Based on results of multiple-choice word comprehension tests administered to students of different grades as well as expert judgments, the author derives several word diffi-culty lists for vocabulary instruction in schools, including: o Words known by most children in grade 2 o Words known by 40-80% of children in grade 2 o Words known by 40-80% of children in grade 6 o Words known by fewer than 40% of chil-dren in grade 6 One would expect the words in these four groups to increase in difficulty, in the order they are pre-sented above.  
                                                            5 http://www.psy.uwa.edu.au/mrcdatabase/uwa_mrc.htm 6 http://language.psy.bris.ac.uk/bristol_norms.html 
303
To verify how these word groups correspond to the word maturity metric, we assign each of the words in the four groups a difficulty rating 1-4 respectively, and measure the correlation with time-to-maturity.    Measure Correlation (-) Frequency 0.43 Time-to-maturity (?=0.45) 0.49 Table 3. Correlations with instruction word lists (n=4176). The word maturity metric shows higher correla-tion with instruction word list norms than word frequency. 5.4 Text Complexity Another way in which our metric can be evaluated is by examining the word maturity in texts that have been leveled, i.e. have been assigned ratings of difficulty. On average, we would expect more difficult texts to contain more difficult words. Thus, the correlation between text difficulty and our word maturity metric can serve as another val-idation of the metric. For this purpose, we obtained a collection of readings that are used as reading comprehension tests by different state websites in the US7. The collection consists of 1,220 readings, each anno-tated with a US school grade level (in the range between 3-12) for which the reading is intended. The average length each passage was approxi-mately 489 words. In this experiment we computed the correlation of the grade level with time-to-maturity, and two other measures, namely: ? Time-to-maturity: average time-to-maturity of unique words in text (excluding stopwords) with ?=0.45. ? Coleman-Liau. The Coleman-Liau reada-bility index (Equation 4). ? Frequency. Average of corpus log-frequency for unique words in the text, ex-cluding stopwords.                                                                 7 The collection was created as part of the ?Aspects of Text Complexity? project funded by the Bill and Melinda Gates Foundation, 2010. 
Measure Correlation Frequency (avg. of unique words) 0.60 Coleman-Liau 0.64 Time-to-maturity (?=0.45) (avg. of unique non-stopwords) 0.70 Table 4. Correlations of grade levels with different metrics. 6 Emphasis on Meaning In this section, we would like to highlight certain properties of the LSA-based word maturity metric, particularly aiming to illustrate the fact that the metric tracks acquisition of meaning from expo-sure to language and not merely more shallow ef-fects, such as word frequency in the training corpus.  6.1 Maturity based on Frequency For a baseline that does not take meaning into ac-count, let us construct a set of maturity-like curves based on frequency statistics alone. More specifi-cally, we define the frequency-maturity for a par-ticular word at a given level as the ratio of the number of occurrences at the intermediate corpus  for that level (l) to the number of occurrences in the reference corpus (a):  
? ?, ? = ?? _ ???? ?(?)?? _ ???? ?(?) Similarly to the original LSA-based word maturity metric, this ratio increases from 0 to 1 for each word as the amount of cumulative language expo-sure increases. The corpora used at each interme-diate level are identical to the original word maturity model, but instead of creating LSA spac-es we simply use the corpora to compute word frequency. The following figure shows the Spearman cor-relations between the external measures used for experiments in Section 5, and time-to-maturity computed based on the two maturity metrics: the new frequency-based maturity and the original LSA-based word maturity.  
304
 Figure 3. Correlations of word maturity computed using fre-quency (as well as the original) against external metrics de-scribed in Section 5.  The results indicate that the original LSA-based word maturity correlates better with real-world data than a maturity metric simply based on fre-quency.  6.2 Homographs Another insight into the fact that the LSA-based word maturity metric tracks word meaning rather than mere frequency may be gained from analysis of words that are homographs: words that contain two or more unrelated meanings in the same writ-ten form, such as the word ?turkey? illustrated in Section 4. (This is related to but distinct from the merely polysemous words that have several related meanings),  Because of the conflation of several unrelated meanings into the same orthographic form, homo-graphs implicitly contain more semantic content in a single word. Therefore, one would expect the meaning of homographs to mature more slowly than would be predicted by frequency alone: all things being equal, a learner has to learn the mean-ings for all of the senses of a homograph word before the word can be considered fully known. More specifically, one would expect the time-to-maturity of homographs to have greater values than words of similar frequency. To test this hy-pothesis, we obtained8 a list 174 common English homographs. For each of them, we compared their time-to-maturity to the average time-to-maturity of words that have the same (+/- 1%) corpus fre-quency.                                                             8 http://en.wikipedia.org/wiki/List_of_English_homographs 
The results of a paired t-test confirms the hy-pothesis that the time-to-maturity of homographs is greater than other words of the same frequency, with the p-value = 5.9e-6. This is consistent with the observation that homographs will take longer to learn and serves as evidence that LSA-based word maturity approximates effects related to meaning. 6.3 Size of the Reference Corpus Another area of investigation is the repercus-sions of the choice of the corpus for the reference (adult) model. The size (and content) of the corpus used to train the reference model is potentially important, since it affects the word maturity calcu-lations, which are comparisons of the intermediate LSA spaces to the reference LSA space built on this corpus.  It is interesting to investigate how the word maturity model would be affected if the adult cor-pus were made significantly more sophisticated. If the word maturity metric were simply based on word frequency (including the frequency-based maturity baseline described in Section 6.1), one would expect the word maturity of the words at each level to decrease significantly if the reference model is made significantly larger, since each in-termediate level will have encountered fewer words by comparison. Intuition about language learning, however, tells us that with enough lan-guage exposure a learner learns virtually all there is to know about any particular word; after the word reaches its adult maturity, subsequent en-counters of natural readings do little to further change the knowledge of that word. Therefore, if word maturity were tracking something similar to real word knowledge, one would expect the word maturity for most words to plateau over time, and subsequently not change significantly, no matter how sophisticated the reference model becomes.  To evaluate this inquiry we created a reference corpus that is twice as large as before (four times as large and of the same difficulty range as the corpus for the last intermediate level), containing roughly 329,000 passages. We computed the word maturity model using this larger reference corpus, while keeping all the original intermediate corpora of the same size and content.  The results show that the average word maturi-ty of words at the last intermediate level (14) de-
0.66	 ?
0.56	 ?
0.42	 ?
0.68	 ?
0.72	 ?
0.64	 ?
0.49	 ?
0.71	 ?
0.0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1.0	 ?
AoA	 ?
(Gilhooly)	 ?
AoA	 ?
(Bristol)	 ?
Word	 ?Lists	 ? Readings	 ?
freq-??WM	 ?(?=0.15)	 ?
LSA-??WM	 ?(?=0.45)	 ?
305
creases by less than 14% as a result of doubling the adult corpus. Furthermore, this number is as low as 6%, if one only considers more common words that occur 50 times or more in the corpus. This relatively small difference, in spite of a two-fold increase of the adult corpus, is consistent with the idea that word knowledge should approach a plateau, after which further exposure to language does little to change most word meanings. 6.4 Integration into Lexicon Another important consideration with respect to word learning mentioned in Wotler (2001), is the ?connections made between [a] particular word and the other words in the mental lexicon.? One implication of that is that measuring word maturity must take into account the way words in the lan-guage are integrated with other words. One way to test this effect is to introduce read-ings where a large part of the important vocabu-lary is not well known to learners at a given level. One would expect learning to be impeded when the learning materials are inappropriate for the learner level. This can be simulated in the word maturity model by rearranging the order of some of the training passages, by introducing certain advanced passages at a very early level. If the results of the word maturity metric were merely based on fre-quency, such a reordering would have no effect on the maturity of important words (measured after all the passages containing these words have been encountered), since the total number of relevant word encounters does not change as a result of this reshuffling. If, however, the metric reflected at least some degree of semantics, we would expect word maturities for important words in these read-ings to be lower as a result of such rearranging, due to the fact that they are being introduced in contexts consisting of words that are not well known at the early levels. To test this effect, we first collected all passag-es in the training corpus of intermediate models containing some advanced words from different topics, namely: ?chromosome?, ?neutron? and ?filibuster? together with their plural variants. We changed the order of inclusion of these 89 passag-es into the intermediate models in each of the two following ways: 
1. All the passages were introduced at the first level (l=1) intermediate corpus 2. All the passages were introduced at the last level (l=14) intermediate corpus. This resulted in two new variants of word ma-turity models, which were computed in all the same ways as before, except that all of these 89 advanced passages were introduced either at the very first level or at the very last level. We then computed the word maturity at the levels they were introduced. The hypothesis consistent with a meaning-based maturity method would be that less learning (i.e. lower word maturity) of the relevant words will occur when passages are introduced prematurely (at level 1). Table 5 shows the word maturities measured for each of those cases, at the level (1 or 14) when all of the passages have been introduced. Word Introduced at l=1 (WM at l=1) Introduced at l=14 (WM at l=14) chromosome 0.51 0.73 neutron 0.51 0.72 filibuster 0.58 0.85 Table 5. Word maturity of words resulting when all the rele-vant passages are introduced early vs late. Indeed, the results show lower word maturity val-ues when advanced passages are introduced too early, and higher ones when the passages are in-troduced at a later stage, when the rest of the sup-porting vocabulary is known. 7 Conclusion We have introduced a new metric for estimating the degree of knowledge of words by learners at different levels. We have also proposed and evalu-ated an implementation of this metric using Latent Semantic Analysis. The implementation is based on unsupervised word meaning acquisition from natural text, from corpora that resemble in volume and complexity the reading materials a typical human learner might encounter.  The metric correlates better than word frequen-cy to a range of external measures, including vo-cabulary word lists, psycholinguistic norms and leveled texts. Furthermore, we have shown that the metric is based on word meaning (to the extent that it can be approximated with LSA), and not merely on shallow measures like word frequency. 
306
Many interesting research questions still re-main pertaining to the best way to select and parti-tion the training corpora, align adult and intermediate LSA models, correlate the results with real school grade levels, as well as other free parameters in the model. Nevertheless, we have shown that LSA can be employed to usefully mimic model word knowledge. The models are currently used (at Pearson Education) to create state-of-the-art personalized vocabulary instruc-tion and assessment tools.   
307
 References Andrew Biemiller (2008). Words Worth Teaching. Co-lumbus, OH:  SRA/McGraw-Hill. John B. Carrol and M. N. White (1973). Age of acquisi-tion norms for 220 picturable nouns. Journal of Ver-bal Learning & Verbal Behavior, 12, 563-576. Meri Coleman and T.L. Liau (1975). A computer read-ability formula designed for machine scoring, Jour-nal of Applied Psychology, Vol. 60, pp. 283-284. Ken J. Gilhooly and R. H. Logie (1980). Age of acqui-sition, imagery, concreteness, familiarity and ambi-guity measures for 1944 words. Behaviour Research Methods & Instrumentation, 12, 395-427. Wojtek J. Krzanowski (2000) Principles of Multivari-ate Analysis: A User?s Perspective (Oxford Statisti-cal Science Series). Oxford University Press, USA. Thomas K Landauer and Susan Dumais (1997). A solu-tion to Plato's problem: The Latent Semantic Analy-sis Theory of the Acquisition, Induction, and Representation of Knowledge. Psychological Re-view, 104, pp 211-240. Thomas K Landauer (2002). On the Computation Basis of Learning and Cognition: Arguments from LSA. In N. Ross (Ed.), The Psychology of Learning and Mo-tivation, 41, 43-84. Thomas K Landauer, Danielle S. McNamara, Simon Dennis, and Walter Kintsch (2007). Handbook of Latent Semantic Analysis. Lawrence Erlbaum. Paul Nation (1993). Measuring readiness for simplified material: a test of the first 1,000 words of English. In Simplification: Theory and Application M. L. Tickoo (ed.), RELC Anthology Series 31: 193-203. Hans Stadthagen-Gonzalez and C. J. Davis (2006). The Bristol Norms for Age of Acquisition, Imageability and Familiarity. Behavior Research Methods, 38, 598-605.  A. Jackson Stenner (1996). Measuring Reading Com-prehension with the Lexile Framework. Forth North American Conference on Adolescent/Adult Literacy. Brent Wolter (2001). Comparing the L1 and L2 Mental Lexicon. Studies in Second Language Acquisition. Cambridge University Press.       
308
