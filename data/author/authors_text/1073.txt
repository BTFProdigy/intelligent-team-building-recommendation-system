Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1145?1152,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multilingual Document Clustering: an Heuristic Approach Based on
Cognate Named Entities
Soto Montalvo
GAVAB Group
URJC
soto.montalvo@urjc.es
Raquel Mart??nez
NLP&IR Group
UNED
raquel@lsi.uned.es
Arantza Casillas
Dpt. EE
UPV-EHU
arantza.casillas@ehu.es
V??ctor Fresno
GAVAB Group
URJC
victor.fresno@urjc.es
Abstract
This paper presents an approach for Mul-
tilingual Document Clustering in compa-
rable corpora. The algorithm is of heuris-
tic nature and it uses as unique evidence
for clustering the identification of cognate
named entities between both sides of the
comparable corpora. One of the main ad-
vantages of this approach is that it does
not depend on bilingual or multilingual re-
sources. However, it depends on the pos-
sibility of identifying cognate named enti-
ties between the languages used in the cor-
pus. An additional advantage of the ap-
proach is that it does not need any infor-
mation about the right number of clusters;
the algorithm calculates it. We have tested
this approach with a comparable corpus
of news written in English and Spanish.
In addition, we have compared the results
with a system which translates selected
document features. The obtained results
are encouraging.
1 Introduction
Multilingual Document Clustering (MDC) in-
volves dividing a set of n documents, written in
different languages, into a specified number k of
clusters, so the documents that are similar to other
documents are in the same cluster. Meanwhile
a multilingual cluster is composed of documents
written in different languages, a monolingual clus-
ter is composed of documents written in one lan-
guage.
MDC has many applications. The increasing
amount of documents written in different lan-
guages that are available electronically, leads to
develop applications to manage that amount of
information for filtering, retrieving and grouping
multilingual documents. MDC tools can make
easier tasks such as Cross-Lingual Information
Retrieval, the training of parameters in statistics
based machine translation, or the alignment of par-
allel and non parallel corpora, among others.
MDC systems have developed different solu-
tions to group related documents. The strate-
gies employed can be classified in two main
groups: the ones which use translation technolo-
gies, and the ones that transform the document into
a language-independent representation.
One of the crucial issues regarding the methods
based on document or features translation is the
correctness of the proper translation. Bilingual re-
sources usually suggest more than one sense for
a source word and it is not a trivial task to select
the appropriate one. Although word-sense disam-
biguation methods can be applied, these are not
free of errors. On the other hand, methods based
on language-independent representation also have
limitations. For instance, those based on thesaurus
depend on the thesaurus scope. Numbers or dates
identification can be appropriate for some types
of clustering and documents; however, for other
types of documents or clustering it could not be so
relevant and even it could be a source of noise.
In this work we dealt with MDC and we pro-
posed an approach based only on cognate Named
Entities (NE) identification. We have tested this
approach with a comparable corpus of news writ-
ten in English and Spanish, obtaining encouraging
results. One of the main advantages of this ap-
proach is that it does not depend on multilingual
resources such as dictionaries, machine translation
systems, thesaurus or gazetteers. In addition, no
information about the right number of clusters has
1145
to be provided to the algorithm. It only depends on
the possibility of identifying cognate named enti-
ties between the languages involved in the corpus.
It could be particularly appropriate for news cor-
pus, where named entities play an important role.
In order to compare the results of our approach
with other based on features translation, we also
dealt with this one, as baseline approach. The sys-
tem uses EuroWordNet (Vossen, 1998) to trans-
late the features. We tried different features cate-
gories and combinations of them in order to deter-
mine which ones lead to improve MDC results in
this approach.
In the following section we relate previous work
in the field. In Section 3 we present our approach
for MDC. Section 4 describes the system we com-
pare our approach with, as well as the experiments
and the results. Finally, Section 5 summarizes the
conclusions and the future work.
2 Related Work
MDC is normally applied with parallel (Silva et.
al., 2004) or comparable corpus (Chen and Lin,
2000), (Rauber et. al., 2001), (Lawrence, 2003),
(Steinberger et. al., 2002), (Mathieu et. al, 2004),
(Pouliquen et. al., 2004). In the case of the com-
parable corpora, the documents usually are news
articles.
Considering the approaches based on transla-
tion technology, two different strategies are em-
ployed: (1) translate the whole document to an an-
chor language, and (2) translate some features of
the document to an anchor language.
With regard to the first approach, some authors
use machine translation systems, whereas others
translate the document word by word consulting
a bilingual dictionary. In (Lawrence, 2003), the
author presents several experiments for clustering
a Russian-English multilingual corpus; several of
these experiments are based on using a machine
translation system. Columbia?s Newsblaster sys-
tem (Kirk et al, 2004) clusters news into events,
it categorizes events into broad topic and summa-
rizes multiple articles on each event. In the clus-
tering process non-English documents are trans-
lated using simple dictionary lookup techniques
for translating Japanese and Russian documents,
and the Systran translation system for the other
languages used in the system.
When the solution involves translating only
some features, first it is necessary to select these
features (usually entities, verbs, nouns) and then
translate them with a bilingual dictionary or/and
consulting a parallel corpus.
In (Mathieu et. al, 2004) before the cluster-
ing process, the authors perform a linguistic anal-
ysis which extracts lemmas and recognizes named
entities (location, organization, person, time ex-
pression, numeric expression, product or event);
then, the documents are represented by a set of
terms (keywords or named entity types). In addi-
tion, they use document frequency to select rele-
vant features among the extracted terms. Finally,
the solution uses bilingual dictionaries to translate
the selected features. In (Rauber et. al., 2001)
the authors present a methodology in which docu-
ments are parsed to extract features: all the words
which appear in n documents except the stop-
words. Then, standard machine translation tech-
niques are used to create a monolingual corpus.
After the translation process the documents are au-
tomatically organized into separate clusters using
an un-supervised neural network.
Some approaches first carry out an independent
clustering in each language, that is a monolingual
clustering, and then they find relations among the
obtained clusters generating the multilingual clus-
ters. Others solutions start with a multilingual
clustering to look for relations between the doc-
uments of all the involved languages. This is the
case of (Chen and Lin, 2000), where the authors
propose an architecture of multilingual news sum-
marizer which includes monolingual and multilin-
gual clustering; the multilingual clustering takes
input from the monolingual clusters. The authors
select different type of features depending on the
clustering: for the monolingual clustering they use
only named entities, for the multilingual clustering
they extract verbs besides named entities.
The strategies that use language-independent
representation try to normalize or standardize the
document contents in a language-neutral way; for
example: (1) by mapping text contents to an inde-
pendent knowledge representation, or (2) by rec-
ognizing language independent text features inside
the documents. Both approaches can be employed
isolated or combined.
The first approach involves the use of exist-
ing multilingual linguistic resources, such as the-
saurus, to create a text representation consisting of
a set of thesaurus items. Normally, in a multilin-
gual thesaurus, elements in different languages are
1146
related via language-independent items. So, two
documents written in different languages can be
considered similar if they have similar representa-
tion according to the thesaurus. In some cases, it
is necessary to use the thesaurus in combination
with a machine learning method for mapping cor-
rectly documents onto thesaurus. In (Steinberger
et. al., 2002) the authors present an approach to
calculate the semantic similarity by representing
the document contents in a language independent
way, using the descriptor terms of the multilingual
thesaurus Eurovoc.
The second approach, recognition of language
independent text features, involves the recognition
of elements such as: dates, numbers, and named
entities. In others works, for instance (Silva
et. al., 2004), the authors present a method
based on Relevant Expressions (RE). The RE are
multilingual lexical units of any length automat-
ically extracted from the documents using the
LiPXtractor extractor, a language independent
statistics-based tool. The RE are used as base
features to obtain a reduced set of new features
for the multilingual clustering, but the clusters
obtained are monolingual.
Others works combine recognition of indepen-
dent text features (numbers, dates, names, cog-
nates) with mapping text contents to a thesaurus.
In (Pouliquen et. al., 2004) the cross-lingual
news cluster similarity is based on a linear com-
bination of three types of input: (a) cognates, (b)
automatically detected references of geographical
place names, and (c) the results of a mapping
process onto a multilingual classification system
which maps documents onto the multilingual the-
saurus Eurovoc. In (Steinberger et. al., 2004) it
is proposed to extract language-independent text
features using gazetteers and regular expressions
besides thesaurus and classification systems.
None of the revised works use as unique evi-
dence for multilingual clustering the identification
of cognate named entities between both sides of
the comparable corpora.
3 MDC by Cognate NE Identification
We propose an approach for MDC based only
on cognate NE identification. The NEs cate-
gories that we take into account are: PERSON,
ORGANIZATION, LOCATION, and MISCEL-
LANY. Other numerical categories such as DATE,
TIME or NUMBER are not considered because
we think they are less relevant regarding the con-
tent of the document. In addition, they can lead to
group documents with few content in common.
The process has two main phases: (1) cognate
NE identification and (2) clustering. Both phases
are described in detail in the following sections.
3.1 Cognate NE identification
This phase consists of three steps:
1. Detection and classification of the NEs in
each side of the corpus.
2. Identification of cognates between the NEs of
both sides of the comparable corpus.
3. To work out a statistic of the number of docu-
ments that share cognates of the different NE
categories.
Regarding the first step, it is carried out in each
side of the corpus separately. In our case we used
a corpus with morphosyntactical annotations and
the NEs identified and classified with the FreeLing
tool (Carreras et al, 2004).
In order to identify the cognates between NEs 4
steps are carried out:
? Obtaining two list of NEs, one for each lan-
guage.
? Identification of entity mentions in each lan-
guage. For instance, ?Ernesto Zedillo?,
?Zedillo?, ?Sr. Zedillo? will be considered
as the same entity after this step since they
refer to the same person. This step is only
applied to entities of PERSON category. The
identification of NE mentions, as well as cog-
nate NE, is based on the use of the Leven-
shtein edit-distance function (LD). This mea-
sure is obtained by finding the cheapest way
to transform one string into another. Trans-
formations are the one-step operations of in-
sertion, deletion and substitution. The result
is an integer value that is normalized by the
length of the longest string. In addition, con-
straints regarding the number of words that
the NEs are made up, as well as the order of
the words are applied.
? Identification of cognates between the NEs
of both sides of the comparable corpus. It
is also based on the LD. In addition, also
1147
constraints regarding the number and the or-
der of the words are applied. First, we tried
cognate identification only between NEs of
the same category (PERSON with PERSON,
. . . ) or between any category and MISCEL-
LANY (PERSON with MISCELLANY, . . . ).
Next, with the rest of NEs that have not been
considered as cognate, a next step is applied
without the constraint of being to the same
category or MISCELLANY. As result of this
step a list of corresponding bilingual cog-
nates is obtained.
? The same procedure carried out for obtaining
bilingual cognates is used to obtain two more
lists of cognates, one per language, between
the NEs of the same language.
Finally, a statistic of the number of documents
that share cognates of the different NE categories
is worked out. This information can be used by the
algorithm (or the user) to select the NE category
used as constraint in the clustering steps 1(a) and
2(b).
3.2 Clustering
The algorithm for clustering multilingual docu-
ments based on cognate NEs is of heuristic nature.
It consists of 3 main phases: (1) first clusters cre-
ation, (2) addition of remaining documents to ex-
isting clusters, and (3) final cluster adjustment.
1. First clusters creation. This phase consists of
2 steps.
(a) First, documents in different languages
that have more cognates in common
than a threshold are grouped into the
same cluster. In addition, at least one of
the cognates has to be of a specific cate-
gory (PERSON, LOCATION or ORGA-
NIZATION), and the number of men-
tions has to be similar; a threshold de-
termines the similarity degree. After
this step some documents are assigned
to clusters while the others are free (with
no cluster assigned).
(b) Next, it is tried to assign each free docu-
ment to an existing cluster. This is pos-
sible if there is a document in the cluster
that has more cognates in common with
the free document than a threshold, with
no constraints regarding the NE cate-
gory. If it is not possible, a new clus-
ter is created. This step can also have as
result free documents.
At this point the number of clusters created is
fixed for the next phase.
2. Addition of the rest of the documents to ex-
isting clusters. This phase is carried out in 2
steps.
(a) A document is added to a cluster that
contains a document which has more
cognates in common than a threshold.
(b) Until now, the cognate NEs have been
compared between both sides of the cor-
pus, that is a bilingual comparison. In
this step, the NEs of a language are com-
pared with those of the same language.
This can be described like a monolin-
gual comparison step. The aim is to
group similar documents of the same
language if the bilingual comparison
steps have not been successful. As in
the other cases, a document is added to
a cluster with at least a document of the
same language which has more cognates
in common than a threshold. In addi-
tion, at least one of the cognates have to
be of a specific category (PERSON, LO-
CATION or ORGANIZATION).
3. Final cluster adjustment. Finally, if there are
still free documents, each one is assigned to
the cluster with more cognates in common,
without constraints or threshold. Nonethe-
less, if free documents are left because they
do not have any cognates in common with
those assigned to the existing clusters, new
clusters can be created.
Most of the thresholds can be customized in or-
der to permit and make the experiments easier. In
addition, the parameters customization allows the
adaptation to different type of corpus or content.
For example, in steps 1(a) and 2(b) we enforce at
least on match in a specific NE category. This pa-
rameter can be customized in order to guide the
grouping towards some type of NE. In Section 4.5
the exact values we used are described.
Our approach is an heuristic method that fol-
lowing an agglomerative approach and in an it-
erative way, decides the number of clusters and
1148
locates each document in a cluster; everything is
based in cognate NEs identification. The final
number of clusters depends on the threshold val-
ues.
4 Evaluation
We wanted not only determine whether our ap-
proach was successful for MDC or not, but we also
wanted to compare its results with other approach
based on feature translation. That is why we try
MDC by selecting and translating the features of
the documents.
In this Section, first the MCD by feature transla-
tion is described; next, the corpus, the experiments
and the results are presented.
4.1 MDC by Feature Translation
In this approach we emphasize the feature selec-
tion based on NEs identification and the grammat-
ical category of the words. The selection of fea-
tures we applied is based on previous work (Casil-
las et. al, 2004), in which several document rep-
resentations are tested in order to study which of
them lead to better monolingual clustering results.
We used this MDC approach as baseline method.
The approach we implemented consists of the
following steps:
1. Selection of features (NE, noun, verb, adjec-
tive, ...) and its context (the whole document
or the first paragraph). Normally, the journal-
ist style includes the heart of the news in the
first paragraph; taking this into account we
have experimented with the whole document
and only with the first paragraph.
2. Translation of the features by using Eu-
roWordNet 1.0. We translate English into
Spanish. When more than one sense for a
single word is provided, we disambiguate by
selecting one sense if it appears in the Span-
ish corpus. Since we work with a comparable
corpus, we expect that the correct translation
of a word appears in it.
3. In order to generate the document represen-
tation we use the TF-IDF function to weight
the features.
4. Use of an clustering algorithm. Particu-
larly, we used a partitioning algorithm of the
CLUTO (Karypis, 2002) library for cluster-
ing.
4.2 Corpus
A Comparable Corpus is a collection of simi-
lar texts in different languages or in different va-
rieties of a language. In this work we com-
piled a collection of news written in Spanish and
English belonging to the same period of time.
The news are categorized and come from the
news agency EFE compiled by HERMES project
(http://nlp.uned.es/hermes/index.html). That col-
lection can be considered like a comparable cor-
pus. We have used three subset of that collection.
The first subset, call S1, consists on 65 news, 32
in Spanish and 33 in English; we used it in order
to train the threshold values. The second one, S2,
is composed of 79 Spanish news and 70 English
news, that is 149 news. The third subset, S3, con-
tains 179 news: 93 in Spanish and 86 in English.
In order to test the MDC results we carried out a
manual clustering with each subset. Three persons
read every document and grouped them consider-
ing the content of each one. They judged inde-
pendently and only the identical resultant clusters
were selected. The human clustering solution is
composed of 12 clusters for subset S1, 26 clus-
ters for subset S2, and 33 clusters for S3. All the
clusters are multilingual in the three subsets.
In the experimentation process of our approach
the first subset, S1, was used to train the parame-
ters and threshold values; with the second one and
the third one the best parameters values were ap-
plied.
4.3 Evaluation metric
The quality of the experimentation results are de-
termined by means of an external evaluation mea-
sure, the F-measure (van Rijsbergen, 1974). This
measure compares the human solution with the
system one. The F-measure combines the preci-
sion and recall measures:
F (i, j) = 2?Recall(i, j)? Precision(i, j)(Precision(i, j) +Recall(i, j)) ,
(1)
where Recall(i, j) = nijni , Precision(i, j) =
nij
nj ,
nij is the number of members of cluster human so-
lution i in cluster j, nj is the number of members
of cluster j and ni is the number of members of
cluster human solution i. For all the clusters:
F =
?
i
ni
n max{F (i)} (2)
The closer to 1 the F-measure value the better.
1149
4.4 Experiments and Results with MDC by
Feature Translation
After trying with features of different grammatical
categories and combinations of them, Table 1 and
Table 2 only show the best results of the experi-
ments.
The first column of both tables indicates the
features used in clustering: NOM (nouns), VER
(verbs), ADJ (adjectives), ALL (all the lemmas),
NE (named entities), and 1rst PAR (those of the
first paragraph of the previous categories). The
second column is the F-measure, and the third one
indicates the number of multilingual clusters ob-
tained. Note that the number of total clusters of
each subset is provided to the clustering algorithm.
As can be seen in the tables, the results depend on
the features selected.
4.5 Experiments and Results with MDC by
Cognate NE
The threshold for the LD in order to determine
whether two NEs are cognate or not is 0.2, except
for entities of ORGANIZATION and LOCATION
categories which is 0.3 when they have more than
one word.
Regarding the thresholds of the clustering phase
(Section 3.2), after training the thresholds with the
collection S1 of 65 news articles we have con-
cluded:
? The first step in the clustering phase, 1(a),
performs a good first grouping with thresh-
old relatively high; in this case 6 or 7. That
is, documents in different languages that have
more cognates in common than 6 or 7 are
grouped into the same cluster. In addition,
at least one of the cognates have to be of an
specific category, and the difference between
the number of mentions have to be equal or
less than 2. Of course, these threshold are ap-
plied after checking that there are documents
that meet the requirements. If they do not,
thresholds are reduced. This first step creates
multilingual clusters with high cohesiveness.
? Steps 1(b) and 2(a) lead to good results with
small threshold values: 1 or 2. They are de-
signed to give priority to the addition of doc-
uments to existing clusters. In fact, only step
1(b) can create new clusters.
? Step 2(b) tries to group similar documents of
the same language when the bilingual com-
parison steps could not be able to deal with
them. This step leads to good results with a
threshold value similar to 1(a) step, and with
the same NE category.
On the other hand, regarding the NE category
enforce on match in steps 1(a) and 2(b), we tried
with the two NE categories of cognates shared by
the most number of documents. Particularly, with
S2 and S3 corpus the NE categories of the cog-
nates shared by the most number of documents
was LOCATION followed by PERSON. We ex-
perimented with both categories.
Table 3 and Table 4 show the results of the ap-
plication of the cognate NE approach to subsets
S2 and S3 respectively. The first column of both
tables indicates the thresholds for each step of the
algorithm. Second and third columns show the re-
sults by selecting PERSON category as NE cat-
egory to be shared by at least a cognate in steps
1(a) and 2(b); whereas fourth and fifth columns are
calculated with LOCATION NE category. The re-
sults are quite similar but slightly better with LO-
CATION category, that is the cognate NE category
shared by the most number of documents. Al-
though none of the results got the exact number of
clusters, it is remarkable that the resulting values
are close to the right ones. In fact, no information
about the right number of cluster is provided to the
algorithm.
If we compare the performance of the two ap-
proaches (Table 3 with Table 1 and Table 4 with
Table 2) our approach obtains better results. With
the subset S3 the results of the F-measure of both
approaches are more similar than with the subset
S2, but the F-measure values of our approach are
still slightly better.
To sum up, our approach obtains slightly bet-
ter results that the one based on feature translation
with the same corpora. In addition, the number of
multilingual clusters is closer to the reference so-
lution. We think that it is remarkable that our ap-
proach reaches results that can be comparable with
those obtained by means of features translation.
We will have to test the algorithm with different
corpora (with some monolingual clusters, differ-
ent languages) in order to confirm its performance.
5 Conclusions and Future Work
We have presented a novel approach for Multilin-
gual Document Clustering based only on cognate
1150
Selected Features F-measure Multilin. Clus./Total
NOM, VER 0.8533 21/26
NOM, ADJ 0.8405 21/26
ALL 0.8209 21/26
NE 0.8117 19/26
NOM, VER, ADJ 0.7984 20/26
NOM, VER, ADJ, 1rst PAR 0.7570 21/26
NOM, ADJ, 1rst PAR 0.7515 22/26
ALL, 1rst PAR 0.7473 19/26
NOM, VER, 1rst PAR 0.7371 20/26
Table 1: MDC results with the feature translation approach and subset S2
Selected Features F-measure Multilin. Clus. /Total
NOM, ADJ 0.8291 26/33
ALL 0.8126 27/33
NOM, VER 0.8028 26/33
NE 0.8015 23/33
NOM, VER, ADJ 0.7917 25/33
NOM, ADJ, 1rst PAR 0.7520 28/33
NOM, VER, ADJ, 1rst PAR 0.7484 26/33
ALL, 1rst PAR 0.7288 26/33
NOM, VER, 1rst PAR 0.7200 24/33
Table 2: MDC results with the feature translation approach and subset S3
Thresholds 1(a), 2(b) match on PERSON 1(a), 2(b) match on LOCATION
Steps Results Clusters Results Clusters
1(a) 1(b) 2(a) 2(b) F-measure Multil./Calc./Total F-measure Multil./Calc./Total
6 2 1 5 0.9097 24/24/26 0.9097 24/24/26
6 2 1 6 0.8961 24/24/26 0.8961 24/24/26
6 2 1 7 0.8955 24/24/26 0.8955 24/24/26
6 2 2 5 0.8861 24/24/26 0.8913 24/24/26
7 2 1 5 0.8859 24/24/26 0.8913 24/24/26
6 2 2 4 0.8785 24/24/26 0.8899 24/24/26
6 2 2 6 0.8773 24/24/26 0.8833 24/24/26
6 2 2 7 0.8773 24/24/26 0.8708 24/24/26
Table 3: MDC results with the cognate NE approach and S2 subset
Thresholds 1(a), 2(b) match on PERSON 1(a), 2(b) match on LOCATION
Steps Results Clusters Results Clusters
1(a) 1(b) 2(a) 2(b) F-measure Multil./Calc./Total F-measure Multil./Calc./Total
7 2 1 5 0.8587 30/30/33 0.8621 30/30/33
6 2 1 5 0.8552 30/30/33 0.8552 30/30/33
6 2 1 6 0.8482 30/30/33 0.8483 30/30/33
6 2 1 7 0.8471 30/30/33 0.8470 30/30/33
6 2 2 5 0.8354 30/30/33 0.8393 30/30/33
6 2 2 6 0.8353 30/30/33 0.8474 30/30/33
6 2 2 4 0.8323 30/30/33 0.8474 30/30/33
6 2 2 7 0.8213 30/30/33 0.8134 30/30/33
Table 4: MDC results with the cognate NE approach and S3 subset
1151
named entities identification. One of the main ad-
vantages of this approach is that it does not depend
on multilingual resources such as dictionaries, ma-
chine translation systems, thesaurus or gazetteers.
The only requirement to fulfill is that the lan-
guages involved in the corpus have to permit the
possibility of identifying cognate named entities.
Another advantage of the approach is that it does
not need any information about the right number
of clusters. In fact, the algorithm calculates it by
using the threshold values of the algorithm.
We have tested this approach with a comparable
corpus of news written in English and Spanish, ob-
taining encouraging results. We think that this ap-
proach could be particularly appropriate for news
articles corpus, where named entities play an im-
portant role. Even more, when there is no previous
evidence of the right number of clusters. In addi-
tion, we have compared our approach with other
based on feature translation, resulting that our ap-
proach presents a slightly better performance.
Future work will include the compilation of
more corpora, the incorporation of machine learn-
ing techniques in order to obtain the thresholds
more appropriate for different type of corpus. In
addition, we will study if changing the order of
the bilingual and monolingual comparison steps
the performance varies significantly for different
type of corpus.
Acknowledgements
We wish to thank the anonymous reviewers for
their helpful and instructive comments. This work
has been partially supported by MCyT TIN2005-
08943-C02-02.
References
Benoit Mathieu, Romanic Besancon and Christian
Fluhr. 2004. ?Multilingual document clusters dis-
covery?. RIAO?2004, p. 1-10.
Arantza Casillas, M. Teresa Gonza?lez de Lena and
Raquel Mart??nez. 2004. ?Sampling and Feature
Selection in a Genetic Algorithm for Document
Clustering?. Computational Linguistics and Intel-
ligent Text Processing, CICLing?04. Lecture Notes
in Computer Science, Springer-Verlag, p. 601-612.
Hsin-Hsi Chen and Chuan-Jie Lin. 2000. ?A Multilin-
gual News Summarizer?. Proceedings of 18th Inter-
national Conference on Computational Linguistics,
p. 159-165.
Xavier Carreras, I. Chao, Lluis Padro? and M.
Padro? 2004 ?An Open-Source Suite of Lan-
guage Analyzers?. Proceedings of the 4th In-
ternational Conference on Language Resources
and Evaluation (LREC?04). Lisbon, Portugal.
http://garraf.epsevg.upc.es/freeling/.
Karypis G. 2002. ? CLUTO: A Clustering Toolkit?.
Technical Report: 02-017. University of Minnesota,
Department of Computer Science, Minneapolis, MN
55455.
David Kirk Evans, Judith L. Klavans and Kathleen
McKeown. 2004. ?Columbian Newsblaster: Multi-
lingual News Summarization on the Web?. Proceed-
ings of the Human Language Technology Confer-
ence and the North American Chapter of the Asso-
ciation for Computational Linguistics Annual Meet-
ing, HLT-NAACL?2004.
Lawrence J. Leftin. 2003. ?Newsblaster Russian-
English Clustering Performance Analysis?.
Columbia computer science Technical Reports.
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat,
Emilia Ksper and Irina Temikova. 2004. ?Multi-
lingual and cross-lingual news topic tracking?. Pro-
ceedings of the 20th International Conference on
computational Linguistics, p. 23-27.
Andreas Rauber, Michael Dittenbach and Dieter Merkl.
2001. ?Towards Automatic Content-Based Organi-
zation of Multilingual Digital Libraries: An English,
French, and German View of the Russian Infor-
mation Agency Novosti News?. Third All-Russian
Conference Digital Libraries: Advanced Methods
and Technologies, Digital Collections Petrozavodsk,
RCDI?2001.
van Rijsbergen, C.J. 1974. ?Foundations of evalua-
tion?. Journal of Documentation, 30 (1974), p. 365-
373.
Joaquin Silva, J. Mexia, Carlos Coelho and Gabriel
Lopes. 2004. ?A Statistical Approach for Multi-
lingual Document Clustering and Topic Extraction
form Clusters?. Pliska Studia Mathematica Bulgar-
ica, v.16,p. 207-228.
Ralf Steinberger, Bruno Pouliquen, and Johan Scheer.
2002. ?Cross-Lingual Document Similarity Cal-
culation Using the Multilingual Thesaurus EU-
ROVOC?. Computational Linguistics and Intelli-
gent Text Processing, CICling?02. Lecture Notes in
Computer Science, Springer-Verlag, p. 415-424.
Ralf Steinberger, Bruno Pouliquen, and Camelia Ignat.
2004. ?Exploiting multilingual nomenclatures and
language-independent text features as an interlingua
for cross-lingual text analysis applications?. Slove-
nian Language Technology Conference. Information
Society, SLTC 2004.
Vossen, P. 1998. ?Introduction to EuroWordNet?.
Computers and the Humanities Special Issue on Eu-
roWordNet.
1152
DTD-Driven Bilingual Document Generation 
...... ,Arantza Casillas . . . . .  
Departamento de Automgtica, Universidad e Alcalgt e -mai l  :arantza@aut,  a lca ia ,  es 
Joseba Abaitua 
Facultad de Filosofia y Letras Universidad e Deusto, Bilbao e -ami l :aba i tua0f i l  .deusto .  es 
Raque l  Mar t inez  
Departamento de  Sis. Informgticos y Programacidn, Facultad de MatemgLticas 
Universidad C0mplutense de Madrid e -mai l  : raquelOeucmos, sire. ucm. es 
Abst ract  
Extensively annotated bilingual parallel corpora 
can be exploited to feed editing tools that in- 
tegrate the processes of document composition 
and translation. Here we discuss the archi- 
tecture of an interactive diting tool that, on 
top of techniques common to most Translation 
Memory-based systems, applies the potential of 
SGML's DTDs to guide the process of bilingual 
document generation. Rather than employing 
just simple task-oriented mark-up, we selected a 
set of TEI's highly complex and versatile collec- 
tion of tags to help disclose the underlying log- 
ical structure of documents in the test-corpus. 
DTDs were automatically induced and later in- 
tegrated in the editing tool to provide the basic 
scheme for new documents. 
1 In t roduct ion  
This paper discusses an approach to the archi- 
tecture of an experimental interactive diting 
tool that integrates the processes of source doc- 
ument composition and translation i to the tar- 
get language. The tool has been conceived as an 
optimal solution for a particular case of bilin- 
gual production of legal documentation, but it 
also illustrates in a more general way how to ex- 
ploit the possibilities of SGML (ISO8879, 1986) 
used extensively to annotate a whole range of 
linguistic and extralinguistic information i  spe- 
cialized bilingual corpora. 
SGML is well established as the coding 
scheme underlying most Translation Memory 
based systems (TMBS), and has been pro- 
posed as the cod-it~g scheme for the interchange 
of existing Translation Memory databases 
Translation Meinories eXchange, TMX (Melby, 
1998). The advantages of SGML have also been 
perceived by a large conmmnity of corpus lin- 
guistics researchers, and big efforts have been 
made in the development of suitable markup 
options to encode a variety of textual types and 
functions -as clearly demonstrated by the Text 
Encoding Initiative, TEI; (Burnard & Speberg- 
MacQueen, 1995). While the tag-sets employed 
by TMBS are simple and task-oriented, TEI has 
offered a highly complex and versatile collection 
of tags. The guiding hypothesis in our experi- 
ment has been the idea that it is possible to 
explore TEI/SGML markup in order to develop 
a system that carries the concept of Translation 
Memory one step further. One important lea- 
ture of SGML is the DTD. DTDs determine the 
logical structure of documents and how to tag 
them accordingly. We have concentrated on the 
accurate description of documents by means of 
TEI conformant SGML markup. The markup 
will help disclose the underlying logical struc- 
ture of documents. From annotated ocumen- 
tation, DTDs can be induced and these DTDs 
provide the basic scheme to produce new doc- 
uments. We have collected a corpus of official 
publications from three main institutions in the 
Basque Autonomous Region in Spain, the Bo- 
letln Oficial de Bizkaia (BOB, 1990-1995), Bo- 
tetln Oficial de Alava (BOA, 1990-1994) and 
Bolet{n Oficial del Pais Vasco (BOPV, 1995). 
Documents in the corpus were composed by Ad- 
nfinistration clerks and translated by transla- 
tors. Both clerks and translators have been us- 
ing a wide variety of word-processors, although 
since 1994 MSWord has been generalized as the 
standard editing tool. Administrative docu- 
mentation shows a regular structure, and is rich 
? in*recurrent textual patterns.  For each docu- ..... 
ment type different document okens share a 
common global distribution of elements. Of- 
ficial document composers learn these global 
structures and apply them consistently. It is 
also the case that composers tend to reuse old 
32 
Document Type 
Orden Form 
Decreto..Foral =-- 
Resolucidn 
Extracto 
Acuerdo 
Norma Foral 
Anuncio 
% 
53% 
..22%,;- 
13% 
5.4% 
3.4% 
1.9% 
0.4% 
documents, where the whole document may 
be considered the translation unit. TM3 can 
.,-., : :- :...~ .~o. ~be~,g. o~strued,as~:~i:hiling~ai,doc,,~ent-database. 
Much redundancy originates from this TM col- 
lection, although it should be noticed that they 
are all by-products derived from the same an- 
notated bitext which subsumes them all. Good 
software packages for TM1 and TM3 already ex- 
ist in the market, and hence their exploitation is 
Table I: 
document files when producing new documents 
of the same type. Despite the fact that no 
SGML software was used at the editing phase, 
texts in the corpus show regular logical struc- 
tures and consistent distribution of text seg- 
ments. Our main goal in tagging the corpus was 
to make all them explicit (Martinez, 1997). The 
most common type of document in the corpus,  
the Orden Foral, was chosen (see Table 1). We 
analysed some 100 tokens and hand-marked the 
most salient elements. The heuristics to identify 
these elements were later expressed in a collec- 
tion of recognition routines in Perl and tested 
against a set of 400 tokens, including the initial 
100. As a result of this process of automatic 
tagging of structural elements we produced a 
TE I /SGML tagged corpus with yet no corre- 
sponding overt DTD. In  Section 2 we will ex- 
plain how DTDs were later induced from the 
tagged corpus. 
Once the corpus was segmented the next step 
was to align it. This was conducted at different 
levels: general document elements (DIV, SEG, 
P), as well as sentential and intra-sentential e - 
ements, such as S, ItS, NUM, DATE, etc. (Mar- 
tinez, 1998b). Aligned in this way, the corpus 
becomes an important resource for translation. 
Four complementary language databases may 
be obtained at any time from the annotated 
corpus: three translation memory databases 
(TM1, TM2, and TM3) as well as a terminology 
database (termbase). The three TMs differ in 
the nature of the translation units they contain. 
TM1 consists of aligned sentences than can feed 
commercial TM software. TM2 contains ele- 
ments which are translation segments ranging 
from whole sections of a document or multi- 
sentence paragraphs to smaller units, such as 
short phrases or proper names. TM3 simply 
hosts the whole collection of aligned bilingual 
Types of documents in the corpus beyond our interest (Trados Translator's Work- 
. . . . .  . bench, Star!s Transit,.,SDLX, D e'j?~fi,. IBM~s. 
browsing tool for TM3). The originality of our 
editing tool lies in a design which benefits from 
joining the potentiality of DTDs  and the ele- 
ments in TM2, as will be shown in sections 4 
and 5. 
2 DTD abst rac t ion  
SGML mark-up determines the logical structure 
of a document and its syntax in the form of a 
context-free grammar. This is called the Doc- 
ument Type Definition (DTD) and it contains 
specifications for: 
? Names and content for all elements that are 
permitted to appear in a document. 
o Order in which these elements must ap- 
pear. 
o Tag attributes with default values for those 
elements. 
DTDs have been abstracted away from the an- 
notations that were automatically introduced 
in the corpus. Similar experiments have been 
reported before in the literature. (Ahonen, 
1995) uses a method to build document in- 
stances from tagged texts that consists of a de- 
terministic finite automaton for each context 
model. Subsequently, these automata re gen- 
eralized and converted into regular expressions 
which are easily transcribed into SGML content 
models. (Shafer, 1995) combines docmnent in- 
stances with simplification rules. Our method 
is similar to  Sharer's, but .with a.modification 
in the way rules reduce document instances. A 
tool to obtain a DTD for all document instances 
has been developed (Casillas, 1999). Given that 
source and target documents how some syn- 
tactic and structural mismatches, two different 
DTDs are induced, one for each language, and 
33 
Spanish Text: 
<div0> 
<div l> ... < /d iv l>  
<seg9 id=9ES2 corresp=gEU2> Contra dicha 
<rs  type=law id=LES12 corresp=LEU10> 
Orden Foral </ rs>,  que agota la vfa ad- 
ministrativa podr~i interponerse recurso 
contencioso-administrativo ante la <rs  
type=organization id=0ES9 corresp=0EUl i> 
Sala de lo Contencioso-Administrativo del Tri- 
? bunal Superior de J usticia del Pais Vasco </ rs  >, 
en el plazo de dos meses, contado desde el d/a 
Basque Text: 
<div0> 
<divl> ... </divl> 
<seg9 i de9EU2 correspe9ES2> <rs type=law 
id=LEUi0 corresp=LESl2> Foru agindu </rs> 
horrek amaiera eman dio administrazio bideari; 
eta beraren aurka <rs type=organizat ion 
id=0EU10> Administrazioarekiko </ rs> 
auzibide-errekurtsoa jarri ahal izango zaio <rs 
type=organization id=0EUll corresp=0ES9> 
Euskal Herriko JustiziAuzitegi Nagusiko Admin- 
istrazioarekiko Auzibideetarako Salari < / rs>,  
siguiente a esta~:m~t.~eaci~m~.sin~p~er~ui~i~deAu~,.~;.~aila~aetek~:~epea~;~4ja~d~mxazpen ~hatl egiten 
utilizacidn de otros medios de defensa que estime den egunaren biharamunetik zenbatuko da epe 
oportunos. </seg9> hori; halo eta guztiz ere, egokiesten diren beste 
defentsabideak ere erabil litezke. </seg9> 
<segl0 id=10ES1 corresp=10EUl> Du- 
rante el referido plazo el expediente BHI-<num 
num=10094> 100/94 </num>-P05-A quedar? de 
manifiesto para su ex~imen en las dependencias 
de <rs type=place id=PES3 corresp=PEU2> 
Bilbao calle Alameda Rekalde </ rs>,  <num 
num=30> 30 </num>, <num num=5> 5.a </num> 
y <hum hum=6> 6.a </hUm> plantas. </segl0> 
</div0> 
<closer id=pESl3 corresp=pEUl3 > <name> 
El Diputado Foral de Urbanismo Pedro Hern?ndez 
Gonz~ilez. </name> </closer> 
<segl0 id=10EU1 corresp=10ESl> Epe hori 
amaitu arte BHI-<num num=10094> 100/94 
</num>-P05-A espedientea gerian egongo da, 
nahi duenak azter dezan, <rs  type=place 
id=PEU2 corresp=PES3> Bilboko Errekalde zu- 
markaleko </ rs> <num num=30> 30.eko </num> 
bulegoetan, <num num=5> 5 </num> eta <num 
num=6> 6.</num> solairuetan. </seg l0> 
</div0> 
<closer  id=pEU13 corresp=pESl3> <name> 
Hirigintzako foru diputatua. Pedro Hern/mdez 
Gonz?1ez. </name> </c loser> 
Figure 1: Ilustrates a sample of the annotated bitext 
are paired through a correspondence table. Cor- 
respondences in this table can be up-dated, or 
deleted. At present, we have six DTDs, one for 
each document ype in each language (there are 
three document ypes; Figure 2 shows a part of 
one of these DTDs). By means of these paired 
DTDs, document elements in each language are 
appropriately placed. In the process of gener- 
ating the bilingual document, a document ype 
must first be selected. Each document ype has 
an associated DTD. This DTD specifies which 
elements are obhgatory and.which are optional. 
With the aid of the DTD, the source document 
is generated. The target document will be gen- 
erated with .the aid of the com~esponding target 
DTD. 
3 Jo in ing  TM2 and  DTD 
TM2 specifically stores a type of translation 
segment class, which we have tagged <seg l>,  
<seg2>... <segn>, <t i t le> and <rs>,  and 
which is relevant o the DTD. Segments tagged 
<segn> are variable recurrent language pat- 
terns very frequent in the specialized domain 
of the corpus and whose occurrence in the text 
is well established. These <segn> tags in- 
clude two attributes: id  and cor respond which 
locate the aligned segment both in the cor- 
pus and in the database (Figure 1). Seg- 
ments tagged <rs> are referring expressions 
which have been recognized, tagged and aligned 
? and which correspond largely to proper names 
(Martinez, 1998a), (Martinez, 1998b). TM2 is 
managed in tile form of a relational database 
-where segments are stored, as records. .Each 
record in the database consists of four fields: 
the segment string, a counter for the occur- 
rences of that string in the corpus, the tag 
and the attributes (type, id and corresp) .  
Table 2 shows how the text fragment inside 
34 
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
LEGE - - (TEXT)> 
TEXT - - (BODY)> 
BODY - - (OPENER,  DIVO,  CLOSER)> 
0PENER - - (TITLE, NUM, DATE, NAME?, SEGI)> 
SEGI - - (SEGIi, (#PCDATAIRSIDATEINUM)+)> 
(SEGii, NUM, DATE, RS, NAME, TITLE) (#PCDATA)> 
<!ELEMENT 
\ [SEGI5)+,  
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
<!ELEMENT 
(DIVO) - - ( (#PCDATA \[RS INUM \[DATE ISEG4\[SEGS ISEG6\[SEG7 ISEG8 ISEGI2 \[SEGi4 
SEG9?, SEGIO?)> 
(SEG4, SEG5, SEG6) (#PCDATA) > 
(SEG9, SEGiO, SEGT, SEG8, SEGi2, SEGi4, SEGi5) - - (#PCDATA\[RS\[DATE\[NUM)+> 
(CLOSER) i i (PLACENAME?,DATE? ,  NAME?)> 
(PLACENAME)  - : (RS)> 
<!ATTLIST RS TYPE (0RGANIZATION\[ LAW\[ PLACE\[ UNCAT) UNCAT> 
Figure 2: Part of the DTD of the type document Orden Foral 
the </d iv l>. . .</d iv0> tags of Figure 1 ren- 
ders three records in the database. Note how 
the content of the string field in the database 
maintains only the initial <segn> and <rs> 
tags. Furthermore, <rs> tagged segments in- 
side <segn> records are simplified so that their 
content is dismissed and only the initial tag is 
kept (Lange et al, 1997). The reason is that 
they are considered variable elements within 
the segment (dates and numbers are also these 
type of elements). The strings Orden Foral of 
record 2 marked as <rs  type=law> and Sala 
de lo Contencioso-Administrativo del Tribunal 
Superior de Justicia del Pais Vasco of record 
3 <rs  type=organ izat ion> are thus not in- 
cluded in record 1 <segg>,  since they may dif- 
fer in other instantiations of the segment. These 
internal elements are largely proper names that 
vary from one instantiation of the segment o 
another. The <rs> tag can be considered 
to be the name of the varying element. The 
value of the type attribute <rs  type=law> 
constraints the kind of referential expression 
that may be inserted in that point of the trans- 
lation segment. Table 2 shows that source 
and target records may not have straight one- 
to-one correspondences. Although this is by 
no means the general:case; only about 5.61%, 
(Martinez, 1998a), such one-to-N correspon- 
dences provide good ground to explain how 
the TM2 is designed. The asymmetry can be 
easily explained. The Spanish term recurso 
contencioso-administrativo has been translated 
into Basque by means of a category changing 
operation, where the Spanish adjective admin- 
istrativo has been translated as a Basque noun 
complement Administrazioarekiko which liter- 
ally means "Administration-the-with-of' trig- 
gering its identification as a proper noun. 
Table 3 shows the way in which source lan- 
guage units are related with their correspond- 
ing target units, which, as can be observed, can 
be one-to-one or one-to-N. This means that one 
source element can have more than one transla- 
tion. 
TM2 is created in three steps: 
? First, non-pertinent ags are filtered out 
from the annotated corpus. Tags marking 
sentence <s> and paragraph <p> align- 
ment are removed because they are of no 
interest for TM2 'recall that they are reg- 
istered in TM1). 
? Second, translation segments <segn>,  
<t i t le> phrases and referential expres- 
sions <rs> are detected in the source doc- 
ument and looked up in the database. 
o Third, if they are not already present in 
the database, they are stored each in its 
database.and values of the id  and  cor resp  
attributes-are~used to set the correspon- 
dence between source and target database. 
4 Compos i t ion  S t ra tegy  
Every phase in tile process is guided by the 
markup contained in TM2 and the paired DTDs 
35 
Spanish Unit 
<seg9> Contra dicha <rs type=law>, 
que agota la via 
administrativa podr~i interponerse r curso 
contencioso-administrativo nte 
la <rs type=organization>, 
en el plazo de dos meses, contado 
desde el dla siguiente 
a esta notificacidn, sin perjuicio 
de la utilizaci6n 
Basque Unit 
<seg9> <rs type=law> 
horrek amaiera eman dio 
? - adrrrinistrazio'bideari; eta:beraren aurka: " 
<rs type=organization> 
auzibide-errekurtsoa jarri 
ahal izango zaio 
<rs type=organization>, 
bi hilabeteko epean; 
jakinarazpen hau egiten den egunaren 
de otros medios de. defensa que estime oportunos, biharamunetik zenbatuko da epe hori; 
que estime oportunos, hala eta guztiz ere, egokiesten diren beste 
. . . . . . . . . . . .  '-. . -. .~' ?.~ ..... :z-:: : :~ : : :de femVs~ideate .~ere :erab i t~htezke~, . . . -  - . . . .  
<rs type=law> Orden Foral <rs type=law> Foru agindu 
<rs type=organization> Administrazioarekiko 
<rs type=organizat ion>-S~ de lo 
Contencioso-Administrativo 
del Tribunal Superior de Justicia del Pals Vasco 
<rs type=organization> Euskal 
Herriko Justizi 
Auzitegi Nagusiko Administrazioarekiko 
Auzibideetarako Salari 
Table 2: Source and targe language record samples in TM2 
Spanish Unit Basque Unit 
<rs type=organization id= corresp=> 
Bolet/n Oficial de Bizkaia 
<rs type=organization id= corresp=> 
Bizkaiko Aldizkari Ofizialea 
<rs  type=organization id= orresp=> 
Bizkaiko Engunkari Ofizialea 
<rs  type=organization id= corresp=> 
Bizkaiko Boletin Ofizialea 
<seg3> dispongo <seg3> xedatu dut 
<seg3> xedatzen duen 
Table 3: Source language units related with their corresponding target language units 
which control the application of this markup. 
The composition process follows two main steps 
which correspond to the traditional source doc- 
ument generation and translation into the tar- 
get document. The markup and the paired 
DTD guides the process in the following man- 
ner: 
1. Before the user starts writing the source 
document, he must select a document type, 
i.e., a DTD. This has two consequences. On 
the one hand, the selected DTD produces 
a source document template that contains 
the logical structure of the document and 
some of its contents. On the other hand, 
the selected source DTD trigger:s .a target 
paired DTD, which will be used later to 
translate the document. There are three 
different types of elements in the source 
document template: 
? Some elements are mandatory and are 
. :  A . . 
provided to the user, who must only 
choose its content among some alter- 
native usages (s/he will get a list of 
alternatives ordered by frequency, for 
example <t i t le>) .  Other obligatory 
elements, such as dates and numbers, 
will also be automatically generated. 
o Some other elements in the template 
are optional (e.g., <seg9>).  Again, 
a list of alternatives will be offered to 
the user. These optional elements are 
.sensitive to the .context (document or 
division type), and markup is also re- 
sponsible for constraining the valid op- 
- ~ tion.s.g:iverlTtQ,the user:. Obligatory and 
optional elements are retrieved from 
TM2, and make a considerable part of 
the source document. 
. All documents have an important part 
of their content which is not deter- 
36 
Word/doc. 'Num.  doc. TM2 
0-500 378 34.91 
500-1,000 -25 . . . .  .M:0t - 
More 1,000 16 3.01 
Weighted mean 31.8 
Table 4: % generated by TM2 
mined by the DTD (<d iv l>) .  It is the 
most variable part, and .the system lets 
the writer input text freely. It is when 
TM2 has nothing to offer that TM1 
and TM3 may provide useful material. 
Given the recurrent style of legal doc- 
umentation, it is quite likely that the 
user will be using many of the bilin- 
gual text choices already aligned and 
available in TM1 and TM3. 
2. Once the source document has been com- 
pleted, the system derives its particular 
logical structure, which, with the aid of the 
target DTD, is projected into the resulting 
target logical structure. 
5 Eva luat ion  
Table 4 shows the number of words that make 
up the segments stored in TM2 from the source 
documents. There is a line for each document 
size considered. We can see that the average 
of segments contained in TM2 is 31.8%, on a 
scale from 34.91% to only 3.01%. The amount 
of segments dealt with in this way largely de- 
pends on the size of the document. Short doc- 
uments (90.21) have about 35% of their text 
composed in this way. This figure goes down to 
3% in documents larger than 1,000 words. This 
is understandable, in the sense that the larger 
the document, the smaller proportion of fixed 
sections it will contain. 
Table 5. shows the Immber of words that are 
proposed for the target document. These trans- 
lations are obtained from what is stored in TM2 
complemented by algorithms designed to trans- 
late dates and numbers. We can see that the 
average of document ranslated is 34%. Short 
documents have 36% of their text translated. 
falling to above 11% in t, he case of large docu- 
I l l ents .  
37 
Word/doc. Num. doc. TM2 Alg. Total 
0-500 378 28.3 7.7 36 
~,500-1;000 25 ' :.: 12.3  . '9.6 ? -.21-3'. 
More 1,000 16 4.7 !6 .41  10.7 
i i 
W.M.  26.5 ' 7.6 I 34.2 
Table 5: % translated by TM2 and algorithms 
6 Conc lus ions  
We have shown how7 DTDs derived from de- 
? scriptive markup can~be"employed to ease the  
process of generating bilingual dedicated ocu- 
mentation. On average, one third of the con- 
tents of thedocuments can be automatically ac- 
counted for. It must also be pointed out that 
the part being dealt with represents the core 
structure, lay-out and logical components of the 
text. The remaining two-thirds of untreated 
document can still be managed with the aid 
of sentence-oriented TMBS, filling in the gaps 
in the ore/all skeleton provided by the target 
template. Composers may also browse TM3 to 
retrieve whole blocks for those parts which are 
not determined by the DTD. One of the clear 
targets for the future is to extend the cover- 
age of the corpus and to test structural taggers 
against other document types. A big challenge 
we face is to develop tools that automatically 
perform the recognition of documents from less 
restricted and more open text types. However, 
we are not sure of the extent of the practicality 
of such an approach. An alternative direction 
we are presently considering is to establish a 
collection of pre-defined ocument types, which 
would be validated by the institutional writers 
themselves. It is a process currently being im- 
plemented in the Basque administration to de- 
fine docmnent models for writers and transla- 
tors to follow. What we have demonstrated is 
that paired DTDs, complemented with rich lan- 
guage resources of the kind defined in this pa- 
per, allow for the design of optimal editing envi- 
ronments which would combine both document 
composition and translation as one single pro- 
cess. All the resourcesneeded (DTDs.and TMs) . 
can be induced from an aligned corpus. 
7 Acknowledgements  
This research is being partially supported by the 
Spanish Research Agency, project ITEM, TIC- 
96-1243-C03-01. 
References  
H. Ahonen. Automatic Generation of SGML 
:Content Models. Electronic :Publishing, 8(2- 
3):195-206, 1995. 
L. Burnard, C. Speberg-McQueen. TEILite: 
An Introduction to Text Encoding 
for Interchange. URL://http://www- 
tel. uic. edu/orgs/tei/intros/teiu5, tei, 1995. 
Casillas A., Abaitua J., Martinez R. Extracci6n 
y aprovechamiento de DTDs emparejadas en 
. . . . . . . . . . . . .  corpus paralelos. Proceesamientq: deL!~enguaje 
Natural, 25:33-41, 1999. 
ISO 8879, Information Processing-Text and Of- 
fice Systems-Standard Generalized Markup 
Language (SGML). International Organiza- 
tion For Standards, 1986, Geneva. 
J. Lang6, I~ Gaussier, B. Daile. Bricks and 
Skeletons: Some Ideas for the Near Future of 
MATH. Machine Translation, 12:39-51, 1997. 
Martinez R., Abaitua J., Casillas R. Bilingual 
parallel text segmentation and tagging for 
specialized ocumentation. Proceedings of the 
International Conference Recent Advances in 
Natural Language Processing (RANLP'97), 
369-372, 1997. 
Martinez R., Abaitua J., Casillas A.. Bi- 
text Correspondences through Rich Mark- 
up. 36th Annual Meeting of the Association 
for Computational Linguistics abd 17 Inter- 
national Conference on Computational Lin- 
guistics (COLING-ACL'98), 812-818, 1998. 
Martinez R., Abaitua J., Casillas A.. Aligning 
tagged bitexts. Sixth Workshop on Very Large 
Corpora, 102-109, 1998. 
A. Melby. Data Exchange from OSCAR and 
MARTIF Projects. First International con- 
ference on Language Resources ~4 Evaluation, 
3-7, 1998. 
38 
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 361?365,
Dublin, Ireland, August 23-24, 2014.
IxaMed: Applying Freeling and a Perceptron Sequential Tagger at the
Shared Task on Analyzing Clinical Texts
Koldo Gojenola, Maite Oronoz, Alicia P
?
erez, Arantza Casillas
IXA Taldea (UPV-EHU)
maite.oronoz@ehu.es
http://ixa.si.ehu.es
Abstract
This paper presents the results of the Ix-
aMed team at the SemEval-2014 Shared
Task 7 on Analyzing Clinical Texts.
We have developed three different sys-
tems based on: a) exact match, b) a
general-purpose morphosyntactic analyzer
enriched with the SNOMED CT termi-
nology content, and c) a perceptron se-
quential tagger based on a Global Linear
Model. The three individual systems re-
sult in similar f-score while they vary in
their precision and recall. We have also
tried direct combinations of the individual
systems, obtaining considerable improve-
ments in performance.
1 Introduction
This paper presents the results of the IxaMed team.
The task is focused on the identification (Task A)
and normalization (Task B) of diseases and disor-
ders in clinical reports.
We have developed three different systems
based on: a) exact match, b) a general-
purpose morphosyntactic analyzer enriched with
the SNOMED CT terminology content, and c) a
perceptron sequential tagger based on a Global
Linear Model. The first system can be seen as
a baseline that can be compared with other ap-
proaches, while the other two represent two alter-
native approaches based on knowledge organized
in dictionaries/ontologies and machine learning,
respectively. We also tried direct combinations of
the individual systems, obtaining considerable im-
provements in performance.
These approaches are representative of different
solutions that have been proposed in the literature
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
(Pradhan et al., 2013), which can be broadly clas-
sified in the following types:
? Knowledge-based. This approach makes use
of large-scale dictionaries and ontologies,
that are sometimes integrated in general tools
adapted to the clinical domain, as MetaMap
(Aronson and Lang, 2010) and cTAKES (Xia
et al., 2013).
? Rule-based. For example, in (Wang and
Akella, 2013) the authors show the use
of a rule-based approach on the output of
MetaMap.
? Statistical techniques. These systems take a
training set as input and apply different vari-
ants of machine learning, such as sequen-
tial taggers based on hidden Markov mod-
els (HMMs) or conditional random fields
(CRFs) (Zuccon et al., 2013; Bodnari et al.,
2013; Gung, 2013; Hervas et al., 2013; Lea-
man et al., 2013).
? Combinations. These approaches try to take
the advantages of different system types, us-
ing methods such as voting or metaclassi-
fiers (Liu et al., 2013).
In the rest of the paper, we will first introduce
the different systems that we have developed in
section 2, presenting the main results in section 3,
and ending with the main conclusions.
2 System Description
The task of detecting diseases and their corre-
sponding concept unique identifiers (CUI) has
been faced using three methods that are described
in the following subsections.
2.1 Exact Match
The system based on Exact Match (EM) simply
obtained a list of terms and their corresponding
361
CUI identifier from the training set and marked
any appearance of those terms in the evaluation
set. This simple method was improved with some
additional extensions:
? Improving precision. In order to reduce the
number of false positives (FP), we applied
first the EM system to the training set it-
self. This process helped to measure FPs,
for example, blood gave 184 FPs and 2 true
positives (TPs). For the sake of not hurting
the recall, we allowed the system to detect
only those terms where TP > FP , that is,
?blood? would not be classified as disorder.
? Treatment of discontinuous terms. For
these terms, our system performed a soft-
matching comparison allowing a limited vari-
ation for the text comprised between the
term elements (for example ?right atrium is
mildly/moderately dilated?). These patterns
were tuned manually.
2.2 Adapting Freeling to the Medical Domain
Freeling is an open-source multilingual language
processing library providing a wide range of ana-
lyzers for several languages (Padr?o et al., 2010),
Spanish and English among others. We had al-
ready adapted Freeling to the medical domain in
Spanish (Oronoz et al., 2013), so we used our pre-
vious experience to adapt the English version to
the same domain. For the sake of clarity, we will
refer to this system as FreeMed henceforth.
The linguistic resources (lexica, grammars,. . . )
in Freeling can be modified, so we took advantage
of this flexibility extending two standard Freel-
ing dictionaries: a basic dictionary of terms con-
sisting of a unique word, and a multiword-term
dictionary. Both of them were enriched with a
dictionary of medical abbreviations
1
and with the
Systematized Nomenclature of Medicine Clinical
Terms (SNOMED CT) version dated 31st of July
of 2013. In addition to the changes in the lexica,
we added regular expressions in the tokenizer to
recognize medical terms as ?Alzheimer?s disease?
as a unique term.
In our approach, the system distinguishes be-
tween morphology and syntax on one side and
semantics on the other side. First, on the mor-
phosyntactic processing, our system only catego-
rizes word-forms using their basic part-of-speech
1
http://www.jdmd.com/abbreviations-glossary.asp
(POS) categories. Next, the semantic distinctions
are applied (the identification of the term as sub-
stance, disorder, procedure,. . . ). Following this
approach, whenever the specific term on the new
domain (biomedicine in this case) was already in
Freeling?s standard dictionaries, the specific en-
tries will not be added to the lexicon. Instead,
medical meanings are added in a later semantic
tagging stage. For example: the widely used term
?fever?, as common noun, was not added to the
lexicon but its semantic class is given in a sec-
ond stage. Only very specific terms not appear-
ing in the lexica as, for instance, ?diskospondyli-
tis? were inserted. This solution helps to avoid
an explosion of ambiguity in the morphosyntactic
analysis and, besides, it enables a clear separation
between morphosyntax and semantics.
In figure 1 the results of both levels of anal-
ysis, morphosyntactic and semantic, are shown.
The linguistic and medical information of medical
texts is stored in the Kyoto Annotation Format or
KAF (Bosma et al., 2009) that is based in the eX-
tended Markup Language (XML). In this example
the term aneurysm is analyzed as NN (meaning
noun) and it is semantically categorized as mor-
phological abnormality and disorder.
SNOMED CT is part of the Metathesaurus,
one of the elements of the Unified Medical Lan-
guage System (UMLS). We used the Metathe-
saurus vocabulary database to extract the map-
ping between SNOMED CT?s concept identifiers
and their corresponding UMLS?s concept unique
identifier (CUI). All the medical terms appearing
in SNOMED CT and analyzed with FreeMed are
tagged with both identifiers. For instance, the term
aneurysm in figure 1 has the 85659009 SNOMED
CT identifier when the term is classified in the
morphological abnormality content hierarchy and
the 432119003 identifier as disorder. Both are
linked to the same concept identifier, C0002940,
in UMLS. This mapping has been used for Task
B, whenever the CUI is the same in all the analy-
sis of the same term.
All the terms from all the 19 content hierarchies
of SNOMED CT were tagged with semantic infor-
mation in the provided texts.
The training corpus was linguistically analyzed
and its format was changed from XML to the for-
mat specified at the shared task. After a manual
inspection of the results and the Gold Standard,
some selection of terms was performed:
362
<term tid=?t241? lemma=?aneurysm? pos=?NN?>
<extRefs>
<extRef resource=?SCT 20130731? reference=?85659009?
reftype=?morphologic abnormality? >
<extRef resource=?UMLS-2010AB? reference=?C0002940?/ >
</extRef>
<extRef resource=?SCT 20130731? reference=?432119003?
reftype=?disorder? >
<extRef resource=?UMLS-2010AB? reference=?C0002940?/>
</extRef>
</extRefs>
</term>
Figure 1: Analysis with augmented information.
? Selection and combination of semantic
classes. All the terms from the disor-
der semantic class (for example ?Hypothy-
roidism?) and from the finding class (for in-
stance ?headache?) are chosen, as well as
some tag combinations (see figure 1). After
analyzing the train corpus we decided to join
into a unique term a body structure immedi-
ately followed by a disorder/finding. In this
way, we identify terms as ?MCA aneurysm?
that are composed of the MCA abbreviation
(meaning ?middle cerebral artery?) and the
inmediately following ?aneurysm? disorder.
? Filtering. Not all the terms from the men-
tioned SNOMED CT hierarchies are identi-
fied as disorders in the Gold Standard. Some
terms are discarded following these criteria:
i) findings describing personal situations (e.g.
?alcoholic?), ii) findings describing current
situations (e.g. ?awake?), iii) findings with
words indicating a negation or normal situ-
ation (e.g. ?stable blood pressure?) and iv)
too general terms (e.g. ?problems?).
The medical terms indicating disorders that are
linked to more than one CUI identifier, were
tagged as CUI-less. That is, we did not perform
any CUI disambiguation.
In subsequent iterations and after analyzing our
misses, new terms and term variations (Hina et
al., 2013) are added to the lexica in Freeling with
the restriction that, at least, one synonym should
appear in SNOMED CT. Thus, equivalent forms
were created for all the terms indicating a cancer,
a tumor, a syndrome, or a specific disease. For in-
stance, variants for the term ?cancer of colon? and
with the same SNOMED CT concept identifier
(number 363406005) are created with the forms
?colon cancer?, ?cancer of the colon? and ?can-
cer in colon?. Some abbreviation variations found
in the Gold Standard are added in the lexica too,
following the same criteria.
2.3 Perceptron Sequential Tagger
This system uses a Global Linear Model (GLM),
a sequential tagger using the perceptron algorithm
(Collins, 2002), that relies on Viterbi decoding of
training examples combined with simple additive
updates. The algorithm is competitive to other op-
tions such as maximum-entropy taggers or CRFs.
The original textual files are firstly processed by
FreeMed, and then the tagger uses all the available
information to assign tags to the text. Each token
contains information about the word form, lemma,
part of speech, and SNOMED CT category.
Our GLM system only deals with Task A, and
it will not tackle the problem of concept normal-
ization, due to time constraints. In this respect, for
Task B the GLM system will simply return the first
SNOMED CT category given by FreeMed. This
does not mean that GLM and FreeMed will give
the same result for Task B, as the GLM system
first categorizes each element as a disease, and it
gives a CUI only when that element is identified.
2.4 Combinations
The previous subsections presented three differ-
ent approaches to the problem that obtain com-
parable scores (see table 1). In the area of auto-
matic tagging, there are several works that com-
bine disparate systems, usually getting good re-
sults. For this reason, we tried the simplest ap-
proach of merging the outputs of the three individ-
ual systems into a single file.
3 Results
Table 1 presents the results of the individual and
combined systems on the development set. Look-
ing at the individual systems on Task A, we can see
that all of them obtain a similar f-score, although
there are important differences in terms of preci-
sion and recall. Contrary to our initial intuition,
the FreeMed system, based on dictionaries and on-
tologies, gives the best precision and the lowest re-
call. In principle, having SNOMED CT as a base,
we could expect that the coverage would be more
complete (attaining the highest recall). However,
the results show that there is a gap between the
writing of the standard SNOMED CT terms and
the terms written by doctors in their notes. On the
other hand, the sequential tagger gives the best re-
363
Task A Task B
Strict Relaxed Strict Relaxed
System Precision Recall F-Score Precision Recall F-Score Accuracy
INDIVIDUAL SYSTEMS
Exact Match (EM) 0.804 0.505 0.620 0.958 0.604 0.740 0.479 0.948
FreeMed 0.822 0.501 0.622 0.947 0.578 0.718 0.240 0.479
GLM 0.715 0.570 0.634 0.908 0.735 0.813 0.298 0.522
COMBINATIONS
FreeMed + EM 0.766 0.652 0.704 0.936 0.754 0.835 0.556 0.855
FreeMed + GLM 0.689 0.668 0.678 0.903 0.790 0.843 0.345 0.518
EM + GLM 0.680 0.679 0.679 0.907 0.819 0.861 0.398 0.598
FreeMed + EM + GLM 0.659 0.724 0.690 0.899 0.845 0.871 0.421 0.584
Table 1: Results of the different systems on the development set.
Task A Task B
Strict Relaxed Strict Relaxed
System Precision Recall F-Score Precision Recall F-Score Accuracy
FreeMed + EM 0.729 0.701 0.715 0.885 0.808 0.845 0.604 0.862
FreeMed + EM + GLM 0.681 0.786 0.730 0.872 0.890 0.881 0.439 0.558
Best system 0.843 0.786 0.813 0.936 0.866 0.900 0.741 0.873
Table 2: Results on the test set.
call. Since the tagger uses both contextual words
and prefixes and suffixes as features for learning,
this method has proven helpful for the recognition
of terms that do not appear in the training data (see
the difference with the EM approach).
Looking at the different combinations in table 1,
we see that two approaches work best, either com-
bining FreeMed and EM, or combining the three
individual systems. The inclusion of GLM results
in the best coverage, but at the expense of preci-
sion. On the other hand, combining FreeMed and
EM gives a better precision but lower coverage.
As pointed out by Collins (2002), the results of
the perceptron tagger are competitive with respect
to other statistical approaches such as CRFs (Zuc-
con et al., 2013; Bodnari et al., 2013; Gung, 2013;
Hervas et al., 2013; Leaman et al., 2013).
Regarding Task B, we can see that the EM sys-
tem is by far the most accurate, while FreeMed
is well below its a priori potential. The reason of
this low result is mainly due to the high ambiguity
found on the output of the SNOMED CT tagger, as
many terms are associated with more than one CUI
and, consequently, are left untagged. This problem
deserves future work on automatic semantic dis-
ambiguation. On the combinations, FreeMed and
EM together give the best result. However, as we
told before, the GLM system was only trained for
Task A, so it is not surprising to see that its results
deteriorate the accuracy in Task B.
We chose these best two combinations for the
evaluation on the test set (using training and de-
velopment for experimentation or training), which
are presented in table 2. Here we can see that re-
sults on the development also hold on the test set.
Given the unsophisticated approach to combine
the systems, we can figure out more elaborated so-
lutions, such as majority or weighted voting, or
even more, the definition of a machine learning
classifier to select the best system for every pro-
posed term. These ideas are left for future work.
4 Conclusions
We have presented the IxaMed approach, com-
posed of three systems that are based on exact
match, linguistic and knowledge repositories, and
a statistical tagger, respectively. The results of in-
dividual systems are comparable, with differences
in precision and recall. We also tested a sim-
ple combination of the systems, which proved to
give significant improvements over each individ-
ual system. The results are competitive, although
still far from the winning system.
For future work, we plan to further improve the
individual systems. Besides, we hope that the ex-
perimentation with new combination approaches
will offer room for improvement.
Acknowledgements
This work was partially supported by the Euro-
pean Commission (325099 and SEP-210087649),
the Spanish Ministry of Science and Innovation
(TIN2012-38584-C06-02) and the Industry of the
Basque Government (IT344-10).
364
References
Alan R Aronson and Francois-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. Journal of the American Medical In-
formatics Association (JAMIA), 17:229?236.
Andreea Bodnari, Louise Deleger, Thomas Lavergne,
Aurelie Neveol, and Pierre Zweigenbaum. 2013.
A Supervised Named-Entity Extraction System for
Medical Text. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Wauter Bosma, Piek Vossen, Aitor Soroa, German
Rigau, Maurizio Tesconi, Andrea Marchetti, Mon-
ica Monachini, and Carlo Aliprandi. 2009. KAF: a
Generic Semantic Annotation Format. In Proceed-
ings of the 5th International Conference on Gener-
ative Approaches to the Lexicon GL, pages 17?19,
Septembre.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Ex-
periments with Perceptron Algorithms. In Proceed-
ings of the 2002 Conference on Empirical Methods
in Natural Language Processing, pages 1?8. Asso-
ciation for Computational Linguistics, July.
James Gung. 2013. Using Relations for Identification
and Normalization of Disorders: Team CLEAR in
the ShARe/CLEF 2013 eHealth Evaluation Lab. In
Online Working Notes of the CLEF 2013 Evaluation
Labs and Workshop, September.
Lucia Hervas, Victor Martinez, Irene Sanchez, and Al-
berto Diaz. 2013. UCM at CLEF eHealth 2013
Shared Task1. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Saman Hina, Eric Atwell, and Owen Johnson. 2013.
SnoMedTagger: A semantic tagger for medical nar-
ratives. In Conference on Intelligent Text Processing
and Computational Linguistics (CICLING).
Robert Leaman, Ritu Khare, and Zhiyong Lu. 2013.
NCBI at 2013 ShARe/CLEF eHealth Shared Task:
Disorder Normalization in Clinical Notes with
Dnorm. In Online Working Notes of the CLEF 2013
Evaluation Labs and Workshop, September.
Hongfang Liu, Kavishwar Wagholikar, Siddhartha Jon-
nalagadda, and Sunghwan Sohn. 2013. Integrated
cTAKES for Concept Mention Detection and Nor-
malization. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Maite Oronoz, Arantza Casillas, Koldo Gojenola, and
Alicia Perez. 2013. Automatic Annotation of
Medical Records in Spanish with Disease, Drug
and Substance Names. In Lecture Notes in Com-
puter Science, 8259. Progress in Pattern Recogni-
tion, ImageAnalysis, ComputerVision, and Applica-
tions 18th Iberoamerican Congress, CIARP 2013,
Havana, Cuba, November 20-23.
Lluis Padr?o, Samuel Reese, Eneko Agirre, and Aitor
Soroa. 2010. Semantic Services in Freeling 2.1:
WordNet and UKB. In Global Wordnet Conference,
Mumbai, India.
Sameer Pradhan, Noemie Elhadad, Brett R. South,
David Martinez, Lee Christensen, Amy Vogel,
Hanna Suominen, Wendy W. Chapman, and Guer-
gana Savova. 2013. Task 1: ShARe/CLEF eHealth
Evaluation Lab 2013. In Online Working Notes
of the CLEF 2013 Evaluation Labs and Workshop,
September.
Chunye Wang and Ramakrishna Akella. 2013. UCSCs
System for CLEF eHealth 2013 Task 1. In Online
Working Notes of the CLEF 2013 Evaluation Labs
and Workshop, September.
Yunqing Xia, Xiaoshi Zhong, Peng Liu, Cheng Tan,
Sen Na, Qinan Hu, and Yaohai Huang. 2013. Com-
bining MetaMap and cTAKES in Disorder Recogni-
tion: THCIB at CLEF eHealth Lab 2013 Task 1. In
Online Working Notes of the CLEF 2013 Evaluation
Labs and Workshop, September.
Guido Zuccon, Alexander Holloway, Bevan Koop-
man, and Anthony Nguyen. 2013. Identify Disor-
ders in Health Records using Conditional Random
Fields and Metamap AEHRC at ShARe/CLEF 2013
eHealth Evaluation Lab Task 1. In Online Working
Notes of the CLEF 2013 Evaluation Labs and Work-
shop, September.
365
Proceedings of BioNLP Shared Task 2011 Workshop, pages 138?142,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Using Kybots for Extracting Events in Biomedical Texts
Arantza Casillas (*)
arantza.casillas@ehu.es
Arantza D??az de Ilarraza (?)
a.diazdeilarraza@ehu.es
Koldo Gojenola (?)
koldo.gojenola@ehu.es
Maite Oronoz (?)
maite.oronoz@ehu.es
German Rigau (?)
german.rigau@ehu.es
IXA Taldea UPV/EHU
(*) Department of Electricity and Electronics
(?) Department of Computer Languages and Systems
Abstract
In this paper we describe a rule-based sys-
tem developed for the BioNLP 2011 GENIA
event detection task. The system applies Ky-
bots (Knowledge Yielding Robots) on anno-
tated texts to extract bio-events involving pro-
teins or genes. The main goal of this work is to
verify the usefulness and portability of the Ky-
bot technology to the domain of biomedicine.
1 Introduction
The aim of the BioNLP?11 Genia Shared Task (Kim
et al, 2011b) concerns the detection of molecular
biology events in biomedical texts using NLP tools
and methods. It requires the identification of events
together with their gene or protein arguments. Nine
event types are considered: localization, binding,
gene expression, transcription, protein catabolism,
phosphorylation, regulation, positive regulation and
negative regulation.
When identifying the events related to the given
proteins, it is mandatory to detect also the event
triggers, together with its associated event-type, and
recognize their primary arguments. There are ?sim-
ple? events, concerning an event together with its
arguments (Theme, Site, ...) and also ?complex?
events, or events that have other events as secundary
arguments. Our system did not participate in the op-
tional tasks of recognizing negation and speculation.
The training dataset contained 909 texts together
with a development dataset of 259 texts. 347 texts
were used for testing the system.
The main objective of the present work was to ver-
ify the applicability of a new Information Extraction
(IE) technology developed in the KYOTO project1
(Vossen et al, 2008), to a new specific domain. The
KYOTO system comprises a general and extensible
multilingual architecture for the extraction of con-
ceptual and factual knowledge from texts, which has
already been applied to the environmental domain.
Currently, our system follows a rule-based ap-
proach (i.e. (Kim et al, 2009), (Kim et al, 2011a),
(Cohen et al, 2011) or (Vlachos, 2009)), using a set
of manually developed rules.
2 System Description
Our system proceeds in two phases. Firstly, text doc-
uments are tokenized and structured using an XML
layered structure called KYOTO Annotation Format
(KAF) (Bosma et al, 2009). Secondly, a set of Ky-
bots (Knowledge Yielding Robots) are applied to de-
tect the biological events of interest occurring in the
KAF documents. Kybots form a collection of gen-
eral morpho-syntactic and semantic patterns on se-
quences of KAF terms. These patterns are defined
in a declarative format using Kybot profiles.
2.1 KAF
Firstly, basic linguistic processors apply segmenta-
tion and tokenization to the text. Additionally, the
offset positions of the proteins given by the task or-
ganizers are also considered. The output of this ba-
sic processing is stored in KAF, where words, terms,
syntactic and semantic information can be stored in
separate layers with references across them.
Currently, our system only considers a minimal
amount of linguistic information. We are only using
1http://www.kyoto-project.eu/
138
the word form and term layers. Figure 1 shows an
example of a KAF document where proteins have
been annotated using a special POS tag (PRT). Note
that our approach did not use any external resource
apart of the basic linguistic processing.
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<KAF xml:lang="en">
<text>
...
<wf wid="w210" sent="10">phosphorylation</wf>
<wf wid="w211" sent="10">of</wf>
<wf wid="w212" sent="10">I</wf>
<wf wid="w213" sent="10">kappaB</wf>
<wf wid="w214" sent="10">alpha<...
</text>
<term tid="t210" type="open" lemma="phosphorylation"
start="1195" end="1210" pos="W">
<span><target id="w210"/></span>
</term>
<term tid="t211" type="open" lemma="of"
start="1211" end="1213" pos="W">
<span><target id="w211"/></span>
</term>
<term tid="T5" type="open" lemma="I kappaB alpha"
start="1214" end="1228" pos="PRT">
<span><target id="w212"/></span>
<target id="w213"/>
<target id="w214"/></span>
</term>...
</terms>
</KAF>
Figure 1: Example of a document in KAF format.
2.2 Kybots
Kybots (Knowledge Yielding Robots) are abstract
patterns that detect actual concept instances and re-
lations in KAF. The extraction of factual knowledge
by the mining module is done by processing these
abstract patterns on the KAF documents. These pat-
terns are defined in a declarative format using Kybot
profiles, which describe general morpho-syntactic
and semantic conditions on sequences of terms. Ky-
bot profiles are compiled to XQueries to efficiently
scan over KAF documents uploaded into an XML
database. These patterns extract and rank the rele-
vant information from each match.
Kybot profiles are described using XML syn-
tax and each one consists of three main declarative
parts:
? Variables: In this part, the entities and its prop-
erties are defined
? Relations: This part specifies the positional re-
lations among the previously defined variables
? Events: describes the output to be produced for
every matching
Variables (see the Kybot section variables in fig-
ure 2) describe the term variables used by the Kybot.
They have been designed with the aim of being flex-
ible enough to deal with many different information
associated with the KAF terms including semantic
and ontological statements.
Relations (see the Kybot section relations in fig-
ure 2) define the sequence of variables the Kybot
is looking for. For example, in the Kybot in fig-
ure 2, the variable named Phosphorylation
is the main pivot, the variable Of must follow
Phosphorylation (immediate is true indi-
cating that it must be the next term in the sequence),
and a variable representing a Proteinmust follow
Of. Proteins and genes are identified with the PRT
tag.
Events (expressions marked as events in figure 2)
describes the output template of the Kybot. For ev-
ery matched pattern, the kybot produces a new event
filling the template structure with the selected pieces
of information. For example, the Kybot in figure 2
selects some features of the event represented with
the variable called Phosphorylation: its term-
identification (@tid), its lemma, part of speech and
offset. The expression also describes that the vari-
able Protein plays the role of being the ?Theme?
of the event. The output obtained when aplying the
Kybot in figure 2 is shown in figure 3. Comparing
the examples in table 1 and in figure 3 we observe
that all the features needed for generating the files
for describing the results are also produced by the
Kybot.
<doc shortname="PMID-9032271.kaf">
<event eid="e1" target="t210" kybot="phosphorylation of P"
type="Phosphorylation"
lemma="phosphorylation" start="1195" end="1210" />
<role target="T5" rtype="Theme"
lemma="I kappaB alpha" start="1214" end="1228" />
</doc>
Figure 3: Output obtained after the application of the Ky-
bot in figure 2.
3 GENIA Event Extraction Task and
Results
We developed a set of basic auxiliary pro-
grams to extract event patterns from the train-
ing corpus. These programs obtain the struc-
139
<?xml version="1.0" encoding="utf-8"?>
<!-- Sentence: phosphorylation of Protein
Event1: phosphorylation
Role: Theme Protein -->
<Kybot id="bionlp">
<variables>
<var name="Phosphorylation" type="term" lemma="phosphorylat*>
<var name="Of" type="term" lemma="of"/>
<var name="Protein" type="term" pos="PRT"/>
</variables>
<relations>
<root span="Phosphorylation"/>
<rel span="Of" pivot="Phosphorylation" direction="following" immediate="true"/>
<rel span="Protein" pivot="Of" direction="following" immediate="true"/>
</relations>
<events>
<event eid="" target="$Phosphorylation/@tid" kybot="phosphorylation of P"
type="Phosphorylation" lemma="$Phosphorylation/@lemma"
pos="$Phosphorylation/@pos" start="$Phosphorylation/@start" end="$Phosphorylation/@end"/>
<role target="$Protein/@tid" rtype="Theme" lemma="$Protein/@lemma" start="$Protein/@start"
end="$Protein/@end"/>
</events>
</Kybot>
Figure 2: Example of a Kybot for the pattern Event of Protein.
.a1 file
T5 Protein 1214 1228 I kappaB alpha
.a2 file
T20 Phosphorylation 1195 1210 phosphorylation
E7 Phosphorylation:T20 Theme:T5
Table 1: Results in the format required in the GENIA
shared task.
ture of the events, their associated trigger words
and their frequency. For example, in the
training corpus, a pattern of the type Event
of Protein appears 35 times, where the
Event is further described as phosporylation,
phosphorylated.... Taking the most fre-
quently occurring patterns in the training data into
account, we manually developed the set of Kybots
used to extract the events from the development and
test corpora. For example, in this way we wrote the
Kybot in figure 2 that fulfils the conditions of the
pattern of interest.
The two phases mentioned in section 2, corre-
sponding to the generation of the KAF documents
and the application of Kybots, have different input
files depending on the type of event we want to
detect: simple or complex events. When extract-
ing simple events (see figure 4), we used the in-
put text and the files containing protein annotations
(?.a1? files in the task) to generate the KAF docu-
ments. These KAF documents and Kybots for sim-
ple events are provided to the mining module. In
the case of complex events (events that have other
KAF generator
.txt .a1
.kaf
Kybot processor
Kybots
(Simple)
.a2
Figure 4: Application of Kybots. Simple events.
events as arguments), the identifiers of the detected
simple events are added to the KAF document in the
first phase. A new set of Kybots describing complex
events and KAF (now with annotations of the simple
events) are used to obtain the final result (see figure
5).
For the evaluation, we also developed some pro-
grams for adapting the output of the Kybots (see fig-
ure 3) to the required format (see table 1).
We used the development corpus to improve the
Kybot performance. We developed 65 Kybots for
detecting simple events. Table 2 shows the number
of Kybots for each event type. Complex events rela-
tive to regulation (also including negative and posi-
tive regulations) were detected using a set of 24 Ky-
bots.
The evaluation of the task was based on the output
140
KAF generator
.a2 .kaf
.kaf
(with simple events)
Kybot processor
Kybots
(Complex)
.a2
Figure 5: Application of Kybots. Complex events.
Event Class Simple Kyb. Complex Kyb.
Transcription 10
Protein Catabolism 5
Binding 5
Regulation 3
Negative Regulation 5 4
Positive Regulation 3 17
Localization 7
Phosphorylation 18
Gene Exrpesion 12
Total 65 24
Table 2: Number of Kybots generated for each event.
of the system when applied to the test dataset of 347
previously unseen texts. Table 3 shows in the Gold
column the number of instances for each event-type
in the test corpus. R, P and F-score columns stand
for the recall, precision and f-score the system ob-
tained for each type of event. As a consequence of
the characteristics of our system, precision is primed
over recall. For example, the system obtains 95%
and 97% precision on Phosphorylation an Localiza-
tion events, respectively, although its recall is con-
siderably lower (41% and 19%).
4 Conclusions and Future work
This work presents the first results of the applica-
tion of the KYOTO text mining system for extracting
events when ported to the biomedical domain. The
KYOTO technology and data formats have shown to
be flexible enough to be easily adapted to a new task
and domain. Although the results are far from satis-
factory, we must take into account the limited effort
we dedicated to adapting the system and designing
the kybots, which can be roughly estimated in two
Event Class Gold R P F-score
Localization 191 19.90 97.44 33.04
Binding 491 5.30 50.00 9.58
Gene Expression 1002 54.19 42.22 47.47
Transcription 174 13.22 62.16 21.80
Protein catabolism 15 26.67 44.44 33.33
Phosphorylation 185 41.62 95.06 57.89
Non-reg total 2058 34.55 47.27 39.92
Regulation 385 7.53 9.63 8.45
Positive regulation 1443 6.38 62.16 11.57
Negative regulation 571 3.15 26.87 5.64
Regulatory total 2399 5.79 26.94 9.54
All total 4457 19.07 42.08 26.25
Table 3: Performance analysis on the test dataset.
person/months.
After the final evaluation, our system obtained the
thirteenth position out of 15 participating systems
in the main task (processing PubMed abstracts and
full paper articles), obtaining 19.07%, 42.08% and
26.25 recall, precision an f-score, respectively, far
from the best competing system (49.41%, 64.75%
and 56.04%). Although they are far from satisfac-
tory, we must take into account the limited time we
dedicated to adapting the system and designing the
kybots. Apart from that, due to time restrictions,
our system did not make use of the ample set of
resources available, such as named entities, corefer-
ence resolution or syntactic parsing of the sentences.
On the other hand, the system, based on manually
developed rules, obtains reasonable accuracy in the
task of processing full paper articles, obtaining 45%
precision and 21% recall, compared to 59% and 47%
for the best system, which means that the rule-based
approach performs more robustly when dealing with
long texts (5 full texts correspond to approximately
150 abstracts). As we have said before, our main
objective was to evaluate the capabilities of the KY-
OTO technology without adding any additional in-
formation. The use of more linguistic information
will probably facilitate our work and will benefit the
system results. In the near future we will study the
application of machine learning techniques for the
automatic generation of Kybots from the training
data. We also plan to include additional linguistic
and semantic processing in the event extraction pro-
cess to exploit the current semantic and ontological
capabilities of the KYOTO technology.
141
Acknowledgments
This research was supported by the the KYOTO
project (STREP European Community ICT-2007-
211423) and the Basque Government (IT344-10).
References
Wauter Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini and Carlo Aliprandi. KAF: a generic semantic
annotation format Proceedings of the 5th International
Conference on Generative Approaches to the Lexicon
GL 2009 Pisa, Italy, September 17-19, 2009
Kevin Bretonnel Cohen, Karin Verspoor, Helen L. John-
son, Chris Roeder, Philip V. Ogren, Willian A. Baum-
gartner, Elizabeth White, Hannah Tipney, and Lawer-
ence Hunter. High-precision biological event extrac-
tion: Effects of system and data. Computational Intel-
ligence, to appear, 2011.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano and Jun?ichi Tsujii. Overview of
BioNLP?09 Shared Task on Event Extraction. Pro-
ceedings of the BioNLP 2009 Workshop. Association
for Computational Linguistics. Boulder, Colorado, pp.
89?96., 2011
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011a. Overview of
BioNLP Shared Task 2011. Proceedings of the
BioNLP 2011 Workshop Companion Volume for
Shared Task. Association for Computational Linguis-
tics. Portland, Oregon, 2011.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task. Association for Computational Linguis-
tics. Portland, Oregon, 2011.
Andreas Vlachos. Two Strong Baselines for the BioNLP
2009 Event Extraction Task. Proceedings of the 2010
Workshop on Biomedical Natural Language Process-
ing. Association for Computational Linguistics Upp-
sala, Sweden, pp. 1?9., 2010
Piek Vossen, Eneko Agirre, Nicoletta Calzolari, Chris-
tiane Fellbaum, Shu-kai Hsieh, Chu-Ren Huang, Hi-
toshi Isahara, Kyoko Kanzaki, Andrea Marchetti,
Monica Monachini, Federico Neri, Remo Raffaelli,
German Rigau, Maurizio Tescon, Joop VanGent. KY-
OTO: A System for Mining, Structuring, and Dis-
tributing Knowledge Across Languages and Cultures.
Proceedings of LREC 2008. Marrakech, Morocco,
2008.
142
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 60?64,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
First approaches on Spanish medical record classification using
Diagnostic Term to class transduction
A. Casillas(1), A. D??az de Ilarraza(2), K. Gojenola(2), M. Oronoz(2), A. Pe?rez(2)
(1) Dep. Electricity and Electronics
(2) Dep. Computer Languages and Systems
University of the Basque Country (UPV/EHU)
arantza.casillas@ehu.es
Abstract
This paper presents an application of finite-
state transducers to the domain of medicine.
The objective is to assign disease codes to
each Diagnostic Term in the medical records
generated by the Basque Health Hospital Sys-
tem. As a starting point, a set of manually
coded medical records were collected in order
to code new medical records on the basis of
this set of positive samples. Since the texts
are written in natural language by doctors, the
same Diagnostic Term might show alternative
forms. Hence, trying to code a new medical
record by exact matching the samples in the
set is not always feasible due to sparsity of
data. In an attempt to increase the coverage
of the data, our work centered on applying a
set of finite-state transducers that helped the
matching process between the positive sam-
ples and a set of new entries. That is, these
transducers allowed not only exact matching
but also approximate matching. While there
are related works in languages such as En-
glish, this work presents the first results on au-
tomatic assignment of disease codes to medi-
cal records written in Spanish.
1 Introduction
During the last years an exponential increase in
the number of electronic documents in the medi-
cal domain has occurred. The automatic process-
ing of these documents allows to retrieve informa-
tion, helping the health professionals in their work.
There are different sort of valuable data that help to
exploit medical information. Our framework lays
on the classification of Medical Records (MRs) ac-
cording to a standard. In our context, the MRs pro-
duced in a hospital have to be classified with re-
spect to the World Health Organization?s 9th Revi-
sion of the International Classification of Diseases1
(ICD-9). ICD-9 is designed for the classification of
morbidity and mortality information and for the in-
dexing of hospital records by disease and procedure.
The already classified MRs are stored in a database
that serves for further classification purposes. Each
MR consists of two pieces of information:
Diagnostic Terms (DTs): one or more terms that
describe the diseases corresponding to the MR.
Body-text: a description of the patient?s details,
antecedents, symptoms, adverse effects, meth-
ods of administration of medicines etc.
Even though the DTs are within a limited domain,
their description is not subject to a standard. Doc-
tors express the DTs in natural language with their
own style and different degrees of precision. Usu-
ally, a given concept might be expressed by alterna-
tive DTs with variations due to modifiers, abbrevia-
tions, acronyms, dates, names, misspellings or style.
This is a typical problem that arises in natural lan-
guage processing due to the fact that doctors focus
on the patients and not so much on the writing of the
MR. On account of this, there is ample variability in
the presentation of the DTs. Consequently, it is not
a straightforward task to get the corresponding ICD-
codes. That is, the task is by far more complex than
a standard dictionary lookup.
1http://www.cdc.gov/nchs/icd/icd9.htm
60
The Basque Health Hospital System is concerned
with the automatization of this ICD-code assign-
ment task. So far, the hospital processes the daily
produced documents in the following sequence:
1. Automatic: exact match of the DTs in a set of
manually coded samples.
2. Semi-automatic: through semantic match,
ranking the DTs by means of machine-learning
techniques. This stage requires that experts se-
lect amongst the ranked choices.
3. Manual: the documents that were not matched
in the previous two stages are examined by pro-
fessional coders assigning the codes manually.
The goal of this paper is to bypass the variability
associated to natural language descriptions in an at-
tempt to maximize the proportion of automatically
assigned codes, as the Hospital System aims to ex-
pand the use of the automatic codification of MRs
to more hospitals. According to experts, even an in-
crease of 1% in exact match would represent a sig-
nificant improvement allowing to gain time and re-
sources.
Related work can be found in the literature. For
instance, Pestian et al (2007) reported on a shared
task involving the assignment of ICD-codes to radi-
ology reports written in English from a reduced set
of 45 codes. In general it implied the examination of
the full MR (including body-text). In our case, the
number of ICD-codes is above 1,000, although we
restrict ourselves to exact and approximate match
over the diagnoses.
Farkas and Szarvas (2008) used machine learning
for the automatic assignment of ICD-9 codes. Their
results showed that hand-crafted systems could be
reproduced by replacing several laborious steps in
their construction with machine learning models.
Tsuruoka et al (2008) presented a system that
tried to normalize different variants of the terms con-
tained in a medical dictionary, automatically getting
normalizing rules for genes, proteins, chemicals and
diseases in English.
The contribution of this work is: i) to collect
manually coded MRs in Spanish; ii) to approximate
transduction with finite-state (FS) models for auto-
matic MR coding and, iii) to assess the performance
of the proposed FS transduction approaches.
2 Approximate transduction
As it was previously mentioned, there are variations
regarding the DT descriptions due to style, miss-
spells, etc. Table 1 shows several pairs of DT and
ICD-codes within the collected samples that illus-
trate some of those variations.
DT ICD
1 Adenocarcinoma de prostata 185
2 Adenocarcinomas pro?stata. 185
3 Ca. prostata 185
4 CA?NCER DE PROSTATA 185
5 adenocarcinoma de pulmon estadio IV 1629
6 CA pulmo?n estadio 4 1629
7 ADENOCARCINOMA PANCREAS 1579
Table 1: Examples of DTs and their ICD-codes.
There are differences in the use of uppercase/lower
case; omissions of accents; use of both standard and
non-standard abbreviations (e.g. ca. for both ca?ncer
and adenocarcinoma); punctuation marks (inciden-
tal use of full-stop as commas, etc.); omission of
prepositions (see rows 1 and 2); equivalence be-
tween Roman and Arabic numerals (rows 5 and 6).
Due to these variations, our problem can be defined
as an approximate lookup in a dictionary.
2.1 Finite-state models
Foma toolkit was used to build the FS machines and
code the evaluation sets. Foma (Hulden, 2009) is
a freely available2 toolkit that allows to both build
and parse FS automata and transducers. Foma of-
fers a versatile layout that supports imports/exports
from/to other tools such as: Xerox XFST (Beesley
and Karttunen, 2003), AT&T (Mehryar Mohri
and Riley, 2003), OpenFST (Riley et al, 2009).
There are, as well, outstanding alternatives such as
HFST (Linde?n et al, 2010). Refer to (Yli-Jyra? et al,
2006) for a thorough inventory on FS resources.
The FS models in Figure 1 perform the conver-
sions necessary to carry out a soft match between
the dictionary entries and their variants.
? First, we define the transducer Accents that
takes into account the correspondences be-
tween standard letters and their versions using
accent text marks.
2http://code.google.com/p/foma
61
define Accents [a:a?|e:e?|i:??|o:o?|u:u?|...];
define Case [a:A|b:B|c:C|d:D|e:E|f:F|...];
define Spaces [..] (->) " " || [.#. | "."] , .#.;
define Punctuation ["."|"-"|" "]:["."|"-"|" "];
define Plurals [..] -> ([s|es]) || [.#. | "." | " "];
define PluralsI [s|es] (->) "" || [.#. | "." | ","| " "];
define Preps [..] (->) [de |del |con |por ] || " " ;
define Disease [enf|enf.|enfermedad]:[enf|enf.|enfermedad];
define AltCa [tumor|ca|ca.|carcinoma|adenocarcinoma|ca?ncer];
define TagNormCa AltCa:AltCa;
define AltIzq [izquierdo|izquierda|izq|izq.|izqda|izqda.|
izqdo|izqdo.|izda|izda.|izdo|izdo.];
define TagNormIzq AltIzq:AltIzq;
Figure 1: A simplified version in Foma source code of the regular expressions and transducers used to
bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs.
? The expression Case matches uppercase and
lowercase versions of the DTs.
? There is a set of transducers (Spaces,
Punctuation, Plurals and PluralsI)
that deal with the addition or deletion of spaces
and separators (as full-stop, comma, and hy-
phen) between words or at the end of the DT.
? Prepositions. Many DTs can be differen-
tiated by the use or absence of prepositions, al-
though they correspond to the same ICD-code.
For that reason, we designed a transducer that
inserts or deletes the prepositions from a re-
duced set that were identified by inspection of
the training set. In this way, expressions as
?Adenocarcinoma prostata? and ?Adenocarci-
noma de prostata? can be mapped to each other.
? Tag Normalization of synonyms, vari-
ants and abbreviations. The examination of the
DTs in the training set revealed that there were
several terms used indistinctly, including syn-
onyms and different kinds of variants (mascu-
line and feminine) and abbreviations. For ex-
ample, the words adenocarcinoma, adenoca.,
carcinoma, ca, ca. and cancer serve to name
the same disease. There are also multiple vari-
ants of left/right, indicating the location of an
illness, that do not affect the assignment of the
ICD-code (e.g. izquierdo, izq., izda.).
Finally, all the FS transducers were composed
into a single machine that served to overcome all the
sources of distortion together.
3 Experimental results
To begin with, coded MRs produced in the hospi-
tal throughout 12 months were collected summing
up a total of 8,020 MRs as described in Table 2.
Note that there are ambiguities in our data-set since
there are 3,313 different DTs that have resulted in
3,407 (DT, ICD-code) different pairs (as shown in
Table 2). That is, the same DT was not always as-
signed the same ICD-code.
DT ICD-code
entries 8,020
different entries 3,407
different forms 3,313 1,011
Table 2: The data-set of (DT, ICD-code) pairs.
Next, the data-set was shuffled and divided into 3
disjoint sets for training, development and test pur-
poses as shown in Table 3.
train dev test
entries 6,020 1,000 1,000
different entries 2,825 734 728
Table 3: The data-set shuffled and divided into 3 sets
Using the set of mappings derived from the train-
ing set we performed the experiments on the devel-
opment set. After several rounds of tuning the sys-
tem, the resulting system was applied to the test set.
62
PERCENTAGE OF UNCLASSIFIED DTs
TRAIN EVAL-SET exact-match + case-ins. + punct. + plurals +preps. + tag-norm.
train dev 30.6 27.0 25.2 24.4 23.9 23.2
train test 29.8 26.7 25.1 24.8 24.3 23.2
train+dev test 27.7 24.5 23.0 22.9 22.5 21.4
Table 4: Performance of different FS machines in terms of the percentage of unclassified entries. All the
classified entries were correctly classified, yielding, as a result, a precision of 100%.
Given a DT, the goal is to find its corresponding
ICD-code despite the variations. Different FS ap-
proaches (described in Section 2.1) were proposed
to bypass particular sources of noise in the DT. Their
performance was assessed by means of the percent-
age of unclassified DTs, as summarized in Table 4.
Note that the lower the number of unclassified DTs
the better the performance. In each of the three rows
of Table 4 the results of different experimental se-
tups are shown: in the first two rows the training set
was used to build the models and either the devel-
opment or the test set was evaluated in their turn;
in the third row, both the training and the devel-
opment sets were used to build the model and the
test set was evaluated. The impact of adding pro-
gressively the FS machines built to tackle particular
sources of noise is shown by columns. Thus, the re-
sults of the last column represent the performance
of the transducer allowing exact-match search to-
gether with case-insensitive search, bypassing punc-
tuation marks, allowing plurals, bypassing preposi-
tions and allowing tag-normalization. The compo-
sition of each transducer outperforms the previous
result, yielding an improvement on the test of 6 ab-
solute points over the exact-match baseline, from
27.7% to 21.4%. As it can be derived from the
first column of Table 4 the test set contributed to the
training+development set with %27.7 of new DTs.
Overall, the FSMs progressively improved the re-
sults for the three series of experiments carried out
in more than 6%. As a result, less and less DTs are
left unclassified. In other words, the FS machines
tackling different sources of errors contribute to as-
sign ICD-codes to previously unassigned DTs.
A manual inspection over the results associated
to the evaluation of the development set (focus on
the first row of Table 4) showed that all the DTs
were correctly classified according to the training
data. Overall, the resulting transducer was unable
to classify 232 DTs out of 1,000 (see last column
in first row). Among the unclassified DTs, 10 out
of 232 were due to misspellings: e.g. cic atriz
(instead of cicatriz), desprendimineot (instead of
desprendimiento). In fact, spelling correction re-
ported improvements in related tasks (Patrick et al,
2010). The remaining DTs showed wider variations
in their forms, as unexpected degree of specificity
(e.g. named entities), spurious dates or numbers.
4 Conclusions
Medical records in Spanish were collected yielding
a data set of 8,020 DT and ICD-code pairs. While
there are a number of references dealing with En-
glish medical records, there are few for Spanish.
The goal of this work was to build a system that
given a DT it would find its corresponding ICD-
code as in a standard key-value dictionary. Yet, the
DTs are far from being standard since they contain
a number of variations. We proposed the use of sev-
eral FS models to bypass different variants and al-
low to provide ICD-codes even when the exact DT
was not found. Each source of variations was tack-
led with a specific transducer based on handwritten
rules. The composition of each machine improved
the performance of the system gradually, leading to
an improvement up to 6% in accuracy, from 27.7%
unclassified DTs with the exact-match baseline to
21.4% with the tag-normalization transducer.
Future work will focus on the unclassified DTs.
Together with FS models, other strategies shall be
explored. Machine-learning strategies in the field of
information retrieval might help to make the most of
the piece of information that was here discarded (i.e.
the body-text). All in all, regardless of the approach,
the command in this MR classification context is to
get an accuracy of 100%, possibly through the inter-
active inference framework (Toselli et al, 2011).
63
Acknowledgments
Authors would like to thank the Hospital Galdakao-
Usansolo for their contributions and support, in par-
ticular to Javier Yetano, responsible of the Clinical
Documentation Service.
This research was supported by the Department of
Industry of the Basque Government (IT344-10, S-
PE11UN114, GIC10/158 IT375-10), the University
of the Basque Country (GIU09/19) and the Span-
ish Ministry of Science and Innovation (MICINN,
TIN2010- 20218).
References
[Beesley and Karttunen2003] Kenneth R. Beesley and
Lauri Karttunen. 2003. Finite State Morphology.
CSLI Publications,.
[Farkas and Szarvas2008] Richa?rd Farkas and Gyo?rgy
Szarvas. 2008. Automatic construction of rule-based
ICD-9-CM coding systems. BMC Bioinformatics., 9
(Suppl 3): S10.
[Hulden2009] Mans Hulden. 2009. Foma: a Finite-State
Compiler and Library. In EACL (Demos), pages 29?
32. The Association for Computer Linguistics.
[Linde?n et al2010] Krister Linde?n, Miikka Silfverberg,
and Tommi Pirinen. 2010. HFST tools for morphol-
ogy ? an efficient open-source package for construc-
tion of morphological analyzers.
[Mehryar Mohri and Riley2003] Fernando C. N. Pereira
Mehryar Mohri and Michael D. Riley. 2003. AT&T
FSM LibraryTM ? Finite-State Machine Library.
www.research.att.com/sw/tools/fsm.
[Patrick et al2010] Jon Patrick, Mojtaba Sabbagh, Suvir
Jain, and Haifeng Zheng. 2010. Spelling correction
in clinical notes with emphasis on first suggestion ac-
curacy. In 2nd Workshop on Building and Evaluating
Resources for Biomedical Text Mining (BioTxtM2010)
LREC. ELRA.
[Pestian et al2007] John P. Pestian, Chris Brew, Pawel
Matykiewicz, D. J Hovermale, Neil Johnson, K. Bre-
tonnel Cohen, and Wlodzislaw Duch. 2007. A shared
task involving multi-label classification of clinical free
text. In Biological, translational, and clinical lan-
guage processing, pages 97?104, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
[Riley et al2009] Michael Riley, Cyril Allauzen, and
Martin Jansche. 2009. OpenFST: An open-source,
weighted finite-state transducer library and its applica-
tions to speech and language. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Tutorial Abstracts, pages 9?10, Boulder, Col-
orado, May. Association for Computational Linguis-
tics.
[Toselli et al2011] Alejandro H. Toselli, Enrique Vi-
dal, and Francisco Casacuberta. 2011. Multi-
modal Interactive Pattern Recognition and Applica-
tions. Springer.
[Tsuruoka et al2008] Yoshimasa Tsuruoka, John Mc-
Naught, and Sophia Ananiadou. 2008. Normalizing
biomedical terms by minimizing ambiguity and vari-
ability. BMC Bioinformatics, 9(Suppl 3):S2.
[Yli-Jyra? et al2006] A. Yli-Jyra?, K. Koskenniemi, and
K.. Linde?n. 2006. Common infrastructure for
finite-state based methods and linguistic descriptions.
In Proceedings of International Workshop Towards
a Research Infrastructure for Language Resources.,
Genoa, May.
64
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 85?89,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Adverse Drug Event prediction combining
shallow analysis and machine learning
Sara Santiso
Alicia P
?
erez
Koldo Gojenola
IXA Taldea (UPV-EHU)
Arantza Casillas
Maite Oronoz
IXA Taldea (UPV-EHU)
http://ixa.si.ehu.es
Abstract
The aim of this work is to infer a model
able to extract cause-effect relations be-
tween drugs and diseases. A two-level
system is proposed. The first level car-
ries out a shallow analysis of Electronic
Health Records (EHRs) in order to iden-
tify medical concepts such as drug brand-
names, substances, diseases, etc. Next,
all the combination pairs formed by a
concept from the group of drugs (drug
and substances) and the group of diseases
(diseases and symptoms) are characterised
through a set of 57 features. A supervised
classifier inferred on those features is in
charge of deciding whether that pair rep-
resents a cause-effect type of event.
One of the challenges of this work is the
fact that the system explores the entire
document. The contributions of this pa-
per stand on the use of real EHRs to dis-
cover adverse drug reaction events even in
different sentences. Besides, the work fo-
cuses on Spanish language.
1 Introduction
This work deals with semantic data mining within
the clinical domain. The aim is to automatically
highlight the Adverse Drug Reactions (ADRs) in
EHRs in order to alleviate the work-load to sev-
eral services within a hospital (pharmacy service,
documentation service,. . . ) that have to read these
reports. Event detection was thoroughly tackled in
the Natural Language Processing for Clinical Data
2010 Challenge. Since then, cause-effect event ex-
traction has emerged as a field of interest in the
Biomedical domain (Bj?orne et al., 2010; Mihaila
et al., 2013). The motivation is, above all, practi-
cal. Electronic Health Records (EHRs) are studied
by several services in the hospital, not only by the
doctor in charge of the patient but also by the phar-
macy and documentation services, amongst oth-
ers. There are some attempts in the literature that
aim to make the reading of the reports in English
easier and less time-consuming by means of an au-
tomatic annotation toolkit (Rink et al., 2011; Bot-
sis et al., 2011; Toldo et al., 2012). This work is
a first approach on automatic learning of relations
between drugs causing diseases in Spanish EHRs.
This work presents a system that entails two
stages in cascade: 1) the first one carries out the
annotation of drugs or substances (from now on-
wards both of them shall be referred to as DRUG)
and diseases or symptoms (referred to as DIS-
EASE); 2) the second one determines whether a
given (DRUG, DISEASE) pair of concepts repre-
sents a cause-effect reaction. Note that we are in-
terested in highlighting events involving (DRUG,
DISEASE) pairs where the drug caused an adverse
reaction or a disease. By contrast, often, (DRUG,
DISEASE) pairs would entail a drug prescribed to
combat a disease, but these correspond to a differ-
ent kind of events (indeed, diametrically opposed).
Besides, (DRUG, DISEASE) pairs might represent
other sort of events or they might even be unre-
lated at all. Finally, the system should present the
ADRs marked in a friendly front-end. To this end,
the aim is to represent the text in the framework
provided by Brat (Stenetorp et al., 2012). Figure 1
shows an example, represented in Brat, of some
cause-effect events manually tagged by experts.
There are related works in this field aiming at
a variety of biomedical event extraction, such as
binary protein-protein interaction (Wong, 2001),
biomolecular event extraction (Kim et al., 2011),
and drug-drug interaction extraction (Segura-
Bedmar et al., 2013). We are focusing on a variety
of interaction extraction: drugs causing diseases.
There are previous works in the literature that try
to warn whether a document contains or not this
type of events. There are more recent works that
85
Figure 1: Some cause-effect events manually annotated in the Brat framework.
cope with event extraction within the same sen-
tence, that is, intra-sentence events. By contrast, in
this work we have realised that around 26% of the
events occur between concepts that are in differ-
ent sentences. Moreover, some of them are at very
long distance. Hence, our method aims at provid-
ing all the (DRUG, DISEASE) concepts within the
document that represent a cause-effect relation.
We cope with real discharge EHRs written by
around 400 different doctors. These records are
not written in a template, that is, the EHRs do not
follow a pre-determined structure, and this, by it-
self entails a challenge. The EHRs we are dealing
with are written in a free structure using natural
language, non-standard abbreviations etc. More-
over, we tackle Spanish language, for which little
work has been carried out. In addition, we do not
only aim at single concept-words but also at con-
cepts based on multi-word terms.
2 System overview
The system, as depicted in Figure 2 entails two
stages.
EHR
Stage 1:
ANNOTATING
CONCEPTS
Stage 2:
EXTRACTING
EVENTS
MARKED 
EHR
Figure 2: The ADR event extraction system.
In the first stage, relevant pairs of concepts have
to be identified within an EHR. Concept annota-
tion is accomplished by means of a shallow anal-
yser system (described in section 2.1). Once the
analyser has detected (DRUG, DISEASE) pairs in
a document, all the pairs will be examined by
an inferred supervised classifier (described in sec-
tion 2.2).
2.1 Annotating concepts by shallow analysis
The first stage of the system has to detect and an-
notate two types of semantic concepts: drugs and
diseases. Each concept, as requested by the phar-
macy service, should gather several sub-concepts
stated as follows:
1. DRUG concept:
(a) Generic names for pharmaceutical
drugs: e.g. corticoids;
(b) Brand-names for pharmaceutical drugs:
e.g. Aspirin;
(c) Active ingredients: e.g. vancomycin;
(d) Substances: e.g. dust, rubber;
2. DISEASE concept:
(a) Diseases
(b) Signs
(c) Symptoms
These concepts were identified by means of a
general purpose analyser available for Spanish,
called FreeLing (Padr?o et al., 2010), that had been
enhanced with medical ontologies and dictionar-
ies, such as SNOMED-CT, BotPLUS, ICD-9-CM,
etc. (Oronoz et al., 2013). This toolkit is able
to identify multi-word context-terms, lemmas and
also POS tags. An example of the morphological,
semantic and syntactic analysis, provided by this
parser is given in Figure 3. In the figure two pieces
of information can be distinguished: for exam-
ple, given the word ?secundarios? (meaning sec-
ondaries) 1) the POS tag provided is AQOM corre-
sponding to Qualificative Adjective Ordinal Mas-
culine Singular; and 2) the provided lemma is ?se-
cundario? (secondary). Besides, in a third layer,
the semantic tag is given, that is, the tag ?ENFER-
MEDAD? (meaning disease) involves the multi-
word concept ?HTP severa? (severe pulmonary
hypertension).
86
Figure 3: Lemmas, POS-tags and semantic tags are identified by the clinic domain analyser (diseases in
yellow and drugs or substances in violet).
2.2 Extracting adverse drug reaction events
using inferred classifiers
The goal of the second stage is to determine if a
given (DRUG, DISEASE) pair represents an ADR
event or not. On account of this, we resorted to
supervised classification models. These models
can be automatically inferred from a set of doc-
uments in which the target concepts had been pre-
viously annotated. Hence, first of all, a set of an-
notated data representative for the task is required.
To this end, our starting point is a manually anno-
tated corpus (presented in section 2.2.1). Besides,
in order to automatically learn the classifier, the
(DRUG, DISEASE) pairs have to be described in an
operative way, that is, in terms of a finite-set of
features (see section 2.2.2). The supervised clas-
sification model selected was a type of ensemble
classifier: Random Forests (for further details turn
to section 2.2.3).
2.2.1 Producing an annotated set
A supervised classifier was inferred from an-
notated real EHRs. The annotation was carried
out by doctors from the same hospital that pro-
duced the EHRs. Given the text with the con-
cepts marked on the first stage (turn to section 2.1)
and represented within the framework provided by
Brat
1
, around 4 doctors from the same hospital an-
notated the events. This annotated set would work
as a source of data to get instances that would
serve to train supervised classification models, as
the one referred in section 2.2.
2.2.2 Operational description of events
As it is well-known, the success of the techniques
based on Machine Learning relies upon the fea-
tures used to describe the instances. Hence, we se-
lected the following features that eventually have
1
Brat is the framework a priori selected as the output
front-end shown in Figure 1
proven useful to capture the semantic relations be-
tween ADRs. The features can be organised in the
following sets:
? Concept-words and context-words: to be
precise, we make use of entire terms
including both single-words and multi-
words.
? DRUG concept-word together with
left and right context words (a con-
text up to 3, yielding, thus, 7 fea-
tures).
? DISEASE concept-word together
with left and right context words (7
features).
? Concept-lemmas and context-lemmas
for both drug and disease (14 features
overall)
? Concept-POS and context-POS for both
drug and disease (14 features)
? Negation and speculation: these are
binary valued features to determine
whether the concept words or their con-
text was either negated or speculated (2
features).
? Presence/absence of other drugs in the
context of the target drug and disease (12
features)
? Distance: the number of characters from
the DRUG concept to the DISEASE con-
cept (1 feature).
2.2.3 Inferring a supervised classifier
Given the operational description of a set of
(DRUG, DISEASE) pairs, this stage has to deter-
87
mine if there exists an ADR event (that is, a cause-
effect relation) or not. To do so, we resorted
to Random Forests (RFs), a variety of ensemble
models. RFs combine a number of decision trees
being each tree built on the basis of the C4.5 algo-
rithm (Quinlan, 1993) but with a distinctive char-
acteristic: some randomness is introduced in the
order in which the nodes are generated. Particu-
larly, each time a node is generated in the tree, in-
stead of chosing the attribute that maximizes the
Information Gain, the attribute is randomly se-
lected amongst the k best options. We made use
of the implementation of this algorithm available
in Weka-6.9 (Hall et al., 2009). Ensemble models
were proved useful on drug-drug interaction ex-
traction tasks (Thomas et al., 2011).
3 Experimental results
We count on data consisting of discharge sum-
maries from Galdakao-Usansolo Hospital. The
records are semi-structured in the sense that there
are two main fields: the first one for personal data
of the patient (age, dates relating to admittance)
that were not provided by the hospital for privacy
issues; and the second one, our target, a single
field that contains the antecedents, treatment, clin-
ical analysis, etc. This second field is an unstruc-
tured section (some hospitals rely upon templates
that divide this field into several subfields, provid-
ing it with further structure). The discharge notes
describe a chronological development of the pa-
tient?s condition, the undergone treatments, and
also the clinical tests that were carried out.
Given the entire set of manually annotated doc-
uments, 34% were randomly selected without re-
placement to produce the evaluation set. The re-
sulting partition is presented in Table 1 (where the
train and evaluation sets are referred to as Train
and Eval respectivelly).
Documents Concepts Relations
Train 144 6,105 4,675
Eval 50 2,206 1,598
Table 1: Quantitative description of the data.
All together, there are 194 EHRs manually
tagged with more than 8,000 concepts (entailing
diseases, symptoms, drugs, substances and proce-
dures). From these EHRs all the (DRUG,DISEASE)
pairs are taken into account as event candidates,
and these are referred to as relations in Table 1.
The system was assessed using per-class aver-
aged precision, recall and f1-measure as presented
in Table 2.
Precision Recall F1-measure
0.932 0.849 0.883
Table 2: Experimental results.
Semantic knowledge and contextual features
have proven very relevant to detect cause-effect re-
lations. Particularly, those used to detect the con-
cepts and also negation or speculation of the con-
text in which the concept appear.
A manual inspection was carried out on both the
false positives and false negative predictions and
the following conclusions were drawn:
? The majority of false positives were caused
by i) pairs of concepts at a very long distance;
ii) pairs where one of the elements is related
to past-events undergone while the other el-
ement is in the current treatment prescribed
(e.g. the disease is in the antecedents and the
drug in the current diagnostics).
? The vast majority of false negatives were
due to concepts in the same sentence where
the context-words are irrelevant (e.g. filler
words, determiners, etc.).
4 Concluding Remarks and Future Work
This work presents a system that first identifies rel-
evant pairs of concepts in EHRs by means of a
shallow analysis and next examines all the pairs
by an inferred supervised classifier to determine if
a given pair represents a cause-effect event. A rel-
evant contribution of this work is that we extract
events occurring between concepts that are in dif-
ferent sentences. In addition, this is one of the first
works on medical event extraction for Spanish.
Our aim for future work is to determine whether
the (DRUG, DISEASE) pair represents either a rela-
tion where 1) the drug is to overcome the disease;
2) the drug causes the disease; 3) there is no rela-
tionship between the drug and the disease.
The aim of context features is to capture charac-
teristics of the text surrounding the relevant con-
cepts that trigger a relation. More features could
also be explored such as trigger words, regular pat-
terns, n-grams, etc.
88
Acknowledgments
The authors would like to thank the Pharmacy
and Pharmacovigilance services of Galdakao-
Usansolo Hospital.
This work was partially supported by the Euro-
pean Commission (325099 and SEP-210087649),
the Spanish Ministry of Science and Innovation
(TIN2012-38584-C06-02) and the Industry of the
Basque Government (IT344-10).
References
Jari Bj?orne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-
jii, and Tapio Salakoski. 2010. Complex event ex-
traction at pubmed scale. Bioinformatics [ISMB],
26(12):382?390.
Taxiarchis Botsis, Michael D. Nguyen, Emily Jane
Woo, Marianthi Markatou, and Robert Ball. 2011.
Text mining for the vaccine adverse event reporting
system: medical text classification using informative
feature selection. JAMIA, 18(5):631?638.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1):10?18.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In
Proceedings of the BioNLP Shared Task 2011
Workshop, pages 1?6. Association for Computa-
tional Linguistics.
Claudiu Mihaila, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. Biocause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14:2.
Maite Oronoz, Arantza Casillas, Koldo Gojenola, and
Alicia Perez. 2013. Automatic annotation of
medical records in Spanish with disease, drug and
substance names. In Lecture Notes in Computer
Science, volume 8259, pages 536?547. Springer-
Verlag.
Lluis Padr?o, S. Reese, Eneko Agirre, and Aitor Soroa.
2010. Semantic Services in Freeling 2.1: WordNet
and UKB. In Global Wordnet Conference, Mumbai,
India.
Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, CA.
Bryan Rink, Sanda Harabagiu, and Kirk Roberts.
2011. Automatic extraction of relations between
medical concepts in clinical texts. JAMIA, 18:594?
600.
Isabel Segura-Bedmar, P Mart??nez, and Mar?a Herrero-
Zazo. 2013. Semeval-2013 task 9: Extraction of
drug-drug interactions from biomedical texts (ddiex-
traction 2013). Proceedings of Semeval, pages 341?
350.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. Brat: A web-based tool for nlp-
assisted text annotation. In In Proceedings of the
Demonstrations Session at EACL 2012.
Philippe Thomas, Mariana Neves, Ill?es Solt,
Domonkos Tikk, and Ulf Leser. 2011. Relation
extraction for drug-drug interactions using ensem-
ble learning. 1st Challenge task on Drug-Drug
Interaction Extraction (DDIExtraction 2011), pages
11?18.
Luca Toldo, Sanmitra Bhattacharya, and Harsha Gu-
rulingappa. 2012. Automated identification of ad-
verse events from case reports using machine learn-
ing. In Workshop on Computational Methods in
Pharmacovigilance.
Limsoon Wong. 2001. A protein interaction extraction
system. In Pacific Symposium on Biocomputing,
volume 6, pages 520?531. Citeseer.
89
