Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 528?536,
Beijing, August 2010
Unsupervised phonemic Chinese word segmentation using Adaptor
Grammars
Mark Johnson
Department of Computing
Macquarie University
Mark.Johnson@mq.edu.au
Katherine Demuth
Department of Linguistics
Macquarie University
Katherine.Demuth@mq.edu.au
Abstract
Adaptor grammars are a framework for
expressing and performing inference over
a variety of non-parametric linguistic
models. These models currently provide
state-of-the-art performance on unsuper-
vised word segmentation from phonemic
representations of child-directed unseg-
mented English utterances. This paper in-
vestigates the applicability of these mod-
els to unsupervised word segmentation of
Mandarin. We investigate a wide vari-
ety of different segmentation models, and
show that the best segmentation accuracy
is obtained frommodels that capture inter-
word ?collocational? dependencies. Sur-
prisingly, enhancing the models to exploit
syllable structure regularities and to cap-
ture tone information does improve over-
all word segmentation accuracy, perhaps
because the information these elements
convey is redundant when compared to the
inter-word dependencies.
1 Introduction and previous work
The word-segmentation task is an abstraction of
part of the problem facing a child learning its na-
tive language. Fluent speech, even the speech di-
rected at children, doesn?t come with silence or
pauses delineating acoustic words the way that
spaces separate orthographic words in writing sys-
tems like that of English. Instead, as most people
listening to a language they don?t understand can
attest, words in fluent speech ?run together?, and a
language user needs to learn how to segment utter-
ances of the language they are learning into words.
This kind of word segmentation is presumably an
important first step in acquiring a language. It is
scientifically interesting to know what informa-
tion might be useful for word segmentation, and
just how this information might be used. These
scientific questions have motivated a body of re-
search on computational models of word segmen-
tation. Since as far as we can tell any child can
learn any human language, our goal is to develop
a single model that can learn to perform accurate
word segmentation given input from any human
language, rather than a model that specialised to
perform well on a single language. This paper
extends the previous work on word segmentation
by investigating whether one class of models that
work very well with English input also work with
Chinese input. These models will permit us to
study the role that syllable structure constraints
and tone in Chinese might play in word segmenta-
tion.
While learners and fluent speakers undoubt-
edly use a wide variety of cues to perform word
segmentation, computational models since El-
man (1990) have tended to focus on the use
of phonotactic constraints (e.g., syllable-structure
constrains) and distributional information. Brent
and Cartwright (1996) introduced the standard
form of theword segmentation task still studied to-
day. They extracted the orthographic representa-
tions of child-directed speech from the Bernstein-
Ratner corpus (Bernstein-Ratner, 1987) and ?pho-
nologised? them by looking up each word in a
pronouncing dictionary. For example, the or-
thographic utterance you want to see the book
is mapped to the sequence of pronunciations yu
want tu si D6 bUk, (the pronunciations are in an
528
ASCII encoding of the International Phonetic Al-
phabet representation of English phonemes). The
input to the learner is obtained by concatenating
together the phonemic representations of each ut-
terance?s words. The learner?s task is to identify
the locations of the word boundaries in this se-
quence, and hence identify the words (up to ho-
mophony). Brent and Cartwright (1996) pointed
out the importance of both distributional informa-
tion and phonotactic (e.g., syllable-structure) con-
straints for word segmentation (see also Swingley
(2005) and Fleck (2008)).
Recently there has been considerable interest in
applying Bayesian inference techniques for non-
parametric models to this problem. Here the term
?non-parametric? does not mean that the models
have no parameters, rather, it is used to distinguish
these models from the usual ?parametric models?
that have a fixed finite vector of parameters.
Goldwater et al (2006) introduced two non-
parametric Bayesian models of word segmenta-
tion, which are discussed in more detail in (Gold-
water et al, 2009). The unigram model, which as-
sumes that each word is generated independently
to form a sentence, turned out to be equivalent
to a model originally proposed by Brent (1999).
The bigram model improves word segmentation
accuracy by modelling bigram inter-word contex-
tual dependencies, ?explaining away? inter-word
dependencies that would otherwise cause the uni-
gram model to under-segment. Mochihashi et al
(2009) showed that segmentation accuracy could
be improved by using a more sophisticated ?base
distribution? and a dynamic programming sam-
pling algorithm very similar to the one used with
the adaptor grammars below. They also applied
their algorithm to Japanese and Chinese word seg-
mentation, albeit from orthographic rather than
phonemic forms, so unfortunately their results are
not comparable with ours.
Johnson et al (2007) introduced adaptor gram-
mars as a grammar-based framework for express-
ing a variety of non-parametric models, and pro-
vided a dynamic programming Markov Chain
Monte Carlo (MCMC) sampling algorithm for
performing Bayesian inference on these models.
For example, the unigram model can be expressed
as a simple adaptor grammar as shown below, and
the generic adaptor grammar inference procedure
provides a dynamic programming sampling algo-
rithm for this model. Johnson (2008b) showed
how a variety of different word segmentation
models can be expressed as adaptor grammars, and
Johnson and Goldwater (2009) described a num-
ber of extensions and specialisations to the adaptor
grammar framework that improve inference speed
and accuracy (we use these techniques in our work
below).
Previous work on unsupervised word segmen-
tation from phonemic input has tended to concen-
trate on English. However, presumably children
the world over segment their first language input
in the same (innately-specified) way, so a correct
procedure should work for all possible human lan-
guages. However, as far as we are aware there has
been relatively little work on word segmentation
from phonemic input except on English. Johnson
(2008a) investigated whether the adaptor gram-
mars models that do very well on English also ap-
ply to Sesotho (a Bantu language spoken in south-
ern Africa with rich agglutinating morphology).
He found that the models in general do very poorly
(presumably because the adaptor grammars used
cannot model the complex morphology found in
Sesotho) and that the best segmentation accuracy
was considerably worse than that obtained for En-
glish, even when that model incorporated some
Bantu-specific information about morphology. Of
course it may also be that the Sesotho and English
corpora are not really comparable: the Bernstein-
Ratner corpus that Brent and other researchers
have used for English was spoken to pre-linguistic
1-year olds, whilemost non-English corpora are of
child-directed speech to older children who are ca-
pable of talking back, and hence these corpora are
presumably more complex. We discuss this issue
in more detail in section 4 below.
2 A Chinese word segmentation corpus
Our goal here is to prepare a Chinese corpus of
child-directed speech that parallels the English
one used by Brent and other researchers. That
corpus was in broad phonemic form, obtained by
looking each word up in a pronouncing dictio-
nary. Here instead we make use of a corpus in
Pinyin format, which we translate into a broad
529
phonemic IPA format using the freely-available
Pinyin-to-IPA translation program ?Pinyin to
IPA Conversion Tools? version 2.1 available on
http://sourceforge.net/projects/py2ipa.
We used the ?Beijing? corpus (Tardif, 1993)
available from the publicly-distributed Childes
collection of corpora (MacWhinney and Snow,
1985). We are interested in child-directed speech
(rather than children?s speech), so we removed all
utterances from participants with an Id containing
?Child?. (Tardif (1993) points out that Chinese-
speaking children typically have a much richer
social environment involving multiple adult care-
givers than middle-class English-speaking chil-
dren do, so we cannot simply collect only the
mother?s utterances, as was done for the English
corpus). We also ignored all utterances with codes
$INTERJ, $UNINT, $VOC and $PRMPT, as these are
not always linguistic utterances. In addition, we
deleted all words that could not be analysed as a
sequence of syllables, such as ?xxx? and ?hmm?,
and also deleted ?cluck?. The first few utterances
of the corpus in Pinyin format are:
zen3me gei3 ta1 bei1 shang4 lai2 (1.) ?
ta1: (.) a1yi2 gei3 de (.) ta1 gei3 de .
hen3 jian3dan1 .
We then fed these into the Pinyin-to-IPA trans-
lation program, producing output of the following
format:
ts?n214m? kei214 t?a55 pei55 ???51 lai35
t?a55 a55i35 kei214 t? t?a55 kei214 t?
x?n214 t?i?n214tan55
In the IPA format, the superscript indices in-
dicate the tone patterns associated with syllables;
these appear at the end of each syllable, as is stan-
dard. While we believe there are good linguistic
reasons to analyse tones as associated with syl-
lables, we moved all the tones so they immedi-
ately followed the final vowel in each syllable.
We did this because we thought that locating tones
after the syllable-final consonant might give our
models a strong cue as to the location of sylla-
ble boundaries, and since words often end at syl-
lable boundaries, this would make the word seg-
mentation problem artificially easier. (Our models
take a sequence of symbols as input, so the tones
must be located somewhere in the sequence. How-
ever, the linguistically ?correct? solution would
probably be to extend the models so they could
process input in an auto-segmental format (Gold-
smith, 1990) where tones would be on a separate
tier and unordered with respect to the segments
within a syllable.)
In order to evaluate the importance of tone
for our word-segmentation models we also con-
structed a version of our corpus in which all tones
were removed. We present results for all of our
models on two versions of the corpus, one that
contains tones following the vowels, and another
that contains no tones at all. These two cor-
pora constitute the ?gold standard? against which
our word segmentation models will be evaluated.
These corpora contain 50,118 utterances, consist-
ing of 187,533 word tokens.
The training data provided to the word segmen-
tation models is obtained by segmenting the gold
data at all possible boundary locations. Conso-
nant clusters, diphthongs and tones (if present) are
treated as single units, so the training data appears
as follows:
ts ? 214 n m ? k e i 214 t? a 55 p e i 55 ? ? 51 ? l ai 35
t? a 55 a 55 i 35 k e i 214 t ? t? a 55 k e i 214 t ?
x ? 214 n t? i? 214 n t a 55 n
The task of a word-segmentation model is
to identify which of these possible bound-
ary locations correspond to actual word bound-
aries. The training corpus without tones contains
531,384 segments, while the training corpus with
tones contains 712,318 segments.
3 Adaptor grammars for word
segmentation
Adaptor grammars were first introduced by John-
son et al (2007) as a grammar-based frame-
work for specifying hierarchical non-parametric
Bayesian models, and Johnson and Goldwater
(2009) describes a number of implementation de-
tails that significantly improve performance; the
interested reader should consult those papers for a
full technical introduction. Johnson (2008b) pro-
posed a number of adaptor grammars for English
word segmentation, which we review and mini-
mally modify here so they can perform Chinese
530
word segmentation as well. In section 4 we evalu-
ate these adaptor grammars on the Chinese corpus
just described.
The grammars vary along two orthogonal di-
mensions, which correspond to the kinds of gen-
eralisations that the model can learn. The sim-
plest grammar is the unigram adaptor grammar,
which generates an utterance as an i.i.d. sequences
of words, where each word is a sequence of
phonemes. The collocation adaptor grammars
capture dependencies above the word level by
generating collocations, or groups of words, as
memoized units. The syllable adaptor grammars
capture dependencies below the word level by
generating words as sequences of syllables rather
than phonemes.
3.1 Unigram adaptor grammars
In order to motivate adaptor grammars as an ex-
tension to Probabilistic Context-Free Grammars
(PCFGs), consider an attempt to perform unsuper-
vised word segmentation with a PCFG containing
the following rules (ignore the underlining of the
Word non-terminal for now).
Words?Words Word
Words?Word
Word? Phons
Phons? Phon
Phons? Phons Phon
Phons? Phons Tone
Phon? ai | o | ? | ? | t?? | ?
Tone? 35 | 55 | 214 | ?
(1)
In this grammar, Phon expands to all the
phonemes appearing in the phonemic training
data, and Tone expands to all of the tone patterns.
(In this and all of the other grammars in this paper,
the start symbol is the non-terminal symbol of the
first rule in the grammar. This grammar, like all
others in this paper, is crafted so that aWord sub-
tree can never begin with a Tone, so the presence
of tones does not make the segmentation problem
harder).
The trees generated by this grammar are suffi-
ciently expressive to represent any possible seg-
mentation of any sequence of phonemes into
words (including the true segmentation); a typi-
cal segmentation is shown in Figure 1. However,
Words
Words
Word
Phons
Phons
Phons
Phon
p
Phon
u
Tone
35
Word
Phons
Phons
Phons
Phons
Phon
k?
Phon
a
Tone
51
Phon
n
Figure 1: A parse tree generated by the unigram
grammar, where adapted and non-adapted non-
terminals are shown. It depicts a possible segmen-
tation of p u 35 k? a 51 n.
it should also be clear that no matter how we vary
the probabilities on the rules of this grammar, the
grammar itself cannot encode the subset of trees
that correspond to words of the language. In or-
der to do this, a model would need to memorise the
probabilities of entire Word subtrees, since these
are the units that correspond to individual words,
but this PCFG simply is not expressive enough to
do this.
Adaptor grammars learn the probabilities of
subtrees in just this way. An adaptor grammar is
specified via a set of rules or productions, just like
a CFG, and the set of trees that an adaptor gram-
mar generates is exactly the same as the CFG with
those rules.
However, an adaptor grammar defines proba-
bility distributions over trees in a completely dif-
ferent fashion to a PCFG: for simplicity we fo-
cus here on the sampling or predictive distribu-
tion, which defines the probability of generating
an entire corpus of trees. In a PCFG, the prob-
ability of each non-terminal expanding using a
given rule is determined by the probability of that
rule, and is independent of the expansions of the
other non-terminals in the tree. In an adaptor
grammar a subset of the non-terminals are des-
531
ignated as adapted. We indicate adapted non-
terminals by underlining them, so Word is the
only adapted non-terminal in (1). Unadapted non-
terminals expand just as in a PCFG: a produc-
tion is chosen according to the production prob-
abilities. An adapted non-terminal can expand
in two different ways. With probability propor-
tional to n(t)? aA an adapted non-terminal A ex-
pands to a tree t rooted in A that has been pre-
viously generated, while with probability propor-
tional to m(A)aA + bA the adapted non-terminal
A expands using some grammar rule, just as in a
PCFG. Here n(t) is the number of times tree t has
been previously generated,m(A) is the number of
trees rooted in A that have been previously gener-
ated using grammar rules, and 0 ? aA ? 1 and
bA > 0 are adjustable parameters associated with
the adapted non-terminal A.
Technically, this is known as a Pitman-Yor Pro-
cess (PYP) with concentration parameters aA and
bA, where the PCFG rules define the base distri-
bution of the process. (The PYP is a generalisa-
tion of the Chinese Restaurant Process (CRP); a
CRP is a PYP with parameter a = 0). Rather
than setting the concentration parameters by hand
(there are two for each adapted non-terminal in
the grammar) we follow Johnson and Goldwater
(2009) and put uniform Beta and vague Gamma
priors on each of these parameters, and use sam-
pling to explore their posterior values.
Because the probability of selecting a tree t is
proportional to n(t), an adaptor grammar is a kind
of ?rich-get-richer? process that generates power-
law distributions. Depending on the values of aA
and bA, most of the probability mass can wind
up concentrated on just a few trees. An adaptor
grammar is a kind of ?cache? model, in which
previously generated subtrees are stored and more
likely to be reused in later sentences. That is, while
an adapted non-terminal A can expand to any tree
rooted inA that can be constructed with the gram-
mar rules, in practice it is increasingly likely to
reuse the same trees over and over again. It can
be viewed as a kind of tree substitution grammar
(Joshi, 2003), but where the tree fragments (as
well as their probabilities) are learnt from the data.
The unigram grammar is the simplest of the
word segmentation models we investigate in this
paper (it is equivalent to the unigram model inves-
tigated in Goldwater et al (2009)). Because the
grammars we present below rapidly become long
and complicated to read if each grammar rule is
explicitly stated, we adopt the following conven-
tions. We use regular expressions to abbreviate
our grammars, with the understanding that the reg-
ular expressions are always expanded produce a
left-recursive structure. For example, the unigram
grammar in (1) is abbreviated as:
Words?Word+
Word? Phon (Phon | Tone)?
Phon? ai | o | ? | ? | t?? | ?
Tone? 35 | 55 | 214 | ?
(2)
3.2 Collocation adaptor grammars
Goldwater et al (2006) and Goldwater et al
(2009) demonstrated the importance of contex-
tual dependencies for word segmentation, and pro-
posed a bigram model in order to capture some
of these. It turns out that while the bigram model
cannot be expressed as an adaptor grammar, a col-
location model, which captures similar kinds of
contextual dependencies, can be expressed as an
adaptor grammar (Johnson et al, 2007). In a col-
location grammar there are two different adapted
non-terminals; Word and Colloc; Word expands
exactly as in the unigram grammar (2), so it is not
repeated here.
Collocs? Colloc+
Colloc?Words
Words?Word+
(3)
A collocation adaptor grammar caches both
words and collocations (which are sequences of
words). An utterance is generated by generating
one or more collocations. The PYP associated
with collocations either regenerates a previously
generated collocation or else generates a ?fresh?
collocation by generating a sequence of words ac-
cording to the PYP model explained above.
The idea of aggregating words into collocations
can be reapplied at a more abstract level by ag-
gregating collocations into ?super-collocations?,
which are sequences of collocations. This in-
volves adding the following additional rules to the
grammar in (3):
532
Colloc2s? Colloc2+
Colloc2? Collocs+ (4)
There are three PYPs in a grammar with 2 lev-
els of collocations, arranged in a strict Bayesian
hierarchy. It should be clear that this process can
be repeated indefinitely; we investigate grammars
with up to three levels of collocations below. (It
should be possible to use Bayesian techniques to
learn the appropriate number of levels in the hier-
archy, but we leave this for future work).
3.3 Syllable structure adaptor grammars
Brent and Cartwright (1996) and others emphasise
the role that syllable-structure and other phono-
tactic constraints might play in word segmenta-
tion. Johnson (2008b) pointed out that adaptor
grammars can learn at least some of these kinds
of generalisations. It?s not unreasonable to as-
sume that language learners can learn to group
phonemes into syllables, and that they can exploit
this syllabic structure to perform word segmenta-
tion. The syllable-structure grammars we describe
below assume that word boundaries are always
aligned with syllable boundaries; this is not uni-
versally true, but it is reliable enough to dramati-
cally improve unsupervised word segmentation in
English.
There is considerable cross-linguistic varia-
tion in the syllable-structure and phonotactic con-
straints operative in the languages of the world, so
we?d like to avoid ?building in? language-specific
constraints into our model. We therefore make the
relatively conservative assumption that the child
can distinguish vowels from consonants, and that
the child knows that syllables consist of Onsets,
Nuclei and Codas, that Onsets and Codas consist
of arbitrary sequences of consonants while Nuclei
are arbitrary sequences of vowels and tones, and
that Onsets and Codas are optional. Notice that
syllable structure in both English and Chinese is
considerably more constrained than this; we use
this simple model here because it has proved suc-
cessful for English word segmentation.
The syllable-structure adaptor grammars re-
place the rules expanding Words with the follow-
ing rules:
Word? Syll
Word? Syll Syll
Word? Syll Syll Syll
Word? Syll Syll Syll Syll
Syll? (Onset)? Rhy
Onset? C+
Rhy? Nucleus (Coda)?
Nucleus? V (V | Tone)?
Coda? C+
C? ? | t?? | ?
V? ai | o | ?
(5)
In these rules the superscript ??? indicates op-
tionality. We used the relatively cumbersome
mechanism of enumerating each possible number
of syllables per word (we permit words to consist
of from 1 to 4 syllables, although ideally this num-
ber would not be hard-wired into the grammar)
because a relatively trivial modification of this
grammar can distinguish word-initial and word-
final consonant clusters from word-internal clus-
ters. Johnson (2008b) demonstrated that this sig-
nificantly improves English word segmentation
accuracy. We do not expect this to improve Chi-
nese word segmentation because Chinese clusters
do not vary depending on their location within the
word, but it will be interesting to see if the addi-
tional cluster flexibility that is useful for English
segmentation hurts Chinese segmentation.
In this version of the syllable-structure gram-
mar, we replace the Word rules in the syllable
adaptor grammar with the following:
Word? SyllIF
Word? SyllI SyllF
Word? SyllI Syll SyllF
Word? SyllI Syll Syll SyllF
(6)
and add the following rules expanding the new
kinds of syllables to the rules in (5).
SyllIF? (OnsetI)? RhyF
SyllI? (OnsetI)? Rhy
SyllF? (OnsetI)? RhyF
Syll? (Onset)? Rhy
OnsetI? C+
RhyF? Nucleus (CodaF)?
CodaF? C+
(7)
533
Syllables
None General Specialised
Unigram 0.57 0.50 0.50
Colloc 0.69 0.67 0.67
Colloc2 0.72 0.75 0.75
Colloc3 0.64 0.77 0.77
Table 1: F-score accuracies of word segmenta-
tions produced by the adaptor grammar models on
the Chinese corpus with tones.
Syllables
None General Specialised
Unigram 0.56 0.46 0.46
Colloc 0.70 0.65 0.65
Colloc2 0.74 0.74 0.73
Colloc3 0.75 0.76 0.77
Table 2: F-score accuracies of word segmenta-
tions produced by the adaptor grammar models on
the Chinese corpus without tones.
These rules distinguish syllable onsets in word-
initial position and syllable codas in word-final
position; the standard adaptor grammarmachinery
will then learn distributions over onsets and codas
in these positions that possibly differ from those
in word-internal positions.
4 Results on Chinese word segmentation
The previous section described two dimensions
along which adaptor grammars for word segmen-
tation can independently vary. Above the Word
level, there can be from zero to three levels of col-
locations, yielding four different values for this di-
mension. Below theWord level, phonemes can ei-
ther be treated as independent entities, or else they
can be grouped into onset, nuclei and coda clus-
ters, and these can vary depending on where they
appear within a word. Thus there are three dif-
ferent values for the syllable dimension, so there
are twelve different adaptor grammars overall. In
addition, we ran all of these grammars on two ver-
sions of the corpus, one with tones and one with-
out tones, so we report results for 24 different runs
here.
The adaptor grammar inference procedure we
used is the one described in Johnson and Goldwa-
ter (2009). We ran 1,000 iterations of 8 MCMC
chains for each run, and we discarded all but last
200 iterations in order to ?burn-in? the sampler.
The segmentation we predict is the one that occurs
the most frequently in the samples that were not
discarded. As is standard, we evaluate the models
in terms of token f-score; the results are presented
in Tables 1 and 2.
In these tables, ?None? indicates that the gram-
mar does not model syllable structure, ?Gen-
eral? indicates that the grammar does not distin-
guish word-peripheral from word-internal clus-
ters, while ?Specialised? indicates that it does.
?Unigram? indicates that the grammar does not
model collocational structure, otherwise the super-
script indicates the number of collocational levels
that the grammar captures.
Broadly speaking, the results are consistent with
the English word segmentation results using adap-
tor grammars presented by Johnson (2008b). The
unigram grammar segmentation accuracy is simi-
lar to that obtained for English, but the results for
the other models are lower than the results for the
corresponding adaptor grammars on English.
We see a general improvement in segmenta-
tion accuracy as the number of collocation levels
increases, just as for English. However, we do
not see any general improvements associated with
modelling syllables; indeed, it seems modelling
syllables causes accuracy to decrease unless collo-
cational structure is also modelled. This is some-
what surprising, as Chinese has a very regular syl-
labic structure. It is not surprising that distin-
guishing word-peripheral and word-medial clus-
ters does not improve segmentation accuracy, as
Chinese does not distinguish these kinds of clus-
ters. There is also no sign of the ?synergies? when
modelling collocations and syllables together that
Johnson (2008b) reported.
It is also surprising that tones seem to make lit-
tle difference to the segmentation accuracy, since
they are crucial for disambiguating lexical items.
The segmentation accuracy of the models that cap-
ture little or no inter-word dependencies (e.g., Un-
igram, Colloc) improved slightly when the input
contains tones, but the best-performing models
that capture a more complex set of inter-word de-
534
pendencies do equally well on the corpus without
tones as they do on the corpus with tones. Because
these models capture rich inter-word context (they
model three levels of collocational structure), it is
possible that this context provides sufficient infor-
mation to segment words even in the absence of
tone information, i.e., the tonal information is re-
dundant given the richer inter-word dependencies
that these models capture. It is also possible that
word segmentation may simply require less infor-
mation than lexical disambiguation.
One surprising result is the relatively poor per-
formance of the Colloc3 model without syllables
but with tones; we have no explanation for this.
However, all 8 of the MCMC chains in this run
produced lower f-scores, so it unlikely to be sim-
ply a random fluctuation produced by a single out-
lier.
Note that one should be cautious when compar-
ing the absolute f-scores from these experiments
with those of the English study, as the English and
Chinese corpora differ in many ways. As Tardif
(1993) (the creator of the Chinese corpus) empha-
sises, this corpus was collected in a much more
diverse linguistic environment with child-directed
speech from multiple caregivers. The children in-
volved in the Chinese corpus were also older than
the children in the English corpus, which may also
have affected the nature of the corpus.
5 Conclusion
This paper applied adaptor grammar models of
phonemic word segmentation originally devel-
oped for English to Chinese data. While the Chi-
nese data was prepared in a very different way
to the English data, the adaptor grammars used
to perform Chinese word segmentation were very
similar to those used for the English word seg-
mentation. They also achieved quite respectable
f-score accuracies, which suggests that the same
models can do well on both languages.
One puzzling result is that incorporating syl-
lable structure phonotactic constraints, which en-
hances English word segmentation accuracy con-
siderably, doesn?t seem to improve Chinese word
segmentation to a similar extent. This may reflect
the fact that the word segmentation adaptor gram-
mars were originally designed and tuned for En-
glish, and perhaps differently formulated syllable-
structure constraints would work well for Chinese.
But even if one can ?tune? the adaptor grammars
to improve performance on Chinese, the challenge
is doing this in a way that improves performance
on all languages, rather than just one.
Acknowledgments
The authors would like to thank the US NSF for
support for this research, which was begun while
they were on the faculty at Brown University in
the USA. The NSF supported this work through
NSF awards 0544127 and 0631667 to Mark John-
son and Katherine Demuth.
The adaptor grammar software is
freely available for download from
http://web.science.mq.edu.au/?mjohnson, and
the Chinese data was obtained from the Childes
archive.
References
Bernstein-Ratner, N. 1987. The phonology of parent-
child speech. In Nelson, K. and A. van Kleeck,
editors, Children?s Language, volume 6. Erlbaum,
Hillsdale, NJ.
Brent, M. and T. Cartwright. 1996. Distributional
regularity and phonotactic constraints are useful for
segmentation. Cognition, 61:93?125.
Brent, M. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
Elman, Jeffrey. 1990. Finding structure in time. Cog-
nitive Science, 14:197?211.
Fleck, Margaret M. 2008. Lexicalized phonotac-
tic word segmentation. In Proceedings of ACL-08:
HLT, pages 130?138, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Goldsmith, John A. 1990. Autosegmental and Metri-
cal Phonology. Basil Blackwell, Oxford, England.
Goldwater, Sharon, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 673?680,
Sydney, Australia. Association for Computational
Linguistics.
535
Goldwater, Sharon, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21 ? 54.
Johnson, Mark and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North AmericanChapter of the Association for
Computational Linguistics, pages 317?325, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Johnson, Mark, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In Scho?lkopf, B., J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641?648. MIT Press, Cambridge,
MA.
Johnson, Mark. 2008a. Unsupervised word seg-
mentation for Sesotho using adaptor grammars. In
Proceedings of the Tenth Meeting of ACL Special
Interest Group on Computational Morphology and
Phonology, pages 20?27, Columbus, Ohio, June.
Association for Computational Linguistics.
Johnson, Mark. 2008b. Using adaptor grammars to
identifying synergies in the unsupervised acquisition
of linguistic structure. In Proceedings of the 46th
AnnualMeeting of the Association of Computational
Linguistics, Columbus, Ohio. Association for Com-
putational Linguistics.
Joshi, Aravind. 2003. Tree adjoining grammars. In
Mikkov, Ruslan, editor, The Oxford Handbook of
Computational Linguistics, pages 483?501. Oxford
University Press, Oxford, England.
MacWhinney, Brian and Catherine Snow. 1985. The
child language data exchange system. Journal of
Child Language, 12:271?296.
Mochihashi, Daichi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of the Joint Conference of the 47th
AnnualMeeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 100?108, Suntec, Singapore,
August. Association for Computational Linguistics.
Swingley, Dan. 2005. Statistical clustering and the
contents of the infant vocabulary. Cognitive Psy-
chology, 50:86?132.
Tardif, Twila. 1993. Adult-to-child speech and lan-
guage acquisition in Mandarin Chinese. Ph.D. the-
sis, Yale University.
536
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 883?891,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Exploiting Social Information in Grounded Language Learning via
Grammatical Reductions
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
Mark.Johnson@MQ.edu.au
Katherine Demuth
Department of Linguistics
Macquarie University
Sydney, Australia
Katherine.Demuth@MQ.edu.au
Michael Frank
Department of Psychology
Stanford University
Stanford, California
mcfrank@Stanford.edu
Abstract
This paper uses an unsupervised model of
grounded language acquisition to study the
role that social cues play in language acqui-
sition. The input to the model consists of (or-
thographically transcribed) child-directed ut-
terances accompanied by the set of objects
present in the non-linguistic context. Each
object is annotated by social cues, indicating
e.g., whether the caregiver is looking at or
touching the object. We show how to model
the task of inferring which objects are be-
ing talked about (and which words refer to
which objects) as standard grammatical in-
ference, and describe PCFG-based unigram
models and adaptor grammar-based colloca-
tion models for the task. Exploiting social
cues improves the performance of all mod-
els. Our models learn the relative importance
of each social cue jointly with word-object
mappings and collocation structure, consis-
tent with the idea that children could discover
the importance of particular social informa-
tion sources during word learning.
1 Introduction
From learning sounds to learning the meanings of
words, social interactions are extremely important
for children?s early language acquisition (Baldwin,
1993; Kuhl et al, 2003). For example, children who
engage in more joint attention (e.g. looking at par-
ticular objects together) with caregivers tend to learn
words faster (Carpenter et al, 1998). Yet compu-
tational or formal models of social interaction are
rare, and those that exist have rarely gone beyond
the stage of cue-weighting models. In order to study
the role that social cues play in language acquisition,
this paper presents a structured statistical model of
grounded learning that learns a mapping between
words and objects from a corpus of child-directed
utterances in a completely unsupervised fashion. It
exploits five different social cues, which indicate
which object (if any) the child is looking at, which
object the child is touching, etc. Our models learn
the salience of each social cue in establishing refer-
ence, relative to their co-occurrence with objects that
are not being referred to. Thus, this work is consis-
tent with a view of language acquisition in which
children learn to learn, discovering organizing prin-
ciples for how language is organized and used so-
cially (Baldwin, 1993; Hollich et al, 2000; Smith et
al., 2002).
We reduce the grounded learning task to a gram-
matical inference problem (Johnson et al, 2010;
Bo?rschinger et al, 2011). The strings presented to
our grammatical learner contain a prefix which en-
codes the objects and their social cues for each ut-
terance, and the rules of the grammar encode rela-
tionships between these objects and specific words.
These rules permit every object to map to every
word (including function words; i.e., there is no
?stop word? list), and the learning process decides
which of these rules will have a non-trivial proba-
bility (these encode the object-word mappings the
system has learned).
This reduction of grounded learning to grammat-
ical inference allows us to use standard grammati-
cal inference procedures to learn our models. Here
we use the adaptor grammar package described in
Johnson et al (2007) and Johnson and Goldwater
(2009) with ?out of the box? default settings; no
parameter tuning whatsoever was done. Adaptor
grammars are a framework for specifying hierarchi-
cal non-parametric models that has been previously
used to model language acquisition (Johnson, 2008).
883
Social cue Value
child.eyes objects child is looking at
child.hands objects child is touching
mom.eyes objects care-giver is looking at
mom.hands objects care-giver is touching
mom.point objects care-giver is pointing to
Figure 1: The 5 social cues in the Frank et al (to appear)
corpus. The value of a social cue for an utterance is a
subset of the available topics (i.e., the objects in the non-
linguistic context) of that utterance.
A semanticist might argue that our view of refer-
ential mapping is flawed: full noun phrases (e.g., the
dog), rather than nouns, refer to specific objects, and
nouns denote properties (e.g., dog denotes the prop-
erty of being a dog). Learning that a noun, e.g., dog,
is part of a phrase used to refer to a specific dog (say,
Fido) does not suffice to determine the noun?s mean-
ing: the noun could denote a specific breed of dog,
or animals in general. But learning word-object rela-
tionships is a plausible first step for any learner: it is
often only the contrast between learned relationships
and novel relationships that allows children to in-
duce super- or sub-ordinate mappings (Clark, 1987).
Nevertheless, in deference to such objections, we
call the object that a phrase containing a given noun
refers to the topic of that noun. (This is also appro-
priate, given that our models are specialisations of
topic models).
Our models are intended as an ?ideal learner? ap-
proach to early social language learning, attempt-
ing to weight the importance of social and structural
factors in the acquisition of word-object correspon-
dences. From this perspective, the primary goal is
to investigate the relationships between acquisition
tasks (Johnson, 2008; Johnson et al, 2010), looking
for synergies (areas of acquisition where attempting
two learning tasks jointly can provide gains in both)
as well as areas where information overlaps.
1.1 A training corpus for social cues
Our work here uses a corpus of child-directed
speech annotated with social cues, described in
Frank et al (to appear). The corpus consists
of 4,763 orthographically-transcribed utterances of
caregivers to their pre-linguistic children (ages 6, 12,
and 18 months) during home visits where children
played with a consistent set of toys. The sessions
were video-taped, and each utterance was annotated
with the five social cues described in Figure 1.
Each utterance in the corpus contains the follow-
ing information:
? the sequence of orthographic words uttered by
the care-giver,
? a set of available topics (i.e., objects in the non-
linguistic objects),
? the values of the social cues, and
? a set of intended topics, which the care-giver
refers to.
Figure 2 presents this information for an example ut-
terance. All of these but the intended topics are pro-
vided to our learning algorithms; the intended top-
ics are used to evaluate the output produced by our
learners.
Generally the intended topics consist of zero or
one elements from the available topics, but not al-
ways: it is possible for the caregiver to refer to two
objects in a single utterance, or to refer to an object
not in the current non-linguistic context (e.g., to a
toy that has been put away). There is a considerable
amount of anaphora in this corpus, which our mod-
els currently ignore.
Frank et al (to appear) give extensive details on
the corpus, including inter-annotator reliability in-
formation for all annotations, and provide detailed
statistical analyses of the relationships between the
various social cues, the available topics and the in-
tended topics. That paper also gives instructions on
obtaining the corpus.
1.2 Previous work
There is a growing body of work on the role of social
cues in language acquisition. The language acqui-
sition research community has long recognized the
importance of social cues for child language acqui-
sition (Baldwin, 1991; Carpenter et al, 1998; Kuhl
et al, 2003).
Siskind (1996) describes one of the first exam-
ples of a model that learns the relationship between
words and topics, albeit in a non-statistical frame-
work. Yu and Ballard (2007) describe an associative
learner that associates words with topics and that
exploits prosodic as well as social cues. The rela-
tive importance of the various social cues are spec-
ified a priori in their model (rather than learned, as
they are here), and unfortunately their training cor-
pus is not available. Frank et al (2008) describes a
Bayesian model that learns the relationship between
words and topics, but the version of their model that
included social cues presented a number of chal-
lenges for inference. The unigram model we de-
scribe below corresponds most closely to the Frank
884
.dog # .pig child.eyes mom.eyes mom.hands # ## wheres the piggie
Figure 2: The photograph indicates non-linguistic context containing a (toy) pig and dog for the utterance Where?s the
piggie?. Below that, we show the representation of this utterance that serves as the input to our models. The prefix (the
portion of the string before the ?##?) lists the available topics (i.e., the objects in the non-linguistic context) and their
associated social cues (the cues for the pig are child.eyes, mom.eyes and mom.hands, while the dog is not associated
with any social cues). The intended topic is the pig. The learner?s goals are to identify the utterance?s intended topic,
and which words in the utterance are associated with which topic.
Sentence
Topic.pig
T.None
.dog
NotTopical.child.eyes
NotTopical.child.hands
NotTopical.mom.eyes
NotTopical.mom.hands
NotTopical.mom.point
#
Topic.pig
T.pig
.pig
Topical.child.eyes
child.eyes
Topical.child.hands
Topical.mom.eyes
Topical.mom.hands
mom.hands
Topical.mom.point
#
Topic.None
##
Words.pig
Word.None
wheres
Words.pig
Word.None
the
Words.pig
Word.pig
piggie
Figure 3: Sample parse generated by the Unigram PCFG. Nodes coloured red show how the ?pig? topic is propagated
from the prefix (before the ?##? separator) into the utterance. The social cues associated with each object are generated
either from a ?Topical? or a ?NotTopical? nonterminal, depending on whether the corresponding object is topical or
not.
885
et al model. Johnson et al (2010) reduces grounded
learning to grammatical inference for adaptor gram-
mars and shows how it can be used to perform word
segmentation as well as learning word-topic rela-
tionships, but their model does not take social cues
into account.
2 Reducing grounded learning with social
cues to grammatical inference
This section explains how we reduce ground learn-
ing problems with social cues to grammatical in-
ference problems, which lets us apply a wide vari-
ety of grammatical inference algorithms to grounded
learning problems. An advantage of reducing
grounded learning to grammatical inference is that
it suggests new ways to generalise grounded learn-
ing models; we explore three such generalisations
here. The main challenge in this reduction is finding
a way of expressing the non-linguistic information
as part of the strings that serve as the grammatical in-
ference procedure?s input. Here we encode the non-
linguistic information in a ?prefix? to each utterance
as shown in Figure 2, and devise a grammar such
that inference for the grammar corresponds to learn-
ing the word-topic relationships and the salience of
the social cues for grounded learning.
All our models associate each utterance with zero
or one topics (this means we cannot correctly anal-
yse utterances with more than one intended topic).
We analyse an utterance associated with zero topics
as having the special topic None, so we can assume
that every utterance has exactly one topic. All our
grammars generate strings of the form shown in Fig-
ure 2, and they do so by parsing the prefix and the
words of the utterance separately; the top-level rules
of the grammar force the same topic to be associated
with both the prefix and the words of the utterance
(see Figure 3).
2.1 Topic models and the unigram PCFG
As Johnson et al (2010) observe, this kind of
grounded learning can be viewed as a specialised
kind of topic inference in a topic model, where the
utterance topic is constrained by the available ob-
jects (possible topics). We exploit this observation
here using a reduction based on the reduction of
LDA topic models to PCFGs proposed by Johnson
(2010). This leads to our first model, the unigram
grammar, which is a PCFG.1
1In fact, the unigram grammar is equivalent to a HMM,
but the PCFG parameterisation makes clear the relationship
Sentence? Topict Wordst ?t ? T
?
TopicNone ? ##
Topict ? Tt TopicNone ?t ? T
?
Topict ? TNone Topict ?t ? T
Tt ? t Topicalc1 ?t ? T
Topicalci ? (ci) Topicalci+1 i = 1, . . . , `? 1
Topicalc` ? (c`) #
TNone ? t NotTopicalc1 ?t ? T
NotTopicalci ? (ci) NotTopicalci+1 i = 1, . . . , `? 1
NotTopicalc` ? (c`) #
Wordst ?WordNone (Wordst) ?t ? T ?
Wordst ?Wordt (Wordst) ?t ? T
Wordt ? w ?t ? T ?, w ?W
Figure 4: The rule schema that generate the unigram
PCFG. Here (c1, . . . , c`) is an ordered list of the so-
cial cues, T is the set of all non-None available topics,
T ? = T ? {None}, and W is the set of words appearing
in the utterances. Parentheses indicate optionality.
Figure 4 presents the rules of the unigram gram-
mar. This grammar has two major parts. The rules
expanding the Topict nonterminals ensure that the
social cues for the available topic t are parsed un-
der the Topical nonterminals. All other available
topics are parsed under TNone nonterminals, so their
social cues are parsed under NotTopical nontermi-
nals. The rules expanding these non-terminals are
specifically designed so that the generation of the so-
cial cues corresponds to a series of binary decisions
about each social cue. For example, the probability
of the rule
Topicalchild.eyes ? .child.eyes Topicalchild.hands
is the probability of an object that is an utterance
topic occuring with the child.eyes social cue. By es-
timating the probabilities of these rules, the model
effectively learns the probability of each social cue
being associated with a Topical or a NotTopical
available topic, respectively.
The nonterminals Wordst expand to a sequence
of Wordt and WordNone nonterminals, each of
which can expand to any word whatsoever. In prac-
tice Wordt will expand to those words most strongly
associated with topic t, while WordNone will expand
to those words not associated with any topic.
between grounded learning and estimation of grammar rule
weights.
886
Sentence? Topict Collocst ?t ? T
?
Collocst ? Colloct (Collocst) ?t ? T ?
Collocst ? CollocNone (Collocst) ?t ? T
Colloct ?Wordst ?t ? T ?
Wordst ?Wordt (Wordst) ?t ? T ?
Wordst ?WordNone (Wordst) ?t ? T
Wordt ?Word ?t ? T ?
Word? w ?w ?W
Figure 5: The rule schema that generate the collocation
adaptor grammar. Adapted nonterminals are indicated via
underlining. Here T is the set of all non-None available
topics, T ? = T ? {None}, and W is the set of words ap-
pearing in the utterances. The rules expanding the Topict
nonterminals are exactly as in unigram PCFG.
2.2 Adaptor grammars
Our other grounded learning models are based on
reductions of grounded learning to adaptor gram-
mar inference problems. Adaptor grammars are a
framework for stating a variety of Bayesian non-
parametric models defined in terms of a hierarchy of
Pitman-Yor Processes: see Johnson et al (2007) for
a formal description. Informally, an adaptor gram-
mar is specified by a set of rules just as in a PCFG,
plus a set of adapted nonterminals. The set of
trees generated by an adaptor grammar is the same
as the set of trees generated by a PCFG with the
same rules, but the generative process differs. Non-
adapted nonterminals in an adaptor grammar expand
just as they do in a PCFG: the probability of choos-
ing a rule is specified by its probability. However,
the expansion of an adapted nonterminal depends on
how it expanded in previous derivations. An adapted
nonterminal can directly expand to a subtree with
probability proportional to the number of times that
subtree has been previously generated; it can also
?back off? to expand using a grammar rule, just as
in a PCFG, with probability proportional to a con-
stant.2
Thus an adaptor grammar can be viewed as
caching each tree generated by each adapted non-
terminal, and regenerating it with probability pro-
portional to the number of times it was previously
generated (with some probability mass reserved to
generate ?new? trees). This enables adaptor gram-
2This is a description of Chinese Restaurant Processes,
which are the predictive distributions for Dirichlet Processes.
Our adaptor grammars are actually based on the more general
Pitman-Yor Processes, as described in Johnson and Goldwater
(2009).
Sentence
Topic.pig
...
Collocs.pig
Colloc.None
Words.None
Word.None
Word
wheres
Collocs.pig
Colloc.pig
Words.pig
Word.None
Word
the
Words.pig
Word.pig
Word
piggie
Figure 6: Sample parse generated by the collocation
adaptor grammar. The adapted nonterminals Colloct and
Wordt are shown underlined; the subtrees they dominate
are ?cached? by the adaptor grammar. The prefix (not
shown here) is parsed exactly as in the Unigram PCFG.
mars to generalise over subtrees of arbitrary size.
Generic software is available for adaptor grammar
inference, based either on Variational Bayes (Cohen
et al, 2010) or Markov Chain Monte Carlo (Johnson
and Goldwater, 2009). We used the latter software
because it is capable of performing hyper-parameter
inference for the PCFG rule probabilities and the
Pitman-Yor Process parameters. We used the ?out-
of-the-box? settings for this software, i.e., uniform
priors on all PCFG rule parameters, a Beta(2, 1)
prior on the Pitman-Yor a parameters and a ?vague?
Gamma(100, 0.01) prior on the Pitman-Yor b pa-
rameters. (Presumably performance could be im-
proved if the priors were tuned, but we did not ex-
plore this here).
Here we explore a simple ?collocation? extension
to the unigram PCFG which associates multiword
collocations, rather than individual words, with top-
ics. Hardisty et al (2010) showed that this signifi-
cantly improved performance in a sentiment analy-
sis task.
The collocation adaptor grammar in Figure 5 gen-
erates the words of the utterance as a sequence of
collocations, each of which is a sequence of words.
Each collocation is either associated with the sen-
tence topic or with the None topic, just like words in
the unigram model. Figure 6 shows a sample parse
generated by the collocation adaptor grammar.
We also experimented with a variant of the uni-
gram and collocation grammars in which the topic-
specific word distributions Wordt for each t ? T
887
Model Social Utterance topic Word topic Lexicon
cues acc. f-score prec. rec. f-score prec. rec. f-score prec. rec.
unigram none 0.3395 0.4044 0.3249 0.5353 0.2007 0.1207 0.5956 0.1037 0.05682 0.5952
unigram all 0.4907 0.6064 0.4867 0.8043 0.295 0.1763 0.9031 0.1483 0.08096 0.881
colloc none 0.4331 0.3513 0.3272 0.3792 0.2431 0.1603 0.5028 0.08808 0.04942 0.4048
colloc all 0.5837 0.598 0.5623 0.6384 0.4098 0.2702 0.8475 0.1671 0.09422 0.7381
unigram? none 0.3261 0.3767 0.3054 0.4914 0.1893 0.1131 0.5811 0.1167 0.06583 0.5122
unigram? all 0.5117 0.6106 0.4986 0.7875 0.2846 0.1693 0.891 0.1684 0.09402 0.8049
colloc? none 0.5238 0.3419 0.3844 0.3078 0.2551 0.1732 0.4843 0.2162 0.1495 0.3902
colloc? all 0.6492 0.6034 0.6664 0.5514 0.3981 0.2613 0.8354 0.3375 0.2269 0.6585
Figure 7: Utterance topic, word topic and lexicon results for all models, on data with and without social cues. The
results for the variant models, in which Wordt nonterminals expand via WordNone, are shown under unigram? and
colloc?. Utterance topic shows how well the model discovered the intended topics at the utterance level, word topic
shows how well the model associates word tokens with topics, and lexicon shows how well the topic most frequently
associated with a word type matches an external word-topic dictionary. In this figure and below, ?colloc? abbreviates
?collocation?, ?acc.? abbreviates ?accuracy?, ?prec.? abbreviates ?precision? and ?rec.? abbreviates ?recall?.
(the set of non-None available topics) expand via
WordNone non-terminals. That is, in the variant
grammars topical words are generated with the fol-
lowing rule schema:
Wordt ?WordNone ?t ? T
WordNone ?Word
Word? w ?w ?W
In these variant grammars, the WordNone nontermi-
nal generates all the words of the language, so it de-
fines a generic ?background? distribution over all the
words, rather than just the nontopical words. An ef-
fect of this is that the variant grammars tend to iden-
tify fewer words as topical.
3 Experimental evaluation
We performed grammatical inference using the
adaptor grammar software described in Johnson and
Goldwater (2009).3 All experiments involved 4 runs
of 5,000 samples each, of which the first 2,500 were
discarded for ?burn-in?.4 From these samples we
extracted the modal (i.e., most frequent) analysis,
3Because adaptor grammars are a generalisation of PCFGs,
we could use the adaptor grammar software to estimate the un-
igram model.
4We made no effort to optimise the computation, but it
seems the samplers actually stabilised after around a hundred
iterations, so it was probably not necessary to sample so exten-
sively. We estimated the error in our results by running our most
complex model (the colloc? model with all social cues) 20 times
(i.e., 20?8 chains for 5,000 iterations) so we could compute the
variance of each of the evaluation scores (it is reasonable to as-
sume that the simpler models will have smaller variance). The
standard deviation of all utterance topic and word topic mea-
sures is between 0.005 and 0.01; the standard deviation for lex-
icon f-score is 0.02, lexicon precision is 0.01 and lexicon recall
is 0.03. The adaptor grammar software uses a sentence-wise
which we evaluated as described below. The results
of evaluating each model on the corpus with social
cues, and on another corpus identical except that the
social cues have been removed, are presented in Fig-
ure 7.
Each model was evaluated on each corpus as fol-
lows. First, we extracted the utterance?s topic from
the modal parse (this can be read off the Topict
nodes), and compared this to the intended topics an-
notated in the corpus. The frequency with which
the models? predicted topics exactly matches the
intended topics is given under ?utterance topic ac-
curacy?; the f-score, precision and recall of each
model?s topic predictions are also given in the table.
Because our models all associate word tokens
with topics, we can also evaluate the accuracy with
which word tokens are associated with topics. We
constructed a small dictionary which identifies the
words that can be used as the head of a phrase to
refer to the topical objects (e.g., the dictionary in-
dicates that dog, doggie and puppy name the topi-
cal object DOG). Our dictionary is relatively conser-
vative; between one and eight words are associated
with each topic. We scored the topic label on each
word token in our corpus as follows. A topic label is
scored as correct if it is given in our dictionary and
the topic is one of the intended topics for the utter-
ance. The ?word topic? entries in Figure 7 give the
results of this evaluation.
blocked sampler, so it requires fewer iterations than a point-
wise sampler. We used 5,000 iterations because this is the soft-
ware?s default setting; evaluating the trace output suggests it
only takes several hundred iterations to ?burn in?. However, we
ran 8 chains for 25,000 iterations of the colloc? model; as ex-
pected the results of this run are within two standard deviations
of the results reported above.
888
Model Social Utterance topic Word topic Lexicon
cues acc. f-score prec. rec. f-score prec. rec. f-score prec. rec.
unigram none 0.3395 0.4044 0.3249 0.5353 0.2007 0.1207 0.5956 0.1037 0.05682 0.5952
unigram +child.eyes 0.4573 0.5725 0.4559 0.7694 0.2891 0.1724 0.8951 0.1362 0.07415 0.8333
unigram +child.hands 0.3399 0.4011 0.3246 0.5247 0.2008 0.121 0.5892 0.09705 0.05324 0.5476
unigram +mom.eyes 0.338 0.4023 0.3234 0.5322 0.1992 0.1198 0.5908 0.09664 0.053 0.5476
unigram +mom.hands 0.3563 0.4279 0.3437 0.5667 0.1984 0.1191 0.5948 0.09959 0.05455 0.5714
unigram +mom.point 0.3063 0.3548 0.285 0.4698 0.1806 0.1086 0.5359 0.09224 0.05057 0.5238
colloc none 0.4331 0.3513 0.3272 0.3792 0.2431 0.1603 0.5028 0.08808 0.04942 0.4048
colloc +child.eyes 0.5159 0.5006 0.4652 0.542 0.351 0.2309 0.7312 0.1432 0.07989 0.6905
colloc +child.hands 0.4827 0.4275 0.3999 0.4592 0.2897 0.1913 0.5964 0.1192 0.06686 0.5476
colloc +mom.eyes 0.4697 0.4171 0.3869 0.4525 0.2708 0.1781 0.5642 0.1013 0.05666 0.4762
colloc +mom.hands 0.4747 0.4251 0.3942 0.4612 0.274 0.1806 0.5666 0.09548 0.05337 0.4524
colloc +mom.point 0.4228 0.3378 0.3151 0.3639 0.2575 0.1716 0.5157 0.09278 0.05202 0.4286
Figure 8: Effect of using just one social cue on the experimental results for the unigram and collocation models. The
?importance? of a social cue can be quantified by the degree to which the model?s evaluation score improves when
using a corpus containing that social cue relative to its evaluation score when using a corpus without any social cues.
The most important social cue is the one which causes performance to improve the most.
Finally, we extracted a lexicon from the parsed
corpus produced by each model. We counted how
often each word type was associated with each topic
in our sampler?s output (including the None topic),
and assigned the word to its most frequent topic.
The ?lexicon? entries in Figure 7 show how well
the entries in these lexicons match the entries in the
manually-constructed dictionary discussed above.
There are 10 different evaluation scores, and no
model dominates in all of them. However, the top-
scoring result in every evaluation is always for a
model trained using social cues, demonstrating the
importance of these social cues. The variant colloca-
tion model (trained on data with social cues) was the
top-scoring model on four evaluation scores, which
is more than any other model.
One striking thing about this evaluation is that the
recall scores are all much higher than the precision
scores, for each evaluation. This indicates that all
of the models, especially the unigram model, are la-
belling too many words as topical. This is perhaps
not too surprising: because our models completely
lack any notion of syntactic structure and simply
model the association between words and topics,
they label many non-nouns with topics (e.g., woof
is typically labelled with the topic DOG).
3.1 Evaluating the importance of social cues
It is scientifically interesting to be able to evalu-
ate the importance of each of the social cues to
grounded learning. One way to do this is to study
the effect of adding or removing social cues from
the corpus on the ability of our models to perform
grounded learning. An important social cue should
have a large impact on our models? performance; an
unimportant cue should have little or no impact.
Figure 8 compares the performance of the uni-
gram and collocation models on corpora containing
a single social cue to their performance on the cor-
pus without any social cues, while Figure 9 com-
pares the performance of these models on corpora
containing all but one social cue to the corpus con-
taining all of the social cues. In both of these evalua-
tions, with respect to all 10 evaluation measures, the
child.eyes social cue had the most impact on model
performance.
Why would the child?s own gaze be more impor-
tant than the caregiver?s? Perhaps caregivers are fol-
lowing in, i.e., talking about objects that their chil-
dren are interested in (Baldwin, 1991). However, an-
other possible explanation is that this result is due to
the general continuity of conversational topics over
time. Frank et al (to appear) show that for the cur-
rent corpus, the topic of the preceding utterance is
very likely to be the topic of the current one also.
Thus, the child?s eyes might be a good predictor be-
cause they reflect the fact that the child?s attention
has been drawn to an object by previous utterances.
Notice that these two possible explanations of the
importance of the child.eyes cue are diametrically
opposed; the first explanation claims that the cue is
important because the child is driving the discourse,
while the second explanation claims that the cue is
important because the child?s gaze follows the topic
of the caregiver?s previous utterance. This sort of
question about causal relationships in conversations
may be very difficult to answer using standard de-
scriptive techniques, but it may be an interesting av-
889
Model Social Utterance topic Word topic Lexicon
cues acc. f-score prec. rec. f-score prec. rec. f-score prec. rec.
unigram all 0.4907 0.6064 0.4867 0.8043 0.295 0.1763 0.9031 0.1483 0.08096 0.881
unigram ?child.eyes 0.3836 0.4659 0.3738 0.6184 0.2149 0.1286 0.6546 0.1111 0.06089 0.6341
unigram ?child.hands 0.4907 0.6063 0.4863 0.8051 0.296 0.1769 0.9056 0.1525 0.08353 0.878
unigram ?mom.eyes 0.4799 0.5974 0.4768 0.7996 0.2898 0.1727 0.9007 0.1551 0.08486 0.9024
unigram ?mom.hands 0.4871 0.5996 0.4815 0.7945 0.2925 0.1746 0.8991 0.1561 0.08545 0.9024
unigram ?mom.point 0.4875 0.6033 0.4841 0.8004 0.2934 0.1752 0.9007 0.1558 0.08525 0.9024
colloc all 0.5837 0.598 0.5623 0.6384 0.4098 0.2702 0.8475 0.1671 0.09422 0.738
colloc ?child.eyes 0.5604 0.5746 0.529 0.6286 0.39 0.2561 0.8176 0.1534 0.08642 0.6829
colloc ?child.hands 0.5849 0.6 0.5609 0.6451 0.4145 0.273 0.8612 0.1662 0.09375 0.7317
colloc ?mom.eyes 0.5709 0.5829 0.5457 0.6255 0.4036 0.2655 0.8418 0.1662 0.09375 0.7317
colloc ?mom.hands 0.5795 0.5935 0.5571 0.6349 0.4038 0.2653 0.8442 0.1788 0.1009 0.7805
colloc ?mom.point 0.5851 0.6006 0.5607 0.6467 0.4097 0.2685 0.8644 0.1742 0.09841 0.7561
Figure 9: Effect of using all but one social cue on the experimental results for the unigram and collocation models.
The ?importance? of a social cue can be quantified by the degree to which the model?s evaluation score degrades when
that just social cue is removed from the corpus, relative to its evaluation score when using a corpus without all social
cues. The most important social cue is the one which causes performance to degrade the most.
enue for future investigation using more structured
models such as those proposed here.5
4 Conclusion and future work
This paper presented four different grounded learn-
ing models that exploit social cues. These models
are all expressed via reductions to grammatical in-
ference problems, so standard ?off the shelf? gram-
matical inference tools can be used to learn them.
Here we used the same adaptor grammar software
tools to learn all these models, so we can be rel-
atively certain that any differences we observe are
due to differences in the models, rather than quirks
in the software.
Because the adaptor grammar software performs
full Bayesian inference, including for model param-
eters, an unusual feature of our models is that we
did not need to perform any parameter tuning what-
soever. This feature is particularly interesting with
respect to the parameters on social cues. Psycholog-
ical proposals have suggested that children may dis-
cover that particular social cues help in establishing
reference (Baldwin, 1993; Hollich et al, 2000), but
prior modeling work has often assumed that cues,
cue weights, or both are prespecified. In contrast, the
models described here could in principle discover a
wide range of different social conventions.
5A reviewer suggested that we can test whether child.eyes
effectively provides the same information as the previous topic
by adding the previous topic as a (pseudo-) social cue. We tried
this, and child.eyes and previous.topic do in fact seem to convey
very similar information: e.g., the model with previous.topic
and without child.eyes scores essentially the same as the model
with all social cues.
Our work instantiates the strategy of investigating
the structure of children?s learning environment us-
ing ?ideal learner? models. We used our models to
investigate scientific questions about the role of so-
cial cues in grounded language learning. Because
the performance of all four models studied in this
paper improve dramatically when provided with so-
cial cues in all ten evaluation metrics, this paper pro-
vides strong support for the view that social cues are
a crucial information source for grounded language
learning.
We also showed that the importance of the differ-
ent social cues in grounded language learning can
be evaluated using ?add one cue? and ?subtract one
cue? methodologies. According to both of these, the
child.eyes cue is the most important of the five so-
cial cues studied here. There are at least two pos-
sible reasons for this: the caregiver?s topic could
be determined by the child?s gaze, or the child.eyes
cue could be providing our models with information
about the topic of the previous utterance.
Incorporating topic continuity and anaphoric de-
pendencies into our models would be likely to im-
prove performance. This improvement might also
help us distinguish the two hypotheses about the
child.eyes cue. If the child.eyes cue is just provid-
ing indirect information about topic continuity, then
the importance of the child.eyes cue should decrease
when we incorporate topic continuity into our mod-
els. But if the child?s gaze is in fact determining the
care-giver?s topic, then child.eyes should remain a
strong cue even when anaphoric dependencies and
topic continuity are incorporated into our models.
890
Acknowledgements
This research was supported under the Australian
Research Council?s Discovery Projects funding
scheme (project number DP110102506).
References
Dare A. Baldwin. 1991. Infants? contribution to the
achievement of joint reference. Child Development,
62(5):874?890.
Dare A. Baldwin. 1993. Infants? ability to consult the
speaker for clues to word reference. Journal of Child
Language, 20:395?395.
Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-
son. 2011. Reducing grounded learning tasks to gram-
matical inference. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1416?1425, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
M. Carpenter, K. Nagell, M. Tomasello, G. Butterworth,
and C. Moore. 1998. Social cognition, joint attention,
and communicative competence from 9 to 15 months
of age. Monographs of the society for research in child
development.
E.V. Clark. 1987. The principle of contrast: A constraint
on language acquisition. Mechanisms of language ac-
quisition, 1:33.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 564?
572, Los Angeles, California, June. Association for
Computational Linguistics.
Michael Frank, Noah Goodman, and Joshua Tenenbaum.
2008. A Bayesian framework for cross-situational
word-learning. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 457?464, Cambridge,
MA. MIT Press.
Michael C. Frank, Joshua Tenenbaum, and Anne Fernald.
to appear. Social and discourse contributions to the
determination of reference in cross-situational word
learning. Language, Learning, and Development.
Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 284?
292, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
G.J. Hollich, K. Hirsh-Pasek, and R. Golinkoff. 2000.
Breaking the language barrier: An emergentist coali-
tion model for the origins of word learning. Mono-
graphs of the Society for Research in Child Develop-
ment.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 317?325, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor Grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641?648. MIT Press, Cambridge, MA.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems
23, pages 1018?1026.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, pages 398?406, Columbus, Ohio. Association for
Computational Linguistics.
Mark Johnson. 2010. PCFGs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148?1157, Uppsala, Sweden, July.
Association for Computational Linguistics.
Patricia K. Kuhl, Feng-Ming Tsao, and Huei-Mei Liu.
2003. Foreign-language experience in infancy: Effects
of short-term exposure and social interaction on pho-
netic learning. Proceedings of the National Academy
of Sciences USA, 100(15):9096?9101.
Jeffrey Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition, 61(1-2):39?91.
L.B. Smith, S.S. Jones, B. Landau, L. Gershkoff-Stowe,
and L. Samuelson. 2002. Object name learning pro-
vides on-the-job training for attention. Psychological
Science, 13(1):13.
Chen Yu and Dana H Ballard. 2007. A unified model of
early word learning: Integrating statistical and social
cues. Neurocomputing, 70(13-15):2149?2165.
891
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1508?1516,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A joint model of word segmentation and phonological variation for
English word-final /t/-deletion
Benjamin Bo?rschinger1,3 and Mark Johnson1 and Katherine Demuth2
(1) Department of Computing, Macquarie University
(2) Department of Linguistics, Macquarie University
(3) Department of Computational Linguistics, Heidelberg University
{benjamin.borschinger, mark.johnson, katherine.demuth}@mq.edu.au
Abstract
Word-final /t/-deletion refers to a common
phenomenon in spoken English where
words such as /wEst/ ?west? are pro-
nounced as [wEs] ?wes? in certain con-
texts. Phonological variation like this is
common in naturally occurring speech.
Current computational models of unsu-
pervised word segmentation usually as-
sume idealized input that is devoid of
these kinds of variation. We extend a
non-parametric model of word segmenta-
tion by adding phonological rules that map
from underlying forms to surface forms
to produce a mathematically well-defined
joint model as a first step towards han-
dling variation and segmentation in a sin-
gle model. We analyse how our model
handles /t/-deletion on a large corpus of
transcribed speech, and show that the joint
model can perform word segmentation and
recover underlying /t/s. We find that Bi-
gram dependencies are important for per-
forming well on real data and for learning
appropriate deletion probabilities for dif-
ferent contexts.1
1 Introduction
Computational models of word segmentation try
to solve one of the first problems language learn-
ers have to face: breaking an unsegmented stream
of sound segments into individual words. Cur-
rently, most such models assume that the input
consists of sequences of phonemes with no pro-
nunciation variation across different occurrences
of the same word type. In this paper we describe
1The implementation of our model as well as
scripts to prepare the data will be made available at
http://web.science.mq.edu.au/~bborschi.
We can?t release our version of the Buckeye Corpus (Pitt et
al., 2007) directly because of licensing issues.
an extension of the Bayesian models of Gold-
water et al (2009) that incorporates phonologi-
cal rules to ?explain away? surface variation. As
a concrete example, we focus on word-final /t/-
deletion in English, although our approach is not
limited to this case. We choose /t/-deletion be-
cause it is a very common and well-studied phe-
nomenon (see Coetzee (2004, Chapter 5) for a
review) and segmental deletion is an interesting
test-case for our architecture. Recent work has
found that /t/-deletion (among other things) is in-
deed common in child-directed speech (CDS) and,
importantly, that its distribution is similar to that in
adult-directed speech (ADS) (Dilley et al, to ap-
pear). This justifies our using ADS to evaluate our
model, as discussed below.
Our experiments are consistent with long-
standing and recent findings in linguistics, in par-
ticular that /t/-deletion heavily depends on the im-
mediate context and that models ignoring context
work poorly on real data. We also examine how
well our models identify the probability of /t/-
deletion in different contexts. We find that models
that capture bigram dependencies between under-
lying forms provide considerably more accurate
estimates of those probabilities than correspond-
ing unigram or ?bag of words? models of underly-
ing forms.
In section 2 we discuss related work on han-
dling variation in computational models and on /t/-
deletion. Section 3 describes our computational
model and section 4 discusses its performance for
recovering deleted /t/s. We look at both a sit-
uation where word boundaries are pre-specified
and only inference for underlying forms has to
be performed; and the problem of jointly finding
the word boundaries and recovering deleted un-
derlying /t/s. Section 5 discusses our findings, and
section 6 concludes with directions for further re-
search.
1508
2 Background and related work
The work of Elsner et al (2012) is most closely
related to our goal of building a model that han-
dles variation. They propose a pipe-line archi-
tecture involving two separate generative models,
one for word-segmentation and one for phonolog-
ical variation. They model the mapping to sur-
face forms using a probabilistic finite-state trans-
ducer. This allows their architecture to handle
virtually arbitrary pronunciation variation. How-
ever, as they point out, combining the segmenta-
tion and the variation model into one joint model
is not straight-forward and usual inference proce-
dures are infeasible, which requires the use of sev-
eral heuristics. We pursue an alternative research
strategy here, starting with a single well-studied
example of phonological variation. This permits
us to develop a joint generative model for both
word segmentation and variation which we plan to
extend to handle more phenomena in future work.
An earlier work that is close to the spirit of our
approach is Naradowsky and Goldwater (2009),
who learn spelling rules jointly with a simple
stem-suffix model of English verb morphology.
Their model, however, doesn?t naturally extend to
the segmentation of entire utterances.
/t/-deletion has received a lot of attention within
linguistics, and we point the interested reader to
Coetzee (2004, Chapter 5) for a thorough review.
Briefly, the phenomenon is as follows: word-final
instances of /t/ may undergo deletion in natural
speech, such that /wEst/ ?west? is actually pro-
nounced as [wEs] ?wes?.2 While the frequency of
this phenomenon varies across social and dialectal
groups, within groups it has been found to be ro-
bust, and the probability of deletion depends on
its phonological context: a /t/ is more likely to
be dropped when followed by a consonant than
a vowel or a pause, and it is more likely to be
dropped when following a consonant than a vowel
as well. We point out two recent publications that
are of direct relevance to our research. Dilley et al
(to appear) study word-final variation in stop con-
sonants in CDS, the kind of input we ideally would
like to evaluate our models on. They find that ?in-
fants largely experience statistical distributions of
non-canonical consonantal pronunciation variants
[including deletion] that mirror those experienced
by adults.? This both directly establishes the need
2Following the convention in phonology, we give under-
lying forms within ?/. . . /? and surface forms within ?[. . . ]?.
for computational models to handle this dimension
of variation, and justifies our choice of using ADS
for evaluation, as mentioned above.
Coetzee and Kawahara (2013) provide a com-
putational study of (among other things) /t/-
deletion within the framework of Harmonic Gram-
mar. They do not aim for a joint model that also
handles word segmentation, however, and rather
than training their model on an actual corpus, they
evaluate on constructed lists of examples, mimick-
ing frequencies of real data. Overall, our findings
agree with theirs, in particular that capturing the
probability of deletion in different contexts does
not automatically result in good performance for
recovering individual deleted /t/s. We will come
back to this point in our discussion at the end of
the paper.
3 The computational model
Our models build on the Unigram and the Bigram
model introduced in Goldwater et al (2009). Fig-
ure 1 shows the graphical model for our joint Bi-
gram model (the Unigram case is trivially recov-
ered by generating the Ui,js directly from L rather
than from LUi,j?1). Figure 2 gives the mathemati-
cal description of the graphical model and Table 1
provides a key to the variables of our model.
The model generates a latent sequence of un-
derlying word-tokens U1, . . . , Un. Each word to-
ken is itself a non-empty sequence of segments or
phonemes, and each Uj corresponds to an under-
lying word form, prior to the application of any
phonological rule. This generative process is re-
peated for each utterance i, leading to multiple
utterances of the form Ui,1, . . . , Ui,ni where ni is
the number of words in the ith utterance, and Ui,j
is the jth word in the ith utterance. Each utter-
ance is padded by an observed utterance bound-
ary symbol $ to the left and to the right, hence
Ui,0 = Ui,ni+1 = $.3 Each Ui,j+1 is generated
conditionally on its predecessor Ui,j from LUi,j ,
as shown in the first row of the lower plate in Fig-
ure 1. Each Lw is a distribution over the pos-
sible words that can follow a token of w and L
is a global distribution over possible words, used
as back-off for all Lw. Just as in Goldwater et
al. (2009), L is drawn from a Dirichlet Process
(DP) with base distribution B and concentration
3Each utterance terminates as soon as a $ is generated,
thus determining the number of words ni in the ith utterance.
See Goldwater et al (2009) for discussion.
1509
Figure 1: The graphical model for our joint
model of word-final /t/-deletion and Bigram
word segmentation. The corresponding math-
ematical description is given in Figure 2. The
generative process mimics the intuitively plau-
sible idea of generating underlying forms from
some kind of syntactic model (here, a Bi-
gram language model) and then mapping the
underlying form to an observed surface-form
through the application of a phonological rule
component, here represented by the collection
of rule probabilities ?c.
L |?, ?0 ?DP (?0, B(? | ?))
Lw |L,?1 ?DP (?1, L)
?c |? ?Beta(1, 1)
Ui,0 = $
Si,0 = $
Ui,j+1 |Ui,j , LUi,j ?LUi,j
Si,j |Ui,j , Ui,j+1,? =PR(? | Ui,j , Ui,j+1)
Wi |Si,1, . . . , Si,ni = CAT(Si,0, . . . , Si,ni)
Figure 2: Mathematical description of our joint
Bigram model. The lexical generator B(? | ?)
is specified in Figure 3 and PR is explained in
the text below. CAT stands for concatenation
without word-boundaries, ni refers to the num-
ber of words in utterance i.
Variable Explanation
B base distribution over possible words
L back-off distribution over words
Lw distribution over words following w
Ui,j underlying form, a word
Si,j surface realization of Ui,j , a word
?c /t/-deletion probability in context c
Wi observed segments for ith utterance
Table 1: Key for the variables in Figure 1 and
Figure 2. See Figure 3 for the definition of B.
parameter ?0, and the word type specific distri-
butions Lw are drawn from a DP (L,?1), result-
ing in a hierarchical DP model (Teh et al, 2006).
The base distribution B functions as a lexical gen-
erator, defining a prior distribution over possible
words. In principle, B can incorporate arbitrary
prior knowledge about possible words, for exam-
ple syllable structure (cf. Johnson (2008)). In-
spired by Norris et al (1997), we use a simpler
possible word constraint that only rules out se-
quences that lack a vowel (see Figure 3). While
this is clearly a simplification it is a plausible as-
sumption for English data.
Instead of generating the observed sequence of
segments W directly by concatenating the under-
lying forms as in Goldwater et al (2009), we
map each Ui,j to a corresponding surface-form
Si,j by a probabilistic rule component PR. The
values over which the Si,j range are determined
by the available phonological processes. In the
model we study here, the phonological processes
only include a rule for deleting word-final /t/s
but in principle, PR can be used to encode a
wide variety of phonological rules. Here, Si,j ?
{Ui,j ,DELF(Ui,j)} if Ui,j ends in a /t/, and Si,j =
Ui,j otherwise, where DELF(u) refers to the same
word as u except that it lacks u?s final segment.
We look at three kinds of contexts on which a
rule?s probability of applying depends:
1. a uniform context that applies to every word-
final position
2. a right context that also considers the follow-
ing segment
3. a left-right context that additionally takes the
preceeding segment into account
For each possible context c there is a prob-
ability ?c which stands for the probability of
the rule applying in this context. Writing
1510
? ?Dir(?0.01, . . . , 0.01?)
B(w = x1:n | ?) =
{ [?ni=1 ?xi ]?#
Z if V(w)
0.0 if ?V(w)
Figure 3: Lexical generator with possible word-
constraint for words in ?+, ? being the alphabet
of available phonemes. x1:n is a sequence of ele-
ments of ? of length n. ? is a probability vector
of length |?| + 1 drawn from a sparse Dirichlet
prior, giving the probability for each phoneme and
the special word-boundary symbol #. The pred-
icate V holds of all sequences containing at least
one vowel. Z is a normalization constant that ad-
justs for the mass assigned to the empty and non-
possible words.
contexts in the notation familiar from genera-
tive phonology (Chomsky and Halle, 1968), our
model can be seen as implementing the fol-
lowing rules under the different assumptions:4
uniform /t/ ? ? / ]word
right /t/ ? ? / ]word ?
left-right /t/ ? ? / ? ]word ?
We let ? range over V(owel), C(onsonant) and $
(utterance-boundary), and ? over V and C. We
define a function CONT that maps a pair of ad-
jacent underlying forms Ui,j , Ui,j+1 to the con-
text of the final segment of Ui,j . For example,
CONT(/wEst/,/@v/) returns ?C ]word V? in the
left-right setting, or simply ? ]word? in the uni-
form setting. CONT returns a special NOT con-
text if Ui,j doesn?t end in a /t/. We stipulate that
?NOT = 0.0. Then we can define PR as follows:
PR(DELFINAL(u) | u, r)) = ?CONT(u,r)
PR(u | u, r) = 1? ?CONT(u,r)
Depending on the context setting used, our
model includes one (uniform), three (right) or six
(left-right) /t/-deletion probabilities ?c. We place a
uniform Beta prior on each of those so as to learn
their values in the LEARN-? experiments below.
Finally, the observed unsegmented utterances
Wi are generated by concatenating all Si,j using
the function CAT.
We briefly comment on the central intuition
of this model, i.e. why it can infer underlying
4For right there are three and for left-right six different
rules, one for every instantiation of the context-template.
from surface forms. Bayesian word segmentation
models try to compactly represent the observed
data in terms of a small set of units (word types)
and a short analysis (a small number of word
tokens). Phonological rules such as /t/-deletion
can ?explain away? an observed surface type such
as [wEs]] in terms of the underlying type /wEst/
which is independently needed for surface tokens
of [wEst]. Thus, the /t/? ? rule makes possi-
ble a smaller lexicon for a given number of sur-
face tokens. Obviously, human learners have ac-
cess to additional cues, such as the meaning of
words, knowledge of phonological similarity be-
tween segments and so forth. One of the advan-
tages of an explicitly defined generative model
such as ours is that it is straight-forward to grad-
ually extend it by adding more cues, as we point
out in the discussion.
3.1 Inference
Just as for the Goldwater et al (2009) segmen-
tation models, exact inference is infeasible for
our joint model. We extend the collapsed Gibbs
breakpoint-sampler described in Goldwater et al
(2009) to perform inference for our extended mod-
els. We refer the reader to their paper for addi-
tional details such as how to calculate the Bigram
probabilities in Figure 4. Here we focus on the
required changes to the sampler so as to perform
inference under our richer model. We consider the
case of a single surface string W , so we drop the
i-index in the following discussion.
Knowing W , the problem is to recover the un-
derlying forms U1, . . . , Un and the surface forms
S1, . . . , Sn for unknown n. A major insight in
Goldwater?s work is that rather than sampling over
the latent variables in the model directly (the num-
ber of which we don?t even know), we can instead
perform Gibbs sampling over a set of boundary
variables b1, . . . , b|W |?1 that jointly determine the
values for our variables of interest where |W | is
the length of the surface string W . For our model,
each bj ? {0, 1, t}, where bj = 0 indicates ab-
sence of a word boundary, bj = 1 indicates pres-
ence of a boundary and bj = t indicates pres-
ence of a boundary with a preceeding underlying
/t/. The relation between the bj and the S1, . . . , Sn
and U1, . . . , Un is illustrated in Figure 5. The re-
quired sampling equations are given in Figure 4.
1511
P (bj = 0 | b?j) ? P (w12,u | wl,u, b?j)? Pr(w12,s | w12,u, wr,u)? P (wr,u | w12,u, b?j ? ?wl,u, w12,u?) (1)
P (bj = t | b?j) ? P (w1,t | wl,u, b?j)? Pr(w1,s | w1,t, w2,u)? P (w2,u | w1,t, b?j ? ?wl,u, w1,t?)
? Pr(w2,s | w2,u, wr,u)? P (wr,u | w2,u, b?j ? ?wl,u, w1,t? ? ?w1,t, w2,u?) (2)
P (bj = 1 | b?j) ? P (w1,s | wl,u, b?j)? Pr(w1,s | w1,s, w2,u)? P (w2,u | w1,s, b?j ? ?wl,u, w1,s?)
? Pr(w2,s | w2,u, wr,u)? P (wr,u | w2,u, b?j ? ?wl,u, w1,s? ? ?w1,s, w2,u?) (3)
Figure 4: Sampling equations for our Gibbs sampler, see figure 5 for illustration. bj = 0 corresponds
to no boundary at this position, bj = t to a boundary with a preceeding underlying /t/ and bj = 1 to a
boundary with no additional underlying /t/. We use b?j for the statistics determined by all but the jth
position and b?j ? ?r, l? for these statistics plus an additional count of the bigram ?r, l?. P (w | l, b)
refers to the bigram probability of ?l, w? given the the statistics b; we refer the reader to Goldwater et
al. (2009) for the details of calculating these bigram probabilities and details about the required statistics
for the collapsed sampler. PR is defined in the text.
1 10 t 1I h      i  i       t $
underlyingsurfaceboundariesobserved I h i i t $
I h      i       t  i       t $
Figure 5: The relation between the observed se-
quence of segments (bottom), the boundary vari-
ables b1, . . . , b|W |?1 the Gibbs sampler operates
over (in squares), the latent sequence of sur-
face forms and the latent sequence of underly-
ing forms. When sampling a new value for
b3 = t, the different word-variables in fig-
ure 4 are: w12,u=w12,s=hiit, w1,t=hit and w1,s=hi,
w2,u=w2,s=it, wl,u=I, wr,u=$. Note that we need
a boundary variable at the end of the utterance as
there might be an underlying /t/ at this position as
well. The final boundary variable is set to 1, not t,
because the /t/ in it is observed.
4 Experiments
4.1 The data
We are interested in how well our model han-
dles /t/-deletion in real data. Ideally, we?d eval-
uate it on CDS but as of now, we know of no
available large enough corpus of accurately hand-
transcribed CDS. Instead, we used the Buckeye
Corpus (Pitt et al, 2007) for our experiments,
a large ADS corpus of interviews with English
speakers that have been transcribed with relatively
fine phonetic detail, with /t/-deletion among the
things manually annotated. Pointing to the re-
cent work by Dilley et al (to appear) we want
to emphasize that the statistical distribution of /t/-
deletion has been found to be similar for ADS and
orthographic I don?t intend to
transcript /aI R oU n I n t E n d @/
idealized /aI d oU n t I n t E n d t U/
t-drop /aI d oU n I n t E n d t U/
Figure 6: An example fragment from the Buckeye-
corpus in orthographic form, the fine transcript
available in the Buckeye corpus, a fully idealized
pronunciation with canonical dictionary pronunci-
ations and our version of the data with dropped
/t/s.
CDS, at least for read speech.
We automatically derived a corpus of 285,792
word tokens across 48,795 utterances from the
Buckeye Corpus by collecting utterances across all
interviews and heuristically splitting utterances at
speaker-turn changes and indicated silences. The
Buckeye corpus lists for each word token a man-
ually transcribed pronunciation in context as well
as its canonical pronunciation as given in a pro-
nouncing dictionary. As input to our model, we
use the canonical pronunciation unless the pronun-
ciation in context indicates that the final /t/ has
been deleted in which case we also delete the final
/t/ of the canonical pronunciation Figure 6 shows
an example from the Buckeye Corpus, indicating
how the original data, a fully idealized version
and our derived input that takes into account /t/-
deletions looks like.
Overall, /t/-deletion is a quite frequent phe-
nomenon with roughly 29% of all underlying /t/s
being dropped. The probabilities become more
peaked when looking at finer context; see Table 3
for the empirical distribution of /t/-dropping for
the six different contexts of the left-right setting.
1512
4.2 Recovering deleted /t/s, given word
boundaries
In this set of experiments we are interested in how
well our model recovers /t/s when it is provided
with the gold word boundaries. This allows us
to investigate the strength of the statistical sig-
nal for the deletion rule without confounding it
with the word segmentation performance, and to
see how the different contextual settings uniform,
right and left-right handle the data. Concretely,
for the example in Figure 6 this means that we tell
the model that there are boundaries between /aI/,
/doUn/, /IntEnd/, /tu/ and /liv/ but we don?t tell it
whether or not these words end in an underlying
/t/. Even in this simple example, there are 5 possi-
ble positions for the model to posit an underlying
/t/. We evaluate the model in terms of F-score, the
harmonic mean of recall (the fraction of underly-
ing /t/s the model correctly recovered) and preci-
sion (the fraction of underlying /t/s the model pre-
dicted that were correct).
In these experiments, we ran a total of 2500 it-
erations with a burnin of 2000. We collect sam-
ples with a lag of 10 for the last 500 iterations and
perform maximum marginal decoding over these
samples (Johnson and Goldwater, 2009), as well
as running two chains so as to get an idea of the
variance.5
We are also interested in how well the model
can infer the rule probabilities from the data, that
is, whether it can learn values for the different ?c
parameters. We compare two settings, one where
we perform inference for these parameters assum-
ing a uniform Beta prior on each ?c (LEARN-?)
and one where we provide the model with the em-
pirical probabilities for each ?c as estimated off
the gold-data (GOLD-?), e.g., for the uniform con-
dition 0.29. The results are shown in Table 2.
Best performance for both the Unigram and
the Bigram model in the GOLD-? condition is
achieved under the left-right setting, in line with
the standard analyses of /t/-deletion as primarily
being determined by the preceding and the follow-
ing context. For the LEARN-? condition, the Bi-
gram model still performs best in the left-right set-
ting but the Unigram model?s performance drops
5As manually setting the hyper-parameters for the DPs in
our model proved to be complicated and may be objected to
on principled grounds, we perform inference for them under
a vague gamma prior, as suggested by Teh et al (2006) and
Johnson and Goldwater (2009), using our own implementa-
tion of a slice-sampler (Neal, 2003).
uniform right left-right
Unigram LEARN-? 56.52 39.28 23.59GOLD-? 62.08 60.80 66.15
Bigram LEARN-? 60.85 62.98 77.76GOLD-? 69.06 69.98 73.45
Table 2: F-score of recovered /t/s with known
word boundaries on real data for the three differ-
ent context settings, averaged over two runs (all
standard errors below 2%). Note how the Uni-
gram model always suffers in the LEARN-? condi-
tion whereas the Bigram model?s performance is
actually best for LEARN-? in the left-right setting.
C C C V C $ V C V V V $
empirical 0.62 0.42 0.36 0.23 0.15 0.07
Unigram 0.41 0.33 0.17 0.07 0.05 0.00
Bigram 0.70 0.58 0.43 0.17 0.13 0.06
Table 3: Inferred rule-probabilities for different
contexts in the left-right setting from one of the
runs. ?C C? stands for the context where the
deleted /t/ is preceded and followed by a conso-
nant, ?V $? stands for the context where it is pre-
ceded by a vowel and followed by the utterance
boundary. Note how the Unigram model severely
under-estimates and the Bigram model slightly
over-estimates the probabilities.
in all settings and is now worst in the left-right and
best in the uniform setting.
In fact, comparing the inferred probabilities
to the ?ground truth? indicates that the Bigram
model estimates the true probabilities more ac-
curately than the Unigram model, as illustrated
in Table 3 for the left-right setting. The Bi-
gram model somewhat overestimates the probabil-
ity for all post-consonantal contexts but the Uni-
gram model severely underestimates the probabil-
ity of /t/-deletion across all contexts.
4.3 Artificial data experiments
To test our Gibbs sampling inference procedure,
we ran it on artificial data generated according to
the model itself. If our inference procedure fails
to recover the underlying /t/s accurately in this set-
ting, we should not expect it to work well on actual
data. We generated our artificial data as follows.
We transformed the sequence of canonical pronun-
ciations in the Buckeye corpus (which we take to
be underlying forms here) by randomly deleting
final /t/s using empirical probabilities as shown in
Table 3 to generate a sequence of artificial sur-
face forms that serve as input to our models. We
1513
uniform right left-right
Unigram LEARN-? 94.35 23.55 (+) 63.06GOLD-? 94.45 94.20 91.83
Bigram LEARN-? 92.72 91.64 88.48GOLD-? 92.88 92.33 89.32
Table 4: F-score of /t/-recovery with known word
boundaries on artificial data, each condition tested
on data that corresponds to the assumption, aver-
aged over two runs (standard errors less than 2%
except (+) = 3.68%)).
Unigram Bigram
LEARN-? 33.58 55.64
GOLD-? 55.92 57.62
Table 5: /t/-recovery F-scores when performing
joint word segmention in the left-right setting, av-
eraged over two runs (standard errors less than
2%). See Table 6 for the corresponding segmenta-
tion F-scores.
did this for all three context settings, always es-
timating the deletion probability for each context
from the gold-standard. The results of these exper-
iments are given in table 4. Interestingly, perfor-
mance on these artificial data is considerably bet-
ter than on the real data. In particular the Bigram
model is able to get consistently high F-scores for
both the LEARN-? and the GOLD-? setting. For
the Unigram model, we again observe the severe
drop in the LEARN-? setting for the right and left-
right settings although it does remarkably well in
the uniform setting, and performs well across all
settings in the GOLD-? condition. We take this to
show that our inference algorithm is in fact work-
ing as expected.
4.4 Segmentation experiments
Finally, we are also interested to learn how well
we can do word segmentation and underlying /t/-
recovery jointly. Again, we look at both the
LEARN-? and GOLD-? conditions but focus on the
left-right setting as this worked best in the exper-
iments above. For these experiments, we perform
simulated annealing throughout the initial 2000 it-
erations, gradually cooling the temperature from
5 to 1, following the observation by Goldwater
et al (2009) that without annealing, the Bigram
model gets stuck in sub-optimal parts of the solu-
tion space early on. During the annealing stage,
we prevent the model from performing inference
for underlying /t/s so that the annealing stage can
be seen as an elaborate initialisation scheme, and
we perform joint inference for the remaining 500
iterations, evaluating on the last sample and av-
eraging over two runs. As neither the Unigram
nor the Bigram model performs ?perfect? word
segmentation, we expect to see a degradation in
/t/-recovery performance and this is what we find
indeed. To give an impression of the impact of
/t/-deletion, we also report numbers for running
only the segmentation model on the Buckeye data
with no deleted /t/s and on the data with deleted
/t/s. The /t/-recovery scores are given in Table 5
and segmentation scores in Table 6. Again the
Unigram model?s /t/-recovery score degrades dra-
matically in the LEARN-? condition. Looking at
the segmentation performance this isn?t too sur-
prising: the Unigram model?s poorer token F-
score, the standard measure of segmentation per-
formance on a word token level, suggests that it
misses many more boundaries than the Bigram
model to begin with and, consequently, can?t re-
cover any potential underlying /t/s at these bound-
aries. Also note that in the GOLD-? condition, our
joint Bigram model performs almost as well on
data with /t/-deletions as the word segmentation
model on data that includes no variation at all.
The generally worse performance of handling
variation as measured by /t/-recovery F-score
when performing joint segmentation is consistent
with the finding of Elsner et al (2012) who report
considerable performance drops for their phono-
logical learner when working with induced bound-
aries (note, however, that their model does not per-
form joint inference, rather the induced boundaries
are given to their phonological learner as ground-
truth).
5 Discussion
There are two interesting findings from our exper-
iments. First of all, we find a much larger differ-
ence between the Unigram and the Bigram model
in the LEARN-? condition than in the GOLD-? con-
dition. We suggest that this is due to the Unigram
model?s lack of dependencies between underlying
forms, depriving it of an important source of ev-
idence. Bigram dependencies provide additional
evidence for underlying /t/ that are deleted on the
surface, and because the Bigram model identifies
these underlying /t/ more accurately, it can also es-
timate the /t/ deletion probability more accurately.
1514
Unigram Bigram
LEARN-? 54.53 72.55 (2.3%)
GOLD-? 54.51 73.18
NO-? 54.61 70.12
NO-VAR 54.12 73.99
Table 6: Word segmentation F-scores for the /t/-
recovery F-scores in Table 5 averaged over two
runs (standard errors less than 2% unless given).
NO-? are scores for running just the word segmen-
tation model with no /t/-deletion rule on the data
that includes /t/-deletion, NO-VAR for running just
the word segmentation model on the data with no
/t/-deletions.
For example, /t/ dropping in ?don?t you? yields
surface forms ?don you?. Because the word bi-
gram probability P (you | don?t) is high, the bi-
gram model prefers to analyse surface ?don? as
underlying ?don?t?. The Unigram model does not
have access to word bigram information so the
underlying forms it posits are less accurate (as
shown in Table 2), and hence the estimate of the
/t/-deletion probability is also less accurate. When
the probabilities of deletion are pre-specified the
Unigram model performs better but still consider-
ably worse than the Bigram model when the word
boundaries are known, suggesting the importance
of non-phonological contextual effects that the Bi-
gram model but not the Unigram model can cap-
ture. This suggests that for example word pre-
dictability in context might be an important factor
contributing to /t/-deletion.
The other striking finding is the considerable
drop in performance between running on natural-
istic and artificially created data. This suggests
that the natural distribution of /t/-deletion is much
more complex than can be captured by statistics
over the phonological contexts we examined. Fol-
lowing Guy (1991), a finer-grained distinction for
the preceeding segments might address this prob-
lem.
Yet another suggestion comes from the recent
work in Coetzee and Kawahara (2013) who claim
that ?[a] model that accounts perfectly for the
overall rate of application of some variable pro-
cess therefore does not necessarily account very
well for the actual application of the process to in-
dividual words.? They argue that in particular the
extremely high deletion rates typical of high fre-
quency items aren?t accurately captured when the
deletion probability is estimated across all types.
A look at the error patterns of our model on a sam-
ple from the Bigram model in the LEARN-? setting
on the naturalistic data suggests that this is in fact a
problem. For example, the word ?just? has an ex-
tremely high rate of deletion with 17462442 = 0.71%.While many tokens of ?jus? are ?explained away?
through predicting underlying /t/s, the (literally)
extra-ordinary frequency of ?jus?-tokens lets our
model still posit it as an underlying form, although
with a much dampened frequency (of the 1746 sur-
face tokens, 1081 are analysed as being realiza-
tions of an underlying ?just?).
The /t/-recovery performance drop when per-
forming joint word segmentation isn?t surprising
as even the Bigram model doesn?t deliver a very
high-quality segmentation to begin with, leading
to both sparsity (through missed word-boundaries)
and potential noise (through misplaced word-
boundaries). Using a more realistic generative
process for the underlying forms, for example an
Adaptor Grammar (Johnson et al, 2007), could
address this shortcoming in future work without
changing the overall architecture of the model al-
though novel inference algorithms might be re-
quired.
6 Conclusion and outlook
We presented a joint model for word segmentation
and the learning of phonological rule probabili-
ties from a corpus of transcribed speech. We find
that our Bigram model reaches 77% /t/-recovery
F-score when run with knowledge of true word-
boundaries and when it can make use of both the
preceeding and the following phonological con-
text, and that unlike the Unigram model it is able
to learn the probability of /t/-deletion in different
contexts. When performing joint word segmen-
tation on the Buckeye corpus, our Bigram model
reaches around above 55% F-score for recovering
deleted /t/s with a word segmentation F-score of
around 72% which is 2% better than running a Bi-
gram model that does not model /t/-deletion.
We identified additional factors that might help
handling /t/-deletion and similar phenomena. A
major advantage of our generative model is the
ease and transparency with which its assump-
tions can be modified and extended. For fu-
ture work we plan to incorporate into our model
richer phonological contexts, item- and frequency-
specific probabilities and more direct use of word
1515
predictability. We also plan to extend our model
to handle additional phenomena, an obvious can-
didate being /d/-deletion.
Also, the two-level architecture we present is
not limited to the mapping being defined in terms
of rules rather than constraints in the spirit of Op-
timality Theory (Prince and Smolensky, 2004); we
plan to explore this alternative path as well in fu-
ture work.
To conclude, we presented a model that pro-
vides a clean framework to test the usefulness of
different factors for word segmentation and han-
dling phonological variation in a controlled man-
ner.
Acknowledgements
We thank the anonymous reviewers for their
valuable comments. This research was sup-
ported under Australian Research Council?s Dis-
covery Projects funding scheme (project numbers
DP110102506 and DP110102593).
References
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Haper & Row, New York.
Andries W. Coetzee and Shigeto Kawahara. 2013. Fre-
quency biases in phonological variation. Natural
Language and Linguisic Theory, 31:47?89.
Andries W. Coetzee. 2004. What it Means to be a
Loser: Non-Optimal Candidates in Optimality The-
ory. Ph.D. thesis, University of Massachusetts ,
Amherst.
Laura Dilley, Amanda Millett, J. Devin McAuley, and
Tonya R. Bergeson. to appear. Phonetic variation
in consonants in infant-directed and adult-directed
speech: The case of regressive place assimilation
in word-final alveolar stops. Journal of Child Lan-
guage.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and
phonetic acquisition. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics, pages 184?193, Jeju Island, Korea. As-
sociation for Computational Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Gregory R. Guy. 1991. Contextual conditioning in
variable lexical phonology. Language Variation and
Change, 3(2):223?39.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317?325,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B. Scho?lkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641?648. MIT Press, Cambridge,
MA.
Mark Johnson. 2008. Using Adaptor Grammars to
identify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-
nual Meeting of the Association of Computational
Linguistics, pages 398?406, Columbus, Ohio. Asso-
ciation for Computational Linguistics.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving morphology induction by learning spelling
rules. In Proceedings of the 21st international jont
conference on Artifical intelligence, pages 1531?
1536, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Dennis Norris, James M. Mcqueen, Anne Cutler, and
Sally Butterfield. 1997. The possible-word con-
straint in the segmentation of continuous speech.
Cognitive Psychology, 34(3):191 ? 243.
Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech.
Alan Prince and Paul Smolensky. 2004. Optimality
Theory: Constraint Interaction in Generative Gram-
mar. Blackwell.
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
1516
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 282?292,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Modelling function words improves unsupervised word segmentation
Mark Johnson
1,2
, Anne Christophe
3,4
, Katherine Demuth
2,6
and Emmanuel Dupoux
3,5
1
Department of Computing, Macquarie University, Sydney, Australia
2
Santa Fe Institute, Santa Fe, New Mexico, USA
3
Ecole Normale Sup?erieure, Paris, France
4
Centre National de la Recherche Scientifique, Paris, France
5
Ecole des Hautes Etudes en Sciences Sociales, Paris, France
6
Department of Linguistics, Macquarie University, Sydney, Australia
Abstract
Inspired by experimental psychological
findings suggesting that function words
play a special role in word learning, we
make a simple modification to an Adaptor
Grammar based Bayesian word segmenta-
tion model to allow it to learn sequences
of monosyllabic ?function words? at the
beginnings and endings of collocations
of (possibly multi-syllabic) words. This
modification improves unsupervised word
segmentation on the standard Bernstein-
Ratner (1987) corpus of child-directed En-
glish by more than 4% token f-score com-
pared to a model identical except that it
does not special-case ?function words?,
setting a new state-of-the-art of 92.4% to-
ken f-score. Our function word model as-
sumes that function words appear at the
left periphery, and while this is true of
languages such as English, it is not true
universally. We show that a learner can
use Bayesian model selection to determine
the location of function words in their lan-
guage, even though the input to the model
only consists of unsegmented sequences of
phones. Thus our computational models
support the hypothesis that function words
play a special role in word learning.
1 Introduction
Over the past two decades psychologists have in-
vestigated the role that function words might play
in human language acquisition. Their experiments
suggest that function words play a special role in
the acquisition process: children learn function
words before they learn the vast bulk of the asso-
ciated content words, and they use function words
to help identify context words.
The goal of this paper is to determine whether
computational models of human language acqui-
sition can provide support for the hypothesis that
function words are treated specially in human
language acquisition. We do this by comparing
two computational models of word segmentation
which differ solely in the way that they model
function words. Following Elman et al (1996)
and Brent (1999) our word segmentation models
identify word boundaries from unsegmented se-
quences of phonemes corresponding to utterances,
effectively performing unsupervised learning of a
lexicon. For example, given input consisting of
unsegmented utterances such as the following:
j u w ? n t t u s i ? ? b ? k
a word segmentation model should segment this as
ju w?nt tu si ?? b?k, which is the IPA representation
of ?you want to see the book?.
We show that a model equipped with the abil-
ity to learn some rudimentary properties of the
target language?s function words is able to learn
the vocabulary of that language more accurately
than a model that is identical except that it is inca-
pable of learning these generalisations about func-
tion words. This suggests that there are acqui-
sition advantages to treating function words spe-
cially that human learners could take advantage of
(at least to the extent that they are learning similar
generalisations as our models), and thus supports
the hypothesis that function words are treated spe-
cially in human lexical acquisition. As a reviewer
points out, we present no evidence that children
use function words in the way that our model does,
and we want to emphasise we make no such claim.
While absolute accuracy is not directly relevant
to the main point of the paper, we note that the
models that learn generalisations about function
words perform unsupervised word segmentation
at 92.5% token f-score on the standard Bernstein-
Ratner (1987) corpus, which improves the previ-
ous state-of-the-art by more than 4%.
As a reviewer points out, the changes we make
to our models to incorporate function words can
be viewed as ?building in? substantive informa-
tion about possible human languages. The model
282
that achieves the best token f-score expects func-
tion words to appear at the left edge of phrases.
While this is true for languages such as English,
it is not true universally. By comparing the pos-
terior probability of two models ? one in which
function words appear at the left edges of phrases,
and another in which function words appear at the
right edges of phrases ? we show that a learner
could use Bayesian posterior probabilities to deter-
mine that function words appear at the left edges
of phrases in English, even though they are not
told the locations of word boundaries or which
words are function words.
This paper is structured as follows. Section 2
describes the specific word segmentation mod-
els studied in this paper, and the way we ex-
tended them to capture certain properties of func-
tion words. The word segmentation experiments
are presented in section 3, and section 4 discusses
how a learner could determine whether function
words occur on the left-periphery or the right-
periphery in the language they are learning. Sec-
tion 5 concludes and describes possible future
work. The rest of this introduction provides back-
ground on function words, the Adaptor Grammar
models we use to describe lexical acquisition and
the Bayesian inference procedures we use to infer
these models.
1.1 Psychological evidence for the role of
function words in word learning
Traditional descriptive linguistics distinguishes
function words, such as determiners and prepo-
sitions, from content words, such as nouns and
verbs, corresponding roughly to the distinction be-
tween functional categories and lexical categories
of modern generative linguistics (Fromkin, 2001).
Function words differ from content words in at
least the following ways:
1. there are usually far fewer function word
types than content word types in a language
2. function word types typically have much
higher token frequency than content word
types
3. function words are typically morphologically
and phonologically simple (e.g., they are typ-
ically monosyllabic)
4. function words typically appear in peripheral
positions of phrases (e.g., prepositions typi-
cally appear at the beginning of prepositional
phrases)
5. each function word class is associated with
specific content word classes (e.g., deter-
miners and prepositions are associated with
nouns, auxiliary verbs and complementisers
are associated with main verbs)
6. semantically, content words denote sets of
objects or events, while function words de-
note more complex relationships over the en-
tities denoted by content words
7. historically, the rate of innovation of function
words is much lower than the rate of innova-
tion of content words (i.e., function words are
typically ?closed class?, while content words
are ?open class?)
Properties 1?4 suggest that function words
might play a special role in language acquisition
because they are especially easy to identify, while
property 5 suggests that they might be useful for
identifying lexical categories. The models we
study here focus on properties 3 and 4, in that
they are capable of learning specific sequences of
monosyllabic words in peripheral (i.e., initial or
final) positions of phrase-like units.
A number of psychological experiments have
shown that infants are sensitive to the function
words of their language within their first year of
life (Shi et al, 2006; Hall?e et al, 2008; Shafer
et al, 1998), often before they have experienced
the ?word learning spurt?. Crucially for our pur-
pose, infants of this age were shown to exploit
frequent function words to segment neighboring
content words (Shi and Lepage, 2008; Hall?e et
al., 2008). In addition, 14 to 18-month-old
children were shown to exploit function words to
constrain lexical access to known words - for in-
stance, they expect a noun after a determiner (Cau-
vet et al, 2014; Kedar et al, 2006; Zangl and
Fernald, 2007). In addition, it is plausible that
function words play a crucial role in children?s
acquisition of more complex syntactic phenom-
ena (Christophe et al, 2008; Demuth and McCul-
lough, 2009), so it is interesting to investigate the
roles they might play in computational models of
language acquisition.
1.2 Adaptor grammars
Adaptor grammars are a framework for Bayesian
inference of a certain class of hierarchical non-
parametric models (Johnson et al, 2007b). They
define distributions over the trees specified by
a context-free grammar, but unlike probabilistic
context-free grammars, they ?learn? distributions
over the possible subtrees of a user-specified set of
?adapted? nonterminals. (Adaptor grammars are
non-parametric, i.e., not characterisable by a finite
283
set of parameters, if the set of possible subtrees
of the adapted nonterminals is infinite). Adaptor
grammars are useful when the goal is to learn a
potentially unbounded set of entities that need to
satisfy hierarchical constraints. As section 2 ex-
plains in more detail, word segmentation is such
a case: words are composed of syllables and be-
long to phrases or collocations, and modelling this
structure improves word segmentation accuracy.
Adaptor Grammars are formally defined in
Johnson et al (2007b), which should be consulted
for technical details. Adaptor Grammars (AGs)
are an extension of Probabilistic Context-Free
Grammars (PCFGs), which we describe first. A
Context-Free Grammar (CFG) G = (N,W,R, S)
consists of disjoint finite sets of nonterminal sym-
bols N and terminal symbols W , a finite set of
rules R of the form A?? where A ? N and
? ? (N ? W )
?
, and a start symbol S ? N . (We
assume there are no ??-rules? inR, i.e., we require
that |?| ? 1 for each A?? ? R).
A Probabilistic Context-Free Grammar (PCFG)
is a quintuple (N,W,R, S,?) where N , W , R
and S are the nonterminals, terminals, rules and
start symbol of a CFG respectively, and ? is a vec-
tor of non-negative reals indexed by R that sat-
isfy
?
??R
A
?
A??
= 1 for each A ? N , where
R
A
= {A?? : A?? ? R} is the set of rules
expanding A.
Informally, ?
A??
is the probability of a node
labelled A expanding to a sequence of nodes la-
belled ?, and the probability of a tree is the prod-
uct of the probabilities of the rules used to con-
struct each non-leaf node in it. More precisely, for
each X ? N ?W a PCFG associates distributions
G
X
over the set of trees T
X
generated by X as
follows:
If X ? W (i.e., if X is a terminal) then G
X
is the distribution that puts probability 1 on the
single-node tree labelled X .
If X ? N (i.e., if X is a nonterminal) then:
G
X
=
?
X?B
1
...B
n
?R
X
?
X?B
1
...B
n
TD
X
(G
B
1
, . . . , G
B
n
) (1)
where R
X
is the subset of rules in R expanding
nonterminal X ? N , and:
TD
X
(G
1
, . . . , G
n
)
(
.
X
t
1
t
n
. . .
)
=
n
?
i=1
G
i
(t
i
).
That is, TD
X
(G
1
, . . . , G
n
) is a distribution over
the set of trees T
X
generated by nonterminal X ,
where each subtree t
i
is generated independently
from G
i
. The PCFG generates the distribution G
S
over the set of trees T
S
generated by the start sym-
bol S; the distribution over the strings it generates
is obtained by marginalising over the trees.
In a Bayesian PCFG one puts Dirichlet priors
Dir(?) on the rule probability vector ?, such that
there is one Dirichlet parameter ?
A??
for each
rule A?? ? R. There are Markov Chain Monte
Carlo (MCMC) and Variational Bayes procedures
for estimating the posterior distribution over rule
probabilities ? and parse trees given data consist-
ing of terminal strings alone (Kurihara and Sato,
2006; Johnson et al, 2007a).
PCFGs can be viewed as recursive mixture
models over trees. While PCFGs are expres-
sive enough to describe a range of linguistically-
interesting phenomena, PCFGs are parametric
models, which limits their ability to describe phe-
nomena where the set of basic units, as well as
their properties, are the target of learning. Lexi-
cal acqusition is an example of a phenomenon that
is naturally viewed as non-parametric inference,
where the number of lexical entries (i.e., words)
as well as their properties must be learnt from the
data.
It turns out there is a straight-forward modifica-
tion to the PCFG distribution (1) that makes it suit-
ably non-parametric. As Johnson et al (2007b)
explain, by inserting a Dirichlet Process (DP)
or Pitman-Yor Process (PYP) into the generative
mechanism (1) the model ?concentrates? mass on
a subset of trees (Teh et al, 2006). Specifically,
an Adaptor Grammar identifies a subset A ? N
of adapted nonterminals. In an Adaptor Gram-
mar the unadapted nonterminals N \ A expand
via (1), just as in a PCFG, but the distributions of
the adapted nonterminals A are ?concentrated? by
passing them through a DP or PYP:
H
X
=
?
X?B
1
...B
n
?R
X
?
X?B
1
...B
n
TD
X
(G
B
1
, . . . , G
B
n
)
G
X
= PYP(H
X
, a
X
, b
X
)
Here a
X
and b
X
are parameters of the PYP asso-
ciated with the adapted nonterminal X . As Gold-
water et al (2011) explain, such Pitman-Yor Pro-
cesses naturally generate power-law distributed
data.
Informally, Adaptor Grammars can be viewed
as caching entire subtrees of the adapted nonter-
minals. Roughly speaking, the probability of gen-
erating a particular subtree of an adapted nonter-
minal is proportional to the number of times that
subtree has been generated before. This ?rich get
284
richer? behaviour causes the distribution of sub-
trees to follow a power-law (the power is speci-
fied by the a
X
parameter of the PYP). The PCFG
rules expanding an adapted nonterminal X de-
fine the ?base distribution? of the associated DP
or PYP, and the a
X
and b
X
parameters determine
how much mass is reserved for ?new? trees.
There are several different procedures for infer-
ring the parse trees and the rule probabilities given
a corpus of strings: Johnson et al (2007b) describe
aMCMC sampler and Cohen et al (2010) describe
a Variational Bayes procedure. We use the MCMC
procedure here since this has been successfully ap-
plied to word segmentation problems in previous
work (Johnson, 2008).
2 Word segmentation with Adaptor
Grammars
Perhaps the simplest word segmentation model is
the unigram model, where utterances are modeled
as sequences of words, and where each word is
a sequence of segments (Brent, 1999; Goldwater
et al, 2009). A unigram model can be expressed
as an Adaptor Grammar with one adapted non-
terminal Word (we indicate adapted nonterminals
by underlining them in grammars here; regular ex-
pressions are expanded into right-branching pro-
ductions).
Sentence?Word
+
(2)
Word?Phone
+
(3)
The first rule (2) says that a sentence consists of
one or more Words, while the second rule (3)
states that a Word consists of a sequence of one or
more Phones; we assume that there are rules ex-
panding Phone into all possible phones. Because
Word is an adapted nonterminal, the adaptor gram-
mar memoises Word subtrees, which corresponds
to learning the phone sequences for the words of
the language.
The more sophisticated Adaptor Grammars dis-
cussed below can be understood as specialis-
ing either the first or the second of the rules
in (2?3). The next two subsections review the
Adaptor Grammar word segmentation models pre-
sented in Johnson (2008) and Johnson and Gold-
water (2009): section 2.1 reviews how phonotac-
tic syllable-structure constraints can be expressed
with Adaptor Grammars, while section 2.2 re-
views how phrase-like units called ?collocations?
capture inter-word dependencies. Section 2.3
presents the major novel contribution of this paper
by explaining how we modify these adaptor gram-
mars to capture some of the special properties of
function words.
2.1 Syllable structure and phonotactics
The rule (3) models words as sequences of inde-
pendently generated phones: this is what Gold-
water et al (2009) called the ?monkey model? of
word generation (it instantiates the metaphor that
word types are generated by a monkey randomly
banging on the keys of a typewriter). However, the
words of a language are typically composed of one
or more syllables, and explicitly modelling the in-
ternal structure of words typically improves word
segmentation considerably.
Johnson (2008) suggested replacing (3) with the
following model of word structure:
Word? Syllable
1:4
(4)
Syllable?(Onset)Rhyme (5)
Onset?Consonant
+
(6)
Rhyme?Nucleus (Coda) (7)
Nucleus?Vowel
+
(8)
Coda?Consonant
+
(9)
Here and below superscripts indicate iteration
(e.g., a Word consists of 1 to 4 Syllables), while
an Onset consists of an unbounded number of
Consonants), while parentheses indicate option-
ality (e.g., a Rhyme consists of an obligatory
Nucleus followed by an optional Coda). We as-
sume that there are rules expanding Consonant
and Vowel to the set of all consonants and vow-
els respectively (this amounts to assuming that the
learner can distinguish consonants from vowels).
Because Onset, Nucleus and Coda are adapted,
this model learns the possible syllable onsets, nu-
cleii and coda of the language, even though neither
syllable structure nor word boundaries are explic-
itly indicated in the input to the model.
The model just described assumes that word-
internal syllables have the same structure as word-
peripheral syllables, but in languages such as
English word-peripheral onsets and codas can
be more complex than the corresponding word-
internal onsets and codas. For example, the
word ?string? begins with the onset cluster str,
which is relatively rare word-internally. Johnson
(2008) showed that word segmentation accuracy
improves if the model can learn different conso-
nant sequences for word-inital onsets and word-
final codas. It is easy to express this as an Adaptor
285
Grammar: (4) is replaced with (10?11) and (12?
17) are added to the grammar.
Word? SyllableIF (10)
Word? SyllableI Syllable
0:2
SyllableF (11)
SyllableIF?(OnsetI)RhymeF (12)
SyllableI?(OnsetI)Rhyme (13)
SyllableF?(Onset)RhymeF (14)
OnsetI?Consonant
+
(15)
RhymeF?Nucleus (CodaF) (16)
CodaF?Consonant
+
(17)
In this grammar the suffix ?I? indicates a word-
initial element, and ?F? indicates a word-final el-
ement. Note that the model simply has the abil-
ity to learn that different clusters can occur word-
peripherally and word-internally; it is not given
any information about the relative complexity of
these clusters.
2.2 Collocation models of inter-word
dependencies
Goldwater et al (2009) point out the detrimental
effect that inter-word dependencies can have on
word segmentation models that assume that the
words of an utterance are independently gener-
ated. Informally, a model that generates words in-
dependently is likely to incorrectly segment multi-
word expressions such as ?the doggie? as single
words because the model has no way to capture
word-to-word dependencies, e.g., that ?doggie? is
typically preceded by ?the?. Goldwater et alshow
that word segmentation accuracy improves when
the model is extended to capture bigram depen-
dencies.
Adaptor grammar models cannot express bi-
gram dependencies, but they can capture similiar
inter-word dependencies using phrase-like units
that Johnson (2008) calls collocations. John-
son and Goldwater (2009) showed that word seg-
mentation accuracy improves further if the model
learns a nested hierarchy of collocations. This can
be achieved by replacing (2) with (18?21).
Sentence?Colloc3
+
(18)
Colloc3?Colloc2
+
(19)
Colloc2?Colloc1
+
(20)
Colloc1?Word
+
(21)
Informally, Colloc1, Colloc2 and Colloc3 define a
nested hierarchy of phrase-like units. While not
designed to correspond to syntactic phrases, by ex-
amining the sample parses induced by the Adaptor
Grammar we noticed that the collocations often
correspond to noun phrases, prepositional phrases
or verb phrases. This motivates the extension to
the Adaptor Grammar discussed below.
2.3 Incorporating ?function words? into
collocation models
The starting point and baseline for our extension
is the adaptor grammar with syllable structure
phonotactic constraints and three levels of collo-
cational structure (5-21), as prior work has found
that this yields the highest word segmentation to-
ken f-score (Johnson and Goldwater, 2009).
Our extension assumes that the Colloc1 ?
Colloc3 constituents are in fact phrase-like, so we
extend the rules (19?21) to permit an optional se-
quence of monosyllabic words at the left edge
of each of these constituents. Our model thus
captures two of the properties of function words
discussed in section 1.1: they are monosyllabic
(and thus phonologically simple), and they appear
on the periphery of phrases. (We put ?function
words? in scare quotes below because our model
only approximately captures the linguistic proper-
ties of function words).
Specifically, we replace rules (19?21) with the
following sequence of rules:
Colloc3?(FuncWords3)Colloc2
+
(22)
Colloc2?(FuncWords2)Colloc1
+
(23)
Colloc1?(FuncWords1)Word
+
(24)
FuncWords3?FuncWord3
+
(25)
FuncWord3? SyllableIF (26)
FuncWords2?FuncWord2
+
(27)
FuncWord2? SyllableIF (28)
FuncWords1?FuncWord1
+
(29)
FuncWord1? SyllableIF (30)
This model memoises (i.e., learns) both the in-
dividual ?function words? and the sequences of
?function words? that modify the Colloc1 ?
Colloc3 constituents. Note also that ?function
words? expand directly to SyllableIF, which in
turn expands to a monosyllable with a word-initial
onset and word-final coda. This means that ?func-
tion words? are memoised independently of the
?content words? that Word expands to; i.e., the
model learns distinct ?function word? and ?con-
tent word? vocabularies. Figure 1 depicts a sample
parse generated by this grammar.
286
.Sentence
Colloc3
FuncWords3
FuncWord3
you
FuncWord3
want
FuncWord3
to
Colloc2
Colloc1
Word
see
Colloc1
FuncWords1
FuncWord1
the
Word
book
Figure 1: A sample parse generated by the ?func-
tion word? Adaptor Grammar with rules (10?18)
and (22?30). To simplify the parse we only show
the root node and the adapted nonterminals, and
replace word-internal structure by the word?s or-
thographic form.
This grammar builds in the fact that function
words appear on the left periphery of phrases. This
is true of languages such as English, but is not true
cross-linguistically. For comparison purposes we
also include results for a mirror-image model that
permits ?function words? on the right periphery,
a model which permits ?function words? on both
the left and right periphery (achieved by changing
rules 22?24), as well as a model that analyses all
words as monosyllabic.
Section 4 explains how a learner could use
Bayesian model selection to determine that func-
tion words appear on the left periphery in English
by comparing the posterior probability of the data
under our ?function word? Adaptor Grammar to
that obtained using a grammar which is identi-
cal except that rules (22?24) are replaced with the
mirror-image rules in which ?function words? are
attached to the right periphery.
3 Word segmentation results
This section presents results of running our Adap-
tor Grammar models on subsets of the Bernstein-
Ratner (1987) corpus of child-directed English.
We use the Adaptor Grammar software available
from http://web.science.mq.edu.au/?mjohnson/
with the same settings as described in Johnson
and Goldwater (2009), i.e., we perform Bayesian
inference with ?vague? priors for all hyperpa-
rameters (so there are no adjustable parameters
in our models), and perform 8 different MCMC
runs of each condition with table-label resampling
for 2,000 sweeps of the training data. At every
10th sweep of the last 1,000 sweeps we use the
model to segment the entire corpus (even if it
is only trained on a subset of it), so we collect
Model
Token
f-score
Boundary
precision
Boundary
recall
Baseline 0.872 0.918 0.956
+ left FWs 0.924 0.935 0.990
+ left + right FWs 0.912 0.957 0.953
Table 1: Mean token f-scores and boundary preci-
sion and recall results averaged over 8 trials, each
consisting of 8 MCMC runs of models trained
and tested on the full Bernstein-Ratner (1987) cor-
pus (the standard deviations of all values are less
than 0.006; Wilcox sign tests show the means of
all token f-scores differ p < 2e-4).
800 sample segmentations of each utterance.
The most frequent segmentation in these 800
sample segmentations is the one we score in the
evaluations below.
3.1 Word segmentation with ?function word?
models
Here we evaluate the word segmentations found
by the ?function word? Adaptor Grammar model
described in section 2.3 and compare it to the base-
line grammar with collocations and phonotactics
from Johnson and Goldwater (2009). Figure 2
presents the standard token and lexicon (i.e., type)
f-score evaluations for word segmentations pro-
posed by these models (Brent, 1999), and Table 1
summarises the token and lexicon f-scores for the
major models discussed in this paper. It is interest-
ing to note that adding ?function words? improves
token f-score by more than 4%, corresponding to
a 40% reduction in overall error rate.
When the training data is very small the Mono-
syllabic grammar produces the highest accuracy
results, presumably because a large proportion of
the words in child-directed speech are monosyl-
labic. However, at around 25 sentences the more
complex models that are capable of finding multi-
syllabic words start to become more accurate.
It?s interesting that after about 1,000 sentences
the model that allows ?function words? only on
the right periphery is considerably less accurate
than the baseline model. Presumably this is be-
cause it tends to misanalyse multi-syllabic words
on the right periphery as sequences of monosyl-
labic words.
The model that allows ?function words? only on
the left periphery is more accurate than the model
that allows them on both the left and right periph-
ery when the input data ranges from about 100 to
about 1,000 sentences, but when the training data
287
0.00
0.25
0.50
0.75
1.00
1 10 100 1000 10000NumberWofWtrainingWsentences
Tok
enWf
-sco
re
Model
Monosyllables
Baseline
+WleftWFWs
+WrightWFWs
+WleftW+WFWs
0.00
0.25
0.50
0.75
1.00
1 10 100 1000 10000NumberWofWtrainingWsentences
Lex
icon
Wf-sc
ore
Model
Monosyllables
Baseline
+WleftWFWs
+WrightWFWs
+WleftW+WrightWFWs
Figure 2: Token and lexicon (i.e., type) f-score on the Bernstein-Ratner (1987) corpus as a function of
training data size for the baseline model, the model where ?function words? can appear on the left pe-
riphery, a model where ?function words? can appear on the right periphery, and a model where ?function
words? can appear on both the left and the right periphery. For comparison purposes we also include
results for a model that assumes that all words are monosyllabic.
is larger than about 1,000 sentences both models
are equally accurate.
3.2 Content and function words found by
?function word? model
As noted earlier, the ?function word? model gen-
erates function words via adapted nonterminals
other than the Word category. In order to bet-
ter understand just how the model works, we give
the 5 most frequent words in each word category
found during 8 MCMC runs of the left-peripheral
?function word? grammar above:
Word : book, doggy, house, want, I
FuncWord1 : a, the, your, little
1
, in
FuncWord2 : to, in, you, what, put
FuncWord3 : you, a, what, no, can
Interestingly, these categories seem fairly rea-
sonable. The Word category includes open-class
nouns and verbs, the FuncWord1 category in-
cludes noun modifiers such as determiners, while
the FuncWord2 and FuncWord3 categories in-
clude prepositions, pronouns and auxiliary verbs.
1
The phone ?l? is generated by both Consonant and
Vowel, so ?little? can be (incorrectly) analysed as one syl-
lable.
Thus, the present model, initially aimed at seg-
menting words from continuous speech, shows
three interesting characteristics that are also ex-
hibited by human infants: it distinguishes be-
tween function words and content words (Shi and
Werker, 2001), it allows learners to acquire at least
some of the function words of their language (e.g.
(Shi et al, 2006)); and furthermore, it may also al-
low them to start grouping together function words
according to their category (Cauvet et al, 2014;
Shi and Melanc?on, 2010).
4 Are ?function words? on the left or
right periphery?
We have shown that a model that expects function
words on the left periphery performs more accu-
rate word segmentation on English, where func-
tion words do indeed typically occur on the left
periphery, leaving open the question: how could
a learner determine whether function words gen-
erally appear on the left or the right periphery of
phrases in the language they are learning? This
question is important because knowing the side
where function words preferentially occur is re-
288
lated to the question of the direction of syntac-
tic headedness in the language, and an accurate
method for identifying the location of function
words might be useful for initialising a syntac-
tic learner. Experimental evidence suggests that
infants as young as 8 months of age already ex-
pect function words on the correct side for their
language ? left-periphery for Italian infants and
right-periphery for Japanese infants (Gervain et
al., 2008) ? so it is interesting to see whether
purely distributional learners such as the ones
studied here can identify the correct location of
function words in phrases.
We experimented with a variety of approaches
that use a single adaptor grammar inference pro-
cess, but none of these were successful. For ex-
ample, we hoped that given an Adaptor Gram-
mar that permits ?function words? on both the
left and right periphery, the inference procedure
would decide that the right-periphery rules simply
are not used in a language like English. Unfortu-
nately we did not find this in our experiments; the
right-periphery rules were used almost as often as
the left-periphery rules (recall that a large fraction
of the words in English child-directed speech are
monosyllabic).
In this section, we show that learners could use
Bayesian model selection to determine that func-
tion words appear on the left periphery in English
by comparing the marginal probability of the data
for the left-periphery and the right-periphery mod-
els.
Instead, we used Bayesian model selection
techniques to determine whether left-peripheral
or a right-peripheral model better fits the un-
segmented utterances that constitute the training
data.
2
While Bayesian model selection is in prin-
ciple straight-forward, it turns out to require the ra-
tio of two integrals (for the ?evidence? or marginal
likelihood) that are often intractable to compute.
Specifically, given a training corpusD of unseg-
mented sentences and model families G
1
and G
2
(here the ?function word? adaptor grammars with
left-peripheral and right-peripheral attachment re-
spectively), the Bayes factor K is the ratio of the
marginal likelihoods of the data:
K =
P(D | G
1
)
P(D | G
2
)
2
Note that neither the left-peripheral nor the right-
peripheral model is correct: even strongly left-headed lan-
guages like English typically contain a few right-headed con-
structions. For example, ?ago? is arguably the head of the
phrase ?ten years ago?.
0
2000
4000
6000
1 10 100 1000 10000Number of training sentences
log B
ayes
 fact
or
Figure 3: Bayes factor in favour of left-peripheral
?function word? attachment as a function of the
number of sentences in the training corpus, cal-
culated using the Harmonic Mean estimator (see
warning in text).
where the marginal likelihood or ?evidence? for a
model G is obtained by integrating over all of the
hidden or latent structure and parameters ?:
P(D | G) =
?
?
P(D,? | G) d? (31)
Here the variable ? ranges over the space ? of all
possible parses for the utterances inD and all pos-
sible configurations of the Pitman-Yor processes
and their parameters that constitute the ?state? of
the Adaptor Grammar G. While the probability of
any specific Adaptor Grammar configuration ? is
not too hard to calculate (the MCMC sampler for
Adaptor Grammars can print this after each sweep
through D), the integral in (31) is in general in-
tractable.
Textbooks such as Murphy (2012) describe a
number of methods for calculating P(D | G), but
most of them assume that the parameter space ?
is continuous and so cannot be directly applied
here. The Harmonic Mean estimator (32) for (31),
which we used here, is a popular estimator for
(31) because it only requires the ability to calcu-
late P(D,? | G) for samples from P(? | D,G):
P(D | G) ?
(
1
n
n
?
i=1
1
P(D,?
i
| G)
)
?1
where ?
i
, . . . ,?
n
are n samples from P(? |
289
D,G), which can be generated by the MCMC pro-
cedure.
Figure 3 depicts how the Bayes factor in favour
of left-peripheral attachment of ?function words?
varies as a function of the number of utter-
ances in the training data D (calculated from the
last 1000 sweeps of 8 MCMC runs of the cor-
responding adaptor grammars). As that figure
shows, once the training data contains more than
about 1,000 sentences the evidence for the left-
peripheral grammar becomes very strong. On the
full training data the estimated log Bayes factor is
over 6,000, which would constitute overwhelming
evidence in favour of left-peripheral attachment.
Unfortunately, as Murphy and others warn, the
Harmonic Mean estimator is extremely unstable
(Radford Neal calls it ?the worst MCMC method
ever? in his blog), so we think it is important to
confirm these results using a more stable estima-
tor. However, given the magnitude of the differ-
ences and the fact that the two models being com-
pared are of similar complexity, we believe that
these results suggest that Bayesian model selec-
tion can be used to determine properties of the lan-
guage being learned.
5 Conclusions and future work
This paper showed that the word segmentation
accuracy of a state-of-the-art Adaptor Grammar
model is significantly improved by extending it
so that it explicitly models some properties of
function words. We also showed how Bayesian
model selection can be used to identify that func-
tion words appear on the left periphery of phrases
in English, even though the input to the model only
consists of an unsegmented sequence of phones.
Of course this work only scratches the surface
in terms of investigating the role of function words
in language acquisition. It would clearly be very
interesting to examine the performance of these
models on other corpora of child-directed English,
as well as on corpora of child-directed speech in
other languages. Our evaluation focused on word-
segmentation, but we could also evaluate the ef-
fect that modelling ?function words? has on other
aspects of the model, such as its ability to learn
syllable structure.
The models of ?function words? we investi-
gated here only capture two of the 7 linguistic
properties of function words identified in section 1
(i.e., that function words tend to be monosyllabic,
and that they tend to appear phrase-peripherally),
so it would be interesting to develop and explore
models that capture other linguistic properties of
function words. For example, following the sug-
gestion by Hochmann et al (2010) that human
learners use frequency cues to identify function
words, it might be interesting to develop computa-
tional models that do the same thing. In an Adap-
tor Grammar the frequency distribution of func-
tion words might be modelled by specifying the
prior for the Pitman-Yor Process parameters asso-
ciated with the function words? adapted nontermi-
nals so that it prefers to generate a small number
of high-frequency items.
It should also be possible to develop models
which capture the fact that function words tend not
to be topic-specific. Johnson et al (2010) and
Johnson et al (2012) show how Adaptor Gram-
mars can model the association between words
and non-linguistic ?topics?; perhaps these models
could be extended to capture some of the semantic
properties of function words.
It would also be interesting to further explore
the extent to which Bayesian model selection is a
useful approach to linguistic ?parameter setting?.
In order to do this it is imperative to develop better
methods than the problematic ?Harmonic Mean?
estimator used here for calculating the evidence
(i.e., the marginal probability of the data) that can
handle the combination of discrete and continuous
hidden structure that occur in computational lin-
guistic models.
As well as substantially improving the accuracy
of unsupervised word segmentation, this work is
interesting because it suggests a connection be-
tween unsupervised word segmentation and the in-
duction of syntactic structure. It is reasonable to
expect that hierarchical non-parametric Bayesian
models such as Adaptor Grammars may be useful
tools for exploring such a connection.
Acknowledgments
This work was supported in part by the Aus-
tralian Research Council?s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593), the European Research Coun-
cil (ERC-2011-AdG-295810 BOOTPHON), the
Agence Nationale pour la Recherche (ANR-10-
LABX-0087 IEC, and ANR-10-IDEX-0001-02
PSL*), and the Mairie de Paris, Ecole des Hautes
Etudes en Sciences Sociales, the Ecole Normale
Sup?erieure, and the Fondation Pierre Gilles de
Gennes.
290
References
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, ed-
itors, Children?s Language, volume 6, pages 159?
174. Erlbaum, Hillsdale, NJ.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
E. Cauvet, R. Limissuri, S. Millotte, K. Skoruppa,
D. Cabrol, and A. Christophe. 2014. Function
words constrain on-line recognition of verbs and
nouns in French 18-month-olds. Language Learn-
ing and Development, pages 1?18.
A. Christophe, S. Millotte, S. Bernal, and J. Lidz.
2008. Bootstrapping lexical and syntactic acquisi-
tion. Language and Speech, 51(1-2):61?75.
S. B. Cohen, D. M. Blei, and N. A. Smith. 2010. Vari-
ational inference for adaptor grammars. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 564?572,
Los Angeles, California, June. Association for Com-
putational Linguistics.
K. Demuth and E. McCullough. 2009. The prosodic
(re-)organization of childrens early English articles.
Journal of Child Language, 36(1):173?200.
J. Elman, E. Bates, M. H. Johnson, A. Karmiloff-
Smith, D. Parisi, and K. Plunkett. 1996. Rethink-
ing Innateness: A Connectionist Perspective on De-
velopment. MIT Press/Bradford Books, Cambridge,
MA.
V. Fromkin, editor. 2001. Linguistics: An Introduction
to Linguistic Theory. Blackwell, Oxford, UK.
J. Gervain, M. Nespor, R. Mazuka, R. Horie, and
J. Mehler. 2008. Bootstrapping word order in
prelexical infants: A japaneseitalian cross-linguistic
study. Cognitive Psychology, 57(1):56 ? 74.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2009.
A Bayesian framework for word segmentation: Ex-
ploring the effects of context. Cognition, 112(1):21?
54.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2011.
Producing power-law distributions and damping
word frequencies with two-stage language models.
Journal of Machine Learning Research, 12:2335?
2382.
P. A. Hall?e, C. Durand, and B. de Boysson-Bardies.
2008. Do 11-month-old French infants process ar-
ticles? Language and Speech, 51(1-2):23?44.
J.-R. Hochmann, A. D. Endress, and J. Mehler. 2010.
Word frequency as a cue for identifying function
words in infancy. Cognition, 115(3):444 ? 457.
M. Johnson and S. Goldwater. 2009. Improving non-
parameteric Bayesian inference: experiments on un-
supervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 317?325, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
M. Johnson, T. Griffiths, and S. Goldwater. 2007a.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?
146, Rochester, New York. Association for Compu-
tational Linguistics.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007b.
Adaptor Grammars: A framework for specifying
compositional nonparametric Bayesian models. In
B. Sch?olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems
19, pages 641?648. MIT Press, Cambridge, MA.
M. Johnson, K. Demuth, M. Frank, and B. Jones.
2010. Synergies in learning words and their refer-
ents. In J. Lafferty, C. K. I. Williams, J. Shawe-
Taylor, R. Zemel, and A. Culotta, editors, Advances
in Neural Information Processing Systems 23, pages
1018?1026.
M. Johnson, K. Demuth, and M. Frank. 2012. Exploit-
ing social information in grounded language learn-
ing via grammatical reduction. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics, pages 883?891, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
M. Johnson. 2008. Using Adaptor Grammars to iden-
tify synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Lin-
guistics, pages 398?406, Columbus, Ohio. Associ-
ation for Computational Linguistics.
Y. Kedar, M. Casasola, and B. Lust. 2006. Getting
there faster: 18- and 24-month-old infants? use of
function words to determine reference. Child De-
velopment, 77(2):325?338.
K. Kurihara and T. Sato. 2006. Variational
Bayesian grammar induction for natural language.
In Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino,
and E. Tomita, editors, Grammatical Inference: Al-
gorithms and Applications, pages 84?96. Springer.
K. P. Murphy. 2012. Machine learning: a probabilistic
perspective. The MIT Press.
V. L. Shafer, D. W. Shucard, J. L. Shucard, and
L. Gerken. 1998. An electrophysiological study of
infants? sensitivity to the sound patterns of English
291
speech. Journal of Speech, Language and Hearing
Research, 41(4):874.
R. Shi and M. Lepage. 2008. The effect of functional
morphemes on word segmentation in preverbal in-
fants. Developmental Science, 11(3):407?413.
R. Shi and A. Melanc?on. 2010. Syntactic categoriza-
tion in French-learning infants. Infancy, 15(517?
533).
R. Shi and J. Werker. 2001. Six-months old infants?
preference for lexical words. Psychological Science,
12:71?76.
R. Shi, A. Cutler, J. Werker, and M. Cruickshank.
2006. Frequency and form as determinants of func-
tor sensitivity in English-acquiring infants. The
Journal of the Acoustical Society of America,
119(6):EL61?EL67.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hi-
erarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566?1581.
R. Zangl and A. Fernald. 2007. Increasing flexibil-
ity in children?s online processing of grammatical
and nonce determiners in fluent speech. Language
Learning and Development, 3(3):199?231.
292
