Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 57?64,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards Conversational QA: Automatic Identification of Problematic
Situations and User Intent ?
Joyce Y. Chai Chen Zhang Tyler Baldwin
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{jchai, zhangch6, baldwi96}@cse.msu.edu
Abstract
To enable conversational QA, it is impor-
tant to examine key issues addressed in
conversational systems in the context of
question answering. In conversational sys-
tems, understanding user intent is criti-
cal to the success of interaction. Recent
studies have also shown that the capabil-
ity to automatically identify problematic
situations during interaction can signifi-
cantly improve the system performance.
Therefore, this paper investigates the new
implications of user intent and problem-
atic situations in the context of question
answering. Our studies indicate that, in
basic interactive QA, there are different
types of user intent that are tied to dif-
ferent kinds of system performance (e.g.,
problematic/error free situations). Once
users are motivated to find specific infor-
mation related to their information goals,
the interaction context can provide useful
cues for the system to automatically iden-
tify problematic situations and user intent.
1 Introduction
Interactive question answering (QA) has been
identified as one of the important directions in QA
research (Burger et al, 2001). One ultimate goal is
to support intelligent conversation between a user
and a QA system to better facilitate user informa-
tion needs. However, except for a few systems that
use dialog to address complex questions (Small et
al., 2003; Harabagiu et al, 2005), the general di-
alog capabilities have been lacking in most ques-
?This work was partially supported by IIS-0347548 from
the National Science Foundation.
tion answering systems. To move towards conver-
sational QA, it is important to examine key issues
relevant to conversational systems in the context
of interactive question answering.
This paper focuses on two issues related to con-
versational QA. The first issue is concerned with
user intent. In conversational systems, understand-
ing user intent is the key to the success of the inter-
action. In the context of interactive QA, one ques-
tion is what type of user intent should be captured.
Unlike most dialog systems where user intent can
be characterized by dialog acts such as question,
reply, and statement, in interactive QA, user in-
puts are already in the form of question. Then
the problems become whether there are different
types of intent behind these questions that should
be handled differently by a QA system and how to
automatically identify them.
The second issue is concerned with problem-
atic situations during interaction. In spoken di-
alog systems, many problematic situations could
arise from insufficient speech recognition and lan-
guage understanding performance. Recent work
has shown that the capability to automatically
identify problematic situations (e.g., speech recog-
nition errors) can help control and adapt dialog
strategies to improve performance (Litman and
Pan, 2000). Similarly, QA systems also face chal-
lenges of technology limitation from language un-
derstanding and information retrieval. Thus one
question is, in the context of interactive QA, how
to characterize problematic situations and auto-
matically identify them when they occur.
In interactive QA, these two issues are inter-
twined. Questions formed by a user not only de-
pend on his/her information goals, but are also in-
fluenced by the answers from the system. Prob-
lematic situations will impact user intent in the
57
follow-up questions, which will further influence
system performance. Both the awareness of prob-
lematic situations and understanding of user in-
tent will allow QA systems to adapt better strate-
gies during interaction and move towards intelli-
gent conversational QA.
To address these two questions, we conducted
a user study where users interacted with a con-
trolled QA system to find information of inter-
est. These controlled studies allowed us to fo-
cus on the interaction aspect rather than informa-
tion retrieval or answer extraction aspects. Our
studies indicate that in basic interactive QA where
users always ask questions and the system always
provides some kind of answers, there are differ-
ent types of user intent that are tied to differ-
ent kinds of system performance (e.g., problem-
atic/error free situations). Once users are moti-
vated to find specific information related to their
information goals, the interaction context can pro-
vide useful cues for the system to automatically
identify problematic situations and user intent.
2 Related Work
Open domain question answering (QA) systems
are designed to automatically locate answers from
large collections of documents to users? natural
language questions. In the past few years, au-
tomated question answering techniques have ad-
vanced tremendously, partly motivated by a se-
ries of evaluations conducted at the Text Retrieval
Conference (TREC) (Voorhees, 2001; Voorhees,
2004). To better facilitate user information needs,
recent trends in QA research have shifted towards
complex, context-based, and interactive question
answering (Voorhees, 2001; Small et al, 2003;
Harabagiu et al, 2005). For example, NIST initi-
ated a special task on context question answering
in TREC 10 (Voorhees, 2001), which later became
a regular task in TREC 2004 (Voorhees, 2004) and
2005. The motivation is that users tend to ask a
sequence of related questions rather than isolated
single questions to satisfy their information needs.
Therefore, the context QA task was designed to
investigate the system capability to track context
through a series of questions. Based on context
QA, some work has been done to identify clarifica-
tion relations between questions (Boni and Man-
andhar, 2003). However context QA is different
from interactive QA in that context questions are
specified ahead of time rather than incrementally
as in an interactive setting.
Interactive QA has been applied to process com-
plex questions. For analytical and non-factual
questions, it is hard to anticipate answers. Clari-
fication dialogues can be applied to negotiate with
users about the intent of their questions (Small et
al., 2003). Recently, an architecture for interactive
question answering has been proposed based on a
notion of predictive questioning (Harabagiu et al,
2005). The idea is that, given a complex ques-
tion, the system can automatically identify a set of
potential follow-up questions from a large collec-
tion of question-answer pairs. The empirical re-
sults have shown the system with predictive ques-
tioning is more efficient and effective for users to
accomplish information seeking tasks in a partic-
ular domain (Harabagiu et al, 2005).
The work reported in this paper addresses a
different aspect of interactive question answering.
Both issues raised earlier (Section 1) are inspired
by earlier work on intelligent conversational sys-
tems. Automated identification of user intent has
played an important role in conversational sys-
tems. Tremendous amounts of work has focused
on this aspect (Stolcke et al, 2000). To improve
dialog performance, much effort has also been put
on techniques to automatically detect errors during
interaction. It has shown that during human ma-
chine dialog, there are sufficient cues for machines
to automatically identify error conditions (Levow,
1998; Litman et al, 1999; Hirschberg et al, 2001;
Walker et al, 2002). The awareness of erroneous
situations can help systems make intelligent de-
cisions about how to best guide human partners
through the conversation and accomplish the tasks.
Motivated by these earlier studies, the goal of this
paper is to investigate whether these two issues can
be applied in question answering to facilitate intel-
ligent conversational QA.
3 User Studies
We conducted a user study to collect data concern-
ing user behavior in a basic interactive QA set-
ting. We are particularly interested in how users
respond to different system performance and its
implication in identifying problematic situations
and user intent. As a starting point, we charac-
terize system performance as either problematic,
which indicates the answer has some problem, or
error-free, which indicates the answer is correct.
In this section, we first describe the methodology
58
and the system used in this effort and then discuss
the observed user behavior and its relation to prob-
lematic situations and user intent.
3.1 Methodology and System
The system used in our experiments has a user in-
terface that takes a natural language question and
presents an answer passage. Currently, our inter-
face only presents to the user the top one retrieved
result. This simplification on one hand helps us
focus on the investigation of user responses to dif-
ferent system performances and on the other hand
represents a possible situation where a list of po-
tential answers may not be practical (e.g., through
PDA or telephone line).
We implemented a Wizard-of-Oz (WOZ) mech-
anism in the interaction loop to control and simu-
late problematic situations. Users were not aware
of the existence of this human wizard and were
led to believe they were interacting with a real
QA system. This controlled setting allowed us
to focus on the interaction aspect rather than in-
formation retrieval or answer extraction aspect of
question answering. More specifically, during in-
teraction after each question was issued, a ran-
dom number generator was used to decide if a
problematic situation should be introduced. If
the number indicated no, the wizard would re-
trieve a passage from a database with correct ques-
tion/answer pairs. Note that in our experiments
we used specific task scenarios (described later),
so it was possible to anticipate user information
needs and create this database. If the number in-
dicated that a problematic situation should be in-
troduced, then the Lemur retrieval engine 1 was
used on the AQUAINT collection to retrieve the
answer. Our assumption is that AQUAINT data
are not likely to provide an exact answer given our
specific scenarios, but they can provide a passage
that is most related to the question. The use of the
random number generator was to control the ratio
between the occurrence of problematic situations
and error-free situations. In our initial investiga-
tion, since we are interested in observing user be-
havior in problematic situations, we set the ratio as
50/50. In our future work, we will vary this ratio
(e.g., 70/30) to reflect the performance of state-of-
the-art factoid QA and investigate the implication
of this ratio in automated performance assessment.
1http://www-2.cs.cmu.edu/ lemur/
3.2 Experiments
Eleven users participated in our study. Each user
was asked to interact with our system to com-
plete information seeking tasks related to four
specific scenarios: the 2004 presidential debates,
Tom Cruise, Hawaii, and Pompeii. The exper-
imental scenarios were further divided into two
types: structured and unstructured. In the struc-
tured task scenarios (for topics Tom Cruise and
Pompeii), users had to fill in blanks on a dia-
gram pertaining to the given topic. Using the dia-
gram was to avoid the influence of these scenarios
on the language formation of the relevant ques-
tions. Because users must find certain informa-
tion, they were constrained in the range of ques-
tions in which they could ask, but not the way they
ask those questions. The task was completed when
all of the blanks on the diagram were filled. The
structured scenarios were designed to mimic the
real information seeking practice in which users
have real motivation to find specific information
related to their information goals. In the unstruc-
tured scenarios (for topics the 2004 presidential
debates and Hawaii), users were given a general
topic to investigate, but were not required to find
specific information. This gave the user the abil-
ity to ask a much wider range of questions than
the structured scenarios. Users were generally in
an exploration mode when performing these un-
structured tasks. They were not motivated to find
specific information and were content with any in-
formation provided by the system. In our view,
the unstructured scenarios are less representative
of the true information seeking situations.
3.3 Observations and Analysis
From our studies, a total of 44 interaction sessions
with 456 questions were collected. Figure 1 shows
an example of a fragment of interaction related to
Tom Cruise. In this example, both problematic sit-
uations applied to answers (e.g., Problematic and
Error-Free) and user intent (described later) ap-
plied to questions are annotated.
There are several observations from this data.
First, questions formed during interactive QA tend
to be self-contained and free of definite noun
phrases, pronouns, or ellipsis. Only one question
in the entire data set has a pronoun (i.e., What
are the best movies with Tom Cruise in them?).
Even in this case, the pronoun them did not re-
fer to any entities that occurred previously in the
59
# Question/Answer Annotation
Q1 When was Tom Cruise born?
Q2 What year was Tom Cruise born? Re-try
Q3 What does Tom Cruise do for a living? Continue
Q4 What is Tom Cruise's profession? Re-try
Q5 What are the best movies with Tom Cruise 
in them?
Switch
A1 ? Based on the memoir of combat veteran 
Ron Kovic, the film stars Tom Cruise as 
Kovic, whose gunshot wound in Vietnam 
left him paralyzed from the chest down.
?.a powerfully intimate portrait that 
unfolds on an epic scale, Born on the 
Fourth of July is arguably Stone's best film 
(if you can forgive its often strident tone), 
?..
Proble-
matic
A2 ?Thomas Cruise Mapother IV was born
on the 3rd of July, 1962 (eerily similar to 
his film Born on the 4th of July), in 
Syracuse, New York. He was the only boy
of four children?.
Error-Free
A3 ?you get a very nice role that shows you
differently, not the heavy, and you're
working with a George Clooney or a Tom
Cruise or a Nicolas Cage or a Martin
Scorsese. I can live with that ?
Proble-
matic
A4 ?So we may agree that Cruise?s
professional standing accounts for some 
measure of his fame?.
Proble-
matic
A5 ?Cruise proved his dramatic talents in the 
1988 drama Rain Man, where he co-
starred with Oscar-winner Dustin Hoffman. 
Oliver Stone's Born on the Fourth of July
(1989) earned him a Best Actor Oscar 
nomination for his hard-hitting portrayal of 
anti-war activist Ron Kovic?.
Error-Free
Figure 1: An example fragment of interaction
QA process. This phenomenon could be caused by
how the answers are presented. Unlike specific an-
swer entities, the answer passages provided by our
system do not support the natural use of referring
expressions in the follow-up questions. Another
possible explanation could be that in an interac-
tive environment, users seem to be more aware of
the potential limitation of a computer system and
thus tend to specify self-contained questions in a
hope to reduce the system?s inference load.
The second observation is about user behavior
in response to different system performances (i.e.,
problematic or error-free situations). We were
hoping to see different strategies users might ap-
ply to deal with the problematic situations. How-
ever, based on the data, we found that when a prob-
lem occurred, users either rephrased their ques-
tions (i.e., the same question expressed in a dif-
ferent way) or gave up the question and went on
specifying a new question. (Here we use Rephrase
and New to denote these two kinds of behaviors.)
We have not observed any sub-dialogs initiated by
Problematic Error-free Total
New Switch Continue
unstruct. 29 90 119
struct. 29 133 162
entire 58 223 281
Rephrase Re-try Negotiate
unstruct. 19 4 23
struct. 102 6 108
entire 121 10 131
Total-unst 48 94 142
Total-st 131 139 270
Total-ent 179 233 412
Table 1: Categorization of user intent with the cor-
responding number of occurrences from the un-
structured scenarios, the structured scenarios, and
the entire dataset.
the user to clarify a previous question or answer.
One possible explanation is that the current inves-
tigation was conducted in a basic interactive mode
where the system was only capable of providing
some sort of answers. This may limit users? expec-
tation in the kind of questions that can be handled
by the system. Our assumption is that, once the
QA system becomes more intelligent and able to
carry on conversation, different types of questions
(i.e., other than rephrase or new) will be observed.
This hypothesis certainly needs to be validated in
a conversational setting.
The third observation is that the rephrased ques-
tions seem to strongly correlate with problematic
situations, although not always. New questions
cannot distinguish a problematic situation from
an error-free situation. Table 1 shows the statis-
tics from our data about different combinations
of new/rephrase questions and performance situ-
ations2. What is interesting is that these different
combinations can reflect different types of user in-
tent behind the questions. More specifically, given
a question, four types of user intent can be cap-
tured with respect to the context (e.g., the previous
question and answer)
Continue indicates that the user is satisfied with
the previous answer and now moves on to this
new question.
Switch indicates that the user has given up on the
previous question and now moves on to this
2The last question from each interaction session is not in-
cluded in these statistics because there is no follow-up ques-
tion after that.
60
new question.
Re-try indicates that the user is not satisfied with
the previous answer and now tries to get a
better answer.
Negotiate indicates that the user is not satisfied
with the previous answer (although it ap-
pears to be correct from the system?s point
of view) and now tries to get a better answer
for his/her own needs.
Table 1 summarizes these different types of
intent together with the number of correspond-
ing occurrences from both structured and unstruc-
tured scenarios. Since in the unstructured sce-
narios it was hard to anticipate user?s questions
and therefore take a correct action to respond to a
problematic/error-free situation, the distribution of
these two situations is much more skewed than the
distribution for the structured scenarios. Also as
mentioned earlier, in unstructured scenarios, users
lacked the motivation to pursue specific informa-
tion, so the ratio between switch and re-try is much
larger than that observed in the structured scenar-
ios. Nevertheless, we did observe different user
behavior in response to different situations. As
discussed later in Section 5, identifying these fine-
grained intents will allow QA systems to be more
proactive in helping users find satisfying answers.
4 Automatic Identification of
Problematic Situations and User Intent
Given the discussion above, the next question is
how to automatically identify problematic situa-
tions and user intent. We formulate this as a classi-
fication problem. Given a question Qi, its answer
Ai, and the follow-up question Qi+1:
(1) Automatic identification of problematic situa-
tions is to decide whether Ai is problematic (i.e.,
correct or incorrect) based on the follow-up ques-
tion Qi+1 and the interaction context. This is a
binary classification problem.
(2) Automatic identification of user intent is to
identify the intent of Qi+1 given the interaction
context. Because we only have very limited in-
stances of Negotiate (see Table 1), we currently
merge Negotiate with Re-try since both of them
represent a situation where a better answer is re-
quested. Thus, this problem becomes a trinary
classification problem.
To build these classifiers, we identified a set of
features, which are illustrated next.
4.1 Features
Given a question Qi, its answer Ai, and the follow-
up question Qi+1, the following set of features are
used:
Target matching(TM): a binary feature indicat-
ing whether the target type of Qi+1 is the same as
the target type of Qi. Our data shows that the rep-
etition of the target type may indicate a rephrase,
which could signal a problematic situation has just
happened.
Named entity matching (NEM): a binary feature
indicating whether all the named entities in Qi+1
also appear in Qi. If no new named entity is in-
troduced in Qi+1, it is likely Qi+1 is a rephrase of
Qi.
Similarity between questions (SQ): a numeric
feature measuring the similarity between Qi+1 and
Qi. Our assumption is that the higher the simi-
larity is, the more likely the current question is a
rephrase to the previous one.
Similarity between content words of questions
(SQC): this feature is similar to the previous fea-
ture (i.e., SQ) except that the similarity measure-
ment is based on the content words excluding
named entities. This is to prevent the similarity
measurement from being dominated by the named
entities.
Similarity between Qi and Ai (SA): this feature
measures how close the retrieved passage matches
the question. Our assumption is that although a re-
trieved passage is the most relevant passage com-
pared to others, it still may not contain the answer
(e.g., when an answer does not even exist in the
data collection).
Similarity between Qi and Ai based on the con-
tent words (SAC): this feature is essentially the
same as the previous feature (SA) except that the
similarity is calculated after named entities are re-
moved from the questions and answers.
Note that since our data is currently collected
from simulation studies, we do not have the confi-
dence score from the retrieval engine associated
with every answer. In practice, the confidence
score can be used as an additional feature.
Since our focus is not on the similarity measure-
ment but rather the use of the measurement in the
classification models, our current similarity mea-
surement is based on a simple approach that mea-
sures commonality and difference between two
objects as proposed by Lin (1998). More specifi-
cally, the following equation is applied to measure
61
the similarity between two chunks of text T
1
and
T
2
:
sim
1
(T
1
, T
2
) =
? logP (T
1
? T
2
)
? logP (T
1
? T
2
)
Assume the occurrence of each word is indepen-
dent, then:
sim
1
(T
1
, T
2
) =
?
?
w?T
1
?T
2
log P (w)
?
?
w?T
1
?T
2
log P (w)
where P (w) was calculated based on the data used
in the previous TREC evaluations.
4.2 Identification of Problematic Situations
To identify problematic situations, we experi-
mented with three different classifiers: Maxi-
mum Entropy Model (MEM) from MALLET3,
SVM from SVM-Light4, and Decision Trees from
WEKA5. A leave-one-out validation was applied
where one interaction session was used for testing
and the remaining interaction sessions were used
for training.
Table 2 shows the performance of the three
models based on different combinations of fea-
tures in terms of classification accuracy. The base-
line result is the performance achieved by sim-
ply assigning the most frequently occurred class.
For the unstructured scenarios, the performance
of the classifiers is rather poor, which indicates
that it is quite difficult to make any generaliza-
tion based on the current feature sets when users
are less motivated in finding specific information.
For the structured scenarios, the best performance
for each model is highlighted in bold in Table 2.
The Decision Tree model achieves the best per-
formance of 77.8% in identifying problematic sit-
uations, which is more than 25% better than the
baseline performance.
4.3 Identification of User Intent
To identify user intent, we formulate the problem
as follows: given an observation feature vector f
where each element of the vector corresponds to
a feature described earlier, the goal is to identify
an intent c? from a set of intents I ={Continue,
Switch, Re-try/Negotiate} that satisfies the follow-
ing equation:
c? = argmaxc?IP (c|f)
3http://mallet.cs.umass.edu/index.php/
4http://svmlight.joachims.org/
5http://www.cs.waikato.ac.nz/ml/weka/
Our assumption is that user intent for a ques-
tion can be potentially influenced by the intent
from a preceding question. For example, Switch
is likely to follow Re-try. Therefore, we have im-
plemented a Maximum Entropy Markov Model
(MEMM) (McCallum et al, 2000) to take the se-
quence of interactions into account.
Given a sequence of questions Q
1
, Q
2
, up to Qt,
there is an observation feature vector f
i
associated
with each Qi. In MEMM, the prediction of user
intent ct for Qt not only depends on the observa-
tion f
t
, but also the intent ct?1 from the preceding
question Qt?1. In fact, this approach finds the best
sequence of user intent C? for Q
1
up to Qt based
on a sequence of observations f
1
, f
2
, ..., ft as fol-
lows:
C? = argmaxC?ItP (C|f1, f2, ..., ft)
where C is a sequence of intent and It is the set of
all possible sequences of intent with length t.
To find this sequence of intent C?, MEMM
keeps a variable ?t(i) which is defined to be the
maximum probability of seeing a particular se-
quence of intent ending at intent i (i ? I) for
question Qt, given the observation sequence for
questions Q
1
up to Qt:
?t(i) = maxc
1
,...,c
t?1
P (c
1
, . . . , ct?1, ct = i|f1, . . . , ft)
This variable can be calculated by a dynamic
optimization procedure similar to the Viterbi algo-
rithm in the Hidden Markov Model:
?t(i) = maxj ?t?1(j) ? P (ct = i|ct?1 = j, ft)
where P (ct = i|ct?1 = j, ft) is estimated by the
Maximum Entropy Model.
Table 3 shows the best results of identifying
user intent based on the Maximum Entropy Model
and MEMM using the leave-one-out approach.
The results have shown that both models did not
work for the data collected from unstructured sce-
narios (i.e., the baseline accuracy for intent iden-
tification is 63.4%). For structured scenarios, in
terms of the overall accuracy, both models per-
formed significantly better than the baseline (i.e.,
49.3%). The MEMM worked only slightly better
than the MEM. Given our limited data, it is not
conclusive whether the transitions between ques-
tions will help identify user intent in a basic inter-
active mode. However, we expect to see more in-
fluence from the transitions in fully conversational
QA.
62
MEM SVM DTree
Features un s ent un s ent un s ent
Baseline 66.2 51.5 56.3 66.2 51.5 56.3 66.2 51.5 56.3
TM, SQC 50.0 57.4 54.9 53.5 60.0 57.8 53.5 55.9 55.1
NEM, SQC 37.3 74.4 61.7 37.3 74.4 61.7 37.3 74.4 61.7
TM, SQ 61.3 64.8 63.6 57.0 64.1 61.7 59.9 64.4 62.9
NEM, SQC, SAC 40.8 76.7 64.3 38.0 74.4 61.9 49.3 77.8 68.0
TM, SQ, SAC 59.2 67.4 64.6 61.3 66.3 64.6 62.7 65.6 64.6
TM, NEM, SQC 54.2 75.2 68.0 54.2 75.2 68.0 53.5 74.4 67.2
TM, SQ, SA 63.4 71.9 68.9 58.5 71.5 67.0 67.6 75.6 72.8
TM, NEM, SQC, SAC 54.9 75.6 68.4 54.2 75.2 68.0 55.6 74.4 68.0
* un - unstructured, s - structured, ent - entire
Table 2: Performance of automatic identification of problematic situations
MEM MEMM
un s un s
CONTINUE P 64.4 69.7 67.3 70.8
R 96.7 85.8 80.0 88.8
F 77.3 76.8 73.1 78.7
RE-TRY P 28.6 76.2 37.1 79.0
/NEGOTIATE R 8.7 74.1 56.5 73.1
F 13.3 75.1 44.8 75.9
SWITCH P - - - 50.0
R 0 0 0 3.6
F - - - 6.7
Overall accuracy 62.7 72.2 59.9 73.7
* un - unstructured, s - structured
Table 3: Performance of automatic identification
of user intent
5 Implications of Problematic Situations
and User Intent
Automated identification of problematic situations
and user intent have potential implications in the
design of conversational QA systems. Identifica-
tion of problematic situations can be considered as
implicit feedback. The system can use this feed-
back to improve its answer retrieval performance
and proactively adapt its strategy to cope with
problematic situations. One might think that an
alternative way is to explicitly ask users for feed-
back. However, this explicit approach will defeat
the purpose of intelligent conversational systems.
Soliciting feedback after each question not only
will frustrate users and lengthen the interaction,
but also will interrupt the flow of user thoughts and
conversation. Therefore, our focus here is to inves-
tigate the more challenging end of implicit feed-
back. In practice, the explicit feedback and im-
plicit feedback should be intelligently combined.
For example, if the confidence for automatically
identifying a problematic situation or an error-free
situation is low, then perhaps explicit feedback can
be solicited.
Automatic identification of user intent also has
important implications in building intelligent con-
versational QA systems. For example, if Con-
tinue is identified during interaction, then the sys-
tem can automatically collect the question answer
pairs for potential future use. If Switch is identi-
fied, the system may put aside the question that has
not been correctly answered and proactively come
back to that question later after more information
is gathered. If Re-try is identified, the system may
avoid repeating the same answer and at the same
time may take the initiative to guide users on how
to rephrase a question. If Negotiate is identified,
the system may want to investigate the user?s par-
ticular needs that may be different from the gen-
eral needs. Overall, different strategies can be de-
veloped to address problematic situations and dif-
ferent intents. We will investigate these strategies
in our future work.
This paper reports our initial effort in investi-
gating interactive QA from a conversational point
of view. The current investigation has several
simplifications. First, our current work has fo-
cused on factoid questions where it is relatively
easy to judge a problematic or error-free situation.
However, as discussed in earlier work (Small et
al., 2003), sometimes it is very hard to judge the
truthfulness of an answer, especially for analyti-
cal questions. Therefore, our future work will ex-
amine the new implications of problematic situa-
tions and user intent for analytical questions. Sec-
63
ond, our current investigation is based on a ba-
sic interactive mode. As mentioned earlier, once
the QA systems become more intelligent and con-
versational, more varieties of user intent are an-
ticipated. How to characterize and automatically
identify more complex user intent under these dif-
ferent situations is another direction of our future
work.
6 Conclusion
This paper presents our initial investigation on
automatic identification of problematic situations
and user intent in interactive QA. Our results have
shown that, once users are motivated in finding
specific information related to their information
goals, user behavior and interaction context can
help automatically identify problematic situations
and user intent. Although our current investigation
is based on the data collected from a controlled
study, the same approaches can be applied dur-
ing online processing as the question answering
proceeds. The identified problematic situations
and/or user intent will provide immediate feed-
back for a QA system to adjust its behavior and
adapt better strategies to cope with different situa-
tions. This is an important step toward intelligent
conversational question answering.
References
Marco De Boni and Suresh Manandhar. 2003. An
analysis of clarification dialogues for question an-
swering. In Proceedings of HLT-NAACL 2003,
pages 48?55.
John Burger, Claire Cardie, Vinay Chaudhri, Robert
Gaizauskas, Sanda Harabagiu, David Israel, Chris-
tian Jacquemin, Chin-Yew Lin, Steve Maiorano,
George Miller, Dan Moldovan, Bill Ogden, John
Prager, Ellen Riloff, Amit Singhal, Rohini Shrihari,
Tomek Strzalkowski, Ellen Voorhees, and Ralph
Weishedel. 2001. Issues, tasks and program struc-
tures to roadmap research in question & answering.
In NIST Roadmap Document.
Sanda Harabagiu, Andrew Hickl, John Lehmann, and
Dan Moldovan. 2005. Experiments with interactive
question-answering. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 205?214, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
Julia Hirschberg, Diane J. Litman, and Marc Swerts.
2001. Identifying user corrections automatically
in spoken dialogue systems. In Proceedings of
the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL?01).
Gina-Anne Levow. 1998. Characterizeing and recog-
nizing spoken corrections in human-computer dia-
logue. In Proceedings of the 36th Annual Meet-
ing of the Association of Computational Linguistics
(COLING/ACL-98), pages 736?742.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of International
Conference on Machine Learning, Madison, Wis-
consin, July.
Diane J. Litman and Shimei Pan. 2000. Predicting
and adapting to poor speech recognition in a spo-
ken dialogue system. In Proceedings of the Seven-
teenth National Conference on Artificial Intelligence
(AAAI-2000), pages 722?728.
Diane J. Litman, Marilyn A. Walker, and Michael S.
Kearns. 1999. Automatic detection of poor speech
recognition at the dialogue level. In Proceedings of
the 37th Annual meeting of the Association of Com-
putational Linguistics (ACL-99), pages 309?316.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov mod-
els for information extraction and segmentation. In
Proceedings of Internatioanl Conference on Ma-
chine Learning (ICML 2000), pages 591?598.
Sharon Small, Ting Liu, Nobuyuki Shimizu, and
Tomek Strzalkowski. 2003. HITIQA: An interac-
tive question answering system: A preliminary re-
port. In Proceedings of the ACL 2003 Workshop on
Multilingual Summarization and Question Answer-
ing.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Marie Meteer, and Carol Van
Ess-Dykema. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. In Computational Linguistics, volume 26.
Ellen Voorhees. 2001. Overview of TREC 2001 ques-
tion answering track. In Proceedings of TREC.
Ellen Voorhees. 2004. Overview of TREC 2004. In
Proceedings of TREC.
Marilyn Walker, Irene Langkilde-Geary, Helen Wright
Hastie, Jerry Wright, and Allen Gorin. 2002. Auto-
matically training a problematic dialogue predictor
for the HMIHY spoken dialog system. In Journal of
Artificial Intelligence Research.
64
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 710?719,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Autonomous Self-Assessment of Autocorrections: Exploring Text Message
Dialogues
Tyler Baldwin
Department of Computer
Science and Engineering
Michigan State University
East Lansing, MI 48824
baldwi96@cse.msu.edu
Joyce Y. Chai
Department of Computer
Science and Engineering
Michigan State University
East Lansing, MI 48824
jchai@cse.msu.edu
Abstract
Text input aids such as automatic correction
systems play an increasingly important role in
facilitating fast text entry and efficient com-
munication between text message users. Al-
though these tools are beneficial when they
work correctly, they can cause significant
communication problems when they fail. To
improve its autocorrection performance, it is
important for the system to have the capabil-
ity to assess its own performance and learn
from its mistakes. To address this, this pa-
per presents a novel task of self-assessment of
autocorrection performance based on interac-
tions between text message users. As part of
this investigation, we collected a dataset of au-
tocorrection mistakes from true text message
users and experimented with a rich set of fea-
tures in our self-assessment task. Our exper-
imental results indicate that there are salient
cues from the text message discourse that al-
low systems to assess their own behaviors with
high precision.
1 Introduction
The use of SMS text messaging is widespread and
growing. Users of text messaging often rely on small
mobile devices with limited user interfaces to com-
municate with each other. To support efficient com-
munication between users, many tools to aid text in-
put such as automatic completion (autocompletion)
and automatic correction (autocorrection) have be-
come available. When they work correctly, these
tools allow users to maintain clear communication
while potentially increasing the rate at which they
input their message, improving efficiency in com-
munication. However, when these tools make a mis-
take, they can cause problematic situations. Con-
sider the following example:
A1: Euthanasia doing tonight?
B1: Euthanasia?!
A2: I typed whatcha and stupid autotype.
In this example, the automatic correction system
on person A?s phone interpreted his attempt to write
the word whatcha as an attempt to write euthanasia
(due to the keyboard adjacency of the w and e keys,
etc.). This completely changed the meaning of the
message, which confused person B. Although this
instance was eventually discovered and corrected,
the natural flow of conversation was interrupted and
the participants were forced to make extra effort to
clarify this confusion.
This example indicates that the cost of a mistake
in autocorrection is potentially high. This is exacer-
bated by the fact that users will often fail to notice
these mistakes in a timely manner, due to their focus
being on the keyboard (Paek et al, 2010) and the
quick and casual conversation style of text messag-
ing. Because of this, autocorrection systems must
have high accuracy to be useful for text messaging.
This example also indicates that, when an autocor-
rection mistake happens (i.e., mistaken correction
of euthanasia), it often causes confusion which re-
quires dialogue participants to use the follow-up dia-
logue to clarify the intent. What this suggests is that
the discourse between text message users may pro-
vide important information for autocorrection sys-
710
tems to assess whether an attempted correction is
indeed what the user intended to type.
Self-assessment of its correction performance will
allow an autocorrection system to detect correction
mistakes, learn from such mistakes, and potentially
improve its correction performance for future opera-
tions. For instance, if a system is able to identify that
its current autocorrection policy results in too many
mistakes it may choose to adopt a more cautious cor-
rection policy in the future. Additionally, if it is able
to discover not only that a mistake has taken place
but what the ideal action should have been, it will be
able to use this data to learn a more refined policy
for future attempts.
Motivated by this observation, this paper inves-
tigates the novel task of self-assessment of auto-
correction performance based on interactions be-
tween dialogue participants. In particular, we
formulate this task as the automatic identification
of correction mistakes and their corresponding in-
tended words based on the discourse. For instance,
in the previous example, the system should automat-
ically detect that the attempted correction ?euthana-
sia? is a mistake and the true term (i.e., intended
word) should have been ?whatcha?. To support our
investigation, we collected a dataset of autocorrec-
tion mistakes from true text message users. We fur-
ther experimented with a rich set of features in our
self-assessment task. Our experimental results in-
dicate that there are salient cues from the text mes-
sage discourse that potentially allow systems to as-
sess their own behavior with high precision.
In the sections that follow, we first introduce and
give an analysis of our dataset. We then highlight
the two interrelated problems that must be solved for
system self-assessment, and outline and evaluate our
approach to each of these problems. Finally, we ex-
amine the results of applying the system assessment
procedure end-to-end and discuss potential applica-
tions of autocorrection self-assessment.
2 Related Work
Spelling autocorrection systems grew naturally out
of the well studied field of spell checking. Most spell
checking systems are based on a noisy channel for-
mulation (Kernighan et al, 1990). Later refinements
allowed for string edit operations of arbitrary length
(Brill and Moore, 2000) and pronunciation modeling
(Toutanova and Moore, 2002). More recent work
has examined the use of the web as a corpus to build
a spell checking and autocorrection system without
the need for labeled training data (Whitelaw et al,
2009).
Traditional spell checking systems generally as-
sume that misspellings are unintentional. However,
much of the spelling variation that appears in text
messages may be produced intentionally. For in-
stance, text message authors make frequent use of
acronyms and abbreviations. This motivates the
task of text message normalization (Aw et al, 2006;
Kobus et al, 2008), which attempts to transform all
non-standard spellings in a text message into their
standard form. The style of misspelling in text mes-
sages is often quite different from that of standard
prose. For instance, Whitelaw et. al. (2009) applied
the Aspell spell checker1 on a corpus of mistakes in
English prose and achieved an error rate of under
5%. Conversely, the same spell checker was found
to have an error rate of over 75% on text message
data (Choudhury et al, 2007).
Autocorrection in text messaging is similar to pre-
dictive texting and word completion technologies
(Dunlop and Crossan, 2000). These technologies
attempt to reduce the number of keystrokes a user
must type (MacKenzie, 2002), potentially speeding
up text entry. There are 2 primary sources of liter-
ature on text prediction. In one (often called auto-
completion), systems attempt to predict the intended
term before the user has finished typing it (Darragh
et al, 1990; Chaudhuri and Kaushik, 2009). In the
second, the system attempts to interpret ambiguous
user input typed on a keyboard with a small number
of keys, such as the 12 key keyboards found on many
mobile phones (MacKenzie and Tanaka-Ishii, 2007).
Few studies have looked at the effects SMS writing
style has on predictive text performance. How and
Kan (2005) analyze a corpus of 10,000 text mes-
sages and conclude that changing the standard map-
ping of letters to keys on 12 key keyboards could
improve input performance on SMS data.
Although never examined in the context of auto-
correction systems, system self-assessment has been
studied in other domains. One of the most com-
1http://aspell.net/
711
Figure 1: Example text message dialogue from our cor-
pus with an automatic correction mistake
mon application domains is spoken dialogue sys-
tems (Levow, 1998; Hirschberg et al, 2001; Litman
et al, 2006), where detecting problematic situations
can help the system better adapt to user behavior.
These systems often make use of prosody and task
specific dialogue acts, two feature sources unavail-
able in general text message dialogues.
In summary, while a large body of work addresses
similar problems, to our knowledge no previous
work has looked into the aspect of self-assessment
of autocorrection based on dialogues between text
message users. The work presented in this paper
represents a first step in this direction.
3 Data Set
To support our investigation, we collected a cor-
pus of data containing true experiences with auto-
correction provided by text message users. The
website ?Damn You Auto Correct?2 (DYAC) posts
screenshots of text message conversations that con-
tain mistakes caused by phone automatic correction
systems, as sent in by cellphone users. An example
screenshot is shown is Figure 1.
Speech bubbles originating from the left of the
image in Figure 1 are messages sent by one dialogue
participant while those originating from the right of
the image are sent by the other. In this example, the
automatic correction system incorrectly decides that
the user?s attempt to write the non-standard word
form thaaaats was an attempt to write the word Tus-
saud. This confuses the reader, and several dialogue
turns are used to resolve the confusion. The author
2www.damnyouautocorrect.com
explicitly corrects her mistake by writing ?I meant
thaaaats?.
Note that, in this example, the word Tussaud
could be an autocompletion or an autocorrection
by the system. However, there may be no signifi-
cant distinction between these two operations from
a user?s point of view. These two operations could
also take place at the same time. For instance, a
system may both suggest possible completions after
the user has only typed a small number of characters
and perform autocorrection once the user presses the
space bar to go on to the next word. Therefore, for
the purposes of our discussion here, we use autocor-
rection to refer to any changes made by the system
(either by autocompletion or autocorrection) with-
out the user explicitly selecting the correction them-
selves.
Throughout the paper, we use the term attempted
correction to refer to any autocorrection made by
the system; for example, Tussaud is an attempted
correction in Figure 1. Some attempted corrections
could correct to the word that the user intended,
which will be referred to as unproblematic cor-
rections or non-erroneous corrections. Other at-
tempted corrections may mistakenly choose a word
that the user did not intend to write, which will be re-
ferred to as correction mistakes or erroneous cor-
rections. For example, Tussaud is an erroneous cor-
rection. We use the term intended word to refer to
the term that the user was attempting to type when
the autocorrection system intervened. For instance,
in the erroneous correction in Figure 1, the intended
term was thaaaats.
To build our dataset, screenshots were extracted
from the site and transcribed, and correction mis-
takes were annotated with their intended words, if
the intended word appeared in the dialogue. Be-
cause the website presents autocorrection mistakes
that submitters find to be humorous or egregious,
there may be an incentive for users to submit fal-
sified instances. To combat this, we performed an
initial filtering phase to remove instances that were
unlikely to have been produced by a typical autocor-
rection system (e.g., instances that substituted letters
that were far from each other on the keyboard and
not phonetically similar) or that were otherwise be-
lieved to be falsified. Using this methodology we
compiled a development set of 300 dialogues and an
712
Figure 2: Text message dialogue with several correction
mistakes for the same intended term.
additional 635 dialogues for evaluation.
Some dialogues contained several correction mis-
takes. It was common for multiple correction mis-
takes to be produced in an attempt at typing a single
word; an example is shown in Figure 2, in which
the intended term cookies is erroneously corrected
at first as commies and then as cockles.
We will use the term message to refer to one SMS
text message sent in the course of the conversation,
while a turn encompasses all messages sent by a user
between messages from the other participant. For
instance, the first 3 speech bubbles in Figure 2 all
represent separate messages, but they are all part of
the same turn.
While this dataset provides us with instances of
autocorrection mistakes, in order to differentiate be-
tween problematic and unproblematic correction at-
tempts we will need a dataset of unproblematic at-
tempts as well. It should be noted that, from the per-
spective of the reader, a successful autocorrection at-
tempt is equivalent to the user typing correctly with-
out any intervention from the system at all. To build
a dataset of unproblematic instances, we collected
text message conversations from pairs of users with-
out the aid of autocorrection. Users were then asked
to correct any mistakes they produced. Snippets
of these conversations that did not contain mistakes
were then extracted to act as a set of unproblematic
autocorrection instances. In total 554 snippets were
extracted. These snippets were combined with the
problematic instances from the DYAC data to make
the final dataset used for training and evaluation.
4 Autocorrection Self-Assessment
It is desirable for an autocorrection system to have
the capability to assess its own performance. For
each correction attempt it makes, if the system can
evaluate its performance based on the dialogue it can
acquire valuable information to learn from its own
mistakes and thus improve its performance for fu-
ture operations. Next we describe how we formulate
the task of self-assessment and what features can be
used for this task.
Because each correction attempt is system gener-
ated, an autocorrection system should have knowl-
edge of all correction attempts it has made. Let C
be the set of all correction attempts performed by an
autocorrection system over the course of a dialogue
and let W be the set of all words in this dialogue
which occur after the correction attempt. We model
this problem as two distinct subtasks: 1) identify at-
tempted corrections ci ? C which are erroneous (if
there are any), and 2) for each erroneous correction
ci, identify a word wj ? W which is the intended
word for ci (i.e., Intended(ci) = wj).
4.1 Identifying Erroneous Corrections
The first task involves a simple binary decision;
given an arbitrarily sized dialogue snippet contain-
ing an automatic correction attempt, we must decide
whether or not the system acted erroneously when
making the correction. We thus model the task as
a binary classification problem in which we classify
every correction attempt c ? C as either erroneous
or non-erroneous.
The proposed method follows a standard proce-
dure for supervised binary classification. First we
must build a set of labeled training data in which
each instance is represented as a vector of features
and a ground truth class label. Given this, we can
train a classifier to differentiate between the two
classes. For the purposes of this work we use a sup-
port vector machine (SVM) classifier.
4.1.1 Feature Set
In order to detect problematic corrections, we
must identify dialogue behaviors that signify an er-
ror has occurred. We examined the dialogues in
our development set to understand which dialogue
behaviors are indicative of autocorrection mistakes.
While in unproblematic dialogues users are able to
converse freely, in problematic dialogues users must
spend dialogue turns reestablishing common ground
(Clark, 1996). Our feature set will focus on two
common ways these attempts to establish common
713
ground manifest themselves: as confusion and as at-
tempts to correct the mistake.
Confusion Detection Features. Because autocor-
rection mistakes often result in misleading or se-
mantically vacuous utterances, they are apt to con-
fuse the reader, who will often express this confu-
sion in the dialogue in order to gain clarification.
These features examine the dialogue of the uncor-
rected user (the dialogue participant that reads the
automatic correction mistake, not the one that was
automatically corrected). One sign of confusion is
the use of the question mark, so one feature captured
the presence of question marks in the messages sent
by the uncorrected user. Similarly, users may often
use a block or repeated punctuation of show suprise
or confusion, so another feature detected instances
of repeated question marks and exclamation points
(???, !?!!, etc.). When confused, readers will often
retype the confusing word as a request for clarifica-
tion (e.g., Tussaud?), or simply type ?what??. We
therefore include features that detect whether or not
the corrected term appears in the first message sent
by the uncorrected user after the correction mistake
has occurred, and whether or not this message con-
tains the word ?what? as its own clause.
Clarification Detection Features. In contrast to ut-
terances of confusion which are generally produced
by the reader of the autocorrection mistake, clarifi-
cation attempts are usually initiated by the user that
was corrected. Several methods are used to indicate
that the term shown by the system was incorrect.
One convention is to use an asterisk (*) either be-
fore or after the corrected term:
A1: Indeed Sid
A2: Sir*
Another common method is to explicitly state
what was intended using phrases such as ?I meant
to type?, ?that was supposed to say?, etc. We in-
cluded several features to capture these word pat-
terns. Another method is to simply quickly reply
with the word that was intended, so we included a
feature to record whether the next message after the
correction attempt contains only a single word. As
users often feel the need to explain why the mistake
occurred, we included a feature that recorded any
mention or autocompletion, autocorrection or spell
Features Precision Recall F-Measure
All Features .861 .751 .803
-Confusion .857 .725 .786
-Clarification .848 .676 .752
-Dialogue .896 .546 .679
Baseline .568 1 .724
Table 1: Feature ablation results for identifying autocor-
rection mistakes
checking. One additional feature recorded whether
or not the corrected user?s dialogue contained words
written in all capital letters.
Dialogue Features. A few features captured infor-
mation more closely tied to the flow of the dialogue
than to confusion or clarification. In our develop-
ment set, we observed a few common dialogue for-
mats. In one, a correction mistake is immediately
followed by confusion, which is then immediately
followed by clarification. The dialogue in Figure 1
gives an example of this. To capture this form, we
included a feature that recorded whether a confusion
feature was present in the message immediately fol-
lowing the correction attempt and whether a clarifi-
cation feature was present in the message immediate
following the confusion message. Similarly, clarifi-
cation attempts are often tried immediately after the
mistake even if no confusion was present, so an ad-
ditional feature captured whether the first message
after the mistake by the corrected user was a clari-
fication attempt. Additionally, we observed that au-
tocorrection mistakes frequently appeared in the last
word in a message, which was recorded by another
binary feature. Finally, we recorded a count of how
often the corrected term appeared in the dialogue.
4.1.2 Evaluation
To build our classifier we used the SVMLight3
implementation of a support vector machine clas-
sifier with an RBF kernel. To ensure validity and
account for the relatively small size of our dataset,
evaluation was done via leave-one-out cross valida-
tion.
Results are shown in Table 1. A majority class
baseline is given for comparison. As shown, using
the entire feature set, the classifier achieves above
baseline precision of 0.861, while still producing re-
call of 0.751.
3Version 6.02, http://svmlight.joachims.org/
714
Although F-measure is reported, it is unlikely that
precision and recall should be weighted equally. Be-
cause one of the primary reasons we may wish to
detect problematic situations is to automatically col-
lect data to improve future performance by the au-
tocorrection system, it is imperative that the data
collected have high precision in order to reduce the
amount of noise present in the collected dataset.
Conversely, because problematic situation detection
can monitor a user?s input continuously for an in-
definite period of time in order to collect more data,
recall is less of a concern.
To study the effect of each feature source, we per-
formed a feature ablation study, the results of which
are included in Table 1. For each run, one feature
type was removed and the model was retrained and
reassessed. As shown, removing any feature source
has a relatively small effect on the precision but a
more substantial effect on the recall. Confusion de-
tection features seem to be the least essential, caus-
ing a comparatively small drop in precision and re-
call values when removed. Removing the dialogue
features results in the greatest drop in recall, return-
ing only slightly above half of the problematic in-
stances. However, as a result, the precision of the
classifier is higher than when all features are used.
4.2 Identifying The Intended Term
Note that one purpose of the proposed self-
assessment is to collect information online and thus
make it possible to build better models. In order
to do so, we need to know not only whether the
system acted erroneously, but also what it should
have done.Therefore, once we have extracted a set of
problematic instances (and their corresponding dia-
logues), we must identify the term which the user
was attempting to type when the system intervened.
First, assume that via the classification task de-
scribed in Section 4.1 we have identified a set of er-
roneous correction attempts, EC. Now the problem
becomes, for every erroneous correction c ? EC,
identify w ? W such that w = Intended(c). We
model this as a ranking task, in which all w ? W
are ranked by their likelihood of being the intended
term for c. We then predict that the top ranked word
is the true intended term.
4.2.1 Feature Set
To support the above processing, we explored a
diverse feature set, consisting of five different fea-
ture sources: contextual, punctuation, word form,
similarity, and pattern features, crafted from an ex-
amination of our development data. Several of the
features are related to those used in the initial clas-
sification phase. However, unlike our classification
features, these feature focus on the relationship be-
tween the erroneous correction c and a candidate in-
tended term w.
Contextual Features. Contextual features capture
relevant phenomena at the discourse level. After an
error is discovered by a user, they may type an in-
tended term several times or type it in a message by
itself in order to draw attention to it. These phe-
nomena are captured in the word repetition and only
word features. Another common discourse related
correction technique is to retype some of the origi-
nal context, which is captured by the word overlap
feature. The same author feature indicates whether
c and w are written by the same author. The author
of the original mistake is likely the one to correct it,
as they know their true intent.
Punctuation Features. Punctuation is occasionally
used by text message writers to signal a correction of
an earlier mistake, as noted previously. We included
features to capture the presence of several different
punctuation marks occurring before or after a candi-
date word such as *,?,!, etc. Each punctuation mark
is represented by a separate feature.
Word Form Features. Word form features cap-
ture variations in how a word is written. One word
form feature captures whether a word was typed in
all capital letters, a technique used by text message
writers to add emphasis. Two word form features
were designed to capture words that were potentially
unknown to the system, out-of-vocabulary words
and words with letter repetition (e.g., ?yaaay?). Be-
cause the system does not know these words, it
will consider them misspellings and may attempt to
change them to an in-vocabulary term.
Similarity Features. Our similarity feature cap-
tured the character level distance between a word
changed by the system and a candidate intended
word. We calculated the normalized levenshtein edit
distance between the two words as a measure of sim-
715
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pre
cis
ion
Recall
All Features
-Contextual
-Punctuation
-SimilarityBaseline
Figure 3: Precision-recall curve for intended term selec-
tion, including feature ablation results
ilarity.
Pattern Features. Pattern features attempt to cap-
ture phrases that are used to explicitly state a cor-
rection. These include phrases such as ?(I) meant
to write w?, ?(that was) supposed to say w?, ?(that)
should have read w?, ?(I) wrote w?, etc.
4.2.2 Evaluation
To find the most likely intended term for a cor-
rection mistake, we rank every candidate word in W
and predict that the top ranked word is the intended
term. We used the ranking mode of SVMlight to
train our ranker. By thresholding our results to only
trust predictions in which the ranker reported a high
ranking value for the top term, we were able to ex-
amine the precision at different recall levels. That
is, if the top ranked term does not meet the thresh-
old, we simply do not predict an intended term for
that instance, hurting recall but hopefully improv-
ing precision by removing instances that we are not
confident about. This thresholding process may also
allow the ranker to exclude instances in which the in-
tended term does not appear in the dialogue, which
are hopefully ranked lower than other cases. As be-
fore, evaluation was done via leave-one-out cross
validation.
Results are shown in Figure 3. As a method
of comparison we report a baseline that selects the
word with the smallest edit distance as the intended
term. As shown, using the entire feature set results
in consistently above baseline performance.
As before, we are more concerned with the pre-
cision of our predictions than the recall. It is diffi-
cult to assess the appropriate precision-recall trade-
off without an in-depth study of autocorrection us-
age by text messagers. However, a few observations
can be made from the precision-recall curve. Most
critically, we can observe that the model is able to
predict the intended term for an erroneous correc-
tion with high precision. Additionally, the precision
stays relatively stable as recall increases, suffering
a comparatively small drop in precision for an in-
crease in recall. At its highest achieved recall values
of 0.892, it maintains high precision at 0.869.
Feature ablation results are also reported in Fig-
ure 3. The most critical feature source was word
similarity; without the similarity feature the perfor-
mance is consistently worse than all other runs, even
falling below baseline performance at high recall
levels. This is not suprising, as the system?s incor-
rect guess must be at least reasonably similar to the
intended term, or the system would be unlikely to
make this mistake. Although not as substantial as
the similarity feature, the contextual and punctuation
features were also shown to have a significant effect
on overall performance. Conversely, removing word
form or pattern features did not cause a significant
change in performance (not shown in Figure 3 to en-
hance readability).
5 An End-To-End System
In order to see the actual effect of the full system,
we ran it end-to-end, with the output of the initial
erroneous correction identification phase used as in-
put when identifying the intended term. Results are
shown in Figure 4. The results of the intended term
classification task on gold standard data from Figure
3 are shown as an upper bound.
As expected, the full end-to-end system produced
lower overall performance than running the tasks in
isolation. The end-to-end system can reach a recall
level of 0.674, significantly lower than the recall of
the ground truth system. However, the system still
peaks at precision of 1, and was able to produce pre-
cision values that were competitive with the ground
truth system at lower recall levels, maintaining a pre-
cision of above 0.90 until recall reached 0.396.
It is worth mentioning that the current evalua-
716
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pre
cis
ion
Recall
Gold StandardEnd To End
Figure 4: Precision-recall curve for the end-to-end sys-
tem
tion is based on a balanced dataset with roughly
even numbers of problematic and unproblematic in-
stances. It is likely that in a realistic setting an au-
tocorrection system will get many more instances
correct than wrong, leading to a data distribution
skewed in favor of unproblematic instances. This
suggests that the evaluation given here may overes-
timate the performance of a self-assessment system
in a real scenario. Although the size of our dataset
is insufficient to do a full analysis on skewed data,
we can get a rough estimate of the performance by
simply counting false positives and false negatives
unevenly. For instance, if the cost of mispredicting
a unproblematic case as problematic is nine times
more severe than the cost of missing a problematic
case, this can give us an estimate of the performance
of the system on a dataset with a 90-10 skew.
We examined the 90-10 skew case to see if the
procedure outlined here was still viable. Results of
an end-to-end system with this data skew are con-
sistently lower than the balanced data case. The
skewed data system can keep performance of 90%
or better until it reaches 13% recall, and 85% or bet-
ter until it reaches 22%. These results suggest that
the system could still potentially be utilized. How-
ever, its performance drops off steadily, to the point
where it would be unlikely to be useful at higher re-
call levels. We leave the full exploration of this to
future work, which can utilize larger data sets to get
a more accurate understanding of the performance.
6 Discussion
When an autocorrection system attempts a correc-
tion, it has perfect knowledge of the behavior of both
itself and the user. It knows the button presses the
user used to enter the term. It knows the term it
chose as a correction. It knows the surrounding con-
text; it has access to both the messages sent and re-
ceived by the user. It has a large amount of the infor-
mation it could use to improve its own performance,
if only it were able to know when it made a mis-
take. The techniques described here attempt to ad-
dress this critical system assessment step. Users may
vary in the speed and accuracy at which they type,
and input on small or virtual keyboards may vary
between users based on the size and shape of their
fingers. The self-assessment task described here can
potentially facilitate the development of autocorrec-
tion models that are tailored to specific user behav-
iors.
Here is a brief outline of how our self-assessment
module might potentially be used in building user-
specific correction models. As a user types input, the
system performs autocorrection by starting with a
general model (e.g., for all text message users). Each
time a correction is performed, the system exam-
ines the surrounding context to determine whether
the correction it chose was actually what the user
had intended to type. Over the course of several
dialogues, the system builds a corpus of erroneous
and non-erroneous correction attempts. This corpus
is then used to train a user-specific correction model
that is targeted toward system mistakes that are most
frequent with this user?s input behavior. The user-
specific model is then applied on future correction
attempts to improve overall performance. This mon-
itoring process can be continued for months or even
longer. The results from self-assessment will al-
low the system to continuously and autonomously
improve itself for a given user (Baldwin and Chai,
2012).
In order to learn a user-specific model that is ca-
pable of improving performance, it is important that
the self-assessment system provides it with training
data without a large amount of noise. This suggests
that the self-assessment system must be able to iden-
tify erroneous instances with high precision. Con-
versely, because the system can monitor user behav-
717
ior indefinitely to collect more data, the overall re-
call may not be as critical. It might then be reason-
able for a self-assessment system to be built to focus
on collecting high accuracy pairs, even if it misses
many system mistakes. Although a full examination
of this tradeoff is left for future work which may
more closely examine user input behavior, we feel
that the results presented here show promise for col-
lecting accurate data in a timely manner.
7 Conclusions and Future Work
This paper describes a novel problem of assessing
its own correction performance for an autocorrection
system based on dialogue between two text mes-
saging users. Our evaluation results indicate that
given a problematic situation caused by an auto-
correction system, the discourse between users pro-
vides important cues for the system to automati-
cally assess its own correction performance. By
exploring a rich set of features from the discourse,
our proposed approach is able to both differentiate
between problematic and unproblematic instances
and identify the term the user intended to type with
high precision, achieving significantly above base-
line performance. As discussed in Section 6, this
self-assessment task can potentially be important for
building user-specific autocorrection models to im-
prove auto-correction performance.
The results presented in this paper represent a
first look at autocorrection self-assessment. There
are several areas of future work. There is certainly
a need to examine additional feature sources. Be-
cause automatic correction mistakes can potentially
create semantically vacuous utterances, a computa-
tional semantics based approach, similar to those
used in semantic autocompletion systems (Hyvnen
and Mkel, 2006), may prove fruitful. Addition-
ally, although this work focused solely on dialogue-
related features, future work may wish to take a
closer look at the autocorrection mistakes them-
selves (e.g., which words are most likely to be mis-
takenly corrected, etc.). Lastly, although our current
work demonstrated some potential, more thorough
evaluation in realistic settings will allow a more full
understanding of the impact and limitations of the
proposed self-assessment approach.
Acknowledgments
This work was supported in part by Award #0957039
from the National Science Foundation and Award
#N00014-11-1-0410 from the Office of Naval Re-
search. The authors would like to thank the review-
ers for their valuable comments and suggestions.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normaliza-
tion. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 33?40, Morristown,
NJ, USA. Association for Computational Linguistics.
Tyler Baldwin and Joyce Chai. 2012. Towards on-
line adaptation and personalization of key-target resiz-
ing for mobile devices. In Proceedings of the 2012
ACM international conference on Intelligent User In-
terfaces, IUI ?12, pages 11?20, New York, NY, USA.
ACM.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
ACL ?00: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, pages
286?293, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Surajit Chaudhuri and Raghav Kaushik. 2009. Extend-
ing autocompletion to tolerate errors. In Proceed-
ings of the 35th SIGMOD international conference on
Management of data, SIGMOD ?09, pages 707?718,
New York, NY, USA. ACM.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. Int. J. Doc. Anal. Recognit.,
10(3):157?174.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
J.J. Darragh, I.H. Witten, and M.L. James. 1990. The
reactive keyboard: a predictive typing aid. Computer,
23(11):41 ?49, November.
Mark Dunlop and Andrew Crossan. 2000. Predictive text
entry methods for mobile phones. Personal and Ubiq-
uitous Computing, 4:134?143. 10.1007/BF01324120.
Julia Hirschberg, Diane J. Litman, and Marc Swerts.
2001. Identifying user corrections automatically in
spoken dialogue systems. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the As-
sociation for Computational Linguistics.
Yijue How and Min yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
718
phones. In in Human Computer Interfaces Interna-
tional (HCII 05). 2005: Las Vegas.
Eero Hyvnen and Eetu Mkel. 2006. Semantic autocom-
pletion. In Proceedings of the first Asia Semantic Web
Conference (ASWC 2006, pages 4?9. Springer-Verlag.
Mark D. Kernighan, Kenneth W. Church, and William A.
Gale. 1990. A spelling correction program based on a
noisy channel model. In Proceedings of the 13th con-
ference on Computational linguistics, pages 205?210,
Morristown, NJ, USA. Association for Computational
Linguistics.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing SMS: are two metaphors
better than one ? In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 441?448, Manchester, UK, August.
Coling 2008 Organizing Committee.
Gina-Anne Levow. 1998. Characterizing and recogniz-
ing spoken corrections in human-computer dialogue.
In Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics,
Volume 1, pages 736?742, Montreal, Quebec, Canada,
August. Association for Computational Linguistics.
Diane Litman, Julia Hirschberg, and Marc Swerts. 2006.
Characterizing and predicting corrections in spoken
dialogue systems. Comput. Linguist., 32:417?438,
September.
I. Scott MacKenzie and Kumiko Tanaka-Ishii. 2007.
Text Entry Systems: Mobility, Accessibility, Universal-
ity (Morgan Kaufmann Series in Interactive Technolo-
gies). Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
I. Scott MacKenzie. 2002. Kspc (keystrokes per charac-
ter) as a characteristic of text entry techniques. In Pro-
ceedings of the 4th International Symposium on Mo-
bile Human-Computer Interaction, Mobile HCI ?02,
pages 195?210, London, UK. Springer-Verlag.
Tim Paek, Kenghao Chang, Itai Almog, Eric Badger,
and Tirthankar Sengupta. 2010. A practical exami-
nation of multimodal feedback and guidance signals
for mobile touchscreen keyboards. In Proceedings of
the 12th international conference on Human computer
interaction with mobile devices and services, Mobile-
HCI ?10, pages 365?368, New York, NY, USA. ACM.
Kristina Toutanova and Robert Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
40th Annual Meeting of the Association for Computa-
tional Linguistics(ACL 2002).
Casey Whitelaw, Ben Hutchinson, Grace Y Chung, and
Ged Ellis. 2009. Using the Web for language indepen-
dent spellchecking and autocorrection. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 890?899, Singapore,
August. Association for Computational Linguistics.
719
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1159?1168,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Adaptive Parser-Centric Text Normalization
Congle Zhang?
Dept of Computer Science and Engineering
University of Washington, Seattle, WA 98195, USA
clzhang@cs.washington.edu
Tyler Baldwin Howard Ho Benny Kimelfeld Yunyao Li
IBM Research - Almaden
650 Harry Road, San Jose, CA 95120, USA
{tbaldwi,ctho,kimelfeld,yunyaoli}@us.ibm.com
Abstract
Text normalization is an important first
step towards enabling many Natural Lan-
guage Processing (NLP) tasks over infor-
mal text. While many of these tasks, such
as parsing, perform the best over fully
grammatically correct text, most existing
text normalization approaches narrowly
define the task in the word-to-word sense;
that is, the task is seen as that of mapping
all out-of-vocabulary non-standard words
to their in-vocabulary standard forms. In
this paper, we take a parser-centric view
of normalization that aims to convert raw
informal text into grammatically correct
text. To understand the real effect of nor-
malization on the parser, we tie normal-
ization performance directly to parser per-
formance. Additionally, we design a cus-
tomizable framework to address the often
overlooked concept of domain adaptabil-
ity, and illustrate that the system allows for
transfer to new domains with a minimal
amount of data and effort. Our experimen-
tal study over datasets from three domains
demonstrates that our approach outper-
forms not only the state-of-the-art word-
to-word normalization techniques, but also
manual word-to-word annotations.
1 Introduction
Text normalization is the task of transforming in-
formal writing into its standard form in the lan-
guage. It is an important processing step for a
wide range of Natural Language Processing (NLP)
tasks such as text-to-speech synthesis, speech
recognition, information extraction, parsing, and
machine translation (Sproat et al, 2001).
?This work was conducted at IBM.
The use of normalization in these applications
poses multiple challenges. First, as it is most often
conceptualized, normalization is seen as the task
of mapping all out-of-vocabulary non-standard
word tokens to their in-vocabulary standard forms.
However, the scope of the task can also be seen as
much wider, encompassing whatever actions are
required to convert the raw text into a fully gram-
matical sentence. This broader definition of the
normalization task may include modifying punc-
tuation and capitalization, and adding, removing,
or reordering words. Second, as with other NLP
techniques, normalization approaches are often fo-
cused on one primary domain of interest (e.g.,
Twitter data). Because the style of informal writ-
ing may be different in different data sources,
tailoring an approach towards a particular data
source can improve performance in the desired do-
main. However, this is often done at the cost of
adaptability.
This work introduces a customizable normal-
ization approach designed with domain transfer in
mind. In short, customization is done by provid-
ing the normalizer with replacement generators,
which we define in Section 3. We show that the
introduction of a small set of domain-specific gen-
erators and training data allows our model to out-
perform a set of competitive baselines, including
state-of-the-art word-to-word normalization. Ad-
ditionally, the flexibility of the model also allows it
to attempt to produce fully grammatical sentences,
something not typically handled by word-to-word
normalization approaches.
Another potential problem with state-of-the-art
normalization is the lack of appropriate evaluation
metrics. The normalization task is most frequently
motivated by pointing to the need for clean text
for downstream processing applications, such as
syntactic parsing. However, most studies of nor-
malization give little insight into whether and to
what degree the normalization process improves
1159
the performance of the downstream application.
For instance, it is unclear how performance mea-
sured by the typical normalization evaluation met-
rics of word error rate and BLEU score (Pap-
ineni et al, 2002) translates into performance on
a parsing task, where a well placed punctuation
mark may provide more substantial improvements
than changing a non-standard word form. To ad-
dress this problem, this work introduces an eval-
uation metric that ties normalization performance
directly to the performance of a downstream de-
pendency parser.
The rest of this paper is organized as follows.
In Section 2 we discuss previous approaches to
the normalization problem. Section 3 presents
our normalization framework, including the actual
normalization and learning procedures. Our in-
stantiation of this model is presented in Section 4.
In Section 5 we introduce the parser driven eval-
uation metric, and present experimental results of
our model with respect to several baselines in three
different domains. Finally, we discuss our exper-
imental study in Section 6 and conclude in Sec-
tion 7.
2 Related Work
Sproat et al (2001) took the first major look at
the normalization problem, citing the need for nor-
malized text for downstream applications. Unlike
later works that would primarily focus on specific
noisy data sets, their work is notable for attempt-
ing to develop normalization as a general process
that could be applied to different domains. The re-
cent rise of heavily informal writing styles such as
Twitter and SMS messages set off a new round of
interest in the normalization problem.
Research on SMS and Twitter normalization has
been roughly categorized as drawing inspiration
from three other areas of NLP (Kobus et al, 2008):
machine translation, spell checking, and automatic
speech recognition. The statistical machine trans-
lation (SMT) metaphor was the first proposed to
handle the text normalization problem (Aw et al,
2006). In this mindset, normalizing SMS can be
seen as a translation task from a source language
(informal) to a target language (formal), which can
be undertaken with typical noisy channel based
models. Work by Choudhury et al (2007) adopted
the spell checking metaphor, casting the problem
in terms of character-level, rather than word-level,
edits. They proposed an HMM based model that
takes into account both grapheme and phoneme
information. Kobus et al (2008) undertook a
hybrid approach that pulls inspiration from both
the machine translation and speech recognition
metaphors.
Many other approaches have been examined,
most of which are at least partially reliant on
the above three metaphors. Cook and Steven-
son (2009) perform an unsupervised method,
again based on the noisy channel model. Pen-
nell and Liu (2011) developed a CRF tagger for
deletion-based abbreviation on tweets. Xue et
al. (2011) incorporated orthographic, phonetic,
contextual, and acronym expansion factors to nor-
malize words in both Twitter and SMS. Liu et
al. (2011) modeled the generation process from
dictionary words to non-standard tokens under an
unsupervised sequence labeling framework. Han
and Baldwin (2011) use a classifier to detect ill-
formed words, and then generate correction can-
didates based on morphophonemic similarity. Re-
cent work has looked at the construction of nor-
malization dictionaries (Han et al, 2012) and on
improving coverage by integrating different hu-
man perspectives (Liu et al, 2012).
Although it is almost universally used as a mo-
tivating factor, most normalization work does not
directly focus on improving downstream appli-
cations. While a few notable exceptions high-
light the need for normalization as part of text-
to-speech systems (Beaufort et al, 2010; Pennell
and Liu, 2010), these works do not give any di-
rect insight into how much the normalization pro-
cess actually improves the performance of these
systems. To our knowledge, the work presented
here is the first to clearly link the output of a nor-
malization system to the output of the downstream
application. Similarly, our work is the first to pri-
oritize domain adaptation during the new wave of
text message normalization.
3 Model
In this section we introduce our normalization
framework, which draws inspiration from our pre-
vious work on spelling correction for search (Bao
et al, 2011).
3.1 Replacement Generators
Our input the original, unnormalized text, repre-
sented as a sequence x = x1, x2, . . . , xn of tokens
xi. In this section we will use the following se-
1160
quence as our running example:
x = Ay1 woudent2 of3 see4 ?em5
where space replaces comma for readability, and
each token is subscripted by its position. Given the
input x, we apply a series of replacement genera-
tors, where a replacement generator is a function
that takes x as input and produces a collection of
replacements. Here, a replacement is a statement
of the form ?replace tokens xi, . . . , xj?1 with s.?
More precisely, a replacement is a triple ?i, j, s?,
where 1 ? i ? j ? n + 1 and s is a sequence of
tokens. Note that in the case where i = j, the se-
quence s should be inserted right before xi; and in
the special case where s is empty, we simply delete
xi, . . . , xj?1. For instance, in our running exam-
ple the replacement ?2, 3,would not? replaces
x2 = woudent with would not; ?1, 2,Ay? re-
places x1 with itself (hence, does not change x);
?1, 2, ? (where  is the empty sequence) deletes
x1; ?6, 6,.? inserts a period at the end of the se-
quence.
The provided replacement generators can be ei-
ther generic (cross domain) or domain-specific, al-
lowing for domain customization. In Section 4,
we discuss the replacement generators used in our
empirical study.
3.2 Normalization Graph
Given the input x and the set of replacements pro-
duced by our generators, we associate a unique
Boolean variable Xr with each replacement r. As
expected, Xr being true means that the replace-
ment r takes place in producing the output se-
quence.
Next, we introduce dependencies among vari-
ables. We first discuss the syntactic consistency
of truth assignments. Let r1 = ?i1, j1, s1? and
r2 = ?i2, j2, s2? be two replacements. We say
that r1 and r2 are locally consistent if the inter-
vals [i1, j1) and [i2, j2) are disjoint. Moreover,
we do not allow two insertions to take place at
the same position; therefore, we exclude [i1, j1)
and [i2, j2) from the definition of local consistency
when i1 = j1 = i2 = j2. If r1 and r2 are locally
consistent and j1 = i2, then we say that r2 is a
consistent follower of r1.
A truth assignment ? to our variables Xr is
sound if every two replacements r and r? with
?(Xr) = ?(Xr?) = true are locally consis-
tent. We say that ? is complete if every token
of x is captured by at least one replacement r
with ?(Xr) = true. Finally, we say that ?
is legal if it is sound and complete. The out-
put (normalized sequence) defined by a legal as-
signment ? is, naturally, the concatenation (from
left to right) of the strings s in the replacements
r = ?i, j, s? with ?(Xr) = true. In Fig-
ure 1, for example, if the nodes with a grey
shade are the ones associated with true vari-
ables under ?, then the output defined by ? is
I would not have seen them.
Our variables carry two types of interdependen-
cies. The first is that of syntactic consistency: the
entire assignment is required to be legal. The sec-
ond captures correlation among replacements. For
instance, if we replace of with have in our run-
ning example, then the next see token is more
likely to be replaced with seen. In this work,
dependencies of the second type are restricted to
pairs of variables, where each pair corresponds to
a replacement and a consistent follower thereof.
The above dependencies can be modeled over a
standard undirected graph using Conditional Ran-
dom Fields (Lafferty et al, 2001). However, the
graph would be complex: in order to model lo-
cal consistency, there should be edges between ev-
ery two nodes that violate local consistency. Such
a model renders inference and learning infeasi-
ble. Therefore, we propose a clearer model by a
directed graph, as illustrated in Figure 1 (where
nodes are represented by replacements r instead
of the variables Xr, for readability). To incorpo-
rate correlation among replacements, we introduce
an edge from Xr to Xr? whenever r? is a consis-
tent follower of r. Moreover, we introduce two
dummy nodes, start and end, with an edge from
start to each variable that corresponds to a prefix
of the input sequence x, and an edge from each
variable that corresponds to a suffix of x to end.
The principal advantage of modeling the depen-
dencies in such a directed graph is that now, the le-
gal assignments are in one-to-one correspondence
with the paths from start to end; this is a straight-
forward observation that we do not prove here.
We appeal to the log-linear model formulation
to define the probability of an assignment. The
conditional probability of an assignment ?, given
an input sequence x and the weight vector ? =
??1, . . . , ?k? for our features, is defined as p(? |
1161
?1, 2,I?
end
?2, 4,would not have?
?1, 2,Ay?
?5, 6,them?
?4, 5,seen?
?2, 3,would?
?4, 6,see him?
?3, 4,of?
start
?6, 6, .?
Figure 1: Example of a normalization graph; the
nodes are replacements generated by the replace-
ment generators, and every path from start to end
implies a legal assignment
x,?) = 0 if ? is not legal, and otherwise,
p(? | x,?) = 1Z(x)
?
X?Y ??
exp(
?
j
?j?j(X,Y,x)) .
Here, Z(x) is the partition function, X ? Y ? ?
refers to an edge X ? Y with ?(X) = true and
?(Y ) = true, and ?1(X,Y,x), . . . , ?k(X,Y,x)
are real valued feature functions that are weighted
by ?1, . . . , ?k (the model?s parameters), respec-
tively.
3.3 Inference
When performing inference, we wish to select
the output sequence with the highest probability,
given the input sequence x and the weight vector
? (i.e., MAP inference). Specifically, we want an
assignment ?? = arg max? p(? | x,?).
While exact inference is computationally hard
on general graph models, in our model it boils
down to finding the longest path in a weighted
and acyclic directed graph. Indeed, our directed
graph (illustrated in Figure 1) is acyclic. We as-
sign the real value ?j ?j?j(X,Y,x) to the edge
X ? Y , as the weight. As stated in Section 3.2,
a legal assignment ? corresponds to a path from
start to end; moreover, the sum of the weights on
that path is equal to log p(? | x,?) + logZ(x).
In particular, a longer path corresponds to an as-
signment with greater probability. Therefore, we
can solve the MAP inference within our model by
finding the weighted longest path in the directed
acyclic graph. The algorithm in Figure 2 summa-
rizes the inference procedure to normalize the in-
put sequence x.
Input:
1. A sequence x to normalize;
2. A weight vector ? = ??1, . . . , ?k?.
Generate replacements: Apply all replace-
ment generators to get a set of replacements r,
each r is a triple ?i, j, s?.
Build a normalization graph:
1. For each replacement r, create a node Xr.
2. For each r? and r, create an edge Xr to
Xr? if r? is a consistent follower of r.
3. Create two dummy nodes start and end,
and create edges from start to all prefix
nodes and end to all suffix nodes.
4. For each edge X ? Y , compute the fea-
tures ?j(X,Y,x), and weight the edge by?
j ?j?j(X,Y,x).
MAP Inference: Find a weighted longest path
P from start to end, and return ??, where
??(Xr) = true iff Xr ? P .
Figure 2: Normalization algorithm
3.4 Learning
Our labeled data consists of pairs (xi,ygoldi ),
where xi is an input sequence (to normalize) and
ygoldi is a (manually) normalized sequence. We
obtain a truth assignment ?goldi from each ygoldi
by selecting an assignment ? that minimizes the
edit distance between ygoldi and the normalized
text implied by ?:
?goldi = arg min? DIST(y(?),y
gold
i ) (1)
Here, y(?) denotes the normalized text implied by
?, and DIST is a token-level edit distance. We
apply a simple dynamic-programming algorithm
to compute ?goldi . Finally, the items in our training
data are the pairs (xi, ?goldi ).
Learning over similar models is commonly
done via maximum likelihood estimation:
L(?) = log
?
i
p(?i = ?goldi | xi,?)
Taking the partial derivative gives the following:
?
i
(
?j(?goldi ,xi)? Ep(?i|xi,?)?j(?i,xi)
)
where ?j(?,x) = ?X?Y ?j(X,Y,x), that is,
the sum of values for the jth feature along the
1162
Input:
1. A set {(xi,ygoldi )}
n
i=1 of sequences andtheir gold normalization;
2. Number T of iterations.
Initialization: Initialize each ?j as zero, and
obtain each ?goldi according to (1).
Repeat T times:
1. Infer each ??i from xi using the current ?;
2. ?j ? ?j+?i(?j(?goldi ,xi)??j(??i ,xi))
for all j = 1, . . . , k.
Output: ? = ??1, . . . , ?k?
Figure 3: Learning algorithm
path defined by ?, andEp(?i|xi,?)?j(?i,xi) is the
expected value of that sum (over all legal assign-
ments ?i), assuming the current weight vector.
How to efficiently compute
Ep(?i|xi,?)?j(?i,xi) in our model is un-
clear; naively, it requires enumerating all legal
assignments. We instead opt to use a more
tractable perceptron-style algorithm (Collins,
2002). Instead of computing the expectation,
we simply compute ?j(??i ,xi), where ??i is the
assignment with the highest probability, generated
using the current weight vector. The result is then:
?
i
(
?j(?goldi ,xi)? ?j(??i ,xi)
)
Our learning applies the following two steps it-
eratively. (1) Generate the most probable sequence
within the current weights. (2) Update the weights
by comparing the path generated in the previous
step to the gold standard path. The algorithm in
Figure 3 summarizes the procedure.
4 Instantiation
In this section, we discuss our instantiation of the
model presented in the previous section. In partic-
ular, we describe our replacement generators and
features.
4.1 Replacement Generators
One advantage of our proposed model is that
the reliance on replacement generators allows for
strong flexibility. Each generator can be seen as a
black box, allowing replacements that are created
heuristically, statistically, or by external tools to be
incorporated within the same framework.
Generator From To
leave intact good good
edit distance bac back
lowercase NEED need
capitalize it It
Google spell disspaear disappear
contraction wouldn?t would not
slang language ima I am going to
insert punctuation  .
duplicated punctuation !? !
delete filler lmao 
Table 1: Example replacement generators
To build a set of generic replacement generators
suitable for normalizing a variety of data types, we
collected a set of about 400 Twitter posts as devel-
opment data. Using that data, a series of gener-
ators were created; a sample of them are shown
in Table 1. As shown in the table, these gener-
ators cover a variety of normalization behavior,
from changing non-standard word forms to insert-
ing and deleting tokens.
4.2 Features
Although the proposed framework supports real
valued features, all features in our system are bi-
nary. In total, we used 70 features. Our feature set
pulls information from several different sources:
N-gram: Our n-gram features indicate the fre-
quency of the phrases induced by an edge. These
features are turned into binary ones by bucketing
their log values. For example, on the edge from
?1, 2,I? to ?2, 3,would? such a feature will indi-
cate whether the frequency of I would is over
a threshold. We use the Corpus of Contemporary
English (Davies, 2008 ) to produce our n-gram in-
formation.
Part-of-speech: Part-of-speech information
can be used to produce features that encourage
certain behavior, such as avoiding the deletion of
noun phrases. We generate part-of-speech infor-
mation over the original raw text using a Twit-
ter part-of-speech tagger (Ritter et al, 2011). Of
course, the part-of-speech information obtained
this way is likely to be noisy, and we expect our
learning algorithm to take that into account.
Positional: Information from positions is used
primarily to handle capitalization and punctuation
insertion, for example, by incorporating features
for capitalized words after stop punctuation or the
insertion of stop punctuation at the end of the sen-
tence.
Lineage: Finally, we include binary features
1163
that indicate which generator spawned the replace-
ment.
5 Evaluation
In this section, we present an empirical study of
our framework. The study is done over datasets
from three different domains. The goal is to eval-
uate the framework in two aspects: (1) usefulness
for downstream applications (specifically depen-
dency parsing), and (2) domain adaptability.
5.1 Evaluation Metrics
A few different metrics have been used to evaluate
normalizer performance, including word error rate
and BLEU score. While each metric has its pros
and cons, they all rely on word-to-word matching
and treat each word equally. In this work, we aim
to evaluate the performance of a normalizer based
on how it affects the performance of downstream
applications. We find that the conventional metrics
are not directly applicable, for several reasons. To
begin with, the assumption that words have equal
weights is unlikely to hold. Additionally, these
metrics tend to ignore other important non-word
information such as punctuation or capitalization.
They also cannot take into account other aspects
that may have an impact on downstream perfor-
mance, such as the word reordering as seen in the
example in Figure 4. Therefore, we propose a new
evaluation metric that directly equates normaliza-
tion performance with the performance of a com-
mon downstream application?dependency pars-
ing.
To realize our desired metric, we apply the fol-
lowing procedure. First, we produce gold standard
normalized data by manually normalizing sen-
tences to their full grammatically correct form. In
addition to the word-to-word mapping performed
in typical normalization gold standard generation,
this annotation procedure includes all actions nec-
essary to make the sentence grammatical, such as
word reordering, modifying capitalization, and re-
moving emoticons. We then run an off-the-shelf
dependency parser on the gold standard normal-
ized data to produce our gold standard parses. Al-
though the parser could still produce mistakes on
the grammatical sentences, we feel that this pro-
vides a realistic benchmark for comparison, as it
represents an upper bound on the possible perfor-
mance of the parser, and avoids an expensive sec-
ond round of manual annotation.
Test Gold SVO
I kinda wanna get
ipad NEW
I kind of want to
get a new iPad.
verb(get) verb(want)verb(get)
precisionv = 11
recallv = 12
subj(get,I)
subj(get,wanna)
obj(get,NEW)
subj(want,I)
subj(get,I)
obj(get,iPad)
precisionso = 13
recallso = 13
Figure 4: The subjects, verbs, and objects identi-
fied on example test/gold text, and corresponding
metric scores
To compare the parses produced over automati-
cally normalized data to the gold standard, we look
at the subjects, verbs, and objects (SVO) identi-
fied in each parse. The metric shown in Equa-
tions (2) and (3) below is based on the identified
subjects and objects in those parses. Note that SO
denotes the set of identified subjects and objects
whereas SOgold denotes the set of subjects and
objects identified when parsing the gold-standard
normalization.
precisionso =
|SO ? SOgold|
|SO | (2)
recallso = |SO ? SO
gold|
|SOgold|
(3)
We similarly define precisionv and recallv, where
we compare the set V of identified verbs to V gold
of those found in the gold-standard normalization.
An example is shown in Figure 4.
5.2 Results
To establish the extensibility of our normaliza-
tion system, we present results in three different
domains: Twitter posts, Short Message Service
(SMS) messages, and call-center logs. For Twitter
and SMS messages, we used established datasets
to compare with previous work. As no estab-
lished call-center log dataset exists, we collected
our own. In each case, we ran the proposed system
with two different configurations: one using only
the generic replacement generators presented in
Section 4 (denoted as generic), and one that adds
additional domain-specific generators for the cor-
responding domain (denoted as domain-specific).
All runs use ten-fold cross validation for training
and evaluation. The Stanford parser1 (Marneffe
et al, 2006) was used to produce all dependency
1Version 2.0.4, http://nlp.stanford.edu/
software/lex-parser.shtml
1164
parses. We compare our system to the following
baseline solutions:
w/oN: No normalization is performed.
Google: Output of the Google spell checker.
w2wN: The output of the word-to-word normal-
ization of Han and Baldwin (2011). Not available
for call-center data.
Gw2wN: The manual gold standard word-to-
word normalizations of previous work (Choud-
hury et al, 2007; Han and Baldwin, 2011). Not
available for call-center data.
Our results use the metrics of Section 5.1.
5.2.1 Twitter
To evaluate the performance on Twitter data, we
use the dataset of randomly sampled tweets pro-
duced by (Han and Baldwin, 2011). Because the
gold standard used in this work only provided
word mappings for out-of-vocabulary words and
did not enforce grammaticality, we reannotated the
gold standard data2. Their original gold standard
annotations were kept as a baseline.
To produce Twitter-specific generators, we ex-
amined the Twitter development data collected for
generic generator production (Section 4). These
generators focused on the Twitter-specific notions
of hashtags (#), ats (@), and retweets (RT). For
each case, we implemented generators that al-
lowed for either the initial symbol or the entire to-
ken to be deleted (e.g., @Hertz to Hertz, @Hertz
to ).
The results are given in Table 2. As shown,
the domain-specific generators yielded perfor-
mance significantly above the generic ones and all
baselines. Even without domain-specific genera-
tors, our system outperformed the word-to-word
normalization approaches. Most notably, both
the generic and domain-specific systems outper-
formed the gold standard word-to-word normal-
izations. These results validate the hypothesis that
simple word-to-word normalization is insufficient
if the goal of normalization is to improve depen-
dency parsing; even if a system could produce
perfect word-to-word normalization, it would pro-
duce lower quality parses than those produced by
our approach.
2Our results and the reannotations of the Twitter and SMS
data are available at https://www.cs.washington.
edu/node/9091/
System Verb Subject-ObjectPre Rec F1 Pre Rec F1
w/oN 83.7 68.1 75.1 31.7 38.6 34.8
Google 88.9 78.8 83.5 36.1 46.3 40.6
w2wN 87.5 81.5 84.4 44.5 58.9 50.7
Gw2w 89.8 83.8 86.7 46.9 61.0 53.0
generic 91.7 88.9 90.3 53.6 70.2 60.8
domain specific 95.3 88.7 91.9 72.5 76.3 74.4
Table 2: Performance on Twitter dataset
5.2.2 SMS
To evaluate the performance on SMS data, we use
the Treasure My Text data collected by Choud-
hury et al (2007). As with the Twitter data, the
word-to-word normalizations were reannotated to
enforce grammaticality. As a replacement genera-
tor for SMS-specific substitutions, we used a map-
ping dictionary of SMS abbreviations.3 No further
SMS-specific development data was needed.
Table 3 gives the results on the SMS data. The
SMS dataset proved to be more difficult than the
Twitter dataset, with the overall performance of
every system being lower. While this drop of per-
formance may be a reflection of the difference in
data styles between SMS and Twitter, it is also
likely a product of the collection methodology.
The collection methodology of the Treasure My
Text dataset dictated that every message must have
at least one mistake, which may have resulted in a
dataset that was noisier than average.
Nonetheless, the trends on SMS data mirror
those on Twitter data, with the domain-specific
generators achieving the greatest overall perfor-
mance. However, while the generic setting still
manages to outperform most baselines, it did not
outperform the gold word-to-word normalization.
In fact, the gold word-to-word normalization was
much more competitive on this data, outperform-
ing even the domain-specific system on verbs
alone. This should not be seen as surprising, as
word-to-word normalization is most likely to be
beneficial for cases like this where the proportion
of non-standard tokens is high.
It should be noted that the SMS dataset as avail-
able has had all punctuation removed. While this
may be appropriate for word-to-word normaliza-
tion, this preprocessing may have an effect on the
parse of the sentence. As our system has the abil-
ity to add punctuation but our baseline systems do
not, this has the potential to artificially inflate our
results. To ensure a fair comparison, we manually
3http://www.netlingo.com/acronyms.php
1165
System Verb Subject-ObjectRec Pre F1 Rec Pre F1
w/oN 76.4 48.1 59.0 19.5 21.5 20.4
Google 85.1 61.6 71.5 22.4 26.2 24.1
w2wN 78.5 61.5 68.9 29.9 36.0 32.6
Gw2wN 87.6 76.6 81.8 38.0 50.6 43.4
generic 86.5 77.4 81.7 35.5 47.7 40.7
domain specific 88.1 75.0 81.0 41.0 49.5 44.8
Table 3: Performance on SMS dataset
System Verb Subject-ObjectPre Rec F1 Pre Rec F1
w/oN 98.5 97.1 97.8 69.2 66.1 67.6
Google 99.2 97.9 98.5 70.5 67.3 68.8
generic 98.9 97.4 98.1 71.3 67.9 69.6
domain specific 99.2 97.4 98.3 87.9 83.1 85.4
Table 4: Performance on call-center dataset
added punctuation to a randomly selected small
subset of the SMS data and reran each system.
This experiment suggested that, in contrast to the
hypothesis, adding punctuation actually improved
the results of the proposed system more substan-
tially than that of the baseline systems.
5.2.3 Call-Center
Although Twitter and SMS data are unmistakably
different, there are many similarities between the
two, such as the frequent use of shorthand word
forms that omit letters. The examination of call-
center logs allows us to examine the ability of our
system to perform normalization in more disparate
domains. Our call-center data consists of text-
based responses to questions about a user?s expe-
rience with a call-center (e.g., their overall satis-
faction with the service). We use call-center logs
from a major company, and collect about 150 re-
sponses for use in our evaluation. We collected
an additional small set of data to develop our call-
center-specific generators.
Results on the call-center dataset are in Table 4.
As shown, the raw call-center data was compar-
atively clean, resulting in higher baseline perfor-
mance than in other domains. Unlike on previ-
ous datasets, the use of generic mappings only
provided a small improvement over the baseline.
However, the use of domain-specific generators
once again led to significantly increased perfor-
mance on subjects and objects.
6 Discussion
The results presented in the previous section sug-
gest that domain transfer using the proposed nor-
malization framework is possible with only a
small amount of effort. The relatively modest
set of additional replacement generators included
in each data set alowed the domain-specific ap-
proaches to significantly outperform the generic
approach. In the call-center case, performance im-
provements could be seen by referencing a very
small amount of development data. In the SMS
case, the presence of a domain-specific dictionary
allowed for performance improvements without
the need for any development data at all. It is
likely, though not established, that employing fur-
ther development data would result in further per-
formance improvements. We leave further investi-
gation to future work.
The results in Section 5.2 establish a point that
has often been assumed but, to the best of our
knowledge, has never been explicitly shown: per-
forming normalization is indeed beneficial to de-
pendency parsing on informal text. The parse of
the normalized text was substantially better than
the parse of the original raw text in all domains,
with absolute performance increases ranging from
about 18-25% on subjects and objects. Further-
more, the results suggest that, as hypothesized,
preparing an informal text for a parsing task re-
quires more than simple word-to-word normaliza-
tion. The proposed approach significantly outper-
forms the state-of-the-art word-to-word normal-
ization approach. Perhaps most interestingly, the
proposed approach performs on par with, and in
several cases superior to, gold standard word-to-
word annotations. This result gives strong evi-
dence for the conclusion that parser-targeted nor-
malization requires a broader understanding of the
scope of the normalization task.
While the work presented here gives promis-
ing results, there are still many behaviors found
in informal text that prove challenging. One
such example is the word reordering seen in Fig-
ure 4. Although word reordering could be incor-
porated into the model as a combination of a dele-
tion and an insertion, the model as currently de-
vised cannot easily link these two replacements
to one another. Additionally, instances of re-
ordering proved hard to detect in practice. As
such, no reordering-based replacement generators
were implemented in the presented system. An-
other case that proved difficult was the insertion
of missing tokens. For instance, the informal
sentence ?Day 3 still don?t freaking
1166
feel good!:(? could be formally rendered
as ?It is day 3 and I still do not
feel good!?. Attempts to address missing to-
kens in the model resulted in frequent false pos-
itives. Similarly, punctuation insertion proved to
be challenging, often requiring a deep analysis
of the sentence. For example, contrast the sen-
tence ?I?m watching a movie I don?t
know its name.? which would benefit from
inserted punctuation, with ?I?m watching a
movie I don?t know.?, which would not.
We feel that the work presented here provides a
foundation for future work to more closely exam-
ine these challenges.
7 Conclusions
This work presents a framework for normalization
with an eye towards domain adaptation. The pro-
posed framework builds a statistical model over a
series of replacement generators. By doing so, it
allows a designer to quickly adapt a generic model
to a new domain with the inclusion of a small set of
domain-specific generators. Tests over three dif-
ferent domains suggest that, using this model, only
a small amount of domain-specific data is neces-
sary to tailor an approach towards a new domain.
Additionally, this work introduces a parser-
centric view of normalization, in which the per-
formance of the normalizer is directly tied to the
performance of a downstream dependency parser.
This evaluation metric allows for a deeper under-
standing of how certain normalization actions im-
pact the output of the parser. Using this met-
ric, this work established that, when dependency
parsing is the goal, typical word-to-word normal-
ization approaches are insufficient. By taking a
broader look at the normalization task, the ap-
proach presented here is able to outperform not
only state-of-the-art word-to-word normalization
approaches but also manual word-to-word annota-
tions.
Although the work presented here established
that more than word-to-word normalization was
necessary to produce parser-ready normalizations,
it remains unclear which specific normalization
tasks are most critical to parser performance. We
leave this interesting area of examination to future
work.
Acknowledgments
We thank the anonymous reviewers of ACL for
helpful comments and suggestions. We also thank
Ioana R. Stanoi for her comments on a prelim-
inary version of this work, Daniel S. Weld for
his support, and Alan Ritter, Monojit Choudhury,
Bo Han, and Fei Liu for sharing their tools and
data. The first author is partially supported by the
DARPA Machine Reading Program under AFRL
prime contract numbers FA8750-09-C-0181 and
FA8750-09-C-0179. Any opinions, findings, con-
clusions, or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA, AFRL, or the
US government. This work is a part of IBM?s Sys-
temT project (Chiticariu et al, 2010).
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normal-
ization. In ACL, pages 33?40.
Zhuowei Bao, Benny Kimelfeld, and Yunyao Li. 2011.
A graph approach to spelling correction in domain-
centric search. In ACL, pages 905?914.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normal-
izing sms messages. In ACL, pages 770?779.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Sriram Raghavan, Frederick Reiss, and Shivaku-
mar Vaithyanathan. 2010. SystemT: An algebraic
approach to declarative information extraction. In
ACL, pages 128?137.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. IJDAR, 10(3-4):157?174.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1?8.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC, pages 71?78.
Mark Davies. 2008-. The corpus of contempo-
rary american english: 450 million words, 1990-
present. Avialable online at: http://corpus.
byu.edu/coca/.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In ACL, pages 368?378.
1167
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In EMNLP-CoNLL, pages 421?432.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing SMS: are two
metaphors better than one? In COLING, pages 441?
448.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML, pages 282?289.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor
supervision. In ACL, pages 71?76.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
broad-coverage normalization system for social me-
dia language. In ACL, pages 1035?1044.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Deana Pennell and Yang Liu. 2010. Normalization of
text messages for text-to-speech. In ICASSP, pages
4842?4845.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
SMS abbreviations. In IJCNLP, pages 974?982.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in Tweets: An ex-
perimental study. In EMNLP, pages 1524?1534.
Richard Sproat, Alan W. Black, Stanley F. Chen,
Shankar Kumar, Mari Ostendorf, and Christopher
Richards. 2001. Normalization of non-standard
words. Computer Speech & Language, 15(3):287?
333.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison.
2011. Normalizing microtext. In Analyzing Micro-
text, volume WS-11-05 of AAAI Workshops.
1168
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 804?809,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automatic Term Ambiguity Detection
Tyler Baldwin Yunyao Li Bogdan Alexe Ioana R. Stanoi
IBM Research - Almaden
650 Harry Road, San Jose, CA 95120, USA
{tbaldwi,yunyaoli,balexe,irs}@us.ibm.com
Abstract
While the resolution of term ambiguity is
important for information extraction (IE)
systems, the cost of resolving each in-
stance of an entity can be prohibitively
expensive on large datasets. To combat
this, this work looks at ambiguity detec-
tion at the term, rather than the instance,
level. By making a judgment about the
general ambiguity of a term, a system is
able to handle ambiguous and unambigu-
ous cases differently, improving through-
put and quality. To address the term
ambiguity detection problem, we employ
a model that combines data from lan-
guage models, ontologies, and topic mod-
eling. Results over a dataset of entities
from four product domains show that the
proposed approach achieves significantly
above baseline F-measure of 0.96.
1 Introduction
Many words, phrases, and referring expressions
are semantically ambiguous. This phenomenon,
commonly referred to as polysemy, represents a
problem for NLP applications, many of which in-
herently assume a single sense. It can be particu-
larly problematic for information extraction (IE),
as IE systems often wish to extract information
about only one sense of polysemous terms. If
nothing is done to account for this polysemy, fre-
quent mentions of unrelated senses can drastically
harm performance.
Several NLP tasks, such as word sense disam-
biguation, word sense induction, and named en-
tity disambiguation, address this ambiguity prob-
lem to varying degrees. While the goals and initial
data assumptions vary between these tasks, all of
them attempt to map an instance of a term seen
in context to an individual sense. While making
a judgment for every instance may be appropri-
ate for small or medium sized data sets, the cost
of applying these ambiguity resolution procedures
becomes prohibitively expensive on large data sets
of tens to hundreds of million items. To combat
this, this work zooms out to examine the ambigu-
ity problem at a more general level.
To do so, we define an IE-centered ambiguity
detection problem, which ties the notion of am-
biguity to a given topical domain. For instance,
given that the terms Call of Juarez and A New
Beginning can both reference video games, we
would like to discover that only the latter case is
likely to appear frequently in non-video game con-
texts. The goal is to make a binary decision as
to whether, given a term and a domain, we can
expect every instance of that term to reference an
entity in that domain. By doing so, we segregate
ambiguous terms from their unambiguous coun-
terparts. Using this segregation allows ambiguous
and unambiguous instances to be treated differ-
ently while saving the processing time that might
normally be spent attempting to disambiguate in-
dividual instances of unambiguous terms.
Previous approaches to handling word ambigu-
ity employ a variety of disparate methods, vari-
ously relying on structured ontologies, gleaming
insight from general word usage patterns via lan-
guage models, or clustering the contexts in which
words appear. This work employs an ambiguity
detection pipeline that draws inspiration from all
of these methods to achieve high performance.
2 Term Ambiguity Detection (TAD)
A term can be ambiguous in many ways. It may
have non-referential senses in which it shares a
name with a common word or phrase, such as in
the films Brave and 2012. A term may have refer-
ential senses across topical domains, such as The
Girl with the Dragon Tattoo, which may reference
either the book or the film adaptation. Terms may
804
also be ambiguous within a topical domain. For
instance, the term Final Fantasy may refer to the
video game franchise or one of several individual
games within the franchise. In this work we con-
cern ourselves with the first two types of ambigu-
ity, as within topical domain ambiguity tends to
pose a less severe problem for IE systems.
IE systems are often asked to perform extrac-
tion over a dictionary of terms centered around
a single topic. For example, in brand manage-
ment, customers may give a list of product names
and ask for sentiment about each product. With
this use case in mind, we define the term ambigu-
ity detection (TAD) problem as follows: Given a
term and a corresponding topic domain, determine
whether the term uniquely references a member
of that topic domain. That is, given a term such
as Brave and a category such as film, the task is
make a binary decision as to whether all instances
of Brave reference a film by that name.
2.1 Framework
Our TAD framework is a hybrid approach consist-
ing of three modules (Figure 1). The first module
is primarily designed to detect non-referential am-
biguity. This module examines n-gram data from
a large text collection. Data from The Corpus of
Contemporary American English (Davies, 2008 )
was used to build our n-grams.
The rationale behind the n-gram module is
based on the understanding that terms appearing
in non-named entity contexts are likely to be non-
referential, and terms that can be non-referential
are ambiguous. Therefore, detecting terms that
have non-referential usages can also be used to
detect ambiguity. Since we wish for the ambigu-
ity detection determination to be fast, we develop
our method to make this judgment solely on the
n-gram probability, without the need to examine
each individual usage context. To do so, we as-
sume that an all lowercased version of the term is
a reasonable proxy for non-named entity usages in
formal text. After removing stopwords from the
term, we calculate the n-gram probability of the
lower-cased form of the remaining words. If the
probability is above a certain threshold, the term
is labeled as ambiguous. If the term is below the
threshold, it is tentatively labeled as unambiguous
and passed to the next module. To avoid making
judgments of ambiguity based on very infrequent
uses, the ambiguous-unambiguous determination
threshold is empirically determined by minimiz-
ing error over held out data.
The second module employs ontologies to de-
tect across domain ambiguity. Two ontologies
were examined. To further handle the common
phrase case, Wiktionary1 was used as a dictionary.
Terms that have multiple senses in Wiktionary
were labeled as ambiguous. The second ontology
used was Wikipedia disambiguation pages. All
terms that had a disambiguation page were marked
as ambiguous.
The final module attempts to detect both non-
referential and across domain ambiguity by clus-
tering the contexts in which words appear. To do
so, we utilized the popular Latent Dirichlet Allo-
cation (LDA (Blei et al, 2003)) topic modeling
method. LDA represents a document as a distri-
bution of topics, and each topic as a distribution
of words. As our domain of interest is Twitter,
we performed clustering over a large collection of
tweets. For a given term, all tweets that contained
the term were used as a document collection. Fol-
lowing standard procedure, stopwords and infre-
quent words were removed before topic modeling
was performed. Since the clustering mechanism
was designed to make predictions over the already
filtered data of the other modules, it adopts a con-
servative approach to predicting ambiguity. If the
category term (e.g., film) or a synonym from the
WordNet synset does not appear in the 10 most
heavily weighted words for any cluster, the term is
marked as ambiguous.
A term is labeled as ambiguous if any one of
the three modules predicts that it is ambiguous,
but only labeled as unambiguous if all three mod-
ules make this prediction. This design allows each
module to be relatively conservative in predicting
ambiguity, keeping precision of ambiguity predic-
tion high, under the assumption that other modules
will compensate for the corresponding drop in re-
call.
3 Experimental Evaluation
3.1 Data Set
Initial Term Sets We collected a data set of terms
from four topical domains: books, films, video
games, and cameras. Terms for the first three do-
mains are lists of books, films, and video games
respectively from the years 2000-2011 from db-
pedia (Auer et al, 2007), while the initial terms
1http://www.wiktionary.org/
805
Tweet Term Category Judgment
Woke up from a nap to find a beautiful mind on. #win A Beautiful Mind film yes
I Love Tyler Perry ; He Has A Beautiful Mind. A Beautiful Mind film no
I might put it in the top 1. RT @CourtesyFlushMo Splice. Top 5 worst movies ever Splice film yes
Splice is a great, free replacement to iMove for your iPhone, Splice film no
Table 1: Example tweet annotations.
Figure 1: Overview of the ambiguity detection
framework.
for cameras includes all the cameras from the six
most popular brands on flickr2.
Gold Standard A set of 100 terms per domain
were chosen at random from the initial term sets.
Rather than annotating each term directly, am-
biguity was determined by examining actual us-
age. Specifically, for each term, usage examples
were extracted from large amounts of Twitter data.
Tweets for the video game and film categories were
extracted from the TREC Twitter corpus.3 The
less common book and camera cases were ex-
tracted from a subset of all tweets from September
1st-9th, 2012.
For each term, two annotators were given the
term, the corresponding topic domain, and 10 ran-
domly selected tweets containing the term. They
were then asked to make a binary judgment as to
whether the usage of the term in the tweet referred
to an instance of the given category. The degree
of ambiguity is then determined by calculating the
percentage of tweets that did not reference a mem-
ber of the topic domain. Some example judgments
are given in Table 1. If all individual tweet judg-
ments for a term were marked as referring to a
2http://www.flickr.com/cameras/
3http://trec.nist.gov/data/tweets/
Configuration Precision Recall F-measure
Baseline 0.675 1.0 0.806
NG 0.979 0.848 0.909
ON 0.979 0.704 0.819
CL 0.946 0.848 0.895
NG + ON 0.980 0.919 0.948
NG + CL 0.942 0.963 0.952
ON + CL 0.945 0.956 0.950
NG + ON + CL 0.943 0.978 0.960
Table 2: Performance of various framework con-
figurations on the test data.
member of the topic domain, the term was marked
as fully unambiguous within the data examined.
All other cases were considered ambiguous.4
Inter-annotator agreement was high, with raw
agreement of 94% (? = 0.81). Most disagree-
ments on individual tweet judgments had little ef-
fect on the final judgment of a term as ambiguous
or unambiguous, and those that did were resolved
internally.
3.2 Evaluation and Results
Effectiveness To understand the contribution of
the n-gram (NG), ontology (ON), and clustering
(CL) based modules, we ran each separately, as
well as every possible combination. Results are
shown in Table 2, where they are compared to a
majority class (ambiguous) baseline.
As shown, all configurations outperform the
baseline. Of the three individual modules, the n-
gram and clustering methods achieve F-measure
of around 0.9, while the ontology-based module
performs only modestly above baseline. Unsur-
prisingly, the ontology method is affected heav-
ily by its coverage, so its poor performance is pri-
marily attributable to low recall. As noted, many
IE tasks may involve sets of entities that are not
found in common ontologies, limiting the ability
of the ontology-based method alone. Additionally,
ontologies may be apt to list cases of strict ambi-
guity, rather than practical ambiguity. That is, an
ontology may list a term as ambiguous if there are
4The annotated data is available at http:
//researcher.watson.ibm.com/researcher/
view_person_subpage.php?id=4757.
806
several potential named entities it could refer to,
even if the vast majority of references were to only
a single entity.
Combining any two methods produced substan-
tial performance increases over any of the individ-
ual runs. The final system that employed all mod-
ules produced an F-measure of 0.960, a significant
(p < 0.01) absolute increase of 15.4% over the
baseline.
Usefulness To establish that term ambiguity de-
tection is actually helpful for IE, we conducted
a preliminary study by integrating our pipeline
into a commercially available rule-based IE sys-
tem (Chiticariu et al, 2010; Alexe et al, 2012).
The system takes a list of product names as input
and outputs tweets associated with each product.
It utilizes rules that employ more conservative ex-
traction for ambiguous entities.
Experiments were conducted over several mil-
lion tweets using the terms from the video game
and camera domains. When no ambiguity detec-
tion was performed, all terms were treated as un-
ambiguous. The system produced very poor pre-
cision of 0.16 when no ambiguity detection was
used, due to the extraction of irrelevant instances
of ambiguous objects. In contrast, the system pro-
duced precision of 0.96 when ambiguity detection
was employed. However, the inclusion of disam-
biguation did reduce the overall recall; the system
that employed disambiguation returned only about
57% of the true positives returned by the system
that did not employ disambiguation. Although
this reduction in recall is significant, the overall
impact of disambiguation is clearly positive, due
to the stark difference in precision. Nonetheless,
this limited study suggests that there is substantial
room for improvement in the extraction system, al-
though this is out of the scope of the current work.
4 Related Work
Polysemy is a known problem for many NLP-
related applications. Machine translation systems
can suffer, as ambiguity in the source language
may lead to incorrect translations, and unambigu-
ous sentences in one language may become am-
biguous in another (Carpuat and Wu, 2007; Chan
et al, 2007). Ambiguity in queries can also hin-
der the performance of information retrieval sys-
tems (Wang and Agichtein, 2010; Zhong and Ng,
2012).
The ambiguity detection problem is similar to
the well studied problems of named entity dis-
ambiguation (NED) and word sense disambigua-
tion (WSD). However, these tasks assume that
the number of senses a word has is given, essen-
tially assuming that the ambiguity detection prob-
lem has already been solved. This makes these
tasks inapplicable in many IE instances where the
amount of ambiguity is not known ahead of time.
Both named entity and word sense disambigua-
tion are extensively studied, and surveys on each
are available (Nadeau and Sekine, 2007; Navigli,
2009).
Another task that shares similarities with TAD
is word sense induction (WSI). Like NED and
WSD, WSI frames the ambiguity problem as one
of determining the sense of each individual in-
stance, rather than the term as a whole. Unlike
those approaches, the word sense induction task
attempts to both figure out the number of senses a
word has, and what they are. WSI is unsupervised,
relying solely on the information that surrounds
word mentions in the text.
Many different clustering-based WSI methods
have been examined. Pantel and Lin (2002) em-
ploy a clustering by committee method that itera-
tively adds words to clusters based on their sim-
ilarities. Topic model-based methods have been
attempted using variations of Latent Dirichlet Al-
location (Brody and Lapata, 2009) and Hierarchi-
cal Dirichlet Processes (Lau et al, 2012). Sev-
eral graph-based methods have also been exam-
ined (Klapaftis and Manandhar, 2010; Navigli and
Crisafulli, 2010). Although the words that sur-
round the target word are the primary source of
contextual information in most cases, additional
feature sources such as syntax (Van de Cruys,
2008) and semantic relations (Chen and Palmer,
2004) have also been explored.
5 Conclusion
This paper introduced the term ambiguity detec-
tion task, which detects whether a term is am-
biguous relative to a topical domain. Unlike other
ambiguity resolution tasks, the ambiguity detec-
tion problem makes general ambiguity judgments
about terms, rather than resolving individual in-
stances. By doing so, it eliminates the need for
ambiguity resolution on unambiguous objects, al-
lowing for increased throughput of IE systems on
large data sets.
Our solution for the term ambiguity detection
807
task is based on a combined model with three dis-
tinct modules based on n-grams, ontologies, and
clustering. Our initial study suggests that the com-
bination of different modules designed for differ-
ent types of ambiguity used in our solution is ef-
fective in determining whether a term is ambigu-
ous for a given domain. Additionally, an exami-
nation of a typical use case confirms that the pro-
posed solution is likely to be useful in improving
the performance of an IE system that does not em-
ploy any disambiguation.
Although the task as presented here was mo-
tivated with information extraction in mind, it is
possible that term ambiguity detection could be
useful for other tasks. For instance, TAD could
be used to aid word sense induction more gener-
ally, or could be applied as part of other tasks such
as coreference resolution. We leave this avenue of
examination to future work.
Acknowledgments
We would like to thank the anonymous review-
ers of ACL for helpful comments and suggestions.
We also thank Howard Ho and Rajasekar Krishna-
murthy for help with data annotation and Shivaku-
mar Vaithyanathan for his comments on a prelim-
inary version of this work.
References
Bogdan Alexe, Mauricio A. Herna?ndez, Kirsten Hil-
drum, Rajasekar Krishnamurthy, Georgia Koutrika,
Meenakshi Nagarajan, Haggai Roitman, Michal
Shmueli-Scheuer, Ioana Roxana Stanoi, Chitra
Venkatramani, and Rohit Wagle. 2012. Surfacing
time-critical insights from social media. In SIG-
MOD Conference, pages 657?660.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: a nucleus for a web of open data.
In Proceedings of the 6th international The seman-
tic web and 2nd Asian conference on Asian semantic
web conference, ISWC?07/ASWC?07, pages 722?
735, Berlin, Heidelberg. Springer-Verlag.
David Blei, Andrew Ng, and Micheal I. Jordan. 2003.
Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, January.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL ?09,
pages 103?111, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 33?40, Prague, Czech Republic,
June. Association for Computational Linguistics.
Jinying Chen and Martha Palmer. 2004. Chinese verb
sense discrimination using an em clustering model
with rich linguistic features. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Sriram Raghavan, Frederick Reiss, and Shivaku-
mar Vaithyanathan. 2010. SystemT: An algebraic
approach to declarative information extraction. In
ACL, pages 128?137.
Mark Davies. 2008-. The corpus of contempo-
rary american english: 450 million words, 1990-
present. Avialable online at: http://corpus.
byu.edu/coca/.
Ioannis P. Klapaftis and Suresh Manandhar. 2010.
Word sense induction & disambiguation using hi-
erarchical random graphs. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 745?755,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
?12, pages 591?601, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
David Nadeau and Satoshi Sekine. 2007. A survey
of named entity recognition and classification. Lin-
guisticae Investigationes, 30(1):3?26.
Roberto Navigli and Giuseppe Crisafulli. 2010. Induc-
ing word senses to improve web search result clus-
tering. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 116?126, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):10:1?10:69,
February.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
808
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?02, pages
613?619, New York, NY, USA. ACM.
Tim Van de Cruys. 2008. Using three way data for
word sense discrimination. In Proceedings of the
22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 929?
936, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Yu Wang and Eugene Agichtein. 2010. Query ambigu-
ity revisited: Clickthrough measures for distinguish-
ing informational and ambiguous queries. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 361?364,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Zhi Zhong and Hwee Tou Ng. 2012. Word sense
disambiguation improves information retrieval. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 273?282, Jeju Island, Korea,
July. Association for Computational Linguistics.
809
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 306?313,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Hand Gestures in Disambiguating Types of You Expressions in Multiparty
Meetings
Tyler Baldwin
Department of Computer
Science and Engineering
Michigan State University
East Lansing, MI 48824
baldwin96@cse.msu.edu
Joyce Y. Chai
Department of Computer
Science and Engineering
Michigan State University
East Lansing, MI 48824
jchai@cse.msu.edu
Katrin Kirchhoff
Department of Electrical
Engineering
University of Washington
Seattle, WA, USA
katrin@ee.washington.edu
Abstract
The second person pronoun you serves dif-
ferent functions in English. Each of these
different types often corresponds to a dif-
ferent term when translated into another
language. Correctly identifying different
types of you can be beneficial to machine
translation systems. To address this is-
sue, we investigate disambiguation of dif-
ferent types of you occurrences in multi-
party meetings with a new focus on the
role of hand gesture. Our empirical re-
sults have shown that incorporation of ges-
ture improves performance on differentiat-
ing between the generic use of you (e.g.,
refer to people in general) and the referen-
tial use of you (e.g., refer to a specific per-
son or a group of people). Incorporation
of gesture can also compensate for limi-
tations in automated language processing
(e.g., reliable recognition of dialogue acts)
and achieve comparable results.
1 Introduction
The second person pronoun you is one of the most
prevalent words in conversation and it serves sev-
eral different functions (Meyers, 1990). For ex-
ample, it can be used to refer to a single addressee
(i.e., the singular case) or multiple addressees (i.e.,
the plural case). It can also be used to represent
people in general (i.e., the generic case) or be used
idiomatically in the phrase ?you know?.
For machine translation systems, these differ-
ent types of you often correspond to different
translations in another language. For example,
in German, there are different second-person pro-
nouns for singular vs. plural you (viz. du vs. ihr);
in addition there are different forms for formal
vs. informal forms of address (du vs. Sie) and for
the generic use (man). The following examples
demonstrate different translations of you from En-
glish (EN) into German (DE):
? Generic you
EN: Sometimes you have meetings where the
decision is already taken.
DE: Manchmal hat man Meetings wo die
Entscheidung schon gefallen ist.
? Singular you:
EN: Do you want an extra piece of paper?
DE: Mo?chtest du noch ein Blatt Papier?
? Plural you:
EN: Hope you are all happy!
DE: Ich hoffe, ihr seid alle zufrieden!
These examples show that correctly identifying
different types of You plays an important role in
the correct translation of you in different context.
To address this issue, this paper investigates the
role of hand gestures in disambiguating different
usages of you in multiparty meetings. Although
identification of you type has been investigated
before in the context of addressee identification
(Gupta et al, 2007b; Gupta et al, 2007a; Framp-
ton et al, 2009; Purver et al, 2009), our work
here focuses on two new angles. First, because of
our different application on machine translation,
rather than processing you at an utterance level to
identify addressee, our work here concerns each
occurrence of you within each utterance. Second
and more importantly, our work investigates the
role of corresponding hand gestures in the disam-
biguation process. This aspect has not been exam-
ined in previous work.
When several speakers are conversing in a sit-
uated environment, they often overtly gesture at
one another to help manage turn order or explic-
itly direct a statement toward a particular partici-
pant (McNeill, 1992). For example, consider the
following snippet from a multiparty meeting:
A: ?Why is that??
B: ?Because, um, based on what ev-
306
erybody?s saying, right, [gestures at
Speaker D] you want something sim-
ple. You [gestures at Speaker C]want
basic stuff and [gestures at Speaker A]
you want something that is easy to use.
Speech recognition might not be the
simplest thing.?
The use of gesture in this example indicates that
each instance of the pronoun you is intended to
be referential, and gives some indication of the in-
dented addressee. Without the aid of gesture, it
would be difficult even for a human listener to be
able to interpret each instance correctly.
Therefore, we conducted an empirical study on
several meeting segments from the AMI meeting
corpus. We formulated our problem as a classifica-
tion problem for each occurrence of you, whether
it is a generic, singular, or plural type. We com-
bined gesture features with several linguistic and
discourse features identified by previous work and
evaluated the role of gesture in two different set-
tings: (1) a two stage classification that first dif-
ferentiates the generic type from the referential
type and then within the referential type distin-
guishes singular and plural usages; (2) a three way
classification between generic, singular, or plural
types. Our empirical results have shown that in-
corporation of gesture improves performance on
differentiating between the generic and the refer-
ential type. Incorporation of gesture can also com-
pensate for limitations in automated language pro-
cessing (e.g., reliable recognition of dialogue acts)
and achieve comparable results. These findings
have important implications for machine transla-
tion of you expressions from multiparty meetings.
2 Related Work
Psychological research on gesture usage in
human-human dialogues has shown that speakers
gesture for a variety of reasons. While speakers of-
ten gesture to highlight objects related to the core
conversation topic (Kendon, 1980), they also ges-
ture for dialogue management purposes (Bavelas
et al, 1995). While not all of the gestures pro-
duced relate directly to the resolution of the word
you, many of them give insight into which partici-
pant is being addressed, which has a close correla-
tion with you resolution. Our investigation here is
closely related to two areas of previous work: ad-
dressee identification based on you and the use of
gestures in coreference resolution.
Addressee Identification. Disambiguation of
you type in the context of addressee identifica-
tion has been examined in several papers in re-
cent years. Gupta et. al. (2007b) examined
two-party dialogues from the Switchboard corpus.
They modeled the problem as a binary classifi-
cation problem of differentiating between generic
and referential usages (referential usages include
the singular and plural types). This work has iden-
tified several important linguistic and discourse
features for this task (which was used and ex-
tended in later work and our work here). Later
work by the same group (Gupta et al, 2007a) ex-
amined the same problem on multiparty dialogue
data. They made adjustments to their previous
methods by removing some oracle features from
annotation and applying simpler and more realis-
tic features. A recent work (Frampton et al, 2009)
has examined both the generic vs. referential and
singular vs. plural classification tasks. A main
difference is that this work incorporated gaze fea-
ture information in both classification tasks (gaze
features are commonly used in addressee identi-
fication). More recent work (Purver et al, 2009)
discovered that large gains in performance can
be achieved by including n-gram based features.
However, they found that many of the most im-
portant n-gram features were topic specific, and
thus required training data consisting of meetings
about the same topic.
Gestures in Coreference Resolution. Eisen-
stein and Davis (2006; 2007) examined corefer-
ence resolution on a corpus of speaker-listener
pairs in which the speaker had to describe the
workings of a mechanical device to the listener,
with the help of visual aids. In this gesture heavy
dataset, they found gesture data to be helpful in re-
solving references. In our previous work (2009),
we examined gestures for the identification of
coreference on multparty meeting data. We found
that gestures only provided limited help in the
coreference identification task. Given the nature
of the meetings under investigation, although ges-
tures have not been shown to be effective in gen-
eral, they are potentially helpful in recognizing
whether two linguistic expressions refer to a same
participant.
Compared to these two areas of earlier work,
our investigation here has two unique aspects.
First, as mentioned earlier, previous work on ad-
dressee identification focused the problem at the
307
utterance level. Because the goal was to find the
addressee of an utterance, the assumption was that
all instances of you in an utterance were of the
same type. However, since several instances of
you in the same utterance may translate differently,
we instead examine the classification task at the
instance level. Second, our work here specifically
investigates the role of gestures in disambiguation
of different types of you. This aspect has not been
examined in previous work.
3 Data
The dataset used in our investigation was the
AMI meeting corpus (Popescu-Belis and Estrella,
2007), the same corpus used in previous work
(Gupta et al, 2007a; Frampton et al, 2009; Purver
et al, 2009; Baldwin et al, 2009). The AMI meet-
ing corpus is a large publicly available corpus of
multiparty design meetings. AMI meeting anno-
tations contain manual speech transcriptions, as
well as annotations of several additional modali-
ties, such as focus of attention and head and hand
gesture.
For this work, six AMI meeting segments
(IS1008a, IS1008b, IS1008c, IS1008d, ES2008a,
TS3005a) were used. These instances were cho-
sen because they contained manual annotations of
hand gesture data, which was not available for all
AMI meeting segments. These six meeting seg-
ments were from AMI ?scenario? meetings, in
which meeting participants had a specific task of
designing a hypothetical remote control.
All instances of the word you and its variants
were manually annotated as either generic, singu-
lar, or plural. This produced a small dataset of 533
instances. Agreement between two human anno-
tators was high (? = 0.9). The distribution of you
types is shown in Figure 1. The most prevalent
type in our data set was the generic type, which
accounted for 47% of all instances of you present.
Of the two referential types, the singular type ac-
counted for about 60% of the referential instances.
A total of 508 gestures are present in our data
set. Table 1 shows the distribution of gestures.
As shown, ?non-communicative gestures?, make
up nearly half (46%) of the gestures produced.
These are gestures that are produced without an
overt communicative intent, such as idly tapping
on the table. The other main categorization of
gestures is ?communicative gestures?, which ac-
counts for 45% of all gestures produced and is
made up of the ?pointing at participants?, ?point-
ing at objects?, ?interact with object?, and ?other
communicative? gesture types from Table 1. A to-
tal of 17% of the gestures produced were pointing
gestures that pointed to people, a type of gesture
that would likely be helpful for you type identifica-
tion. A small percentage of the gestures produced
were not recorded by the meeting recording cam-
eras (i.e., off camera), and thus are of unknown
type.
4 Methodology
Our general methodology followed previous work
and formulated this problem as a classification
problem. We evaluated how gesture data may
help you type identification using two different ap-
proaches: (1) two stage binary classification, and
(2) a single three class classification problem. In
two stage binary classification, we first attempt
to distinguish between instances of you that are
generic and those that are referential. We then take
those cases that are referential and attempt to sub-
divide them into instances that are intended to re-
fer to a single person and those that refer to several
people.
Our feature set includes features used by Gupta
et. al. (2007a) (Hereafter referred to as Gupta) and
Frampton et. al. (2009) (Hereafter Frampton), as
well as new features incorporating gestures. We
summarize these features as follows.
Sentential Features. We used several senten-
tial features to capture important phrase patterns.
Most of our sentential features were drawn from
Gupta (2007a). These features captured the pat-
terns ?you guys?, ?you know?, ?do you? (and sim-
ilar variants), ?which you? (and variants), ?if you?,
and ?you hear? (and variants). Another sentential
feature captured the number of times the word you
appeared in the sentence. Additionally, other fea-
tures captured sentence patterns not related to you,
such as the presence of the words ?I? and ?we?.
A few other sentential features were drawn from
Frampton et. al. (2009). These include the pattern
?<auxiliary> you? (a more general version of the
?do you? feature) and a count of the number of
total words in the utterances.
Part-of-Speech Features. Several features
based on automatic part-of-speech tagging of the
sentence containing you were used. Quality of au-
tomatic tagging was not assessed. From the tagged
results, we extracted 5 features based on sentence
308
Distrib
ution 
of You
 Types
050100150200250300
Generi
c
Singula
r
Plural
Type
Count
(a) Distribution of You types
Gestu
re Dis
tributi
on
050100150200250 Non- Comm
unicative Po
inting at Pa
rticipant
s Pointi
ng at Objec
ts Other Co
mmunica
tive Inte
ract with O
bject
Off_came
ra
Type
Count
(b) Distribution of gesture types
Figure 1: Data distributions
and tag patterns: whether or not the sentence that
contained you also contained I, or we followed by
a verb tag (3 separate features), and whether or
not the sentence contains a comparative JJR (ad-
jective) tag. All of these features were adapted
from Gupta (2007a).
Dialog Act Features. We used the manually an-
notated dialogue act tags provided by the AMI cor-
pus to produce our dialogue act features. Three di-
alogue act features were used: the dialogue act tag
of the current sentence, the previous sentence, and
the sentence prior to that. Dialog act tags were in-
corporated into the feature set in one of two differ-
ent ways: 1) using the full tag set provided by the
AMI corpus, and 2) using a binary feature record-
ing if the dialogue act tag was of the elicit type.
The latter way of dialogue act incorporation rep-
resents a simpler and more realistic treatment of
dialogue acts.
Question Mark Feature. The question mark
feature captures whether or not the current sen-
tence ends in a question mark. This feature cap-
tures similar information to the elicit dialogue act
tag and was used in Gupta as an automatically ex-
tractable replacement to the manually extracted di-
alogue act tags (2007a).
Backward Looking/Forward Looking Fea-
tures. Several features adapted from Frampton et.
al. (2009) used information about previous and
next sentences and speakers. These features con-
nected the current utterance with previous utter-
ances by the other participants in the room. For
each listener, a feature was recorded that indicated
how many sentences elapsed between the current
sentence and the last/next time the person spoke.
Additionally, two features captured the number of
speakers in the previous and next five sentences.
Gesture Features. Several different features
were used to capture gesture information. Three
types of gesture data were considered: all pro-
duced gestures, only those gestures that were
manually annotated as being communicative, and
only those gestures that were manually annotated
as pointing towards another meeting participant.
For each of these types, one gesture feature cap-
tures the total number of gestures that co-occur
with the current sentence, while another feature
records only whether or not a gesture co-occurs
with the utterance of you. Since previous work
(Kendon, 1980) has indicated that gesture produc-
tion tends to precede the onset of the expression,
gestures were considered to have co-occurred with
instances if they directly overlapped with them or
preceded them by a short window of 2.5 seconds.
Note that in this investigation, we used anno-
tated gestures provided by the AMI corpus. Al-
though automated extraction of reliable gesture
features can be challenging and should be pursued
in the future, the use of manual annotation allows
us to focus on our current goal, which is to under-
stand whether and to what degree hand gestures
may help disambiguation of you Type.
It is also important to note that although previ-
ous work (Purver et al, 2009) showed that n-gram
features produced large performance gains, these
features were heavily topic dependent. The AMI
meeting corpus provides several meetings on ex-
actly the same topic, which allowed the n-gram
features to learn topic-specific words such as but-
ton, channel, and volume. However, as real world
309
Accuracy
Majority Class Baseline 53.3%
Gupta automatic 70.7%
Gupta manual 74.7%
Gupta + Frampton automatic 73.2%
Gupta + Frampton manual 74.3%
All (+ gesture) 79.0%
Table 1: Accuracy values for Generic vs. Referen-
tial Classification
meetings occur with a wider range of goals and
topics, we would like to build a topic and domain
independent model that does not require a corpus
of topic specific training data. As such, we have
excluded n-gram features from our study.
Additionally, we have not implemented gaze
features. Although previous work (Frampton et
al., 2009) showed that these features were able to
improve performance, we decided to focus solely
on gesture to the exclusion of other non-speech
modalities. However, we are currently in the pro-
cess of evaluating the overlap between gesture and
gaze feature coverage.
5 Results
Due to the small number of meeting segments in
our data, leave-one-out cross validation was pre-
formed for evaluation. Since a primary focus of
this paper is to understand whether and to what
degree gesture is able to aid in the you type iden-
tification task, experiments were run using a deci-
sion tree classifier due to its simplicity and trans-
parency 1.
5.1 Two Stage Classification
We first evaluated the role of gesture via two stage
binary classification. That is, we performed two
binary classification tasks, first differentiating be-
tween generic and referential instances, and then
further dividing the referential instances into the
singular and plural types. This provides a more
detailed analysis of where gesture may be helpful.
Results for the generic vs. referential and singu-
lar vs. plural binary classification tasks are shown
in Table 1 and Table 2, respectively. Tables 1
and 2 present several different configurations. The
1In order to get a more direct comparison to previous work
(Gupta et al, 2007a; Frampton et al, 2009), we also experi-
mented with classification via a bayesian network. We found
that the overall results were comparable to those obtained
with the decision tree.
Accuracy
Majority Class Baseline 59.5%
Gupta automatic 72.2%
Gupta manual 73.6%
Gupta + Frampton automatic 73.2%
Gupta + Frampton manual 72.5%
All (+ gesture) 74.6%
Table 2: Accuracy values for Singular vs. Plural
Classification
?Gupta? feature configurations consist of all fea-
tures used by Gupta et. al. (2007a). These in-
clude all part-of-speech features, all dialogue act
features, the question mark feature, and all sen-
tential features except the ?<auxiliary> you? fea-
ture and the word count feature. Results from two
types of processing are presented: automatic and
manual.
? Automatic feature extraction (automatic) -
The automatic configurations consist of only
features that were automatically extracted
from the text. This includes all of the features
we examined except for the dialogue act and
gesture features. These features are extracted
from meeting transcriptions.
? Manual feature extraction (manual) - Manual
configurations apply manual annotations of
dialogue acts and gestures together with the
automatically extracted features.
The Frampton configurations add the addi-
tional sentential features as well as the backward-
looking and forward-looking features. As before,
results are presented for a manual and an auto-
matic run. The final configuration (?All?) includes
the entire feature set with the addition of gesture
features. The All configuration is the only config-
uration that includes gesture features.
Although they are not directly comparable, the
results for generic vs. referential classification
shown in Table 1 appear consistent with those re-
ported by Gupta (2007a). Adding additional fea-
tures from Frampton et. al. did not produce an
overall increase in performance when dialogue act
features were present. Including gesture features
leads to a significant increase in performance (Mc-
Nemar Test, p < 0.01), an absolute increase of
4.3% over the best performing feature set that does
not include gesture. This result seems to confirm
our hypothesis that, because gestures are likely
310
Accuracy
Majority Class Baseline 46.7%
Gupta automatic 61.5%
Gupta manual 66.2%
Gupta + Frampton automatic 63.6%
Gupta + Frampton manual 70.2%
All (+ gesture) 70.4%
Table 3: Accuracy values for several different fea-
ture configurations on the three class classification
problem.
to accompany referential instances of you but not
generic instances, gesture information is able to
help differentiate between the two. Manual in-
spection of the decision tree produced indicates
that gesture features were among the most dis-
criminative features.
The results on the singular vs. plural task shown
in Table 2 are less clear. Although (Gupta et al,
2007a) did not report results on singular vs. plural
classification, their feature set produced reason-
able classification accuracy of 73.6%. Including
gesture and other features did not produce a statis-
tically significant improvement in the overall ac-
curacy. This suggests that while gesture is helpful
for predicting referentiality, it does not appear to
be a reliable predictor of whether an instance of
you is singular or plural. Inspection on the deci-
sion tree confirms that gesture features were not
seen to be highly discriminative.
5.2 Three Class Classification
The results presented for singular vs. plural classi-
fication are based on performance on the subset of
you instances that are referential, which assumes
that we are able to filter out generic references
with 100% accuracy. While this gives us an eval-
uation of how well the singular vs. plural task can
be performed without the generic references pre-
senting a confounding factor, it presents unrealis-
tic performance for a real system. To account for
this, we present results on a three class problem of
determining whether an instance of you is generic,
singular, or plural. The results are shown in Table
3. A simple majority class classifier yields accu-
racy of 46.7% (In our data, the generic class was
the majority class).
As we can see from Table 3, adding addi-
tional features gives improved performance over
the original implementation by Gupta et. al., re-
sulting in an overall accuracy of about 70%. We
also observed that the dialogue act features were
important; manual configurations produced abso-
lute gains of about 7% accuracy over fully auto-
matic configurations. The gesture feature, how-
ever, did not provide a significant increase in per-
formance over the same feature set without gesture
information.
Table 4 shows the precision, recall, and F-
measure values for each you type for several dif-
ferent configurations. As shown, the generic class
proved to be the easiest for the classifiers to iden-
tify. This is not suprising, as not only are generic
instance our majority class, but many of the fea-
tures used were originally tailored towards the two
class problem of differentiating generic instances
from the other classes. The performance on the
plural and singular classes is comparable to one
another when the basic feature set is used. How-
ever, as more features are added, the performance
on the singular class increases while the perfor-
mance on the plural class does not. This seems
to suggest that future work should attempt to in-
clude more features that are indicative of plural
instances.
When manual dialogue acts are applied, it ap-
pears incorporation of gestures does not lead to
any overall performance improvement (as shown
in Table 3). One possible explanation is that ges-
ture features as they are incorporated here do pro-
vide some disambiguating information (as shown
in the two stage classification), but this informa-
tion is subsumed by other features, such as dia-
logue acts. To test this hypothesis, we ran an ex-
periment with a feature set that contained all fea-
tures except dialogue act features. That is, a fea-
ture set that contains all of the automatic features,
as well as gesture features. Results are shown in
Table 5.
Our ?automatic + gesture? feature configuration
produced accuracy of 66.2%. When compared to
the same feature set without gesture features (the
?Gupta + Frampton automatic? row in Table 3) we
see a statistically significant (p < 0.01) absolute
accuracy improvement of about 2.6%. This seems
to suggest that gesture features are providing some
small amount of relevant information that is not
captured by our automatically extractable features.
Up until this point we have incorporated dia-
logue acts using the full set of dialogue act tags
provided by the AMI corpus. As we have men-
311
Precision Recall F-Measure
Gupta automatic Plural 0.553 0.548 0.550
Singular 0.657 0.408 0.504
Generic 0.624 0.787 0.696
Gupta manual Plural 0.536 0.513 0.524
Singular 0.675 0.503 0.576
Generic 0.704 0.839 0.766
All (+ gesture) Plural 0.542 0.565 0.553
Singular 0.745 0.604 0.667
Generic 0.754 0.835 0.792
Table 4: Precision, recall, and F-measure results for each you type based on three class classification.
Accuracy
Gupta + Frampton automatic 63.6%
Gupta + Frampton automatic + gesture 66.2%
Gupta + Frampton automatic + simple dialogue act 66.6%
Gupta + Frampton automatic + simple dialogue act + gesture 69.0%
Table 5: Accuracy for 3-way classification by combining gesture information with automatically ex-
tracted features based on the Decision Tree model
tioned, this level of granularity may not be prac-
tically extractable for use in a current state-of-
the-art system. As a result, we implemented the
simpler dialogue act incorporation method pro-
posed by (Gupta et al, 2007a), in which only
the presence or absence of the elicit dialogue act
type is considered. Using this feature with the
automatically extracted features yielded accuracy
of 66.6%, a statistically significant improvement
(p < 0.01) of an absolute 3% over a fully auto-
matic run. Furthermore, if we incorporate gesture
features with this configuration, the performance
increases to 69.0% (statistically significantly, p <
0.01). This suggests that while gesture features
may be redundant with information provided by
the full set of dialogue act tags, it is largely com-
plementary with the simpler dialogue act incorpo-
ration. The incorporation of gesture along with
simpler and more reliable dialogue acts can po-
tentially approach the performance gained by in-
corporation of more complex dialogue acts, which
are often difficult to obtain. Of course, gesture fea-
tures themselves are often difficult to obtain. How-
ever, redundancy in two potentially error-prone
feature sources can be an asset, as data from one
source may help to compensate for errors in the
other. Although addressing a different problem of
multimodal integration, previous work (Oviatt et
al., 1997) appears to indicate that this is the case.
6 Conclusion
In this paper, we investigate the role of hand ges-
tures in disambiguating types of You expressions
in multiparty meetings for the purpose of machine
translation.
Our results have shown that on the binary
generic vs. referential classification problem, the
inclusion of gesture data provides a statistically
significant increase in performance over the same
feature set without gesture. This result is consis-
tent with our hypothesis that gesture data would be
helpful because speakers are more likely to gesture
when producing referential instances of you.
To produce results more akin to those that
would be expected during incorporation in a real
machine translation system, we experimented with
the type identification problem as a three class
classification problem. It was discovered that
when a full set of dialogue act tags were used as
features, the incorporation of gesture features does
not provide an increase in performance. However,
when simpler dialogue act tags are used, the in-
corporation of gestures helps to make up for lost
performance. Since it remains a difficult prob-
lem to automatically predict complex dialog acts
with high accuracy, the incorporation of gesture
features may prove beneficial to current systems.
312
7 Acknowledgement
This work was supported by IIS-0855131 (to the
first two authors) and IIS-0840461 (to the third au-
thor) from the National Science Foundation. The
authors would like to thank anonymous reviewers
for valuable comments and suggestions.
References
Tyler Baldwin, Joyce Y. Chai, and Katrin Kirchhoff.
2009. Communicative gestures in coreference iden-
tification in multiparty meetings. In ICMI-MLMI
?09: Proceedings of the 2009 international con-
ference on Multimodal interfaces, pages 211?218.
ACM.
J. B. Bavelas, N. Chovil, L. Coates, and L. Roe. 1995.
Gestures specialized for dialogue. Personality and
Social Psychology Bulletin, 21:394?405.
Jacob Eisenstein and Randall Davis. 2006. Gesture
improves coreference resolution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages
37?40, New York City, USA, June. Association for
Computational Linguistics.
Jacob Eisenstein and Randall Davis. 2007. Condi-
tional modality fusion for coreference resolution. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 352?
359, Prague, Czech Republic, June. Association for
Computational Linguistics.
Matthew Frampton, Raquel Ferna?ndez, Patrick Ehlen,
Mario Christoudias, Trevor Darrell, and Stanley Pe-
ters. 2009. Who is ?you??: combining linguis-
tic and gaze features to resolve second-person refer-
ences in dialogue. In EACL ?09: Proceedings of the
12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 273?
281, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Dan Jurafsky. 2007a. Resolving you in multi-party
dialog. In Proceedings of the 8th SIGdial Workshop
on Discourse and Dialogue.
Surabhi Gupta, Matthew Purver, and Dan Jurafsky.
2007b. Disambiguating between generic and refer-
ential you in dialog. In Proceedings of the 42th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Adam Kendon. 1980. Gesticulation and speech: Two
aspects of the process of utterance. In Mary Richie
Key, editor, The Relationship of Verbal and Nonver-
bal Communication, pages 207?227.
D. McNeill. 1992. Hand and Mind: What Gestures
Reveal about Thought. University of Chicago Press.
W. M. Meyers. 1990. Current generic pronoun usage.
American Speech, 65(3):228?237.
Sharon Oviatt, Antonella DeAngeli, and Karen Kuhn.
1997. Integration and synchronization of input
modes during multimodal human-computer interac-
tion. In CHI ?97: Proceedings of the SIGCHI con-
ference on Human factors in computing systems,
pages 415?422, New York, NY, USA. ACM.
Andrei Popescu-Belis and Paula Estrella. 2007. Gen-
erating usable formats for metadata and annotations
in a large meeting corpus. In Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 93?96,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Matthew Purver, Raquel Ferna?ndez, Matthew Framp-
ton, and Stanley Peters. 2009. Cascaded lexicalised
classifiers for second-person reference resolution.
In SIGDIAL ?09: Proceedings of the SIGDIAL 2009
Conference, pages 306?309, Morristown, NJ, USA.
Association for Computational Linguistics.
313
