Proceedings of ACL-08: HLT, pages 496?504,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Combining EM Training and the MDL Principle for an
Automatic Verb Classification incorporating Selectional Preferences
Sabine Schulte im Walde, Christian Hying, Christian Scheible, Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart, Germany
{schulte,hyingcn,scheibcn,schmid}@ims.uni-stuttgart.de
Abstract
This paper presents an innovative, complex
approach to semantic verb classification that
relies on selectional preferences as verb prop-
erties. The probabilistic verb class model un-
derlying the semantic classes is trained by
a combination of the EM algorithm and the
MDL principle, providing soft clusters with
two dimensions (verb senses and subcategori-
sation frames with selectional preferences) as
a result. A language-model-based evaluation
shows that after 10 training iterations the verb
class model results are above the baseline re-
sults.
1 Introduction
In recent years, the computational linguistics com-
munity has developed an impressive number of se-
mantic verb classifications, i.e., classifications that
generalise over verbs according to their semantic
properties. Intuitive examples of such classifica-
tions are the MOTION WITH A VEHICLE class, in-
cluding verbs such as drive, fly, row, etc., or the
BREAK A SOLID SURFACE WITH AN INSTRUMENT
class, including verbs such as break, crush, frac-
ture, smash, etc. Semantic verb classifications are
of great interest to computational linguistics, specifi-
cally regarding the pervasive problem of data sparse-
ness in the processing of natural language. Up to
now, such classifications have been used in applica-
tions such as word sense disambiguation (Dorr and
Jones, 1996; Kohomban and Lee, 2005), machine
translation (Prescher et al, 2000; Koehn and Hoang,
2007), document classification (Klavans and Kan,
1998), and in statistical lexical acquisition in gen-
eral (Rooth et al, 1999; Merlo and Stevenson, 2001;
Korhonen, 2002; Schulte im Walde, 2006).
Given that the creation of semantic verb classi-
fications is not an end task in itself, but depends
on the application scenario of the classification, we
find various approaches to an automatic induction of
semantic verb classifications. For example, Siegel
and McKeown (2000) used several machine learn-
ing algorithms to perform an automatic aspectual
classification of English verbs into event and sta-
tive verbs. Merlo and Stevenson (2001) presented
an automatic classification of three types of English
intransitive verbs, based on argument structure and
heuristics to thematic relations. Pereira et al (1993)
and Rooth et al (1999) relied on the Expectation-
Maximisation algorithm to induce soft clusters of
verbs, based on the verbs? direct object nouns. Sim-
ilarly, Korhonen et al (2003) relied on the Informa-
tion Bottleneck (Tishby et al, 1999) and subcate-
gorisation frame types to induce soft verb clusters.
This paper presents an innovative, complex ap-
proach to semantic verb classes that relies on se-
lectional preferences as verb properties. The un-
derlying linguistic assumption for this verb class
model is that verbs which agree on their selec-
tional preferences belong to a common seman-
tic class. The model is implemented as a soft-
clustering approach, in order to capture the poly-
semy of the verbs. The training procedure uses the
Expectation-Maximisation (EM) algorithm (Baum,
1972) to iteratively improve the probabilistic param-
eters of the model, and applies the Minimum De-
scription Length (MDL) principle (Rissanen, 1978)
to induce WordNet-based selectional preferences for
arguments within subcategorisation frames. Our
model is potentially useful for lexical induction
(e.g., verb senses, subcategorisation and selectional
preferences, collocations, and verb alternations),
496
and for NLP applications in sparse data situations.
In this paper, we provide an evaluation based on a
language model.
The remainder of the paper is organised as fol-
lows. Section 2 introduces our probabilistic verb
class model, the EM training, and how we incor-
porate the MDL principle. Section 3 describes the
clustering experiments, including the experimental
setup, the evaluation, and the results. Section 4 re-
ports on related work, before we close with a sum-
mary and outlook in Section 5.
2 Verb Class Model
2.1 Probabilistic Model
This paper suggests a probabilistic model of verb
classes that groups verbs into clusters with simi-
lar subcategorisation frames and selectional prefer-
ences. Verbs may be assigned to several clusters
(soft clustering) which allows the model to describe
the subcategorisation properties of several verb read-
ings separately. The number of clusters is defined
in advance, but the assignment of the verbs to the
clusters is learnt during training. It is assumed that
all verb readings belonging to one cluster have simi-
lar subcategorisation and selectional properties. The
selectional preferences are expressed in terms of se-
mantic concepts from WordNet, rather than a set of
individual words. Finally, the model assumes that
the different arguments are mutually independent for
all subcategorisation frames of a cluster. From the
last assumption, it follows that any statistical depen-
dency between the arguments of a verb has to be ex-
plained by multiple readings.
The statistical model is characterised by the fol-
lowing equation which defines the probability of a
verb v with a subcategorisation frame f and argu-
ments a1, ..., anf :
p(v, f, a1, ..., anf ) =
?
c
p(c) p(v|c) p(f |c) ?
nf
?
i=1
?
r?R
p(r|c, f, i) p(ai|r)
The model describes a stochastic process which gen-
erates a verb-argument tuple like ?speak, subj-pp.to,
professor, audience? by
1. selecting some cluster c, e.g. c3 (which might
correspond to a set of communication verbs),
with probability p(c3),
2. selecting a verb v, here the verb speak, from
cluster c3 with probability p(speak|c3),
3. selecting a subcategorisation frame f , here
subj-pp.to, with probability p(subj-pp.to|c3);
note that the frame probability only depends on
the cluster, and not on the verb,
4. selecting a WordNet concept r for each argu-
ment slot, e.g. person for the first slot with
probability p(person|c3, subj-pp.to, 1) and so-
cial group for the second slot with probability
p(social group|c3, subj-pp.to, 2),
5. selecting a word ai to instantiate each con-
cept as argument i; in our example, we
might choose professor for person with
probability p(professor|person) and au-
dience for social group with probability
p(audience|social group).
The model contains two hidden variables, namely
the clusters c and the selectional preferences r. In or-
der to obtain the overall probability of a given verb-
argument tuple, we have to sum over all possible val-
ues of these hidden variables.
The assumption that the arguments are indepen-
dent of the verb given the cluster is essential for ob-
taining a clustering algorithm because it forces the
EM algorithm to make the verbs within a cluster as
similar as possible.1 The assumption that the differ-
ent arguments of a verb are mutually independent is
important to reduce the parameter set to a tractable
size
The fact that verbs select for concepts rather than
individual words also reduces the number of param-
eters and helps to avoid sparse data problems. The
application of the MDL principle guarantees that no
important information is lost.
The probabilities p(r|c, f, i) and p(a|r) men-
tioned above are not represented as atomic enti-
ties. Instead, we follow an approach by Abney
1The EM algorithm adjusts the model parameters in such a
way that the probability assigned to the training tuples is max-
imised. Given the model constraints, the data probability can
only be maximised by making the verbs within a cluster as sim-
ilar to each other as possible, regarding the required arguments.
497
and Light (1999) and turn WordNet into a Hidden
Markov model (HMM). We create a new pseudo-
concept for each WordNet noun and add it as a hy-
ponym to each synset containing this word. In ad-
dition, we assign a probability to each hypernymy?
hyponymy transition, such that the probabilities of
the hyponymy links of a synset sum up to 1. The
pseudo-concept nodes emit the respective word with
a probability of 1, whereas the regular concept nodes
are non-emitting nodes. The probability of a path
in this (a priori) WordNet HMM is the product of
the probabilities of the transitions within the path.
The probability p(a|r) is then defined as the sum
of the probabilities of all paths from the concept r
to the word a. Similarly, we create a partial Word-
Net HMM for each argument slot ?c, f, i? which en-
codes the selectional preferences. It contains only
the WordNet concepts that the slot selects for, ac-
cording to the MDL principle (cf. Section 2.3), and
the dominating concepts. The probability p(r|c, f, i)
is the total probability of all paths from the top-most
WordNet concept entity to the terminal node r.
2.2 EM Training
The model is trained on verb-argument tuples of
the form described above, i.e., consisting of a verb
and a subcategorisation frame, plus the nominal2
heads of the arguments. The tuples may be ex-
tracted from parsed data, or from a treebank. Be-
cause of the hidden variables, the model is trained
iteratively with the Expectation-Maximisation algo-
rithm (Baum, 1972). The parameters are randomly
initialised and then re-estimated with the Inside-
Outside algorithm (Lari and Young, 1990) which is
an instance of the EM algorithm for training Proba-
bilistic Context-Free Grammars (PCFGs).
The PCFG training algorithm is applicable here
because we can define a PCFG for each of our mod-
els which generates the same verb-argument tuples
with the same probability. The PCFG is defined as
follows:
(1) The start symbol is TOP.
(2) For each cluster c, we add a rule TOP ? Vc Ac
whose probability is p(c).
2Arguments with lexical heads other than nouns (e.g., sub-
categorised clauses) are not included in the selectional prefer-
ence induction.
(3) For each verb v in cluster c, we add a rule
Vc ? v with probability p(v|c).
(4) For each subcategorisation frame f of cluster c
with length n, we add a rule Ac ? f Rc,f,1,entity
... Rc,f,n,entity with probability p(f |c).
(5) For each transition from a node r to a node r?
in the selectional preference model for slot i of
the subcategorisation frame f of cluster c, we
add a rule Rc,f,i,r ? Rc,f,i,r? whose probability
is the transition probability from r to r? in the
respective WordNet-HMM.
(6) For each terminal node r in the selectional pref-
erence model, we add a rule Rc,f,i,r ? Rr whose
probability is 1. With this rule, we ?jump? from
the selectional restriction model to the corre-
sponding node in the a priori model.
(7) For each transition from a node r to a node r?
in the a priori model, we add a rule Rr ? Rr?
whose probability is the transition probability
from r to r? in the a priori WordNet-HMM.
(8) For each word node a in the a priori model, we
add a rule Ra ? a whose probability is 1.
Based on the above definitions, a partial ?parse? for
?speak subj-pp.to professor audience?, referring to
cluster 3 and one possible WordNet path, is shown in
Figure 1. The connections within R3 (R3,...,entity?
R3,...,person/group) and within R (Rperson/group?
Rprofessor/audience) refer to sequential applications
of rule types (5) and (7), respectively.
TOP
V3
speak
A3
subj-pp.to R3,subj?pp.to,1,entity
R3,subj?pp.to,1,person
Rperson
Rprofessor
professor
R3,subj?pp.to,2,entity
R3,subj?pp.to,2,group
Rgroup
Raudience
audience
Figure 1: Example parse tree.
The EM training algorithm maximises the likelihood
of the training data.
498
2.3 MDL Principle
A model with a large number of fine-grained con-
cepts as selectional preferences assigns a higher
likelihood to the data than a model with a small num-
ber of general concepts, because in general a larger
number of parameters is better in describing train-
ing data. Consequently, the EM algorithm a pri-
ori prefers fine-grained concepts but ? due to sparse
data problems ? tends to overfit the training data. In
order to find selectional preferences with an appro-
priate granularity, we apply the Minimum Descrip-
tion Length principle, an approach from Information
Theory. According to the MDL principle, the model
with minimal description length should be chosen.
The description length itself is the sum of the model
length and the data length, with the model length
defined as the number of bits needed to encode the
model and its parameters, and the data length de-
fined as the number of bits required to encode the
training data with the given model. According to
coding theory, an optimal encoding uses ?log2p
bits, on average, to encode data whose probability
is p. Usually, the model length increases and the
data length decreases as more parameters are added
to a model. The MDL principle finds a compromise
between the size of the model and the accuracy of
the data description.
Our selectional preference model relies on Li and
Abe (1998), applying the MDL principle to deter-
mine selectional preferences of verbs and their argu-
ments, by means of a concept hierarchy ordered by
hypernym/hyponym relations. Given a set of nouns
within a specific argument slot as a sample, the ap-
proach finds the cut3 in a concept hierarchy which
minimises the sum of encoding both the model and
the data. The model length (ML) is defined as
ML = k2 ? log2 |S|,
with k the number of concepts in the partial hierar-
chy between the top concept and the concepts in the
cut, and |S| the sample size, i.e., the total frequency
of the data set. The data length (DL) is defined as
DL = ?
?
n?S
log2 p(n).
3A cut is defined as a set of concepts in the concept hier-
archy that defines a partition of the ?leaf? concepts (the lowest
concepts in the hierarchy), viewing each concept in the cut as
representing the set of all leaf concepts it dominates.
The probability of a noun p(n) is determined by di-
viding the total probability of the concept class the
noun belongs to, p(concept), by the size of that
class, |concept|, i.e., the number of nouns that are
dominated by that concept:
p(n) = p(concept)|concept| .
The higher the concept within the hierarchy, the
more nouns receive an equal probability, and the
greater is the data length.
The probability of the concept class in turn is de-
termined by dividing the frequency of the concept
class f(concept) by the sample size:
p(concept) = f(concept)|S| ,
where f(concept) is calculated by upward propaga-
tion of the frequencies of the nominal lexemes from
the data sample through the hierarchy. For exam-
ple, if the nouns coffee, tea, milk appeared with fre-
quencies 25, 50, 3, respectively, within a specific ar-
gument slot, then their hypernym concept beverage
would be assigned a frequency of 78, and these 78
would be propagated further upwards to the next hy-
pernyms, etc. As a result, each concept class is as-
signed a fraction of the frequency of the whole data
set (and the top concept receives the total frequency
of the data set). For calculating p(concept) (and the
overall data length), though, only the concept classes
within the cut through the hierarchy are relevant.
Our model uses WordNet 3.0 as the concept hier-
archy, and comprises one (complete) a priori Word-
Net model for the lexical head probabilities p(a|r)
and one (partial) model for each selectional proba-
bility distribution p(r|c, f, i), cf. Section 2.1.
2.4 Combining EM and MDL
The training procedure that combines the EM train-
ing with the MDL principle can be summarised as
follows.
1. The probabilities of a verb class model with c
classes and a pre-defined set of verbs and frames
are initialised randomly. The selectional preference
models start out with the most general WordNet con-
cept only, i.e., the partial WordNet hierarchies un-
derlying the probabilities p(r|c, f, i) initially only
contain the concept r for entity.
499
2. The model is trained for a pre-defined num-
ber of iterations. In each iteration, not only the
model probabilities are re-estimated and maximised
(as done by EM), but also the cuts through the con-
cept hierarchies that represent the various selectional
preference models are re-assessed. In each iteration,
the following steps are performed.
(a) The partial WordNet hierarchies that represent
the selectional preference models are expanded to
include the hyponyms of the respective leaf con-
cepts of the partial hierarchies. I.e., in the first itera-
tion, all models are expanded towards the hyponyms
of entity, and in subsequent iterations each selec-
tional preference model is expanded to include the
hyponyms of the leaf nodes in the partial hierarchies
resulting from the previous iteration. This expansion
step allows the selection models to become more and
more detailed, as the training proceeds and the verb
clusters (and their selectional restrictions) become
increasingly specific.
(b) The training tuples are processed: For each tu-
ple, a PCFG parse forest as indicated by Figure 1
is done, and the Inside-Outside algorithm is applied
to estimate the frequencies of the ?parse tree rules?,
given the current model probabilities.
(c) The MDL principle is applied to each selectional
preference model: Starting from the respective leaf
concepts in the partial hierarchies, MDL is calcu-
lated to compare each set of hyponym concepts that
share a hypernym with the respective hypernym con-
cept. If the MDL is lower for the set of hyponyms
than the hypernym, the hyponyms are left in the par-
tial hierarchy. Otherwise the expansion of the hyper-
nym towards the hyponyms is undone and we con-
tinue recursively upwards the hierarchy, calculating
MDL to compare the former hypernym and its co-
hyponyms with the next upper hypernym, etc. The
recursion allows the training algorithm to remove
nodes which were added in earlier iterations and are
no longer relevant. It stops if the MDL is lower for
the hyponyms than for the hypernym.
This step results in selectional preference models
that minimally contain the top concept entity, and
maximally contain the partial WordNet hierarchy
between entity and the concept classes that have
been expanded within this iteration.
(d) The probabilities of the verb class model are
maximised based on the frequency estimates ob-
tained in step (b).
3 Experiments
The model is generally applicable to all languages
for which WordNet exists, and for which the Word-
Net functions provided by Princeton University are
available. For the purposes of this paper, we choose
English as a case study.
3.1 Experimental Setup
The input data for training the verb class mod-
els were derived from Viterbi parses of the whole
British National Corpus, using the lexicalised PCFG
for English by Carroll and Rooth (1998). We took
only active clauses into account, and disregarded
auxiliary and modal verbs as well as particle verbs,
leaving a total of 4,852,371 Viterbi parses. Those in-
put tuples were then divided into 90% training data
and 10% test data, providing 4,367,130 training tu-
ples (over 2,769,804 types), and 485,241 test tuples
(over 368,103 types).
As we wanted to train and assess our verb class
model under various conditions, we used different
fractions of the training data in different training
regimes. Because of time and memory constraints,
we only used training tuples that appeared at least
twice. (For the sake of comparison, we also trained
one model on all tuples.) Furthermore, we dis-
regarded tuples with personal pronoun arguments;
they are not represented in WordNet, and even if
they are added (e.g. to general concepts such as
person, entity) they have a rather destructive ef-
fect. We considered two subsets of the subcate-
gorisation frames with 10 and 20 elements, which
were chosen according to their overall frequency in
the training data; for example, the 10 most frequent
frame types were subj:obj, subj, subj:ap, subj:to,
subj:obj:obj2, subj:obj:pp-in, subj:adv, subj:pp-in,
subj:vbase, subj:that.4 When relying on theses
10/20 subcategorisation frames, plus including the
above restrictions, we were left with 39,773/158,134
and 42,826/166,303 training tuple types/tokens, re-
spectively. The overall number of training tuples
4A frame lists its arguments, separated by ?:?. Most argu-
ments within the frame types should be self-explanatory. ap is
an adjectival phrase.
500
was therefore much smaller than the generally avail-
able data. The corresponding numbers including tu-
ples with a frequency of one were 478,717/597,078
and 577,755/701,232.
The number of clusters in the experiments was ei-
ther 20 or 50, and we used up to 50 iterations over
the training tuples. The model probabilities were
output after each 5th iteration. The output comprises
all model probabilities introduced in Section 2.1.
The following sections describe the evaluation of the
experiments, and the results.
3.2 Evaluation
One of the goals in the development of the presented
verb class model was to obtain an accurate statistical
model of verb-argument tuples, i.e. a model which
precisely predicts the tuple probabilities. In order
to evaluate the performance of the model in this re-
spect, we conducted an evaluation experiment, in
which we computed the probability which the verb
class model assigns to our test tuples and compared
it to the corresponding probability assigned by a
baseline model. The model with the higher proba-
bility is judged the better model.
We expected that the verb class model would
perform better than the baseline model on tuples
where one or more of the arguments were not ob-
served with the respective verb, because either the
argument itself or a semantically similar argument
(according to the selectional preferences) was ob-
served with verbs belonging to the same cluster. We
also expected that the verb class model assigns a
lower probability than the baseline model to test tu-
ples which frequently occurred in the training data,
since the verb class model fails to describe precisely
the idiosyncratic properties of verbs which are not
shared by the other verbs of its cluster.
The Baseline Model The baseline model decom-
poses the probability of a verb-argument tuple into a
product of conditional probabilities:5
p(v, f, anf1 ) = p(v) p(f |v)
nf
?
i=1
p(ai|ai?11 , ?v, f?, fi)
5fi is the label of the ith slot. The verb and the subcategori-
sation frame are enclosed in angle brackets because they are
treated as a unit during smoothing.
The probability of our example tuple ?speak,
subj-pp.to, professor, audience? in the base-
line model is then p(speak) p(subj-pp.to|speak)
p(professor|?speak, subj-pp.to?, subj) p(audience|
professor, ?speak, subj-pp.to?, pp.to).
The model contains no hidden variables. Thus the
parameters can be directly estimated from the train-
ing data with relative frequencies. The parameter
estimates are smoothed with modified Kneser-Ney
smoothing (Chen and Goodman, 1998), such that
the probability of each tuple is positive.
Smoothing of the Verb Class Model Although
the verb class model has a built-in smoothing capac-
ity, it needs additional smoothing for two reasons:
Firstly, some of the nouns in the test data did not
occur in the training data. The verb class model
assigns a zero probability to such nouns. Hence
we smoothed the concept instantiation probabilities
p(noun|concept) with Witten-Bell smoothing (Chen
and Goodman, 1998). Secondly, we smoothed the
probabilities of the concepts in the selectional pref-
erence models where zero probabilities may occur.
The smoothing ensures that the verb class model
assigns a positive probability to each verb-argument
tuple with a known verb, a known subcategorisation
frame, and arguments which are in WordNet. Other
tuples were excluded from the evaluation because
the verb class model cannot deal with them.
3.3 Results
The evaluation results of our classification experi-
ments are presented in Table 1, for 20 and 50 clus-
ters, with 10 and 20 subcategorisation frame types.
The table cells provide the loge of the probabilities
per tuple token. The probabilities increase with the
number of iterations, flattening out after approx. 25
iterations, as illustrated by Figure 2. Both for 10
and 20 frames, the results are better for 50 than for
20 clusters, with small differences between 10 and
20 frames. The results vary between -11.850 and
-10.620 (for 5-50 iterations), in comparison to base-
line values of -11.546 and -11.770 for 10 and 20
frames, respectively. The results thus show that our
verb class model results are above the baseline re-
sults after 10 iterations; this means that our statis-
tical model then assigns higher probabilities to the
test tuples than the baseline model.
501
No. of Iteration
Clusters 5 10 15 20 25 30 35 40 45 50
10 frames
20 -11.770 -11.408 -10.978 -10.900 -10.853 -10.841 -10.831 -10.823 -10.817 -10.812
50 -11.850 -11.452 -11.061 -10.904 -10.730 -10.690 -10.668 -10.628 -10.625 -10.620
20 frames
20 -11.769 -11.430 -11.186 -10.971 -10.921 -10.899 -10.886 -10.875 -10.873 -10.869
50 -11.841 -11.472 -11.018 -10.850 -10.737 -10.728 -10.706 -10.680 -10.662 -10.648
Table 1: Clustering results ? BNC tuples.
Figure 2: Illustration of clustering results.
Including input tuples with a frequency of one in
the training data with 10 subcategorisation frames
(as mentioned in Section 3.1) decreases the loge per
tuple to between -13.151 and -12.498 (for 5-50 it-
erations), with similar training behaviour as in Fig-
ure 2, and in comparsion to a baseline of -17.988.
The differences in the result indicate that the mod-
els including the hapax legomena are worse than the
models that excluded the sparse events; at the same
time, the differences between baseline and cluster-
ing model are larger.
In order to get an intuition about the qualitative
results of the clusterings, we select two example
clusters that illustrate that the idea of the verb class
model has been realised within the clusters. Ac-
cording to our own intuition, the clusters are over-
all semantically impressive, beyond the examples.
Future work will assess by semantics-based eval-
uations of the clusters (such as pseudo-word dis-
ambiguation, or a comparison against existing verb
classifications), whether this intuition is justified,
whether it transfers to the majority of verbs within
the cluster analyses, and whether the clusters cap-
ture polysemic verbs appropriately.
The two examples are taken from the 10 frame/50
cluster verb class model, with probabilities of 0.05
and 0.04. The ten most probable verbs in the first
cluster are show, suggest, indicate, reveal, find, im-
ply, conclude, demonstrate, state, mean, with the
two most probable frame types subj and subj:that,
i.e., the intransitive frame, and a frame that subcat-
egorises a that clause. As selectional preferences
within the intransitive frame (and quite similarly
in the subj:that frame), the most probable concept
classes6 are study, report, survey, name, research,
result, evidence. The underlined nouns represent
specific concept classes, because they are leaf nodes
in the selectional preference hierarchy, thus refer-
ring to very specific selectional preferences, which
are potentially useful for collocation induction. The
ten most probable verbs in the second cluster are
arise, remain, exist, continue, need, occur, change,
improve, begin, become, with the intransitive frame
being most probable. The most probable concept
classes are problem, condition, question, natural
phenomenon, situation. The two examples illustrate
that the verbs within a cluster are semantically re-
lated, and that they share obvious subcategorisation
frames with intuitively plausible selectional prefer-
ences.
4 Related Work
Our model is an extension of and thus most closely
related to the latent semantic clustering (LSC) model
(Rooth et al, 1999) for verb-argument pairs ?v, a?
which defines their probability as follows:
p(v, a) =
?
c
p(c) p(v|c) p(a|c)
In comparison to our model, the LSC model only
considers a single argument (such as direct objects),
6For readability, we only list one noun per WordNet concept.
502
or a fixed number of arguments from one particu-
lar subcategorisation frame, whereas our model de-
fines a probability distribution over all subcategori-
sation frames. Furthermore, our model specifies se-
lectional preferences in terms of general WordNet
concepts rather than sets of individual words.
In a similar vein, our model is both similar and
distinct in comparison to the soft clustering ap-
proaches by Pereira et al (1993) and Korhonen et
al. (2003). Pereira et al (1993) suggested determin-
istic annealing to cluster verb-argument pairs into
classes of verbs and nouns. On the one hand, their
model is asymmetric, thus not giving the same in-
terpretation power to verbs and arguments; on the
other hand, the model provides a more fine-grained
clustering for nouns, in the form of an additional hi-
erarchical structure of the noun clusters. Korhonen
et al (2003) used verb-frame pairs (instead of verb-
argument pairs) to cluster verbs relying on the Infor-
mation Bottleneck (Tishby et al, 1999). They had
a focus on the interpretation of verbal polysemy as
represented by the soft clusters. The main difference
of our model in comparison to the above two models
is, again, that we incorporate selectional preferences
(rather than individual words, or subcategorisation
frames).
In addition to the above soft-clustering models,
various approaches towards semantic verb classifi-
cation have relied on hard-clustering models, thus
simplifying the notion of verbal polysemy. Two
large-scale approaches of this kind are Schulte im
Walde (2006), who used k-Means on verb subcat-
egorisation frames and verbal arguments to cluster
verbs semantically, and Joanis et al (2008), who ap-
plied Support Vector Machines to a variety of verb
features, including subcategorisation slots, tense,
voice, and an approximation to animacy. To the
best of our knowledge, Schulte im Walde (2006) is
the only hard-clustering approach that previously in-
corporated selectional preferences as verb features.
However, her model was not soft-clustering, and
she only used a simple approach to represent selec-
tional preferences by WordNet?s top-level concepts,
instead of making use of the whole hierarchy and
more sophisticated methods, as in the current paper.
Last but not least, there are other models of se-
lectional preferences than the MDL model we used
in our paper. Most such models also rely on the
WordNet hierarchy (Resnik, 1997; Abney and Light,
1999; Ciaramita and Johnson, 2000; Clark and Weir,
2002). Brockmann and Lapata (2003) compared
some of the models against human judgements on
the acceptability of sentences, and demonstrated that
the models were significantly correlated with human
ratings, and that no model performed best; rather,
the different methods are suited for different argu-
ment relations.
5 Summary and Outlook
This paper presented an innovative, complex ap-
proach to semantic verb classes that relies on se-
lectional preferences as verb properties. The prob-
abilistic verb class model underlying the semantic
classes was trained by a combination of the EM al-
gorithm and the MDL principle, providing soft clus-
ters with two dimensions (verb senses and subcate-
gorisation frames with selectional preferences) as a
result. A language model-based evaluation showed
that after 10 training iterations the verb class model
results are above the baseline results.
We plan to improve the verb class model with re-
spect to (i) a concept-wise (instead of a cut-wise)
implementation of the MDL principle, to operate on
concepts instead of combinations of concepts; and
(ii) variations of the concept hierarchy, using e.g. the
sense-clustered WordNets from the Stanford Word-
Net Project (Snow et al, 2007), or a WordNet ver-
sion improved by concepts from DOLCE (Gangemi
et al, 2003), to check on the influence of concep-
tual details on the clustering results. Furthermore,
we aim to use the verb class model in NLP tasks, (i)
as resource for lexical induction of verb senses, verb
alternations, and collocations, and (ii) as a lexical
resource for the statistical disambiguation of parse
trees.
References
Steven Abney and Marc Light. 1999. Hiding a Seman-
tic Class Hierarchy in a Markow Model. In Proceed-
ings of the ACL Workshop on Unsupervised Learning
in Natural Language Processing, pages 1?8, College
Park, MD.
Leonard E. Baum. 1972. An Inequality and Associated
Maximization Technique in Statistical Estimation for
Probabilistic Functions of Markov Processes. Inequal-
ities, III:1?8.
503
Carsten Brockmann and Mirella Lapata. 2003. Evaluat-
ing and Combining Approaches to Selectional Prefer-
ence Acquisition. In Proceedings of the 10th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 27?34, Budapest,
Hungary.
Glenn Carroll and Mats Rooth. 1998. Valence Induction
with a Head-Lexicalized PCFG. In Proceedings of the
3rd Conference on Empirical Methods in Natural Lan-
guage Processing, Granada, Spain.
Stanley Chen and Joshua Goodman. 1998. An Empirical
Study of Smoothing Techniques for Language Model-
ing. Technical Report TR-10-98, Center for Research
in Computing Technology, Harvard University.
Massimiliano Ciaramita and Mark Johnson. 2000. Ex-
plaining away Ambiguity: Learning Verb Selectional
Preference with Bayesian Networks. In Proceedings
of the 18th International Conference on Computa-
tional Linguistics, pages 187?193, Saarbru?cken, Ger-
many.
Stephen Clark and David Weir. 2002. Class-Based Prob-
ability Estimation using a Semantic Hierarchy. Com-
putational Linguistics, 28(2):187?206.
Bonnie J. Dorr and Doug Jones. 1996. Role of Word
Sense Disambiguation in Lexical Acquisition: Predict-
ing Semantics from Syntactic Cues. In Proceedings of
the 16th International Conference on Computational
Linguistics, pages 322?327, Copenhagen, Denmark.
Aldo Gangemi, Nicola Guarino, Claudio Masolo, and
Alessandro Oltramari. 2003. Sweetening WordNet
with DOLCE. AI Magazine, 24(3):13?24.
Eric Joanis, Suzanne Stevenson, and David James. 2008?
A General Feature Space for Automatic Verb Classifi-
cation. Natural Language Engineering. To appear.
Judith L. Klavans and Min-Yen Kan. 1998. The Role
of Verbs in Document Analysis. In Proceedings of
the 17th International Conference on Computational
Linguistics and the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 680?686,
Montreal, Canada.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 868?876, Prague, Czech Republic.
Upali S. Kohomban and Wee Sun Lee. 2005. Learning
Semantic Classes for Word Sense Disambiguation. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 34?41, Ann
Arbor, MI.
Anna Korhonen, Yuval Krymolowski, and Zvika Marx.
2003. Clustering Polysemic Subcategorization Frame
Distributions Semantically. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 64?71, Sapporo, Japan.
Anna Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, Computer Lab-
oratory. Technical Report UCAM-CL-TR-530.
Karim Lari and Steve J. Young. 1990. The Estimation of
Stochastic Context-Free Grammars using the Inside-
Outside Algorithm. Computer Speech and Language,
4:35?56.
Hang Li and Naoki Abe. 1998. Generalizing Case
Frames Using a Thesaurus and the MDL Principle.
Computational Linguistics, 24(2):217?244.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
Verb Classification Based on Statistical Distributions
of Argument Structure. Computational Linguistics,
27(3):373?408.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional Clustering of English Words. In Pro-
ceedings of the 31st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 183?190,
Columbus, OH.
Detlef Prescher, Stefan Riezler, and Mats Rooth. 2000.
Using a Probabilistic Class-Based Lexicon for Lexical
Ambiguity Resolution. In Proceedings of the 18th In-
ternational Conference on Computational Linguistics.
Philip Resnik. 1997. Selectional Preference and Sense
Disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How?, Washington, DC.
Jorma Rissanen. 1978. Modeling by Shortest Data De-
scription. Automatica, 14:465?471.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a Semantically
Annotated Lexicon via EM-Based Clustering. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics, Maryland, MD.
Sabine Schulte im Walde. 2006. Experiments on the Au-
tomatic Induction of German Semantic Verb Classes.
Computational Linguistics, 32(2):159?194.
Eric V. Siegel and Kathleen R. McKeown. 2000.
Learning Methods to Combine Linguistic Indica-
tors: Improving Aspectual Classification and Reveal-
ing Linguistic Insights. Computational Linguistics,
26(4):595?628.
Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-
drew Y. Ng. 2007. Learning to Merge Word Senses.
In Proceedings of the joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, Prague, Czech
Republic.
Naftali Tishby, Fernando Pereira, and William Bialek.
1999. The Information Bottleneck Method. In Pro-
ceedings of the 37th Annual Conference on Communi-
cation, Control, and Computing, Monticello, IL.
504
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 91?95,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
A Graph-Theoretic Algorithm for Automatic Extension of Translation
Lexicons
Beate Dorow Florian Laws Lukas Michelbacher Christian Scheible Jason Utt
Institute for Natural Language Processing
Universita?t Stuttgart
{dorowbe,lawsfn,michells,scheibcn,uttjn}@ims.uni-stuttgart.de
Abstract
This paper presents a graph-theoretic
approach to the identification of yet-
unknown word translations. The proposed
algorithm is based on the recursive Sim-
Rank algorithm and relies on the intuition
that two words are similar if they estab-
lish similar grammatical relationships with
similar other words. We also present a for-
mulation of SimRank in matrix form and
extensions for edge weights, edge labels
and multiple graphs.
1 Introduction
This paper describes a cross-linguistic experiment
which attempts to extend a given translation dic-
tionary with translations of novel words.
In our experiment, we use an English and
a German text corpus and represent each cor-
pus as a graph whose nodes are words and
whose edges represent grammatical relationships
between words. The corpora need not be parallel.
Our intuition is that a node in the English and a
node in the German graph are similar (that is, are
likely to be translations of one another), if their
neighboring nodes are. Figure 1 shows part of the
English and the German word graph.
Many of the (first and higher order) neighbors
of food and Lebensmittel translate to one another
(marked by dotted lines), indicating that food and
Lebensmittel, too, are likely mutual translations.
Our hypothesis yields a recursive algorithm for
computing node similarities based on the simi-
larities of the nodes they are connected to. We
initialize the node similarities using an English-
German dictionary whose entries correspond to
known pairs of equivalent nodes (words). These
node equivalences constitute the ?seeds? from
which novel English-German node (word) corre-
spondences are bootstrapped.
We are not aware of any previous work using a
measure of similarity between nodes in graphs for
cross-lingual lexicon acquisition.
Our approach is appealing in that it is language
independent, easily implemented and visualized,
and readily generalized to other types of data.
Section 2 is dedicated to related research on
the automatic extension of translation lexicons. In
Section 3 we review SimRank (Jeh and Widom,
2002), an algorithm for computing similarities of
nodes in a graph, which forms the basis of our
work. We provide a formulation of SimRank in
terms of simple matrix operations which allows
an efficient implementation using optimized ma-
trix packages. We further present a generalization
of SimRank to edge-weighted and edge-labeled
graphs and to inter-graph node comparison.
Section 4 describes the process used for build-
ing the word graphs. Section 5 presents an experi-
ment for evaluating our approach to bilingual lex-
icon acquisition. Section 6 reports the results. We
present our conclusions and directions for future
research in Section 7.
2 Related Work on cross-lingual lexical
acquisition
The work by Rapp (1999) is driven by the idea
that a word and its translation to another lan-
guage are likely to co-occur with similar words.
Given a German and an English corpus, he com-
putes two word-by-word co-occurrence matrices,
one for each language, whose columns span a vec-
tor space representing the corresponding corpus.
In order to find the English translation of a Ger-
man word, he uses a base dictionary to translate
all known column labels to English. This yields
a new vector representation of the German word
in the English vector space. This mapped vector
is then compared to all English word vectors, the
most similar ones being candidate translations.
91
food Lebensmittel
receive erhalten
award Preis
provide liefern
evidence Beweis
buy kaufen
book Buch
publish verlegen
boat Haus
waste ablehnen
Figure 1: Likely translations based on neighboring nodes
Rapp reports an accuracy of 72% for a small
number of test words with well-defined meaning.
Diab and Finch (2000) first compute word sim-
ilarities within each language corpus separately
by comparing their co-occurrence vectors. Their
challenge then is to derive a mapping from one
language to the other (i.e. a translation lexicon)
which best preserves the intra-language word sim-
ilarities. The mapping is initialized with a few seed
?translations? (punctuation marks) which are as-
sumed to be common to both corpora.
They test their method on two corpora written
in the same language and report accuracy rates of
over 90% on this pseudo-translation task. The ap-
proach is attractive in that it does not require a
seed lexicon. A drawback is its high computational
cost.
Koehn and Knight (2002) use a (linear) com-
bination of clues for bootstrapping an English-
German noun translation dictionary. In addition to
similar assumptions as above, they consider words
to be likely translations of one another if they have
the same or similar spelling and/or occur with sim-
ilar frequencies. Koehn and Knight reach an accu-
racy of 39% on a test set consisting of the 1,000
most frequent English and German nouns. The
experiment excludes verbs whose semantics are
more complex than those of nouns.
Otero and Campos (2005) extract English-
Spanish pairs of lexico-syntactic patterns from a
small parallel corpus. They then construct con-
text vectors for all English and Spanish words by
recording their frequency of occurrence in each of
these patterns. English and Spanish vectors thus
reside in the same vector space and are readily
compared.
The approach reaches an accuracy of 89% on a
test set consisting of 100 randomly chosen words
from among those with a frequency of 100 or
higher. The authors do not report results for low-
frequency words.
3 The SimRank algorithm
An algorithm for computing similarities of nodes
in graphs is the SimRank algorithm (Jeh and
Widom, 2002). It was originally proposed for di-
rected unweighted graphs of web pages (nodes)
and hyperlinks (links).
The idea of SimRank is to recursively com-
pute node similarity scores based on the scores
of neighboring nodes. The similarity Sij of two
different nodes i and j in a graph is defined as
the normalized sum of the pairwise similarities of
their neighbors:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Skl. (1)
N(i) and N(j) are the set of i?s and j?s neigh-
bors respectively, and c is a multiplicative factor
smaller than but close to 1 which demotes the con-
tribution of higher order neighbors. Sij is set to 1
if i and j are identical, which provides a basis for
the recursion.
3.1 Matrix formulation of SimRank
We derive a formulation of the SimRank similarity
updates which merely consists of matrix multipli-
cations as follows. In terms of the graph?s (binary)
adjacency matrix A, the SimRank recursion reads:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Aik Ajl Skl
(2)
noting that AikAjl = 1, iff k is a neighbor of i
and l is a neighbor of j at the same time. This is
92
equivalent to
Sij = c
?
k,l
Aik
|N(i)|
Ajl
|N(j)| Skl (3)
= c
?
k,l
Aik
?
? Ai?
Ajl
?
? Aj?
Skl.
The Sij can be assembled in a square node sim-
ilarity matrix S, and it is easy to see that the indi-
vidual similarity updates can be summarized as:
Sk = c A? Sk?1A?T (4)
where A? is the row-normalized adjacency matrix
and k denotes the current level of recursion. A? is
obtained by dividing each entry of A by the sum of
the entries in its row. The SimRank iteration is ini-
tialized with S = I , and the diagonal of S, which
contains the node self-similarities, is reset to ones
after each iteration.
This representation of SimRank in closed ma-
trix form allows the use of optimized off-the-shelf
sparse matrix packages for the implementation of
the algorithm. This rendered the pruning strate-
gies proposed in the original paper unnecessary.
We also note that the Bipartite SimRank algorithm
introduced in (Jeh and Widom, 2002) is just a spe-
cial case of Equation 4.
3.2 Extension with weights and link types
The SimRank algorithm assumes an unweighted
graph, i.e. a binary adjacency matrix A. Equa-
tion 4 can equally be used to compute similarities
in a weighted graph by letting A? be the graph?s
row-normalized weighted adjacency matrix. The
entries of A? then represent transition probabili-
ties between nodes rather than hard (binary) adja-
cency. The proof of the existence and uniqueness
of a solution to this more general recursion pro-
ceeds in analogy to the proof given in the original
paper.
Furthermore, we allow the links in the graph to
be of different types and define the following gen-
eralized SimRank recursion, where T is the set of
link types and Nt(i) denotes the set of nodes con-
nected to node i via a link of type t.
Sij =
c
|T |
?
t?T
1
|Nt(i)| |Nt(j)|
?
k?Nt(i),l?Nt(j)
Skl.
(5)
In matrix formulation:
Sk =
c
|T |
?
t?T
A?t Sk?1A?t
T (6)
where At is the adjacency matrix associated with
link type t and, again, may be weighted.
3.3 SimRank across graphs
SimRank was originally designed for the com-
parison of nodes within a single graph. However,
SimRank is readily and accordingly applied to
the comparison of nodes of two different graphs.
The original SimRank algorithm starts off with the
nodes? self-similarities which propagate to other
non-identical pairs of nodes. In the case of two dif-
ferent graphs A and B, we can instead initialize the
algorithm with a set of initially known node-node
correspondences.
The original SimRank equation (2) then be-
comes
Sij =
c
|N(i)| |N(j)|
?
k,l
Aik Bjl Skl, (7)
which is equivalent to
Sk = c A? Sk?1 B?T , (8)
or, if links are typed,
Sk =
c
|T |
?
t?T
A?t Sk?1 B?t
T . (9)
The similarity matrix S is now a rectangular
matrix containing the similarities between nodes
in A and nodes in B. Those entries of S which
correspond to known node-node correspondences
are reset to 1 after each iteration.
4 The graph model
The grammatical relationships were extracted
from the British National Corpus (BNC) (100 mil-
lion words), and the Huge German Corpus (HGC)
(180 million words of newspaper text). We com-
piled a list of English verb-object (V-O) pairs
based on the verb-argument information extracted
by (Schulte im Walde, 1998) from the BNC. The
German V-O pairs were extracted from a syntactic
analysis of the HGC carried out using the BitPar
parser (Schmid, 2004).
We used only V-O pairs because they consti-
tute far more sense-discriminative contexts than,
for example, verb-subject pairs, but we plan to ex-
amine these and other grammatical relationships
in future work.
We reduced English compound nouns to their
heads and lemmatized all data. In English phrasal
93
English German
Low Mid High Low Mid High
N V N V N V N V N V N V
0.313 0.228 0.253 0.288 0.253 0.255 0.232 0.247 0.205 0.237 0.211 0.205
Table 1: The 12 categories of test words, with mean relative ranks of test words
verbs, we attach the particles to the verbs to dis-
tinguish them from the original verb (e.g put off
vs. put). Both the English and German V-O pairs
were filtered using stop lists consisting of modal
and auxiliary verbs as well as pronouns. To reduce
noise, we decided to keep only those relationships
which occurred at least three times in the respec-
tive corpus.
The English and German data alike are then rep-
resented as a bipartite graph whose nodes divide
into two sets, verbs and nouns, and whose edges
are the V-O relationships which connect verbs to
nouns (cf. Figure 1). The edges of the graph are
weighted by frequency of occurrence.
We ?prune? both the English and German graph
by recursively removing all leaf nodes (nodes with
a single neighbor). As these correspond to words
which appear only in a single relationship, there is
only limited evidence of their meaning.
After pruning, there are 4,926 nodes (3,365
nouns, 1,561 verbs) and 43,762 links in the En-
glish, and 3,074 nodes (2,207 nouns, 867 verbs)
and 15,386 links in the German word graph.
5 Evaluation experiment
The aim of our evaluation experiment is to test
the extended SimRank algorithm for its ability to
identify novel word translations given the English
and German word graph of the previous section
and an English-German seed lexicon. We use the
dict.cc English-German dictionary 1.
Our evaluation strategy is as follows. We se-
lect a set of test words at random from among the
words listed in the dictionary, and remove their en-
tries from the dictionary. We run six iterations of
SimRank using the remaining dictionary entries
as the seed translations (the known node equiv-
alences), and record the similarities of each test
word to its known translations. As in the original
SimRank paper, c is set to 0.8.
We include both English and German test words
and let them vary in frequency: high- (> 100),
1http://www.dict.cc/ (May 5th 2008)
mid- (> 20 and ? 100), and low- (? 20) fre-
quent as well as word class (noun, verb). Thus, we
obtain 12 categories of test words (summarized in
Table 1), each of which is filled with 50 randomly
selected words, giving a total of 600 test words.
SimRank returns a matrix of English-German
node-node similarities. Given a test word, we ex-
tract its row from the similarity matrix and sort the
corresponding words by their similarities to the
test word. We then scan this sorted list of words
and their similarities for the test word?s reference
translations (those listed in the original dictionary)
and record their positions (i.e. ranks) in this list.
We then replace absolute ranks with relative ranks
by dividing by the total number of candidate trans-
lations.
6 Results
Table 1 lists the mean relative rank of the reference
translations for each of the test categories. The
values of around 0.2-0.3 clearly indicate that our
approach ranks the reference translations much
higher than a random process would.
Relative rank
Fr
eq
ue
nc
y
0.0 0.2 0.4 0.6 0.8 1.0
0
5
15
25
Figure 2: Distribution of the relative ranks of the
reference translations in the English-High-N test
set.
Exemplary of all test sets, Figure 2 shows the
distribution of the relative ranks of the reference
translations for the test words in English-High-N.
The bulk of the distribution lies below 0.3, i.e. in
the top 30% of the candidate list.
In order to give the reader an idea of the results,
we present some examples of test words and their
94
Test word Top 10 predicted translations Ranks
sanction Ausgangssperre Wirtschaftssanktion
Ausnahmezustand Embargo Moratorium
Sanktion Todesurteil Geldstrafe Bu?geld
Anmeldung
Sanktion(6)
Ma?nahme(1407)
delay anfechten revidieren zuru?ckstellen
fu?llen verku?nden quittieren vertagen
verschieben aufheben respektieren
verzo?gern(78)
aufhalten(712)
Kosten hallmark trouser blouse makup uniform
armour robe testimony witness jumper
cost(285)
o?ffnen unlock lock usher step peer shut guard
hurry slam close
open(12)
undo(481)
Table 2: Some examples of test words, their pre-
dicted translations, and the ranks of their true
translations.
predicted translations in Table 2.
Most of the 10 top-ranked candidate transla-
tions of sanction are hyponyms of the correct
translations. This is mainly due to insufficient
noun compound analysis. Both the English and
German nouns in our graph model are single
words. Whereas the English nouns consist only of
head nouns, the German nouns include many com-
pounds (as they are written without spaces), and
thus tend to be more specific.
Some of the top candidate translations of de-
lay are correct (verschieben) or at least acceptable
(vertagen), but do not count as such as they are
missing in the gold standard dictionary.
The mistranslation of the German noun Kosten
is due to semantic ambiguity. Kosten co-occurs of-
ten with the verb tragen as in to bear costs. The
verb tragen however is ambiguous and may as
well be translated as to wear which is strongly as-
sociated with clothes.
We find several antonyms of o?ffnen among its
top predicted translations. Verb-object relation-
ships alone do not suffice to distinguish synonyms
from antonyms. Similarly, it is extremely difficult
to differentiate between the members of closed
categories (e.g. the days of the week, months of
the year, mass and time units) using only syntactic
relationships.
7 Conclusions and Future Research
The matrix formulation of the SimRank algorithm
given in this paper allows an implementation using
efficient off-the-shelf software libraries for matrix
computation.
We presented an extension of the SimRank
algorithm to edge-weighted and edge-labeled
graphs. We further generalized the SimRank equa-
tions to permit the comparison of nodes from two
different graphs, and proposed an application to
bilingual lexicon induction.
Our system is not yet accurate enough to be
used for actual compilation of translation dictio-
naries. We further need to address the problem of
data sparsity. In particular, we need to remove the
bias towards low-degree words whose similarities
to other words are unduly high.
In order to solve the problem of ambiguity, we
intend to apply SimRank to the incidence repre-
sentation of the word graphs, which is constructed
by putting a node on each link. The proposed al-
gorithm will then naturally return similarities be-
tween the more sense-discriminative links (syn-
tactic relationships) in addition to similarities be-
tween the often ambiguous nodes (isolated words).
References
M. Diab and S. Finch. 2000. A statistical word-
level translation model for comparable corpora. In
In Proceedings of the Conference on Content-Based
Multimedia Information Access (RIAO).
G. Jeh and J. Widom. 2002. Simrank: A measure of
structural-context similarity. In KDD ?02: Proceed-
ings of the eighth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 538?543.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
of the ACL-02 Workshop on Unsupervised Lexical
Acquisition, pages 9?16.
P. Gamallo Otero and J. Ramon Pichel Campos. 2005.
An approach to acquire word translations from non-
parallel texts. In EPIA, pages 600?610.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics on Com-
putational Linguistics, pages 519?526.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING ?04: Proceedings of the 20th International
Conference on Computational Linguistics, page 162.
Sabine Schulte im Walde. 1998. Automatic Se-
mantic Classification of Verbs According to Their
Alternation Behaviour. Master?s thesis, Insti-
tut fu?r Maschinelle Sprachverarbeitung, Universita?t
Stuttgart.
95
Coling 2010: Poster Volume, pages 614?622,
Beijing, August 2010
A Linguistically Grounded Graph Model for Bilingual Lexicon
Extraction
Florian Laws, Lukas Michelbacher, Beate Dorow, Christian Scheible,
Ulrich Heid, Hinrich Schu?tze
Institute for Natural Language Processing
Universita?t Stuttgart
{lawsfn,michells,dorowbe}@ims.uni-stuttgart.de
Abstract
We present a new method, based on
graph theory, for bilingual lexicon ex-
traction without relying on resources with
limited availability like parallel corpora.
The graphs we use represent linguis-
tic relations between words such as ad-
jectival modification. We experiment
with a number of ways of combining
different linguistic relations and present
a novel method, multi-edge extraction
(MEE), that is both modular and scalable.
We evaluate MEE on adjectives, verbs
and nouns and show that it is superior
to cooccurrence-based extraction (which
does not use linguistic analysis). Finally,
we publish a reproducible baseline to es-
tablish an evaluation benchmark for bilin-
gual lexicon extraction.
1 Introduction
Machine-readable translation dictionaries are an
important resource for bilingual tasks like ma-
chine translation and cross-language information
retrieval. A common approach to obtaining bilin-
gual translation dictionaries is bilingual lexicon
extraction from corpora. Most work has used
parallel text for this task. However, parallel cor-
pora are only available for few language pairs and
for a small selection of domains (e.g., politics).
For other language pairs and domains, monolin-
gual comparable corpora and monolingual lan-
guage processing tools may be more easily avail-
able. This has prompted researchers to investigate
bilingual lexicon extraction based on monolingual
corpora (see Section 2) .
In this paper, we present a new graph-theoretic
method for bilingual lexicon extraction. Two
monolingual graphs are constructed based on syn-
tactic analysis, with words as nodes and relations
(such as adjectival modification) as edges. Each
relation acts as a similarity source for the node
types involved. All available similarity sources
interact to produce one final similarity value for
each pair of nodes. Using a seed lexicon, nodes
from the two graphs can be compared to find a
translation.
Our main contributions in this paper are: (i) we
present a new method, based on graph theory,
for bilingual lexicon extraction without relying
on resources with limited availability like paral-
lel corpora; (ii) we show that with this graph-
theoretic framework, information obtained by lin-
guistic analysis is superior to cooccurrence data
obtained without linguistic analysis; (iii) we ex-
periment with a number of ways of combining dif-
ferent linguistic relations in extraction and present
a novel method, multi-edge extraction, which is
both modular and scalable; (iv) progress in bilin-
gual lexicon extraction has been hampered by the
lack of a common benchmark; we therefore pub-
lish a benchmark and the performance of MEE as
a baseline for future research.
The paper discusses related work in Section 2.
We then describe our translation model (Sec-
tion 3) and multi-edge extraction (Section 4). The
benchmark we publish as part of this paper is de-
scribed in Section 5. Section 6 presents our ex-
perimental results and Section 7 analyzes and dis-
cusses them. Section 8 summarizes.
2 Related Work
Rapp (1999) uses word cooccurrence in a vector
space model for bilingual lexicon extraction. De-
tails are given in Section 5.
Fung and Yee (1998) also use a vector space
approach, but use TF/IDF values in the vector
components and experiment with different vec-
tor similarity measures for ranking the translation
candidates. Koehn and Knight (2002) combine
614
a vector-space approach with other clues such as
orthographic similarity and frequency. They re-
port an accuracy of .39 on the 1000 most frequent
English-German noun translation pairs.
Garera et al (2009) use a vector space model
with dependency links as dimensions instead of
cooccurring words. They report outperforming
a cooccurrence vector model by 16 percentage
points accuracy on English-Spanish.
Haghighi et al (2008) use a probabilistic model
over word feature vectors containing cooccur-
rence and orthographic features. They then use
canonical correlation analysis to find matchings
between words in a common latent space. They
evaluate on multiple languages and report high
precision even without a seed lexicon.
Most previous work has used vector spaces and
(except for Garera et al (2009)) cooccurrence
data. Our approach uses linguistic relations like
subcategorization, modification and coordination
in a graph-based model. Further, we evaluate our
approach on different parts of speech, whereas
some previous work only evaluates on nouns.
3 Translation Model
Our model has two components: (i) a graph repre-
senting words and the relationships between them
and (ii) a measure of similarity between words
based on these relationships. Translation is re-
garded as cross-lingual word similarity. We rank
words according to their similarity and choose the
top word as the translation.
We employ undirected graphs with typed nodes
and edges. Node types represent parts of speech
(POS); edge types represent different kinds of re-
lations. We use a modified version of SimRank
(Jeh and Widom, 2002) as a similarity measure
for our experiments (see Section 4 for details).
SimRank is based on the idea that two nodes
are similar if their neighbors are similar. We ap-
ply this notion of similarity across two graphs. We
think of two words as translations if they appear
in the same relations with other words that are
translations of each other. Figure 1 illustrates this
idea with verbs and nouns in the direct object rela-
tion. Double lines indicate seed translations, i.e.,
known translations from a dictionary (see Sec-
tion 5). The nodes buy and kaufen have the same
house
magazine
book
thought
buy
read
Haus
Zeitschrift
Buch
Gedanke
kaufen
lesen
Figure 1: Similarity through seed translations
objects in the two languages; one of these (maga-
zine ? Zeitschrift) is a seed translation. This re-
lationship contributes to the similarity of buy ?
kaufen. Furthermore, book and Buch are similar
(because of read ? lesen) and this similarity will
be added to buy ? kaufen in a later iteration. By
repeatedly applying the algorithm, the initial sim-
ilarity introduced by seeds spreads to all nodes.
To incorporate more detailed linguistic infor-
mation, we introduce typed edges in addition to
typed nodes. Each edge type represents a linguis-
tic relation such as verb subcategorization or ad-
jectival modification. By designing a model that
combines multiple edge types, we can compute
the similarity between two words based on mul-
tiple sources of similarity. We superimpose dif-
ferent sets of edges on a fixed set of nodes; a node
is not necessarily part of every relation.
The graph model can accommodate any kind of
nodes and relations. In this paper we use nodes
to represent content words (i.e., non-function
words): adjectives (a), nouns (n) and verbs (v).
We extracted three types of syntactic relations
from a corpus: see Table 1.
Nouns participate in two bipartite relations
(amod, dobj) and one unipartite relation (ncrd).
This means that the computation of noun similar-
ities will benefit from three different sources.
Figure 2 depicts a sample graph with all node
and edge types. For the sake of simplicity, a
monolingual example is shown. There are four
nouns in the sample graph all of which are (i)
modified by the adjectives interesting and polit-
ical and (ii) direct objects of the verbs like and
615
relation entities description example
used in this paper
amod a, n adjectival modification a fast car
dobj v, n object subcategorization drive a car
ncrd n, n noun coordination cars and busses
other possible relations
vsub v, n subject subcategorization a man sleeps
poss n, n possessive the child?s toy
acrd a, a adjective coordination red or blue car
Table 1: Relations used in this paper (top) and
possible extensions (bottom).
dobj
amod
ncrd
verb
adjective
noun
like promote
idea
article book
magazine
interesting political
Figure 2: Graph snippet with typed edges
promote. Based on amod and dobj, the four nouns
are equally similar to each other. However, the
greater similarity of article, book, and magazine
to each other can be deduced from the fact that
these three nouns also occur in the relation ncrd.
We exploit this information in the MEE method.
Data and Preprocessing. Our corpus in this
paper is the Wikipedia. We parse all German
and English articles with BitPar (Schmid, 2004)
to extract verb-argument relations. We extract
adjective-noun modification and noun coordina-
tions with part-of-speech patterns based on a
version of the corpus tagged with TreeTagger
(Schmid, 1994). We use lemmas instead of sur-
face forms. Because we perform the SimRank
matrix multiplications in memory, we need to fil-
ter out rare words and relations; otherwise, run-
ning SimRank to convergence would not be feasi-
ble. For adjective-noun pairs, we apply a filter on
pair frequency (? 3). We process noun pairs by
applying a frequency threshold on words (? 100)
and pairs (? 3). Verb-object pairs (the smallest
data set) were not frequency-filtered. Based on
the resulting frequency counts, we calculate asso-
ciation scores for all relationships using the log-
likelihood measure (Dunning, 1993). For noun
pairs, we discard all pairs with an association
score < 3.84 (significance at ? = .05). For all
three relations, we discard pairs whose observed
frequency was smaller than their expected fre-
quency (Evert, 2004, p. 76). As a last step,
we further reduce noise by removing nodes of de-
gree 1. Key statistics for the resulting graphs are
given in Table 2.
We have found that accuracy of extraction is
poor if unweighted edges are used. Using the
log-likelihood score directly as edge weight gives
too much weight to ?semantically weak? high-
frequency words like put and take. We there-
fore use the logarithms of the log-likelihood score
as edge weights in all SimRank computations re-
ported in this paper.
nodes n a v
de 34,545 10,067 2,828
en 22,257 12,878 4,866
edges ncrd amod dobj
de 65,299 417,151 143,906
en 288,889 686,073 510,351
Table 2: Node and edge statistics
4 SimRank
Our work is based on the SimRank graph similar-
ity algorithm (Jeh and Widom, 2002). In (Dorow
et al, 2009), we proposed a formulation of Sim-
Rank in terms of matrix operations, which can be
applied to (i) weighted graphs and (ii) bilingual
problems. We now briefly review SimRank and
its bilingual extension. For more details we refer
to (Dorow et al, 2009).
The basic idea of SimRank is to consider two
nodes as similar if they have similar neighbor-
hoods. Node similarity scores are recursively
computed from the scores of neighboring nodes:
the similarity Sij of two nodes i and j is computed
616
as the normalized sum of the pairwise similarities
of their neighbors:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Skl.
where N(i) and N(j) are the sets of i?s and j?s
neighbors. As the basis of the recursion, Sij is set
to 1 if i and j are identical (self-similarity). The
constant c (0 < c < 1) dampens the contribution
of nodes further away. Following Jeh and Widom
(2002), we use c = 0.8. This calculation is re-
peated until, after a few iterations, the similarity
values converge.
For bilingual problems, we adapt SimRank for
comparison of nodes across two graphs A and B.
In this case, i is a node in A and j is a node in B,
and the recursion basis is changed to S(i, j) = 1 if
i and j are a pair in a predefined set of node-node
equivalences (seed translation pairs).
Sij =
c
|NA(i)| |NB(j)|
?
k?NA(i),l?NB(j)
Skl.
Multi-edge Extraction (MEE) Algorithm To
combine different information sources, corre-
sponding to edges of different types, in one Sim-
Rank computation, we use multi-edge extrac-
tion (MEE), a variant of SimRank (Dorow et al,
2009). It computes an aggregate similarity matrix
after each iteration by taking the average similar-
ity value over all edge types T :
Sij =
c
|T |
?
t?T
1
f(|NA,t(i)|)f(|NB,t(j)|)
?
k?NA,t(i),
l?NB,t(j)
Skl.
f is a normalization function (either f = g,
g(n) = n as before or the normalization discussed
in the next section).
While we have only reviewed the case of un-
weighted graphs, the extended SimRank can also
be applied to weighted graphs. (See (Dorow et
al., 2009) for details.) In what follows, all graph
computations are weighted.
Square Root Normalization Preliminary ex-
periments showed that SimRank gave too much
influence to words with few neighbors. We there-
fore modified the normalization function g(n) =
n. To favor words with more neighbors, we want
f to grow sublinearly with the number of neigh-
bors. On the other hand, it is important that,
even for nodes with a large number of neigh-
bors, the normalization term is not much smaller
than |N(i)|, otherwise the similarity computation
does not converge. We use the function h(n) =?n?
?
maxk(|N(k)|). h grows quickly for small
node degrees, while returning values close to the
linear term for large node degrees. This guaran-
tees that nodes with small degrees have less influ-
ence on final similarity scores. In all experiments
reported in this paper, the matrices A?, B? are nor-
malized with f = h (rather than using the stan-
dard normalization f = g). In one experiment,
accuracy of the top-ranked candidate (acc@1) was
.52 for h and .03 for g, demonstrating that the
standard normalization does not work in our ap-
plication.
Threshold Sieving For larger experiments,
there is a limit to scalability, as the similarity ma-
trix fills up with many small entries, which take up
a large amount of memory. Since these small en-
tries contribute little to the final result, Lizorkin et
al. (2008) proposed threshold sieving: an approxi-
mation of SimRank using less space by deleting
all similarity values that are below a threshold.
The quality of the approximation is set by a pa-
rameter ? that specifies maximum acceptable dif-
ference of threshold-sieved similarity and the ex-
act solution. We adapted this to the matrix formu-
lation by integrating the thresholding step into a
standard sparse matrix multiplication algorithm.
We verified that this approximation yields use-
ful results by comparing the ranks of exact and ap-
proximate solutions. We found that for the high-
ranked words that are of interest in our task, siev-
ing with a suitable threshold does not negatively
affect results.
5 Benchmark Data Set
Rapp?s (1999) original experiment was carried out
on newswire corpora and a proprietary Collins
dictionary. We use the free German (280M to-
kens) and English (850M tokens) Wikipedias as
source and target corpora. Reinhard Rapp has
generously provided us with his 100 word test set
617
n a v
training set .61 .31 .08
TS100 .65 .28 .07
TS1000 .66 .14 .20
Table 3: Percentages of POS in test and training
(TS100) and given us permission to redistribute
it. Additionally, we constructed a larger test set
(TS1000) consisting of the 1000 most frequent
words from the English Wikipedia. Unlike the
noun-only test sets used in other studies, (e.g.,
Koehn and Knight (2002), Haghighi et al (2008)),
TS1000 also contains adjectives and verbs. As
seed translations, we use a subset of the dict.cc
online dictionary. For the creation of the sub-
set we took raw word frequencies from Wikipedia
as a basis. We extracted all verb, noun and ad-
jective translation pairs from the original dictio-
nary and kept the pairs whose components were
among the 5,000 most frequent nouns, the 3,500
most frequent adjectives and the 500 most fre-
quent verbs for each language. These numbers are
based on percentages of the different node types
in the graphs. The resulting dictionary contains
12,630 pairs: 7,767 noun, 3,913 adjective and 950
verb pairs. Table 3 shows the POS composition of
the training set and the two test sets. For experi-
ments evaluated on TS100 (resp. TS1000), the set
of 100 (resp. 1000) English words it contains and
all their German translations are removed from the
seed dictionary.
Baseline. Our baseline is a reimplementation
of the vector-space method of Rapp (1999). Each
word in the source corpus is represented as a word
vector, the dimensions of which are words of seed
translation pairs. The same is done for corpus
words in the target language, using the translated
seed words as dimensions. The value of each di-
mension is determined by association statistics of
word cooccurrence. For a test word, a vector is
constructed in the same way. The labels on the
dimensions are then translated, yielding an input
vector in the target language vector space. We
then find the closest corpus word vector in the tar-
get language vector space using the city block dis-
tance measure. This word is taken as the transla-
tion of the test word.
We went to great lengths to implement Rapp?s
method, but omit the details for reasons of space.
Using the Wikipedia/dict.cc-based data set, we
achieve 50% acc@1 when translating words from
English to German. While this is somewhat lower
than the performance reported by Rapp, we be-
lieve this is due to Wikipedia being more hetero-
geneous and less comparable than news corpora
from identical time periods used by Rapp.
Publication. In conjunction with this paper we
publish the benchmark for bilingual lexicon ex-
traction described. It consists of (i) two Wikipedia
dumps from October 2008 and the linguistic re-
lations extracted from them, (ii) scripts to recre-
ate the training and test sets from the dict.cc
data base, (iii) the TS100 and TS1000 test sets,
and (iv) performance numbers of Rapp?s system
and MEE. These can serve as baselines for fu-
ture work. Note that (ii)?(iv) can be used in-
dependently of (i) ? but in that case the effect
of the corpus on performance would not be con-
trolled. The data and scripts are available at
http://ifnlp.org/wiki/extern/WordGraph
6 Results
In addition to the vector space baseline experi-
ment described above, we conducted experiments
with the SimRank model. Because TS100 only
contains one translation per word, but words can
have more than one valid translation, we manu-
ally extended the test set with other translations,
which we verified using dict.cc and leo.org. We
report the results separately for the original test set
(?strict?) and the extended test set in Table 4. We
also experimented with single-edge models con-
sisting of three separate runs on each relation.
The accuracy columns report the percentage of
test cases where the correct translation was found
among the top 1 (acc@1) or top 10 (acc@10)
candidate words found by the translation mod-
els. Some test words are not present in the data at
all; we count these as 0s when computing acc@1
and acc@10. The acc@10 measure is more use-
ful for indicating topical similarity while acc@1
measures translation accuracy.
MRR is Mean Reciprocal Rank of correct trans-
lations: 1n
?n
i
1
ranki (Voorhees and Tice, 1999).
MRR is a more fine-grained measure than acc@n,
618
TS100, strict TS100, extended TS1000
acc@1 acc@10 MRR acc@1 acc@10 MRR acc@1 acc@10 MRR
baseline .50 .67 .56 .54 .70 .60 .33 .56 .41
single .44 .67 .52 .49 .68 .56 .40? .70? .50
MEE .52 .79? .62 .58 .82? .68 .48? .76? .58
Table 4: Results compared to baseline?
e.g., it will distinguish ranks 2 and 10. All MRR
numbers reported in this paper are consistent with
acc@1/acc@10 and support our conclusions.
The results for acc@1, the measure that most
directly corresponds to utility in lexicon extrac-
tion, show that the SimRank-based models out-
perform the vector space baseline ? only slightly
on TS100, but significantly on TS1000. Using the
various relations separately (single) already yields
a significant improvement compared to the base-
line. Using all relations in the integrated MEE
model further improves accuracy. With an acc@1
score of 0.48, MEE outperforms the baseline by
.15 compared to TS1000. This shows that a com-
bination of several sources of information is very
valuable for finding the correct translation.
MEE outperforms the baseline on TS1000 for
all parts of speech, but performs especially well
compared to the baseline for adjectives and verbs
(see Table 5). It has been suggested that vector
space models perform best for nouns and poorly
for other parts of speech. Our experiments seem to
confirm this. In contrast, MEE exhibits good per-
formance for nouns and adjectives and a marked
improvement for verbs.
On acc@10, MEE is consistently better than the
baseline, on both TS100 and TS1000. All three
differences are statistically significant.
6.1 Relation Comparison
Table 5 compares baseline, single-edge and MEE
accuracy for the three parts of speech covered.
Each single-edge experiment can compute noun
similarity; for adjectives and verbs, only amod,
dobj and MEE can be used.
Performance for nouns varies greatly depend-
ing on the relation used in the model. ncrd per-
?We indicate statistical significance at the ? = 0.05 (?)
and 0.01 level (?) when compared to the baseline. We did
not calculate significance for MRR.
forms best, while dobj shows the worst perfor-
mance. We hypothesize that dobj performs badly
because (i) many verbs are semantically non-
restrictive with respect to their arguments, (e.g.,
use, contain or include) and as a result seman-
tically unrelated nouns become similar because
they share the same verb as a neighbor; (ii) light
verb constructions (e.g., take a walk or give an ac-
count) dilute the extracted relations; and (iii) dobj
is the only relation we extracted with a syntac-
tic parser. The parser was trained on newswire
text, a genre that is very different from Wikipedia.
Hence, parsing is less robust than the relatively
straightforward POS patterns used for the other
relations.
Similarly, many semantically non-restrictive
adjectives such as first and new can modify vir-
tually any noun, diluting the quality of the amod
source. We conjecture that ncrd exhibits the best
performance because there are fewer semantically
non-restrictive nouns than non-restrictive adjec-
tives and verbs.
MEE performance for nouns (.45) is signifi-
cantly better than that of the single-edge models.
The information about nouns that is contained in
the verb-object and adjective-noun data is inte-
grated in the model and helps select better trans-
lations. This, however, is only true for the noun
noun adj verb all
TS100 baseline .55 .43 .29 .50
amod .15 .71 - .30
ncrd .34 - - .22
dobj .02 - .43 .04
MEE .45 .71 .43 .52
TS1000 baseline .42 .26 .18 .33
MEE .53 .55 .27 .48
Table 5: Relation comparison, acc@1
619
source acc@1 acc@10
dobj .02 .10
amod .15 .37
amod+dobj .22 .43
ncrd+dobj .32 .65
ncrd .34 .60
ncrd+amod .49 .74
MEE .45 .77
Table 6: Accuracy of sources for nouns
node type, the ?pivot? node type that takes part in
edges of all three types. For adjectives and verbs,
the performance of MEE is the same as that of the
corresponding single-edge model.
We ran three additional experiments each of
which combines only two of the three possible
sources for noun similarity, namely ncrd+amod,
ncrd+dobj and amod+dobj and performed strict
evaluation (see Table 6). We found that in gen-
eral combination increases performance except
for ncrd+dobj vs. ncrd. We attribute this to the
lack of robustness of dobj mentioned above.
6.2 Comparison MEE vs. All-in-one
An alternative to MEE is to use untyped edges in
one large graph. In this all-in-one model (AIO),
we connect two nodes with an edge if they are
linked by any of the different linguistic relations.
While MEE consists of small adjacency matrices
for each type, the two adjacency matrices for AIO
are much larger. This leads to a much denser sim-
ilarity matrix taking up considerably more mem-
ory. One reason for this is that AIO contains simi-
larity entries between words of different parts of
speech that are 0 (and require no memory in a
sparse matrix representation) in MEE.
Since AIO requires more memory, we had to
filter the data much more strictly than before to be
able to run an experiment. We applied the follow-
ing stricter thresholds on relationships to obtain
a small graph: 5 instead of 3 for adjective-noun
MEEsmall AIOsmall
acc@1 .51 .52
acc@10 .72 .75
MRR .62 .59
Table 7: MEE vs. AIO
pairs, and 3 instead of 0 for verb-object pairs,
thereby reducing the total number of edges from
2.1M to 1.4M. We also applied threshold sieving
(see Section 4) with ? = 10?10 for AIO. The re-
sults on TS100 (strict evaluation) are reported in
Table 7. For comparison, MEE was also run on
the smaller graph. Performance of the two models
is very similar, with AIO being slightly better (not
significant). The slight improvement does not jus-
tify the increased memory requirements. MEE is
able to scale to more nodes and edge types, which
allows for better coverage and performance.
7 Analysis and Discussion
Error analysis. We examined the cases where a
reference translation was not at the top of the sug-
gested list of translation candidates. There are a
number of elements in the translation process that
can cause or contribute to this behavior.
Our method sometimes picks a cohyponym of
the correct translation. In many of these cases, the
correct translation is in the top 10 (together with
other words from the same semantic field). For
example, the correct translation of moon, Mond, is
second in a list of words belonging to the semantic
field of celestial phenomena: Komet (comet), Mond
(moon), Planet (planet), Asteroid (asteroid), Stern (star),
Galaxis (galaxy), Sonne (sun), . . . While this behavior
is undesirable for strict lexicon extraction, it can
be exploited for other tasks, e.g. cross-lingual se-
mantic relatedness (Michelbacher et al, 2010).
Similarly, the method sometimes puts the
antonym of the correct translation in first place.
For example, the translation for swift (schnell) is
in second place behind langsam (slow). Based
on the syntactic relations we use, it is difficult to
discriminate between antonyms and semantically
similar words if their syntactic distributions are
similar.
Ambiguous source words also pose a problem
for the system. The correct translation of square
(the geometric shape) is Quadrat. However, 8 out
of its top 10 translation candidates are related to
the location sense of square. The other two are ge-
ometric shapes, Quadrat being listed second. This
is only a concern for strict evaluation, since cor-
rect translations of a different sense were included
in the extended test set.
620
bed is also ambiguous (piece of furniture vs.
river bed). This introduces translation candidates
from the geographical domain. As an additional
source of errors, a number of bed?s neighbors
from the furniture sense have the German transla-
tion Bank which is ambiguous between the furni-
ture sense and the financial sense. This ambiguity
in the target language German introduces spurious
translation candidates from the financial domain.
Discussion. The error analysis demonstrates
that most of the erroneous translations are words
that are incorrect, but that are related, in some ob-
vious way, to the correct translation, e.g. by co-
hyponymy or antonymy. This suggests another
application for bilingual lexicon extraction. One
of the main challenges facing statistical machine
translation (SMT) today is that it is difficult to
distinguish between minor errors (e.g., incorrect
word order) and major errors that are completely
implausible and undermine the users? confidence
in the machine translation system. For example,
at some point Google translated ?sarkozy sarkozy
sarkozy? into ?Blair defends Bush?. Since bilin-
gual lexicon extraction, when it makes mistakes,
extracts closely related words that a human user
can understand, automatically extracted lexicons
could be used to discriminate smaller errors from
grave errors in SMT.
As we discussed earlier, parallel text is not
available in sufficient quantity or for all impor-
tant genres for many language pairs. The method
we have described here can be used in such cases,
provided that large monolingual corpora and ba-
sic linguistic processing tools (e.g. POS tagging)
are available. The availability of parsers is a more
stringent constraint, but our results suggest that
more basic NLP methods may be sufficient for
bilingual lexicon extraction.
In this work, we have used a set of seed trans-
lations (unlike e.g., Haghighi et al (2008)). We
believe that in most real-world scenarios, when
accuracy and reliability are important, seed lexica
will be available. In fact, seed translations can be
easily found for many language pairs on the web.
Although a purely unsupervised approach is per-
haps more interesting from an algorithmic point
of view, the semisupervised approach taken in this
paper may be more realistic for applications.
In this paper, we have attempted to reimplement
Rapp?s system as a baseline, but have otherwise
refrained from detailed comparison with previous
work as far as the accuracy of results is concerned.
The reason is that none of the results published so
far are easily reproducible. While previous publi-
cations have tried to infer from differences in per-
formance numbers that one system is better than
another, these comparisons have to be viewed with
caution since neither the corpora nor the gold stan-
dard translations are the same. For example, the
paper by Haghighi et al (2008) (which demon-
strates how orthography and contextual informa-
tion can be successfully used) reports 61.7% ac-
curacy on the 186 most confident predictions of
nouns. But since the evaluation data sets are not
publicly available it is difficult to compare other
work (including our own) with this baseline. We
simply do not know how methods published so far
stack up against each other.
For this reason, we believe that a benchmark
is necessary to make progress in the area of bilin-
gual lexicon extraction; and that our publication of
such a benchmark as part of the research reported
here is an important contribution, in addition to
the linguistically grounded extraction and the new
graph-theoretical method we present.
8 Summary
We have presented a new method, based on graph
theory, for bilingual lexicon extraction without re-
lying on resources with limited availability like
parallel corpora. We have shown that with this
graph-theoretic framework, information obtained
by linguistic analysis is superior to cooccurrence
data obtained without linguistic analysis. We have
presented multi-edge extraction (MEE), a scalable
graph algorithm that combines different linguis-
tic relations in a modular way. Finally, progress
in bilingual lexicon extraction has been hampered
by the lack of a common benchmark. We publish
such a benchmark with this paper and the perfor-
mance of MEE as a baseline for future research.
9 Acknowledgement
This research was funded by the German Re-
search Foundation (DFG) within the project A
graph-theoretic approach to lexicon acquisition.
621
References
Dorow, Beate, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In EACL 2009 Workshop on Geo-
metrical Models of Natural Language Semantics.
Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Evert, Stefan. 2004. The Statistics of Word Cooccur-
rences - Word Pairs and Collocations. Ph.D. thesis,
Institut fu?r maschinelle Sprachverarbeitung (IMS),
Universita?t Stuttgart.
Fung, Pascale and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts. In COLING-ACL, pages 414?
420.
Garera, Nikesh, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon
induction from monolingual corpora via depen-
dency contexts and part-of-speech equivalences. In
CoNLL ?09: Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 129?137, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June.
Association for Computational Linguistics.
Jeh, Glen and Jennifer Widom. 2002. Simrank: A
measure of structural-context similarity. In KDD
?02, pages 538?543.
Koehn, Philipp and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition, pages 9?16.
Lizorkin, Dmitry, Pavel Velikhov, Maxim N. Grinev,
and Denis Turdakov. 2008. Accuracy estimate and
optimization techniques for simrank computation.
PVLDB, 1(1):422?433.
Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-
rich Heid, and Hinrich Schu?tze. 2010. Building
a cross-lingual relatedness thesaurus using a graph
similarity measure. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta, may.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In COLING 1999.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Schmid, Helmut. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In COLING ?04, page 162.
Voorhees, Ellen M. and Dawn M. Tice. 1999. The
TREC-8 question answering track evaluation. In
Proceedings of the 8th Text Retrieval Conference.
622
Coling 2010: Poster Volume, pages 1104?1112,
Beijing, August 2010
Sentiment Translation through Multi-Edge Graphs
Christian Scheible, Florian Laws, Lukas Michelbacher, and Hinrich Schu?tze
Institute for Natural Language Processing
University of Stuttgart
{scheibcn, lawsfn, michells}@ims.uni-stuttgart.de
Abstract
Sentiment analysis systems can benefit
from the translation of sentiment informa-
tion. We present a novel, graph-based ap-
proach using SimRank, a well-established
graph-theoretic algorithm, to transfer sen-
timent information from a source lan-
guage to a target language. We evaluate
this method in comparison with semantic
orientation using pointwise mutual infor-
mation (SO-PMI), an established unsuper-
vised method for learning the sentiment of
phrases.
1 Introduction
Sentiment analysis is an important topic in com-
putational linguistics that is of theoretical interest
but is also useful in many practical applications.
Usually, two aspects are of importance in senti-
ment analysis. The first is the detection of sub-
jectivity, i.e., whether a text or an expression is
meant to express sentiment at all; the second is the
determination of sentiment orientation, i.e., what
sentiment is to be expressed in a structure that is
considered subjective.
Work on sentiment analysis most often cov-
ers resources or analysis methods in a single lan-
guage, usually English. However, the transfer
of sentiment analysis between languages can be
advantageous by making use of resources for a
source language to improve the analysis of the tar-
get language.
This paper presents an approach to the transfer
of sentiment information between two languages
that does not rely on resources with limited avail-
ability like parallel corpora. It is built around Sim-
Rank, a graph similarity algorithm that has suc-
cessfully been applied to the acquisition of bilin-
gual lexicons (Laws et al, 2010) and semantic
similarity (Michelbacher et al, 2010). It uses
linguistic relations extracted from two monolin-
gual corpora to determine the similarity of words
in different languages. One of the main benefits
of our method is its ability to handle sparse data
about the relations between the languages well
(i.e., a small seed lexicon). Further, we experi-
ment with combining multiple types of linguistic
relations for graph-based translation. Our exper-
iments are carried out using English as a source
language and German as a target language. We
evaluate our method using a hand-annotated set of
German adjectives which we intend to publish.
In the following section, related work is dis-
cussed. Section 3.1 gives an introduction to Sim-
Rank and its application to lexicon induction,
while section 3.2 reviews SO-PMI (Turney, 2002),
an unsupervised baseline method for the genera-
tion of sentiment lexicons. In section 4, we define
our sentiment transfer method which we apply in
experiments in section 5.
2 Related Work
Mihalcea et al (2007) propose two methods for
translating sentiment lexicons. The first method
simply uses bilingual dictionaries to translate an
English sentiment lexicon. A sentence-based clas-
sifier built with this list achieved high precision,
but low recall on a small Romanian test set. The
second method is based on parallel corpora. The
source language in the corpus is annotated with
sentiment information, and the information is then
projected to the target language. Problems arise
due to mistranslations.
Banea et al (2008) use machine translation for
multilingual sentiment analysis. Given a corpus
annotated with sentiment information in one lan-
guage, machine translation is used to produce an
annotated corpus in the target language, by pre-
serving the annotations. The original annotations
1104
can be produced either manually or automatically.
Wan (2009) constructs a multilingual classi-
fier using co-training. In co-training, one classi-
fier produces additional training data for a second
classifier. In this case, an English classifier assists
in training a Chinese classifier.
The induction of a sentiment lexicon is the sub-
ject of early work by Hatzivassiloglou and McK-
eown (1997). They construct graphs from coordi-
nation data from large corpora based on the intu-
ition that adjectives with the same sentiment ori-
entation are likely to be coordinated. For example,
fresh and delicious is more likely than rotten and
delicious. They then apply a graph clustering al-
gorithm to find groups of adjectives with the same
orientation. Finally, they assign the same label to
all adjectives that belong to the same cluster.
Corpus work and bilingual dictionaries are
promising resources for translating sentiment. In
contrast to previous approaches, the work pre-
sented in this paper uses corpora that are not an-
notated with sentiment.
Turney (2002) suggests a corpus-based extrac-
tion method based on his pointwise mutual infor-
mation (PMI) synonymy measure. He assumes
that the sentiment orientation of a phrase can be
determined by comparing its pointwise mutual in-
formation with a positive (excellent) and a nega-
tive phrase (poor). An introduction to this method
is given in Section 3.2.
3 Background
3.1 Lexicon Induction via SimRank
We use the extension of the SimRank (Jeh and
Widom, 2002) node similarity algorithm proposed
by Dorow et al (2009). Given two graphs A and
B, the similarity between two nodes a in A and b
in B is computed in each iteration as:
S(a, b) = c
|NA(a)||NB(b)|
?
k?NA(a),l?NB(b)
S(k, l).
NX(x) is the neighborhood of node x in graph
X . To compute similarities between two graphs,
some initial links between these graphs have to be
given, called seed links. These form the recursion
basis which sets S(a, b) = 1 if there is a seed
link between a and b. At the beginning of each
iteration, all known equivalences between nodes
are reset to 1.
Multi-Edge Extraction (MEE). MEE is an ex-
tension of SimRank that, in each iteration, com-
putes the average node-node similarity of several
different SimRank matrices. In our case, we use
two different SimRank matrices, one for coordi-
nations and one for adjective modification. See
(Dorow et al, 2009) for details. We also used
the node degree normalization function h(n) =?n ??maxk(|N(k)|) (where n is the node de-
gree, and N(k) the degree of node k) to decrease
the harmful effect of high-degree nodes on final
similarity values. See (Laws et al, 2010) for de-
tails.
3.2 SO-PMI
Semantic orientation using pointwise mutual in-
formation (SO-PMI) (Turney, 2002) is an algo-
rithm for the unsupervised learning of semantic
orientation of words or phrases. A word has pos-
itive (resp. negative) orientation if it is associ-
ated with positive (resp. negative) terms more
frequently than with negative (resp. positive)
terms. Association of terms is measured using
their pointwise mutual information (PMI) which
is defined for two words w1 and w2 as follows:
PMI(w1, w2) = log
( p(w1, w2)
p(w1)p(w2)
)
Using PMI, Turney defines SO-PMI for a word
w as
SO-PMI(w) =
log
?
p?P hits(word NEAR p)?
?
n?N hits(n)?
n?N hits(word NEAR n)?
?
p?P hits(p)
hits is a function that returns the number of hits
in a search engine given the query. P is a set of
known positive words, N a set of known negative
words, and NEAR an operator of a search engine
that returns documents in which the operands oc-
cur within a close range of each other.
1105
4 Sentiment Translation
Unsupervised methods like SO-PMI are suitable
to acquire basic sentiment information in a lan-
guage. However, since hand-annotated resources
for sentiment analysis exist in other languages,
it seems plausible to use automatic translation of
sentiment information to leverage these resources.
In order to translate sentiment, we will use multi-
ple sources of information that we represent in a
MEE graph as given in Section 3.1.
In our first experiments (Scheible, 2010), coor-
dinated adjectives were used as the sole training
source. Two adjectives are coordinated if they are
linked with a conjunction like and or but. The
intuition behind using coordinations ? based on
work by Hatzivassiloglou and McKeown (1997)
and Widdows and Dorow (2002) ? was that words
which are coordinated share properties. In partic-
ular, coordinated adjectives usually express sim-
ilar sentiments even though there are exceptions
(e.g., ?The movie was both good and bad?).
In this paper, we focus on using multiple edge
types for sentiment translation. In particular, the
graph we will use contains two types of relations,
coordinations and adjective-noun modification. In
the sentence ?The movie was enjoyable and fun?,
enjoyable and fun are coordinated. In This is an
enjoyable movie, the adjective enjoyable modifies
the noun movie.
We selected these two relation types for two
reasons. First, the two types provide clues for
sentiment analysis. Coordination information is
an established source for sentiment similarity (e.g.
Hatzivassiloglou and McKeown (1997)) while
adjective-noun relations provide a different type
of information on sentiment. For example, nouns
with positive associations (vacation) tend to occur
with positive adjectives and nouns with negative
associations (pain) tend to occur with negative ad-
jectives. Second, we have successfully used these
two types for a similar acquisition task, the acqui-
sition of word-to-word translation pairs (Laws et
al., 2010).
In the resulting graph, adjectives and nouns are
represented as nodes, each containing a word and
its part of speech, and relations are represented as
links which are distinguished by their edge types.
Two graphs, one in the source language and one in
the target language, are needed to translate words
between those languages. Figure 1 shows an ex-
ample for such a setup. Black links in this graph
are coordinations, grey links are seed relations.
In order to calculate sentiment for all nodes in
the target language, we apply the SimRank algo-
rithm to the graphs which gives us similarities be-
tween all nodes in the source graph and all nodes
in the target graph. Using the similarity S(ns, nt)
between a node ns in the source language graph
S and a node nt in the target language graph T ,
the sentiment score (sent(nt)) is the similarity-
weighted average of all sentiment scores in the
target language:
sent(nt) =
?
ns?S
simnorm(ns, nt) sent(ns)
We assume that sentiment scores in the source
language are expressed on a numeric scale. The
normalized similarity simnorm is defined as
simnorm(ns, nt) = S(ns, nt)?
ns?S S(ns, nt)
.
The normalization assures that all resulting sen-
timent values are within [?1, 1], with ?1 being
the most negative sentiment and 1 the most posi-
tive.
5 Experiments
5.1 Data Acquisition
For our experiments, we needed coordination data
to build weighted graphs and a bilingual lexi-
con to define seed relations between those graphs.
Coordinations were extracted from the English
and German versions of Wikipedia1 by applying
pattern-based search using the Corpus Query Pro-
cessor (CQP) (Christ et al, 1999). We annotated
both corpora with parts of speech using the Tree
Tagger (Schmid, 1994). A total of 477,291 En-
glish coordinations and 112,738 German coordi-
nations were collected. A sample of this data is
given in Figure 2. We restrict these experiments
to the use of and/und since other coordinations
1http://www.wikipedia.org/ (01/19/2009)
1106
affordable
delicious
nutritiousjuicy
tasty
healthylovely
schmackhaft
gesundstrange
frisch
wertvoll
nahrhaft angesehen
ertragreich
Figure 1: A German and an English graph with coordinated adjectives including seed links
affordable
delicious
diverse
popularnutritious
inexpensive
original
varied
melodious
rare
strange
juicy
tasty
exotic healthy
tempting
lovely
hearty fragrant
dangerous
beautiful
charming authentic
Figure 2: English sample coordinations (adjectives)
1107
behave differently and might even express dissim-
ilarity (e.g. Was the weather good or bad?).
The seed lexicon was constructed from the
dict.cc dictionary2. While the complete dictionary
contains 30,551 adjective pairs, we reduced the
number of pairs used in the experiments to 1,576.
To produce a smaller seed lexicon which still
makes sense from a semantic point of view, we
used the General Service List (GSL) (West, 1953)
which contains about 2000 words the author con-
sidered central to the English language. More
specifically, a revised list was used3.
SO-PMI needs a larger amount of training data.
Since Wikipedia does not satisfy this need, we
collected additional coordination data from the
web using search result counts from Google. In
Turney?s original paper, he uses the NEAR oper-
ator, which returns documents that contain two
search terms that are within a certain distance of
each other, to collect collocations. Unfortunately,
Google does not support this operator, so instead,
we searched for coordinations using the queries
+ "w and s" and
+ "w und s"
for English and German, respectively. We added
the quotes and the + operator to make sure that
both spelling correction and synonym replace-
ments were disabled.
The original experiments were made for En-
glish, so we had to construct our own set of
seed words. For German, we chose gut (good),
nett (nice), richtig (right), scho?n (beautiful), or-
dentlich (neat), angenehm (pleasant), aufrichtig
(honest), gewissenhaft (faithful), and hervorra-
gend (excellent) as positive seed words, and
schlecht (bad), teuer (expensive), falsch (wrong),
bo?se (evil), feindlich (hostile), verhasst (invidi-
ous), widerlich (disgusting), fehlerhaft (faulty),
and mangelhaft (flawed) as negative ones.
5.2 Sentiment Lexicon
For our experiments, we used two different polar-
ity lexicons. The lexicon of Wilson et al (2005)
contains sentiment annotations for 8,221 words
2http://www.dict.cc
3http://jbauman.com/aboutgsl.html
annotation value
positive 1.0
weakpos 0.5
neutral 0.0
weakneg ?0.5
negative ?1.0
Table 1: Assigned values for Wilson et al set
which are tagged as positive, neutral, or nega-
tive. A few words are tagged as weakneg, imply-
ing weak negativity. These categorial annotations
are mapped to the range [-1,1] using the assign-
ment scheme given in Table 1.
5.3 Human Ratings
In order to manually annotate a test set, we
chose 200 German adjectives that occurred in the
Wikipedia corpus and that were part of a coor-
dination. From these words, we removed those
which we deemed uncommon, too complicated,
or which were mislabeled as adjectives by the tag-
ger. The test set contained 150 adjectives of which
seven were excluded after annotators discarded
them.
We asked 9 native speakers of German to anno-
tate the adjectives. Possible annotations were very
positive, slightly positive, neutral, slightly nega-
tive, or very negative. These categories are the
same as the ones used in the training data.
In order to capture the general sentiment, i.e.,
sentiment that is not related to a specific context,
the judges were asked to stay objective and not
let their personal opinions influence the annota-
tion. However, some words with strong political
implications were annotated by some judges as
non-neutral which led to disagreement beyond the
usual level. Nuklear (nuclear) is an example for
such a word. We measured the agreement of the
judges with Kendall?s coefficient of concordance
(W ) with tie correction (Legendre, 2005), yield-
ing W = 0.674 with a high level of significance
(p < .001); thus, inter-annotator agreement was
high (Landis and Koch, 1977).
5.4 Experimental Setup
Given the relations extracted from Wikipedia, we
built a German and an English graph by setting
1108
Method r
MEE 0.63
MEE-GSL 0.47
SR 0.63
SR-GSL 0.48
SO-PMI 0.58
Table 2: Correlation with human ratings
the weight of each link to the log-likelihood ra-
tio of the two words it connects according to the
corpus frequencies. There are two properties of
the graph transfer algorithm that we intend to in-
vestigate. First, we are interested in the merits of
applying multi edge extraction (MEE) for senti-
ment transfer. Second, we are interested in how
the transfer quality changes when the seed lexi-
con is reduced in size. This way, a sparse data
situation is simulated where large dictionaries are
unavailable. Having these two properties in mind,
four possible setups are evaluated: (i) using the
full seed lexicon with all 30,551 entries, but using
only coordination data (SR), (ii) reducing the seed
lexicon to 1,576 entries from the General Service
List (SR-GSL), (iii) applying MEE by adding ad-
jective modification data (MEE), and (iv) using
MEE with a reduced seed lexicon (MEE-GSL).
SimRank was run for 6 iterations in all experi-
ments. All experiments use the weight function
h as described above. We show that this function
improves similarities and thus lexicon induction
in Laws et al (2010).
Correlation. First, we will examine the correla-
tion between the automatic methods (SO-PMI and
the aforementioned SimRank variations) and the
gold standard as done by Turney in his evaluation.
For this purpose, the human ratings are mapped
to float values following Table 1 and the aver-
age rating over all judges for each word is used.
The correlation coefficients r are given in Table 2.
Judging from these results, the ordering of SR and
MEE matches the human ratings better than SO-
PMI, however it decreases when using any of the
GSL variations instead which can be attributed to
using less data.
Classification. The correct identification of the
classes positive, neutral, and negative is more im-
portant than the correct assignment of values on
a scale since the rank ordering is debatable ? this
becomes apparent when measuring the agreement
of human annotators. Since the assignments made
by the human judges are not unanimous in most
cases, the averages are distributed across the in-
terval [-1,1]; this means that the borders between
the three distinct categories are not clear. Since
there is no standard evaluation for this particu-
lar problem, we need to devise a way to make
the range of the neutral category dynamic. In or-
der to find possible borders, we first assume that
sentiment is distributed symmetrically around 0.
We then define a threshold x which assumes the
values x ? { i20 |0 ? i ? 20}, covering the in-terval [0,0.5]. Since 0.5 is slightly positive, we
do not believe that values above it are plausible.
Then, each word w is positive if its human rating
scoreh(w) ? x, negative if scoreh(w) ? ?x, and
neutral if ?x < scoreh(w) < x. The result of
this process is a gold standard for the three cate-
gories for each of the values for x. The percentiles
of the sizes of those categories are mapped to the
values produced by the automatic methods. For
example, if x = 0.35 means that the top 21% of
all adjectives are in the positive class, the top 21%
of all adjectives as assigned by SO-PMI and the
SimRank varieties are positive as well.
The size of the neutral category increases the
larger x becomes. Thus, high values for x are
unlikely to produce a correct partitioning of the
data. Since slightly positive was defined as 0.5,
we expect the highest plausible value for x to be
below that. The size of the neutral category for
each value of x is given in Table 3. (Recall that
the total size of the set is 143.)
We can then compute the assignment accu-
racy on the positive, neutral, and negative classes,
as well macro- and micro-averages over these
classes.
5.5 Results and Discussion
Figures 3 and 4 show the macro- and micro-
averaged accuracies over the positive, negative,
and neutral class for each automatic method, re-
spectively. Overall, the SimRank variations per-
form better for x in the interval [0, 0.3]. In partic-
ular, MEE has a slightly higher accuracy than SR,
1109
x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50
# neutral 0 13 35 46 56 64 74 82 92 99 99
Table 3: Size of neutral category given x
word (translation) humans SO MEE MEE-GSL SR SR-GSL
chemisch (chemical) 0.00 -20.20 0.185 0.185 0.186 0.184
auferstanden (resurrected) 0.39 -10.96 -0.075 -0.577 -0.057 -0.493
intelligent (intelligent) 0.94 46.59 0.915 0.939 0.834 0.876
versiert (skilled) 0.67 -5.26 0.953 0.447 0.902 0.404
mean -0.04 -9.58 0.003 0.146 0.010 0.142
median 0.00 -15.60 0.110 0.157 0.114 0.157
Table 4: Example adjectives including translation, and their scores
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5
A
c
c
u
r
a
c
y
x
SO-PMI (macro)MEE (macro)MEE-GSL (macro)SR (macro)SR-GSL (macro)
Figure 3: Macro-averaged Accuracy
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5
A
c
c
u
r
a
c
y
x
SO-PMI (micro)MEE (micro)MEE-GSL (micro)SR (micro)SR-GSL (micro)
Figure 4: Micro-averaged Accuracy
1110
however, not significantly.
Table 4 shows selected example words with
their scores. These values can be understood bet-
ter together with the means and medians of the
respective methods which are given in the table as
well. These values give us an idea of where we
might expect the neutral point of a particular dis-
tribution of polarities.
Chemisch (chemical) is misclassified by SO-
PMI since it occurs in negative contexts on the
web. SimRank in turn was able to recognize
that most words similar to chemisch are neutral,
the most similar one being its literal translation,
chemical. Auferstanden (resurrected) is an exam-
ple for misclassification by SimRank which hap-
pens because the word is usually coordinated with
words that have negative sentiment, e.g. gestor-
ben (deceased) and gekreuzigt (crucified). This
problem could not be fixed by including adjective-
noun modification data since the coordinations
produced high log-likelihood values which lead to
dead being the most similar word to auferstanden.
Intelligent receives a score close to neutral with
the original (coordination-only) training method,
which could be corrected by applying MEE sim-
ply because the ordering of similar words changes
through the new weighting method. Nouns modi-
fied by intelligent include Leben (life) and Wesen
(being) whose translations are modified by pos-
itive adjectives. Many words, such as versiert
(skilled) are classified more accurately due to the
new weighting method when compared to our pre-
vious experiments (Scheible, 2010) where it re-
ceived a SimRank polarity of only 0.224.
The inclusion of adjective modifications does
not improve the classification results as often as
we had hoped. For some cases (cf. intelligent
mentioned above), the scores do improve, but the
overall impact is limited.
6 Conclusion and Outlook
We were able to show that sentiment translation
with SimRank is able to classify adjectives more
accurately than SO-PMI, an unsupervised base-
line method. We demonstrated that SO-PMI is
outperformed by SimRank when choosing a rea-
sonable region of neutral adjectives. In addition,
we showed that the improvements of SimRank
lead to better accuracy in sentiment translation in
some cases. In future work, we will apply a senti-
ment lexicon generated with SimRank in a senti-
ment classification task for reviews.
The algorithms we compared are different in
their purpose of application. While SO-PMI is
applicable when large corpora are available for a
language, it fails when used in a sparse-data situ-
ation, as noted by Turney (2002). We showed that
despite reducing the seed lexicon for SimRank to
a small fraction of its original size, it still performs
better than SO-PMI.
Currently, our experiments are limited by the
choice of using adjectives for our test set. While
the examination of adjectives is highly important
for sentiment analysis (as shown by Pang et al
(2002) who were able to achieve high accuracy
even when using only adjectives), the application
of our algorithms to a broader set of linguistic
units is an important goal for future work.
Acknowledgments. We are grateful to
Deutsche Forschungsgemeinschaft for fund-
ing this research as part of the WordGraph
project.
References
Banea, Carmen, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Empirical
Methods in Natural Language Processing, pages
127?135.
Christ, O., B.M. Schulze, A. Hofmann, and E. Koenig.
1999. The IMS Corpus Workbench: Corpus Query
Processor (CQP): User?s Manual. University of
Stuttgart, March, 8:1999.
Dorow, Beate, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In Workshop on Geometrical Mod-
els of Natural Language Semantics, pages 91?95.
Hatzivassiloglou, Vasileios and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation of
adjectives. In Proceedings of the 35th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 174?181.
Jeh, Glen and Jennifer Widom. 2002. Simrank: A
measure of structural-context similarity. In Pro-
ceedings of the Eighth ACM SIGKDD Interna-
1111
tional Conference on Knowledge Discovery and
Data Mining, pages 538?543.
Landis, J.R. and G.G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33(1):159?174.
Laws, Florian, Lukas Michelbacher, Beate Dorow,
Christian Scheible, Ulrich Heid, and Hinrich
Schu?tze. 2010. A linguistically grounded graph
model for bilingual lexicon extraction. In Proceed-
ings of the 23nd International Conference on Com-
putational Linguistics.
Legendre, P. 2005. Species associations: the Kendall
coefficient of concordance revisited. Journal of
Agricultural Biological and Environment Statistics,
10(2):226?245.
Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-
rich Heid, and Hinrich Schu?tze. 2010. Building
a cross-lingual relatedness thesaurus using a graph
similarity measure. In Proceedings of the Seventh
Conference on International Language Resources
and Evaluation.
Mihalcea, Rada, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 976?983.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, pages 79?86.
Scheible, Christian. 2010. Sentiment translation
through lexicon induction. In Proceedings of the
ACL 2010 Student Research Workshop, Uppsala,
Sweden. Association for Computational Linguis-
tics.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Turney, Peter. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 417?424.
Wan, Xiaojun. 2009. Co-training for cross-lingual
sentiment classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 235?
243, Suntec, Singapore, August. Association for
Computational Linguistics.
West, Michael. 1953. A general service list of english
words.
Widdows, Dominic and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. InCOL-
ING.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 347?354, October.
1112
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 311?321, Dublin, Ireland, August 23-29 2014.
Picking the Amateur?s Mind ? Predicting Chess Player Strength from
Game Annotations
Christian Scheible
Institute for Natural Language Processing
University of Stuttgart, Germany
scheibcn@ims.uni-stuttgart.de
Hinrich Sch?utze
Center for Information
and Language Processing
University of Munich, Germany
Abstract
Results from psychology show a connection between a speaker?s expertise in a task and the lan-
guage he uses to talk about it. In this paper, we present an empirical study on using linguistic
evidence to predict the expertise of a speaker in a task: playing chess. Instructional chess litera-
ture claims that the mindsets of amateur and expert players differ fundamentally (Silman, 1999);
psychological science has empirically arrived at similar results (e.g., Pfau and Murphy (1988)).
We conduct experiments on automatically predicting chess player skill based on their natural lan-
guage game commentary. We make use of annotated chess games, in which players provide their
own interpretation of game in prose. Based on a dataset collected from an online chess forum,
we predict player strength through SVM classification and ranking. We show that using textual
and chess-specific features achieves both high classification accuracy and significant correlation.
Finally, we compare our findings to claims from the chess literature and results from psychology.
1 Introduction
It has been recognized that the language used when describing a certain topic or activity may differ
strongly depending on the speaker?s level of expertise. As shown in empirical experiments in psychology
(e.g., Solomon (1990), Pfau and Murphy (1988)), a speaker?s linguistic choices are influenced by the way
he thinks about the topic. While writer expertise has been addressed previously, we know of no work
that uses linguistic indicators to rank experts.
We present a study on predicting chess expertise from written commentary. Chess is a particularly
interesting task for predicting expertise: First, using data from competitive online chess, we can compare
and rank players within a well-defined ranking system. Second, we can collect textual data for experi-
mental evaluation from web resources, eliminating the need for manual annotation. Third, there is a large
amount of terminology associated with chess, which we can exploit for n-gram based classification.
Chess is difficult for humans because it requires long-term foresight (strategy) as well as the capacity
for internally simulating complicated move sequences (calculation and tactics). For these reasons, the
game for a long time remained challenging even for computers. Players have thus developed general
principles of chess strategy on which many expert players agree. The dominant expert view is that the
understanding of fundamental strategical notions, supplemented by the ability of calculation, is the most
important skill of a chess player. A good player develops a long-term plan for the course of the game.
This view is the foundation of many introductory works to chess (e.g., Capablanca (1921), one of the
earliest works).
Silman (1999) presents games he played with chess students, analyzing their commentary about the
progress of the game. He claims that players who fail to adhere to the aforementioned basic princi-
ples tend to perform worse and argues that the students? thought processes reflect their playing strength
directly. Lack of strategical understanding marks the difference between amateur and expert players.
Experts are mostly concerned with positional aspects, i.e., the optimal placement of pieces that offers a
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
311
8rm0ZkZ0s
7opo0lpa0
60Z0ZpZpo
5Z0Z0O0Z0
40Z0Z0ZbZ
3Z0MBZNZ0
2POPZ0OPO
1S0ZQZRJ0
a b c d e f g h
Figure 1: Example chess position, white to play
long-lasting advantage. Amateurs often have tactical aspects in mind, i.e., short-term attacking oppor-
tunities and exploits that potentially lead to loss of material for their opponents. A correlation between
chess strength and verbalization skills has been shown empirically by Pfau and Murphy (1988), who
used experts to assess the quality of the subjects? writing.
In this paper, we investigate the differences between the mindset of amateurs and experts expressed in
written game commentary, also referred to as annotated games. When studying chess, it is best practice
to review one?s own games to further one?s understanding of the game (Heisman, 1995). Students are
encouraged to annotate the games, i.e., writing down their thought process at each move. We address the
problem of predicting the player?s strength from the text of these annotations. Specifically, we want to
predict the rank of the player at the point when a given game was played. In competitive play, the rank
is determined through a numerical rating system ? such as the Elo rating system (Elo, 1978) used in this
paper ? that measures the players? relative strength using pairwise win expectations.
This paper makes the following contributions. First, we introduce a novel training dataset of games
annotated by the players themselves ? collected from online chess forum. We then formulate the task of
playing strength prediction. For each annotated game, each game viewed as a document, we predict the
rating class or overall rank of the player. We show that (i) an SVM model with n-gram features succeeds
at partitioning the players into two rating classes (above and below the mean rating); and (ii) that ranking
SVMs achieve significant correlation between the true and predicted ranking of the players. In addition,
we introduce novel chess-specific features that significantly improve the results. Finally, we compare the
predictions made by our model to claims from instructional chess literature and results from psychology
research.
We next give an overview of basic chess concepts (Section 2). Then, we introduce the dataset (Sec-
tion 3) and task (Section 4). We present our experimental results in Section 5. Section 6 contains an
overview of related work.
2 Basic Chess Concepts
2.1 Chess Terminology
We assume that the reader has basic familiarity with chess, its rules, and the value of individual pieces.
For clarity, we review some basic concepts of chess terminology, particularly elementary concepts related
to tactics and strategy in an example position (Figure 1).
1
From a positional point of view, white is ahead in development: all his minor pieces (bishops and
knights) have moved from their starting point while black?s knight remains on b8. White has also castled
(a move where the rook and king move simultaneously to get the king to a safer spot on either side of the
board) while black has not. White has a space advantage as he occupies the e5-square (which is in black?s
1
Modified from the game Dzindzichashvili ? Yermolinsky (1993) which is the first position discussed in (Silman, 1999)
312
1.e4 e5 2.Nf3 Nc6 3.Bc4 Nh6 4.Nc3 Bd6 Trying to follow basic opening principals, control center,
develop. etc 5.d3 Na5 6.Bb5 Moved bishop not wanting to trade, but realized after the move that my
bishop would be harassed by the pawn on c7 6...c6 7.Ba4 Moved bishop to safety, losing tempo 7...Qf6
8.Bg5 Qg6 9.O-O b5 Realized my bishop was done, might as well get 2 pawns 10.Nxb5 cxb5 11.Bxb5
Ng4 12.Nxe5 Flat out blunder, gave up a knight, at least I had a knight I could capture back 12...Bxe5
13.Qxg4 Bxb2 14.Rab1 Bd4 15.Rfe1 Moved rook to E file hoping to eventually attack the king. 15...h6
16.c3 Poor attempt to move the bishop, I realized it after I made the move 16...Bxc3 17.Rec1 Be5
18.d4 Another crappy attempt to move that bishop 18...Bxd4 19.Rd1 O-O 20.Rxd4 d6 21.Qd1 I don?t
remember why I made this move. 21...Qxg5 22.Rxd6 Bh3 23.Bf1 Protecting g2 23...Nc4 24.Rd5 Qg6
25.Rc1 Qxe4 26.f3 Qe3+ 27.Kh1 Nb2 28.Qc2 Rac8 29.Qe2 Qxc1 30.gxh3 Nc4 31.Qe4 Qxf1#
Figure 2: Example of an annotated game from the dataset (by user aevans410, rated 974)
half of the board) with a pawn. This pawn is potentially weak as it cannot easily be defended by another
pawn. Black has both of his bishops (the bishop pair) which is considered advantageous as bishops
are often superior to knights in open positions. Black?s light-square bishop is bad as it is obstructed by
black?s own pawns (although it is outside the pawn chain and thus flexible). Strategically, black might
want to improve the position of the light-square bishop, make use of his superior dark-square bishop,
and try to exploit the weak e5 pawn. Conversely, white should try create posts for his knights in black?s
territory. Tactically, white has an opportunity to move his knight to b5 (written Nb5 in algebraic chess
notation), from where it would attack the pawn on c7. If the knight could reach c7 (currently defended
by black?s queen), it would fork (double attack) black?s king and rook, which could lead to the trade of
the knight for the rook on the next move (which is referred to as winning the exchange). White?s knight
on f3 is pinned, i.e., the queen would be lost if the knight moved. Black can win a pawn by removing the
defender of e5, the knight on f3, by capturing it with the bishop.
This brief analysis of the position shows the complex theory and terminology that has developed
around chess. The paragraph also shows an example of game annotation (although not every move in the
game will be covered as elaborately in practice in amateur analyses).
2.2 Elo Rating System
Our goal in this paper is to predict the ranking of chess players based on their game annotations. We will
give a brief overview of the Elo system (Elo, 1978) that is commonly used to rank players. Each player
is assigned a score that is changed after each game depending on the expected and actual outcome. On
chess.com, a new player starts with an initial rating of 1200 (an arbitrary number chosen for historical
reasons, which has since become a wide-spread convention in chess). Assuming the current ratings R
a
and R
b
of two players a and b, the expected outcome of the game is defined as
E
a
=
1
1 + 10
?
R
a
?R
b
400
.
E
a
is then used to conduct a (weighted) update of R
a
and R
b
given the actual outcome of the game.
Thus, Elo ratings make pairwise adjustments to the scores. The differences between the ratings of two
players predict the probability of one winning against the other. However, the absolute ratings do not
carry any meaning by themselves.
3 Annotated Chess Game Data
For supervised training, we require a collection of chess games annotated by players of various strengths.
An annotated chess game is a sequence of chess moves with natural language text commentary associated
to specific moves. While many chess game collections are available, some of them containing millions of
games, the majority are unannotated. The small fraction of annotated games mostly features commentary
by masters rather than amateurs, which is not interesting for a contrastive study.
The game analysis forum on chess.com encourages players to post their annotated games for review
through the community. While several games are posted each day, we can only use a small subset of them.
313
Parameter Value
# games 182
# different players 130
mean # moves by game 42
mean # annotated moves by game 16
mean # words by game 114
Table 1: Dataset statistics
0 500 1000 1500 2000 2500
0
.
0
0
0
.
0
5
0
.
1
0
0
.
1
5
Rating (Elo)
%
 
P
l
a
y
e
r
s
? ? ? ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? ? ? ? ? ?
?
chess.com overall
our dataset
Figure 3: Rating distribution on chess.com and our dataset.
4
Each point shows the percentage of
players in a bin of width 50 around the value. Dotted line: Median on our dataset used for binning.
Many games are posted without annotations, instead soliciting annotation from the community. Others
are missing the rating of the player at the time the game was played ? the user profile shows only the
current rating for the player which may differ strongly from their historical one.
We first downloaded all available games from the forum archive. The games are stored in portable
game notation (PGN, Edwards (1994)). Next, we manually removed games where the annotation had
been conducted automatically by a chess program. We also removed games that had annotations at fewer
than three moves. The final dataset consists of 182 games with annotations in English and known player
rating.
2
We reproduce an example game from the data in Figure 2. This game is typical as the first
couple of moves are not commented (as opening moves are typically well-known). Then, the annotator
comments on select moves that he believes are key to the progress of the game. Table 1 shows some
statistics about the dataset.
The distribution of the ratings in our dataset is shown in Figure 3 in comparison to the overall standard
chess rating distribution on chess.com.
3
Elo ratings assume a normal distribution of players. We see
that overall, the distributions are quite similar, although we have a higher peak and our sample mean is
shifted towards higher ratings (1347 overall vs 1462 on our dataset). It is more common for mid-level
players to request annotation advice than it is for low-rated players (who might not know about this
practice) or high-rated players (who do not look for support by the lower-rated community).
The dataset is still somewhat noisy as players may obtain different ratings depending on the type of
venue (over-the-board tournament vs online chess) or the amount of time the players had available (time
control). Differences in these parameters lead to different rating distributions.
4
For this reason, the total
ordering given through the ratings may be difficult to predict. Thus, we will conduct experiments both
2
Available at http://www.ims.uni-stuttgart.de/data/chess
3
Data from http://www.chess.com/echess/players
4
cf. http://www.chess.com/article/view/chesscom-rating-comparisons
314
on ranking and on classification where the rating range is binned into two rating classes.
4 Predicting Chess Strength from Annotations
4.1 Classification and Ranking
The task addressed in this paper is prediction on the game level, i.e., predicting the strength of the player
of each game at the time when the game was played. We view a game as a document ? the concatenation
of the annotations at each move ? and extract feature vectors as described in Section 4.2. We pursue two
different machine learning approaches based on support vector machines (SVMs) to predicting chess
strength: classification and ranking.
The simplest way to approach the problem is classification. For this purpose, we divide the range of
observed rating into two evenly spaced rating classes at the median of the overall rating range (henceforth
amateur and expert). The classification view has obvious disadvantages. At the boundaries of the bins,
the distinction between them becomes difficult.
To predict a total ordering of all players, we use a ranking SVM (Herbrich et al., 1999). This model
casts ranking as learning a binary classification function that decides whether rank(x
1
) > rank(x
2
) over
all possible pairs of example feature vectors x
1
and x
2
with differing rank.
Note that since Elo ratings are continuous real numbers, it would be conceivable to fit a regression
model. However, Elo is designed as a pairwise ranking measure. While a relative difference in Elo
represents the probability of one player beating the other, the absolute Elo rating is not directly inter-
pretable.
5
4.2 Features
We extract unigrams (UG) and bigrams (BG) from the texts. In addition, we propose the following two
chess-specific feature sets derived from the text:
6
Notation (NOT). We introduce two indicators for whether the annotations contain certain types of
formal chess notation. The feature SQUARE is added if the annotation contains a reference to a specific
square on the chess board (e.g., d4). If the annotation contains a move in algebraic notation (e.g., Nxb4+,
meaning that a knight moved to b4, captured a piece there and put the enemy king in check), the feature
MOVE is added.
Similarity to master annotations (MS). This feature is intended to compensate for the lack of training
data. We used a master-annotated database consisting of 500 games annotated by chess masters which
is available online.
7
As we do not know the exact rating of the annotators, and to avoid strong class
imbalances, we cannot make use of the games directly through supervision. Instead, we calculate the
cosine similarity between the centroid
8
of the n-gram feature vectors of the master games and each game
in the chess.com dataset. The cosine similarity between each game and the master centroid is added
as a numerical feature.
Additionally, the master similarity scores can be used on their own to rank the games. This can be
viewed distant supervision as strength is learned from an external database. We will evaluate this ranking
in comparison with our trained models.
5 Experiments
This section, contains experimental results on classifying and ranking chess players. We first present
quantitative evaluation of the classification and ranking models and discuss the effect of chess-specific
5
Preliminary experiments with SVM regression showed little improvements over a baseline of assigning the mean rating to
all games. This suggests that the distribution of rankings is difficult to model ? possibly due to the low number of annotated
games on which the model can be trained.
6
We also tried using the length of the annotation as well as the number of annotated moves as a feature, which did not
contribute any improvements.
7
http://www.angelfire.com/games3/smartbridge/famous_games.zip
8
We also tried a k-NN approach where we computed the mean similarity of a game from our dataset to its k nearest
neighbors among the master games (k ? 1, 2, 5,?), but found that this approach performed worse.
315
Model Features F
(?)
1
F
(?)
1
F
(?)
1
1 Majority BL 67.2 0.0 33.6
2 SVM (linear) UG 73.4 71.6 72.5
3 SVM (linear) UG, BG 74.1 72.0 73.1
4 SVM (linear) UG, BG, NOT 75.7 74.9 75.3
5 SVM (linear) UG, BG, NOT, MS 74.2 73.0 73.6
(a) Results (F
1
in %)
1 2 3 4 5
1
2 **
3 **
4 ** ?
5 **
(b) Statistical significance of
differences in F
1
. **: p < 0.01,
*: p < 0.05, ?: p < 0.1
Table 2: Classification results
Class Features
Amateur (?) bishop, d4, opening, instead, trying, should, did, where, do, even, rook, get, good, he, coming, point i,
exchange, thought, did not, his, clock, too, or, on clock, knight for
Expert (?) this, game, can, will, winning, NOT:move, time, draw, because, white, back, black, mate, that, but, moves,
can?t, very, on, won, really, so, i know, now, only
Table 3: Top 25 features with most negative (amateur) and positive (expert) weights (mean over all folds)
in the best setup (UG, BG, NOT)
features. Second, we qualitatively compare the predictions of our models with findings and claims from
the literature about the connection between a player?s mindset and strength.
5.1 Experimental Setup
To generate feature vectors, we first concatenate all the annotations for a game, tokenize and lowercase
the texts, and remove punctuation as well as a small number of stopwords. We exclude rare words
to avoid overfitting: We remove all n-grams that occur fewer than 5 times, and add the chess-specific
features proposed above. Finally, we L
2
-normalize each vector.
We use linear SVMs from LIBLINEAR and SVMs with RBF kernel from LIBSVM (Chang and Lin,
2011). We run all experiments in a 10-fold cross-validation setup.
Wemeasure macro-averaged F
1
for our classification results. We evaluate the ranking model using two
measures: pairwise ranking accuracy (Acc
r
), i.e., the accuracy over the binary ranking decision for each
player pair; and Spearman?s rank correlation coefficient ? for the overall ranking. To test whether differ-
ences between results are statistical significant, we apply approximate randomization (Noreen, 1989) for
F
1
, and the test by Steiger (1980) for correlations, which is applicable to ?.
5.2 Classification
We first investigate the classification case, i.e., whether we can distinguish players below and above the
rating mean. Table 2 shows the results for this experiment. We show F
1
scores for the lower and higher
half of the players (F
(?)
1
and F
(?)
1
, respectively), and the macro average of these two scores (F
(?)
1
). We
first note that all SVM classifiers (lines 2?5) score significantly higher than the majority baseline (line 1).
When adding bigrams (line 3) and chess-specific notation features (line 4), F
1
increases. However, these
improvements are not statistically significant. The master similarity feature (line 5) leads to a drop in
F
1
from the previous line. The relatively low rank correlation between the master similarity scores and
the two classes (? = 0.334) leads to this effect. The low correlation itself may occur because the master
games were annotated by a third party (instead of the players), leading to strong differences in style.
There are several reasons for misclassification. Many errors occur in the dense region around the
class boundary. Also, shorter game annotations are more difficult to classify than longer ones. For
detailed error analysis, we first examine the most positively and negatively weighted features of the
trained models (Table 3). We will provide a more detailed look into the features in Section 5.4. We
316
Model Features Acc
r
? sig
1 MS (standalone) ? ? 0.279 ?
2 SVM (linear) UG 58.7 0.266 ?
3 SVM (linear) UG, BG 58.8 0.286 ?
4 SVM (linear) UG, BG, NOT 60.0 0.307 ?
5 SVM (linear) UG, BG, NOT, MS 59.8 0.310 ?
6 SVM (RBF) UG 64.0 0.389 ?
7 SVM (RBF) UG, BG 63.9 0.395 ?
8 SVM (RBF) UG, BG, NOT 63.8 0.400 ?
9 SVM (RBF) UG, BG, NOT, MS 63.5 0.397 ?
(a) Ranking results (accuracy in % and ?)
1 2 3 4 5 6 7 8 9
1
2
3
4 *
5 *
6 ? ?
7 ? * ?
8 ? * * ? ?
9 ? ? ?
(b) Statistical significance of differences in ?.
**: p < 0.01, *: p < 0.05, ?: p < 0.1
Table 4: Ranking results for standalone master similarity and SVM (linear and RBF kernel). Check in
sig column denote significance of correlation with true ranking (p < 0.05). Numbers in sigdiff column
denote a significant improvement (p < 0.05) in ? over the respective line.
find that there are noticeable differences in the writing styles of amateurs and experts. According to
the model, one of the most prominent distinctions is that amateurs tend to refer to the opponent as he,
whereas experts use white and black more frequently. However, it is of course not universally true, which
leads to the misclassification of some experts as amateurs. Another difference in style is that amateur
players tend to write about the game in the past tense. This is a manifestation of an important distinction:
Amateurs often state the obvious developments of the game (e.g., Flat out blunder, gave up a knight
in Figure 2) or speculate about options (e.g., hoping to eventually attack), while experts provide more
thorough positional analysis at key points.
5.3 Ranking
We now turn to ranking experiments (Table 4). We first evaluate the ranking produced by ordering the
games by their similarity to the master centroid (line 1). We find that the resulting rank correlation is low
but significant.
The results for the linear SVM ranker are shown in lines 2?5. Total ranking is considerably more diffi-
cult than binary classification of rating classes. Using a linear SVM, we again achieve low but significant
correlations. The linear classifiers (lines 2?5) do not significantly outperform the standalone master sim-
ilarity (MS) baseline (line 1). Chess-specific features (lines 4 and 5) boost the results, outperforming the
bigram models (line 3) significantly. The improvement from adding the MS centroid score feature is not
significant.
We again perform error analysis by examining the feature weights (Table 5). We find an overall picture
similar to the classification setup (cf. Table 3). The notation feature serves as a good indicator for the
upper rating range (cf. Table 3) as experienced players find it easier to express themselves through
notation. We observed that lower players tend to express moves in words (e.g., ?move my knight to d5?)
rather than through notation (Nd5), which could serve as an explanation for why pieces (bishop, knight,
rook) appear among the top features for amateur players.
However, some features change signs between the two experiments (e.g., king, square). This effect
may indicate that the binary ranking problem is not linearly separable, which is plausible; mid-rated
players may use terms that neither low-rated nor high-rated players use. Examining correlations at dif-
ferent ranking ranges confirms this suggestion. In top and bottom thirds of the rating scale, the true and
predicted ranks are not correlated significantly. This means that the ranking SVM only succeeds at rank-
ing players in middle third of the rating scale. To introduce non-linearity, we conduct further experiments
with an SVM with a radial basis function (RBF) kernel.
The results of this experiment are shown in lines 6?9 of Table 4. All RBF models perform better than
317
Class Features
Weaker instead, king, thinking, one my, fight, d4, even, should, should i, bishop, decided, did, i didn?t, opening, feel,
put, defense, knight on, black king, been, with my, where, get, cover, pin
Stronger NOT:move, moves, game, time, won, i know, already, will, stop, way, winning, line, can?t, can, black has, this,
MS, king side, computer, threaten, first, back, any way, my knight, win pawn, d
Table 5: Top 25 features with most negative (lower rating) and positive (higher rating) weights, mean
over all folds (rank(x
1
) > rank(x
2
) or vice versa) in the best ranking setup (linear SVM, UG, BG, NOT)
Feature Coefficient
capture -0.29
take -0.21
bishop -1.06
knight -0.19
rook -0.54
king 0.19
queen 0.08
pawn 0.44
pin -0.26
fork -0.27
Feature Coefficient
threat 0.13
danger 0.25
stop 0.50
weakness 0.34
light 0.21
dark 0.37
variation 0.41
winning 0.87
losing 0.08
like -0.16
hate -0.05
good -0.27
bad 0.52
Feature Coefficient
white 0.74
black 0.71
he -0.51
fight -0.17
know 0.41
will 0.88
thinking -0.44
believe -0.02
maybe -0.19
hoping -0.30
Feature Coefficient
time 0.81
clock -0.47
time pressure -0.12
blunder -0.31
tempo -0.36
checkmate -0.24
mate 0.69
opening -0.63
castle -0.33
fall -0.22
eat -0.28
Table 6: Selected SVM weights in the best 2-class setup, mean over all folds
the unigram and bigram linear models; all except for the unigram model (lines 7?9) also yield weakly
significant improvements over the MS baseline. Adding the notation features (line 8 improves the results
and leads to improvements with stronger significance. The RBF kernel makes feature weight analysis
impossible, so we cannot perform further error analysis.
5.4 Comparing the Learned Models and Strength Indicators from the Chess Literature
There are many conjectures from instructional chess literature and results from psychological research
about various aspects of player behavior. In this section, we compare these to the predictions made by
our supervised expertise model. In Table 6, we list selected weights from the best classification model
(line 3 in Table 2). We opt for analzying the classifier rather than the ranker as we find the former more
directly interpretable.
Long-Term vs Short-Term Planning. The SVM model reflect the short-term nature of the amateurs?
thoughts in several ways: (i) Amateurs focus on specific moves rather than long-term plans, and thus,
terms like capture and take are deemed predictive for lower ratings. (ii) Amateurs often think piece-
specific (Silman, 1999), particularly about moves with minor pieces (bishop or knight), and these terms
receive high negative weights, pointing to lower ratings. Related to this, Reynolds (1982) observed that
amateurs often focus on the current location of a piece, whereas experts mostly consider possible future
locations. The SVM model learns this by weighting bigrams of the form * on, where * is a piece, as
indicators for low ratings. (iii) Many terms related to elementary tactics (e.g., pin, fork) indicate lower-
rated players, whereas terms relating to tactical foresight (e.g., threat, danger, stop) as well as positional
terms (e.g., weakness, light and dark squares, variation) indicate higher-rated players.
Emotions. A popular and wide-spread claim is that weaker chess players often lose because they are
too emotionally invested in the game and thus get carried away (e.g., Cleveland (1907), Silman (1999)).
We experimented with a sentiment feature, counting polar terms in the annotations using a polarity
lexicon (Wilson et al., 2005). However, this feature did not improve our results.
Manual examination of features expressing sentiment reveals that both amateurs and experts use sub-
jective terms. We note that the vocabulary of subjective expressions is very constrained for stronger
318
players while it is open for weaker ones. Expert players tend to assess positions as winning or losing
for a side, whereas weaker players tend to use terms such as like and hate. Both terms are identified as
indicators of the respective strength class in our models. Other subjective assessments (e.g., good and
bad) are divided among the classes. Emotional tendencies of amateurs can also be observed through
objective indicators. As discussed above, stronger players talk about the game with a more distanced
view, often referring to their opponent by their color (white or black) rather than using the pronoun he.
Lower-rated players appear to use terms indicating competitions more frequently, such as fight.
Confidence. Silman (1999) argues that weaker players lack confidence, which leads to them losing
track of their own plans and to eventually follow their opponent?s will (often called losing the initiative).
This process is indeed captured by our trained models. Terms of high confidence (such as know, will) are
weighted towards the stronger class, whereas terms with higher uncertainty (such as thinking, believe,
maybe, hoping) indicate the weaker class. This observation is in line with findings on self-assigned
confidence judgments of chess players (Reynolds, 1992). The sets of terms expressing certainty and
uncertainty, respectively, are small in our dataset, so weights for most terms can be learned directly on
the n-grams.
Time Management. It has been suggested that deficiencies in time management are responsible for
many losses at the amateur level, particularly in fast games (e.g., blitz chess, where each player has 5
minutes to complete the game), for example due to poor pattern recognition skills of beginners (Calder-
wood et al., 1988). In the trained models, we see that the term time itself is actually considered a good
indicator for stronger players. Time is often used to signify number of moves. So, when used on its own,
time is referring to efficient play, which is indicative of strong players. Conversely, the terms clock and
time pressure are deemed good features to identify weaker players.
Chess Terminology. As shown in Section 2.1 and throughout this paper, there is a vast amount of chess
terminology. We observe that frequent usage of such terms (e.g., blunder ? a grave mistake, tempo, check-
mate ? experts use mate, opening, castle) actually indicate a weaker player. This seems counterintuitive
at first, as we may expect lower-rated players to be less familiar with such terms. However, it appears
that they are frequently overused by weaker players. This also holds for metaphorical terms, such as fall
or eat instead of capture.
6 Related Work
The treatment of writer expertise in extralinguistic tasks in NLP has mostly focused on two problems:
(i) retrieval of experts for specific areas ? i.e., predicting the area of expertise of a writer (e.g., Tu et al.
(2010; Kivim?aki et al. (2013)); and (ii) using expert status in different downstream applications such as
sentiment analysis (e.g., Liu et al. (2008)) or dialog systems (e.g., Komatani et al. (2003)). Conversely,
our work is concerned with predicting a ranking by expertise within a single task.
Several publications have dealt with natural language processing related to games. Chen and Mooney
(2008) investigate grounded language learning where commentary describing the specific course of a
game is automatically generated. Commentator expertise is not taken into account in this study. Branavan
et al. (2012) introduced a model for using game manuals to increase the strength of a computer playing
the strategy video game Civilization II. Cadilhac et al. (2013) investigated the prediction of player actions
in the strategy board game The Settlers of Catan. Our approach differs conceptually from theirs as their
main focus lies on modeling concrete actions in the game (either predicting or learning them); our goal
is to predict player strength, i.e., to learn to compare players among each other. Rather than explicitly
modeling the game, commentary analysis aims to provide insight into specific thought processes.
Work in psychology research by Pfau and Murphy (1988) showed the quality of chess players? verbal-
ization about positions is correlated significantly with their rating. While they use manual assessments
by chess masters to determine the quality of a player?s writing, our approach is to learn this distinction
is automatically given the ratings.
319
7 Conclusion
In this paper, we presented experiments on predicting the expertise of speakers in a task using linguistic
evidence. We introduced a classification and a ranking task for automatically ranking chess players by
playing strength using their natural language commentary. SVM models succeed at predicting either a
rating class or an overall ranking. In the ranking case, we could significantly boost the results by using
chess-specific features extracted from the text. Finally, we compared the predictions of the SVM with
popular claims from instructional chess literature as well as results from psychology research. We found
that many of the traditional findings are reflected in the features learned by our models.
Acknowledgements
We thank Daniel Quernheim for providing his chess expertise, Kyle Richardson and Jason Utt for helpful
suggestions, and the anonymous reviewers for their comments.
References
SRK Branavan, David Silver, and Regina Barzilay. 2012. Learning to win by reading manuals in a monte-carlo
framework. Journal of Artificial Intelligence Research, 43(1):661?704.
Anais Cadilhac, Nicholas Asher, Farah Benamara, and Alex Lascarides. 2013. Grounding strategic conversation:
Using negotiation dialogues to predict trades in a win-lose game. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 357?368.
Roberta Calderwood, Gary A Klein, and Beth W Crandall. 1988. Time pressure, skill, and move quality in chess.
The American Journal of Psychology, 101(4):481?493.
Jos?e R Capablanca. 1921. Chess Fundamentals. Harcourt.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions
on Intelligent Systems and Technology (ACM TIST), 2(3):1?27.
David L Chen and Raymond J Mooney. 2008. Learning to sportscast: a test of grounded language acquisition. In
Proceedings of the 25th International Conference on Machine learning (ICML), pages 128?135.
Alfred A Cleveland. 1907. The psychology of chess and of learning to play it. The American Journal of Psychol-
ogy, 18(3):269?308.
Steven J Edwards. 1994. Portable game notation specification and implementation guide.
Arpad E Elo. 1978. The Rating of Chessplayers, Past and Present. Batsford.
Dan Heisman. 1995. The Improving Annotator ? From Beginner to Master. Chess Enterprises.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 1999. Large margin rank boundaries for ordinal regression.
Advances in Neural Information Processing Systems (NIPS), pages 115?132.
Ilkka Kivim?aki, Alexander Panchenko, Adrien Dessy, Dries Verdegem, Pascal Francq, Hugues Bersini, and Marco
Saerens. 2013. A graph-based approach to skill extraction from text. In Proceedings of TextGraphs-8, pages
79?87.
Kazunori Komatani, Shinichi Ueno, Tatsuya Kawahara, and Hiroshi G. Okuno. 2003. Flexible guidance genera-
tion using user model in spoken dialogue systems. In Proceedings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 256?263.
Yang Liu, Xiangji Huang, Aijun An, and Xiaohui Yu. 2008. Modeling and predicting the helpfulness of online
reviews. In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining (ICDM), pages
443?452.
Eric W Noreen. 1989. Computer Intensive Methods for Hypothesis Testing: An Introduction. Wiley.
H Douglas Pfau and Martin D Murphy. 1988. Role of verbal knowledge in chess skill. The American Journal of
Psychology, 101(1):73?86.
320
Robert I Reynolds. 1982. Search heuristics of chess players of different calibers. The American journal of
psychology, 95(3):383?392.
Robert I Reynolds. 1992. Recognition of expertise in chess players. The American journal of psychology,
105(3):409?415.
Jeremy Silman. 1999. The Amateur?s Mind: Turning Chess Misconceptions into Chess Mastery. Siles Press.
Gregg E A Solomon. 1990. Psychology of novice and expert wine talk. The American Journal of Psychology,
103(4):495?517.
James H Steiger. 1980. Tests for comparing elements of a correlation matrix. Psychological Bulletin, 87(2):245.
Yuancheng Tu, Nikhil Johri, Dan Roth, and Julia Hockenmaier. 2010. Citation author topic model in expert search.
In Proceedings of the 2010 Conference on Computational Linguistics (Coling): Posters, pages 1265?1273.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sen-
timent analysis. In Proceedings of the Conference on Human Language Technology (HLT) and Empirical
Methods in Natural Language Processing (EMNLP), pages 347?354.
321
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1546?1556,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Active Learning with Amazon Mechanical Turk
Florian Laws Christian Scheible Hinrich Schu?tze
Institute for Natural Language Processing
Universita?t Stuttgart
{lawsfn, scheibcn}@ims.uni-stuttgart.de
Abstract
Supervised classification needs large amounts
of annotated training data that is expensive to
create. Two approaches that reduce the cost
of annotation are active learning and crowd-
sourcing. However, these two approaches
have not been combined successfully to date.
We evaluate the utility of active learning in
crowdsourcing on two tasks, named entity
recognition and sentiment detection, and show
that active learning outperforms random selec-
tion of annotation examples in a noisy crowd-
sourcing scenario.
1 Introduction
Supervised classification is the predominant tech-
nique for a large number of natural language pro-
cessing (NLP) tasks. The large amount of labeled
training data that supervised classification relies on
is time-consuming and expensive to create, espe-
cially when experts perform the data annotation.
Recently, crowdsourcing services like Amazon Me-
chanical Turk (MTurk) have become available as an
alternative that offers acquisition of non-expert an-
notations at low cost. MTurk is a software service
that outsources small annotation tasks ? called HITs
? to a large group of freelance workers. The cost of
MTurk annotation is low, but a consequence of us-
ing non-expert annotators is much lower annotation
quality. This requires strategies for quality control
of the annotations.
Another promising approach to the data acqui-
sition bottleneck for supervised learning is active
learning (AL). AL reduces annotation effort by set-
ting up an annotation loop where, starting from a
small seed set, only the maximally informative ex-
amples are chosen for annotation. With these an-
notated examples, the classifier is then retrained to
again select more informative examples for further
annotation. In general, AL needs a lot fewer anno-
tations to achieve a desired performance level than
random sampling.
AL has been successfully applied to a number of
NLP tasks such as part-of-speech tagging (Ringger
et al, 2007), parsing (Osborne and Baldridge, 2004),
text classification (Tong and Koller, 2002), senti-
ment detection (Brew et al, 2010), and named entity
recognition (NER) (Tomanek et al, 2007). Until
recently, most AL studies focused on simulating the
annotation process by using already available gold
standard data. In reality, however, human annota-
tors make mistakes, leading to noise in the annota-
tions. For this reason, some authors have questioned
the applicability of AL to noisy annotation scenarios
such as MTurk (Baldridge and Palmer, 2009; Re-
hbein et al, 2010).
AL and crowdsourcing are complementary ap-
proaches: AL reduces the number of annotations
used while crowdsourcing reduces the cost per an-
notation. Combined, the two approaches could sub-
stantially lower the cost of creating training sets.
Our main contribution in this paper is that we
show for the first time that AL is significantly bet-
ter than randomly selected annotation examples in
a real crowdsourcing annotation scenario. Our
experiments directly address two tasks, named en-
tity recognition and sentiment detection, but our
1546
evidence suggests that AL is of general benefit in
crowdsourcing. We also show that the effectiveness
of MTurk annotation with AL can be further en-
hanced by using two techniques that increase label
quality: adaptive voting and fragment recovery.
2 Related Work
2.1 Crowdsourcing
Pioneered by Snow et al (2008), Crowdsourcing,
especially using MTurk, has become a widely used
service in the NLP community. A number of stud-
ies have looked at crowdsourcing for NER. Voyer et
al. (2010) use a combination of expert and crowd-
sourced annotations. Finin et al (2010) annotate
Twitter messages ? short sequences of words ? and
this is reflected in their vertically oriented user in-
terface. Lawson et al (2010) choose an annotation
interface where annotators have to drag the mouse
to select entities. Carpenter and Poesio (2010) ar-
gue that dragging is less convenient for workers than
marking tokens.
These papers do not address AL in crowdsourc-
ing. Another important difference is that previous
studies on NER have used data sets for which no
?linguistic? gold annotation is available. In con-
trast, we reannotate the CoNLL-2003 English NER
dataset. This allows us to conduct a detailed com-
parison of MTurk AL to conventional expert anno-
tation.
2.2 Active Learning with Noisy Labels
Hachey et al (2005) were among the first to in-
vestigate the effect of actively sampled instances
on agreement of labels and annotation time. They
demonstrate applicability of AL when annotators are
trained experts. This is an important result. How-
ever, AL depends on accurate assessments of uncer-
tainty and informativeness and such an accurate as-
sessment is made more difficult if labels are noisy
as is the case in crowdsourcing. For this reason, the
problem of AL performance with noisy labels has
become a topic of interest in the AL community. Re-
hbein et al (2010) investigate AL with human expert
annotators for word sense disambiguation, but do
not find convincing evidence that AL reduces anno-
tation cost in a realistic (non-simulated) annotation
scenario. Brew et al (2010) carried out experiments
on sentiment active learning through crowdsourcing.
However, they use a small set of volunteer labelers
instead of anonymous paid workers.
Donmez and Carbonell (2008) propose a method
to choose annotators from a set of noisy annotators.
However, in a crowdsourcing scenario, it is not pos-
sible to ask specific annotators for a label, as crowd-
sourcing workers join and leave the site. Further-
more, they only evaluate their approach in simula-
tions. We use the actual labels of human annotators
to avoid the risk of unrealistic assumptions when
modeling annotators.
We are not aware of any study that shows that AL
is significantly better than a simple baseline of hav-
ing annotators annotate randomly selected examples
in a highly noisy annotation setting like crowdsourc-
ing. While AL generally is superior to this base-
line in simulated experiments, it is not clear that
this result carries over to crowdsourcing annotation.
Crowdsourcing differs in a number of ways from
simulated experiments: the difficulty and annotation
consistency of examples drawn by AL differs from
that drawn by random sampling; crowdsourcing la-
bels are noisy; and because of the noisiness of labels
statistical classifiers behave differently in simulated
and real annotation experiments.
3 Annotation System
One fundamental design criterion for our annotation
system was the ability to select examples in real time
to support, e.g., the interactive annotation experi-
ments presented in this paper. Thus, we could not
use the standard MTurk workflow or services like
CrowdFlower.1
We therefore designed our own system for anno-
tation experiments. It consists of a two-tiered ap-
plication architecture. The frontend tier is a web
application that serves two purposes. First, the ad-
ministrator can manage annotation experiments us-
ing a web interface and publish annotation tasks as-
sociated with an experiment on MTurk. The front-
end also provides tools for efficient review of the
received answers. Second, the frontend web appli-
cation presents annotation tasks to MTurk workers.
Because we wanted to implement interactive anno-
tation experiments, we used the ?external question?
1http://crowdflower.com/
1547
feature of MTurk. An external question contains
an URL to our frontend web application, which is
queried when a worker views an annotation task.
Our frontend then in turn queries our backend com-
ponent for an example to be annotated and renders it
in HTML.
The backend component is responsible for selec-
tion of an example to be annotated in response to a
worker?s request for an annotation task. The back-
end implements a diverse choice of random and ac-
tive selection strategies as well as the multilabel-
ing strategies described in section 3.2. The backend
component runs as a standalone server and is queried
by the frontend via REST-like HTTP calls.
For the NER task, we present one sentence per
HIT, segmented into tokens, with a select box under-
neath each token containing the classes. The defini-
tion of the classes is based on the CoNLL-2003 an-
notation guidelines (Tjong Kim Sang and De Meul-
der, 2003). Examples were given for every class.
Annotators are forced to make a selection for upper-
case tokens. Lowercase tokens are prelabeled with
?O? (no named entity), but annotators are encour-
aged to change this label if the token is in fact part
of an entity phrase.
For sentiment annotation, we found in prelim-
inary experiments that using simple radio button
selection for the choice of the document label
(positive or negative) leads to a very high
amount of spam submissions, taking the overall clas-
sification accuracy down to around 55%. We then
designed a template that forced annotators to type
the label as well as a randomly chosen word from
the text. Individual label accuracy was around 75%
in this scheme.
3.1 Concurrent example selection
AL works by setting up an interactive annotation
loop where at each iteration, the most informative
example is selected for annotation. We use a pool-
based AL setup where the most informative exam-
ple is selected from a pool of unlabeled examples.
Informativeness is calculated as uncertainty (Lewis
and Gale, 1994) using the margin metric (Schein
and Ungar, 2007). This metric chooses examples for
which the margin of probabilities from the classifier
between the two most probable classes is the small-
est:
Mn = |P? (c1|xn) ? P? (c2|xn)|
Here, xn is the instance to be classified, c1 and c2
are the two most likely classes, and P? the classifier?s
estimate of probability.
For NER, the margins of the tokens are averaged
to get an uncertainty assessment of the sentence. For
sentiment, whole documents are classified, thus un-
certainties can be used directly.
After annotation, the selected example is removed
from the unlabeled pool and, together with its la-
bel(s), added to the set of labeled examples. The
classifier is then retrained on the labeled examples
and the informativeness of the remaining examples
in the pool is re-evaluated.
Depending on the classifier and the sizes of pool
and labeled set, retraining and reevaluation can take
some time. To minimize wait times, traditional AL
implementations select examples in batches of the
n most informative examples. However, batch se-
lection might not give the optimum selection (exam-
ples in a batch are likely to be redundant, see Brinker
(2003)) and wait times can still occur between one
batch and the next.
When performing annotation with MTurk, wait
times are unacceptable. Thus, we perform the re-
training and uncertainty rescoring concurrently with
the annotation user interface. The unlabeled pool is
stored in a priority queue that is ordered according to
the examples? informativeness. The annotation user
interface takes the most informative example from
the pool and presents it to the annotator. The la-
beled example is then inserted into a second queue
that feeds and updates retraining and rescoring pro-
cesses. The pool queue then is resorted according to
the new informativeness. In this way, annotation and
example selection can run in parallel. This is similar
to Haertel et al (2010).
3.2 Adaptive voting and fragment recovery
MTurk labels often have a high error rate. A com-
mon strategy for improving label quality is to ac-
quire multiple labels by different workers for each
example and then consolidate the annotations into
a single label of higher quality. To trade off num-
ber of annotated examples against quality of anno-
tations, we adopt adaptive voting. It uses majority
1548
NER Sentiment
Budget 5820 6931 1130 1756
#train F1 cost/sent w.-accuracy #train F1 #train Acc cost/doc w.-accuracy #train Acc
RS 1 S 5820 59.6 1.00 51.6 ? ? 1130 70.4 1 74.8 ? ?
2 3-v 1624 61.4? 3.58 70.1 ? ? ? ? ? ? ? ?
3 5/4-v 1488 63.0? 3.91 71.6 1774 63.5 450 71.2 2.51 89.6 735 79.2
4 5-v+f 1996 63.6? 2.91 71.8 2385 64.9? ? ? ? ? ? ?
AL 5 S 5820 67.0 1.00 66.5 ? ? 1130 74.8 1 76.0 ? ?
6 3-v 1808 70.0? 3.21 78.8 ? ? ? ? ? ? ? ?
7 5/4-v 1679 70.4? 3.46 79.6 1966 70.6 455 77.4 2.48 89.0 715 81.8
8 5-v+f 2165 70.5 2.68 79.3 2691 71.2 ? ? ? ? ? ?
Table 1: For NER, active learning consistently beats random sampling on MTurk. NER F1 evaluated on
CoNLL test set A. #train = number of sentences in training set, S = single, 3-v = 3-voting, 5/4-voting = 5-
and 4-voting for NER and sentiment resp., +f = using fragments; sentiment budget 1130 for run 1, sentiment
budget 1756 averaged over 2 runs.
voting and is adaptive in the number of repeated an-
notations. For NER, a sentence is first annotated by
two workers. Then majority voting is performed for
each token individually. If there is a majority for ev-
ery token that is greater than an agreement threshold
?, the sentence is accepted with each token labeled
with the majority label. Otherwise additional anno-
tations are requested. A sentence is discarded if the
number of repeated annotations exceeds a discard
threshold d (d-voting).2 We use the same scheme
for sentiment; note that there is just one decision per
HIT in this case, not several as in NER.
For NER, we also use fragment recovery: we sal-
vage tokens with agreeing labels from discarded sen-
tences. We cut the token sequence of a discarded
sentence into several fragments that have agreeing
tokens and discard only those parts that disagree. We
then include these recovered fragments in the train-
ing data just like complete sentences.
Software release. Our active learning framework
used can be downloaded at http://www.ims.
uni-stuttgart.de/?lawsfn/active/.
4 Experiments, Results and Analysis
4.1 Experiments
In our NER experiments, we have workers reanno-
tate the English corpus of the CoNLL-2003 NER
shared task. We chose this corpus to be able to com-
pare crowdsourced annotations with gold standard
2It can take a while in this scheme for annotators to agree
on a final annotation for a sentence. We make tentative labels
of a sentence available to the classifier immediately and replace
them with the final labels once voting is completed.
annotations. A HIT is one sentence and is offered
for a base payment of $0.01. We filtered out answers
that contained unannotated tokens or were obvious
spam (e.g., all tokens labeled as MISC). For test-
ing NER performance, we used a system based on
conditional random fields with standard named en-
tity features including the token itself, orthographic
features like the occurrence of capitalization or spe-
cial characters and context information about the to-
kens to the left/right of the current token.
The sentiment detection task was modeled after a
well-known document analysis setup for sentiment
classification, introduced by Pang et al (2002). We
use their corpus of 1000 positive and 1000 negative
movie reviews and the Stanford maximum entropy
classifier (Manning and Klein, 2003) to predict the
sentiment label of each document d from a unigram
representation of d. We randomly split this corpus
into a test set of 500 reviews and an active learn-
ing pool of 1500 reviews. Each HIT consists of one
document, valued at $0.01.
We compare random sampling (RS) and AL in
combination with the proposed voting and fragment
strategies with different parameters. We want to
avoid rerunning experiments on MTurk over and
over again, but on the other hand, we believe that us-
ing synthetic data for simulations is problematic be-
cause it is difficult to generate synthetic data with a
realistic model of annotator errors. Thus, we logged
a play-by-play record of the annotator interactions
and labels. With this recording, we can then rerun
strategies with different parameters.
We chose voting with at most d = 5 repetitions as
1549
our main reannotation strategy for both random and
active sampling for NER annotation. We use simple
majority voting (? = .5) for NER.
For sentiment, we set d = 4 and minimum agree-
ment ? = .75 because the number of labels is
smaller (2 vs. 5) and so random agreement is more
likely for sentiment.
To get results for 3-voting NER, we take the
recording and discard 5-voting votes not needed in
3-voting. This will result in roughly the same num-
ber of annotated sentences, but at a lower cost. This
simulation of 3-voting is not exactly what would
have happened on MTurk (e.g., the final vote on a
sentence might be different, which then influences
AL example selection), but we will assume that dif-
ferences are rare and simulated and actual results
are similar. The same considerations apply to sin-
gle votes and to the sentiment experiments.
We always compare two strategies for the same
annotation budget. For example, the number of
training sentences in Table 1 differ in the two rel-
evant columns, but all strategies compared use ex-
actly the same annotation budget (5820, 6931, 1130,
and 1756, respectively).
For the single annotation strategy, each interac-
tion record contained only about 40% usable anno-
tations, the rest were repeats. A comparison with
the single annotation strategy over approx. 2000 sen-
tences or 450 documents would not have been mean-
ingful; therefore we chose to run an extra experiment
with the single annotation strategy to match this up
with the budgets of the voting strategies. The re-
sults are presented in two separate columns of Ta-
ble 1 (budgets 6931 and 1756).
4.2 Results
For sentiment detection, worker accuracy or label
quality ? the percentage of correctly annotated doc-
uments ? is 74.8. In contrast, for NER, worker accu-
racy ? the percentage of non-O tokens annotated cor-
rectly ? is only 51.6 (Table 1, line 1). This demon-
strates the challenge of using MTurk for NLP an-
notation tasks. When we use single annotations of
each sentence, NER performance is 59.6 F1 for ran-
dom sampling (line 1). When training with gold la-
bels on the same sentences, the performance is 80.0
(not shown). This means we lose more than 20%
due to poor worker accuracy. Adaptive voting and
fragment recovery manage to recover a small part of
the lost performance (lines 2?4); each of the three
F1 scores is significantly better than the one above
it as indicated by ? (Approximate Randomization
Test (Noreen, 1989; Chinchor et al, 1993) as im-
plemented by Pado? (2006)).
Using AL turns out to be quite successful for NER
performance. For single annotations, NER perfor-
mance is 67.0 (line 5), an improvement of 7.4%
compared to random sampling. Adaptive voting
and fragment recovery again increase worker accu-
racy (lines 6?8) although total improvement of 3.5%
(lines 8 vs. 5) is smaller than 4% for random (lines
4 vs. 1). The learning curves of AL vs. random in
Figure 1 (top left) confirm this good result for AL.
These learning curves are for tokens ? not for sen-
tences ? to show that the reason for AL?s better per-
formance is not that it selects slightly longer sen-
tences than random. In addition, the relative advan-
tage of AL vs random decreases over time, which is
typical of pool-based AL experiments.
We carried out two runs of the same experiment
for sentiment to validate our first positive result since
the difference between the two conditions is not as
large as in NER (Figure 1, top right). After about
300 documents, active learning consistently outper-
forms random sampling. The first AL run performs
better because of higher label quality in the begin-
ning. The overall advantage of AL over random
is lower than for NER because the set of labels is
smaller in sentiment, making the classification task
easier. Second, there is a large amount of simple lex-
ical clues for detecting sentiment (cf. Wilson et al
(2005)). It is likely that some of them can be learned
well through random sampling at first; however, ac-
tive learning can gain accuracy over time because it
selects examples with more difficult clues.
In Figure 1 (bottom), we compare single annota-
tion with adaptive voting. The graphs show F1 as
a function of cost. Adaptive voting trades quantity
of sampled sentences for quality of labels and thus
incurs higher net costs per sentence. This results in
a smaller dataset for a given budget, but this dataset
is still more useful for classifier training. For NER
(Figure 1, bottom left), the single annotation strat-
egy has a faster start; so for small budgets, cover-
ing a somewhat larger portion of the sample space
is beneficial. For larger budgets, however, quality of
1550
0 5000 10000 15000 20000
0.
4
0.
5
0.
6
0.
7
0.
8
Tokens
F?
Sc
or
e
active, 5?voting
random, 5?voting
0 200 400 600
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
Documents
Ac
cu
ra
cy
active 1
active 2
random 1
random 2
0 1000 2000 3000 4000 5000 6000
0.
4
0.
5
0.
6
0.
7
0.
8
Cost
F?
Sc
or
e
single ann.
3?voting
5?voting
5?voting +frags
0 500 1000 1500 2000
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
Cost
Ac
cu
ra
cy
single ann.
4?voting
Figure 1: Top: Active learning vs. Random sampling for NER (left) and sentiment (right). Bottom: Active
learning: adaptive voting vs. single annotation for NER (left) and sentiment (right).
the voted labels trumps quantity.
For sentiment (Figure 1, bottom right), results are
similar: voting has no benefit initially, but as find-
ing maximally informative examples to annotate be-
comes harder in later stages of learning, adaptive
voting gains an advantage over single annotations.
The main result of the experiment is that active
learning is better by about 7% F1 than random sam-
pling for NER and by 2.6% accuracy for sentiment
(averaged over two runs at budget 1756). Adaptive
voting further improves AL performance for both
NER and sentiment.
4.3 Annotation time per token
Most AL work assumes constant cost per annotation
unit. This assumption has been questioned because
AL often selects hard examples that take longer to
annotate (Hachey et al, 2005; Settles et al, 2008).
In annotation with MTurk, cost is not a function
of annotation time because workers are paid a fixed
amount per HIT. Nevertheless, annotation time plays
a part in whether workers are willing to work on a
given task for the offered reward. This is particularly
problematic for NER since workers have to examine
each token individually. We therefore investigate
for NER whether the time MTurk workers spend on
annotating sentences differs for random vs. AL.
We first compute median and mean annotation
times and number of tokens per sentence:
sec/sentence tokens/sentence
strategy median mean all required
random 17.2 33.1 15.0 3.4
AL 17.8 33.0 17.7 4.0
We see that most sentences are annotated in a very
short time; but the mean is much larger than the me-
dian because there are outliers of up to eight min-
utes. AL tends to select slightly longer sentences as
1551
0 500 1000 1500 2000
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
Sentences
F?
Sc
or
e
gold selection, gold labels
MTurk selection, gold labels
MTurk selection, MTurk labels
0 200 400 600
0.
50
0.
60
0.
70
0.
80
Documents
Ac
cu
ra
cy
gold selection, gold labels
MTurk selection, gold labels
MTurk selection, MTurk labels
Figure 3: Performance on gold labels. Left: NER. Right: sentiment (run 1).
0 2 4 6 8 10 13 16 19 23 29
0
10
0
20
0
30
0
40
0
Number of uppercase tokens
An
no
ta
tio
n 
tim
e 
(se
co
nd
s)
random
active
Figure 2: Annotation time vs. # uppercase tokens
well as sentences with slightly more uppercase to-
kens that require annotation.
In a more detailed analysis, we attempt to distin-
guish between (i) the effect of more uppercase (?an-
notation required?) tokens vs. (ii) the effect of ex-
ample difficulty. We fit a linear regression model
to annotation time vs. the number of uppercase to-
kens. For the regression fit, we removed all annota-
tion times > 60 seconds. Such long times indicate
distraction of the worker and are not a reliable mea-
sure of difficulty.
Figure 2 shows the distribution of annotation
times for both cases combined and the fitted models
for each. The model estimated an annotation time of
2.3 secs for each required token for random vs. 2.7
secs for AL. We conclude that the difference in dif-
ficulty between sentences selected by random sam-
pling vs. AL is small, but noticeable.
4.4 Influence of noise on the selection process
While NER performance for AL is much higher than
for random sampling, it is still quite a bit lower than
what is possible on gold labels. In the case of AL,
there are two reasons why this happens: (i) The
noisy labels negatively affect the classifier?s ability
to learn a good model that is used for classifying the
test set. (ii) The noisy labels result in bad interme-
diate models that then select suboptimal examples
to be annotated next. The AL selection process is
?misled? by the noisy examples.
We conduct an experiment to determine the con-
tribution of factors (i) and (ii) to the performance
loss. First, we preserve the sequence of sentences
chosen by our AL experiments on MTurk, with 5-
voting for NER and 4-voting for sentiment but re-
place the noisy worker-provided labels by gold la-
bels. The performance of classifiers trained on this
sequence is the dashed line ?MTurk selection, gold
labels? in Figure 3 for NER (left) and sentiment
(right).
Second, we compare with a traditional simulated
AL experiment with gold labels. Here, the selection
too is controlled by gold labels, so the selection has
a noiseless classifier available for scoring and can
perform optimal uncertainty selection. These are the
1552
1 5 10 50 100 500
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Number of sentences
Qu
ali
ty 
(%
 co
rre
ct 
en
tity
 to
ke
n
s)
1 2 5 10 20 50 100 200
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Number of documents
Qu
ali
ty 
(%
 co
rre
ct 
do
cu
me
nt 
lab
els
)
Figure 4: Worker accuracy vs. number of HITs. Each point corresponds to one worker (? = active, +
=random sampling; black and grey for different runs). Left: NER. Right: Sentiment.
dotted lines ?gold selection, gold labels? in Figure 3.
We used a batch-mode AL setup for this compari-
son experiment. For a fair comparison, we adjust the
batchsize to be equal to the average staleness of a se-
lected example in concurrent MTurk active learning.
The staleness of an example is defined as the num-
ber of annotations the system has received, but not
yet incorporated in the computation of an example?s
uncertainty score (Haertel et al, 2010).
For our concurrent NER system, the average stal-
eness of an example was about 12 (min: 1, max: 40),
for sentiment it was about 2. The figure for NER is
higher than the number cited by Haertel et al (2010)
because there are more annotators accessing our sys-
tem at the same time via MTurk but not as high for
sentiment since documents are longer and retraining
the sentiment classifier is faster. The average stale-
ness of an example in a batch-mode system is half
the batch size. Thus, we set the batch size of our
comparison system to 25 for NER and to 4 for sen-
timent.
Returning to the two factors introduced above ?
(i) final effect of noise on test set performance vs.
(ii) intermediate effect of noise on example selec-
tion ? we see in Figure 3 that (i) has a large effect
on NER whereas (ii) has a noticeable, but small ef-
fect.3 For example, at 1966 sentences, F1 scores are
3Our comparison unit for NER is the sentence. We can-
not compare on cost here since we do not know what the per-
sentence cost of a ?gold? expert annotation is.
70.6 (MTurk-MTurk), 81.4 (MTurk-gold) and 84.9
(gold-gold). This means that a performance differ-
ence of 10 points F1 has to be attributed to noisy
labels resulting in a worse final classifier (effect i),
and another 3.5 points are lost due to sub-optimal
example selection (effect ii).
For sentiment, the results are different. There is
no clear difference between the three runs. We at-
tribute this to the fact that the quality of the labels
is higher in sentiment than in NER. Our initial ex-
periments on sentiment were all negative (showing
no improvement of AL compared to random) be-
cause label quality was too low. Only after we intro-
duced the template described in Section 3 and used
4-voting with ? = .75 did we get positive results for
AL. This leads to an overall label quality of about
90% (over all runs) which is so high that the differ-
ence to using gold labels is small if present at all.
5 Worker Quality
So far we have assumed that all workers provide
annotations of the same quality. However, this is
not the case. Figure 4 shows plots of worker accu-
racy as a function of worker productivity (number
of annotated examples). Some workers submit only
one or two HITs just to try out the task. For NER,
the majority of workers submit between 5 and 10
sentences, with label qualities between 0.5 and 0.8.
The chance level for correctness is around 0.25 (four
1553
different named entity categories for uppercase to-
kens). For sentiment, most workers submit 1 to 5
documents, with label qualities between 0.5 and 1.
Chance level lies at around 0.5 (for two equally dis-
tributed labels).
While quality for highly productive workers is
mediocre in our experiments, other researchers have
found extremely bad quality for their most prolific
workers (Callison-Burch, 2009). Some of these
workers might be spammers who try to submit an-
swers with automatic scripts. We encountered some
spammers that our heuristics did not detect (shown
in the bottom-right areas of Figure 4, left), but the
voting mechanism was able to mitigate their nega-
tive influence.
Given the large variation in Figure 4, using worker
quality in crowdsourcing for improved training set
creation seems promising. We now test two such
strategies for NER in an oracle setup.
5.1 Blocking low-quality workers
A simple approach is to refuse annotations from
workers that have been determined to provide low
quality answers. We simulated this strategy on NER
data using oracle quality ratings. We chose NER be-
cause of its lower overall label quality. The re-
sults are presented in Figure 5 for random (a) and
AL (b). For random, quality filtering with low cut-
offs helps by removing bad annotations that likely
come from spammers. While the voting strategy
prevented a performance decrease with bad anno-
tations, it needed to expend many extra annotations
for correction. With filtering, these extra annotations
become unnecessary and the system can learn faster.
When low-quality workers are less active, as in the
AL dataset, we find no meaningful performance in-
crease for low cutoffs up to 0.4. For very high cut-
offs (0.7), the beginning of the performance curve
shows that further cost reductions can be achieved.
However, we did not have enough recorded human
annotations available to perform a simulation for the
full budget.
5.2 Trusting high-quality workers
The complementary approach is to take annotations
from highly rated workers at face value and imme-
diately accept them as the correct label, bypassing
the voting procedure. Bypassing saves the cost of
repeated annotation of the same sentence. Figure 5
shows learning curves for two bypass thresholds on
worker quality (measured as proportion of correct
non-O tokens) for random (c) and AL (d). Bypass-
ing performs surprisingly well. We find a steeper
rise of the learning curve, meaning less cost for the
same performance. Not only do we find substantial
cost reductions, but also higher overall performance.
We believe this is because high-quality annotations
can sometimes be voted down by other annotations.
If we can identify high-quality workers and directly
use their annotations, this can be avoided.
These experiments are oracle experiments using
gold data that is normally not available. In future
work, we would like to repeat the experiments using
methods for worker quality estimation (Ipeirotis et
al., 2010; Donmez et al, 2009). For AL, the choice
as to which labels are used (as a result of voting, by-
passing or other) also has an influence on the selec-
tion. However, we had to keep the sequence of the
selected sentences fixed in the simulations reported
above. While our method of sample selection for
AL proved to be quite robust even in the presence
of noise, higher quality labels do have an influence
on the sample selection (see section 4.4), so the im-
provement could be even better than indicated here.
5.3 Differences in quality between AL and
random
The essence of AL is to select examples that are dif-
ficult to classify. As observed in our experiments
on annotation time, this difficulty is reflected in the
amount of time a human needs to work on examples
selected through AL. Another effect to expect from
difficulty could be lower annotation accuracy. We
therefore examined the accuracies for each worker
who contributed to both the AL and the random ex-
periment. We found that in the NER task, the 20
workers in this group had a slightly higher (0.07) av-
erage quality for randomly selected examples. This
difference is low and does not suggest a significant
drop in accuracy for examples selected in AL.
6 Conclusion
We have investigated the use of AL in a real-life
annotation experiment with human annotators in-
stead of traditional simulations with gold labels for
1554
(a) (b) (c) (d)
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
min. quality 0.1
min. quality 0.4
min. quality 0.7
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
min. quality 0.1
min. quality 0.4
min. quality 0.7
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
bypass 0.9
bypass 0.7
0 1000 2000 3000 4000
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
Cost
F?
Sc
or
e
baseline 5?voting
bypass 0.9
bypass 0.7
Figure 5: Blocking low-quality workers: (a) random, (b) AL. Bypass voting: (c) random, (d) AL.
named entity recognition and sentiment classifica-
tion. The annotation was performed using MTurk in
an AL framework that features concurrent example
selection without wait times. We also evaluated two
strategies, adaptive voting and fragment recovery, to
improve label quality at low additional cost. We find
that even for the relatively high noise levels of anno-
tations gathered with MTurk, AL is successful, im-
proving performance by +6.9 points F1 compared to
random sampling for NER and by +2.6% accuracy
for sentiment. Furthermore, this performance level
is reached at a smaller MTurk cost compared to ran-
dom sampling. Thus AL not only reduces annotation
costs, but also offers an improvement in absolute
performance for these tasks. This is clear evidence
that active learning and crowdsourcing are comple-
mentary methods for lowering annotation cost and
should be used together in training set creation for
natural language processing tasks.
We have also conducted oracle experiments that
show that further performance gains and cost sav-
ings can be achieved by using information about
worker quality. We plan to confirm these results by
using estimates of quality in the future.
7 Acknowledgments
Florian Laws is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by his fellowship.
Christian Scheible is supported by the Deutsche
Forschungsgemeinschaft project Sonderforschungs-
bereich 732.
References
Jason Baldridge and Alexis Palmer. 2009. How well
does active learning actually work? Time-based eval-
uation of cost-reduction strategies for language docu-
mentation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 296?305.
Anthony Brew, Derek Greene, and Pa?draig Cunningham.
2010. Using crowdsourcing and active learning to
track sentiment in online media. In Proceeding of the
2010 conference on ECAI 2010: 19th European Con-
ference on Artificial Intelligence, pages 145?150.
Klaus Brinker. 2003. Incorporating diversity in active
learning with support vector machines. In Proceed-
ings of the Twentieth International Conference on Ma-
chine Learning (ICML 2003), pages 59?66.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon?s
Mechanical Turk. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 286?295.
Bob Carpenter and Massimo Poesio. 2010. Models
of data annotation. Tutorial at the seventh interna-
tional conference on Language Resources and Eval-
uation (LREC 2010).
Nancy Chinchor, David D. Lewis, and Lynette
Hirschman. 1993. Evaluating message understanding
systems: an analysis of the third message understand-
ing conference (muc-3). Computational Linguistics,
19(3):409?449.
Pinar Donmez and Jaime G. Carbonell. 2008. Proactive
learning: cost-sensitive active learning with multiple
imperfect oracles. In Proceeding of the 17th ACM con-
ference on Information and knowledge management,
pages 619?628.
Pinar Donmez, Jaime G. Carbonell, and Jeff Schnei-
der. 2009. Efficiently learning the accuracy of la-
1555
beling sources for selective sampling. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 259?
268.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Annotating named entities in twitter data with
crowdsourcing. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 80?88.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL ?05: Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144?151.
Robbie Haertel, Paul Felt, Eric K. Ringger, and Kevin
Seppi. 2010. Parallel active learning: Eliminating
wait time with minimal staleness. In Proceedings of
the NAACL HLT 2010 Workshop on Active Learning
for Natural Language Processing, pages 33?41.
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010. Quality management on amazon mechanical
turk. In Proceedings of the ACM SIGKDD Workshop
on Human Computation (HCOMP ?10).
Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen-Yildiz. 2010. Annotating large email
datasets for named entity recognition with mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 71?79.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 3?12.
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, maxent models, and conditional estimation with-
out magic. In Proceedings of the 2003 Conference
of the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology: Tutorials - Volume 5, pages 8?8.
Eric W. Noreen. 1989. Computer-intensive methods for
testing hypotheses: an introduction. Wiley.
Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In
Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, HLT-NAACL 2004: Main Proceedings, pages 89?
96.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79?86.
Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer.
2010. Bringing active learning to life. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 949?957.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop at
ACL-2007, pages 101?108.
Andrew Schein and Lyle Ungar. 2007. Active learn-
ing for logistic regression: An evaluation. Machine
Learning, 68(3):235?265.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proceed-
ings of the NIPS Workshop on Cost-Sensitive Learn-
ing, pages 1069?1078.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 254?263.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL (CoNLL 2003), pages 142?147.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 486?495.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. The Journal of Machine Learning Re-
search, 2:45?66.
Robert Voyer, Valerie Nygaard, Will Fitzgerald, and Han-
nah Copperman. 2010. A hybrid model for anno-
tating named entity training corpora. In Proceedings
of the Fourth Linguistic Annotation Workshop, pages
243?246.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354.
1556
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 669?680,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
The Topology of Semantic Knowledge
Jimmy Dubuisson Jean-Pierre Eckmann
De?partement de Physique The?orique and Section de Mathe?matiques
Universite? de Gene`ve
Jimmy.Dubuisson@unige.ch
Christian Scheible
Institut fu?r Maschinelle Sprachverarbeitung
University of Stuttgart
scheibcn@ims.uni-stuttgart.de
Hinrich Schu?tze
Center for Information
and Language Processing
University of Munich
Abstract
Studies of the graph of dictionary definitions
(DD) (Picard et al, 2009; Levary et al, 2012)
have revealed strong semantic coherence of
local topological structures. The techniques
used in these papers are simple and the main
results are found by understanding the struc-
ture of cycles in the directed graph (where
words point to definitions). Based on our ear-
lier work (Levary et al, 2012), we study a dif-
ferent class of word definitions, namely those
of the Free Association (FA) dataset (Nelson
et al, 2004). These are responses by subjects
to a cue word, which are then summarized by
a directed, free association graph.
We find that the structure of this network is
quite different from both the Wordnet and the
dictionary networks. This difference can be
explained by the very nature of free associa-
tion as compared to the more ?logical? con-
struction of dictionaries. It thus sheds some
(quantitative) light on the psychology of free
association.
In NLP, semantic groups or clusters are inter-
esting for various applications such as word
sense disambiguation. The FA graph is tighter
than the DD graph, because of the large num-
ber of triangles. This also makes drift of
meaning quite measurable so that FA graphs
provide a quantitative measure of the seman-
tic coherence of small groups of words.
1 Introduction
The computer study of semantic networks has been
around since the advent of computers (Brunet, 1974)
and has been used to study semantic relations be-
tween concepts and for analyzing semantic data.
Traditionally, a popular lexical database of English
is Wordnet (Miller, 1995; Miller and Fellbaum,
1998), which organizes the semantic network in
terms of graph theory. In contrast to manual ap-
proaches, the automatic analysis of semantically in-
teresting graph structures of language has received
increasing attention. For example, it has become
clear more recently that cycles and triangles play
an important role in semantic networks, see e.g.,
(Dorow et al, 2005). These results suggest that the
underlying semantic structure of language may be
discovered through graph-theoretical methods. This
is in line with similar findings in much wider realms
than NLP (Eckmann and Moses, 2002).
In this paper, we compare two different types
of association networks. The first network is con-
structed from an English dictionary (DD), the sec-
ond from a free association (FA) database (Nelson
et al, 2004). We represent both datasets through
directed graphs. For DD, the nodes are words and
the directed edges point from a word to its defini-
tion(s). For FA, the nodes are again words, and each
cue word has a directed edge to each association it
elicits.
Although the links in these graphs were not con-
structed by following a rational centralized process,
their graph exhibits very specific features and we
concentrate on the study of its topological proper-
ties. We will show that these graphs are quite dif-
ferent in global and local structure, and we inter-
pret this as a reflection of the different nature of
DD vs. FA. The first is an objective set of rela-
669
tions between words and their meaning, as explained
by other words, while the second reveals the nature
of subjective reactions to cue words by individuals.
This matter of fact is reflected by several quantita-
tive differences in the structure of the corresponding
graphs.
The main contribution of this paper is an empiri-
cal analysis of the way semantic knowledge is struc-
tured, comparing two different types of association
networks (DD and FA). We conduct a mathemati-
cal analysis of the structure of the graphs to show
that the way humans express their thoughts exhibits
structural properties in which one can find seman-
tic patterns. We show that a simple graph-based
approach can leverage the information encoded in
free association to narrow down the ambiguity of
meaning, resulting in precise semantic groups. In
particular, we find that the main strongly connected
component of the FA graph (the so-called core) is
very cyclic in nature and contains a large predom-
inance of short cycles (i.e., co-links and triangles).
In contrast to the DD graph, bunches of triangles
form well-delimited lexical fields of collective se-
mantic knowledge. This property may be promising
for downstream tasks. Further, the methods devel-
oped in this paper may be applicable to graph rep-
resentations that occur in other problems such as
word sense disambiguation (e.g., (Heylighen, 2001;
Agirre and Soroa, 2009)) or sentiment polarity in-
duction (Hassan and Radev, 2010; Scheible, 2010).
To show the semantic coherence of these lexi-
cal fields of the FA graph, we perform an exper-
iment with human raters and find that cycles are
strongly semantically connected even when com-
pared to close neighbors in the graph.
The reader might wonder why sets of pairwise
associations can lead to any interesting structure.
One of the deep results in graph theory, (Bolloba?s,
2001), is that in sparse graphs, i.e., in graphs with
few links per node, the number of triangles is ex-
tremely rare. Therefore, if one does find many tri-
angles in a graph, they must be not only a signal
of non-randomness, but carry relevant information
about the domain of research as shown earlier (Eck-
mann and Moses, 2002).
2 The USF FA dataset
This dataset is one of the largest existing databases
of free associations (FA) and has been collected at
the University of South Florida since 1973 by re-
searchers in psychology (Nelson et al, 2004). Over
the years, more than 6?000 participants produced
about 750?000 responses to 5?019 stimulus words.
The procedure for collecting the data is called dis-
crete association task and consists in asking partici-
pants to give the first word that comes to mind (tar-
get) when presented a stimulus word (cue).
For creating the initial set of stimulus words,
the Jenkins and Palermo word association norms
(Palermo and Jenkins, 1964) proved useful but too
limited as they consist of only 200 words. For this
reason, additional words have been regularly added
to the pool of normed words, unfortunately without
well established rules being followed. For instance,
some were selected as potentially interesting cues,
some were added as responses to the first sets of cues
and, some others were collected for supporting new
studies on verbs. We still work with this database,
because of its breadth.
The final pool of stimuli comprises 5?019 words
of which 76% are nouns, 13% adjectives, and 7%
verbs. A word association is said to be normed
when the target is also part of the set of norms, i.e.,
a cue. The USF dataset of free associations con-
tains 72?176 cue-target pairs, 63?619 of which are
normed. As an example, the association puberty-sex
is normed whereas the association puberty-thirteen
is not, because thirteen is not a cue.
3 Mathematical definitions
We collect here those notions we need for the analy-
sis of the data.
A directed graph is a pair G = (V,E) of a set
V of vertices and, a set E of ordered pairs of ver-
tices also called directed edges. For a directed edge
(u, v) ? E, u is called the tail and v the head of
the edge. The number of edges incident to a vertex
v ? V is called the degree of v. The in-degree
(resp. out-degree) of a vertex v is the number of
edge heads (resp. edge tails) adjacent to it. A vertex
with null in-degree is called a source and a vertex
with null out-degree is called a sink.
A directed path is a sequence of vertices such
670
that a directed edge exists between each consecutive
pair of vertices of the graph. A directed graph is
said to be strongly connected, (resp. weakly con-
nected) if for every pair of vertices in the graph,
there exists a directed path (resp. undirected path)
between them. A strongly connected component,
SCC, (resp. weakly connected component, WCC)
of a directed graph G is a maximal strongly con-
nected (resp. weakly connected) subgraph of G.
A directed cycle is a directed path such that its
start vertex is the same as its end vertex. A co-
link is a directed cycle of length 2 and a triangle a
directed cycle of length 3.
The distance between two vertices in a graph is
the number of edges in the shortest path connecting
them. The diameter of a graph G is the greatest
distance between any pair of vertices. The charac-
teristic path length is the average distance between
any two vertices of G.
The density of a directed graph G(V,E) is the
proportion of existing edges over the total number
of possible edges and is defined as:
d = |E|/(|V |(|V | ? 1))
The neighborhoodNi of a vertex vi isNi = {vj :
eij ? E or eji ? E}.
The local clustering coefficient Ci for a vertex vi
corresponds to the density of its neighborhood sub-
graph. For a directed graph, it is thus given by:
Ci =
|{ejk : vj , vk ? Ni, ejk ? E}|
|Ni|(|Ni| ? 1)
The clustering coefficient of a graph G is the aver-
age of the local clustering coefficients of all its ver-
tices.
The efficiency Eff of a directed graph G is an in-
dicator of the traffic capacity of a network. It is the
harmonic mean of the distance between any two ver-
tices of G. It is defined as:
Eff =
1
|V |(|V | ? 1)
?
i 6=j?V
1
dij
The linear correlation coefficient between two
random variables X and Y is defined as:
?(X,Y ) = (E[XY ]? ?X?Y )/(?X?Y )
where ?X and ?X are respectively the mean and
standard deviation of the random variable X .
The linear degree correlation coefficient of a
graph is called assortativity and is expressed as:
?D =
?
xy
xy(exy ? axby)/(?a?b)
where exy is the fraction of all links that connect
nodes of degree x and y and where ax and by are re-
spectively the fraction of links whose tail is adjacent
to nodes with degree x and whose head is adjacent to
nodes with degree y, satisfying the following three
conditions:
?
xy
exy = 1, ax =
?
y
exy, by =
?
x
exy
When ?D is positive, the graph possesses assor-
tative mixing and high-degree nodes tend to con-
nect to other high-degree nodes. On the other hand,
when ?D is negative, the graph features disassorta-
tive mixing and high-degree nodes tend to connect
to low degree nodes.
The intersection graph of sets Ai, i = 1, . . . ,m,
is constructed by representing each setAi as a vertex
vi ? V and adding an edge for each pair of sets with
a non-empty intersection:
E = {(vi, vj) : Ai ?Aj 6= ?}
4 Graph topology analysis
4.1 Graph generation
Our goal being to study the FA network topology,
we first concentrate on the generation of an un-
weighted directed graph. We generate the corre-
sponding graph by adding a directed edge for each
cue-target pair of the dataset. We only consider pairs
whose target was normed in order to avoid overload-
ing the graph with noisy data (e.g., a response mean-
ingful only to a specific participant). The graph has
5?019 vertices and 63?619 edges. It is composed of
a single WCC and 166 SCCs.
For comparison with dictionary definitions (DD),
we construct a graph from the Wordnet2 dictionary
(nouns only), following (Levary et al, 2012). This
graph contains 54?453 vertices and 179?848 edges.
671
4.2 Core extraction
The so-called core was defined previously in (Picard
et al, 2009; Levary et al, 2012) as that subset of
nodes in which a random walker gets trapped after
only a few steps.
The shave algorithm was used in (Levary et al,
2012) to isolate this subset. It consists in recursively
removing the source and sink nodes from a weakly
connected directed graph and permits to get the sub-
graph induced by the union of its strongly connected
components. Note that the dictionary graph (DD)
has no sinks (i.e., words that never get defined) and
that it contains a giant SCC whose size is compara-
ble to the one of the initial graph.
It turns out that the FA graph also contains a giant
SCC, therefore getting the core consists more simply
in extracting the main SCC of the initial graph. We
use Tarjan?s algorithm (Tarjan, 1972) for isolating
the FA core.
4.3 Vertex degree analysis
The FA core has a maximum in-degree of 313, a
maximum out-degree of 33 and an average degree
of 25.42. The in-degree distribution follows a power
law (? = 1.93) and the out-degrees are Poisson-like
distributed with a peak at 14 (Steyvers and Tenen-
baum, 2005; Gravino et al, 2012).
Words having a high in-degree are targets that
tend to be cited more frequently. On the other hand,
words having a high out-degree are cues that evoke
many different targets.
The most evocative cues are, in decreasing order
of out-degree: field (33), body (31), condemn (29),
farmer (29), crisis (28), plan (28), attention (27),
animal (27), and hang (27). Interestingly, the most
cited targets (i.e., targets with highest in-degree) are
in decreasing order: food (313), money (295), water
(271), car (251), good (246), bad (221), work (187),
house (183), school (182), love (179).
4.4 Cycle decomposition of the core
We define the vertex k-cycle multiplicity
(resp. edge k-cycle multiplicity) as the num-
ber of k-cycles a given vertex (resp. edge) belongs
to. We call core-ER the set of Erdo?s-Re?nyi (ER)
random graphs G(n,M) having the same number
of nodes and the same number of edges as the FA
2 4 6 8 10 12 14 1610
1
102
103
104
Cycles length
#
of
sh
or
te
st
cy
cl
es
core
ER
Figure 1: Distribution of shortest cycles lengths in the
core compared to equivalent ER models
One should bear in mind that we only consider the set of short-
est cycles. Thus, a k-cycle is not counted if each of its nodes
belongs to a cycle whose length is < k. Although the num-
ber of 4-shortest cycles is comparable in the core and core-ER
graphs for example, there are in reality far more 4-cycles in the
core (i.e., 42?738 versus 6?517). We see that when considering
shortest cycles, short cycles tend to hide long ones, and, as a
large proportion of nodes in the core belong to 2- and 3-cycles,
many longer cycles do not get counted at all.
core. We start by extracting the 2- and 3-cycles by
using a customized version of Johnson?s algorithm
(Johnson, 1975). The first thing we observe is that
the core has a very high density of short cycles: the
subset of nodes belonging to 2-cycles (i.e., nodes
with 2-cycle multiplicities > 0) cover 95% of the
core vertices and the 3-cycles cover 88% of the
core vertices. The corresponding core-ER graphs
have on average about 100 times fewer 2-cycles and
almost 20 times fewer 3-cycles.
This shows that the core is very cyclic in nature
and that it remains very well connected for short-
length cycles: most vertices of the core indeed be-
long to at least one co-link or triangle.
In order to limit computation times, we only con-
sidered shortest cycles for lengths ? 3 and analyzed
the distribution of the number of shortest cycles
in the core compared to equivalent random graphs.
Whereas there are many more short cycles in the
core, we observe a predominance of 4, 5 and 6-
cycles in core-ER graphs. However, we find again a
slight predominance of long cycles (length between
7 and 15) in the core (see Fig. 1). See (Levary et al,
2012), Fig. 3, where the cycle distribution is very
different, with a minimum at length 5.
672
4.5 Interpretation of cycles
2-cycles are composed of concretely related words
(e.g., drug-coke, destiny-fate, einstein-genius, . . . ).
The vertex with highest 2-cycle multiplicity is music
(22).
Words in 3- and 4-cycles often belong to the
same lexical field. Examples of 3-cycles: protect-
guard-defend or space-universe-star. The vertex
(resp. edge) with highest 3-cycle multiplicity is
car (86) (resp. bad-crime (11)). Examples of 4-
cycles: monster-dracula-vampire-ghost or flu-virus-
infection-sick.
Longer cycles are more difficult to describe: Re-
lations linking words of a given cycle exhibit se-
mantic drift with increasing length (cf. (Levary et
al., 2012)). Examples of 5-cycles: yellow-coward-
chicken-soup-noodles and sleep-relax-music-art-
beauty.
The cumulated set of free associations reflects the
way in which a group of people retrieved its seman-
tic knowledge. As the associated graph is highly
circular, this suggests that this knowledge is not
stored in a hierarchical way (Steyvers and Tenen-
baum, 2005). The large predominance of short cy-
cles in the core may indeed be a natural conse-
quence of the semantic information being acquired
by means of associative learning (Ashcraft and Rad-
vansky, 2009; Shanks, 1995).
4.6 FA core clustering
4.6.1 The walktrap community algorithm
Complex networks are globally sparse but con-
tain locally dense subgraphs. These groups of highly
interconnected vertices are called communities and
convey important properties of the network.
Although the notion of community is difficult to
define formally, the current consensus establishes
that a partition P = {C1, C2, . . . , Ck} of the ver-
tex set of a graph G represents a good community
structure if the proportion of edges inside the Ci is
higher than the proportion of edges between them
(Fortunato, 2010).
Computing such communities in a large graph is
generally computationally expensive (Lancichinetti
and Fortunato, 2009). We use the so-called ?Walk-
trap? community detection algorithm (Pons and Lat-
apy, 2006) for extracting communities from the FA
networks. The idea lying behind this algorithm is
that random walks on a graph will tend to get trapped
in the densely connected subgraphs.
Let P tij be the probability of going from vertex i
to vertex j through a random walk of length t. The
distance between two vertices i and j of the graph is
defined as:
rij(t) =
?
?
?
?
n?
k=1
(P tik ? P
t
jk)
2
d(k)
where d(k) is the degree of vertex k.
One defines the probability P tC,j to go from
community C to vertex j in t steps: P tC,j =?
i?C P
t
ij/|C|, and then the distance is easily gen-
eralized for two communities C1, C2.
The algorithm starts with a partition P1 = {{v} ?
V } of the initial graph into n communities each of
which is a single vertex. At each step, two communi-
ties are chosen and merged according to the criterion
described below and the distances between commu-
nities are updated. The process goes on until we ob-
tain the partition Pn = {V }.
In order to reduce complexity, only adjacent com-
munities are considered for merging. The decision
is then made according to Ward?s method (Everitt
et al, 2001): at each step k, the two communities
that minimize the mean ?k of the squared distances
between each vertex and its community are merged:
?k =
1
n
?
C?Pk
?
i?C
r2iC
4.6.2 Clustering of the core
We first identify the communities of the FA core
using the Walktrap algorithm. We immediately
observe that when the path length parameter in-
creases, the number of identified communities de-
creases (i.e., for a length of 2, we find 35 communi-
ties whereas for a length of 9, we only find 8 com-
munities).
For a path length of 2, the algorithm extracts 35
communities, 7 of which contain more than 100 ver-
tices, 3 of which contain between 100 and 50 ver-
tices and 25 of which contain less than 50 vertices.
We observe that for most small communities (i.e.,
the ones containing less than 50 vertices), there ex-
ists a clear relation between the labels of their ver-
673
tices. Typically, the labels are part of the same lexi-
cal field (e.g., all the planets (except earth) or related
by a common grammatical function (such as why,
where, what, . . . ).
4.6.3 Clustering of the core co-links
We define the k-cycle induced subgraph of a
graph G as the subgraph of G induced by the set
of its vertices with k-cycle multiplicity > 0.
The co-link graph of a graphG(V,E) is the undi-
rected graph obtained by replacing each co-link (i.e.,
2-cycle) of the 2-cycle induced subgraph of G by a
single undirected edge and removing all other edges.
The co-link graph of the FA core has 4?508 ver-
tices and 8?309 edges for a density of 8?10?4. It
is composed of a single weakly connected compo-
nent that can be seen as a projection of the strongest
semantic links from the original graph. Extracting
the co-link graph is thus an efficient way of select-
ing the set of most important semantic links (i.e., the
set of 2-cycles that appear in large predominance in
the core compared to what is found in an equivalent
random graph) while filtering out the noisy or negli-
gible ones.
The sets of communities extracted by the Walk-
trap algorithm exhibit different degrees of granular-
ity depending on the length parameter. For short
paths, a large number of very small communities are
returned (e.g., 923 communities when length equals
2) whereas for longer paths the average size of the
communities increases more and more.
The community detection exhibits thus a far finer
degree of granularity for the core co-links graph than
for the core itself. The size of the communities being
much smaller in average, it is striking to notice to
which extent the words of a given community are
semantically related.
Examples of communities found in the core co-
links graph include (standards, values, morals,
ethics), (hopeless, romantic, worthless, useless),
(thesaurus, dictionary, vocabulary, encyclopedia)
or (molecule, atom, electron, nucleus, proton, neu-
tron).
4.6.4 DD core clustering vs FA core clustering
The clustering of both cores has very different
characteristics: We illustrate the neighborhoods of
conflict for both cases in Fig. 2 and 3.
quarrel
personality
confusion
anger
disagreement
dilemma
fight
war
battle
trouble
conflict
argument
struggle
disagree
schedule
frustration
problem
Figure 2: Neighborhood of conflict in the FA core
The set of words belonging to the neighborhood of conflict are
clearly part of the same lexical field. The high density of co-
links leads to cyclicity and we see that many directed triangles
are present in the local subgraph (e.g., conflict-trouble-fight,
conflict-argument-disagree). We can even find triangles of co-
links that link together words semantically strongly related (e.g.,
fight-war-battle, fight-quarrel-argument). Nodes that are part of
the neighborhood of conflict in both FA and DD are in empty
circles.
On one hand, the words in communities of the DD
core are in most cases either synonyms, e.g., (decla-
ration, assertion, claim) or an instance-of kind of
relation, e.g., (signal, gesture, motion) or (zero, inte-
ger).
On the other hand, communities of the FA core
are generally composed of words belonging to the
same lexical field and sharing the same level of ab-
straction.
Moreover, we notice that it is often difficult to es-
tablish the semantic relation existing between words
of many small communities (i.e., containing less
than 10 words) of the DD core. Two such examples
are: (choice, probate, executor, chosen, certificate,
testator, will) and (numeral, monarchy, monarch,
crown, significance, autocracy, symbol, interpreta-
tion).
The comparison of DD and FA reveals, in a quan-
titative way, fundamental differences between the
two realms. The interesting data are shown in ta-
ble 1.
674
friction
war
conflict
two
disagreement
group
Figure 3: Neighborhood conflict in the DD core
First, we note that the neighborhood has a lower density than
in the FA core. We also see that there is no cycle and there
seems to be a flow going from source nodes to sink nodes. As
it generally happens in the neighborhood subgraphs of the DD
core, source nodes are rather specific words whereas sink nodes
are generic words.
FA core DD core
# vertices 4?843 1?496
# edges 61?544 4?766
density 2.5?10?3 2.1?10?3
avg degree 25.4 6.37
max in-degree 313 59
directed diameter 10 29
characteristic path length 4.26 10.42
efficiency 2.5?10?1 1.2?10?1
clustering coefficient 8.5?10?2 5.1?10?2
assortativity 5.5?10?2 6.1?10?2
Table 1: Comparison FA vs DD
Note that while the FA core is in fact larger than
the DD core, its diameter is smaller. This illustrates
in a beautiful way the nature of free association as
compared to the more neutral dictionary. In par-
ticular, the characteristic path length is smaller in
the FA graph, because humans use generalized event
knowledge (McRae and Matsuki, 2009) in free asso-
ciation, producing semantic shortcuts. For example,
FA contains a direct link mirage?water, whereas
in DD, the shortest path between the two words is
mirage?refraction?wave?water.
5 The Bricks of Meaning
5.1 Extraction of the seed
We already saw that most vertices of the core be-
long to directed 2- and 3-cycles. Whereas 2-cycles
establish strong semantic links (i.e., synonymy or
antonymy relations) and provide cyclicity to the un-
derlying directed graph, we claim that 3-cycles (i.e.,
triangles) form the set of elementary concepts of the
core.
These structues are common to DD and to FA, but
we will see that the links in FA are somehow more
direct than in DD.
We call seed the subgraph of the core induced by
the set V3 of vertices belonging to directed triangles
and shell the subgraph of the core induced by the
set V \V3 (i.e., the set of vertices with a null 3-cycle
multiplicity), see Fig. 4.
Initial graph
core
sh
el
l
se
ed
Figure 4: Composition of the FA graph
The graph of FA contains a giant SCC (the core). The subgraph
of the core induced by the set of nodes belonging to at least one
triangle also forms a giant component we call the ?seed?. The
subgraph of the core induced by the set of nodes not belonging
to any triangle is called the ?shell? and is composed of many
small SCCs, including single vertices. Although the shell has a
low density, its nodes are very well connected to the seed.
The shell contains 530 nodes and 309 edges.
There are 7?035 edges connecting the shell to the
seed. The shell consists of of many small SCCs and
although its average degree is low (1.17), its ver-
tices have on average many (13.27) connections to
the seed.
The seed contains 4?313 vertices (89% of the
core) and 54?197 edges. The first thing to notice
is that it has 100 times more co-links (7?895) and
20 times more triangles (13?119) than an equivalent
random graph. We call shortcuts the 32?773 edges
of the seed that do not belong to 3-cycles, see Fig. 5.
The seed obviously also contains cycles whose
length is greater than 3. One can check that there ex-
ist only 5 basic motifs involving 2 attached triangles
and 1 shortcut for creating 4- and 5-cycles, and that
linking 2 isolated triangles with 2 shortcuts also per-
675
SFigure 5: Shortcut edges between two triangles sharing a
single vertex S
Two triangles can share 0, 1 or 2 vertices. For each of these three
basic motifs, we count the maximum number of shortcut edges
(i.e., edges not belonging to 3-cycles) that can be added. By
linking two triangles, these shortcuts permit to move two basic
semantic units closer together and create longer cycles (i.e., 4,
5, and 6-cycles). Long cycles can be thus considered as group-
ings of basic semantic units. In the case of two triangles sharing
one vertex for example, it is possible to add at most 6 short-
cuts, whereas, for two triangles sharing two vertices, at most 2
shortcuts can be added.
mit to form 4-, 5- and 6-cycles. All longer cycles are
simply made of a juxtaposition of these basic motifs.
Furthermore, there is a limit on the number of
shortcuts that can possibly be added in the seed be-
fore it gets saturated, as all its vertices belong to at
least one triangle. We show that at most 16 shortcuts
can be added between two isolated triangles, at most
6 between 2 triangles sharing 2 vertex and at most 2
between 2 triangles sharing 2 vertices (see Fig. 5).
5.2 The elementary lexical fields
Once the seed is isolated, we go on digging into its
structure. We focus on the arrangements of triangles
as they constitute the set of elementary concepts.
We start by removing all shortcuts from the seed
and convert it then to an undirected graph, in order
to get a homogeneous simplicial 2-complex.
Let t be the graph operator which transforms a
graph G into the intersection graph tG of its 2-
simplices (i.e., triangles sharing an edge). We apply
t to the homogeneous simplicial 2-complex found
previously. The result represents the links between
the basic semantic units of the seed. We call seed-
crux the giant WCC in the intersection graph.
We enumerate the 8?380 maximal cliques of FA
seed-crux and get the list of words composing each
Distance Acc ? KS
original ? 0.404 30
1 74 0.522 42
2 97 0.899 89
? 99 0.899 89
Table 2: Accuracy, ?, and count(p < 0.05) for KS
clique. By removing the ones that are subsets of big-
ger lists, we finally obtain 3?577 lists of words .
These lists of words have a rather small and ho-
mogeneous size (between 4 and 17) and 95% have
a size comprised between 4 and 10. More in-
terestingly, they clearly define well-delimited lexi-
cal fields. We will show this through two experi-
ments in the following sections. A few examples
include (honest, trustworthy, reliable, responsible),
(stress, problem, worry, frustration) and (data, pro-
cess, computer, information).
From a topological perspective, we deduce that
bunches of triangles (i.e., cliques of elementary con-
cepts) span the seed in a homogeneous way. These
bunches form a set of cohesive lexical fields and
constitute essential bricks of semantic knowledge.
5.3 Semantic similarity of the lexical fields
In order to quantify the relative meaning of words
in the lexical fields of the seed-crux, we define the
following semantic similarity metric based on the
Wordnet WUP metric (Wu and Palmer, 1994) for a
given set of words L:
S`(L) = 2
?
wi,wj?L,wi 6=wj
Sw(wi, wj)/(|L|(|L| ? 1))
where Sw(wi, wj) = max
Sk3wiandS`3wj
{wup(Sk, S`)}
and wup is the WUP semantic metric and Sk and S`
are Wordnet synsets.
The average value of S` for the set of cliques of
seed-crux is 0.6 whereas it is only 0.43 for randomly
sampled set of words. This suggests the correspond-
ing lists of words are indeed semantically related.
We will show the strength of this relation in the fol-
lowing experiment with human raters.
5.4 Human evaluation of the lexical fields
To validate our findings, we conducted an empirical
evaluation through human annotators. Starting from
676
the 1?204 4-groups, we designed the following ex-
periment: We corrupt the groups by exchanging one
of the 4 elements with a randomly chosen word at a
distance from the group of 1, 2, and ?infinity? (i.e.,
any word of the whole core). We presented 100 ran-
dom samples for each of the 3 distances as well as
100 unperturbed groups (original) to annotators at
Amazon Mechanical Turk1, asking which word fits
the group the least. Intuitively, the closer the ran-
domly chosen words get to the group, the closer the
distribution of the votes for each sample should be
to the uniform distribution. We collected 10 votes
for each of the 4 problems of 100 random samples.
We calculated accuracy (i.e., the relative frequency
of correctly identified random words) for the 3 ran-
dom confounder experiments and Fleiss? ?. Fur-
ther, we used the Kolmogorov-Smirnov (KS) test for
how uniform the label distribution is, reporting the
relative frequency of samples that are significantly
(p < 0.05) different from the uniform distribution.
The results of this experiment are summarized in Ta-
ble 2 and show clearly that the certainty about the
?odd man out? increases together with the distance.
5.5 Error analysis
If we view our results as a resource for a downstream
task, it is important to know about possible down-
sides. First, we note that there are words which are
not in a triangle and will thus be missing in the in-
tersection graph. This is an indication that the corre-
sponding word is less well embedded contextually,
so conversely, any prediction made about it from the
data may be less reliable. Additionally, semantic
leaps caused by generalized event knowledge may
lead to lesser-connected groups such as (steel, pipe,
lead, copper). Jumps like these may or may not be
desired in a subsequent application.
6 The Case of the EAT FA dataset
The Edinburgh Associative Thesaurus (EAT) (Kiss
et al, 1973) is a large dataset of free associations.
We extract the EAT FA seed-crux with the previ-
ously described methods.
We start by generating the initial graph (23?219
vertices and 325?589 edges), then extract its core
(7?754 vertices and 247?172 edges) and its seed
1http://www.mturk.com
(7?500 vertices and 238?677 edges). It is interest-
ing to notice at this stage that the EAT seed contains
74% of the words belonging to the USF seed. Af-
ter generating the seed-crux which contains 63?363
vertices, 6?825?731 edges, and 342?490 maximal
cliques, we finally obtain 40?998 lists of words.
These lists comprise between 4 and 233 words but
80% of them have a relatively small size between 4
and 20. Although we find exceptions for this graph,
most of the extracted lists again form well-delimited
lexical fields (e.g., (health, resort, spa, bath, salts) or
(god, devil, angel, satan).
Comparing the two association experiments, we
see that the local topologies are quite similar. Both
FA cores have a high density of connected trian-
gles, whereas cycles in the DD graph tend to be
longer and most triangles are isolated. This can be
attributed to the different ways in which DD and FA
are obtained, the former being built rationally by fol-
lowing a humanly-driven process and the latter re-
flecting an implicit collective semantic knowledge.
7 Related Work
A number of metrics like Latent Semantic Analy-
sis (Deerwester et al, 1990) and Word Association
Spaces (Steyvers et al, 2004) have been recently
developed for quantifying the relative meaning of
words. As the topological properties of free associ-
ation graphs reflect key aspects of semantic knowl-
edge, we believe some graph theory metrics could
be used efficiently to derive new ways of measuring
semantic similarity between words.
Topological analysis of the Florida Word Associa-
tions (FA) was started by (Steyvers and Tenenbaum,
2005; Gravino et al, 2012), who extracted global
statistics. We follow the basic methodology of these
studies, but extend their approach. First, we conduct
deeper analyses by examining the neighborhood of
nodes and extracting the statistics of cycles. Second,
we compare the properties of FA and DD graphs.
Word clustering based on graphs has been the sub-
ject of various earlier studies. Close to our work
is (Widdows and Dorow, 2002). These authors rec-
ognize that nearest-neighbor-based clustering of co-
occurrence give rise to semantic groups. This type of
approach has since been applied in various modified
forms, e.g., by (Biemann, 2006) who performs label-
677
propagation based on randomized nearest neighbors,
or Matsuo et al (2006) who perform greedy cluster-
ing. Hierarchical clustering algorithms (e.g., (Jonyer
et al, 2002; Manning et al, 2008)) are related as
well, however, the key difference is that in hierarchi-
cal clustering, the granularity of a cluster is difficult
to determine.
Dorow et al (2005) recognize that triangles form
semantically strongly cohesive groups and apply
clustering coefficients for word sense disambigua-
tion. Their work focuses on undirected graphs of
corpus co-occurrences whereas our work builds on
directed associations. Building on this work, we
take finer topological graph structures into account,
which is one of the main contributions in this paper.
8 Conclusion
The cognitive process of discrete free association be-
ing an epiphenomenon of our semantic memory at
work, the cumulative set of free associations of the
USF dataset can be viewed as the projection of a col-
lective semantic memory.
To analyze the semantic memory, we use the tools
of graph theory, and compare it also to dictionary
graphs. In both cases, triangles play a crucial role
in the local topology and they form the set of ele-
mentary concepts of the underlying graph. We also
show that cohesive lexical fields (taking the form
of cliques of concepts) constitute essential bricks of
meaning, and span the core homogeneously at the
global level; 89% of all words in the core belong
to at least one triangle, and 77% belong to cliques
of triangles containing 4 words (i.e., pairs of trian-
gles sharing an edge or forming tetrahedras). As the
words of a graph of free associations acquire their
meaning from the set of associations they are in-
volved in (Deese, 1962), we go a step further by
examining the neighborhood of nodes and extracting
the statistics of cycles. We further check through hu-
man evaluation that the clustering is strongly related
to meaning, and furthermore, the meaning becomes
measurably more confused as one walks away from
a cluster.
-? -?I call the pairs of triangles sharing an edge
the 2-clovers ;-)
Comparing dictionaries to free association, we
find the free association graph being more concept
driven, with words in small clusters being on the
same level of abstraction. Moreover, we think that
graphs of free associations could find interesting
applications for Word Sense Disambiguation (e.g.,
(Heylighen, 2001; Agirre and Soroa, 2009)), and
could be used for detecting psychological disorders
(e.g., depression, psychopathy) or whether someone
is lying (Hancock et al, 2013; Kent and Rosanoff,
1910).
Finally, we believe that studying the dynamics of
graphs of free associations may be of particular in-
terest for observing the change in meaning of certain
words (Deese, 1967), or more generally to follow the
cultural evolution arising among a social group.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ?09, pages 33?41, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark H Ashcraft and Gabriel A Radvansky. 2009. Cog-
nition. Pearson Prentice Hall.
Chris Biemann. 2006. Chinese whispers: an efficient
graph clustering algorithm and its application to natu-
ral language processing problems. In Proceedings of
the First Workshop on Graph Based Methods for Natu-
ral Language Processing, TextGraphs-1, pages 73?80,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Be?la Bolloba?s. 2001. Random graphs, volume 73 of
Cambridge Studies in Advanced Mathematics. Cam-
bridge University Press, Cambridge, second edition.
Etienne Brunet. 1974. Le traitement des faits
linguistiques et stylistiques sur ordinateur. Texte
d?application: Giraudoux, Statistique et Linguistique.
David, J. y Martin, R.(eds.). Paris: Klincksieck, pages
105?137.
Scott Deerwester, Susan T. Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American society for information science, 41(6):391?
407.
James Deese. 1962. On the structure of associative
meaning. Psychological review, 69:161.
James Deese. 1967. Meaning and change of meaning.
The American psychologist, 22(8):641.
Beate Dorow, Dominic Widdows, Katerina Ling, Jean-
Pierre Eckmann, Danilo Sergi, and Elisha Moses.
678
2005. Using curvature and Markov clustering in
graphs for lexical acquisition and word sense discrim-
ination. In MEANING-2005, 2nd Workshop organized
by the MEANING Project, February 3rd-4th 2005,
Trento, Italy.
Jean-Pierre Eckmann and Elisha Moses. 2002. Curva-
ture of co-links uncovers hidden thematic layers in
the World Wide Web. Proc. Natl. Acad. Sci. USA,
99(9):5825?5829 (electronic).
Brian Everitt, Sabine Landau, and Morven Leese. 2001.
Cluster analysis. 4th Edition. Arnold, London.
Santo Fortunato. 2010. Community Detection in Graphs.
Physics Reports, 486(3):75?174.
Pietro Gravino, Vito DP Servedio, Alain Barrat, and Vit-
torio Loreto. 2012. Complex structures and semantics
in free word association. Advances in Complex Sys-
tems, 15(03n04).
Jeffrey T Hancock, Michael T Woodworth, and Stephen
Porter. 2013. Hungry like the wolf: A word-pattern
analysis of the language of psychopaths. Legal and
Criminological Psychology, 18(1):102?114.
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 395?403. Association for
Computational Linguistics.
Francis Heylighen. 2001. Mining associative meanings
from the web: from word disambiguation to the global
brain. In Proceedings of Trends in Special Language
& Language Technology, pages 15?44.
Donald B Johnson. 1975. Finding all the elementary
circuits of a directed graph. SIAM Journal on Com-
puting, 4(1):77?84.
Istvan Jonyer, Diane J Cook, and Lawrence B Holder.
2002. Graph-based hierarchical conceptual clustering.
The Journal of Machine Learning Research, 2:19?43.
Grace H Kent and Aaron J Rosanoff. 1910. A study of
association in insanity. American Journal of Insanity.
George R Kiss, Christine Armstrong, Robert Milroy, and
James Piper. 1973. An associative thesaurus of en-
glish and its computer analysis. The computer and lit-
erary studies, pages 153?165.
Andrea Lancichinetti and Santo Fortunato. 2009. Com-
munity detection algorithms: A comparative analysis.
Physical review E, 80(5):056117.
David Levary, Jean-Pierre Eckmann, Elisha Moses, and
Tsvi Tlusty. 2012. Loops and self-reference in the
construction of dictionaries. Phys. Rev. X, 2:031018.
Christopher D Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to information re-
trieval, volume 1. Cambridge University Press Cam-
bridge.
Yutaka Matsuo, Takeshi Sakaki, Ko?ki Uchiyama, and
Mitsuru Ishizuka. 2006. Graph-based word cluster-
ing using a web search engine. In Proceedings of
the 2006 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?06, pages 542?550,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ken McRae and Kazunaga Matsuki. 2009. People use
their knowledge of common events to understand lan-
guage, and do so as quickly as possible. Language and
linguistics compass, 3(6):1417?1429.
George Miller and Christiane Fellbaum. 1998. WordNet:
An Electronic Lexical database. MIT Press, Cam-
bridge, MA.
George A Miller. 1995. WordNet: a lexical database for
english. Communications of the ACM, 38(11):39?41.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida free
association, rhyme, and word fragment norms. Be-
havior Research Methods, Instruments, & Computers,
36(3):402?407.
David S Palermo and James J Jenkins. 1964. Word asso-
ciation norms: Grade school through college. Univer-
sity of Minnesota Press.
Olivier Picard, Alexandre Blondin-Masse?, Stevan Har-
nad, Odile Marcotte, Guillaume Chicoisne, and Yas-
sine Gargouri. 2009. Hierarchies in dictionary defini-
tion space. In Annual Conference on Neural Informa-
tion Processing Systems.
Pascal Pons and Matthieu Latapy. 2006. Computing
communities in large networks using random walks.
In Journal of Graph Algorithms and Applications,
pages 284?293. Springer.
Christian Scheible. 2010. Sentiment translation through
lexicon induction. In Proceedings of the ACL 2010
Student Research Workshop, pages 25?30, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
David R Shanks. 1995. The psychology of associative
learning, volume 13. Cambridge University Press.
Mark Steyvers and Joshua B Tenenbaum. 2005. The
large-scale structure of semantic networks: Statistical
analyses and a model of semantic growth. Cognitive
Science, 29(1):41?78.
Mark Steyvers, Richard M Shiffrin, and Douglas L Nel-
son. 2004. Word association spaces for predicting
semantic similarity effects in episodic memory. Ex-
perimental cognitive psychology and its applications:
Festschrift in honor of Lyle Bourne, Walter Kintsch,
and Thomas Landauer, pages 237?249.
Robert Tarjan. 1972. Depth-first search and linear graph
algorithms. SIAM journal on computing, 1(2):146?
160.
679
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Pro-
ceedings of the 19th international conference on Com-
putational linguistics - Volume 1, COLING ?02, pages
1?7, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, pages 133?138. Association for Computa-
tional Linguistics.
680
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 200?204,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Multi-Domain Sentiment Relevance Classification with Automatic
Representation Learning
Christian Scheible
Institut f?ur Maschinelle Sprachverarbeitung
University of Stuttgart
scheibcn@ims.uni-stuttgart.de
Hinrich Sch?utze
Center for Information
and Language Processing
University of Munich
Abstract
Sentiment relevance (SR) aims at identify-
ing content that does not contribute to sen-
timent analysis. Previously, automatic SR
classification has been studied in a limited
scope, using a single domain and feature
augmentation techniques that require large
hand-crafted databases. In this paper, we
present experiments on SR classification
with automatically learned feature repre-
sentations on multiple domains. We show
that a combination of transfer learning and
in-task supervision using features learned
unsupervisedly by the stacked denoising
autoencoder significantly outperforms a
bag-of-words baseline for in-domain and
cross-domain classification.
1 Introduction
Many approaches to sentiment analysis rely on
term-based clues to detect the polarity of sentences
or documents, using the bag-of-words (BoW)
model (Wang and Manning, 2012). One drawback
of this approach is that the polarity of a clue is
often treated as fixed, which can be problematic
when content is not intended to contribute to the
polarity of the entity but contains a term with a
known lexical non-neutral polarity.
For example, movie reviews often have plot
summaries which contain subjective descriptions,
e.g., ?April loves her new home and friends.?, con-
taining ?loves?, commonly a subjective positive
term. Other domains contain different types of
nonrelevant content: Music reviews may contain
track listings, product reviews on retail platforms
contain complaints that do not concern the prod-
uct, e.g., about shipping and handling. Filtering
such nonrelevant content can help to improve sen-
timent analysis (Pang and Lee, 2004). Sentiment
relevance (Scheible and Sch?utze, 2013; Taboada
et al., 2009; T?ackstr?om and McDonald, 2011) for-
malizes this distinction: Content that contributes
to the overall sentiment of a document is said to
be sentiment relevant (SR), other content is senti-
ment nonrelevant (SNR).
The main bottleneck in automatic SR classifi-
cation is the lack of annotated data. On the sen-
tence level, it has been attempted for the movie
review domain (Scheible and Sch?utze, 2013) on
a manually annotated dataset that covers around
3,500 sentences. The sentiment analysis data by
T?ackstr?om and McDonald (2011) contains SR an-
notations for five product review domains, four of
which have fewer than 1,000 annotated examples.
As the amount of labeled data is low, we adopt
transfer learning (TL, (Thrun, 1995)), which has
been used before for SR classification. In this
setup, we train a classifier on a different task, using
subjectivity-labeled data ? for which a large num-
ber of annotated examples is available ? and ap-
ply it for SR classification. To enable knowledge
transfer between the tasks, feature space augmen-
tation has been proposed. For this purpose, we em-
ploy automatic representation learning, using the
stacked denoising autoencoder (SDA, (Vincent et
al., 2010)) which has been applied successfully to
other domain adaptation problems such as cross-
domain sentiment analysis (Glorot et al., 2011).
In this paper, we present experiments on both
multi-domain and cross-domain SR classification.
We show that compared to the in-domain base-
line, TL with SDA features increases F
1
by 6.8%
on average. We find that domain adaptation using
TL with the SDA compensates for strong domain
shifts, reducing the average classification transfer
loss by 12.7%.
2 Stacked Denoising Autoencoders
The stacked denoising autoencoder (SDA, (Vin-
cent et al., 2010)) is a neural network (NN) model
for unsupervised feature representation learning.
200
An autoencoder takes an input vector x, uses an
NN layer with a (possibly) nonlinear activation
function to generate a hidden feature representa-
tion h. A second NN layer reconstructs x at the
output, minimizing the error.
Denoising autoencoders reconstruct x from a
corrupted version of the input, x?. As the model
learns to be robust to noise, the representations are
expected to generalize better. For discrete data,
masking noise is a natural choice, where each in-
put unit is randomly set to 0 with probability p.
Autoencoders can be stacked by using the h
i
produced by the i
th
autoencoder as the input to
the (i+1)
th
one, yielding the representation h
i+1
.
The h of the topmost autoencoder is the final rep-
resentation output by the SDA. We let k-SDA de-
note a stack of k denoising autoencoders.
Chen et al. (2012) introduced a marginalized
closed-form version, the mSDA. We opt for this
version as it is faster to train and allows us to use
the full feature space which would be inefficient
with iterative backpropagation training.
3 Task and Experimental Setup
The task in this paper is multi- and cross-domain
SR classification. Two aspects motivate our work:
First, we need to address the sparse data situa-
tion. Second, we are interested in how cross-
domain effects influence SR classification. We
classify SR in three different setups: in-domain
(ID), in which we take the training and test data
from the same domain; domain adaptation (DA),
where training and test data are from different do-
mains; and transfer learning (TL), where we use a
much larger amount of data from a different but re-
lated task. To improve the generalization capabili-
ties of the models, we use representations learned
by the SDA. We will next describe our classifica-
tion setup in more detail.
Data We use the following datasets for our ex-
periments. Table 1 shows statistics on the datasets.
CINEMA: The movie SR data (CINEMA)
by Scheible and Sch?utze (2013) contains SR-
annotated sentences for the movie review domain.
Ambiguous sentences are marked as unknown; we
exclude them.
PRODUCTS: The multi-domain product data
(PRODUCTS) by T?ackstr?om and McDonald (2011)
contains labeled sentences from five Amazon.com
product review domains: BOOKS, DVDS, electron-
ics (EL), MUSIC, and video games (VG). This
Dataset #doc #sent #SR #SNR
CINEMA 125 3,487 2,759 728
PRODUCTS 294 3,836 2,689 1,147
?BOOKS 59 739 424 315
?DVDS 59 799 524 275
?ELECTRONICS 57 628 491 137
?MUSIC 59 638 448 190
?VIDEOGAMES 60 1032 802 230
P&L ? 10,000 5,000 5,000
UNLAB 7,500 68,927 ? ?
Table 1: Dataset statistics
dataset differs from CINEMA firstly in the product
domains (except obviously for DVDS which also
covers movies). Secondly, the data was collected
from a retail site, which introduces further facets
of sentiment nonrelevance, as discussed above.
Thirdly, the annotation style has no unknown cat-
egory: ambiguous examples are marked as SR.
P&L: The subjectivity data (P&L) by Pang and
Lee (2004) serves as our cross-task training data
for transfer learning. The dataset was heuristically
created for subjectivity detection on the movie do-
main by sampling snippets from Rotten Tomatoes
as subjective and sentences from IMDb plot sum-
maries as objective examples.
UNLAB: To improve generalization on PROD-
UCTS, we use additional unlabeled sentences
(UNLAB) for SDA training. We extract the sen-
tences of 1,500 randomly selected documents for
each of the five domains from the Amazon.com
review data by Jindal and Liu (2008).
SDA setup We train the SDA with 10,000 hid-
den units and tanh nonlinearity on the BoW fea-
tures of all available data as the input. We opti-
mize the noise level p with 2-fold cross-validation
on the in-domain training folds.
Classification setup We perform SR classifica-
tion with a linear support vector machine (SVM)
using LIBLINEAR (Chang and Lin, 2011). We
perform 2-fold cross-validation for all training
data but P&L. We report overall macro-averaged
F
1
over both folds. The feature representation for
the SVM is either bag of words (BoW) or the k-
SDA output. Unlike Chen et al. (2012), we do not
use concatenations of BoW and SDA vectors as
we found them to perform worse.
Evaluation As class distributions are heavily
skewed, we use macro-averaged F
1
(s, t) (train-
ing on s and evaluating on t) as the basic eval-
uation measure. We evaluate DA with transfer
loss, the difference in F
1
of a classifier CL with
201
Features Setup CINEMA BOOKS DVDS EL MUSIC VG ?
1 Majority BL ? 39.6 28.9 32.6 39.2 35.1 39.0 35.7
2 BoW ID 74.0 57.5 49.8 55.1 55.5 55.0 58.4
3 1-SDA ID 73.6 55.3 48.4 43.8 41.8 44.1 52.6
4 2-SDA ID 76.0 54.5 52.5 43.9 41.2 46.7 53.6
5 BoW TL 71.5 60.7 60.2 50.3 55.1 53.2 59.6
6 1-SDA TL 73.3 62.9 60.6 59.0 59.9 57.0 63.1
7 2-SDA TL 76.2 62.9 65.8 59.7 59.9 60.5 64.9
8 BoW ID+TL 76.6 63.5 61.7 52.4 56.7 57.0 62.3
9 1-SDA ID+TL 79.0 62.7 62.1 57.7 57.8 57.4 63.9
10 2-SDA ID+TL 80.4 62.7 65.2 59.0 58.7 58.9 65.2
Table 2: Macro-averaged F
1
(%) evaluating on each test domain on both folds. ? = row mean. Bold:
best result in each column and results in that column not significantly different from it.
respect to the in-domain baseline BL: L(s, t) =
F
(BL)
1
(t, t)?F
(CL)
1
(s, t). L is negative if the clas-
sifier surpasses the baseline. As a statistical sig-
nificance test (indicated by
?
in the text), we use
approximate randomization (Noreen, 1989) with
10,000 iterations at p < 0.05.
4 Experiments
In-Domain Classification (ID) Table 2 shows
macro-averaged F
1
for different SR models. We
first turn to fully supervised SR classification with
bag-of-words (BoW) features using ID training
(line 2). While the results for CINEMA are high,
on par with the reported results in related work,
they are low for the PRODUCTS data. This is not
surprising as the SVM is trained with fewer than
600 examples on each domain. Also, no unknown
category exists in the latter dataset. While am-
biguous examples on CINEMA are annotated as un-
known, they receive an SR label on PRODUCTS.
Thus, many examples are ambiguous and thus dif-
ficult to classify. SDA features worsen results
significantly
?
(lines 3?4) on all domains except
CINEMA and DVDS due to data sparsity. They are
the two most homogeneous domains where plot
descriptions make up a large part of the SNR con-
tent. On many domains, there is no single proto-
typical type of SNR which could be learned from
a small amount of training data.
Transfer Learning (TL) TL with training on
P&L and evaluation on CINEMA/PRODUCTS with
BoW features (line 5) performs slightly worse than
ID classification, except on BOOKS and DVDS
where we see strong improvements. This result
is easy to explain: Both BOOKS and DVDS contain
SNR descriptions of narratives, which are covered
well in P&L. This distinction is less helpful on
domains like EL where SNR content is different,
so we achieve worse results even with the much
larger P&L data.
We find that 1-SDA (line 6) already performs
significantly
?
better than the ID baseline on all do-
mains except CINEMA which has a much larger
amount of ID training data available than the other
domains (approx. 1700 sentences vs. fewer than
600). Using stacking, 2-SDA (line 7) improves
the results on three domains significantly,
?
and
performs on par with the ID classifier on CIN-
EMA. We found that stack depths of k > 2 do
not significantly
?
increase performance.
Finally, we try a combination of ID and TL
(ID+TL), training on both P&L and the respective
ID training fold of CINEMA/PRODUCTS. The re-
sults for this experiment are shown in lines 8?10
in Table 2. Comparing BoW models, we beat both
ID and TL across all domains (lines 2 and 5). With
SDA features, we are able to beat ID for CINEMA.
The results on the other domains are comparable
to plain TL. This is a promising result, showing
that with SDA features, ID+TL performs as well
as or better than plain TL. This property could be
exploited for domains where labeled data is not
available. We will show below that SDA features
become important when we apply ID+TL to do-
main adaptation.
We also conducted experiments using only the
5,000 most frequent features but found that the
SDA does not generalize well from this input rep-
resentation, particularly on EL and MUSIC. This
confirms that in SR, rare features make an im-
portant contribution (such as named entities in the
movie domain).
Domain Adaptation (DA) We now evaluate the
task in a DA setting, comparing the ID and ID+TL
202
CC BC DC EC MC VC CB BB DB EB MB VB CD BD DD ED MD VD CE BE DE EE ME VE CM BM DM EM MM VM C
V BV DV EV MV VV
BoW ID 2?SDA ID BOW ID+TL 2?SDA ID+TL
Tra
ns
fer
 Lo
ss
 (%
)
?15
?10
?5
0
5
10
15
20
25
Figure 1: Transfer losses (%) for DA. Training-test pairs grouped by target domain and abbreviated by
first letter (e.g., CD: training on CINEMA, evaluating on DVDS). In-domain results shown for comparison
to Table 2.
setups with BoW and 2-SDA features. We mea-
sure the transfer losses we suffer from training on
one domain and evaluating on another (Figure 1).
The overall picture is the same as above: ID+TL
2-SDA models perform best. In the baseline BoW
ID setup, domain shifts have a strong influence on
the results. The combination of out-of-domain and
out-of-task data in ID+TL keeps losses uniformly
low. 2-SDA features lower almost all losses fur-
ther. On average, 2-SDA ID+TL reduces trans-
fer loss by 12.7 points compared to the baseline
(Table 3). As expected, pairings of thematically
strongly related domains (e.g., BOOKS and DVDS)
have lower losses in all setups.
The biggest challenge is the strong domain shift
between the CINEMA and PRODUCTS domains
(concerning mainly the retail aspects). With BoW
ID, losses on CINEMA reach up to 25 points, and
using CINEMA for training causes high losses for
PRODUCTS in most cases. Our key result is that
the ID+TL 2-SDA setup successfully compensates
for these problems, reducing the losses below 0.
Losses across the PRODUCTS domains are less
pronounced. The DVDS baseline classifier has the
lowest F
1
(cf. Table 2) and shows the highest im-
provements in domain adaptation: BoW models of
other domains perform better than the in-domain
classifier. Analyzing the DVDS model shows over-
fitting to specific movie terms which occur fre-
quently across each review in the training data.
SNR content in movies is mostly concerned with
named entity types which cannot easily be learned
from BoW representations. Out-of-domain mod-
els are less specialized and perform better than in-
BoW 2-SDA
ID 6.7 8.9
ID+TL -1.8 -6.0
Table 3: Mean transfer losses (%) for the different
training data and feature representation setups. In-
domain results not included.
domain models. TL and SDA increase the cover-
age of movie terms and provide better generaliza-
tion, which improves performance further.
BOOKS is the most challenging domain in all se-
tups. It is particularly heterogeneous, containing
both fiction and non-fiction reviews which feature
different SNR aspects. Both results illustrate that
domain effects depend on how diverse SNR con-
tent is within the domain.
Overall, the results show that ID+TL leads to a
successful compensation of cross-domain effects.
SDA features improve the results significantly
?
for
ID+TL. In particular, we find that the SDA suc-
cessfully compensates for the strong domain shift
between CINEMA and PRODUCTS.
5 Conclusion
We presented experiments on multi- and cross-
domain sentiment relevance classification. We
showed that transfer learning (TL) using stacked
denoising autoencoder (SDA) representations sig-
nificantly increases performance by 6.8% F
1
for
in-domain classification. Moreover, the average
transfer loss in domain adaptation is reduced by
12.7 percentage points where the SDA features
compensate for strong domain shifts.
203
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(ACM TIST), 2(3):1?27.
Minmin Chen, Zhixiang Xu, Kilian Weinberger, and
Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In Proceedings of the
29th International Conference on Machine Learning
(ICML).
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML), pages 513?520.
Nitin Jindal and Bing Liu. 2008. Opinion spam
and analysis. In Proceedings of the International
Conference on Web Search and Web Data Mining
(WSDM), pages 219?230.
Eric W. Noreen. 1989. Computer Intensive Methods
for Hypothesis Testing: An Introduction. Wiley.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL), pages 271?278.
Christian Scheible and Hinrich Sch?utze. 2013. Senti-
ment relevance. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 954?963.
Maite Taboada, Julian Brooke, and Manfred Stede.
2009. Genre-based paragraph classification for sen-
timent analysis. In Proceedings of the SIGDIAL
2009 Conference, pages 62?70.
Oscar T?ackstr?om and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 569?574.
Sebastian Thrun. 1995. Is learning the n-th thing any
easier than learning the first? In Proceedings of Ad-
vances in Neural Information Processing Systems 8
(NIPS), pages 640?646.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. The Journal of Machine Learn-
ing Research (JMLR), 11:3371?3408.
Sida Wang and Christopher Manning. 2012. Baselines
and bigrams: Simple, good sentiment and topic clas-
sification. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 90?94.
204
Proceedings of the ACL 2010 Student Research Workshop, pages 25?30,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Sentiment Translation through Lexicon Induction
Christian Scheible
Institute for Natural Language Processing
University of Stuttgart
scheibcn@ims.uni-stuttgart.de
Abstract
The translation of sentiment information
is a task from which sentiment analy-
sis systems can benefit. We present a
novel, graph-based approach using Sim-
Rank, a well-established vertex similar-
ity algorithm to transfer sentiment infor-
mation between a source language and a
target language graph. We evaluate this
method in comparison with SO-PMI.
1 Introduction
Sentiment analysis is an important topic in compu-
tational linguistics that is of theoretical interest but
also implies many real-world applications. Usu-
ally, two aspects are of importance in sentiment
analysis. The first is the detection of subjectivity,
i.e. whether a text or an expression is meant to ex-
press sentiment at all; the second is the determina-
tion of sentiment orientation, i.e. what sentiment
is to be expressed in a structure that is considered
subjective.
Work on sentiment analysis most often cov-
ers resources or analysis methods in a single lan-
guage, usually English. However, the transfer
of sentiment analysis between languages can be
advantageous by making use of resources for a
source language to improve the analysis of the tar-
get language.
This paper presents an approach to the transfer
of sentiment information between languages. It is
built around an algorithm that has been success-
fully applied for the acquisition of bilingual lexi-
cons. One of the main benefits of the method is its
ability of handling sparse data well.
Our experiments are carried out using English
as a source language and German as a target lan-
guage.
2 Related Work
The translation of sentiment information has been
the topic of multiple publications.
Mihalcea et al (2007) propose two methods for
translating sentiment lexicons. The first method
simply uses bilingual dictionaries to translate an
English sentiment lexicon. A sentence-based clas-
sifier built with this list achieved high precision
but low recall on a small Romanian test set. The
second method is based on parallel corpora. The
source language in the corpus is annotated with
sentiment information, and the information is then
projected to the target language. Problems arise
due to mistranslations, e.g., because irony is not
recognized.
Banea et al (2008) use machine translation for
multilingual sentiment analysis. Given a corpus
annotated with sentiment information in one lan-
guage, machine translation is used to produce an
annotated corpus in the target language, by pre-
serving the annotations. The original annotations
can be produced either manually or automatically.
Wan (2009) constructs a multilingual classifier
using co-training. In co-training, one classifier
produces additional training data for a second clas-
sifier. In this case, an English classifier assists in
training a Chinese classifier.
The induction of a sentiment lexicon is the sub-
ject of early work by (Hatzivassiloglou and McK-
eown, 1997). They construct graphs from coor-
dination data from large corpora based on the in-
tuition that adjectives with the same sentiment ori-
entation are likely to be coordinated. For example,
fresh and delicious is more likely than rotten and
delicious. They then apply a graph clustering al-
gorithm to find groups of adjectives with the same
orientation. Finally, they assign the same label to
all adjectives that belong to the same cluster. The
authors note that some words cannot be assigned a
unique label since their sentiment depends on con-
25
text.
Turney (2002) suggests a corpus-based extrac-
tion method based on his pointwise mutual infor-
mation (PMI) synonymy measure He assumes that
the sentiment orientation of a phrase can be deter-
mined by comparing its pointwise mutual infor-
mation with a positive (excellent) and a negative
phrase (poor). An introduction to SO-PMI is given
in Section 5.1
3 Bilingual Lexicon Induction
Typical approaches to the induction of bilingual
lexicons involve gathering new information from
a small set of known identities between the lan-
guages which is called a seed lexicon and incor-
porating intralingual sources of information (e.g.
cooccurrence counts). Two examples of such
methods are a graph-based approach by Dorow et
al. (2009) and a vector-space based approach by
Rapp (1999). In this paper, we will employ the
graph-based method.
SimRank was first introduced by Jeh and
Widom (2002). It is an iterative algorithm that
measures the similarity between all vertices in a
graph. In SimRank, two nodes are similar if their
neighbors are similar. This defines a recursive pro-
cess that ends when the two nodes compared are
identical. As proposed by Dorow et al (2009), we
will apply it to a graph G in which vertices repre-
sent words and edges represent relations between
words. SimRank will then yield similarity values
between vertices that indicate the degree of relat-
edness between them with regard to the property
encoded through the edges. For two nodes i and
j in G, similarity according to SimRank is defined
as
sim(i, j) = c
|N(i)||N(j)
?
k?N(i),l?N(j)
sim(k, l),
where N(x) is the neighborhood of x and c is
a weight factor that determines the influence of
neighbors that are farther away. The initial con-
dition for the recursion is sim(i, i) = 1.
Dorow et al (2009) further propose the applica-
tion of the SimRank algorithm for the calculation
of similarities between a source graph S and a tar-
get graph T . Initially, some relations between the
two graphs need to be known. When operating on
word graphs, these can be taken from a bilingual
lexicon. This provides us with a framework for
the induction of a bilingual lexicon which can be
constructed based on the obtained similarity val-
ues between the vertices of the two graphs.
One problem of SimRank observed in experi-
ments by Laws et al (2010) was that while words
with high similarity were semantically related,
they often were not exact translations of each
other but instead often fell into the categories of
hyponymy, hypernomy, holonymy, or meronymy.
However, this makes the similarity values appli-
cable for the translation of sentiment since it is a
property that does not depend on exact synonymy.
4 Sentiment Transfer
Although unsupervised methods for the design of
sentiment analysis systems exist, any approach
can benefit from using resources that have been
established in other languages. The main problem
that we aim to deal with in this paper is the trans-
fer of such information between languages. The
SimRank lexicon induction method is suitable for
this purpose since it can produce useful similarity
values even with a small seed lexicon.
First, we build a graph for each language. The
vertices of these graphs will represent adjectives
while the edges are coordination relations between
these adjectives. An example for such a graph is
given in Figure 1.
Figure 1: Sample graph showing English coordi-
nation relations.
The use of coordination information has been
shown to be beneficial for example in early work
by Hatzivassiloglou and McKeown (1997).
Seed links between those graphs will be taken
from a universal dictionary. Figure 2 shows an ex-
ample graph. Here, intralingual coordination rela-
tions are represented as black lines, seed relations
as solid grey lines, and relations that are induced
through SimRank as dashed grey lines.
After computing similarities in this graph, we
26
Figure 2: Sample graph showing English and German coordination relations. Solid black lines represent
coordinations, solid grey lines represent seed relations, and dashed grey lines show induced relations.
need to obtain sentiment values. We will define
the sentiment score (sent) as
sent(n
t
) =
?
n
s
?S
simnorm(ns, nt) sent(ns),
where n
t
is a node in the target graph T , and S
the source graph. This way, the sentiment score
of each node is an average over all nodes in S
weighted by their normalized similarity, simnorm.
We define the normalized similarity as
simnorm(ns, nt) =
sim(n
s
, n
t
)
?
n
s
?S
sim(n
s
, n
t
)
.
Normalization guarantees that all sentiment
scores lie within a specified range. Scores are not
a direct indicator for orientation since the similar-
ities still include a lot of noise. Therefore, we
interpret the scores by assigning each word to a
category by finding score thresholds between the
categories.
5 Experiments
5.1 Baseline Method (SO-PMI)
We will compare our method to the well-
established SO-PMI algorithm by Turney (2002)
to show an improvement over an unsupervised
method. The algorithm works with cooccurrence
counts on large corpora. To determine the seman-
tic orientation of a word w, the hits near positive
(Pwords) and negative (Nwords) seed words is
used. The SO-PMI equation is given as
SO-PMI(word) =
log
2
(
?
pword?Pwords
hits(word NEAR pword)
?
nword?Nwords
hits(word NEAR nword)
?
?
nword?Nwords
hits(nword)
?
pword?Pwords
hits(pword)
)
5.2 Data Acquisition
We used the English and German Wikipedia
branches as our corpora. We extracted coor-
dinations from the corpus using a simple CQP
pattern search (Christ et al, 1999). For our ex-
periments, we looked only at coordinations with
and. For the English corpus, we used the pattern
[pos = "JJ"] ([pos = ","] [pos =
"JJ"])*([pos = ","]? "and" [pos
= "JJ"])+, and for the German corpus, the
pattern [pos = "ADJ.*"] ([pos = ","]
[pos = "ADJ.*"])* ("und" [pos =
"ADJ"])+ was used. This yielded 477,291 pairs
of coordinated English adjectives and 44,245
German pairs. We used the dict.cc dictionary1 as
a seed dictionary. It contained a total of 30,551
adjectives.
After building a graph out of this data as de-
scribed in Section 4, we apply the SimRank algo-
rithm using 7 iterations.
Data for the SO-PMI method had to be col-
lected from queries to search engines since the in-
formation available in the Wikipedia corpus was
too sparse. Since Google does not provide a sta-
ble NEAR operator, we used coordinations instead.
For each of the test words w and the SO-PMI seed
words s we made two queries +"w und s" and
+"s und w" to Google. The quotes and + were
added to ensure that no spelling correction or syn-
onym replacements took place. Since the original
experiments were designed for an English corpus,
a set of German seed words had to be constructed.
We chose gut, nett, richtig, scho?n, ordentlich, an-
genehm, aufrichtig, gewissenhaft, and hervorra-
gend as positive seeds, and schlecht, teuer, falsch,
bo?se, feindlich, verhasst, widerlich, fehlerhaft, and
1http://www.dict.cc/
27
word value
strongpos 1.0
weakpos 0.5
neutral 0.0
weakneg ?0.5
strongneg ?1.0
Table 1: Assigned values for positivity labels
mangelhaft as negative seeds.
We constructed a test set by randomly selecting
200 German adjectives that occurred in a coordi-
nation in Wikipedia. We then eliminated adjec-
tives that we deemed uncommon or too difficult to
understand or that were mislabeled as adjectives.
This resulted in a 150 word test set. To deter-
mine the sentiment of these adjectives, we asked
9 human judges, all native German speakers, to
annotate them given the classes neutral, slightly
negative, very negative, slightly positive, and very
positive, reflecting the categories from the train-
ing data. In the annotation process, another 7 ad-
jectives had to be discarded because one or more
annotators marked them as unknown.
Since human judges tend to interpret scales
differently, we examine their agreement using
Kendall?s coefficient of concordance (W ) includ-
ing correction for ties (Legendre, 2005) which
takes ranks into account. The agreement was cal-
culated as W = 0.674 with a significant confi-
dence (p < .001), which is usually interpreted as
substantial agreement. Manual examination of the
data showed that most disagreement between the
annotators occurred with adjectives that are tied
to political implications, for example nuklear (nu-
clear).
5.3 Sentiment Lexicon Induction
For our experiments, we used the polarity lexi-
con of Wilson et al (2005). It includes annota-
tions of positivity in the form of the categories
neutral, weakly positive (weakpos), strongly posi-
tive (strongpos), weakly negative (weakneg), and
strongly positive (strongneg). In order to con-
duct arithmetic operations on these annotations,
mapped them to values from the interval [?1, 1]
by using the assignments given in Table 1.
5.4 Results
To compare the two methods to the human raters,
we first reproduce the evaluation by Turney (2002)
and examine the correlation coefficients. Both
methods will be compared to an average over the
human rater values. These values are calculated
on values asserted based on Table 1. The corre-
lation coefficients between the automatic systems
and the human ratings, SO-PMI yields r = 0.551,
and SimRank yields r = 0.587 which are not sig-
nificantly different. This shows that SO and SR
have about the same performance on this broad
measure.
Since many adjectives do not express sentiment
at all, the correct categorization of neutral adjec-
tives is as important as the scalar rating. Thus,
we divide the adjectives into three categories ?
positive, neutral, and negative. Due to disagree-
ments between the human judges there exists no
clear threshold between these categories. In order
to try different thresholds, we assume that senti-
ment is symmetrically distributed with mean 0 on
the human scores. For x ? { i
20
|0 ? i ? 19}, we
then assign word w with human rating score(w)
to negative if score(w) ? ?x, to neutral if ?x <
score(w) < x and to positive otherwise. This
gives us a three-category gold standard for each
x that is then the basis for computing evaluation
measures. Each category contains a certain per-
centile of the list of adjectives. By mapping these
percentiles to the rank-ordered scores for SO-PMI
and SimRank, we can create three-category par-
titions for them. For example if for x = 0.35
21% of the adjectives are negative, then the 21%
of adjectives with the lowest SO-PMI scores are
deemed to have been rated negative by SO-PMI.
 0
 0.2
 0.4
 0.6
 0.8
 1
0.950.90.850.80.750.70.650.60.550.50.450.40.350.30.250.20.150.10.050
Ac
cu
ra
cy
x
SO-PMI (macro)
SimRank (macro)
SO-PMI (micro)
SimRank (micro)
Figure 3: Macro- and micro-averaged Accuracy
First, we will look at the macro- and micro-
averaged accuracies for both methods (cf. Fig-
ure 3). Overall, SimRank performs better for x
28
between 0.05 and 0.4 which is a plausible inter-
val for the neutral threshold on the human ratings.
The results diverge for very low and high values
of x, however these values can be considered un-
realistic since they implicate neutral areas that are
too small or too large. When comparing the ac-
curacies for each of the classes (cf. Figure 4), we
observe that in the aforementioned interval, Sim-
Rank has higher accuracy values than SO-PMI for
all of them.
 0
 0.2
 0.4
 0.6
 0.8
 1
0.950.90.850.80.750.70.650.60.550.50.450.40.350.30.250.20.150.10.050
Ac
cu
ra
cy
x
positive (SO-PMI)
positive (SimRank)
neutral (SO-PMI)
neutral (SimRank)
negative (SO-PMI)
negative (SimRank)
Figure 4: Accuracy for individual classes
Table 2 lists some interesting example words in-
cluding their human ratings and SO-PMI and Sim-
Rank scores which illustrate advantages and pos-
sible shortcomings of the two methods. The medi-
ans of SO-PMI and SimRank scores are ?15.58
and ?0.05, respectively. The mean values are
?9.57 for SO-PMI and 0.08 for SimRank, the
standard deviations are 13.75 and 0.22. SimRank
values range between ?0.67 and 0.41, SO-PMI
ranges between ?46.21 and 46.59. We will as-
sume that the medians mark the center of the set
of neutral adjectives.
Ausdrucksvoll receives a positive score from
SO-PMI which matches the human rating, how-
ever not from SimRank, which assigns a score
close to 0 and would likely be considered neutral.
This error can be explained by examining the sim-
ilarity distribution for ausdrucksvoll which reveals
that there are no nodes that are similar to this node,
which was most likely caused by its low degree.
Auferstanden (resurrected) is perceived as a posi-
tive adjective by the human judges, however it is
misclassified by SimRank as negative due to its
occurrence with words like gestorben (deceased)
and gekreuzigt (crucified) which have negative as-
word (translation) SR SO judges
ausdrucksvoll (expressive) 0.069 22.93 0.39
grafisch (graphic) -0.050 -4.75 0.00
kriminell (criminal) -0.389 -15.98 -0.94
auferstanden (resurrected) -0.338 -10.97 0.34
Table 2: Example adjectives including translation,
and their scores
sociations. This suggests that coordinations are
sometimes misleading and should not be used as
the only data source. Grafisch (graphics-related)
is an example for a neutral word misclassified by
SO-PMI due to its occurrence in positive contexts
on the web. Since SimRank is not restricted to re-
lations between an adjective and a seed word, all
adjective-adjective coordinations are used for the
estimation of a sentiment score. Kriminell is also
misclassified by SO-PMI for the same reason.
6 Conclusion and Outlook
We presented a novel approach to the translation
of sentiment information that outperforms SO-
PMI, an established method. In particular, we
could show that SimRank outperforms SO-PMI
for values of the threshold x in an interval that
most likely leads to the correct separation of pos-
itive, neutral, and negative adjectives. We intend
to compare our system to other available work in
the future. In addition to our findings, we created
an initial gold standard set of sentiment-annotated
German adjectives that will be publicly available.
The two methods are very different in nature;
while SO-PMI is suitable for languages in which
very large corpora exist, this might not be the
case for knowledge-sparse languages. For some
German words (e.g. schwerstkrank (seriously
ill)), SO-PMI lacked sufficient results on the web
whereas SimRank correctly assigned negative sen-
timent. SimRank can leverage knowledge from
neighbor words to circumvent this problem. In
turn, this information can turn out to be mislead-
ing (cf. auferstanden). An advantage of our
method is that it uses existing resources from an-
other language and can thus be applied without
much knowledge about the target language. Our
future work will include a further examination of
the merits of its application for knowledge-sparse
languages.
The underlying graph structure provides a foun-
dation for many conceivable extensions. In this
paper, we presented a fairly simple experiment re-
stricted to adjectives only. However, the method
29
is suitable to include arbitrary parts of speech as
well as phrases, as used by Turney (2002). An-
other conceivable application would be the direct
combination of the SimRank-based model with a
statistical model.
Currently, our input sentiment list exists only of
prior sentiment values, however work by Wilson
et al (2009) has advanced the notion of contextual
polarity lists. The automatic translation of this in-
formation could be beneficial for sentiment analy-
sis in other languages.
Another important problem in sentiment anal-
ysis is the treatment of ambiguity. The senti-
ment expressed by a word or phrase is context-
dependent and is for example related to word sense
(Akkaya et al, 2009). Based on regularities in
graph structure and similarity, ambiguity resolu-
tion might become possible.
References
C. Akkaya, J. Wiebe, and R. Mihalcea. 2009. Sub-
jectivity Word Sense Disambiguation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 190?199.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 127?135, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
O. Christ, B.M. Schulze, A. Hofmann, and E. Koenig.
1999. The IMS Corpus Workbench: Corpus Query
Processor (CQP): User?s Manual. University of
Stuttgart, March, 8:1999.
Beate Dorow, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In Proceedings of the Workshop on
Geometrical Models of Natural Language Seman-
tics, pages 91?95, Athens, Greece, March. Associ-
ation for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting
of the Association for Computational Linguistics,
pages 174?181, Madrid, Spain, July. Association for
Computational Linguistics.
Glen Jeh and Jennifer Widom. 2002. Simrank: a mea-
sure of structural-context similarity. In KDD ?02:
Proceedings of the eighth ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 538?543, New York, NY, USA. ACM.
F. Laws, L. Michelbacher, B. Dorow, U. Heid, and
H. Schu?tze. 2010. Building a Cross-lingual Re-
latedness Thesaurus Using a Graph Similarity Mea-
sure. Submitted on Nov 7, 2009, to the International
Conference on Language Resources and Evaluation
(LREC).
P. Legendre. 2005. Species associations: the Kendall
coefficient of concordance revisited. Journal of
Agricultural Biological and Environment Statistics,
10(2):226?245.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 976?983, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 519?526, College Park, Maryland, USA,
June. Association for Computational Linguistics.
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 417?424, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 235?
243, Suntec, Singapore, August. Association for
Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 347?354, Vancouver, British
Columbia, Canada, October. Association for Com-
putational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: an Explo-
ration of Features for Phrase-level Sentiment Analy-
sis. Computational Linguistics, 35(3):399?433.
30
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 954?963,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Sentiment Relevance
Christian Scheible
Institute for Natural Language Processing
University of Stuttgart, Germany
scheibcn@ims.uni-stuttgart.de
Hinrich Schu?tze
Center for Information
and Language Processing
University of Munich, Germany
Abstract
A number of different notions, including
subjectivity, have been proposed for dis-
tinguishing parts of documents that con-
vey sentiment from those that do not. We
propose a new concept, sentiment rele-
vance, to make this distinction and argue
that it better reflects the requirements of
sentiment analysis systems. We demon-
strate experimentally that sentiment rele-
vance and subjectivity are related, but dif-
ferent. Since no large amount of labeled
training data for our new notion of sen-
timent relevance is available, we investi-
gate two semi-supervised methods for cre-
ating sentiment relevance classifiers: a dis-
tant supervision approach that leverages
structured information about the domain
of the reviews; and transfer learning on
feature representations based on lexical
taxonomies that enables knowledge trans-
fer. We show that both methods learn sen-
timent relevance classifiers that perform
well.
1 Introduction
It is generally recognized in sentiment analy-
sis that only a subset of the content of a doc-
ument contributes to the sentiment it conveys.
For this reason, some authors distinguish the
categories subjective and objective (Wilson and
Wiebe, 2003). Subjective statements refer to the
internal state of mind of a person, which cannot be
observed. In contrast, objective statements can be
verified by observing and checking reality. Some
sentiment analysis systems filter out objective lan-
guage and predict sentiment based on subjective
language only because objective statements do not
directly reveal sentiment.
Even though the categories subjective/objective
are well-established in philosophy, we argue that
they are not optimal for sentiment analysis. We in-
stead introduce the notion of sentiment relevance
(S-relevance or SR for short). A sentence or lin-
guistic expression is S-relevant if it contains infor-
mation about the sentiment the document conveys;
it is S-nonrelevant (SNR) otherwise.
Ideally, we would like to have at our disposal
a large annotated training set for our new con-
cept of sentiment relevance. However, such a
resource does not yet exist. For this reason,
we investigate two semi-supervised approaches to
S-relevance classification that do not require S-
relevance-labeled data. The first approach is dis-
tant supervision (DS). We create an initial label-
ing based on domain-specific metadata that we ex-
tract from a public database and show that this
improves performance by 5.8% F1 compared to a
baseline. The second approach is transfer learning
(TL) (Thrun, 1996). We show that TL improves
F1 by 12.6% for sentiment relevance classification
when we use a feature representation based on lex-
ical taxonomies that supports knowledge transfer.
In our approach, we classify sentences as S-
(non)relevant because this is the most fine-grained
level at which S-relevance manifests itself; at the
word or phrase level, S-relevance classification
is not possible because of scope and context ef-
fects. However, S-relevance is also a discourse
phenomenon: authors tend to structure documents
into S-relevant passages and S-nonrelevant pas-
sages. To impose this discourse constraint, we em-
ploy a sequence model. We represent each docu-
ment as a graph of sentences and apply a minimum
cut method.
The rest of the paper is structured as follows.
Section 2 introduces the concept of sentiment rel-
evance and relates it to subjectivity. In Section 3,
we review previous work related to sentiment rel-
evance. Next, we describe the methods applied in
this paper (Section 4) and the features we extract
(Section 5). Finally, we turn to the description and
954
results of our experiments on distant supervision
(Section 6) and transfer learning (Section 7). We
end with a conclusion in Section 8.
2 Sentiment Relevance
Sentiment Relevance is a concept to distinguish
content informative for determining the sentiment
of a document from uninformative content. This
is in contrast to the usual distinction between sub-
jective and objective content. Although there is
overlap between the two notions, they are differ-
ent. Consider the following examples for subjec-
tive and objective sentences:
(1) Subjective example: Bruce Banner, a genet-
ics researcher with a tragic past, suffers a horrible
accident.
(2) Objective example: The movie won a
Golden Globe for best foreign film and an Oscar.
Sentence (1) is subjective because assessments
like tragic past and horrible accident are subjec-
tive to the reader and writer. Sentence (2) is objec-
tive since we can check the truth of the statement.
However, even though sentence (1) has negative
subjective content, it is not S-relevant because it
is about the plot of the movie and can appear in
a glowingly positive review. Conversely, sentence
(2) contributes to the positive opinion expressed
by the author. Subjectivity and S-relevance are
two distinct concepts that do not imply each other:
Generally neutral and objective sentences can be
S-relevant while certain subjective content is S-
nonrelevant. Below, we first describe the annota-
tion procedure for the sentiment relevance corpus
and then demonstrate empirically that subjectivity
and S-relevance differ.
2.1 Sentiment Relevance Corpus
For our initial experiments, we focus on senti-
ment relevance classification in the movie domain.
To create a sentiment-relevance-annotated corpus,
the SR corpus, we randomly selected 125 docu-
ments from the movie review data set (Pang et al,
2002).1 Two annotators annotated the sentences
for S-relevance, using the labels SR and SNR. If no
decision can be made because a sentence contains
both S-relevant and S-nonrelevant linguistic ma-
terial, it is marked as uncertain. We excluded
360 sentences that were labeled uncertain from the
1We used the texts from the raw HTML files since the
processed version does not have capitalization.
evaluation. In total, the SR corpus contains 2759
S-relevant and 728 S-nonrelevant sentences. Fig-
ure 1 shows an excerpt from the corpus. The full
corpus is available online.2
First, we study agreement between human an-
notators. We had 762 sentences annotated for S-
relevance by both annotators with an agreement
(Fleiss? ?) of .69. In addition, we obtained sub-
jectivity annotations for the same data on Amazon
Mechanical Turk, obtaining each label through a
vote of three, with an agreement of ? = .61. How-
ever, the agreement of the subjectivity and rele-
vance labelings after voting, assuming that sub-
jectivity equals relevance, is only at ? = .48.
This suggests that there is indeed a measurable
difference between subjectivity and relevance. An
annotator who we asked to examine the 225 ex-
amples where the annotations disagree found that
83.5% of these cases are true differences.
2.2 Contrastive Classification Experiment
We will now examine the similarities of S-
relevance and an existing subjectivity dataset.
Pang and Lee (2004) introduced subjectivity data
(henceforth P&L corpus) that consists of 5000
highly subjective (quote) review snippets from rot-
tentomatoes.com and 5000 objective (plot) sen-
tences from IMDb plot descriptions.
We now show that although the P&L selection
criteria (quotes, plot) bear resemblance to the def-
inition of S-relevance, the two concepts are differ-
ent.
We use quote as S-relevant and plot as S-
nonrelevant data in TL. We divide both the SR
and P&L corpora into training (50%) and test sets
(50%) and train a Maximum Entropy (MaxEnt)
classifier (Manning and Klein, 2003) with bag-of-
word features. Macro-averaged F1 for the four
possible training-test combinations is shown in Ta-
ble 1. The results clearly show that the classes
defined by the two labeled sets are different. A
classifier trained on P&L performs worse by about
8% on SR than a classifier trained on SR (68.5 vs.
76.4). A classifier trained on SR performs worse
by more than 20% on P&L than a classifier trained
on P&L (67.4 vs. 89.7).
Note that the classes are not balanced in the
S-relevance data while they are balanced in the
subjectivity data. This can cause a misestimation
2http://www.ims.uni-stuttgart.
de/forschung/ressourcen/korpora/
sentimentrelevance/
955
O SNR Braxton is a gambling addict in deep to Mook (Ellen Burstyn), a local bookie.
S SNR Kennesaw is bitter about his marriage to a socialite (Rosanna Arquette), believing his wife
to be unfaithful.
S SR The plot is twisty and complex, with lots of lengthy flashbacks, and plenty of surprises.
S SR However, there are times when it is needlessly complex, and at least one instance the
storytelling turns so muddled that the answers to important plot points actually get lost.
S SR Take a look at L. A. Confidential, or the film?s more likely inspiration, The Usual Suspects
for how a complex plot can properly be handled.
Figure 1: Example data from the SR corpus with subjectivity (S/O) and S-relevance (SR/SNR) annota-
tions
test
P&L SR
tra
in P&L 89.7 68.5
SR 67.4 76.4
Table 1: TL/in-task F1 for P&L and SR corpora
vocabulary fpSR fpSNR
{actor, director, story} 0 7.5
{good, bad, great} 11.5 4.8
Table 2: % incorrect sentences containing specific
words
of class probabilities and lead to the experienced
performance drops. Indeed, if we either balance
the S-relevance data or unbalance the subjectivity
data, we can significantly increase F1 to 74.8%
and 77.9%, respectively, in the noisy label trans-
fer setting. Note however that this step is difficult
in practical applications if the actual label distri-
bution is unknown. Also, in a real practical ap-
plication the distribution of the data is what it is ?
it cannot be adjusted to the training set. We will
show in Section 7 that using an unsupervised se-
quence model is superior to artificial manipulation
of class-imbalances.
An error analysis for the classifier trained on
P&L shows that many sentences misclassified as
S-relevant (fpSR) contain polar words; for exam-
ple, Then, the situation turns bad. In contrast, sen-
tences misclassified as S-nonrelevant (fpSNR) con-
tain named entities or plot and movie business vo-
cabulary; for example, Tim Roth delivers the most
impressive acting job by getting the body language
right.
The word count statistics in Table 2 show this
for three polar words and for three plot/movie
business words. The P&L-trained classifier seems
to have a strong bias to classify sentences with po-
lar words as S-relevant even if they are not, per-
haps because most training instances for the cat-
egory quote are highly subjective, so that there
is insufficient representation of less emphatic S-
relevant sentences. These snippets rarely con-
tain plot/movie-business words, so that the P&L-
trained classifier assigns almost all sentences with
such words to the category S-nonrelevant.
3 Related Work
Many publications have addressed subjectivity in
sentiment analysis. Two important papers that are
based on the original philosophical definition of
the term (internal state of mind vs. external real-
ity) are (Wilson and Wiebe, 2003) and (Riloff and
Wiebe, 2003). As we argue above, if the goal is to
identify parts of a document that are useful/non-
useful for sentiment analysis, then S-relevance is
a better notion to use.
Researchers have implicitly deviated from the
philosophical definition because they were primar-
ily interested in satisfying the needs of a particular
task. For example, Pang and Lee (2004) use a min-
imum cut graph model for review summarization.
Because they do not directly evaluate the results
of subjectivity classification, it is not clear to what
extent their method is able to identify subjectivity
correctly.
In general, it is not possible to know what the
underlying concepts of a statistical classification
are if no detailed annotation guidelines exist and
no direct evaluation of manually labeled data is
performed.
Our work is most closely related to (Taboada
et al, 2009) who define a fine-grained classifica-
tion that is similar to sentiment relevance on the
highest level. However, unlike our study, they
fail to experimentally compare their classification
scheme to prior work in their experiments and
956
to show that this scheme is different. In addi-
tion, they work on the paragraph level. How-
ever, paragraphs often contain a mix of S-relevant
and S-nonrelevant sentences. We use the mini-
mum cut method and are therefore able to incorpo-
rate discourse-level constraints in a more flexible
fashion, giving preference to ?relevance-uniform?
paragraphs without mandating them.
Ta?ckstro?m and McDonald (2011) develop a
fine-grained annotation scheme that includes S-
nonrelevance as one of five categories. However,
they do not use the category S-nonrelevance di-
rectly in their experiments and do not evaluate
classification accuracy for it. We do not use their
data set as it would cause domain mismatch be-
tween the product reviews they use and the avail-
able movie review subjectivity data (Pang and Lee,
2004) in the TL approach. Changing both the do-
main (movies to products) and the task (subjectiv-
ity to S-relevance) would give rise to interactions
that we would like to avoid in our study.
The notion of annotator rationales (Zaidan et
al., 2007) has some overlap with our notion of
sentiment relevance. Yessenalina et al (2010)
use rationales in a multi-level model to integrate
sentence-level information into a document classi-
fier. Neither paper presents a direct gold standard
evaluation of the accuracy of rationale detection.
In summary, no direct evaluation of sentiment
relevance has been performed previously. One
contribution in this paper is that we provide a
single-domain gold standard for sentiment rele-
vance, created based on clear annotation guide-
lines, and use it for direct evaluation.
Sentiment relevance is also related to review
mining (e.g., (Ding et al, 2008)) and sentiment
retrieval techniques (e.g., (Eguchi and Lavrenko,
2006)) in that they aim to find phrases, sentences
or snippets that are relevant for sentiment, either
with respect to certain features or with a focus on
high-precision retrieval (cf. (Liu, 2010)). How-
ever, finding a few S-relevant items with high pre-
cision is much easier than the task we address: ex-
haustive classification of all sentences.
Another contribution is that we show that gen-
eralization based on semantic classes improves S-
relevance classification. While previous work has
shown the utility of other types of feature gen-
eralization for sentiment and subjectivity analysis
(e.g., syntax and part-of-speech (Riloff and Wiebe,
2003)), semantic classes have so far not been ex-
ploited.
Named-entity features in movie reviews were
first used by Zhuang et al (2006), in the form
of feature-opinion pairs (e.g., a positive opinion
about the acting). They show that recognizing plot
elements (e.g., script) and classes of people (e.g.,
actor) benefits review summarization. We follow
their approach by using IMDb to define named
entity features. We extend their work by intro-
ducing methods for labeling partial uses of names
and pronominal references. We address a different
problem (S-relevance vs. opinions) and use differ-
ent methods (graph-based and statistical vs. rule-
based).
Ta?ckstro?m and McDonald (2011) also solve a
similar sequence problem by applying a distantly
supervised classifier with an unsupervised hidden
sequence component. Their setup differs from
ours as our focus lies on pattern-based distant su-
pervision instead of distant supervision using doc-
uments for sentence classification.
Transfer learning has been applied previously in
sentiment analysis (Tan and Cheng, 2009), target-
ing polarity detection.
4 Methods
Due to the sequential properties of S-relevance (cf.
Taboada et al (2009)), we impose the discourse
constraint that an S-relevant (resp. S-nonrelevant)
sentence tends to follow an S-relevant (resp. S-
nonrelevant) sentence. Following Pang and Lee
(2004), we use minimum cut (MinCut) to formal-
ize this discourse constraint.
For a document with n sentences, we create a
graph with n + 2 nodes: n sentence nodes and
source and sink nodes. We define source and
sink to represent the classes S-relevance and S-
nonrelevance, respectively, and refer to them as
SR and SNR.
The individual weight ind(s, x) between a sen-
tence s and the source/sink node x ? {SR,SNR}
is weighted according to some confidence mea-
sure for assigning it to the corresponding class.
The weight on the edge from the document?s
ith sentence si to its j th sentence sj is set to
assoc(si, sj) = c/(j ? i)2 where c is a parame-
ter (cf. (Pang and Lee, 2004)). The minimum cut
is a tradeoff between the confidence of the clas-
sification decisions and ?discourse coherence?.
The discourse constraint often has the effect that
high-confidence labels are propagated over the se-
957
quence. As a result, outliers with low confidence
are eliminated and we get a ?smoother? label se-
quence.
To compute minimum cuts, we use the push-
relabel maximum flow method (Cherkassky and
Goldberg, 1995).3
We need to find values for multiple free param-
eters related to the sequence model. Supervised
optimization is impossible as we do not have any
labeled data. We therefore resort to a proxy mea-
sure, the run count. A run is a sequence of sen-
tences with the same label. We set each param-
eter p to the value that produces a median run
count that is closest to the true median run count
(or, in case of a tie, closest to the true mean run
count). We assume that the optimal median/mean
run count is known. In practice, it can be estimated
from a small number of documents. We find the
optimal value of p by grid search.
5 Features
Choosing features is crucial in situations where
no high-quality training data is available. We are
interested in features that are robust and support
generalization. We propose two linguistic feature
types for S-relevance classification that meet these
requirements.
5.1 Generalization through Semantic
Features
Distant supervision and transfer learning are set-
tings where exact training data is unavailable. We
therefore introduce generalization features which
are more likely to support knowledge transfer. To
generalize over concepts, we use knowledge from
taxonomies. A set of generalizations can be in-
duced by making a cut in the taxonomy and defin-
ing the concepts there as base classes. For nouns,
the taxonomy is WordNet (Miller, 1995) for which
CoreLex (Buitelaar, 1998) gives a set of basic
types. For verbs, VerbNet (Kipper et al, 2008)
already contains base classes.
We add for each verb in VerbNet and for each
noun in CoreLex its base class or basic type as
an additional feature where words tagged by the
mate tagger (Bohnet, 2010) as NN.* are treated as
nouns and words tagged as VB.* as verbs. For ex-
ample, the verb suggest occurs in the VerbNet base
class say, so we add a feature VN:say to the fea-
3using the HIPR tool (www.avglab.com/andrew/
soft.html)
ture representation. We refer to these feature sets
as CoreLex (CX) and VerbNet (VN) features and to
their combination as semantic features (SEM).
5.2 Named Entities
As standard named entity recognition (NER) sys-
tems do not capture categories that are relevant to
the movie domain, we opt for a lexicon-based ap-
proach similar to (Zhuang et al, 2006). We use
the IMDb movie metadata database4 from which
we extract names for the categories <ACTOR>,
<PERSONNEL> (directors, screenwriters, and
composers), and <CHARACTER> (movie charac-
ters). Many entries are unsuitable for NER, e.g.,
dog is frequently listed as a character. We filter
out all words that also appear in lower case in a list
of English words extracted from the dict.cc dictio-
nary.5
A name n can be ambiguous between the cat-
egories (e.g., John Williams). We disambiguate
by calculating the maximum likelihood estimate
of p(c|n) = f(n,c)P
c? f(n,c?)
where c is one of the
three categories and f(n, c) is the number of times
n occurs in the database as a member of cat-
egory c. We also calculate these probabilities
for all tokens that make up a name. While this
can cause false positives, it can help in many
cases where the name obviously belongs to a cat-
egory (e.g., Skywalker in Luke Skywalker is very
likely a character reference). We always inter-
pret a name preceding an actor in parentheses
as a character mention, e.g., Reese Witherspoon
in Tracy Flick (Reese Witherspoon) is an over-
achiever [. . . ] This way, we can recognize charac-
ter mentions for which IMDb provides insufficient
information.
In addition, we use a set of simple rules to prop-
agate annotations to related terms. If a capitalized
word occurs, we check whether it is part of an al-
ready recognized named entity. For example, if
we encounter Robin and we previously encoun-
tered Robin Hood, we assume that the two enti-
ties match. Personal pronouns will match the most
recently encountered named entity. This rule has
precedence over NER, so if a name matches a la-
beled entity, we do not attempt to label it through
NER.
The aforementioned features are encoded as bi-
nary presence indicators for each sentence. This
4www.imdb.com/interfaces/
5dict.cc
958
feature set is referred to as named entities (NE).
5.3 Sequential Features
Following previous sequence classification work
with Maximum Entropy models (e.g., (Ratna-
parkhi, 1996)), we use selected features of adja-
cent sentences. If a sentence contains a feature F,
we add the feature F+1 to the following sentence.
For example, if a <CHARACTER> feature occurs
in a sentence, <CHARACTER+1> is added to the
following sentence. For S-relevance classification,
we perform this operation only for NE features as
they are restricted to a few classes and thus will
not enlarge the feature space notably. We refer to
this feature set as sequential features (SQ).
6 Distant Supervision
Since a large labeled resource for sentiment rele-
vance classification is not yet available, we inves-
tigate semi-supervised methods for creating sen-
timent relevance classifiers. In this section, we
show how to bootstrap a sentiment relevance clas-
sifier by distant supervision (DS) .
Even though we do not have sentiment rele-
vance annotations, there are sources of metadata
about the movie domain that we can leverage for
distant supervision. Specifically, movie databases
like IMDb contain both metadata about the plot,
in particular the characters of a movie, and meta-
data about the ?creators? who were involved in the
production of the movie: actors, writers, direc-
tors, and composers. On the one hand, statements
about characters usually describe the plot and are
not sentiment relevant and on the other hand, state-
ments about the creators tend to be evaluations of
their contributions ? positive or negative ? to the
movie. We formulate a classification rule based
on this observation: Count occurrences of NE fea-
tures and label sentences that contain a majority
of creators (and tied cases) as SR and sentences
that contain a majority of characters as SNR. This
simple labeling rule covers 1583 sentences with
an F1 score of 67.2% on the SR corpus. We call
these labels inferred from NE metadata distant su-
pervision (DS) labels. This is a form of distant
supervision in that we use the IMDb database as
described in Section 5 to automatically label sen-
tences based on which metadata from the database
they contain.
To increase coverage, we train a Maximum En-
tropy (MaxEnt) classifier (Manning and Klein,
2003) on the labels. The MaxEnt model achieves
an F1 of 61.2% on the SR corpus (Table 3, line 2).
As this classifier uses training data that is biased
towards a specialized case (sentences containing
the named entity types creators and characters),
it does not generalize well to other S-relevance
problems and thus yields lower performance on
the full dataset. This distant supervision setup suf-
fers from two issues. First, the classifier only sees
a subset of examples that contain named entities,
making generalization to other types of expres-
sions difficult. Second, there is no way to control
the quality of the input to the classifier, as we have
no confidence measure for our distant supervision
labeling rule. We will address these two issues by
introducing an intermediate step, the unsupervised
sequence model introduced in Section 4.
As described in Section 4, each document is
represented as a graph of sentences and weights
between sentences and source/sink nodes repre-
senting SR/SNR are set to the confidence values
obtained from the distantly trained MaxEnt clas-
sifier. We then apply MinCut as described in the
following paragraphs and select the most confident
examples as training material for a new classifier.
6.1 MinCut Setup
We follow the general MinCut setup described in
Section 4. As explained above, we assume that
creators and directors indicate relevance and char-
acters indicate nonrelevance. Accordingly, we
define nSR to be the number of <ACTOR> and
<PERSONNEL> features occurring in a sentence,
and nSNR the number of <CHARACTER> features.
We then set the individual weight between a sen-
tence and the source/sink nodes to ind(s, x) = nx
where x ? {SR,SNR}. The MinCut parameter c
is set to 1; we wish to give the association scores
high weights as there might be long spans that
have individual weights with zero values.
6.2 Confidence-based Data Selection
We use the output of the base classifier to train su-
pervised models. Since the MinCut model is based
on a weak assumption, it will make many false de-
cisions. To eliminate incorrect decisions, we only
use documents as training data that were labeled
with high confidence. As the confidence measure
for a document, we use the maximum flow value f
? the ?amount of fluid? flowing through the docu-
ment. The max-flow min-cut theorem (Ford and
Fulkerson, 1956) implies that if the flow value
959
Model Features FSR FSNR Fm
1 Majority BL ? 88.3 0.0 44.2
2 MaxEnt (DSlabels) NE 79.8 42.6 61.21
3 DSlabels+MinCut NE 79.6 48.2 63.912
4 DS MaxEnt NE 84.8 46.4 65.612
5 DS MaxEnt NE+SEM 85.2 48.0 66.6124
6 DS CRF NE 83.4 49.5 66.412
7 DS MaxEnt NE+SQ 84.8 49.2 67.01234
8 DS MaxEnt NE+SQ+SEM 84.5 49.1 66.81234
Table 3: Classification results: FSR (S-relevant F1), FSNR (S-nonrelevant F1), and Fm (macro-averaged
F1). Superscript numbers indicate a significant improvement over the corresponding line.
is low, then the cut was found more quickly and
thus can be easier to calculate; this means that the
sentence is more likely to have been assigned to
the correct segment. Following this assumption,
we train MaxEnt and Conditional Random Field
(CRF, (McCallum, 2002)) classifiers on the k%
of documents that have the lowest maximum flow
values f , where k is a parameter which we op-
timize using the run count method introduced in
Section 4.
6.3 Experiments and Results
Table 3 shows S-relevant (FSR), S-nonrelevant
(FSNR) and macro average (Fm) F1 values for dif-
ferent setups with this parameter. We compare the
following setups: (1) The majority baseline (BL)
i.e., choosing the most frequent label (SR). (2) a
MaxEnt baseline trained on DS labels without ap-
plication of MinCut; (3) the base classifier using
MinCut (DSlabels+MinCut) as described above.
Conditions 4-8 train supervised classifiers based
on the labels from DSlabels+MinCut: (4) MaxEnt
with named entities (NE); (5) MaxEnt with NE
and semantic (SEM) features; (6) CRF with NE;
(7) MaxEnt with NE and sequential (SQ) features;
(8) MaxEnt with NE, SQ, and SEM.
We test statistical significance using the approx-
imate randomization test (Noreen, 1989) on doc-
uments with 10,000 iterations at p < .05. We
achieve classification results above baseline using
the MinCut base classifier (line 3) and a consider-
able improvement through distant supervision. We
found that all classifiers using DS labels and Min-
cut are significantly better than MaxEnt trained on
purely rule-based DS labels (line 2). Also, the
MaxEnt models using SQ features (lines 7,8) are
significantly better than the MinCut base classi-
fier (line 3). For comparison to a chain-based se-
quence model, we train a CRF (line 6); however,
the improvement over MaxEnt (line 4) is not sig-
nificant.
We found that both semantic (lines 5,8) and se-
quential (lines 7,8) features help to improve the
classifier. The best model (line 7) performs bet-
ter than MinCut (3) by 3.1% and better than train-
ing on purely rule-generated DS labels (line 2) by
5.8%. However, we did not find a cumulative ef-
fect (line 8) of the two feature sets.
Generally, the quality of NER is crucial in this
task. While IMDb is in general a thoroughly com-
piled database, it is not perfect. For example, all
main characters in Groundhog Day are listed with
their first name only even though the full names
are given in the movie. Also, some entries are in-
tentionally incomplete to avoid spoiling the plot.
The data also contains ambiguities between char-
acters and titles (e.g., Forrest Gump) that are im-
possible to resolve with our maximum likelihood
method. In some types of movies, e.g., documen-
taries, the distinction between characters and ac-
tors makes little sense. Furthermore, ambiguities
like occurrences of common names such as John
are impossible to resolve if there is no earlier full
referring expression (e.g., John Williams).
Feature analysis for the best model using DS
labels (7) shows that NE features are dominant.
This correlation is not surprising as the seed la-
bels were induced based on NE features. Interest-
ingly, some subjective features, e.g., horrible have
high weights for S-nonrelevance, as they are asso-
ciated with non-relevant content such as plot de-
scriptions.
To summarize, the results of our experiments
using distant supervision show that a sentiment
relevance classifier can be trained successfully by
labeling data with a few simple feature rules, with
960
MinCut-based input significantly outperforming
the baseline. Named entity recognition, accom-
plished with data extracted from a domain-specific
database, plays a significant rule in creating an ini-
tial labeling.
7 Transfer Learning
To address the problem that we do not have
enough labeled SR data we now investigate a sec-
ond semi-supervised method for SR classification,
transfer learning (TL). We will use the P&L data
(introduced in Section 2.2) for training. This data
set has labels that are intended to be subjectivity
labels. However, they were automatically created
using heuristics and the resulting labels can be ei-
ther viewed as noisy SR labels or noisy subjectiv-
ity labels. Compared to distant supervision, the
key advantage of training on P&L is that the train-
ing set is much larger, containing around 7 times
as much data.
In TL, the key to success is to find a general-
ized feature representation that supports knowl-
edge transfer. We use a semantic feature gener-
alization method that relies on taxonomies to in-
troduce such features.
We again use MinCut to impose discourse con-
straints. This time, we first classify the data us-
ing a supervised classifier and then use MinCut to
smooth the sequences. The baseline (BL) uses a
simple bag-of-words representation of sentences
for classification which we then extend with se-
mantic features.
7.1 MinCut Setup
We again implement the basic MinCut setup from
Section 4. We set the individual weight ind(s, x)
on the edge between sentence s and class x to the
estimate p(x|s) returned by the supervised classi-
fier. The parameter c of the MinCut model is tuned
using the run count method described in Section 4.
7.2 Experiments and Results
As we would expect, the baseline performance of
the supervised classifier on SR is low: 69.9% (Ta-
ble 4, line 1). MinCut significantly boosts the per-
formance by 7.9% to 77.5% (line 1), a result sim-
ilar to (Pang and Lee, 2004). Adding semantic
features improves supervised classification signif-
icantly by 5.7% (75.6% on line 4). When MinCut
and both types of semantic features are used to-
gether, these improvements are partially cumula-
0.2 0.4 0.6 0.8 1.0
2
4
6
8
10
c
run
 leng
th
77
78
79
80
81
82
F 1
median run countmean run countF1
Figure 2: F1 measure for different values of c.
Horizontal line: optimal median run count. Cir-
cle: selected point.
tive: an improvement over the baseline by 12.6%
to 82.5% (line 4).
We also experiment with a training set where an
artificial class imbalance is introduced, matching
the 80:20 imbalance of SR:SNR in the S-relevance
corpus. After applying MinCut, we find that while
the results for BL with and without imbalances
does not differ significantly. However, models us-
ing CX and VN features and imbalances are ac-
tually significantly inferior to the respective bal-
anced versions. This result suggests that MinCut
is more effective at coping with class imbalances
than artificial balancing.
MinCut and semantic features are successful for
TL because both impose constraints that are more
useful in a setup where noise is a major problem.
MinCut can exploit test set information without
supervision as the MinCut graph is built directly
on each test set review. If high-confidence infor-
mation is ?seeded? within a document and then
spread to neighbors, mistakes with low confidence
are corrected. This way, MinCut also leads to a
compensation of different class imbalances.
The results are evidence that semantic features
are robust to the differences between subjectivity
and S-relevance (cf. Section 2). In the CX+VN
model, meaningful feature classes receive high
weights, e.g., the human class from CoreLex
which contains professions that are frequently as-
sociated with non-relevant plot descriptions.
To illustrate the run-based parameter optimiza-
tion criterion, we show F1 and median/mean run
lengths for different values of c for the best TL
961
Model base classifier MinCutFSR FSNR Fm FSR FSNR Fm
1 BL 81.1 58.6 69.9 87.2 67.8 77.5B
2 CX 82.9 60.1 71.5B 89.0 70.3 79.7BM
3 VN 85.6 62.1 73.9B 91.4 73.6 82.5BM
4 CX+VN 88.3 62.9 75.6B 92.7 72.2 82.5BM
Table 4: Classification results: FSR (S-relevant F1), FSNR (S-nonrelevant F1), and Fm (macro-averaged
F1). B indicates a significant improvement over the BL base classifier (69.9), M over BL MinCut (77.5).
setting (line 4) in Figure 2. Due to differences in
the base classifier, the optimum of c may vary be-
tween the experiments. A weaker base classifier
may yield a higher weight on the sequence model,
resulting in a larger c. The circled point shows the
data point selected through optimization. The op-
timization criterion does not always correlate per-
fectly with F1. However, we find no statistically
significant difference between the selected result
and the highest F1 value.
These experiments demonstrate that S-
relevance classification improves considerably
through TL if semantic feature generalization
and unsupervised sequence classification through
MinCut are applied.
8 Conclusion
A number of different notions, including subjec-
tivity, have been proposed for distinguishing parts
of documents that convey sentiment from those
that do not. We introduced sentiment relevance to
make this distinction and argued that it better re-
flects the requirements of sentiment analysis sys-
tems. Our experiments demonstrated that senti-
ment relevance and subjectivity are related, but
different. To enable other researchers to use this
new notion of S-relevance, we have published the
annotated S-relevance corpus used in this paper.
Since a large labeled sentiment relevance re-
source does not yet exist, we investigated semi-
supervised approaches to S-relevance classifica-
tion that do not require S-relevance-labeled data.
We showed that a combination of different tech-
niques gives us the best results: semantic gener-
alization features, imposing discourse constraints
implemented as the minimum cut graph-theoretic
method, automatic ?distant? labeling based on a
domain-specific metadata database and transfer
learning to exploit existing labels for a related
classification problem.
In future work, we plan to use sentiment rele-
vance in a downstream task such as review sum-
marization.
Acknowledgments
This work was funded by the DFG through the
Sonderforschungsbereich 732. We thank Charles
Jochim, Wiltrud Kessler, and Khalid Al Khatib for
many helpful comments and discussions.
References
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August. Coling 2010 Organizing Com-
mittee.
P. Buitelaar. 1998. CoreLex: systematic polysemy and
underspecification. Ph.D. thesis, Brandeis Univer-
sity.
B. Cherkassky and A. Goldberg. 1995. On imple-
menting push-relabel method for the maximum flow
problem. Integer Programming and Combinatorial
Optimization, pages 157?171.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In WSDM 2008,
pages 231?240.
K. Eguchi and V. Lavrenko. 2006. Sentiment retrieval
using generative models. In EMNLP 2006, pages
345?354.
L.R. Ford and D.R. Fulkerson. 1956. Maximal flow
through a network. Canadian Journal of Mathemat-
ics, 8(3):399?404.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer.
2008. A large-scale classification of English verbs.
Language Resources and Evaluation, 42(1):21?40.
B. Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, pages
978?1420085921.
C. Manning and D. Klein. 2003. Optimization, maxent
models, and conditional estimation without magic.
In NAACL-HLT 2003: Tutorials, page 8.
962
A.K. McCallum. 2002. Mallet: A machine learning
for language toolkit.
G.A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?
41.
E.W. Noreen. 1989. Computer Intensive Methods for
Hypothesis Testing: An Introduction. Wiley.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACL 2004, pages 271?
278.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In ACL-EMNLP 2002, pages 79?86.
A.M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 339?346. Association for Compu-
tational Linguistics.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the con-
ference on empirical methods in natural language
processing, volume 1, pages 133?142.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In EMNLP 2003,
pages 105?112.
M. Taboada, J. Brooke, and M. Stede. 2009. Genre-
based paragraph classification for sentiment analy-
sis. In SIGdial 2009, pages 62?70.
O. Ta?ckstro?m and R. McDonald. 2011. Discover-
ing fine-grained sentiment with latent variable struc-
tured prediction models. In ECIR 2011, pages 368?
374.
S. Tan and X. Cheng. 2009. Improving SCL model
for sentiment-transfer learning. In ACL 2009, pages
181?184.
S. Thrun. 1996. Is learning the n-th thing any easier
than learning the first? In NIPS 1996, pages 640?
646.
T. Wilson and J. Wiebe. 2003. Annotating opinions in
the world press. In 4th SIGdial Workshop on Dis-
course and Dialogue, pages 13?22.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level senti-
ment classification. In EMNLP 2010, pages 1046?
1056.
O. Zaidan, J. Eisner, and C. Piatko. 2007. Using anno-
tator rationales to improve machine learning for text
categorization. In NAACL-HLT 2007, pages 260?
267.
L. Zhuang, F. Jing, and X. Zhu. 2006. Movie review
mining and summarization. In CIKM 2006, pages
43?50.
963
