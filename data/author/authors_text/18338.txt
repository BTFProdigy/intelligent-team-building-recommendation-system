Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 670?680,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Chinese Poetry Generation with Recurrent Neural Networks
Xingxing Zhang and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
x.zhang@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
We propose a model for Chinese poem
generation based on recurrent neural net-
works which we argue is ideally suited to
capturing poetic content and form. Our
generator jointly performs content selec-
tion (?what to say?) and surface realization
(?how to say?) by learning representations
of individual characters, and their com-
binations into one or more lines as well
as how these mutually reinforce and con-
strain each other. Poem lines are gener-
ated incrementally by taking into account
the entire history of what has been gen-
erated so far rather than the limited hori-
zon imposed by the previous line or lexical
n-grams. Experimental results show that
our model outperforms competitive Chi-
nese poetry generation systems using both
automatic and manual evaluation methods.
1 Introduction
Classical poems are a significant part of China?s
cultural heritage. Their popularity manifests itself
in many aspects of everyday life, e.g., as a means
of expressing personal emotion, political views,
or communicating messages at festive occasions
as well as funerals. Amongst the many differ-
ent types of classical Chinese poetry, quatrain and
regulated verse are perhaps the best-known ones.
Both types of poem must meet a set of structural,
phonological, and semantic requirements, render-
ing their composition a formidable task left to the
very best scholars.
An example of a quatrain is shown in Table 1.
Quatrains have four lines, each five or seven char-
acters long. Characters in turn follow specific
phonological patterns, within each line and across
lines. For instance, the final characters in the sec-
ond, fourth and (optionally) first line must rhyme,
??
Missing You
?????? (* Z P P Z)
Red berries born in the warm southland.
?????? (P P Z Z P)
How many branches flush in the spring?
? ????? (* P P Z Z)
Take home an armful, for my sake,
?????? (* Z Z P P)
As a symbol of our love.
Table 1: An example of a 5-char quatrain ex-
hibiting one of the most popular tonal patterns.
The tone of each character is shown at the end of
each line (within parentheses); P and Z are short-
hands for Ping and Ze tones, respectively; * indi-
cates that the tone is not fixed and can be either.
Rhyming characters are shown in boldface.
whereas there are no rhyming constraints for the
third line. Moreover, poems must follow a pre-
scribed tonal pattern. In traditional Chinese, ev-
ery character has one tone, Ping (level tone) or Ze
(downward tone). The poem in Table 1 exempli-
fies one of the most popular tonal patterns (Wang,
2002). Besides adhering to the above formal crite-
ria, poems must exhibit concise and accurate use
of language, engage the reader/hearer, stimulate
their imagination, and bring out their feelings.
In this paper we are concerned with generat-
ing traditional Chinese poems automatically. Al-
though computers are no substitute for poetic cre-
ativity, they can analyze very large online text
repositories of poems, extract statistical patterns,
maintain them in memory and use them to gen-
erate many possible variants. Furthermore, while
amateur poets may struggle to remember and ap-
ply formal tonal and structural constraints, it is rel-
atively straightforward for the machine to check
670
whether a candidate poem conforms to these re-
quirements. Poetry generation has received a fair
amount of attention over the past years (see the
discussion in Section 2), with dozens of computa-
tional systems written to produce poems of vary-
ing sophistication. Beyond the long-term goal of
building an autonomous intelligent system capa-
ble of creating meaningful poems, there are po-
tential short-term applications for computer gen-
erated poetry in the ever growing industry of elec-
tronic entertainment and interactive fiction as well
as in education. An assistive environment for
poem composition could allow teachers and stu-
dents to create poems subject to their require-
ments, and enhance their writing experience.
We propose a model for Chinese poem genera-
tion based on recurrent neural networks. Our gen-
erator jointly performs content selection (?what
to say?) and surface realization (?how to say?).
Given a large collection of poems, we learn repre-
sentations of individual characters, and their com-
binations into one or more lines as well as how
these mutually reinforce and constrain each other.
Our model generates lines in a poem probabilis-
tically: it estimates the probability of the current
line given the probability of all previously gener-
ated lines. We use a recurrent neural network to
learn the representations of the lines generated so
far which in turn serve as input to a recurrent lan-
guage model (Mikolov et al., 2010; Mikolov et al.,
2011b; Mikolov et al., 2011a) which generates the
current line. In contrast to previous approaches
(Greene et al., 2010; Jiang and Zhou, 2008), our
generator makes no Markov assumptions about the
dependencies of the words within a line and across
lines.
We evaluate our approach on the task of qua-
train generation (see Table 1 for a human-written
example). Experimental results show that our
model outperforms competitive Chinese poetry
generation systems using both automatic and man-
ual evaluation methods.
2 Related Work
Automated poetry generation has been a popular
research topic over the past decades (see Colton
et al. (2012) and the references therein). Most ap-
proaches employ templates to construct poems ac-
cording to a set of constraints (e.g., rhyme, me-
ter, stress, word frequency) in combination with
corpus-based and lexicographic resources. For
example, the Haiku poem generator presented in
Wu et al. (2009) and Tosa et al. (2008) produces
poems by expanding user queries with rules ex-
tracted from a corpus and additional lexical re-
sources. Netzer et al. (2009) generate Haiku
with Word Association Norms, Agirrezabal et
al. (2013) compose Basque poems using patterns
based on parts of speech and WordNet (Fellbaum,
1998), and Oliveira (2012) presents a generation
algorithm for Portuguese which leverages seman-
tic and grammar templates.
A second line of research uses genetic algo-
rithms for poem generation (Manurung, 2003;
Manurung et al., 2012; Zhou et al., 2010). Ma-
nurung et al. (2012) argue that at a basic level
all (machine-generated) poems must satisfy the
constraints of grammaticality (i.e., a poem must
syntactically well-formed), meaningfulness (i.e., a
poem must convey a message that is meaningful
under some interpretation) and poeticness (i.e., a
poem must exhibit features that distinguishes it
from non-poetic text, e.g., metre). Their model
generates several candidate poems and then uses
stochastic search to find those which are grammat-
ical, meaningful, and poetic.
A third line of research draws inspiration from
statistical machine translation (SMT) and re-
lated text-generation applications such as sum-
marization. Greene et al. (2010) infer meters
(stressed/unstressed syllable sequences) from a
corpus of poetic texts which they subsequently
use for generation together with a cascade of
weighted finite-state transducers interpolated with
IBM Model 1. Jiang and Zhou (2008) generate
Chinese couplets (two line poems) using a phrase-
based SMT approach which translates the first line
to the second line. He et al. (2012) extend this al-
gorithm to generate four-line quatrains by sequen-
tially translating the current line from the previous
one. Yan et al. (2013) generate Chinese quatrains
based on a query-focused summarization frame-
work. Their system takes a few keywords as input
and retrieves the most relevant poems from a cor-
pus collection. The retrieved poems are segmented
into their constituent terms which are then grouped
into clusters. Poems are generated by iteratively
selecting terms from clusters subject to phonolog-
ical, structural, and coherence constraints.
Our approach departs from previous work in
two important respects. Firstly, we model the tasks
of surface realization and content selection jointly
671
?(spring)
??(lute)
?(drunk)
Keywords
ShiXueHanYing
spring
lute drunk
? ? ? ? ?
? ? ? ? ?
...
Candidate lines
Line 1Line 2Line 3Line 4
First line
generation
Next line
generation
Figure 1: Poem generation with keywords spring, lute, and drunk. The keywords are expanded into
phrases using a poetic taxonomy. Phrases are then used to generate the first line. Following lines are
generated by taking into account the representations of all previously generated lines.
using recurrent neural networks. Structural, se-
mantic, and coherence constraints are captured
naturally in our framework, through learning the
representations of individual characters and their
combinations. Secondly, generation proceeds by
taking into account multi-sentential context rather
than the immediately preceding sentence. Our
work joins others in using continuous representa-
tions to express the meaning of words and phrases
(Socher et al., 2012; Mikolov et al., 2013) and
how these may be combined in a language mod-
eling context (Mikolov and Zweig, 2012). More
recently, continuous translation models based on
recurrent neural networks have been proposed as
a means to map a sentence from the source lan-
guage to sentences in the target language (Auli
et al., 2013; Kalchbrenner and Blunsom, 2013).
These models are evaluated on the task of rescor-
ing n-best lists of translations. We use neural net-
works more directly to perform the actual poem
generation task.
3 The Poem Generator
As common in previous work (Yan et al., 2013;
He et al., 2012) we assume that our generator op-
erates in an interactive context. Specifically, the
user supplies keywords (e.g., spring, lute, drunk )
highlighting the main concepts around which the
poem will revolve. As illustrated in Figure 1, our
generator expands these keywords into a set of re-
lated phrases. We assume the keywords are re-
stricted to those attested in the ShiXueHanYing po-
etic phrase taxonomy (He et al., 2012; Yan et al.,
2013). The latter contains 1,016 manual clusters
of phrases (Liu, 1735); each cluster is labeled with
a keyword id describing general poem-worthy top-
ics. The generator creates the first line of the poem
based on these keywords. Subsequent lines are
generated based on all previously generated lines,
subject to phonological (e.g., admissible tonal pat-
terns) and structural constraints (e.g., whether the
quatrain is five or seven characters long).
To create the first line, we select all phrases
corresponding to the user?s keywords and gener-
ate all possible combinations satisfying the tonal
pattern constraints. We use a language model to
rank the generated candidates and select the best-
ranked one as the first line in the poem. In im-
plementation, we employ a character-based recur-
rent neural network language model (Mikolov et
al., 2010) interpolated with a Kneser-Ney trigram
and find the n-best candidates with a stack de-
coder (see Section 3.5 for details). We then gen-
erate the second line based on the first one, the
third line based on the first two lines, and so on.
Our generation model computes the probability
of line S
i+1
= w
1
,w
2
, . . . ,w
m
, given all previously
generated lines S
1:i
(i? 1) as:
P(S
i+1
|S
1:i
) =
m?1
?
j=1
P(w
j+1
|w
1: j
,S
1:i
) (1)
Equation (1), decomposes P(S
i+1
|S
1:i
) as the prod-
uct of the probability of each character w
j
in
the current line given all previously generated
characters w
1: j?1
and lines S
1:i
. This means
that P(S
i+1
|S
1:i
) is sensitive to previously gener-
ated content and currently generated characters.
The estimation of the term P(w
j+1
|w
1: j
,S
1:i
)
lies at the heart of our model. We learn repre-
sentations for S
1:i
, the context generated so far,
using a recurrent neural network whose output
672
serves as input to a second recurrent neural net-
work used to estimate P(w
j+1
|w
1: j
,S
1:i
). Figure 2
illustrates the generation process for the ( j+ 1)th
character w
j+1
in the (i + 1)th line S
i+1
. First,
lines S
1:i
are converted into vectors v
1:i
with a
convolutional sentence model (CSM; described in
Section 3.1). Next, a recurrent context model
(RCM; see Section 3.2) takes v
1:i
as input and
outputs u
j
i
, the representation needed for gener-
ating w
j+1
? S
i+1
. Finally, u
1
i
,u
2
i
, . . . ,u
j
i
and the
first j characters w
1: j
in line S
i+1
serve as input to
a recurrent generation model (RGM) which esti-
mates P(w
j+1
= k|w
1: j
,S
1:i
) with k ?V , the prob-
ability distribution of the ( j + 1)th character over
all words in the vocabulary V . More formally, to
estimate P(w
j+1
|w
1: j
,S
1:i
) in Equation (1), we ap-
ply the following procedure:
v
i
= CSM(S
i
) (2a)
u
j
i
= RCM(v
1:i
, j) (2b)
P(w
j+1
|w
1: j
,S
1:i
) = RGM(w
1: j+1
,u
1: j
i
) (2c)
We obtain the probability of the (i + 1)th sen-
tence P(S
i+1
|S
1:i
), by running the RGM in (2c)
above m? 1 times (see also Equation (1)). In the
following, we describe how the different compo-
nents of our model are obtained.
3.1 Convolutional Sentence Model (CSM)
The CSM converts a poem line into a vector. In
principle, any model that produces vector-based
representations of phrases or sentences could be
used (Mitchell and Lapata, 2010; Socher et al.,
2012). We opted for the convolutional sentence
model proposed in Kalchbrenner and Blunsom
(2013) as it is n-gram based and does not make
use of any parsing, POS-tagging or segmentation
tools which are not available for Chinese poems.
Their model computes a continuous representation
for a sentence by sequentially merging neighbor-
ing vectors (see Figure 3).
Let V denote the character vocabulary in our
corpus; L ? R
q?|V |
denotes a character embed-
ding matrix whose columns correspond to char-
acter vectors (q represents the hidden unit size).
Such vectors can be initialized randomly or ob-
tained via a training procedure (Mikolov et al.,
2013). Let w denote a character with index k;
e(w) ?R
|V |?1
is a vector with zero in all positions
except e(w)
k
= 1; T
l
? R
q?N
l
is the sentence rep-
resentation in the lth layer, where N
l
is the num-
ber of columns in the lth layer (N
l
= 1 in the
v
i
u
j
i
h
i
h
i?1
u
k
i
(k 6= j)
RCM
1-of-N encoding of
w
j
=(0,. . . ,1,. . . ,0)
r
j
r
j?1
P(w
j+1
|w
1: j
,S
1:i
)
RGM
Figure 2: Generation of the ( j + 1)th charac-
ter w
j+1
in the (i + 1)th line S
i+1
. The recur-
rent context model (RCM) takes i lines as in-
put (represented by vectors v
1
, . . . ,v
i
) and cre-
ates context vectors for the recurrent generation
model (RGM). The RGM estimates the probabil-
ity P(w
j+1
|w
1: j
,S
1:i
).
top layer); C
l,n
? R
q?n
is an array of weight ma-
trices which compress neighboring n columns in
the lth layer to one column in the (l + 1)th layer.
Given a sentence S = w
1
,w
2
, . . . ,w
m
, the first layer
is represented as:
T
1
= [L ? e(w
1
),L ? e(w
2
), . . . ,L ? e(w
m
)]
N
1
= m
(3)
The (l +1)th layer is then computed as follows:
T
l+1
:, j
= ?(
n
?
i=1
T
l
:, j+i?1
C
l,n
:,i
)
N
l+1
= N
l
?n+1
1? j ? N
l+1
(4)
where T
l
is the representation of the previous
layer l, C
l,n
a weight matrix,  element-wise vec-
tor product, and ? a non-linear function. We com-
press two neighboring vectors in the first two lay-
ers and three neighboring vectors in the remaining
layers. Specifically, for quatrains with seven char-
acters, we use C
1,2
, C
2,2
, C
3,3
, C
4,3
to merge vec-
tors in each layer (see Figure 3); and for quatrains
with five characters we use C
1,2
, C
2,2
, C
3,3
.
673
? ? ? ? ? ? ?
Far off I watch the waterfall plunge to the
long river.
C
1,2
C
2,2
C
3,3
C
4,3
Figure 3: Convolutional sentence model for 7-char
quatrain. The first layer has seven vectors, one
for each character. Two neighboring vectors are
merged to one vector in the second layer with
weight matrix C
1,2
. In other layers, either two or
three neighboring vectors are merged.
3.2 Recurrent Context Model (RCM)
The RCM takes as input the vectors representing
the i lines generated so far and reduces them to a
single context vector which is then used to gener-
ate the next character (see Figure 2). We compress
the i previous lines to one vector (the hidden layer)
and then decode the compressed vector to different
character positions in the current line. The output
layer consists thus of several vectors (one for each
position) connected together. This way, different
aspects of the context modulate the generation of
different characters.
Let v
1
, . . . ,v
i
(v
i
?R
q?1
) denote the vectors of
the previous i lines; h
i
? R
q?1
is their compressed
representation (hidden layer) which is obtained
with matrix M ? R
q?2q
; matrix U
j
decodes h
i
to
u
j
i
? R
q?1
in the (i+ 1)th line. The computation
of the RCM proceeds as follows:
h
0
= 0
h
i
= ?(M ?
[
v
i
h
i?1
]
)
u
j
i
= ?(U
j
?h
i
) 1? j ? m?1
(5)
where ? is a non-linear function such as sigmoid
and m the line length. Advantageously, lines in
classical Chinese poems have a fixed length of five
or seven characters. Therefore, the output layer of
the recurrent context model only needs two weight
matrices (one for each length) and the number of
parameters still remains tractable.
3.3 Recurrent Generation Model (RGM)
As shown in Figure 2, the RGM estimates the
probability distribution of the next character (over
the entire vocabulary) by taking into account the
context vector provided by the RCM and the
1-of-N encoding of the previous character. The
RGM is essentially a recurrent neural network lan-
guage model (Mikolov et al., 2010) with an aux-
iliary input layer, i.e., the context vector from
the RCM. Similar strategies for encoding addi-
tional information have been adopted in related
language modeling and machine translation work
(Mikolov and Zweig, 2012; Kalchbrenner and
Blunsom, 2013; Auli et al., 2013).
Let S
i+1
= w
1
,w
2
, . . . ,w
m
denote the line
to be generated. The RGM must esti-
mate P(w
j+1
|w
1: j
,S
1:i
), however, since the first
i lines have been encoded in the context vector u
j
i
,
we compute P(w
j+1
|w
1: j
,u
j
i
) instead. Therefore,
the probability P(S
i+1
|S
1:i
) becomes:
P(S
i+1
|S
1:i
) =
m?1
?
j=1
P(w
j+1
|w
1: j
,u
j
i
)
(6)
Let |V | denote the size of the character vocabu-
lary. The RGM is specified by a number of ma-
trices. Matrix H ? R
q?q
(where q represents the
hidden unit size) transforms the context vector to
a hidden representation; matrix X ? R
q?|V |
trans-
forms a character to a hidden representation, ma-
trix R ? R
q?q
implements the recurrent transfor-
mation and matrix Y ? R
|V |?q
decodes the hidden
representation to weights for all words in the vo-
cabulary. Let w denote a character with index k
in V ; e(w) ? R
|V |?1
represents a vector with zero
in all positions except e(w)
k
= 1, r
j
is the hidden
layer of the RGM at step j, and y
j+1
the output of
the RGM, again at step j. The RGM proceeds as
follows:
r
0
= 0 (7a)
r
j
= ?(R ? r
j?1
+X ? e(w
j
)+H ?u
j
i
) (7b)
y
j+1
= Y ? r
j
(7c)
where ? is a nonlinear function (e.g., sigmoid).
674
The probability of the ( j+1)th word given the
previous j words and the previous i lines is esti-
mated by a softmax function:
P(w
j+1
= k|w
1: j
,u
j
i
) =
exp(y
j+1,k
)
?
|V |
k=1
exp(y
j+1,k
)
(8)
We obtain P(S
i+1
|S
1:i
) by multiplying all the terms
in the right hand-side of Equation (6).
3.4 Training
The objective for training is the cross entropy er-
rors of the predicted character distribution and the
actual character distribution in our corpus. An
l
2
regularization term is also added to the objec-
tive. The model is trained with back propagation
through time (Rumelhart et al., 1988) with sen-
tence length being the time step. The objective
is minimized by stochastic gradient descent. Dur-
ing training, the cross entropy error in the output
layer of the RGM is back-propagated to its hid-
den and input layers, then to the RCM and finally
to the CSM. The same number of hidden units
(q = 200) is used throughout (i.e., in the RGM,
RCM, and CSM). In our experiments all param-
eters were initialized randomly, with the excep-
tion of the word embedding matrix in the CSM
which was initialized with word2vec embeddings
(Mikolov et al., 2013) obtained from our poem
corpus (see Section 4 for details on the data we
used).
To speed up training, we employed word-
classing (Mikolov et al., 2011b). To compute the
probability of a character, we estimate the proba-
bility of its class and then multiply it by the proba-
bility of the character conditioned on the class. In
our experiments we used 82 (square root of |V |)
classes which we obtained by applying hierarchi-
cal clustering on character embeddings. This strat-
egy outperformed better known frequency-based
classing methods (Zweig and Makarychev, 2013)
on our task.
Our poem generator models content selection
and lexical choice and their interaction, but does
not have a strong notion of local coherence,
as manifested in poetically felicitous line-to-line
transitions. In contrast, machine translation mod-
els (Jiang and Zhou, 2008) have been particu-
larly successful at generating adjacent lines (cou-
plets). To enhance coherence, we thus interpolate
our model with two machine translation features
(i.e., inverted phrase translation model feature and
inverted lexical weight feature). Also note, that
in our model surface generation depends on the
last observed character and the state of the hidden
layer before this observation. This way, there is no
explicitly defined context, and history is captured
implicitly by the recurrent nature of the model.
This can be problematic for our texts which must
obey certain stylistic conventions and sound po-
etic. In default of a better way of incorporating
poeticness into our model, we further interpolate it
with a language model feature (i.e., a Kneser-Ney
trigram model).
Throughout our experiments, we use the
RNNLM toolkit to train the character-based recur-
rent neural network language model (Mikolov et
al., 2010). Kneser-Ney n-grams were trained with
KenLM (Heafield, 2011).
3.5 Decoding
Our decoder is a stack decoder similar to Koehn
et al. (2003). In addition, it implements the tonal
pattern and rhyming constraints necessary for gen-
erating well-formed Chinese quatrains. Once the
first line in a poem is generated, its tonal pattern
is determined. During decoding, phrases violat-
ing this pattern are ignored. As discussed in Sec-
tion 1, the final characters of the second and the
fourth lines must rhyme. We thus remove during
decoding fourth lines whose final characters do not
rhyme with the second line. Finally, we use MERT
training (Och, 2003) to learn feature weights for
the decoder.
4 Experimental Design
Data We created a corpus of classical Chinese
poems by collating several online resources: Tang
Poems, Song Poems, Song Ci, Ming Poems, Qing
Poems, and Tai Poems. The corpus consists
of 284,899 poems in total. 78,859 of these are
quatrains and were used for training and evalu-
ating our model.
1
Table 2 shows the different
partitions of this dataset (POEMLM) into train-
ing (QTRAIN)
2
, validation (QVALID) and testing
(QTEST). Half of the poems in QVALID and
QTEST are 5-char quatrains and the other half
are 7-char quatrains. All poems except QVALID
1
The data used in our experiments can be downloaded
from http://homepages.inf.ed.ac.uk/mlap/index.
php?page=resources.
2
Singleton characters in QTRAIN (6,773 in total) were re-
placed by <R> to reduce data sparsity.
675
Poems Lines Characters
QTRAIN 74,809 299,236 2,004,460
QVALID 2,000 8,000 48,000
QTEST 2,050 8,200 49,200
POEMLM 280,849 2,711,034 15,624,283
Table 2: Dataset partitions of our poem corpus.
and QTEST were used for training the character-
based language models (see row POEMLM in Ta-
ble 2). We also trained word2vec embeddings on
POEMLM. In our experiments, we generated qua-
trains following the eight most popular tonal pat-
terns according to Wang (2002).
Perplexity Evaluation Evaluation of machine-
generated poetry is a notoriously difficult task.
Our evaluation studies were designed to assess
Manurung et al.?s (2012) criteria of grammatical-
ity, meaningfulness, and poeticness. As a san-
ity check, we first measured the perplexity of our
model with respect to the goldstandard. Intu-
itively, a better model should assign larger proba-
bility (and therefore lower perplexity) to goldstan-
dard poems.
BLEU-based Evaluation We also used BLEU
to evaluate our model?s ability to generate the sec-
ond, third and fourth line given previous goldstan-
dard lines. A problematic aspect of this evalu-
ation is the need for human-authored references
(for a partially generated poem) which we do not
have. We obtain references automatically follow-
ing the method proposed in He et al. (2012). The
main idea is that if two lines share a similar topic,
the lines following them can be each other?s ref-
erences. Let A and B denote two adjacent lines
in a poem, with B following A. Similarly, let line
B
?
follow line A
?
in another poem. If lines A and
A
?
share some keywords in the same cluster in the
Shixuehanying taxonomy, then B and B
?
can be
used as references for both A and A
?
. We use this
algorithm on the Tang Poems section of our corpus
to build references for poems in the QVALID and
QTEST data sets. Poems in QVALID (with auto-
generated references) were used for MERT train-
ing and Poems in QTEST (with auto-generated ref-
erences) were used for BLEU evaluation.
Human Evaluation Finally, we also evaluated
the generated poems by eliciting human judg-
Models Perplexity
KN5 172
RNNLM 145
RNNPG 93
Table 3: Perplexities for different models.
ments. Specifically, we invited 30 experts
3
on
Chinese poetry to assess the output of our gen-
erator (and comparison systems). These experts
were asked to rate the poems using a 1?5 scale on
four dimensions: fluency (is the poem grammati-
cal and syntactically well-formed?), coherence (is
the poem thematically and logically structured?),
meaningfulness (does the poem convey a mean-
ingful message to the reader?) and poeticness
(does the text display the features of a poem?).
We also asked our participants to evaluate system
outputs by ranking the generated poems relative to
each other as a way of determining overall poem
quality (Callison-Burch et al., 2012).
Participants rated the output of our model and
three comparison systems. These included He et
al.?s (2012) SMT-based model (SMT), Yan et al.?s
(2013) summarization-based system (SUM), and
a random baseline which creates poems by ran-
domly selecting phrases from the Shixuehanying
taxonomy given some keywords as input. We
also included human written poems whose content
matched the input keywords. All systems were
provided with the same keywords (i.e., the same
cluster names in the ShiXueHanYing taxonomy).
In order to compare all models on equal footing,
we randomly sampled 30 sets of keywords (with
three keywords in each set) and generated 30 qua-
trains for each system according to two lengths,
namely 5-char and 7-char. Overall, we obtained
ratings for 300 (5?30?2) poems.
5 Results
The results of our perplexity evaluation are sum-
marized in Table 3. We compare our RNN-based
poem generator (RNNPG) against Mikolov?s
(2010) recurrent neural network language model
(RNNLM) and a 5-gram language model with
Kneser-Ney smoothing (KN5). All models were
trained on QTRAIN and tuned on QVALID. The
perplexities were computed on QTEST. Note that
3
27 participants were professional or amateur poets and
three were Chinese literature students who had taken at least
one class on Chinese poetry composition.
676
Models
1? 2 2? 3 3? 4 Average
5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-char
SMT 0.0559 0.0906 0.0410 0.1837 0.0547 0.1804 0.0505 0.1516
RNNPG 0.0561 0.1868 0.0515 0.2102 0.0572 0.1800 0.0549 0.1923
Table 4: BLEU-2 scores on 5-char and 7-char quatrains. Given i goldstandard lines, BLEU-2 scores are
computed for the next (i+1)th lines.
Models
Fluency Coherence Meaning Poeticness Rank
5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-char 5-char 7-char
Random 2.52 2.18 2.22 2.16 2.02 1.93 1.77 1.71 0.31 0.26
SUM 1.97 1.91 2.08 2.33 1.84 1.98 1.66 1.73 0.25 0.22
SMT 2.81 3.01 2.47 2.76 2.33 2.73 2.08 2.36 0.43 0.53
RNNPG 4.01
**
3.44
*
3.18
**
3.12
*
3.20
**
3.02 2.80
**
2.68
*
0.73
**
0.64
*
Human 4.31
+
4.19
++
3.81
++
4.00
++
3.61
+
3.91
++
3.29
++
3.49
++
0.79 0.84
++
Table 5: Mean ratings elicited by humans on 5-char and 7-char quatrains. Diacritics
**
(p < 0.01)
and
*
(p < 0.05) indicate our model (RNNPG) is significantly better than all other systems except Human.
Diacritics
++
(p < 0.01) and
+
(p < 0.05) indicate Human is significantly better than all other systems.
the RNNPG estimates the probability of a poem
line given at least one previous line. Therefore, the
probability of a quatrain assigned by the RNNPG
is the probability of the last three lines. For a fair
comparison, RNNLM and KN5 only leverage the
last three lines of each poem during training, vali-
dation and testing. The results in Table 3 indicate
that the generation ability of the RNNPG is better
than KN5 and RNNLM. Note that this perplexity-
style evaluation is not possible for models which
cannot produce probabilities for gold standard po-
ems. For this reason, other related poem gener-
ators (Yan et al., 2013; He et al., 2012) are not
included in the table.
The results of our evaluation using BLEU-2 are
summarized in Table 4. Here, we compare our
system against the SMT-based poem generation
model of He et al. (2012).
4
Their system is a
linear combination of two translation models (one
with five features and another one with six). Our
model uses three of their features, namely the in-
verted phrase translation model feature, the lexical
weight feature, and a Kneser-Ney trigram feature.
Unfortunately, it is not possible to evaluate Yan
et al.?s (2013) summarization-based system with
BLEU, as it creates poems as a whole and there is
no obvious way to generate next lines with their
4
Our re-implementation of their system delivered very
similar scores to He et al. (2012). For example, we ob-
tained an average BLEU-1 of 0.167 for 5-char quatrains and
0.428 for 7-char quatrains compared to their reported scores
of 0.141 and 0.380, respectively.
algorithm. The BLEU scores in Table 4 indicate
that, given the same context lines, the RNNPG is
better than SMT at generating what to say next.
BLEU scores should be, however, viewed with
some degree of caution. Aside from being an ap-
proximation of human judgment (Callison-Burch
et al., 2012), BLEU might be unnecessarily con-
servative for poem composition which by its very
nature is a creative endeavor.
The results of our human evaluation study are
shown in Table 5. Each column reports mean rat-
ings for a different dimension (e.g., fluency, co-
herence). Ratings for 5-char and 7-char quatrains
are shown separately. The last column reports
rank scores for each system (Callison-Burch et al.,
2012). In a ranked list of N items (N = 5 here), the
score of the ith ranked item is
(N?i)
(N?1)
. The numer-
ator indicates how many times a systems won in
pairwise comparisons, while the denominator nor-
malizes the score.
With respect to 5-char quatrains, RNNPG is
significantly better than Random, SUM and SMT
on fluency, coherence, meaningfulness, poeticness
and ranking scores (using a t-test). On all dimen-
sions, human-authored poems are rated as signif-
icantly better than machine-generated ones, with
the exception of overall ranking. Here, the dif-
ference between RNNPG and Human is not sig-
nificant. We obtain similar results with 7-char
quatrains. In general, RNNPG seems to perform
better on the shorter poems. The mean ratings
677
?????, ???????,
Egrets stood, peeping fishes. Budding branches are full of romance.
?????. ???????.
Water was still, reflecting mountains. Plum blossoms are invisible but adorable.
?????, ???????,
The wind went down by nightfall, With the east wind comes Spring.
?????. ???????.
as the moon came up by the tower. Where on earth do I come from?
Table 6: Example output produced by our model (RNNPG).
are higher and the improvements over other sys-
tems are larger. Also notice, that the score mar-
gins between the human- and machine-written po-
ems become larger for 7-char quatrains. This in-
dicates that the composition of 7-char quatrains is
more difficult compared to 5-char quatrains. Ta-
ble 6 shows two example poems (5-char and 7-
char) produced by our model which received high
scores with respect to poeticness.
Interestingly, poems generated by SUM
5
are
given ratings similar to Random. In fact SUM
is slightly worse (although not significantly) than
Random on all dimensions, with the exception of
coherence. In the human study reported in Yan et
al. (2013), SUM is slightly better than SMT. There
are several reasons for this discrepancy. We used
a more balanced experimental design: all systems
generated poems from the same keywords which
were randomly chosen. We used a larger dataset
to train the SMT model compared to Yan et al.
(284,899 poems vs 61,960). The Random baseline
is not a straw-man; it selects phrases from a taxon-
omy of meaningful clusters edited by humans and
closely related to the input keywords.
6 Conclusions
In this paper we have presented a model for Chi-
nese poem generation based on recurrent neural
networks. Our model jointly performs content se-
lection and surface realization by learning repre-
sentations of individual characters and their com-
binations within and across poem lines. Previous
work on poetry generation has mostly leveraged
contextual information of limited length (e.g., one
sentence). In contrast, we introduced two recur-
rent neural networks (the recurrent context model
and recurrent generation model) which naturally
5
We made a good-faith effort to re-implement their poem
generation system. We are grateful to Rui Yan for his help
and technical advice.
capture multi-sentential content. Experimental re-
sults show that our model yields high quality po-
ems compared to the state of the art. Perhaps un-
surprisingly, our human evaluation study revealed
that machine-generated poems lag behind human-
generated ones. It is worth bearing in mind that
poetry composition is a formidable task for hu-
mans, let alone machines. And that the poems
against which our output was compared have been
written by some of the most famous poets in Chi-
nese history!
Avenues for future work are many and varied.
We would like to generate poems across differ-
ent languages and genres (e.g., Engish sonnets or
Japanese haiku). We would also like to make the
model more sensitive to line-to-line transitions and
stylistic conventions by changing its training ob-
jective to a combination of cross-entropy error and
BLEU score. Finally, we hope that some of the
work described here might be of relevance to other
generation tasks such as summarization, concept-
to-text generation, and machine translation.
Acknowledgments
We would like to thank Eva Halser for valuable
discussions on the machine translation baseline.
We are grateful to the 30 Chinese poetry experts
for participating in our rating study. Thanks to
Gujing Lu, Chu Liu, and Yibo Wang for their help
with translating the poems in Table 6 and Table 1.
References
Manex Agirrezabal, Bertol Arrieta, Aitzol Astigarraga,
and Mans Hulden. 2013. POS-Tag Based Po-
etry Generation with WordNet. In Proceedings of
the 14th European Workshop on Natural Language
Generation, pages 162?166, Sofia, Bulgaria.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
678
Modeling with Recurrent Neural Networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054, Seattle, Washington, USA.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the 7th Work-
shop on Statistical Machine Translation, pages 10?
51, Montr?eal, Canada.
Simon Colton, Jacob Goodwin, and Tony Veale. 2012.
Full-FACE Poetry Generation. In Proceedings of the
International Conference on Computational Cre-
ativity, pages 95?102, Dublin, Ireland.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic Analysis of Rhythmic Poetry with
Applications to Generation and Translation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 524?
533, Cambridge, MA.
Jing He, Ming Zhou, and Long Jiang. 2012. Gener-
ating Chinese Classical Poems with Statistical Ma-
chine Translation Models. In Proceedings of the
26th AAAI Conference on Artificial Intelligence,
pages 1650?1656, Toronto, Canada.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Long Jiang and Ming Zhou. 2008. Generating Chinese
Couplets using a Statistical MT Approach. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics, pages 377?384, Manch-
ester, UK, August.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Continuous Translation Models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700?1709, Seattle,
Washington.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54, Edmonton, Canada.
Wenwei Liu. 1735. ShiXueHanYing.
Ruli Manurung, Graeme Ritchie, and Henry Thomp-
son. 2012. Using Genetic Algorithms to Create
Meaningful Poetic Text. Journal of Experimental
Theoretical Artificial Intelligence, 24(1):43?64.
Ruli Manurung. 2003. An Evolutionary Algorithm Ap-
proach to Poetry Generation. Ph.D. thesis, Univer-
sity of Edinburgh.
Tomas Mikolov and Geoffrey Zweig. 2012. Con-
text Dependent Recurrent Neural Network Lan-
guage Model. In Proceedings of 2012 IEEE Work-
shop on Spoken Language Technology, pages 234?
239, Miami, Florida.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proceedings of INTERSPEECH, pages 1045?1048,
Makuhari, Japan.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011a. Strategies
for Training Large Scale Neural Network Language
Models. In Proceedings of ASRU 2011, pages 196?
201, Hilton Waikoloa Village, Big Island, Hawaii,
US.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of Recurrent Neural Network Language
Model. In Proceedings of the 2011 IEEE Interna-
tional Conference on Acoustics, Speech and Signal
Processing, pages 5528?5531, Prague, Czech Re-
public.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed Representa-
tions of Words and Phrases and their Composition-
ality. In Advances in Neural Information Process-
ing Systems, pages 3111?3119, Lake Tahoe, Nevada,
United States.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1439.
Yael Netzer, David Gabay, Yoav Goldberg, and
Michael Elhadad. 2009. Gaiku: Generating Haiku
with Word Associations Norms. In Proceedings of
the Workshop on Computational Approaches to Lin-
guistic Creativity, pages 32?39, Boulder, Colorado.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
Hugo Gonc?alo Oliveira. 2012. PoeTryMe: a Versa-
tile Platform for Poetry Generation. Computational
Creativity, Concept Invention, and General Intelli-
gence, 1:21.
David Rumelhart, Geoffrey Hinton, and Ronald
Williams. 1988. Learning Representations by Back-
propagating Errors. MIT Press, Cambridge, MA,
USA.
679
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Compo-
sitionality through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1201?1211, Jeju Island, Korea.
Naoko Tosa, Hideto Obara, and Michihiko Minoh.
2008. Hitch Haiku: An Interactive Supporting Sys-
tem for Composing Haiku Poem How I Learned to
Love the Bomb: Defcon and the Ethics of Com-
puter Games. In Proceedings of the 7th Inter-
national Conference on Entertainment Computing,
pages 209?216, Pittsburgh, PA.
Li Wang. 2002. A Summary of Rhyming Constraints
of Chinese Poems (Shi Ci Ge Lv Gai Yao). Beijing
Press, 2002.
Xiaofeng Wu, Naoko Tosa, and Ryohei Nakatsu. 2009.
New Hitch Haiku: An Interactive Renku Poem
Composition Supporting Tool Applied for Sightsee-
ing Navigation System. In Proceedings of the 8th
International Conference on Entertainment Com-
puting, pages 191?196, Paris, France.
Rui Yan, Han Jiang, Mirella Lapata, Shou-De Lin,
Xueqiang Lv, and Xiaoming Li. 2013. I, Poet:
Automatic Chinese Poetry Composition Through a
Generative Summarization Framework Under Con-
strained Optimization. In Proceedings of the 23rd
International Joint Conference on Artificial Intelli-
gence, pages 2197?2203, Beijing, China.
Cheng-Le Zhou, Wei You, and Xiaojun Ding. 2010.
Genetic Algorithm and its Implementation of Au-
tomatic Generation of Chinese SongCi. Journal of
Software, pages 427?437.
Geoffrey Zweig and Konstantin Makarychev. 2013.
Speed Regularization and Optimality in Word Class-
ing. In Proceedings of the 2014 IEEE International
Conference on Acoustics, Speech, and Signal Pro-
cessing, pages 8237?8241, Florence, Italy.
680
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 810?815,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Towards Accurate Distant Supervision for Relational Facts Extraction
Xingxing Zhang1 Jianwen Zhang2? Junyu Zeng3 Jun Yan2 Zheng Chen2 Zhifang Sui1
1Key Laboratory of Computational Linguistics (Peking University), Ministry of Education,China
2Microsoft Research Asia
3Beijing University of Posts and Telecommunications
1{zhangxingxing,szf}@pku.edu.cn
2{jiazhan,junyan,zhengc}@microsoft.com
3junyu.zeng@gmail.com
Abstract
Distant supervision (DS) is an appealing
learning method which learns from exist-
ing relational facts to extract more from
a text corpus. However, the accuracy is
still not satisfying. In this paper, we point
out and analyze some critical factors in
DS which have great impact on accuracy,
including valid entity type detection,
negative training examples construction
and ensembles. We propose an approach
to handle these factors. By experimenting
on Wikipedia articles to extract the facts in
Freebase (the top 92 relations), we show
the impact of these three factors on the
accuracy of DS and the remarkable im-
provement led by the proposed approach.
1 Introduction
Recently there are great efforts on building large
structural knowledge bases (KB) such as Free-
base, Yago, etc. They are composed of relational
facts often represented in the form of a triplet,
(SrcEntity, Relation, DstEntity),
such as ?(Bill Gates, BornIn, Seattle)?. An impor-
tant task is to enrich such KBs by extracting more
facts from text. Specifically, this paper focuses on
extracting facts for existing relations. This is dif-
ferent from OpenIE (Banko et al, 2007; Carlson et
al., 2010) which needs to discover new relations.
Given large amounts of labeled sentences,
supervised methods are able to achieve good
performance (Zhao and Grishman, 2005; Bunescu
and Mooney, 2005). However, it is difficult to
handle large scale corpus due to the high cost
of labeling. Recently an approach called distant
supervision (DS) (Mintz et al, 2009) was pro-
posed, which does not require any labels on the
text. It treats the extraction problem as classifying
? The contact author.
a candidate entity pair to a relation. Then an
existing fact in a KB can be used as a labeled
example whose label is the relation name. Then
the features of all the sentences (from a given text
corpus) containing the entity pair are merged as
the feature of the example. Finally a multi-class
classifier is trained.
However, the accuracy of DS is not satisfying.
Some variants have been proposed to improve
the performance (Riedel et al, 2010; Hoffmann
et al, 2011; Takamatsu et al, 2012). They ar-
gue that DS introduces a lot of noise into the
training data by merging the features of all the
sentences containing the same entity pair, because
a sentence containing the entity pair of a relation
may not talk about the relation. Riedel et al
(2010) and Hoffmann et al (2011) introduce
hidden variables to indicate whether a sentence
is noise and try to infer them from the data.
Takamatsu et al (2012) design a generative model
to identify noise patterns. However, as shown in
the experiments (Section 4), the above variants do
not lead to much improvement in accuracy.
In this paper, we point out and analyze some
critical factors in DS which have great impact on
the accuracy but has not been touched or well han-
dled before. First, each relation has its own schema
definition, i.e., the source entity and the destina-
tion entity should be of valid types, which is over-
looked in DS. Therefore, we propose a component
of entity type detection to check it. Second, DS
introduces many false negative examples into the
training set and we propose a new method to con-
struct negative training examples. Third, we find it
is difficult for a single classifier to achieve high ac-
curacy and hence we train multiple classifiers and
ensemble them.
We also notice that Nguyen and Moschitti
(2011a) and Nguyen and Moschitti (2011b) utilize
external information such as more facts from Yago
and labeled sentences from ACE to improve the
810
performance. These methods can also be equipped
with the approach proposed in this paper.
2 Critical Factors Affecting the Accuracy
DS has four steps: (1) Detect candidate entity
pairs in the corpus. (2) Label the candidate pairs
using the KB. (3) Extract features for the pair
from sentences containing the pair. (4) Train a
multi-class classifier. Among these steps, we find
the following three critical factors have great
impact on the accuracy (see Section 4 for the
experimental results).
Valid entity type detection. In DS, a sentence
with a candidate entity pair a sentence with two
candidate entities is noisy. First, the schema of
each relation in the KB requires that the source
and destination entities should be of valid types,
e.g., the source and destination entity of the
relation ?DirectorOfFilm? should be of the types
?Director? and ?Film? respectively. If the two
entities in a sentence are not of the valid types, the
sentence is noisy. Second, the sentence may not
talk about the relation even when the two entities
are of the valid types. The previous works (Riedel
et al, 2010; Hoffmann et al, 2011; Takamatsu et
al., 2012) do not distinguish the two types of noise
but directly infer the overall noise from the data.
We argue that the first type of noise is very difficult
to be inferred just from the noisy relational labels.
Instead, we decouple the two types of noise, and
utilize external labeled data, i.e., the Wikipedia
anchor links, to train an entity type detection mod-
ule to handle the first type of noise. We notice that
when Ling and Weld (2012) studied a fine-grained
NER method, they applied the method to relation
extraction by adding the recognized entity tags to
the features. We worry that the contribution of the
entity type features may be drowned when many
other features are used. Their method works well
on relatively small relations, but not that well on
big ones (Section 4.2).
Negative examples construction. DS treats the
relation extraction as a multi-class classification
task. For a relation, it implies that the facts of all
the other relations together with the ?Other? class
are negative examples. This introduces many false
negative examples into the training data. First,
many relations are not exclusive with each other,
e.g., ?PlaceOfBorn? and ?PlaceOfDeath?, the
born place of a person can be also the death place.
Second, in DS, the ?Other? class is composed
of all the candidate entity pairs not existed in
the KB, which actually contains many positive
facts of non-Other relations because the KB is
not complete. Therefore we use a different way to
construct negative training examples.
Feature space partition and ensemble. The
features used in DS are very sparse and many
examples do not contain any features. Thus we
employ more features. However we find it is
difficult for a single classifier on all the features
to achieve high accuracy and hence we divide
the features into different categories and train
a separate classifier for each category and then
ensemble them finally.
3 Accurate Distant Supervision (ADS)
Different from DS, we treat the extraction
problem as N binary classification problems,
one for each relation. We modify the four steps
of DS (Section 2). In step (1), when detecting
candidate entity pairs in sentences, we use our
entity type detection module (Section 3.1) to filter
out the sentences where the entity pair is of invalid
entity types. In step (2), we use our new method
to construct negative examples (Section 3.2). In
step (3), we employ more features and design an
ensemble classifier (Section 3.3). In step (4), we
train N binary classifiers separately.
3.1 Entity Type Detection
We divide the entity type detection into two steps.
The first step, called boundary detection, is to
detect phrases as candidate entities. The second
step, called named entity disambiguation, maps
a detected candidate entity to some entity types,
e.g., ?FilmDirector?. Note that an entity might be
mapped to multiple types. For instance, ?Ventura
Pons? is a ?FilmDirector? and a ?Person?.
Boundary Detection Two ways are used for
boundary detection. First, for each relation, from
the training set of facts, we get two dictionaries
(one for source entities and one for destination en-
tities). The two dictionaries are used to detect the
source and destination entities. Second, an exist-
ing NER tool (StanfordNER here) is used with the
following postprocessing to filter some unwanted
entities, because a NER tool sometimes produces
too many entities. We first find the compatible N-
ER tags for an entity type in the KB. For example,
811
for the type ?FilmDirector?, the compatible NER
tag of Standford NER is ?Person?. To do this,
for each entity type in the KB, we match all the
entities of that type (in the training set) back to the
training corpus and get the probability Ptag(ti) of
each NER tag (including the ?NULL? tag meaning
not recognized as a named entity) recognized
by the NER tool. Then we retain the top k tags
Stags = {t1, ? ? ? , tk} with the highest probabil-
ities to account for an accumulated mass z:
k = argmin
k
(( k?
i=1
Ptag(ti)
)
? z
)
(1)
In the experiments we set z = 0.9. The compati-
ble ner tags are Stags\{?NULL?}. If the retained
tags contain only ?NULL?, the candidate entities
recognized by NER tool will be discarded.
Named Entity Disambiguation (NED) With
a candidate entity obtained by the boundary
detection, we need a NED component to assign
some entity types to it. To obtain such a NED, we
leverage the anchor text in Wikipedia to generate
training data and train a NED component. The
referred Freebase entity and the types of an anchor
link in Wikipedia can be obtained from Freebase.
The following features are used to train the
NED component. Mention Features: Uni-grams,
Bi-grams, POS tags, word shapes in the mention,
and the length of the mention. Context Features:
Uni-grams and Bi-grams in the windows of the
mention (window size = 5).
3.2 Negative Examples Construction
Treating the problem as a multi-class classification
implies introducing many false negative examples
for a relation; therefore, we handle each relation
with a separate binary classifier. However, a KB
only tells us which entity pairs belong to a relation,
i.e., it only provides positive examples for each re-
lation. But we also need negative examples to train
a binary classifier. To reduce the number of false
negative examples, we propose a new method
to construct negative examples by utilizing the
1-to-1/1-to-n/n-to-1/n-to-n property of a relation.
1-to-1/n-to-1/1-to-n Relation A 1-to-1 or n-to-
1 relation is a functional relation: for a relation r,
for each valid source entity e1, there is only one
unique destination entity e2 such that (e1, e2) ? r.
However, in a real KB like Freebase, very few
relations meet the exact criterion. Thus we use the
following approximate criterion instead: relation
r is approximately a 1-to-1/n-to-1 relation if the
Inequalities (2,3) hold, where M is the number of
unique source entities in relation r, and ?(?) is an
indicator function which returns 1 if the condition
is met and returns 0 otherwise. Inequality (2)
says the proportion of source entities which have
exactly one counterpart destination entity should
be greater than a given threshold. Inequality (3)
says the average number of destination entities of
a source entity should be less than the threshold.
To check whether r is a 1-to-n relation, we simply
swap the source and destination entities of the
relation and check whether the reversed relation
is a n-to-1 relation by the above two inequalities.
In experiments we set ? = 0.7 and ? = 1.1.
1
M
M?
i=1
?
(??{e?|(ei, e?) ? r}
?? = 1
)
? ? (2)
1
M
M?
i=1
??{e?|(ei, e?) ? r}
?? ? ? (3)
n-to-n Relation Relations other than 1-to-1/n-
to-1/1-to-n are n-to-n relations. We approximately
categorize a n-to-n relation to n-to-1 or 1-to-n by
checking which one it is closer to. This is done
by computing the following two values ?src and
?dst. r is treated as a 1-to-n relation if ?src > ?dst
and as a 1-to-n relation otherwise.
?src =
1
Msrc
Msrc?
i=1
??{e?|(ei, e?) ? r}
??
?dst =
1
Mdst
Mdst?
i=1
??{e?|(e?, ei) ? r}
??
(4)
Negative examples For a candidate entity pair
(e1, e2) not in the relation r of the KB, we first
determine whether it is 1-to-n or n-to-1 using the
above method. If r is 1-to-1/n-to-1 and e1 exists in
some fact of r as the source entity, then (e1, e2) is
a negative example as it violates the 1-to-1/n-to-1
constraint. If r is 1-to-n, the judgement is similar
and just simply swap the source and destination
entities of the relation.
3.3 Feature Space Partition and Ensemble
The features of DS (Mintz et al, 2009) are very
sparse in the corpus. We add some features in (Yao
et al, 2011): Trigger Words (the words on the
dependency path except stop words) and Entity
String (source entity and destination entity).
812
Relation Taka Ensemble
works written 0.76 0.98
river/basin countries 0.48 1
/film/director/film 0.82 1
Average 0.79 0.89
Table 1: Manual evaluation of top-ranked 50 rela-
tion instances for the most frequent 15 relations.
We find that without considering the reversed
order of entity pairs in a sentence, the precision
can be higher, but the recall decreases. For exam-
ple, for the entity pair ?Ventura Pons, Actrius?, we
only consider sentences with the right order (e.g.
Ventura Pons is directed by Actrius.). For each re-
lation, we train four classifiers: C1 (without con-
sidering reversed order), C2 (considering reversed
order), C1more (without considering reversed or-
der and employ more feature) and C2more (con-
sidering reversed order and employ more feature).
We then ensemble the four classifiers by averaging
the probabilities of predictions:
P (y|x) = P1 + P2 + P1more + P2more4 (5)
4 Experiments
4.1 Dataset and Configurations
We aimed to extract facts of the 92 most frequent
relations in Freebase 2009. The facts of each
relation were equally split to two parts for training
and testing. Wikipedia 2009 was used as the target
corpus, where 800,000 articles were used for
training and 400,000 for testing. During the NED
phrase, there are 94 unique entity types (they are
also relations in Freebase) for the source and desti-
nation entities. Note that some entity types contain
too few entities and they are discarded. We used
500,000 Wikipedia articles (2,000,000 sentences)
for generating training data for the NED compo-
nent. We used Open NLP POS tagger, Standford
NER (Finkel et al, 2005) and MaltParser (Nivre
et al, 2006) to label/tag sentences. We employed
liblinear (Fan et al, 2008) as classifiers for NED
and relation extraction and the solver is L2LR.
4.2 Performance of Relation Extraction
Held-out Evaluation. We evaluate the perfor-
mance on the half hold-on facts for testing. We
compared performance of the n = 50, 000 best ex-
tracted relation instances of each method and the
Precision-Recall (PR) curves are in Figure 1 and
0 0.1 0.2 0.3 0.4 0.5 0.6 0.70
0.2
0.4
0.6
0.8
1
Recall
Pre
cis
ion
 
 
OrigDS
MultiR
Taka
ADS
Figure 1: Performance of different methods.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0.25
0.4
0.6
0.8
1
Recall
Pre
cisi
on
 
 OrigDSDS_FigerETDETD+NegMoreEnsemble(ADS)
Figure 2: Contributions of different components.
Figure 2. For a candidate fact without any enti-
ty existing in Freebase, we are not able to judge
whether it is correct. Thus we only evaluate the
candidate facts that at least one entity occurs as
the source or destination entity in the test fact set.
In Figure 1, we compared our method with
two previous methods: MultiR (Hoffmann et al,
2011) and Takamatsu et al (2012) (Taka). For
MultiR, we used the author?s implementation1.
We re-implemented Takamatsu?s algorithm. As
Takamatsu?s dataset (903,000 Wikipedia articles
for training and 400,000 for testing) is very similar
to ours, we used their best reported parameters.
Our method leads to much better performance.
Manual Evaluation. Following (Takamatsu et
al., 2012), we selected the top 50 ranked (accord-
ing to their classification probabilities) relation
facts of the 15 largest relations. We compared our
results with those of Takamatsu et al (2012) and
we achieved greater average precision (Table 1).
1available at http://www.cs.washington.edu/ai/raphaelh/mr
We set T = 120, which leads to the best performance.
813
Pmicro Rmicro Pmacro Rmacro
0.950 0.845 0.947 0.626
Table 2: Performance of the NED component
4.3 Contribution of Each Component
In Figure 2, with the entity type detection (ETD),
the performance is better than the original DS
method (OrigDS). As for the performance of NED
in the Entity Type Detection, the Micro/Macro
Precision-Recall of our NED component are in
Table 2. ETD is also better than adding the entity
types of the pair to the feature vector (DS Figer)2
as in (Ling and Weld, 2012). If we also employ the
negative example construction strategy in Section
3.2 (ETD+Neg), the precision of the top ranked
instances is improved. By adding more features
(More) and employing the ensemble learning
(Ensemble(ADS)) to ETD+Neg, the performance
is further improved.
5 Conclusion
This paper dealt with the problem of improving the
accuracy of DS. We find some factors are crucial-
ly important, including valid entity type detection,
negative training examples construction and en-
sembles. We have proposed an approach to handle
these issues. Experiments show that the approach
is very effective.
References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence, IJCAI?07, pages 2670?
2676, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of Human Language Technolo-
gy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 724?
731, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Bur-
r Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010), volume 2, pages 3?3.
2We use Figer (Ling and Weld, 2012) to detect entity types
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05). Association for Computational Linguis-
tics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 541?550, Portland, Oregon, USA,
June. Association for Computational Linguistics.
X. Ling and D.S. Weld. 2012. Fine-grained entity
recognition. In Proceedings of the 26th Conference
on Artificial Intelligence (AAAI).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Truc-Vien T. Nguyen and Alessandro Moschitti.
2011a. End-to-end relation extraction using distant
supervision from external semantic repositories. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 277?282, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Truc-Vien T Nguyen and AlessandroMoschitti. 2011b.
Joint distant and direct supervision for relation ex-
traction. In Proceeding of the International Joint
Conference on Natural Language Processing, pages
732?740.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In In Proc. of LREC-2006, pages
2216?2219.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the Sixteenth Eu-
ropean Conference on Machine Learning (ECML-
2010), pages 148?163.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervision
for relation extraction. In Proceedings of the 50th
814
Annual Meeting of the Association for Computation-
al Linguistics (Volume 1: Long Papers), pages 721?
729, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation dis-
covery using generative models. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1456?1466, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistic-
s (ACL?05), pages 419?426, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
815
