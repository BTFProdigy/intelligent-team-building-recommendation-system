Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 65?72, Vancouver, October 2005. c?2005 Association for Computational Linguistics
NeurAlign: Combining Word Alignments Using Neural Networks
Necip Fazil Ayan, Bonnie J. Dorr and Christof Monz
Department of Computer Science
University of Maryland
College Park, MD 20742
{nfa,bonnie,christof}@umiacs.umd.edu
Abstract
This paper presents a novel approach to
combining different word alignments. We
view word alignment as a pattern classifi-
cation problem, where alignment combi-
nation is treated as a classifier ensemble,
and alignment links are adorned with lin-
guistic features. A neural network model
is used to learn word alignments from the
individual alignment systems. We show
that our alignment combination approach
yields a significant 20-34% relative er-
ror reduction over the best-known align-
ment combination technique on English-
Spanish and English-Chinese data.
1 Introduction
Parallel texts are a valuable resource in natural lan-
guage processing and essential for projecting knowl-
edge from one language onto another. Word-level
alignment is a critical component of a wide range of
NLP applications, such as construction of bilingual
lexicons (Melamed, 2000), word sense disambigua-
tion (Diab and Resnik, 2002), projection of language
resources (Yarowsky et al, 2001), and statistical ma-
chine translation. Although word-level aligners tend
to perform well when there is sufficient training data,
the quality decreases as the size of training data de-
creases. Even with large amounts of training data,
statistical aligners have been shown to be suscepti-
ble to mis-aligning phrasal constructions (Dorr et al,
2002) due to many-to-many correspondences, mor-
phological language distinctions, paraphrased and
free translations, and a high percentage of function
words (about 50% of the tokens in most texts).
This paper presents a novel approach to align-
ment combination, NeurAlign, that treats each align-
ment system as a black box and merges their outputs.
We view word alignment as a pattern classification
problem and treat alignment combination as a classi-
fier ensemble (Hansen and Salamon, 1990; Wolpert,
1992). The ensemble-based approach was devel-
oped to select the best features of different learning
algorithms, including those that may not produce a
globally optimal solution (Minsky, 1991).
We use neural networks to implement the
classifier-ensemble approach, as these have previ-
ously been shown to be effective for combining clas-
sifiers (Hansen and Salamon, 1990). Neural nets
with 2 or more layers and non-linear activation func-
tions are capable of learning any function of the
feature space with arbitrarily small error. Neural
nets have been shown to be effective with (1) high-
dimensional input vectors, (2) relatively sparse data,
and (3) noisy data with high within-class variability,
all of which apply to the word alignment problem.
The rest of the paper is organized as follows: In
Section 2, we describe previous work on improv-
ing word alignments and use of classifier ensembles
in NLP. Section 3 gives a brief overview of neu-
ral networks. In Section 4, we present a new ap-
proach, NeurAlign, that learns how to combine indi-
vidual word alignment systems. Section 5 describes
our experimental design and the results on English-
Spanish and English-Chinese. We demonstrate that
NeurAlign yields significant improvements over the
best-known alignment combination technique.
65
j
i
Hidden layer
Output layer
Input layer
wij
ai
Figure 1: Multilayer Perceptron Overview
2 Related Work
Previous algorithms for improving word alignments
have attempted to incorporate additional knowledge
into their modeling. For example, Liu (2005) uses
a log-linear combination of linguistic features. Ad-
ditional linguistic knowledge can be in the form of
part-of-speech tags. (Toutanova et al, 2002) or de-
pendency relations (Cherry and Lin, 2003). Other
approaches to improving alignment have combined
alignment models, e.g., using a log-linear combina-
tion (Och and Ney, 2003) or mutually independent
association clues (Tiedemann, 2003).
A simpler approach was developed by Ayan et
al. (2004), where word alignment outputs are com-
bined using a linear combination of feature weights
assigned to the individual aligners. Our method is
more general in that it uses a neural network model
that is capable of learning nonlinear functions.
Classifier ensembles are used in several NLP ap-
plications. Some NLP applications for classifier en-
sembles are POS tagging (Brill and Wu, 1998; Ab-
ney et al, 1999), PP attachment (Abney et al, 1999),
word sense disambiguation (Florian and Yarowsky,
2002), and parsing (Henderson and Brill, 2000).
The work reported in this paper is the first appli-
cation of classifier ensembles to the word-alignment
problem. We use a different methodology to com-
bine classifiers that is based on stacked general-
ization (Wolpert, 1992), i.e., learning an additional
model on the outputs of individual classifiers.
3 Neural Networks
A multi-layer perceptron (MLP) is a feed-forward
neural network that consists of several units (neu-
rons) that are connected to each other by weighted
links. As illustrated in Figure 1, an MLP consists
of one input layer, one or more hidden layers, and
one output layer. The external input is presented to
the input layer, propagated forward through the hid-
den layers and creates the output vector in the output
layer. Each unit i in the network computes its output
with respect to its net input neti =
?
j wijaj , where
j represents all units in the previous layer that are
connected to the unit i. The output of unit i is com-
puted by passing the net input through a non-linear
activation function f , i.e. ai = f(neti).
The most commonly used non-linear activation
functions are the log sigmoid function f(x) =
1
1+e?x or hyperbolic tangent sigmoid function
f(x) = 1?e
?2x
1+e?2x . The latter has been shown to be
more suitable for binary classification problems.
The critical question is the computation of
weights associated with the links connecting the
neurons. In this paper, we use the resilient back-
propagation (RPROP) algorithm (Riedmiller and
Braun, 1993), which is based on the gradient descent
method, but converges faster and generalizes better.
4 NeurAlign Approach
We propose a new approach, NeurAlign, that learns
how to combine individual word alignment sys-
tems. We treat each alignment system as a classi-
fier and transform the combination problem into a
classifier ensemble problem. Before describing the
NeurAlign approach, we first introduce some termi-
nology used in the description below.
Let E = e1, . . . , et and F = f1, . . . , fs be two
sentences in two different languages. An alignment
link (i, j) corresponds to a translational equivalence
between words ei and fj . Let Ak be an align-
ment between sentences E and F , where each el-
ement a ? Ak is an alignment link (i, j). Let
A = {A1, . . . , Al} be a set of alignments between
E and F . We refer to the true alignment as T , where
each a ? T is of the form (i, j). A neighborhood
of an alignment link (i, j)?denoted by N(i, j)?
consists of 8 possible alignment links in a 3?3 win-
dow with (i, j) in the center of the window. Each
element of N(i, j) is called a neighboring link of
(i, j).
Our goal is to combine the information in
A1, . . . , Al such that the resulting alignment is
closer to T . A straightforward solution is to take the
intersection or union of the individual alignments, or
66
perform a majority voting for each possible align-
ment link (i, j). Here, we use an additional model
to learn how to combine outputs of A1, . . . , Al.
We decompose the task of combining word align-
ments into two steps: (1) Extract features; and (2)
Learn a classifier from the transformed data. We de-
scribe each of these two steps in turn.
4.1 Extracting Features
Given sentences E and F , we create a (potential)
alignment instance (i, j) for all possible word com-
binations. A crucial component of building a classi-
fier is the selection of features to represent the data.
The simplest approach is to treat each alignment-
system output as a separate feature upon which we
build a classifier. However, when only a few align-
ment systems are combined, this feature space is not
sufficient to distinguish between instances. One of
the strategies in the classification literature is to sup-
ply the input data to the set of features as well.
While combining word alignments, we use two
types of features to describe each instance (i, j):
(1) linguistic features and (2) alignment features.
Linguistic features include POS tags of both words
(ei and fj) and a dependency relation for one of
the words (ei). We generate POS tags using the
MXPOST tagger (Ratnaparkhi, 1996) for English
and Chinese, and Connexor for Spanish. Depen-
dency relations are produced using a version of the
Collins parser (Collins, 1997) that has been adapted
for building dependencies.
Alignment features consist of features that are ex-
tracted from the outputs of individual alignment sys-
tems. For each alignmentAk ? A, the following are
some of the alignment features that can be used to
describe an instance (i, j):
1. Whether (i, j) is an element of Ak or not
2. Translation probability p(fj |ei) computed
over Ak1
3. Fertility of (i.e., number of words in F that are
aligned to) ei in Ak
4. Fertility of (i.e., number of words in E that are
aligned to) fj in Ak
5. For each neighbor (x, y) ? N(i, j), whether
(x, y) ? Ak or not (8 features in total)
6. For each neighbor (x, y) ? N(i, j), transla-
tion probability p(fy|ex) computed overAk (8
features in total)
It is also possible to use variants, or combinations,
of these features to reduce feature space.
Figure 2 shows an example of how we transform
the outputs of 2 alignment systems, A1 and A2, for
an alignment link (i, j) into data with some of the
features above. We use -1 and 1 to represent the
absence and existence of a link, respectively. The
neighboring links are presented in row-by-row order.
X
XX
X
X
X
A1
A2
ei-1
ei
ei+1
fj-1 fj fj+1
1 (for A1), 0 (for A2)fertility(fj)
2 (for A1), 1 (for A2)fertility(ei)
2 (for A1), 3 (for A2)total neighbors
1, -1, -1, 1, 1, -1, -1, 1neighbors (A1? A2)
1, -1, -1, -1, 1, -1, -1, 1neighbors (A2)
-1, -1, -1, 1, -1, -1, -1, 1neighbors (A1)
1 (for A1), -1 (for A2)outputs of aligners
Modifierrel(ei)
Noun, Preppos(ei) , pos(fj)
Features for the alignment link ( i , j )
ei-1
ei
ei+1
fj-1 fj fj+1
Figure 2: An Example of Transforming Alignments
into Classification Data
For each sentence pair E = e1, . . . , et and F =
f1, . . . , fs, we generate s ? t instances to represent
the sentence pair in the classification data.
Supervised learning requires the correct output,
which here is the true alignment T . If an alignment
link (i, j) is an element of T , then we set the correct
output to 1, and to ?1, otherwise.
4.2 Learning A Classifier
Once we transform the alignments into a set of in-
stances with several features, the remaining task is to
learn a classifier from this data. In the case of word
alignment combination, there are important issues to
consider for choosing an appropriate classifier. First,
there is a very limited amount of manually annotated
data. This may give rise to poor generalizations be-
cause it is very likely that unseen data include lots
of cases that are not observed in the training data.
Second, the distribution of the data according to
the classes is skewed. In a preliminary study on an
English-Spanish data set, we found out that only 4%
of the all word pairs are aligned to each other by hu-
mans, among a possible 158K word pairs. More-
over, only 60% of those aligned word pairs were
1The translation probabilities can be borrowed from the ex-
isting systems, if available. Otherwise, they can be generated
from the outputs of individual alignment systems using likeli-
hood estimates.
67
A1 AlAi
FeatureExtraction ClassificationData Neural NetLearning
Output
Truth
EnrichedCorpus
Figure 3: NeurAlign1?Alignment Combination
Using All Data At Once
also aligned by the individual alignment systems
that were tested.
Finally, given the distribution of the data, it is dif-
ficult to find the right features to distinguish between
instances. Thus, it is prudent to use as many features
as possible and let the learning algorithm filter out
the redundant features.
Below, we describe how neural nets are used at
different levels to build a good classifier.
4.2.1 NeurAlign1: Learning All At Once
Figure 3 illustrates how we combine align-
ments using all the training data at the same time
(NeurAlign1). First, the outputs of individual align-
ments systems and the original corpus (enriched
with additional linguistic features) are passed to the
feature extraction module. This module transforms
the alignment problem into a classification problem
by generating a training instance for every pair of
words between the sentences in the original corpus.
Each instance is represented by a set of features (de-
scribed in Section 4.1). The new training data is
passed to a neural net learner, which outputs whether
an alignment link exists for each training instance.
4.2.2 NeurAlign2: Multiple Neural Networks
The use of multiple neural networks (NeurAlign2)
enables the decomposition of a complex problem
into smaller problems. Local experts are learned
for each smaller problem and these are then merged.
Following Tumer and Ghosh (1996), we apply spa-
tial partitioning of training instances using proxim-
ity of patterns in the input space to reduce the com-
plexity of the tasks assigned to individual classifiers.
We conducted a preliminary analysis on 100 ran-
domly selected English-Spanish sentence pairs from
a mixed corpus (UN + Bible + FBIS) to observe the
SPANISH
Adj Adv Comp Det Noun Prep Verb
E Adj 18 - - 82 40 96 66
N Adv - 8 - - 50 67 75
G Comp - - 12 - 46 37 96
L Det - - - 10 60 100 -
I Noun 42 77 100 94 23 98 84
S Prep - - - 93 70 22 100
H Verb 42 - - 100 66 78 43
Table 1: Error Rates according to POS Tags for
GIZA++ (E-to-S) (in percentages)
ClassificationData
DataPartitioning
Output
Truth
Parta
Parti
Partz
NNa
NNz
NNi NNCombination
Figure 4: NeurAlign2?Alignment Combination
with Partitioning
distribution of errors according to POS tags in both
languages. We examined the cases in which the in-
dividual alignment and the manual annotation were
different?a total of 3,348 instances, where 1,320 of
those are misclassified by GIZA++ (E-to-S).2 We
use a standard measure of error, i.e., the percentage
of misclassified instances out of the total number of
instances. Table 1 shows error rates (by percentage)
according to POS tags for GIZA++ (E-to-S).3
Table 1 shows that the error rate is relatively low
in cases where both words have the same POS tag.
Except for verbs, the lowest error rate is obtained
when both words have the same POS tag (the er-
ror rates on the diagonal). On the other hand, the
error rates are high in several other cases, as much
as 100%, e.g., when the Spanish word is a deter-
miner or a preposition.4 This suggests that dividing
the training data according to POS tag, and training
neural networks on each subset separately might be
better than training on the entire data at once.
Figure 4 illustrates the combination approach
with neural nets after partitioning the data into dis-
2For this analysis, we ignored the cases where both systems
produced an output of -1 (i.e., the words are not aligned).
3Only POS pairs that occurred at least 10 times are shown.
4The same analysis was done for the other direction and re-
sulted in similar distribution of error rates.
68
joint subsets (NeurAlign2). Similar to NeurAlign1,
the outputs of individual alignment systems, as well
as the original corpus, are passed to the feature ex-
traction module. Then the training data is split into
disjoint subsets using a subset of the available fea-
tures for partitioning. We learn different neural nets
for each partition, and then merge the outputs of the
individual nets. The advantage of this is that it re-
sults in different generalizations for each partition
and that it uses different subsets of the feature space
for each net.
5 Experiments and Results
This section describes our experimental design, in-
cluding evaluation metrics, data, and settings.
5.1 Evaluation Metrics
Let A be the set of alignment links for a set of sen-
tences. We take S to be the set of sure alignment
links and P be the set of probable alignment links
(in the gold standard) for the same set of sentences.
Precision (Pr), recall (Rc) and alignment error rate
(AER) are defined as follows:
Pr =
|A ? P |
|A|
Rc =
|A ? S|
|S|
AER = 1?
|A ? S|+ |A ? P |
|A|+ |S|
A manually aligned corpus is used as our gold stan-
dard. For English-Spanish data, the manual annota-
tion is done by a bilingual English-Spanish speaker.
Every link in the English-Spanish gold standard is
considered a sure alignment link (i.e., P = S).
For English-Chinese, we used 2002 NIST MT
evaluation test set. Each sentence pair was aligned
by two native Chinese speakers, who are fluent in
English. Each alignment link appearing in both an-
notations was considered a sure link, and links ap-
pearing in only one set were judged as probable. The
annotators were not aware of the specifics of our ap-
proach.
5.2 Evaluation Data and Settings
We evaluated NeurAlign1 and NeurAlign2, using 5-
fold cross validation on two data sets:
1. A set of 199 English-Spanish sentence pairs
(nearly 5K words on each side) from a mixed
corpus (UN + Bible + FBIS).
2. A set of 491 English-Chinese sentence pairs
(nearly 13K words on each side) from 2002
NIST MT evaluation test set.
We computed precision, recall and error rate on the
entire set of sentence pairs for each data set.5
To evaluate NeurAlign, we used GIZA++ in both
directions (E-to-F and F -to-E, where F is either
Chinese (C) or Spanish (S)) as input and a refined
alignment approach (Och and Ney, 2000) that uses
a heuristic combination method called grow-diag-
final (Koehn et al, 2003) for comparison. (We
henceforth refer to the refined-alignment approach
as ?RA.?)
For the English-Spanish experiments, GIZA++
was trained on 48K sentence pairs from a mixed
corpus (UN + Bible + FBIS), with nearly 1.2M of
words on each side, using 10 iterations of Model 1,
5 iterations of HMM, and 5 iterations of Model 4.
For the English-Chinese experiments, we used 107K
sentence pairs from FBIS corpus (nearly 4.1M En-
glish and 3.3M Chinese words) to train GIZA++, us-
ing 5 iterations of Model 1, 5 iterations of HMM, 3
iterations of Model 3, and 3 iterations of Model 4.
5.3 Neural Network Settings
In our experiments, we used a multi-layer percep-
tron (MLP) consisting of 1 input layer, 1 hidden
layer, and 1 output layer. The hidden layer consists
of 10 units, and the output layer consists of 1 unit.
All units in the hidden layer are fully connected to
the units in the input layer, and the output unit is
fully connected to all the units in the hidden layer.
We used hyperbolic tangent sigmoid function as the
activation function for both layers.
One of the potential pitfalls is overfitting as the
number of iterations increases. To address this, we
used the early stopping with validation set method.
In our experiments, we held out (randomly selected)
1/4 of the training set as the validation set.
Neural nets are sensitive to the initial weights. To
overcome this, we performed 5 runs of learning for
each training set. The final output for each training
is obtained by a majority voting over 5 runs.
5The number of alignment links varies over each fold.
Therefore, we chose to evaluate all data at once instead of eval-
uating on each fold and then averaging.
69
5.4 Results
This section describes the experiments on English-
Spanish and English-Chinese data for testing the
effects of feature selection, training on the en-
tire data (NeurAlign1) or on the partitioned data
(NeurAlign2), using two input alignments: GIZA++
(E-to-F ) and GIZA++ (F -to-E). We used the fol-
lowing additional features, as well as the outputs of
individual aligners, for an instance (i, j) (set of fea-
tures 2?7 below are generated separately for each
input alignment Ak):
1. posEi, posFj , relEi: POS tags and depen-
dency relation for ei and fj .
2. neigh(i, j): 8 features indicating whether a
neighboring link exists in Ak.
3. f ertEi, f ertFj : 2 features indicating the fer-
tility of ei and fj in Ak.
4. NC(i, j): Total number of existing links in
N(i, j) in Ak.
5. TP (i, j): Translation probability p(fj |ei) in
Ak.
6. NghTP(i, j): 8 features indicating the trans-
lation probability p(fy|ex) for each (x, y) ?
N(i, j) in Ak.
7. AvTP (i, j): Average translation probability
of the neighbors of (i, j) in Ak.
We performed statistical significance tests using
two-tailed paired t-tests. Unless otherwise indi-
cated, the differences between NeurAlign and other
alignment systems, as well as the differences among
NeurAlign variations themselves, were statistically
significant within the 95% confidence interval.
5.4.1 Results for English-Spanish
Table 2 summarizes the precision, recall and
alignment error rate values for each of our two
alignment system inputs plus the three alternative
alignment-combination approaches. Note that the
best performing aligner among these is the RA
method, with an AER of 21.2%. (We include this
in subsequent tables for ease of comparison.)
Feature Selection for Training All Data At Once:
NeurAlign1 Table 3 presents the results of train-
ing neural nets using the entire data (NeurAlign1)
with different subsets of the feature space. When we
used POS tags and the dependency relation as fea-
tures, NeurAlign1 performs worse than RA. Using
Alignments Pr Rc AER
E-to-S 87.0 67.0 24.3
S-to-E 88.0 67.5 23.6
Intersection 98.2 59.6 25.9
Union 80.6 74.9 22.3
RA 83.8 74.4 21.2
Table 2: Results for GIZA++ Alignments and Their
Simple Combinations
the neighboring links as the feature set gave slightly
(not significantly) better results than RA. Using POS
tags, dependency relations, and neighboring links
also resulted in better performance than RA but the
difference was not statistically significant.
When we used fertilities along with the POS tags
and dependency relations, the AER was 20.0%?a
significant relative error reduction of 5.7% over RA.
Adding the neighboring links to the previous feature
set resulted in an AER of 17.6%?a significant rela-
tive error reduction of 17% over RA.
Interestingly, when we removed POS tags and de-
pendency relations from this feature set, there was
no significant change in the AER, which indicates
that the improvement is mainly due to the neighbor-
ing links. This supports our initial claim about the
clustering of alignment links, i.e., when there is an
alignment link, usually there is another link in its
neighborhood. Finally, we tested the effects of using
translation probabilities as part of the feature set, and
found out that using translation probabilities did no
better than the case where they were not used. We
believe this happens because the translation proba-
bility p(fj |ei) has a unique value for each pair of ei
and fj ; therefore it is not useful to distinguish be-
tween alignment links with the same words.
Feature Selection for Training on Partitioned
Data: NeurAlign2 In order to train on partitioned
data (NeurAlign2), we needed to establish appropri-
ate features for partitioning the training data. Ta-
ble 4 presents the evaluation results for NeurAlign1
(i.e., no partitioning) and NeurAlign2 with different
features for partitioning (English POS tag, Spanish
POS tag, and POS tags on both sides). For training
on each partition, the feature space included POS
tags (e.g., Spanish POS tag in the case where parti-
tioning is based on English POS tag only), depen-
dency relations, neighborhood features, and fertili-
ties. We observed that partitioning based on POS
tags on one side reduced the AER to 17.4% and
70
Features Pr Rc AER
posEi, posFj , relEi 90.6 67.7 22.5
neigh(i, j) 91.3 69.5 21.1
posEi, posFj , relEi, 91.7 70.2 20.5
neigh(i, j)
posEi, posFj , relEi, 91.4 71.1 20.0
f ertEi, f ertFj
posEi, posFj , relEi, 89.5 76.3 17.6
neigh(i, j), NC(i, j)
f ertEi, f ertFj
neigh(i, j), NC(i, j) 89.7 75.7 17.9
f ertEi, f ertFj
posEi, posFj , relEi, 90.0 75.7 17.9
f ertEi, f ertFj ,
neigh(i, j), NC(i, j),
TP (i, j), AvTP (i, j)
RA 83.8 74.4 21.2
Table 3: Combination with Neural Networks:
NeurAlign1 (All-Data-At-Once)
17.1%, respectively. Using POS tags on both sides
reduced the error rate to 16.9%?a significant rel-
ative error reduction of 5.6% over no partitioning.
All four methods yielded statistically significant er-
ror reductions over RA?we will examine the fourth
method in more detail below.
Alignment Pr Rc AER
NeurAlign1 89.7 75.7 17.9
NeurAlign2[posEi] 91.1 75.4 17.4
NeurAlign2[posFj ] 91.2 76.0 17.1
NeurAlign2[posEi, posFj ] 91.6 76.0 16.9
RA 83.8 74.4 21.2
Table 4: Effects of Feature Selection for Partitioning
Once we determined that partitioning by POS tags
on both sides brought about the biggest gain, we ran
NeurAlign2 using this partitioning, but with differ-
ent feature sets. Table 5 shows the results of this
experiment. Using dependency relations, word fer-
tilities and translation probabilities (both for the link
in question and the neighboring links) yielded a sig-
nificantly lower AER (18.6%)?a relative error re-
duction of 12.3% over RA. When the feature set
consisted of dependency relations, word fertilities,
and neighborhood links, the AER was reduced to
16.9%?a 20.3% relative error reduction over RA.
We also tested the effects of adding translation prob-
abilities to this feature set, but as in the case of
NeurAlign1, this did not improve the alignments.
In the best case, NeurAlign2 achieved substan-
tial and significant reductions in AER over the in-
put alignment systems: a 28.4% relative error re-
duction over S-to-E and a 30.5% relative error re-
Features Pr Rc AER
relEi, f ertEi, f ertFj , 91.9 73.0 18.6
TP (i, j), AvTP (i, j),
NghTP (i, j)
neigh(i, j) 90.3 74.0 18.7
relEi, f ertEi, f ertFj , 91.6 76.0 16.9
neigh(i, j), NC(i, j)
relEi, f ertEi, f ertFj , 91.4 76.1 16.9
neigh(i, j), NC(i, j),
TP (i, j), AvTP (i, j)
RA 83.8 74.4 21.2
Table 5: Combination with Neural Networks:
NeurAlign2 (Partitioned According to POS tags)
duction over E-to-S. Compared to RA, NeurAlign2
also achieved significantly better results over RA:
relative improvements of 9.3% in precision, 2.2% in
recall, and 20.3% in AER.
5.4.2 Results for English-Chinese
The results of the input alignments to NeurAlign,
i.e., GIZA++ alignments in two different directions,
NeurAlign1 (i.e., no partitioning) and variations of
NeurAlign2 with different features for partitioning
(English POS tag, Chinese POS tag, and POS tags
on both sides) are shown in Table 6. For compar-
sion, we also include the results for RA in the table.
For brevity, we include only the features resulting
in the best configurations from the English-Spanish
experiments, i.e., POS tags, dependency relations,
word fertilities, and neighborhood links (the features
in the third row of Table 5). The ground truth used
during the training phase consisted of all the align-
ment links with equal weight.
Alignments Pr Rc AER
E-to-C 70.4 68.3 30.7
C-to-E 66.0 69.8 32.2
NeurAlign1 85.0 71.4 22.2
NeurAlign2[posEi] 85.7 74.6 20.0
NeurAlign2[posFj ] 85.7 73.2 20.8
NeurAlign2[posEi, posFj ] 86.3 74.7 19.7
RA 61.9 82.6 29.7
Table 6: Results on English-Chinese Data
Without any partitioning, NeurAlign achieves an
alignment error rate of 22.2%?a significant relative
error reduction of 25.3% over RA. Partitioning the
data according to POS tags results in significantly
better results over no partitioning. When the data is
partitioned according to both POS tags, NeurAlign
reduces AER to 19.7%?a significant relative error
reduction of 33.7% over RA. Compared to the input
71
alignments, the best version of NeurAlign achieves
a relative error reduction of 35.8% and 38.8%, re-
spectively.
6 Conclusions
We presented NeurAlign, a novel approach to com-
bining the outputs of different word alignment sys-
tems. Our approach treats individual alignment sys-
tems as black boxes, and transforms the individual
alignments into a set of data with features that are
borrowed from their outputs and additional linguis-
tic features (such as POS tags and dependency re-
lations). We use neural nets to learn the true align-
ments from these transformed data.
We show that using POS tags to partition the
transformed data, and learning a different classifier
for each partition is more effective than using the en-
tire data at once. Our results indicate that NeurAlign
yields a significant 28-39% relative error reduction
over the best of the input alignment systems and
a significant 20-34% relative error reduction over
the best known alignment combination technique on
English-Spanish and English-Chinese data.
We should note that NeurAlign is not a stand-
alone word alignment system but a supervised learn-
ing approach to improve already existing alignment
systems. A drawback of our approach is that it re-
quires annotated data. However, our experiments
have shown that significant improvements can be
obtained using a small set of annotated data. We
will do additional experiments to observe the effects
of varying the size of the annotated data while learn-
ing neural nets. We are also planning to investigate
whether NeurAlign helps when the individual align-
ers are trained using more data.
We will extend our combination approach to com-
bine word alignment systems based on different
models, and investigate the effectiveness of our tech-
nique on other language pairs. We also intend to
evaluate the effectiveness of our improved alignment
approach in the context of machine translation and
cross-language projection of resources.
Acknowledgments This work has been supported in
part by ONR MURI Contract FCPO.810548265, Coopera-
tive Agreement DAAD190320020, and NSF ITR Grant IIS-
0326553.
References
Steven Abney, Robert E. Schapire, and Yoram Singer. 1999.
Boosting applied to tagging and PP attachment. In Proceed-
ings of EMNLP?1999, pages 38?45.
Necip F. Ayan, Bonnie J. Dorr, and Nizar Habash. 2004. Multi-
Align: Combining linguistic and statistical techniques to
improve alignments for adaptable MT. In Proceedings of
AMTA?2004, pages 17?26.
Eric Brill and Jun Wu. 1998. Classifier combination for im-
proved lexical disambiguation. In Proc. of ACL?1998.
Colin Cherry and Dekang Lin. 2003. A probability model to
improve word alignment. In Proceedings of ACL?2003.
Micheal Collins. 1997. Three generative lexicalized models for
statistical parsing. In Proceedings of ACL?1997.
Mona Diab and Philip Resnik. 2002. An unsupervised method
for word sense tagging using parallel corpora. In Proceed-
ings of ACL?2002.
Bonnie J. Dorr, Lisa Pearl, Rebecca Hwa, and Nizar Habash.
2002. DUSTer: A method for unraveling cross-language di-
vergences for statistical word?level alignment. In Proceed-
ings of AMTA?2002.
Radu Florian and David Yarowsky. 2002. Modeling consensus:
Classifier combination for word sense disambiguation. In
Proceedings of EMNLP?2002, pages 25?32.
L. Hansen and P. Salamon. 1990. Neural network ensembles.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 12:993?1001.
John C. Henderson and Eric Brill. 2000. Bagging and boosting
a treebank parser. In Proceedings of NAACL?2000.
Philip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL/HLT?2003.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear models
for word alignment. In Proceedings of ACL?2005.
I. Dan Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221?249.
Marvin Minsky. 1999. Logical Versus Analogical or Symbolic
Versus Connectionist or Neat Versus Scruffy. AI Magazine,
12:34?51.
Franz J. Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings of ACL?2000.
Franz J. Och and Hermann Ney. 2003. A systematic compari-
son of various statistical alignment models. Computational
Linguistics, 29(1):9?51, March.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of EMNLP?1996.
Martin Riedmiller and Heinrich Braun. 1993. A direct adaptive
method for faster backpropagation learning: The RPROP al-
gorithm. In Proceedings of the IEEE Intl. Conf. on Neural
Networks, pages 586?591.
Jorg Tiedemann. 2003. Combining clues for word alignment.
In Proceedings of EACL?2003, pages 339?346.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Man-
ning. 2002. Extensions to HMM-based statistical word
alignment models. In Proceedings of EMNLP?2002.
Kagan Tumer and Joydeep Ghosh. 1996. Error correlation and
error reduction in ensemble classifiers. Connection Science,
Special Issue on Combining Artificial Neural Networks: En-
semble Approaches, 8(3?4):385?404, December.
David H. Wolpert. 1992. Stacked generalization. Neural Net-
works, 5(2):241?259.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001.
Inducing multilingual text analysis tools via robust projec-
tion across aligned corpora. In Proceedings of HLT?2001.
72
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 185?192, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Alignment Link Projection Using Transformation-Based Learning
Necip Fazil Ayan, Bonnie J. Dorr and Christof Monz
Department of Computer Science
University of Maryland
College Park, MD 20742
{nfa,bonnie,christof}@umiacs.umd.edu
Abstract
We present a new word-alignment ap-
proach that learns errors made by ex-
isting word alignment systems and cor-
rects them. By adapting transformation-
based learning to the problem of word
alignment, we project new alignment links
from already existing links, using features
such as POS tags. We show that our align-
ment link projection approach yields a sig-
nificantly lower alignment error rate than
that of the best performing alignment sys-
tem (22.6% relative reduction on English-
Spanish data and 23.2% relative reduction
on English-Chinese data).
1 Introduction
Word-level alignment is a critical component of a
wide range of NLP applications, such as construc-
tion of bilingual lexicons (Melamed, 2000), word
sense disambiguation (Diab and Resnik, 2002), pro-
jection of language resources (Yarowsky et al,
2001), and statistical machine translation. Although
word-level aligners tend to perform well when there
is enough training data, the quality of word align-
ment decreases as the size of training data de-
creases. Moreover, word-alignment systems are of-
ten tripped up by many-to-many correspondences,
morphological language distinctions, paraphrased
and free translations, and a high percentage of func-
tion words (about 50% of the tokens in most texts).
At the heart of the matter is a set of assumptions
that word-alignment algorithms must make in order
to reduce the hypothesis space, since word align-
ment is an exponential problem. Because of these
assumptions, learning algorithms tend to make sim-
ilar errors throughout the entire data.
This paper presents a new approach?Alignment
Link Projection (ALP)?that learns common align-
ment errors made by an alignment system and at-
tempts to correct them. Our approach assumes the
initial alignment system adequately captures certain
kinds of word correspondences but fails to handle
others. ALP starts with an initial alignment and then
fills out (i.e., projects) new word-level alignment re-
lations (i.e., links) from existing alignment relations.
ALP then deletes certain alignment links associated
with common errors, thus improving precision and
recall.
In our approach, we adapt transformation-based
learning (TBL) (Brill, 1995; Brill, 1996) to the prob-
lem of word alignment. ALP attempts to find an
ordered list of transformation rules (within a pre-
specified search space) to improve a baseline anno-
tation. The rules decompose the search space into
a set of consecutive words (windows) within which
alignment links are added, to or deleted from, the
initial alignment. This window-based approach ex-
ploits the clustering tendency of alignment links,
i.e., when there is a link between two words, there
is frequently another link in close proximity.
TBL is an appropriate choice for this problem for
the following reasons:
1. It can be optimized directly with respect to an
evaluation metric.
2. It learns rules that improve the initial predic-
tion iteratively, so that it is capable of correct-
ing previous errors in subsequent iterations.
3. It provides a readable description (or classifi-
cation) of errors made by the initial system,
thereby enabling alignment refinements.
185
The rest of the paper is organized as follows: In
the next section we describe previous work on im-
proving word alignments. Section 3 presents a brief
overview of TBL. Section 4 describes the adapta-
tion of TBL to the word alignment problem. Sec-
tion 5 compares ALP to various alignments and
presents results on English-Spanish and English-
Chinese. We show that ALP yields a significant re-
ductions in alignment error rate over that of the best
performing alignment system.
2 Related Work
One of the major problems with the IBM models
(Brown et al, 1993) and the HMM models (Vogel et
al., 1996) is that they are restricted to the alignment
of each source-language word to at most one target-
language word. The standard method to overcome
this problem to use the model in both directions
(interchanging the source and target languages) and
applying heuristic-based combination techniques to
produce a refined alignment (Och and Ney, 2000;
Koehn et al, 2003)?henceforth referred to as ?RA.?
Several researchers have proposed algorithms for
improving word alignment systems by injecting ad-
ditional knowledge or combining different align-
ment models. These approaches include an en-
hanced HMM alignment model that uses part-of-
speech tags (Toutanova et al, 2002), a log-linear
combination of IBM translation models and HMM
models (Och and Ney, 2003), techniques that rely
on dependency relations (Cherry and Lin, 2003),
and a log-linear combination of IBM Model 3 align-
ment probabilities, POS tags, and bilingual dictio-
nary coverage (Liu et al, 2005). A common theme
for these methods is the use of additional features
for enriching the alignment process. These methods
perform better than the IBM models and their vari-
ants but still tend to make similar errors because of
the bias in their alignment modeling.
We adopt an approach that post-processes a given
alignment using linguistically-oriented rules. The
idea is similar to that of Ayan et al (2004), where
manually-crafted rules are used to correct align-
ment links related to language divergences. Our
approach differs, however, in that the rules are ex-
tracted automatically?not manually?by examin-
ing an initial alignment and categorizing the errors
according to features of the words.
Initial Annotation
Corpus
Templates
Rule Instantiation
Best Rule Selection
Rule Application
Rules
CorpusAnnotated
Ground Truth
Figure 1: TBL Architecture
3 Transformation-based Learning
As shown in Figure 1, the input to TBL is an unanno-
tated corpus that is first passed to an initial annotator
and then iteratively updated through comparison to a
manually-annotated reference set (or ground truth).
On each iteration, the output of the previous iteration
is compared against the ground truth, and an ordered
list of transformation rules is learned that make the
previous annotated data better resemble the ground
truth.
A set of rule templates determines the space of
allowable transformation rules. A rule template has
two components: a triggering environment (condi-
tion of the rule) and a rewrite rule (action taken). On
each iteration, these templates are instantiated with
features of the constituents of the templates when
the condition of the rule is satisfied.
This process eventually identifies all possible in-
stantiated forms of the templates. Among all these
possible rules, the transformation whose application
results in the best score?according to some objec-
tive function?is identified. This transformation is
added to the ordered list of transformation rules.
The learning stops when there is no transformation
that improves the current state of the data or a pre-
specified threshold is reached.
When presented with new data, the transforma-
tion rules are applied in the order that they were
added to the list of transformations. The output of
the system is the annotated data after all transforma-
tions are applied to the initial annotation.
4 Alignment Link Projection (ALP)
ALP is a TBL implementation that projects align-
ment links from an initial input alignment. We in-
duce several variations of ALP by setting four pa-
rameters in different ways:
186
ei
fj fj+1
NULL ei
fj fj+1
Figure 2: Graphical Representation of a Template
1. Initial alignment
2. Set of templates
3. Simple or generalized instantiation
4. Best rule selection
We describe each of these below using the following
definitions and notation:
? E = e1, . . . , ei, . . . , et is a sentence in lan-
guage L1 and F = f1, . . . , fj , . . . , fs is a sen-
tence in language L2.
? An alignment link (i, j) corresponds to a trans-
lational equivalence between ei and fj .
? A neighborhood of an alignment link (i, j)?
denoted by N(i, j)?consists of 8 possible
alignment links in a 3 ? 3 window with (i, j)
in the center of the window. Each element of
N(i, j) is called a neighboring link of (i, j).
? nullEA(i) is true if and only if ei is not
aligned to any word in F in a given alignment
A. Similarly, nullFA(j) is true if and only if
fj is not aligned to any word in E in a given
alignment A.
4.1 Initial Alignment
Any existing word-alignment system may be used
for the initial annotation step of the TBL algo-
rithm. For our experiments, we chose GIZA++ (Och
and Ney, 2000) and the RA approach (Koehn et
al., 2003)? the best known alignment combination
technique? as our initial aligners.1
4.2 TBL Templates
Our templates consider consecutive words (of size
1, 2 or 3) in both languages. The condition por-
tion of a TBL rule template tests for the existence
of an alignment link between two words. The ac-
tion portion involves the addition or deletion of an
alignment link. For example, the rule template in
Figure 2 is applicable only when a word (ei) in one
language is aligned to the second word (fj+1) of a
phrase (fj , fj+1) in the other language, and the first
1We treat these initial aligners as black boxes.
word (fj) of the phrase is unaligned in the initial
alignment. The action taken by this rule template is
to add a link between ei and fj .2
ALP employs 3 different sets of templates to
project new alignment links or delete existing links
in a given alignment:
1. Expansion of the initial alignment according
to another alignment
2. Deletion of spurious alignment links
3. Correction of multi-word (one-to-many or
many-to-one) correspondences
Each of these is described below.
4.2.1 Expansion Templates
Expansion templates are used to extend an initial
alignment given another alignment as the validation
set. This approach is similar to the one used in the
RA method in that it adds links based on knowl-
edge about neighboring links, but it differs in that it
also uses features of the words themselves to decide
which neighboring links to add.
Our expansion templates are presented in Table 1.
The first 8 templates add a new link to the initial
alignmentA if there is a neighboring link in the vali-
dation alignment V . The final two templates enforce
the presence of at least two neighboring links in the
validation set V before adding a new link.
Condition Action
(i, j) ? A, (i? 1, j ? 1) ? V add (i? 1, j ? 1)
(i, j) ? A, (i? 1, j) ? V add (i? 1, j)
(i, j) ? A, (i? 1, j + 1) ? V add (i? 1, j + 1)
(i, j) ? A, (i, j ? 1) ? V add (i, j ? 1)
(i, j) ? A, (i, j + 1) ? V add (i, j + 1)
(i, j) ? A, (i+ 1, j ? 1) ? V add (i+ 1, j ? 1)
(i, j) ? A, (i+ 1, j) ? V add (i+ 1, j)
(i, j) ? A, (i+ 1, j + 1) ? V add (i+ 1, j + 1)
(i? 1, j ? 1) ? A, (i+ 1, j + 1) ? A, add (i, j)
(i, j) ? V
(i+ 1, j ? 1) ? A, (i? 1, j + 1) ? A, add (i, j)
(i, j) ? V
Table 1: Templates for Expanding the Alignment A
According to a Validation Alignment V
4.2.2 Deletion Templates
Existing alignment algorithms (e.g., GIZA++) are
biased toward aligning some words, especially in-
frequent ones, in one language to many words in the
other language in order to minimize the number of
unaligned words, even if many incorrect alignment
2A thick line indicates an added link.
187
links are induced.3 Deletion templates are useful for
eliminating the resulting spurious links.
The basic idea is to remove alignment links
that do not have a neighboring link if the word
in question has already been aligned to another
word. Table 2 lists two simple templates to
clean up spurious links. We define the predicate
neighbor existsA(i, j) to denote whether there is
an alignment link in the neighborhood of the link
(i, j) in a given alignment A. For example, the first
template deletes spurious links for a particular word
ei in E.
Condition Action
(i, j) ? A, (i, k) ? A,
neighbor existsA(i, j), del (i, k)
not(neighbor existsA(i, k))
(i, j) ? A, (k, j) ? A,
neighbor existsA(i, j), del (e, j)
not(neighbor existsA(k, j))
Table 2: Templates for Deleting Spurious Links in a
Given Alignment A
4.2.3 Multi-Word Correction Templates
Current alignment algorithms produce one-to-one
word correspondences quite successfully. However,
accurate alignment of phrasal constructions (many-
to-many correspondences) is still problematic. On
the one hand, the ability to provide fully correct
phrasal alignments is impaired by the occurrence of
high-frequency function words and/or words that are
not exact translations of the words in the other lan-
guage. On the other hand, we have observed that
most alignment systems are capable of providing
partially correct phrasal alignments.4
Our templates for handling multi-word correspon-
dences are grounded in the outcome of this finding.
That is, we make the (frequently correct) assumption
that at least one alignment link in a many-to-many
correspondence is correctly identified in the initial
3This is a well-known characteristic of statistical alignment
systems?motivated by the need to ensure a target-word trans-
lation ei for each source word fj while modeling p(F |E) ?for
downstream MT.
4Specifically, we conducted a preliminary study using 40
manually-aligned English-Spanish sentences from a mixed cor-
pus (UN + Bible + FBIS) as our gold standard. We found that,
in most cases where the human annotator aligned one word to
two words, an existing alignment system identified at least one
of the two alignment links correctly.
Condition Action
nullFA(j), (i, j + 1) ? A add (i, j)
nullFA(j + 1), (i, j) ? A add (i, j + 1)
(i, j) ? A, (i, j + 1) ? A del (i, j)
(i, j) ? A, (i, j + 1) ? A del (i, j + 1)
nullFA(j), nullFA(j + 1) add (i, j),
add (i, j + 1)
nullEA(i), (i+ 1, j) ? A add (i, j)
nullEA(i+ 1), (i, j) ? A add (i+ 1, j)
(i, j) ? A, (i+ 1, j) ? A del (i, j)
(i, j) ? A, (i+ 1, j) ? A del (i+ 1, j)
nullEA(i), nullEA(i+ 1) add (i, j)
add (i+ 1, j)
(i+ 1, j + 1) ? A add (i, j)
nullEA(i), nullFA(j),
(i, j) ? A, nullEA(i+ 1), add (i+ 1, j + 1)
nullFA(j + 1)
(i, j) ? A, (i+ 1, j) ? A, add (i, j + 1)
(i+ 1, j + 1) ? A
(i, j) ? A, (i, j + 1) ? A, add (i+ 1, j)
(i+ 1, j + 1) ? A
(i? 1, j) ? A, (i+ 1, j) ? A add (i, j)
nullEA(i)
(i, j ? 1) ? A, (i, j + 1) ? A add (i, j)
nullFA(j)
Table 3: Templates for Handling Multi-Word Corre-
spondences in a Given Alignment A
Condition Action
(i, j) ? A del (i, j)
nullEA(i), nullFA(j) add (i, j)
Table 4: Templates for Correcting One-to-One Cor-
respondences in a Given Alignment A
alignment. Table 3 lists the templates for correct-
ing alignment links in multi-word correspondences.
The first five templates handle (ei ? fjfj+1) cor-
respondences, the next five handle (eiei+1 ? fj)
correspondences, the next four handle (eiei+1 ?
fjfj+1) correspondences, and the final two handle
(ei?1eiei+1 ? fj) and (ei ? fj?1fjfj+1) corre-
spondences.
The alignment rules given above may introduce
errors that require additional cleanup. Thus, we in-
troduce two simple templates (shown in Table 4) to
accommodate the deletion or addition of links be-
tween a single pair of words.
4.3 Instantiation of Templates
ALP starts with a set of templates and an initial
alignment and attempts to instantiate the templates
during the learning process. The templates can be
instantiated using two methods: Simple (a word is
instantiated with a specific feature) or Generalized (a
word is instantiated using a special keyword any-
188
thing).
ALP requires only a small amount of manually
aligned data for this process?a major strength of
the system. However, if we were to instantiate the
templates with the actual words of the manual align-
ment, the frequency counts (from such a small data
set) would not be high enough to derive reasonable
generalizations. Thus, ALP adds new links based on
linguistic features of words, rather than the words
themselves. Using these features is what sets ALP
apart from systems like the RA approach. Specifi-
cally, three features are used to instantiate the tem-
plates:
? POS tags on both sides: We assign POS
tags using the MXPOST tagger (Ratnaparkhi,
1996) for English and Chinese, and Connexor
for Spanish.
? Dependency relations: ALP utilizes depen-
dencies for a better generalization?if a depen-
dency parser is available in either language.
In our experiments, we used a dependency
parser only in English (a version of the Collins
parser (Collins, 1997) that has been adapted
for building dependencies) but not in the other
language.
? A set of closed-class words: We use 16 dif-
ferent classes, 9 of which are different seman-
tic verb classes while the other 7 are function
words, prepositions, and complementizers.5
If both POS tags and dependency relations are
available, they can be used together to instantiate
the templates. That is, a word can be instantiated
in a TBL template with: (1) a POS tag (e.g., Noun,
Adj); (2) a relation (e.g., Subj, Obj); (3) a parameter
class (e.g., Change of State); or (4) different subsets
of (1)?(3). We also employ a more generalized form
of instantiation, where words in the templates may
match the keyword anything.
4.4 Best Rule Selection
The rules are selected using two different metrics:
The accuracy of the rule or the overall impact of the
application of the rule on the entire data.
Two different mechanisms may be used for select-
ing the best rule after generating all possible instan-
tiations of templates:
5These are based on the parameter classes of (Dorr et al,
2002).
1. Rule Accuracy: The goal is to minimize the
errors introduced by the application of a trans-
formation rule. To measure accuracy of a rule
r, we use good(r)?2?bad(r), where good(r)
is the number of alignment links that are cor-
rected by the rule, and bad(r) is the number of
incorrect alignment links produced.
2. Overall impact on the training data: The ac-
curacy mechanism (above) is useful for bias-
ing the system toward higher precision. How-
ever, if the overall system is evaluated using a
metric other than precision (e.g., recall), the
accuracy mechanism may not guarantee that
the best rule is chosen at each step. Thus, we
choose the best rule according to the evalua-
tion metric to be used for the overall system.
5 Experiments and Results
This section describes our evaluation of ALP vari-
ants using different combinations of settings of the
four parameters described above. The two language
pairs examined are English-Spanish and English-
Chinese.
5.1 Evaluation Metrics
Let A be the set of alignment links for a set of sen-
tences. We take S to be the set of sure alignment
links and P be the set of probable alignment links
(in the gold standard) for the same set of sentences.
Precision (Pr), recall (Rc) and alignment error rate
(AER) are defined as follows:
Pr =
|A ? P |
|A|
Rc =
|A ? S|
|S|
AER = 1?
|A ? S|+ |A ? P |
|A|+ |S|
A manually aligned corpus is used as our gold stan-
dard. For English-Spanish data, the manual an-
notation was done by a bilingual English-Spanish
speaker. Every link in the English-Spanish gold
standard is considered a sure alignment link.
For English-Chinese, we used 2002 NIST MT
evaluation test set, and each sentence pair was
aligned by two native Chinese speakers who are flu-
ent in English. Each alignment link appearing in
both annotations was considered a sure link, and
189
links appearing in only one set were judged as prob-
able. The annotators were not aware of the specifics
of our approach.
5.2 Evaluation Data
We evaluated ALP using 5-fold cross validation on
two different data sets:
1. A set of 199 English-Spanish sentence pairs
(nearly 5K words on each side) from a mixed
corpus (UN + Bible + FBIS).
2. A set of 491 English-Chinese sentence pairs
(nearly 13K words on each side) from 2002
NIST MT evaluation test set.
We divided the pairs of sentences randomly into 5
groups. Then, for each fold, we used 4 groups as the
ground truth (for training), and used the other group
as our gold standard (for evaluation). This process
was repeated 5 times so that each sentence pair was
tested exactly once. We computed precision, recall
and error rate on the entire set for each data set.6
For an initial alignment, we used GIZA++ in both
directions (E-to-F and F -to-E, where F is either
Chinese (C) or Spanish (S)), and also two different
combined alignments: intersection of E-to-F and
F -to-E; and RA using a heuristic combination ap-
proach called grow-diag-final (Koehn et al, 2003).
For the English-Spanish experiments, GIZA++
was trained on 48K sentence pairs from a mixed
corpus (UN + Bible + FBIS), with nearly 1.2M of
words on each side, using 10 iterations of Model 1,
5 iterations of HMM and 5 iterations of Model 4.
For the English-Chinese experiments, we used 107K
sentence pairs from FBIS corpus (nearly 4.1M En-
glish and 3.3M Chinese words) to train GIZA++, us-
ing 5 iterations of Model 1, 5 iterations of HMM, 3
iterations of Model 3, and 3 iterations of Model 4.
5.3 Results for English-Spanish
For our initial alignments we used: (1) Intersec-
tion of GIZA++ English-to-Spanish and Spanish-
to-English; (2) GIZA++ English-to-Spanish; (3)
GIZA++ Spanish-to-English; and (4) RA. Of these,
RA is the best, with an error rate of 21.2%. For ease
of comparison, the RA score appears in all result ta-
bles below.
6The number of alignment links varies over each fold.
Therefore, we chose to evaluate all data at once instead of eval-
uating on each fold and then averaging.
Tables 5?7 compare ALP to each of these four
alignments using different settings of 4 parameters:
ALP[IA, T, I, BRS], where IA is the initial align-
ment, T is the set of templates, I is the instantia-
tion method, and BRS is the metric for the best rule
selection at each iteration. TE is the set of expan-
sion templates from Table 1, TD is the set of dele-
tion templates from Table 2, and TMW is the set of
multi-word templates from Table 3 (supplemented
with templates from Table 4).
As mentioned in Section 4.3, we use two instanti-
ation methods: (1) simple instantiation (sim), where
the words are instantiated using a specific POS tag,
relation, parameter class or combination of those;
and (2) generalized instantiation (gen), where the
words can be instantiated using the keyword any-
thing. Two different metrics are used to select the
best rule: The accuracy of the rule (acc) and the
AER on the entire training data after applying the
rule (aer).7
We performed statistical significance tests using
two-tailed paired t-tests. Unless otherwise indicated,
the differences between ALP and initial alignments
(for all ALP variations and all initial alignments)
were found to be statistically significant within the
95% confidence interval. Moreover, the differences
among ALP variations themselves were statistically
significant within 95% confidence interval.
Using Intersection as Initial Alignment We ran
ALP using the intersection of GIZA++ (E-to-S)
and GIZA++(S-to-E) alignments as the initial align-
ment in two different ways: (1) With TE using the
union of the unidirectional GIZA++ alignments as
the validation set, and (2) with TD and TMW applied
one after another. Table 5 presents the precision, re-
call and AER results.
Alignments Pr Rc AER
Intersection (Int) 98.2 59.6 25.9
ALP[Int, TE , gen, aer] 90.9 69.9 21.0
ALP[Int, (TD, TMW ), gen, aer] 88.8 72.3 20.3
RA 83.8 74.4 21.2
Table 5: ALP Results Using GIZA++ Intersection as
Initial Alignment for English-Spanish
Using the expansion templates (TE) against a val-
7We use only sure alignment links as the ground truth to
learn rules inside ALP. Therefore, AER here refers to the AER
of sure alignment links.
190
Alignments Pr Rc AER
E-to-S 87.0 67.0 24.3
ALP[E-to-S,(TD, TMW ), gen, aer] 85.6 76.4 19.3
S-to-E 88.0 67.5 23.6
ALP[S-to-E,(TD, TMW ), gen, aer] 87.1 76.7 18.4
RA 83.8 74.4 21.2
Table 6: ALP Results Using GIZA++ (Each Direc-
tion) as Initial Alignment for English-Spanish
idation set produced results comparable to the RA
method. The major difference is that ALP resulted
in a much higher precision but in a lower recall be-
cause ALP is more selective in adding a new link
during the expansion stage. This difference is due to
the additional constraints provided by word features.
The version of ALP that applies deletion (TD) and
multi-word (TMW ) templates sequentially achieves
lower recall but higher precision than RA. In the best
case, ALP achieves a statistically significant rela-
tive reduction of 21.6% in AER over the Intersection
alignment. When compared to RA, ALP achieves a
lower AER but the difference is not significant.
Using Unidirectional GIZA++ Alignments as Ini-
tial Alignment In a second set of experiments, we
applied ALP to the unidirectional GIZA++ align-
ments, using deletion (TD) and multi-word (TMW )
templates, generalized instantiation, and AER for
the best rule selection. Table 6 presents the preci-
sion, recall and AER results.
For both directions, ALP achieves a lower preci-
sion but much higher recall than that of the initial
unidirectional alignment. Overall, there was a rela-
tive reduction of 20.6?22.0% in AER. When com-
pared to RA, the version of ALP that uses unidirec-
tional GIZA++ alignments brings about significant
reductions in AER: 9.0% relative reduction in one
direction and 13.2% relative reduction in the other
direction.
Using RA as Initial Alignment In a third experi-
ment, we compared RA with variations of ALP us-
ing RA as the initial alignment. We used the tem-
plates in two different ways: (1) with a combination
of TD and TMW (i.e., TD ?TMW ), and (2) with two
consecutive runs of ALP, first with TD and then with
TMW using the output of the first run as the initial
annotation in the second run (i.e., TD, TMW ). Ta-
ble 7 presents precision, recall and AER results, us-
ing different methods for template instantiation and
Alignments Pr Rc AER
ALP[RA, (TD, TMW ), sim, acc] 87.8 77.7 17.6
ALP[RA, (TD, TMW ), sim, aer] 87.9 79.0 16.8
ALP[RA, (TD ? TMW ), gen, aer] 86.2 80.0 17.0
ALP[RA, (TD, TMW ), gen, aer] 86.9 80.5 16.4
RA 83.8 74.4 21.2
Table 7: ALP Results Using RA as Initial Alignment
for English-Spanish
best rule selection.
The results indicate that using AER is better than
using accuracy for choosing the best rule. Using
generalized instantiation instead of simple instantia-
tion results in a better AER. Running ALP with dele-
tion (TD) templates followed by multi-word (TMW )
templates results in a lower AER than running ALP
only once with combined templates.
The highest performing variant of ALP, shown
in the fourth line of the table, uses RA as the ini-
tial alignment, template sets TD, TMW , general-
ized instantiation, and AER for best rule selection.
This variant is significantly better than RA, with a
22.6% relative reduction in AER. When compared
to the unidirectional alignments (E-to-S and S-to-
E) given in Table 6, this variant of ALP yields nearly
the same precision (around 87.0%) but a 19.2% rel-
ative improvement in recall. The overall relative re-
duction in AER is 30.5% in the S-to-E direction and
32.5% in the E-to-S direction.
5.4 Results for English-Chinese
Our experiments for English-Chinese were designed
with a similar structure to that of English-Spanish,
i.e., the same four initial alignments. Once again,
RA performs the best out of these initial alignments,
with an error rate of 29.7%. The results of the ini-
tial alignments, and variations of ALP based on dif-
ferent initial alignments are shown in Table 8. For
brevity, we include only the ALP parameter settings
resulting in the best configurations from the English-
Spanish experiments. For learning rules from the
templates, we used only the sure alignment links as
the ground truth while learning rules inside ALP.
On the English-Chinese data, ALP yields signif-
icantly lower error rates with respect to the initial
alignments. When ALP is run with the intersection
of two GIZA++ alignments, the relative reduction
is 5.4% in AER. When ALP is run with E-to-C as
initial alignment, the relative reduction in AER is
13.4%. For the other direction, ALP produces a rel-
191
Alignments Pr Rc AER
Intersection (Int) 94.8 53.6 31.2
ALP[Int, (TD, TMW ), gen, aer] 91.7 56.8 29.5
E-to-C 70.4 68.3 30.7
ALP[E-to-C,(TD, TMW ), gen, aer] 79.1 68.1 26.6
C-to-E 66.0 69.8 32.2
ALP[C-to-E,(TD, TMW ), gen, aer] 83.3 66.0 26.2
RA 61.9 82.6 29.7
ALP[RA,(TD, TMW ), gen, aer] 82.1 72.7 22.8
Table 8: ALP Results Using Different Initial Align-
ments for English-Chinese
ative reduction of 18.6% in AER. Finally, when RA
is given to ALP as an initial alignment, ALP results
in a relative reduction of 23.2% in AER. When com-
pared to RA, all variations of ALP, except the one
starting with the intersection, yield statistically sig-
nificantly lower AER. Another important finding is
that ALP yields significantly higher precision than
the initial alignments but usually lower recall.
6 Conclusion
We have presented ALP, a new approach that re-
fines alignments by identifying the types of errors
made by existing alignment systems and correcting
them. Our approach adapts TBL to the problem of
word-level alignment by examining word features
as well as neighboring links. We use POS tags,
closed-class words in both languages, and depen-
dency relations in one language to classify the er-
rors made by the initial alignment system. We show
that ALP yields at least a 22.6% relative reduction
on English-Spanish data and 23.2% relative reduc-
tion on English-Chinese data in alignment error rate
over that of the best performing system.
We should note that ALP is not a stand-alone
word alignment system but a supervised learning ap-
proach to improve already existing alignment sys-
tems. ALP takes advantage of clustering of align-
ment links to project new links given a reasonable
initial alignment. We have shown that ALP is quite
successful in projecting alignment links for two dif-
ferent languages?Spanish and Chinese.
Statistical alignment systems are more successful
with increasing amount of training data. Whether
ALP improves the statistical alignment systems
when they are trained on more data is an interesting
research problem, which we plan to tackle in future.
Finally, we will evaluate the improved alignments
in the context of an end-to-end application, such as
machine translation.
Acknowledgments This work has been supported, in
part, by ONR MURI Contract FCPO.810548265, Coopera-
tive Agreement DAAD190320020, and NSF ITR Grant IIS-
0326553.
References
Necip F. Ayan, Bonnie J. Dorr, and Nizar Habash. 2004. Multi-
Align: Combining linguistic and statistical techniques to
improve alignments for adaptable MT. In Proceedings of
AMTA?2004, pages 17?26.
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?565.
Eric Brill. 1996. Learning to parse with transformations. In
Recent Advances in Parsing Technology. Kluwer Academic
Publishers.
Peter F. Brown, Stephan A. Della-Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A probability model
to improve word alignment. In Proceedings of ACL?2003,
pages 88?95.
Micheal Collins. 1997. Three generative lexicalized models for
statistical parsing. In Proceedings of ACL?1997.
Mona Diab and Philip Resnik. 2002. An unsupervised method
for word sense tagging using parallel corpora. In Proceed-
ings of ACL?2002.
Bonnie J. Dorr, Lisa Pearl, Rebecca Hwa, and Nizar Habash.
2002. DUSTer: A method for unraveling cross-language di-
vergences for statistical word?level alignment. In Proceed-
ings of AMTA?2002.
Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL/HLT?2003.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear models
for word alignment. In Proceedings of ACL?2005.
I. Dan Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221?249.
Franz J. Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings of ACL?2000, pages 440?
447.
Franz J. Och and Hermann Ney. 2003. A systematic compari-
son of various statistical alignment models. Computational
Linguistics, 29(1):9?51, March.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of EMNLP?1996.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Man-
ning. 2002. Extensions to HMM-based statistical word
alignment models. In Proceedings of EMNLP?2002, pages
87?94.
Stefan Vogel, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
Proceedings of COLING?1996, pages 836?841.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001.
Inducing multilingual text analysis tools via robust projec-
tion across aligned corpora. In Proceedings of HLT?2001,
pages 109?116.
192
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 96?103,
New York, June 2006. c?2006 Association for Computational Linguistics
A Maximum Entropy Approach to Combining Word Alignments
Necip Fazil Ayan and Bonnie J. Dorr
Institute of Advanced Computer Studies (UMIACS)
University of Maryland
College Park, MD 20742
{nfa,bonnie}@umiacs.umd.edu
Abstract
This paper presents a new approach to
combining outputs of existing word align-
ment systems. Each alignment link is rep-
resented with a set of feature functions
extracted from linguistic features and in-
put alignments. These features are used
as the basis of alignment decisions made
by a maximum entropy approach. The
learning method has been evaluated on
three language pairs, yielding significant
improvements over input alignments and
three heuristic combination methods. The
impact of word alignment on MT quality
is investigated, using a phrase-based MT
system.
1 Introduction
Word alignment?detection of corresponding words
between two sentences that are translations of each
other?is usually an intermediate step of statisti-
cal machine translation (MT) (Brown et al, 1993;
Och and Ney, 2003; Koehn et al, 2003), but also
has been shown useful for other applications such
as construction of bilingual lexicons, word-sense
disambiguation, projection of resources, and cross-
language information retrieval.
Maximum entropy (ME) models have been used
in bilingual sense disambiguation, word reordering,
and sentence segmentation (Berger et al, 1996),
parsing, POS tagging and PP attachment (Ratna-
parkhi, 1998), machine translation (Och and Ney,
2002), and FrameNet classification (Fleischman et
al., 2003). They have also been used to solve the
word alignment problem (Garcia-Varea et al, 2002;
Ittycheriah and Roukos, 2005; Liu et al, 2005), but
a sentence-level approach to combining knowledge
sources is used rather than a word-level approach.
This paper describes an approach to combin-
ing evidence from alignments generated by exist-
ing systems to obtain an alignment that is closer
to the true alignment than the individual align-
ments. The alignment-combination approach (called
ACME) operates at the level of alignment links,
rather than at the sentence level (as in previous ME
approaches). ACME uses ME to decide whether
to include/exclude a particular alignment link based
on feature functions that are extracted from the in-
put alignments and linguistic features of the words.
Since alignment combination relies on evidence
from existing alignments, we focus on alignment
links that exist in at least one input alignment. An
important challenge in this approach is the selection
of appropriate links when two aligners make differ-
ent alignment choices.
We show that ACME yields a significant relative
error reduction over the input alignment systems and
heuristic-based combinations on three different lan-
guage pairs. Using a higher number of input align-
ments and partitioning the training data into disjoint
subsets yield further error-rate reductions.
The next section briefly overviews ME models.
Section 3 presents a new ME approach to combin-
ing existing word alignment systems. Section 4 de-
scribes the evaluation data, input alignments, and
evaluation metrics. Section 5 presents experiments
on three language pairs, upper bounds for alignment
error rate in alignment combination, and MT evalu-
ation on English-Chinese and English-Arabic. Sec-
tion 6 describes previous work on alignment combi-
nation and ME models on word alignment.
2 Maximum Entropy (ME) Models
In a statistical classification problem, the goal is to
estimate the probability of a class y in a given con-
text x, i.e., p(y|x). In an ideal scenario, if the train-
ing data contain evidence for all pairs of (y, x), it is
96
trivial to compute the probability distribution p. Un-
fortunately, due to training-data sparsity, p is gener-
ally modeled using only the available evidence.
Given a collection of facts, ME chooses a model
consistent with all the facts, but otherwise as uni-
form as possible (Berger et al, 1996). Formally, the
evidence is represented as feature functions, i.e., bi-
nary valued functions that map a class y and a con-
text x to either 0 or 1, i.e., hm : Y ? X ? {0, 1},
where Y is the set of all classes and X is the set of all
facts. The biggest advantage of maximum entropy
models is that they are able to focus on the selection
of feature functions rather than on how such func-
tions are used. Any context can be used to define
feature functions without concern for the indepen-
dence of the feature functions from each other or the
relevance of the feature functions to the final deci-
sion (Ratnaparkhi, 1998).
Each feature function hm is associated with a
model parameter ?m. Given a set of M feature func-
tions h1, . . . , hM , the probability of class y given a
context x is equal to:
p(y|x) = 1Zx exp
( M?
m=1
?mhm(y, x)
)
where Zx is a normalization constant. The contri-
bution of each feature function to the final decision,
i.e., ?m, can be automatically computed using Gen-
eralized Iterative Scaling (GIS) algorithm (Darroch
and Ratcliff, 1972). The final classification for a
given instance is the class y that maximizes p(y|x).
3 Alignment Combination: ACME
Let e = e1, . . . , eI and f = f1, . . . , fJ be two
sentences in two different languages. An align-
ment link (i, j) corresponds to a translational equiv-
alence between words ei and fj . Let Ak be an
alignment between sentences e and f , where each
element a ? Ak is an alignment link (i, j). Let
A = {A1, . . . , An} be a set of alignments between
e and f . We refer to the true alignment as T , where
each a ? T is of the form (i, j). The goal of
ACME is to combine the information in A such
that the combined alignment AC is closer to T . A
straightforward solution is to take the intersection or
union of the individual alignments. In this paper, an
additional model is learned to combine outputs of
A1, . . . , An.
In our combination framework, first, n differ-
ent word-alignment systems, A1, . . . , An, generate
word alignments between a given English sentence
and a foreign-language (FL) sentence. Then a Fea-
ture Extractor takes the output of these alignment
systems and the parallel corpus (which might be en-
riched with linguistic features) and extracts a set of
feature functions based on linguistic properties of
the words and the input alignments. Each feature
function hm is associated with a model parameter
?m. Next, an Alignment Combiner decides whether
to include or exclude an alignment link based on the
extracted feature functions and the model parame-
ters associated with them.
For each possible alignment link a set of features
is extracted from the input alignments and linguistic
properties of words. The features that are used for
representing an alignment link (i, j) are as follows:
1. Part-of-speech tags (posE, posF, prevposE,
prevposF, nextpostE, nextposF): POS tags for
the previous, current, and the next English and
FL words.
2. Outputs of input aligners (out): Whether
(i, j) exists in a given input alignment Ak.
3. Neighbors (neigh): A neighborhood of an
alignment link (i, j)?denoted by N(i, j)?
consists of 8 possible alignment links in a 3?3
window with (i, j) in the center of the window.
Each element of N(i, j) is called a neighbor-
ing link of (i, j). Neighbor features include:
(1) Whether a particular neighbor of (i, j) ex-
ists in a given input alignment Ak; and (2) To-
tal number of neighbors of (i, j) in a given in-
put alignment Ak.
4. Fertilities (fertE, fertF): The number of
words that ei (or fj) is aligned to in a given
input alignment Ak.
5. Monotonicity (mon): The absolute difference
between i and j.
Our combination approach employs feature func-
tions derived from a subset of the features above.
Assuming Y = {yes, no} represents the set of
classes, where each class denotes the existence or
absence of a link in the combined alignment, and
X is the set of features above, we generate various
feature functions h(y, x), where y ? Y and x are
instantiations of one or more features in X . Table 1
lists the feature sets with an example feature func-
97
Features Example Feature Function
posE h(?yes?, i, j) = 1 if (i, j) ? AC and pos(ei) = Noun
posF h(?no?, i, j) = 1 if (i, j) /? AC and pos(fj) = V erb
out h(?yes?, i, j, k) = 1 if (i, j) ? AC and (i, j) ? Ak
out, neigh h(?yes?, i, j, k) = 1 if (i, j) ? AC and (i? 1, j + 1) ? Ak
h(?yes?, i, j, k) = 1 if (i, j) ? AC and |NC| = 2 where NC = {n|n ? N(i, j), n ? Ak}
out, fertE h(?no?, i, j, k) = 1 if (i, j) /? AC and |FT | = 0 where FT = {t|(i, t) ? Ak}
out, fertF h(?no?, i, j, k) = 1 if (i, j) /? AC and |FT | = 1 where FT = {t|(t, j) ? Ak}
mon h(?yes?, i, j) = 1 if (i, j) ? AC and |i? j| = 2
Table 1: Feature Functions.
tion for each.1 For example, the feature function in
the fifth row has a value of 1 if there are 2 neighbor-
ing links to (i, j) that exist in the input alignment Ak
and the alignment link (i, j) exists in AC .
In combining evidence from different alignments,
it is assumed that, when an alignment link is left
out by all aligners, that particular link should not
be included in the final output. Since the majority
of all possible word pairs are unaligned in real data,
the inclusion of all possible word pairs in the train-
ing data leads to skewed results, where the learning
algorithm is biased toward labeling the links as in-
valid. To offset this problem, our training data in-
cludes only alignment links that appear in at least
one input alignment.
Once the feature functions are extracted, we learn
the model parameters using the YASMET ME pack-
age (Och, 2002), which is an efficient implementa-
tion of the GIS algorithm.
4 Experiment Data, Alignment Inputs, and
Metrics
The alignment combination techniques are evaluated
in this paper using data from three language pairs, as
shown in Table 2.
Lang # of # Words Source
Pair Sent?s (en/fl)
en-ch 491 13K/13K NIST MTEval ?022
en-ar 450 11K/13K NIST MTEval ?033
en-ro 248 5.5K/5.5K HLT Workshop ?034
Table 2: Data Used for Combination Experiments.
Input alignments are generated using two exist-
ing word alignment systems: GIZA++ (Och, 2000)
1In Table 1, NC corresponds to the set of (i, j)?s neighbors
that exist in the alignment Ak, and FT represents the set of
words that ei (or fj) is aligned to.
2From (Ayan et al, 2005).
3From (Ittycheriah and Roukos, 2005).
4From (Mihalcea and Pedersen, 2003).
and SAHMM (Lopez and Resnik, 2005). Both sys-
tems are run in two different directions with default
configurations. We indicate the two directions using
the notation Aligner(en ? fl) and Aligner(fl ?
en), where en is English, fl is either Chinese (ch),
Arabic (ar), or Romanian (ro).
To train both systems, additional data was used
for the three language pairs: 107K English-Chinese
sentence pairs (4.1M/3.3M English/Chinese words);
44K English-Arabic sentence pairs (1.4M/1M En-
glish/Arabic words); 48K English-Romanian sen-
tence pairs (1M/1M English/Romanian words).5
POS tags were generated using the MXPOST tag-
ger (Ratnaparkhi, 1998). POS tagger for English
was trained on Sections 0-18 of the Penn Treebank
Wall Street Journal corpus. On the FL side, we used
POS tagger for only Chinese and it was trained on
Sections 16-299 of Chinese Treebank.
For comparison purposes, three additional
heuristically-induced alignments are generated
for each system: (1) Intersection of both direc-
tions (Aligner(int)); (2) Union of both directions
(Aligner(union)); and (3) The previously best-
known heuristic combination approach called grow-
diag-final (Koehn et al, 2003) (Aligner(gdf)).
In our evaluation, we take A to be the set of align-
ment links for a set of sentences, S to be the set
of sure alignment links, and P be the set of proba-
ble alignment links (in the gold standard). Precision
(Pr), recall (Rc) and alignment error rate (AER)
are defined as follows:6
Pr = |A ? P ||A| Rc =
|A ? S|
|S|
AER = 1? |A ? S| + |A ? P ||A| + |S|
5Note that both GIZA++ and SAHMM are unsupervised
learning systems. Sentence-aligned parallel texts are the only
required input.
6Note that AER= 1 - F-score when there is no distinction
between probable and sure alignment links.
98
Our gold standard for each language pair is a
manually aligned corpus. English-Chinese annota-
tions distinguish between sure and probable align-
ment links (i.e., S ? P ), but there is no such distinc-
tion for the other two language pairs (i.e., P = S).
Because of the availability of limited manually
annotated data, evaluations are performed using 5-
fold cross validation. Once the alignments are gen-
erated for each fold (using one as the test set and the
other 4 folds as training set), the results are concate-
nated to compute precision, recall and error rate on
the entire set of sentence pairs for each data set.7
5 Experiments and Results
This section presents several experiments and re-
sults comparing AER of ACME to those of standard
alignment approaches on English-Chinese data. We
also present experiments on additional languages,
analyses based on precision and recall, an upper-
bound oracle analysis, and MT evaluations.
5.1 English-Chinese Experiments
The experiments below test the effects of input
alignments, feature set, data partitioning, number of
inputs, and size of training data on the performance
of ACME.
2 Input alignments: Table 3 shows the AER for
GIZA++ and SAHMM (in each direction), three
heuristic-based combinations and ACME using 2
uni-directional alignments as input and all features
described in Section 3.8 (We use ?ACME[2]? in
this section to refer to ACME applied to two input
alignments and ACME[4] in later sections to refer
to ACME applied to four input alignments.)
Using 2 GIZA++ uni-directional alignments as in-
put, ACME yields a 22.0% AER?a relative error re-
duction of 25.9% over GIZA++(gdf). Similarly, us-
ing 2 SAHMM uni-directional alignments as input,
ACME produces a 20.6% AER?a relative error re-
duction of 28.0% and 25.4% over SAHMM(gdf) and
SAHMM(int), respectively.
7Because the NIST MTEval data include sentences that may
be related (according to the document in which they appear), the
training and test material could potentially be related; however,
given the types of features used in our experiments, we do not
believe this biases our results.
8For ease of readability, in the rest of this paper, we will
report precision, recall, and AER in percentages.
Alignments GIZA++ SAHMM
Aligner(en ? fl) 30.7 26.5
Aligner(fl ? en) 32.2 31.3
Aligner(int) 31.2 27.6
Aligner(union) 31.6 29.8
Aligner(gdf) 29.7 28.6
ACME[2] 22.0 20.6
Table 3: Comparison of GIZA++ and SAHMM to
ACME[2] (on English-Chinese).
Feature Set: To examine the effects of each fea-
ture on the performance of ACME, we compute the
AER under a variety of conditions, removing each
feature one at a time. ACME is evaluated using
2 uni-directional GIZA++ alignments as input on
English-Chinese data. Using all features, the AER
is 22.0%. Our experiments show that there is no sig-
nificant increase in AER for the removal of features
corresponding to monotonicity (22.1%), neighbors
(22.8%), POS on English side (22.9%), POS on
foreign-language side (22.9%). On the other hand,
deleting POS tags on both sides yields an AER of
25.2% and deleting the fertility features increases
the AER to 25.9%. This indicates that both POS
tags (or fertilities) contribute heavily toward the de-
cision as to whether a particular alignment should be
included/excluded.
Partitioning Data: Previous work showed that
partitioning the data into disjoint subsets and learn-
ing a different model for each partition improves
the performance of the alignment systems (Ayan et
al., 2005). To test whether this same principle ap-
plies to alignment combination with maximum en-
tropy modeling, the training data was partitioned us-
ing POS tags for English and the FL, and different
weights were learned for each partition.
Alignments GIZA++ SAHMM
ACME[2] 22.0 20.6
ACME[2]-Part[posE] 19.8 18.0
ACME[2]-Part[posF ] 20.0 18.1
ACME[2]-Part[posE, posF ] 20.0 18.4
Table 4: Application of ACME[2] on Partitioned
Data (on English-Chinese).
Table 4 presents the AER for ACME[2], using ei-
ther two GIZA++ alignments or two SAHMM align-
ments, on English-Chinese data. Without any parti-
tioning, ACME achieves an AER of 22.0 (GIZA++)
and 20.6 (SAHMM). Using English POS tags for
data partitioning results in a significant reduction
99
in AER: 19.8% (GIZA++) and 18.0% (SAHMM).
Interestingly, using foreign-language (FL) tags on
their own or together with English POS tags does not
provide any improvement. Overall when ACME[2]
is applied to partitioned data (using posE for parti-
tioning) a relative error reduction of 33?37% over
GIZA++(gdf) and SAHMM(gdf) is achieved.
Number of Input Alignments: Table 5 presents
the English-Chinese AER for ACME[1] (using ei-
ther GIZA++ or SAHMM in only one direction),
ACME[2] (using either GIZA++ or SAHMM in
two directions) and ACME[4] (using GIZA++ and
SAHMM, each in two directions).
Regardless of the number of inputs, partitioning
the data (using English POS tags) yields lower AER
than no partitioning. Using one GIZA++ alignment
as input, ACME[1] with partitioning improves the
AER to 26.9% and 25.5% for each direction, respec-
tively. Similarly, using one SAHMM alignment as
input, ACME[1] with partitioning reduces the AER
to 22.9% and 24.7%. ACME[2] with partitioning
reduces the AER to 19.8% and 18.0% for GIZA++
and SAHMM, respectively. Finally, using all four
input alignments, ACME[4] with partitioning yields
a 15.6% AER?a relative error reduction of 21.2%
and 13.3% over each ACME[2] case.
Alignments GIZA++ SAHMM
ACME[1](en ? fl) 28.1 24.4
ACME[1]-Part[posE](en ? fl) 26.9 22.9
ACME[1](fl ? en) 26.6 26.9
ACME[1]-Part[posE](fl ? en) 25.5 24.7
ACME[2] 22.0 20.6
ACME[2]-Part[posE] 19.8 18.0
ACME[4] 17.8
ACME[4]-Part[posE] 15.6
Table 5: Application of ACME to 1, 2 and 4 Input
Alignments (on English-Chinese).
Size of Training Data to Obtain Input Align-
ments: In general, statistical alignment systems
improve as the size of the training data increases.
We present the AER for GIZA++ and ACME[2] us-
ing GIZA++ alignments as input, where GIZA++ is
trained on different sizes of data. We started with
20K sentence pairs of FBIS data and increased it to
all available FBIS data (241K sentence pairs).
Figure 1 compares the alignment performance
of: (1) uni-directional GIZA++ (each direction);
(2) GIZA++(gdf); and (3) ACME[2] with all fea-
Figure 1: Effects of Training Data Size Used for Ini-
tial Alignments on the performance of GIZA++ and
ACME[2] (on English-Chinese).
tures and English POS partitioning. With only
20K sentence pairs, ACME[2] achieves an AER of
23.7% in contrast to 34.3% AER for GIZA++(gdf).
With 241K sentence pairs, ACME[2] yields 18.3%
AER in contrast to 27.7% AER for GIZA++(gdf).
We should emphasize that ACME[2] on only 20K
sentence pairs yields a lower AER than those of
all GIZA++ alignments obtained on 241K sen-
tence pairs. Overall ACME[2] achieves a relative
error reduction of 31?38% over the input align-
ments, and a relative error reduction of 31?34% over
GIZA++(gdf) for different sizes of training data.
5.2 Expanding to Additional Languages
We also investigated the applicability of ACME to
additional language pairs. Table 6 presents the
AER for GIZA++ and SAHMM (in each direction),
three combination heuristics (gdf, int and union),
and ACME[2] and ACME[4] on English-Arabic and
English-Romanian data. We should emphasize that
no POS tagger on the FL side was used for these
experiments.
On English-Arabic data, ACME[2] (with POS
partitioning and including all features) yields 21.4%
(20.7%) AER?a relative error reduction of 24.6%
(13.0%) over the best combination heuristic with
GIZA++ (SAHMM) alignments. ACME[4] re-
duces the AER to 18.1%?a relative error reduc-
tion of 36.3% and 23.9% over GIZA++(int) and
SAHMM(int), respectively.
On English-Romanian data, ACME[2] (with POS
partitioning and including all features) yields 24.7%
(26.2%) AER?a relative error reduction of 14.3%
(10.6%) over the best combination heuristic with
GIZA++ (SAHMM) alignments. ACME[4] re-
100
English-Arabic English-Romanian
Alignments GIZA++ SAHMM GIZA++ SAHMM
Aligner(en ? fl) 34.5 27.8 32.7 31.0
Aligner(fl ? en) 27.9 29.5 30.0 29.8
Aligner(int) 28.4 23.8 32.7 29.3
Aligner(union) 32.8 32.0 30.5 31.2
Aligner(gdf) 30.2 30.4 28.8 30.3
ACME[2] 23.2 21.9 25.2 27.0
ACME[2]-Part[posE] 21.4 20.7 24.7 26.2
ACME[4] 19.8 24.0
ACME[4]-Part[posE] 18.1 22.3
Table 6: AER for Input Alignments, Heuristic-based Alignments, and ACME Using 2 and 4 Input Align-
ments (on English-Arabic and English-Romanian).
duces the AER to 22.3%?a relative error reduc-
tion of 22.6% and 23.9% over GIZA++(int) and
SAHMM(int), respectively.
5.3 Precision, Recall and Upper-Bound
Analysis
We now turn to a precision vs. recall analysis of dif-
ferent alignments to elucidate the nature of the dif-
ferences between two alignments.
Figure 2 presents precision and recall values
for three combined alignments using GIZA++ (int,
union, gdf) as well as results for ACME[2] and
ACME[4] on three different language pairs. For
all three pairs, the ranking of the combined align-
ments is the same with respect to precision and
recall. GIZA++(int) yields the highest precision
(nearly 95%) but the lowest recall (53?57%). Both
union and gdf methods achieve low precision (56?
68%) but high recall (75?83%), and gdf is better
than union. By contrast, ACME[2] yields signifi-
cantly higher precision (nearly 87%) but lower recall
(67?75%) with respect to union and gdf. ACME[4]
has higher precision and recall than ACME[2]?an
absolute increase of 2?3% and 4%, respectively.
Next we compute an oracle upper-bound in AER
where mismatched input alignments are assumed to
be resolved perfectly within the alignment combina-
tion framework (i.e., an oracle chooses the correct
output in cases where the input aligners make differ-
ent choices).9
Table 7 presents the upper bounds using a generic
alignment combiner (denoted Oracle) with 2 and 4
input alignments on three language pairs, assuming
a perfect resolution of mismatched input alignments.
For English-Chinese, the upper bound is 9.4% (us-
9If the input aligners agree on a particular link, that decision
is taken as the final output in computing the upper bound.
Alignments GIZA++ SAHMM
Oracle[2] (en-ch) 9.4 8.4
Oracle[4] (en-ch) 4.7
Oracle[2] (en-ar) 9.8 11.1
Oracle[4] (en-ar) 5.5
Oracle[2] (en-ro) 15.4 17.7
Oracle[4] (en-ro) 11.3
Table 7: Oracle Upper Bounds on AER for Align-
ment Combination
ing Oracle[2]) and 4.7% (using Oracle[4]). The
English-Arabic data exhibits a slightly higher upper
bound of 5.5% for Oracle[4]. The upper bounds for
AER on English-Romanian data are even higher (up
to 17.7%), which indicates that the input alignments
are significantly worse than others. This may be
one of the main contributing factors to the lower im-
provement of ACME on English-Romanian in com-
parison to the other two language pairs.
5.4 MT Evaluation
To determine the contribution of improved align-
ment in an external application, we examined the
improvement in an off-the-shelf phrase-based MT
system Pharaoh (Koehn, 2004) on both Chinese and
Arabic data. In these experiments, all components
of the MT system were kept the same except for
the component that generates a phrase table from a
given alignment.
The input alignments were generated using
GIZA++ and SAHMM on 107K (44K) sentence
pairs for Chinese (Arabic). ACME (with English
POS partitioning) combines alignments using model
parameters learned from the corresponding manu-
ally aligned data. MT output is evaluated using the
standard MT evaluation metric BLEU (Papineni et
al., 2002).10 Table 8 presents the BLEU scores on
10We used the NIST script (version 11a) with its default set-
101
Figure 2: Precision and Recall Scores for GIZA++ and ACME Using 2 and 4 Input Alignments.
MTEval?03 data for 5 different Pharaoh runs, one for
each alignment. The parameters of the MT system
were optimized on MTEval?02 data using minimum
error rate training (Och, 2003).
For the language model, the SRI Language Mod-
eling Toolkit was used to train a trigram model with
modified Kneser-Ney smoothing on 155M words of
English newswire text, mostly from the Xinhua por-
tion of the Gigaword corpus. During decoding, the
number of English phrases per FL phrase was lim-
ited to 100 and the distortion of phrases was lim-
ited by 4. Based on the observations in (Koehn et
al., 2003), we also limited the phrase length to 3 for
computational reasons.
Alignment Chinese Arabic
GIZA++(union) 22.66 41.72
GIZA++(gdf) 23.79 43.82
GIZA++(int) 23.97 42.76
ACME[2] 25.20 44.94
ACME[4] 25.59 45.54
Table 8: Evaluation of Pharaoh with Different Initial
Alignments using BLEU (in percentages)
For both languages, ACME[2] and ACME[4]
outperform the other three alignment combination
techniques. ACME[4], for instance, yields the
BLEU scores of 25.59% for Chinese and 45.54% for
Arabic?an absolute 1.6-1.7% BLEU point increase
over the best of the other three alignment combina-
tions. The differences between the BLEU scores for
ACME and the other three BLEU scores are statisti-
cally significant, using a significance test with boot-
strap resampling (Zhang et al, 2004).
6 Related Work
ME models have been previously applied to several
NLP problems, including word alignments. For in-
tings: case-insensitive matching of n-grams up to n = 4, and
the shortest reference sentence for the brevity penalty.
stance, the IBM models (Brown et al, 1993) can be
improved by adding more context dependencies into
the translation model using a ME framework rather
than using only p(fj |ei) (Garcia-Varea et al, 2002).
In a later study, Och and Ney (2003) present a log-
linear combination of the HMM and IBM Model 4
that produces better alignments than either of those.
The major advantage of these two methods is that
they do not require manually annotated data.
The alignment process can be modeled as a prod-
uct of a transition model and an observation model,
where ME models the observations (Ittycheriah and
Roukos, 2005). Significant improvements are re-
ported using this approach but the need for large
manually aligned data is a bottleneck. An alterna-
tive ME approach models alignment directly as a
log-linear combination of feature functions (Liu et
al., 2005). Moore (2005) and Taskar et al (2005)
represent alignments with several feature functions
that are then combined in a weighted sum to model
word alignments. Once a confidence score is as-
signed to all links, a non-trivial search is invoked to
find the best alignment using the scores associated
with the links. The major difference between these
approaches and that of ACME is that we use the ME
model to predict the correct class for each align-
ment link independently using outputs of existing
alignment systems, instead of generating them from
scratch at the level of the whole sentence, thus elim-
inating the need for an exhaustive search over all
possible alignments, i.e., previous approaches work
globally while ACME is a localized model. A dis-
cussion of these two contrasting approaches can be
found in (Tillmann and Zhang, 2005).
A recent attempt to combine outputs of differ-
ent alignments views the combination problem as a
classifier ensemble in the neural network framework
102
(Ayan et al, 2005). However, this method is subject
to the unpredictability of random network initializa-
tion, whereas ACME is guaranteed to find the model
that maximizes the likelihood of training data.
7 Conclusions
We presented a new approach, ACME, to combin-
ing the outputs of different word alignment systems
by reducing the combination problem to the level
of alignment links and using a maximum entropy
model to learn whether a particular alignment link
is included in the final alignment.
Our results indicate that ACME yields significant
relative error reduction over the input alignments
and their heuristic-based combinations on three dif-
ferent language pairs. Moreover, ACME provides
similar relative improvements for different sizes of
training data for the input alignment systems. We
have also shown that using a higher number of input
alignments, and partitioning the training data into
disjoint subsets and learning a different model for
each partition yield further improvements.
We have tested impact of the reduced AER on
MT and have shown that alignments generated by
ACME yield statistically significant improvements
in BLEU scores in two different languages, even
if we don?t employ a POS tagger on the FL side.
However, additional studies are needed to investi-
gate why huge improvements in AER result in rela-
tively smaller improvements in BLEU scores.
Because ACME is a supervised learning ap-
proach, it requires annotated data; however, our ex-
periments have shown that significant improvements
can be obtained using a small set of annotated data.
Acknowledgments This work has been supported, in
part, under ONR MURI Contract FCPO.810548265 and the
GALE program of the Defense Advanced Research Projects
Agency, Contracts No. HR0011-06-2-0001. We also thank
anonymous reviewers for their helpful comments.
References
Necip F. Ayan, Bonnie J. Dorr, and Christof Monz. 2005. Neu-
ralign: Combining word alignments using neural networks.
In Proceedings of EMNLP?2005, pages 65?72.
Adam L. Berger, Stephan A. Della-Pietra, and Vincent J. Della-
Pietra. 1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 22(1).
Peter F. Brown, Stephan A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263?311.
J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scal-
ing for log-linear models. Annals of Mathematical Statistics,
43:1470?1480.
Michael Fleischman, Namhee Kwon, and Eduard Hovy. 2003.
Maximum entropy models for framenet classification. In
Proceedings of EMNLP?2003.
Ismael Garcia-Varea, Franz Josef Och, Hermann Ney, and Fran-
cisco Casacuberta. 2002. Improving alignment quality in
statistical machine translation using context-dependent max-
imum entropy models. In Proceedings of COLING?2002.
Abraham Ittycheriah and Salim Roukos. 2005. A maximum
entropy word aligner for arabic-english machine translation.
In Proceedings of EMNLP?2005.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of HLT-
NAACL?2003.
Philipp Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation. In Proceedings
of AMTA?2004.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear models
for word alignment. In Proceedings of ACL?2005.
Adam Lopez and Philip Resnik. 2005. Improved HMM align-
ment models for languages with scarce resources. In Pro-
ceedings of the ACL?2005 Workshop on Building and Using
Parallel Texts: Data Driven Machine Translation and Be-
yond, pages 83?86.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation ex-
ercise for word alignment. In Proceedings of the HLT-
NAACL?2003 Workshop: Building and Using Parallel Texts:
Data Driven Machine Translation and Beyond, pages 1?10.
Robert C. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proceedings of EMNLP?2005.
Franz J. Och and Hermann Ney. 2002. Discriminative training
and maximum entropy models for statistical machine trans-
lation. In Proceedings of ACL?2002, pages 295?302.
Franz J. Och and Hermann Ney. 2003. A systematic compari-
son of various statistical alignment models. Computational
Linguistics, 29(1):9?51, March.
Franz J. Och. 2000. GIZA++: Training of statistical transla-
tion models. Technical report, RWTH Aachen, University
of Technology.
Franz J. Och. 2002. Yet another maxent toolkit: YASMET.
Available at http://www.fjoch.com/YASMET.html.
Franz J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL?2003, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: A method for automatic evaluation of
machine translation. In Proceedings of ACL?2002, pages
311?318.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Nat-
ural Language Ambiguity Resolution. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A
discriminative matching approach to word alignment. In
Proceedings of EMNLP?2005.
Christoph Tillmann and Tong Zhang. 2005. A localized predic-
tion model for statistical machine translation. In Proceed-
ings of ACL?2005.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do we
need to have a better system? In Proceedings of LREC?2004,
pages 2051?2054.
103
Proceedings of NAACL HLT 2007, pages 228?235,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Outputs from Multiple Machine Translation Systems
Antti-Veikko I. Rosti   and Necip Fazil Ayan

and Bing Xiang   and
Spyros Matsoukas   and Richard Schwartz   and Bonnie J. Dorr

  BBN Technologies, 10 Moulton Street, Cambridge, MA 02138

arosti,bxiang,smatsouk,schwartz  @bbn.com

Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742

nfa,bonnie  @umiacs.umd.edu
Abstract
Currently there are several approaches to
machine translation (MT) based on differ-
ent paradigms; e.g., phrasal, hierarchical
and syntax-based. These three approaches
yield similar translation accuracy despite
using fairly different levels of linguistic
knowledge. The availability of such a
variety of systems has led to a growing
interest toward finding better translations
by combining outputs from multiple sys-
tems. This paper describes three differ-
ent approaches to MT system combina-
tion. These combination methods oper-
ate on sentence, phrase and word level
exploiting information from  -best lists,
system scores and target-to-source phrase
alignments. The word-level combination
provides the most robust gains but the
best results on the development test sets
(NIST MT05 and the newsgroup portion
of GALE 2006 dry-run) were achieved by
combining all three methods.
1 Introduction
In recent years, machine translation systems based
on new paradigms have emerged. These systems
employ more than just the surface-level information
used by the state-of-the-art phrase-based translation
systems. For example, hierarchical (Chiang, 2005)
and syntax-based (Galley et al, 2006) systems have
recently improved in both accuracy and scalability.
Combined with the latest advances in phrase-based
translation systems, it has become more attractive
to take advantage of the various outputs in forming
consensus translations (Frederking and Nirenburg,
1994; Bangalore et al, 2001; Jayaraman and Lavie,
2005; Matusov et al, 2006).
System combination has been successfully ap-
plied in state-of-the-art speech recognition evalua-
tion systems for several years (Fiscus, 1997). Even
though the underlying modeling techniques are sim-
ilar, many systems produce very different outputs
with approximately the same accuracy. One of the
most successful approaches is consensus network
decoding (Mangu et al, 2000) which assumes that
the confidence of a word in a certain position is
based on the sum of confidences from each system
output having the word in that position. This re-
quires aligning the system outputs to form a con-
sensus network and ? during decoding ? simply
finding the highest scoring path through this net-
work. The alignment of speech recognition outputs
is fairly straightforward due to the strict constraint in
word order. However, machine translation outputs
do not have this constraint as the word order may be
different between the source and target languages.
MT systems employ various re-ordering (distortion)
models to take this into account.
Three MT system combination methods are pre-
sented in this paper. They operate on the sentence,
phrase and word level. The sentence-level combi-
nation is based on selecting the best hypothesis out
of the merged N-best lists. This method does not
generate new hypotheses ? unlike the phrase and
word-level methods. The phrase-level combination
228
is based on extracting sentence-specific phrase trans-
lation tables from system outputs with alignments
to source and running a phrasal decoder with this
new translation table. This approach is similar to
the multi-engine MT framework proposed in (Fred-
erking and Nirenburg, 1994) which is not capable of
re-ordering. The word-level combination is based
on consensus network decoding. Translation edit
rate (TER) (Snover et al, 2006) is used to align
the hypotheses and minimum Bayes risk decoding
under TER (Sim et al, 2007) is used to select the
alignment hypothesis. All combination methods use
weights which may be tuned using Powell?s method
(Brent, 1973) on  -best lists. Both sentence and
phrase-level combination methods can generate  -
best lists which may also be used as new system out-
puts in the word-level combination.
Experiments on combining six machine transla-
tion system outputs were performed. Three sys-
tems were phrasal, two hierarchical and one syntax-
based. The systems were evaluated on NIST MT05
and the newsgroup portion of the GALE 2006 dry-
run sets. The outputs were evaluated on both TER
and BLEU. As the target evaluation metric in the
GALE program was human-mediated TER (HTER)
(Snover et al, 2006), it was found important to im-
prove both of these automatic metrics.
This paper is organized as follows. Section 2
describes the evaluation metrics and a generic dis-
criminative optimization technique used in tuning of
the various system combination weights. Sentence,
phrase and word-level system combination methods
are presented in Sections 3, 4 and 5. Experimental
results on Arabic and Chinese to English newswire
and newsgroup test data are presented in Section 6.
2 Evaluation Metrics and Discriminative
Tuning
The official metric of the 2006 DARPA GALE
evaluation was human-mediated translation edit rate
(HTER). HTER is computed as the minimum trans-
lation edit rate (TER) between a system output and
a targeted reference which preserves the meaning
and fluency of the sentence (Snover et al, 2006).
The targeted reference is generated by human post-
editors who make edits to a reference translation so
as to minimize the TER between the reference and
the MT output without changing the meaning of the
reference. Computing the HTER is very time con-
suming due to the human post-editing. It is desir-
able to have an automatic evaluation metric that cor-
relates well with the HTER to allow fast evaluation
of the MT systems during development. Correla-
tions of different evaluation metrics have been stud-
ied (Snover et al, 2006) but according to various
internal HTER experiments it is not clear whether
TER or BLEU correlates better. Therefore it is prob-
ably safest to try and not degrade either.
The TER of a translation   is computed as

 	
 
ffProceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 9?16,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Going Beyond AER: An Extensive Analysis of Word Alignments and
Their Impact on MT
Necip Fazil Ayan and Bonnie J. Dorr
Institute of Advanced Computer Studies (UMIACS)
University of Maryland
College Park, MD 20742
{nfa,bonnie}@umiacs.umd.edu
Abstract
This paper presents an extensive evalua-
tion of five different alignments and in-
vestigates their impact on the correspond-
ing MT system output. We introduce
new measures for intrinsic evaluations and
examine the distribution of phrases and
untranslated words during decoding to
identify which characteristics of different
alignments affect translation. We show
that precision-oriented alignments yield
better MT output (translating more words
and using longer phrases) than recall-
oriented alignments.
1 Introduction
Word alignments are a by-product of statistical
machine translation (MT) and play a crucial role
in MT performance. In recent years, researchers
have proposed several algorithms to generate word
alignments. However, evaluating word alignments
is difficult because even humans have difficulty
performing this task.
The state-of-the art evaluation metric?
alignment error rate (AER)?attempts to balance
the precision and recall scores at the level of
alignment links (Och and Ney, 2000). Other met-
rics assess the impact of alignments externally,
e.g., different alignments are tested by comparing
the corresponding MT outputs using automated
evaluation metrics (e.g., BLEU (Papineni et al,
2002) or METEOR (Banerjee and Lavie, 2005)).
However, these studies showed that AER and
BLEU do not correlate well (Callison-Burch et al,
2004; Goutte et al, 2004; Ittycheriah and Roukos,
2005). Despite significant AER improvements
achieved by several researchers, the improvements
in BLEU scores are insignificant or, at best, small.
This paper demonstrates the difficulty in assess-
ing whether alignment quality makes a difference
in MT performance. We describe the impact of
certain alignment characteristics on MT perfor-
mance but also identify several alignment-related
factors that impact MT performance regardless of
the quality of the initial alignments. In so doing,
we begin to answer long-standing questions about
the value of alignment in the context of MT.
We first evaluate 5 different word alignments
intrinsically, using: (1) community-standard
metrics?precision, recall and AER; and (2) a
new measure called consistent phrase error rate
(CPER). Next, we observe the impact of differ-
ent alignments on MT performance. We present
BLEU scores on a phrase-based MT system,
Pharaoh (Koehn, 2004), using five different align-
ments to extract phrases. We investigate the im-
pact of different settings for phrase extraction, lex-
ical weighting, maximum phrase length and train-
ing data. Finally, we present a quantitative analy-
sis of which phrases are chosen during the actual
decoding process and show how the distribution of
the phrases differ from one alignment into another.
Our experiments show that precision-oriented
alignments yield better phrases for MT than recall-
oriented alignments. Specifically, they cover a
higher percentage of our test sets and result in
fewer untranslated words and selection of longer
phrases during decoding.
The next section describes work related to our
alignment evaluation approach. Following this
we outline different intrinsic evaluation measures
of alignment and we propose a new measure to
evaluate word alignments within phrase-basedMT
framework. We then present several experiments
to measure the impact of different word align-
ments on a phrase-based MT system, and inves-
tigate how different alignments change the phrase
9
selection in the same MT system.
2 Related Work
Starting with the IBM models (Brown et al,
1993), researchers have developed various statis-
tical word alignment systems based on different
models, such as hidden Markov models (HMM)
(Vogel et al, 1996), log-linear models (Och and
Ney, 2003), and similarity-based heuristic meth-
ods (Melamed, 2000). These methods are un-
supervised, i.e., the only input is large paral-
lel corpora. In recent years, researchers have
shown that even using a limited amount of manu-
ally aligned data improves word alignment signif-
icantly (Callison-Burch et al, 2004). Supervised
learning techniques, such as perceptron learn-
ing, maximum entropy modeling or maximum
weighted bipartite matching, have been shown to
provide further improvements on word alignments
(Ayan et al, 2005; Moore, 2005; Ittycheriah and
Roukos, 2005; Taskar et al, 2005).
The standard technique for evaluating word
alignments is to represent alignments as a set of
links (i.e., pairs of words) and to compare the gen-
erated alignment against manual alignment of the
same data at the level of links. Manual align-
ments are represented by two sets: Probable (P )
alignments and Sure (S) alignments, where S ?
P . Given A,P and S, the most commonly used
metrics?precision (Pr), recall (Rc) and alignment
error rate (AER)?are defined as follows:
Pr =
|A ? P |
|A|
Rc =
|A ? S|
|S|
AER = 1?
|A ? S|+ |A ? P |
|A|+ |S|
Another approach to evaluating alignments is to
measure their impact on an external application,
e.g., statistical MT. In recent years, phrase-based
systems (Koehn, 2004; Chiang, 2005) have been
shown to outperform word-based MT systems;
therefore, in this paper, we use a publicly-available
phrase-based MT system, Pharaoh (Koehn, 2004),
to investigate the impact of different alignments.
Although it is possible to estimate phrases di-
rectly from a training corpus (Marcu and Wong,
2002), most phrase-based MT systems (Koehn,
2004; Chiang, 2005) start with a word alignment
and extract phrases that are consistent with the
given alignment. Once the consistent phrases are
extracted, they are assigned multiple scores (such
Test
Lang # of # Words Source
Pair Sent?s (en/fl)
en-ch 491 14K/12K NIST MTEval?2002
en-ar 450 13K/11K NIST MTEval?2003
Training
en-ch 107K 4.1M/3.3M FBIS
en-ar 44K 1.4M/1.1M News + Treebank
Table 1: Test and Training Data Used for Experiments
as translation probabilities and lexical weights),
and the decoder?s job is to choose the correct
phrases based on those scores using a log-linear
model.
3 Intrinsic Evaluation of Alignments
Our goal is to compare different alignments and
to investigate how their characteristics affect the
MT systems. We evaluate alignments in terms of
precision, recall, alignment error rate (AER), and
a new measure called consistent phrase error rate
(CPER).
We focus on 5 different alignments obtained by
combining two uni-directional alignments. Each
uni-directional alignment is the result of running
GIZA++ (Och, 2000b) in one of two directions
(source-to-target and vice versa) with default con-
figurations. The combined alignments that are
used in this paper are as follows:
1. Union of both directions (SU),
2. Intersection of both directions (SI),
3. A heuristic based combination technique
called grow-diag-final (SG), which is the
default alignment combination heuristic
employed in Pharaoh (Koehn, 2004),
4-5. Two supervised alignment combination
techniques (SA and SB) using 2 and 4 in-
put alignments as described in (Ayan et
al., 2005).
This paper examines the impact of alignments
according to their orientation toward precision or
recall. Among the five alignments above, SU and
SG are recall-oriented while the other three are
precision-oriented. SB is an improved version of
SA which attempts to increase recall without a sig-
nificant sacrifice in precision.
Manually aligned data from two language pairs
are used in our intrinsic evaluations using the five
combinations above. A summary of the training
and test data is presented in Table 1.
Our gold standard for each language pair is
a manually aligned corpus. English-Chinese an-
10
notations distinguish between sure and probable
alignment links, but English-Arabic annotations
do not. The details of how the annotations are
done can be found in (Ayan et al, 2005) and (Itty-
cheriah and Roukos, 2005).
3.1 Precision, Recall and AER
Table 2 presents the precision, recall, and AER for
5 different alignments on 2 language pairs. For
each of these metrics, a different system achieves
the best score ? respectively, these are SI, SU, and
SB. SU and SG yield low precision, high recall
alignments. In contrast, SI yields very high pre-
cision but very low recall. SA and SB attempt to
balance these two measures but their precision is
still higher than their recall. Both systems have
nearly the same precision but SB yields signifi-
cantly higher recall than SA.
Align. en-ch en-ar
Sys. Pr Rc AER Pr Rc AER
SU 58.3 84.5 31.6 56.0 84.1 32.8
SG 61.9 82.6 29.7 60.2 83.0 30.2
SI 94.8 53.6 31.2 96.1 57.1 28.4
SA 87.0 74.6 19.5 88.6 71.1 21.1
SB 87.8 80.5 15.9 90.1 76.1 17.5
Table 2: Comparison of 5 Different Alignments using AER
(on English-Chinese and English-Arabic)
3.2 Consistent Phrase Error Rate
In this section, we present a new method, called
consistent phrase error rate (CPER), for evalu-
ating word alignments in the context of phrase-
based MT. The idea is to compare phrases con-
sistent with a given alignment against phrases that
would be consistent with human alignments.
CPER is similar to AER but operates at the
phrase level instead of at the word level. To com-
pute CPER, we define a link in terms of the posi-
tion of its start and end words in the phrases. For
instance, the phrase link (i1, i2, j1, j2) indicates
that the English phrase ei1 , . . . , ei2 and the FL
phrase fj1 , . . . , fj2 are consistent with the given
alignment. Once we generate the set of phrases
PA and PG that are consistent with a given align-
ment A and a manual alignment G, respectively,
we compute precision (Pr), recall (Rc), and CPER
as follows:1
Pr =
|PA ? PG|
|PA|
Rc =
|PA ? PG|
|PG|
CPER = 1?
2? Pr ?Rc
Pr + Rc
1Note that CPER is equal to 1 - F-score.
Chinese Arabic
Align. CPER-3 CPER-7 CPER-3 CPER-7
SU 63.2 73.3 55.6 67.1
SG 59.5 69.4 52.0 62.6
SI 50.8 69.8 50.7 67.6
SA 40.8 51.6 42.0 54.1
SB 36.8 45.1 36.1 46.6
Table 3: Consistent Phrase Error Rates with Maximum
Phrase Lengths of 3 and 7
CPER penalizes incorrect or missing alignment
links more severely than AER. While comput-
ing AER, an incorrect alignment link reduces the
number of correct alignment links by 1, affecting
precision and recall slightly. Similarly, if there is
a missing link, only the recall is reduced slightly.
However, when computing CPER, an incorrect or
missing alignment link might result in more than
one phrase pair being eliminated from or added to
the set of phrases. Thus, the impact is more severe
on both precision and recall.
Figure 1: Sample phrases that are generated from a human
alignment and an automated alignment: Gray cells show the
alignment links, and rectangles show the possible phrases.
In Figure 1, the first box represents a manual
alignment and the other two represent automated
alignments A. In the case of a missing align-
ment link (Figure 1b), PA includes 9 valid phrases.
For this alignment, AER = 1 ? (2 ? 2/2 ?
2/3)/(2/2 + 2/3) = 0.2 and CPER = 1? (2?
5/9? 5/6)/(5/9+5/6) = 0.33. In the case of an
incorrect alignment link (Figure 1c), PA includes
only 2 valid phrases, which results in a higher
CPER (1? (2? 2/2? 2/6)/(2/2+2/6) = 0.49)
but a lower AER (1 ? (2 ? 3/4 ? 3/3)/(3/4 +
3/3) = 0.14).
Table 3 presents the CPER values on two dif-
ferent language pairs, using 2 different maximum
phrase lengths. For both maximum phrase lengths,
SA and SB yield the lowest CPER. For all 5
alignments?in both languages?CPER increases
as the length of the phrase increases. For all
alignments except SI, this amount of increase is
nearly the same on both languages. Since SI con-
tains very few alignment points, the number of
generated phrases dramatically increases, yielding
11
poor precision and CPER as the maximum phrase
length increases.
4 Evaluating Alignments within MT
We now move from intrinsic measurement to ex-
trinsic measurement using an off-the-shelf phrase-
based MT system Pharaoh (Koehn, 2004). Our
goal is to identify the characteristics of alignments
that change MT behavior and the types of changes
induced by these characteristics.
All MT system components were kept the same
in our experiments except for the component that
generates a phrase table from a given alignment.
We used the corpora presented in Table 1 to train
the MT system. The phrases were scored using
translation probabilities and lexical weights in two
directions and a phrase penalty score. We also use
a language model, a distortion model and a word
penalty feature for MT.
We measure the impact of different alignments
on Pharaoh using three different settings:
1. Different maximum phrase length,
2. Different sizes of training data, and
3. Different lexical weighting.
For maximum phrase length, we used 3 (based
on what was suggested by (Koehn et al, 2003) and
7 (the default maximum phrase length in Pharaoh).
For lexical weighting, we used the original
weighting scheme employed in Pharaoh and a
modified version. We realized that the publicly-
available implementation of Pharaoh computes
the lexical weights only for non-NULL alignment
links. As a consequence, loose phrases contain-
ing NULL-aligned words along their edges receive
the same lexical weighting as tight phrases with-
out NULL-aligned words along the edges. We
therefore adopted a modified weighting scheme
following (Koehn et al, 2003), which incorporates
NULL alignments.
MT output was evaluated using the standard
evaluation metric BLEU (Papineni et al, 2002).2
The parameters of the MT System were opti-
mized for BLEU metric on NIST MTEval?2002
test sets using minimum error rate training (Och,
2003), and the systems were tested on NIST
MTEval?2003 test sets for both languages.
2We used the NIST script (version 11a) for BLEU with
its default settings: case-insensitive matching of n-grams up
to n = 4, and the shortest reference sentence for the brevity
penalty. The words that were not translated during decoding
were deleted from the MT output before running the BLEU
script.
The SRI Language Modeling Toolkit was used
to train a trigrammodel with modified Kneser-Ney
smoothing on 155M words of English newswire
text, mostly from the Xinhua portion of the Gi-
gaword corpus. During decoding, the number of
English phrases per FL phrase was limited to 100
and phrase distortion was limited to 4.
4.1 BLEU Score Comparison
Table 4 presents the BLEU scores for Pharaoh runs
on Chinese with five different alignments using
different settings for maximum phrase length (3
vs. 7), size of training data (107K vs. 241K), and
lexical weighting (original vs. modified).3
The modified lexical weighting yields huge im-
provements when the alignment leaves several
words unaligned: the BLEU score for SA goes
from 24.26 to 25.31 and the BLEU score for SB
goes from 23.91 to 25.38. In contrast, when the
alignments contain a high number of alignment
links (e.g., SU and SG), modifying lexical weight-
ing does not bring significant improvements be-
cause the number of phrases containing unaligned
words is relatively low. Increasing the phrase
length increases the BLEU scores for all systems
by nearly 0.7 points and increasing the size of the
training data increases the BLEU scores by 1.5-2
points for all systems. For all settings, SU yields
the lowest BLEU scores while SB clearly outper-
forms the others.
Table 5 presents BLEU scores for Pharaoh runs
on 5 different alignments on English-Arabic, using
different settings for lexical weighting and max-
imum phrase lengths.4 Using the original lexi-
cal weighting, SA and SB perform better than the
others while SU and SI yield the worst results.
Modifying the lexical weighting leads to slight re-
ductions in BLEU scores for SU and SG, but im-
proves the scores for the other 3 alignments signif-
icantly. Finally, increasing the maximum phrase
length to 7 leads to additional improvements in
BLEU scores, where SG and SU benefit nearly 2
BLEU points. As in English-Chinese, the worst
BLEU scores are obtained by SU while the best
scores are produced by SB.
As we see from the tables, the relation between
intrinsic alignment measures (AER and CPER)
3We could not run SB on the larger corpus because of the
lack of required inputs.
4Due to lack of additional training data, we could not do
experiments using different sizes of training data on English-
Arabic.
12
Original Modified Modified Modified
Alignment Max Phr Len = 3 Max Phr Len=3 Max Phr Len=7 Max Phr Len=3
|Corpus| = 107K |Corpus| = 107K |Corpus| = 107K |Corpus| = 241K
SU 22.56 22.66 23.30 24.40
SG 23.65 23.79 24.48 25.54
SI 23.60 23.97 24.76 26.06
SA 24.26 25.31 25.99 26.92
SB 23.91 25.38 26.14 N/A
Table 4: BLEU Scores on English-Chinese with Different Lexical Weightings, Maximum Phrase Lengths and Training Data
LW=Org LW=Mod LW=Mod
Alignment MPL=3 MPL=3 MPL=7
SU 41.97 41.72 43.50
SG 44.06 43.82 45.78
SI 42.29 42.76 43.88
SA 44.49 45.23 46.06
SB 44.92 45.39 46.66
Table 5: BLEU Scores on English-Arabic with Different
Lexical Weightings and Maximum Phrase Lengths
and the corresponding BLEU scores varies, de-
pending on the language, lexical weighting, maxi-
mum phrase length, and training data size. For ex-
ample, using a modified lexical weighting, the sys-
tems are ranked according to their BLEU scores as
follows: SB, SA, SG, SI, SU?an ordering that dif-
fers from that of AER but is identical to that of
CPER (with a phrase length of 3) for Chinese. On
the other hand, in Arabic, both AER and CPER
provide a slightly different ranking from that of
BLEU, with SG and SI swapping places.
4.2 Tight vs. Loose Phrases
To demonstrate how alignment-related compo-
nents of the MT system might change the trans-
lation quality significantly, we did an additional
experiment to compare different techniques for ex-
tracting phrases from a given alignment. Specifi-
cally, we are comparing two techniques for phrase
extraction:
1. Loose phrases (the original ?consistent
phrase extraction? method)
2. Tight phrases (the set of phrases where
the first/last words on each side are forced
to align to some word in the phrase pair)
Using tight phrases penalizes alignments with
many unaligned words, whereas using loose
phrases rewards them. Our goal is to compare
the performance of precision-oriented vs. recall-
oriented alignments when we allow only tight
phrases in the phrase extraction step. To sim-
plify things, we used only 2 alignments: SG, the
best recall-oriented alignment, and SB, the best
precision-oriented alignment. For this experiment,
we used modified lexical weighting and a maxi-
mum phrase length of 7.
Chinese Arabic
Alignment Loose Tight Loose Tight
SG 24.48 23.19 45.78 43.67
SB 26.14 22.68 46.66 40.10
Table 6: BLEU Scores with Loose vs. Tight Phrases
Table 6 presents the BLEU scores for SG and SB
using two different phrase extraction techniques
on English-Chinese and English-Arabic. In both
languages, SB outperforms SG significantly when
loose phrases are used. However, when we use
only tight phrases, the performance of SB gets sig-
nificantly worse (3.5 to 6.5 BLEU-score reduction
in comparison to loose phrases). The performance
of SG also gets worse but the degree of BLEU-
score reduction is less than that of SB. Overall
SG performs better than SB with tight phrases;
for English-Arabic, the difference between the two
systems is more than 3 BLEU points. Note that, as
before, the relation between the alignment mea-
sures and the BLEU scores varies, this time de-
pending on whether loose phrases or tight phrases
are used: both CPER and AER track the BLEU
rankings for loose (but not for tight) phrases.
This suggests that changing alignment-related
components of the system (i.e., phrase extraction
and phrase scoring) influences the overall trans-
lation quality significantly for a particular align-
ment. Therefore, when comparing two align-
ments in the context of a MT system, it is im-
portant to take the alignment characteristics into
account. For instance, alignments with many un-
aligned words are severely penalized when using
tight phrases.
4.3 Untranslated Words
We analyzed the percentage of words left untrans-
lated during decoding. Figure 2 shows the per-
centage of untranslated words in the FL using the
Chinese and Arabic NIST MTEval?2003 test sets.
On English-Chinese data (using all four settings
given in Table 4) SU and SG yield the highest per-
centage of untranslated words while SI produces
the lowest percentage of untranslated words. SA
and SB leave about 2% of the FL words phrases
13
Figure 2: Percentage of untranslated words out of the total
number of FL words
without translating them. Increasing the training
data size reduces the percentage of untranslated
words by nearly half with all five alignments. No
significant impact on untranslated words is ob-
served from modifying the lexical weights and
changing the phrase length.
On English-Arabic data, all alignments result
in higher percentages of untranslated words than
English-Chinese, most likely due to data spar-
sity. As in Chinese-to-English translation, SU
is the worst and SB is the best. SI behaves
quite differently, leaving nearly 7% of the words
untranslated?an indicator of why it produces a
higher BLEU score on Chinese but a lower score
on Arabic compared to other alignments.
4.4 Analysis of Phrase Tables
This section presents several experiments to an-
alyze how different alignments affect the size of
the generated phrase tables, the distribution of the
phrases that are used in decoding, and the cover-
age of the test set with the generated phrase tables.
Size of Phrase Tables The major impact of
using different alignments in a phrase-based MT
system is that each one results in a different phrase
table. Table 7 presents the number of phrases
that are extracted from five alignments using two
different maximum phrase lengths (3 vs. 7) in
two languages, after filtering the phrase table for
MTEval?2003 test set. The size of the phrase table
increases dramatically as the number of links in
the initial alignment gets smaller. As a result, for
both languages, SU and SG yield a much smaller
Chinese Arabic
Alignment MPL=3 MPL=7 MPL=3 MPL=7
SU 106 122 32 38
SG 161 181 48 55
SI 1331 3498 377 984
SA 954 1856 297 594
SB 876 1624 262 486
Table 7: Number of Phrases in the Phrase Table Filtered for
MTEval?2003 Test Sets (in thousands)
phrase table than the other three alignments. As
the maximum phrase length increases, the size of
the phrase table gets bigger for all alignments;
however, the growth of the table is more signifi-
cant for precision-oriented alignments due to the
high number of unaligned words.
Distribution of Phrases To investigate how the
decoder chooses phrases of different lengths, we
analyzed the distribution of the phrases in the fil-
tered phrase table and the phrases that were used
to decode Chinese MTEval?2003 test set.5 For the
remaining experiments in the paper, we use mod-
ified lexical weighting, a maximum phrase length
of 7, and 107K sentence pairs for training.
The top row in Figure 3 shows the distribution
of the phrases generated by the five alignments
(using a maximum phrase length of 7) according
to their length. The ?j-i? designators correspond
to the phrase pairs with j FL words and i English
words. For SU and SG, the majority of the phrases
contain only one FL word, and the percentage of
the phrases with more than 2 FL words is less than
18%. For the other three alignments, however, the
distribution of the phrases is almost inverted. For
SI, nearly 62% of the phrases contain more than 3
words on either FL or English side; for SA and SB,
this percentage is around 45-50%.
Given the completely different phrase distribu-
tion, the most obvious question is whether the
longer phrases generated by SI, SA and SB are
actually used in decoding. In order to investigate
this, we did an analysis of the phrases used to de-
code the same test set.
The bottom row of Figure 3 shows the per-
centage of phrases used to decode the Chinese
MTEval?2003 test set. The distribution of the ac-
tual phrases used in decoding is completely the re-
verse of the distribution of the phrases in the en-
tire filtered table. For all five alignments, the ma-
jority of the used phrases is one-to-one (between
5Due to lack of space, we will present results on Chinese-
English only in the rest of this paper but the Arabic-English
results show the same trends.
14
Figure 3: Distribution of the phrases in the phrase table
filtered for Chinese MTEval?2003 test set (top row) and the
phrases used in decoding the same test set (bottom row) ac-
cording to their lengths
50-65% of the total number of phrases used in de-
coding). SI, SA and SB use the other phrase pairs
(particularly 1-to-2 phrases) more than SU and SG.
Note that SI, SA and SB use only a small portion
of the phrases with more than 3 words although the
majority of the phrase table contains phrases with
more than 3 words on one side. It is surprising
that the inclusion of phrase pairs with more than
3 words in the search space increases the BLEU
score although the majority of the phrases used in
decoding is mostly one-to-one.
Length of the Phrases used in Decoding We
also investigated the number and length of phrases
that are used to decode the given test set for dif-
ferent alignments. Table 8 presents the average
number of English and FL words in the phrases
used in decoding Chinese MTEval?2003 test set.
The decoder uses fewer phrases with SI, SA and
SB than for the other two, thus yielding a higher
number of FL words per phrase. The number of
English words per phrase is also higher for these
three systems than the other two.
Coverage of the Test Set Finally, we examine
the coverage of a test set using phrases of a spe-
cific length in the phrase table. Table 9 presents
Alignment |Eng| |FL|
SU 1.39 1.28
SG 1.45 1.33
SI 1.51 1.55
SA 1.54 1.55
SB 1.56 1.52
Table 8: The average length of the phrases that are used in
decoding Chinese MTEval?2003 test set
the coverage of the Chinese MTEval?2003 test set
(source side) using only phrases of a particular
length (from 1 to 7). For this experiment, we as-
sume that a word in the test set is covered if it is
part of a phrase pair that exists in the phrase table
(if a word is part of multiple phrases, it is counted
only once). Not surprisingly, using only phrases
with one FL word, more than 90% of the test set
can be covered for all 5 alignments. As the length
of the phrases increases, the coverage of the test
set decreases. For instance, using phrases with 5
FL words results in less than 5% coverage of the
test set.
Phrase Length (FL)
A 1 2 3 4 5 6 7
SU 92.2 59.5 21.4 6.7 1.3 0.4 0.1
SG 95.5 64.4 24.9 7.4 1.6 0.5 0.3
SI 97.8 75.8 38.0 13.8 4.6 1.9 1.2
SA 97.3 75.3 36.1 12.5 3.8 1.5 0.8
SB 97.5 74.8 35.7 12.4 4.2 1.8 0.9
Table 9: Coverage of Chinese MTEval?2003 Test Set Using
Phrases with a Specific Length on FL side (in percentages)
Table 9 reveals that the coverage of the test set
is higher for precision-oriented alignments than
recall-oriented alignments for all different lengths
of the phrases. For instance, SI, SA, and SB cover
nearly 75% of the corpus using only phrases with
2 FL words, and nearly 36% of the corpus using
phrases with 3 FL words. This suggests that recall-
oriented alignments fail to catch a significant num-
ber of phrases that would be useful to decode this
test set, and precision-oriented alignments would
yield potentially more useful phrases.
Since precision-oriented alignments make a
higher number of longer phrases available to the
decoder (based on the coverage of phrases pre-
sented in Table 9), they are used more during
decoding. Consequently, the major difference
between the alignments is the coverage of the
phrases extracted from different alignments. The
more the phrase table covers the test set, the more
the longer phrases are used during decoding, and
precision-oriented alignments are better at gener-
ating high-coverage phrases than recall-oriented
alignments.
15
5 Conclusions and Future Work
This paper investigated how different alignments
change the behavior of phrase-based MT. We
showed that AER is a poor indicator of MT
performance because it penalizes incorrect links
less than is reflected in the corresponding phrase-
based MT. During phrase-based MT, an incorrect
alignment link might prevent extraction of several
phrases, but the number of phrases affected by that
link depends on the context.
We designed CPER, a new phrase-oriented met-
ric that is more informative than AER when the
alignments are used in a phrase-based MT system
because it is an indicator of how the set of phrases
differ from one alignment to the next according to
a pre-specified maximum phrase length.
Even with refined evaluation metrics (including
CPER), we found it difficult to assess the impact
of alignment on MT performance because word
alignment is not the only factor that affects the
choice of the correct words (or phrases) during
decoding. We empirically showed that different
phrase extraction techniques result in better MT
output for certain alignments but the MT perfor-
mance gets worse for other alignments. Simi-
larly, adjusting the scores assigned to the phrases
makes a significant difference for certain align-
ments while it has no impact on some others. Con-
sequently, when comparing two BLEU scores, it is
difficult to determine whether the alignments are
bad to start with or the set of extracted phrases is
bad or the phrases extracted from the alignments
are assigned bad scores. This suggests that finding
a direct correlation between AER (or even CPER)
and the automated MT metrics is infeasible.
We demonstrated that recall-oriented alignment
methods yield smaller phrase tables and a higher
number of untranslated words when compared to
precision-oriented methods. We also showed that
the phrases extracted from recall-oriented align-
ments cover a smaller portion of a given test set
when compared to precision-oriented alignments.
Finally, we showed that the decoder with recall-
oriented alignments uses shorter phrases more fre-
quently as a result of unavailability of longer
phrases that are extracted.
Future work will involve an investigation into
how the phrase extraction and scoring should be
adjusted to take the nature of the alignment into
account and how the phrase-table size might be re-
duced without sacrificing the MT output quality.
Acknowledgments This work has been supported, in
part, under ONR MURI Contract FCPO.810548265 and the
GALE program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-2-0001. We also thank
Adam Lopez for his very helpful comments on earlier drafts
of this paper.
References
Necip F. Ayan, Bonnie J. Dorr, and Christof Monz. 2005.
Neuralign: Combining word alignments using neural net-
works. In Proceedings of EMNLP?2005, pages 65?72.
Stanjeev Banerjee and Alon Lavie. 2005. Meteor: An au-
tomatic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of Work-
shop on Intrinsic and Extrinsic Evaluation Measures for
MT and/or Summarization at ACL-2005.
Peter F. Brown, Stephan A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263?311.
Chris Callison-Burch, David Talbot, and Miles Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In Proceedings of
ACL?2004.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL?2005.
Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004. Align-
ing words using matrix factorisation. In Proceedings of
ACL?2004, pages 502?509.
Abraham Ittycheriah and Salim Roukos. 2005. A maximum
entropy word aligner for arabic-english machine transla-
tion. In Proceedings of EMNLP?2005.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of HLT-
NAACL?2003.
Philipp Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation. In Proceed-
ings of AMTA?2004.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine translation.
In Proceedings of EMNLP?2002.
I. Dan Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221?
249.
Robert C. Moore. 2005. A discriminative frame-
work for bilingual word alignment. In Proceedings of
EMNLP?2005.
Franz J. Och and Hermann Ney. 2000. A comparison of
alignment models for statistical machine translation. In
Proceedings of COLING?2000.
Franz J. Och. 2000b. GIZA++: Training of statistical transla-
tion models. Technical report, RWTH Aachen, University
of Technology.
Franz J. Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):9?51, March.
Franz J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of ACL?2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a method for automatic evaluation of
machine translation. In Proceedings of ACL?2002.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A
discriminative matching approach to word alignment. In
Proceedings of EMNLP?2005.
Stefan Vogel, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
Proceedings of COLING?1996, pages 836?841.
16
Proceedings of the Second Workshop on Statistical Machine Translation, pages 120?127,
Prague, June 2007. c?2007 Association for Computational Linguistics
Using Paraphrases for Parameter Tuning in Statistical Machine Translation
Nitin Madnani, Necip Fazil Ayan, Philip Resnik & Bonnie J. Dorr
Institute for Advanced Computer Studies
University of Maryland
College Park, MD, 20742
{nmadnani,nfa,resnik,bonnie}@umiacs.umd.edu
Abstract
Most state-of-the-art statistical machine
translation systems use log-linear models,
which are defined in terms of hypothesis fea-
tures and weights for those features. It is
standard to tune the feature weights in or-
der to maximize a translation quality met-
ric, using held-out test sentences and their
corresponding reference translations. How-
ever, obtaining reference translations is ex-
pensive. In this paper, we introduce a new
full-sentence paraphrase technique, based
on English-to-English decoding with an MT
system, and we demonstrate that the result-
ing paraphrases can be used to drastically re-
duce the number of human reference transla-
tions needed for parameter tuning, without a
significant decrease in translation quality.
1 Introduction
Viewed at a very high level, statistical machine
translation involves four phases: language and trans-
lation model training, parameter tuning, decoding,
and evaluation (Lopez, 2007; Koehn et al, 2003).
Since their introduction in statistical MT by Och and
Ney (2002), log-linear models have been a standard
way to combine sub-models in MT systems. Typi-
cally such a model takes the form
?
i
?i?i(f? , e?) (1)
where ?i are features of the hypothesis e and ?i are
weights associated with those features.
Selecting appropriate weights ?i is essential
in order to obtain good translation performance.
Och (2003) introduced minimum error rate train-
ing (MERT), a technique for optimizing log-linear
model parameters relative to a measure of translation
quality. This has become much more standard than
optimizing the conditional probability of the train-
ing data given the model (i.e., a maximum likelihood
criterion), as was common previously. Och showed
that system performance is best when parameters are
optimized using the same objective function that will
be used for evaluation; BLEU (Papineni et al, 2002)
remains common for both purposes and is often re-
tained for parameter optimization even when alter-
native evaluation measures are used, e.g., (Banerjee
and Lavie, 2005; Snover et al, 2006).
Minimum error rate training?and more gener-
ally, optimization of parameters relative to a trans-
lation quality measure?relies on data sets in which
source language sentences are paired with (sets of)
reference translations. It is widely agreed that, at
least for the widely used BLEU criterion, which is
based on n-gram overlap between hypotheses and
reference translations, the criterion is most accu-
rate when computed with as many distinct reference
translations as possible. Intuitively this makes sense:
if there are alternative ways to phrase the meaning
of the source sentence in the target language, then
the translation quality criterion should take as many
of those variations into account as possible. To do
otherwise is to risk the possibility that the criterion
might judge good translations to be poor when they
fail to match the exact wording within the reference
translations that have been provided.
This reliance on multiple reference translations
creates a problem, because reference translations are
labor intensive and expensive to obtain. A com-
mon source of translated data for MT research is the
Linguistic Data Consortium (LDC), where an elab-
orate process is undertaken that involves translation
agencies, detailed translation guidelines, and qual-
ity control processes (Strassel et al, 2006). Some
120
efforts have been made to develop alternative pro-
cesses for eliciting translations, e.g., from users on
the Web (Oard, 2003) or from informants in low-
density languages (Probst et al, 2002). However,
reference translations for parameter tuning and eval-
uation remain a severe data bottleneck for such ap-
proaches.
Note, however, one crucial property of reference
translations: they are paraphrases, i.e., multiple ex-
pressions of the same meaning. Automatic tech-
niques exist for generating paraphrases. Although
one would clearly like to retain human transla-
tions as the benchmark for evaluation of translation,
might it be possible to usefully increase the number
of reference translations for tuning by using auto-
matic paraphrase techniques?
In this paper, we demonstrate that it is, in fact,
possible to do so. Section 2 briefly describes our
translation framework. Section 3 lays out a novel
technique for paraphrasing, designed with the ap-
plication to parameter tuning in mind. Section 4
presents evaluation results using a state of the art sta-
tistical MT system, demonstrating that half the hu-
man reference translations in a standard 4-reference
tuning set can be replaced with automatically gener-
ated paraphrases, with no significant decrease in MT
system performance. In Section 5 we discuss related
work, and in Section 6 we summarize the results and
discuss plans for future research.
2 Translation Framework
The work described in this paper makes use
of the Hiero statistical MT framework (Chiang,
2007). Hiero is formally based on a weighted syn-
chronous context-free grammar (CFG), containing
synchronous rules of the form
X ? ?e?, f? , ?k1(f? , e?, X)? (2)
where X is a symbol from the nonterminal alpha-
bet, and e? and f? can contain both words (terminals)
and variables (nonterminals) that serve as placehold-
ers for other phrases. In the context of statistical
MT, where phrase-based models are frequently used,
these synchronous rules can be interpreted as pairs
of hierarchical phrases. The underlying strength
of a hierarchical phrase is that it allows for effec-
tive learning of not only the lexical re-orderings, but
phrasal re-orderings, as well. Each ?(e?, f? , X) de-
notes a feature function defined on the pair of hierar-
chical phrases.1 Feature functions represent condi-
tional and joint co-occurrence probabilities over the
hierarchical paraphrase pair.
The Hiero framework includes methods to learn
grammars and feature values from unannotated par-
allel corpora, without requiring syntactic annotation
of the data. Briefly, training a Hiero model proceeds
as follows:
? GIZA++ (Och and Ney, 2000) is run on the
parallel corpus in both directions, followed by
an alignment refinement heuristic that yields a
many-to-many alignment for each parallel sen-
tence.
? Initial phrase pairs are identified following the
procedure typically employed in phrase based
systems (Koehn et al, 2003; Och and Ney,
2004).
? Grammar rules in the form of equation (2)
are induced by ?subtracting? out hierarchical
phrase pairs from these initial phrase pairs.
? Fractional counts are assigned to each pro-
duced rule:
c(X ? ?e?, f??) =
m?
j=1
1
njr
(3)
where m is the number of initial phrase pairs
that give rise to this grammar rule and njr is
the number of grammar rules produced by the
jth initial phrase pair.
? Feature functions ?k1(f? , e?, X) are calculated
for each rule using the accumulated counts.
Once training has taken place, minimum error rate
training (Och, 2003) is used to tune the parameters
?i.
Finally, decoding in Hiero takes place using a
CKY synchronous parser with beam search, aug-
mented to permit efficient incorporation of language
model scores (Chiang, 2007). Given a source lan-
guage sentence f, the decoder parses the source lan-
guage sentence using the grammar it has learned
1Currently only one nonterminal symbol is used in Hiero
productions.
121
during training, with parser search guided by the
model; a target-language hypothesis is generated
simultaneously via the synchronous rules, and the
yield of that hypothesized analysis represents the hy-
pothesized string e in the target language.
3 Generating Paraphrases
As discussed in Section 1, our goal is to make it pos-
sible to accomplish the parameter-tuning phase us-
ing fewer human reference translations. We accom-
plish this by beginning with a small set of human
reference translations for each sentence in the devel-
opment set, and expanding that set by automatically
paraphrasing each member of the set rather than by
acquiring more human translations.
Most previous work on paraphrase has focused
on high quality rather than coverage (Barzilay and
Lee, 2003; Quirk et al, 2004), but generating ar-
tificial references for MT parameter tuning in our
setting has two unique properties compared to other
paraphrase applications. First, we would like to ob-
tain 100% coverage, in order to avoid modifications
to our minimum error rate training infrastructure.2
Second, we prefer that paraphrases be as distinct as
possible from the original sentences, while retaining
as much of the original meaning as possible.
In order to satisfy these two properties, we ap-
proach sentence-level paraphrase for English as
a problem of English-to-English translation, con-
structing the model using English-F translation, for
a second language F , as a pivot. Following Ban-
nard and Callison-Burch (2005), we first identify
English-to-F correspondences, then map from En-
glish to English by following translation units from
English to F and back. Then, generalizing their ap-
proach, we use those mappings to create a well de-
fined English-to-English translation model. The pa-
rameters of this model are tuned using MERT, and
then the model is used in an the (unmodified) sta-
tistical MT system, yielding sentence-level English
paraphrases by means of decoding input English
sentences. The remainder of this section presents
this process in detail.
2Strictly speaking, this was not a requirement of the ap-
proach, but rather a concession to practical considerations.
3.1 Mapping and Backmapping
We employ the following strategy for the induction
of the required monolingual grammar. First, we train
the Hiero system in standard fashion on a bilingual
English-F training corpus. Then, for each exist-
ing production in the resulting Hiero grammar, we
create multiple new English-to-English productions
by pivoting on the foreign hierarchical phrase in the
rule. For example, assume that we have the follow-
ing toy grammar for English-F , as produced by Hi-
ero:
X ? ?e?1, f?1?
X ? ?e?3, f?1?
X ? ?e?1, f?2?
X ? ?e?2, f?2?
X ? ?e?4, f?2?
If we use the foreign phrase f?1 as a pivot and
backmap, we can extract the two English-to-English
rules: X ? ?e?1, e?3? and X ? ?e?3, e?1?. Backmap-
ping using both f?1 and f?2 produces the following
new rules (ignoring duplicates and rules that map
any English phrase to itself):
X ? ?e?1, e?2?
X ? ?e?1, e?3?
X ? ?e?1, e?4?
X ? ?e?2, e?1?
X ? ?e?2, e?4?
3.2 Feature values
Each rule production in a Hiero grammar is
weighted by several feature values defined on the
rule themselves. In order to perform accurate
backmapping, we must recompute these feature
functions for the newly created English-to-English
grammar. Rather than computing approximations
based on feature values already existing in the bilin-
gual Hiero grammar, we calculate these features
in a more principled manner, by computing max-
imum likelihood estimates directly from the frac-
tional counts that Hiero accumulates in the penul-
timate training step.
We use the following features in our induced
English-to-English grammar:3
3Hiero also uses lexical weights (Koehn et al, 2003) in both
122
? The joint probability of the two English hierar-
chical paraphrases, conditioned on the nonter-
minal symbol, as defined by this formula:
p(e?1, e?2|x) =
c(X ? ?e?1, e?2?)
?
e?1?,e?2? c(X ? ?e?1
?, e?2??)
=
c(X ? ?e?1, e?2?)
c(X)
(4)
where the numerator is the fractional count of
the rule under consideration and the denomina-
tor represents the marginal count over all the
English hierarchical phrase pairs.
? The conditionals p(e?1, x|e?2) and p(e?2, x|e?1)
defined as follows:
p(e?1, x|e?2) =
c(X ? ?e?1, e?2?)
?
e?1? c(X ? ?e?1
?, e?2?)
(5)
p(e?2, x|e?1) =
c(X ? ?e?1, e?2?)
?
e?2? c(X ? ?e?1, e?2
??)
(6)
Finally, for all induced rules, we calculate a word
penalty exp(?T (e?2)), where T (e?2) just counts the
number of terminal symbols in e?2. This feature al-
lows the model to learn whether it should produce
shorter or longer paraphrases.
In addition to the features above that are estimated
from the training data, we also use a trigram lan-
guage model. Since we are decoding to produce
English sentences, we can use the same language
model employed in a standard statistical MT setting.
Calculating the proposed features is complicated
by the fact that we don?t actually have the counts
for English-to-English rules because there is no
English-to-English parallel corpus. This is where
the counts provided by Hiero come into the picture.
We estimate the counts that we need as follows:
c(X ? ?e?1, e?2?) =
?
f?
c(X ? ?e?1, f??)c(X ? ?e?2, f??) (7)
An intuitive way to think about the formula above
is by using an example at the corpus level. As-
sume that, in the given bilingual parallel corpus,
there are m sentences in which the English phrase
directions as features but we don?t use them for our grammar.
e?1 co-occurs with the foreign phrase f? and n sen-
tences in which the same foreign phrase f? co-occurs
with the English phrase e?2. The problem can then
be thought of as defining a function g(m,n) which
computes the number of sentences in a hypotheti-
cal English-to-English parallel corpus wherein the
phrases e?1 and e?1 co-occur. For this paper, we de-
fine g(m,n) to be the upper bound mn.
Tables 1 and 2 show some examples of para-
phrases generated by our system across a range of
paraphrase quality for two different pivot languages.
3.3 Tuning Model Parameters
Although the goal of the paraphrasing approach
is to make it less data-intensive to tune log-linear
model parameters for translation, our paraphrasing
approach, since it is based on an English-to-English
log-linear model, also requires its own parameter
tuning. This, however, is straightforward: regard-
less of how the paraphrasing model will be used
in statistical MT, e.g., irrespective of source lan-
guage, it is possible to use any existing set of English
paraphrases as the tuning set for English-to-English
translation. We used the 2002 NIST MT evaluation
test set reference translations. For every item in the
set, we randomly chose one sentence as the source
sentence, and the remainder as the ?reference trans-
lations? for purposes of minimum error rate training.
4 Evaluation
Having developed a paraphrasing approach based on
English-to-English translation, we evaluated its use
in improving minimum error rate training for trans-
lation from a second language into English.
Generating paraphrases via English-to-English
translation makes use of a parallel corpus, from
which a weighted synchronous grammar is automat-
ically acquired. Although nothing about our ap-
proach requires that the paraphrase system?s training
bitext be the same one used in the translation exper-
iments (see Section 6), doing so is not precluded, ei-
ther, and it is a particularly convenient choice when
the paraphrasing is being done in support of MT.4
The training bitext comprised of Chinese-English
4The choice of the foreign language used as the pivot should
not really matter but it is worth exploring this using other lan-
guage pairs as our bitext.
123
O: we must bear in mind the community as a whole .
P: we must remember the wider community .
O: thirdly , the implications of enlargement for the union ?s regional policy cannot be overlooked .
P: finally , the impact of enlargement for eu regional policy cannot be ignored .
O: how this works in practice will become clear when the authority has to act .
P: how this operate in practice will emerge when the government has to play .
O: this is an ill-advised policy .
P: this is an unwelcome in europe .
Table 1: Example paraphrases with French as the pivot language. O = Original Sentence, P = Paraphrase.
O: alcatel added that the company?s whole year earnings would be announced on february 4 .
P: alcatel said that the company?s total annual revenues would be released on february 4 .
O: he was now preparing a speech concerning the us policy for the upcoming world economic forum .
P: he was now ready to talk with regard to the us policies for the forthcoming international economic forum .
O: tibet has entered an excellent phase of political stability, ethnic unity and people living in peace .
P: tibetans have come to cordial political stability, national unity and lived in harmony .
O: its ocean and blue-sky scenery and the mediterranean climate make it world?s famous scenic spot .
P: its harbour and blue-sky appearance and the border situation decided it world?s renowned tourist attraction .
Table 2: Example paraphrases with Chinese as the pivot language. O = Original Sentence, P = Paraphrase.
Corpus # Sentences # Words
HK News 542540 11171933
FBIS 240996 9121210
Xinhua 54022 1497562
News1 9916 314121
Treebank 3963 125848
Total 851437 22230674
Table 3: Chinese-English corpora used as training
bitext both for paraphrasing and for evaluation.
parallel corpora containing 850, 000 sentence pairs ?
approx. 22 million words (details shown in Table 3).
As the source of development data for minimum
error rate training, we used the 919 source sen-
tences and human reference translations from the
2003 NIST Chinese-English MT evaluation exer-
cise. As raw material for experimentation, we gen-
erated a paraphrase for each reference sentence via
1-best decoding using the English-to-English trans-
lation approach of Section 3.
As our test data, we used the 1082 source sen-
tences and human reference translations from the
2005 NIST Chinese-English MT evaluation.
Our core experiment involved three conditions
where the only difference was the set of references
for the development set used for tuning feature
weights. For each condition, once the weights were
tuned, they were used to decode the test set. Note
that for all the conditions, the decoded test set was
always scored against the same four high-quality hu-
man reference translations included with the set.
The three experimental conditions were designed
around the constraint that our development set con-
tains a total of four human reference translations per
sentence, and therefore a maximum of four human
references with which to compute an upper bound:
? Baseline (2H): For each item in the devel-
opment set, we randomly chose two of the
four human-constructed reference translations
as references for minimum error rate training.
? Expanded (2H + 2P): For each of the two hu-
man references in the baseline tuning set, we
automatically generated a corresponding para-
phrase using (1-best) English-to-English trans-
lation, decoding using the model developed in
Section 3. This condition represents the critical
case in which you have a limited number of hu-
124
man references (two, in this case) and augment
them with artificially generated reference trans-
lations. This yields a set of four references for
minimum error rate training (two human, two
paraphrased), which permits a direct compar-
ison against the upper bound of four human-
generated reference translations.
? Upper bound: 4H: We performed minimum
error rate training using the four human refer-
ences from the development set.
In addition to these core experimental conditions,
we added a fourth condition to assess the effect on
performance when all four human reference trans-
lations are used in expanding the reference set via
paraphrase:
? Expanded (4H + 4P): This is the same as Con-
dition 2, but using all four human references.
Note that since we have only four human references
per item, this fourth condition does not permit com-
parison with an upper bound of eight human refer-
ences.
Table 4 shows BLEU and TER scores on the test
set for all four conditions.5 If only two human ref-
erences were available (simulated by using only two
of the available four), expanding to four using para-
phrases would yield a clear improvement. Using
bootstrap resampling to compute confidence inter-
vals (Koehn, 2004), we find that the improvement in
BLEU score is statistically significant at p < .01.
Equally interesting, expanding the number of ref-
erence translations from two to four using para-
phrases yields performance that approaches the up-
per bound obtained by doing MERT using all four
human reference translations. The difference in
BLEU between conditions 2 and 3 is not significant.
Finally, our fourth condition asks whether it is
possible to improve MT performance given the
typical four human reference translations used for
MERT in most statistical MT systems, by adding a
paraphrase to each one for a total eight references
per translation. There is indeed further improve-
ment, although the difference in BLEU score does
not reach significance.
5We plan to include METEOR scores in future experiments.
Condition References used BLEU TER
1 2 H 30.43 59.82
2 2 H + 2 P 31.10 58.79
3 4 H 31.26 58.66
4 4 H + 4 P 31.68 58.24
Table 4: BLEU and TER scores showing utility of
paraphrased reference translations. H = human ref-
erences, P = paraphrased references.
We also evaluated our test set using TER (Snover
et al, 2006) and observed that the TER scores follow
the same trend as the BLEU scores. Specifically, the
TER scores demonstrate that using paraphrases to
artificially expand the reference set is better than us-
ing only 2 human reference translations and as good
as using 4 human reference translations.6
5 Related Work
The approach we have taken here arises from a typ-
ical situation in NLP systems: the lack of sufficient
data to accurately estimate a model based on super-
vised training data. In a structured prediction prob-
lem such as MT, we have an example input and a
single labeled, correct output. However, this output
is chosen from a space in which the number of pos-
sible outputs is exponential in the input size, and in
which there are many good outputs in this space (al-
though they are vastly outnumbered by the bad out-
puts). Various discriminative learning methods have
attempted to deal with the first of these issues, often
by restricting the space of examples. For instance,
some max-margin methods restrict their computa-
tions to a set of examples from a ?feasible set,?
where they are expected to be maximally discrim-
inative (Tillmann and Zhang, 2006). The present
approach deals with the second issue: in a learning
problem where the use of a single positive example
is likely to be highly biased, how can we produce a
set of positive examples that is more representative
of the space of correct outcomes? Our method ex-
ploits alternative sources of information to produce
new positive examples that are, we hope, reasonably
likely to represent a consensus of good examples.
Quite a bit of work has been done on paraphrase,
6We anticipate doing significance tests for differences in
TER in future work.
125
some clearly related to our technique, although in
general previous work has been focused on human
readability rather than high coverage, noisy para-
phrases for use downstream in an automatic process.
At the sentence level, (Barzilay and Lee, 2003)
employed an unsupervised learning approach to
cluster sentences and extract lattice pairs from
comparable monolingual corpora. Their technique
produces a paraphrase only if the input sentence
matches any of the extracted lattice pairs, leading to
a bias strongly favoring quality over coverage. They
were able to generate paraphrases for 59 sentences
(12%) out of a 484-sentence test set, generating no
paraphrases at all for the remainder.
Quirk et al (2004) also generate sentential para-
phrases using a monolingual corpus. They use
IBM Model-1 scores as the only feature, and em-
ploy a monotone decoder (i.e., one that cannot pro-
duce phrase-level reordering). This approach em-
phasizes very simple ?substitutions of words and
short phrases,? and, in fact, almost a third of their
best sentential ?paraphrases? are identical to the in-
put sentence.
A number of other approaches rely on parallel
monolingual data and, additionally, require pars-
ing of the training sentences (Ibrahim et al, 2003;
Pang et al, 2003). Lin and Pantel (2001) use a
non-parallel corpus and employ a dependency parser
and computation of distributional similarity to learn
paraphrases.
There has also been recent work on using para-
phrases to improve statistical machine translation.
Callison-Burch et al (2006) extract phrase-level
paraphrases by mapping input phrases into a phrase
table and then mapping back to the source language.
However, they do not generate paraphrases of entire
sentences, but instead employ paraphrases to add en-
tries to an existing phrase table solely for the pur-
pose of increasing source-language coverage.
Other work has incorporated paraphrases into MT
evaluation: Russo-Lassner et al (2005) use a com-
bination of paraphrase-based features to evaluate
translation output; Zhou et al (2006) propose a new
metric that extends n-gram matching to include syn-
onyms and paraphrases; and Lavie?s METEOR met-
ric (Banerjee and Lavie, 2005) can be used with ad-
ditional knowledge such as WordNet in order to sup-
port inexact lexical matches.
6 Conclusions and Future Work
We introduced an automatic paraphrasing technique
based on English-to-English translation of full sen-
tences using a statistical MT system, and demon-
strated that, using this technique, it is possible to
cut in half the usual number of reference transla-
tions used for minimum error rate training with no
significant loss in translation quality. Our method
enables the generation of paraphrases for thousands
of sentences in a very short amount of time (much
shorter than creating other low-cost human refer-
ences). This might prove beneficial for various dis-
criminative training methods (Tillmann and Zhang,
2006).
This has important implications for data acquisi-
tion strategies For example, it suggests that rather
than obtaining four reference translations per sen-
tence for development sets, it may be more worth-
while to obtain fewer translations for a wider range
of sentences, e.g., expanding into new topics and
genres. In addition, this approach can significantly
increase the utility of datasets which include only a
single reference translation.
A number of future research directions are pos-
sible. First, since we have already demonstrated
that noisy paraphrases can nonetheless add value,
it would be straightforward to explore the quan-
tity/quality tradeoff by expanding the MERT refer-
ence translations with n-best paraphrases for n > 1.
We also plan to conduct an intrinsic evaluation of
the quality of paraphrases that our technique gener-
ates. It is important to note that a different tradeoff
ratio may lead to even better results, e.g, using only
the paraphrased references when they pass some
goodness threshold, as used in Ueffing?s (2006) self-
training MT approach.
We have also observed that named entities are
usually paraphrased incorrectly if there is a genre
mismatch between the training and the test data. The
Hiero decoder allows spans of source text to be an-
notated with inline translations using XML. We plan
to identify and annotate named entities in the En-
glish source so that they are left unchanged.
Also, since the languageF for English-F pivoting
is arbitrary, we plan to investigate using English-to-
English grammars created using multiple English-F
grammars based on different languages, both indi-
126
vidually and in combination, in order to improve
paraphrase quality.
We also plan to explore a wider range of
paraphrase-creation techniques, ranging from sim-
ple word substitutions (e.g., based on WordNet) to
using the pivot technique with other translations sys-
tems.
7 Acknowledgments
We are indebted to David Chiang, Adam Lopez
and Smaranda Muresan for insights and comments.
This work has been supported under the GALE pro-
gram of the Defense Advaned Research Projects
Agency, Contract No. HR0011-06-2-001. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this paper are those of the authors and do
not necessarily reflect the view of DARPA.
References
S. Banerjee and A. Lavie. 2005. Meteor: An auto-
matic metric for mt evaluation with improved correla-
tion with human judgments. In Proceedings of Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization at ACL.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of HLT-NAACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
A. Ibrahim, B. Katz, and J. Lin. 2003. Extracting struc-
tural paraphrases from aligned monolingual corpora.
In Proceedings the Second International Workshop on
Paraphrasing (ACL 2003).
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT-NAACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
A. Lopez. 2007. A survey of statistical machine transla-
tion. Technical Report 2006-47, University of Mary-
land, College Park.
D. W. Oard. 2003. The surprise langauge exercises.
ACM Transactions on Asian Language Information
Processing, 2(3).
Franz J. Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of ACL.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL.
Franz Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Com-
putational Linguistics, 30(4).
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
K. Probst, L. Levin, E. Peterson, A. Lavie, and J. Car-
bonell. 2002. Mt for minority languages using
elicitation-based learning of syntactic transfer rules.
Machine Translation, 17(4).
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP 2004.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2005. A paraphrase-based approach to machine trans-
lation evaluation. Technical Report UMIACS-TR-
2005-57, University of Maryland, College Park.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of AMTA.
S. Strassel, C. Cieri, A. Cole, D. DiPersio, M. Liberman,
X. Ma, M. Maamouri, and K. Maeda. 2006. Inte-
grated linguistic resources for language exploitation
technologies. In Proceedings of LREC.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proceedings of ACL.
Nicola Ueffing. 2006. Using monolingual source-
language data to improve MT performance. In Pro-
ceedings of IWSLT.
L. Zhou, C.-Y. Lin, D. Muntenau, and E. Hovy. 2006.
ParaEval: Using paraphrases to evaluate summaries
automatically. In Proceedings of HLT-NAACL.
127
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 33?40
Manchester, August 2008
Improving Alignments for Better Confusion Networks
for Combining Machine Translation Systems
Necip Fazil Ayan and Jing Zheng and Wen Wang
SRI International
Speech Technology and Research Laboratory (STAR)
333 Ravenswood Avenue
Menlo Park, CA 94025
{nfa,zj,wwang}@speech.sri.com
Abstract
The state-of-the-art system combination
method for machine translation (MT) is
the word-based combination using confu-
sion networks. One of the crucial steps in
confusion network decoding is the align-
ment of different hypotheses to each other
when building a network. In this paper, we
present new methods to improve alignment
of hypotheses using word synonyms and a
two-pass alignment strategy. We demon-
strate that combination with the new align-
ment technique yields up to 2.9 BLEU
point improvement over the best input sys-
tem and up to 1.3 BLEU point improve-
ment over a state-of-the-art combination
method on two different language pairs.
1 Introduction
Combining outputs of multiple systems perform-
ing the same task has been widely explored in
various fields such as speech recognition, word
sense disambiguation, and word alignments, and it
had been shown that the combination approaches
yielded significantly better outputs than the in-
dividual systems. System combination has also
been explored in the MT field, especially with
the emergence of various structurally different MT
systems. Various techniques include hypothesis
selection from different systems using sentence-
level scores, re-decoding source sentences using
phrases that are used by individual systems (Rosti
et al, 2007a; Huang and Papineni, 2007) and
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
word-based combination techniques using confu-
sion networks (Matusov et al, 2006; Sim et al,
2007; Rosti et al, 2007b). Among these, confu-
sion network decoding of the system outputs has
been shown to be more effective than the others in
terms of the overall translation quality.
One of the crucial steps in confusion network
decoding is the alignment of hypotheses to each
other because the same meaning can be expressed
with synonymous words and/or with a different
word ordering in different hypotheses. Unfortu-
nately, all the alignment algorithms used in confu-
sion network decoding are insensitive to synonyms
of words when aligning two hypotheses to each
other. This paper extends the previous alignment
approaches to handle word synonyms more effec-
tively to improve alignment of different hypothe-
ses. We also present a two-pass alignment strategy
for a better alignment of hypotheses with similar
words but with a different word ordering.
We evaluate our system combination approach
using variants of an in-house hierarchical MT sys-
tem as input systems on two different language
pairs: Arabic-English and Chinese-English. Even
with very similar MT systems as inputs, we show
that the improved alignments yield up to an abso-
lute 2.9 BLEU point improvement over the best
input system and up to an absolute 1.3 BLEU
point improvement over the old alignments in a
confusion-network-based combination.
The rest of this paper is organized as follows.
Section 2 presents an overview of previous sys-
tem combination techniques for MT. Section 3 dis-
cusses the confusion-network-based system com-
bination. In Section 4, we present the new hy-
pothesis alignment techniques. Finally, Section 5
presents our experiments and results on two lan-
guage pairs.
33
2 Related Work
System combination for machine translation can
be done at three levels: Sentence-level, phrase-
level or word-level.
Sentence-level combination is done by choosing
one hypothesis amongmultipleMT system outputs
(and possibly among n-best lists). The selection
criterion can be a combination of translation model
and language model scores with multiple compar-
ison tests (Akiba et al, 2002), or statistical confi-
dence models (Nomoto, 2004).
Phrase-level combination systems assume that
the input systems provide some internal informa-
tion about the system, such as phrases used by the
system, and the task is to re-decode the source sen-
tence using this additional information. The first
example of this approach was the multi-engine MT
system (Frederking and Nirenburg, 1994), which
builds a chart using the translation units inside
each input system and then uses a chart walk algo-
rithm to find the best cover of the source sentence.
Rosti et al (2007a) collect source-to-target corre-
spondences from the input systems, create a new
translation option table using only these phrases,
and re-decode the source sentence to generate bet-
ter translations. In a similar work, it has been
demonstrated that pruning the original phrase ta-
ble according to reliable MT hypotheses and en-
forcing the decoder to obey the word orderings in
the original system outputs improves the perfor-
mance of the phrase-based combination systems
(Huang and Papineni, 2007). In the absence of
source-to-target phrase alignments, the sentences
can be split into simple chunks using a recursive
decomposition as input to MT systems (Mellebeek
et al, 2006). With this approach, the final output
is a combination of the best chunk translations that
are selected by majority voting, system confidence
scores and language model scores.
The word-level combination chooses the best
translation units from different translations and
combine them. The most popular method for
word-based combination follows the idea behind
the ROVER approach for combining speech recog-
nition outputs (Fiscus, 1997). After reordering
hypotheses and aligning to each other, the com-
bination system builds a confusion network and
chooses the path with the highest score. The fol-
lowing section describes confusion-network-based
system combination in detail.
? 2005 SRI International
Confusion Network ExampleHypothesis 1: she went homeHypothesis 2: she was at schoolHypothesis 3: at home was sheshe school#eps# homeatwentwasshe homewas school#eps#atwent she#eps#was#eps# wentat#eps# home #eps#school<s><s><s> </s></s></s>
Figure 1: Alignment of three hypotheses to each
other using different hypotheses as skeletons.
3 System Combination with Confusion
Networks
The general architecture of a confusion-network-
based system combination is as follows:
1. Extract n-best lists from MT systems.
2. Pick a skeleton translation for each segment.
3. Reorder all the other hypotheses by aligning
them to the skeleton translation.
4. Build a confusion network from the re-
ordered translations for each segment.
5. Decode the confusion network using vari-
ous arc features and sentence-level scores
such as LM score and word penalty.
6. Optimize feature weights on a held-out test
set and re-decode.
In this framework, the success of confusion net-
work decoding for system combination depends on
two important choices: Selection of the skeleton
hypothesis and alignment of other hypotheses to
the skeleton.
For selecting the best skeleton, two common
methods are choosing the hypothesis with the Min-
imum Bayes Risk with translation error rate (TER)
(Snover et al, 2006) (i.e., the hypothesis with the
minimum TER score when it is used as the ref-
erence against the other hypotheses) (Sim et al,
2007) or choosing the best hypotheses from each
system and using each of those as a skeleton in
multiple confusion networks (Rosti et al, 2007b).
In this paper, we use the latter since it performs
slightly better than the first method in our exper-
iments. An example confusion network on three
translations is presented in Figure 1.
1
The major difficulty when using confusion net-
works for system combination for MT is aligning
different hypotheses to the skeleton since the word
1
In this paper, we use multiple confusion networks that are
attached to the same start and end node. Throughout the rest
of the paper, the term confusion network refers to one network
among multiple networks used for system combination.
34
order might be different in different hypotheses
and it is hard to align words that are shifted from
one hypothesis to another. Four popular methods
to align hypotheses to each other are as follows:
1. Multiple string-matching algorithm based
on Levenshtein edit distance (Bangalore et
al., 2001)
2. A heuristic-based matching algorithm (Ja-
yaraman and Lavie, 2005)
3. Using GIZA++ (Och and Ney, 2000) with
possibly additional training data (Matusov
et al, 2006)
4. Using TER (Snover et al, 2006) between
the skeleton and a given hypothesis (Sim et
al., 2007; Rosti et al, 2007b)
None of these methods takes word synonyms
into account during alignment of hypotheses.
2
In
this work, we extend the TER-based alignment
to use word stems and synonyms using the pub-
licly available WordNet resource (Fellbaum, 1998)
when aligning hypotheses to each other and show
that this additional information improves the align-
ment and the overall translation significantly.
4 Confusion Networks with Word
Synonyms and Two-pass Alignment
When building a confusion network, the goal is to
put the same words on the same arcs as much as
possible. Matching similar words between two hy-
potheses is necessary to achieve this goal.
When we align two different hypotheses using
TER, it is necessary that two words have the iden-
tical spelling to be considered a match. However,
in natural languages, it is possible to represent the
same meaning using synonyms of words in pos-
sibly different positions. For example, in the fol-
lowing sentences, ?at the same time? and ?in the
meantime?, ?waiting for? and ?expect?, and ?set?
and ?established? correspond to each other, re-
spectively:
Skeleton: at the same time expect israel
to abide by the deadlines set by .
Hypothesis: in the meantime , we are
waiting for israel to abide by the
established deadlines .
Using TER, synonymous words might be
aligned to each other if they appear in the same po-
2
Note that the approach by Matusov et al (2006) at-
tempts to align synonyms and different morphological forms
of words to each other but this is done implicitly, relying on
the parallel text to learn word alignments.
sition in two hypotheses but this is less likely when
two words appear in different positions. With-
out knowing that two words are synonyms of each
other, they are considered two separate words dur-
ing TER alignment.
Our goal is to create equivalence classes for
each word in the given translations and modify the
alignment algorithm to give priority to the match-
ing of words that are in the same equivalence class.
In this paper, the equivalence classes are generated
using WordNet by extracting synonyms of each
word in the translations.
To incorporate matching of word synonyms into
the alignment, we followed three steps:
1. Use WordNet to extract synonyms of the
words that appear in all hypotheses.
2. Augment each skeleton word with all syn-
onymous words that appear in all the hy-
potheses.
3. Modify TER script to handle words with
alternatives using an additional synonym
matching operation.
In the following subsections, we describe how
each of these tasks is performed.
4.1 Extracting Synonyms from WordNet
The first step is to use WordNet to extract syn-
onyms of each word that appears in all hypotheses.
This is simply done using the publicly available
WordNet processing tools to extract all synonyms
of the given word. To allow matching words that
have the same stem or variations of the same word
with different part-of-the-speech (POS) tags, we
extract all synonyms of the given word regardless
of their POS tag in the given translation.
3
In the example above, it is clear that the verbs
wait and expect have the same meaning but TER
is unable to align these two words to each other
because of different word positions. Using Word-
Net, we get the following two entries for the words
waiting and expect, indicating that two words can
be considered as synonyms of each other:
expect: Sense 1: expect, anticipate ? evalu-
ate, pass judgment, judge. Sense 2: ask, require,
expect ? demand. Sense 3: expect, look, await,
wait. Sense 4: expect ? see, consider, reckon,
3
Our goal is to add as many synonyms as possible to in-
crease the chances of a word aligning to one of its possible
synonyms rather than to any other word. Therefore, we do
not distinguish between the synonyms of the same word ac-
cording to their confidence value or their POS tag.
35
ONE-PASS ALIGNMENT
khaddam receives italian house speaker
khaddam receives the head of the italian chamber of deputies
khaddam will meet with the president of the italian chamber of deputies
TWO-PASS ALIGNMENT
khaddam receives italian house speaker
khaddam receives of the head italian chamber of the deputies
khaddam will meet with president of the italian chamber of the deputies
Figure 2: Example alignment of three hypotheses with one-pass and two-pass alignment
view, regard. Sense 5: expect ? expect, look,
await, wait. Sense 6: have a bun in the oven, bear,
carry, gestate, expect ? give birth, deliver, bear,
birth, have.
waiting: Sense 1: wait. Sense 2: wait, hold
off, hold back ? act, move. Sense 3: wait, expect,
look, await. Sense 4: wait, waitress ? work.
We should note that WordNet contains only
open-class words, i.e., nouns, verbs, adjectives,
and adverbs. There are no entries for determin-
ers, prepositions, pronouns, conjunctions, and par-
ticles. For better matching of these additional POS
tags, we manually created a different equivalence
class for each POS tag that is not included in the
WordNet so that words with the same POS tag can
be considered synonymous.
4.2 Augmenting References
After extracting the synonyms of each word in the
given translations, the next step is to augment each
reference word with its synonyms. To avoid over-
generation of synonyms, we make the assumption
that words w
i
and w
j
are synonyms of each other
only if w
i
appears in the synonym list of w
j
, and
w
j
appears in the synonym list of w
i
. To make
the alignment task more efficient and faster, we re-
strict the synonym list to only words that appear
in the given translations. In our running exam-
ple, the augmented (extended) skeleton according
to the second hypothesis is as follows:
Extended skeleton: at the same time meantime
expect waiting israel to abide by the
deadlines set established by .
4.3 Modifications to TER Script
The final step is to modify TER script to favor
matching of a word to its synonyms rather than to
any other word. To achieve this goal, we modi-
fied the publicly available TER script, TERCOM
(Snover et al, 2006), to match words in the same
equivalence class at an additional synonym cost.
In its original implementation, TERCOM builds a
hash table for the n-grams that appear in both the
reference and the hypothesis translation to deter-
mine possible shifts of words. To allow synony-
mous words to be shifted and aligned to each other,
we extend the hash table for all possible synonyms
of words in the skeleton. Formally, if the skeleton
includes two consecutive words w
i
s
i
and w
j
s
j
,
where s
i
(s
j
) is a synonym of w
i
(w
j
), we put
all four possible combinations to the hash table:
w
i
w
j
, w
i
s
j
, s
i
w
j
, and s
i
s
j
.
4
To give higher priority to the exact matching
of words (which has zero cost during edit dis-
tance computation), we used a slightly higher cost
for synonym matching, a cost of 0.1.
5
All the
other operations (i.e., insertion, deletion, substitu-
tion and shifting of words) have a cost of 1.0.
4.4 Two-pass Alignment Strategy
When building a confusion network, the usual
strategy is first to align each hypothesis to the
skeleton separately and reorder them so that the
word ordering in the given hypothesis matches the
word ordering in the skeleton translation. Next a
confusion network is built between all these re-
ordered hypotheses.
One of the major problems with this process oc-
curs when the hypotheses include additional words
that do not appear in the skeleton translation, as
depicted in Figure 2. Since the alignments of two
different hypotheses are done independently, two
hypotheses other than the skeleton may not align
perfectly, especially when the additional words ap-
pear in different positions.
To overcome this issue, we employ a two-pass
alignment strategy. In the first pass, we align all
hypotheses to the skeleton independently and build
a confusion network. Next an intermediate refer-
ence sentence is created from the confusion net-
work generated in the first pass. To create this in-
termediate reference, we find the best position for
each word that appears in the confusion network
4
Note that the hash table is built in an iterative fashion.
We consider adding a new n-gram only if the previous n? 1
words appear in the hypothesis as well.
5
Synonym matching cost was determined empirically, try-
ing different costs from 0 to 0.5.
36
WITHOUT SYNONYM MATCHING and ONE-PASS ALIGNMENT:
at the same time expect israel to abide by
at the same time we expect israel to abide by
at the same time , we are waiting for israel to abide by
at the same time we expect israel to abide by
at the same time , we expect israel to abide by
at the same time , waiting for israel to comply with
in the meantime , waiting for israel to abide by
WITH SYNONYM MATCHING and TWO-PASS ALIGNMENT:
at the same time expect israel to abide by
at the same time we expect israel to abide by
at the same time , we are waiting for israel to abide by
at the same time we expect israel to abide by
at the same time , we expect israel to abide by
at the same time , waiting for israel to comply with
in the meantime , waiting for israel to abide by
Figure 3: Example alignment via confusion networks with and without synonym matching and two-pass
alignment (using the first sentence as the skeleton)
using majority voting. The second pass uses this
intermediate reference as the skeleton translation
to generate the final confusion network.
When we create the intermediate reference, the
number of positions for a given word is bounded
by the maximum number of occurrences of the
same word in any hypothesis. It is possible that
two different words are mapped to the same po-
sition in the intermediate reference. If this is the
case, these words are treated as synonyms when
building the second confusion network, and the in-
termediate reference looks like the extended refer-
ence in Section 4.2.
Finally, Figure 3 presents our running example
with the old alignments versus the alignments with
synonym matching and two-pass alignment.
4.5 Features
Each word in the confusion network is represented
by system-specific word scores. For computing
scores, each hypothesis is assigned a score based
on three different methods:
1. Uniform weighting: Each hypothesis in the
n-best list has the same score of 1/n.
2. Rank-based weighting: Each hypothesis is
assigned a score of 1/(1+r), where r is the
rank of the hypothesis.
3. TM-based weighting: Each hypothesis is
weighted by the score that is assigned to the
hypothesis by the translation model.
The total score of an arc with word w for a given
system S is the sum of all the scores of the hy-
potheses in system S that contain the word w in
the given position. The score for a specific arc be-
tween nodes n
i
and n
j
is normalized by the sum of
the scores for all the arcs between n
i
and n
j
.
Our experiments demonstrated that rank-based
weighting performs the best among all three
weighting methods although the differences are
small. In the rest of the paper, we report only the
results with rank-based weighting.
Besides the arc scores, we employ the following
features during decoding:
1. Skeleton selection features for each system,
2. NULL-word (or epsilon) insertion score,
3. Word penalty, and
4. Language model score.
Skeleton selection feature is intended to help
choose the best skeleton among the input systems.
NULL-word feature controls the number of ep-
silon arcs used in the chosen translation during
the decoding and word penalty feature controls
the length of the translation. For language model
scores, we used a 4-gram LM that we used to train
the input systems.
5 Evaluation and Results
In this section, we describe how we train the input
systems and how we evaluate the proposed system
combination method.
5.1 Systems and Data
To evaluate the impact of the new alignments,
we tested our system combination approach using
the old alignments and improved alignments on
two language pairs: Arabic-English and Chinese-
English. We ran the system combination on three
system outputs that were generated by an in-house
hierarchical phrase-based decoder, as in (Chiang,
2007). The major difference between the three sys-
tems is that they were trained on different subsets
of the available training data using different word
alignments.
For generating the system outputs, first a hier-
archical phrase-based decoder was used to gener-
37
Data for Training/Tuning/Testing
Arabic-English Chinese-English
# of segments # of tokens # of segments # of tokens
Training Data (System1) 14.8M 170M 9.1M 207M
Training Data (System2) 618K 8.1M 13.4M 199M
Training Data (System3) 2.4M 27.5M 13.9M 208M
Tuning Set (Input Systems) 1800 51K 1800 51K
Tuning Set (System Combination) 1259 37K 1785 55K
Test Set - NIST MTEval?05 1056 32K 1082 32K
Test Set - NIST MTEval?06 1797 45K 1664 41K
Test Set - NIST MTEval?08 1360 43K 1357 34K
Table 1: Number of segments and source-side words in the training and test data.
ate three sets of unique 3000-best lists. Nine fea-
tures were used in the hierarchical phrase-based
systems under the log-linear model framework: a
4-gram language model (LM) score (trained on
nearly 3.6 billion words using the SRILM toolkit
(Stolcke, 2002)), conditional rule/phrase probabil-
ities and lexical weights (in both directions), rule
penalty, phrase penalty, and word penalty. Rules
and phrases were extracted in a similar manner
as described in (Chiang, 2007) from the training
data with word alignments generated by GIZA++.
The n-best lists were then re-scored with three ad-
ditional LMs: a count-based LM built from the
Google Tera word corpus, an almost parsing LM
based on super-ARV tagging, and an approximated
full-parser LM (Wang et al, 2007).
For Arabic-English, the first system was trained
on all available training data (see Table 1 for de-
tails), with long sentences segmented into multiple
segments based on IBM model 1 probabilities (Xu
et al, 2005). The second system was trained on a
small subset of the training data, which is mostly
newswire. The third system was trained on an au-
tomatically extracted subset of the training data ac-
cording to n-gram overlap in the test sets.
For Chinese-English, the first system used all
the training data without any sentence segmenta-
tion. The second system used all training data af-
ter IBM-1 based sentence segmentation, with dif-
ferent weightings on different corpora. The third
system is the same as the second system except
that it used different word alignment symmetriza-
tion heuristics (grow-diag-final-and vs. grow-diag-
final (Koehn et al, 2003)).
5.2 Empirical Results
All input systems were optimized on a ran-
domly selected subset of the NIST MTEval?02,
MTEval?03, and MTEval?04 test sets using min-
System MT?05 MT?06 MT?08
System 1 53.4 43.8 43.2
System 2 53.9 46.0 42.8
System 3 56.1 45.3 43.3
No Syns, 1-pass 56.7 47.5 44.9
w/Syns, 2-pass 57.9 48.4 46.2
Table 2: Lowercase BLEU scores (in percentages)
on Arabic NIST MTEval test sets.
imum error rate training (MERT) (Och, 2003)
to maximize BLEU score (Papineni et al, 2002).
System combination was optimized on the rest of
this data using MERT to maximize BLEU score.
As inputs to the system combination, we used 10-
best hypotheses from each of the re-ranked n-best
lists. To optimize system combination, we gener-
ated unique 1000-best lists from a lattice we cre-
ated from the input hypotheses, and used MERT in
a similar way to MT system optimization.
We evaluated system combination with im-
proved alignments on three different NIST
MTEval test sets (MTEval?05, MTEval?06 NIST
portion, and MTEval?08). The final MT outputs
were evaluated using lowercased BLEU scores.
6
Tables 2 and 3 present the BLEU scores (in per-
centages) for the input systems and for different
combination strategies on three test sets in Arabic-
English and Chinese-English, respectively.
On Arabic-English, the combination with syn-
onym matching and two-pass alignment yields ab-
solute improvements of 1.8 to 2.9 BLEU point on
three test sets over the best input system. When
compared to the combination algorithm with the
old alignments (i.e., 1-pass alignment with no syn-
onymmatching), the improved alignments yield an
additional improvement of 0.9 to 1.3 BLEU point
6
We used the NIST script (version 11b) for BLEU with its
default settings: case-insensitive matching of up to 4-grams,
and the shortest reference sentence for the brevity penalty.
38
System MT?05 MT?06 MT?08
System 1 35.8 34.3 27.6
System 2 35.9 34.2 27.8
System 3 36.0 34.3 27.8
No Syns, 1-pass 38.1 36.5 27.9
w/Syns, 2-pass 38.6 37.0 28.3
No Syns, 1-pass, tuning set w/webtext 28.4
w/Syns, 2-pass, tuning set w/webtext 29.3
Table 3: Lowercase BLEU scores (in percentages)
on Chinese NIST MTEval test sets.
on the three test sets.
For Chinese-English, the improvements over the
previous combination algorithm are smaller. The
new combination system yields up to an absolute
2.6 BLEU point improvement over the best input
system and up to 0.5 BLEU point improvement
over the previous combination algorithm on three
different test sets. Note that for Arabic-English,
the individual systems show a high variance in
translation quality when compared to Chinese-
English systems. This might explain why the im-
provements on Chinese-English are modest when
compared to Arabic-English results.
We also noticed that system combination
yielded much smaller improvement on Chinese
MTEval?08 data when compared to other test
sets, regardless of the alignment method (only 0.5
BLEU point over the best input system). We
suspected that this might happen because of a
mismatch between the genres of the test set and
the tuning set (the amount of web text data in
MTEval?08 test set is high although the tuning set
does not include any web text data). To test this
hypothesis, we created a new tuning set for system
combination, which consists of 2000 randomly se-
lected sentences from the previous MTEval test
sets and includes web text data. Using this new
tuning set, combination with the improved align-
ments yields a BLEU score of 29.3 on MTEval?08
data (an absolute improvement of 1.5 BLEU point
over the best input system, and 0.9 BLEU point
improvement over the combination with the old
alignments). These new results again validate the
usefulness of the improved alignments when the
tuning set matches the genre of the test set.
5.3 A Comparison of the Impact of Synonym
Matching and Two-pass Alignment
One last evaluation investigated the impact of each
component on the overall improvement. For this
Synon. 2-pass MT?05 MT?06 MT?08
No No 56.7 47.5 44.9
Yes No 57.3 47.8 45.2
No Yes 57.7 48.0 45.9
Yes Yes 57.9 48.4 46.2
Table 4: Comparison of Synonym Matching and
Two-pass Alignment on Arabic-English
purpose, we ran system combination by turning on
and off each component. Table 4 presents the sys-
tem combination results in terms of BLEU scores
on Arabic-English test sets when each component
is used on its own or when they are used together.
The results indicate that synonym matching on
its own yields improvements of 0.3-0.6 BLEU
points over not using synonym matching. Two-
pass alignment turns out to be more useful than
synonym matching, yielding an absolute improve-
ment of up to 1 BLEU point over one-pass align-
ment.
6 Conclusions
We presented an extension to the previous align-
ment approaches to handle word synonyms more
effectively in an attempt to improve the align-
ments between different hypotheses during confu-
sion network decoding. We also presented a two-
pass alignment strategy for a better alignment of
hypotheses with similar words but with a different
word ordering.
We evaluated our system combination ap-
proach on two language pairs: Arabic-English
and Chinese-English. Combination with improved
alignments yielded up to an absolute 2.9 BLEU
point improvement over the best input system and
up to an absolute 1.3 BLEU point improvement
over combination with the old alignments. It is
worth noting that these improvements are obtained
using very similar input systems. We expect that
the improvements will be higher when we use
structurally different MT systems as inputs to the
combiner.
Our future work includes a more effective use
of existing linguistic resources to handle alignment
of one word to multiple words (e.g., al-nahayan
vs. al nahyan, and threaten vs. pose threat)
and matching of similar (but not necessarily syn-
onymous) words (polls vs. elections). We are
also planning to extend word lattices to include
phrases from the individual systems (i.e., not just
the words) for more grammatical outputs.
39
Acknowledgments This material is based upon work
supported by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-06-C-0023.
References
Akiba, Yasuhiro, Taro Watanabe, and Eiichiro Sumita.
2002. Using language and translation models to se-
lect the best among outputs from multiple MT sys-
tems. In Proc. of the 19th Intl. Conf. on Computa-
tional Linguistics (COLING?2002).
Bangalore, Srinivas, German Bordel, and Giuseppe
Riccardi. 2001. Computing consensus translation
from multiple machine translation systems. In Proc.
of IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU?2001).
Chiang, David. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Fellbaum, Christiane. 1998. WordNet: An Electronic
Lexical Database. Bradford Books, March. Avail-
able at http://wordnet.princeton.edu.
Fiscus, Jonathan G. 1997. A post-processing system
to yield reduced word error rates: Recognizer output
voting error reduction (ROVER). In Proc. of IEEE
Automatic Speech Recognition and Understanding
Workshop (ASRU?1997).
Frederking, Robert and Sergei Nirenburg. 1994.
Three heads are better than one. In Proc. of the
4th Conf. on Applied Natural Language Processing
(ANLP?1994).
Huang, Fei and Kishore Papineni. 2007. Hierarchi-
cal system combination for machine translation. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP?2007).
Jayaraman, Shyamsundar and Alon Lavie. 2005.
Multi-engine machine translation guided by explicit
word matching. In Proc. of the 10th Annual Conf. of
the European Association for Machine Translation
(EAMT?2005).
Koehn, Philipp, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of the
Human Language Technology and the Meeting of the
North American Chapter of the Association for Com-
putational Linguistics Conf. (HLT/NAACL?2003).
Matusov, Evgeny, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Proc. of the 11th Conf. of the
European Chapter of the Association for Computa-
tional Linguistics (EACL?2006).
Mellebeek, Bart, Karolina Owczarzak, Josef Van Gen-
abith, and Andy Way. 2006. Multi-engine machine
translation by recursive sentence decomposition. In
Proc. of the 7th Conf. of the Association for Machine
Translation in the Americas (AMTA?2006).
Nomoto, Tadashi. 2004. Multi-engine machine trans-
lation with voted language model. In Proc. of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL?04).
Och, Franz J. and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics (ACL?2000).
Och, Franz J. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL?2003).
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?2002).
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007a. Combining outputs from multiple ma-
chine translation systems. In Proc. of the Human
Language Technology and the Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics Conf. (HLT/NAACL?2007).
Rosti, Antti-Veikko, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system com-
bination for machine translation. In Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?2007).
Sim, Khe Chai, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007.
Consensus network decoding for statistical machine
translation system combination. In Proc. of the 32nd
Intl. Conf. on Acoustics, Speech, and Signal Process-
ing (ICASSP?2007).
Snover, Matthew, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of the 7th Conf. of the Association for Ma-
chine Translation in the Americas (AMTA?2006).
Stolcke, Andreas. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the Intl. Conf. on
Spoken Language Processing (ICSLP?2002).
Wang, Wen, Andreas Stolcke, and Jing Zheng. 2007.
Reranking machine translation hypotheses with
structured and web-based language models. In Proc.
of the IEEE Automatic Speech Recognition and Un-
derstanding Workshop (ASRU?2007).
Xu, Jia, Richard Zens, and Hermann Ney. 2005.
Sentence segmentation using IBM word alignment
model 1. In Proc. of the 10th Annual Conf. of
the European Association for Machine Translation
(EAMT?2005).
40
