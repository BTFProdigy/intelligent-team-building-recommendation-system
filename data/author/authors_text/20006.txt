Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 29?33, Dublin, Ireland, August 23-29 2014.
UIMA Ruta Workbench: Rule-based Text Annotation
Peter Kluegl
12
Martin Toepfer
2
1
Comprehensive Heart Failure Center
University of W?urzburg, Straubm?uhlweg 2a
W?urzburg, Germany
pkluegl@uni-wuerzburg.de
Philip-Daniel Beck
2
Georg Fette
2
Frank Puppe
2
2
Department of Computer Science VI
University of W?urzburg, Am Hubland
W?urzburg, Germany
first.last@uni-wuerzburg.de
Abstract
UIMA Ruta is a rule-based system designed for information extraction tasks, but it is also ap-
plicable for many natural language processing use cases. This demonstration gives an overview
of the UIMA Ruta Workbench, which provides a development environment and tooling for the
rule language. It was developed to ease every step in engineering rule-based applications. In ad-
dition to the full-featured rule editor, the user is supported by explanation of the rule execution,
introspection in results, automatic validation and rule induction. Furthermore, the demonstration
covers the usage and combination of arbitrary components for natural language processing.
1 Introduction
Components for natural language processing and information extraction nowadays often rely on statis-
tical methods and their models are trained using machine learning techniques. However, components
based on manually written rules still play an important role in real world applications and especially in
industry (Chiticariu et al., 2013). The reasons for this are manifold: The necessity for traceable results,
the absence or aggravated creation of labeled data, or unclear specifications favor rule-based approaches.
When the specification changes, for example, the complete training data potentially needs to be annotated
again. In rule-based components, adaptions in a small selection of rules typically suffice. Rule-based ap-
proaches are also used in combination with or when developing statistical components. While models are
often trained to solve one specific task in an application, the remaining parts need to be implemented as
well. Furthermore, rules can be applied for high-level feature extraction and for semi-automatic creation
of labeled data sets. It is often faster to define one rule for a specific pattern than to annotate repeating
mentions of a specific type.
Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004) is a frame-
work for analyzing unstructured data and is extensively applied for building natural language processing
applications. Two popular systems built upon UIMA are the DeepQA system Watson (Ferrucci et al.,
2010) and the clinical Text Analysis and Knowledge Extraction System (cTAKES) (Savova et al., 2010).
UIMA allows the definition of scalable pipelines of interoperable components called analysis engines,
which incrementally add and modify meta information of documents mostly in form of annotations. The
semantics and features of annotations are given by their types, which are specified in type systems.
This demonstration gives an overview on the UIMA Ruta Workbench (Kluegl et al., 2014), a develop-
ment environment for the UIMA Ruta language. The rule language provides a compact representation of
patterns while still supporting high expressivity necessary for solving arbitrary tasks. The UIMA Ruta
Workbench includes additional tools that accelerate the efficient creation of components and complete
pipelines. The user is supported in all aspects of the development process like specification of rules and
type systems, debugging, introspection of the results, and quality assessment. UIMA Ruta is developed
by an active community
1
and is released like UIMA under the industry-friendly Apache License 2.0.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://uima.apache.org/ruta.html
29
The rest of the paper is structured as follows: Section 2 compares UIMA Ruta to a selection of related
systems. The rule language and the tooling support are introduced in Section 3. Section 4 provides an
overview of the content of the demonstration and Section 5 concludes with a summary.
2 Related Systems
Rule-based systems for information extraction and natural language processing in general have a long
history and thus we can only give a short comparison to a selection of related systems especially based on
UIMA. The probably most noted rule-based system is JAPE (Cunningham et al., 2000). It is open source
and integrated into the GATE (Cunningham et al., 2011) framework. JAPE propagates a clear separation
of condition and action parts, and an aggregated execution of rules of one phase in a finite state transducer
whereas UIMA Ruta allows freer positioning of actions for a more compact representation and applies
the rules sequentially. Nevertheless, UIMA Ruta is able to compete well concerning performance and
provides a high expressiveness. AFST (Boguraev and Neff, 2010) is also based on finite state transduc-
tion over annotations and additionally allows vertical patterns, which are also supported in UIMA Ruta.
SystemT (Chiticariu et al., 2010) defines rules in declarative statements similar to SQL and applies them
using an optimized operator graph. Our system takes first steps in this direction with the concept of
variable matching directions, but still provides a compact language for more flexible operations. Other
rule-based systems for UIMA are the IBM LanguageWare Resource Workbench
2
, Zanzibar
3
and UIMA
Regexp
4
.
Most of the freely available rule-based systems provide only minimal development support. Good
development environments and tooling are usually found in at least partially commercial systems. The
development environment of SystemT (Chiticariu et al., 2011), for example, provides an editor with
syntax highlighting and hyperlink navigation, an annotation provenance viewer, a contextual clue dis-
coverer, regular expression learner, and a rule refiner. One intention of UIMA Ruta consists in providing
strong tooling support that facilitates every step in the development process. The UIMA Ruta Workbench
includes most features of related systems, but still introduces a few new and useful tools.
3 UIMA Ruta
UIMA Ruta (Rule-based Text Annotation) consists of a rule-based language interpreted by a generic
analysis engine and of the UIMA Ruta Workbench, a development environment with additional tooling.
Rules are sequentially applied and are composed of regular expressions of rule elements. The rule ele-
ments typically define the annotation type to be matched and an optional list of conditions and actions.
The language provides a variety of diverse elements to elegantly solve arbitrary tasks. The rule matching
supports overlapping alternatives and a coverage-based visibility. The following example illustrates the
rule syntax:
(ANY{INLIST(MonthsList) -> Month} PERIOD? NUM{REGEXP(".{2,4}") -> Year}){-> Date};
This rule matches on any token present in an external dictionary MonthsList followed by an optional
period and a number that contains two to four characters. If this pattern was recognized, then the actions
create new annotations of the types Month, Year and Date for the corresponding matched segments.
Following this, the rule identifies and annotates dates in the form of ?Dec. 2004?, ?July 85? or ?11.2008?.
Rule scripts can additionally include and apply external components, or specify additional types. A
detailed description of the UIMA Ruta language can be found in the documentation
5
.
The UIMA Ruta Workbench is an Eclipse-based
6
development environment for the rule language.
A screenshot of the Workbench?s main perspective is depicted in Figure 1. Rule scripts are organized
in UIMA Ruta projects and take advantage of the well-known features of Eclipse like version control
2
http://www.alphaworks.ibm.com/tech/lrw
3
https://code.google.com/p/zanzibar/
4
https://sites.google.com/site/uimaregex/
5
http://uima.apache.org/d/ruta-current/tools.ruta.book.html
6
http://www.eclipse.org/
30
Figure 1: A selection of views in the UIMA Ruta Workbench: (A) Script Explorer with UIMA Ruta
projects. Script files containing rules are indicated by the pencil icon. (B) Full-featured editor for
specifying rules. (C) CAS Editor provided by UIMA for visualizing the results and manual creation
of annotations. (D) Overview of annotations sorted by type. (E) Annotations overlapping the selected
position in the active CAS Editor.
or search. The full featured editor for writing rules provides most of the characteristics known by ed-
itors for programming languages like instant syntax checking, syntactic and semantic highlighting, or
context-sensitive auto-completion. An informative explanation of each step of the rule execution ex-
tended with profiling information helps to identify unintended behavior of the rules. The user is able to
automatically evaluate the quality of rules on labeled documents and also on unlabeled documents using
formalized background knowledge. Furthermore, the tools support pattern-based queries in collections
of documents, semi-automatic creation of gold standards and different algorithms for supervised rule
induction.
The rule language and the tooling are both extensible. The rule language can be enhanced with new
actions, conditions, functions and even constructs that adapt aspects of the rule execution. Further func-
tionality can be integrated by implementing supplementary analysis engines. The UIMA Ruta Work-
bench straightforwardly supports extensions due to its integration in Eclipse, e.g., new views can be
added for improving specific development processes. Additional evaluation measures and rule learning
algorithms are directly added by extension points. The Workbench also supports workspaces where the
user develops interdependent Java and UIMA Ruta analysis engines simultaneously, and it seamlessly
integrates components from Maven repositories.
The UIMA Ruta Workbench has been successfully utilized to develop diverse natural language ap-
plications. These include template-based information extraction in curricula vitae, segmentation of
scientific references, or extraction of characters in novels, but also segmentation, chunking and rela-
tion extraction in clinical notes. Furthermore, the system is applied for feature extraction, creation of
gold standards, and different pre- and post-processing tasks in combination with statistical models. The
effectiveness of the Workbench can hardly be measured, but it is highlighted by the fact that several
applications have been engineered in only a few hours of work.
31
4 Contents of Demonstration
The demonstration of the system concentrates on the general usage of the UIMA Ruta Workbench and
how to develop rule-based analysis engines with it. We address the following use cases and aspects:
? General rule engineering: Simple examples are given for common tasks in UIMA Ruta, e.g., how
to create new projects and script files, and how an initial set of rules is applied on a collection of
documents. Several examples highlight the expressivity and effectiveness of the rule language.
? Debugging erroneous rules: Manual specification of rules is prone to errors. We will demonstrate
the explanation component of the Workbench that facilitates the traceability of the rule matching
and allows the identification of unintended behavior.
? Definition of type systems and pipelines: The UIMA Ruta language can be applied for textual
definition of type systems and rule-based pipelines of arbitrary analysis engines, which enables
rapid prototyping without switching tools.
? Introspection of results: The usage of the rule language as query statements allows the user to
investigate different aspects of documents that have been annotated by arbitrary analysis engines,
e.g., also based on statistical methods.
? Quality assessment: The automatic assessment of the analysis engines? quality is a central aspect
in their development process. We demonstrate test-driven development using gold standard docu-
ments, and constraint-driven evaluation for unlabeled documents based on formalized background
knowledge.
? Document preprocessing: The UIMA Ruta language can be applied for many tasks in the Work-
bench. These include different preprocessing steps like converting HTML files, anonymization,
cutting paragraphs or rule-based document sorting.
? Rule learning: The provided rule induction algorithms based on boundary matching, extraction
patterns and transformations are illustrated for simple examples.
? Extension of the language: Specific projects take advantage of specialized language elements.
Extensions of the language and their seamless integration in the Workbench are shown with several
examples.
? Combinations with Java projects: Some functionality can hardly be specified with rules, even
with an extended language specification. Thus, examples provide insights how to make use of
additional functionality implemented in Java.
? Integration of external analysis engines: Repositories like DKPro (Gurevych et al., 2007) or
ClearTK (Ogren et al., 2008) provide a rich selection of well-known components for natural lan-
guage processing. We demonstrate how these analysis engines and type systems can easily be
integrated and how the user is able to specify rules based on the their results, e.g., for improving a
part-of-speech tagger or for exploiting their annotations for relation extraction.
? Semi-automatic annotation: A semi-automatic process supported by rules can speed up the cre-
ation of gold documents. An exemplary use case will illustrate how the user can efficiently accept
or reject the annotations created by rules.
A detailed description of these use cases will be available as part of the documentation of the project.
5 Conclusions
This demonstration presents the UIMA Ruta Workbench, a useful general-purpose tool for the UIMA
community. The system helps to fill the gap of rule-based support in the UIMA ecosystem and is ap-
plicable for many different tasks and use cases. The user is optimally supported during the engineering
process and is able to create complex and well-maintained applications as well as rapid prototypes. The
UIMA Ruta Workbench is up-to-date unique concerning the combination of the provided features and
tools, availability as open source, and integration in UIMA.
32
An interesting option for future work consists in making the functionality of UIMA Ruta and its tooling
also available in web-based systems like the Argo UIMA platform (Rak et al., 2012).
Acknowledgements
This work was supported by the Competence Network Heart Failure, funded by the German Federal
Ministry of Education and Research (BMBF01 EO1004), and is used for information extraction from
medical reports and discharge letters.
References
Branimir Boguraev and Mary Neff. 2010. A Framework for Traversing dense Annotation Lattices. Language
Resources and Evaluation, 44(3):183?203.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Sriram Raghavan, Frederick R. Reiss, and Shivakumar
Vaithyanathan. 2010. SystemT: An Algebraic Approach to Declarative Information Extraction. In Proceed-
ings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ?10, pages 128?137,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Laura Chiticariu, Vivian Chu, Sajib Dasgupta, Thilo W. Goetz, Howard Ho, Rajasekar Krishnamurthy, Alexander
Lang, Yunyao Li, Bin Liu, Sriram Raghavan, Frederick R. Reiss, Shivakumar Vaithyanathan, and Huaiyu Zhu.
2011. The SystemT IDE: An Integrated Development Environment for Information Extraction Rules. In Pro-
ceedings of the 2011 ACM SIGMOD International Conference on Management of Data, SIGMOD ?11, pages
1291?1294, New York, NY, USA. ACM.
Laura Chiticariu, Yunyao Li, and Frederick R. Reiss. 2013. Rule-based Information Extraction is Dead! Long Live
Rule-based Information Extraction Systems! In Proceedings of the 2013 Conference on Empirical Methods in
Natural Language Processing, pages 827?832, Seattle, Washington, USA, October. Association for Computa-
tional Linguistics.
Hamish Cunningham, Diana Maynard, and Valentin Tablan. 2000. JAPE: a Java Annotation Patterns Engine (Sec-
ond Edition). Research Memorandum CS?00?10, Department of Computer Science, University of Sheffield,
November.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva, Valentin Tablan, Niraj Aswani, Ian Roberts, Genevieve
Gorrell, Adam Funk, Angus Roberts, Danica Damljanovic, Thomas Heitz, Mark A. Greenwood, Horacio Sag-
gion, Johann Petrak, Yaoyong Li, and Wim Peters. 2011. Text Processing with GATE (Version 6).
David Ferrucci and Adam Lally. 2004. UIMA: An Architectural Approach to Unstructured Information Processing
in the Corporate Research Environment. Natural Language Engineering, 10(3/4):327?348.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A. Welty. 2010.
Building Watson: An Overview of the DeepQA Project. AI Magazine, 31(3):59?79.
Iryna Gurevych, Max M?uhlh?auser, Christof M?uller, J?urgen Steimle, Markus Weimer, and Torsten Zesch. 2007.
Darmstadt Knowledge Processing Repository Based on UIMA. In Proceedings of the UIMA Workshop at
GLDV, T?ubingen, Germany, April.
Peter Kluegl, Martin Toepfer, Philip-Daniel Beck, Georg Fette, and Frank Puppe. 2014. UIMA Ruta: Rapid
Development of Rule-based Information Extraction Applications. Natural Language Engineering. submitted.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard. 2008. ClearTK: A UIMA Toolkit for statistical Natural
Language Processing. In UIMA for NLP Workshop at LREC.
Rafal Rak, Andrew Rowley, William Black, and Sophia Ananiadou. 2012. Argo: an Integrative, Interactive, Text
Mining-based Workbench Supporting Curation. Database, 2012:bas010.
Guergana K. Savova, James J. Masanz, Philip V. Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-Schuler,
and Christopher G. Chute. 2010. Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES):
architecture, component evaluation and applications. Journal of the American Medical Informatics Association,
17(5):507?513, September.
33
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 83?92,
Dublin, Ireland, August 23rd 2014.
Integrated Tools for Query-driven Development
of Light-weight Ontologies and Information Extraction Components
Martin Toepfer
1
Georg Fette
1
1
Department of Computer Science VI
University of W?urzburg, Am Hubland
W?urzburg, Germany
first.last@uni-wuerzburg.de
Philip-Daniel Beck
1
Peter Kluegl
12
Frank Puppe
1
2
Comprehensive Heart Failure Center
University of W?urzburg, Straubm?uhlweg 2a
W?urzburg, Germany
pkluegl@uni-wuerzburg.de
Abstract
This paper reports on a user-friendly terminology and information extraction development envi-
ronment that integrates into existing infrastructure for natural language processing and aims to
close a gap in the UIMA community. The tool supports domain experts in data-driven and manual
terminology refinement and refactoring. It can propose new concepts and simple relations and
includes an information extraction algorithm that considers the context of terms for disambigua-
tion. With its tight integration of easy-to-use and technical tools for component development
and resource management, the system is especially designed to shorten times necessary for do-
main adaptation of such text processing components. Search support provided by the tool fosters
this aspect and is helpful for building natural language processing modules in general. Special-
ized queries are included to speed up several tasks, for example, the detection of new terms and
concepts, or simple quality estimation without gold standard documents. The development en-
vironment is modular and extensible by using Eclipse and the Apache UIMA framework. This
paper describes the system?s architecture and features with a focus on search support. Notably,
this paper proposes a generic middleware component for queries in a UIMA based workbench.
1 Introduction
According to general understanding, a specification of relevant concepts, relations, and their types is
required to build Information Extraction (IE) components. Named Entity Recognition (NER) systems
in the newspaper domain, for example, try to detect concepts like persons, organizations, or locations.
Regarding clinical text, it has been shown that lookup-based approaches (Tanenblatt et al., 2010) can
achieve high precision and recall if a terminology exists that maps terms to their meanings. However,
this approach is not directly applicable if such resources are not available for a certain language or
subdomain, or if the domain and its terminology are changing. Unsupervised methods can help to find
and to group the relevant terms of a domain, for example, into concept hierarchies (Cimiano et al., 2005).
Nevertheless, automatically derived terminologies are not perfect, and there are many applications that
require high precision knowledge resources and representations, for instance, to build up a clinical data
warehouse. In this case, automatically generated ontologies have to be refined by domain experts like
clinicians, which imposes special requirements on the usability of the tools.
There have been several efforts to support ontology extraction and refinement (Cimiano and V?olker,
2005), predominantly with text processing based on the GATE (Cunningham et al., 2011) infrastructure.
In the Apache UIMA (Ferrucci and Lally, 2004) community
1
, several tools exist that ease system de-
velopment and management. Much work has, for instance, been spent on pipeline management, rule
development (Kluegl et al., 2009), or evaluation. Terminology and ontology development support, how-
ever, have not gained as much attention in the context of this framework by now. This is surprising since
the integration of tools for terminology development and especially terminology generation and infor-
mation extraction into existing infrastructure for text processing is promising. Actually, the approach
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://uima.apache.org/
83
taken in this paper regards terminology creation and information extraction as two related tasks. The
proposed system aims to assist users in the development of components that extract information with
lexical resources gathered during the specification of the concepts of the domain.
This paper introduces Edtgar: a user-friendly integrated terminology development environment. It
provides many features that help domain experts to construct and refine light-weight ontologies driven
by flexible corpus queries. In this work, ?light-weight ontology? means that we focus on simple rela-
tions, as well as restricted inference. We call the knowledge representation ?terminology? since the tool
aims to manage lexical information for information extraction. The major components of the system are
a terminology editor, a plug-in concept for terminology extraction and an information extraction API, as
well as support for corpus queries. Special views show extraction statistics, provide semi-automatic an-
notation of gold standard documents, as well as evaluation and deployment support. The tool comprises
an implementation for terminology induction and an information extraction algorithm that considers con-
texts. In order to keep the system modular and extensible, it integrates into Eclipse
2
and uses the Apache
UIMA framework. Apache UIMA provides a well-established framework for text processing, hence, a
variety of natural language processing components can easily be integrated, for example, by accessing
component repositories like DKPro Core
3
. At the technical level, the default processing components
of the proposed system use a combination of Apache UIMA Ruta
4
scripts and custom analysis engines
implemented in Java. As a consequence, the tight integration of the terminology development tools into
Apache UIMA?s Eclipse Tools and Apache UIMA Ruta?s rule engine and workbench allows technical
engineers to use several existing features.
The structure of the paper is as follows: Section 2 gives a brief overview of ontology development
systems and tools. Section 3 and 4 introduce the tool and its support for corpus queries. Results of a case
study are given in Section 5. Finally, we conclude in Section 6.
2 Related Work
Most of all, our work relates to environments and frameworks for ontology learning, editing, and refine-
ment. We first give a brief overview of such systems with a focus on open source and research related
systems
5
. Afterwards, we discuss some popular query tools that come into question for integration into
natural language processing environments.
OntoLT (Buitelaar et al., 2004) is a plugin for the ontology editor Prot?eg?e. It aims to derive ontological
concepts and relations from plain text by defining XPath expressions over linguistic structures, e.g.,
subject object relations under constraints like specific lemmas. Manual annotation of ontology concept
mentions can be performed with the Prot?eg?e plugin Knowtator
6
.
Very similar to our work is the NeOn toolkit
7
. It is an Eclipse based ontology development environ-
ment. There are many plugins available that extend NeOn toolkit?s functionality. For instance, the GATE
Webservice plugin
8
and its TermRaider component automatically generate ontological information. One
of the plugins for NeOn is the work by Cimiano and V?olker, who proposed Text2Onto
9
(Cimiano and
V?olker, 2005), which is a framework that allows to apply ontology learning and change discovery al-
gorithms. Its central data structure is called probabilistic ontology model (POM). It is not providing
statistics but stores values that represent concept or relation extraction certainty. Text2Onto?s natural
language processing is based on GATE and the rule engine JAPE. Similar to our work, Text2Onto aims
to provide an easy-to-use user interface.
Most of the tools mentioned above either use GATE or proprietary data formats for linguistic represen-
tations. We believe that there is a need for a tool based on UIMA. Our tool aims to provide an integrated
2
http://eclipse.org/
3
https://www.ukp.tu-darmstadt.de/software/dkpro-core/?no_cache=1
4
https://uima.apache.org/ruta.html
5
There are related commercial solutions, for instance, http://www.poolparty.biz/
6
http://knowtator.sourceforge.net/
7
http://neon-toolkit.org
8
http://neon-toolkit.org/wiki/Gate_Webservice
9
https://code.google.com/p/text2onto/
84
development environment that includes terminology development and allows to use already available
features provided by Eclipse plugins related to UIMA. Thereby, we ease development, introspection,
debugging, and testing of all kinds of UIMA annotation engines involved in system and terminology
development. Hence, the components that extract and refine terminologies based on natural language
processing can easily be adapted, for example, segmentation rules written in UIMA Ruta?s workbench.
The same argumentation applies to information extraction components or linguistic preprocessors like
chunkers that are involved in both tasks. Contrary to most other tools, we do not consider complex
relations. Instead, we focus on easy-to-understand inference and relations.
In the UIMA community, Fiorelli et al. proposed the computer-aided ontology development architec-
ture (CODA) (Fiorelli et al., 2010) that consists of the tasks: ontology learning, population of ontologies,
and linguistic enrichment. Fiorelli et al. describe how an integrated ontology development system based
on UIMA could look like, however, their system UIMAST concentrates on ontology population aspects.
By contrast, this paper also considers construction tasks and describes tooling for editing and refining
light-weight ontologies either manually or based on document collections.
Table 1 compares qualitative features of different query tools which are described below. Since seman-
tic search is an active field of research, we can only give a brief overview focused on popular tools for
UIMA and GATE. We put special emphasis on tools that can be easily integrated into an Eclipse-based
environment.
Tool Framework Index Syntax IDE Integration
Ruta Query View UIMA no
a
expert Eclipse
Lucas / Lucene UIMA yes user-friendly -
GATE M??mir GATE yes medium -
GATE Annic GATE yes medium GATE Developer
b
a
uses only UIMA?s annotation index
b
https://gate.ac.uk/family/developer.html
Table 1: Qualitative comparison of query tools.
The UIMA Ruta workbench contains a query view that enables to search in document collections with
arbitrary queries formulated as rules. For instance, Segment{-CONTAINS(CONCEPT)} matches
segment annotations that do not contain any concept annotation. With regard to usability for domain
experts, this tool has a drawback: users have to be familiar with Ruta?s syntax which is in general too
complex for terminology engineers. They should not have to learn a programming language to pose a
query on a corpus. Another drawback of the query view is that it has no option to group search results,
e.g. by their covered text. The Ruta query view is not designed for fast results on very large corpora.
It iteratively processes all documents of a folder that match a user-defined filename filter, thus, queries
do not run as fast as with index structures for the whole corpus. A combination of rule-based query
formulation and the new middleware (Section 4) would be useful for the community.
Apache Lucene
10
is a popular search engine. Existing mapping tools like the UIMA Lucene in-
dexer Lucas
11
show how UIMA pipeline results can be indexed with Lucene. This solution is attractive
since Lucene?s query syntax allows complex query patterns but still remains easy-to-use. For example,
valve -pulmo
*
searches for documents that contain the term ?valve? but do not contain terms be-
ginning with ?pulmo?. Inexperienced users have a higher chance to understand the syntax because it is
more similar to the one of web search engines. Lucene itself does not provide a user interface but there
are tools like Apache Solr, or Apache Stanbol which is a semantic search project based on Apache Solr.
Our requirements are similar in certain aspects to Gate m??mir
12
, which is an indexing and retrieval
tool for Gate. It allows to find text passages that match certain annotation or text patterns. For example,
10
http://lucene.apache.org/core/
11
http://svn.apache.org/repos/asf/uima/addons/trunk/Lucas/
12
http://gate.ac.uk/mimir/
85
transistor IN {Abstract}
13
searches for abstracts regarding transistors. In combination with
GATE?s rule engine Jape
14
, similar functionality can be achieved for terminology development. A query
tool for document processing and retrieval based on Apache Lucene and GATE is Annic
15
(ANNotations
In Context) (Aswani et al., 2005). It provides a viewer for nested annotation structures and features.
3 Edtgar: Terminology Development Environment
The system has been developed as part of a medical information extraction project that populates the
data warehouse of a German hospital. Documents in clinical domains differ from the kind of text that is
widely used for ontology development, hence common approaches to learn ontologies, for example, with
lexical patterns, have relatively low entity acceptance rates on clinical documents (Liu et al., 2011). As a
consequence, ontology learning and enrichment methods and information extraction components have to
be adapted to the domain. We address this adaptation step integrating new editors, views and application
logic into existing tooling for working with UIMA documents. Some features of our system are espe-
cially useful for processing clinical text but they also work for similar domains, such as advertisements,
product descriptions, or semi-structured product reviews.
In order to assist users during development, we qualitatively analyzed workflows in a project with
clinical reports. As a result, we identified the following steps:
1. Linguistic preprocessing: A technical engineer chooses a component for preprocessing steps like
tokenization, sentence detection, part-of-speech tagging, chunking, or parsing. In our projects, a
rule engineer typically adapts general rules to fit to a special domain. He modifies tokenization
rules and lists of abbreviations, and specifies segmentation rules that detect relevant parts of the
document, annotate sections, subsections, sentences, and segments.
2. Initial terminology generation: an algorithm automatically derives a terminology based on a corpus
of plain text documents.
3. Terminology refinement: a domain expert changes the terminology until it meets subjective or ob-
jective quality criteria:
(a) Automatically extract information according to the current state of the terminology.
(b) Inspect extraction results.
(c) Refine terminology: edit/add new concepts, add/update variants of existing concepts.
4. Annotation of gold standard documents. Evaluation of the information extraction component and
the coverage of the terminology.
Further analysis showed different aspects of improvement that conform with common expectations:
1. Search: terminology engineers frequently need to pose different kinds of search patterns on docu-
ment collections. Some of them are related to quality estimation without a gold standard.
2. Redundancy: modeling concepts independently of each other causes false negatives. Some concepts
have highly redundant aspects that should be managed in a central way.
In the following, we first sketch the terminology model and then briefly report on the terminology
induction, validation, and information extraction components. In this work, we put special emphasis on
analysing search tools that can be used in a UIMA-based workbench. Our approach to pose queries in
the workbench is discussed in Section 4.
13
from: http://services.gate.ac.uk/mimir/query-session-examples.pdf
14
http://gate.ac.uk/sale/tao/splitch8.html
15
http://gate.ac.uk/sale/tao/splitch9.html
86
Figure 1: Edtgar T-Box Perspective: (A) Toolbar; (B) Project Structure; (C) Terminology Editor with
(C1) Concept Tree, (C2) Variants; (D) Information Extraction Statistics; (E) Search View (Lucene); (F)
Search Results (Filesystem: TreeViewer mode)
3.1 Terminology Model
In order to describe the information contained in documents of a domain, we follow an attribute-value
model extended with concepts of type entity (object) that disambiguate different meanings of attributes.
The central aspects of our knowledge representation are given by a tuple
O = (T,C,D, V,R
T?C
, R
T?T
, R
C?V
, R
V?D
)
where T , C, D, V contain templates, concepts, dictionaries, and variants, respectively. The relation
R
T?C
defines which concepts use certain templates, R
T?T
models inter-template references. Concepts
are the main elements of the terminology. There are several types of concepts such as objects, attributes,
and values. Each concept is expressed by lexical variants (relation R
C?V
) which can be grouped by
dictionaries (relation R
V?D
). Variants can be either simple strings or regular expressions. Attributes
that belong to the same kind of semantic category typically share the same sets of values. Ontologies
model this kind of relation between concepts typically by inheritance (subclass-of) or instantiation. In our
terminology model, users can use templates to group concepts and even templates into semantic classes
that share all values that are part of the template. As a consequence, the template mechanism avoids
redundancy and errors in the terminology. Templates can also be used just to tag concepts. To allow
users to store domain specific information, concepts can have arbitrary properties (key-value-pairs). All
information of the model is de-/serializable as a single easy-to-read XML file.
3.2 Terminology Induction
Developing a terminology from scratch is costly, but some domains allow that a considerable amount
of attributes and values can be automatically found. Algorithms that induce terminologies and relations
can be easily plugged into the workbench through certain APIs. Per default, a simple rule-based ap-
proach based on part-of-speech tags is used. It basically finds different kinds of patterns for attributes
(nouns) and values (adjectives). The algorithm uses the lemmas of lexical items to group concepts and
87
Figure 2: Ambiguous attributes: the attribute insufficiency (German: Insuffizienz) is both valid for the
entities pulmonary valve (German: Pulmonalklappe) and tricuspid valve (German: Trikuspidalklappe).
Insufficiency itself can either be negated (German: keine) or minor (German: gering). Pulmonary valve
and tricuspid valve are both regular in this example (German: Klappe zart).
creates different variants of the concepts respectively. Probabilities that tell users how certain a concept
extraction is can be optionally shown if they are provided by the algorithm.
3.3 Terminology Validation
Terminologies can become quite large, for example, in medical domains, which makes it difficult to
manage them manually. For instance, it is important to avoid redundancy because keeping redundant
concepts or variants in sync gets difficult. We provide different types of validators that check certain
aspects of improvement and show errors and warnings. For example, missing dictionary references
cause insufficient synonym lists and false negative extractions. We have a validator that creates warnings
if a dictionary should be referenced instead of using only some part of this dictionary in a certain concept.
There are several other validators, for instance, to detect missing template references. New validators
can easily be integrated into the framework.
3.4 Information Extraction
Similar to its terminology induction module, our system has a plug-in mechanism for information ex-
traction algorithms. By default, it contains an information extraction algorithm which allows for context-
sensitive disambiguation of terms with multiple meanings. It can, for example, resolve the correct inter-
pretation for ambiguous terms like ?Klappe zart? or ?Insuffizienz? as shown in Figure 2.
At first, the terminology is transformed into appropriate data structures for inference. The processing
pipeline begins with finding lexemes by matching the text against regular expressions and simple strings.
The next annotator segments the document into hierarchically ordered parts such as sections, subsec-
tions, sentences, segments, and tokens. This component is implemented with Ruta rules which enables
rule engineers to adapt this stage to different document types and styles easily. The following stage is
implemented as a Java analysis engine. At first, it finds and annotates objects, i.e., entities that are close
to the root of the terminology and belong to the object type in the terminology. These concepts are typ-
ically expressed by unique variants and should not need any disambiguation. Afterwards, the algorithm
tries to assign unresolved attribute entities to objects, or directly annotates special entities. Finally, the
algorithm performs light-weight inference: first, value extractions are removed when the corresponding
attribute has been negated. Second, presence annotations are added if an attribute entity requires a status
value and is not negated in the sentence.
3.5 Knowledge-driven Evaluation and Status Reports
Similar to quality assurance or quality assessment in factories, users can specify assertions for text pro-
cessing tasks where system behavior is expected to conform to these assertions (Wittek et al., 2013).
Such expectations allow to compute quality estimates even without annotated documents. To this end,
we provide special annotation types for these cases that can be categorized to distinguish different tasks
or types of misclassifications. By now, knowledge-driven evaluation is realized by the contributed search
commands (see Section 4). They allow to find, count, and group constraint violations which helps to es-
timate the quality of the text processing component and to understand the errors it makes. For example,
the user can expect that all nouns should either be assigned to a concept annotation or listed in a blacklist.
Elaboration of this aspect of the tool is planned for future releases.
88
3.6 Edtgar Workbench Overview
Edtgar?s graphical user interface (see Figure 1) provides two perspectives and several views to assist the
user. The heart of the tool is the terminology editor that allows to create or modify concepts (attributes
and values, etc.), manage variants and dictionaries. If the main terminology of a project is opened,
users can set the active corpus for the project, or trigger several actions. They can start terminology
induction/enrichment, information extraction, or run different types of specialized or general searches
on the active corpus, for example, by pressing the corresponding buttons in the toolbar. The tool also
provides several other features that we do not discuss here, e.g., support for semi-automatic gold standard
annotation, evaluation, documentation, and much more.
3.7 Typesystem Handling
Inspecting the results of processed documents is a visual process. Representing each concept type by a
distinct annotation type has a technical advantage because occurrences of a certain type of concept can
be highlighted in the UIMA CAS editor with a different color. During terminology induction, however,
the terminology and the corresponding annotation types do not exist in the typesystem of the processed
document. As a result, the presented framework uses a hybrid concept representation. Engines can
use generic annotation types with an integer id feature to create proposal annotations for terminology
generation and enrichment. These annotations are subsequently mapped to type based representations
when the terminology and its typesystem have been completely defined. As a natural side-effect of
terminology development, identifiers of concepts may change, for instance, if concepts are rejected or
merged. The terminology editor keeps identifiers stable as long as possible since both representation
schemes have problems with invalid IDs. An advantage of the ID feature based approach is that it is
able to retain invalid references whereas a type-based approach looses information when documents are
loaded leniently.
Besides concept annotations, the system provides framework types for different purposes. For in-
stance, IgnoredRegion, UnhandledSegment, or Sealed. They allow to configure irrelevant text detection
for each subdomain, enable users to find new terms, or that have been manually inspected and contain
gold standard annotations, respectively.
4 Search Tools for the Workbench
An important aspect of terminology development is search support. It should assist users with predefined
queries, e.g., to find new concepts, synonyms of concepts, or different meanings of concepts. Technical
users, however, must be able to perform searches with arbitrary complex patterns. Several tasks can be
regarded as corpus queries, for instance, finding measurement units or abbreviations.
In order to support domain experts, we identified three main types of queries that fit to their workflows.
First of all, Unhandled Segments Search is a kind of false negative predictor (cf. Section 3.5). It lists
all segments in the corpus that have not yet been covered either by terminological concepts or exclude
patterns. This is in particular useful in clinical domains where nearly every noun phrase contains relevant
information. It can, however, easily be adapted to other domains. Second, querying specific types of
extracted information is necessary, for instance, to inspect extraction results and to judge their quality.
It allows to create partial gold standards and it helps to decide if attributes have ambiguous meanings
dependent on their context. Third, constrained search supports the terminology engineer when editing
concepts or creating new entries: users often search for attributes to find their value candidates and need
to see documents containing synonyms of the attribute but leaving known value terms out. Finally, if
domain experts are not familiar with regular expressions, they benefit from fast lexical search to test
their patterns.
Query Tools in Edtgar
Most of the queries mentioned above can be implemented as searches for certain annotation types (UIMA
Type Search). For instance, segments that do not contain any extracted information (Unhandled Segments
Search) can be annotated with a special type, and concept mentions can be annotated with types based on
89
(a) Search Page for UIMA annotation types: results can be
grouped by a certain feature or their covered text.
(b) Search Results Page (TableViewer mode): it shows distinct
unhandled segments ordered by frequency
Figure 3: Generic Search Components
concept identifiers, e.g., mietas.concept.C24 for the concept with the ID 24. The system provides
two types of query commands that support annotation type searches.
The first query command uses a separate index over a whole corpus and provides very fast retrieval. It
is based on Apache Lucene, thus Lucene?s user-friendly query syntax can be used. For instance, querying
?insufficiency AND aortic -moderate? retrieves sentences that contain ?insufficiency? and ?aortic? but
not ?moderate?. The interface to Lucene-based queries can be seen in Figure 1 (E). The index is based on
sentences and contains special fields for extracted concept annotations. For instance, ?concept:24? shows
sentences where the concept with the ID 24 has been extracted. Indexing can be triggered manually.
The second query command iterates over files in the filesystem and uses UIMA?s index structures to
find annotations of requested types. It integrates into the search framework of Eclipse. As a consequence,
users can easily expand or collapse search results, browse search results, or iteratively open respective
documents in a shared editor instance. The component implements a tree view (confer Figure 1 (F)) and
a table view (confer Figure 3b) to show results. The programmatic search command needs a type name,
a type system, and a folder. As a special feature, search results can be grouped based on their covered
text or the string value of a feature of the annotations. Grouping allows to show absolute counts for each
group. It helps users to find and rank relevant candidate terms and phrases. It can, however, also be used
in other use cases, for example, to list all distinct person mentions in a text collection. Results can be
exported as an HTML report. Figure 3a shows the generic search page for posing UIMA based queries.
The folder can be selected in the script explorer (see Fig. 1 (B)). The dialog provides a combo box to
choose the type system and auto-completion for the type name and group-by text widgets.
Technical users can already use the middleware in combination with rule-based queries when they
create rule scripts just for this purpose. These scripts define queries and types for results. Subsequently,
users apply one of the type-based search commands.
Only one of the task specific query types in the terminology development environment is not imple-
mented as a Lucene search or an annotation type based search command: to support quick testing of
regular expressions, the tool accesses the text search command of Eclipse which allows very fast search
and rapid feedback.
5 Case Study
The main application of the tool is terminology generation and subsequent information extraction for a
wide range of medical reports. We evaluated it in a small subdomain of the physical examination con-
cerning the heart (cor). We gathered 200 anonymized documents from a hospital information system
and divided the corpus into a training and a test set (150:50). For terminology generation, we first ex-
tracted all nouns from the training set as attributes. Then we merged similar entries and deleted irrelevant
candidates. For each attribute we generated and manually adapted relevant value candidates assigning
templates wherever possible. For both tasks, we applied multiple searches and used tool support for fast
inspection of the relevant segments in the documents. The final terminology was used for information
90
extraction on the test set. We measured the time necessary for terminology adaption and the precision
and recall of the information extraction on the test set and additionally estimated the recall with a query
as described in Section 4. The gold standard for information extraction in this feasibility study was de-
fined by ourselves. From the training set, we extracted 20 attributes and 44 boolean and 6 numerical
values. Manual correction took about 4 hours. Microaveraged precision and recall of the information
extraction were 99% and 90% on the test set. The estimated recall on the test set was 84%. Roughly
one half of the errors was due to unknown terminology in the test documents. The other half was mainly
induced by missing variants of already known concepts. With a larger training set, these kinds of errors
can be considerably reduced.
6 Summary
Ontology development and query-driven workflows have not gained as much attention in the UIMA
community as, for example, pipeline management, rule development, or evaluation. Especially if ontolo-
gies are developed in order to build information extraction systems, it is desirable to have a workbench
environment that integrates both tools for ontology development and information extraction. The tool
suggested in this paper aims to fill this gap. It supports the creation of light-weight ontologies for in-
formation extraction, that is, it helps to find attributes and their values, and to encode simple relations
between concepts. It integrates into Eclipse and lowers the bridge between different frameworks and
tools. Notably, we assist users in query-driven workflows, which includes a simple way to assess quality
without manually annotated documents. We plan to release the tool under an open source license.
Acknowledgments
This work was supported by the Competence Network Heart Failure, funded by the German Federal
Ministry of Education and Research (BMBF01 EO1004).
References
N. Aswani, V. Tablan, K. Bontcheva, and H. Cunningham. 2005. Indexing and Querying Linguistic Metadata and
Document Content. In Proceedings of Fifth International Conference on Recent Advances in Natural Language
Processing (RANLP2005), Borovets, Bulgaria.
Paul Buitelaar, Daniel Olejnik, and Michael Sintek. 2004. A Prot?eg?e Plug-In for Ontology Extraction from Text
Based on Linguistic Analysis. In ChristophJ. Bussler, John Davies, Dieter Fensel, and Rudi Studer, editors, The
Semantic Web: Research and Applications, volume 3053 of Lecture Notes in Computer Science, pages 31?44.
Springer Berlin Heidelberg.
Philipp Cimiano and Johanna V?olker. 2005. Text2Onto: A Framework for Ontology Learning and Data-driven
Change Discovery. In Proceedings of the 10th International Conference on Natural Language Processing and
Information Systems, NLDB?05, pages 227?238, Berlin, Heidelberg. Springer-Verlag.
Philipp Cimiano, Andreas Hotho, and Steffen Staab. 2005. Learning Concept Hierarchies from Text Corpora
Using Formal Concept Analysis. J. Artif. Int. Res., 24(1):305?339, August.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva, Valentin Tablan, Niraj Aswani, Ian Roberts, Genevieve
Gorrell, Adam Funk, Angus Roberts, Danica Damljanovic, Thomas Heitz, Mark A. Greenwood, Horacio Sag-
gion, Johann Petrak, Yaoyong Li, and Wim Peters. 2011. Text Processing with GATE (Version 6).
David Ferrucci and Adam Lally. 2004. UIMA: An Architectural Approach to Unstructured Information Processing
in the Corporate Research Environment. Natural Language Engineering, 10(3/4):327?348.
Manuel Fiorelli, Maria Teresa Pazienza, Steve Petruzza, Armando Stellato, and Andrea Turbati. 2010. Computer-
aided Ontology Development: an integrated environment. In Ren?e Witte, Hamish Cunningham, Jon Patrick,
Elena Beisswanger, Ekaterina Buyko, Udo Hahn, Karin Verspoor, and Anni R. Coden, editors, New Challenges
for NLP Frameworks (NLPFrameworks 2010), pages 28?35, Valletta, Malta, May 22. ELRA.
Peter Kluegl, Martin Atzmueller, and Frank Puppe. 2009. TextMarker: A Tool for Rule-Based Information
Extraction. In Christian Chiarcos, Richard Eckart de Castilho, and Manfred Stede, editors, Proceedings of the
Biennial GSCL Conference 2009, 2nd UIMA@GSCL Workshop, pages 233?240. Gunter Narr Verlag.
91
K. Liu, W. W. Chapman, G. Savova, C. G. Chute, N. Sioutos, and R. S. Crowley. 2011. Effectiveness of Lexico-
syntactic Pattern Matching for Ontology Enrichment with Clinical Documents. Methods of Information in
Medicine, 50(5):397?407.
Michael Tanenblatt, Anni Coden, and Igor Sominsky. 2010. The ConceptMapper Approach to Named Entity
Recognition. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International
Conference on Language Resources and Evaluation (LREC?10), Valletta, Malta, may. European Language Re-
sources Association (ELRA).
Andreas Wittek, Martin Toepfer, Georg Fette, Peter Kluegl, and Frank Puppe. 2013. Constraint-driven Evaluation
in UIMA Ruta. In Peter Kluegl, Richard Eckart de Castilho, and Katrin Tomanek, editors, UIMA@GSCL,
volume 1038 of CEUR Workshop Proceedings, pages 58?65. CEUR-WS.org.
92
