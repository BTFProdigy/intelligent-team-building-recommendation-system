Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 215?218,
Prague, June 2007. c?2007 Association for Computational Linguistics
LCC-SRN: LCC?s SRN System for SemEval 2007 Task 4 
Adriana Badulescu 
Language Computer Corporation  
1701 N Collins Blvd #2000 
Richardson, TX, 75080 
adriana@languagecomputer.com 
Munirathnam Srikanth 
Language Computer Corporation 
1701 N Collins Blvd #2000 
Richardson, TX, 75080 
srikanth@languagecomputer.com 
 
 
Abstract 
This document provides a description of 
the Language Computer Corporation (LCC) 
SRN System that participated in the SemE-
val 2007 Semantic Relation between 
Nominals task. The system combines the 
outputs of different binary and multi-class 
classifiers build using machine learning al-
gorithms like Decision Trees, Semantic 
Scattering, Iterative Semantic Specializa-
tion, and Support Vector Machines. 
1 Introduction 
The Semantic Relations between Nominals task 
from SemEval 2007 focuses on identifying the se-
mantic relations that hold between two arguments 
manually annotated with word senses (Girju et al 
2007).  
The previous work in identifying semantic rela-
tions between nominals focuses on finding one or 
more relations in text for specific syntactic patterns 
or constructions (like genitives and noun com-
pounds) using semi-automated and automated sys-
tems. An overview of some of these methods can 
be found in (Badulescu, 2004).   
The LCC SRN system, developed during the 
SRN training period, was for us, the beginning of a 
different approach to semantic relations detection: 
detecting semantic relations in text without using a 
syntactic pattern. Our existing work on semantic 
relation detection was on detecting semantic rela-
tions in text (one or more at a time) at different 
levels in the sentence using different syntactic pat-
terns like genitives, noun compounds, verb-
arguments, etc.  
For SRN, we built a new system that combines 
the output of the pattern dependent classifiers with 
the new pattern-independent classifiers for better 
results.  
The remainder of this paper is organized as fol-
lows:  Section 2 describes our system, Section 3 
details the experimental results, and Section 4 
summarizes the conclusions.  
2 System description 
The system consists of two types of classifiers: 
classifiers that do not use the syntactic parsed tree 
and that were built specifically for the SemEval 
2007 Task 4(SRN) and classifiers that use specific 
syntactic pattern to determine the semantic rela-
tions and there were previously developed at LCC 
and then adapted to the SRN task (SRNPAT).  
The classifiers for each type were built from an-
notated examples using supervised machine learn-
ing algorithms like Decision Trees (DT)1, Support 
Vector Machines (SVM) 2 , Semantic Scattering 
(SS) (Moldovan and Badulescu, 2005) , Iterative 
Semantic Specialization (ISS) (Girju, Badulescu, 
and Moldovan, 2006), Na?ve Bayes (NB) 3  and 
Maximum Entropy (ME)4.  
The outputs of different classifiers (built using 
different types of machine learning algorithms 
were combined and ranked using predefined rules.  
Figure 1 shows the architecture of our SRN 
system. 
 
                                                 
1
 C5.0., http://www.rulequest.com/see5-info.html 
2
 LIBSVM, www.csie.ntu.edu.tw/~cjlin/libsvm/ 
3
 jBNC, http://jbnc.sourceforge.net 
4
 http://homepages.inf.ed.ac.uk/s0450736/maxent_tool-
kit.html 
215
Pr
e
-
pr
o
c
es
s
Annotated tree
Text Processing Tools
Cl
a
s
si
fic
a
tio
n
[Arg1, Arg2, Relation, 
Score,Classifier] 
[NPArg1, NPArg2, Relation, 
Score, Classifier] 
SRN 
REL=1, ..7, All
REL1, ?, REL7, or NUL
DT SV ME
SRNPAT
PAT=1-M REL=1, ..,7, All
REL1, ?, REL7, or NUL
DT SVSS ISS
Fe
a
tu
re
s Feature  Extraction
FeatureSetsSRN
[Arg1Arg2Pattern, Arg1, Arg2]
FeatureSetsSRNPAT
[Pattern, NPArg1, NPArg2]
Se
le
c
tio
n Relation Selection
[Arg1, Arg2, Relation, Score]
Sentences Annotations
Pa
tte
rn
s
[Pattern, NPArg1, NPArg2]
Pattern Matching
[Arg1Arg2Pattern, Arg1, Arg2]
Argument Detection
O
u
tp
u
t Generate Output
REL1: SENTID Value REL7: SENTID Value?
Pr
e
-
pr
o
c
es
s
Pr
e
-
pr
o
c
es
s
Cl
a
s
si
fic
a
tio
n
Cl
a
s
si
fic
a
tio
n
Fe
a
tu
re
s
Fe
a
tu
re
s
Se
le
c
tio
n
Se
le
c
tio
n
Pa
tte
rn
s
Pa
tte
rn
s
O
u
tp
u
t
 
Figure 1. The architecture of our SRN system.  
2.1 Text Preprocessing 
The sentences were processed using an in-house 
text tokenizer, Brill?s part-of-speech tagger, an in-
house WordNet?based concept detector, an in-
house Named Entity Recognizer, and an in-house 
syntactic parser.  
Then, the syntactic and semantic information 
obtained using these tools (concepts, part of 
speech, named entities, etc) or obtained from the 
sensekeys for the arguments as provided by the 
Task 4 organizers (e.g. word senses, lemmas, etc) 
were mapped into the syntactic trees. If an argu-
ment corresponds to more than one tree node, the 
annotation was mapped to the phrase containing 
the two nodes.  
2.2 Learning and Classification Methods 
The core of our system is the learning and clas-
sification module.  
We used two types of methods: pattern-
dependent that uses the syntactic parsed trees for 
extracting and assigning a label to the arguments 
and pattern-independent that creates classifiers 
form all the examples disregarding the pattern in 
the tree. 
2.2.1 Pattern-independent Methods (SRN) 
Considering the limited number of examples for 
each pattern, we developed pattern-independent 
methods for classifying the semantic relations us-
ing the provided argument annotations and the 
context from the sentence.  
We built two types of classifiers: binary that 
focuses on building a classifier for a specific rela-
tion (SRNREL) and multi-class methods that build 
classifiers for all the SRN relations (SRN). Table 1 
presents the accuracy of the classifiers built using 
different machine learning algorithms.  
Relation DT SVM ME 
1 52.10 46.15 46.67 
2 41.40 30.76 60.00 
3 61.70 51.61 63.33 
4 59.30 52.17 53.33 
5 58.60 39.99 50.00 
6 71.70 24.99 73.33 
7 50.00 57.13 43.33 
Avg 56.40 43.26 55.71 
Table 1. The accuracy of the SRNREL classifiers 
built using different machine learning algorithms.  
The classifiers were built using lexical, seman-
tic, and syntactic features of the arguments, their 
phrases, their clauses, their common phrase/clause, 
and their modifier or head phrase. The system uses 
WordNet, an in-house Named Entity Recognizer, 
and an in-house Syntactic Parser for determining 
the values of some of these features. Table 2 pre-
sents the list of features used by the SRN classifi-
ers. 
Argument?s lexical, semantic, and syntactic features: the 
surface form, the label (POS tag or phrase label), the named 
entity (human, group, location, etc), the WordNet hierarchy 
(entity, group, abstraction, etc), the Semantic Scattering 
class (e.g. object, substance, etc), the grammatical role (sub-
ject or object of the clause), the syntactic parser structure, 
the POS Pattern (the sequence of POS of the words from the 
argument), and the phrase pattern (the sequence of labels of 
the phrases, words from the argument);  
Argument's phrase features:  surface form, label, gram-
matical role, named entity, POS pattern, Phrase patterns;  
Argument's Modifier/Head features: the label, surface 
forms, NE, and WN Hierarchy  for the first modifier, post 
modifier, pre-modifier, and head; 
Arguments' common tree node features: label, named 
entity, grammatical role, POS pattern, and phrase pattern, 
the tree path between arguments, and their order in tree; 
Arguments' clause: label, verb, voice, POS pattern, phrase 
pattern. 
Table 2. The list of features used for the SRN classi-
fiers.  
2.2.2 Pattern-dependent Methods (SRNPAT) 
The second type of methods we used, were for 
particular patterns frequent in the training corpus. 
Table 3 shows the list of most frequent patterns in 
the training corpus. For having general pattern and 
covering the arguments that correspond to more 
216
than one node in a tree, we considered as argument 
the noun phrase that contains the nominal instead 
of the node for the nominal.  
Pattern name Example 
Noun compounds: 
NN1 NN2 
If you are cleaning a <e1>coffee</e1> 
<e2>maker</e2> that hasn't been 
cleaned regularl271076ad 
y, repeat this step again with a fresh 
vinegar and water mixture. 
Of-genitives: 
NP1 of NP2  
The incoming <e1>chairman</e1> of 
the <e2>committee</e2> is promising 
an array of oversight investigations 
that could provoke sharp disagreement 
with Republicans and the White 
House. 
S-genitives: 
NP1 ?s NP2 
This is the <e1>government</e1>'s 
<e2>effort</e2> to encourage more 
employers to open up childcare centres 
at the respective ministries and gov-
ernment departments. 
Prepositional 
constructions: 
NP1 IN NP2 
I believe that unless we take this issue 
seriously, the red squirrel is facing 
eventual <e1>extinction</e1> from 
the <e2>woods</e2> of Scotland. 
Verbal construc-
tions: 
NP1 VB NP2 
On both of my systems, the 
<e1>reboot</e1> produced the omi-
nous <e2>message</e2> 'Missing 
operating system'. 
Verbal preposi-
tional construc-
tions: 
NP1 VB IN NP2 
Manila radio station DZMM quoted 
survivors as saying that the 
<e1>fire</e1> started with an 
<e2>explosion</e2> in the cargo hold 
and spread across the ship within min-
utes. 
Table 3. The most frequent patterns found in the 
training corpus. 
For the pattern-dependent methods we adapted 
some of our existing binary and multi-class classi-
fiers to work with the SRN relations.  
For the SRN system we used only one binary 
classifier built for the Part-Whole relation (relation 
6) using the ISS learning algorithm and 
trained/tested on the examples used in (Girju, 
Badulescu, and Moldovan, 2006) and different 
multi-class classifiers for the first 4 patterns from 
Table 3 built using DT, SVM, SS, and NB learning 
algorithms trained on a corpus annotated with 40 
semantic relations (extracted from Wall Street 
Journal articles from the TreeBank collection and 
LATimes articles from TREC 9 collection) that 
includes the 7 SRN relations (or equivalents). 
(Badulescu, 2004) gives more details on this list of 
relations (definitions, examples, distribution on 
corpus, etc). Table 4 shows the accuracy of these 
classifiers on other WSJ and LAT articles for the 
40 LCC relations and respectively Part-Whole rela-
tion for the most frequent patterns from the SRN 
corpus (Table 3). 
Pattern cluster SS DT NB SVM ISS 
Noun compounds 52.54 47.8 53.45 74.79 73.59 
S-genitives 62.27 56.2 58.27 72.66 87.26 
Of-genitives 67.55 53.1 54.63 72 87.26 
Prepositional con-
structions 
43.48 43.3 41.92 64.52 75.97 
Table 4. The accuracy of the SRNPAT classifiers for 
the list of 40 LCC relations and the Part-Whole Rela-
tion.  
2.3 Relation Selection 
Any of the SRN or SRNPAT classifiers can return 
a relation for a pair of arguments. The best relation 
is selected by weighting them using the following 
predefined rules:  
 The relations returned by the SRN classifiers 
weight more than the ones returned by SRNPAT 
classifiers because they were trained on the task 
annotated examples 
 The relations returned by the binary classifiers 
weight more than the ones returned by multi-class 
classifiers because they focus on one relation and 
therefore are more precise.  
3 Experimental Results 
3.1 Experiments on Testing 
During the competition we performed several 
experiments to assess the correct combination of 
classifiers that leads to the best results. 
The organizer provided 140 examples for each 
of the 7 relations. For testing the classifiers we 
trained the system on the first 110 examples and 
tested it on the last 30 of them.  
We performed different sets of experiments. 
 Experiments with one type of classifiers. 
These experiments showed that ME has a best per-
formance (55.1) 10.05 more than DT and 8.05 
more than SV. ME also got the highest score for 
Cause-Effect, while DT obtained the best score for 
Product-Producer.  
 Experiments with multiple classifiers. These 
experiments showed that DT+SV+SS+ISS has the 
best score (66.72) followed by DT+SS+ISS with 
55.66. Also by adding the SS and ISS classifiers 
the DT score increased with 10.51, the SV score 
with 5.81 and the DT+SV with 20.57.   
217
 Experiments with types of methods. These 
experiments showed that the SRN methods (with a 
score 0.44) are better than the SRNPAT methods 
(with a score of 0.41) with 0.03 which was ex-
pected since SRN were trained on provided exam-
ples. 
Table 5 shows the results of our SRN system 
when using specific classifiers or a combination of 
classifiers. The time did not permit us to do any 
experiments with the ME and NB classifiers.  
Classifier Combination Average F-measure 
DT 45.05 
SV 47.05 
ME 55.10 
DT+SV 46.15 
DT+SS+ISS 55.66 
SV+SS+ISS 52.96 
DT+SV+SS+ISS 66.72 
SRN 44.31 
SRNPAT 41.15 
Table 5. The results of some of our experiments with 
the different classifiers on the testing corpus. 
We submitted the DT+SS+ISS version because 
of its closeness to the normal distribution rather 
than DT+SV+SS+ISS that had a better f-measure 
but it was closer to All-True. The evaluation re-
sults showed that the testing examples we used 
were representative and the DT+SV+SS+ISS pro-
duce better results.  
3.2 Results 
Table 6 shows the results obtained by our sys-
tem on the evaluation corpus for the B4 case (using 
WordNet but not the query and all the training ex-
amples.  
Relation Precision Recall F-measure Accuracy 
1 50.8 73.2 60.0 50.0 
2 54.5 31.6 40.0 53.8 
3 66.7 100.0 80.0 66.7 
4 80.0 22.2 34.8 63.0 
5 42.2 65.5 51.4 42.3 
6 39.6 80.8 53.2 48.6 
7 57.1 31.6 40.7 51.4 
Avg 55.9 57.8 51.4 53.7 
Table 6. The results of our system on the evaluation 
corpus. 
Table 7 shows a comparison of our results with 
the following baseline systems: All-True, a system 
that always returns true, Majority, a system that 
always returns the majority value from the training, 
and Prob-Match, a system that randomly generate 
the value. We have obtained a larger precision and 
accuracy than the All-True and the Prob-Match 
systems. However, we obtained a lower recall and 
therefore an F-measure. 
System Preci-
sion 
Recall F-
measure 
Accuracy 
All-True 48.5 100.0 64.8 48.5 
Majority 81.3 42.9 30.8 57.0 
Prob-Match 48.5 48.5 48.5 51.7 
LCC-SRN 55.9 57.8 51.4 53.7 
Table 7. Comparison with the baselines. 
3.3 Discussions 
The results are promising. However, there is still 
room for improvement. The system was developed 
in a limited time, and therefore it could have been 
benefited from more features, feature selection, 
more experiments, a more complex relation selec-
tion scheme (using learning), more patterns, and 
more types of machine learning algorithms (espe-
cially unsupervised ones).  
4 Conclusion 
We presented a system for classifying the semantic 
relations between nominals that combines the re-
sults of different methods (pattern-dependent or 
pattern-independent) and machine learning algo-
rithms (decision tree, support vector machines, se-
mantic scattering, maximum entropy, na?ve bayes, 
etc). The classifiers use lexical, semantic, and syn-
tactic features and external resources like WordNet 
and an in-house Named Entity dictionary.    
References 
Adriana Badulescu. 2004. Classification of Semantic 
Relations between Nouns. PhD Dissertation. Univer-
sity of Texas at Dallas. 
Dan Moldovan and Adriana Badulescu. 2005. A Seman-
tic Scattering Model for the Automatic Interpretation 
of Genitives. In Proceedings of HLT/EMNLP 2005. 
Roxana Girju, Adriana Badulescu, and Dan Moldovan. 
2006. Automatic Discovery of Part-Whole Relations. 
Computation Linguistics, 32:1.  
Roxana Girju et al 2007. Classification of Semantic 
Relations between Nominals: Description of Task 4 
in SemEval-1, In Proceedings of ACL-2007, SemE-
val-1 Workshop.  
218
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 219?222,
Prague, June 2007. c?2007 Association for Computational Linguistics
LCC-TE: A Hybrid Approach to  
Temporal Relation Identification in News Text 
Congmin Min 
Language Computer Corporation 
1701 N. Collins Blvd. Suite 2000 
Richardson, TX 75080 
cmin@languagecomputer.com 
 
Munirathnam Srikanth 
Language Computer Corporation 
1701 N. Collins Blvd, Suite 2000 
Richardson, TX 75080 
Srikanth.munirathnam 
@languagecomputer.com 
Abraham Fowler 
Language Computer Corporation 
1701 N. Collins Blvd, Suite 2000 
Richardson, TX 75080 
abraham@languagecomputer.com 
 
 
 
Abstract 
This paper explores a hybrid approach to 
temporal information extraction within the 
TimeML framework. Particularly, we focus on 
our initial efforts to apply machine learning 
techniques to identify temporal relations as 
defined in a constrained manner by the 
TempEval-2007 task. We explored several 
machine learning models and human rules to 
infer temporal relations based on the features 
available in TimeBank, as well as a number of 
other features extracted by our in-house tools. 
We participated in all three sub-tasks of the 
TempEval task in SemEval-2007 workshop 
and the evaluation shows that we achieved 
comparable results in Task A & B and 
competitive results in Task C.       
1 Introduction 
There has been a growing interest in temporal 
information extraction in recent years, as more and 
more operational NLP systems demands dealing 
with time-related issues in natural languages. In 
this paper, we report on an end-to-end system that 
is capable of automating identification of temporal 
referring expressions, events and temporal 
relations in text by leveraging various NLP tools 
and linguistic resources at LCC.  
It has to be noted that the system we report here 
is not only intended for TempEval 2007 
evaluation, but will also be used as a NLP tool for 
our other applications (e.g. temporal Question 
Answering). That is why we experimented to use 
our own temporal and event extraction capabilities 
in this work, although time and event tags have 
already been provided in the testing/training data. 
Another reason we use our own temporal tagging 
is that our temporal tagger extracts more 
information than that available in the 
training/testing data. For instance, temporal signals 
are removed from the data that the task organizers 
provide, but our temporal tagger detects that, as 
part of the tagging procedure. The following is an 
example for the tagged expression ?on this coming 
Sunday?. 
 <ArgStructure id="65" type="timex"> 
      <argRef type="determiner" tokStr="this"/> 
       <argRef type="directionIndicator?  tokStr="coming"/> 
       <argRef type="focus"  tokStr="Sunday"/> 
       <argRef type="prepSignal?  tokStr="on"/> 
       <argRef type="head"  tokStr="this coming Sunday"/> 
       <argRef type="root"  tokStr="on this coming Sunday"/> 
      <argValue type="focusType" value="weekOfDay"/> 
      <argValue type="subType" value="Fuzzy"/> 
      <argValue type="type" value="Date"/> 
</ArgStructure> 
Our data structure allows us to easily access and 
manipulate any part of the tagged chunk of text, 
which leaves the interpretation of whether the 
temporal signal on in the example is part of the 
temporal expression to users of temporal tagger. 
Taking as input this data structure, the 
normalization, including relative date resolution, is 
a straightforward process, provided that the 
reference time can be computed from the context. 
For temporal relation identification, by 
leveraging the capabilities of our temporal tagger, 
event tagger and several other in-house NLP tools, 
we derive a rich set of syntactic and semantic 
features for use by machine learning. We also 
explored the possibility of combining the rule-
based approach with machine learning in an 
integrated manner so that our system can take 
advantage of these two approaches for temporal 
relation identification. 
2 System Architecture 
The overall architecture of our end-to-end system 
is illustrated in Figure 1 (Page 2).  
219
In addition to several common NLP tools, e.g. 
Named Entity Recognizer, we use syntactic and 
semantic parsers to identify syntactic and semantic 
roles (e.g. AGENT or SUBJECT) of event terms 
and a context detector to detect linguistic contexts 
in a discourse. We use such information as 
extended features for machine learning. The 
Temporal Tagger tags and normalizes temporal 
expressions conforming to the TimeML guideline. 
The Temporal Merger compares our own temporal 
and event tagging with those supplied in 
training/testing data. If there is any inconsistency, 
it will replace the former with the latter, which 
guarantees that our temporal and event tagging are 
the same as those in training/testing data. Feature 
Extractor extracts and composes features from 
documents processed by the NLP tools. Machine 
Learner and Human Rule Predictor take as input 
the feature vector for each instance to predict 
temporal relation. The Human Rule Predictor is a 
rule interpreter that read hand-crafted rules from 
plain text file to match each event instance 
represented by a feature vector.  
Note that in Figure 1, Syntactic Parsing is done 
by a probabilistic chart parser, which generates full 
parse tree for each sentence. Syntactic Pattern 
Matching is performed by a syntactic pattern 
matcher, which operates on parse trees produced 
by chart parser and used by Temporal Tagger to 
tag and normalize temporal expressions.  
 
 
 
 
 
 
 
 
 
 
 
 
      
   Figure 1. Overall System Architecture 
3 Feature Engineering 
While temporal tagging and normalization is rule-
based in our system, temporal relation 
identification is a combination of machine learning 
and rule-based approaches. For machine learning, 
the feature set for the three tasks A, B and C we 
engineered consist of what we call 1) first-class 
features; 2) derived features; 3) extended features, 
and 4) merged features. The way we name the type 
of features is primarily for illustrating purpose.  
3.1 First-class Features 
The first-class features consist of: 
? Event Class 
? Event Stem 
? Event and time strings 
? Part of Speech of event terms 
? Event Polarity 
? Event Tense 
? Event Aspect 
? Type of temporal expression 
? Value of temporal expression 
The set of first-class features, which are directly 
obtained from the markups of training/testing data, 
are important, because most of them, including 
Event Class, Event Stem, POS, Tense and Type of 
Temporal Expression, have a great impact on 
performance of machine learning classifiers, 
compared with effects of other features. 
3.2.2 Derived Features  
From the first-class features, we derive and 
compute a number of other features: 
? Tense and aspect shifts1 
? Temporal Signal 
? Whether an event is enclosed in quotes 
? Whether an event has modals prior to it  
? Temporal relation between the Document 
Creation Time and temporal expression in the 
target sentence. 
The way we compute tense and aspect shifts is 
taking pair of contiguous events and assign a 
true/false value to each relation instance based on 
whether tense or shift change in this pair. Our 
experiments show that these two features didn't 
contribute to the overall score, probably because 
they are redundant with the Tense and Aspect 
features of each event term. Temporal Signal 
                                                 
1 Initially used in (Mani, et. al. 2003) 
Human Rule Predictor Machine Learning 
ML Testing Documents with New 
TLINKs 
Word Sense Disambiguation 
NE & POS Tagging 
Syntactic Parsing 
Syntactic Pattern Matching 
Semantic Parsing 
Context Detection 
Temporal Tagging & Normalizing 
Temporal Merging 
Feature Extraction & Composition 
Documents with 
TempEval Markups 
220
represents temporal prepositions and they slightly 
contribute to the overall score of classifiers.  
The last feature in this category is the Temporal 
Relation between the Document Creation Time and 
the Temporal Expression in the target sentence. 
The value of this feature could be ?greater than?, 
?less than?, ?equal?, or ?none?. Experiments show 
that this is an important feature for Task A and B, 
because it contributes several points to the overall 
score. This value may be approximate for a 
number of reasons. For example, we can?t directly 
compare a temporal expression of type Date with 
another expression of type Duration. However, 
even if we apply a simple algorithm to compute 
this relationship, it results in a noticeably positive 
effect on the performance of the classifier.      
3.2.3 Extended Features 
Features in the third category are extracted by our 
in-house tools, including: 
? Whether an event term plays primary semantic 
or syntactic roles in a sentence 
? Whether an event and a temporal expression 
are situated within the same linguistic context 
? Whether two event terms co-refer in a 
discourse (This feature is only used for Task C) 
Investigation reveals that different types of 
events defined in TimeML may or may not have 
specific semantic or syntactic roles (e.g. THM or 
OBJECT) in a particular context, therefore having 
an impact on their ways to convey temporal 
meanings. Experiments show that use of semantic 
and syntactic roles as binary features slightly 
increases performance.              
The second feature in this category is Context 
feature. We use a context detection tool, which 
detects typical linguistic contexts, such as 
Reporting, Belief, Modal, etc. to decide whether an 
event and a temporal expression are within one 
context. For example2,  
? The company has reported declines in 
operating profit in each of the past three 
years, despite steady sales growth.  
In this example, we identify a Reporting context 
with its signal reported. The temporal expression 
each of the past three years and the event declines 
are within the same context (the feature value 
would be TRUE). We intend this feature can help 
                                                 
2 This sentence is taken from the file wsj_0027.tml in 
TempEval 2007?s training data.  
solve the problem of anchoring an event to its 
actual temporal expressions. In fact, we don't 
benefit from the use of this feature, probably 
because detecting those linguistic contexts is a 
problem in itself. 
The third feature in this category is co-
referential feature, which is only used for Task C. 
This feature indicates if two event terms within or 
outside one sentence are referring to the same 
event. Experiments show that this global feature 
produces a positive effect on the overall 
performance of the classifier.    
3.2.4 Merged Features  
The last type of feature we engineered is the 
merged feature. Due to time constraint, as well as 
the fact that the system for Task B produces better 
results than Task A and C, we only experimented 
merging the output of the system for Task B into 
the feature set of Task C and we achieved 
noticeable improvements because of adding this 
feature. 
Most of the features introduced above are 
experimented in all three tasks A, B and C, except 
that the co-referential feature and the merged 
feature are only used in Task C. Also, in Task C 
since for each relation there are two events and 
possibly two temporal expressions, the number of 
features used is much more than that in Task A and 
B. The total number of features for Task C's 
training is 35 and 33 for testing.  
3.1 Combination of Machine Learning and 
Human Rule 
The design of our system allows both human rule-
based and machine learning-based decision 
making. However, we have not decided exactly in 
what situations machine learning and human rule 
prediction should be used given a particular 
instance. The basic idea here is that we want to 
have the option to call either component on the fly 
in different situations so that we can take 
advantage of the two empirical approaches in an 
integrated way. We did some initial experiments 
on dynamically applying Human Rule Predictor 
and Machine Learner on Task B and we were able 
to obtain comparable results with or without using 
hand-crafted rules. As pointed out in (Li, et, al. 
2006), Support Vector Machine, as well as other 
classifiers, makes most mistakes near the decision 
plane in feature space. We will investigate the 
221
possibility of applying human rule prediction to 
those relation instances where Machine Learning 
makes most mistakes.  
3.2 Experiments and Results 
Based on the features discussed in Section 3.3, we 
did a series of experiments for each task on four 
models: Naive-Bayes, Decision Tree (C5.0), 
Maximum Entropy and Support Vector Machine. 
Due to space constraint, we only report results 
from SVM model 3 , which produces best 
performance in our case.  
We here report two sets of performance numbers. 
The first set is based on our evaluation against a 
set of held-out data, 20 documents for each task, 
which were taken from the training data. The 
second set of performance numbers is based on 
evaluation against the final testing data provided 
by task organizers.  
strict relaxed  
P R F P R F 
Task A 0.68 0.68 0.68 0.69 0.69 0.69 
Task B 0.80 0.80 0.80 0.82 0.82 0.82 
Task C 0.63 0.63 0.63 0.67 0.67 0.67 
Table 1. Performance figures evaluated against held-out data 
 
strict relaxed  
P R F P R F 
Task A 0.59 0.57 0.58 0.61 0.60 0.60 
Task B 0.75 0.71 0.73 0.75 0.72 0.74 
Task C 0.55 0.55 0.55 0.60 0.60 0.60 
Table 2. Performance figures evaluated against testing data 
 
strict relax Team 
P R F P R F 
Ours 0.59 0.57 0.58 0.61 0.60 0.60 
Average 0.59 0.54 0.56 0.62 0.57 0.59 
Best 0.62 0.62 0.62 0.64 0.64 0.64 
Table 3. Performance figures in Comparison for Task A 
 
strict relax Team 
P R F P R F 
Ours 0.75 0.71 0.73 0.76 0.72 0.74 
Average 0.76 0.72 0.74 0.78 0.74 0.75 
Best 0.80 0.80 0.80 0.84 0.81 0.81 
Table 4. Performance figures in comparison for Task B 
 
strict relax Team 
P R F P R F 
Ours 0.55 0.55 0.55 0.60 0.60 0.60 
Average 0.51 0.51 0.51 0.60 0.60 0.60 
Best 0.55 0.55 0.55 0.66 0.66 0.66 
Table 5. Performance figures in comparison for Task C 
                                                 
3 We use the LIBSVM implementation of SVM, 
available at http://www.csie.ntu.edu.tw/cjlin/libsvm 
According to Table 1 and 2, it appears that there 
are significant differences between the TLINK 
patterns in the held-out data and the final testing 
data, since the performance of the classifier shows 
an apparent discrepancy in two cases.  
Table 3, 4 and 5 show performance numbers of 
our system, the average and the best system in 
comparison. There are six teams in total 
participating in the TempEval 2007 evaluation this 
year.   
4 Conclusion 
We participated in the SemEval2007 workshop and 
achieved encouraging results by devoting our 
initial efforts in this area. In next step, we plan to 
seek ways to expand the training data, implement 
quality human rules by performing rigorous data 
analysis, and explore use of more features for 
machine learning through feature engineering.   
References 
B. Boguraev and R.K. Ando. 2005. TimeML-compliant 
Text Analysis for Temporal Reasoning. Proceedings 
of IJCAI, UK. 
D. Ahn, S.F. Adafre and M.D. Rijke. 2005. Towards 
Task-based Temporal Extraction and Recognition. 
Dagstuhl Seminar Proceedings 05151.  
Inderjeet Mani and George Wilson. 2000. Robust 
Temporal Processing of News. Proceedings of 
ACL?2000.  
Inderjeet Mani, Barry Schiffman, and Jianping Zhang. 
2003. Inferring Temporal Ordering of Events in 
News. Proceedings of HLT-NAACL?03, 55-57. 
K. Hacioglu, Y. Chen and B. Douglas. 2005. Automatic 
Time Expression Labeling for English and Chinese 
Text, Proceedings of CICLing-2005. 
L. Li, T. Mao, D. Huang and Y. Yang. 2006. Hybrid 
Models for Chinese Named Entity Recognition. 
Proceedings of the Fifth SIGHAN Workshop on 
Chinese Language Processing. 
The TimeML Working Group. 2005. The TimeML 1.2 
Specification. 
http://www.timeml.org/site/publications/specs.html 
222
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 223?226,
Prague, June 2007. c?2007 Association for Computational Linguistics
LCC-WSD: System Description for English Coarse Grained All Words Task
at SemEval 2007
Adrian Novischi, Munirathnam Srikanth and Andrew Bennett
Language Computer Corp.
Richardson, TX
 
adrian,srikanth,abennet  @languagecomputer.com
Abstract
This document describes the Word Sense Disam-
biguation system used by Language Computer Cor-
poration at English Coarse Grained All Word Task
at SemEval 2007. The system is based on two su-
pervised machine learning algorithms: Maximum
Entropy and Support Vector Machines. These algo-
rithms were trained on a corpus created from Sem-
Cor, Senseval 2 and 3 all words and lexical sample
corpora and Open Mind Word Expert 1.0 corpus.
We used topical, syntactic and semantic features.
Some semantic features were created using WordNet
glosses with semantic relations tagged manually and
automatically as part of eXtended WordNet project.
We also tried to create more training instances from
the disambiguated WordNet glosses found in XWN
project (XWN, 2003). For words for which we could
not build a sense classifier, we used First Sense in
WordNet as a back-off strategy in order to have cov-
erage of 100%. The precision and recall of the over-
all system is 81.446% placing it in the top 5 systems.
1 Introduction
The performance of a Word Sense Disambiguation
(WSD) system using a finite set of senses depends
greatly on the definition of the word senses. Fine
grained senses are hard to distinguish while coarse
grained senses tend to be more clear. Word Sense
Disambiguation is not a final goal, but it is an in-
termediary step used in other Natural Processing ap-
plications like detection of Semantic Relations, In-
formation Retrieval or Machine Translation. Word
Sense Disambiguation is not useful if it is not per-
formed with high accuracy (Sanderson, 1994). A
coarse grained set of sense gives the opportunity to
make more precise sense distinction and to make a
Word Sense Disambiguation system more useful to
other tasks.
Our goal at SemEval 2007 was to measure the per-
formance of known supervised machine learning al-
gorithm using coarse grained senses. The idea of us-
ing supervised machine learning for WSD is not new
and was used for example in (Ng and Lee, 1996).
We made experiments with two supervised methods:
Maximum Entropy (ME) and Support Vector Ma-
chines (SVM). These supervised algorithms were
used with topical, syntactic and semantic features.
We trained a classifier for each word using both su-
pervised algorithms. New features were added in
3 incremental steps. After an initial set of experi-
ments the algorithm performance was enhanced us-
ing a greedy feature selection algorithm similar to
one in (Mihalcea, 2002). In order to increase the
number of training instances, we tried to use the
disambiguated WordNet glosses from XWN project
(XWN, 2003). Combining other corpora with dis-
ambiguated glosses from XWN did not provide any
improvement so we used XWN as a fall back strat-
egy for 70 words that did not have any training ex-
amples in other corpora but XWN.
Section 2 describes the supervised methods used
by our WSD system, the pre-processing module and
the set of features. Section 3 presents the exper-
iments we performed and their results. Section 4
draws the conclusions.
223
2 System Description
The system contains a preprocessing module used
before computing the values of the features needed
by the machine learning classifiers. The preprocess-
ing module perform the following steps:
  Tokenization: using an in house text tokenizer
  Named Entity Recognition: using an in house
system
  Part of Speech Tagging: normally we use the
Brill tagger, but we took advantage of the part
of speech tags given in the test file
  WordNet look-up to check if the word exists
in WordNet and to get its lemma, possible part
of speech for that lemma and if the word has
a single sense or not. For SemEval English
Coarse All Words task we took advantage by
the lemma provided in the test file.
  Compound concept detection: using a classifier
based on WordNet
  Syntactic Parsing: using an in-house imple-
mentation of Collin?s parser (Glaysher and
Moldovan, 2006)
The Maximum Entropy classifier is a C++ imple-
mentation found on web (Le, 2006). The classifier
was adapted to accept symbolic features for classifi-
cation tasks in Natural Language Processing.
For training SVM classifiers we used LIBSVM
package (Chang and Lin, 2001). Each symbolic fea-
ture can have a single value from a finite set of val-
ues or can be assigned a subset of values from the set
of all possible values. For each value we created a
mapping between the feature value and a dimension
in the N-dimensional classification space and we as-
signed the number 1.0 to that dimension if the fea-
ture had the corresponding value or 0.0 otherwise.
We first performed experiments with our existing
set of features used at Senseval 3 All Words task. We
call this set

. Then we made three incremental
changes to improve the performance.
The initial set contains the following features:
current word form (CRT WORD) and part of speech
(CRT POS), contextual features (CTX WORD) in
a window (-3,3) words, collocations in a window
of (-3,3) words (COL WORD), keywords (KEY-
WORDS) and bigrams (BIGRAMS) in a window of
(-3,3) sentences, verb mode (VERB MODE) which
can take 4 values: ACTIVE, INFINITIVE, PAST,
GERUND, verb voice (VERB VOICE) which can
take 2 values ACTIVE, PASSIVE, the parent of the
current verb in the parse tree (CRT PARENT) (ex:
VP, NP), the first ancestor that is not VP in the parse
tree (RAND PARENT) (like S, NP, PP, SBAR) and
a boolean flag indicating if the current verb belongs
to the main clause or not (MAIN CLAUSE).
We added new features to the initial set. We call
this set

.
  The lemmas of the contextual words in the win-
dow of (-3, 3) words around the target word
(CTX LEMMA).
  Collocations formed with the lemma of sur-
rounding words in a window of (-3, 3)
(COL LEMMA)
  The parent of the contextual words in the parse
tree in the window of (-3, 3) words around tar-
get word.
  Collocations formed with the parents of the sur-
rounding words in the window (-3, 3) words
around the target word (COL PARENT).
  Occurrences in the current sentence of the
words that are linked to the current word with
a semantic relation of AGENT or THEME in
WordNet 2.0 glosses (XWN LEMMA).
We used files from XWN project (XWN, 2003)
containing WordNet 2.0 glosses that were sense
disambiguated and tagged with semantic rela-
tions both manually and automatically. For
each word to be disambiguated we created a
signature consisting of the set of words that
are linked with a semantic relation of THEME
or AGENT in all WordNet glosses. For every
word in this set we created a feature showing if
that word appears in the current sentence con-
taining the target word.
Then we added a new feature consisting of all
the named entities in a window of (-5,5) sentences
around the target word. We called this feature
NAMED ENTITIES. We created the feature set
	 by adding this new feature to 
 .
In the end we applied a greedy feature selection
algorithm to features in


inspired by (Mihal-
cea, 2002). Because feature selection was running
very slow, the feature selection algorithm was run
224
CTX WORD 1 CTX WORD -2 CTX LEMMA 1 COL POS -2 0
CTX POS 1 CTX WORD -1 CTX LEMMA 2 COL LEMMA 0 1
CTX WORD 2 COL PARENT -3 -1 CTX LEMMA 3 COL PARENT -2 2
CRT WORD COL PARENT -3 2 NAMED ENTITIES CTX POS 3
CTX WORD -3 CTX WORD 3 COL PARENT -1 1 COL WORD -1 1
Table 1: The feature set   obtained from the features most selected by the greedy selection algorithm
applied to all the words in Senseval 2
only for words in Senseval 2 English lexical sample
task and the top 20 features appearing the most often
(at least 5 times) in the selected feature set for each
word were used to create feature set    presented
in table 1.
3 Experiments and results
For SemEval 2007 we performed several experi-
ments: we tested ME and SVM classifiers on the
4 feature sets described in the previous section and
then we tried to improve the performance using dis-
ambiguated glosses from XWN project. Each set of
experiments together with the final submission is de-
scribed in detail below.
3.1 Experiments with different feature sets
Initially we made experiments with the set of fea-
tures used at Senseval 3 All Words task. For training
the ME and SVM classifiers, we used a combined
corpus made from SemCor, Senseval 3 All Words
corpus, Senseval 3 Lexical Sample testing and train-
ing corpora and Senseval 2 Lexical sample train-
ing corpus. For testing we used Senseval 2 Lexi-
cal Sample corpus. We made 3 experiments for the
first three feature sets


,


,


. Both algo-
rithms attempted to disambiguate all the words (cov-
erage=100%) so the precision is equal with recall.
The precision of each algorithm on each feature set
is presented in table 2.
Algorithm  	 
 
ME 76.03% 75.86% 76.03% 77.56%
SVM 73.30% 71.36% 71.46% 71.90%
Table 2: The precision of ME and SVM classifiers
using 4 sets of features.
After the first 3 experiments we noticed that both
ME and SVM classifiers had good results using the
first set of features

. This seemed odd since we
Corpus Precision
SemCor 79.61%
XWN 57.21%
SemCor+XWN 79.44%
Table 3: The precision using SemCor and disam-
biguated glosses from XWN project
expected an increase in performance with the addi-
tional features. This led us to the idea that not all
the features are useful for all words. So we created a
greedy feature selection algorithm based on the per-
formance of the SVM classifier (Mihalcea, 2002).
The feature selection algorithm starts with an empty
set of features

, and iteratively adds one feature
from the set of unused features  . Initially the set
 contains all the features. The algorithm iterates
as long as the overall performance increase. At each
step the algorithm adds tentatively one feature from
the set  to the existing feature list

and measures
the performance of the classifier on a 10 fold cross
validation on the training corpus. The feature pro-
viding the greatest increase in performance is finally
added to  and removed from  .
The feature selection algorithm turned out to be
very slow, so we could not use it to train all the
words. Therefore we used it to train only the words
from Senseval 2 Lexical Sample task and then we
computed a global set of features by selecting the
first 20 features that were selected the most (at least
5 times).
This list of features was named     . Table 2 that
SVM classifier with 
  did not get a better per-
formance than
  
while ME surprisingly did get
1.53% increase in performance. Given the higher
precision of ME classifier, it was selected for creat-
ing the submission file.
225
3.2 Experiments using disambiguated glosses
from XWN project
The ME classifier works well for words with enough
training examples. However we found many words
for which the number of training examples was too
small. We tried to increase the number of training
examples using the disambiguated WordNet glosses
from XWN project. Not all the senses in the dis-
ambiguated glosses were assigned manually and the
text of the glosses is different than normal running
text. However we were curious if we could im-
prove the overall performance by adding more train-
ing examples. We made 3 experiments showed in
table 3. For all three experiments we used Sense-
val 2 English All Words corpus for testing. On the
first experiment we used SemCor for training, on the
second we used disambiguated glosses from XWN
project and on the third we used both. XWN did not
bring an improvement to the overall precision, so we
decided to use XWN as a fall back strategy only for
70 words that did not have training examples is other
corpora.
3.3 Final Submission
For final submission we used trained ME models
using feature set
 
for 852 words, representing
1715 instances using SemCor, Senseval 2 and 3
English All Words and Lexical Sample testing and
training and OMWE 1.0. For 50 words represent-
ing 70 instances, we used disambiguated WordNet
glosses from XWN project to train ME classifiers
using feature set
 
. For the rest of 484 words for
which we could not find training examples we used
the First Sense in WordNet strategy. The submitted
answer had a 100% coverage and a 81.446% preci-
sion presented in table 4.
LCC-WSD 81.446%
Best submission 83.208%
Table 4: The LCC-WSD and the best submission at
SemEval 2007 Coarse All Words Task
4 Conclusions
LCC-WSD team used two supervised approaches
for performing experiments using coarse grained
senses: Maximum Entropy and Support Vector Ma-
chines. We used 4 feature sets: the first one was the
feature set used in Senseval 3 and next two repre-
senting incremental additions. The fourth feature set
represents a global set of features obtained from the
individual feature sets for each word resulted from
the greedy feature selection algorithm used to im-
prove the performance of SVM classifiers. In addi-
tion we used disambiguated WordNet glosses from
XWN to measure the improvement made by adding
additional training examples. The submitted answer
has a coverage of 100% and a precision of 81.446%.
References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Elliot Glaysher and Dan I. Moldovan. 2006. Speeding up
full syntactic parsing by leveraging partial parsing de-
cisions. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 295?300, Sydney, Australia. Association
for Computational Linguistics.
Zhang Le, 2006. Maximum Entropy Modeling
Toolkit for Python and C++. Software avail-
able at http://homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
Rada Mihalcea. 2002. Instance based learning with au-
tomatic feature selection applied to word sense dis-
ambiguation. In Proceedings of the 19th Interna-
tional Conference on Computational Linguistics COL-
ING 2002, Taiwan.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: an exemplar-based approach. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 40?47, Morristown, NJ,
USA. Association for Computational Linguistics.
Mark Sanderson. 1994. Word sense disambiguation and
information retrieval. In Proceedings of SIGIR-94,
17th ACM International Conference on Research and
Development in Information Retrieval, pages 49?57,
Dublin, IE.
XWN, 2003. eXtended WordNet. Software available at
http://xwn.hlt.utdallas.edu.
226
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 857?864
Manchester, August 2008
Experiments with Reasoning for Temporal Relations between Events
Marta Tatu and Munirathnam Srikanth
Lymba Corporation
Richardson, Texas, United States
marta,srikanth@lymba.com
Abstract
Few attempts have been made to inves-
tigate the utility of temporal reasoning
within machine learning frameworks for
temporal relation classification between
events in news articles. This paper presents
three settings where temporal reasoning
aids machine learned classifiers of tempo-
ral relations: (1) expansion of the dataset
used for learning; (2) detection of inconsis-
tencies among the automatically identified
relations; and (3) selection among multiple
temporal relations. Feature engineering is
another effort in our work to improve clas-
sification accuracy.
1 Introduction
In recent years, there has been a growing inter-
est in temporal information extraction, as more
and more operational natural language processing
(NLP) systems demand dealing with time-related
issues in natural language texts. Machine learning-
based temporal relation identification has been ex-
plored by only a few researchers, including Bogu-
raev and Ando (2005), Mani et al (2006), Cham-
bers et al (2007), and the TempEval 2007 partici-
pants (Verhagen et al, 2007).
For a given ordered pair of elements (x
1
, x
2
),
where x
1
and x
2
are events or times, temporal re-
lation resolution is the task of automatic identifi-
cation of the relation r
i
? TempRel that tem-
porally links x
1
and x
2
. For example, given
the statement Mr. Antar was charged
e
137
last
month
t
237
in a civil suit
e
138
filed
e
140
in federal
c
? Lymba Corporation 2008. Licensed under the Cre-
ative Commons Attribution-Noncommercial-Share Alike 3.0
Unported license (http://creativecommons.org/licenses/by-
nc-sa/3.0/). Some rights reserved.
court in Newark by the Securities and Exchange
Commission (wsj 07781) and the pairs (e
137
, t
137
),
(e
137
, e
138
), and (e
138
, e
140
), the task is to automat-
ically label the given pairs with the is included,
is included, and simultaneous relations, respec-
tively. We note that the granularity of the temporal
relations (TempRel) varies from TimeML?s 14 re-
lations to TempEval?s three coarse-grain relations.
While machine learning approaches attempt to
improve classification accuracy through feature
engineering, Mani et al (2006) introduced a tem-
poral reasoning component to greatly expand the
training data. By computing the temporal closure
of the training data relations, they increased the
training set by a factor of 10. They reported en-
couraging accuracy of classification on event-event
and event-time relations. According to their ex-
periments, the event-event relation accuracy goes
from 62.5% to 94.95% and the event-time rela-
tion accuracy ranges from 73.68% to 90.16%. Re-
cently, extensions of Mani et al (2006)?s research
is briefly described in (Mani et al, 2007). This
technical report addresses two problems found
in (Mani et al, 2006): (1) feature vector dupli-
cation caused by the data normalization process
(once fixed, the accuracy drops to 76.56% and
83.23%) and (2) a somewhat unrealistic evaluation
scheme (we describe Mani et al (2007)?s results in
Section 4.1).
TempEval 2007 is the first standard evaluation
arena that consists of three temporal relation clas-
sification tasks (Verhagen et al, 2007). The par-
ticipants reported F-measure scores ranging from
42% to 55% for event-event relations, and 73% to
80% for event-time relations.
Because of their different experimental settings,
1All examples shown here are taken from TimeBank 1.2.
857
the results reported in (Mani et al, 2007) cannot
be directly compared with those of TempEval 2007
participants. Among others, the three major differ-
ences are:
1. significantly different training and testing
data. Although the datasets used Time-
Bank 1.2 (Pustejovsky et al, 2003), Mani
et al (2007) added the AQUAINT Corpus
(www.timeml.org) to their experimental data;
2. different sets of temporal relations. Mani et
al. (2006; 2007) target six normalized rela-
tions (before, immediately before (ibefore),
includes, ends, begins and simultaneous). In
TempEval 2007, a set of three coarse-grain
temporal relations was used (before, after,
and overlap).
3. different relation scope. In (Mani et al, 2006;
Mani et al, 2007), event-event temporal re-
lations are discourse-wide, i.e. any pair of
events can be temporally linked. For Tem-
pEval 2007, the event-event relations are re-
stricted to events within two consecutive sen-
tences.
These two modeling frameworks for solving the
problem of temporal relation classification pro-
duce highly dissimilar results. With this in mind,
we are interested in two issues in this paper: (1)
How might temporal reasoning assist in tempo-
ral relation identification? (2) What other features
might be used to improve the performance of clas-
sification? As a byproduct of our exploration to
these two questions, we hope to find some insights
on why the same problem explored under different
environment produces highly divergent results.
In this paper, we investigate several interactions
between temporal reasoning and a machine learn-
ing approach for temporal ordering of events in
natural language texts. We continue by describing
the data used for our experiments. In Section 3, we
briefly describe the set of features we currently use
to build Support Vector Machine (SVM) (Chang
and Lin, 2001) and Maximum Entropy (ME) mod-
els for temporal relation resolution. The three in-
teractions we envision between temporal reasoning
and the learned models are presented in Section 4.
In conclusion, we present a discussion of our ex-
perimental results and future research directions.
2 Data Preparation and Analysis
2.1 TimeBank 1.2
In this paper, we use the TimeBank 1.2 data (Puste-
jovsky et al, 2003). This is the first attempt to
create a corpus with human annotated temporal re-
lations. It contains 183 news documents collected
from several news agencies.
2.2 Data normalization
Similar to (Mani et al, 2006; Mani et al, 2007),
we use a normalized version of the 14 tempo-
ral relations annotated in TimeBank where the
inverse relations are removed and simultaneous
and identity are collapsed as well as during and
is included. The distribution of the normalized
event-event temporal relations annotated in the
data we used for training our temporal resolution
models is shown in Table 2.
2.3 Experimental data
For the experiments described in this paper, we
used a random 80-20 percent split of the TimeBank
data to train and test the learned classifiers (36 ran-
domly selected documents for testing and the re-
maining 147 for training the models) and 5-fold-
cross-validation of the training data for parame-
ter tuning. We note that our experimental setup
is closer to the one used in (Mani et al, 2006;
Mani et al, 2007). Noting that we do not use the
AQUAINT Corpus in our experiments, our results
can be compared with theirs, but not with the Tem-
pEval system performances.
3 Feature Engineering
As reported by participants in TempEval
2007 (Verhagen et al, 2007), (Boguraev and
Ando, 2005), (Chambers et al, 2007), and (Mani
et al, 2007), most of the features used for learning
are syntactic attributes extracted for single event
terms. In our work, we have experimented with
semantic features and attributes which take the
event?s linguistic context into consideration (Min
et al, 2007). Our experiments show that only
few features are critical and impact the classifier?s
accuracy (Table 1). These include the basic
features available in TimeBank, e.g. event-class,
tense, aspect, polarity, modality, stem, and part of
speech of event terms (Baseline row in Table 1).
Additional features that we explored include:
858
Feature set Accuracy (%)
Baseline 46.3
Baseline with sameActor +0.4
Baseline with eventCoref +0.2
Baseline with oneSent +4.0
Baseline with relToDocDate +0.2
Baseline with tensebigram +0.8
Baseline with tensetrigram +0.6
Baseline with all 57.4
Table 1: New features impact
1. sameActor. This binary feature indicates
whether the two events share the same se-
mantic role AGENT. The motivation behind
this feature is that two event terms, especially,
verbs, which have the same agent, have a
closer semantic relationship and, accordingly,
are temporally related.
2. eventCoref. This binary attribute captures the
event co-reference information. If two events
co-refer, even though they have different sur-
face forms, they must take place simultane-
ously. For instance, the offer and deal events,
mentioned in the following sentences, refer to
the same transaction and, therefore, must be
linked by a simultaneous relation.
a) Sony Corp. completed its tender offer for Columbia
Pictures Entertainment Inc., with Columbia sharehold-
ers tendering 99.3% of all common shares outstanding
by the Tuesday deadline.
b) Sony Columbia Acquisition Corp., formed for the
Columbia deal, will formally take ownership of the
movie studio later this month, a spokesman said.
3. oneSent. This binary feature is true if the two
events are part of the same sentence. Cham-
bers et al(2007) introduced this feature in
their experiments, and our analysis shows that
this attribute has a relatively larger contribu-
tion to the overall performance. The intu-
ition behind this feature is that the closer two
events get, the closer their temporal relation-
ship is.
4. relToDocDate. This feature encodes the tem-
poral relation between each event and the
Document Date. This was one of the sub-
tasks of TempEval 2007 and we used this re-
lationship as a feature. Our motivation is that
we might be able to infer the relationship be-
tween two events e
1
and e
2
from the tem-
poral relations they have with the Document
Date. For example, if before(e
1
, DocDate)
and after(e
2
, DocDate) are true, then
before(e
1
, e
2
). There may be two reasons for
the low impact of this feature: (1) an accu-
rate computation of the temporal relation be-
tween an event and the Document Date is not
easy, as demonstrated in TempEval 2007 and
(2) if two events have the same relation with
the Document Date, there is no way to deter-
mine the event-event relation.
5. tenseBigram and tenseTrigram. Going be-
yond using the tense attribute for single event
terms, we extract bigrams and trigrams with
the tense values of the current event and im-
mediately preceding and following events.
This feature is intended to reflect the tense
shifts of sequential events as part of a larger
context of the current event.
All these features have a positive impact on the
performance of the learned classifiers (Table 1).
Further improvement is desired and we use tem-
poral reasoning in three different settings in an at-
tempt to obtain more accurate temporal relations.
4 Temporal Reasoning
Following our feature set improvements for ma-
chine learned models of temporal relations, we
turned to temporal reasoning and explored differ-
ent ways in which it can aid the resolution of tem-
poral relations. We experimented with three dif-
ferent interactions between our temporal reasoning
and temporal relation resolution modules.
Our natural language reasoning engine (Tatu
and Moldovan, 2005; Tatu and Moldovan, 2007)
makes use of (1) a first-order logical representation
of the input document which captures the concepts
mentioned in the text, their attributes including
named entity class values, event class or normal-
ized values (for times) and the syntactic as well as
the semantic dependencies between concepts2; (2)
a rich set of axioms which encode the knowledge
needed to derive meaningful information from a
document; (3) a logic prover which operates in a
proof by contradiction manner (a hypothesis H is
entailed by a text T assumed to be true, denoted
by T ? H , if and only if (T ? ?H) ? ?, where ?
is false). Given the logical transformation of a text
T , the prover uses the knowledge encoded in the
2These dependencies include the temporal relations iden-
tified either by human annotators or by the models presented
in Section 3.
859
axioms (Bk) to derive new information (T ?) about
T
3 and scores the best mapping of the hypothesis
H to T ?.
For the temporal relation resolution experiments
presented in this paper, we are interested in de-
riving additional temporal information from an in-
put document without checking the entailment be-
tween this document and a hypothesis. There-
fore, for the following tasks, the text T is a
TimeBank 1.2 document and the set of axioms
Bk used by the prover contains 94 temporal
axioms which link each temporal relation with
its inverse (R?1(x, y) ? R(y, x), for example
before(x, y) ? after(y, x)) and define the tem-
poral relation resulting from the combination of
two relations (R
1
(x, y) ? R
2
(y, z) ? R
3
(x, z),
for example, before(x, y) ? before(y, z) ?
before(x, z)). These axioms were derived from
Allen?s interval algebra (Allen, 1983).
We note that the prover computes and uses tem-
poral relations between any two temporal expres-
sions mentioned in an input TimeBank document4
(e.g. now [19891101] and last year [1988] are
linked by a before temporal relation in wsj 0324).
Within this reasoning setup, the information de-
rived by the prover (T ?) will include the temporal
closure of the input text?s relations. We note that
the temporal closure includes event-event, event-
time and time-time temporal relations. We also
note that the temporal axioms are considered 100%
accurate (if the temporal relations given as input
are correct, then the temporal relations derived us-
ing the axioms are also correct).
4.1 Training data expansion
Our first effort to create more accurate temporal re-
lation resolution classifiers given our temporal rea-
soning engine is to augment the gold training data
with new relations from the temporal closure of the
relations identified by human annotators. There-
fore, given the 3,527 temporal relations annotated
in the TimeBank data used to train our initial tem-
poral resolution models, we derived 12,270 new
relations (an increase of 3.47 times). We show in
Table 2 statistics of the normalized event-event re-
lations for both the original and the closed train-
ing data. We note that the temporal inconsisten-
cies identified in the original training data (by the
3
T
? contains all the information the prover can derive
from T given the axioms Bk.
4We restricted the time-time relations to only before, si-
multaneous, and after.
Relation Original data Closed (?) data
Freq. % Freq. %
ibefore 51 2.06 137 1.59
begins 52 2.10 119 1.38
ends 61 2.47 125 1.45
includes 434 17.59 1,161 13.47
before 885 35.88 3,165 36.73
simultaneous 983 39.86 3,909 45.36
Total 2,466 100.00 8,616 100.00
Table 2: Normalized training data (event-event re-
lations)
procedure described in Section 4.2) were resolved
manually by one of the authors of this paper.
We built SVM and ME models from the total
of 8,616 normalized temporal relations using the
set of 15 features described in Section 3. Table 3
shows the performance of the learned models on
the test data (original data as well as closed test
data). Unlike (Mani et al, 2006), the accuracy of
Accuracy for event-event relations
Training data Original test Closed test Train
(845) (4,189)
ME models
Original (2,466) 50.4 46.1 83.3
Closed (8,616) 47.0 41.0 76.1
SVM models
Original (2,466) 56.9 45.8 74.2
Closed (8,616) 52.4 52.0 77.5
Table 3: Event-event temporal resolution
the learned classifiers drops when they are trained
on the closed training dataset. By analyzing the re-
sults from Table 3, one cannot help but notice the
high accuracy on the data used for training and the
significant difference between the performance on
the training and testing datasets. This may suggest
that (1) the machine learners overfit the models on
the training data and they are not able to gener-
alize and resolve the relations in the test data5 or
(2) the two datasets are very different (in terms
of feature values) and the data split happened to
create a training data which is not (fully) charac-
teristic to the problem we are trying to solve (the
two datasets have different distributions). There-
fore, we measured the accuracy of ME models for
event-event relation resolution using 5-fold-cross-
validation of the entire TimeBank data (Table 4).
For these experiments, each TimeBank document
(with all its temporal relations) was used as part
5The accuracy of the SVM models is lower on the training
data when compared with the ME models while their perfor-
mance on the test dataset is better.
860
Test data Train data
1/5 of the data remaining 4/5s
5-fold-cross split at the document level
Original (3,311) 57.4 89.5
Closed (11,530) 58.2 85.5
5-fold-cross split at the relation level
Original (3,311) 58.3 90.0
Closed (11,530) 73.4 85.3
Table 4: Average ME accuracy for event-event re-
lations using 5-fold-cross-validation on the entire
TimeBank data
of either the training or the testing dataset. Our re-
sults for the random 5-fold-cross split of the data at
the document level are similar to the ones obtained
for the models learned on the pre-established train-
ing data (top two rows in Table 4). Thus, our ini-
tial split of the data was not an ?unfortunate? divi-
sion. The same significant difference between the
performance on the unseen data and the training
set can be seen. This suggests that some overfit-
ting occurs. Features, such as the event term stem,
with a large number of possible values mislead the
machine learning algorithms and the models they
create are not able to correctly classify event pairs
with unseen values for these high-valued features.
For instance, showdown is part of a single Time-
Bank document (AP900816-0139) and the mod-
els learned using other documents will misclassify
showdown?s temporal relations. We note that, by
expanding the training data using its temporal clo-
sure, no new events are added to the training set,
only new temporal relations between the same set
of events are added. Long-term solutions include
(1) the expansion of the annotated data or (2) the
reduction in the number of values for certain fea-
tures (for example, by generalizing the event term
stem to its WordNet hypernym). In an attempt to
homogenize the feature values for the training and
the testing datasets, we split the set of normalized
event-event temporal relations annotated in Time-
Bank into training and testing without considering
the document boundaries. The performance of the
learned classifiers increases by 1% when trained
on unclosed data and by more than 15% when the
closed data is used (Table 4).
In their most recent technical paper, Mani et
al. (2007) revise their evaluation method and report
performance values for classifiers learned by par-
titioning the data at the document level (accuracy
drops from 59.68% to 51.14% when closed train-
ing data is used). These results are consistent with
our findings. In the near future, we shall experi-
ment with the second solution we propose above.
4.2 Testing data validation
Given that almost half of the temporal relations
automatically identified for the testing data are
incorrect when compared to the gold annotation,
we decided, as our second experiment, to use
temporal reasoning to find temporal inconsisten-
cies and replace some of the relations contribut-
ing to the inconsistency by the immediate lower
confidence relation returned by the learned classi-
fiers. For this purpose, we use an additional set
of 77 temporal axioms which encode the irreflex-
ivity of temporal relations (?R(x, x), for exam-
ple, ?before(x, x)) and their empty intersections
(R
1
(x, y) ? ?R
2
(x, y) when R
1
6= R
2
, for ex-
ample before(x, y) ? ?simultaneous(x, y)).
Our process of testing data validation is itera-
tive. Once a temporal inconsistency is identified
in the test data, it is resolved and the procedure
which computes the temporal closure is re-started.
A temporal inconsistency in a TimeBank docu-
ment (T ) is detected every time ? ? T ?. The
automatically identified temporal relations (part of
the text T ) which contributed to the derivation of
? become candidates for the resolution of the in-
consistency6. These candidates are sorted based
on the confidence assigned by the machine learn-
ing algorithm7 and the lowest confidence relation
is replaced either by the temporal relation found
by the prover which directly contradicted the auto-
matically identified relation8 (Figure 1(a)) or, for
the cases where such a relation does not exist, by
the immediate lower confidence relation identified
by the learned models (Figure 1(b)).
If, for example, for the statement The US is
bolstering its military presence in the gulf, as
President Clinton discussed
e
1
the Iraq crisis with
the one ally who has backed
e
2
his threat
e
3
of
force, British prime minister Tony Blair, the ME
classifier built in Section 3 identifies the tempo-
ral relations before(e
2
, e
1
) (confidence: 0.53),
before(e
3
, e
2
) (0.47) and includes(e
3
, e
1
) (0.42),
the prover identifies the temporal inconsistency
6The temporal closure axioms are accurate and do not ?in-
troduce? incorrect temporal relations.
7For all experiments which exploit the confidence as-
signed by the machine learning algorithm, we use the learned
ME models (SVM models do not provide a confidence for
their decision).
8The confidence of a relation derived by the prover is the
average of the its ?parent?s confidence values.
861
 event1 
event3 
event4 
event2 c2 
cmin 
c3 
c1 R34 
R23 
R12 R?13 
R?14 
R14 
(a) R
14
replaced by R?
14
 
event1 
event3 
event4 
event2 cmin 
c4 
c3 
c1 R34 
R23 
R12 R?13 
R?14 
R14 
(b) R
23
replaced by the next
best relation identified for
(event
2
, event
3
)
Figure 1: Temporal inconsistency resolution
generated by these three relations and replaces the
lowest confidence relation (includes(e
3
, e
1
)) with
the relation it derives from the closure of the other
two relations (before(e
3
, e
1
), confidence: 0.50).
We note that, during the inconsistency checking
process, all types of temporal relations are used
(event-event, event-time and time-time). For this
inconsistency resolution process, we make the as-
sumption that only one of the temporal relations
which generated the inconsistency is incorrect and
should be replaced.
For the testing dataset described in Section 2.3,
the validation algorithm found inconsistencies in
only 25% of the test documents. This is not very
encouraging, given that the accuracy of the tem-
poral relations identified in the other 75% of the
documents is 50.4%. The documents marked as
inconsistent include, on average, 3.66 temporal in-
consistencies (with a maximum of 8 in a single
document). For each pair of events, we consid-
ered only the top three temporal relations (in terms
of confidence) identified by the learned classifiers.
When the third relation identified for a given pair
of events had to be removed by the inconsistency
resolution algorithm, no other temporal relation
was added to replace it. Table 5 shows the impact
of the validation step on the unclosed test data.
Precision Recall F-measure
Baseline 50.4 50.4 50.4
With test validation 50.1 49.7 49.9
Table 5: Performance change after the testing data
validation step. The baseline is the ME model
learned on the original (unclosed) training data.
Our error analysis shows that, for each discov-
ered temporal inconsistency, more than one incor-
rect relation lead to an inconsistent temporal clo-
sure. Frequently enough, replacing the lowest con-
fidence relation does not resolve the inconsistency
and the temporal relations used to replace it are,
in turn, replaced in the next iterations (the confi-
dence of the replacing relation is lower than the
confidence of the replaced relation). The ME clas-
sifier?s numerous errors and the low applicability
of this process make its contribution to the overall
temporal relation resolution process negative. Our
future work will focus on (1) experimenting with
less erroneous data for which our one-incorrect-
relation-per-inconsistency assumption holds ((per-
haps) the models learned from closed training data
using a data split at the relation level) and (2) test-
ing the existence of a consistent temporal closure
in the absence of the lowest confidence relation. If
none of the six temporal relations that we use to la-
bel an event-event relation can replace the lowest
confidence relation and lead to a consistent tempo-
ral closure, then our candidate incorrect relation is
among the other higher confidence relations. We
also note that we rely heavily on the confidences
automatically assigned by the ME classifiers.
Mani et al (2007) briefly describe a Greedy
method for ensuring global consistency of auto-
matically labeled testing data. No evaluation re-
sults are reported. As far as we can tell, Mani at
el. (2007) use this algorithm to decide whether or
not to assign the top 1 relation automatically iden-
tified by ME classifiers to a given pair of events.
No attempts are made to replace this relation. Our
validation algorithm uses lower confidence rela-
tions found by the learned models for the same pair
of events to replace the lowest confidence relation
that leads to a temporal inconsistency.
4.3 Temporal relation alignment
In the previous section, we used temporal reason-
ing to replace certain relations automatically iden-
tified by the learned temporal relation resolution
models with the next best relation (in terms of
the confidence) found for the same pair of events.
For our third experiment, we use the top n re-
lations automatically identified for a single pair
of events. Across a document, these relation-
ships can be grouped to form different tempo-
ral orderings of the events mentioned in the doc-
ument. For instance, for four pairs of events,
81 different temporal settings can be created us-
ing the top 3 temporal relations. Figure 2 shows
two of these 81 facets ({R
12
, R
?
23
, R
?
34
, R
41
} and
{R?
12
, R
23
, R?
34
, R?
41
}) for events e
1
, . . . , e
4
.
For these event temporal orderings, we pro-
862
 R12,c1 
(event1,event2) (event2,event3) (event3,event4) (event4,event1) 
R?12,c?1 
R?12,c?1 
R23,c2 
R?23,c?2 
R?23,c?2 
R34,c3 
R?34,c?3 
R?34,c?3 
R41,c4 
R?41,c?4 
R?41,c?4 
Figure 2: Two possible relation alignments
pose to use our temporal reasoning module to de-
rive, score, and rank their temporal closures. We
make the assumption that the correct document-
level event ordering, the document?s temporal co-
hesion can be identified by measuring the closure
of the document?s temporal relations and that or-
derings that use incorrect relations do not gener-
ate good closures. Thus, the relations that generate
the best temporal closure will be considered final
and will be used to label the test document?s event-
event and event-time pairs.
For the example shown in Figure 2, 81 different
temporal closures are generated depending on the
set of relations used to derive them from. In order
to find the final four temporal relations between
events e
1
, . . . , e
4
, we score and order the derived
temporal closures. The best closure decides, for
each pair of events, which relation among the top
3 should be selected as final.
Our first step is to identify the best value for
n. Table 6 shows the maximum gain in perfor-
mance when multiple relations are considered for
the same pair of events (an instance is considered
correct if the gold annotation is among the top n
relations returned by the system).
Top n relations Accuracy (%)
1 57.40
2 80.66
3 94.49
4 97.38
5 99.12
6 100.00
Table 6: Top n oracle performance using 5-fold-
cross-validation on the TimeBank data
Because there is substantial improvement in the
top 3 relation set, we use for our experiments the
first three relations identified by the ME classi-
fiers. But, if we consider the top 3 relations for
each pair of events in a document, we end up with
3
N possible alignments, where N is the number of
event-event and event-time pairs9 and the scoring
9For each time-time pair, there is a single temporal relation
with confidence equal to 1.
Accuracy
Baseline - top 1 50.4
Oracle (upper bound) - top 3 92.4
With test alignment 47.5
Table 7: Test dataset performance change after the
testing data alignment step. The top 1 and top
3 baselines were generated using the ME model
learned on the original (unclosed) training data.
and ranking of all 3N temporal closures becomes
hardly possible. Therefore, we use a more Greedy
approach. Iteratively, we score and rank temporal
closures derived from a small set of top 3 relations
between N ? event-event pairs (N ? < N ) and any
final temporal relations. The best closure is used
to decide on N ? temporal relations which will be
added to the best partial alignment and will be used
to compute all the following temporal closures.
Secondly, we must identify the temporal clo-
sure scoring function. For our experiments, this
function takes into account the size of the tempo-
ral closure (|T ?|) as well as the confidence val-
ues of the relations identified by the ME classi-
fiers in the test set (not derived by the tempo-
ral closure algorithm) (only {c
12
, c
?
23
, c
?
34
, c
41
} and
{c?
12
, c
23
, c?
34
, c?
41
} for the example shown in
Figure 2). The correlation between these param-
eters and the scoring function is not straightfor-
ward. A preference for the confidence values fa-
vors closures which use only the top relations (in
terms of confidence). However, weighing the size
of the temporal closure leads to a result dominated
by relations that close very well10, such as simulta-
neous or before (which are also very frequent in the
dataset). In the settings which produced the results
shown in Table 7, we used the score
1
function: for
T = {(R
1
, c
1
), . . . , (R
k
, c
k
)},
score
1
(T
?
) = lg(|T
?
|)?
k
?
i=1
c
i
.
The temporal relation accuracy drops by 3% after
the relation selection among the top 3 best tem-
poral relations for the testing documents. Posibile
explanation: score
1
does not promote the close-
to-gold temporal closures. The difinition of a good
scoring function is not an easy process. Machine
learning approaches might give us better coefi-
cients for the parameters we consider. Alternativ-
elly, our main assumption might prove incorrect:
10When present, these relations will quickly generate many
others in the temporal closure.
863
temporal closure is not a good indicator of a docu-
ment?s event ordering. The information conveyed
by a document need not disclose a rich total order-
ing of its events.
5 Conclusion
In this paper, we briefly described our feature en-
gineering efforts for temporal relation resolution
and we analyzed three methods that exploit tem-
poral reasoning and, more specifically, the closure
of temporal relations, for the purpose of improving
the performance of machine learned classifiers of
temporal relations between events in text.
Based on our experiments, we find that feature
engineering helps improve the classification prob-
lem, when compared with several baseline perfor-
mances. However, given our current NLP capabil-
ities, it is clear that we are faced with the perfor-
mance bottleneck problem (accuracy below 60%).
Any attempt to derive more advanced features de-
mands more sophisticated methodologies of mod-
eling temporal expressions, events and their re-
lationships as well as advanced discourse under-
standing capabilities. For instance, the temporal
duration or the start/end time points of events are
highly useful for learning temporal relations. But,
this introduces an even more challenging problem.
In terms of the utility of temporal reasoning
in classifying temporal relation, the idea of using
temporal reasoning to boost training data is cer-
tainly sound. But in order for the boosted train-
ing data to really take effect, more advanced fea-
tures need to be investigated. Certainly, the pro-
cess of dividing the data into training and testing
has its impact on the system?s performance and we
are faced with the data sparseness problem. Tem-
poral inconsistencies in our automatically labeled
test dataset occurred in just a few test documents
and the resolution process did not impact the sys-
tem?s performance. Improvements are needed in
the process of selection of the to-be-replaced re-
lations. Temporal data alignment largely depends
on the function used to score the temporal closures
and we plan to analyze the temporal closure of
the training data and to explore other scoring func-
tions.
References
Allen, J. F. 1983. Maintaining Knowledge about
Temporal Intervals. Communications of the ACM,
26(11):832?843.
Boguraev, B. and R. K. Ando. 2005. TimeML-
Compliant Text Analysis for Temporal Reasoning.
In Proceedings of Nineteenth International Joint
Conference on Artificial Intelligence (IJCAI-05),
pages 997?1003, Edinburgh, Scotland, August.
Chambers, N., S. Wang, and D. Jurafsky. 2007. Clas-
sifying Temporal Relations Between Events. In Pro-
ceedings of the ACL 2007 Demo and Poster Sessions,
pages 173?176, Prague, Czech Republic, June.
Chang, C. and C. Lin, 2001. LIBSVM: a library
for support vector machines. Software available at
www.csie.ntu.edu.tw/?cjlin/libsvm.
Mani, I., M. Verhagen, B. Wellner, C. Min Lee, and
J. Pustejovsky. 2006. Machine Learning of Tem-
poral Relations. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the ACL, pages 753?760,
Sydney, Australia, July.
Mani, I., B. Wellner, M. Verhagen, and J. Pustejovsky.
2007. Three Approaches to Learning TLINKs in
TimeML. Technical Report CS-07-268, Computer
Science Department, Brandeis University, Waltham,
USA.
Min, Congmin, Munirathnam Srikanth, and Abraham
Fowler. 2007. LCC-TE: A Hybrid Approach to
Temporal Relation Identification in News Text. In
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages
219?222, Prague, Czech Republic, June.
Pustejovsky, J., P. Hanks, R. Sauri, A. See,
R. Gaizauskas, A. Setzer, D. Radev, B. Sundheim,
D. Day, L. Ferro, and M. Lazo. 2003. The TIME-
BANK Corpus. In Proceedings of Corpus Lin-
guistics, pages 647?656, Lancaster University (UK),
March.
Tatu, Marta and Dan Moldovan. 2005. A Semantic Ap-
proach to Recognizing Textual Entailment. In Pro-
ceedings of the HTL-EMNLP 2005, pages 371?378,
Vancouver, BC Canada, October.
Tatu, Marta and Dan Moldovan. 2007. COGEX at
RTE3. In Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing, pages
22?27, Prague, Czech Republic, June.
Verhagen, M., R. Gaizauskas, F. Schilder, M. Hep-
ple, G. Katz, and J. Pustejovsky. 2007. SemEval-
2007 Task 15: TempEval Temporal Relation Identi-
fication. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 75?80, Prague, Czech Republic, June.
864
Extracting Exact Answers to Questions Based on Structural Links?
Wei Li, Rohini K. Srihari, Xiaoge Li, M. Srikanth, Xiuhong Zhang, Cheng Niu
Cymfony Inc.
600 Essjay Road, Williamsville, NY 14221. USA.
{wei, rohini, xli, srikanth, xzhang, cniu}@cymfony.com
Keywords:  Question Answering, Information Extraction, Semantic Parsing, Dependency Link
?  This  work was partly supported by a grant from the Air Force Research Laboratory?s Information Directorate 
(AFRL/IF), Rome, NY, under contracts F30602-00-C-0037 and F30602-00-C-0090.
Abstract
This paper presents a novel approach to
extracting phrase-level answers in a question 
answering system. This approach uses
structural support provided by an integrated 
Natural Language Processing (NLP) and
Information Extraction (IE) system. Both
questions and the sentence-level candidate
answer strings are parsed by this NLP/IE
system into binary dependency structures.
Phrase-level answer extraction is modelled by 
comparing the structural similarity involving 
the question-phrase and the candidate answer-
phrase.
There are two types of structural support. The 
first type involves predefined, specific entity 
associa tions such as Affiliation, Position, Age 
for a person entity. If a question asks about 
one of these associations, the answer-phrase
can be determined as long as the system
decodes such pre-defined dependency links 
correctly, despite the syntactic difference
used in expressions between the question and 
the candidate answer string. The second type 
involves generic grammatical relationships
such as V-S (verb-subject), V-O (verb-
object).
Preliminary experimental results show an
improvement in both precision and recall in 
extracting phrase-level answers, compared
with a baseline system which only uses Named 
Entity constraints. The proposed methods are 
particularly effective in cases where the
question-phrase does not correspond to a
known named entity type and in cases where 
there are multiple candidate answer-phrases
satisfying the named entity constraints.
Introduction
Natural language Question Answering (QA) is 
recognized as a capability with great potential.
The NIST-sponsored Text Retrieval Conference
(TREC) has been the driving force for developing 
this technology through its QA track since TREC-8
(Voorhees 1999). There has been significant
progress and interest in QA research in recent
years (Voorhees 2000, Pasca and Harabagiu 2001).
QA is different than search engines in two aspects: 
(i) instead of a string of keyword search terms, the 
query is a natural language question, necessitating 
question parsing, (ii) instead of a list of documents 
or URLs, a list of candidate answers at phrase level 
or sentence level are expected to be returned in 
response to a query, hence the need for text
processing beyond keyword indexing, typically
supported by Natural Language Processing (NLP) 
and Information Extraction (IE) (Chinchor and
Marsh 1998, Hovy, Hermjakob and Lin 2001, Li 
and Srihari 2000). Examples of the use of NLP and 
IE in Question Answering include shallow parsing 
(Kupiec, 1993), semantic parsing (Litkowski
1999), Named Entity tagging (Abney et al 2000, 
Srihari and Li 1999) and high-level IE (Srihari 
and Li, 2000).
Identifying exact or phrase-level answers is a
much more challenging task than sentence-level
answers. Good performance on the latter can be 
achieved by using sophisticated passage retrieval 
techniques and/or shallow level NLP/IE
processing (Kwok et al 2001, Clarke et al 2001). 
The phrase-level answer identification involves
sophisticated NLP/IE and it is difficult to apply 
only IR techniques for this task (Prager et al 
1999). These two tasks are closely related. Many 
systems (e.g. Prager et al1999; Clark et al2001) 
take a two-stage approach. The first stage
involves retrieving sentences or paragraphs in
documents as candidate answer strings. Stage
Two focuses on extracting phrase-level exact
answers from the candidate answer strings.
This paper focuses on methods involving Stage 
Two. The input is a sentence pair consisting of a 
question and a sentence-level candidate answer 
string. The output is defined to be a phrase, called 
answer-point, extracted from the candidate
answer string. In order to identify the answer-
point, the pair of strings are parsed by the same 
system to generate binary dependency structures 
for both specific entity associations and generic 
grammatical relationships. An integrated Natural 
Language Processing (NLP) and Information
Extraction (IE) engine is used to extract named 
entities (NE) and their associations and to decode 
grammatical relationships. The system searches
for an answer-point by comparing the structural 
similarity involving the question-phrase and a
candidate answer-phrase. Generic grammatical
relationships are used as a back-off for specific 
entity associations when the question goes beyond 
the scope of the specific associations or when the 
system fails to identify the answer-point which 
meets the specific  entity association constraints. 
The proposed methods are particularly helpful in 
cases where the question-phrase does not
correspond to a known named entity type and in 
cases where there are multiple candidate answer-
points to select from.
The rest of the paper is structured as follows: 
Section 1 presents the NLP/IE engine used,
sections 2 discusses how to identify and formally 
represent what is being asked, section 3 presents 
the algorithm on identifying exact answers
leveraging structural support, section 4 presents 
case studies and benchmarks, and section 5 is the 
conclusion.
Kernel IE Modules Linguistic  Modules
Entity
Association
Named
Entity
Part-Of-
Speech
Asking-point
Identification
O
ut
pu
t(
En
ti
ty
, 
Ph
ra
se
 a
nd
 S
tr
uc
tu
ra
l 
Li
nk
s)
Shallow
Parsing
Semantic
Parsing
Tokenizer
Input
Figure 1: InfoXtract? NLP/IE System Architecture
1 NLP/IE Engine Description
The NLP/IE engine used in the QA system
described here is named InfoXtract?. It consists 
of an NLP component and IE component, each 
consisting of a set of pipeline modules (Figure 1). 
The NLP component serves as underlying support 
for IE. A brief description of these modules is 
given below.
? Part-of-Speech Tagging: tagging syntactic
categories such as noun, verb, adjective, etc. 
? Shallow Parsing: grouping basic linguistic
units as building blocks for structural links, 
such as Basic Noun Phrase, Verb Group, etc. 
? Asking-point Identification: analysis of
question sentences to determine what is being 
asked
? Semantic Parsing: decoding grammatical
dependency relationships at the logical level 
between linguistic units, such as Verb-Subject
(V-S), Verb-Object (V-O), Head-Modifier
(H-M) relationships; both active patterns and 
passive patterns will be parsed into the same 
underlying logical S-V-O relationships
? Named Entity Tagger: classifying proper
names and other phrases to different
categories such as Person, Organization,
Location, Money, etc.
? Entity Association Extractor: relating named 
entities with predefined associations such as 
Affiliation, Position, Age, Spouse, Address,
etc.
The NE tagger in our system is benchmarked to 
achieve close to human performance, around or 
above 90% precision and recall for most
categories of NE. This performance provides
fundamental support to QA. Many questions
require a named entity or information associated 
with a named entity as answers. A subset of the 
NE hierarchy used in our system is illustrated
below:
Person: woman, man
Organization: company, government,
association, school, army, mass-media
Location: city, province, country, continent, 
ocean, lake, etc.
Time Expressions: hour, part-of-day, day-of-
week, date, month, season, year, decade, 
century, duration
Numerical Expressions: percentage, money, 
number, weight, length, area, etc.
Contact expressions: email, address,
telephone, etc.
The Entity Association module correlates named 
entities and extracts their associations with other 
entities or phrases.  These are specific, predefined 
relationships for entities of person and
organization. Currently, our system can extract
the following entity associations with high
precision (over 90%) and modest recall ranging 
from 50% to 80% depending on the size of
grammars written for each specific association. 
Person: affiliation, position, age, spouse,
birth-place, birth-time, etc.
Organization: location, staff, head, products,
found-time, founder, etc.
Entity associations are semantic structures very
useful in supporting QA. For example, from the 
sentence Grover Cleveland , who in June 1886
married 21-year-old Frances Folsom,?the IE
engine can identify the following associations:
Spouse: Grover Cleveland ?Frances Folsom
Spouse: Frances?Grover Cleveland 
Age:  Frances Folsom?21-year-old
A question asking about such an association, say, 
Q11: Who was President Cleveland ?s wife, will be 
parsed into the following association link between 
a question-phrase ?Who? and the entity ?Cleveland? 
(see Section 2): Spouse: Cleveland ? Who. The 
semantic similarity between this structure and the 
structure Spouse: Grover Cleveland ? Frances 
Folsom can determine the answer point to be
?Frances Folsom?.
The Semantic Parsing module decodes the
grammatical dependency relationships: V-S, V-O,
V-C (Verb-Complement), H-M of time, location, 
reason, manner, purpose, result, etc. This module 
extends the shallow parsing module through the 
use of a cascade of handcrafted pattern matching 
rules.  Manual benchmarking shows results with 
the following performance:
H-M: Precision 77.5%
V-O: Precision 82.5%
V-S: Precision 74%
V-C: Precision 81.4%
In our semantic parsing, not only passive patterns
will be decoded into the same underlying
structures as active patterns, but structures for
verbs such as acquire and for de-verbal nouns such 
as acquisition lead to the same dependency links, 
as shown below.
AOL acquired Netscape in 1998. ?
V-S: acquired? AOL
V-O: acquired ? Netscape
H-M: acquired ? in 1998 (time-modifier)
Netscape was acquired by AOL in 1998. ?
V-S: was acquired ? by AOL
V-O: was acquired ? Netscape
H-M: was acquired ? in 1998 (time-modifier)
the acquisition of Netscape by AOL in 1998??
V-S: acquisition ? by AOL
V-O: acquisition ? of Netscape 
H-M: acquired ? in 1998 (time-modifier)
These links can be used as structural support to 
answer questions like Who acquired Netscape or
which company was acquired by AOL.
Obviously, our semantic parser goes one step
further than parsers which only decode syntactic 
relationships. It consumes some surface structure 
variations to provide the power of comparing the 
structural similarity at logical level. However,
compared with the entity association structures 
which sits at deep semantic level, the logical SVO 
(Subject-Verb-Object) structures still cannot
capture semantic relations which are expressed
using different head verbs with different
structures. An example is the pair : X borrows Y 
from Z versus Z lends Y to X.
2 Asking Point Link Identification
Asking point link identification is a crucial step in 
a QA system. It provides the necessary
information decoded from question processing for 
a system to locate the corresponding answer-
points from candidate answer strings. 
The Asking-point (Link) Identification Module is 
charged with the task of parsing wh-phrases in 
their context into three categories: NE Asking-
point, Asking-point Association  Link and
Asking-point Grammar  Link. Asking Point refers
to the question phrases with its constraints  that a 
corresponding answer-point should satisfy in
matching. Asking-point Link is the decoded binary 
relationship from the asking point to another unit 
in the question. 
The identification of the NE asking point is
essentially mapping the wh-phrase to the NE
types or subtypes. For example, which year is 
mapped to [which year]/NeYear, how old mapped 
to [how old]/NeAge, and how long mapped to 
[how long]/NeLength or [how long]/NeDuration, 
etc.
The identification of the Asking-point Association
Link is to decide whether the incoming question 
asks about a predefined association relationship. 
For Asking-point Association  Link, the module 
needs to identify the involved entity and the asked 
association. For example, the Asking-point
Association  Link for How old is John Smith is the 
AGE relationship of the NePerson John Smith,
represented as AGE: John Smith ? [how
old]/NeAge.
The wh-phrases which may or may not be mapped 
to NE asking points and whose dependency links 
are beyond predefined associations lead to Asking-
point Grammar Links, e.g. How did Julian Hill 
discover nylon? This asking-point link is
represented as H-M: discover ? [How]/manner-
modifier. As seen, an asking-point grammar link 
only involves generic grammatical constraints: in 
this case, the constraints for a candidate answer-
point to satisfy during matching are H-M link with 
?discover? as head and a phrase which must be a 
modifier of manner. 
These three types of asking points and their
possible links form a natural hierarchy that can be 
used to facilitate the backoff strategy for the
answer-point extraction module (see Section 3): 
Asking-point Association Link ? Asking-point
Grammar Link ? NE Asking Point.  This
hierarchy defines the sequence of matching steps 
which should be followed during the answer-point
extraction.
The backoff from Asking-point Association  Link 
to Asking-point Grammar  Link is necessary as the 
latter represents more generic structural constraints 
than the former. For example, in the sentence
where is IBM located, the Asking-point
Association Link is LOCATION: IBM ?
[where]/NeLocation while the default Grammar
Link is H-M: located ? [where]/location-
modifier. When the specific association constraints 
cannot be satisfied, the system should attempt to 
locate an answer-point by searching for a location-
modifier of the key verb ?located?.
The NE asking point constraints are also marked 
for asking-point association links and those asking-
point grammar links whose wh-phrases can be
mapped to NE asking points. Backing off to the 
NE asking point is required in cases where the 
asking-point association constraints and
grammatical structural constraints cannot be
satisfied. For How old is John Smith, the asking-
point grammar  link is represented as H-M: John 
Smith ? [how old]/NeAge. If the system cannot 
find a corresponding AGE association or a
modifier of NeAge for the entity John Smith to
satisfy the structural constraints, it will at least 
attempt to locate a candidate answer-point by
enforcing the NE asking point constraints NeAge. 
When there is only one NeAge in the answer
string, the system can extract it as the only
possible answer-point even if the structural
constraints are not honored.
3 Answer Point Identification
The answer-point identification is accomplished 
through  matching the asking-point to candidate 
answer-points using the following back-off
algorithm based on the processing results of the 
question and the sentence-level candidate answer 
string.
(1) if there is Asking-point Association
Link, call Match(asking-point association 
link, candidate answer-point association 
link) to search for the corresponding
association to locate answer-point
(2) if step (1) fails and there is an asking-
point grammar link, call Match(asking-
point grammar link, candidate answer-
point grammar link) to search for the
corresponding grammar link to locate the 
answer-point
(3) if step (2) fails and there is an NE asking 
point, search for the corresponding NEs: 
if there is only one corresponding NE, 
then extract this as the answer-point else 
mark all corresponding NEs as candidate 
answer-points
The function Match(asking-point link, candidate 
answer-point link) is defined as (i) exact match or 
synonym match of the related units (synonym
match currently confined to verb vs. de-verbal
noun); (ii) match the relation type directly (e.g. V-
S matches V-S, AGE matches AGE, etc.); (iii) 
match the type of asking point and answer point 
(e.g. NePerson asking point matches NePerson and
its sub-types NeMan and NeWoman; ?how?
matches manner-modifier; etc.): either through
direct link or indirect link based on conjunctive 
link (ConjLink) or equivalence link (S-P, subject-
predicative or appositive relations between two
NPs).
Step (1) and Step (2) attempt to leverage the
structural support from parsing and high-level
information extraction beyond NE. It is worth
noticing that in our experiment, the structural
support used for answer-point identification only 
checks the binary links involving the asking point 
and the candidate answer points, instead of full 
template matching as proposed in (Srihari and Li, 
2000).
Full template matching is best exemplified by the 
following example. If the incoming question is 
Who won the Nobel Prize in 1991, and the
candidate answer string is John Smith won the
Nobel Prize in 1991, the question template and 
answer template are shown below:
win
V-S: NePerson [Who]
V-O: NP [the Nobel Prize]
H-M: NeYear [1991]
win
V-S: NePerson [John Smith]
V-O: NP [the Nobel Prize]
H-M: NeYear [1991]
The template matching will match the asking point 
Who with the answer point John Smith because for 
all the dependency links in the trees, the
information is all compatible (in this case, exact
match). This is the ideal case of full template
matching and guarantees the high precision of the 
extracted answer point.
However, in practice, full template matching is 
neither realistic for most of cases nor necessary for 
achieving the objective of extracting answer points 
in a two-stage approach. It is not realistic because 
natural language semantic parsing is such a
challenging problem that a perfect dependency tree 
(or full template) which pieces together every
linguistic unit is not always easy to decode. For
InfoXtract,, in most cases, the majority, but not 
all, of the decoded binary dependency links are 
accurate, as shown in the benchmarks above. In 
such situations, insisting on checking every
dependency link of a template tree is too strong a 
condition to meet. On the other hand, it is actually 
not necessary to check all the links in the
dependency trees for full template matching. With 
the modular design and work division between
sentence level candidate answer string generation 
module (Stage One) and answer-point extraction 
from the candidate answer strings (Stage Two), 
all the candidate answer strings are already
determined by previous modules as highly
relevant. In this situation, a simplified partial
template matching, namely, ?asking/answer point 
binary relation matching?, will be sufficient to 
select the answer-point, if present, from the
candidate answer string. In other words, the
system only needs to check this one dependency 
link in extracting the answer-point. For the
previous example, only the asking/answer point 
binary dependency links need to be matched as 
illustrated below:
V-S win?[Who]/NePerson
V-S win?[John Smith]/NeMan
Some sample results are given in section 4 to
illustrate how answer-points are identified based 
on matching binary relations involving
asking/answer points. 
4 Experiments and Results
In order to conduct the feasibility study on the 
proposed method, we selected the first 100
questions from the TREC-8 QA track pool and 
the corresponding first candidate answer
sentences for this preliminary experiment. The 
Stage One processing for generating candidate 
answer sentences was conducted by the existing 
ranking module of our QA system. The Stage
Two processing for answer-point identification
was accomplished by using the algorithm
described in Section 3.
As shown in Table 1, out of the 100 question-
answer pairs we selected, 9 have detected
association links involving asking/answer points, 
44 are found to have grammar links involving 
asking/answer points. 
Table 1: Experiment Results
detected correct fail precision recall
Association
Links 9 8 1 89% 8%
Grammar
Links 44 39 6 89% 39%
NE Points 
(Baseline) 76 41 35 54% 41%
Overall
performance 86 71 14 83% 71%
As for NE asking points, 76 questions were
identified to require some type of NE as answers.
Assume that a baseline answer-point identification 
system only uses NE asking points as constraints, 
out of the 76 questions requiring NEs as answers, 
41 answer-points were identified successfully
because there was only one NE in the answer
string which matches the required NE type. The 
failed cases in matching NE asking point
constraints include two situations: (i) no NE exists 
in the answer string; (ii) multiple NEs satisfy the 
type constraints of NE asking points (i.e. more 
than one candidate answer-points found from the 
answer string) or there is type conflict during the 
matching of NE asking/answer points. Therefore, 
the baseline system would achieve 54% precision 
and 41% recall based on the standard precision and 
recall formulas: 
Precision = Correct / Detected
Recall = Correct / Relevant. 
In comparison, in our answer-point identification 
system which leverages structural support from
both the entity association links and grammar links 
as well as the NE asking points, both the precision 
and recall are raised: from the baseline 54% to 
83% for precision and from 41% to 71% for recall. 
The significant improvement in precision and
recall is attributed to the performance of structural 
matching in identifying exact answers. This
demonstrates the benefits of making use of
sophisticated NLP/IE technology, beyond NE and 
shallow parsing.
Using grammar links alone, exact answers were
identified for 39 out of the 44 candidate answer-
points satisfying the types of grammar links in 100 
cases. During matching, 6 cases failed either due to 
the parsing error or due to the type conflict
between the asking/answer points (e.g. violating 
the type constraints such as manner-modifier on 
the answer-point for ?how? question). The high 
precision and modest recall in using the grammar 
constraints is understandable as the grammar links 
impose very strong constraints on both the nodes 
and the structural type. The high precision
performance indicates that grammar links not
only have the distinguishing power to identify
exact answers in the presence of multiple NE 
options but also recognize answers in the absence 
of asking point types.
Even stronger structural support comes from the 
semantic relations decoded by the entity
association extraction module.  In this case, the 
performance is naturally high-precision (89%)
low-recall (8%) as predefined association links 
are by nature more sparse than generic
grammatical relations.
In the following, we illustrate with some
examples with questions from the TREC-8 QA 
task on how the match function identified in
Section 3 applies to different question types.
Q4: How much did Mercury spend on
advertising in 1993? ? asking-point grammar 
link:
V-O spend ? [How much]/NeMoney
A: Last year the company spent Pounds 12m
on advertising. ? candidate answer-point
grammar link:
V-O spent?[Pounds 12m]/NeMoney
Answer-point Output: Pounds 12m
This case requires (i) exact match in its original 
verb form between spend and spent; (ii) V-O type 
match; and (iii) asking/answer point type
NeMoney match through direct link.
Q63: What nuclear-powered Russian
submarine sank in the Norwegian Sea on April 
7, 1989?? asking-point grammar link: 
H-M submarine?[What]
A: NEZAVISIMAYA GAZETA on the
Komsomolets nuclear-powered submarine
which sank in the Norwegian Sea five years 
ago:? candidate answer-point grammar link:
H-M submarine?Komsomolets
Answer-point Output: Komsomolets
This case requires (i) exact match of submarine;
(ii) H-M type match; and (iii) asking/answer point 
match through direct link:  there are no asking
point type constraints because the asking point 
goes beyond existing NE. This case highlights the 
power of semantic parsing in answer-point
extraction. Since there are no type constraints on 
answer point,1 candidate answer points cannot be 
extracted without bringing in structural context by 
checking the NE type. Most of what-related asking 
points such as those in the patterns
?what/which?N?, ?what type/kind of ?N? go
beyond NE and require this type of structural
relation checking to locate the exact answer. The 
case below is another example.
Q79: What did Shostakovich write for
Rostropovich?? asking-point grammar link: 
V-O write?[What]
A: The Polonaise from Tchaikovsky?s opera
Eugene was a brief but cracking opener and its 
brilliant bluster was no sooner in our ears than 
forcibly contradicted by the bleak depression of 
Shostakovich?s second cello concerto, Op. 126,
a late work written for Rostropovich in 1966 
between the thirteenth and fourteenth
symphonies. ? candidate answer-point
grammar link:
V-O written?[a late work]/NP
S-P [Op. 126]/NP ?[a late work]/NP
Answer-point Output: Op. 126
This case requires (i) exact match in its original 
verb form between ?written? and ?write?;
(ii) V-O type match; and (iii) asking/answer point 
match through indirect link based on equivalence 
link S-P. When there are no NE constraints on the 
answer point, a proper name or an initial-
capitalized NP is preferred over an ordinary,
lower-case NP as an answer point. This heuristic is 
built-in so that ?Op. 126? is output as the answer-
point in this case instead of ?a late work?.
1 Strictly speaking, there are some type constraints on 
the answer point. The type constraints are something to 
the effect of ?a name for a kind of ship? which goes 
beyond the existing NE types defined.
Conclusion
This paper presented an approach to exact answer 
identification to questions using only binary
structural links involving the question-phrases.
Based on the experiments conducted, some
preliminary conclusions can be arrived at.
? The Entity Association extraction helps in 
pinpointing exact answers precisely
? Grammar dependency links enable the
system to not only identify exact answers 
but answer questions not covered by the 
predefined set of available
NEs/Associations
? Binary dependency links instead of full 
structural templates provide sufficient and 
effective structural leverage for extracting 
exact answers 
Some cases remain difficult however, beyond the 
current level of NLP/IE.  For example,
Q92: Who released the Internet worm in the 
late 1980s?? asking point link: 
V-S (released, NePerson[Who])
A: Morris, suspended from graduate studies at 
Cornell University at Syracuse, N,Y,, is
accused of designing and disseminating in
November, 1988, a rogue program or ?worm? 
that immobilized some 6,000 computers linked 
to a research network, including some used by 
NASA and the Air Force.? answer point link:
V-S (disseminating, NePerson[Morris]) 
In order for this case to be handled, the following 
steps are required: (i) the semantic parser should 
be able to ignore the past participle postmodifier 
phrase headed by ?suspended?; (ii) the V-O
dependency should be decoded between ?is
accused? and ?Morris?; (iii) the V-S dependency 
should be decoded between ?designing and
disseminating? and ?Morris? based on the pattern 
rule ?accuse NP of Ving?? V-S(Ving, NP); (iv) 
the conjunctive structure should map the V-S
(?designing and disseminating?, ?Morris?) into two 
V-S links; (v)  ?disseminate? and ?release? should
be linked somehow for synonym expansion.  It 
may be unreasonable to expect an NLP/IE system 
to accomplish all of these, but each of the above 
challenges indicates some directions for further 
research in this topic.
We would like to extend the experiments on a 
larger set of questions to further investigate the 
effectiveness of structural support in extracting
exact answers. The TREC-9 and TREC 2001 QA 
pool and the candidate answer sentences generated 
by both NLP-based or IR-based QA systems would 
be ideal for further testing this method.
5 Acknowledgement
The authors wish to thank Walter Gadz and Carrie 
Pine of AFRL for supporting this work. Thanks 
also go to anonymous reviewers for their valuable 
comments.
References
Abney, S., Collins, M and Singhal, A. (2000) Answer
Extraction. In Proceedings of ANLP -2000, Seattle.
Chinchor, N. and Marsh, E. (1998) MUC -7 Information 
Extraction Task Definition (version 5.1), In
?Proceedings of MUC-7?. Also published at
http://www.muc.saic.com/
Clarke, C. L. A., Cormack, G. V. and Lynam, T. R. 
(2001), Exploiting Redundancy in Question
Answering. In Proceedings of SIGIR?01, New
Orleans, LA.
Hovy, E.H., U. Hermjakob, and Chin-Yew Lin. 2001. 
The Use of External Knowledge of Factoid QA. In 
Proceedings of the 10th Text Retrieval Conference 
(TREC 2001), Gaithersburg, MD, U.S.A., November 
13-16, 2001
Kupiec, J. (1993) MURAX: A Robust Linguistic
Approach For Question Answering Using An On-Line
Encyclopaedia . In Proceedings of SIGIR-93,
Pittsburgh, PA.
Kwok, K. L., Grunfeld, L., Dinstl, N. and Chan, M. 
(2001), TREC2001 Question-Answer, Web and Cross 
Language Experiments using PIRCS. In Proceedings 
of TREC-10, Gaithersburg, MD.
Li, W. and Srihari, R. (2000) A Domain Independent 
Event Extraction Toolkit , Phase 2 Final Technical 
Report, Air Force Research Laboratory/Rome, NY.
Litkowski, K. C. (1999) Question-Answering Using
Semantic Relation Triples. In Proceedings of TREC-
8, Gaithersburg, MD.
Pasca, M. and Harabagiu, S. M. High Performance
Question/Answering. In Proceedings of SIGIR 2001: 
pages 366-374
Prager, J., Radev, D., Brown, E., Coden, A. and Samn, 
V., The use of predictive annotation for question
answering in TREC8. In Proceedings of TREC-8,
Gaithersburg, MD.
Srihari, R. and Li, W. (1999) Information Extraction 
supported Question Answering. In Proceedings of
TREC-8, Gaithersberg, MD.
Srihari, R and Li, W. (2000b). A Question Answering 
System Supported by Information Extraction. In 
Proceedings of ANLP 2000, Seattle.
Voorhees, E. (1999), The TREC-8 Question Answering 
Track Report, In Proceedings of TREC-8,
Gaithersburg, MD. 
Voorhees, E. (2000), Overview of the TREC-9
Question Answering Track , In Proceedings of
TREC-9, Gaithersburg, MD. 
