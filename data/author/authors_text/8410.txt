Proceedings of NAACL HLT 2007, pages 484?491,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Comparison of Pivot Methods for Phrase-based Statistical Machine
Translation
Masao Utiyama and Hitoshi Isahara
National Institute of Information and Communications Technology
3-5 Hikari-dai, Soraku-gun, Kyoto 619-0289 Japan
{mutiyama,isahara}@nict.go.jp
Abstract
We compare two pivot strategies for
phrase-based statistical machine transla-
tion (SMT), namely phrase translation
and sentence translation. The phrase
translation strategy means that we di-
rectly construct a phrase translation ta-
ble (phrase-table) of the source and tar-
get language pair from two phrase-tables;
one constructed from the source language
and English and one constructed from En-
glish and the target language. We then use
that phrase-table in a phrase-based SMT
system. The sentence translation strat-
egy means that we first translate a source
language sentence into n English sen-
tences and then translate these n sentences
into target language sentences separately.
Then, we select the highest scoring sen-
tence from these target sentences. We con-
ducted controlled experiments using the
Europarl corpus to evaluate the perfor-
mance of these pivot strategies as com-
pared to directly trained SMT systems.
The phrase translation strategy signifi-
cantly outperformed the sentence transla-
tion strategy. Its relative performance was
0.92 to 0.97 compared to directly trained
SMT systems.
1 Introduction
The rapid and steady progress in corpus-based ma-
chine translation (Nagao, 1981; Brown et al, 1993)
has been supported by large parallel corpora such
as the Arabic-English and Chinese-English paral-
lel corpora distributed by the Linguistic Data Con-
sortium and the Europarl corpus (Koehn, 2005),
which consists of 11 European languages. How-
ever, large parallel corpora do not exist for many
language pairs. For example, there are no pub-
licly available Arabic-Chinese large-scale parallel
corpora even though there are Arabic-English and
Chinese-English parallel corpora.
Much work has been done to overcome the lack
of parallel corpora. For example, Resnik and Smith
(2003) propose mining the web to collect parallel
corpora for low-density language pairs. Utiyama
and Isahara (2003) extract Japanese-English parallel
sentences from a noisy-parallel corpus. Munteanu
and Marcu (2005) extract parallel sentences from
large Chinese, Arabic, and English non-parallel
newspaper corpora.
Researchers can also make the best use of exist-
ing (small) parallel corpora. For example, Nie?en
and Ney (2004) use morpho-syntactic information to
take into account the interdependencies of inflected
forms of the same lemma in order to reduce the
amount of bilingual data necessary to sufficiently
cover the vocabulary in translation. Callison-Burch
et al (2006a) use paraphrases to deal with unknown
source language phrases to improve coverage and
translation quality.
In this paper, we focus on situations where no par-
allel corpus is available (except a few hundred paral-
lel sentences for tuning parameters). To tackle these
extremely scarce training data situations, we pro-
pose using a pivot language (English) to bridge the
484
source and target languages in translation. We first
translate source language sentences or phrases into
English and then translate those English sentences
or phrases into the target language, as described in
Section 3. We thus assume that there is a parallel
corpus consisting of the source language and En-
glish as well as one consisting of English and the tar-
get language. Selecting English as a pivot language
is a reasonable pragmatic choice because English is
included in parallel corpora more often than other
languages are, though any language can be used as a
pivot language.
In Section 2, we describe a phrase-based statisti-
cal machine translation (SMT) system that was used
to develop the pivot methods described in Section
3. This is the shared task baseline system for the
2006 NAACL/HLT workshop on statistical machine
translation (Koehn and Monz, 2006) and consists of
the Pharaoh decoder (Koehn, 2004), SRILM (Stol-
cke, 2002), GIZA++ (Och and Ney, 2003), mkcls
(Och, 1999), Carmel,1 and a phrase model training
code.
2 Phrase-based SMT
We use a phrase-based SMT system, Pharaoh,
(Koehn et al, 2003; Koehn, 2004), which is based
on a log-linear formulation (Och and Ney, 2002). It
is a state-of-the-art SMT system with freely avail-
able software, as described in the introduction.
The system segments the source sentence into so-
called phrases (a number of sequences of consecu-
tive words). Each phrase is translated into a target
language phrase. Phrases may be reordered.
Let f be a source sentence (e.g, French) and e be a
target sentence (e.g., English), the SMT system out-
puts an e? that satisfies
e? = argmaxe Pr(e|f) (1)
= argmaxe
M?
m=1
?mhm(e, f) (2)
where hm(e, f) is a feature function and ?m is a
weight. The system uses a total of eight feature
functions: a trigram language model probability of
the target language, two phrase translation probabil-
ities (both directions), two lexical translation prob-
1http://www.isi.edu/licensed-sw/carmel/
abilities (both directions), a word penalty, a phrase
penalty, and a linear reordering penalty. For details
on these feature functions, please refer to (Koehn et
al., 2003; Koehn, 2004; Koehn et al, 2005). To set
the weights, ?m, we carried out minimum error rate
training (Och, 2003) using BLEU (Papineni et al,
2002) as the objective function.
3 Pivot methods
We use the phrase-based SMT system described in
the previous section to develop pivot methods. We
use English e as the pivot language. We use French
f and German g as examples of the source and target
languages in this section.
We describe two types of pivot strategies, namely
phrase translation and sentence translation.
The phrase translation strategy means that we di-
rectly construct a French-German phrase translation
table (phrase-table for short) from a French-English
phrase-table and an English-German phrase-table.
We assume that these French-English and English-
German tables are built using the phrase model train-
ing code in the baseline system described in the
introduction. That is, phrases are heuristically ex-
tracted from word-level alignments produced by do-
ing GIZA++ training on the corresponding parallel
corpora (Koehn et al, 2003).
The sentence translation strategy means that we
first translate a French sentence into n English sen-
tences and translate these n sentences into German
separately. Then, we select the highest scoring sen-
tence from the German sentences.
3.1 Phrase translation strategy
The phrase translation strategy is based on the fact
that the phrase-based SMT system needs a phrase-
table and a language model for translation. Usually,
we have the language model of a target language.
Consequently, we only need to construct a phrase-
table to train the phrase-based SMT system.
We assume that we have a French-English phrase-
table TFE and an English-German phrase-table
TEG. From these tables, we construct a French-
German phrase-table TFG, which requires estimat-
ing four feature functions; phrase translation prob-
abilities for both directions, ?(f? |g?) and ?(g?|f?) and
lexical translation probabilities for both directions,
485
pw(f? |g?) and pw(g?|f?), where f? and g? are French and
German phrases that are parts of phrase translation
pairs in TFE and TEG, respectively.2
We estimate these probabilities using the proba-
bilities available in TFE and TEG as follows.3
?(f? |g?) =
?
e??TFE?TEG
?(f? |e?)?(e?|g?) (3)
?(g?|f?) =
?
e??TFE?TEG
?(g?|e?)?(e?|f?) (4)
pw(f? |g?) =
?
e??TFE?TEG
pw(f? |e?)pw(e?|g?) (5)
pw(g?|f?) =
?
e??TFE?TEG
pw(g?|e?)pw(e?|f?) (6)
where e? ? TFE?TEG means that the English phrase
e? is included in both TFE and TEG as part of phrase
translation pairs. ?(f? |e?) and ?(e?|f?) are phrase
translation probabilities for TFE and ?(e?|g?) and
?(g?|e?) are those for TEG. pw(f? |e?) and pw(e?|f?) are
lexical translation probabilities for TFE and pw(e?|g?)
and pw(g?|e?) are those for TEG.
The definitions of the phrase and lexical transla-
tion probabilities are as follows (Koehn et al, 2003).
?(f? |e?) = count(f? , e?)?
f? ? count(f? ?, e?)
(7)
where count(f? , e?) gives the total number of times
the phrase f? is aligned with the phrase e? in the par-
allel corpus. Eq. 7 means that ?(f? |e?) is calculated
using maximum likelihood estimation.
The definition of the lexical translation probabil-
ity is
pw(f? |e?) = maxa pw(f? |e?,a) (8)
pw(f? |e?,a) =
n?
i=1
Ew(fi|e?,a) (9)
Ew(fi|e?,a) = 1|{j|(i, j) ? a}|
?
?(i,j)?a
w(fi|ej)
(10)
2Feature functions scores are calculated using these proba-
bilities. For example, for a translation probability of a French
sentence f = f?1 . . . f?K and a German sentence g = g?1 . . . g?K ,
h(g, f) = log?Ki=1 ?(f?i|g?i), where K is the number of
phrases.
3Wang et al (2006) use essentially the same definition to
induce the translation probability of the source and target lan-
guage word alignment that is bridged by an intermediate lan-
guage. Callison-Burch et al (2006a) use a similar definition for
a paraphrase probability.
w(f |e) = count(f, e)?
f ? count(f ?, e)
(11)
where count(f, e) gives the total number of times
the word f is aligned with the word e in the par-
allel corpus. Thus, w(f |e) is the maximum likeli-
hood estimation of the word translation probability
of f given e. Ew(fi|e?,a) is calculated from a word
alignment a between a phrase pair f? = f1f2 . . . fn
and e? = e1e2 . . . em where fi is connected to several
(|{j|(i, j) ? a}|) English words. Thus, Ew(fi|e?,a)
is the average (or mixture) of w(fi|ej). This means
that Ew(fi|e?,a) is an estimation of the probabil-
ity of fi in a. Consequently, pw(f? |e?,a) estimates
the probability of f? given e? and a using the prod-
uct of the probabilities Ew(fi|e?,a). This assumes
that the probability of fi is independent given e? and
a. pw(f? |e?) takes the highest pw(f? |e?,a) if there
are multiple alignments a. This discussion, which
is partly based on Section 4.1.2 of (Och and Ney,
2004), means that the lexical translation probability
pw(f? |e?) is another probability estimated using the
word translation probability w(f |e).
The justification of Eqs. 3?6 is straightforward.
From the discussion above, we know that the prob-
abilities, ?(f? |e?), ?(e?|f?), ?(g?|e?), ?(e?|g?), pw(f? |e?),
pw(e?|f?), pw(g?|e?), and pw(e?|g?) are probabilities in
the ordinary sense. Thus, we can derive ?(f? |g?),
?(g?|f?), pw(f? |g?), and pw(g?|f?) by assuming that
these probabilities are independent given an English
phrase e? (e.g., ?(f? |g?, e?) = ?(f? |e?)).
We construct a TFG that consists of all French-
German phrases whose phrase and lexical transla-
tion probabilities as defined in Eqs. 3?6 are greater
than 0. We use the term PhraseTrans to denote SMT
systems that use the phrase translation strategy de-
scribed above.
3.2 Sentence translation strategy
The sentence translation strategy uses two inde-
pendently trained SMT systems. We first trans-
late a French sentence f into n English sentences
e1, e2, ..., en using a French-English SMT system.
Each ei (i = 1 . . . n) has the eight scores calcu-
lated from the eight feature functions described in
Section 2. We denote these scores hei1, hei2, . . . hei8.
Second, we translate each ei into n German sen-
tences gi1,gi2, . . . ,gin using an English-German
486
SMT system. Each gij (j = 1 . . . n) has the eight
scores, which are denoted as hgij1, hgij2, . . . , hgij8.
This situation is depicted as
f ? ei (hei1, hei2, . . . , hei8)
? gij (hgij1, hgij2, . . . , hgij8)
We define the score of gij , S(gij), as
S(gij) =
8?
m=1
(?emheim + ?gmhgijm) (12)
where ?em and ?gm are weights set by performing
minimum error rate training4 as described in Section
2. We select the highest scoring German sentence
g? = argmaxgij S(gij) (13)
as the translation of the French sentence f .
A drawback of this strategy is that translation
speed is about O(n) times slower than those of the
component SMT systems. This is because we have
to run the English-German SMT system n times for
a French sentence. Consequently, we cannot set n
very high. When we used n = 15 in the experi-
ments described in Section 4, it took more than two
days to translate 3064 test sentences on a 3.06GHz
LINUX machine.
Note that when n = 1, the above strategy pro-
duces the same translation with the simple sequen-
tial method that we first translate a French sentence
into an English sentence and then translate that sen-
tence into a German sentence.
We use the terms SntTrans15 and SntTrans1 to de-
note SMT systems that use the sentence translation
strategy with n = 15 and n = 1, respectively.
4 Experiments
We conducted controlled experiments using the
Europarl corpus. For each language pair de-
scribed below, the Europarl corpus provides three
4We use a reranking strategy for the sentence translation
strategy. We first obtain n2 German sentences for each French
sentence by applying two independently trained French-English
and English-German SMT systems. Each of the translated Ger-
man sentences has the sixteen scores as described above. The
weights in Eq. 12 are tuned against reference German sentences
by performing minimum error rate training. These weights are
in general different from those of the original French-English
and English-German SMT systems.
types of parallel corpora; the source language?
English, English?the target language, and the source
language?the target language. This means that we
can directly train an SMT system using the source
and target language parallel corpus as well as pivot
SMT systems using English as the pivot language.
We use the term Direct to denote directly trained
SMT systems. For each language pair, we com-
pare four SMT systems; Direct, PhraseTrans, Snt-
Trans15, and SntTrans1.5
4.1 Training, tuning and testing SMT systems
We used the training data for the shared task of
the SMT workshop (Koehn and Monz, 2006) to
train our SMT systems. It consists of three paral-
lel corpora: French-English, Spanish-English, and
German-English.
We used these three corpora to extract a set of
sentences that were aligned to each other across all
four languages. For that purpose, we used English
as the pivot. For each distinct English sentence, we
extracted the corresponding French, Spanish, and
German sentences. When an English sentence oc-
curred multiple times, we extracted the most fre-
quent translation. For example, because ?Resump-
tion of the session? was translated into ?Wiederauf-
nahme der Sitzungsperiode? 120 times and ?Wieder-
aufnahme der Sitzung? once, we extracted ?Wieder-
aufnahme der Sitzungsperiode? as its translation.
Consequently, we extracted 585,830 sentences for
each language. From these corpora, we constructed
the training parallel corpora for all language pairs.
We followed the instruction of the shared task
baseline system to train our SMT systems.6 We
used the trigram language models provided with the
shared task. We did minimum error rate training on
the first 500 sentences in the shared task develop-
ment data to tune our SMT systems and used the
5As discussed in the introduction, we intend to use the pivot
strategies in a situation where a very limited amount of parallel
text is available. The use of the Europarl corpus is not an accu-
rate simulation of the intended situation because it enables us to
use a relatively large parallel corpus for direct training. How-
ever, it is necessary to evaluate the performance of the pivot
strategies against that of Direct SMT systems under controlled
experiments in order to determine how much the pivot strate-
gies can be improved. This is a first step toward the use of pivot
methods in situations where training data is extremely scarce.
6The parameters for the Pharaoh decoder were ?-dl 4 -b 0.03
-s 100?. The maximum phrase length was 7.
487
3064 test sentences for each language as our test set.
Our evaluation metric was %BLEU scores, as cal-
culated by the script provided along with the shared
task.7 We lowercased the training, development and
test sentences.
4.2 Results
Table 1 compares the BLEU scores of the four SMT
systems; Direct, PhraseTrans, SntTrans15, and Snt-
Trans1 for each language pair. The columns SE and
ET list the BLEU scores of the Direct SMT sys-
tems trained on the source language?English and
English?the target language parallel corpora. The
numbers in the parentheses are the relative scores
of the pivot SMT systems, which were obtained
by dividing their BLEU scores by that of the cor-
responding Direct system. For example, for the
Spanish?French language pair, the BLEU score of
the Direct SMT system was 35.78, that of the
PhraseTrans SMT system was 32.90, and the rela-
tive performance was 0.92 = (32.90/35.78). For
the SntTrans15 SMT system, the BLEU score was
29.49 and the relative performance was 0.82 =
(29.49/35.78).
The BLEU scores of the Direct SMT systems
were higher than those of the PhraseTrans SMT sys-
tems for all six source-target language pairs. The
PhraseTrans SMT systems performed better than
the SntTrans15 SMT systems for all pairs. The
SntTrans15 SMT systems were better than the Snt-
Trans1 SMT systems for four pairs. According
to the sign test, under the null hypothesis that the
BLEU scores of two systems are equivalent, finding
one system obtaining better BLEU scores on all six
language pairs is statistically significant at the 5 %
level. Obtaining four better scores is not statistically
significant. Thus, Table 1 indicates
Direct > PhraseTrans > SntTrans15 ? SntTrans1
where ?>? and ??? means that the differences of
the BLEU scores of the corresponding SMT systems
are statistically significant and insignificant, respec-
tively.
7Callison-Burch et al (2006b) show that in general a higher
BLEU score is not necessarily indicative of better translation
quality. However, they also suggest that the use of BLEU is
appropriate for comparing systems that use similar translation
strategies, which is the case with our experiments.
As expected, the Direct SMT systems outper-
formed the other systems. We regard the BLEU
scores of the Direct systems as the upperbound. The
SntTrans15 SMT systems did not significantly out-
perform the SntTrans1 SMT systems. We think that
this is because n = 15 was not large enough to cover
good translation candidates.8 Selecting the highest
scoring translation from a small pool did not always
lead to better performance. To improve the perfor-
mance of the sentence translation strategy, we need
to use a large n. However, this is not practical be-
cause of the slow translation speed, as discussed in
Section 3.2.
The PhraseTrans SMT systems significantly out-
performed the SntTrans15 and SntTrans1 systems.
That is, the phrase translation strategy is better
than the sentence translation strategy. Since the
phrase-tables constructed using the phrase transla-
tion strategy can be integrated into the Pharaoh de-
coder as well as the directly extracted phrase-tables,
the PhraseTrans SMT systems can fully exploit the
power of the decoder. This led to better performance
even when the induced phrase-tables were noisy, as
described below.
The relative performance of the PhraseTrans
SMT systems compared to the Direct SMT systems
was 0.92 to 0.97. These are very promising re-
sults. To show how these systems translated the
test sentences, we translated some outputs of the
Spanish-French Direct and PhraseTrans SMT sys-
tems into English using the French-English Direct
system. These are shown in Table 3 with the refer-
ence English sentences.
The relative performance seems to be related to
the BLEU scores for the Direct SMT systems. It
was relatively high (0.95 to 0.97) for the difficult (in
terms of BLEU) language pairs but relatively low
(0.92) for the easy language pairs; Spanish?French
and French?Spanish. There is a lot of room for
improvement for the relatively easy language pairs.
This relationship is stronger than the relationship be-
tween the BLEU scores for SE/ET and those for the
PhraseTrans systems, where no clear trend exists.
Table 2 shows the number of phrases stored in the
phrase-tables. The Direct SMT systems had 7.3 to
8A typical reranking approach to SMT (Och et al, 2004)
uses a 1000?best list.
488
Source?Target Direct PhraseTrans SntTrans15 SntTrans1 SE ET
Spanish?French 35.78 > 32.90 (0.92) > 29.49 (0.82) > 29.16 (0.81) 29.31 28.80
French?Spanish 34.16 > 31.49 (0.92) > 28.41 (0.83) > 27.99 (0.82) 27.59 29.07
German?French 23.37 > 22.47 (0.96) > 22.03 (0.94) > 21.64 (0.93) 22.40 28.80
French?German 15.27 > 14.51 (0.95) > 14.03 (0.92) < 14.21 (0.93) 27.59 15.81
German?Spanish 22.34 > 21.76 (0.97) > 21.36 (0.96) > 20.97 (0.94) 22.40 29.07
Spanish?German 15.50 > 15.11 (0.97) > 14.46 (0.93) < 14.61 (0.94) 29.31 15.81
Table 1: BLEU scores and relative performance
No. of phrases (?M? means 106)
Direct PhraseTrans common R P
S?F 18.2M 190.8M 6.3M 34.7 3.3
F?S 18.2M 186.8M 6.3M 34.7 3.4
G?F 7.3M 174.9M 3.1M 43.2 1.8
F?G 7.3M 168.2M 3.1M 43.2 1.9
G?S 7.5M 179.6M 3.3M 44.1 1.9
S?G 7.6M 176.6M 3.3M 44.1 1.9
?S?, ?F?, and ?G? are the acronyms of Spanish, French, and
German, respectively. ?X?Y? means that ?X? is the source lan-
guage and ?Y? is the target language.
Table 2: Statistics for the phrase-tables
18.2 million phrases, and the PhraseTrans systems
had 168.2 to 190.8 million phrases. The numbers of
phrases stored in the PhraseTrans systems were very
large compared to those of Direct systems.9 How-
ever, this does not cause a computational problem in
decoding because those phrases that do not appear in
source sentences are filtered so that only the relevant
phrases are used during decoding.
The figures in the common column are the number
of phrases common to the Direct and PhraseTrans
systems. R (recall) and P (precision) are defined as
follows.
R = No. of common phrases ? 100
No. of phrases in Direct system
9In Table 2, the PhraseTrans systems have more than 10x
as many phrases as the Direct systems. This can be explained
as follows. Let fi be the fanout of an English phrase i, i.e.,
fi is the number of phrase pairs containing the English phrase
i in a phrase-table, then the size of the phrase-table is s1 =?n
i=1 fi, where n is the number of distinct English phrases.
When we combine two phrase-tables, the size of the combined
phrase table is roughly s2 =
?n
i=1 f2i . Thus, the relative size
of the combined phrase table is roughly r = s2s1 =
E(f2)
E(f) ,
where E(f) = s1n and E(f2) = s2n are the averages over
fi and f2i , respectively. As an example, we calculated these
averages for the German-English phrase table. E(f) was 1.5,
E(f2) was 43.7, and r was 28.9. This shows that even if an
average fanout is small, the size of a combined phrase table can
be very large.
P = No. of common phrases ? 100
No. of phrases in PhraseTrans system
Recall was reasonably high. However, the upper
bound of recall was 100 percent because we used
a multilingual corpus whose sentences were aligned
to each other across all four languages, as described
in Section 4.1. Thus, there is a lot of room for im-
provement with respect to recall. Precision, on the
other hand, was very low. However, translation per-
formance was not significantly affected by this low
precision, as is shown in Table 1. This indicates that
recall is more important than precision in building
phrase-tables.
5 Related work
Pivot languages have been used in rule-based ma-
chine translation systems. Boitet (1988) discusses
the pros and cons of the pivot approaches in multi-
lingual machine translation. Schubert (1988) argues
that a pivot language needs to be a natural language,
due to the inherent lack of expressiveness of artifi-
cial languages.
Pivot-based methods have also been used in other
related areas, such as translation lexicon induc-
tion (Schafer and Yarowsky, 2002), word alignment
(Wang et al, 2006), and cross language information
retrieval (Gollins and Sanderson, 2001). The trans-
lation disambiguation techniques used in these stud-
ies could be used for improving the quality of phrase
translation tables.
In contrast to these, very little work has been
done on pivot-based methods for SMT. Kauers et
al. (2002) used an artificial interlingua for spoken
language translation. Gispert and Marin?o (2006)
created an English-Catalan parallel corpus by auto-
matically translating the Spanish part of an English-
Spanish parallel corpus into Catalan with a Spanish-
Catalan SMT system. They then directly trained an
SMT system on the English-Catalan corpus. They
489
showed that this direct training method is superior
to the sentence translation strategy (SntTrans1) in
translating Catalan into English but is inferior to
it in the opposite translation direction (in terms of
the BLEU score). In contrast, we have shown that
the phrase translation strategy consistently outper-
formed the sentence translation strategy in the con-
trolled experiments.
6 Conclusion
We have compared two types of pivot strategies,
namely phrase translation and sentence translation.
The phrase translation strategy directly constructs a
phrase translation table from a source language and
English phrase-table and a target language and En-
glish phrase-table. It then uses this phrase table in
a phrase-based SMT system. The sentence transla-
tion strategy first translates a source language sen-
tence into n English sentences and translates these n
sentences into target language sentences separately.
Then, it selects the highest scoring sentence from the
target language sentences.
We conducted controlled experiments using the
Europarl corpus to compare the performance of
these two strategies to that of directly trained SMT
systems. The experiments showed that the perfor-
mance of the phrase translation strategy was statis-
tically significantly better than that of the sentence
translation strategy and that its relative performance
compared to the directly trained SMT systems was
0.92 to 0.97. These are very promising results.
Although we used the Europarl corpus for con-
trolled experiments, we intend to use the pivot strate-
gies in situations where very limited amount of par-
allel corpora are available for a source and target lan-
guage but where relatively large parallel corpora are
available for the source language?English and the
target language?English. In future work, we will
further investigate the pivot strategies described in
this paper to confirm that the phrase translation strat-
egy is better than the sentence translation strategy in
the intended situation as well as with the Europarl
corpus.10
10As a first step towards real situations, we conducted addi-
tional experiments. We divided the training corpora in Section
4 into two halves. We used the first 292915 sentences to train
source-English SMT systems and the remaining 292915 ones
to train English-target SMT systems. Based on these source-
References
Christian Boitet. 1988. Pros and cons of the pivot and
transfer approaches in multilingual machine transla-
tion. In Dan Maxwell, Klaus Schubert, and Toon
Witkam, editors, New Directions in Machine Trans-
lation. Foris. (appeared in Sergei Nirenburg, Harold
Somers and Yorick Wilks (eds.) Readings in Machine
Translation published by the MIT Press in 2003).
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006a. Improved statistical machine transla-
tion using paraphrases. In NAACL.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006b. Re-evaluating the role of BLEU in
machine translation research. In EACL.
Adria? de Gispert and Jose? B. Mari no. 2006. Catalan-
English statistical machine translation without parallel
corpus: Bridging through Spanish. In Proc. of LREC
5th Workshop on Strategies for developing Machine
Translation for Minority Languages.
Tim Gollins and Mark Sanderson. 2001. Improving
cross language information retrieval with triangulated
translation. In SIGIR.
Manuel Kauers, Stephan Vogel, Christian Fu?gen, and
Alex Waibel. 2002. Interlingua based statistical ma-
chine translation. In ICSLP.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proceedings on the Workshop on
Statistical Machine Translation, pages 102?121, New
York City, June. Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
English and English-target SMT systems, we trained Phrase-
Trans and SntTrans1 SMT systems. Other experimental condi-
tions were the same as those described in Section 4. The table
below shows the BLUE scores of these SMT systems. It indi-
cates that the PhraseTrans systems consistently outperformed
the SntTrans1 systems.
Source-Target PhraseTrans SntTrans1
Spanish-French 31.57 28.36
French-Spanish 30.18 27.75
German-French 20.48 19.83
French-German 14.38 14.11
German-Spanish 19.58 18.67
Spanish-German 14.80 14.46
490
Ref i hope with all my heart , and i must say this quite emphatically , that an opportunity will arise when this
document can be incorporated into the treaties at some point in the future .
Dir i hope with conviction , and put great emphasis , that again is a serious possibility of including this in the treaties .
PT i hope with conviction , and i very much , insisted that never be a serious possibility of including this in the
treaties .
Ref should this fail to materialise , we should not be surprised if public opinion proves sceptical about europe , or even
rejects it .
Dir otherwise , we must not be surprised by the scepticism , even the rejection of europe in the public .
PT otherwise , we must not be surprised by the scepticism , and even the rejection of europe in the public .
Ref the intergovernmental conference - to address a third subject - on the reform of the european institutions is also of
decisive significance for us in parliament .
Dir the intergovernmental conference - and this i turn to the third issue on the reform of the european institutions is of
enormous importance for the european parliament .
PT the intergovernmental conference - and this brings me to the third issue - on the reform of the european institutions
has enormous importance for the european parliament .
Table 3: Reference sentences (Ref) and the English translations (by the French-English Direct system) of
the outputs of the Spanish-French Direct and PhraseTrans SMT systems (Dir and PT).
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
IWSLT.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Makoto Nagao. 1981. A framework of a mechani-
cal translation between Japanese and English by anal-
ogy principle. In the International NATO Symposium
on Artificial and Human Intelligence. (appeared in
Sergei Nirenburg, Harold Somers and Yorick Wilks
(eds.) Readings in Machine Translation published by
the MIT Press in 2003).
Sonja Nie?en and Hermann Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Computational Linguistics,
30(2):181?204.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In ACL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In HLT-NAACL.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In EACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In CoNLL.
Klaus Schubert. 1988. Implicitness as a guiding princi-
ple in machine translation. In COLING.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In ICSLP.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news articles
and sentences. In ACL, pages 72?79.
Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word
alignment for languages with scarce resources using
bilingual corpora of other language pairs. In COL-
ING/ACL 2006 Main Conference Poster Sessions.
491
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 117?120, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Organizing English Reading Materials for Vocabulary Learning
Masao Utiyama, Midori Tanimura and Hitoshi Isahara
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Souraku-gun, Kyoto 619-0289 Japan
{mutiyama,mtanimura,isahara}@nict.go.jp
Abstract
We propose a method of organizing read-
ing materials for vocabulary learning. It
enables us to select a concise set of
reading texts (from a target corpus) that
contains all the target vocabulary to be
learned. We used a specialized vocab-
ulary for an English certification test as
the target vocabulary and used English
Wikipedia, a free-content encyclopedia, as
the target corpus. The organized reading
materials would enable learners not only
to study the target vocabulary efficiently
but also to gain a variety of knowledge
through reading. The reading materials
are available on our web site.
1 Introduction
EFL (English as a foreign language) learners and
teachers can easily access a wide range of English
reading materials on the Internet. For example, cur-
rent news stories can be read on web sites such as
those for CNN,1 TIME,2 or the BBC.3 Specialized
reading materials for EFL learners are also provided
on web sites like EFL Reading.4
This situation, however, does not mean that EFL
learners and teachers can easily select proper texts
suited to their specific purposes, for example, learn-
ing vocabulary through reading. On the contrary,
1http://www.cnn.com/
2http://www.time.com/time/
3http://www.bbc.co.uk/
4http://www.gradedreading.pwp.blueyonder.co.uk/
EFL teachers have to carefully select texts, if they
want their students to learn a specialized vocabulary
through reading in a particular discipline such as
medicine, engineering, or economics. However, it is
problematic for teachers to select materials for learn-
ing a target vocabulary with short authentic texts.
It is possible to automate this selection process
given the target vocabulary to be learned and the tar-
get corpus from which texts are gathered (Utiyama
et al, 2004). In this research (Utiyama et al, 2004),
we used a specialized vocabulary for an English
certification test as the target vocabulary and used
newspaper articles from The Daily Yomiuri as the
target corpus. We then organized a set of reading
materials, which we called courseware5, using the
algorithm in Section 2. The courseware consisted
of 116 articles and contained all the target vocabu-
lary. We used the courseware in university English
classes from May 2004 to January 2005. We found
that the courseware was effective in learning vocab-
ulary (Tanimura and Utiyama, in preparation).
Based on the promising results, our next goal is
to distribute courseware (produced with our algo-
rithm) to EFL teachers and learners so that we can
receive wider feedback. To this end, the course-
ware we constructed (Utiyama et al, 2004) is inade-
quate because it was prepared from The Daily Yomi-
uri, which is copyrighted. We therefore replaced
The Daily Yomiuri with English Wikipedia,6 a free-
content encyclopedia, and developed new course-
5Courseware usually includes software in addition to other
materials. However, in this paper, the term courseware is used
to refer to the reading materials only.
6http://en.wikipedia.org/wiki/Main Page
117
ware. It is available on our web site.7
In the following, will we first summarize our al-
gorithm and then describe details on the courseware
we constructed from English Wikipedia.
2 Algorithm
We want to prepare efficient courseware for learning
a target vocabulary. We defined efficiency in terms
of the amount of reading materials that must be read
to learn a required vocabulary. That is, efficient
courseware is as short as possible, while containing
the required vocabulary. We used a greedy method
to develop the efficient courseware (Utiyama et al,
2004).
Let C be the courseware under development and
V be the target vocabulary to be learned. We iter-
atively select a document (from the target corpus)
that has the largest number of new types8 (types con-
tained in V but not in C) and put it into C until C
covering all of V . ?C covers all of V ? means that
each word in V occurs at least once in a document
in C.
More concretely, let Vtodo be the part of V not
covered by C, and let Vdone be V ?Vtodo. We iter-
atively put document d into C that maximizes G(?),
G(d|?, Vtodo, Vdone)
= ?g(d|Vtodo) + (1? ?)g(d|Vdone), (1)
until C covers all of V . We then define g(?) as
g(d|Vx)
= k1 + 1
k1((1? b) + b |W (d)|E(|W (?)|) ) + 1
|W (d) ? Vx|, (2)
where W (d) is the set of types in d, E(|W (?)|) is
the average for |W (?)| over the whole corpus, and
k1 and b are parameters that depend on the corpus.
We set k1 as 1.5 and b as 0.75. g(d|Vx) takes a large
value when there is a large number of common types
between W (d) and Vx and d is short. These effects
are due to |W (d)?Vx| and |W (d)|E(|W (?)|) respectively. As
g(?) is based on the Okapi BM25 function (Robert-
son and Walker, 2000), which has been shown to be
quite efficient in information retrieval,9 we expected
7http://www.kotonoba.net/?mutiyama/vocabridge/
8A type refers to a unique word, while a token refers to each
occurrence of a type.
9BM25 and its variants have been proven to be quite effi-
cient in information retrieval. Readers are referred to papers by
the Text REtrieval Conference (TREC, http://trec.nist.gov/), for
example.
g(?) to be effective in retrieving documents relevant
to the target vocabulary.
In Eq. (1), ? is used to combine the scores of
document d, which are obtained by using Vtodo andVdone. It is defined as
? = |Vdone|1 + |Vdone|
(3)
This implies that even if |W (d) ? Vtodo| is 1, it is
as important as |W (d) ? Vdone| = |Vdone|. Con-
sequently, G(?) uses documents that have new types
of the given vocabulary in preference to documents
that have covered types.
To summarize, efficient courseware is constructed
by putting document d with maximum G(?) into C
until C covers all of V . This allows us to construct
efficient courseware because G(?) takes a large value
when a document has a large number of new types
and is short.
3 Experiment
This section describes how the courseware was con-
structed by applying the method described in the
previous section. We will first describe the vocab-
ulary and corpus used to construct the courseware
and then present the statistics for the courseware.
3.1 Vocabulary
We used the specialized vocabulary used in the
Test of English for International Communication
(TOEIC) because it is one of the most popular En-
glish certification tests in Japan. The vocabulary was
compiled by Chujo (2003) and Chujo et al (2004),
who confirmed that the vocabulary was useful in
preparing for the TOEIC test. The vocabulary had
640 entries and we used 638 words from it that oc-
curred at least once in the corpus as the target vocab-
ulary.
3.2 Corpus
We used articles from English Wikipedia as the tar-
get corpus, which is a free-content encyclopedia that
anyone can edit. The version we used in this study
had 478,611 articles. From these, we first discarded
stub and other non-normal articles. We also dis-
carded short articles of less than 150 words. We then
selected 60,498 articles that were referred to (linked)
by more than 15 articles. This 15-link threshold was
118
set empirically to screen out noisy articles. Finally,
we extracted a 150-word excerpt from the lead part
of each of these 60,498 articles to prepare the target
corpus. We set 150-word limit on an empirical basis
to reduce the burden imposed on learners. In short,
the target corpus consisted of 60,498 excerpts from
the English Wikipedia. In the rest of the paper, we
will use the term an article to refer to an excerpt that
was extracted according to this procedure.
3.3 Example article
Figure 1 has an example of the articles in the course-
ware. It was the first article obtained with the al-
gorithm. It shares 27 types and 49 tokens with the
target vocabulary. These words are printed in bold.
Corporate finance
Corporate finance is the specific area of finance dealing with the fi-
nancial decisions corporations make, and the tools and analysis used
to make the decisions. The discipline as a whole may be divided between
long-term and short-term decisions and techniques. Both share the same
goal of enhancing firm value by ensuring that return on capital exceeds
cost of capital. Capital investment decisions comprise the long-term
choices about which projects receive investment, whether to finance that
investment with equity or debt, and when or whether to pay dividends to
shareholders. Short-term corporate finance decisions are called working
capital management and deal with balance of current assets and cur-
rent liabilities by managing cash, inventories, and short-term borrowing
and lending (e.g., the credit terms extended to customers). Corporate fi-
nance is closely related to managerial finance, which is slightly broader in
scope, describing the financial techniques available to all forms of busi-
ness ... (more)
Figure 1: Example article
3.4 Courseware statistics
3.4.1 Basic courseware statistics
Table 1 lists basic statistics for the courseware
constructed from the target vocabulary and corpus.10
The courseware consisted of 131 articles. Each
article was 150 words long because only excerpts
were used. The average number of tokens per ar-
ticle shared with the vocabulary (?num. of com-
mon tokens? in the Table) was 18.4 and that of
types (?num. of common types?) was 12.4. About
12.3%(= 18.4150 ? 100) of the tokens in each article
were covered by the vocabulary. Each article in the
10On our web site, we prepared 10 sets of article sets called
course-1 to course-10. These 10 courses were obtained by re-
peatedly applying our algorithm to the English Wikipedia re-
moving articles included in earlier courses. The statistics pre-
sented in this paper were calculated from the first courseware,
course-1.
courseware was referred to by 70.7 articles on av-
erage as can be seen from the bottom row. Table
1 indicates that articles in the courseware included
many target words and were heavily referred to by
other articles.
3.4.2 Distribution of covered types
Figure 2 plots the increase in the number of cov-
ered types against the order (ranking) of articles that
were put into the courseware. The horizontal axis
represents the ranking of articles. The vertical axis
indicates the number of covered types. The increase
was sharpest when the ranking value was lowest (left
of figure). The dotted horizontal lines indicate 50%
and 90% of the target vocabulary. These lines cross
the curved solid line at the 22nd and 83rd articles,
i.e., 16.8% and 63.4% of the courseware, respec-
tively. This means that learners can learn most of the
target vocabulary from the beginning of the course-
ware. This is desirable because learners sometimes
do not have enough time to read all the courseware.
0
100
200
300
400
500
600
700
0 20 40 60 80 100 120 140
nu
m.
 of 
typ
es
 
article ranking
90%50%
Figure 2: Increase in the number of covered types
3.4.3 Document frequency distribution
Figure 3 has target words that occurred in eight ar-
ticles or more. The numbers in parentheses indicate
the document frequencies (DFs) of the words, where
the DF of a word is the number of articles in which
the word occurred. These words were the most ba-
sic words in the target vocabulary with respect to the
courseware.
Table 2 lists the distribution of DFs. The first
column lists the different DFs of the target words.
The values in the ?#DF? column are the numbers of
119
Table 1: Basic courseware statistics (number of articles: 131, length of each article: 150 words)
Average SD Min Median Max
Num. of common tokens 18.4 10.8 1 16 55
Num. of common types 12.4 5.5 1 12 27
Num. of incoming links 70.7 145.3 16 32 1056
SD means standard deviation.
words that occurred in the corresponding DF arti-
cles. The ?CUM? and ?CUM%? columns show the
cumulative numbers and percentages of words cal-
culated from the values in the second column. As we
can see from Table 2, more than 50% of the target
words occurred in multiple articles. Consequently,
learners were likely to be sufficiently exposed to ef-
ficiently learn the target vocabulary.
service (19), form (17), information (12), feature (12), op-
eration (11), cost (11), individual (10), department (10),
consumer (9), company (9), product (9), complete (9),
range (9), law (9), associate (9), cause (9), consider (9),
offer (9), provide (9), present (8), activity (8), due (8),
area (8), bill (8), require (8), order (8)
Figure 3: Target words and their DFs.
Table 2: Document frequency distribution
DF #DF CUM CUM%
19 1 1 0.2
17 1 2 0.3
12 2 4 0.6
11 2 6 0.9
10 2 8 1.3
9 11 19 3.0
8 7 26 4.1
7 20 46 7.2
6 25 71 11.1
5 35 106 16.6
4 36 142 22.3
3 71 213 33.4
2 118 331 51.9
1 307 638 100.0
4 Conclusion
While many teachers agree that vocabulary learn-
ing can be fostered by presenting words in context
rather than isolating them from this, it is very dif-
ficult to prepare reading materials that contain the
specialized vocabulary to be learned. We have pro-
posed a method of automating this preparation pro-
cess (Utiyama et al, 2004). We have found that our
reading materials prepared from The Daily Yomiuri
were effective in vocabulary learning (Tanimura and
Utiyama, in preparation).
Our next goal is to distribute courseware (pro-
duced with our algorithm) to EFL teachers and
learners so that we can receive wider feedback. To
this end, we replaced The Daily Yomiuri, which
is copyrighted, with the English Wikipedia, which
is a free-content encyclopedia, and developed new
courseware whose statistics were presented and dis-
cussed in this paper. This courseware, which is
available on our web site, can be used to supplement
classroom learning activities as well as self-study.
We hope it will help EFL learners to learn and teach-
ers to teach a broader range of vocabulary.
References
K. Chujo, T. Ushida, A. Yamazaki, M. Genung, A. Uchi-
bori, and C. Nishigaki. 2004. Bijuaru beishikku
niyoru TOEIC-yoo goiryoku yoosei sofutowuea no
shisaku (3) [The development of English CD-ROM
material to teach vocabulary for the TOEIC test (uti-
lizing Visual Basic): Part 3]. Journal of the College of
Industrial Technology, Nihon University, 37, 29-43.
K. Chujo. 2003. Eigo shokyuushamuke TOEIC Goi 1 &
2 no sentei to sono kouka [Selecting TOEIC vocabu-
lary 1 & 2 for beginning-level students and measuring
its effect on a sample TOEIC test]. Journal of the Col-
lege of Industrial Technology Nihon University, 36:
27-42.
S. E. Robertson and S. Walker. 2000. Okapi/Keenbow at
TREC-8. In Proc. of TREC 8, pages 151?162.
Midori Tanimura and Masao Utiyama. in prepara-
tion. Reading materials for learning TOEIC vocabu-
lary based on corpus data.
Masao Utiyama, Midori Tanimura, and Hitoshi Isahara.
2004. Constructing English reading courseware. In
PACLIC-18, pages 173?179.
120
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 449?456,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Relevance Feedback Models for Recommendation
Masao Utiyama
National Institute of Information and Communications Technology
3-5 Hikari-dai, Soraku-gun, Kyoto 619-0289 Japan
mutiyama@nict.go.jp
Mikio Yamamoto
University of Tsukuba, 1-1-1 Tennodai, Tsukuba, 305-8573 Japan
myama@cs.tsukuba.ac.jp
Abstract
We extended language modeling ap-
proaches in information retrieval (IR) to
combine collaborative filtering (CF) and
content-based filtering (CBF). Our ap-
proach is based on the analogy between
IR and CF, especially between CF and rel-
evance feedback (RF). Both CF and RF
exploit users? preference/relevance judg-
ments to recommend items. We first in-
troduce a multinomial model that com-
bines CF and CBF in a language modeling
framework. We then generalize the model
to another multinomial model that approx-
imates the Polya distribution. This gener-
alized model outperforms the multinomial
model by 3.4% for CBF and 17.4% for
CF in recommending English Wikipedia
articles. The performance of the gener-
alized model for three different datasets
was comparable to that of a state-of-the-
art item-based CF method.
1 Introduction
Recommender systems (Resnick and Varian,
1997) help users select particular items (e.g,
movies, books, music, and TV programs) that
match their taste from a large number of choices
by providing recommendations. The systems ei-
ther recommend a set of N items that will be of
interest to users (top-N recommendation problem)
or predict the degree of users? preference for items
(prediction problem).
For those systems to work, they first have to
aggregate users? evaluations of items explicitly or
implicitly. Users may explicitly evaluate certain
movies as rating five stars to express their prefer-
ence. These evaluations are used by the systems
as explicit ratings (votes) of items or the systems
infer the evaluations of items from the behavior of
users and use these inferred evaluations as implicit
ratings. For example, systems can infer that users
may like certain items if the systems learn which
books they buy, which articles they read, or which
TV programs they watch.
Collaborative filtering (CF) (Resnick et al,
1994; Breese et al, 1998) and content-based (or
adaptive) filtering (CBF) (Allan, 1996; Schapire
et al, 1998) are two of the most popular types
of algorithms used in recommender systems. A
CF system makes recommendations to current
(active) users by exploiting their ratings in the
database. User-based CF (Resnick et al, 1994;
Herlocker et al, 1999) and item-based CF (Sarwar
et al, 2001; Karypis, 2001), among other CF algo-
rithms, have been studied extensively. User-based
CF first identifies a set of users (neighbors) that
are similar to the active user in terms of their rat-
ing patterns in the database. It then uses the neigh-
bors? rating patterns to produce recommendations
for the active user. On the other hand, item-based
CF calculates the similarity between items before-
hand and then recommends items that are similar
to those preferred by the active user. The perfor-
mance of item-based CF has been shown to be
comparable to or better than that of user-based CF
(Sarwar et al, 2001; Karypis, 2001). In contrast
to CF, CBF uses the contents (e.g., texts, genres,
authors, images, and audio) of items to make rec-
ommendations for the active user. Because CF
and CBF are complementary, much work has been
done to combine them (Basu et al, 1998; Yu et
al., 2003; Si and Jin, 2004; Basilico and Hofmann,
2004).
The approach we took in this study is designed
to solve top-N recommendation problems with im-
449
plicit ratings by using an item-based combination
of CF and CBF. The methods described in this
paper will be applied to recommending English
Wikipedia1 articles based on those articles edited
by active users. (This is discussed in Section 3.)
We use their editing histories and the contents of
their articles to make top-N recommendations. We
regard users? editing histories as implicit ratings.
That is, if users have edited articles, we consider
that they have positive attitudes toward the arti-
cles. Those implicit ratings are regarded as pos-
itive examples. We do not have negative examples
for learning their negative attitudes toward arti-
cles. Consequently, handling our application with
standard machine learning algorithms that require
both positive and negative examples for classifica-
tion (e.g., support vector machines) is awkward.
Our approach is based on the advancement in
language modeling approaches to information re-
trieval (IR) (Croft and Lafferty, 2003) and extends
these to incorporate CF. The motivation behind our
approach is the analogy between CF and IR, espe-
cially between CF and relevance feedback (RF).
Both CF and RF recommend items based on user
preference/relevance judgments. Indeed, RF tech-
niques have been applied to CBF, or adaptive fil-
tering, successfully (Allan, 1996; Schapire et al,
1998). Thus, it is likely that RF can also be applied
to CF.
To apply RF, we first extend the representation
of items to combine CF and CBF under the models
developed in Section 2. In Section 3, we report
our experiments with the models. Future work and
conclusion are in Sections 4 and 5.
2 Relevance feedback models
The analogy between IR and CF that will be ex-
ploited in this paper is as follows.2 First, a docu-
ment in IR corresponds to an item in CF. Both are
represented as vectors. A document is represented
as a vector of words (bag-of-words) and an item
is represented as a vector of user ratings (bag-of-
user ratings). In RF, a user specifies documents
that are relevant to his information need. These
documents are used by the system to retrieve new
1http://en.wikipedia.org/wiki/Main Page
2The analogy between IR and CF has been recognized.
For example, Breese et al (1998) used the vector space
model to measure the similarity between users in a user-based
CF framework. Wang et al (2005) used a language modeling
approach different from ours. These works, however, treated
only CF. In contrast with these, our model extends language
modeling approaches to incorporate both CF and CBF.
relevant documents. In CF, an active user (implic-
itly) specifies items that he likes. These items are
used to search new items that will be preferred by
the active user.
We use relevance models (Lavrenko and Croft,
2001; Lavrenko, 2004) as the basic framework
of our relevance feedback models because (1)
they perform relevance feedback well (Lavrenko,
2004) and (2) they can simultaneously handle dif-
ferent kinds of features (e.g., different language
texts (Lavrenko et al, 2002), such as texts and im-
ages (Leon et al, 2003). These two points are es-
sential in our application.
We first introduce a multinomial model follow-
ing the work of Lavrenko (2004). This model is
a novel one that extends relevance feedback ap-
proaches to incorporate CF. It is like a combina-
tion of relevance feedback (Lavrenko, 2004) and
cross-language information retrieval (Lavrenko et
al., 2002). We then generalize that model to an ap-
proximated Polya distribution model that is better
suited to CF and CBF. This generalized model is
the main technical contribution of this work.
2.1 Preparation
Lavrenko (2004) adopts the method of kernels to
estimate probabilities: Let d be an item in the
database or training data, the probability of item x
is estimated as p(x) = 1M
?
d p(x|?d), where M
is the number of items in the training data, ?d is the
parameter vector estimated from d, and p(x|?d) is
the conditional probability of x given ?d.3 This
means that once we have defined a probability dis-
tribution p(x|?) and the method of estimating ?d
from d, then we can assign probability p(x) to x
and apply language modeling approaches to CF
and CBF.
To begin with, we define the representation
of item x as the concatenation of two vectors
{wx,ux}, where wx = wx1wx2 . . . is the se-
quence of words (contents) contained in x and
ux = ux1ux2 . . . is the sequence of users who
have rated x implicitly. We use Vw and Vu to de-
note the set of words and users in the database.
The parameter vector ? is also the concatenation
of two vectors {?, ?}, where ? and ? are the pa-
rameter vectors for Vw and Vu, respectively. The
probability of x given ? is defined as p(x|?) =
p?(wx|?)p?(ux|?).
3Item d in summation ?d and word w in
?
w and
?
wgo over every distinct item d and word w in the training data,
unless otherwise stated.
450
2.2 Multinomial model
Our first model regards that both p? and p? follow
multinomial distributions. In this case, ?(w) and
?(u) are the probabilities of word w and user u.
Then, p?(wx|?) is defined as
p?(wx|?) =
|wx|?
i=1
?(wxi) =
?
w?Vw
?(w)n(w,wx)
(1)
where n(w,wx) is the number of occurrences of w
in wx. In this model, we use a linear interpolation
method to estimate probability ?d(w).
?d(w) = ??Pl(w|wd) + (1? ??)Pg(w) (2)
where Pl(w|wd) = n(w,wd)?
w? n(w?,wd)
, Pg(w) =?
d n(w,wd)?
d
?
w? n(w?,wd)
and ?? (0 ? ?? ? 1) is
a smoothing parameter. The estimation of user
probabilities goes similarly: Let n(u,ux) be the
number of times user u implicitly rated item x,
we define or estimate p?, ?? and ?d in the same
way. In summary, we have defined a probability
distribution p(x|?) and the method of estimating
?d = {?d, ?d} from d.
To recommend top-N items, we have to rank
items in the database in response to the implicit
ratings of active users. We call those implicit rat-
ings query q. It is a set of items and is represented
as q = {q1 . . .qk}, where qi is an item implic-
itly rated by an active user and k is the size of q.
We next estimate ?q = {?q, ?q}. Then, we com-
pare ?q and ?d to rank items by using Kullback-
Leibler divergence D(?q||?d) (Lafferty and Zhai,
2001; Lavrenko, 2004).
?q(w) can be approximated as
?q(w) = 1k
k?
i=1
?qi(w) (3)
where ?qi(w) is obtained by Eq. 2 (Lavrenko,
2004). However, we found in preliminary experi-
ments that smoothing query probabilities hurt per-
formance in our application. Thus, we use
?qi(w) = Pl(w|wqi) =
n(w,wqi)?
w? n(w?,wqi)
(4)
instead of Eq. 2 when qi is in a query.
Because KL-divergence is a distance measure,
we use a score function derived from ?D(?q||?d)
to rank items. We use Sq(d) to denote the score
of d given q. Sq(d) is derived as follows. (We
ignore terms that are irrelevant to ranking items.)
?D(?q||?d) = ?D(?q||?d)?D(?q||?d)
?D(?q||?d) rank= 1k
k?
i=1
S(?qi ||?d) (5)
where
S(?qi ||?d) =
?
w
Pl(w|wqi)?log
(
??Pl(w|wd)
(1? ??)Pg(w) + 1
)
.
(6)
The summation goes over every word w that
is shared by both wqi and wd. We define
S(?qi ||?d) similarly.4 Then, the score of d given
qi, Sqi(d) is defined as
Sqi(d) = ?sS(?qi ||?d) + (1? ?s)S(?qi ||?d)
(7)
where ?s (0 ? ?s ? 1) is a free parameter. Fi-
nally, the score of d given q is
Sq(d) = 1k
k?
i=1
Sqi(d). (8)
The calculation of Sq(d) can be very efficient
because once we cache Sqi(d) for each item pair
of qi and d in the database, we can reuse it to cal-
culate Sq(d) for any query q. We further optimize
the calculation of top-N recommendations by stor-
ing only the top 100 items (neighbors) in decreas-
ing order of Sqi(?) for each item qi and setting
the scores of lower ranked items as 0. (Note that
Sqi(d) >= 0 holds.) Consequently, we only have
to search small part of the search space without
affecting the performance very much. These two
types of optimization are common in item-based
CF (Sarwar et al, 2001; Karypis, 2001).
2.3 Polya model
Our second model is based on the Polya distribu-
tion. We first introduce (hyper) parameter ? =
{??, ??} and denote the probability of x given
? as p(x|?) = p?(wx|??)p?(ux|??). ?? and
?? are the parameter vectors for words and users.
p?(wx|??) is defined as follows.
p?(wx|??) = ?(
?
w ??w)
?(?w nxw + ??w)
?
w
?(nxw + ??w)
?(??w)
(9)
4S(?qi ||?d) =
?
u Pl(u|uqi) ?
log
(
??Pl(u|ud)
(1???)Pg(u) + 1
)
, where Pl(u|uqi) =
n(u,?qi )?
u? n(u
?,?qi )
, Pl(u|ud) = n(u,ud)?
u? n(u
?,ud)
, and
Pg(u) =
?
d n(u,ud)?
d
?
u? n(u
?,ud)
.
451
0
1
2
3
4
5
6
7
8
9
10
0 2 4 6 8 10
0
1
2
3
4
5
6
7
8
9
10
nu
(n,a
lph
a)
n
alpha=1e+5
alpha=38.8
alpha=16.4
alpha=9.0
alpha=5.4
alpha=3.3
alpha=2.0
alpha=1.1
alpha=0.4
alpha=1e-5
Figure 1: Relationship between original count n
and dumped count ?(n, ?)
where ? is known as the gamma function, ??w is a
parameter for word w and nxw = n(w,wx). This
can be approximated as follows (Minka, 2003).
p?(wx|??) ?
?
w
?(w)n?(w,wx) (10)
where
n?(w,wx) = ??w(?(nxw + ??w)??(??w))
? ?(nxw, ??w) (11)
? is known as the digamma function and is sim-
ilar to the natural logarithm. We call Eq. 10 the
approximated Polya model or simply the Polya
model in this paper.
Eq. 10 indicates that the Polya distribution
can be interpreted as a multinomial distribution
over a modified set of counts n?(?) (Minka, 2003).
These modified counts are dumped as shown in
Fig. 1. When ??w ? ?, ?(nxw, ??w) approaches
nxw. When ??w ? 0, ?(nxw, ??w) = 0 if nxw = 0
otherwise it is 1. For intermediate values of ??w,
the mapping ? dumps the original counts.
Under the approximation of Eq. 10, the es-
timation of parameters can be understood as the
maximum-likelihood estimate of a multinomial
distribution from dumped counts n?(?) (Minka,
2003). Indeed, all we have to do to estimate the
parameters for ranking items is replace Pl and Pg
from Section 2.2 with Pl(w|wd) = n?(w,wd)?
w? n?(w?,wd)
,
Pg(w) =
?
d n?(w,wd)?
d
?
w? n?(w?,wd)
, and Pl(w|wqi) =
n?(w,wqi )?
w? n?(w?,wqi )
. Then, as in the multinomial model,
we can define S(?qi ||?d) with these probabilities.
This argument also applies to S(?qi ||?d).
The approximated Polya model is a generaliza-
tion of the multinomial model described in Sec-
tion 2.2. If we set ??w and ??u very large then the
Polya model is identical to the multinomial model.
By comparing Eqs. 1 and 10, we can see why the
Polya model is superior to the multinomial model
for modeling the occurrences of words (and users).
In the multinomial model, if a word with probabil-
ity p occurs twice, its probability becomes p2. In
the Polya model, the word?s probability becomes
p1.5, for example, if we set ??w = 1. Clearly,
p2 < p1.5; therefore, the Polya model assigns
higher probability. In this example, the Polya
model assigns probability p to the first occurrence
and p0.5(> p) to the second. Since words that oc-
cur once are likely to occur again (Church, 2000),
the Polya model is better suited to model the oc-
currences of words and users. See Yamamoto and
Sadamitsu (2005) for further discussion on apply-
ing the Polya distribution to text modeling.
Zaragoza et al(2003) applied the Polya distri-
bution to ad hoc IR. They introduced the exact
Polya distribution (see Eq. 9) as an extension
to the Dirichlet prior method (Zhai and Lafferty,
2001). However, we have introduced a multino-
mial approximation of the Polya distribution. This
approximation allows us to use the linear interpo-
lation method to mix the approximated Polya dis-
tributions. Thus, our model is similar to two-stage
language models (Zhai and Lafferty, 2002) that
combine the Dirichlet prior method and the lin-
ear interpolation method. In contrast to our model,
Zaragoza et al(2003) had difficulty in mixing the
Polya distributions and did not treat that in their
paper.
3 Experiments
We first examined the behavior of the Polya model
by varying the parameters. We tied ??w for every
w and ??u for every u; for any w and u, ??w = ??
and ??u = ??. We then compared the Polya model
to an item-based CF method.
3.1 Behavior of Polya model
3.1.1 Dataset
We made a dataset of articles from English
Wikipedia5 to evaluate the Polya model. English
Wikipedia is an online encyclopedia that anyone
5We downloaded 20050713 pages full.xml.gz
and 20050713 pages current.xml.gz from
http://download.wikimedia.org/wikipedia/en/.
452
can edit, and it has many registered users. Our
aim is to recommend a set of articles to each user
that is likely to be of interest to that user. If we
can successfully recommend interesting articles,
this could be very useful to a wide audience be-
cause Wikipedia is very popular. In addition, be-
cause wikis are popular media for sharing knowl-
edge, developing effective recommender systems
for wikis is important.
In our Wikipedia dataset, each item (article) x
consisted of wx and ux. ux was the sequence of
users who had edited x. If users had edited x mul-
tiple times, then those users occurred in ux multi-
ple times. wx was the sequence of words that were
typical in x. To make wx, we removed stop words
and stemmed the remaining words with a Porter
stemmer. Next, we identified 100 typical words
in each article and extracted only those words
(|wx| ? 100 because some of them occurred
multiple times). Typicality was measured using
the log-likelihood ratio test (Dunning, 1993). We
needed to reduce the number of words to speed up
our recommender system.
To make our dataset, we first extracted 302,606
articles, which had more than 100 tokens after the
stop words were removed. We then selected typi-
cal words in each article. The implicit rating data
were obtained from the histories of users editing
these articles. Each rating consisted of {user, ar-
ticle, number of edits}. The size of this original
rating data was 3,325,746. From this data, we ex-
tracted a dense subset that consisted of users and
articles included in at least 25 units of the original
data. We discarded the users who had edited more
than 999 articles because they were often software
robots or system operators, not casual users. The
resulting 430,096 ratings consisted of 4,193 users
and 9,726 articles. Each user rated (edited) 103
articles on average (the median was 57). The av-
erage number of ratings per item was 44 and the
median was 36.
3.1.2 Evaluation of Polya model
We conducted a four-fold cross validation of
this rating dataset to evaluate the Polya model. We
used three-fourth of the dataset to train the model
and one-fourth to test it.6 All users who existed in
6We needed to estimate probabilities of users and words.
We used only training data to estimate the probabilities of
users. However, we used all 9,726 articles to estimate the
probabilities of words because the articles are usually avail-
able even when editing histories of users are not.
both training and test data were used for evalua-
tion. For each user, we regarded the articles in the
training data that had been edited by the user as a
query and ranked articles in response to it. These
ranked top-N articles were then compared to the
articles in the test data that were edited by the
same user to measure the precisions for the user.
We used P@N (precision at rank N = the ratio of
the articles edited by the user in the top-N articles),
S@N (success at rank N = 1 if some top-N articles
were edited by the user, else 0), and R-precision (=
P@N, where N is the number of articles edited by
the user in the test data). These measures for each
user were averaged over all users to get the mean
precision of each measure. Then, these mean pre-
cisions were averaged over the cross validation re-
peats.
Here, we report the averaged mean pre-
cisions with standard deviations. We first
report how R-precision varied depend-
ing on ? (?? or ??). ? was varied over
10?5, 0.4, 1.1, 2, 3.3, 5.4, 9, 16.4, 38.8, and 105.
The values of ?(10, ?) were approximately 1, 2,
3, 4, 5, 6, 7, 8, 9, and 10, respectively, as shown
in Fig. 1. When ? = 105, the Polya model
represents the multinomial model as discussed in
Section 2.3. For each value of ?, we varied ? (??
or ??) over 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
0.7, 0.8, 0.9, 0.95, and 0.99 to obtain the optimum
R-precision. These optimum R-precisions are
shown in Fig. 2. In this figure, CBF and CF
represent the R-precisions for the content-based
and collaborative filtering part of the Polya model.
The values of CBF and CF were obtained by
setting ?s = 0 and ?s = 1 in Eq. 7 (which
is applied to the Polya model instead of the
multinomial model), respectively. The error bars
represent standard deviations.
At once, we noticed that CBF outperformed
CF. This is reasonable because the contents of
Wikipedia articles should strongly reflect the users
(authors) interest. In addition, each article had
about 100 typical words, and this was richer than
the average number of users per article (44). This
observation contrasts with other work where CBF
performed poorly compared with CF, e.g., (Ali and
van Stam, 2004).
Another important observation is that both
curves in Fig. 2 are concave. The best R-
precisions were obtained at intermediate values of
? for both CF and CBF as shown in Table 1.
453
0.065
0.07
0.075
0.08
0.085
0.09
0.095
1 2 3 4 5 6 7 8 9 10
R-
pre
cis
ion
nu(10,alpha)
CBF
CF
Figure 2: R-precision for Polya model
Table 1: Improvement in R-precision (RP)
best RP (?(?)/?) RP (?(?)/?) %change
CBF 0.091 (7/9.0) 0.088 (10/105) +3.4%
CF 0.081 (2/0.4) 0.069 (10/105) +17.4%
When ? = 105 or ?(10, ?) ? 10, the Polya
model represents the multinomial model as dis-
cussed in Section 2.3. Thus, Fig. 2 and Table 1
show that the best R-precisions achieved by the
Polya model were better than those obtained by
the multinomial model. The improvement was
3.4% for CBF and 17.4% for CF as shown in Ta-
ble 1. The improvement of CF was larger than
that of CBF. This implies that the occurrences of
users are more clustered than those of words. In
other words, the degree of repetition in the editing
histories of users is greater than that in word se-
quences. A user who edits an article are likely to
edit the article again.
From Fig. 2 and Table 1, we concluded that the
generalization of a multinomial model achieved by
the Polya model is effective in improving recom-
mendation performance.
3.1.3 Combination of CBF and CF
Next, we show how the combination of CBF
and CF improves recommendation performance.
We set ? (?? and ??) to the optimum values in
Table 1 and varied ? (?s, ?? and ??) to obtain the
R-precisions for CBF+CF, CBF and CF in Fig. 3.
The values of CBF were obtained as follows. We
first set ?s = 0 in Eq. 7 to use only CBF scores
and then varied ??, which is the smoothing pa-
rameter for word probabilities, in Eq. 2. To get
the values of CF, we set ?s = 1 in Eq. 7 and then
varied ??, which is the smoothing parameter for
user probabilities. The values of CBF+CF were
obtained by varying ?s in Eq. 7 while setting ??
and ?? to the optimum values obtained from CBF
0.055
0.06
0.065
0.07
0.075
0.08
0.085
0.09
0.095
0.1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
R-
pre
cis
ion
lambda
CBF+CF
CBF
CF
Figure 3: Combination of CBF and CF.
Table 2: Precision and Success at top-N
CBF+CF CBF CF
N P@N S@N P@N S@N P@N S@N
5 0.166 0.470 0.149 0.444 0.137 0.408
10 0.135 0.585 0.123 0.562 0.112 0.516
15 0.117 0.650 0.107 0.628 0.098 0.582
20 0.105 0.694 0.096 0.671 0.089 0.627
R-precision 0.099 0.091 0.081
optimum ? ?s = 0.2 ?? = 0.01 ?? = 0.2
and CF (see Table 2). These parameters (?s, ??
and ??) were defined in the context of the multi-
nomial model in Section 2.2 and used similarly in
the Polya model in this experiment.
We can see that the combination was quite ef-
fective as CBF+CF outperformed both CBF and
CF. Table 2 shows R-precision, P@N and S@N
for N = 5, 10, 15, 20. These values were obtained
by using the optimum values of ? in Fig. 3.
Table 2 shows the same tendency as Fig. 3. For
all values of N , CBF+CF outperformed both CBF
and CF. We attribute this effectiveness of the com-
bination to the feature independence of CBF and
CF. CBF used words as features and CF used user
ratings as features. They are very different kinds
of features and thus can provide complementary
information. Consequently, CBF+CF can exploit
the benefits of both methods. We need to do fur-
ther work to confirm this conjecture.
3.2 Comparison with a baseline method
We compared the Polya model to an implementa-
tion of a state-of-the-art item-based CF method,
CProb (Karypis, 2001). CProb has been tested
with various datasets and found to be effective in
top-N recommendation problems. CProb has also
been used in recent work as a baseline method
(Ziegler et al, 2005; Wang et al, 2005).
In addition to the Wikipedia dataset, we used
two other datasets for comparison. The first was
454
R-precision P@10
WP ML BX WP ML BX
Polya-CF 0.081 0.272 0.066 0.112 0.384 0.054
CProb 0.082 0.258 0.071 0.113 0.373 0.057
%change -1.2% +5.4% -7.0% -0.9% +2.9% -5.3%
Table 3: Comparison of Polya-CF and CProb
the 1 million MovieLens dataset.7 This data con-
sists of 1,000,209 ratings of 3,706 movies by 6,040
users. Each user rated an average of 166 movies
(the median was 96). The average number of rat-
ings per movie was 270 and the median was 124.
The second was the BookCrossing dataset (Ziegler
et al, 2005). This data consists of 1,149,780 rat-
ings of 340,532 books by 105,283 users. From
this data, we removed books rated by less than 20
users. We also removed users who rated less than
5 books. The resulting 296,471 ratings consisted
of 10,345 users and 5,943 books. Each user rated
29 books on average (the median was 10). The av-
erage number of ratings per book was 50 and the
median was 33. Note that in our experiments, we
regarded the ratings of these two datasets as im-
plicit ratings. We regarded the number of occur-
rence of each rating as one.
We conducted a four-fold cross validation for
each dataset to compare CProb and Polya-CF,
which is the collaborative filtering part of the
Polya model as described in the previous section.
For each cross validation repeat, we tuned the pa-
rameters of CProb and Polya-CF on the test data to
get the optimum R-precisions, in order to compare
best results for these models.8 P@N and S@N
were calculated with the same parameters. These
measures were averaged as described above. R-
precision and P@10 are in Table 3. The max-
imum standard deviation of these measures was
0.001. We omitted reporting other measures be-
cause they had similar tendencies. In Table 3, WP,
ML and BX represent the Wikipedia, MovieLens,
and BookCrossing datasets.
In Table 3, we can see that the variation of per-
formance among datasets was greater than that be-
tween Polya-CF and CProb. Both methods per-
7http://www.grouplens.org/
8CProb has two free parameters. Polya-CF also has two
free parameters (?? and ??). However, for MovieLens and
BookCrossing datasets, Polya-CF has only one free parame-
ter ??, because we regarded the number of occurrence of each
rating as one, which means ?(1, ??) = 1 for all ?? > 0 (See
Fig. 1). Consequently, we don?t have to tune ??. Since the
number of free parameters is small, the comparison of perfor-
mance shown in Table 3 is likely to be reproduced when we
tune the parameters on separate development data instead of
test data.
formed best against ML. We think that this is be-
cause ML had the densest ratings. The average
number of ratings per item was 270 for ML while
that for WP was 44 and that for BX was 50.
Table 3 also shows that Polya-CF outperformed
CProb when the dataset was ML and CProb was
better than Polya-CF in the other cases. However,
the differences in precision were small. Overall,
we can say that the performance of Polya-CF is
comparable to that of CProb.
An important advantage of the Polya model
over CProb is that the Polya model can unify CBF
and CF in a single language modeling framework
while CProb handles only CF. Another advantage
of the Polya model is that we can expect to im-
prove its performance by incorporating techniques
developed in IR because the Polya model is based
on language modeling approaches in IR.
4 Future work
We want to investigate two areas in our future
work. One is the parameter estimation and the
other is the refinement of the query model.
We tuned the parameters of the Polya model by
exhaustively searching the parameter space guided
by R-precision. We actually tried to learn ??
and ?? from the training data by using an EM
method (Minka, 2003; Yamamoto and Sadamitsu,
2005). However, the estimated parameters were
about 0.05, too small for better recommendations.
We need further study to understand the relation
between the probabilistic quality (perplexity) of
the Polya model and its recommendation quality.
We approximate the query model as Eq. 3. This
allows us to optimize score calculation consider-
ably. However, this does not consider the interac-
tion among items, which may deteriorate the qual-
ity of probability estimation. We want to inves-
tigate more efficient query models in our future
work.
5 Conclusion
Recommender systems help users select particular
items from a large number of choices by provid-
ing recommendations. Much work has been done
to combine content-based filtering (CBF) and col-
laborative filtering (CF) to provide better recom-
mendations. The contributions reported in this pa-
per are twofold: (1) we extended relevance feed-
back approaches to incorporate CF and (2) we in-
troduced the approximated Polya model as a gen-
455
eralization of the multinomial model and showed
that it is better suited to CF and CBF. The perfor-
mance of the Polya model is comparable to that of
a state-of-the-art item-based CF method.
Our work shows that language modeling ap-
proaches in information retrieval can be extended
to CF. This implies that a large amount of work
in the field of IR could be imported into CF. This
would be interesting to investigate in future work.
References
Kamal Ali and Wijnand van Stam. 2004. TiVo: Mak-
ing show recommendations using a distributed col-
laborative filtering architecture. In KDD?04.
James Allan. 1996. Incremental relevance feedback
for information filtering. In SIGIR?96.
Justin Basilico and Thomas Hofmann. 2004. Uni-
fying collaborative and content-based filtering. In
ICML?04.
Chumki Basu, Haym Hirsh, and William Cohen. 1998.
Recommendation as classification: Using social and
content-based information in recommendation. In
AAAI-98.
John S. Breese, David Heckerman, and Carl Kadie.
1998. Empirical analysis of predictive algorithms
for collaborative filtering. Technical report, MSR-
TR-98-12.
Kenneth W. Church. 2000. Empirical estimates of
adaptation: The chance of two Noriegas is closer to
p/2 than p2. In COLING-2000, pages 180?186.
W. Bruce Croft and John Lafferty, editors. 2003. Lan-
guage Modeling for Information Retrieval. Kluwer
Academic Publishers.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Jonathan L. Herlocker, Joseph A. Konstan,
Al Borchers, and John Riedl. 1999. An algo-
rithmic framework for performing collaborative
filtering. In SIGIR?99, pages 230?237.
George Karypis. 2001. Evaluation of item-based top-
N recommendation algorithms. In CIKM?01.
John Lafferty and ChengXiang Zhai. 2001. Document
language models, query models and risk minimiza-
tion for information retrieval. In SIGIR?01.
Victor Lavrenko and W. Bruce Croft. 2001.
Relevance-based language models. In SIGIR?01.
Victor Lavrenko, Martin Choquette, and W. Bruce
Croft. 2002. Cross-lingual relevance models. In
SIGIR?02, pages 175?182.
Victor Lavrenko. 2004. A Generative Theory of Rele-
vance. Ph.D. thesis, University of Massachusetts.
J. Leon, V. Lavrenko, and R. Manmatha. 2003. Au-
tomatic image annotation and retrieval using cross-
media relevance models. In SIGIR?03.
Thomas P. Minka. 2003. Es-
timating a Dirichlet distribution.
http://research.microsoft.com/?minka/papers/dirichlet/.
Paul Resnick and Hal R. Varian. 1997. Recommender
systems. Communications of the ACM, 40(3):56?
58.
Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Pe-
ter Bergstrom, and John Riedl. 1994. GroupLens:
An open architecture for collaborative filtering of
netnews. In CSCW?94, pages 175?186.
Badrul Sarwar, George Karypis, Joseph Konstan, and
John Riedl. 2001. Item-based collaborative filtering
recommendation algorithms. In WWW10.
Robert E. Schapire, Yoram Singer, and Amit Singhal.
1998. Boosting and Rocchio applied to text filtering.
In SIGIR?98, pages 215?223.
Luo Si and Rong Jin. 2004. Unified filtering by com-
bining collaborative filtering and content-based fil-
tering via mixture model and exponential model. In
CIKM-04, pages 156?157.
Jun Wang, Marcel J.T. Reinders, Reginald L. La-
gendijk, and Johan Pouwelse. 2005. Self-
organizing distributed collaborative filtering. In SI-
GIR?05, pages 659?660.
Mikio Yamamoto and Kugatsu Sadamitsu. 2005.
Dirichlet mixtures in text modeling. Technical re-
port, University of Tsukuba, CS-TR-05-1.
Kai Yu, Anton Schwaighofer, Volker Tresp, Wei-Ying
Ma, and HongJiang Zhang. 2003. Collaborative
ensemble learning: Combining collaborative and
content-based information filtering via hierarchical
Bayes. In UAI-2003.
Hugo Zaragoza, Djoerd Hiemstra, and Michael Tip-
ping. 2003. Bayesian extension to the language
model for ad hoc information retrieval. In SIGIR?03.
ChengXiang Zhai and John Lafferty. 2001. A study of
smoothing methods for language models applied to
ad hoc information retrieval. In SIGIR?01.
ChengXiang Zhai and John Lafferty. 2002. Two-stage
language models for information retrieval. In SI-
GIR?02, pages 49?56.
Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Kon-
stan, and Georg Lausen. 2005. Improving rec-
ommendation lists through topic diversification. In
WWW?05, pages 22?32.
456
A Statistical Approach to the Processing of Metonymy 
Masao Ut iyama,  Masak i  Murata ,  and H i tosh i  I sahara  
Communicat ions  Research L~boratory, MPT ,  
588-2, Iwaoka, Nishi-ku, Kobe, Hyogo 651-2492 Japa l  
{mut iyam~,murat~, isahara} ~crl.go.j  p
Abst ract  
This paper describes a statistical approach to 
tile interpretation of metonymy. A metonymy 
is received as an input, then its possible inter- 
p retations are ranked by al)t)lying ~ statistical 
measure. The method has been tested experi- 
mentally. It; correctly interpreted 53 out of 75 
metonymies in Jat)anese. 
1 I n t roduct ion  
Metonymy is a figure of st)eech in which tile 
name of one thing is substituted for that of 
something to which it is related. The czplicit 
tc.~m is 'the name of one thing' and the implicit 
t;c~"m is 'the name of something to which it; is 
related'. A typical examt)le of m(;tonymy is
He read Shal(esl)eare. (1) 
'Slmkesl)(~are' is substitut(~d for 'the works of 
Shakespeare'. 'Shakest)eare' is the explicit term 
and 'works' is the implicit term. 
Metonymy is pervasive in natural language. 
The correc~ treatment of lnetonylny is vital tbr 
natural language l)rocessing api)lications , es- 
1)ecially for machine translation (Kamei and 
Wakao, 19!)2; Fass, 1997). A metonymy may be 
aecel)table in a source language but unaccet)t- 
able in a target language. For example, a direct 
translation of 'he read Mao', which is acceptable 
in English an(1 Japanese, is comt)letely unac- 
ceptal)le in Chinese (Kamei and Wakao, 1992). 
In such cases, the machine trmlslation system 
has to interl)ret metonynfies to generate accept- 
able translations. 
Previous approaches to processing lnetonymy 
have used hand-constructed ontologies or se- 
mantic networks (.\]?ass, 1988; Iverson and Hehn- 
reich, 1992; B(maud et al, 1996; Fass, 1997). 1 
1As for metal)her l)rocessing, I 'errari (1996) used t;ex- 
Such al)t)roaches are restricted by the knowl- 
edge bases they use, and may only be applicable 
to domain-specific tasks because the construc- 
tion of large knowledge bases could be very d i f  
ficult. 
The method outlined in this I)apcr, on the 
other hand, uses cortms statistics to interpret 
metonymy, so that ~ variety of metonynfies 
can be handled without using hand-constructed 
knowledge bases. The method is quite t)romis- 
ing as shown by the exl)erimental results given 
in section 5. 
2 Recogn i t ion  and  In terpretat ion  
Two main steps, recognition and i'ntc.'q~vc- 
ration, are involved in the processing of 
metonyn~y (Fass, 1.!)97). in tile recognition st;el), 
metonylnic exl)ressions are labeled. 1111 the in- 
tel'l)r(:tation st;el) , the meanings of those ext)res- 
sions me int, eri)reted. 
Sentence (1), for examl)le, is first recognized 
as a metonymy an(t ~Shakespeare' is identified 
as the explicit term. 't'he interpretation 'works' 
is selected as an implicit term and 'Shakespeare' 
is replaced 1)y 'the works of Shakespeare'. 
A conq)rehensive survey by Fass (\]997) shows 
that the most COllllllOll metho(1 of recogniz- 
ing metonymies i by selection-restriction vio- 
lations. Whether or not statistical approaches 
can recognize metonymy as well as the selection- 
restriction violation method is an interesting 
question. Our concern here, however, is the 
interpretation of metonymy, so we leave that 
question for a future work. 
In interpretation, an implicit term (or terms) 
that is (are) related to the explicit term is (are) 
selected. The method described in this paper 
uses corpus st~tistics for interpretation. 
tual clues obtained through corl)us mmlysis tor detecting 
metal)lmrs. 
885 
This method, as applied to Japanese 
metonymies, receives a metonymy in a phrase 
of the tbnn 'Noun A Case-Marker R Predicate 
V' and returns a list of nouns ranked in or- 
der of the system's estimate of their suitability 
as interpretations of the metonylny, aSSulning 
that noun A is the explicit tenn. For exam- 
ple, given For'a  wo (accusative-case) kau (buy) 
(buy a Ford),  Vay .sya (ear), V .st .sdl  , 
r'uma (vehicle), etc. are returned, in that order. 
Tile method fbllows tile procedure outlined 
below to interpret a inetonymy. 
1. Given a metonymy in the form 'Noun A 
Case-Marker R Predicate V', nouns that 
can 1)e syntactically related to the explicit 
term A are extracted from a corpus. 
2. The extracted nouns are rmlked according 
to their appropriateness a interpretations 
of the metonymy by applying a statistical 
measure. 
The first step is discussed in section 3 and the 
second in section 4. 
3 In fo rmat ion  Source  
\?e use a large corpus to extract nouns which 
can be syntactically related to the exl)licit term 
of a metonylny. A large corpus is vahmble as a 
source of such nouns (Church and Hanks, 1990; 
Brown et al, 1992). 
We used Japanese noun phrases of the fornl 
A no B to extract nouns that were syntactically 
related to A. Nouns in such a syntactic relation 
are usually close semantic relatives of each other 
(Murata et al, 1999), and occur relatively infre- 
quently. We thus also used an A near B rela- 
tion, i.e. identifying tile other nouns within the 
target sentence, to extract nouns that may be 
more loosely related to A, trot occur more fre- 
quently. These two types of syntactic relation 
are treated differently by the statistical nleasure 
which we will discuss in section 4. 
The Japanese noun phrase A no B roughly 
corresponds to the English noun phrase B of A, 
lint it has a nmch broader ange of usage (Kuro- 
hashi and Sakai, 1999). In fact, d no B can ex- 
press most of the possible types of semmltic re- 
lation between two nouns including metonymic 
2~Ford' is spelled qtSdo' ill Japanese. We have used 
English when we spell Japanese loan-words from English 
for the sake of readability. 
concepts uch as that the name of a container 
can represent its contents and the name of an 
artist can imply an art~brnl (conta iner  for 
contents and artist for a r t fo rm below).a Ex- 
amples of these and similar types of metonymic 
concepts (Lakoff and Johnson, 1980; Fass, 1997) 
are given below. 
Container for contents  
? glass no mizu (water) 
? naV  (pot) , y6 i (food) 
Art ist  for artform 
? Beethoven o kyoku (music) 
? Picas.so no e (painting) 
Object  for user 
? ham .sandwich no kyaku (customer) 
? sax no .sO.sya (t)erformer) 
Whole  tbr part 
? kuruma (car) no tirc 
? door" no knob 
These exalnt)les uggest hat we can extract 
semantically related nouns by using tile A no B 
relation. 
4 Stat is t ica l  Measure  
A nletonymy 'Noun A Case-Marker R, Predi- 
cate V' can be regarded as a contraction of 
'Noun A Syntactic-Relation (2 Noun B Case- 
Marker R Predicate V', where A has relation 
Q to B (Yamamoto et al, 1998). For exam- 
ple, Shakc.spcare wo yomu (read) (read Shake- 
speare) is regarded as a contraction of Shake- 
speare no .sakuhin (works) 'wo yomu (read the 
works of Shakespeare), where A=Shake.spcare, 
Q=no, B=.sakuhin, R=wo,  and V=yomu. 
Given a metonymy in the fbrln A R 17, the 
appropriateness of noun B as an interpretation 
of the metonymy under the syntactic relation Q 
is defined by 
LQ(BIA,/~, V) - Pr(BIA, (2, 1~, V), (2) 
ayamamoto et al (\]998) also used A no /3 relation 
to interpret metonymy. 
886 
where Pr( . - . )  represents l)robal/ility and Q is 
either an A no B relation or an A near \]3 re- 
lation. Next;, the appropriateness of noun \]3 is 
defined by 
M(BIA, Ie, V) -nlaxLc~(BIA, l~,V ). (3) 
O 
We rank nouns 1)y at)plying the measure 214. 
Equation (2) can be decomposed as follows: 
LQ(!31A, R,, V) 
= Pr (B IA  , Q, R,, V)  
Pr(A, Q, B, R,, V) 
Pr( A, Q, R, v) 
Pr(A, Q, 13)lh'(R, VIA, Q, Ix) 
Pr(A, Q) Pr(R, VIA, Q) 
Pr(BIA , Q)Pr(R, VIB) 
-~ er(R, v) ' (4) 
where (A, O) and {\]~,, V} are assumed to l)e in- 
del)endent of each other. 
Let f(event)1)e the frequen(:y of an cve'nt and 
Classc.s(\])) be the set of semantic (:lasses to 
which B belongs. 'l'he expressions in Equation 
(4) are then detined t)y 4 
I'r(~lA, Q) - .t'(A, Q, ~x) _ f (A,  Q, ~) 
f (A ,  Q) ~1~ f (A ,  Q, 13)' 
(5) 
Pr(~., riB) 
IU~,I~,v) i' ' *: .1 (U, ~, V) > 0, 
.~- ~,c~cl .......... (10 Pr(l)'l(/)f(C/'R'V) 
J'US) 
otherwise, 
((0 
Pr (B IC  ) - .f(13)/ICI-s.w-.XB)l j ( c )  (r) 
We onfitted Pr(H,, 17) fi'om Equat ion (4) whell 
we calculated Equation (3) in the experiment 
de, scribed in section 5 for the sake of simplicit> 
4Strictly speaking, Equation (6) does not satist\]y 
X',,e,vpr(R, vl/x) -- 1. We h~wc adopted this det- 
inition for the sake of simplicity. This simplifi- 
cation has little effect on the tilml results because 
~--;c'cc~ ........ (m Pr(l~lC)f(C,I~', V) << I will usually 
hohl. More Sol)histieated methods (M;mning ml(t 
Schiitze, 1999) of smoothing f)robability distribution 
m~y I)e I)eneticial. itowever, al)l)lying such methods 
and comparing their effects on the interpretation of
metonymy is beyond the scope of this l)aper. 
This t reatment  does not alter the order of the 
nouns ranked by the syst;em because l?r(H., V) 
is a constant for a given metonymy of the form 
AR V. 
Equations (5) and (6) difl'er in their t reatment  
of zero frequency nouns. In Equat ion (5), a 
noun B such that  f (A ,  Q, B) = 0 will l)e ignored 
(assigned a zero probal)ility) because it is un- 
likely that  such a noml will have a close relation- 
shii / with noun A. In Equation (6), on the other 
hand, a noun B such that  f (B ,  R, V) = 0 is as- 
signed a non-zero probability. These treatments 
reflect the asymmetrical  proper~y of inetonymy, 
i.e. ill a nletonylny of the form A 1{ 1~ an 
implicit term 13 will have a much t ighter rela- 
tionship with the explicit term A than with the 
predicate V. Consequently, a nouil \]3 such that 
f (A ,Q ,  B) >> 0 A f (B ,  JR, V) = 0 may be ap- 
propri~te as an interpretation of the metonymy. 
Therefore, a non-zero t)robat)ility should be as- 
sign(;d to Pr(l~., VI1X ) ev~,n it' I (B ,  2e, V) ; (). ~ 
Equation (7) is the probabil ity that  noun J3 
occurs as a member of (::lass C. This is reduced to 
fU~) if13 is not ambiguous, i.e. IC/a,~,sc.,s,(/3)\[ = f(c) 
1. If it is ambiguous, then f (B )  is distr ibuted 
equally to all classes in Classes(B).  
The frequency of class C is ol)tained simi- 
larly: 
.f(B) (8) 
. f (c )  = ~ ICl(-~c..~(13)1' 11C-.(7 
where 13 is a noun which belongs to the class C. 
Finally we derive 
f(13, ~, v) 
BqC 
(.0) 
In summary,  we use the measure M as de- 
fined in Equat ion (3), and cah:ulated by apply- 
ing Equat ion (4) to Equation (9), to rank nouns 
according to their apl)ropriateness as possible 
interpretat ions of a metonymy. 
Example  Given the statistics below, bottle we 
akeru (open) (open a bottle) will be interpreted 
5The use of Equation (6) takes into account a noun/3 
such that J'(l:~, l{, V) = 0. But, Stlch & llOtlll is usually ig- 
nored if there is another noun B' such that f(13', H., V) > 
0 be~,~,,se. Eo'~ct ....... U~)P, USIO)J'(C,~e.,V) << a < 
J'(lY, H,, V) will usually hokl. This means thai the co- 
occurrence 1)rol)al)iliW between implicit terms and verbs 
are also important in eliminating inapl)rol)riate nomls. 
887 
as described in the fbllowing t)aragraphs, assum- 
ing that cap and rcizSko (refl'igerator) are the 
candidate implicit terms. 
Statistics: 
f(bottlc, no, cap) = 1, 
f(bottlc, no, reizgko) = O, 
f(bottlc, no) = 2, 
f ( bottlc, ncar, cap) = 1, 
f (bottle, near, rciz6ko) = 2, 
f(bottlc, ncar) = 503, 
f(cap) = 478, 
f(rcizSko) = 1521, 
f(cap, wo, akcru) = 8, and 
f(rciz6ko, wo, akcru) = 23. 
f(bottlc, no, rciz6ko) = 0 indicates that bottle 
and rcizSko are not close semantic relatives of 
each other. This shows the effectiveness of us- 
ing A no B relation to filter out loosely related 
words. 
Measure: 
L,o(cap) 
Lncar(Cap) = 
Lno(reizSko) = 
Lncar ( reizS ko ) -~ 
f ( bott:le, no, cap) 
.f ( bottlc, no) 
\](ca,p, wo, a\]~c'ru) 
X 
1 8 
-8 .37?10 -3 , 
2 478 
f (bottle, near, cap) 
f(bottlc, near) 
f ( caI), "wo, a\]~cru) 
X 
.f ( ) 
1 8 
50--3 47-8 = 3.33 ? 10 -5, 
.f ( bottlc, no, rcizSko )
.f ( bottlc, no) 
f ( rcizako, wo, ahcru ) 
? 
.f ( rcizdko 
0 23 
2 1521 
.f ( bottlc, near, rcizSko) 
f (bottlc, near) 
f(rcizSko, wo, akcru) 
X 
f ( rciz~ko ) 
2 23 
503 1521 
- 6.01 x 1() -~,  
M(c p) 
= max{Lno(cap),Lnea.,.(cap)} 
= 8.37 x lO-3, and 
~r ( reizSko )
= 6.01? 10 -5 , 
where L,,o(Cap) = L,~o(Caplbo~tle, wo, akeru), 
M(c p) = M(c pl ot tz , and so o51. 
Since M > M we conclude 
that cap is a more appropriate imt)licit term 
than rcizSho. This conclusion agrees with our 
intuition. 
5 Exper iment  
5 .1  Mater ia l  
Metonymies  Seventy-five lnetonymies were 
used in an ext)erilnent to test tile prol)osed 
lnethod. Sixty-two of them were collected from 
literature oll cognitive linguistics (Yamanashi, 
1988; Yamam~shi, 1995) and psycholinguistics 
(Kusumi, 1995) in Japanese, paying attention 
so that the types of metonymy were sufficiently 
diverse. The remaining 13 metonymies were 
direct translations of the English metonymies 
listed in (Kalnei and Wakao, 1992). These 13 
metonylnies are shown in Table 2, along with 
the results of the experiment. 
Corpus  A corpus which consists of seven 
years of issues of the Mainichi Newspaper (Dora 
1991 to 1997) was used in the experiment. The 
sentences in tlle cortms were mort)hologically 
analyzed by ChaSen version 2.0b6 (Matsumoto 
et al, 1999). The corpus consists of about 153 
million words. 
Semant ic  Class A Japanese thesaurus, Bun- 
rui Goi-tty6 (The N~tional Language Research 
Institute, 1996), was used in the experiment. It 
has a six-layered hierarchy of abstractions and 
contains more than 55,000 nouns. A class was 
defined as a set of nouns which are classified in 
the same abstractions in the top three layers. 
The total nmnber of classes thus obtained was 
43. If a noun was not listed in the thesaurus, it 
was regarded as being in a class of its own. 
888 
5.2 Method 
'.1.11(; method we have dcseril)e,d was applied I;O 
the metonynfie, s (lescril)e,(t ill section 5.1. Tile 
1)r()eedure described 1)clew was followed in in- 
tert)rel;ing a metonynly. 
1. Given a mel,onymy of the, form :Noun A 
Case-Marker R Predicate, V', nouns re- 
\]al;e(l to A 1)y A 'n,o .1:1 relation an(l/or A 
near H relation were extra(:ix'~(l from 1;he, 
corl)us described in Se(:tion 5.\]. 
2. The exl;racted llOllllS @an(lidatcs) were 
ranked acc()rding t() the nw, asure M d(;tined 
in \]{quation (3). 
5.3 Resu l ts  
The r(;sult of at)l)lying the proi)osexl me, thod to 
our sol; of metol~ymies i  summarized in 'l'alfle 
1. A reasonably good result (:an 1)e s(;cn for 
q)oi;h r(,\]ai;ions', i.e. l;he result ot)i;aincd \])y us- 
ing both A no 11 an(t d ncm" 1\] l'elal;ion~; wllen 
extracting nouus fl'onl th(' cOllmS, \[1'1~(', a(:(:u- 
ra(:y of q)ol;h re, l~tions', the ratio ()f lhe nllnil)er 
of (:orrc(:l;ly intcrl)r(;te,(1 ; t()l)-rank(;(l (:an(li(lates 
to l;he, total mmfl)er of m(',l;()nymies in ()it\]' set, 
w,,s 0.7:, (=5' ,V isa+22))  alld ('ol,ti(t(' l,ce 
inWwva.1 estimal;e was t)(;l;ween ().6\] an(t 0.8\].. 
\?e regard this result as quite t)ronfising. 
Since the mc, i;onymies we used wcr(; g(m(u'a\]: 
(lomain-in(lel)(',ndca~t, on(s, l;h(~ (legr(', ~, ()f a(:cu- 
racy achi(;ve, l in this (~xp(;rim(;nt i~; likely t() t)(; 
r(',t)(',al;e(l when our me?hod is ~q)l)lie(l t() oth(;r 
genural sets ()f mel;onymies. 
'.\['~l)l(; l : tt3xl)erimental r('sults. 
I{,elal;ions used Corre(;t \?'rong 
Both relations 53 22 
Only A 'no B 50 25 
Only A near  13 d3 32 
Tal)le 1 also shows that  'both relations' is 
more ae(:ural;e than (',il;her the result obtained 
1)y solely using the A no \]3 relation or the A 
near  B relation. The use of multit)le relations 
in mel, onyn~y int(;rl)retation is I;hus seen to l)e 
1)enefieial. 
aThe correct;hess was judged by the authors. A candi- 
dat(; was judged correct when it; made sense in .Ial)anese. 
For examl)le, we rcgard(;d bet:r, cola, all(l mizu (W;d;el') 
as all (:orr(!c\[; intcrl)r(~l;ations R)r glas.s we nom, u (drink) 
(drink a glass) because lhey llla(le ,q(~llSC in some (:ontcxt. 
Table 2 shows the, results of applying the 
method to the, thirteen directly translated 
metonymies dcscril)ed in sect;ion 5.1.. Aster- 
isks (*) in the tirst (;ohlillll indicate that  direct 
translation of the sentences result in unaccel)t- 
able Japanes(;. The, C's and W's in t;he sec- 
ond eohmm respectively indicate that  the top- 
ranked ('andi(latcs were correct and wrong. The 
s(;nten(:es in the l;hir(t column are the original 
English metonymi(;s adol)tc, d fl'om (Kamci and 
\?akao, t992). The Japanese llletollylllies in 
th(: form h loun  ease-lnarker predi(:ate 7', in the 
fourth column, are the illputs I;o the method. 
In this ('ohunn, we and  9 a mainly r(;present 
I;he ac(:usal;ive-casc and nominative-ease, re- 
Sl)ectively. The nouns listed in the last eolmnn 
m'e the tot) three candidates, in order, according 
to the. measure M that was defined ill Equation 
(3). 
Th(,,se, l'csull;s ( lemonstrate the et\[~(:tiveness of 
lhe m(',thod. '.l>n out of t;11(: 13 m(;tonynfies 
w(u'c intc, rt)rete,(l (:orre, ctly. Moreover, if we 
rcsl;ri(:t our al;l;(',nti()n to the ten nietonylHics 
i}mt m'e a(:(:Cl)tal)le, ill ,/al)anese, all l)ut one 
w(;rc, inl;('rl)r(;te(t (:orrectly. The a(:curacy was 
0.9 ---- (/)/\]0), higher than that  for q)oth rela- 
tions' in Tal)le i. The reason fi)r the higher de- 
gl'ee of ac(:tlra(;y is l;\]lal; the lll(;|;Ollyllli(;s in Tal)le 
2 arc semi,what yi)ical and relativ(;ly easy to 
int(~rl)rel; , while, the lnel;(nlynlics (:olle(:l;c(t fl'()m 
,lal)anese sour(:es included a (liversity of l;yl)es 
and wcr(~ more difficult to intext)let. 
Finally, 1;11(', efl'ecl;iv(umss of using scnlanl;i(: 
classes is discussed. The, l;op candidates ot! six 
out of the 75 metonynfies were assigned their 
al)prot)riatenc, ss by using their semantic lasses, 
i.e. the wducs of 1;11o measure 114 was calculated 
with f (H , /~ ,  V) = 0 in lgquat;ion (6). Of the, se, 
l;hrce were corrccl,. 011 l;hc, other hand, if sc- 
manl;ic class is not use(l, then three of the six 
are still COITeC|;. Here there was no lint)rove- 
merit. However, when we surveyed the results 
of the whole experiment, wc found that  nouns 
for wlfich .f iB, R,, V) -- 0 often lind (:lose re- 
lationship with exl)licit terms ill m(;tonynfics 
and were al)propriate as interpretat ions of the 
metonynfics. We need more research betbre we 
(:an ju(lgc the etl'ectivc, ness of utilizing semantic 
classes. 
rPl'edicatcs are lemmatized. 
889 
Table 2: Results of applying the proposed lnethod to direct translat ions of the metonymies in
(Kanmi and Wakao, 1992). 
Sentences Noun Case-Mm'l~er Pred. Candidates 
C Dave drank the glasses. 
C The .kettle is boiling. 
C Ile bought a Ford. 
C lie has got a Pieasso in his room. 
C Atom read Stcinbeck. 
C 
C 
W 
C 
W 
C 
Ted played J3ach. 
Ite read Mao. 
We need a couple of strong bodies 
tbr our team. 
There a r___q a lot of good heads in the 
university. 
Exxon has raised its price again. 
glass we nomu 
yakan ga waku 
Ford we kau 
Picasso we motu 
Stcinbcck we yomu 
Bach we hiku 
Mao we yomu 
karada ga hituy5 
atama ga iru 
Exxon 9 a agcru 
Washington is insensitive to the 
needs of the people. 
Washington ga musinkci 
C The T.V. said it was very crowded 
at; the festival. 
W The sign said fishing was prohibited 
here .  
T. V. 9a in 
hy&siki ga iu 
beer, cola, mizu (water) 
yu (hot water), 
oyu (hot water), 
nett5 (boiling water) 
zy@Ssya (car), best seller, 
kuruma (vehicle) 
c (painting), image, aizin (love,') 
gensaku (original work), 
mcisaku (fmnous tory), 
daihySsaku (important work) 
mcnuetto (minuet), kyoku (music), 
piano 
si (poem), tyosyo (writings), 
tyosaku (writings) 
carc, ky~tsoku (rest;), 
kaigo (nursing) 
hire (person),tomodati (friend), 
bySnin (sick person) 
Nihon ( Japan) ,ziko (accident), 
kigy5 (company) 
zikanho (assistant vice-minister), 
scikai (political world), 
9ikai (Congress) 
cotn l l lentgto l '~ anl lOl l l lcer  I (:~stel" 
mawari (surrmmding), 
zugara (design) 
.seibi (lnaintclmnce) 
6 Discuss ion  
Semant ic  Re la t ion  The method proposed in 
this pnper identifies implicit terms fbr tile ex- 
plicit term in a metonymy. However, it is not 
concerned with the semantic relation between 
an explicit; term and implicit term, because such 
semantic relations are not directly expressed ill 
corpora, i.e. noun phrases of the form A no 
B can be found in corpora bul; their senmntic 
relations are not. If we need such semantic re- 
lations, we must semantical ly analyze the noun 
phrases (Kurohashi and Sakai, 1999). 
App l i cab i l i ty  to  o ther  languages  Japan- 
ese noun phrases of the form A no B are specitie 
to Japanese. The proposed method, however, 
could easily be extended to other languages. For 
exmnple, in English, noun phrases B of d could 
be used to extract semantical ly related nouns. 
Nouns related by is-a relations or par t -o f  re- 
lations could also be extracted from corpora 
(Hearst, 1992; Berland and Charniak, 1999). If 
such semantical ly related nouns are extracted, 
then they can be ranked according to the mea- 
sure M defined in Equat ion (3). 
Lex ica l ly  based  approaches  Generative 
Lexicon theory (Pustejovsky, 1995) proposed 
the qualia structure which encodes emantic re- 
lations among words explicitly. It is useflfl to 
infer an implicit term of the explicit term in 
a metonymy. The proposed approach, on the 
other hand, uses corpora to infer implicit terms 
and thus sidesteps the construction of qualia 
structure. 8 
7 Conc lus ion  
This paper discussed a statistical approach to 
the interpretat ion of metonymy. The method 
tbllows the procedure described below to inter- 
pret a metonymy in Japanese: 
1. Given a metonymy of the tbrm 'Noun A 
SBriscoe t al. (1990) discusses the use o1" machine- 
readable dictionaries and corpora for acquMng lexical 
semantic information. 
890 
Case-Marker 1{ Predicate V', nouns that 
are syntactically related to the explicit 
terlll A are extracted front a corpus. 
'.2. The extracted nouns are ranked according 
to their degree of appropriateness as inter- 
pretations of the metonymy by applying a 
statistical measure. 
The method has been tested experimentally. 
Fifty-three out of seventy-five metonymies were 
correctly interpreted. This is quite a prolnis- 
ing first; step towm'd the statistical processing 
of metonymy. 
References  
Matthew Berland and Eugene Charniak. 1999. 
Finding parts in very large corpora. In A (7L- 
99, pages 57- 64. 
Jacques Bouaud, Bruno Bachimont, and Pierre 
Zwcigenbaum. 1996. Processing nletonyllly: 
a domain-model heuristic graph travcrsal 3t> 
preach. In COLINC-95, pages 137-142. 
Ted Briscoc, Ann Copestake, and Bran Bogu- 
racy. 1990. Enjoy the paper: L(;xi(:al seman- 
tics via lexicology. In COLING-90, pages 4:2-- 
4:7. 
I)(fi;cr F. l~rown, gincenl; ,l. Delia Pietra, Pe- 
ter V. deSouza, ,\]enifer C. \]~ai, m~d l/.ol)(',rl; I,. 
Mercer. 1992. Class-1)ased n-gram models of 
m~l;ur~l lmlguage. C~o'm,p'u, tat  ioruzl Li'n, guistics, 
1.8(4) :467 479. 
Kelmeth Ward Church and Patrick Hanks. 
1990. Word association orms, mutual in- 
formation, and lexicography. Uomputatio'n, al 
Lin.quistics, 16(1):22 29. 
Dan Fass. 1988. Metonymy and lnel;al)hor: 
What's the difference? In COLING-88, 
pages \]77-181. 
Dan Fass. 1997. Processin9 Mctonymy and 
Me.taph, or, volume 1 of Cont, cm.porar'y Studies 
in Cognitive Science and '\]'cch, nology. Ablcx 
Publishing Corporation. 
Steplmne Fcrrari. 1996. Using textual clues 
to improve metaphor processing. In ACL-95, 
pages 351-354. 
Marl;i A. Hearst. 1992. Automatic acquisition 
of hyponyms fi:om large text corpora. In 
COLING-92, pages 539 545. 
Eric iverson mid Stephen Helmreich. 1992. 
Metallel: An integrated approach to non- 
literal phrase interpretation. Computational 
Intelligence, 8(3):477 493. 
Shin-ichiro I(amei and Takahiro Wakao. 1992. 
Metonymy: Itcassessment, survey of accept- 
ability, and its treatment in a machine trans- 
lation system. In ACL-92, pages 309-311. 
Sadao Kurohashi and Yasuyuki Sakai. 1999. 
Semantic mmlysis of ,Japmmse noun phrases: 
A new approach to dictionary-lmsed under- 
standing. In ACL-99, pages 481 488. 
Takashi Kusumi. 1995. ttiyu-no S'yori-Katci- 
t;o lmi-Kdzfi (Pr'occssin 9 and Semantic Struc- 
ture of "\]'ropes). Kazama Pul)lisher. (in 
Jalmnese). 
George Lakoff and Mm'k Johnson. 1980. 
Meta, phors lye Live By. Chicago University 
Press. 
Christopher D. Mmming and Hinrich Schiitze, 
1999. Fou'ndations of Statistical Nat.ur(d Lan- 
guage \])recessing, chapter 6. The MIT Press. 
Yuji Matsmnoto, Akira Kitauchi, Tatsuo 
Yamashita, and Yoshitalm Hirano. 1999. 
Japanese morphological anMysis system 
ChaScn mmmal. Nara Institute of Science 
and Technology. 
Masaki Murata, Hitoshi Isalmra, and Makoto 
Nagao. 1999. IX.csolut, ion of indirect anal)hera 
in Jal)anese s(;ntcn('es using examples "X no 
Y (X of Y)". In A 6%'99 Work.shop orl, Core/" 
e.'l'(:,ncc and It.s AppIica, tio'ns, 1)ages 31 38. 
,lames l'ustejovsky. 1995. 2Yt, c Generative Lex- 
icon. 'J?he MI'I' Press. 
Tim National Language I/.ese~rch lalstitute. 
1996. Bv, nr',ui Goi-hyO Z~h,o-bav,(Th:l;o'nom, y 
of ,lapo, nc.s'e., e'nla'ulcd cditio@. (in ,Japancse). 
Atsmnu Yammnoto. Masaki Murata, and 
Makoto Nagao. 1998. Example-based 
metonymy interpretation. In \])'roe. of the 
~t,h. Annual \]lgcel;in 9 of th, c Association for 
Natural Language Prwccssing, pages 606 609. 
(in Japanese). 
Masa-aki Yamanashi. 1988. Hiyu-to \]~ikai 
('1;ropes and Understanding). Tokyo Univer- 
sity Publisher. (in Jalmnese ).
Masa-aki Yamalmshi. 1995. Ninti Bunpa-ron 
(Cognitive Linguistics). Hitsuji Publisher. 
(ill Japanese). 
891 
Multi-Topic Mult i -Document Summarization 
UTIYAMA Masao 
Communicat ions  Research Laboratory  
588-2, Iwaoka, Nishi-ku, Kobe, 
Hyogo 651-2492, Japan 
mutiyama@erl .go. jp 
HASIDA K6 i t i  
E lectrotechnical  Laboratory 
1-1-4, Umezono, Tukuba, 
Ibaraki 305-8568, Japan 
hasida@etl.go.jp 
Abst ract  
Summarization of multiple documents featur- 
ing multiple topics is discussed. The exam- 
ple trea.ted here consists of fifty articles about 
the Peru hostage incident tbr \])ecember 1996 
through April 1997. They include a. lot of top- 
ics such as opening, negotiation, ending, and 
so on. The method proposed in this paper is 
based on spreading activation over documents 
syntactically and semantically annotated with 
GI)A (Global l)ocument Annotation) tags. The 
method extracts important documents aald im- 
portant parts therein, and creates a network 
consisting of important entities and relations 
among them. It also identifies cross-document 
coreferences to replace expressions with more 
concrete ones. The method is essentially multi~ 
lingua\] due to the language-independence of the 
GDA tagset. This tagset can provide a stan- 
dard fornm.t br the study on the transfbrmation 
and/or generation stage of summarization pro- 
cess, among other natural language processing 
tasks. 
1 I n t roduct ion  
A large ('.vent consists of a, number of smaller 
events. These component events are usually 
related trot such relations may not be strong 
enough to define larger topics. For example, a 
war may consist of opening, battles, negotia- 
tions, and so on. These relatively independent 
events are considered to be topics by themselves 
and would accordingly be reported in multiple 
news re'titles. 
Summarization of such a large event, or mul- 
tiple documents about multiple topics, is the 
concern of this paper. Summarization of multi- 
ple documents containing nmltiple topics is an 
unexplored research issue. Some previous tud- 
ies on summarization (McKeown and Radev, 
1995; Barzilay et al, 1999; Mani and Bloedorn, 
1999) deal with multiple docmnents about a sin- 
gle topic, but not about multiple topics 1. 
In order to smnmarize lnultiple docmne, nts 
with multiple topics, one needs a general, 
semantics-oriented method for evaluating im- 
portance. Summarization of a single document 
may largely exploit the doculnent structure. As 
an extreme example, the first paragraph of a 
newspaper article often serves as a smmnary of 
the entire article. On the other hand, summa.- 
rization of multiple, documents in general must 
be more based on their semantic structures, be- 
cause the, re is no overall consistent document 
structure across them. 
Selection of multiple important topics (not 
keywords) tbr nmltiple-topic summarization has 
not; yet been really addressed in the previ- 
ous literatm:e. The present paper proposes a 
method, based on spreading a.ctivation, for ex- 
tracting important opics and important docu- 
ments. Another method proposed which is use- 
fifl for grasping the overview of nlultiple docu- 
ments is visualization of important entities men- 
tioned and relationships among them. Visu- 
alization of relationships among keywords has 
been studied in the context of information re- 
trieval (Niwa et al, 1997; Sanderson and Croft, 
\] 999), but to the authors' knowledge the present 
study is the first to address uch visualization in 
the context of summm'ization. Of conrse a. con- 
cise summary of the entire set of multiple docu- 
lnents can be obtained by recovering sentences 
from important entities and their relationships 
~s demonstrated in section 3.3. 
The present study assumes documents anno- 
tated with GDA (Global Document Annota- 
1Maybury (1999) discusses smnmarlzation f multiple 
topics, but in his study the smmnaries are made ffonl an 
event database lint not fl'om documents. 
892 
tion) Lags (Itasida, 1997; Nagao and llasida, 
1!)98). Since the GI)A tagset is designed to be 
inclel)endent of any particular natural language, 
the proposed method is essent, ially multilingual. 
Another merit of using annotate, d documents is 
that we ca.n separate the a,nalysis phase from 
the whole process of summarization so that we 
ca,n locus on the latter, generation t)hase of sum- 
ma.rization process. Annotated documents can 
also be useflfl for a common input format for 
the study of summarization, among other nat- 
ural language processing tasks. 
2 The  GDA Tagset  
GI)A is a project to make on-line documents 
ntachinc-ullderstanda.ble on the basis of a lin- 
guistic ta.gset, while developing and si)read- 
ing technologies of content-based presentation, 
retrieval, question-answering, smnma.rization, 
translation, among othe, rs, with much higher 
quality than before. GI)A thus proposes an 
integrated global plattbrm for e,h',ctronic on- 
tent authoring, t)resental;ion, a,nd reuse. The 
GI)A tagset 2 is an XM1, (eXtensible Markup 
l,anguage) insta,nce which allows ma.chines to 
automatically infex the semantic and pra.gma.tic 
structures uncle, flying the raw (locuments. 
Under the current sta.te of the art, GI)A- 
tagging is senfiautomatic and calls for manual 
correction by human mmotators; othe, rwise an- 
notation would ma,ke no sense. "l~h( ,, cost in- 
volved here pays, because annota,ted ocuments 
are generic information contents from which to 
rend(',r diverse types of 1)resenta.tions, poi;en- 
tially involving summariza.tion, arra,tion, visu- 
alization, translation, information retriewfl, in- 
formation extra.ction, and so forth. The present 
p~,per concerns summarization only, trot the 
merit of GI)A-tagging is not a,t all restricted to 
smmnarization, and that is why it is considered 
reasonable to assume Gl)A-tagged input here. 
2.1. Syntact ic structure 
An example of a. Ol)A-tagged sentence is shown 
in Figure 1. <su> means sentential unit. <np>, 
<v>, and <adp> stand for noun t)hrase, verb, 
and adnominal or adverbial phrase. 
<su> and the tags whose name end with 'p' 
(such as <adp> and <vp>) a,re called phrnsal 
ta.qs. In a sentence, an (;lement (a text Sl)an 
2http ://www. etl. go. j p/etl/nl/gDk/tagset, html 
<su> 
<np>Time</np> 
<v>f l ies</v> 
<adp> 
l ike 
<np>an ar row</np> 
</adp> 
</su> 
Figure 1: A Gl)A-tagged sentence. 
from a begin tag to the corresponding end tag) 
is usually a syntactic onstituent. The elements 
enclosed in phrasal tags are called ph,~asal ele- 
ments, which cannot be the head of larger ele- 
ments. So in Figure 1. 'flies' is specified to be 
the hea.d of the <su> element and qike' the head 
of the <adp> element. 
2.2 Coreferences and Anaphora 
Each element ma.y have an identifier as the va.lue 
for l;he id attrit)ute,. Corefe, rences, including 
identity ana.t)hora , are annotated by the eq at- 
tribute, as follows: 
<np id="j0">John</np> beats 
<adp eq="j0">his</adp> dog. 
When the shared sc, nm.ntic content is not the 
rctb, renl; lint the typ(', (kind, se, t, etc.) of the 
retb, rents, the eq.ab attribute is used like the 
following: 
You bought a <np id="cl">car</np>. 
3 bought <np eq. ab="cl">one</np>, 
too .  
A zero anaphora is encoded as follows: 
Tom visited <np id="ml">Mary</np>. 
He had <v iob="ml">brought</v> a 
present. 
iob="ml" means that the indirect object of 
brough, t is elemenl~ whose id value is ml, that 
is, Mary. 
Other relations, such as sub and sup, can also 
be encoded, sub represents subset, t)art, or ele- 
ment. An example follows: 
She has <np id="bl">many 
books</np>. 
<namep sub =''b i "> c c AI i ce ~ s 
893 
Adventures in Wonderland' '</namep> 
is her favorite. 
sup is the inverse of sub, i.e., ineluder of any 
sort, which is superset a.s to subset, whole as to 
part, or set as to element. 
Syntactic structures and corefc, rences are es- 
sential for the summarization method described 
in section 3. l?urther details such as semantics, 
coordination, scoping, illocutionary act, and so 
on, are omitted here. 
3 Mu l t i -Document  Summar izat ion  
3.1 Spreading activation 
A set of GDA-tagged documents is regarded as 
a network in which nodes roughly correspond to 
GI)A elements and links represent he syntac- 
tic and semantic relations among them. This 
network is the tree of GI)A elements plus cross- 
reference (via eq, eq.ab,  sub, sup, and so on) 
links among them. Cross-reference \]inks nlay 
encompass different documents. Figure 2 shows 
a schematic, graphical representation f the net- 
work. 
document  
su IM iv i s ion  or  
paragrat :h  " " " 
sentence  " " " 
- -  s~, t~tact  i c  l i nk  
suhser~te~l t ia l  - ? ? 
e l?he l  l l t s  -- - - -  c ross  te  e ronce  l J l l k  
Figure 2: Multi-document network. 
Spreading activation is carried out in this 
network to assess the importance of the ele- 
ments. Spreading activation has been applied 
to summarization of single GDA-tagged docu- 
ments (Hasida et al, 1987; Naga.o and Hasida, 
1998). The main conjecture of the present study 
is that the merit of spreading activation in that 
it evaluates importances of semantic entities 
is greater in summarization of multiple docu- 
ments with multiple topics, because smnmariza- 
tion techniques using docnment structures do 
not; apply here, as mentioned em:lier. 
To fit the semantic interpretation, activations 
spread under the condition that coreferent ele- 
ments should have the same activation vahm. 
The algorithm a is shown in Figure 3. Iiere the 
external input c(i) to node i represents a pri- 
ori importance of i, which is set on an empir- 
ical basis; for instance, an entity 4 referred to 
in the title of an article tend to be hnportant, 
and thus c(i) should be relatively large for the 
corresponding node i. Tile weight w(i, j) of an- 
other kind of link Dora node i to node j may 
also be set empMcally, but it is fixed to a uni- 
tbrm value in tile present work. Let E(i) be tile 
equivalence class of node i, that is the set of 
nodes which are coreferent with i (linked with i 
via eq relationships). Condition 
E E\] ,,,(k,0)_< 1
~eU(i) .iCE(i) 
should be satisfied in order for the spreading ac- 
t ivat ion to converge. This  condit ion is satisfied 
if we treat each equivalence class of nodes as a 
virtual node while setting the weights of other 
types of links to be 1/5, where D is the maximum 
degree of equivalence classes: 
O=max ~ ~ @'i 
i ~eE(,i) .iCE(.i) 
where 5&~ is \] if there is a link between ode k 
and node j ,  otherwise it is 0. 
The score score(i) of node i is calculated by 
summing the activation wdues of all the nodes 
m~dcr node i in the syntactic tree strueture: 
.~.'o,'~'.(i) =a( i )+  ~ .~co,'4j ) (l) 
jCch(i) 
where a(i) is the activation vahle of node i and 
oh(i) is the set of child nodes of node i. oh(i) is 
empty if node i is a leaf node, or a word. This 
score is regarded as the importance of node i. 
3.2 Extraction of important documents 
and sentences 
Extraction of i lnportant documents is simple 
once the scores of the nodes in the network are 
obtained. Sorting the document nodes accord- 
ing to their scores and extracting higher-rm~ked 
ones is sufticient for the purpose. 
aAnother spreading activation algorithm is discussed 
by Mani aim Bloedorn (1999). The comparison is a fu- 
ture work. 
awe use tim terms 'entity'~ 'node', aim 'clenmnt' in- 
terclmngeably. 
894 
Variables: 
N: munlier of nodes. 
D: nmxinnnn oul;-degree of equivalence classes, 
c( i ) :  external input I;o node i. 
w(i, j): weight of the link from node i to node j: 
0 if not eonncel;ed, 
1 if connected via eq, 
1/D ot;herwise. 
a(i): actival;ion value of node i. The initial value 
is O. a(i) is the sum of all a.(j,i). 
a(?, j): acl;ivaLion value of l, he link from node i
1;o node j. The inil;ia\] value is 0. 
Algoril;hm: 
repeat { 
for(i=O; i<N; i++){ 
av = c(i) ; 
for(j=O; j<N; j++){ 
a( j , i )  = w( j , i ) * (a ( j )  - a ( i , j ) )  
av += a(j, i) 
} 
a(i) = av; 
} 
} unti l  convergence. 
Figure 3: Spreading activation a.lgorithm. 
Simila.r procedure is used to extract impe l  
Laid; sentences from mi importa.nl; docmnent. 
Extra.cLod sentences aJ'e pruned according to 
their syntactic structures. Ana.phoric expres- 
sions such a.s h.e, or she are substituted \])y their 
a.ld;ecedents if neeessa.ry. 
An experiment ha,s been conducted to test the 
effeetiwmess of the proposed aJgorithm. The ex- 
a.mple set contains fifty Japanese articles about 
the Peru hostage incident which continued over 
four months fl'om I)ecember 1996 to April \] 997. 
They include a lot of topics such a.s opening, 
negotiation, settlement, mid so on. The GI)A- 
tagging of these articles has involved automatic 
morphological na.lysis by JUMAN (Kurohashi 
mid Na.ga.o, 1998), automatic syntactic ana.lysis 
by KNP (Kurohashi, 1998), and ma.nual anno- 
tation encompassing morphology, syntax, coref- 
erence, and anaphora.. The types of a.naphora, 
identified here are ma.inly pla.in coreference and 
zero ana.phora. Cross-document coreferenees 
among entities ha.ve been a.utomatieaJly identi- 
fled by exa,et string mt~tching. 5 They oonta,ined 
erl'OlTS but those el'l;ors %7e17o llOt corl'eCl;(',d for 
the experiment. Cross-document coreferenees 
found were 'l'eru'(49), 'Japa.n'(39), 'Peru Pres- 
ident' (15), 'members of Tupac Amaru'(9), ... 
and so on, where the mmfi~ors indicate the num- 
bers of documents which contain these expres- 
sions. 
The externa.1 inputs to nodes have been de- 
fined a.ccording to the corresponding nodes: 
c(i) = 10 if node i's antecedent domina.tes sen- 
fences (e.g., a. node eoreferring with a. pa.ra.- 
graph). This sets a, preference for nodes which 
summa.rize preceding sentences, c(i) = 5 if node 
i is in the title of an article, beemlse a. title is 
usually importa.nt. Otherwise c(i) = 1. These 
crude t)a,raJneter va.hles have been set by the au- 
thors on the basis of the investigation of sum- 
ma.riza.tions of va.rious documents. 
Two importa,nt topics, the opening (first a.l;- 
tack by %lpac Ama.ru) a.nd the settlement (a.t- 
tack by the Peruvia.u government comma.n- 
(los), have been extracted fl'om the four highest 
rmiked articles, even though temporal informa.- 
tion has not been incort)or~tted in the aJgorithm. 
Tile opening aa'ticle, is the first a.rticle of the 
sample document set. However, the settlement 
a ri, ich; is the sixth \]a.st one. So mere extra.ction 
of the last m'tic, le would miss the settlement. 
The 25% sunuim.ries of the two a.rticles made 
by extra.cting a.nd priming sentences are shown 
below together with their English trmisla,tions: 
H$,  4 )b -09~{,~D~AS#t lc  a 61~ 
Armed guerrillas broke inl;o a party al; 
Japanese ambassador's re,sidenco. Gunshots. 
200 hold in hostage. Peru. 
Many people from Japmlese and Peruvian 
sides were held in hostage. The arnled group 
consisl;s of about twenl;y people, several of 
which broke into the anlbassador's esideime. 
IL is reported thai; there are interinitl;enl; 
shool;ings now. 
a.nd 
'~\?e are planning to incorporate r cmlt results (Bagga 
and Baldwin, 1998) to iden|;i\[y cross-docunmnt corefei'- 
elites. 
895 
H $:L~.~!.~II,~i}R!:\] r-{q:c0 f\]~:~-'O- :d>-::: o U K ,, 7 
{~!l!j~m l:q:igff~J:,l: ~ +i7~-:-, '~. ~ ~ m m P_ ~ l~lS/~f,j d\]-.<, 
.\]al)an(~sc, ~ui\]l)assa, dol"~; I;(~,si(t(HI(',C \[)o:3,:\](~,~;si()ll 
incidonl, in lknm. All lio:d,ag(;,~ r(~lea.<;e(I. Aim 
al; r(!cov01'ill~ lii,<~ pow0,r I)a:ds I>r(,,sidcid; l,'li- 
.i \]IlIOFio 
lq'cMdenl, Fujinlori  demon.%ral;cd hhnscl\[ as 
II HI;l 'Ollg polil;ician t)y r(;solvinl,; i}lt; ,lapau(~(! 
nlill);/s,<~adol'~s i'c.<dd(:ilCC, t)os,~es:don in(:id(;nL 
l ie  cill;(;ie(l l;h(; resi(h:n(',e <di;e. 'l'hi,~ vi,<;il; Ix) 
I;h(; re,<-;i(Icnc(~ ill\]t)l'(::;sc(\] l;\]l;ll; \]:(; wa:.; \]eadhq{ 
l;he ol)(;ral;ion hin~s(!ll'. Why did he choo,<~c i,o 
I'CHI_)I'\[ l;(J } i l ; l l lS? \'V(~ C; l l l  S~t\[)' Lhal ,  \]io n, hnc, d 
;ql; I'O, OOVel"l l l l '  ~ his polil,ical I(:a(l(:r:di\]il hy ro- 
,~;()\]Villg~ |,hlOllp;h l i l i \ ] i lAi ly \])O\V(W l;h(; l'(?:;id(',llC(~ 
inci(\]clil;, which is al, i, he rool; of  l;h(' po\]i l; icn\] 
crisi.<~. 
3.3  \[E:u.t;i ty - r ( : la / ; ion  graph  
'l'h(; ,q(:ore, s(:oY<.;(i,j) of  a r(;la,i;ioii I)e,l;we(:u i, wo 
cnl;ii;ies i a,lid j ,  i:4 de, t ined by: 
j) 
:: I ,":( ' i) l . .( ' , .) i  IS':(;)l<-.,(.7) 
\ . 
t / J 
>,.~ ,S'( S';( i) )f~,S'( l , ;( j)  ) 
wt,(3re , ' ; ( l ' ; ( i ) )  is l;I,(, <;el; o f  s(,,,lx.,,c,; ,,odc,<~ whic i ,  
( lomiiia,lx; olie, o1: l;h(; l ,odes hi lO(7) mid I S',:(":)I 
, . . , ,1 ,o , .  or  :,, z,:(.s.), z,:(.,:): ,.,(?:): 
and score(s) ha.v,p b('on d,c:iined in ,S(;ci;hm 3. I. 
I ,:,,ii (,r <zd >." idJ':, wii ic l l  is 
~1, 111(;;/,SI11"(; Ot: l, OFill i int )or l ,  a,nce wid(;ly ll,qe(\] \]11 
in forma.l;ion r(;tl:ieva,l. 
If s(:o'#'e(i;j) i,~; sntti(-i(;ntly la,l:,~V; , IJl(m 
.<;'(so(i)) n ,s'(s,;(:/)) , : , , , ,mi . i , ,e  
1)olJi l;h(; elli;il;i(;s) caal consisil;uC(; a. cross- 
t./O(:lliilOlli: gUlr l l i la, l 'y C.Ol\](:(;rilill~ i aai(l j 6  
An cil l J /;y-rdatioll gra,ph (E-I{ gra,1)h) is mad(; 
of Llie r(;l~tl;ions h igh ly  ra,nked in t(;rnis of l;h(; 
score defined hi (2). Figur(; ,\] sliow,~ the E - I{  
gra,1)h ilia,d(; o \ [  l;he, top  (;\](w011 lela,t, iOllS oxl;ra,cLe(l 
frol i i  l;h(; a.rlJiclo,~; a,bout Peru lio,sl;a,l~(; \]ncid(;lll;. 
'\]'h(; llllrlll)(;l'S llO,q,l" 1;h0 lille,'-; l:(;\])l'O,,qOlll, |;lie, l'a,ll\](S 
of the r(',la.i;ions. 
6COl:eferoliC(; chains are used 1;o slinll i larizo single doc-- 
liilleiil;s 1)3; Azza l l i  el; a\]. (\] 99.()). 
\ ])C?'U ho:;t:<-tLre \]n<J c-{(!Ilt: 
tt \] )(1 i:'11V:: {lll 
P?-~Z U {ll\[/b\[l~;*<scido\]" # k', 
()1):3 (xr. V(XIT~ 
t;'u:i ?:::or\] 'J'tll)F'tf: Alhct?'ll 
?.I) I of  l~(;ru no~-;l,a.g(} iii(:i(l(,,ili,. 
T l i{ ;  l;ot)-la, i i ked l:cla,i;ioH wa,,~; i;h(; one  t)(;. 
I;w(,,e.ll /)(:?'7t a,n(\] +\](I/f)(I,?I,(',,';(D (hTll,\[J(l,,?,q(l,(\[,Ol",q 7'(.~,q7 
U(;u(:('. 'l'}wee f;(',III;I:HICO, S (;KIXa,(:t;(X\] fl '()ll l i;lie ei~,;hl; 
,{;01ii;(;lic(',~; wh ich  co~il;a,hl(;d I)ofiti of  t;he (;nl;il;:i(;,<~ 
were  as fo l low:< Y 'l'}le, y wet(; i is/ ;ed i l l  chl?() i /o 
\]o<9~ica.\] ()v(leY which wa.s i(le\]~l;iN(;d I)y i;il(; (la.i;c 
i l~l'oi:lna, IJoi~ iti d ie a,li,icl(;,~. 
1. ,-.t)> -;so., ~ ?),~@tl._ x: t'._, <'-:. i:Tfill 9 "..dTl.:<h~, ! l.'b:A: 
I::.~j~!{",,!:;~ a . 1 i .'i~. ,<'.;I.,: :~;)l, i , i!,. i l~f~:,~DV\!('.{::: 
,-'-t g a~, 2: ,, 
A,::coMin~,, i;o rv, porl.~ from Ik~ru, on l;h(; :17t;h Lh(! 
,J;+lp~lll(;~q(} HI I I I )} ' I~g~I{\]OF~; I (}+qi(l(~l\](;I.~ i l l  I , \ ] l iH i !  {;1l(~ C~II)i  
lal, w~L.'; al.l;acked I)y o~ nmn,od r,~J'otlp~<;, allo/,,e,.lly 
hd'l,i::;I; {~ll{~l-ril\]alf;, ,~iil(\] i117i113 ~ p(X)l)\](; \[FOlll })ol;ll 
,Japali():;(,, ~lll(\] l~(H' l l \ ' ia i l  ~ddc,~ WClC held i l l  ho:d;H~'>c. 
2. ,'{)b, -7) t I ~?k,f~ltll'd~2Vl.:l, 7< :7t7~-4:" U :.7 i,: <t: ~s 
~f'Pli:.f'l= e.,~a:j~i~i: is H. ,'~;l,--}~;~:;i::#li ,A'.,'~>)'7:'<7 
~(d~JT~;~iT~'ig-<,< ~. P-i:A:. ~i4Jfi"~'o)JJ~ii4gI~s,~l,i.h)l< 
(3Oll('.(!l'lliill~" | ;h i ;  i l os l , ; i l , ; l ;  i nc idm~l ;  i l l ;  I ; lm 
J apaneso  aii3bassadol?.~; i-e<~;ltl.c;nc{; c, atls(~.d \]~y 
armed F;lmrloilla, mi i ,  he \]8/;h Lhe f ' ;OV(Wli i i l(! l l l  
reqil0slJcd t;h0 I)crllvian ?jOV(}I'III|I(}\]Ii; l;O IIH~;III'(~ l:\]m 
,,mfel;.y of l;he hoed,agcs~ ~llld ,%(~,11\[ Mr. t I ( ) IUUT I  
Taka,hil~o, (;oordinat~or, I)ivi:don cd Middle alld 
fToill;h Am(wica . . - -  
LI~D, 5 t-". ~. l,_.;"~  \]?7,?5 9 .  
llre.<ddenl; ,'njiniori'.<; p,.)liliical aul:horil;y will ICCOV(W 
1)ccause lle mmceedod in IJic operal,ion to I)reak inlo 
l;\]le ,J:4pallO,~;o, atllba>,;Hntlor~.~; residence in Peru (ill 
the  22r id.  
Thcs(' ,scnt(;uce,% (;xlxa,clx;d \['rom dillS;rent a.i:- 
t;icles, ha,',;e be(;u I)a.ra,plwa.sed (m i;ho basis o\[  
7'F\]IOSO ~Olll;(}llC,(;,'-; \vo, ro  soloc\[ ,o( \ ]  I IHHl l l ,~l l ly t;o dO l l \ ]O l l -  
si;ral;e I;he possibi l i ty of cross docunmld; sulmnarizat; ion 
Imsed on o:)relcrence. 
896 
(:(,l~(;\[urellu(;,';. ~iin(:(: (,ll(,, il;ull(,, ()i i;h(.' Kuer i l la  
, , ; foul)  i,~; i~()l; i( l(ni l , i f ie(I  ill i,h(', I)(,,~,;illiiillj,; ()I: Hi(' 
ili(:i(h;ni,: i;h(; (;xt)r(;,~;,~;h)l) ~/'~t)}~QT"-" U ~ J ' i / ?  1");{ b,~O 
l(;l'i;i',;i; Ku( ; r r i lhu ; )  i~', u:;(',(I i)~ i,ll(,, \[in;i; :;(,.illX,ll(:(; 
(;h(;r('.. ' l ' l i i : ;  ('.:~l)r(;~;:;i()n hal; 1)('('.~ i('.l)ia(:('.(l \vi(,ll 
:Z :1~;~2 " U . ;  ( i "  ' J / \ '  ~/? 'Y': , 'ql~): (1( ,i < ;(, ~,;uu,ritlu;: 
('l',,~,:,,(: A,, . : , .  , ) )  I):y ,t,,;i,u,; <:r,>:;;; d(,,: . , , ,( , , , , i  ,:(),..r 
()F(;IICO;;. ~\]'\]1(', (;( luiva,hum(; ()f l,ll(; lit:;(, ',;(;lli,Q, ll(:(', 
a , .d  i;h(; tit,% hour i  !>lu-a,,'~(; uf  1;1~(' ~;(,,(:<;~l(I ,u('.~)(;(;~(:(;. 
' .? . )b-  u) I \[ >'i~ .'t~'7~6\i~i'( ' ' j '~zi: 1,/,:.i,;t{;;J,(i-f; + U ~., t,. ? i '  J %1.  +,  k? -~ l  7lt  I 
"-" 7,:) ;U, \]++ ,:}: ~,.)_A.!..(: ,'I~I - ((,Iw \]i(),%a,v,(: hi(:i(huil, (:;t,~!,~;(:(! l).y 
;I./Ju(;(i ?;u(;rr\] lhu; u,i, I;1~(, ,\]H,\[;,il,ll(;,<q( ~, ;tud;iu'>,,-;mh)r:,<; 
r,:~:li(h':li(:(, iu I ' ( ' ru ) :  ',.\, l , r( ,. 1,r(il)(~l'l.v ~1('(,(~(:1,(~(I ;I.il<l 
',',, /,<; V('l)lU.u('(i I)y ;l, ilOi;\[i(;i" (~X})i'(%>'-;i()li I)(~.<;I.tlH( ', i,1~('. 
(;(\[tiiv;t,/(;ii(:(~ ()1' (;V(;lli,:; ;i(q-()'H,<; 1)():;',',il)15 , difl '~u:eiil. 
(\]()('.l i l i l(;i it,'; (i\t(:l/.('.c)w~) (fl, .i.\].~ l{){)!J: l gu.i.,.il;i 5 ('.i, 
M . ,  I,()!)9) ti;l,<; I)('(;ll ;i.l,<;() (h't,(~<:l,(~(} l).y (:(>itlli);ti+ilt~'; 
I)t(~di(:ub.'.  ;I,l~,~lllli(;ll(, i ; l ,) i l( ; l ,  lii(;,<; ()\]' I'(;I(;V;I,ili, :;(:u 
ix;n(:(;:;. })~l.,l,(' (;~':l)r(',,~;,<;Joti:; :;ll(:\[i ~i,<; ~17' I t '  ((,l i(; 
I 71,\]1) \] i lw('  t)(!(;il ;I,Ill';lil(',lil;(;(! iil,:(' ' i  !)!)(i f f l  t7 ) j 
I ' (  I \[' (\])('.(:. 1'(, J{){)( i) .  ' I ' l l ( ;  l;(',,'-;lii\[,\]li~'; t);I,";,<;;I.t>~C,% 
.~/,1(; l)( ' .h)w (ul i (h; i \ ] i l l ( , , : ;  i l i(l it';l, l,J iW; i);ir;i.i)liru.<;(~,<0~ 
l,()p;(;l,h(;r wil,\]i I, he\]r  I+;ul,;li,<',li i,i'a,ii,~;hl, l,i()im (I)(>ht 
l'a(:(; \]ii(Ih'.;t.1,\]li~,j i)u.r;~,l)}ll'm;('..;): 
I. ~ )b - - D' C, (j))714 }}t t . . L  ~5 ('t. i ' i  7fit :J , "  ~li l. 
;b d,> I\[ \]~ :)< {'1! $} J~l/ D I 1!)gu 41-12 ) i  l ' , l l ,  
)il, 421/  (/)l,i,iiql~ffi~-~~f2.D~,'('{I, >" ~',>)~/~ ., 
h(:c<)r(iiu K 1,() m'~w:; I\]:om \])(!lu, on  I)(~,c(~nlt)(n. :1 '(, 
;I 99(i  t im ,}ul)ml(!.~(! unll>u~,uador':~ r(!sid(!H(x! iu I, inm, 
l;he (:ul)il,al, wa~ al,l,m:ked 1)y lef'l;i:;(; Ku(n'r i \] lu:s 
( ' . l 'u lmC Amarn) ,  and mmly  l)u()t)l(! lr()lH 1)ol,h 
,hG)mmse mM Ih,ruvian M(le,'~ wet(,, lm\](1 iN h():;tage. 
(JOll(;(!rllillg l;ho. hos tage  iu ( ' ident ,  t lm l.;m:(!rn- 
ili(;nL r(?qll(?;.;1,cd (;\]1(~ l"ci;ilViall ~t'~()VCI'IIIII(HI|~ 1,() ~lY;NIII'C 
l;he hosl;ages ~ ,~;al'cty (m \ ] ) ( ; c ( ;mhor  (;he, \] St;h, an(t 
sent  Mr.  \ ] IO IHU ' I ' I  Tal<ahiko, co()r(linal,or, Div i -  
s ion of Middle, and Sou(;h America, Ministry o1 ln- 
t(;rnal;i(mal Aft'air.% I;o \]hn:u on (,hat, nigh(,. 
/)r(;side, nl, I?u.iil:nori's l)olil;i(:al authori(,y will l'()cov(}l" 
bocause he succe,(xlod in IJ~(; Ol)(',ral ion (;o IJr(~ak into 
L\]I(~ ,}ltI)allCS(: ~UIII);ISS~I(IOI':bi l'o,si(l(~n(;(~ in l)(n'u (ill 
Apr i l  22~ 1997.  
d . \]  F, vuhut t ; io .u  
!'Z*aluu,i,i()l~ ()\[ lIlll\]\[~i-.(\]()(;lllll(;lli, :;(Immm'iz;~.l,\]():~) 
( :~l l  \]>; ti)~" i';~r~" J \]; ~'('; ~'ll(~}" ( :( )~; l; 1;hm~ I,li;((, oF ',;iu~',lu 
(h)(:tl,i)('}H, :;mtlt)tu.vizu.(,i()\]i. 'l~(;:;t:l)(,(t',; J'()r evu.\]im.- 
(,i{))l ()\[' i l l l l \ ] \ [ i  (\[()CilHI(;II\[; N/IlIlllI;)~i'iZD,\[,i(.)I/ }I;I,V(; I1()\[; 
I)('.ml dev(;hg)('xt 3'(%. S() (;l~(; t)r(;,u(',ul, (wuhm, l,i()u i:; 
\[i~il,(,(t i,(" ih(~ :;m)G)le ,uu(; ()1' ,~l:l'(;i(:\](;S lu('J!l,\]()il(;(:l 
;d )()v(', /)ul, (,h(; ()\])(,aine(l I (':~ul(,;; sn?;K(',% p;cu(;ra\] 
,g)l)l i(:al)i l i l ,y (ff t, ll(, i)\]()/)(),~(~(l m(;l~hod mid  : ;u l )  
l)()cl;:; t,h(; c()~Li('(:i,ttr(, i;im,I, ,~;l):r('adi:uK :ml, ival,h)l! 
i:; (;\[l'e(:l;iv(' i'()r )uuli; i-d()(:ulH(;ul, mull; i / ;() l) i(:  :;mll- 
,i ~m'i zu,l,i( )u. 
/\:; di:;cu;~,",e(l ill I,Iw, i)re\,-i(m,~; ,~;eul, i (m, l,he IW() 
I'():;e(i iI~('l;ll()(I (:'ul (;xl,vm:(, i)ui)()rl,;uli, a,v(,i(;}(~;, 
I , I lui h>. l l le  ()l)(UdiG! i al!(I :;el,L\]('!li('~ll, ar(,i('.l(~;, 
t'r()i,i lit(,y url,i(:lu,~; I,\])()/11; \]'(',I ' l l h(),%a,Ku i}miduul,. 
..'\\]:;(). :~.~ I';-I1. ~,?ul)h (:()ll,~;i:;I,ilt~,; ()\[ i tu/)()r l ,a. l l i ,  r(',lu 
I,i()~ u; mu(  )\] i?~ iu )1 )o\]t,a.\]l 1; (',\]J l,i i,i(,,% \] '(:?"u, ,J(rp(um:>;(: 
(VIl/,l)(t,$;,(;(I,(/O'l','; : 'I'CS;?I(/(YIIC(:~ '\["II,'\])(LC A'IH,(L'I"II~ ;/,li(\[ ,%0 
(hi.  \ ]m:;  \])('X'n :;U(:(:(',~;:d~U\]\].y (:O1L;d;I;ll(:l,e(t ()\]1 \[;}li,~; 
l)u:d:;. 'f'h(', u,1)(w('.-~w, ul;i(u~(;d :~('.(,1~()(1 al:;() u,~;u,;; 
(:r(),';:; (\]()(:u~(;n(, (:()r(fi'(',r(,,ll(:(,,u For r(,l)\]m',ilG,; ('X- 
I)I'(};;V;i()II,H Wi\[,ll II\]()I'(~ C()II(I\]'(',IiC ()ll(;~;. 
/\}1 (I1(;;;(' u)(; ;,,)'<:lliv(;d (~:~.~('n/;\]u.lly l) u,~i}w; ili- 
\] '()rl l l ; l l ,h)l l  ill (,h(', ( I I )A  I,a,J';g;ilIJ'? () l i ly,  I)lli, lit)l, 
(h;(l iu (,('ull)\]u,(,(;,~ li)r int'()Him, t,i()li (;xi:ra(:l,h)u. 
'1'11(,, 1))'()1)(),,;(,(l u}(;i,h(>(l i,~; h(;n(:e (,Xl)('x:l;('xt (,(> (lu 
(u'(;a,(,(~ UN Ul)l)r()l)ri;d,(; I'2,,1{, gra,1)h whui l  a l)l)l ie(| 
1;() a,u()i,h(;r ,~('1, ol! do(:mu(,n(,,~ ~t)()(ll, mul( , ipl( ;  t,o\])-- 
4?2, qYanMi ) rma I;i()n 
t\]'\[l(; \])I;()CC,HH ()V f;llllllllgtl'iZ~l,\[;i()ll c~l,l} })(; (1(;(:O111 
l)O,~;ud in{,o i;lu'(;(', ,~;l;~g(',~; (Sl)a,rc\]~ Jones ,  .I 999) :  
\ ] .  SO/IIC(; |,(;X(, ill,\[,(:1'\])l"(),\[,(tt'~O'\]l, l;O ~OllF(;(; \[;(;X(; 
r(' 4)r(',,~(ul LM;ion, 
2. s()/ll 'C()r(,l)r(;,s(;nl,;~l;ion L?'a,'u,'tfo,r'm,(U, ion  go 
,~llllllll~W.y ~OX1; \];('~\])l'(;S(~lli;~L(;iOll,, ~l,ll(| 
3. , smmua.ry  (;0x(, 9c',,c'mtion f rom ,smnmary  
r(; 1)r( ;s( 'ntat i ( )n.  
(~ l )A - tagg( ;d  do(:mn(~ults a rc  rega, rded  as sour( :u 
(;('xl; rc t ) resen i ;a t ions .  The  m(; l ;hod (lescril)(~'d 
at )or ( ,  \['ocuse:~ on  the  t rm~sforma,  l;ion sl~g(;. \]b; 
mul l ,  i - l ingua.l i i ,y ( :om('s f rom t, he  mul l ; i - l inguMi i ;y  
oJ' i;h(' ,%a,g(.',. 
897 
5 Conc lus ion  
Summarization of multiple documents about 
nmltiple topics has been discussed in this pa- 
pet'. The method proposed here uses spread- 
ing activation over documents syntactically and 
semanticMly annotated with GDA tags. It is 
capable of: 
? extraction of the opening and settlement 
articles from fifty articles about a hostage 
incident, 
? creation of an entity-relation graph of im- 
portant relations among important entities, 
? extraction and pruning of important sen- 
tences, gnd 
? substitution of expressions with more con- 
crete ones using cross-document corefer- 
ences. 
The inethod is essentially multilingual because 
it is based on GDA tags gild the GDA tagset 
is designed to address nmltilingual coverage. 
Since this tagset can en,bed various linguistic 
intbrination into documents, it could be a stan- 
dard tbrmat for the study of the transformation 
and/or generdtion stage of doculnent summa- 
rization, among other natural language process- 
ing tasks. 
Re ferences  
Saliha Azzam, Kevin Ihlmphreys, and Robert 
Gaizauskas. 1999. Using coretbrence chains 
for text sunnnarization. In A CL'99 Work- 
shop on Cor@;rcncc and Its Applications, 
pages 77 84. 
Amit Bagga and Breck Baldwin. 1998. Entity- 
based cross-document coreferencing using the 
vector space model. In COLING-A UL'98, 
pages 79 85. 
Regina Barzilay, Kathleen Ft. McKeown, and 
Michael Elhadad. 1999. information fltsion 
in the context of multi-document summariza- 
tion. In A CL'99, pages 550-557. 
K6iti Hasida, Syun Ishizaki, and Hitoshi Isa- 
hara. 1987. A connectionist approach to the 
generation of abstracts. In Gerard Keinpen, 
editor, Natural Langauge Generation: New 
Results in Artificial Intelligence, Psychology, 
and Linguistics, pages 149-156. Martinus Ni- 
jhoff. 
Kgiti Hasida. 1997. Global l)ocument Annota- 
tion. Ill NLPRS'9Z pages 505-508. 
Sadao Kurohashi and Makoto Nagao. 1998. 
Japanese morphological nalysis sysl, e ln JU- 
MAN manual. 
Sadao Kurohashi. 1998. Japanese syntactic 
analysis ystem KNP manual. 
Inderjeet Mani and Eric Bloedorn. 1999. Sum- 
marizing similarities and differences anlong 
related documents. Ill hlderjeet Mani and 
Mark T. Maybury, editors, ADVANCES IN 
A UTOMATIC TEXT SUMMARIZATION, 
chapter 23, pages 357 379. The MIT Press. 
Mark T. Maybury. 1999. Generating sum- 
maries from event data. \]n Indel:jeet IVIani 
and Mark T. Maybury, editors, ADVANCES 
IN A UTOMA2TC TEXT SUMMARIZA- 
TION, chapter 17, pages 265-281. The MIT 
Press. 
Kathleen McKeown and Dragolnir R. Radev. 
1995. Generating summaries of imfltiple news 
articles. In SIGIR'95, pages 74-82. 
Kathleen R. McKeown, Judith L. Klavans, 
Vasileios Itatzivassiloglou, Regina Barzilay, 
and Elezar Eskin. 1999. Towards multi- 
docuinent summarization by retbrmulation: 
Progress and prospects. In AAAI-99, pages 
453460. 
Katashi Nagao and KSiti IIasida. 1998. Au- 
tomatic Text Summarization B~Lsed on the 
Global Docmnent Annotation. In COLING- 
ACL'98, pages 917 921. 
Yoshiki Niwa, Shingo Nishiokg, Makoto 
Iwayama, Akihiko Takano, and Yosihiko 
Nitta. 1997. Topic graph generation for 
query naviagation: Use of fl:equency classes 
tbr topic extraction. In NLPRS'9Z pages 
95 100. 
Mark Sanderson and Bruce Croft. 1999. De- 
riving concept hierarchies from text. In SL 
GIR'99, pages 206 213. 
Karen Sparck Jones. 1999. Automatic summa- 
rizing: factors and directions. In Inderjeet 
Mani and Mark T. Maybury, editors, AD- 
VANCES IN A UTOMATIC TEXT SUMMA- 
RIZATION, chapter 1, pages 1-12. The MIT 
Press. 
898 
A Statistical Model for Domain-Independent Text Segmentation
Masao Utiyama and Hitoshi Isahara
Communications Research Laboratory
2-2-2 Hikaridai Seika-cho, Soraku-gun,
Kyoto, 619-0289 Japan
mutiyama@crl.go.jp and isahara@crl.go.jp
Abstract
We propose a statistical method that
finds the maximum-probability seg-
mentation of a given text. This method
does not require training data because
it estimates probabilities from the given
text. Therefore, it can be applied to
any text in any domain. An experi-
ment showed that the method is more
accurate than or at least as accurate as
a state-of-the-art text segmentation sys-
tem.
1 Introduction
Documents usually include various topics. Identi-
fying and isolating topics by dividing documents,
which is called text segmentation, is important
for many natural language processing tasks, in-
cluding information retrieval (Hearst and Plaunt,
1993; Salton et al, 1996) and summarization
(Kan et al, 1998; Nakao, 2000). In informa-
tion retrieval, users are often interested in par-
ticular topics (parts) of retrieved documents, in-
stead of the documents themselves. To meet such
needs, documents should be segmented into co-
herent topics. Summarization is often used for a
long document that includes multiple topics. A
summary of such a document can be composed
of summaries of the component topics. Identifi-
cation of topics is the task of text segmentation.
A lot of research has been done on text seg-
mentation (Kozima, 1993; Hearst, 1994; Oku-
mura and Honda, 1994; Salton et al, 1996; Yaari,
1997; Kan et al, 1998; Choi, 2000; Nakao, 2000).
A major characteristic of the methods used in this
research is that they do not require training data
to segment given texts. Hearst (1994), for exam-
ple, used only the similarity of word distributions
in a given text to segment the text. Consequently,
these methods can be applied to any text in any
domain, even if training data do not exist. This
property is important when text segmentation is
applied to information retrieval or summarization,
because both tasks deal with domain-independent
documents.
Another application of text segmentation is
the segmentation of a continuous broadcast news
story into individual stories (Allan et al, 1998).
In this application, systems relying on supervised
learning (Yamron et al, 1998; Beeferman et al,
1999) achieve good performance because there
are plenty of training data in the domain. These
systems, however, can not be applied to domains
for which no training data exist.
The text segmentation algorithm described in
this paper is intended to be applied to the sum-
marization of documents or speeches. Therefore,
it should be able to handle domain-independent
texts. The algorithm thus does not use any train-
ing data. It requires only the given documents for
segmentation. It can, however, incorporate train-
ing data when they are available, as discussed in
Section 5.
The algorithm selects the optimum segmen-
tation in terms of the probability defined by a
statistical model. This is a new approach for
domain-independent text segmentation. Previous
approaches usually used lexical cohesion to seg-
ment texts into topics. Kozima (1993), for exam-
ple, used cohesion based on the spreading activa-
tion on a semantic network. Hearst (1994) used
the similarity of word distributions as measured
by the cosine to gauge cohesion. Reynar (1994)
used word repetition as a measure of cohesion.
Choi (2000) used the rank of the cosine, rather
than the cosine itself, to measure the similarity of
sentences.
The statistical model for the algorithm is de-
scribed in Section 2, and the algorithm for ob-
taining the maximum-probability segmentation is
described in Section 3. Experimental results are
presented in Section 4. Further discussion and our
conclusions are given in Sections 5 and 6, respec-
tively.
2 Statistical Model for Text
Segmentation
We first define the probability of a segmentation
of a given text in this section. In the next section,
we then describe the algorithm for selecting the
most likely segmentation.
Let
  
			 be a text consisting of
 words, and let      
			  be a segmen-
tation of
 
consisting of  segments. Then the
probability of the segmentation  is defined by:

Reliable Measures for Aligning Japanese-English News Articles and
Sentences
Masao Utiyama and Hitoshi Isahara
Communications Research Laboratory
3-5 Hikari-dai, Seika-cho, Souraku-gun, Kyoto 619-0289 Japan
mutiyama@crl.go.jp and isahara@crl.go.jp
Abstract
We have aligned Japanese and English
news articles and sentences to make a
large parallel corpus. We first used a
method based on cross-language informa-
tion retrieval (CLIR) to align the Japanese
and English articles and then used a
method based on dynamic programming
(DP) matching to align the Japanese and
English sentences in these articles. How-
ever, the results included many incorrect
alignments. To remove these, we pro-
pose two measures (scores) that evaluate
the validity of alignments. The measure
for article alignment uses similarities in
sentences aligned by DP matching and
that for sentence alignment uses similar-
ities in articles aligned by CLIR. They
enhance each other to improve the accu-
racy of alignment. Using these measures,
we have successfully constructed a large-
scale article and sentence alignment cor-
pus available to the public.
1 Introduction
A large-scale Japanese-English parallel corpus is an
invaluable resource in the study of natural language
processing (NLP) such as machine translation and
cross-language information retrieval (CLIR). It is
also valuable for language education. However, no
such corpus has been available to the public.
We recently have obtained a noisy parallel cor-
pus of Japanese and English newspapers consisting
of issues published over more than a decade and
have tried to align their articles and sentences. We
first aligned the articles using a method based on
CLIR (Collier et al, 1998; Matsumoto and Tanaka,
2002) and then aligned the sentences in these articles
by using a method based on dynamic programming
(DP) matching (Gale and Church, 1993; Utsuro et
al., 1994). However, the results included many in-
correct alignments due to noise in the corpus.
To remove these, we propose two measures
(scores) that evaluate the validity of article and sen-
tence alignments. Using these, we can selectively
extract valid alignments.
In this paper, we first discuss the basic statistics
on the Japanese and English newspapers. We next
explain methods and measures used for alignment.
We then evaluate the effectiveness of the proposed
measures. Finally, we show that our aligned corpus
has attracted people both inside and outside the NLP
community.
2 Newspapers Aligned
The Japanese and English newspapers used as
source data were the Yomiuri Shimbun and the Daily
Yomiuri. They cover the period from September
1989 to December 2001. The number of Japanese
articles per year ranges from 100,000 to 350,000,
while English articles ranges from 4,000 to 13,000.
The total number of Japanese articles is about
2,000,000 and the total number of English articles is
about 110,000. The number of English articles rep-
resents less than 6 percent that of Japanese articles.
Therefore, we decided to search for the Japanese ar-
ticles corresponding to each of the English articles.
The English articles as of mid-July 1996 have tags
indicating whether they are translated from Japanese
articles or not, though they don?t have explicit links
to the original Japanese articles. Consequently, we
only used the translated English articles for the arti-
cle alignment. The number of English articles used
was 35,318, which is 68 percent of all of the arti-
cles. On the other hand, the English articles before
mid-July 1996 do not have such tags. So we used all
the articles for the period. The number of them was
59,086. We call the set of articles before mid-July
1996 ?1989-1996? and call the set of articles after
mid-July 1996 ?1996-2001.?
If an English article is a translation of a Japanese
article, then the publication date of the Japanese ar-
ticle will be near that of the English article. So we
searched for the original Japanese articles within 2
days before and after the publication of each English
article, i.e., the corresponding article of an English
article was searched for from the Japanese articles of
5 days? issues. The average number of English arti-
cles per day was 24 and that of Japanese articles per
5 days was 1,532 for 1989-1996. For 1996-2001, the
average number of English articles was 18 and that
of Japanese articles was 2,885. As there are many
candidates for alignment with English articles, we
need a reliable measure to estimate the validity of
article alignments to search for appropriate Japanese
articles from these ambiguous matches.
Correct article alignment does not guarantee the
existence of one-to-one correspondence between
English and Japanese sentences in article alignment
because literal translations are exceptional. Original
Japanese articles may be restructured to conform to
the style of English newspapers, additional descrip-
tions may be added to fill cultural gaps, and detailed
descriptions may be omitted. A typical example of a
restructured English and Japanese article pair is:
Part of an English article: ?e1? Two bullet holes were found at
the home of Kengo Tanaka, 65, president of Bungei Shunju, in Ak-
abane, Tokyo, by his wife Kimiko, 64, at around 9 a.m. Monday.
?/e1? ?e2? Police suspect right-wing activists, who have mounted
criticism against articles about the Imperial family appearing in
the Shukan Bunshun, the publisher?s weekly magazine, were re-
sponsible for the shooting. ?/e2? ?e3? Police received an anony-
mous phone call shortly after 1 a.m. Monday by a caller who
reported hearing gunfire near Tanaka?s residence. ?/e3? ?e4? Po-
lice found nothing after investigating the report, but later found a
bullet in the Tanakas? bedroom, where they were sleeping at the
time of the shooting. ?/e4?
Part of a literal translation of a Japanese article: ?j1? At about
8:55 a.m. on the 29th, Kimiko Tanaka, 64, the wife of Bungei
Shunju?s president Kengo Tanaka, 65, found bullet holes on the
eastern wall of their two-story house at 4 Akabane Nishi, Kita-
ku, Tokyo.?/j1? ?j2? As a result of an investigation, the officers of
the Akabane police station found two holes on the exterior wall of
the bedroom and a bullet in the bedroom.?/j2? ?j3? After receiv-
ing an anonymous phone call shortly after 1 a.m. saying that two
or three gunshots were heard near Tanaka?s residence, police offi-
cers hurried to the scene for investigation, but no bullet holes were
found.?/j3? ?j4?When gunshots were heard, Mr. and Mrs. Tanaka
were sleeping in the bedroom.?/j4? ?j5? Since Shukan Bunshun, a
weekly magazine published by Bungei Shunju, recently ran an ar-
ticle criticizing the Imperial family, Akabane police suspect right-
wing activists who have mounted criticism against the recent arti-
cle to be responsible for the shooting and have been investigating
the incident.?/j5?
where there is a three-to-four correspondence be-
tween {e1, e3, e4} and {j1, j2, j3, j4}, together with
a one-to-one correspondence between e2 and j5.
Such sentence matches are of particular interest
to researchers studying human translations and/or
stylistic differences between English and Japanese
newspapers. However, their usefulness as resources
for NLP such as machine translation is limited for
the time being. It is therefore important to extract
sentence alignments that are as literal as possible.
To achieve this, a reliable measure of the validity of
sentence alignments is necessary.
3 Basic Alignment Methods
We adopt a standard strategy to align articles and
sentences. First, we use a method based on CLIR
to align Japanese and English articles (Collier et
al., 1998; Matsumoto and Tanaka, 2002) and then
a method based on DP matching to align Japanese
and English sentences (Gale and Church, 1993; Ut-
suro et al, 1994) in these articles. As each of these
methods uses existing NLP techniques, we describe
them briefly focusing on basic similarity measures,
which we will compare with our proposed measures
in Section 5.
3.1 Article alignment
Translation of words
We first convert each of the Japanese articles into
a set of English words. We use ChaSen1 to seg-
ment each of the Japanese articles into words. We
next extract content words, which are then translated
into English words by looking them up in the EDR
Japanese-English bilingual dictionary,2 EDICT, and
ENAMDICT,3 which have about 230,000, 100,000,
1http://chasen.aist-nara.ac.jp/
2http://www.iijnet.or.jp/edr/
3http://www.csse.monash.edu.au/?jwb/edict.html
and 180,000 entries, respectively. We select two En-
glish words for each of the Japanese words using
simple heuristic rules based on the frequencies of
English words.
Article retrieval
We use each of the English articles as a query and
search for the Japanese article that is most similar
to the query article. The similarity between an En-
glish article and a (word-based English translation
of) Japanese article is measured by BM25 (Robert-
son and Walker, 1994). BM25 and its variants have
been proven to be quite efficient in information re-
trieval. Readers are referred to papers by the Text
REtrieval Conference (TREC)4, for example.
The definition of BM25 is:
BM25(J,E) =
?
T?E
w(1) (k1 + 1)tfK + tf
(k3 + 1)qtf
k3 + qtf
where
J is the set of translated English words of a
Japanese article and E is the set of words of an
English article. The words are stemmed and stop
words are removed.
T is a word contained in E.
w(1) is the weight of T , w(1) = log (N?n+0.5)(n+0.5) .
N is the number of Japanese articles to be searched.
n is the number of articles containing T .
K is k1((1 ? b) + b dlavdl ). k1, b and k3 are pa-
rameters set to 1, 1, and 1000, respectively. dl is
the document length of J and avdl is the average
document length in words.
tf is the frequency of occurrence of T in J . qtf is
the frequency of T in E.
To summarize, we first translate each of the
Japanese articles into a set of English words. We
then use each of the English articles as a query and
search for the most similar Japanese article in terms
of BM25 and assume that it corresponds to the En-
glish article.
3.2 Sentence alignment
The sentences5 in the aligned Japanese and English
articles are aligned by a method based on DP match-
ing (Gale and Church, 1993; Utsuro et al, 1994).
4http://trec.nist.gov/
5We split the Japanese articles into sentences by using sim-
ple heuristics and split the English articles into sentences by
using MXTERMINATOR (Reynar and Ratnaparkhi, 1997).
We allow 1-to-n or n-to-1 (1 ? n ? 6) alignments
when aligning the sentences. Readers are referred
to Utsuro et al (1994) for a concise description of
the algorithm. Here, we only discuss the similarities
between Japanese and English sentences for align-
ment. Let Ji and Ei be the words of Japanese and
English sentences for i-th alignment. The similar-
ity6 between Ji and Ei is:
SIM(Ji, Ei) = co(Ji ?Ei) + 1l(Ji) + l(Ei)? 2co(Ji ? Ei) + 2
where
l(X) =?x?X f(x)
f(x) is the frequency of x in the sentences.
co(Ji ? Ei) =
?
(j,e)?Ji?Ei min(f(j), f(e))
Ji ? Ei = {(j, e)|j ? Ji, e ? Ei} and Ji ? Ei is
a one-to-one correspondence between Japanese and
English words.
Ji and Ei are obtained as follows. We use ChaSen to
morphologically analyze the Japanese sentences and
extract content words, which consists of Ji. We use
Brill?s tagger (Brill, 1992) to POS-tag the English
sentences, extract content words, and use Word-
Net?s library7 to obtain lemmas of the words, which
consists of Ei. We use simple heuristics to obtain
Ji ? Ei, i.e., a one-to-one correspondence between
the words in Ji and Ei, by looking up Japanese-
English and English-Japanese dictionaries made up
by combining entries in the EDR Japanese-English
bilingual dictionary and the EDR English-Japanese
bilingual dictionary. Each of the constructed dictio-
naries has over 300,000 entries.
We evaluated the implemented program against a
corpus consisting of manually aligned Japanese and
English sentences. The source texts were Japanese
white papers (JEIDA, 2000). The style of translation
was generally literal reflecting the nature of govern-
ment documents. We used 12 pairs of texts for eval-
uation. The average number of Japanese sentences
per text was 413 and that of English sentences was
495.
The recall, R, and precision, P , of the program
against this corpus were R = 0.982 and P = 0.986,
respectively, where
6SIM(Ji, Ei) is different from the similarity function used
in Utsuro et al (1994). We use SIM because it performed well
in a preliminary experiment.
7http://www.cogsci.princeton.edu/?wn/
R = number of correctly aligned sentence pairs
total number of sentence pairs aligned in corpus
P = number of correctly aligned sentence pairs
total number of sentence pairs proposed by program
The number of pairs in a one-to-n alignment is n.
For example, if sentences {J1} and {E1, E2, E3}
are aligned, then three pairs ?J1, E1?, ?J1, E2?, and
?J1, E3? are obtained.
This recall and precision are quite good consid-
ering the relatively large differences in the language
structures between Japanese and English.
4 Reliable Measures
We use BM25 and SIM to evaluate the similarity
in articles and sentences, respectively. These mea-
sures, however, cannot be used to reliably discrim-
inate between correct and incorrect alignments as
will be discussed in Section 5. This motivated us to
devise more reliable measures based on basic simi-
larities.
BM25 measures the similarity between two bags
of words. It is not sensitive to differences in the
order of sentences between two articles. To rem-
edy this, we define a measure that uses the similari-
ties in sentence alignments in the article alignment.
We define AVSIM(J,E) as the similarity between
Japanese article, J , and English article, E:
AVSIM(J,E) =
?m
k=1 SIM(Jk, Ek)
m
where (J1, E1), (J2, E2), . . . (Jm, Em) are the sen-
tence alignments obtained by the method described
in Section 3.2. The sentence alignments in a cor-
rectly aligned article alignment should have more
similarity than the ones in an incorrectly aligned ar-
ticle alignment. Consequently, article alignments
with high AVSIM are likely to be correct.
Our sentence alignment program aligns sentences
accurately if the English sentences are literal trans-
lations of the Japanese as discussed in Section 3.2.
However, the relation between English news sen-
tences and Japanese news sentences are not literal
translations. Thus, the results for sentence align-
ments include many incorrect alignments. To dis-
criminate between correct and incorrect alignments,
we take advantage of the similarity in article align-
ments containing sentence alignments so that the
sentence alignments in a similar article alignment
will have a high value. We define
SntScore(Ji, Ei) = AVSIM(J,E)? SIM(Ji, Ei)
SntScore(Ji, Ei) is the similarity in the i-th align-
ment, (Ji, Ei), in article alignment J and E. When
we compare the validity of two sentence alignments
in the same article alignment, the rank order of sen-
tence alignments obtained by applying SntScore is
the same as that of SIM because they share a com-
mon AVSIM. However, when we compare the va-
lidity of two sentence alignments in different article
alignments, SntScore prefers the sentence alignment
with the more similar (high AVSIM) article align-
ment even if their SIM has the same value, while
SIM cannot discriminate between the validity of two
sentence alignments if their SIM has the same value.
Therefore, SntScore is more appropriate than SIM if
we want to compare sentence alignments in different
article alignments, because, in general, a sentence
alignment in a reliable article alignment is more re-
liable than one in an unreliable article alignment.
The next section compares the effectiveness of
AVSIM to that of BM25, and that of SntScore to
that of SIM.
5 Evaluation of Alignment
Here, we discuss the results of evaluating article and
sentence alignments.
5.1 Evaluation of article alignment
We first estimate the precision of article alignments
by using randomly sampled alignments. Next, we
sort them in descending order of BM25 and AVSIM
to see whether these measures can be used to provide
correct alignments with a high ranking. Finally, we
show that the absolute values of AVSIM correspond
well with human judgment.
Randomly sampled article alignments
Each English article was aligned with a Japanese
article with the highest BM25. We sampled 100 ar-
ticle alignments from each of 1996-2001 and 1989-
1996. We then classified the samples into four cate-
gories: ?A?, ?B?, ?C?, and ?D?. ?A? means that there
was more than 50% to 60% overlap in the content of
articles. ?B? means more than 20% to 30% and less
than 50% to 60% overlap. ?D? means that there was
no overlap at all. ?C? means that alignment was not
included in ?A?,?B? or ?D?. We regard alignments
that were judged to be A or B to be suitable for NLP
because of their relatively large overlap.
1996-2001 1989-1996type lower ratio upper lower ratio upper
A 0.49 0.59 0.69 0.20 0.29 0.38
B 0.06 0.12 0.18 0.08 0.15 0.22
C 0.03 0.08 0.13 0.03 0.08 0.13
D 0.13 0.21 0.29 0.38 0.48 0.58
Table 1: Ratio of article alignments
The results of evaluations are in Table 1.8 Here,
?ratio? means the ratio of the number of articles
judged to correspond to the respective category
against the total number of articles. For example,
0.59 in line ?A? of 1996-2001 means that 59 out of
100 samples were evaluated as A. ?Lower? and ?up-
per? mean the lower and upper bounds of the 95%
confidence interval for ratio.
The table shows that the precision (= sum of the
ratios of A and B) for 1996-2001 was higher than
that for 1989-1996. They were 0.71 for 1996-2001
and 0.44 for 1989-1996. This is because the En-
glish articles from 1996-2001 were translations of
Japanese articles, while those from 1989-1996 were
not necessarily translations as explained in Section
2. Although the precision for 1996-2001 was higher
than that for 1989-1996, it is still too low to use them
as NLP resources. In other words, the article align-
ments included many incorrect alignments.
We want to extract alignments which will be eval-
uated as A or B from these noisy alignments. To
do this, we have to sort all alignments according to
some measures that determine their validity and ex-
tract highly ranked ones. To achieve this, AVSIM is
more reliable than BM25 as is explained below.
8The evaluations were done by the authors. We double
checked the sample articles from 1996-2001. Our second
checks are presented in Table 1. The ratio of categories in the
first check were A=0.62, B=0.09, C=0.09, and D=0.20. Com-
paring these figures with those in Table 1, we concluded that
first and second evaluations were consistent.
Sorted alignments: AVSIM vs. BM25
We sorted the same alignments in Table 1 in de-
creasing order of AVSIM and BM25. Alignments
judged to be A or B were regarded as correct. The
number, N, of correct alignments and precision, P,
up to each rank are shown in Table 2.
1996-2001 1989-1996
AVSIM BM25 AVSIM BM25rank
N P N P N P N P
5 5 1.00 5 1.00 5 1.00 2 0.40
10 10 1.00 8 0.80 10 1.00 4 0.40
20 20 1.00 16 0.80 19 0.95 9 0.45
30 30 1.00 25 0.83 28 0.93 16 0.53
40 40 1.00 34 0.85 34 0.85 24 0.60
50 50 1.00 39 0.78 37 0.74 28 0.56
60 60 1.00 47 0.78 42 0.70 30 0.50
70 66 0.94 55 0.79 42 0.60 35 0.50
80 70 0.88 62 0.78 43 0.54 38 0.47
90 71 0.79 68 0.76 43 0.48 40 0.44
100 71 0.71 71 0.71 44 0.44 44 0.44
Table 2: Rank vs. precision
From the table, we can conclude that AVSIM
ranks correct alignments higher than BM25. Its
greater accuracy indicates that it is important to
take similarities in sentence alignments into account
when estimating the validity of article alignments.
AVSIM and human judgment
Table 2 shows that AVSIM is reliable in ranking
correct and incorrect alignments. This section re-
veals that not only rank order but also absolute val-
ues of AVSIM are reliable for discriminating be-
tween correct and incorrect alignments. That is,
they correspond well with human evaluations. This
means that a threshold value is set for each of 1996-
2001 and 1989-1996 so that valid alignments can be
extracted by selecting alignments whose AVSIM is
larger than the threshold.
We used the same data in Table 1 to calculate
statistics on AVSIM. They are shown in Tables 3
and 4 for 1996-2001 and 1989-1996, respectively.
type N lower av. upper th. sig.
A 59 0.176 0.193 0.209 0.168 **
B 12 0.122 0.151 0.179 0.111 **
C 8 0.077 0.094 0.110 0.085 *
D 21 0.065 0.075 0.086
Table 3: Statistics on AVSIM (1996-2001)
In these tables, ?N? means the number of align-
ments against the corresponding human judgment.
type N lower av. upper th. sig.
A 29 0.153 0.175 0.197 0.157 *
B 15 0.113 0.141 0.169 0.131
C 8 0.092 0.123 0.154 0.097 **
D 48 0.076 0.082 0.088
Table 4: Statistics on AVSIM (1989-1996)
?Av.? means the average value of AVSIM. ?Lower?
and ?upper? mean the lower and upper bounds of
the 95% confidence interval for the average. ?Th.?
means the threshold for AVSIM that can be used to
discriminate between the alignments estimated to be
the corresponding evaluations. For example, in Ta-
ble 3, evaluations A and B are separated by 0.168.
These thresholds were identified through linear dis-
criminant analysis. The asterisks ?**? and ?*? in the
?sig.? column mean that the difference in averages
for AVSIM is statistically significant at 1% and 5%
based on a one-sided Welch test.
In these tables, except for the differences in the
averages for B and C in Table 4, all differences
in averages are statistically significant. This indi-
cates that AVSIM can discriminate between differ-
ences in judgment. In other words, the AVSIM val-
ues correspond well with human judgment. We then
tried to determine why B and C in Table 4 were not
separated by inspecting the article alignments and
found that alignments evaluated as C in Table 4 had
relatively large overlaps compared with alignments
judged as C in Table 3. It was more difficult to dis-
tinguish B or C in Table 4 than in Table 3.
We next classified all article alignments in 1996-
2001 and 1989-1996 based on the thresholds in Ta-
bles 3 and 4. The numbers of alignments are in Table
5. It shows that the number of alignments estimated
to be A or B was 46738 (= 31495 + 15243). We
regard about 47,000 article alignments to be suffi-
ciently large to be useful as a resource for NLP such
as bilingual lexicon acquisition and for language ed-
ucation.
1996-2001 1989-1996 total
A 15491 16004 31495
B 9244 5999 15243
C 4944 10258 15202
D 5639 26825 32464
total 35318 59086 94404
Table 5: Number of articles per evaluation
In summary, AVSIM is more reliable than BM25
and corresponds well with human judgment. By us-
ing thresholds, we can extract about 47,000 article
alignments which are estimated to be A or B evalu-
ations.
5.2 Evaluation of sentence alignment
Sentence alignments in article alignments have
many errors even if they have been obtained from
correct article alignments due to free translation as
discussed in Section 2. To extract only correct
alignments, we sorted whole sentence alignments
in whole article alignments in decreasing order of
SntScore and selected only the higher ranked sen-
tence alignments so that the selected alignments
would be sufficiently precise to be useful as NLP
resources.
The number of whole sentence alignments was
about 1,300,000. The most important category for
sentence alignment is one-to-one. Thus, we want
to discard as many errors in this category as pos-
sible. In the first step, we classified whole one-
to-one alignments into two classes: the first con-
sisted of alignments whose Japanese and English
sentences ended with periods, question marks, ex-
clamation marks, or other readily identifiable char-
acteristics. We call this class ?one-to-one?. The
second class consisted of the one-to-one alignments
not belonging to the first class. The alignments
in this class, together with the whole one-to-n
alignments, are called ?one-to-many?. One-to-one
had about 640,000 alignments and one-to-many had
about 660,000 alignments.
We first evaluated the precision of one-to-one
alignments by sorting them in decreasing order of
SntScore. We randomly extracted 100 samples from
each of 10 blocks ranked at the top-300,000 align-
ments. (A block had 30,000 alignments.) We clas-
sified these 1000 samples into two classes: The
first was ?match? (A), the second was ?not match?
(D). We judged a sample as ?A? if the Japanese and
English sentences of the sample shared a common
event (approximately a clause). ?D? consisted of the
samples not belonging to ?A?. The results of evalua-
tion are in Table 6.9
9Evaluations were done by the authors. We double checked
all samples. In the 100 samples, there were a maximum of two
or three where the first and second evaluations were different.
range # of A?s # of D?s
1 - 100 0
30001 - 99 1
60001 - 99 1
90001 - 97 3
120001 - 96 4
150001 - 92 8
180001 - 82 18
210001 - 74 26
240001 - 47 53
270001 - 30 70
Table 6: One-to-one: Rank vs. judgment
This table shows that the number of A?s decreases
rapidly as the rank increases. This means that
SntScore ranks appropriate one-to-one alignments
highly. The table indicates that the top-150,000 one-
to-one alignments are sufficiently reliable.10 The ra-
tio of A?s in these alignments was 0.982.
We then evaluated precision for one-to-many
alignments by sorting them in decreasing order of
SntScore. We classified one-to-many into three cat-
egories: ?1-90000?, ?90001-180000?, and ?180001-
270000?, each of which was covered by the range of
SntScore of one-to-one that was presented in Table
6. We randomly sampled 100 one-to-many align-
ments from these categories and judged them to be A
or D (see Table 7). Table 7 indicates that the 38,090
alignments in the range from ?1-90000? are suffi-
ciently reliable.
range # of one-to-many # of A?s # of D?s
1 - 38090 98 2
90001 - 59228 87 13
180001 - 71711 61 39
Table 7: One-to-many: Rank vs. judgment
Tables 6 and 7 show that we can extract valid
alignments by sorting alignments according to
SntScore and by selecting only higher ranked sen-
tence alignments.
Overall, evaluations between the first and second check were
consistent.
10The notion of ?appropriate (correct) sentence alignment?
depends on applications. Machine translation, for example,
may require more precise (literal) alignment. To get literal
alignments beyond a sharing of a common event, we will select
a set of alignments from the top of the sorted alignments that
satisfies the required literalness. This is because, in general,
higher ranked alignments are more literal translations, because
those alignments tend to have many one-to-one corresponding
words and to be contained in highly similar article alignments.
Comparison with SIM
We compared SntScore with SIM and found that
SntScore is more reliable than SIM in discriminating
between correct and incorrect alignments.
We first sorted the one-to-one alignments in de-
creasing order of SIM and randomly sampled 100
alignments from the top-150,000 alignments. We
classified the samples into A or D. The number of
A?s was 93, and that of D?s was 7. The precision was
0.93. However, in Table 6, the number of A?s was
491 and D?s was 9, for the 500 samples extracted
from the top-150,000 alignments. The precision was
0.982. Thus, the precision of SntScore was higher
than that of SIM and this difference is statistically
significant at 1% based on a one-sided proportional
test.
We then sorted the one-to-many alignments by
SIM and sampled 100 alignments from the top
38,090 and judged them. There were 89 A?s and
11 D?s. The precision was 0.89. However, in Ta-
ble 7, there were 98 A?s and 2 D?s for samples from
the top 38,090 alignments. The precision was 0.98.
This difference is also significant at 1% based on a
one-sided proportional test.
Thus, SntScore is more reliable than SIM. This
high precision in SntScore indicates that it is im-
portant to take the similarities of article alignments
into account when estimating the validity of sen-
tence alignments.
6 Related Work
Much work has been done on article alignment. Col-
lier et al (1998) compared the use of machine trans-
lation (MT) with the use of bilingual dictionary term
lookup (DTL) for news article alignment in Japanese
and English. They revealed that DTL is superior to
MT at high-recall levels. That is, if we want to ob-
tain many article alignments, then DTL is more ap-
propriate than MT. In a preliminary experiment, we
also compared MT and DTL for the data in Table
1 and found that DTL was superior to MT.11 These
11We translated the English articles into Japanese with an MT
system. We then used the translated English articles as queries
and searched the database consisting of Japanese articles. The
direction of translation was opposite to the one described in
Section 3.1. Therefore this comparison is not as objective as
it could be. However, it gives us some idea into a comparison
of MT and DTL.
experimental results indicate that DTL is more ap-
propriate than MT in article alignment.
Matsumoto and Tanaka (2002) attempted to align
Japanese and English news articles in the Nikkei In-
dustrial Daily. Their method achieved a 97% preci-
sion in aligning articles, which is quite high. They
also applied their method to NHK broadcast news.
However, they obtained a lower precision of 69.8%
for the NHK corpus. Thus, the precision of their
method depends on the corpora. Therefore, it is not
clear whether their method would have achieved a
high accuracy in the Yomiuri corpus treated in this
paper.
There are two significant differences between our
work and previous works.
(1) We have proposed AVSIM, which uses sim-
ilarities in sentences aligned by DP matching, as
a reliable measure for article alignment. Previous
works, on the other hand, have used measures based
on bag-of-words.
(2) A more important difference is that we have
actually obtained not only article alignments but also
sentence alignments on a large scale. In addition to
that, we are distributing the alignment data for re-
search and educational purposes. This is the first
attempt at a Japanese-English bilingual corpus.
7 Availability
As of late-October 2002, we have been distributing
the alignment data discussed in this paper for re-
search and educational purposes.12 All the informa-
tion on the article and sentence alignments are nu-
merically encoded so that users who have the Yomi-
uri data can recover the results of alignments. The
data also contains the top-150,000 one-to-one sen-
tence alignments and the top-30,000 one-to-many
sentence alignments as raw sentences. The Yomiuri
Shimbun generously allowed us to distribute them
for research and educational purposes.
We have sent over 30 data sets to organizations
on their request. About half of these were NLP-
related. The other half were linguistics-related. A
few requests were from high-school and junior-high-
school teachers of English. A psycho-linguist was
also included. It is obvious that people from both in-
side and outside the NLP community are interested
12http://www.crl.go.jp/jt/a132/members/mutiyama/jea/index.html
in this Japanese-English alignment data.
8 Conclusion
We have proposed two measures for extracting valid
article and sentence alignments. The measure for ar-
ticle alignment uses similarities in sentences aligned
by DP matching and that for sentence alignment
uses similarities in articles aligned by CLIR. They
enhance each other and allow valid article and sen-
tence alignments to be reliably extracted from an ex-
tremely noisy Japanese-English parallel corpus.
We are distributing the alignment data discussed
in this paper so that it can be used for research and
educational purposes. It has attracted the attention of
people both inside and outside the NLP community.
We have applied our measures to a Japanese and
English bilingual corpus and these are language in-
dependent. It is therefore reasonable to expect that
they can be applied to any language pair and still re-
tain good performance, particularly since their effec-
tiveness has been demonstrated in such a disparate
language pair as Japanese and English.
References
Eric Brill. 1992. A simple rule-based part of speech tagger. In
ANLP-92, pages 152?155.
Nigel Collier, Hideki Hirakawa, and Akira Kumano. 1998. Ma-
chine translation vs. dictionary term translation ? a com-
parison for English-Japanese news article alignment. In
COLING-ACL?98, pages 263?267.
William A. Gale and Kenneth W. Church. 1993. A program
for aligning sentences in bilingual corpora. Computational
Linguistics, 19(1):75?102.
Japan Electronic Industry Development Association JEIDA.
2000. Sizen Gengo Syori-ni Kan-suru Tyousa Houkoku-syo
(Report on natural language processing systems).
Kenji Matsumoto and Hideki Tanaka. 2002. Automatic align-
ment of Japanese and English newspaper articles using an
MT system and a bilingual company name dictionary. In
LREC-2002, pages 480?484.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maxi-
mum entropy approach to identifying sentence boundaries.
In ANLP-97.
S. E. Robertson and S. Walker. 1994. Some simple effec-
tive approximations to the 2-Poisson model for probabilistic
weighted retrieval. In SIGIR?94, pages 232?241.
Takehito Utsuro, Hiroshi Ikeda, Masaya Yamane, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Bilingual text match-
ing using bilingual dictionary and statistics. In COLING?94,
pages 1076?1082.
  
Criterion for Judging Request Intention  
in Response texts of Open-ended Questionnaires 
INUI Hiroko 
Communications Research 
Laboratory 
Graduate School of Science and 
Technology Kobe University 
hinui@crl.go.jp 
UTIYAMA Masao 
 
Communications Research 
Laboratory 
 
mutiyama@crl.go.jp 
ISAHARA Hitoshi  
Communications Research 
Laboratory 
Graduate School of Science and 
Technology Kobe University  
isahara@crl.go.jp 
 
Abstract 
Our general research aim is to extract the 
actual intentions of persons when they 
respond to open-ended questionnaires. 
These intentions include the desire to 
make requests, complaints, expressions of 
resignation and so forth, but here we 
focus on extracting the intention to make 
a request. To do so, we first have to judge 
whether their responses contain the intent 
to make a request. Therefore, as a first 
step, we have developed a criterion for 
judging the existence of request intentions 
in responses. This criterion, which is 
based on paraphrasing, is described in 
detail in this paper. Our assumption is that 
a response with request intentions can be 
paraphrased into a typical request 
expression, e.g., ?I would like to ...?, 
while responses without request are not 
paraphrasable. The criterion is evaluated 
in terms of objectivity, reproducibility and 
effectiveness. Objectivity is demonstrated 
by showing that machine learning 
methods can learn the criterion from a set 
of intention-tagged data, while 
reproducibility, that the judgments of 
three annotators are reasonably consistent, 
and effectiveness, that judgments based 
not on the criterion but on intuition do not 
agree. This means the criterion is 
necessary to achieve reproducibility. 
These experiments indicate that the 
criterion can be used to judge the 
existence of request intentions in 
responses reliably. 
1 Introduction 
In every aspect of society, it is necessary for us 
to ?know what the request is.? This is because 
knowing what the request is plays an important 
role in allowing us to identify and solve problems 
to achieve improvements. 
In recent years, the spread of electronic devices 
such as personal computers and the Internet has 
allowed us to save most requests in machine-
readable texts. On the basis of these texts, research 
and development have been conducted ?to know 
what the request is? as an element technology in 
natural language processing. For example, the 
research includes text mining (Nasukawa, 2001) 
and information extraction (Tateno, 2003) for 
customer claims and inquiries, development of an 
FAQ generation support system to a call center 
(Yanase et al, 2002; Matsuzawa, 2002), an FAQ 
navigation system using Q&A stored a call center 
(Matsui, 2002), and the development of 
requirement capturing methods for extracting 
requests made in meetings for software 
development (Doi, 2003). However, ?to know 
what the request is? means to know the intention of 
various people in society such as residents, users, 
customers and patients, and it is inadequate to 
extract only request expressions expressed literally 
in texts. For this reason, previous works are not 
sufficient to understand intentions. 
Against this background, (Inui et al, 1998; Inui 
et al, 2001; Inui and Isahara, 2002) have been 
studying how to extract and classify request 
intentions of respondents from responses of open-
ended questionnaires (OEQs) which are 
accumulated requests. This paper describes the 
development of a criterion for judging request 
intentions and an evaluation of the criterion in 
terms of objectivity, reproducibility and 
effectiveness. 
  
2 Development of the criterion for 
judging request intentions  
2.1 Problems of an existing theory of 
modality 
Response texts of OEQs are the focus of attention 
as data for text mining. Researchers have tried to 
extract various types of information from those 
texts (Lebart et al, 1998; Li and Yamanishi, 2001; 
Osumi and Lebart, 2000; Takahashi, 2000). 
However, they have mainly used only keywords 
(mostly nouns) as the basic units of extraction. If 
only the characteristic key words are analyzed with 
regard to sentences such as  ?Company A?s beer 
tastes good,? ?Company A?s beer does not seem to 
taste good,? and ?Company B?s beer tastes better 
than company A?s,? the attention is directed 
toward ?company A/company B/beer/tastes/good,? 
and it is not possible to differentiate the meaning 
of the passages. 
Because of this, as (Toyoda, 2002) points out, 
text mining in the future needs to treat modality, 
which often changes the meaning of the sentences 
completely. Two separate studies (Inui et al, 1998; 
Morohashi et al, 1998) have tried to process texts 
using words like auxiliary verbs and auxiliary verb 
equivalents as modality information. The modality 
information focused on in both studies, however, is 
grammatical expressions that have been accepted 
in a previous Japanese language study. Therefore, 
it is not possible to mechanically interpret requests 
and questions displayed by respondents, speakers 
and writers if they don?t contain an auxiliary verb 
or an auxiliary verb equivalent. 
In Japanese language syntax, modality is 
defined as the intention of the writer that is 
represented by grammatical expressions expressed 
grammatically (Nitta and Masuoka ed., 1989) and 
typically appears in the form of particles and 
auxiliary verbs in the sentence structure. Although 
previous text mining has focused on these 
expressions,  modality does not always appear in 
the forms of grammatical expressions, and other 
expressions are more frequently used in real world 
texts. Thus, processing only those grammatical 
expressions listed so far is not sufficient for 
extracting intentions, and it is necessary to have a 
wide coverage of modality that expresses 
intentions. 
2.2 Criterion to judge request intentions 
using paraphrasing  
Surveyors try to know request intentions on the 
respondents through questionnaires, and 
respondents try to convey their request intentions 
to surveyors by responding to questionnaires. 
Therefore, it is important to establish a method that 
can extract the request intentions of the 
respondents based on the expressions given in the 
response texts. In this section, we propose a 
criterion to judge the existence of request 
intentions. 
First, we will analyze the request expressions 
deductively. Native Japanese speakers can 
recognize expressions such as te-hoshii (would like 
you to), te-moraitai (would like you to), te-kudasai 
(please do) and te-kure (do) as request. These are 
linguistically called direct request expressions 
Request? 
1) Whether it can be 
judged to be a request by 
linguistic intuition or not 
Response
2) Whether it can judged
by some criterion to be a
request or not 
Fig. 1 Layers to judge expressions of requests 
YES NO 
Non-Request 
Request? Others
YES NO 
Direct expressions 
of request 
1) Whether it includes 
expressions of direct 
request or not
Response
2) Whether it can be
paraphrased into a sentence 
containing ?te-hoshii? as 
typical request or not 
Fig. 2 Criterion to judge request intentions 
Expressions of
request intention
Others 
YES NO 
YES NO 
Others 
  
(NIJLA, 1960) and able to indicate request 
intentions. Especially, te-hoshii is a typical request 
expression.  
In other words, these direct request expressions 
are a clue to understand that there is a request 
intended. This recognition process is equivalent to 
the first judgment in Fig.1, that is, ?whether a 
response can be judged to be a request by linguistic 
intuition or not.? We regarded this as the first level 
criterion to judge request intentions. It corresponds 
to the first level in Fig.2, the intent of which is 
equal to judge whether the response includes a 
direct request expression or not.  
Second, we consider the case that a response 
does not contain a direct request expression. In this 
case, non-requests in Fig.1 may be judged as 
requests. For example, based on the relation with 
surveyors, respondents and the situation, 
?Guardrails should be built along sidewalks of 
heavily congested roads? and ?Building eco-
friendly roads is important? can be interpreted as 
?We want guardrails along the sidewalks? and 
?We want you to think about the environment.? 
However, the interpretation is due to ?some? 
implicit criterion as shown in the second judgment 
in Fig. 1. As the implicit criterion depends on the 
judges, it is possible that the judgments differ1 . 
This means that the results of the judgment, 
namely request ?  in Fig. 1, are not re-created 
consistently. Therefore, the second judge in Fig.1 
is not reproducible.  
Consequently we attempted to manifest the 
implicit criterion as an explicit criterion to judge 
the existence of request intentions. This 
manifestation is the criterion ?whether a response 
can be paraphrased into a sentence containing te-
hoshii as a typical request expression or not? as the 
second judge in Fig. 2. As this criterion is explicit, 
the judgment of the criterion does not depend on 
the judges and agree consistently. Therefore, the 
second judge in Fig.2, namely the proposed 
criterion is reproducible and the results of the 
judgment, namely the expression of request 
intentions in Fig.2 is re-created consistently2. 
As mentioned above, we propose a criterion for 
judging request intentions by paraphrasing a 
response sentence into a typical request sentence 
                                                          
1  This is demonstrated by the results of the experiment 
described in Section 4.2. 
2  This reproducibility is described in detail in Section 4.1. 
contained te-hoshii. In Section 3, we evaluate the 
proposed criterion by a single judge analytically 
and objectively. In Section 4, we evaluate the 
results of experiments conducted by different 
judges from the viewpoint of reproducibility and 
effectiveness. These evaluations enable to 
demonstrate that the criterion, namely paraphrasing 
is an important method to determine the intentions 
independent of variety of surface expressions and 
differences among individual judgments. 
3 Evaluation by a single judge 
3.1 Analysis of response texts 
Using the proposed criterion described in Section 
2.2, we analyzed and classified response sentences 
manually according to two considerations: (1) if 
they include direct request expressions such as te-
hoshii and te-moraitai; and (2) if it is possible to 
paraphrase them into a sentence ending with te-
hoshii. To make the judgment for (1), we used 
request expressions listed by (Morita and Matsui, 
1989).  
 Expressions of 
direct requests 
Paraphrase Out of 3000 
sentences  
? Included  Possible 547 
? Included Not possible 3 
? Not included  Possible 1190 
? Not included  Not possible 1252 
Table 1 Results of applying criterion  
for judging request intentions3  
The analysis data are part of the response texts 
of OEQs carried out to make the best use of the 
opinions of the citizens in future road planning 
(Voice report, 1996). The original OEQ corpus 
contains a total of 35,674 respondents and 113,316 
opinions. The analysis data comprised 3,000 
sentences sampled at random after separating the 
plural sentences contained in the response text into 
single sentences. The criterion in Section 2.2 was 
used and the results are shown in Table 1. 
Line ? in Table 1 includes sentences with direct 
request expressions such as te-hoshii, te-kudasai 
and te-kure. All of these could be paraphrased into 
te-hoshii and accounted for about 20% of the 3,000 
sentences. Line ?  includes direct request 
expressions that could not be paraphrased because 
they were used in quotations. These examples are 
exceptional. Expressions in line ? correspond to 
                                                          
3 Eight sentences were excluded from Table 1 because they 
were ambiguous out of contexts. 
  
expressions of request intentions in Fig.2 in 
Section 2.2. These expressions are shown in Table 
2. Line ? includes non-request expressions. 
Table 2 shows various forms of expressions 
based on parts of speech (POS), i.e., verbs, nouns 
and adjectives, that have not been considered 
acceptable as modality expressions, even though 
they are paraphrasable by te-hoshii, and thus they 
are request expressions. As described in Section 
2.1, several studies have been made on modality in 
terms of  particles, auxiliary verbs, and auxiliary 
verb equivalents. However, little attention has been 
given to other POS in this regard. This is because 
modality expressions have been primarily 
connected with the grammatical elements such as 
auxiliary verbs in syntax. However, Table 2, which 
lists expressions of request intentions, shows that 
verbs, nouns and adjectives are actually also 
important elements that express modality.  
Previous works that aim to extract requests have 
used pattern matching methods, and patterns that  
mainly consist of the direct request expressions 
corresponding to ?  in Table 1. However, the 
results of manual analysis for paraphrasability 
shown in Table 2 indicate that using the proposed 
criterion enables many expressions of request 
intentions to be extracted from responses. In 
addition, we found a tendency for the number of 
expressions of request intentions direct request 
expressions, as shown in Table 1. In this section, 
we have provided explanation for the coverage of 
the criterion by analyzing response texts.  
3.2 Evaluation of objectivity through 
machine learning methods 
This section shows that the possibility of 
paraphrasing is learnable by machine learning 
methods. The data for the machine learning 
methods were tagged by the expert that analyzed 
the data in Table 1. Our assumption is that if 
machine learning methods can learn the 
paraphrasability from the data, then the data are 
said to have been tagged consistently enough to be 
mechanically learnable. This  indicates that the 
criterion proposed in Section 2 is objectively 
applicable to tag data. 
Machine learning methods 
We use two machine learning methods in this 
section. They are maximum entropy method (ME) 
(Beger et al 96) and support vector machine 
(SVM) (cristianini00)4, both of which have been 
shown to be quite effective in natural language 
processing. 
The task of a machine learning method is to 
make a classifier that can decide whether a 
response is paraphrasable by te-hoshii or not. A 
response X is tagged possible if it is paraphrasable 
                                                          4 We used maxent (http://www.crl.go.jp/jt/a132/ 
members/mutiyama/software.html) for ME learning and 
TinySVM(http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM/) 
for SVM learning.  
Type of POS Types of form of expression Example Sentence 
End-form in 
verbs and 
adjectives 
-??????(make? to do) /-?????
(control)  etc. 
??????????????????????
??(Increase greenbelt and make it easier to see 
signposts) 
Used as noun -??(secure)/-??(equipment) etc. ???????(Secure car parks) 
Predicates 
abbreviated 
-?   etc. ????????????????????(Road 
building from the standpoint of the elderly, 
children, and the disabled) 
Verbs and 
adjectives of 
expectation 
and desire 
-????  (seek) /-?????  (expect) / -??
??  (desire) /-?????  (is desirable) /-??
???  (is desired)/-???  (desire) /-????
?(request) etc.  
???????????????????????
????????(Roads and streets that give 
priority to the disabled, the elderly, children, and 
the weak are desirable) 
<attribute: emergency> 
-??????  (matter of urgency) /-????
?(first priority) /-???????(think that 
the first thing to do) etc.  
???????????????????????
?????????????????(It is all right 
to build expressways in provincial areas, but why 
can?t improving congested places come first?) 
<attribute:importance> 
-????  (is important) /-?????????
?(think that it is also an important matter) /-?
???  (is important) /-??????(should 
be important) /-???  (that is ideal) etc. 
??????????????????????(I 
think that the important matter is to make the 
manner of stopping vehicles thorough ) 
Nouns for 
judging value 
<attribute: necessity> 
-????????  (it may also be necessary) 
/-???????  (feel the necessity for) /-??
???(is indispensable) etc. 
????????????????????
(Cooperation of landowners is indispensable in 
road building) 
Table 2 Expressions of requests and intention obtained by  
using the criterion for judging request intentions 
  
and impossible if not. X is represented by a feature 
vector x = [x1, x2, ??, xl]where  
 
 
Given training data, a machine learning method 
produces a classifier that outputs possible or 
impossible according to a given feature vector. We 
omit the details of ME and SVM. Readers are 
referred to the above references.  
We will compare three sets of features, F1, F2 
and F3, in the experiments below. F1 consists of 
word 1-grams, F2, 1-grams and 2-grams, and F3, 
word 1-grams, 2-grams and 3-grams. For example, 
let X be a response consisting of a word sequence5 
w1, w1,?.., wm where w1 = ?b?and wm = ?e?
are special symbols representing the beginning and 
the ending of a response. Let S1 be the set of 1-
grams in X {wi |2 ? i ?m-1}, S2, 2-grams in X 
{ wiwi+1 |1 ?  i ?m-1} and S3, 3-grams in X 
{ wiwi+1wi+2 |1 ?  i ?m-2}. The F1, F2 and F3 
features contained in X are S1, S1?S2, and S1?S2
?S3, respectively. 
Experiments 
The data used for the experiments consisted of 
3,001 responses6. The numbers of the responses 
tagged possible and impossible were 1,944 and 
1,057, respectively. We used 10-fold cross 
validation to evaluate the accuracies of ME and 
SVM7. For each iteration in the cross validation, 
8/10 of the data was used for training, 1/10, for 
parameter adjustment, and 1/10, for testing. The 
precision, Pi, for iteration i is   
Pi =   
We define P as the mean of the precisions for each 
iterations, i.e., P = ?i Pi /10. We henceforth call P 
precision. The precisions of ME and SVM are in 
Table 3, together with a baseline precision 0.648 
(=1944/3001), which was obtained by tagging all 
the responses possible. In the table, the figures in 
columns ?ME? and ?SVM? are the precisions of 
ME and SVM. Line Fi (i=1,2,3) indicates that the 
precisions in that line were obtained by using Fi as 
                                                          
5 We used ChaSen (http://chasen.aist-nara.ac.jp/) to segment 
an answer into a word sequence. 
6 This data was different from the response text analyzed in 
Section 3.1. 
7 We used the polynomial kernel for SVM. We tried degrees 1 
and 2 d=1,2. Since d=1 outperformed d=2, the results of 
d=1 are in Table 3 
a  feature set. We use one-sided Welch tests to 
measure the differences between precisions and 
say ?statistically significant? or simply 
?significant? when the differences were 
statistically significant at 1% level. 
Table 3 indicates that both ME and SVM 
outperform the baseline by a large margin. The 
differences were, of course, statistically significant. 
Therefore, we can conclude that these methods are 
quite effective in this task.  
 ME SVM Baseline 
F1 0.892 0.887 0.648 
F2 0.912 0.909 0.648 
F3 0.913 0.915 0.648 
Table 3 Precision of ME and SVM  
This table also indicates that ME and SVM are 
comparable in precision. The differences of 
precision were not statistically significant. We next 
compared the highest precisions in lines F1, F2, and 
F3. F1 was significantly outperformed by both F2 
and F3, but there was not a significant difference 
between F2 and F3. Consequently, we can use 
either ME or SVM as a machine learning method 
and F2 or F3 as a feature set. 
Table 3 demonstrates that we can expect about 
91% precision in deciding the paraphrasability by 
using either ME or SVM. This is a reasonably high 
precision. Therefore, we can conclude that the 
criterion proposed in Section 2.2 is sufficiently 
objective and stable. 
4 Evaluation by different judges 
In Section 3, we described the manual analytical 
evaluation by a single judge and the objective 
evaluation by machine learning that uses a corpus 
prepared based on the analytical evaluation. 
Section 4 refers to experiments carried out by 
multiple different judges.  
4.1 Evaluation of reproducibility: judgment 
of paraphrasing by multiple judges 
The subjects of this experiment were three male 
native speakers of Japanese in their twenties who 
were engineering majors. The experiment was 
carried out using a total of 24,000 random 
sentences from the OEQ corpus described in 
Section 3.1 by applying the criterion proposed in 
Section 2.2. If a response text included plural 
sentences, they were separated into single 
sentences as mentioned in Section 3.1. Of the 
xi = 
number of correctly tagged answers
total number of answers in the test data
1   if X has feature i 
0   otherwise 
  
24,000 sentences, the three subjects A, B and C 
were each given 8,000 of them. However, the pairs 
A and B, B and C, and A and C were each given 
4,000 common sentences, so that a variation of 
sentence totaled 12,000. 
As shown in Table 1 in Section 3.1, direct 
request expressions can be paraphrased with te-
hoshii, therefore, we deal only with the judgment 
of the second level in Fig.2, namely the 
paraphrasing into te-hoshii. For the evaluation, we 
prepared a set of work instructions for the subjects, 
part of which is shown below.  
Work instructions  
1) Not only the end expression but also case 
particles, case particle equivalents and those 
containing such expressions or expressions of 
connection are to be paraphrased. 
2) If te-hoshii is to be changed to a negative 
request of shite-hoshiku-nai (do not want), place 
the word negative at the end. 
3) Not only functional words but also content 
words, furthermore, word order may be changed in 
paraphrasing  
#1 S(ource): ???????????? (We  
think that there are not enough 
car parks.) 
? T(arget): ???????????(We want  
car parks to be increased.)  
The experimental results are given in Table 4, 
where P means possible to paraphrase and NP 
means not possible. KC is the kappa coefficient 
between subjects (Cohen 1960).   
 B  
A P NP Total KC 
P 2372 970 3342 0.48 
NP 36 622 658  
Total 2408 1592 4000  
 C  
A P NP Total KC 
P 3123 264 3387 0.61 
NP 171 442 613  
Total 3294 706 4000  
 C  
B P NP Total KC 
P 2119 50 2169 0.49 
NP 934 897 1831  
Total 3053 947 4000  
Table 4 Results of paraphrasability  
using the criterion   
Generally, the closer the kappa coefficient is to 
1, the higher the degree of agreement is obtained. 
There is a complete agreement when it is 1. In 
general, the ranges [0.81-1.00], [0.61-0.80], [0.41-
0.60], [0.21-0.40] and [0.00-0.20] correspond to 
full, practical, medium, low, and no agreement, 
respectively.  
Therefore, as Table 4 indicates, the results of the 
judging and the paraphrasing using the criterion by 
the three subjects showed that there was substantial 
agreement between subject A and C, and medium 
agreement between A and B, and B and C. 
These results indicate that the method based on 
the criterion, whether used by a single judge or by 
different judges(=subjects) for analysis and 
experiment, enables requests and non-requests to 
be distinguished. Therefore, we can conclude that 
using the criterion enables even untrained people 
to reproduce the extraction of requests. 
Sentences such as #2 and #3 below are 
examples of sentences that were agreed to be non-
paraphrasable. These include expressions of 
intentions in which the current situation is accepted 
passively such as #2 ???????(I think that it cannot 
be helped),? or in which the current situation is 
actively accepted such as #3 ?? ? ? ? ? (are 
wonderful)?. Furthermore, #4 is a sentence that 
begins with a clear statement of reason ????
(the reason is).? This indicates that a motive for 
requests exists, and that a response formed by 
multiple sentences often composes request-motive 
adjacency in discourse structure.   
Examples of sentences that could not be 
paraphrased: 
#2 ????????????????????
???(I think that it cannot be 
helped if rise in charges is 
necessary.)  
?3 ???????????????????
?????????????????? (The 
town and roads are wonderful as even 
people in wheelchairs can do 
shopping by themselves here and 
there with ease and wander about.)  
#4 ?????????????????(The 
reason is that overall development 
cannot be hoped for.)  
This analysis shows that paraphrasable sentences 
indicate requests and non-paraphrasable sentences 
indicate the acceptance of the current situations or 
the motives for requests. 
4.2 Evaluation of effectiveness: judging 
intention without using the criterion 
To evaluate whether the proposed criterion 
described in Section 2.2 is effective or not, we 
carried out an experiment to see if a response 
  
shows requests or not without the criterion. The 
two subjects, D and E, who took part in this 
experiment were both native speakers of Japanese. 
Subject D was a male student in his twenties from 
the education department of a university, and  
subject E was a female student also in her twenties 
from the literature department of a university. They 
used the same data of 4,000 sentences that were  
used by the subjects B and C in Section 4.1. The 
subjects D and E did not consult with each other 
and carried out the work separately. We provided 
them with the following instructions before asking 
them to start the work. 
? Each response sentence is context-free. 
? Judge intuitively, and mark 1 if you think the 
sentence shows a request, and mark 0 if you 
do not . 
? Make sure to mark either 1 or 0. 
The results of the experiment are given in Table 
5, where 1 and 0 in the right table correspond to P 
and NP in Table 4. We show the data again 
because subjects B and C used the same data as 
subjects D and E. In Table 5, the kappa coefficient 
(KC), between D and E is lower than that between 
B and C. Moreover, it is the lowest among all those 
given in Tables 4 and 5. The KC of 0.17 means 
there is no agreement between D and E.   
The results indicate the rate of agreement is 
higher for judgments made using the criterion than 
for subjective judgments. That is to say, this proves 
the effectiveness of the criterion.   
 ?  ? 
? 1 0 Total ? P NP Total
1 562 1880 2442 P 2119 50 2169
0 39 1517 1556 NP 934 897 1831
total 601 3397 3998 total 3053 947 4000
KC for D&E 0.17 KC for B&C 0.49 
Table 5 Results for experiment for effectiveness 
4.3 Examination of evaluation results 
We examine here mainly the cases in which no 
agreement was obtained with respect to 
paraphrasing in the experiment described in 
Section 4.1. Table 4 shows the cases where 
disagreement was considerable. The results for 
these cases, shown in Table 6, indicate that 
disagreement is obtained when the sentences are 
paraphrased into the forms including clauses of 
cause and reason indicated by ?node?(because) as 
#5. The clause is underlined in the target sentence 
in #5. 
#5 S:????????????????(A narrow 
road is made even narrower) 
T: ?????????????????(node)?
??????????(Because the narrow 
road is made even narrower, I would 
like to see something done about 
it.)  
The source sentence #5 is a statement showing the 
condition of the road being narrow. This statement 
can be seen as a motive for a request in the target 
sentence of #5. That is to say, the source sentence 
#5 itself shows not the content of a request but the 
?motive for request.? The three subjects disagreed 
in their judgments on whether or not the ?motive 
for request? sentence was paraphrasable as shown 
in the bottom line of Table 6. As the table indicates, 
disagreement rates of 64.4%, 51.5%, and 9.0% 
were obtained between A and B, A and C, and B 
and C. The reason for these high disagreement 
rates was that we did not give clear directions in 
the work instructions. The sentences which the 
paraphrasing includes ?node? are not requests and 
should not be extracted. This means these 
sentences should have been considered to be non-
paraphrasable.  
On the other hand, with regard to ?motive for 
request? sentences, there was an example #1 in 
Section 4.1 in which the work instructions 
requested the subjects to paraphrase such a 
sentence. That is, the work instructions suggested 
that the source sentence #1 ?I think that we do not 
have enough car parks? is a motive for the request 
?I want car parks to be increased.? This kind of 
inadequate instruction led to instability in the work 
done and might have increased the disagreement 
rates obtained in the judgment. 
However, according to the data prepared by the 
expert referred to in Section 3.2, ?motive for 
request? sentences cannot be paraphrased into te-
hoshii, and machine learning has confirmed that 
the data are objective. Therefore, it can be 
considered that the work of removing ?motive for 
requests? sentences can be done stably. This means 
Examinees ?? ?? ??
No. of paraphrase includes node 648 224 89 
A 645 194 --- 
B 3 --- 3 
 
subject 
C --- 30 86 
No. of disagreed paraphrasing  1006 435 984
Rates of node in disagreements (%) 64.4 51.5 9.0
Table 6 Disagreed paraphrase including cause  
and reason clauses ?node?  
  
that if the work instructions give clear directions  
like ?if you are able to add node at the end of a 
sentence, that sentence should be regarded not as a 
content of request, but a motive of request,? then 
the rate of agreement may be improved. 
5 Conclusion 
We have developed a criterion for judging request 
intentions. We evaluated this criterion from three 
points of view. The first evaluation was to analyze 
the data applying the criterion by a single judge. 
From this analysis, it was found that this criterion 
makes it possible to extract requests and that the 
coverage can be guaranteed compared with 
previous studies. Moreover, a corpus was prepared 
based on the analysis and was used for a machine 
learning experiment. From this experiment results, 
we confirmed the criterion using a paraphrase was 
objective. 
Furthermore, by different judges, the second 
evaluation was made from the experiment 
conducted by three subjects. The rate of agreement 
for the paraphrasability was high, which indicated 
that the results of requests extraction were re-
created using the criterion. This proves the 
reproducibility of the criterion.  
In the third experiment, two subjects judged the 
sentences without using the criterion to see 
whether or not there was a request in each response 
sentence. A comparison of the results of the second 
and the third experiments showed that a higher rate 
of agreement was obtained with the method using 
the criterion. This confirmed the effectiveness of 
the criterion. 
In future work, we will analyze ?motives for 
request? sentences found from the examinations, 
and prepare a criterion for distinguishing between 
request motives and the contents of request 
intentions. 
References 
Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della 
Pietra. 1996. A maximum entropy approach to natural 
language processing. Computaional Linguistics, Vol.22, 
No.1, pp39-71. 
Jacob Cohen. 1960. A Coefficient of Agreement for Nominal 
Scales. Educational and Psychological Measurement. 20, 
37-46.  
Nello Cristianini and John Shawe-Taylor. 2000. An 
Introduction to Support Vector Machines. Cambridge 
University Press. 
Kouichi Doi, Naoyuki Horai, Isamu Watanabe, Yoshinori 
Katayama and Masayuki. Sonobe. 2003. User-oriented 
Requirements Capturing Method in Analyzing 
Requirements Capturing Meeting. Transactions of IPSJ, 
vol.44 No.1, pp48-58. 
The Committee for Roads in the 21st Century Basic Policy 
Board, Road Council. 1996. Voice Report. 
Hiroko Inui, Kiyotaka Ucihmoto and Hitoshi Isahara. 1998. 
Classification of Open-Ended Questionnaires based on 
Analysis of Modality. Proceedings of the 4th Annual 
Meeting of the ANLP, pp540-543. 
Hiroko Inui, Masaki Murata, Kiyotaka Uchimoto and Hitoshi 
Isahara. 2001. Classification of Open-Ended Questionnaires 
based on Surface Information in Sentence Structure. 
Proceedings of the 6th NLPRS2001, pp315-322. 
Hiroko Inui and Hitoshi Isahara. 2002. Proposition for 
?Extended Modality? ?Extraction of Intention in Open-
ended response texts-. Technical Report of EICE, Vol.102 
No.414, NLC2002-43, pp31-36. 
Ludovic Lebart, Andre Salem and Lisette Berry. 1998. 
Exploring Textual Data, Kluwer Academic Publishers, 14-
20. 
Hang Li and Kenji Yamanishi. 2001. Mining from Open 
Answers in Questionnaire Data Using Statistical Learning 
Techniques. Proceedings of the 4 IBIS2001. pp129-134. 
Kunio Matsui and Hozumi Tanaka. 2002. The Navigation to 
the Stored Q&A data using Simple Questions. Technical 
Report of IEICE, Vol.102 No.414, NLC2002-40, pp13-18. 
Hirofumi Matsuzawa. 2002. FAQ Generation Support System 
Using Structured Association Pattern Mining and Natural 
Language Processing. Proceedings of the FIT2002, pp69-70. 
Yoshiyuki Morita and Masae Matsuki. 1989. Expression 
Pattarn of Japanese, ALC 
Masayuki Morohashi, Tetsuya Nasukawa and Touru Nagano. 
1998. Text Mining: Knowledge Acquisition from enormous 
text data ? recognition of intention -. Proceedings of the 
57th Annual Meeting of IPSJ 
Tetsuya Nasukawa. 2001. Text Mining Application for Call 
Centers. Journal of the Japanese Society for Artificial 
Intelligence, Vol.16, No.2, pp219-225. 
Noboru Ohsumi and Ludovic Lebart. 2000. Analyzing Open-
ended Questions: Some Experimental Results for Textual 
Data Analysis Based on InfoMiner. Proceedings of the 
Institute of Statistical Mathematics. Vol.48, No.2, pp339-
376 
The National Institute for Japanese Language. 1960. A 
research for making sentence patterns in colloquial 
Japanese. 1. On materials in conversation. Shuei Publishers. 
Yoshio Nitta and Takashi Masuoka. 1989. Japanese Modality. 
Kurosio Publishers. 
Kazuko Takahashi. 2000. A supporting System for Cording of 
the answers from Open-Ended Question. Sociological 
Theory and Methods, vol.15, No.1. 149-164. 
Masakazu Tateno. 2003. The Method to extract Textual 
?Kansei? Expression in the Custmer?s Voice. IPSJ SIG 
Notes, NL-153-14, pp105-112. 
Yuki Toyoda. 2002. Translation from Text Data to Numeric 
Data ?Points for Attention in Text Mining Preparatory 
Processing as Seen from the Analyst?s. Journal of the 
Japanese Society for Artificial Intelligence, Vol.17 No.6. 
pp738-743. 
Takashi Yanase, Satoko Marumoto, Isao Nanba and Ryo 
Ochitani. 2002. Parsing Question Texts Using the Predicate 
Expressions of the Sentence End. Proceedings of the 8th 
Annual Meeting of the Association for NLP, pp647-650.  
 
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 845?850,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Converting Continuous-Space Language Models into
N-gram Language Models for Statistical Machine Translation
Rui Wang1,2,3, Masao Utiyama2, Isao Goto2, Eiichro Sumita2, Hai Zhao1,3 and Bao-Liang Lu1,3
1 Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering,
Shanghai Jiao Tong Unviersity, Shanghai, 200240, China
2 Multilingual Translation Laboratory, MASTAR Project,
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
3 MOE-Microsoft Key Lab. for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong Unviersity, Shanghai 200240 China
wangrui.nlp@gmail.com, mutiyama/igoto/eiichiro.sumita@nict.go.jp, zhaohai@cs.sjtu.edu.cn, bllu@sjtu.edu.cn
Abstract
Neural network language models, or
continuous-space language models (CSLMs),
have been shown to improve the performance
of statistical machine translation (SMT)
when they are used for reranking n-best
translations. However, CSLMs have not
been used in the first pass decoding of SMT,
because using CSLMs in decoding takes a lot
of time. In contrast, we propose a method
for converting CSLMs into back-off n-gram
language models (BNLMs) so that we can
use converted CSLMs in decoding. We show
that they outperform the original BNLMs and
are comparable with the traditional use of
CSLMs in reranking.
1 Introduction
Language models are important in natural language
processing tasks such as speech recognition and
statistical machine translation. Traditionally, back-
off n-gram language models (BNLMs) (Chen and
Goodman, 1996; Chen and Goodman, 1998;
Stolcke, 2002) are being widely used for these tasks.
Recently, neural network language models,
or continuous-space language models (CSLMs)
(Bengio et al, 2003; Schwenk, 2007; Le et al, 2011)
are being used in statistical machine translation
(SMT) (Schwenk et al, 2006; Son et al, 2010;
Schwenk et al, 2012; Son et al, 2012; Niehues
and Waibel, 2012). These works have shown that
CSLMs can improve the BLEU (Papineni et al,
2002) scores of SMT when compared with BNLMs,
on the condition that the training data for language
modeling are the same size. However, in practice,
CSLMs have not been widely used in SMT.
One reason is that the computational costs of
training and using CSLMs are very high. Various
methods have been proposed to tackle the training
cost issues (Son et al, 2010; Schwenk et al, 2012;
Mikolov et al, 2011). However, there has been little
work on reducing using costs. Since the using costs
of CSLMs are very high, it is difficult to use CSLMs
in decoding directly.
A common approach in SMT using CSLMs is
the two pass approach, or n-best reranking. In this
approach, the first pass uses a BNLM in decoding
to produce an n-best list. Then, a CSLM is used to
rerank those n-best translations in the second pass.
(Schwenk et al, 2006; Son et al, 2010; Schwenk et
al., 2012; Son et al, 2012)
Another approach is using restricted Boltzmann
machines (RBMs) (Niehues and Waibel, 2012)
instead of using multi-layer neural networks
(Bengio et al, 2003; Schwenk, 2007; Le et al,
2011). Since probability in a RBM can be calculated
very efficiently (Niehues and Waibel, 2012), they
can use the RBM language model in SMT decoding.
However, the RBM was just used in an adaptation of
SMT, not in a large SMT task, because the training
costs of RBMs are very high.
The last approach is using a BNLM to simulate
a CSLM (Deoras et al, 2011; Arsoy et al, 2013).
(Deoras et al, 2011) used a recurrent neural network
language model (RNNLM) to generate a large
amount of text, which was generated by sampling
words from the probability distributions calculated
by the RNNLM. Then, they trained the BNLM
845
from the text using the interpolated Kneser-Ney
smoothing method. (Arsoy et al, 2013) converted
neural network language models of increasing order
to pruned back-off language models, using lower-
order models to constrain the n-grams allowed in
higher-order models.
Both of these methods were used in decoding for
speech recognition. These methods were applied
to not-so-large scale experiments (55 million (M)
words for training their BNLMs) (Arsoy et al,
2013). In contrast, our method is applied to SMT
and can be used to improve a BNLM created from
746 M words by using a CSLM trained from 42 M
words.
Because BNLMs can be trained from much larger
corpora than those that can be used for training
CSLMs, improving a BNLM by using a CSLM
trained from a smaller corpus is very important.
Actually, a CSLM trained from a smaller corpus
can improve the BLEU scores of SMT if it is used
in the n-best reranking (Schwenk, 2010; Huang et
al., 2013). In contrast, we will demonstrate that a
BNLM simulating a CSLM can improve the BLEU
scores of SMT in the first pass decoding.
Our approach is as follows: (1) First, we train a
CSLM (Schwenk, 2007) from a corpus. (2) Second,
we also train a BNLM from the same corpus or
larger corpus. (3) Finally, we rewrite the probability
of each n-gram of the BNLM with that probability
calculated from the CSLM.We also re-normalize the
probabilities of the BNLM, then use the re-written
BNLM in SMT decoding.
In Section 2, we describe the BNLM and CSLM
(Schwenk, 2010) used for re-writing BNLMs. In
Section 3, we describe the method of converting
a CSLM into a BNLM. In Sections 4 and 5, we
evaluate our method and conclude.
2 Language Models
In this section, we will introduce the standard
BNLM and CSLM structure and probability
calculation.
2.1 Standard back-off ngram language model
A BNLM predicts the probability of a wordwi given
its preceding n ? 1 words hi = wi?1i?n+1. But
it will suffer from data sparseness if the context,
hi, does not appear in the training data. So an
estimation by ?backing-off? to models with smaller
histories is necessary. In the case of the modified
Kneser-Ney smoothing (Chen and Goodman, 1998),
the probability of wi given hi under a BNLM,
Pb(wi|hi), is:
Pb(wi|hi) = P?b(wi|hi) + ?(hi)Pb(wi|wi?1i?n+2) (1)
where P?b(wi|hi) is a discounted probability and
?(hi) is the back-off weight. A BNLM is used with
a CSLM as shown below.
2.2 CSLM structure and probability
calculation
The main structure of a CSLM using a multi-
layer neural network contains four layers: the input
layer projects all words in the context hi onto
the projection layer (the first hidden layer); the
second hidden layer and the output layer achieve the
non-liner probability estimation and calculate the
language model probability P (wi|hi) for the given
context. (Schwenk, 2007).
The CSLM calculates the probabilities of all
words in the vocabulary of the corpus given
the context at once. However, because the
computational complexity of calculating the
probabilities of all words is quite high, the CSLM is
only used to calculate the probabilities of a subset
of the whole vocabulary. This subset is called
a short-list, which consists of the most frequent
words in the vocabulary. The CSLM also calculates
the sum of the probabilities of all words not in the
short-list by assigning a neuron for that purpose.
The probabilities of other words not in the short-list
are obtained from a BNLM (Schwenk, 2007;
Schwenk, 2010).
Let wi, hi be the current word and history. The
CSLM with a BNLM calculates the probability of
wi given hi, P (wi|hi), as follows:
P (wi|hi) =
{
Pc(wi|hi)
1?Pc(o|hi)
Ps(hi) if wi ? short-list
Pb(wi|hi) otherwise
(2)
where Pc(?) is the probability calculated by the
CSLM, Pc(o|hi) is the probability of the neuron
for the words not in the short-list, Pb(?) is the
probability calculated by the BNLM as in Eq. 1,
and
Ps(hi) =
?
v?short-list
Pb(v|hi). (3)
846
It can be considered that the CSLM redistributes
the probability mass of all words in the short-list.
This probability mass is calculated by using the
BNLM.
3 Conversion of CSLM into BNLM
As described in the introduction, we first train a
CSLM from a corpus. We also train a BNLM from
the same corpus or a larger corpus. Then, we rewrite
the probability of each ngram in the BNLM with the
probability calculated from the CSLM.
First, we use the probabilities of 1-grams in
the BNLM as they are. Next, we rewrite the
probabilities of n-grams (n=2,3,4,5) in the BNLM
with the probabilities calculated by using the n-gram
CSLM, respectively. Note that the n-gram CSLM
means that the length of its history is n ? 1. Note
also that we only need to rewrite the probabilities
of n-grams ending with a word in the short-list.
Finally, we re-normalize the probabilities of the
BNLM using the SRILM?s ?-renorm? option.
When we rewrite a BNLM trained from a larger
corpus, the ngrams in the BNLM often contain
unknown words for the CSLM. In that case, we use
the probabilities in the BNLM as they are.
4 Experiments
4.1 Common settings
We used the patent data for the Chinese to English
patent translation subtask from the NTCIR-9 patent
translation task (Goto et al, 2011). The parallel
training, development, and test data consisted of 1
M, 2,000, and 2,000 sentences, respectively.
We followed the settings of the NTCIR-9 Chinese
to English translation baseline system (Goto et al,
2011) except that we used various language models
to compare them. We used the MOSES phrase-
based SMT system (Koehn et al, 2003), together
with Giza++ (Och and Ney, 2003) for alignment and
MERT (Och, 2003) for tuning on the development
data. The translation performance was measured by
the case-insensitive BLEU scores on the tokenized
test data. We used mteval-v13a.pl for
calculating BLEU scores.1
1It is available at http://www.itl.nist.gov/iad/
mig/tests/mt/2009/
We used the 14 standard SMT features: five
translation model scores, one word penalty score,
seven distortion scores and one language model
score. Each of the different language models was
used to calculate the language model score.
As the baseline BNLM, we trained a 5-gram
BNLM with modified Kneser-Ney smoothing using
the English side of the 1 M sentences training data,
which consisted of 42 M words. We did not discard
any n-grams in training this model. That is, we
did not use count cutoffs. We call this BNLM as
BNLM42.
A 5-gram CSLM was trained on the same
1 M training sentences using the CSLM toolkit
(Schwenk, 2010). The settings for the CSLM
were: projection layer of dimension 256 for each
word, hidden layer of dimension 384 and output
layer (short-list) of dimension 8192, which were
recommended in the CSLM toolkit. We call this
CSLM CSLM42. CSLM42 used BNLM42 as the
background BNLM.
We also trained a larger 5-gram BNLM with
modified Kneser-Ney smoothing by adding
sentences from the 2005 US patent data distributed
in the NTCIR-8 patent translation task (Fujii et al,
2010) to the 42 M words. The data consisted of
746 M words. We call this BNLM BNLM746. We
discarded 3,4,5-grams that occurred only once when
we created BNLM746.
Next, we re-wrote BNLM42 with CSLM42 by
using the method described in Section 3. This
re-written BNLM was interpolated with BNLM42.
The interpolation weight was determined by the grid
search. That is, we changed the interpolation weight
to 0.1, 0.3, 0.5, 0.7, 0.9 to create an interpolated
BNLM. Then we used that BNLM in the SMT
system to tune the weight parameters on the first
half of the development data. Next, we selected
the interpolation weight that obtained the highest
BLEU score on the second half of the development
data. After we selected the interpolation weight,
we applied MERT again to the 2,000 sentence
development data to tune the weight parameters.2
We call this BNLM CONV42. We also obtained
CONV746 by re-writing BNLM746 with CSLM42
2We aware that the interpolation weight might be
determined by minimizing the perplexity on the development
data. However, we opted to directly maximize the BLEU score.
847
in the same way.
The vocabulary of these language models was the
same, which was extracted from the 1 M training
sentences.
4.2 Experimental results
Table 1 shows the percent BLEU scores on the test
data. The figures in the ?1st pass? column show
the BLEU scores in the first pass decoding when
we changed the language model. The figures in the
?reranking? column show the BLEU scores when
we applied CSLM42 to rerank the 100-best lists for
the different language models. When we applied
CSLM42 for reranking, we added the CSLM42
score as the additional 15th feature. The weight
parameters were tuned by using Z-MERT (Zaidan,
2009).
LMs 1st pass rerank
BNLM42 31.60 32.44
CONV42 32.58 32.98
BNLM746 32.83 33.36
CONV746 33.22 33.54
Table 1: Comparison of BLEU scores
We also performed the paired bootstrap re-
sampling test (Koehn, 2004).3 We sampled 2000
samples for each significance test.
Table 2 shows the results of a statistical
significance test, in which the ?1st? is short for
the ?1st pass?. The marks indicate whether the
LM to the left of a mark is significantly better
than that above the mark at a certain level. (???:
significantly better at ? = 0.01, ?>?: ? = 0.05,
???: not significantly better at ? = 0.05)
First, as shown in the tables, the reranking
by applying CSLM42 increased the BLEU scores
for all language models. This observation is in
accordance with those of previous work (Schwenk,
2010; Huang et al, 2013).
Second, the reranking results of BNLM42 (32.44)
were not better than those of the first pass of
BNLM746 (32.83). This indicates that if the
underlying BNLM is made from a small corpus, the
reranking using CSLM can not compensate for it.
3We used the code available at http://www.ark.cs.
cmu.edu/MT/.
BN
LM
74
6
(re
ra
nk
)
CO
N
V
74
6
(1
st)
CO
N
V
42
(re
ra
nk
)
BN
LM
74
6
(1
st)
CO
N
V
42
(1
st)
BN
LM
42
(re
ra
nk
)
BN
LM
42
(1
st)
CONV746 (rerank) ? ? ? ? ? ? ?
BNLM746 (rerank) ? ? > ? ? ?
CONV746 (1st) ? ? ? ? ?
CONV42 (rerank) ? ? ? ?
BNLM746 (1st) ? ? ?
CONV42 (1st) ? ?
BNLM42 (rerank) ?
Table 2: Significance tests for systems with different LMs
Third, CONV42 was better than BNLM42 for
both first-pass and reranking. This also holds in the
case of CONV746 and BNLM746. This indicated
that our conversion method improved the BNLMs,
even if the underlying BNLMwas trained on a larger
corpus than that used for training the CSLM. As
described in the introduction, this is very important
because BNLMs can be trained from much larger
corpora than those that can be used for training
CSLMs. This observation has not been found in the
previous work.
In addition, the first-pass of CONV42 and
CONV746 (32.58 and 33.22) were comparable with
those of the reranking results of BNLM42 and
BNLM746 (32.44 and 33.36), respectively. That is,
there were no significant differences between these
results. This indicates that our conversion method
preserves the performance of the reranking using
CSLM.
5 Conclusion
We have proposed a method for converting CSLMs
into BNLMs. The method can be used to improve
a BNLM by using a CSLM trained from a smaller
corpus than that used for training the BNLM. We
have also shown that BNLMs created by our method
performs as good as the reranking using CSLMs.
Our future work is to compare our conversion
method with that of (Arsoy et al, 2013).4
4We aware that (Arsoy et al, 2013) compared their method
with the one that is identical with our method. However, the
experiments were conducted on a speech recognition task and
the scale of the experiment was not so large. Since we noticed
their work just before the submission of our paper, we did not
have time to compare their method with our method in SMT.
848
Acknowledgments
We appreciate the helpful discussion with Andrew
Finch and Paul Dixon, and three anonymous
reviewers for many invaluable comments and
suggestions to improve our paper. This work
is supported by the National Natural Science
Foundation of China (Grant No. 60903119, No.
61170114 and No. 61272248), the National
Basic Research Program of China (Grant No.
2013CB329401) and the Science and Technology
Commission of Shanghai Municipality (Grant No.
13511500200).
References
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2013. Converting neural network
language models into back-off language models for
efficient decoding in automatic speech recognition.
In Proc. of IEEE Int. Conf. on Acoustics, Speech
and Signal Processing (ICASSP 2013), Vancouver,
Canada, May. IEEE.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic
language model. Journal of Machine Learning
Research (JMLR), 3:1137?1155, March.
Stanley F. Chen and Joshua Goodman. 1996. An
empirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meeting
on Association for Computational Linguistics, ACL
?96, pages 310?318, Santa Cruz, California, June.
Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical report, Computer Science Group,
Harvard Univ.
A. Deoras, T. Mikolov, S. Kombrink, M. Karafiat,
and Sanjeev Khudanpur. 2011. Variational
approximation of long-span language models for lvcsr.
In Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages 5532?
5535, Prague, Czech Republic, May. IEEE.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2010. Overview of the patent
translation task at the ntcir-8 workshop. In In
Proceedings of the 8th NTCIR Workshop Meeting
on Evaluation of Information Access Technologies:
Information Retrieval, Question Answering and Cross-
lingual Information Access, pages 293?302, Tokyo,
Japan, June.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR-9 Workshop Meeting, pages
559?578, Tokyo, Japan, December.
Zhongqiang Huang, Jacob Devlin, and Spyros
Matsoukas. 2013. Bbn?s systems for the chinese-
english sub-task of the ntcir-10 patentmt evaluation.
In NTCIR-10, Tokyo, Japan, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the
North American Chapter of the Association for
Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54,
Edmonton, Canada. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and
F. Yvon. 2011. Structured output layer neural
network language model. In Acoustics, Speech and
Signal Processing (ICASSP), 2011 IEEE International
Conference on, pages 5524?5527, Prague, Czech
Republic, May. IEEE.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernock. 2011. Strategies for
training large scale neural network language models.
In Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages 196?
201, Prague, Czech Republic, May. IEEE.
Jan Niehues and Alex Waibel. 2012. Continuous space
language models using restricted boltzmann machines.
In Proceedings of the International Workshop for
Spoken Language Translation, IWSLT 2012, pages
311?318, Hong Kong.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for
Computational Linguistics, ACL ?02, pages 311?
849
318, Philadelphia, Pennsylvania, June. Association for
Computational Linguistics.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proceedings
of the COLING/ACL on Main conference poster
sessions, COLING-ACL ?06, pages 723?730, Sydney,
Australia, July. Association for Computational
Linguistics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of LanguageModeling for HLT,
WLM ?12, pages 11?19, Montreal, Canada, June.
Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492?
518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. The Prague
Bulletin of Mathematical Linguistics, pages 137?146.
Le Hai Son, Alexandre Allauzen, Guillaume Wisniewski,
and Franc?ois Yvon. 2010. Training continuous
space language models: some practical issues. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 778?788, Cambridge, Massachusetts,
October. Association for Computational Linguistics.
Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL HLT ?12, pages
39?48, Montreal, Canada, June. Association for
Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings International
Conference on Spoken Language Processing, pages
257?286, November.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
850
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183?188,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning Hierarchical Translation Spans
Jingyi Zhang
1,2
, Masao Utiyama
3
, Eiichro Sumita
3
, Hai Zhao
1,2
1
Center for Brain-Like Computing and Machine Intelligence, Department of Computer
Science and Engineering, Shanghai Jiao Tong Unviersity, Shanghai, 200240, China
2
Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong Unviersity, Shanghai, 200240, China
3
National Institute of Information and Communications Technology
3-5Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
zhangjingyizz@gmail.com, mutiyama/eiichiro.sumita@nict.go.jp,
zhaohai@cs.sjtu.edu.cn
Abstract
We propose a simple and effective ap-
proach to learn translation spans for
the hierarchical phrase-based translation
model. Our model evaluates if a source
span should be covered by translation
rules during decoding, which is integrated
into the translation system as soft con-
straints. Compared to syntactic con-
straints, our model is directly acquired
from an aligned parallel corpus and does
not require parsers. Rich source side
contextual features and advanced machine
learning methods were utilized for this
learning task. The proposed approach was
evaluated on NTCIR-9 Chinese-English
and Japanese-English translation tasks and
showed significant improvement over the
baseline system.
1 Introduction
The hierarchical phrase-based (HPB) translation
model (Chiang, 2005) has been widely adopted in
statistical machine translation (SMT) tasks. The
HPB translation rules based on the synchronous
context free grammar (SCFG) are simple and pow-
erful.
One drawback of the HPB model is the appli-
cations of translation rules to the input sentence
are highly ambiguous. For example, a rule whose
English side is ?X1 by X2? can be applied to any
word sequence that has ?by? in them. In Figure 1,
this rule can be applied to the whole sentence as
well as to ?experiment by tomorrow?.
In order to tackle rule application ambiguities,
a few previous works used syntax trees. Chi-
ang (2005) utilized a syntactic feature in the HPB
I  will  nish  this  experiment  by  tomorrow
?  ?  ?  ??  ??  ??  ??  ??
Figure 1: A translation example.
model, which represents if the source span cov-
ered by a translation rule is a syntactic constituent.
However, the experimental results showed this
feature gave no significant improvement. Instead
of using the undifferentiated constituency feature,
(Marton and Resnik, 2008) defined different soft
syntactic features for different constituent types
and obtained substantial performance improve-
ment. Later, (Mylonakis and Sima?an, 2011) in-
troduced joint probability synchronous grammars
to integrate flexible linguistic information. (Liu
et al., 2011) proposed the soft syntactic constraint
model based on discriminative classifiers for each
constituent type and integrated all of them into the
translation model. (Cui et al., 2010) focused on
hierarchical rule selection using many features in-
cluding syntax constituents.
These works have demonstrated the benefits of
using syntactic features in the HPB model. How-
ever, high quality syntax parsers are not always
easily obtained for many languages. Without this
problem, word alignment constraints can also be
used to guide the application of the rules.
Suppose that we want to translate the English
sentence into the Chinese sentence in Figure 1, a
translation rule can be applied to the source span
?finish this experiment by tomorrow?. Nonethe-
less, if a rule is applied to ?experiment by?, then
the Chinese translation can not be correctly ob-
tained, because the target span projected from ?ex-
183
periment by? contains words projected from the
source words outside ?experiment by?.
In general, a translation rule projects one con-
tinuous source word sequence (source span) into
one continuous target word sequence. Meanwhile,
the word alignment links between the source and
target sentence define the source spans where
translation rules are applicable. In this paper, we
call a source span that can be covered by a trans-
lation rule without violating word alignment links
a translation span.
Translation spans that have been correctly iden-
tified can guide translation rules to function prop-
erly, thus (Xiong et al., 2010) attempted to use
extra machine learning approaches to determine
boundaries of translation spans. They used two
separate classifiers to learn the beginning and end-
ing boundaries of translation spans, respectively.
A source word is marked as beginning (ending)
boundary if it is the first (last) word of a translation
span. However, a source span whose first and last
words are both boundaries is not always a transla-
tion span. In Figure 1, ?I? is a beginning boundary
since it is the first word of translation span ?I will?
and ?experiment? is an ending boundary since it is
the last word of translation span ?finish this exper-
iment? , but ?I will finish this experiment? is not a
translation span. This happens because the trans-
lation spans are nested or hierarchical. Note that
(He et al., 2010) also learned phrase boundaries to
constrain decoding, but their approach identified
boundaries only for monotone translation.
In this paper, taking fully into account that
translation spans being nested, we propose an
approach to learn hierarchical translation spans
directly from an aligned parallel corpus that
makes more accurate identification over transla-
tion spans.
The rest of the paper is structured as follows:
In Section 2, we briefly review the HPB transla-
tion model. Section 3 describes our approach. We
describe experiments in Section 4 and conclude in
Section 5.
2 Hierarchical Phrase-based Translation
Chiang?s HPB model is based on a weighted
SCFG. A translation rule is like: X ? ??, ?,??,
where X is a nonterminal, ? and ? are source and
target strings of terminals and nonterminals, and?
is a one-to-one correspondence between nontermi-
nals in ? and ?. The weight of each rule is:
w (X ? ??, ?,??) =
?
t
h
t
(X ? ??, ?,??)
?
t
(1)
where h
t
are the features defined on the rules.
Rewriting begins with a pair of linked start sym-
bols and ends when there is no nonterminal left.
Let D be a derivation of the grammar, f (D) and
e (D) be the source and target strings generated
by D. D consists of a set of triples ?r, i, j?, each
of which stands for applying a rule r on a span
f (D)
j
i
. The weight of D is calculated as:
w (D) =
?
?r,i,j??D
w (r)? P
lm
(e)
?
lm
? exp (??
wp
|e|)
(2)
where w (r) is the weight of rule r, the last two
terms represent the language model and word
penalty, respectively.
3 Learning Translation Spans
We will describe how to learn translation spans in
this section.
3.1 Our Model
We make a series of binary classifiers
{C
1
, C
2
, C
3
, ...} to learn if a source span
f (D)
j
i
should be covered by translation rules dur-
ing translation. C
k
is trained and tested on source
spans whose lengths are k, i.e., k = j ? i+ 1.
1
C
k
learns the probability
P
k
(v|f (D) , i, j) (3)
where v ? {0, 1}, v = 1 represents a rule is ap-
plied on f (D)
j
i
, otherwise v = 0.
Training instances for these classifiers are ex-
tracted from an aligned parallel corpus according
to Algorithm 1. For example, ?I will? and ?will
finish? are respectively extracted as positive and
negative instances in Figure 1.
Note that our model in Equation 3 only uses
the source sentence f (D) in the condition. This
means that the probabilities can be calculated be-
fore translation. Therefore, the predicted prob-
abilities can be integrated into the decoder con-
veniently as soft constraints and no extra time is
added during decoding. This enables us to use
rich source contextual features and various ma-
chine learning methods for this learning task.
1
We indeed can utilize just one classifier for all source
spans. However, it will be difficult to design features for such
a classifier unless only boundary word features are adopted.
On the contrary, we can fully take advantage of rich informa-
tion about inside words as we turn to the fixed span length
approach.
184
3.2 Integration into the decoder
It is straightforward to integrate our model into
Equation 2. It is extended as
w (D) =
?
?r,i,j??D
w (r)? P
lm
(e)
?
lm
? exp (??
wp
|e|)
? P
k
(v = 1|f (D) , i, j)
?
k
(4)
where ?
k
is the weight for C
k
.
During decoding, the decoder looks up the
probabilities P
k
calculated and stored before de-
coding.
Algorithm 1 Extract training instances.
Input: A pair of parallel sentence f
n
1
and e
m
1
with
word alignments A.
Output: Training examples for {C
1
, C
2
, C
3
, ...}.
1: for i = 1 to n do
2: for j = i to n do
3: if ?e
q
p
, 1 ? p ? q ? m
& ? (k, t) ? A, i ? k ? j, p ? t ? q
& ? (k, t) ? A, i ? k ? j ? p ? t ? q
then
4: f
j
i
is a positive instance for C
j?i+1
5: else
6: f
j
i
is a negative instance for C
j?i+1
7: end if
8: end for
9: end for
3.3 Classifiers
We compare two machine learning methods for
learning a series of binary classifiers.
For the first method, each C
k
is individually
learned using the maximum entropy (ME) ap-
proach (Berger et al., 1996):
P
k
(v|f (D) , i, j) =
exp
(?
t
?
t
h
t
(v, f (D) , i, j)
)
?
v
?
exp
(?
t
?
t
h
t
(v
?
, f (D) , i, j)
)
(5)
where h
t
is a feature function and ?
t
is weight
of h
t
. We use rich source contextual fea-
tures: unigram, bigram and trigram of the phrase
[f
i?3
, ..., f
j+3
].
As the second method, these classification tasks
are learned in the continuous space using feed-
forward neural networks (NNs). Each C
k
has
the similar structure with the NN language model
(Vaswani et al., 2013). The inputs to the NN are
indices of the words: [f
i?3
, ..., f
j+3
]. Each source
word is projected into an N dimensional vector.
The output layer has two output neurons, whose
values correspond to P
k
(v = 0|f (D) , i, j) and
P
k
(v = 1|f (D) , i, j).
For both ME and NN approaches, words that
occur only once or never occur in the training
corpus are treated as a special word ?UNK? (un-
known) during classifier training and predicting,
which can reduce training time and make the clas-
sifier training more smooth.
4 Experiment
We evaluated the effectiveness of the proposed ap-
proach for Chinese-to-English (CE) and Japanese-
to-English (JE) translation tasks. The datasets of-
ficially provided for the patent machine translation
task at NTCIR-9 (Goto et al., 2011) were used in
our experiments. The detailed training set statis-
tics are given in Table 1. The development and test
SOURCE TARGET
CE
#Sents 954k
#Words 37.2M 40.4M
#Vocab 288k 504k
JE
#Sents 3.14M
#Words 118M 104M
#Vocab 150k 273k
Table 1: Data sets.
sets were both provided for CE task while only the
test set was provided for JE task. Therefore, we
used the sentences from the NTCIR-8 JE test set
as the development set. Word segmentation was
done by BaseSeg (Zhao et al., 2006; Zhao and Kit,
2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao
et al., 2013) for Chinese and Mecab
2
for Japanese.
To learn the classifiers for each translation task,
the training set and development set were put to-
gether to obtain symmetric word alignment us-
ing GIZA++ (Och and Ney, 2003) and the grow-
diag-final-and heuristic (Koehn et al., 2003). The
source span instances extracted from the aligned
training and development sets were used as the
training and validation data for the classifiers.
The toolkit Wapiti (Lavergne et al., 2010) was
adopted to train ME classifiers using the classi-
cal quasi-newton optimization algorithm with lim-
ited memory. The NNs are trained by the toolkit
NPLM (Vaswani et al., 2013). We chose ?recti-
fier? as the activation function and the logarithmic
loss function for NNs. The number of epochs was
set to 20. Other parameters were set to default
2
http://sourceforge.net/projects/mecab/files/
185
Span
length
CE JE
Rate
ME NN
Rate
ME NN
P N P N P N P N
1 2.67 0.93 0.63 0.93 0.64 1.08 0.85 0.79 0.86 0.80
2 1.37 0.83 0.70 0.82 0.75 0.73 0.69 0.84 0.71 0.87
3 0.86 0.70 0.80 0.73 0.83 0.52 0.56 0.89 0.63 0.90
4 0.62 0.57 0.81 0.67 0.88 0.36 0.48 0.93 0.54 0.93
5 0.48 0.52 0.90 0.61 0.91 0.26 0.30 0.96 0.47 0.95
6 0.40 0.47 0.91 0.58 0.92 0.20 0.25 0.97 0.41 0.96
7 0.34 0.40 0.93 0.53 0.93 0.16 0.14 0.98 0.33 0.97
8 0.28 0.35 0.94 0.46 0.94 0.13 0 1 0.32 0.97
9 0.22 0.28 0.96 0.37 0.96 0.10 0 1 0.25 0.98
10 0.15 0.21 0.97 0.28 0.97 0.08 0 1 0.23 0.99
Table 2: Classification accuracies. The Rate column represents ratio of positive instances to negative
instances; the P and N columns give classification accuracies for positive and negative instances.
values. The training time of one classifier on a
12-core 3.47GHz Xeon X5690 machine was 0.5h
(2.5h) using ME (NN) approach for CE task; 1h
(4h) using ME (NN) approach for JE task .
The classification results are shown in Table 2.
Instead of the undifferentiated classification accu-
racy, we present separate classification accuracies
for positive and negative instances. The big differ-
ence between classification accuracies for positive
and negative instances was caused by the unbal-
anced rate of positive and negative instances in the
training corpus. For example, if there are more
positive training instances, then the classifier will
tend to classify new instances as positive and the
classification accuracy for positive instances will
be higher. In our classification tasks, there are less
positive instances for longer span lengths.
Since the word order difference of JE task is
much more significant than that of CE task, there
are more negative Japanese translation span in-
stances than Chinese. In JE tasks, the ME classi-
fiers C
8
, C
9
and C
10
predicted all new instances to
be negative due to the heavily unbalanced instance
distribution.
As shown in Table 2, NN outperformed ME ap-
proach for our classification tasks. As the span
length growing, the advantage of NN became
more significant. Since the classification accura-
cies deceased to be quite low for source spans with
more than 10 words, only {C
1
, ..., C
10
} were inte-
grated into the HPB translation system.
For each translation task, the recent version
of Moses HPB decoder (Koehn et al., 2007)
with the training scripts was used as the base-
line (Base). We used the default parameters for
Moses, and a 5-gram language model was trained
on the target side of the training corpus by IRST
LM Toolkit
3
with improved Kneser-Ney smooth-
ing. {C
1
, ..., C
10
} were integrated into the base-
line with different weights, which were tuned by
MERT (Och, 2003) together with other feature
weights (language model, word penalty,...) under
the log-linear framework (Och and Ney, 2002).
BLEU-n n-gram precisions
Method TER 4 1 2 3 4
CE
Base 49.39- - 33.07- - 69.9/40.7/25.8/16.9
BLM 48.60 33.93 70.0/41.4/26.6/17.6
ME 49.02- 33.63- 70.0/41.2/26.3/17.4
NN 48.09++ 34.35++ 70.1/41.9/27.0/18.0
JE
Base 57.39- - 30.13- - 67.1/38.3/23.0/14.0
BLM 56.79 30.81 67.7/38.9/23.6/14.5
ME 56.48 31.01 67.6/39.0/23.8/14.7
NN 55.96++ 31.77++ 67.8/39.7/24.6/15.4
Table 3: Translation results. The symbol ++ (- -)
represents a significant difference at the p < 0.01
level and - represents a significant difference at the
p < 0.05 level against the BLM.
We compare our method with the baseline and
the boundary learning method (BLM) (Xiong et
al., 2010) based on Maximum Entropy Markov
Models with Markov order 2. Table 3 reports
BLEU (Papineni et al., 2002) and TER (Snover
et al., 2006) scores. Significance tests are con-
ducted using bootstrap sampling (Koehn, 2004).
Our ME classifiers achieve comparable translation
improvement with the BLM and NN classifiers en-
hance translation system significantly compared to
others. Table 3 also shows that the relative gain
was higher for higher n-grams, which is reason-
able since the higher n-grams have higher ambi-
guities in the translation rule application.
It is true that because of multiple parallel sen-
tences, a source span can be applied with transla-
3
http://hlt.fbk.eu/en/irstlm
186
tion rules in one sentence pair but not in another
sentence pair. So we used the probability score
as a feature in the decoding. That is, we did not
use classification results directly but use the prob-
ability score for softly constraining the decoding
process.
5 Conclusion
We have proposed a simple and effective transla-
tion span learning model for HPB translation. Our
model is learned from aligned parallel corpora and
predicts translation spans for source sentence be-
fore translating, which is integrated into the trans-
lation system conveniently as soft constraints. We
compared ME and NN approaches for this learn-
ing task. The results showed that NN classifiers on
the continuous space model achieved both higher
classification accuracies and better translation per-
formance with acceptable training times.
Acknowledgments
Hai Zhao were partially supported by CSC fund
(201304490199), the National Natural Science
Foundation of China (Grant No.60903119, Grant
No.61170114, and Grant No.61272248), the Na-
tional Basic Research Program of China (Grant
No.2013CB329401), the Science and Technol-
ogy Commission of Shanghai Municipality (Grant
No.13511500200), the European Union Seventh
Framework Program (Grant No.247619), and the
art and science interdiscipline funds of Shang-
hai Jiao Tong University, a study on mobilization
mechanism and alerting threshold setting for on-
line community, and media image and psychology
evaluation: a computational intelligence approach.
References
Adam Berger, Vincent Della Pietra, and Stephen Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational linguis-
tics, 22(1):39?71.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270. As-
sociation for Computational Linguistics.
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and
Tiejun Zhao. 2010. A joint rule selection model
for hierarchical phrase-based translation. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 6?11. Association for Computational Linguis-
tics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the ntcir-9 workshop. In
Proceedings of NTCIR, volume 9, pages 559?578.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Learn-
ing phrase boundaries for hierarchical phrase-based
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 383?390. Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.
2010. Practical very large scale crfs. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 504?513, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Lemao Liu, Tiejun Zhao, Chao Wang, and Hailong
Cao. 2011. A unified and discriminative soft syn-
tactic constraint model for hierarchical phrase-based
translation. In the Thirteenth Machine Translation
Summit, pages 253?260. Asia-Pacific Association
for Machine Translation.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In ACL, pages 1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learn-
ing hierarchical translation structure with linguis-
tic annotations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 642?652. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295?302. Association for
Computational Linguistics.
187
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223?231.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387?1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010.
Learning translation boundaries for phrase-based
decoding. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 136?144. Association for Computa-
tional Linguistics.
Hai Zhao and Chunyu Kit. 2008. Exploiting unla-
beled text with different unsupervised segmentation
criteria for chinese word segmentation. Research in
Computing Science, 33:93?104.
Hai Zhao and Chunyu Kit. 2011. Integrating unsu-
pervised and supervised word segmentation: The
role of goodness measures. Information Sciences,
181(1):163?183.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, pages 162?165. Sydney: July.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 9(2):5.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word seg-
mentation for chinese machine translation. In Com-
putational Linguistics and Intelligent Text Process-
ing, pages 248?263. Springer.
188
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189?195,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Neural Network Based Bilingual Language Model Growing
for Statistical Machine Translation
Rui Wang
1,3,?
, Hai Zhao
1,3
, Bao-Liang Lu
1,3
, Masao Utiyama
2
and Eiichro Sumita
2
1
Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering,
Shanghai Jiao Tong University, Shanghai, 200240, China
2
Multilingual Translation Laboratory, MASTAR Project,
National Institute of Information and Communications Technology,
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
3
Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China
wangrui.nlp@gmail.com, {zhaohai, blu}@cs.sjtu.edu.cn,
{mutiyama, eiichiro.sumita}@nict.go.jp
Abstract
Since larger n-gram Language Model
(LM) usually performs better in Statistical
Machine Translation (SMT), how to con-
struct efficient large LM is an important
topic in SMT. However, most of the ex-
isting LM growing methods need an extra
monolingual corpus, where additional LM
adaption technology is necessary. In this
paper, we propose a novel neural network
based bilingual LM growing method, only
using the bilingual parallel corpus in SMT.
The results show that our method can im-
prove both the perplexity score for LM e-
valuation and BLEU score for SMT, and
significantly outperforms the existing LM
growing methods without extra corpus.
1 Introduction
?Language Model (LM) Growing? refers to adding
n-grams outside the corpus together with their
probabilities into the original LM. This operation
is useful as it can make LM perform better through
letting it become larger and larger, by only using a
small training corpus.
There are various methods for adding n-grams
selected by different criteria from a monolingual
corpus (Ristad and Thomas, 1995; Niesler and
Woodland, 1996; Siu and Ostendorf, 2000; Si-
ivola et al., 2007). However, all of these approach-
es need additional corpora. Meanwhile the extra
corpora from different domains will not result in
better LMs (Clarkson and Robinson, 1997; Iyer et
al., 1997; Bellegarda, 2004; Koehn and Schroeder,
?
Part of this work was done as Rui Wang visited in NICT.
2007). In addition, it is very difficult or even im-
possible to collect an extra large corpus for some
special domains such as the TED corpus (Cettolo
et al., 2012) or for some rare languages. There-
fore, to improve the performance of LMs, without
assistance of extra corpus, is one of important re-
search topics in SMT.
Recently, Continues Space Language Model
(CSLM), especially Neural Network based Lan-
guage Model (NNLM) (Bengio et al., 2003;
Schwenk, 2007; Mikolov et al., 2010; Le et al.,
2011), is being actively used in SMT (Schwenk
et al., 2006; Son et al., 2010; Schwenk, 2010;
Schwenk et al., 2012; Son et al., 2012; Niehues
and Waibel, 2012). One of the main advantages
of CSLM is that it can more accurately predic-
t the probabilities of the n-grams, which are not in
the training corpus. However, in practice, CSLM-
s have not been widely used in the current SMT
systems, due to their too high computational cost.
Vaswani and colleagues (2013) propose a
method for reducing the training cost of CSLM
and apply it to SMT decoder. However, they do
not show their improvement for decoding speed,
and their method is still slower than the n-gram
LM. There are several other methods for attempt-
ing to implement neural network based LM or
translation model for SMT (Devlin et al., 2014;
Liu et al., 2014; Auli et al., 2013). However, the
decoding speed using n-gram LM is still state-of-
the-art one. Some approaches calculate the prob-
abilities of the n-grams n-grams before decoding,
and store them in the n-gram format (Wang et al.,
2013a; Arsoy et al., 2013; Arsoy et al., 2014). The
?converted CSLM? can be directly used in SMT.
Though more n-grams which are not in the train-
189
ing corpus can be generated by using some of
these ?converting? methods, these methods only
consider the monolingual information, and do not
take the bilingual information into account.
We observe that the translation output of a
phrase-based SMT system is concatenation of
phrases from the phrase table, whose probabilities
can be calculated by CSLM. Based on this obser-
vation, a novel neural network based bilingual LM
growing method is proposed using the ?connecting
phrases?. The remainder of this paper is organized
as follows: In Section 2, we will review the exist-
ing CSLM converting methods. The new neural
network based bilingual LM growing method will
be proposed in Section 3. In Section 4, the exper-
iments will be conducted and the results will be
analyzed. We will conclude our work in Section
5.
2 Existing CSLM Converting Methods
Traditional Backoff N -gram LMs (BNLMs) have
been widely used in many NLP tasks (Zhang and
Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013;
Zhang et al., 2012; Xu and Zhao, 2012; Wang et
al., 2013b; Jia and Zhao, 2013; Wang et al., 2014).
Recently, CSLMs become popular because they
can obtain more accurate probability estimation.
2.1 Continues Space Language Model
A CSLM implemented in a multi-layer neural net-
work contains four layers: the input layer projects
(first layer) all words in the context h
i
onto the
projection layer (second layer); the hidden layer
(third layer) and the output layer (fourth layer)
achieve the non-liner probability estimation and
calculate the LM probability P (w
i
|h
i
) for the giv-
en context (Schwenk, 2007).
CSLM is able to calculate the probabilities of
all words in the vocabulary of the corpus given the
context. However, due to too high computational
complexity, CSLM is mainly used to calculate the
probabilities of a subset of the whole vocabulary
(Schwenk, 2007). This subset is called a short-
list, which consists of the most frequent words in
the vocabulary. CSLM also calculates the sum of
the probabilities of all words not included in the
short-list by assigning a neuron with the help of
BNLM. The probabilities of other words not in the
short-list are obtained from an BNLM (Schwenk,
2007; Schwenk, 2010; Wang et al., 2013a).
Let w
i
and h
i
be the current word and history,
respectively. CSLM with a BNLM calculates the
probability P (w
i
|h
i
) of w
i
given h
i
, as follows:
P (w
i
|h
i
) =
?
?
?
P
c
(w
i
|h
i
)
?
w?V
0
P
c
(w|h
i
)
P
s
(h
i
) if w
i
? V
0
P
b
(w
i
|h
i
) otherwise
(1)
where V
0
is the short-list, P
c
(?) is the probabil-
ity calculated by CSLM,
?
w?V
0
P
c
(w|h
i
) is the
summary of probabilities of the neuron for all the
words in the short-list, P
b
(?) is the probability cal-
culated by the BNLM, and
P
s
(h
i
) =
?
v?V
0
P
b
(v|h
i
). (2)
We may regard that CSLM redistributes the
probability mass of all words in the short-list,
which is calculated by using the n-gram LM.
2.2 Existing Converting Methods
As baseline systems, our approach proposed in
(Wang et al., 2013a) only re-writes the probabil-
ities from CSLM into the BNLM, so it can only
conduct a convert LM with the same size as the o-
riginal one. The main difference between our pro-
posed method in this paper and our previous ap-
proach is that n-grams outside the corpus are gen-
erated firstly and the probabilities using CSLM are
calculated by using the same method as our previ-
ous approach. That is, the proposed new method
is the same as our previous one when no grown
n-grams are generated.
The method developed by Arsoy and colleagues
(Arsoy et al., 2013; Arsoy et al., 2014) adds al-
l the words in the short-list after the tail word of
the i-grams to construct the (i+1)-grams. For ex-
ample, if the i-gram is ?I want?, then the (i+1)-
grams will be ?I want *?, where ?*? stands for any
word in the short list. Then the probabilities of
the (i+1)-grams are calculated using (i+1)-CSLM.
So a very large intermediate (i+1)-grams will have
to be grown
1
, and then be pruned into smaller
suitable size using an entropy-based LM pruning
method modified from (Stolcke, 1998). The (i+2)-
grams are grown using (i+1)-grams, recursively.
1
In practice, the probabilities of all the target/tail words
in the short list for the history i-grams can be calculated by
the neurons in the output layer at the same time, which will
save some time. According to our experiments, the time cost
for Arsoy?s growing method is around 4 times more than our
proposed method, if the LMs which are 10 times larger than
the original one are grown with other settings all the same.
190
3 Bilingual LM Growing
The translation output of a phrase-based SMT sys-
tem can be regarded as a concatenation of phrases
in the phrase table (except unknown words). This
leads to the following procedure:
Step 1. All the n-grams included in the phrase
table should be maintained at first.
Step 2. The connecting phrases are defined in
the following way.
The w
b
a
is a target language phrase starting from
the a-th word ending with the b-th word, and ?w
b
a
?
is a phrase includingw
b
a
as a part of it, where ? and
? represent any word sequence or none. An i-gram
phrase w
k
1
w
i
k+1
(1 ? k ? i ? 1) is a connecting
phrase
2
, if :
(1) w
k
1
is the right (rear) part of one phrase ?w
k
1
in the phrase table, or
(2) w
i
k+1
is the left (front) part of one phrase
w
i
k+1
? in the phrase table.
After the probabilities are calculated using C-
SLM (Eqs.1 and 2), we combine the n-grams in
the phrase table from Step 1 and the connecting
phrases from Step 2.
3.1 Ranking the Connecting Phrases
Since the size of connecting phrases is too huge
(usually more than one Terabyte), it is necessary
to decide the usefulness of connecting phrases for
SMT. The more useful connecting phrases can be
selected, by ranking the appearing probabilities of
the connecting phrases in SMT decoding.
Each line of a phrase table can be simplified
(without considering other unrelated scores in the
phrase table) as
f ||| e ||| P (e|f), (3)
where the P (e|f) means the translation probabili-
ty from f(source phrase) to e(target phrase),
which can be calculated using bilingual parallel
training data. In decoding, the probability of a tar-
get phrase e appearing in SMT should be
P
t
(e) =
?
f
P
s
(f) ? P (e|f), (4)
2
We are aware that connecting phrases can be applied to
not only two phrases, but also three or more. However the ap-
pearing probabilities (which will be discussed in Eq. 5 of next
subsection) of connecting phrases are approximately estimat-
ed. To estimate and compare probabilities of longer phrases
in different lengths will lead to serious bias, and the experi-
ments also showed using more than two connecting phrases
did not perform well (not shown for limited space), so only
two connecting phrases are applied in this paper.
where the P
s
(f) means the appearing probability
of a source phrase, which can be calculated using
source language part in the bilingual training data.
Using P
t
(e)
3
, we can select the connecting
phrases e with high appearing probabilities as
the n-grams to be added to the original n-
grams. These n-grams are called ?grown n-
grams?. Namely, we build all the connecting
phrases at first, and then we use the appearing
probabilities of the connecting phrases to decide
which connecting phrases should be selected. For
an i-gram connecting phrasew
k
1
w
i
k+1
, wherew
k
1
is
part of ?w
k
1
and w
i
k+1
is part of w
i
k+1
? (the ?w
k
1
and w
i
k+1
? are from the phrase table), the prob-
ability of the connecting phrases can be roughly
estimated as
P
con
(w
k
1
w
i
k+1
) =
i?1
?
k=1
(
?
?
P
t
(?w
k
1
)?
?
?
P
t
(w
i
k+1
?)).
(5)
A threshold for P
con
(w
k
1
w
i
k+1
) is set, and only
the connecting phrases whose appearing probabil-
ities are higher than the threshold will be selected
as the grown n-grams.
3.2 Calculating the Probabilities of Grown
N -grams Using CSLM
To our bilingual LM growing method, a 5-gram
LM and n-gram (n=2,3,4,5) CSLMs are built by
using the target language of the parallel corpus,
and the phrase table is learned from the parallel
corpus.
The probabilities of unigram in the original n-
gram LM will be maintained as they are. The
n-grams from the bilingual phrase table will be
grown by using the ?connecting phrases? method.
As the whole connecting phrases are too huge, we
use the ranking method to select the more useful
connecting phrases. The distribution of different
n-grams (n=2,3,4,5) of the grown LMs are set as
the same as the original LM.
The probabilities of the grown n-grams
(n=2,3,4,5) are calculated using the 2,3,4,5-
CSLM, respectively. If the tail (target) words of
the grown n-grams are not in the short-list of C-
SLM, the P
b
(?) in Eq. 1 will be applied to calcu-
late their probabilities.
3
This P
t
(e) hence provides more bilingual information,
in comparison with using monolingual target LMs only.
191
We combine the n-grams (n=1,2,3,4,5) togeth-
er and re-normalize the probabilities and backof-
f weights of the grown LM. Finally the original
BNLM and the grown LM are interpolated. The
entire process is illustrated in Figure 1.
Corpus
Phrase Table
Grown n-grams 
with Probabilities
Grown LM
Output
Input
Interpolate
Grown n-grams
CSLM
BNLM
Connecting
Phrases
Figure 1: NN based bilingual LM growing.
4 Experiments and Results
4.1 Experiment Setting up
The same setting up of the NTCIR-9 Chinese to
English translation baseline system (Goto et al.,
2011) was followed, only with various LMs to
compare them. The Moses phrase-based SMT
system was applied (Koehn et al., 2007), togeth-
er with GIZA++ (Och and Ney, 2003) for align-
ment and MERT (Och, 2003) for tuning on the de-
velopment data. Fourteen standard SMT features
were used: five translation model scores, one word
penalty score, seven distortion scores, and one LM
score. The translation performance was measured
by the case-insensitive BLEU on the tokenized test
data.
We used the patent data for the Chinese to En-
glish patent translation subtask from the NTCIR-9
patent translation task (Goto et al., 2011). The par-
allel training, development, and test data sets con-
sist of 1 million (M), 2,000, and 2,000 sentences,
respectively.
Using SRILM (Stolcke, 2002; Stolcke et al.,
2011), we trained a 5-gram LM with the interpo-
lated Kneser-Ney smoothing method using the 1M
English training sentences containing 42M words
without cutoff. The 2,3,4,5-CSLMs were trained
on the same 1M training sentences using CSLM
toolkit (Schwenk, 2007; Schwenk, 2010). The set-
tings for CSLMs were: input layer of the same
dimension as vocabulary size (456K), projection
layer of dimension 256 for each word, hidden lay-
er of dimension 384 and output layer (short-list) of
dimension 8192, which were recommended in the
CSLM toolkit and (Wang et al., 2013a)
4
.
4
Arsoy used around 55 M words as the corpus, including
4.2 Results
The experiment results were divided into four
groups: the original BNLMs (BN), the CSLM
Re-ranking (RE), our previous converting (WA),
the Arsoy?s growing, and our growing methods.
For our bilingual LM growing method, 5 bilingual
grown LMs (BI-1 to 5) were conducted in increas-
ing sizes. For the method of Arsoy, 5 grown LMs
(AR-1 to 5) with similar size of BI-1 to 5 were also
conducted, respectively.
For the CSLM re-ranking, we used CSLM to
re-rank the 100-best lists of SMT. Our previous
converted LM, Arsoy?s grown LMs and bilingual
grown LMs were interpolated with the original
BNLMs, using default setting of SRILM
5
. To re-
duce the randomness of MERT, we used twometh-
ods for tuning the weights of different SMT fea-
tures, and two BLEU scores are corresponding to
these twomethods. TheBLEU-s indicated that the
same weights of the BNLM (BN) features were
used for all the SMT systems. The BLEU-i indi-
cated that the MERT was run independently by
three times and the average BLEU scores were
taken.
We also performed the paired bootstrap re-
sampling test (Koehn, 2004)
6
. Two thousands
samples were sampled for each significance test.
The marks at the right of the BLEU score indicated
whether the LMs were significantly better/worse
than the Arsoy?s grown LMs with the same IDs
for SMT (?++/???: significantly better/worse at
? = 0.01, ?+/??: ? = 0.05, no mark: not signif-
icantly better/worse at ? = 0.05).
From the results shown in Table 1, we can get
the following observations:
(1) Nearly all the bilingual grown LMs outper-
formed both BNLM and our previous converted
LM on PPL and BLEU. As the size of grown LM-
s is increased, the PPL always decreased and the
BLEU scores trended to increase. These indicated
that our proposed method can give better probabil-
ity estimation for LM and better performance for
SMT.
(2) In comparison with the grown LMs in Ar-
84K words as vocabulary, and 20K words as short-list. In this
paper, we used the same setting as our previous work, which
covers 92.89% of the frequency of words in the training cor-
pus, for all the baselines and our method for fair comparison.
5
In our previous work, we used the development data to
tune the weights of interpolation. In this paper, we used the
default 0.5 as the interpolation weights for fair comparison.
6
We used the code available at http://www.ark.cs.
cmu.edu/MT
192
Table 1: Performance of the Grown LMs
LMs n-grams PPL BLEU-s BLEU-i ALH
BN 73.9M 108.8 32.19 32.19 3.03
RE N/A 97.5 32.34 32.42 N/A
WA 73.9M 104.4 32.60 32.62 3.03
AR-1 217.6M 103.3 32.55 32.75 3.14
AR-2 323.8M 103.1 32.61 32.64 3.18
AR-3 458.5M 103.0 32.39 32.71 3.20
AR-4 565.6M 102.8 32.67 32.51 3.21
AR-5 712.2M 102.5 32.49 32.60 3.22
BI-1 223.5M 101.9 32.81+ 33.02+ 3.20
BI-2 343.6M 101.0 32.92+ 33.11++ 3.24
BI-3 464.5M 100.6 33.08++ 33.25++ 3.26
BI-4 571.0M 100.3 33.15++ 33.12++ 3.28
BI-5 705.5M 100.1 33.11++ 33.24++ 3.31
soy?s method, our grown LMs obtained better P-
PL and significantly better BLEU with the sim-
ilar size. Furthermore, the improvement of PPL
and BLEU of the existing methods became satu-
rated much more quickly than ours did, as the LMs
grew.
(3) The last column was the Average Length of
the n-grams Hit (ALH) in SMT decoding for dif-
ferent LMs using the following function
ALH =
5
?
i=1
P
i?gram
? i, (6)
where the P
i?gram
means the ratio of the i-grams
hit in SMT decoding. There were also positive
correlations between ALH, PPL and BLEUs. The
ALH of bilingual grown LM was longer than that
of the Arsoy?s grown LM of the similar size. In
another word, less back-off was used for our pro-
posed grown LMs in SMT decoding.
4.3 Experiments on TED Corpus
The TED corpus is in special domain as discussed
in the introduction, where large extra monolingual
corpora are hard to find. In this subsection, we
conducted the SMT experiments on TED corpora
using our proposed LM growing method, to eval-
uate whether our method was adaptable to some
special domains.
We mainly followed the baselines of the IWSLT
2014 evaluation campaign
7
, only with a few mod-
ifications such as the LM toolkits and n-gram or-
der for constructing LMs. The Chinese (CN) to
English (EN) language pair was chosen, using de-
v2010 as development data and test2010 as evalu-
ation data. The same LM growing method was ap-
7
https://wit3.fbk.eu/
plied on TED corpora as on NTCIR corpora. The
results were shown in Table 2.
Table 2: CN-EN TED Experiments
LMs n-grams PPL BLEU-s
BN 7.8M 87.1 12.41
WA 7.8M 85.3 12.73
BI-1 23.1M 79.2 12.92
BI-2 49.7M 78.3 13.16
BI-3 73.4M 77.6 13.24
Table 2 indicated that our proposed LM grow-
ing method improved both PPL and BLEU in com-
parison with both BNLM and our previous CSLM
converting method, so it was suitable for domain
adaptation, which is one of focuses of the current
SMT research.
5 Conclusion
In this paper, we have proposed a neural network
based bilingual LM growing method by using the
bilingual parallel corpus only for SMT. The results
show that our proposed method can improve both
LM and SMT performance, and outperforms the
existing LM growing methods significantly with-
out extra corpus. The connecting phrase-based
method can also be applied to LM adaptation.
Acknowledgments
We appreciate the helpful discussion with Dr.
Isao Goto and Zhongye Jia, and three anony-
mous reviewers for valuable comments and sug-
gestions on our paper. Rui Wang, Hai Zhao
and Bao-Liang Lu were partially supported by
the National Natural Science Foundation of Chi-
na (No. 60903119, No. 61170114, and No.
61272248), the National Basic Research Program
of China (No. 2013CB329401), the Science and
Technology Commission of Shanghai Municipali-
ty (No. 13511500200), the European Union Sev-
enth Framework Program (No. 247619), the Cai
Yuanpei Program (CSC fund 201304490199 and
201304490171), and the art and science interdis-
cipline funds of Shanghai Jiao Tong University
(A study on mobilization mechanism and alerting
threshold setting for online community, and media
image and psychology evaluation: a computation-
al intelligence approach). The corresponding au-
thor of this paper, according to the meaning given
to this role by Shanghai Jiao Tong University, is
Hai Zhao.
193
References
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2013. Converting neural net-
work language models into back-off language mod-
els for efficient decoding in automatic speech recog-
nition. In Proceedings of ICASSP-2013, Vancouver,
Canada, May. IEEE.
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2014. Converting neural net-
work language models into back-off language mod-
els for efficient decoding in automatic speech recog-
nition. IEEE/ACM Transactions on Audio, Speech,
and Language, 22(1):184?192.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
cessings of EMNLP-2013, pages 1044?1054, Seat-
tle, Washington, USA, October. Association for
Computational Linguistics.
Jerome R Bellegarda. 2004. Statistical language mod-
el adaptation: review and perspectives. Speech
Communication, 42(1):93?108. Adaptation Meth-
ods for Speech Recognition.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search (JMLR), 3:1137?1155, March.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit
3
: Web inventory of transcribed
and translated talks. In Proceedings of EAMT-2012,
pages 261?268, Trento, Italy, May.
Philip Clarkson and A.J. Robinson. 1997. Lan-
guage model adaptation using mixtures and an ex-
ponentially decaying cache. In Proceedings of
ICASSP-1997, volume 2, pages 799?802 vol.2, Mu-
nich,Germany.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of ACL-
2014, pages 1370?1380, Baltimore, Maryland, June.
Association for Computational Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the paten-
t machine translation task at the NTCIR-9 work-
shop. In Proceedings of NTCIR-9 Workshop Meet-
ing, pages 559?578, Tokyo, Japan, December.
Rukmini Iyer, Mari Ostendorf, and Herbert Gish.
1997. Using out-of-domain data to improve in-
domain language models. Signal Processing Letter-
s, IEEE, 4(8):221?223.
Zhongye Jia and Hai Zhao. 2013. Kyss 1.0: a
framework for automatic evaluation of chinese input
method engines. In Proceedings of IJCNLP-2013,
pages 1195?1201, Nagoya, Japan, October. Asian
Federation of Natural Language Processing.
Zhongye Jia and Hai Zhao. 2014. A joint graph mod-
el for pinyin-to-chinese conversion with typo cor-
rection. In Proceedings of ACL-2014, pages 1512?
1523, Baltimore, Maryland, June. Association for
Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of ACL-2007 Workshop
on Statistical Machine Translation, pages 224?227,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL-2007, pages 177?180,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of EMNLP-2004, pages 388?395, Barcelona, Spain,
July. Association for Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gau-
vain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceed-
ings of ICASSP-2011, pages 5524?5527, Prague,
Czech Republic, May. IEEE.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proceedings of ACL-2014,
pages 1491?1500, Baltimore, Maryland, June. As-
sociation for Computational Linguistics.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Re-
current neural network based language model. In
Proceedings of INTERSPEECH-2010, pages 1045?
1048.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using restricted boltzman-
n machines. In Proceedings of IWSLT-2012, pages
311?318, Hong Kong.
Thomas Niesler and Phil Woodland. 1996. A variable-
length category-based n-gram language model. In
Proceedings of ICASSP-1996, volume 1, pages 164?
167 vol. 1.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignmen-
t models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of ACL-2003, pages 160?167, Sapporo, Japan, July.
Association for Computational Linguistics.
194
Eric Sven Ristad and Robert G. Thomas. 1995. New
techniques for context modeling. In Proceedings
of ACL-1995, pages 220?227, Cambridge, Mas-
sachusetts. Association for Computational Linguis-
tics.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of
COLING ACL-2006, pages 723?730, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
HLT, WLM ?12, pages 11?19, Montreal, Canada,
June. Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space lan-
guage models. Computer Speech and Language,
21(3):492?518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. The
Prague Bulletin of Mathematical Linguistics, pages
137?146.
Vesa Siivola, Teemu Hirsimki, and Sami Virpioja.
2007. On growing and pruning kneser-ney s-
moothed n-gram models. IEEE Transactions on Au-
dio, Speech, and Language, 15(5):1617?1624.
Manhung Siu and Mari Ostendorf. 2000. Variable n-
grams and extensions for conversational speech lan-
guage modeling. IEEE Transactions on Speech and
Audio, 8(1):63?75.
Le Hai Son, Alexandre Allauzen, Guillaume Wis-
niewski, and Franc?ois Yvon. 2010. Training con-
tinuous space language models: some practical is-
sues. In Proceedings of EMNLP-2010, pages 778?
788, Cambridge, Massachusetts, October. Associa-
tion for Computational Linguistics.
Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of NAACL HLT-
2012, pages 39?48, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Andreas Stolcke, Jing Zheng, Wen Wang, and Vic-
tor Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings of INTERSPEECH 2011,
Waikoloa, HI, USA, December.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270?274, Lansdowne, VA, USA.
Andreas Stolcke. 2002. Srilm-an extensible
language modeling toolkit. In Proceedings of
INTERSPEECH-2002, pages 257?286, Seattle, US-
A, November.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of EMNLP-2013, pages 1387?1392,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumi-
ta, Hai Zhao, and Bao-Liang Lu. 2013a. Convert-
ing continuous-space language models into n-gram
language models for statistical machine translation.
In Proceedings of EMNLP-2013, pages 845?850,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Xiaolin Wang, Hai Zhao, and Bao-Liang Lu. 2013b.
Labeled alignment for recognizing textual entail-
ment. In Proceedings of IJCNLP-2013, pages 605?
613, Nagoya, Japan, October. Asian Federation of
Natural Language Processing.
Xiao-Lin Wang, Yang-Yang Chen, Hai Zhao, and Bao-
Liang Lu. 2014. Parallelized extreme learning ma-
chine ensemble based on minmax modular network.
Neurocomputing, 128(0):31 ? 41.
Qiongkai Xu and Hai Zhao. 2012. Using deep lin-
guistic features for finding deceptive opinion spam.
In Proceedings of COLING-2012, pages 1341?1350,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Jingyi Zhang and Hai Zhao. 2013. Improving function
word alignment with frequency and syntactic infor-
mation. In Proceedings of IJCAI-2013, pages 2211?
2217. AAAI Press.
Xiaotian Zhang, Hai Zhao, and Cong Hui. 2012.
A machine learning approach to convert CCGbank
to Penn treebank. In Proceedings of COLING-
2012, pages 535?542, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word
segmentation for chinese machine translation. In
Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, volume 7817 of
Lecture Notes in Computer Science, pages 248?263.
Springer Berlin Heidelberg.
195
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1654?1664,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Refining Word Segmentation Using a Manually Aligned Corpus
for Statistical Machine Translation
Xiaolin Wang Masao Utiyama Andrew Finch Eiichiro Sumita
National Institute of Information and Communications Technology
{xiaolin.wang,mutiyama,andrew.finch,eiichiro.sumita}@nict.go.jp
Abstract
Languages that have no explicit word de-
limiters often have to be segmented for sta-
tistical machine translation (SMT). This is
commonly performed by automated seg-
menters trained on manually annotated
corpora. However, the word segmentation
(WS) schemes of these annotated corpora
are handcrafted for general usage, and
may not be suitable for SMT. An analysis
was performed to test this hypothesis us-
ing a manually annotated word alignment
(WA) corpus for Chinese-English SMT.
An analysis revealed that 74.60% of the
sentences in the WA corpus if segmented
using an automated segmenter trained on
the Penn Chinese Treebank (CTB) will
contain conflicts with the gold WA an-
notations. We formulated an approach
based on word splitting with reference to
the annotated WA to alleviate these con-
flicts. Experimental results show that the
refined WS reduced word alignment error
rate by 6.82% and achieved the highest
BLEU improvement (0.63 on average) on
the Chinese-English open machine trans-
lation (OpenMT) corpora compared to re-
lated work.
1 Introduction
Word segmentation is a prerequisite for many
natural language processing (NLP) applications
on those languages that have no explicit space
between words, such as Arabic, Chinese and
Japanese. As the first processing step, WS affects
all successive steps, thus it has a large potential
impact on the final performance. For SMT, the
unsupervised WA, building translation models and
reordering models, and decoding are all based on
segmented words.
Automated word segmenters built through
supervised-learning methods, after decades of in-
tensive research, have emerged as effective so-
lutions to WS tasks and become widely used in
many NLP applications. For example, the Stan-
ford word segmenter (Xue et al., 2002)1 which is
based on conditional random field (CRF) is em-
ployed to prepare the official corpus for NTCIR-
9 Chinese-English patent translation task (Goto et
al., 2011).
However, one problem with applying these
supervised-learning word segmenters to SMT is
that the WS scheme of annotating the training cor-
pus may not be optimal for SMT. (Chang et al.,
2008) noticed that the words in CTB are often too
long for SMT. For example, a full Chinese per-
sonal name which consists of a family name and a
given name is always taken as a single word, but
its counterpart in English is usually two words.
Manually WA corpora are precious resources
for SMT research, but they used to be only avail-
able in small volumes due to the production cost.
For example, (Och and Ney, 2000) initially an-
notated 447 English-French sentence pairs, which
later became the test data set in ACL 2003 shared
task on word alignment (Mihalcea and Pedersen,
2003), and was used frequently thereafter (Liang
et al., 2006; DeNero and Klein, 2007; Haghighi et
al., 2009)
For Chinese and English, the shortage of man-
ually WA corpora has recently been relieved
by the linguistic data consortium (LDC) 2
GALE Chinese-English word alignment and tag-
ging training corpus (the GALE WA corpus)3.
The corpus is considerably large, containing 4,735
documents, 18,507 sentence pairs, 620,189 Chi-
nese tokens, 518,137 English words, and 421,763
1http://nlp.stanford.edu/software/
segmenter.shtml
2http://catalog.ldc.upenn.edu
3Catalog numbers: LDC2012T16, LDC2012T20,
LDC2012T24 and LDC2013T05.
1654
alignment annotations. The corpus carries no Chi-
nese WS annotation, and the WA annotation was
performed between Chinese characters and En-
glish words. The alignment identifies minimum
translation units and relations 4, referred as atomic
blocks and atomic edges, respectively, in this pa-
per. Figure 1 shows an example that contains six
atomic edges.
Visual inspection of the segmentation of an au-
tomatic segmenter with reference to a WA cor-
pus revealed a number of inconsistencies. For ex-
ample, consider the word ?bao fa? in Figure 1.
Empirically we observed that this word is seg-
mented as a single token by an automatic seg-
menter trained on the CTB, however, this segmen-
tation differs with the alignment in the WA cor-
pus, since its two components are aligned to two
different English words. Our hypothesis was that
the removal of these inconsistencies would benefit
machine translation performance (this is explained
further in Section 2.3), and we explored this idea
in this work.
This paper focuses on optimizing Chinese WS
for Chinese-English SMT, but both the research
method and the proposed solution are language-
independent. They can be applied to other lan-
guage pairs.
The major contributions of this paper include,
? analyze the CTB WS scheme for Chinese-
English SMT;
? propose a lexical word splitter to refine the
WS;
? achieve a BLEU improvement over a baseline
Stanford word segmenter, and a state-of-the-
art extension, on Chinese-English OpenMT
corpora.
The rest of this paper is organized as follows:
first, Section 2 analyzes WS using a WA corpus;
next, Section 3 proposes a lexical word splitter
to refine WS; then, Section 4 evaluates the pro-
posed method on end-to-end SMT as well as word
segmentation and alignment; after that, Section 5
compares this work to related work; finally, Sec-
tion 6 concludes this paper.
4Guidelines for Chinese-English Word Align-
ment(Version 4.0)
2 Analysis of a General-purpose
Automatic Word Segmenter
This section first briefly describes the GALE WA
corpus, then presents an analysis of the WS arising
from a CTB-standard word segmenter with refer-
ence to the segmentation of the atomic blocks in
the GALE WA corpus, finally the impact of the
findings on SMT is discussed.
2.1 GALE WA corpus
The GALE WA corpus was developed by the
LDC, and was used as training data in the DARPA
GALE global autonomous language exploitation
program 5. The corpus incorporates linguistic
knowledge into word aligned text to help improve
automatic WA and translation quality. It em-
ploys two annotation schemes: alignment and tag-
ging (Li et al., 2010). Alignment identifies min-
imum translation units and translation relations;
tagging adds contextual, syntactic and language-
specific features to the alignment annotation. For
example, the sample shown in Figure 1 carries tags
on both alignment edges and tokens.
The GALE WA corpus contains 18,057 man-
ually word aligned Chinese and English parallel
sentences which are extracted from newswire and
web blogs. Table 1 presents the statistics on the
corpus. One third of the sentences are approxi-
mately newswire text, and the remainder consists
of web blogs.
2.2 Analysis of WS
In order to produce a Chinese word segmenta-
tion consistent with the CTB standard we used the
Stanford Chinese word segmenter with a model
trained on the CTB corpus. We will refer to this
as the ?CTB segmenter? in the rest of this paper.
The Chinese sentences in the GALE WA cor-
pus were first segmented by the CTB segmenter,
and the predicted words were compared against
the atomic blocks with respect to the granularity of
segmentation. The analysis falls into the following
three categories, two of which may be potentially
harmful to SMT:
? Fully consistent: the word locates within the
block of one atomic alignment edge. For ex-
ample, in Figure 2(a), the Chinese text has
5https://catalog.ldc.upenn.edu/
LDC2012T16
1655
 	
 
 
        	 
  
     
	
 	
 
       	 
	  
Figure 1: Example from the GALE WA corpus. Each line arrow represents an atomic edge, and each box
represents an atomic block. SEM (semantic), GIS (grammatically inferred semantic) and FUN (function)
are tags of edges. INC (not translated), TOI (to-infinitive) and DET (determiner) are tags of tokens.
Genre # Files # Sentences? # CN tokens # EN tokens # Alignment edges
Newswire 2,175 6,218 246,371 205,281 164,033
Web blog 2,560 11,839 373,818 312,856 257,730
Total 4,735 18,057 620,189 518,137 421,763
Table 1: GALE WA corpus. ? Sentences rejected by the annotators are excluded.
four atomic blocks; the CTB segmenter pro-
duces five words which all locate within the
blocks, so they are all small enough.
? Alignment inconsistent: the word aligns to
more than one atomic block, but the target
expression is contiguous, allowing for cor-
rect phrase pair extraction (Zens et al., 2002).
For example, in Figure 2(b), the characters in
the word ?shuang fang?, which is produced
by the CTB segmenter, contains two atomic
blocks, but the span of the target ?to both
side? is continuous, therefore the phrase pair
?shuang fang ||| to both sides? can be ex-
tracted.
? Alignment inconsistent and extraction hin-
dered: the word aligned to more than one
atomic block, and the target expression is not
contiguous, which hinders correct phrase pair
extractions. For example, in Figure 2(c), the
word ?zeng chan? has to be split in order to
match the target language.
Table 2 shows the statistics of the three cat-
egories of CTB WS on the GALE WA corpus.
90.74% of the words are fully consistent, while the
remaining 9.26% of the words have inconsistent
alignments. 74.60% of the sentences contain this
problem. The category with inconsistent align-
ment and extraction hindered only accounts for
0.46% of the words, affecting 9.06% of the sen-
tences.
2.3 Impact of WS on SMT
The word alignment has a direct impact on the na-
ture of both the translation model, and lexical re-
ordering model in a phrase-base SMT system. The
words in last two categories are all longer than an
atomic block, which might lead to problems in the
word alignment in two ways:
? First, longer words tend to be more sparse in
the training corpus, thus the estimated distri-
bution of their target phrases are less accu-
rate.
? Second, the alignment from them to target
sides are one-to-many, which is much more
complicated and requires fertilized alignment
models such as IBM model 4 ? 6 (Och and
Ney, 2000).
The words in the category of ?fully consistent?
can be aligned using simple models, because the
alignment from them to the target side are one-to-
one or many-to-one, and simple alignment models
such as IBM model 1, IBM model 2 and HMM
model are sufficient (Och and Ney, 2000).
3 Refining the Word Segmentation
In the last subsection, it was shown that 74.60% of
parallel sentences were affected by issues related
to under-segmentation of the corpus. Our hypoth-
esis is that if these words are split into pieces that
match English words, the accuracy of the unsuper-
vised WA as well as the translation quality will be
improved. To achieve this, we adopt a splitting
1656
	
						
   	
 	  
(a)
	



	






    

  	 	    
(b)
	
 
 	 	
	
     	

	 	 	  
(c)
Figure 2: Examples of automated WS on manually WA corpus: (a) Fully consistent; (b) Alignment
inconsistent; (c) Alignment inconsistent and extraction hindered. The Chinese words separated by white
space are the output of the CTB segmenter. Arrows represent the alignment of atomic blocks. Note that
?shuang fang? and ?zeng chan? are words produced by the CTB segmenter, but consist of two atomic
blocks.
Category Count Word Ratio Sentence Ratio
Fully consistent 355,702 90.74% 25.40%?
Alignment inconsistent 34,464 8.81% 65.54%
Alignment inconsistent & extraction hindered 1,830 0.46% 9.06%
Sum of conflict ? 36,294 9.26% 74.60%
Table 2: CTB WS on GALE WA corpus: ? All words are fully consistent; ? Alignment inconsistent plus
alignment inconsistent & extraction hindered
strategy, based on a supervised learning approach,
to re-segment the corpus. This subsection first for-
malizes the task, and then presents the approach.
3.1 Word splitting task
The word splitting task is formalized as a sequence
labeling task as follows: each word (represented
by a sequence of characters x = x
1
. . . x
T
where
T is the length of sample) produced by the CTB
segmenter is a sample, and a corresponding se-
quence of binary boundary labels y = y
1
. . . y
T
is the learning target,
y
t
=
?
?
?
1 if there is a split point
between c
t
and c
t?1
;
0 otherwise.
(1)
The sequence of boundary labels is derived
from the gold WA annotation as follows: for a
sequence of two atomic blocks, where the first
character of the second block is x
t
, then the la-



	



	




 	
 
Figure 3: Samples of word splitting task
bel y
t
= 1. Figure 3 presents several samples ex-
tracted from the examples in Figure 2.
Each word sample may have no split point, one
split point or multiple split points, depending on
the gold WA annotation. Table 3 shows the statis-
tics of the word splitting data set which is built
from the GALE manual WA corpus and the CTB
segmenter?s output, where 2000 randomly sam-
pled sentences are taken as a held-out test set.
1657
Set # Sentences # Samples # Split points # Split points per sample
Train. 16,057 348,086 32,337 0.0929
Test 2,000 43,910 3,929 0.0895
Table 3: Data set for learning the word splitting
3.2 CRF approach
This paper employs a condition random field
(CRF) to solve this sequence labeling task (Laf-
ferty et al., 2001). A linear-chain CRF defines the
conditional probability of y given x as,
P
?
(y|x) =
1
Z
x
(
T
?
t=1
?
k
?
k
f
k
(y
t?1
, y
t
,x, t)),
(2)
where ? = {?
1
, . . .} are parameters, Z
x
is a per-
input normalization that makes the probability of
all state sequences sum to one; f
k
(y
t?1
, y
t
,x, t) is
a feature function which is often a binary-valued
sparse feature. The training of CRF model is to
maximize the likelihood of training data together
with a regularization penalty to avoid over-fitting
as (Peng et al., 2004; Peng and McCallum, 2006),
?
?
= argmax
?
(
?
i
logP
?
(y
i
|x
i
) ?
?
k
?
2
k
2?
2
k
),
(3)
where (x,y) are training samples; the hyperparam-
eter ?
k
can be understood as the variance of the
prior distribution of ?
k
. When predicting the la-
bels of test samples, the CRF decoder searches for
the optimal label sequence y? that maximizes the
conditional probability,
y
?
= argmax
y
P
?
(y|x). (4)
In (Chang et al., 2008) a method is proposed to
select an appropriate level of segmentation gran-
ularity (in practical terms, to encourage smaller
segments). We call their method ?length tuner?.
The following artificial feature is introduced into
the learned CRF model:
f
0
(x, y
t?1
, y
t
, 1) =
{
1 if y
t
= +1
0 otherwise
(5)
The weight ?
0
of this feature is set by hand to
bias the output of CRF model. By way of expla-
nation, a very large positive ?
0
will cause every
character to be segmented, or conversely a very
large negative ?
0
will inhibit the output of segmen-
tation boundaries. In their experiments, ?
0
= 2
was used to force a CRF segmenter to adopt an in-
termediate granularity between character and the
CTB WS scheme. Compared to the length tuner,
our proposed method exploits lexical knowledge
about word splitting, and we will therefore refer to
it as the ?lexical word splitter? or ?lexical splitter?
for short.
3.3 Feature Set
The features f
k
(y
t?1
, y
t
,x, t) we used include the
WS features from the Chinese Stanford word seg-
menter and a set of extended features described
below. The WS features are included because the
target split points may share some common char-
acteristics with the boundaries in the CTB WS
scheme.
The extended features consists of four types ?
named entities, word frequency, word length and
character-level unsupervised WA. For each type of
the feature, the value and value concatenated with
previous or current character are taken as sparse
features (see Table 4 for details). The real val-
ues of word frequency, word length and character-
level unsupervised WA are converted into sparse
features due to the routine of CRF model.
The character-level unsupervised alignment
feature is inspired by the related works of unsu-
pervised bilingual WS (Xu et al., 2008; Chung and
Gildea, 2009; Nguyen et al., 2010; Michael et al.,
2011). The idea is that the character-level WA can
approximately capture the counterpart English ex-
pression of each Chinese token, and source tokens
aligned to different target expressions should be
split into different words (see Figure 4 for an illus-
tration).
The values of the character-level alignment fea-
tures are obtained through building a dictionary.
First, unsupervised WA is performed on the SMT
training corpus where the Chinese sentences are
treated as sequences of characters; then, the Chi-
nese sentences are segmented by CTB segmenter
and a dictionary of segmented words are built; fi-
nally, for each word in the dictionary, the relative
frequency of being split at a certain position is cal-
1658
Feature Definition Example
NE NE tag of current word Geography:NE
NE-C
?1
NE concatenated with previous character Geo.-ding:NE-C
?1
NE-C
0
NE concatenated with current character Geo.-mei:NE-C
0
Frequency Nearest integer of negative logarithm of word frequency 5?:Freq
Freq.-C
?1
Frequency concatenated with previous character 5-ding:Freq-C
?1
Freq.-C
0
Frequency concatenated with current character 5-mei:Freq-C
0
Length Length of current word (1,2,3,4,5,6,7 or >7) 4:Len
Len.-Position Length concatenated with the position 4-2:Len-Pos
Len.-C
?1
Length concatenated with previous character 4-ding:Len-C
?1
Len.-C
0
Length concatenated with current character 4-mei:Len-C
0
Char. Align. Five-level relative frequency of being split 0.4?:CA
C.A.-C
?1
C.A. concatenated with previous character 0.4-ding:CA-C
?1
C.A.-C
0
C.A. concatenated with current character 0.4-mei:CA-C
0
Table 4: Extended features used in the CRF model for word splitting. The example shows the features
used in the decision whether to split the Chinese word ?la ding mei zhou? (Latin America, the first
four Chinese characters in Figure 4) after the second Chinese character. ? Round(-log
10
(0.00019)); ?
Round(0.43 ? 5 ) / 5
	

	
        	 

	      
Figure 4: Illustration of character-level unsuper-
vised alignment features. The dotted lines are
word boundaries suggested by the alignment.
culated as,
f
CA
(w, i) =
n
i
n
w
(6)
where w is a word, i is a splitting position (from
1 to the length of w minus 1); n
i
is the number of
times the words as split at position i according to
the character-level alignment, that is, the character
before and after i are aligned to different English
expressions; n
w
is occurrence count of word w in
the training corpus.
4 Experiments
In the last section we found that 9.26% of words
produced by the CTB segmenter have the poten-
tial to cause problems for SMT, and propose a
lexical word splitter to address this issue through
segmentation refinement. This section contains
experiments designed to empirically evaluate the
proposed lexical word splitter in three aspects:
first, whether the WS accuracy is improved; sec-
ond, whether the accuracy of the unsupervised WA
during training SMT systems is improved; third,
whether the end-to-end translation quality is im-
proved.
This section first describes the experimental
methodology, then presents the experimental re-
sults, and finally illustrates the operation of our
proposed method using a real example.
4.1 Experimental Methodology
4.1.1 Experimental Corpora
The GALE manual WA corpus and the Chinese to
English corpus from the shared task of the NIST
open machine translation (OpenMT) 2006 evalua-
tion 6 were employed as the experimental corpus
(Table 5).
The experimental corpus for WS was con-
structed by first segmenting 2000 held out sen-
tences from the GALE manual WA corpus with
the Stanford segmenter, and then refining the seg-
mentation with the gold alignment annotation. For
example, the gold segmentation for the examples
in Figure 2 is presented in Figure 5. Note that
this test corpus is intended to represent an oracle
segmentation for our proposed method, and serves
primarily to gauge the improvement of our method
over the baseline Stanford segmenter, relative to
an upper bound.
6http://www.itl.nist.gov/iad/mig/
tests/mt/2006/
1659
    	

     
   
 Proceedings of the ACL 2010 Conference Short Papers, pages 1?5,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Paraphrase Lattice for Statistical Machine Translation
Takashi Onishi and Masao Utiyama and Eiichiro Sumita
Language Translation Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, JAPAN
{takashi.onishi,mutiyama,eiichiro.sumita}@nict.go.jp
Abstract
Lattice decoding in statistical machine
translation (SMT) is useful in speech
translation and in the translation of Ger-
man because it can handle input ambigu-
ities such as speech recognition ambigui-
ties and German word segmentation ambi-
guities. We show that lattice decoding is
also useful for handling input variations.
Given an input sentence, we build a lattice
which represents paraphrases of the input
sentence. We call this a paraphrase lattice.
Then, we give the paraphrase lattice as an
input to the lattice decoder. The decoder
selects the best path for decoding. Us-
ing these paraphrase lattices as inputs, we
obtained significant gains in BLEU scores
for IWSLT and Europarl datasets.
1 Introduction
Lattice decoding in SMT is useful in speech trans-
lation and in the translation of German (Bertoldi
et al, 2007; Dyer, 2009). In speech translation,
by using lattices that represent not only 1-best re-
sult but also other possibilities of speech recogni-
tion, we can take into account the ambiguities of
speech recognition. Thus, the translation quality
for lattice inputs is better than the quality for 1-
best inputs.
In this paper, we show that lattice decoding is
also useful for handling input variations. ?Input
variations? refers to the differences of input texts
with the same meaning. For example, ?Is there
a beauty salon?? and ?Is there a beauty par-
lor?? have the same meaning with variations in
?beauty salon? and ?beauty parlor?. Since these
variations are frequently found in natural language
texts, a mismatch of the expressions in source sen-
tences and the expressions in training corpus leads
to a decrease in translation quality. Therefore,
we propose a novel method that can handle in-
put variations using paraphrases and lattice decod-
ing. In the proposed method, we regard a given
source sentence as one of many variations (1-best).
Given an input sentence, we build a paraphrase lat-
tice which represents paraphrases of the input sen-
tence. Then, we give the paraphrase lattice as an
input to the Moses decoder (Koehn et al, 2007).
Moses selects the best path for decoding. By using
paraphrases of source sentences, we can translate
expressions which are not found in a training cor-
pus on the condition that paraphrases of them are
found in the training corpus. Moreover, by using
lattice decoding, we can employ the source-side
language model as a decoding feature. Since this
feature is affected by the source-side context, the
decoder can choose a proper paraphrase and trans-
late correctly.
This paper is organized as follows: Related
works on lattice decoding and paraphrasing are
presented in Section 2. The proposed method is
described in Section 3. Experimental results for
IWSLT and Europarl dataset are presented in Sec-
tion 4. Finally, the paper is concluded with a sum-
mary and a few directions for future work in Sec-
tion 5.
2 Related Work
Lattice decoding has been used to handle ambigu-
ities of preprocessing. Bertoldi et al (2007) em-
ployed a confusion network, which is a kind of lat-
tice and represents speech recognition hypotheses
in speech translation. Dyer (2009) also employed
a segmentation lattice, which represents ambigui-
ties of compound word segmentation in German,
Hungarian and Turkish translation. However, to
the best of our knowledge, there is no work which
employed a lattice representing paraphrases of an
input sentence.
On the other hand, paraphrasing has been used
to enrich the SMT model. Callison-Burch et
1
Input sentence 
Paraphrase Lattice
Output sentence 
Paraphrase
List
SMT model
Parallel Corpus
(for paraphrase)
Parallel Corpus
(for training)
Paraphrasing
Lattice Decoding
Figure 1: Overview of the proposed method.
al. (2006) and Marton et al (2009) augmented
the translation phrase table with paraphrases to
translate unknown phrases. Bond et al (2008)
and Nakov (2008) augmented the training data by
paraphrasing. However, there is no work which
augments input sentences by paraphrasing and
represents them in lattices.
3 Paraphrase Lattice for SMT
Overview of the proposed method is shown in Fig-
ure 1. In advance, we automatically acquire a
paraphrase list from a parallel corpus. In order to
acquire paraphrases of unknown phrases, this par-
allel corpus is different from the parallel corpus
for training.
Given an input sentence, we build a lattice
which represents paraphrases of the input sentence
using the paraphrase list. We call this lattice a
paraphrase lattice. Then, we give the paraphrase
lattice to the lattice decoder.
3.1 Acquiring the paraphrase list
We acquire a paraphrase list using Bannard and
Callison-Burch (2005)?s method. Their idea is, if
two different phrases e1, e2 in one language are
aligned to the same phrase c in another language,
they are hypothesized to be paraphrases of each
other. Our paraphrase list is acquired in the same
way.
The procedure is as follows:
1. Build a phrase table.
Build a phrase table from parallel corpus us-
ing standard SMT techniques.
2. Filter the phrase table by the sigtest-filter.
The phrase table built in 1 has many inappro-
priate phrase pairs. Therefore, we filter the
phrase table and keep only appropriate phrase
pairs using the sigtest-filter (Johnson et al,
2007).
3. Calculate the paraphrase probability.
Calculate the paraphrase probability p(e2|e1)
if e2 is hypothesized to be a paraphrase of e1.
p(e2|e1) =
?
c
P (c|e1)P (e2|c)
where P (?|?) is phrase translation probability.
4. Acquire a paraphrase pair.
Acquire (e1, e2) as a paraphrase pair if
p(e2|e1) > p(e1|e1). The purpose of this
threshold is to keep highly-accurate para-
phrase pairs. In experiments, more than 80%
of paraphrase pairs were eliminated by this
threshold.
3.2 Building paraphrase lattice
An input sentence is paraphrased using the para-
phrase list and transformed into a paraphrase lat-
tice. The paraphrase lattice is a lattice which rep-
resents paraphrases of the input sentence. An ex-
ample of a paraphrase lattice is shown in Figure 2.
In this example, an input sentence is ?is there a
beauty salon ??. This paraphrase lattice contains
two paraphrase pairs ?beauty salon? = ?beauty
parlor? and ?beauty salon? = ?salon?, and rep-
resents following three sentences.
? is there a beauty salon ?
? is there a beauty parlor ?
? is there a salon ?
In the paraphrase lattice, each node consists of
a token, the distance to the next node and features
for lattice decoding. We use following four fea-
tures for lattice decoding.
? Paraphrase probability (p)
A paraphrase probability p(e2|e1) calculated
when acquiring the paraphrase.
hp = p(e2|e1)
? Language model score (l)
A ratio between the language model proba-
bility of the paraphrased sentence (para) and
that of the original sentence (orig).
hl = lm(para)lm(orig)
2
0 -- ("is"     , 1, 1, 1, 1)
1 -- ("there"  , 1, 1, 1, 1)
2 -- ("a"      , 1, 1, 1, 1)
3 -- ("beauty" , 1, 1, 1, 2) ("beauty" , 0.250, 1.172, 1, 1) ("salon" , 0.133, 0.537, 0.367, 3)
4 -- ("parlor" , 1, 1, 1, 2)
5 -- ("salon"  , 1, 1, 1, 1)
6 -- ("?"      , 1, 1, 1, 1)
Paraphrase probability (p)
Language model score (l)
Paraphrase length (d)
Distance to the next node Features for lattice decodingToken
Figure 2: An example of a paraphrase lattice, which contains three features of (p, l, d).
? Normalized language model score (L)
A language model score where the language
model probability is normalized by the sen-
tence length. The sentence length is calcu-
lated as the number of tokens.
hL = LM(para)LM(orig) ,
where LM(sent) = lm(sent)
1
length(sent)
? Paraphrase length (d)
The difference between the original sentence
length and the paraphrased sentence length.
hd = exp(length(para)? length(orig))
The values of these features are calculated only
if the node is the first node of the paraphrase, for
example the second ?beauty? and ?salon? in line
3 of Figure 2. In other nodes, for example ?par-
lor? in line 4 and original nodes, we use 1 as the
values of features.
The features related to the language model, such
as (l) and (L), are affected by the context of source
sentences even if the same paraphrase pair is ap-
plied. As these features can penalize paraphrases
which are not appropriate to the context, appropri-
ate paraphrases are chosen and appropriate trans-
lations are output in lattice decoding. The features
related to the sentence length, such as (L) and (d),
are added to penalize the language model score
in case the paraphrased sentence length is shorter
than the original sentence length and the language
model score is unreasonably low.
In experiments, we use four combinations of
these features, (p), (p, l), (p, L) and (p, l, d).
3.3 Lattice decoding
We use Moses (Koehn et al, 2007) as a decoder
for lattice decoding. Moses is an open source
SMT system which allows lattice decoding. In
lattice decoding, Moses selects the best path and
the best translation according to features added in
each node and other SMT features. These weights
are optimized using Minimum Error Rate Training
(MERT) (Och, 2003).
4 Experiments
In order to evaluate the proposed method, we
conducted English-to-Japanese and English-to-
Chinese translation experiments using IWSLT
2007 (Fordyce, 2007) dataset. This dataset con-
tains EJ and EC parallel corpus for the travel
domain and consists of 40k sentences for train-
ing and about 500 sentences sets (dev1, dev2
and dev3) for development and testing. We used
the dev1 set for parameter tuning, the dev2 set
for choosing the setting of the proposed method,
which is described below, and the dev3 set for test-
ing.
The English-English paraphrase list was ac-
quired from the EC corpus for EJ translation and
53K pairs were acquired. Similarly, 47K pairs
were acquired from the EJ corpus for EC trans-
lation.
4.1 Baseline
As baselines, we used Moses and Callison-Burch
et al (2006)?s method (hereafter CCB). In Moses,
we used default settings without paraphrases. In
CCB, we paraphrased the phrase table using the
automatically acquired paraphrase list. Then,
we augmented the phrase table with paraphrased
phrases which were not found in the original
phrase table. Moreover, we used an additional fea-
ture whose value was the paraphrase probability
(p) if the entry was generated by paraphrasing and
3
Moses (w/o Paraphrases) CCB Proposed Method
EJ 38.98 39.24 (+0.26) 40.34 (+1.36)
EC 25.11 26.14 (+1.03) 27.06 (+1.95)
Table 1: Experimental results for IWSLT (%BLEU).
1 if otherwise. Weights of the feature and other
features in SMT were optimized using MERT.
4.2 Proposed method
In the proposed method, we conducted experi-
ments with various settings for paraphrasing and
lattice decoding. Then, we chose the best setting
according to the result of the dev2 set.
4.2.1 Limitation of paraphrasing
As the paraphrase list was automatically ac-
quired, there were many erroneous paraphrase
pairs. Building paraphrase lattices with all erro-
neous paraphrase pairs and decoding these para-
phrase lattices caused high computational com-
plexity. Therefore, we limited the number of para-
phrasing per phrase and per sentence. The number
of paraphrasing per phrase was limited to three and
the number of paraphrasing per sentence was lim-
ited to twice the size of the sentence length.
As a criterion for limiting the number of para-
phrasing, we use three features (p), (l) and (L),
which are same as the features described in Sub-
section 3.2. When building paraphrase lattices, we
apply paraphrases in descending order of the value
of the criterion.
4.2.2 Finding optimal settings
As previously mentioned, we have three choices
for the criterion for building paraphrase lattices
and four combinations of features for lattice de-
coding. Thus, there are 3 ? 4 = 12 combinations
of these settings. We conducted parameter tuning
with the dev1 set for each setting and used as best
the setting which got the highest BLEU score for
the dev2 set.
4.3 Results
The experimental results are shown in Table 1. We
used the case-insensitive BLEU metric for eval-
uation. In EJ translation, the proposed method
obtained the highest score of 40.34%, which
achieved an absolute improvement of 1.36 BLEU
points over Moses and 1.10 BLEU points over
CCB. In EC translation, the proposed method also
obtained the highest score of 27.06% and achieved
an absolute improvement of 1.95 BLEU points
over Moses and 0.92 BLEU points over CCB. As
the relation of three systems is Moses < CCB <
Proposed Method, paraphrasing is useful for SMT
and using paraphrase lattices and lattice decod-
ing is especially more useful than augmenting the
phrase table. In ProposedMethod, the criterion for
building paraphrase lattices and the combination
of features for lattice decoding were (p) and (p, L)
in EJ translation and (L) and (p, l) in EC transla-
tion. Since features related to the source-side lan-
guage model were chosen in each direction, using
the source-side language model is useful for de-
coding paraphrase lattices.
We also tried a combination of Proposed
Method and CCB, which is a method of decoding
paraphrase lattices with an augmented phrase ta-
ble. However, the result showed no significant im-
provements. This is because the proposed method
includes the effect of augmenting the phrase table.
Moreover, we conducted German-English
translation using the Europarl corpus (Koehn,
2005). We used the WMT08 dataset1, which
consists of 1M sentences for training and 2K sen-
tences for development and testing. We acquired
5.3M pairs of German-German paraphrases from
a 1M German-Spanish parallel corpus. We con-
ducted experiments with various sizes of training
corpus, using 10K, 20K, 40K, 80K, 160K and 1M.
Figure 3 shows the proposed method consistently
get higher score than Moses and CCB.
5 Conclusion
This paper has proposed a novel method for trans-
forming a source sentence into a paraphrase lattice
and applying lattice decoding. Since our method
can employ source-side language models as a de-
coding feature, the decoder can choose proper
paraphrases and translate properly. The exper-
imental results showed significant gains for the
IWSLT and Europarl dataset. In IWSLT dataset,
we obtained 1.36 BLEU points over Moses in EJ
translation and 1.95 BLEU points over Moses in
1http://www.statmt.org/wmt08/
4
20
21
22
23
24
25
26
27
28
29
10 100 1000
Corpus size (K)
BLE
U s
cor
e (%
)
Moses
CCB
Proposed
Figure 3: Effect of training corpus size.
EC translation. In Europarl dataset, the proposed
method consistently get higher score than base-
lines.
In future work, we plan to apply this method
with paraphrases derived from a massive corpus
such as the Web corpus and apply this method to a
hierarchical phrase based SMT.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
597?604.
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In Proceedings of the International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), pages 1297?1300.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving Statistical Machine
Translation by Paraphrasing the Training Data. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 150?157.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of the
Human Language Technology conference - North
American chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pages 17?24.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for MT. In Proceed-
ings of the Human Language Technology confer-
ence - North American chapter of the Association
for Computational Linguistics (HLT-NAACL), pages
406?414.
Cameron S. Fordyce. 2007. Overview of the IWSLT
2007 Evaluation Campaign. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT), pages 1?12.
J Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving Translation Qual-
ity by Discarding Most of the Phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 177?180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the 10th Machine Translation Summit (MT Summit),
pages 79?86.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved Statistical Machine
Translation Using Monolingually-Derived Para-
phrases. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 381?390.
Preslav Nakov. 2008. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In
Proceedings of the European Conference on Artifi-
cial Intelligence (ECAI), pages 338?342.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL), pages 160?167.
5
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 434?438,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Reordering Constraint Based on Document-Level Context
Takashi Onishi and Masao Utiyama and Eiichiro Sumita
Multilingual Translation Laboratory, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, JAPAN
{takashi.onishi,mutiyama,eiichiro.sumita}@nict.go.jp
Abstract
One problem with phrase-based statistical ma-
chine translation is the problem of long-
distance reordering when translating between
languages with different word orders, such as
Japanese-English. In this paper, we propose a
method of imposing reordering constraints us-
ing document-level context. As the document-
level context, we use noun phrases which sig-
nificantly occur in context documents contain-
ing source sentences. Given a source sen-
tence, zones which cover the noun phrases are
used as reordering constraints. Then, in de-
coding, reorderings which violate the zones
are restricted. Experiment results for patent
translation tasks show a significant improve-
ment of 1.20% BLEU points in Japanese-
English translation and 1.41% BLEU points in
English-Japanese translation.
1 Introduction
Phrase-based statistical machine translation is use-
ful for translating between languages with similar
word orders. However, it has problems with long-
distance reordering when translating between lan-
guages with different word orders, such as Japanese-
English. These problems are especially crucial when
translating long sentences, such as patent sentences,
because many combinations of word orders cause
high computational costs and low translation qual-
ity.
In order to address these problems, various meth-
ods which use syntactic information have been pro-
posed. These include methods where source sen-
tences are divided into syntactic chunks or clauses
and the translations are merged later (Koehn and
Knight, 2003; Sudoh et al, 2010), methods where
syntactic constraints or penalties for reordering are
added to a decoder (Yamamoto et al, 2008; Cherry,
2008; Marton and Resnik, 2008; Xiong et al, 2010),
and methods where source sentences are reordered
into a similar word order as the target language in
advance (Katz-Brown and Collins, 2008; Isozaki
et al, 2010). However, these methods did not
use document-level context to constrain reorderings.
Document-level context is often available in real-life
situations. We think it is a promising clue to improv-
ing translation quality.
In this paper, we propose a method where re-
ordering constraints are added to a decoder using
document-level context. As the document-level con-
text, we use noun phrases which significantly oc-
cur in context documents containing source sen-
tences. Given a source sentence, zones which cover
the noun phrases are used as reordering constraints.
Then, in decoding, reorderings which violate the
zones are restricted. By using document-level con-
text, contextually-appropriate reordering constraints
are preferentially considered. As a result, the trans-
lation quality and speed can be improved. Ex-
periment results for the NTCIR-8 patent transla-
tion tasks show a significant improvement of 1.20%
BLEU points in Japanese-English translation and
1.41%BLEU points in English-Japanese translation.
2 Patent Translation
Patent translation is difficult because of the amount
of new phrases and long sentences. Since a patent
document explains a newly-invented apparatus or
method, it contains many new phrases. Learning
phrase translations for these new phrases from the
434
Source ?????????????????????????????????
???????????????
Reference the pad electrode 11 is formed on the top surface of the semiconductor substrate 10 through an
interlayer insulation film 12 that is a first insulation film .
Baseline output an interlayer insulating film 12 is formed on the surface of a semiconductor substrate 10 , a
pad electrode 11 via a first insulating film .
Source + Zone ????????? <zone>??? <zone>??? </zone>????? <zone>?
?? </zone>?? </zone>???????????????????????
Proposed output pad electrode 11 is formed on the surface of the semiconductor substrate 10 through the inter-
layer insulating film 12 of the first insulating film .
Table 1: An example of patent translation.
training corpora is difficult because these phrases
occur only in that patent specification. Therefore,
when translating such phrases, a decoder has to com-
bine multiple smaller phrase translations. More-
over, sentences in patent documents tend to be long.
This results in a large number of combinations of
phrasal reorderings and a degradation of the transla-
tion quality and speed.
Table 1 shows how a failure in phrasal reorder-
ing can spoil the whole translation. In the baseline
output, the translation of ??????????
?? ?? ? ? ?? (an interlayer insulation film
12 that is a first insulation film) is divided into two
blocks, ?an interlayer insulating film 12? and ?a first
insulating film?. In this case, a reordering constraint
to translate ???????????????
??? as a single block can reduce incorrect reorder-
ings and improve the translation quality. However,
it is difficult to predict what should be translated as
a single block.
Therefore, how to specify ranges for reordering
constraints is a very important problem. We propose
a solution for this problem that uses the very nature
of patent documents themselves.
3 Proposed Method
In order to address the aforementioned problem, we
propose a method for specifying phrases in a source
sentence which are assumed to be translated as sin-
gle blocks using document-level context. We call
these phrases ?coherent phrases?. When translat-
ing a document, for example a patent specification,
we first extract coherent phrase candidates from the
document. Then, when translating each sentence in
the document, we set zones which cover the coher-
ent phrase candidates and restrict reorderings which
violate the zones.
3.1 Coherent phrases in patent documents
As mentioned in the previous section, specifying
coherent phrases is difficult when using only one
source sentence. However, we have observed that
document-level context can be a clue for specify-
ing coherent phrases. In a patent specification, for
example, noun phrases which indicate parts of the
invention are very important noun phrases. In pre-
vious example, ?? ? ? ?? ? ? ?? ?? ?
? ? ? ?? is a part of the invention. Since this
is not language dependent, in other words, this noun
phrase is always a part of the invention in any other
language, this noun phrase should be translated as a
single block in every language. In this way, impor-
tant phrases in patent documents are assumed to be
coherent phrases.
We therefore treat the problem of specifying co-
herent phrases as a problem of specifying important
phrases, and we use these phrases as constraints on
reorderings. The details of the proposed method are
described below.
3.2 Finding coherent phrases
We propose the following method for finding co-
herent phrases in patent sentences. First, we ex-
tract coherent phrase candidates from a patent docu-
ment. Next, the candidates are ranked by a criterion
which reflects the document-level context. Then,
we specify coherent phrases using the rankings. In
this method, using document-level context is criti-
cally important because we cannot rank the candi-
dates without it.
435
3.2.1 Extracting coherent phrase candidates
Coherent phrase candidates are extracted from a
context document, a document that contains a source
sentence. We extract all noun phrases as co-
herent phrase candidates since most noun phrases
can be translated as single blocks in other lan-
guages (Koehn and Knight, 2003). These noun
phrases include nested noun phrases.
3.2.2 Ranking with C-value
The candidates which have been extracted are nested
and have different lengths. A naive method can-
not rank these candidates properly. For example,
ranking by frequency cannot pick up an important
phrase which has a long length, yet, ranking by
length may give a long but unimportant phrase a
high rank. In order to select the appropriate coher-
ent phrases, measurements which give high rank to
phrases with high termhood are needed. As one such
measurement, we use C-value (Frantzi and Anani-
adou, 1996).
C-value is a measurement of automatic term
recognition and is suitable for extracting important
phrases from nested candidates. The C-value of a
phrase p is expressed in the following equation:
C-value(p)=
{
(l(p)?1)n(p) (c(p)=0)
(l(p)?1)
(
n(p)? t(p)c(p)
)
(c(p)>0)
where
l(p) is the length of a phrase p,
n(p) is the frequency of p in a document,
t(p) is the total frequency of phrases which contain
p as a subphrase,
c(p) is the number of those phrases.
Since phrases which have a large C-value fre-
quently occur in a context document, these phrases
are considered to be a significant unit, i.e., a part of
the invention, and to be coherent phrases.
3.2.3 Specifying coherent phrases
Given a source sentence, we find coherent phrase
candidates in the sentence in order to set zones for
reordering constraints. If a coherent phrase candi-
date is found in the source sentence, the phrase is re-
garded a coherent phrase and annotated with a zone
tag, which will be mentioned in the next section.
We check the coherent phrase candidates in the sen-
tence in descending C-value order, and stop when
the C-value goes below a certain threshold. Nested
zones are allowed, unless their zones conflict with
pre-existing zones. We then give the zone-tagged
sentence, an example is shown in Table 1, as a de-
coder input.
3.3 Decoding with reordering constraints
In decoding, reorderings which violate zones, such
as the baseline output in Table 1, are restricted and
we get a more appropriate translation, such as the
proposed output in Table 1.
We use the Moses decoder (Koehn et al, 2007;
Koehn and Haddow, 2009), which can specify re-
ordering constraints using <zone> and </zone> tags.
Moses restricts reorderings which violate zones and
translates zones as single blocks.
4 Experiments
In order to evaluate the performance of the proposed
method, we conducted Japanese-English (J-E) and
English-Japanese (E-J) translation experiments us-
ing the NTCIR-8 patent translation task dataset (Fu-
jii et al, 2010). This dataset contains a training set of
3 million sentence pairs, a development set of 2,000
sentence pairs, and a test set of 1,251 (J-E) and 1,119
(E-J) sentence pairs. Moreover, this dataset contains
the patent specifications from which sentence pairs
are extracted. We used these patent specifications as
context documents.
4.1 Baseline
We usedMoses as a baseline system, with all the set-
tings except distortion limit (dl) at the default. The
distortion limit is a maximum distance of reorder-
ing. It is known that an appropriate distortion-limit
can improve translation quality and decoding speed.
Therefore, we examined the effect of a distortion-
limit. In experiments, we compared dl = 6, 10, 20,
30, 40, and ?1 (unlimited). The feature weights
were optimized to maximize BLEU score by MERT
(Och, 2003) using the development set.
4.2 Compared methods
We compared two methods, the method of specify-
ing reordering constraints with a context document
436
w/o Context in ( this case ) , ( the leading end ) 15f of ( the segment operating body ) ( ( 15 swings ) in
( a direction opposite ) ) to ( the a arrow direction ) .
w/ Context in ( this case ) , ( ( the leading end ) 15f ) of ( ( ( the segment ) operating body ) 15 )
swings in a direction opposite to ( the a arrow direction ) .
Table 3: An example of the zone-tagged source sentence. <zone> and </zone> are replaced by ?(? and ?)?.
J?E E?J
System dl BLEU Time BLEU Time
Baseline
6 27.83 4.8 35.39 3.5
10 30.15 6.9 38.14 4.9
20 30.65 11.9 38.39 8.5
30 30.72 16.0 38.32 11.5
40 29.96 19.6 38.42 13.9
?1 30.35 28.7 37.80 18.4
w/o Context ?1 30.01 8.7 38.96 5.9
w/ Context ?1 31.55 12.0 39.21 8.0
Table 2: BLEU score (%) and average decoding time
(sec/sentence) in J-E/E-J translation.
(w/ Context) and the method of specifying reorder-
ing constraints without a context document (w/o
Context). In both methods, the feature weights used
in decoding are the same value as those for the base-
line (dl = ?1).
4.2.1 Proposed method (w/ Context)
In the proposed method, reordering constraints were
defined with a context document. For J-E transla-
tion, we used the CaboCha parser (Kudo and Mat-
sumoto, 2002) to analyze the context document. As
coherent phrase candidates, we extracted all sub-
trees whose heads are noun. For E-J translation, we
used the Charniak parser (Charniak, 2000) and ex-
tracted all noun phrases, labeled ?NP?, as coherent
phrase candidates. The parsers are used only when
extracting coherent phrase candidates. When speci-
fying zones for each source sentence, strings which
match the coherent phrase candidates are defined to
be zones. Therefore, the proposed method is robust
against parsing errors. We tried various thresholds
of the C-value and selected the value that yielded
the highest BLEU score for the development set.
4.2.2 w/o Context
In this method, reordering constraints were defined
without a context document. For J-E translation,
we converted the dependency trees of source sen-
tences processed by the CaboCha parser into brack-
eted trees and used these as reordering constraints.
For E-J translation, we used all of the noun phrases
detected by the Charniak parser as reordering con-
straints.
4.3 Results and Discussions
The experiment results are shown in Table 2. For
evaluation, we used the case-insensitive BLEU met-
ric (Papineni et al, 2002) with a single reference.
In both directions, our proposed method yielded
the highest BLEU scores. The absolute improve-
ment over the baseline (dl = ?1) was 1.20% in J-E
translation and 1.41% in E-J translation. Accord-
ing to the bootstrap resampling test (Koehn, 2004),
the improvement over the baseline was statistically
significant (p<0.01) in both directions. When com-
pared to the method without context, the absolute
improvement was 1.54% in J-E and 0.25% in E-J.
The improvement over the baseline was statistically
significant (p < 0.01) in J-E and almost significant
(p < 0.1) in E-J. These results show that the pro-
posed method using document-level context is effec-
tive in specifying reordering constraints.
Moreover, as shown in Table 3, although zone
setting without context is failed if source sen-
tences have parsing errors, the proposed method can
set zones appropriately using document-level con-
text. The Charniak parser tends to make errors on
noun phrases with ID numbers. This shows that
document-level context can possibly improve pars-
ing quality.
As for the distortion limit, while an appropriate
distortion-limit, 30 for J-E and 40 for E-J, improved
the translation quality, the gains from the proposed
method were significantly better than the gains from
the distortion limit. In general, imposing strong
constraints causes fast decoding but low translation
quality. However, the proposed method improves
the translation quality and speed by imposing appro-
priate constraints.
437
5 Conclusion
In this paper, we proposed a method for imposing
reordering constraints using document-level context.
In the proposed method, coherent phrase candidates
are extracted from a context document in advance.
Given a source sentence, zones which cover the co-
herent phrase candidates are defined. Then, in de-
coding, reorderings which violate the zones are re-
stricted. Since reordering constraints reduce incor-
rect reorderings, the translation quality and speed
can be improved. The experiment results for the
NTCIR-8 patent translation tasks show a significant
improvement of 1.20% BLEU points for J-E trans-
lation and 1.41% BLEU points for E-J translation.
We think that the proposed method is indepen-
dent of language pair and domains. In the future,
we want to apply our proposed method to other lan-
guage pairs and domains.
References
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132?139.
Colin Cherry. 2008. Cohesive Phrase-Based Decoding
for Statistical Machine Translation. In Proceedings of
ACL-08: HLT, pages 72?80.
Katerina T. Frantzi and Sophia Ananiadou. 1996. Ex-
tracting Nested Collocations. In Proceedings of COL-
ING 1996, pages 41?46.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-
hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and
Sayori Shimohata. 2010. Overview of the Patent
Translation Task at the NTCIR-8 Workshop. In Pro-
ceedings of NTCIR-8 Workshop Meeting, pages 371?
376.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A Simple Re-
ordering Rule for SOV Languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 244?251.
Jason Katz-Brown and Michael Collins. 2008. Syntac-
tic Reordering in Preprocessing for Japanese?English
Translation: MIT System Description for NTCIR-7
Patent Translation Task. In Proceedings of NTCIR-7
Workshop Meeting, pages 409?414.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
Submission to all Tracks of the WMT 2009 Shared
Task with Reordering and Speed Improvements to
Moses. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation, pages 160?164.
Philipp Koehn and Kevin Knight. 2003. Feature-Rich
Statistical Translation of Noun Phrases. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 177?180.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. In Pro-
ceedings of CoNLL-2002, pages 63?69.
Yuval Marton and Philip Resnik. 2008. Soft Syntac-
tic Constraints for Hierarchical Phrased-Based Trans-
lation. In Proceedings of ACL-08: HLT, pages 1003?
1011.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsutomu
Hirao, and Masaaki Nagata. 2010. Divide and Trans-
late: Improving Long Distance Reordering in Statisti-
cal Machine Translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, pages 418?427.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing Translation Boundaries for Phrase-Based Decod-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
136?144.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro
Sumita. 2008. Imposing Constraints from the Source
Tree on ITG Constraints for SMT. In Proceedings
of the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages 1?
9.
438
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311?316,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Post-ordering by Parsing for Japanese-English Statistical
Machine Translation
Isao Goto Masao Utiyama
Multilingual Translation Laboratory, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
{igoto, mutiyama, eiichiro.sumita}@nict.go.jp
Eiichiro Sumita
Abstract
Reordering is a difficult task in translating
between widely different languages such as
Japanese and English. We employ the post-
ordering framework proposed by (Sudoh et
al., 2011b) for Japanese to English transla-
tion and improve upon the reordering method.
The existing post-ordering method reorders
a sequence of target language words in a
source language word order via SMT, while
our method reorders the sequence by: 1) pars-
ing the sequence to obtain syntax structures
similar to a source language structure, and 2)
transferring the obtained syntax structures into
the syntax structures of the target language.
1 Introduction
The word reordering problem is a challenging one
when translating between languages with widely
different word orders such as Japanese and En-
glish. Many reordering methods have been proposed
in statistical machine translation (SMT) research.
Those methods can be classified into the following
three types:
Type-1: Conducting the target word selection and
reordering jointly. These include phrase-based SMT
(Koehn et al, 2003), hierarchical phrase-based SMT
(Chiang, 2007), and syntax-based SMT (Galley et
al., 2004; Ding and Palmer, 2005; Liu et al, 2006;
Liu et al, 2009).
Type-2: Pre-ordering (Xia and McCord, 2004;
Collins et al, 2005; Tromble and Eisner, 2009; Ge,
2010; Isozaki et al, 2010b; DeNero and Uszkoreit,
2011; Wu et al, 2011). First, these methods re-
order the source language sentence into the target
language word order. Then, they translate the re-
ordered source word sequence using SMT methods.
Type-3: Post-ordering (Sudoh et al, 2011b; Ma-
tusov et al, 2005). First, these methods translate
the source sentence almost monotonously into a se-
quence of the target language words. Then, they
reorder the translated word sequence into the target
language word order.
This paper employs the post-ordering framework
for Japanese-English translation based on the dis-
cussions given in Section 2, and improves upon the
reordering method. Our method uses syntactic struc-
tures, which are essential for improving the target
word order in translating long sentences between
Japanese (a Subject-Object-Verb (SOV) language)
and English (an SVO language).
Before explaining our method, we explain the pre-
ordering method for English to Japanese used in the
post-ordering framework.
In English-Japanese translation, Isozaki et al
(2010b) proposed a simple pre-ordering method that
achieved the best quality in human evaluations,
which were conducted for the NTCIR-9 patent ma-
chine translation task (Sudoh et al, 2011a; Goto et
al., 2011). The method, which is called head final-
ization, simply moves syntactic heads to the end of
corresponding syntactic constituents (e.g., phrases
and clauses). This method first changes the English
word order into a word order similar to Japanese
word order using the head finalization rule. Then,
it translates (almost monotonously) the pre-ordered
311
Japanese
HFE
monotone translation
English
post-ordering
Figure 1: Post-ordering framework.
English words into Japanese.
There are two key reasons why this pre-ordering
method works for estimating Japanese word order.
The first reason is that Japanese is a typical head-
final language. That is, a syntactic head word comes
after nonhead (dependent) words. Second, input En-
glish sentences are parsed by a high-quality parser,
Enju (Miyao and Tsujii, 2008), which outputs syn-
tactic heads. Consequently, the parsed English in-
put sentences can be pre-ordered into a Japanese-
like word order using the head finalization rule.
Pre-ordering using the head finalization rule nat-
urally cannot be applied to Japanese-English trans-
lation, because English is not a head-final language.
If we want to pre-order Japanese sentences into an
English-like word order, we therefore have to build
complex rules (Sudoh et al, 2011b).
2 Post-ordering for Japanese to English
Sudoh et al (2011b) proposed a post-ordering
method for Japanese-English translation. The trans-
lation flow for the post-ordering method is shown in
Figure 1, where ?HFE? is an abbreviation of ?Head
Final English?. An HFE sentence consists of En-
glish words in a Japanese-like structure. It can be
constructed by applying the head-finalization rule
(Isozaki et al, 2010b) to an English sentence parsed
by Enju. Therefore, if good rules are applied to this
HFE sentence, the underlying English sentence can
be recovered. This is the key observation of the post-
ordering method.
The process of post-ordering translation consists
of two steps. First, the Japanese input sentence is
translated into HFE almost monotonously. Then, the
word order of HFE is changed into an English word
order.
Training for the post-ordering method is con-
ducted by first converting the English sentences in
a Japanese-English parallel corpus into HFE sen-
tences using the head-finalization rule. Next, a
monotone phrase-based Japanese-HFE SMT model
is built using the Japanese-HFE parallel corpus
Japanese: kare    wa        kinou        hon        wo       katta
HFE:
he    
_va0
    yesterday    books    
_va2
    bought
HFE:
he    
_va0
    yesterday    books    
_va2
    bought
NP_ST NP_ST
VP_SW
VP_SW
S_ST
English:
he   (
_va0
)   bought    books   (
_va2
)   yesterday
NP NP
VP
VP
S
Parsing
Reordering
Figure 2: Example of post-ordering by parsing.
whose HFE was converted from English. Finally,
an HFE-to-English word reordering model is built
using the HFE-English parallel corpus.
3 Post-ordering Models
3.1 SMT Model
Sudoh et al (2011b) have proposed using phrase-
based SMT for converting HFE sentences into En-
glish sentences. The advantage of their method is
that they can use off-the-shelf SMT techniques for
post-ordering.
3.2 Parsing Model
Our proposed model is called the parsing model.
The translation process for the parsing model is
shown in Figure 2. In this method, we first parse the
HFE sentence into a binary tree. We then swap the
nodes annotated with ? SW? suffixes in this binary
tree in order to produce an English sentence.
The structures of the HFE sentences, which are
used for training our parsing model, can be obtained
from the corresponding English sentences as fol-
lows.1 First, each English sentence in the training
Japanese-English parallel corpus is parsed into a bi-
nary tree by applying Enju. Then, for each node in
this English binary tree, the two children of each
node are swapped if its first child is the head node
(See (Isozaki et al, 2010b) for details of the head
1The explanations of pseudo-particles ( va0 and va2) and
other details of the HFE is given in Section 4.2.
312
final rules). At the same time, these swapped nodes
are annotated with ? SW?. When the two nodes are
not swapped, they are annotated with ? ST? (indi-
cating ?Straight?). A node with only one child is
not annotated with either ? ST? or ? SW?. The re-
sult is an HFE sentence in a binary tree annotated
with ? SW? and ? ST? suffixes.
Observe that the HFE sentences can be regarded
as binary trees annotated with syntax tags aug-
mented with swap/straight suffixes. Therefore, the
structures of these binary trees can be learnable by
using an off-the-shelf grammar learning algorithm.
The learned parsing model can be regarded as an
ITG model (Wu, 1997) between the HFE and En-
glish sentences. 2
In this paper, we used the Berkeley Parser (Petrov
and Klein, 2007) for learning these structures. The
HFE sentences can be parsed by using the learned
parsing model. Then the parsed structures can be
converted into their corresponding English struc-
tures by swapping the ? SW? nodes. Note that this
parsing model jointly learns how to parse and swap
the HFE sentences.
4 Detailed Explanation of Our Method
This section explains the proposed method, which
is based on the post-ordering framework using the
parsing model.
4.1 Translation Method
First, we produce N-best HFE sentences us-
ing Japanese-to-HFE monotone phrase-based SMT.
Next, we produce K-best parse trees for each HFE
sentence by parsing, and produce English sentences
by swapping any nodes annotated with ? SW?. Then
we score the English sentences and select the En-
glish sentence with the highest score.
For the score of an English sentence, we use
the sum of the log-linear SMT model score for
Japanese-to-HFE and the logarithm of the language
model probability of the English sentence.
2There are works using the ITG model in SMT: ITG was
used for training pre-ordering models (DeNero and Uszkoreit,
2011); hierarchical phrase-based SMT (Chiang, 2007), which is
an extension of ITG; and reordering models using ITG (Chen et
al., 2009; He et al, 2010). These methods are not post-ordering
methods.
4.2 HFE and Articles
This section describes the details of HFE sentences.
In HFE sentences: 1) Heads are final except for
coordination. 2) Pseudo-particles are inserted after
verb arguments: va0 (subject of sentence head),
va1 (subject of verb), and va2 (object of verb).
3) Articles (a, an, the) are dropped.
In our method of HFE construction, unlike that
used by (Sudoh et al, 2011b), plural nouns are left
as-is instead of converted to the singular.
Applying our parsing model to an HFE sentence
produces an English sentence that does not have
articles, but does have pseudo-particles. We re-
moved the pseudo-particles from the reordered sen-
tences before calculating the probabilities used for
the scores of the reordered sentences. A reordered
sentence without pseudo-particles is represented by
E. A language model P (E) was trained from En-
glish sentences whose articles were dropped.
In order to output a genuine English sentence E?
fromE, articles must be inserted intoE. A language
model trained using genuine English sentences is
used for this purpose. We try to insert one of the
articles {a, an, the} or no article for each word in E.
Then we calculate the maximum probability word
sequence through dynamic programming for obtain-
ing E?.
5 Experiment
5.1 Setup
We used patent sentence data for the Japanese to
English translation subtask from the NTCIR-9 and
8 (Goto et al, 2011; Fujii et al, 2010). There
were 2,000 test sentences for NTCIR-9 and 1,251
for NTCIR-8. XML entities included in the data
were decoded to UTF-8 characters before use.
We used Enju (Miyao and Tsujii, 2008) v2.4.2 for
parsing the English side of the training data. Mecab
3 v0.98 was used for the Japanese morphological
analysis. The translation model was trained using
sentences of 64 words or less from the training cor-
pus as (Sudoh et al, 2011b). We used 5-gram lan-
guage models using SRILM (Stolcke et al, 2011).
We used the Berkeley parser (Petrov and Klein,
2007) to train the parsing model for HFE and to
3http://mecab.sourceforge.net/
313
parse HFE. The parsing model was trained using 0.5
million sentences randomly selected from training
sentences of 40 words or less. We used the phrase-
based SMT system Moses (Koehn et al, 2007) to
calculate the SMT score and to produce HFE sen-
tences. The distortion limit was set to 0. We used
10-best Moses outputs and 10-best parsing results
of Berkeley parser.
5.2 Compared Methods
We used the following 5 comparison methods:
Phrase-based SMT (PBMT), Hierarchical phrase-
based SMT (HPBMT), String-to-tree syntax-based
SMT (SBMT), Post-ordering based on phrase-based
SMT (PO-PBMT) (Sudoh et al, 2011b), and Post-
ordering based on hierarchical phrase-based SMT
(PO-HPBMT).
We used Moses for these 5 systems. For
PO-PBMT, a distortion limit 0 was used for the
Japanese-to-HFE translation and a distortion limit
20 was used for the HFE-to-English translation.
The PO-HPBMT method changes the post-ordering
method of PO-PBMT from a phrase-based SMT
to a hierarchical phrase-based SMT. We used a
max-chart-span 15 for the hierarchical phrase-based
SMT. We used distortion limits of 12 or 20 for
PBMT and a max-chart-span 15 for HPBMT.
The parameters for SMT were tuned by MERT
using the first half of the development data with HFE
converted from English.
5.3 Results and Discussion
We evaluated translation quality based on the case-
insensitive automatic evaluation scores of RIBES
v1.1 (Isozaki et al, 2010a) and BLEU-4. The results
are shown in Table 1.
Ja-to-En NTCIR-9 NTCIR-8
RIBES BLEU RIBES BLEU
Proposed 72.57 31.75 73.48 32.80
PBMT (limit 12) 68.44 29.64 69.18 30.72
PBMT (limit 20) 68.86 30.13 69.63 31.22
HPBMT 69.92 30.15 70.18 30.94
SBMT 69.22 29.53 69.87 30.37
PO-PBMT 68.81 30.39 69.80 31.71
PO-HPBMT 70.47 27.49 71.34 28.78
Table 1: Evaluation results (case insensitive).
From the results, the proposed method achieved
the best scores for both RIBES and BLEU for
NTCIR-9 and NTCIR-8 test data. Since RIBES is
sensitive to global word order and BLEU is sensitive
to local word order, the effectiveness of the proposed
method for both global and local reordering can be
demonstrated through these comparisons.
In order to investigate the effects of our post-
ordering method in detail, we conducted an ?HFE-
to-English reordering? experiment, which shows the
main contribution of our post-ordering method in
the framework of post-ordering SMT as compared
with (Sudoh et al, 2011b). In this experiment, we
changed the word order of the oracle-HFE sentences
made from reference sentences into English, this is
the same way as Table 4 in (Sudoh et al, 2011b).
The results are shown in Table 2.
This results show that our post-ordering method
is more effective than PO-PBMT and PO-HPBMT.
Since RIBES is based on the rank order correla-
tion coefficient, these results show that the proposed
method correctly recovered the word order of the
English sentences. These high scores also indicate
that the parsing results for high quality HFE are
fairly trustworthy.
oracle-HFE-to-En NTCIR-9 NTCIR-8
RIBES BLEU RIBES BLEU
Proposed 94.66 80.02 94.93 79.99
PO-PBMT 77.34 62.24 78.14 63.14
PO-HPBMT 77.99 53.62 80.85 58.34
Table 2: Evaluation resutls focusing on post-ordering.
In these experiments, we did not compare our
method to pre-ordering methods. However, some
groups used pre-ordering methods in the NTCIR-9
Japanese to English translation subtask. The NTT-
UT (Sudoh et al, 2011a) and NAIST (Kondo et al,
2011) groups used pre-ordering methods, but could
not produce RIBES and BLEU scores that both were
better than those of the baseline results. In contrast,
our method was able to do so.
6 Conclusion
This paper has described a new post-ordering
method. The proposed method parses sentences that
consist of target language words in a source lan-
guage word order, and does reordering by transfer-
ring the syntactic structures similar to the source lan-
guage syntactic structures into the target language
syntactic structures.
314
References
Han-Bin Chen, Jian-Cheng Wu, and Jason S. Chang.
2009. Learning Bilingual Linguistic Reordering
Model for Statistical Machine Translation. In Pro-
ceedings of Human Language Technologies: The 2009
NAACL, pages 254?262, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the 43rd ACL, pages
531?540, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
John DeNero and Jakob Uszkoreit. 2011. Inducing Sen-
tence Structure from Parallel Corpora for Reordering.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 193?
203, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Yuan Ding and Martha Palmer. 2005. Machine Transla-
tion Using Probabilistic Synchronous Dependency In-
sertion Grammars. In Proceedings of the 43rd ACL,
pages 541?548, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-
hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and
Sayori Shimohata. 2010. Overview of the Patent
Translation Task at the NTCIR-8 Workshop. In Pro-
ceedings of NTCIR-8, pages 371?376.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
273?280, Boston, Massachusetts, USA, May 2 - May
7. Association for Computational Linguistics.
Niyu Ge. 2010. A Direct Syntax-Driven Reordering
Model for Phrase-Based Machine Translation. In Pro-
ceedings of NAACL-HLT, pages 849?857, Los Ange-
les, California, June. Association for Computational
Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent Ma-
chine Translation Task at the NTCIR-9 Workshop. In
Proceedings of NTCIR-9, pages 559?578.
Yanqing He, Yu Zhou, Chengqing Zong, and Huilin
Wang. 2010. A Novel Reordering Model Based on
Multi-layer Phrase for Statistical Machine Translation.
In Proceedings of the 23rd Coling, pages 447?455,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic Eval-
uation of Translation Quality for Distant Language
Pairs. In Proceedings of the 2010 EMNLP, pages 944?
952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head Finalization: A Simple Re-
ordering Rule for SOV Languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 244?251, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of the 2003 HLT-NAACL, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th ACL, pages 177?180, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Shuhei Kondo, Mamoru Komachi, Yuji Matsumoto, Kat-
suhito Sudoh, Kevin Duh, and Hajime Tsukada. 2011.
Learning of Linear Ordering Problems and its Applica-
tion to J-E Patent Translation in NTCIR-9 PatentMT.
In Proceedings of NTCIR-9, pages 641?645.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proceedings of the 21st ACL, pages
609?616, Sydney, Australia, July. Association for
Computational Linguistics.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
Tree-to-Tree Translation with Packed Forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558?566, Suntec, Singapore, August.
Association for Computational Linguistics.
E. Matusov, S. Kanthak, and Hermann Ney. 2005. On
the Integration of Speech Recognition and Statistical
Machine Translation. In Proceedings of Interspeech,
pages 3177?3180.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics, Volume 34, Number 1, pages 81?88.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. InNAACL-HLT, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at Sixteen: Update and
Outlook. In Proceedings of IEEE Automatic Speech
Recognition and Understanding Workshop.
315
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Masaaki
Nagata, Xianchao Wu, Takuya Matsuzaki, and
Jun?ichi Tsujii. 2011a. NTT-UT Statistical Machine
Translation in NTCIR-9 PatentMT. In Proceedings of
NTCIR-9, pages 585?592.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011b. Post-ordering
in Statistical Machine Translation. In Proceedings of
the 13th Machine Translation Summit, pages 316?323.
Roy Tromble and Jason Eisner. 2009. Learning Linear
Ordering Problems for Better Translation. In Proceed-
ings of the 2009 EMNLP, pages 1007?1016, Singa-
pore, August. Association for Computational Linguis-
tics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting Pre-
ordering Rules from Chunk-based Dependency Trees
for Japanese-to-English Translation. In Proceedings
of the 13th Machine Translation Summit, pages 300?
307.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
Fei Xia and Michael McCord. 2004. Improving a Statis-
tical MT System with Automatically Learned Rewrite
Patterns. In Proceedings of Coling, pages 508?514,
Geneva, Switzerland, Aug 23?Aug 27. COLING.
316
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 155?165,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Distortion Model Considering Rich Context
for Statistical Machine Translation
Isao Goto?,? Masao Utiyama? Eiichiro Sumita?
Akihiro Tamura? Sadao Kurohashi?
?National Institute of Information and Communications Technology
?Kyoto University
goto.i-es@nhk.or.jp
{mutiyama, eiichiro.sumita, akihiro.tamura}@nict.go.jp
kuro@i.kyoto-u.ac.jp
Abstract
This paper proposes new distortion mod-
els for phrase-based SMT. In decoding, a
distortion model estimates the source word
position to be translated next (NP) given
the last translated source word position
(CP). We propose a distortion model that
can consider the word at the CP, a word
at an NP candidate, and the context of the
CP and the NP candidate simultaneously.
Moreover, we propose a further improved
model that considers richer context by dis-
criminating label sequences that specify
spans from the CP to NP candidates. It
enables our model to learn the effect of
relative word order among NP candidates
as well as to learn the effect of distances
from the training data. In our experiments,
our model improved 2.9 BLEU points for
Japanese-English and 2.6 BLEU points for
Chinese-English translation compared to
the lexical reordering models.
1 Introduction
Estimating appropriate word order in a target lan-
guage is one of the most difficult problems for
statistical machine translation (SMT). This is par-
ticularly true when translating between languages
with widely different word orders.
To address this problem, there has been a lot
of research done into word reordering: lexical
reordering model (Tillman, 2004), which is one
of the distortion models, reordering constraint
(Zens et al, 2004), pre-ordering (Xia and Mc-
Cord, 2004), hierarchical phrase-based SMT (Chi-
ang, 2007), and syntax-based SMT (Yamada and
Knight, 2001).
In general, source language syntax is useful for
handling long distance word reordering. However,
obtaining syntax requires a syntactic parser, which
is not available for many languages. Phrase-based
SMT (Koehn et al, 2007) is a widely used SMT
method that does not use a parser.
Phrase-based SMT mainly1 estimates word re-
ordering using distortion models2. Therefore, dis-
tortion models are one of the most important com-
ponents for phrase-based SMT. On the other hand,
there are methods other than distortion models for
improving word reordering for phrase-based SMT,
such as pre-ordering or reordering constraints.
However, these methods also use distortion mod-
els when translating by phrase-based SMT. There-
fore, distortion models do not compete against
these methods and are commonly used with them.
If there is a good distortion model, it will improve
the translation quality of phrase-based SMT and
benefit to the methods using distortion models.
In this paper, we propose two distortion mod-
els for phrase-based SMT. In decoding, a distor-
tion model estimates the source word position to
be translated next (NP) given the last translated
source word position (CP). The proposed models
are the pair model and the sequence model. The
pair model utilizes the word at the CP, a word at
an NP candidate site, and the words surrounding
the CP and the NP candidates (context) simultane-
ously. In addition, the sequence model, which is
the further improved model, considers richer con-
text by identifying the label sequence that spec-
ify the span from the CP to the NP. It enables
our model to learn the effect of relative word or-
der among NP candidates as well as to learn the
effect of distances from the training data. Our
model learns the preference relations among NP
1A language model also supports the estimation.
2In this paper, reordering models for phrase-based SMT,
which are intended to estimate the source word position to
be translated next in decoding, are called distortion models.
This estimation is used to produce a hypothesis in the target
language word order sequentially from left to right.
155
kinou  kare  wa  pari  de  hon  wo  katta
he   bought   books   in   Paris   yesterday
Source:
Target:
Figure 1: An example of left-to-right translation
for Japanese-English. Boxes represent phrases
and arrows indicate the translation order of the
phrases.
candidates. Our model consists of one probabilis-
tic model and does not require a parser. Exper-
iments confirmed the effectiveness of our method
for Japanese-English and Chinese-English transla-
tion, using NTCIR-9 Patent Machine Translation
Task data sets (Goto et al, 2011).
2 Distortion Model for Phrase-Based
SMT
A Moses-style phrase-based SMT generates target
hypotheses sequentially from left to right. There-
fore, the role of the distortion model is to esti-
mate the source phrase position to be translated
next whose target side phrase will be located im-
mediately to the right of the already generated hy-
potheses. An example is shown in Figure 1. In
Figure 1, we assume that only the kare wa (En-
glish side: ?he?) has been translated. The target
word to be generated next will be ?bought? and the
source word to be selected next will be its corre-
sponding Japanese word katta. Thus, a distortion
model should estimate phrases including katta as
a source phrase position to be translated next.
To explain the distortion model task in more de-
tail, we need to redefine more precisely two terms,
the current position (CP) and next position (NP) in
the source sentence. CP is the source sentence po-
sition corresponding to the rightmost aligned tar-
get word in the generated target word sequence.
NP is the source sentence position corresponding
to the leftmost aligned target word in the target
phrase to be generated next. The task of the distor-
tion model is to estimate the NP3 from NP candi-
dates (NPCs) for each CP in the source sentence.4
3NP is not always one position, because there may be mul-
tiple correct hypotheses.
4This definition is slightly different from that of existing
methods such as Moses and (Green et al, 2010). In existing
methods, CP is the rightmost position of the last translated
source phrase and NP is the leftmost position of the source
phrase to be translated next. Note that existing methods do
kinou
1
 kare
2
 wa
3
 pari
4
 de
5
 hon
6
 wo
7
 katta
8
he   bought   books   in   Paris   yesterday
(a)
kinou
1
 kare
2
 wa
3
 pari
4
 de
5
 ni
6
 satsu
7
 hon
8
 wo
9
 katta
10
he   bought   two   books   in   Paris   yesterday
(b)
kinou
1
 kare
2
 wa
3
 hon
4
 wo
5
 karita
6
 ga
7
 kanojo
8
 wa
9
 katta
10
he   borrowed   books  yesterday  but  she  bought
(c)
kinou
1
 kare
2
 wa
3
 kanojo
4
 ga
5
 katta
6
 hon
7
 wo
8
 karita
9
yesterday  he  borrowed  the  books  that  she  bought
(e)
kinou
1
 kare
2
 wa
3
 hon
4
 wo
5
 katta
6
 ga
7
 kanojo
8
 wa
9
 karita
10
he   bought   books   yesterday   but   she   borrowed
(d)
??
?~
??
??
?~
??
?~
???~
CP NP
Figure 2: Examples of CP and NP for Japanese-
English translation. The upper sentence is the
source sentence and the sentence underneath is a
target hypothesis for each example. The NP is in
bold, and the CP is in bold italics. The point of an
arrow with a ? mark indicates a wrong NP candi-
date.
Estimating NP is a difficult task. Figure 2 shows
some examples. The superscript numbers indicate
the word position in the source sentence.
In Figure 2 (a), the NP is 8. However, in Fig-
ure 2 (b), the word (kare) at the CP is the same as
(a), but the NP is different (the NP is 10). From
these examples, we see that distance is not the es-
sential factor in deciding an NP. And it also turns
out that the word at the CP alone is not enough to
estimate the NP. Thus, not only the word at the CP
but also the word at a NP candidate (NPC) should
be considered simultaneously.
In (c) and (d) in Figure 2, the word (kare) at the
CP is the same and karita (borrowed) and katta
(bought) are at the NPCs. Karita is the word at
the NP and katta is not the word at the NP for
(c), while katta is the word at the NP and karita
is not the word at the NP for (d). From these ex-
amples, considering what the word is at the NP
not consider word-level correspondences.
156
is not enough to estimate the NP. One of the rea-
sons for this difference is the relative word order
between words. Thus, considering relative word
order is important.
In (d) and (e) in Figure 2, the word (kare) at the
CP and the word order between katta and karita
are the same. However, the word at the NP for
(d) and the word at the NP for (e) are different.
From these examples, we can see that selecting
a nearby word is not always correct. The differ-
ence is caused by the words surrounding the NPCs
(context), the CP context, and the words between
the CP and the NPC. Thus, these should be con-
sidered when estimating the NP.
In summary, in order to estimate the NP, the fol-
lowing should be considered simultaneously: the
word at the NP, the word at the CP, the relative
word order among the NPCs, the words surround-
ing NP and CP (context), and the words between
the CP and the NPC.
There are distortion models that do not require
a parser for phrase-based SMT. The linear dis-
tortion cost model used in Moses (Koehn et al,
2007), whose costs are linearly proportional to
the reordering distance, always gives a high cost
to long distance reordering, even if the reorder-
ing is correct. The MSD lexical reordering model
(Tillman, 2004; Koehn et al, 2005; Galley and
Manning, 2008) only calculates probabilities for
the three kinds of phrase reorderings (monotone,
swap, and discontinuous), and does not consider
relative word order or words between the CP and
the NPC. Thus, these models are not sufficient for
long distance word reordering.
Al-Onaizan and Papineni (2006) proposed a
distortion model that used the word at the CP and
the word at an NPC. However, their model did not
use context, relative word order, or words between
the CP and the NPC.
Ni et al (2009) proposed a method that adjusts
the linear distortion cost using the word at the CP
and its context. Their model does not simultane-
ously consider both the word specified at the CP
and the word specified at the NPCs.
Green et al (2010) proposed distortion mod-
els that used context. Their model (the outbound
model) estimates how far the NP should be from
the CP using the word at the CP and its con-
text.5 Their model does not simultaneously con-
5They also proposed another model (the inbound model)
sider both the word specified at the CP and the
word specified at an NPC. For example, the out-
bound model considers the word specified at the
CP, but does not consider the word specified at an
NPC. Their models also do not consider relative
word order.
In contrast, our distortion model solves the
aforementioned problems. Our distortion models
utilize the word specified at the CP, the word spec-
ified at an NPC, and also the context of the CP
and the NPC simultaneously. Furthermore, our se-
quence model considers richer context including
the relative word order among NPCs and also in-
cluding all the words between the CP and the NPC.
In addition, unlike previous methods, our models
learn the preference relations among NPCs.
3 Proposed Method
In this section, we first define our distortion model
and explain our learning strategy. Then, we de-
scribe two proposed models: the pair model and
the sequence model that is the further improved
model.
3.1 Distortion Model and Learning Strategy
First, we define our distortion model. Let i be a
CP, j be an NPC, S be a source sentence, andX be
the random variable of the NP. In this paper, dis-
tortion probability is defined as P (X = j|i, S),
which is the probability of an NPC j being the NP.
Our distortion model is defined as the model cal-
culating the distortion probability.
Next, we explain the learning strategy for our
distortion model. We train this model as a dis-
criminative model that discriminates the NP from
NPCs. Let J be a set of word positions in S other
than i. We train the distortion model subject to
?
j?J
P (X = j|i, S) = 1.
The model parameters are learned to maximize the
distortion probability of the NP among all of the
NPCs J in each source sentence. This learning
strategy is a kind of preference relation learning
(Evgniou and Pontil, 2002). In this learning, the
that estimates reverse direction distance. Each NPC is re-
garded as an NP, and the inbound model estimates how far
the corresponding CP should be from the NP using the word
at the NP and its context.
157
distortion probability of the actual NP will be rel-
atively higher than those of all the other NPCs J .
This learning strategy is different from that of
(Al-Onaizan and Papineni, 2006; Green et al,
2010). For example, Green et al (2010) trained
their outbound model subject to ?c?C P (Y =
c|i, S) = 1, where C is the set of the nine distor-
tion classes6 and Y is the random variable of the
correct distortion class that the correct distortion is
classified into. Distortion is defined as j ? i ? 1.
Namely, the model probabilities that they learned
were the probabilities of distortion classes in all
of the training data, not the relative preferences
among the NPCs in each source sentence.
3.2 Pair Model
The pair model utilizes the word at the CP, the
word at an NPC, and the context of the CP and the
NPC simultaneously to estimate the NP. This can
be done by our distortion model definition and the
learning strategy described in the previous section.
In this work, we use the maximum entropy
method (Berger et al, 1996) as a discriminative
machine learning method. The reason for this
is that a model based on the maximum entropy
method can calculate probabilities. However, if
we use scores as an approximation of the distor-
tion probabilities, various discriminative machine
learning methods can be applied to build the dis-
tortion model.
Let s be a source word and sn1 = s1s2...sn be
a source sentence. We add a beginning of sen-
tence (BOS) marker to the head of the source sen-
tence and an end of sentence (EOS) marker to the
end, so the source sentence S is expressed as sn+10
(s0 = BOS, sn+1 = EOS). Our distortion model
calculates the distortion probability for an NPC
j ? {j|1 ? j ? n + 1 ? j ?= i} for each CP
i ? {i|0 ? i ? n}
P (X = j|i, S) = 1Zi
exp
(
wTf (i, j, S, o, d)
)
(1)
where
o =
{
0 (i < j)
1 (i > j) , d =
?
??
??
0 (|j ? i| = 1)
1 (2 ? |j ? i| ? 5)
2 (6 ? |j ? i|)
,
6(??,?8], [?7,?5], [?4,?3], ?2, 0, 1, [2, 3], [4, 6],
and [7,?). In (Green et al, 2010), ?1 was used as one of
distortion classes. However, ?1 represents the CP in our def-
inition, and CP is not an NPC. Thus, we shifted all of the
distortion classes for negative distortions by ?1.
Template
?o?, ?o, sp?1, ?o, ti?, ?o, tj?, ?o, d?, ?o, sp, sq?2,
?o, ti, tj?, ?o, ti?1, ti, tj?, ?o, ti, ti+1, tj?,
?o, ti, tj?1, tj?, ?o, ti, tj , tj+1?, ?o, si, ti, tj?,
?o, sj , ti, tj?
1 p ? {p|i? 2 ? p ? i + 2 ? j ? 2 ? p ? j + 2}
2 (p, q) ? {(p, q)|i ? 2 ? p ? i + 2 ? j ? 2 ? q ?
j + 2 ? (|p? i| ? 1 ? |q ? j| ? 1)}
Table 1: Feature templates. t is the part of speech
of s.
w is a weight parameter vector, each element
of f(?) is a binary feature function, and Zi =?
j?{j|1?j?n+1 ? j ?=i}(numerator of Equation 1)
is a normalization factor. o is an orientation of i to
j and d is a distance class.
The binary feature function that constitutes an
element of f(?) returns 1 when its feature is
matched and if else, returns 0. Table 1 shows the
feature templates used to produce the features. A
feature is an instance of a feature template.
In Equation 1, i, j, and S are used by the feature
functions. Thus, Equation 1 can utilize features
consisting of both si, which is the word specified
at i, and sj , which is the word specified at j, or
both the context of i and the context of j simulta-
neously. Distance is considered using the distance
class d. Distortion is represented by distance and
orientation. The pair model considers distortion
using six joint classes of d and o.
3.3 Sequence Model
The pair model does not consider relative word or-
der among NPCs or all the words between the CP
and an NPC. In this section, we propose a further
improved model, the sequence model, which con-
siders richer context including relative word order
among NPCs and also including all the words be-
tween the CP and an NPC.
In (c) and (d) in Figure 2, karita (borrowed) and
katta (bought) occur in the source sentences. The
pair model considers the effect of distances using
only the distance class d. If these positions are
in the same distance class, the pair model cannot
consider the differences in distances. In this case,
these are conflict instances during training and it
is difficult to distinguish the NP for translation.
Now to explain how to consider the relative
word order by the sequence model. The sequence
model considers the relative word order by dis-
criminating the label sequence corresponding to
the NP from the label sequences corresponding to
158
Label Description
C A position is the CP.
I A position is a position between the CP
and the NPC.
N A position is the NPC.
Table 2: The ?C, I, and N? label set.
La
be
ls
eq
ue
nc
e
ID
1 N C
3 C N
4 C I N
5 C I I N
6 C I I I N
7 C I I I I N
8 C I I I I I N
9 C I I I I I I N
10 C I I I I I I I N
11 C I I I I I I I I N
B
O
S0
ki
no
u1
ka
re
2
w
a3
ho
n4
w
o5
ka
ri
ta
6
ga
7
ka
no
jo
8
w
a9
ka
tta
10
EO
S1
1
(y
es
te
rd
ay
)
(h
e)
(b
oo
k)
(b
or
ro
w
ed
)
(s
he
)
(b
ou
gh
t)
Source sentence
Figure 3: Example of label sequences that specify
spans from the CP to each NPC for the case of
Figure 2 (c). The labels (C, I, and N) in the boxes
are the label sequences.
each NPC in each sentence. Each label sequence
corresponds to one NPC. Therefore, if we identify
the label sequence that corresponds to the NP, we
can obtain the NP. The label sequences specify the
spans from the CP to each NPC using three kinds
of labels indicating the type of word positions in
the spans. The three kinds of labels, ?C, I, and N,?
are shown in Table 2. Figure 3 shows examples
of the label sequences for the case of Figure 2 (c).
In Figure 3, the label sequences are represented by
boxes and the elements of the sequences are labels.
The NPC is used as the label sequence ID for each
label sequence.
The label sequence can treat relative word or-
der. For example, the label sequence ID of 10 in
Figure 3 knows that karita exists to the left of the
NPC of 10. This is because karita6 carries a la-
bel I while katta10 carries a label N, and a position
with label I is defined as relatively closer to the CP
than a position with label N. By utilizing the label
sequence and corresponding words, the model can
reflect the effect of karita existing between the CP
and the NPC of 10 on the probability.
For the sequence model, karita (borrowed) and
katta (bought) in (c) and (d) in Figure 2 are not
conflict instances in training, whereas they are
conflict instances in training for the pair model.
The reason is as follows. In order to make the
probability of the NPC of 10 smaller than the NPC
of 6, instead of making the weight parameters for
the features with respect to the word at the position
of 10 with label N smaller than the weight param-
eters for the features with respect to the word at
the position of 6 with label N, the sequence model
can give negative weight parameters for the fea-
tures with respect to the word at the position of 6
with label I.
We use a sequence discrimination technique
based on CRF (Lafferty et al, 2001) to identify the
label sequence that corresponds to the NP. There
are two differences between our task and the CRF
task. One difference is that CRF discriminates la-
bel sequences that consist of labels from all of the
label candidates, whereas we constrain the label
sequences to sequences where the label at the CP
is C, the label at an NPC is N, and the labels be-
tween the CP and the NPC are I. The other dif-
ference is that CRF is designed for discriminat-
ing label sequences corresponding to the same ob-
ject sequence, whereas we do not assign labels to
words outside the spans from the CP to each NPC.
However, when we assume that another label such
as E has been assigned to the words outside the
spans and there are no features involving label E,
CRF with our label constraints can be applied to
our task. In this paper, the method designed to
discriminate label sequences corresponding to the
different word sequence lengths is called partial
CRF.
The sequence model based on partial CRF is de-
rived by extending the pair model. We introduce
the label l and extend the pair model to discrimi-
nating the label sequences. There are two exten-
sions to the pair model. One extension uses la-
bels. We suppose that label sequences specify the
spans from the CP to each NPC. We conjoined all
the feature templates in Table 1 with an additional
feature template ?li, lj? to include the labels into
features where li is the label corresponding to the
position of i. The other extension uses sequence.
In the pair model, the position pair of (i, j) is used
to derive features. In contrast, to descriminate la-
bel sequences in the sequence model, the position
pairs of (i, k), k ? {k|i < k ? j ? j ? k < i}
159
and (k, j), k ? {k|i ? k < j ? j < k ? i}
are used to derive features. Note that in the feature
templates in Table 1, i and j are used to specify
two positions. When features are used for the se-
quence model, one of the positions is regarded as
k.
The distortion probability for an NPC j being
the NP given a CP i and a source sentence S is
calculated as:
P (X = j|i, S) =
1
Zi
exp
( ?
k?M?{j}
wTf (i, k, S, o, d, li, lk)
+
?
k?M?{i}
wTf (k, j, S, o, d, lk, lj)
)
(2)
where
M =
{
{m|i < m < j} (i < j)
{m|j < m < i} (i > j)
and Zi = ?j?{j|1?j?n+1 ? j ?=i}(numerator of
Equation 2) is a normalization factor. Since j is
used as the label sequence ID, discriminating j
also means discriminating label sequence IDs.
The first term in exp(?) in Equation 2 considers
all of the word pairs located at i and other posi-
tions in the sequence, and also their context. The
second term in exp(?) in Equation 2 considers all
of the word pairs located at j and other positions
in the sequence, and also their context.
By designing our model to discriminate among
different length label sequences, our model can
naturally handle the effect of distances. Many fea-
tures are derived from a long label sequence be-
cause it will contain many labels between the CP
and the NPC. On the other hand, fewer features
are derived from a short label sequence because a
short label sequence will contain fewer labels be-
tween the CP and the NPC. The bias from these
differences provides important clues for learning
the effect of distances.7
7Note that the sequence model does not only consider
larger context than the pair model, but that it also considers
labels. The pair model does not discriminate labels, whereas
the sequence model uses label N and label I for the positions
except for the CP, depending on each situation. For example,
in Figure 3, at position 6, label N is used in the label sequence
ID of 6, but label I is used in the label sequence IDs of 7 to
11. Namely, even if they are at the same position, the labels
in the label sequences are different. The sequence model dis-
criminates the label differences.
BOS  kare  wa  pari  de  hon  wo  katta  EOS
BOS  he  bought  books  in  Paris  EOS
Source:
Target:
training data
Figure 4: Examples of supervised training data.
The lines represent word alignments. The English
side arrows point to the nearest word aligned on
the right.
3.4 Training Data for Discriminative
Distortion Model
To train our discriminative distortion model, su-
pervised training data is needed. The training data
is built from a parallel corpus and word alignments
between corresponding source words and target
words. Figure 4 shows examples of training data.
We select the target words aligned to the source
words sequentially from left to right (target side
arrows). Then, the order of the source words in
the target word order is decided (source side ar-
rows). The source sentence and the source side
arrows are the training data.
4 Experiment
In order to confirm the effects of our distortion
model, we conducted a series of Japanese to En-
glish (JE) and Chinese to English (CE) translation
experiments.8
4.1 Common Settings
We used the patent data for the Japanese to En-
glish and Chinese to English translation subtasks
from the NTCIR-9 Patent Machine Translation
Task (Goto et al, 2011). There were 2,000 sen-
tences for the test data and 2,000 sentences for the
development data.
Mecab9 was used for the Japanese morpholog-
ical analysis. The Stanford segmenter10 and tag-
ger11 were used for Chinese segmentation and
POS tagging. The translation model was trained
using sentences of 40 words or less from the train-
ing data. So approximately 2.05 million sen-
tence pairs consisting of approximately 54 million
8We conducted JE and CE translation as examples of
language pairs with different word orders and of languages
where there is a great need for translation into English.
9http://mecab.sourceforge.net/
10http://nlp.stanford.edu/software/segmenter.shtml
11http://nlp.stanford.edu/software/tagger.shtml
160
Japanese tokens whose lexicon size was 134k and
50 million English tokens whose lexicon size was
213k were used for JE. And approximately 0.49
million sentence pairs consisting of 14.9 million
Chinese tokens whose lexicon size was 169k and
16.3 million English tokens whose lexicon size
was 240k were used for CE. GIZA++ and grow-
diag-final-and heuristics were used to obtain word
alignments. In order to reduce word alignment er-
rors, we removed articles {a, an, the} in English
and particles {ga, wo, wa} in Japanese before per-
forming word alignments because these function
words do not correspond to any words in the other
languages. After word alignment, we restored the
removed words and shifted the word alignment po-
sitions to the original word positions. We used 5-
gram language models that were trained using the
English side of each set of bilingual training data.
We used an in-house standard phrase-based
SMT system compatible with the Moses decoder
(Koehn et al, 2007). The SMT weighting param-
eters were tuned by MERT (Och, 2003) using the
development data. To stabilize the MERT results,
we tuned three times by MERT using the first half
of the development data and we selected the SMT
weighting parameter set that performed the best on
the second half of the development data based on
the BLEU scores from the three SMT weighting
parameter sets.
We compared systems that used a common
SMT feature set from standard SMT features and
different distortion model features. The com-
mon SMT feature set consists of: four translation
model features, phrase penalty, word penalty, and
a language model feature. The compared different
distortion model features are: the linear distortion
cost model feature (LINEAR), the linear distortion
cost model feature and the six MSD bidirectional
lexical distortion model (Koehn et al, 2005) fea-
tures (LINEAR+LEX), the outbound and inbound
distortion model features discriminating nine dis-
tortion classes (Green et al, 2010) (9-CLASS), the
proposed pair model feature (PAIR), and the pro-
posed sequence model feature (SEQUENCE).
4.2 Training for the Proposed Models
Our distortion model was trained as follows: We
used 0.2 million sentence pairs and their word
alignments from the data used to build the trans-
lation model as the training data for our distortion
models. The features that were selected and used
were the ones that had been counted12, using the
feature templates in Table 1, at least four times
for all of the (i, j) position pairs in the training
sentences. We conjoined the features with three
types of label pairs ?C, I?, ?I,N?, or ?C,N? as in-
stances of the feature template ?li, lj? to produce
features for SEQUENCE. The L-BFGS method
(Liu and Nocedal, 1989) was used to estimate the
weight parameters of maximum entropy models.
The Gaussian prior (Chen and Rosenfeld, 1999)
was used for smoothing.
4.3 Training for the Compared Models
For 9-CLASS, we used the same training data as
for our distortion models. Let ti be the part of
speech of si. We used the following feature tem-
plates to produce features for the outbound model:
?si?2?, ?si?1?, ?si?, ?si+1?, ?si+2?, ?ti?, ?ti?1, ti?,
?ti, ti+1?, and ?si, ti?. These feature templates corre-
spond to the components of the feature templates
of our distortion models. In addition to these fea-
tures, we used a feature consisting of the relative
source sentence position as the feature used by
(Green et al, 2010). The relative source sentence
position is discretized into five bins, one for each
quintile of the sentence. For the inbound model13,
i of the feature templates was changed to j. Fea-
tures occurring four or more times in the train-
ing sentences were used. The maximum entropy
method with Gaussian prior smoothing was used
to estimate the model parameters.
The MSD bidirectional lexical distortion model
was built using all of the data used to build the
translation model.
4.4 Results and Discussion
We evaluated translation quality based on the case-
insensitive automatic evaluation score BLEU-4
(Papineni et al, 2002). We used distortion lim-
its of 10, 20, 30, and unlimited (?), which limited
the number of words for word reordering to a max-
imum number. Table 3 presents our main results.
The proposed SEQUENCE outperformed the base-
lines for both Japanese to English and Chinese to
English translation. This demonstrates the effec-
tiveness of the proposed SEQUENCE. The scores
of the proposed SEQUENCE were higher than those
12When we counted features for selection, we only counted
features that were from the feature templates of ?si, sj?,
?ti, tj?, ?si, ti, tj?, and ?sj , ti, tj? in Table 1 when j was not
the NP, in order to avoid increasing the number of features.
13The inbound model is explained in footnote 5.
161
Japanese-English Chinese-English
Distortion limit 10 20 30 ? 10 20 30 ?
LINEAR 27.98 27.74 27.75 27.30 29.18 28.74 28.31 28.33
LINEAR+LEX 30.25 30.37 30.17 29.98 30.81 30.24 30.16 30.13
9-CLASS 30.74 30.98 30.92 30.75 31.80 31.56 31.31 30.84
PAIR 31.62 32.36 31.96 32.03 32.51 32.30 32.25 32.32
SEQUENCE 32.02 32.96 33.29 32.81 33.41 33.44 33.35 33.41
Table 3: Evaluation results for each method. The values are case-insensitive BLEU scores. Bold numbers
indicate no significant difference from the best result in each language pair using the bootstrap resampling
test at a significance level ? = 0.01 (Koehn, 2004).
Japanese-English Chinese-English
HIER 30.47 32.66
Table 4: Evaluation results for hierarchical phrase-
based SMT.
of the proposed PAIR. This confirms the effective-
ness for considering relative word order and words
between the CP and an NPC. The proposed PAIR
outperformed 9-CLASS, confirming that consider-
ing both the word specified at the CP and the word
specified at the NPC simultaneously was more ef-
fective than that of 9-CLASS.
For translating between languages with widely
different word orders such as Japanese and En-
glish, a small distortion limit is undesirable be-
cause there are cases where correct translations
cannot be produced with a small distortion limit,
since the distortion limit prunes the search space
that does not meet the constraint. Therefore,
a large distortion limit is required to translate
correctly. For JE translation, our SEQUENCE
achieved significantly better results at distortion
limits of 20 and 30 than that at a distortion limit
of 10, while the baseline systems of LINEAR,
LINEAR+LEX, and 9-CLASS did not achieve this.
This indicate that SEQUENCE could treat long
distance reordering candidates more appropriately
than the compared methods.
We also tested hierarchical phrase-based SMT
(Chiang, 2007) (HIER) using the Moses imple-
mentation. The common data was used to train
HIER. We used unlimited max-chart-span for the
system setting. Results are given in Table 4. Our
SEQUENCE outperformed HIER. The gain for JE
was large but the gain for CE was modest. Since
phrase-based SMT is generally faster in decod-
ing speed than hierarchical phrase-based SMT,
achieving better or comparable scores is worth-
Distortion
P
r
o
b
a
b
i
l
i
t
y
Figure 5: Average probabilities for large distortion
for Japanese-English translation.
while.
To investigate the tolerance for sparsity of the
training data, we reduced the training data for
the sequence model to 20,000 sentences for JE
translation.14 SEQUENCE using this model with
a distortion limit of 30 achieved a BLEU score
of 32.22.15 Although the score is lower than the
score of SEQUENCE with a distortion limit of 30
in Table 3, the score was still higher than those
of LINEAR, LINEAR+LEX, and 9-CLASS for JE
in Table 3. This indicates that the sequence model
also works even when the training data is not large.
This is because the sequence model considers not
only the word at the CP and the word at an NPC
but also rich context, and rich context would be ef-
fective even for a smaller set of training data.
14We did not conduct experiments using larger training
data because there would have been a very high computa-
tional cost to build models using the L-BFGS method.
15To avoid effects from differences in the SMT weighting
parameters, we used the same SMT weighting parameters for
SEQUENCE, with a distortion limit of 30, in Table 3.
162
To investigate how well SEQUENCE learns the
effect of distance, we checked the average distor-
tion probabilities for large distortions of j ? i? 1.
Figure 5 shows three kinds of probabilities for dis-
tortions from 3 to 20 for Japanese-English transla-
tion. One is the average distortion probabilities
in the Japanese test sentences for each distortion
for SEQUENCE, and another is this for PAIR. The
third (CORPUS) is the probabilities for the actual
distortions in the training data that were obtained
from the word alignments used to build the trans-
lation model. The probability for a distortion for
CORPUS was calculated by the number of the dis-
tortion divided by the total number of distortions
in the training data.
Figure 5 shows that when a distance class fea-
ture used in the model was the same (e.g., distor-
tions from 5 to 20 were the same distance class
feature), PAIR produced average distortion prob-
abilities that were almost the same. In contrast,
the average distortion probabilities for SEQUENCE
decreased when the lengths of the distortions in-
creased, even if the distance class feature was
the same, and this behavior was the same as that
of CORPUS. This confirms that the proposed
SEQUENCE could learn the effect of distances ap-
propriately from the training data.16
5 Related Works
We discuss related works other than discussed in
Section 2. Xiong et al (2012) proposed a model
predicting the orientation of an argument with re-
spect to its verb using a parser. Syntactic struc-
tures and predicate-argument structures are useful
for reordering. However, orientations do not han-
dle distances. Thus, our distortion model does not
compete against the methods predicting orienta-
tions using a parser and would assist them if used
16We also checked the average distortion probabilities for
the 9-CLASS outbound model in the Japanese test sentences
for Japanese-English translation. We averaged the average
probabilities for distortions in a distortion span of [4, 6] and
also averaged those in a distortion span of [7, 20], where the
distortions in each span are in the same distortion class. The
average probability for [4, 6] was 0.058 and that for [7, 20]
was 0.165. From CORPUS, the average probabilities in the
training data for each distortion in [4, 6] were higher than
those for each distortion in [7, 20]. However, the converse
was true for the comparison between the two average prob-
abilities for the outbound model. This is because the sum
of probabilities for distortions from 7 and above was larger
than the sum of probabilities for distortions from 4 to 6 in the
training data. This comparison indicates that the 9-CLASS
outbound model could not appropriately learn the effects of
large distances for JE translation.
together.
There are word reordering constraint methods
using ITG (Wu, 1997) for phrase-based SMT
(Zens et al, 2004; Yamamoto et al, 2008; Feng et
al., 2010). These methods consider sentence level
consistency with respect to ITG. The ITG con-
straint does not consider distances of reordering
and was used with other distortion models. Our
distortion model does not consider sentence level
consistency, so our distortion model and ITG con-
straint methods are thought to be complementary.
There are tree-based SMT methods (Chiang,
2007; Galley et al, 2004; Liu et al, 2006). In
many cases, tree-based SMT methods do not use
the distortion models that consider reordering dis-
tance apart from translation rules because it is not
trivial to use distortion scores considering the dis-
tances for decoders that do not generate hypothe-
ses from left to right. If it could be applied to these
methods, our distortion model might contribute to
tree-based SMT methods. Investigating the effects
will be for future work.
6 Conclusion
This paper described our distortion models for
phrase-based SMT. Our sequence model simply
consists of only one probabilistic model, but it can
consider rich context. Experiments indicate that
our models achieved better performance and the
sequence model could learn the effect of distances
appropriately. Since our models do not require a
parser, they can be applied to many languages. Fu-
ture work includes application to other language
pairs, incorporation into ITG constraint methods
and other reordering methods, and application to
tree-based SMT methods.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 529?536, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):39?71, March.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical report.
163
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Theodoros Evgniou and Massimiliano Pontil. 2002.
Learning preference relations from data. Neural
Nets Lecture Notes in Computer Science, 2486:23?
32.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010.
An efficient shift-reduce decoding algorithm for
phrased-based machine translation. In Coling 2010:
Posters, pages 285?293, Beijing, China, August.
Coling 2010 Organizing Committee.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 273?280, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR-9, pages 559?578.
Spence Green, Michel Galley, and Christopher D.Man-
ning. 2010. Improved models of distortion cost
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 867?875, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of 18th International Conference on Machine Learn-
ing, pages 282?289.
D.C. Liu and J. Nocedal. 1989. On the limited memory
method for large scale optimization. Mathematical
Programming B, 45(3):503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609?616, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-
hesan Niranjan. 2009. Handling phrase reorder-
ings for machine translation. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
241?244, Suntec, Singapore, August. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 101?
104, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of Coling 2004, pages 508?
514, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 902?911, Jeju
Island, Korea, July. Association for Computational
Linguistics.
164
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523?530, Toulouse,
France, July. Association for Computational Lin-
guistics.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro
Sumita. 2008. Imposing constraints from the source
tree on ITG constraints for SMT. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
1?9, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In Pro-
ceedings of Coling 2004, pages 205?211, Geneva,
Switzerland, Aug 23?Aug 27. COLING.
165
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155?160,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Dependency-based Pre-ordering for Chinese-English Machine Translation
Jingsheng Cai
??
Masao Utiyama
?
Eiichiro Sumita
?
Yujie Zhang
?
?
School of Computer and Information Technology, Beijing Jiaotong University
?
National Institute of Information and Communications Technology
joycetsai99@gmail.com
{mutiyama, eiichiro.sumita}@nict.go.jp
yjzhang@bjtu.edu.cn
Abstract
In statistical machine translation (SMT),
syntax-based pre-ordering of the source
language is an effective method for deal-
ing with language pairs where there are
great differences in their respective word
orders. This paper introduces a novel
pre-ordering approach based on depen-
dency parsing for Chinese-English SMT.
We present a set of dependency-based pre-
ordering rules which improved the BLEU
score by 1.61 on the NIST 2006 evalua-
tion data. We also investigate the accuracy
of the rule set by conducting human eval-
uations.
1 Introduction
SMT systems have difficulties translating between
distant language pairs such as Chinese and En-
glish. The reason for this is that there are great
differences in their word orders. Reordering there-
fore becomes a key issue in SMT systems between
distant language pairs.
Previous work has shown that the approaches
tackling the problem by introducing a pre-ordering
procedure into phrase-based SMT (PBSMT) were
effective. These pre-ordering approaches first
parse the source language sentences to create parse
trees. Then, syntactic reordering rules are ap-
plied to these parse trees with the goal of re-
ordering the source language sentences into the
word order of the target language. Syntax-based
pre-ordering by employing constituent parsing
have demonstrated effectiveness in many language
pairs, such as English-French (Xia and McCord,
2004), German-English (Collins et al, 2005),
Chinese-English (Wang et al, 2007; Zhang et al,
2008), and English-Japanese (Lee et al, 2010).
?
This work was done when the first author was on an
internship in NICT.
As a kind of constituent structure, HPSG (Pol-
lard and Sag, 1994) parsing-based pre-ordering
showed improvements in SVO-SOV translations,
such as English-Japanese (Isozaki et al, 2010; Wu
et al, 2011) and Chinese-Japanese (Han et al,
2012). Since dependency parsing is more concise
than constituent parsing in describing sentences,
some research has used dependency parsing in
pre-ordering approaches for language pairs such
as Arabic-English (Habash, 2007), and English-
SOV languages (Xu et al, 2009; Katz-Brown et
al., 2011). The pre-ordering rules can be made
manually (Collins et al, 2005; Wang et al, 2007;
Han et al, 2012) or extracted automatically from
a parallel corpus (Xia and McCord, 2004; Habash,
2007; Zhang et al, 2007; Wu et al, 2011).
The purpose of this paper is to introduce a novel
dependency-based pre-ordering approach through
creating a pre-ordering rule set and applying it to
the Chinese-English PBSMT system. Experiment
results showed that our pre-ordering rule set im-
proved the BLEU score on the NIST 2006 evalua-
tion data by 1.61. Moreover, this rule set substan-
tially decreased the total times of rule application
about 60%, compared with a constituent-based ap-
proach (Wang et al, 2007). We also conducted hu-
man evaluations in order to assess its accuracy. To
our knowledge, our manually created pre-ordering
rule set is the first Chinese-English dependency-
based pre-ordering rule set.
The most similar work to this paper is that of
Wang et al (2007). They created a set of pre-
ordering rules for constituent parsers for Chinese-
English PBSMT. In contrast, we propose a set of
pre-ordering rules for dependency parsers. We
argue that even though the rules by Wang et al
(2007) exist, it is almost impossible to automati-
cally convert their rules into rules that are appli-
cable to dependency parsers. In fact, we aban-
doned our initial attempts to automatically convert
their rules into rules for dependency parsers, and
155
(a) A constituent parse tree
(b) Stanford typed dependency parse tree
Figure 1: A constituent parse tree and its cor-
responding Stanford typed dependency parse tree
for the same Chinese sentence.
spent more than two months discovering the rules
introduced in this paper. By applying our rules
and Wang et al?s rules, one can use both depen-
dency and constituency parsers for pre-ordering in
Chinese-English PBSMT.
This is especially important on the point of the
system combination of PBSMT systems, because
the diversity of outputs from machine translation
systems is important for system combination (Cer
et al, 2013). By using both our rules and Wang et
al.?s rules, one can obtain diverse machine trans-
lation results because the pre-ordering results of
these two rule sets are generally different.
Another similar work is that of (Xu et al, 2009).
They created a pre-ordering rule set for depen-
dency parsers from English to several SOV lan-
guages. In contrast, our rule set is for Chinese-
English PBSMT. That is, the direction of transla-
tion is opposite. Because there are a lot of lan-
guage specific decisions that reflect specific as-
pects of the source language and the language pair
combination, our rule set provides a valuable re-
source for pre-ordering in Chinese-English PB-
SMT.
2 Dependency-based Pre-ordering Rule
Set
Figure 1 shows a constituent parse tree and its
Stanford typed dependency parse tree for the same
Figure 2: An example of a preposition phrase with
a plmod structure. The phrase translates into ?in
front of the US embassy?.
Chinese sentence. As shown in the figure, the
number of nodes in the dependency parse tree
(i.e. 9) is much fewer than that in its correspond-
ing constituent parse tree (i.e. 17). Because de-
pendency parse trees are generally more concise
than the constituent ones, they can conduct long-
distance reorderings in a finer way. Thus, we at-
tempted to conduct pre-ordering based on depen-
dency parsing. There are two widely-used de-
pendency systems ? Stanford typed dependencies
and CoNLL typed dependencies. For Chinese,
there are 45 types of grammatical relations for
Stanford typed dependencies (Chang et al, 2009)
and 25 for CoNLL typed dependencies. As we
thought that Stanford typed dependencies could
describe language phenomena more meticulously
owing to more types of grammatical relations, we
preferred to use it for searching candidate pre-
ordering rules.
We designed two types of formats in our
dependency-based pre-ordering rules. They are:
Type-1: x : y
Type-2: x - y
Here, both x and y are dependency relations
(e.g., plmod or lobj in Figure 2). We define the
dependency structure of a dependency relation as
the structure containing the dependent word (e.g.,
the word directly indicated by plmod, or ??? in
Figure 2) and the whole subtree under the depen-
dency relation (all of the words that directly or
indirectly depend on the dependent word, or the
words under ??? in Figure 2). Further, we define
X and Y as the corresponding dependency struc-
tures of the dependency relations x and y, respec-
tively. We define X\Y as structure X except Y. For
example, in Figure 2, let x and y denote plmod and
lobj dependency relations, then X represents ???
and all words under ???, Y represents ?????
and all words under ?????, and X\Y represents
156
Figure 3: An example of rcmod structure within
an nsubj structure. The phrase translates into ?a
senior official close to Sharon said?.
???. For Type-1, Y is a sub-structure of X. The
rule repositions X\Y to the position before Y. For
Type-2, X and Y are ordered sibling structures un-
der a same parent node. The rule repositions X to
the position after Y.
We obtained rules as the following steps:
1 Search the Chinese dependency parse trees
in the corpus and rank all of the structures
matching the two types of rules respectively
according to their frequencies. Note that
while calculating the frequencies of Type-
1 structures, we dismissed the structures in
which X occurred before Y originally.
2 Filtration. 1) Filter out the structures which
occurred less than 5,000 times. 2) Filter
out the structures from which it was almost
impossible to derive candidate pre-ordering
rules because x or y was an ?irrespective? de-
pendency relation, for example, root, conj, cc
and so on.
3 Investigate the remaining structures. For each
kind of structure, we selected some of the
sample dependency parse trees that contained
it, tried to restructure the parse trees accord-
ing to the matched rule and judged the re-
ordered Chinese phrases. If the reordering
produced a Chinese phrase that had a closer
word order to that of the English one, this
structure would be a candidate pre-ordering
rule.
4 Conduct primary experiments which used the
same training set and development set as the
experiments described in Section 3. In the
primary experiments, we tested the effective-
ness of the candidate rules and filtered the
ones that did not work based on the BLEU
scores on the development set.
Figure 4: An example of rcmod structure with a
preposition modifier. The phrase translates into ?a
press conference held in Kabul?.
As a result, we obtained eight pre-ordering rules
in total, which can be divided into three depen-
dency relation categories. They are: plmod (lo-
calizer modifier of a preposition), rcmod (relative
clause modifier) and prep (preposition modifer).
Each of these categories are discussed in detail be-
low.
plmod Figure 2 shows an example of a preposi-
tional phrase with a plmod structure, which trans-
lates literally into ?in the US embassy front?. In
Chinese, the dependent word of a plmod relation
(e.g., ??? in Figure 2) occurs in the last position
of the prepositional phrase. However, in English,
this kind of word (e.g., ?front? in the caption of
Figure 2) always occur directly after prepositions,
which is to say, in the second position in a preposi-
tional phrase. Therefore, we applied a rule plmod
: lobj (localizer object) to reposition the depen-
dent word of the plmod relation (e.g., ??? in Fig-
ure 2) to the position before the lobj structure (e.g.,
??? ???? in Figure 2). In this case, it also
comes directly after the preposition. Similarly, we
created a rule plmod : lccomp (clausal comple-
ment of a localizer).
rcmod Figure 3 shows an example of an rcmod
structure under an nsubj (nominal subject) struc-
ture. Here ?mw? means ?measure word?. As
shown in the figure, relative clause modifiers in
Chinese (e.g., ??? ?? ?? in Figure 3) oc-
curs before the noun being modified, which is in
contrast to English (e.g., ?close to Sharon? in the
caption of Figure 3), where they come after. Thus,
we introduced a series of rules NOUN : rcmod
to restructure rcmod structures so that the noun
is moved to the head. In this example, with the
application of an nsubj : rcmod rule, the phrase
can be translated into ?a senior official close to
Sharon say?, which has a word order very close
to English. Since a noun can be nsubj, dobj (di-
rect object), pobj (prepositional object) and lobj
157
Type System Parser BLEU Counts #Sent.
- No pre-ordering - 29.96 - -
Constituent WR07 Berkeley 31.45 2,561,937 852,052
Dependency OUR DEP 1 Berkeley Const. 31.54 978,013 556,752
OUR DEP 2 Mate 31.57 947,441 547,084
Table 1: The comparison of four systems, including the performance (BLEU) on the test set, the total
count of each rule set and the number of sentences they were applied to on the training set.
Figure 5: An example of verb phrase with a
preposition modifier. The phrase translates into
?Musharraf told reporters here?.
in Stanford typed dependencies, we created four
rules from the NOUN pattern. Note that for some
preposition modifiers, we needed a rule rcmod :
prep to conduct the same work. For instance, the
Chinese phrase in Figure 4 can be translated into
?hold in Kabul press conference? with the appli-
cation of this rule.
prep Within verb phrases, the positions of prep
structures are quite different between Chinese and
English. Figure 5 shows an example of a verb
phrase with a preposition modifier (prep), which
literally translates into ?Musharraf at this place tell
reporter?. Recognizing that prep structures occur
before the verb in Chinese (e.g., ????? in Fig-
ure 5) but after the verb in English (usually in the
last position of a verb phrase, e.g., ?here? in the
caption of Figure 5), we applied a rule prep - dobj
to reposition prep structures after their sibling dobj
structures.
In summary, the dependency-based pre-
ordering rule set has eight rules: plmod : lobj,
plmod : lccomp, nsubj : rcmod, dobj : rcmod,
pobj : rcmod, lobj : rcmod, rcmod : prep, and
prep - dobj.
3 Experiments
We used the MOSES PBSMT system (Koehn et
al., 2007) in our experiments. The training data,
which included those data used in Wang et al
(2007), contained 1 million pairs of sentences ex-
tracted from the Linguistic Data Consortium?s par-
allel news corpora. Our development set was
the official NIST MT evaluation data from 2002
to 2005, consisting of 4476 Chinese-English sen-
tences pairs. Our test set was the NIST 2006 MT
evaluation data, consisting of 1664 sentence pairs.
We employed the Stanford Segmenter
1
to segment
all of the data sets. For evaluation, we used BLEU
scores (Papineni et al, 2002).
We implemented the constituent-based pre-
ordering rule set in Wang et al (2007) for compar-
ison, which is called WR07 below. The Berkeley
Parser (Petrov et al, 2006) was employed for pars-
ing the Chinese sentences. For training the Berke-
ley Parser, we used Chinese Treebank (CTB) 7.0.
We conducted our dependency-based pre-
ordering experiments on the Berkeley Parser and
the Mate Parser (Bohnet, 2010), which were
shown to be the two best parsers for Stanford
typed dependencies (Che et al, 2012). First, we
converted the constituent parse trees in the re-
sults of the Berkeley Parser into dependency parse
trees by employing a tool in the Stanford Parser
(Klein and Manning, 2003). For the Mate Parser,
POS tagged inputs are required both in training
and in inference. Thus, we then extracted the
POS information from the results of the Berke-
ley Parser and used these as the pre-specified POS
tags for the Mate Parser. Finally, we applied our
dependency-based pre-ordering rule set to the de-
pendency parse trees created from the converted
Berkeley Parser and the Mate Parser, respectively.
Table 1 presents a comparison of the system
without pre-ordering, the constituent system us-
ing WR07 and two dependency systems employ-
ing the converted Berkeley Parser and the Mate
Parser, respectively. It shows the BLEU scores on
the test set and the statistics of pre-ordering on the
training set, which includes the total count of each
rule set and the number of sentences they were ap-
1
http://nlp.stanford.edu/software/segmenter.shtml
158
Category Count Correct Incorrect Accuracy
plmod 42 26 16 61.9%
rcmod 89 49 40 55.1%
prep 54 36 18 66.7%
All 185 111 74 60.0%
Table 2: Accuracy of the dependency-based pre-ordering rules on a set of 200 sentences randomly se-
lected from the development set.
plied to. Both of our dependency systems outper-
formed WR07 slightly but were not significant at
p = 0.05. However, both of them substantially de-
creased the total times about 60% (or 1,600,000)
for pre-ordering rule applications on the training
set, compared with WR07. In our opinion, the rea-
son for the great decrease was that the dependency
parse trees were more concise than the constituent
parse trees in describing sentences and they could
also describe the reordering at the sentence level in
a finer way. In contrast, the constituent parse trees
were more redundant and they needed more nodes
to conduct long-distance reordering. In this case,
the affect of the performance of the constituent
parsers on pre-ordering is larger than that of the
dependency ones so that the constituent parsers are
likely to bring about more incorrect pre-orderings.
Similar to Wang et al (2007), we carried out
human evaluations to assess the accuracy of our
dependency-based pre-ordering rules by employ-
ing the system ?OUR DEP 2? in Table 1. The
evaluation set contained 200 sentences randomly
selected from the development set. Among them,
107 sentences contained at least one rule and the
rules were applied 185 times totally. Since the
accuracy check for dependency parse trees took
great deal of time, we did not try to select er-
ror free (100% accurately parsed) sentences. A
bilingual speaker of Chinese and English looked
at an original Chinese phrase and the pre-ordered
one with their corresponding English phrase and
judged whether the pre-ordering obtained a Chi-
nese phrase that had a closer word order to the En-
glish one. Table 2 shows the accuracies of three
categories of our dependency-based pre-ordering
rules. The overall accuracy of this rule set is
60.0%, which is almost at the same level as the
WR07 rule set (62.1%), according to the similar
evaluation (200 sentences and one annotator) con-
ducted in Wang et al (2007). Notice that some
of the incorrect pre-orderings may be caused by
erroneous parsing as also suggested by Wang et
al. (2007). Through human evaluations, we found
that 19 out of the total 74 incorrect pre-orderings
resulted from errors in parsing. Among them, 13
incorrect pre-orderings applied the rules of the rc-
mod category. The analysis suggests that we need
to introduce constraints on the rule application of
this category in the future.
4 Conclusion
In this paper, we introduced a novel pre-ordering
approach based on dependency parsing for a
Chinese-English PBSMT system. The results
showed that our approach achieved a BLEU score
gain of 1.61. Moreover, our dependency-based
pre-ordering rule set substantially decreased the
time for applying pre-ordering rules about 60%
compared with WR07, on the training set of 1M
sentences pairs. The overall accuracy of our rule
set is 60.0%, which is almost at the same level as
the WR07 rule set. These results indicated that
dependency parsing is more effective for conduct-
ing pre-ordering for Chinese-English PBSMT. Al-
though our work focused on Chinese, the ideas can
also be applied to other languages.
In the future, we attempt to create more efficient
pre-ordering rules by exploiting the rich informa-
tion in dependency structures.
Acknowledgments
We thank the anonymous reviewers for their valu-
able comments and suggestions. This work is sup-
ported in part by the International Science & Tech-
nology Cooperation Program of China (Grant No.
2014DFA11350) and Key Lab of Intelligent In-
formation Processing of Chinese Academy of Sci-
ences (CAS), Institute of Computing Technology,
CAS, Beijing 100190, China.
References
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
159
ings of the 23rd International Conference on Com-
putational Linguistics (COLING 2010).
Daniel Cer, Christopher D. Manning, and Dan Juraf-
sky. 2013. Positive Diversity Tuning for Machine
Translation System Combination. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation (WMT 2013).
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the HLT-NAACL Workshop
on Syntax and Structure in Statistical Translation,
pages 51-59.
Wanxiang Che, Valentin Spitkovsky, and Ting Liu.
2012. A comparison of Chinese parsers for Stan-
ford dependencies. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics, pages 11-16.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 531-540.
Dan Han, Katsuhito Sudoh, Xianchao Wu, Kevin Duh,
Hajime Tsukada, and Masaaki Nagata. 2012. Head
Finalization reordering for Chinese-to-Japanese ma-
chine translation. In Proceedings of SSST-6, Sixth
Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 57-66.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Proceedings of the
11th Machine Translation Summit (MT-Summit).
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A simple re-
ordering rule for SOV languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 250-257.
Jason Katz-Brown, Slav Petrov, Ryan McDonald,
Franz J. Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 183-
192.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 423-430.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177-180.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010. Constituent reordering and syntax models for
English-to-Japanese statistical machine translation.
In Proceedings of the 23rd International of Confer-
ence on Computational Linguistics, pages 626-634.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311-318.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433-440.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 737-745.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
preordering rules from predicate-argument struc-
tures. In Proceedings of 5th International Joint Con-
ference on Natural Language Processing, pages 29-
37.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling 2004,
pages 508-514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz J.
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings of HLT-NAACL, pages 245-253.
Jiajun Zhang, Chengqing Zong, and Shoushan Li.
2008. Sentence type based reordering model for sta-
tistical machine translation. In Proceedings of the
22nd International Conference on Computational
Linguistics, pages 1089-1096.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2011.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statisti-
cal machine translation. In HLT-NAACL Workshop
on Syntax and Structure in Statistical Translation,
pages 1-8.
160
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752?758,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Empirical Study of Unsupervised Chinese Word Segmentation Methods
for SMT on Large-scale Corpora
Xiaolin Wang Masao Utiyama Andrew Finch Eiichiro Sumita
National Institute of Information and Communications Technology
{xiaolin.wang,mutiyama,andrew.finch,eiichiro.sumita}@nict.go.jp
Abstract
Unsupervised word segmentation (UWS)
can provide domain-adaptive segmenta-
tion for statistical machine translation
(SMT) without annotated data, and bilin-
gual UWS can even optimize segmenta-
tion for alignment. Monolingual UWS ap-
proaches of explicitly modeling the proba-
bilities of words through Dirichlet process
(DP) models or Pitman-Yor process (PYP)
models have achieved high accuracy, but
their bilingual counterparts have only been
carried out on small corpora such as ba-
sic travel expression corpus (BTEC) due to
the computational complexity. This paper
proposes an efficient unified PYP-based
monolingual and bilingual UWS method.
Experimental results show that the pro-
posed method is comparable to super-
vised segmenters on the in-domain NIST
OpenMT corpus, and yields a 0.96 BLEU
relative increase on NTCIR PatentMT cor-
pus which is out-of-domain.
1 Introduction
Many languages, especially Asian languages such
as Chinese, Japanese and Myanmar, have no ex-
plicit word boundaries, thus word segmentation
(WS), that is, segmenting the continuous texts of
these languages into isolated words, is a prerequi-
site for many natural language processing applica-
tions including SMT.
Though supervised-learning approaches which
involve training segmenters on manually seg-
mented corpora are widely used (Chang et al,
2008), yet the criteria for manually annotat-
ing words are arbitrary, and the available anno-
tated corpora are limited in both quantity and
genre variety. For example, in machine transla-
tion, there are various parallel corpora such as
BTEC for tourism-related dialogues (Paul, 2008)
and PatentMT in the patent domain (Goto et
al., 2011)1, but researchers working on Chinese-
related tasks often use the Stanford Chinese seg-
menter (Tseng et al, 2005) which is trained on a
small amount of annotated news text.
In contrast, UWS, spurred by the findings that
infants are able to use statistical cues to determine
word boundaries (Saffran et al, 1996), relies on
statistical criteria instead of manually crafted stan-
dards. UWS learns from unsegmented raw text,
which are available in large quantities, and thus
it has the potential to provide more accurate and
adaptive segmentation than supervised approaches
with less development effort being required.
The approaches of explicitly modeling the
probability of words(Brent, 1999; Venkataraman,
2001; Goldwater et al, 2006; Goldwater et al,
2009; Mochihashi et al, 2009) significantly out-
performed a heuristic approach (Zhao and Kit,
2008) on the monolingual Chinese SIGHAN-MSR
corpus (Emerson, 2005), which inspired the work
of this paper.
However, bilingual approaches that model word
probabilities suffer from computational complex-
ity. Xu et al (2008) proposed a bilingual method
by adding alignment into the generative model, but
was only able to test it on small-scale BTEC data.
Nguyen et al (2010) used the local best alignment
to increase the speed of the Gibbs sampling in
training but the impact on accuracy was not ex-
plored.
This paper is dedicated to bilingual UWS on
large-scale corpora to support SMT. To this end,
we model bilingual UWS under a similar frame-
work with monolingual UWS in order to improve
efficiency, and replace Gibbs sampling with ex-
pectation maximization (EM) in training.
We aware that variational bayes (VB) may be
used for speeding up the training of DP-based
1http://ntcir.nii.ac.jp/PatentMT
752
or PYP-based bilingual UWS. However, VB re-
quires formulating the m expectations of (m?1)-
dimensional marginal distributions, where m is
the number of hidden variables. For UWS, the
hidden variables are indicators that identify sub-
strings of sentences in the corpus as words. These
variables are large in number and it is not clear
how to apply VB to UWS, and as far the authors
aware there is no previous work related to the ap-
plication of VB to monolingual UWS. Therefore,
we have not explored VB methods in this paper,
but we do show that our method is superior to the
existing methods.
The contributions of this paper include,
? state-of-the-art accuracy in monolingual
UWS;
? the first bilingual UWS method practical for
large corpora;
? improvement of BLEU scores compared
to supervised Stanford Chinese word seg-
menter.
2 Methods
This section describes our unified monolingual
and bilingual UWS scheme. Table 1 lists the main
notation. The set F is chosen to represent an un-
segmented foreign language sentence (a sequence
of characters), because an unsegmented sentence
can be seen as the set of all possible segmentations
of the sentence denoted F , i.e. F ? F .
Notation Meaning
F an unsegmented foreign sentence
F
k
?
k
unsegmented substring of the un-
derlying string of F from k to k?
F a segmented foreign sentence
f
j
the j-th foreign word
M monolingual segmentation model
P
M
(x) probability of x being a word ac-
cording to M
E a tokenized English sentence
e
i
the i-th English word
(F ,E) a bilingual sentence pair
B bilingual segmentation model
P
B
(x|e
i
) probability of x being a word ac-
cording to B given e
i
Table 1: Main Notation.
Monolingual and bilingual WS can be formu-
lated as follows, respectively,
?
F (F) = argmax
F?F
P (F |F ,M), (1)
?
F (F , E) = argmax
F?F
?
a
P (F, a|F , E,B), (2)
where a is an alignment between F and E. The
English sentence E is used in the generation of a
segmented sentence F .
UWS learns models by maximizing the likeli-
hood of the unsegmented corpus, formulated as,
?
M = argmax
M
?
F?F
(
?
F?F
P (F |M)
)
, (3)
?
B = argmax
B
?
(F ,E)?B
(
?
F?F
?
a
P (F, a|F , E,B)
)
.
(4)
Our method of learning M and B proceeds in a
similar manner to the EM algorithm. The follow-
ing two operations are performed iteratively for
each sentence (pair).
? Exclude the previous expected counts of the
current sentence (pair) from the model, and
then derive the current sentence in all pos-
sible ways, calculating the new expected
counts for the words (see Section 2.1), that
is, we calculate the expected probabilities of
the Fk?
k
being words given the data excluding
F , i.e. E
F/{F}
(P (F
k
?
k
|F)) = P (F
k
?
k
|F ,M)
in a similar manner to the marginalization in
the Gibbs sampling process which we are re-
placing;
? Update the respective model M or B accord-
ing to these expectations (see Section2.2).
2.1 Expectation
2.1.1 Monolingual Expectation
P (F
k
?
k
|F ,M) is the marginal probability of all
the possible F ? F that contain Fk?
k
as a word,
which can be calculated efficiently through dy-
namic programming (the process is similar to the
foreward-backward algorithm in training a hidden
Markov model (HMM) (Rabiner, 1989)):
P
a
(k) =
U
?
u=1
P
a
(k ? u)P
M
(F
k
k?u
)
P
b
(k
?
) =
U
?
u=1
P
b
(k
?
+ u)P
M
(F
k
?
+u
k
?
)
P (F
k
?
k
|F ,M) = P
a
(k)P
M
(F
k
?
k
)P
b
(k
?
), (5)
753
where U is the predefined maximum length of for-
eign language words, P
a
(k) and P
b
(k
?
) are the
forward and backward probabilities, respectively.
This section uses a unigram model for description
convenience, but the method can be extended to
n-gram models.
2.1.2 Bilingual Expectation
P (F
k
?
k
|F , E,B) is the marginal probability of all
the possible F ? F that contain Fk?
k
as a word and
are aligned with E, formulated as:
P (F
k
?
k
|F , E,B) =
?
F?F
F
k
?
k
?F
?
a
P (F, a|E,B)
?
?
F?F
F
j
k
=F
k
?
k
?
a
J
?
j=1
P (a
j
|j, I, J)P
B
(f
j
|e
a
j
)
=
?
F?F
f
j
k
=F
k
?
k
J
?
j=1
?
a
P (a
j
|j, I, J)P
B
(f
j
|e
a
j
),
(6)
where J and I are the number of foreign and En-
glish words, respectively, and a
j
is the position of
the English word that is aligned to f
j
in the align-
ment a. For the alignment we employ an approx-
imation to IBM model 2 (Brown et al, 1993; Och
and Ney, 2003) described below.
We define the conditional probability of f
j
given the corresponding English sentence E and
the model B as:
P
B
(f
j
|E) =
?
a
P (a
j
|j, I, J)P
B
(f
j
|e
a
j
) (7)
Then, the previous dynamic programming
method can be extended to the bilingual expecta-
tion
P
a
(k|E) =
U
?
u=1
P
a
(k ? u|E)P
B
(F
k
k?u
|E)
P
b
(k
?
|E) =
U
?
u=1
P
b
(k
?
+ u|E)P
B
(F
k
?
+u
k
?
|E)
P (F
k
?
k
|F , E,B) = P
a
(k|E)P
B
(F
k
?
k
|E)P
b
(k
?
|E).
(8)
Eq. 7 can be rewritten (as in IBM model 2):
P
B
(f
j
|E) =
I
?
i=1
P
?
(i|j, I, J)P
B
(f
j
|e
i
) (9)
P
?
(i|j, I, J) =
?
a:a
j
=i
P (a
j
|, j, I, J)
In order to maintain both speed and accuracy, the
following window function is adopted
P
?
(i|j, I, J) ? P
?
(i|k, I,K) =
?
?
?
e
?|i?kI/K|
/? |i? kI/K| 6 ?
b
/2
?
?
e
i
is empty word
0 otherwise
(10)
where K is the number of characters in F , and
the k-th character is the start of the word f
j
, since
j and J are unknown during the computation of
dynamic programming. ?
b
is the window size, ?
?
is the prior probability of an empty English word,
and ? ensures all the items sum to 1.
2.2 Maximization
Inspired by (Teh, 2006; Mochihashi et al, 2009;
Neubig et al, 2010; Teh and Jordan, 2010), we
employ a Pitman-Yor process model to build the
segmentation model M or B. The monolingual
model M is
P
M
(f
j
) =
max
(
n(f
j
)? d, 0
)
+ (? + d ? n
M
)G
0
(f
j
)
?
f
?
j
n(f
?
j
) + ?
n
M
=
?
?
{f
j
|n(f
j
) > d}
?
?
, (11)
where f
j
is a foreign language word, and n(f
j
) is
the observed counts of f
j
, ? is named the strength
parameter, G
0
(f
j
) is named the base distribution
of f
j
, and d is the discount.
The bilingual model is
P
B
(f
j
|e
i
) =
max
(
n(f
j
, e
i
)? d, 0
)
+ (? + d ? n
e
i
)G
0
(f
j
|e
i
)
?
f
?
j
n(f
?
j
, e
i
) + ?
n
e
i
=
?
?
{x |n(x, e
i
) > d}
?
?
. (12)
In Eqs. 11 and 12,
n(f
j
) =
?
F?F
P (f
j
|F ,M) (13)
n(f
j
, e
i
) =
?
(F ,E)?B
P (f
j
|F , E,B)
P
?
(i|j, I, J)P
B
(f
j
|e
i
)
?
I
i
?
=1
P
?
(i
?
|j, I, J)P
B
(f
j
|e
i
?
)
.
(14)
754
3 Complexity Analysis
The computational complexity of our method is
linear in the number of iterations, the size of the
corpus, and the complexity of calculating the ex-
pectations on each sentence or sentence pair. In
practical applications, the size of the corpus is
fixed, and we found empirically that the number
of iterations required by the proposed method for
convergence is usually small (less than five itera-
tions). We now look in more detail at the complex-
ity of the expectation calculation in monolingual
and bilingual models.
The monolingual expectation is calculated ac-
cording to Eq. 5; the complexity is linear in the
length of sentences and the square of the prede-
fined maximum length of words. Thus its overall
complexity is
O
unigram
monoling = O(Ni|F|KU
2
), (15)
where Ni is the number of iterations, K is the av-
erage number of characters per sentence, and U is
the predefined maximum length of words.
For the monolingual bigram model, the number
of states in the HMM is U times more than that
of the monolingual unigram model, as the states at
specific position of F are not only related to the
length of the current word, but also related to the
length of the word before it. Thus its complexity
is U2 times the unigram model?s complexity:
O
bigram
monoling = O(Ni|F|KU
4
). (16)
The bilingual expectation is given by Eq. 8,
whose complexity is the same as the monolingual
case. However, the complexity of calculating the
transition probability, in Eqs. 9 and 10, is O(?
b
).
Thus its overall complexity is:
O
unigram
biling = O(Ni|F|KU
2
?
b
). (17)
4 Experiments
In this section, the proposed method is first val-
idated on monolingual segmentation tasks, and
then evaluated in the context of SMT to study
whether the translation quality, measured by
BLEU, can be improved.
4.1 Experimental Settings
4.1.1 Experimental Corpora
Two monolingual corpora and two bilingual cor-
pora are used (Table 2). CHILDES (MacWhin-
ney and Snow, 1985) is the most common test
Corpus Type # Sentences # Characters
CHILDES Mono. 9,790 95,809
SIGHAN-MSR Mono. 90,903 4,234,824
OpenMT06 Biling. 437,004 19,692,605
PatentMT9 Biling. 1,004,000 63,130,757
Table 2: Experimental Corpora
corpus for UWS methods. The SIGHAN-MSR
corpus (Emerson, 2005) consists of manually seg-
mented simplified Chinese news text, released in
the SIGHAN bakeoff 2005 shared tasks.
The first bilingual corpus: OpenMT06 was used
in the NIST open machine translation 2006 Eval-
uation 2. We removed the United Nations cor-
pus and the traditional Chinese data sets from the
constraint training resources. The data sets of
NIST Eval 2002 to 2005 were used as the develop-
ment for MERT tuning (Och, 2003). This data set
mainly consists of news text 3. PatentMT9 is from
the shared task of NTCIR-9 patent machine trans-
lation . The training set consists of 1 million par-
allel sentences extracted from patent documents,
and the development set and test set both consist
of 2000 sentences.
4.1.2 Performance Measurement and
Baseline Methods
For the monolingual tasks, the F
1
score against
the gold annotation is adopted to measure the ac-
curacy. The results reported in related papers are
listed for comparison.
For the bilingual tasks, the publicly available
system of Moses (Koehn et al, 2007) with default
settings is employed to perform machine transla-
tion, and BLEU (Papineni et al, 2002) was used
to evaluate the quality. Character-based segmen-
tation, LDC segmenter and Stanford Chinese seg-
menters were used as the baseline methods.
4.1.3 Parameter settings
The parameters are tuned on held-out data sets.
The maximum length of foreign language words
is set to 4. For the PYP model, the base distri-
bution adopts the formula in (Chung and Gildea,
2009), and the strength parameter is set to 1.0, and
the discount is set to 1.0? 10?6.
For bilingual segmentation,the size of the align-
ment window is set to 6; the probability ?
?
of for-
eign language words being generated by an empty
2http://www.itl.nist.gov/iad/mig/
/tests/mt/2006/
3It also contains a small number of web blogs
755
Method Accuracy Time
CHILD. MSR CHILD. MSR
NPY(bigram)a 0.750 0.802 17 m ?
NPY(trigram)a 0.757 0.807 ? ?
HDP(bigram)b 0.723 ? 10 h ?
Fitnessc ? 0.667 ? ?
Prop.(unigram) 0.729 0.804 3 s 50 s
Prop.(bigram) 0.774 0.806 15 s 2530 s
a by (Mochihashi et al,2009);
b by (Goldwater et al,2009);
c by (Zhao and Kit, 2008).
Table 3: Results on Monolingual Corpora.
English word, was set to 0.3.
The training was started from assuming that
there was no previous segmentations on each sen-
tence (pair), and the number of iterations was
fixed. It was set to 3 for the monolingual unigram
model, and 2 for the bilingual unigram model,
which provided slightly higher BLEU scores on
the development set than the other settings. The
monolingual bigram model, however, was slower
to converge, so we started it from the segmenta-
tions of the unigram model, and using 10 itera-
tions.
4.2 Monolingual Segmentation Results
In monolingual segmentation, the proposed meth-
ods with both unigram and bigram models were
tested. Experimental results show that they are
competitive to state-of-the-art baselines in both ac-
curacy and speed (Table 3). Note that the com-
parison of speed is only for reference because the
times are obtained from their respective papers.
4.3 Bilingual Segmentation Results
Table 4 presents the BLEU scores for Moses using
different segmentation methods. Each experiment
was performed three times. The proposed method
with monolingual bigram model performed poorly
on the Chinese monolingual segmentation task;
thus, it was not tested. We intended to test (Mochi-
hashi et al, 2009), but found it impracticable on
large-scale corpora.
The experimental results show that the proposed
UWS methods are comparable to the Stanford seg-
menters on the OpenMT06 corpus, while achieves
a 0.96 BLEU increase on the PatentMT9 corpus.
This is because this corpus is out-of-domain for
the supervised segmenters. The CTB and PKU
Stanford segmenter were both trained on anno-
tated news text, which was the major domain of
OpenMT06.
Method BLEU
OpenMT06 PatentMT9
Character 29.50 ? 0.03 28.36 ? 0.09
LDC 31.33 ? 0.10 30.22 ? 0.14
Stanford(CTB) 31.68 ? 0.25 30.77 ? 0.13
Stanford(PKU) 31.54 ? 0.13 30.86 ? 0.04
Prop.(mono.) 31.47 ? 0.18 31.62 ? 0.06
Prop.(biling.) 31.61 ? 0.14 31.73 ? 0.05
Table 4: Results on Bilingual Corpora.
Method Time
OpenMT06 PatentMT9
Prop.(mono.) 28 m 1 h 01 m
Prop.(biling.) 2 h 25 m 5 h 02 m
Table 5: Time Costs on Bilingual Corpora.
Table 5 presents the run times of the proposed
methods on the bilingual corpora. The program
is single threaded and implemented in C++. The
time cost of the bilingual models is about 5 times
that of the monolingual model, which is consistent
with the complexity analysis in Section 3.
5 Conclusion
This paper is devoted to large-scale Chinese UWS
for SMT. An efficient unified monolingual and
bilingual UWS method is proposed and applied to
large-scale bilingual corpora.
Complexity analysis shows that our method is
capable of scaling to large-scale corpora. This was
verified by experiments on a corpus of 1-million
sentence pairs on which traditional MCMC ap-
proaches would struggle (Xu et al, 2008).
The proposed method does not require any
annotated data, but the SMT system with it
can achieve comparable performance compared
to state-of-the-art supervised word segmenters
trained on precious annotated data. Moreover,
the proposed method yields 0.96 BLEU improve-
ment relative to supervised word segmenters on
an out-of-domain corpus. Thus, we believe that
the proposed method would benefit SMT related to
low-resource languages where annotated data are
scare, and would also find application in domains
that differ too greatly from the domains on which
supervised word segmenters were trained.
In future research, we plan to improve the bilin-
gual UWS through applying VB and integrating
more accurate alignment models such as HMM
models and IBM model 4.
756
References
Michael R Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34(1-3):71?105.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational linguistics, 19(2):263?311.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the 3rd Workshop on Statistical Machine
Translation, pages 224?232. Association for Com-
putational Linguistics.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 718?726. Association for Com-
putational Linguistics.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings
of the 4th SIGHAN Workshop on Chinese Language
Processing, volume 133.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsu-
pervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 673?
680. Association for Computational Linguistics.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A Bayesian framework for word seg-
mentation: exploring the effects of context. Cogni-
tion, 112(1):21?54.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR, volume 9, pages 559?578.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Brian MacWhinney and Catherine Snow. 1985. The
child language data exchange system. Journal of
child language, 12(2):271?296.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, pages 100?108.
Association for Computational Linguistics.
Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a language
model from continuous speech. In InterSpeech,
pages 1053?1056.
ThuyLinh Nguyen, Stephan Vogel, and Noah A Smith.
2010. Nonparametric word segmentation for ma-
chine translation. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 815?823. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318. Association
for Computational Linguistics.
Michael Paul. 2008. Overview of the IWSLT 2008
evaluation campaign. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 1?17.
Lawrence R Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
286.
Jenny R Saffran, Richard N Aslin, and Elissa L New-
port. 1996. Statistical learning by 8-month-old in-
fants. Science, 274(5294):1926?1928.
Yee Whye Teh and Michael I Jordan. 2010. Hierar-
chical Bayesian nonparametric models with appli-
cations. Bayesian Nonparametrics: Principles and
Practice, pages 158?207.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting on Association for Computational Linguis-
tics, pages 985?992. Association for Computational
Linguistics.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter for SIGHAN
Bakeoff 2005. In Proceedings of the 4th SIGHAN
Workshop on Chinese Language Processing, volume
171. Jeju Island, Korea.
757
Anand Venkataraman. 2001. A statistical model for
word discovery in transcribed speech. Computa-
tional Linguistics, 27(3):351?372.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised
Chinese word segmentation for statistical machine
translation. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics-
Volume 1, pages 1017?1024. Association for Com-
putational Linguistics.
Hai Zhao and Chunyu Kit. 2008. An empirical com-
parison of goodness measures for unsupervised chi-
nese word segmentation with a unified framework.
In Proceedings of the 3rd International Joint Con-
ference on Natural Language Processing, pages 9?
16.
758
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 63?66,
Beijing, August 2010
Helping Volunteer Translators, Fostering Language Resources
Masao Utiyama
MASTAR Project
NICT
mutiyama@nict.go.jp
Takeshi Abekawa
National Institute
of Informatics
abekawa@nii.ac.jp
Eiichiro Sumita
MASTAR Project
NICT
eiichiro.sumita@nict.go.jp
Kyo Kageura
Tokyo University
kyo@p.u-tokyo.ac.jp
Abstract
This paper introduces a website called
Minna no Hon?yaku (MNH, ?Translation
for All?), which hosts online volunteer
translators. Its core features are (1) a
set of translation aid tools, (2) high qual-
ity, comprehensive language resources,
and (3) the legal sharing of translations.
As of May 2010, there are about 1200
users and 4 groups registered to MNH.
The groups using it include such major
Figure 1: Screenshot of ?Minna no Hon?yaku?NGOs as Amnesty International Japan
site (http://trans- )and Democracy Now! Japan. aid.jp
1 Introduction
This paper introduces a website called Minna Second, MNH provides comprehensive lan-
no Hon?yaku (MNH, ?Translation for All?, Fig- guage resources, which are easily looked up in
ure 1), which hosts online volunteer translators QRedit. MNH, in cooperation with Sanseido,
(Utiyama et al, 2009).1 Its core features are (1) a provides ?Grand Concise English Japanese Dic-
set of translation aid tools, (2) high quality, com- tionary? (Sanseido, 2001) and plans to provide
prehensive language resources, and (3) the legal ?Grand Concise Japanese English Dictionary?
sharing of translations. (Sanseido, 2002) in fiscal year 2010. These dic-
First, the translation aid tools in MNH con- tionaries have about 360,000 and 320,000 en-
sist of the translation aid editor, QRedit, a bilin- tries, respectively, and are widely accepted as
gual concordancer, and a bilingual term extrac- standard and comprehensive dictionaries among
tion tool. These tools help volunteer translators translators. MNH also provides seamless access
to translate their documents easily as described to the web. For example, MNH provides a dictio-
in Section 3. These tools also produce language nary that was made from the English Wikipedia.
resources that are useful for natural language This enable translators to reference Wikipedia
processing as the byproduct of their use as de- articles during the translation process as if they
scribed in Section 4. are looking up dictionaries.
1Currently, MNH hosts volunteer translators who trans- Third, MNH uses Creative Commons Li-
late Japanese (English) documents into English (Japanese). censes (CCLs) to help translators share their
The English and Japanese interfaces are available at http: translations. CCLs are essential for sharing and//trans-aid.jp/en and http://trans-aid.
jp/ja, respectively. opening translations.
63
Figure 2: Screenshot of QRedit
2 Related work
There are many translation support tools, such
as Google Translator Toolkit, WikiBABEL (Ku-
maran et al, 2009), BEYtrans (Bey et al, 2008),
Caitra (Koehn, 2009) and Idiom WorldServer
system,2 an online multilingual document man-
agement system with translation memory func-
tions.
The functions that MNH provides are closer
to those provided by Idiom WorldServer, but
MNH provides a high-quality bilingual dictio-
naries and functions for seamless Wikipedia and
web searches within the integrated translation
aid editor QRedit. It also enables translators to
share their translations, which are also used as
language resources.
3 Helping Volunteer translators
This section describes a set of translation aid
tools installed in MNH.
3.1 QRedit
QRedit is a translation aid system which is de-
signed for volunteer translators working mainly
online (Abekawa and Kageura, 2007). When a
URL of a source language (SL) text is given to
QRedit, it loads the corresponding text into the
left panel, as shown in Figure 2. Then, QRedit
automatically looks up all words in the SL text.
When a user clicks an SL word, its translation
candidates are displayed in a pop-up window.
2http://www.idiominc.com/en/
Figure 3: Screenshot of bilingual concordancer
3.2 Bilingual concordancer
The translations published on MNH are used
to make a parallel corpus by using a sentence
alignment method (Utiyama and Isahara, 2003).
MNH also has parallel texts from the Amnesty
International Japan, Democracy Now! Japan,
and open source software manuals (Ishisaka et
al., 2009). These parallel texts are searched by
using a simple bilingual concordancer as shown
in Figure 3.
3.3 Bilingual term extraction tool
MNH has a bilingual term extraction tool that
is composed of a translation estimation tool
(Tonoike et al, 2006) and a term extraction tool
(Nakagawa and Mori, 2003).
First, we apply the translation estimation tool
to extract Japanese term candidates and their En-
glish translation candidates. Next, we apply the
term extraction tool to extract English term can-
didates. If these English term candidates are
found in the English translation candidates, then,
we accept these term candidates as the transla-
tions of those Japanese term candidates.
4 Fostering language resources
Being a ?one stop? translation aid tool for on-
line translators, MNH incorporates mechanisms
which enable users to naturally foster impor-
tant translation resources, i.e. terminological re-
sources and translation logs.
64
4.1 Terminological resources
As with most translation-aid systems, MNH pro-
vides functions that enable users to register their
own terminologies. Users can assign the status
of availability to the registered terms. They can
keep the registered terms for private use, make
them available for a specified group of people,
or make them publicly available. Several NGO
groups are using MNH for their translation activ-
ities. For instance, Amnesty International, which
uses MNH, maintains a list of term translations
in the field of human rights by which translators
should abide. Thus groups such as Amnesty up-
load a pre-compiled list of terms and make them
available among volunteers. It is our assumption
and aim that these groups make their termino-
logical resources not only available among the
group but also publicly available, which will cre-
ate win-win situation: NGOs and other groups
which make their lists of terms available will
have more chance of recruiting volunteer trans-
lators, while MNH has more chance of attracting
further users.
At the time of writing this paper (May 2010),
56,319 terms are registered, of which 45,843 are
made publicly available. More than 80 per cent
of the registered terms are made public. Cur-
rently, MNH does not identify duplicated terms
registered by different users, but when the num-
ber of registered terms become larger, this and
other aspects of quality control of registered
terms will become an important issue.
4.2 Translation corpus
Another important language resources accumu-
lated on MNH is the translation corpus. As
mentioned in the introduction, being a hosting
site, MNH naturally accumulates source and tar-
get documents with a clear copyright status. Of
particular importance in MNH, however, is that
it can accumulate a corpus that contains draft
and final translations made by human together
with their source texts (henceforth SDF corpus
for succinctness). This type of corpus is im-
portant and useful, because it can be used for
the training of inexperienced translators (for in-
stance, the MeLLANGE corpus, which contains
different versions of translation, is well known
for its usefulness in translator training (MeL-
LANGE, 2009)) and also because it provides
a useful information for improving the perfor-
mance of machine translation and translation-aid
systems. While the importance of such corpora
has been widely recognized, the construction of
such a corpus is not easy because the data are
not readily available due to the reluctance on the
side of translators of releasing the draft transla-
tion data.
The basic mechanisms of accumulating SDF
corpus is simple. Translators using MNH save
their translations to keep the data when they fin-
ish the translation. MNH keeps the log of up
to 10 versions of translation for each document.
MNH introduced two saving modes, i.e. snap-
shot mode and normal mode. The translation
version saved in the normal mode is overwrit-
ten when the next version is saved. Translation
versions saved in snapshot mode are retained, up
to 10 versions. Translators can thus consciously
keep the versions of their translations.
MNH can collect not only draft and final trans-
lations made by a single translator, but also those
made by different translators. MNH has a func-
tion that enables users to give permission for
other translators registered with MNH to edit
their original translations, thus facilitating the
collaborative translations. Such permission can
be open-ended, or restricted to a particular group
of users.
This function is of particular importance
for NGOs, NPOs, university classes and other
groups involved in group-based translation. In
these groups, it is a common process in transla-
tion that a draft translation is first made by inex-
perienced translators, which is then revised and
finalized by experienced translators. If an inex-
perienced translator gives permission of editing
his/her draft translations to experienced transla-
tors, the logs of revisions, including the draft and
final versions, will be kept on MNH database.
This is particularly important and useful for
the self-training of inexperienced translators and
thus potentially extremely effective for NGOs
and other groups that rely heavily on volunteer
65
Figure 4: Comparative view of different transla-
tion versions
translators. Many NGOs face chronically the
problem of a paucity of good volunteer transla-
tors. The retention rate of volunteer translators is
low, which increase the burden of a small num-
ber of experienced translators, leaving them no
time to give advice to inexperienced translators,
which further reduce the retention rate of volun-
teers. To overcome this vicious cycle, mecha-
nisms to enable inexperienced volunteer trans-
lators to train themselves in the cycle of actual
translation activities is urgently needed and ex-
pected to be highly effective. MNH provides a
comparative view function of any pairwise trans-
lation versions of the same document, as shown
in Figure 4. Translators can check which parts
are modified very easily through the compara-
tive view screen, which can effectively works as
a transfer of translation knowledge from experi-
enced translators to inexperienced translators.
At the time of writing this paper, MNH con-
tains 1850 documents that have more than one
translation versions, of which 764 are published.
The number of documents translated by a group
(more than one translator) is 110, of which 48 are
published. Although the number of translations
made by more than one translators is relatively
small, they are steadily increasing both in num-
ber and in ratio.
5 Conclusion
We have developed a website called Minna no
Hon?yaku (MNH, ?Translation for All?), which
hosts online volunteer translators. We plan to ex-
tend MNH to other language pairs in our future
work.
References
Abekawa, Takeshi and Kyo Kageura. 2007. QRedit:
An integrated editor system to support online vol-
unteer translators. In Digital humanities, pages 3?
5.
Bey, Y., K. Kageura, and C. Boitet. 2008. BEY-
Trans: A Wiki-based environment for helping on-
line volunteer translators. Yuste, E. ed. Topics in
Language Resources for Translation and Localisa-
tion. Amsterdam: John Benjamins. p. 139?154.
Ishisaka, Tatsuya, Masao Utiyama, Eiichiro Sumita,
and Kazuhide Yamamoto. 2009. Development of
a Japanese-English software manual parallel cor-
pus. In MT summit.
Koehn, Philipp. 2009. A web-based interactive com-
puter aided translation tool. In ACL-IJCNLP Soft-
ware Demonstrations.
Kumaran, A, K Saravanan, Naren Datha, B Ashok,
and Vikram Dendi. 2009. Wikibabel: A wiki-style
platform for creation of parallel data. In ACL-
IJCNLP Software Demonstrations.
MeLLANGE. 2009. Mellange. ttp://corpus.
leeds.ac.uk/mellange/ltc.tml.
Nakagawa, Hiroshi and Tatsunori Mori. 2003. Au-
tomaic term recognition based on statistics of com-
pound nouns and their components. Terminology,
9(2):201?209.
Sanseido. 2001. Grand Concise English Japanese
Dictionary. Tokyo, Sanseido.
Sanseido. 2002. Grand Concise Japanese English
Dictionary. Tokyo, Sanseido.
Tonoike, Masatsugu, Mitsuhiro Kida, Toshihiro Tak-
agi, Yasuhiro Sasaki, Takehito Utsuro, and Satoshi
Sato. 2006. A comparative study on composi-
tional translation estimation usign a domain/topic-
specific corpus collected from the web. In Proc. of
the 2nd International Workshop on Web as Corpus,
pages 11?18.
Utiyama, Masao and Hitoshi Isahara. 2003. Reli-
able measures for aligning Japanese-English news
articles and sentences. In ACL, pages 72?79.
Utiyama, Masao, Takeshi Abekawa, Eiichiro Sumita,
and Kyo Kageura. 2009. Hosting volunteer trans-
lators. In MT summit.
66
