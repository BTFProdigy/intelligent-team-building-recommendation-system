Robust Knowledge Discovery from Parallel Speech and
Text Sources
F. Jelinek, W. Byrne, S. Khudanpur, B. Hladka?. CLSP, Johns Hopkins University, Baltimore, MD.
H. Ney, F. J. Och. RWTH Aachen University, Aachen, Germany
J. Cur???n. Charles University, Prague, Czech Rep.
J. Psutka. University of West Bohemia, Pilsen, Czech Rep.
1. INTRODUCTION
As a by-product of the recent information explosion, the same
basic facts are often available from multiple sources such as the In-
ternet, television, radio and newspapers. We present here a project
currently in its early stages that aims to take advantage of the re-
dundancies in parallel sources to achieve robustness in automatic
knowledge extraction.
Consider, for instance, the following sampling of actual news
from various sources on a particular day:
CNN: James McDougal, President Bill Clinton?s former business
partner in Arkansas and a cooperating witness in the White-
water investigation, died Sunday while serving a federal prison
term. He was 57.
MSNBC: Fort Worth, Texas, March 8. Whitewater figure James
McDougal died of an apparent heart attack in a private com-
munity hospital in Fort Worth, Texas, Sunday. He was 57.
ABC News: Washington, March 8. James McDougal, a key figure
in Independent Counsel Kenneth Starr?s Whitewater investi-
gation, is dead.
The Detroit News: Fort Worth. James McDougal, a key witness
in Kenneth Starr?s Whitewater investigation of President Clin-
ton and First Lady Hillary Rodham Clinton, died of a heart
attack in a prison hospital Sunday. He was 57.
San Jose Mercury News: James McDougal, the wily Arkansas
banking rogue who drew Bill Clinton and Hillary Rodham
Clinton into real estate deals that have come to haunt them,
died Sunday of cardiac arrest just months before he hoped to
be released from prison. He was 57.
The Miami Herald: Washington. James McDougal, the wily
Arkansas financier and land speculator at the center of the
original Whitewater probe against President Clinton, died
Sunday.
.
Story
Alignment
Speech
RecognitionSpeech Sources
Basic Models:
acoustic
lexical
language
Topic specific
acoustic and language
models
stories
Aligned Sentence
retrieval
Ranked
Answers
Query
Text sources
Figure 1: Information Flow in Alignment and Extraction
We propose to align collections of stories, much like the exam-
ple above, from multiple text and speech sources and then develop
methods that exploit the resulting parallelism both as a tool to im-
prove recognition accuracy and to enable the development of sys-
tems that can reliably extract information from parallel sources.
Our goal is to develop systems that align text sources and rec-
ognize parallel speech streams simultaneously in several languages
by making use of all related text and speech. The initial systems
we intend to develop will process each language independently.
However, our ultimate and most ambitious objective is to align text
sources and recognize speech using a single, integrated multilin-
gual ASR system. Of course, if sufficiently accurate automatic ma-
chine translation (MT) techniques ([1]) were available, we could
address multilingual processing and single language systems in the
same way. However MT techniques are not yet reliable enough
that we expect all words and phrases recognized within languages
to contribute to recognition across languages. We intend to develop
methods that identify the particular words and phrases that both can
be translated reliably and also used to improve story recognition.
As MT technology improves it can be incorporated more exten-
sively within the processing paradigm we propose. We consider
this proposal a framework within which successful MT techniques
can eventually be used for multilingual acoustic processing.
2. PROJECT OBJECTIVES
The first objective is to enhance multi-lingual information sys-
tems by exploiting the processing capabilities for resource-rich lan-
guages to enhance the capabilities for resource-impoverished lan-
guage. The second objective is to advance information retrieval and
knowledge information systems by providing them with consider-
ably improved multi-lingual speech recognition capabilities. Our
research plan proceeds in several steps to (i) collect and (ii) align
multi-lingual parallel speech and text sources, (iii) exploit paral-
lelism for improving ASR within a language, and to (iv) exploit
parallelism for improving ASR across languages. The main infor-
mation flows involved in aligning and exploiting parallel sources
are illustrated in Figure 1. We will initially focus on German, En-
glish and Czech language sources. This section summarizes the
major components of our project.
2.1 Parallel Speech and Text Sources
The monolingual speech and text collections that we will use
to develop techniques to exploit parallelism for improving ASR
within a language are readily available. For instance, the North
American News Text corpus of parallel news streams from 16 US
newspapers and newswire is available from LDC. A 3-year period
yields over 350 million words of multi-source news text.
In addition to data developed within the TIDES and other HLT
programs, we are in the process of identifying and creating our own
multilingual parallel speech and text sources.
FBIS TIDES Multilingual Newstext Collection
For the purposes of developing multilingual alignment techniques,
we intend to use the 240 day, contemporaneous, multilingual news
text collection made available for use to TIDES projects by FBIS.
This corpus contains news in our initial target languages of English,
German, and Czech. The collections are highly parallel, in that
much of the stories are direct translations.
Radio Prague Multilingual Speech and Text Corpus
Speech and news text from Radio Prague was collected under the
direction of J. Psutka with the consent of Radio Prague. The col-
lection contains speech and text in 5 languages: Czech, English,
German, French, and Spanish. The collection began June 1, 2000
and continued for approximately 3 months. The text collection con-
tains the news scripts used for the broadcast; the broadcasts more
or less follow the scripts. The speech is about 3 minutes per day
in each language, which should yield a total of about 5 hours of
speech per language.
Our initial analysis of the Radio Prague corpus suggest that only
approximately 5% of the stories coincide in topic, and that there
is little, if any, direct translation of stories. We anticipate that this
sparseness will make this corpus significantly hard to analyze than
another, highly-parallel corpus. However, we expect this is the
sort of difficulty that will likely be encountered in processing ?real-
world? multilingual news sources.
2.2 Story-level Alignment
Once we have the multiple streams of information we must be
able to align them according to story. A story is the description of
one or more events that happened in a single day and that are re-
ported in a single article by a daily news source the next day. We
expect that we will use the same techniques used in the Topic De-
tection (TDT) field ([5]). Independently of the specific details of
the alignment procedure, there is now substantial evidence that re-
lated stories from parallel streams can be identified using standard
statistical Information Retrieval (IR) techniques.
Sentence Alignment As part of the infrastructure needed to in-
corporate cross-lingual information into language models, we are
employing statistical MT systems to generate English/German and
English/Czech alignments of sentences in the FBIS Newstext Col-
lection. For the English/German sentence and single-word based
alignments, we plan to use statistical models ([4]) [3] which gen-
erate both sentence and word alignments. For English/Czech sen-
tence alignment, we will employ the statistical models trained as
part of the Czech-English MT system developed during the 1999
Johns Hopkins Summer Workshop ([2]).
2.3 Multi-Source Automatic Speech
Recognition
The scenario we propose is extraction of information from paral-
lel text followed by repeated recognition of parallel broadcasts, re-
sulting in a gradual lowering the WER. The first pass is performed
in order to find the likely topics discussed in the story and to iden-
tify the topics relevant to the query. In this process, the acoustic
model will be improved by deriving pronunciation specifications
for out-of-vocabulary words and fixed phrases extracted from the
parallel stories. The language model will be improved by extending
the coverage of the underlying word and phrase vocabulary, and by
specializing the model?s statistics to the narrow topic at hand. As
long as a round of recognition yields new information, the corre-
sponding improvement is incorporated into the recognizer modules
and bootstrapping of the system continues.
Story-specific Language Models from Parallel Speech and Text
Our goal is to create language models combining specific but sparse
statistics, derived from relevant parallel material, with reliable but
unspecific statistics obtainable from large general corpora. We will
create special n-gram language models from the available text, re-
lated or parallel to the spoken stories. We can then interpolate
this special model with a larger pre-existing model, possibly de-
rived from training text associated to the topic of the story. Our
recent STIMULATE work demonstrated success in construction of
topic-specific language models on the basis of hierarchically topic-
organized corpora [8].
Unlike building models from parallel texts, the training of story
specific language models from recognized speech is also affected
by recognition errors in the data which will be used for language
modeling. Confidence measures can be used to estimate the cor-
rectness of individual words or phrases on the recognizer output.
Using this information, n-gram statistics can be extracted from the
recognizer output by selecting those events which are likely to be
correct and which can therefore be used to adjust the original lan-
guage model without introducing new errors to the recognition sys-
tem.
Language Models with Cross-Lingual Lexical Triggers
A trigger language model ([6], [7]) will be constructed for the tar-
get language from the text corpus, where the lexical triggers are not
from the word-history in the target language, but from the aligned
recognized stories in the source language. The trigger informa-
tion becomes most important in those cases in which the baseline
n-gram model in the target language does not supply sufficient in-
formation to predict a word. We expect that content words in the
source language are good predictors for content words in the target
language and that these words are difficult to predict using the tar-
get language alone, and the mutual information techniques used to
identify trigger pairs will be useful here.
Once a spoken source-language story has been recognized, the
words found here there will be used as triggers in the language
model for the recognition of the target-language news broadcasts.
3. SUMMARY
Our goal is to align collections of stories from multiple text and
speech sources in more than one language and then develop meth-
ods that exploit the resulting parallelism both as a tool to improve
recognition accuracy and to enable the development of systems that
can reliably extract information from parallel sources. Much like
a teacher rephrases a concept in a variety of ways to help a class
understand it, the multiple sources, we expect, will increase the po-
tential of success in knowledge extraction. We envision techniques
that will operate repeatedly on multilingual sources by incorporat-
ing newly discovered information in one language into the models
used for all the other languages. Applications of these methods ex-
tend beyond news sources to other multiple-source domains such
as office email and voice-mail, or classroom materials such as lec-
tures, notes and texts.
4. REFERENCES
[1] P. F. Brown, S. A. DellaPietra, V. J. D. Pietra, and R. L.
Mercer. The mathematics of statistical translation.
Computational Linguistics, 19(2), 1993.
[2] K. K. et al Statistical machine translation, WS?99 Final
Report, Johns Hopkins University, 1999.
http://www.clsp.jhu.edu/ws99/projects/mt.
[3] F. J. Och and H. Ney. Improved statistical alignment models.
In ACL?00, pages 440?447, 2000.
[4] F. J. Och, C. Tillmann, and H. Ney. Improved alignment
models for statistical machine translation. In EMNLP/VLC?99,
pages 20?28, 1999.
[5] Proceedings of the Topic Detection and Tracking workshop.
University of Maryland, College Park, MD, October 1997.
[6] C. Tillmann and H. Ney. Selection criteria for word trigger
pairs in language modelling. In ICGI?96, pages 95?106, 1996.
[7] C. Tillmann and H. Ney. Statistical language modeling and
word triggers. In SPECOM?96, pages 22?27, 1996.
[8] D. Yarowsky. Exploiting nonlocal and syntactic word
relationships in language models for conversational speech
recognition, a NSF STIMULATE Project IRI9618874, 1997.
Johns Hopkins University.
A Generative Probabilistic OCR Model for NLP Applications
Okan Kolak
Computer Science and UMIACS
University of Maryland
College Park, MD 20742, USA
okan@umiacs.umd.edu
William Byrne
CLSP
The Johns Hopkins University
Baltimore, MD 21218, USA
byrne@jhu.edu
Philip Resnik
Linguistics and UMIACS
University of Maryland
College Park, MD 20742, USA
resnik@umiacs.umd.edu
Abstract
In this paper, we introduce a generative prob-
abilistic optical character recognition (OCR)
model that describes an end-to-end process in
the noisy channel framework, progressing from
generation of true text through its transforma-
tion into the noisy output of an OCR system.
The model is designed for use in error correc-
tion, with a focus on post-processing the output
of black-box OCR systems in order to make
it more useful for NLP tasks. We present an
implementation of the model based on finite-
state models, demonstrate the model?s ability
to significantly reduce character and word er-
ror rate, and provide evaluation results involv-
ing automatic extraction of translation lexicons
from printed text.
1 Introduction
Although a great deal of text is now available in elec-
tronic form, vast quantities of information still exist pri-
marily (or only) in print. Critical applications of NLP
technology, such as rapid, rough document translation in
the field (Holland and Schlesiger, 1998) or information
retrieval from scanned documents (Croft et al, 1994), can
depend heavily on the quality of optical character recog-
nition (OCR) output. Doermann (1998) comments, ?Al-
though the concept of a raw document image database is
attractive, comprehensive solutions which do not require
complete and accurate conversion to a machine-readable
form continue to be elusive for practical systems.?
Unfortunately, the output of commercial OCR systems
is far from perfect, especially when the language in ques-
tion is resource-poor (Kanungo et al, in revision). And
efforts to acquire new language resources from hardcopy
using OCR (Doermann et al, 2002) face something of a
chicken-and-egg problem. The problem is compounded
by the fact that most OCR system are black boxes that do
not allow user tuning or re-training ? Baird (1999, re-
ported in (Frederking, 1999)) comments that the lack of
ability to rapidly retarget OCR/NLP applications to new
languages is ?largely due to the monolithic structure of
current OCR technology, where language-specific con-
straints are deeply enmeshed with all the other code.?
In this paper, we describe a complete probabilistic,
generative model for OCR, motivated specifically by (a)
the need to deal with monolithic OCR systems, (b) the fo-
cus on OCR as a component in NLP applications, and (c)
the ultimate goal of using OCR to help acquire resources
for new languages from printed text. After presenting
the model itself, we discuss the model?s implementation,
training, and its use for post-OCR error correction. We
then present two evaluations: one for standalone OCR
correction, and one in which OCR is used to acquire a
translation lexicon from printed text. We conclude with
a discussion of related research and directions for future
work.
2 The Model
Generative ?noisy channel? models relate an observable
string
 
to an underlying sequence, in this case recog-
nized character strings and underlying word sequences

. This relationship is modeled by  
 
, decom-
posed by Bayes?s Rule into steps modeled by  
 (the
source model) and   	   (comprising sub-steps gen-
erating
 
from
 ). Each step and sub-step is completely
modular, so one can flexibly make use of existing sub-
models or devise new ones as necessary.1
We begin with preliminary definitions and notation,
illustrated in Figure 1. A true word sequence
 



corresponds to a true character sequence
1Note that the process of ?generating?  from  is a math-
ematical abstraction, not necessarily related to the operation of
any particular OCR system.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 55-62
                                                         Proceedings of HLT-NAACL 2003
Figure 1: Word and character segmentation
 



 
 
 

, and the OCR system?s output char-
acter sequence is given by
 



 

 

.
A segmentation of the true character sequence into
 subsequences is represented as

 


 

. Seg-
ment boundaries are only allowed between characters.
Subsequences are denoted using segmentation positions





 

	
 
, where 



,



, and




. The 
 define character subsequences
 





 A Weighted Finite State Transducer Implementation of the Alignment
Template Model for Statistical Machine Translation
Shankar Kumar and William Byrne
Center for Language and Speech Processing, Johns Hopkins University,
3400 North Charles Street, Baltimore, MD, 21218, USA
 
skumar,byrne  @jhu.edu
Abstract
We present a derivation of the alignment tem-
plate model for statistical machine translation
and an implementation of the model using
weighted finite state transducers. The approach
we describe allows us to implement each con-
stituent distribution of the model as a weighted
finite state transducer or acceptor. We show
that bitext word alignment and translation un-
der the model can be performed with standard
FSM operations involving these transducers.
One of the benefits of using this framework
is that it obviates the need to develop special-
ized search procedures, even for the generation
of lattices or N-Best lists of bitext word align-
ments and translation hypotheses. We evaluate
the implementation of the model on the French-
to-English Hansards task and report alignment
and translation performance.
1 Introduction
The Alignment Template Translation Model
(ATTM) (Och et al, 1999) has emerged as a promising
modeling framework for statistical machine translation.
The ATTM attempts to overcome the deficiencies of
word-to-word translation models (Brown et al, 1993)
through the use of phrasal translations. The overall
model is based on a two-level alignment between the
source and the target sentence: a phrase-level alignment
between source and target phrases and a word-level
alignment between words in these phrase pairs.
The goal of this paper is to reformulate the ATTM
so that the operations we intend to perform under a sta-
tistical translation model, namely bitext word alignment
and translation, can be implementation using standard
weighted finite state transducer (WFST) operations. Our
main motivation for a WFST modeling framework lies
in the resulting simplicity of alignment and translation
processes compared to dynamic programming or  de-
coders. The WFST implementation allows us to use stan-
dard optimized algorithms available from an off-the-shelf
FSM toolkit (Mohri et al, 1997). This avoids the need to
develop specialized search procedures, even for the gen-
TEMPLATE
SEQUENCE
MODEL
PERMUTATION
MODEL
PHRASE
PHRASAL
TRANSLATION
MODEL
TARGET
LANGUAGE MODEL
v 2 v 31v
SOURCE
SEGMENTATION
MODEL
u
z
 
TARGET LANGUAGE SENTENCE 
SENTENCESOURCE LANGUAGE
source language phrases
alignment templates
target language phrases
f f f ff f2 3 4 5 6 f7
v 2 1v v 3
z z1 2 3
u u1 2
e1 e2
1
e4 e5 e6 eee3e
3
7 8 9
a aa 2 31
Figure 1: ATTM Architecture.
eration of lattices or N-best lists of bitext word alignment
or translation hypotheses.
Weighted Finite State Transducers for Statistical Ma-
chine Translation (SMT) have been proposed in the
literature to implement word-to-word translation mod-
els (Knight and Al-Onaizan, 1998) or to perform trans-
lation in an application domain such as the call routing
task (Bangalore and Ricardi, 2001). One of the objec-
tives of these approaches has been to provide an imple-
mentation for SMT that uses standard FSM algorithms
to perform model computations and therefore make SMT
techniques accessible to a wider community. Our WFST
implementation of the ATTM has been developed with
similar objectives.
We start off by presenting a derivation of the ATTM
that identifies the conditional independence assumptions
that underly the model. The derivation allows us to spec-
ify each component distribution of the model and imple-
ment it as a weighted finite state transducer. We then
show that bitext word alignment and translation can be
performed with standard FSM operations involving these
transducers. Finally we report bitext word alignment
and translation performance of the implementation on the
Canadian French-to-English Hansards task.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 63-70
                                                         Proceedings of HLT-NAACL 2003
2 Alignment Template Translation Models
We present here a derivation of the alignment template
translation model (ATTM) (Och et al, 1999; Och, 2002)
and give an implementation of the model using weighted
finite state transducers (WFSTs). The finite state model-
ing is performed using the AT&T FSM Toolkit (Mohri et
al., 1997).
In this model, the translation of a source language sen-
tence to a target language sentence is described by a joint
probability distribution over all possible segmentations
and alignments. This distribution is presented in Figure 1
and Equations 1-7. The components of the overall trans-
lation model are the source language model (Term 2),
the source segmentation model (Term 3), the phrase per-
mutation model (Term 4), the template sequence model
(Term 5), the phrasal translation model (Term 6) and the
target language model (Term 7). Each of these condi-
tional distributions is modeled independently and we now
define each in turn and present its implementation as a
weighted finite state acceptor or transducer.
 
	
Desparately Seeking Cebuano
Douglas W. Oard, David Doermann, Bonnie Dorr, Daqing He, Philip Resnik, and Amy Weinberg
UMIACS, University of Maryland, College Park, MD, 20742
(oard,doermann,bonnie,resnik,weinberg)@umiacs.umd.edu
William Byrne, Sanjeev Khudanpur and David Yarowsky
CLSP, Johns Hopkins University, 3400 North Charles Street, Barton Hall, Baltimore, MD 21218
(byrne,khudanpur,yarowsky)@jhu.edu
Anton Leuski, Philipp Koehn and Kevin Knight
USC Information Sciences Institute, 4676 Admiralty Way, Marina Del Rey, CA 90292
(leuski,koehn,knight)@isi.edu
Abstract
This paper describes an effort to rapidly de-
velop language resources and component tech-
nology to support searching Cebuano news sto-
ries using English queries. Results from the
first 60 hours of the exercise are presented.
1 Introduction
The Los Angeles Times reported that at about 5:20 P.M.
on Tuesday March 4, 2003, a bomb concealed in a back-
pack exploded at the airport in Davao City, the second
largest city in the Philippines. At least 23 people were
reported dead, with more than 140 injured, and Pres-
ident Arroyo of the Philippines characterized the blast
as a terrorist act. With the 13 hour time difference, it
was then 4:20 A.M on the same date in Washington, DC.
Twenty-four hours later, at 4:13 A.M. on March 5, partic-
ipants in the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program were notified
that Cebuano had been chosen as the language of interest
for a ?surprise language? practice exercise that had been
planned quite independently to begin on that date. The
notification observed that Cebuano is spoken by 24% of
the population of the Philippines, and that it is the lingua
franca in the south Philippines, where the event occurred.
One goal of the TIDES program is to develop the abil-
ity to rapidly deploy a broad array of language technolo-
gies for previously unforeseen languages in response to
unexpected events. That capability will be formally ex-
ercised for the first time during June 2003, in a month-
long ?Surprise Language Experiment.? To prepare for
that event, the Linguistic Data Consortium (LDC) orga-
nized a ?dry run? for March 5-14 in order to refine pro-
cedures for rapidly developing language resources of the
type that the TIDES community will need during the July
evaluation.
Development of interactive Cross-Language Informa-
tion Retrieval (CLIR) systems that can be rapidly adapted
to accommodate new languages has been the focus of
extensive collaboration between the University of Mary-
land and The Johns Hopkins University, and more re-
cently with the University of Southern California. The
capability for rapid development of necessary language
resources is an essential part of that process, so we had
been planning to participate in the surprise language dry
run to refine our procedures for sharing those resources
with other members of the TIDES community. Naturally,
we chose CLIR as a driving application to focus our ef-
fort. Our goal, therefore, was to build an interactive sys-
tem that would allow a searcher posing English queries
to find relevant Cebuano news articles from the period
immediately following the bombing.
2 Obtaining Language Resources
Our basic approach to development of an agile system for
interactive CLIR relies on three strategies: (1) create an
infrastructure in advance for English as a query language
that makes only minimal assumptions about the docu-
ment language; (2) leverage the asymmetry inherent in
the problem by assembling strong resources for English
in advance; and (3) develop a robust suite of capabilities
to exploit any language resources that can be found for
the ?surprise language.? We defer the first two topics to
the next section, and focus here on the third. We know of
five possible sources of translation expertise:
People. People who know the language are an excellent
source of insight, and universities are an excellent
place to find such people. We were able to locate
a speaker of Cebuano within 50 feet of one of our
offices, and to schedule an interview with a second
Cebuano speaker within 36 hours of the announce-
ment of the language.
Scholarly literature. Major research universities are
also an excellent place to find written materials de-
scribing a broad array of languages. Within 12 hours
of the announcement, reference librarians at the Uni-
versity of Maryland had identified a textbook on
?Beginning Cebuano,? and we had located a copy
at the University of Southern California. Together
with the excellent electronic resources located by the
LDC, this allowed us to develop a rudimentary stem-
mer within 36 hours.
Translation lexicons. Simple bilingual term lists are
available for many language pairs. Using links pro-
vided by the LDC and our own Web searches, we
were able to construct an English-Cebuano term list
with over 14,000 translation pairs within 12 hours of
the announcement. This largely duplicated a simul-
taneous effort at the LDC, and we later merged our
term list with theirs.
Parallel text. Translation-equivalent documents, when
aligned at the word level, provide an excellent
source of information about not just possible trans-
lations, but their relative predominance. Within 24
hours of the announcement, we had aligned Ce-
buano and English versions of the Holy Bible at
the word level using Giza++. An evaluation by a
native Cebuano speaker of a stratified random sam-
ple of 88 translation pairs showed remarkably high
precision. On a 4-point scale with 1=correct and
4=incorrect the most frequent 100 words averaged
1.3, the next 400 most frequent terms averaged 1.6,
and the 500 next most frequent terms after that aver-
aged 1.7. The Bible?s vocabulary covers only about
half of the words found in typical English news text
(counted by-token), so it is useful to have additional
sources of parallel text. For this reason, we have ex-
tended our previously developed STRAND system
to locate likely translations in the Internet Archive.
Those runs were not yet complete when this paper
was submitted.
Printed Dictionaries. People learning a new language
make extensive use of bilingual dictionaries, so we
have developed a system that mimics that process
to some extent. Within 12 hours of the announce-
ment we had zoned page images from a Cebuano-
English dictionary that was available commercially
in Adobe Page Description Format (PDF) to iden-
tify each dictionary entry, performed optical charac-
ter recognition, and parsed the entries to construct a
bilingual term list. We were aided in this process by
the fact that Cebuano is written in a Roman script.
Again, we achieved good precision, with a sampled
word error rate for OCR of 6.9% and a precision for
a random sample of translation pairs of 87%. Part of
speech tags were also extracted, although they are
not used in our process.
As this description illustrates, these five sources pro-
vide complementary information. Since there is some
uncertainty at the outset about how long it will be before
each delivers useful results, we chose a strategy based
on concurrency, balancing our investment over each the
five sources. This allowed us to use whatever resources
became available first to get an initial system running,
with refinements subsequently being made as additional
resources became available. Because Cebuano and En-
glish are written in the same script, we did not need char-
acter set conversion or phonetic cognate matching in this
case. The CLIR system described in the next section
was therefore constructed using only English resources
that were (or could have been) pre-assembled, plus a
Cebuano-English bilingual term list, a rule-based stem-
mer, and the Cebuano Bible.
3 Building a Cross-Language Retrieval
System
Ideally, we would like to build a system that would find
whatever documents the searcher would wish to read in a
fully automatic mode. In practice, fully automatic search
systems are imperfect even in monolingual applications.
We therefore have developed an interactive approach that
functions something like a typical Web search engine: (1)
the searcher poses their query in English, (2) the sys-
tem ranks the Cebuano documents in decreasing order
of likely relevance to the query, (3) the searcher exam-
ines a list of document titles in something approximat-
ing English, and (4) the searcher may optionally exam-
ine the full text of any document in something approx-
imating English. The intent is to support an iterative
process in which searchers learn to better express their
query through experience. We are only able to provide
very rough translations, so we expect that such a sys-
tem would be used in an environment where searchers
could send documents that appear promising off for pro-
fessional translation when necessary.
At the core of our system is the capability to au-
tomatically rank Cebuano documents based on an En-
glish query. We chose a query translation architecture
using backoff translation and Pirkola?s structured query
method, implemented using Inquery version 3.1p1. The
key idea in backoff translation is to first try to find con-
secutive sequences of query words on the English side
of the bilingual term list, where that fails to try to find
the surface form of each remaining English term, to fall
back to stem matching when necessary, and ultimately to
fall back to retaining the English term unchanged in the
hope that it might be a proper name or some other form
of cognate with Cebuano. Accents are stripped from the
documents and all language resources to facilitate match-
ing at that final step.
Although we have chosen techniques that are relatively
robust and therefore require relatively little domain-
specific tuning, stemmer design is an area of uncertainty
that could adversely affect retrieval effectiveness. We
therefore needed a test collection on which we could try
out variants of the Cebuano stemmer. We built this test
collection using 34,000 Cebuano Bible verses and 50 En-
glish questions that we found on the Web for which ap-
propriate Bible verses were known. Each question was
posed as a query using the batch mode of Inquery, and
the rank of the known relevant verse was taken as a mea-
sure of effectiveness. We took the mean reciprocal rank
(the inverse of the harmonic mean) as a figure of merit
for each configuration, and used a paired two-tailed   -
test (with p  0.05) to assess the statistical significance of
observed differences. Our initial configuration, without
stemming, obtained a mean inverse rank of 0.14, which
is a statistically significant improvement over no transla-
tion at all (mean inverse rank 0.02 from felicitous cognate
and loan word matches). The addition of Cebuano stem-
ming resulted in a reduction in mean inverse rank to 0.09.
Although the reduction is not statistically significant in
that case, the result suggests that our initial stemmer is
not yet useful for information retrieval tasks.
The other key capability that is needed is title and doc-
ument translation. We can accomplish this in one of two
ways. The simplest approach is to reverse the bilingual
term list, and to reverse the role of Cebuano and En-
glish in the process described above for query transla-
tion. Our user interface is capable of displaying multi-
ple translations for a single term (arranged horizontally
for compact depiction or vertically for clearer depiction),
but searchers can choose to display only the single most
likely translation. When reliable translation probability
statistics (from parallel text) are not available, we use the
relative word unigram frequency of each translation of a
Cebuano term in a representative English collection as a
substitute for that probability. A more sophisticated way
is to build a statistical machine translation system using
parallel text. We built our first statistical machine trans-
lation system within 40 hours of the announcement, and
one sentence of the resulting translation using each tech-
nique is shown below:
Cebuano: ?ang rebeldeng milf, kinsa
lakip sa nangamatay, nagdala og
backpack nga dunay explosives nga
niguba sa waiting lounge sa airport,
matod sa mga defense official.?
Term-by-term translation:
?(carelessness, circumference,
conveyence) rebeldeng milf, who lakip
(at in of) nangamatay, nagdala og
backpack nga valid explosives nga
niguba (at, in of) waiting lounge
(at, in, of) airport, matod (at, in,
of) mga defense official?
Statistical translation: ?who was
accused of rank, ehud og niguba
waiting lounge defense of those dumah
milf rebeldeng explosives backpack
airport matod official.?
At this point, term-by-term translation is clearly the bet-
ter choice. But as more parallel text becomes available,
we expect the situation to reverse. The LDC is prepar-
ing a set of human reference translations that will allow
us to detect that changeover point automatically using the
NIST variant of the BLEU measure for machine transla-
tion effectiveness.
4 Conclusion
The results reported in this paper were accomplished by
a team of 20 people with expertise in various facets of te
task that invested about 250 person-hours over two and
a half days. As additional Cebuano-specific evaluation
resources are developed, we expect to gain additional in-
sight into the quality of these early resources. Moreover,
once we see what works best for Cebuano by the end of
the process, we plan to revisit our process design with
an eye towards better optimizing our initial time invest-
ments. We expect to be able to address both of those
points in detail by the time of the conference.
This exercise was originally envisioned as a dry run to
work out the kinks in our process, and indeed we have
already learned a lot on that score. First, we learned that
our basic approach seems sound; we built the key com-
ponents of an interactive CLIR system in about 40 hours,
and by the 60-hour point we had some basis for believing
that each of those components could at least minimally
fulfill their role in a fully integrated system. Some of our
time was, however, spent on things that could have been
done in advance. Perhaps the most important of these
was the development of an information retrieval test col-
lection using the Bible. That job, and numerous smaller
ones, are now done, so we expect that we will be able to
obtain similar results with about half the effort next time
around.
Acknowledgments
Thanks to Clara Cabezas, Tim Hackman, Margie Hi-
nonangan, Burcu Karagol-Ayan, Okan Kolak, Huanfeng
Ma, Grazia Russo-Lassner, Michael Subotin, Jianqiang
Wang and the LDC! This work has been supported in part
by DARPA contract N660010028910.
Minimum Bayes-Risk Decoding for Statistical Machine Translation
Shankar Kumar and William Byrne  
Center for Language and Speech Processing, Johns Hopkins University,
3400 North Charles Street, Baltimore, MD, 21218, USA

skumar,byrne  @jhu.edu
Abstract
We present Minimum Bayes-Risk (MBR) de-
coding for statistical machine translation. This
statistical approach aims to minimize expected
loss of translation errors under loss functions
that measure translation performance. We de-
scribe a hierarchy of loss functions that incor-
porate different levels of linguistic information
from word strings, word-to-word alignments
from an MT system, and syntactic structure
from parse-trees of source and target language
sentences. We report the performance of the
MBR decoders on a Chinese-to-English trans-
lation task. Our results show that MBR decod-
ing can be used to tune statistical MT perfor-
mance for specific loss functions.
1 Introduction
Statistical Machine Translation systems have achieved
considerable progress in recent years as seen from their
performance on international competitions in standard
evaluation tasks (NIST, 2003). This rapid progress has
been greatly facilitated by the development of automatic
translation evaluation metrics such as BLEU score (Pa-
pineni et al, 2001), NIST score (Doddington, 2002)
and Position Independent Word Error Rate (PER) (Och,
2002). However, given the many factors that influence
translation quality, it is unlikely that we will find a single
translation metric that will be able to judge all these fac-
tors. For example, the BLEU, NIST and the PER metrics,

This work was supported by the National Science Foun-
dation under Grant No. 0121285 and an ONR MURI Grant
N00014-01-1-0685. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the au-
thors and do not necessarily reflect the views of the National
Science Foundation or the Office of Naval Research.
though effective, do not take into account explicit syntac-
tic information when measuring translation quality.
Given that different Machine Translation (MT) eval-
uation metrics are useful for capturing different aspects
of translation quality, it becomes desirable to create MT
systems tuned with respect to each individual criterion. In
contrast, the maximum likelihood techniques that under-
lie the decision processes of most current MT systems do
not take into account these application specific goals. We
apply the Minimum Bayes-Risk (MBR) techniques devel-
oped for automatic speech recognition (Goel and Byrne,
2000) and bitext word alignment for statistical MT (Ku-
mar and Byrne, 2002), to the problem of building au-
tomatic MT systems tuned for specific metrics. This is
a framework that can be used with statistical models of
speech and language to develop decision processes opti-
mized for specific loss functions.
We will show that MBR decoding can be applied to
machine translation in two scenarios. Given an automatic
MT metric, we design a loss function based on the met-
ric and use MBR decoding to tune MT performance un-
der the metric. We also show how MBR decoding can
be used to incorporate syntactic structure into a statistical
MT system by building specialized loss functions. These
loss functions can use information from word strings,
word-to-word alignments and parse-trees of the source
sentence and its translation. In particular we describe
the design of a Bilingual Tree Loss Function that can ex-
plicitly use syntactic structure for measuring translation
quality. MBR decoding under this loss function allows
us to integrate syntactic knowledge into a statistical MT
system without building detailed models of linguistic fea-
tures, and retraining the system from scratch.
We first present a hierarchy of loss functions for trans-
lation based on different levels of lexical and syntactic
information from source and target language sentences.
This hierarchy includes the loss functions useful in both
situations where we intend to apply MBR decoding. We
then present the MBR framework for statistical machine
translation under the various translation loss functions.
We finally report the performance of MBR decoders op-
timized for each loss function.
2 Translation Loss Functions
We now introduce translation loss functions to measure
the quality of automatically generated translations. Sup-
pose we have a sentence   in a source language for
which we have generated an automatic translation 
with word-to-word alignment  relative to   . The word-
to-word alignment  specifies the words in the source
sentence   that are aligned to each word in the transla-
tion  . We wish to compare this automatic translation
with a reference translation  with word-to-word align-
ment  relative to   .
We will now present a three-tier hierarchy of trans-
lation loss functions of the form 	


 
that measure 
 against 
 . These loss func-
tions will make use of different levels of information from
word strings, MT alignments and syntactic structure from
parse-trees of both the source and target strings as illus-
trated in the following table.
Loss Function Functional Form
Lexical 
Target Language Parse-Tree Minimum Bayes-Risk Word Alignments of Bilingual Texts
Shankar Kumar and William Byrne
Center for Language and Speech Processing, Johns Hopkins University,
3400 North Charles Street, Baltimore, MD, 21218, USA
skumar,byrne @jhu.edu
Abstract
We present Minimum Bayes-Risk word
alignment for machine translation. This
statistical, model-based approach attempts
to minimize the expected risk of align-
ment errors under loss functions that mea-
sure alignment quality. We describe var-
ious loss functions, including some that
incorporate linguistic analysis as can be
obtained from parse trees, and show that
these approaches can improve alignments
of the English-French Hansards.
1 Introduction
The automatic determination of word alignments in
bilingual corpora would be useful for Natural Lan-
guage Processing tasks such as statistical machine
translation, automatic dictionary construction, and
multilingual document retrieval. The development
of techniques in all these areas would be facili-
tated by automatic performance metrics, and align-
ment and translation quality metrics have been pro-
posed (Och and Ney, 2000b; Papineni et al, 2002).
However, given the difficulty of judging translation
quality, it is unlikely that a single, global metric will
be found for any of these tasks. It is more likely
that specialized metrics will be developed to mea-
sure specific aspects of system performance. This is
even desirable, as these specialized metrics could be
used in tuning systems for particular applications.
We have applied Minimum Bayes-Risk (MBR)
procedures developed for automatic speech recog-
nition (Goel and Byrne, 2000) to word alignment of
bitexts. This is a modeling approach that can be used
with statistical models of speech and language to de-
velop algorithms that are optimized for specific loss
functions. We will discuss loss functions that can
be used for word alignment and show how the over-
all alignment process can be improved by the use
of loss functions that incorporate linguistic features,
such as parses and part-of-speech tags.
2 Word-to-Word Bitext Alignment
We will study the problem of aligning an English
sentence to a French sentence and we will use the
word alignment of the IBM statistical translation
models (Brown et al, 1993).
Let and denote a pair of translated
English and French sentences. An English word is
defined as an ordered pair
, where the index refers to the posi-
tion of the word in the English sentence; is the
vocabulary of English; and the word at position is
the NULL word to which ?spurious? French words
may be aligned. Similarly, a French word is written
as .
An alignment between and is defined to be
a sequence where
. Under the alignment
, the French word is connected to the English
word . For every alignment , we define a link
set defined as whose ele-
ments are given by the alignment links
.
3 Alignment Loss Functions
In this section we introduce loss functions to mea-
sure the quality of automatically produced align-
ments. Suppose we wish to compare an automat-
ically produced alignment to a reference align-
ment , which we assume was produced by a com-
petent translator. We will define various loss func-
tions that measure the quality of relative
to through their link sets and .
The desirable qualities in translation are fluency
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 140-147.
                         Proceedings of the Conference on Empirical Methods in Natural
and adequacy. We assume here that both word se-
quences are fluent and adequate translations but that
the word and phrase correspondences are unknown.
It is these correspondences that we wish to deter-
mine and evaluate automatically.
We now present two general classes of loss func-
tions that measure alignment quality. In subsequent
sections, we will give specific examples of these and
show how to construct decoders that are optimized
for each loss function.
3.1 Alignment Error
The Alignment Error Rate (AER) introduced by
Och and Ney (2000b) measures the fraction of links
by which the automatic alignment differs from the
reference alignment. Links to the NULL word are
ignored. This is done by defining modified link sets
for the reference alignment
and the automatic alignment
.
The reference annotation procedure allowed the
human transcribers to identify which links in they
judged to be unambiguous. In addition to the ref-
erence alignment, this gives a set of sure links (S)
which is a subset of .
AER is defined as (Och and Ney, 2000b)
(1)
Since our modeling techniques require loss func-
tions rather than error rates, we introduce the Align-
ment Error loss function
(2)
We consider error rates to be ?normalized? loss
functions. We also note that, unlike AER, does
not distinguish between ambiguous and unambigu-
ous links. However, if a decoder generates an align-
ment for which is zero, the AER is
also zero. Therefore if AER is the metric of inter-
est, we will design alignment procedures to mini-
mize .
3.2 Generalized Alignment Error
We are interested in extending the Alignment Er-
ror loss function to incorporate various linguistic
features into the measurement of alignment quality.
The Generalized Alignment Error loss is defined as
(3)
where and
(4)
Here we have introduced the word-to-word distance
measure which compares the
links and as a function of the words in
the translation. refers to all loss functions that
have the form of Equation 3. Specific loss functions
are determined through the choice of . To see the
value in this, suppose is a verb in the French sen-
tence and that it is aligned in the reference alignment
to , the verb in the English sentence. If our goal is
to ensure verb alignment, then can be constructed
to penalize any link in the automatic align-
ment in which is not a verb. We will later give ex-
amples of distances in which is based on Part-
of-Speech (POS) tags, parse tree distances, and au-
tomatically determined word clusters. We note that
the can almost be reduced to , except for
the treatment of NULL in the English sentence.
4 Minimum Bayes-Risk Decoding For
Automatic Word Alignment
We present the Minimum Bayes-Risk alignment for-
mulation and derive MBR alignment procedures un-
der the loss functions of Section 3.
Given a translated pair of English-French sen-
tences , the decoder produces an align-
ment . Relative to a reference align-
ment , the decoder performance is measured as
. Our goal is to find the decoder that
has the best performance over all translated sen-
tences. This is measured through the Bayes Risk
. The ex-
pectation is taken with respect to the true distribu-
tion that describes ?human quality? align-
ments of translations as they are found in bitext.
Given a loss function and a probability distribu-
tion, it is well known that the decision rule which
minimizes the Bayes Risk is given by the follow-
ing expression (Bickel and Doksum, 1977; Goel and
Byrne, 2000).
(5)
Several modeling assumptions have been made to
obtain this form of the decoder. We do not have ac-
cess to the true distribution over translations. We
therefore use statistical MT models to approximate
. We furthermore assume that the space of
alignment alternatives can be restricted to an align-
ment lattice , which is a compact representation of
the most likely word alignments of the sentence pair
under the baseline models.
It is clear from Equation 5 that the MBR de-
coder is determined by the loss function. The Sen-
tence Alignment Error refers to the loss function
that gives a penalty of 1 for any errorful alignment:
, where is the indi-
cator function of the set . The MBR decoder un-
der this loss can easily be seen to be the Maximum
Likelihood (ML) alignment under the MT models:
. This illustrates why we
are interested in MBR decoders based on other loss
functions: the ML decoder is optimal with respect to
a loss function that is overly harsh. It does not dis-
tinguish between different types of alignment errors
and good alignments receive the same penalty as
poor alignments. Moreover, such a harsh penalty is
particularly inappropriate when unambiguous word-
to-word alignments cannot be provided in all cases
even by human translators who produce the refer-
ence alignments. The AER makes an explicit dis-
tinction between ambiguous and unambiguous word
alignments. Ideally, the decoder should be able to do
so as well. Motivated by this, the MBR hypothesis
can be thought of as the consensus hypothesis un-
der a particular loss function: Equation 5 selects the
hypothesis that is, in an average sense, close to the
other likely hypotheses. In this way, ambiguity can
be reduced by selecting the hypothesis that is ?most
similar? to the collection of most likely competing
hypotheses.
We now describe the alignment lattice (Sec-
tion 4.1) and introduce the lattice based probabilities
required for the MBR alignment (Section 4.2). The
derivation of the MBR alignment under the AE and
GAE loss functions is presented in Sections 4.3 and
4.4.
4.1 Alignment Lattice
The lattice is represented as a Weighted Finite
State Transducer (WFST) (Mohri et al, 2000)
with a finite set of states , a set of
transition labels , an initial state , the set of fi-
nal states , and a finite set of transitions . A
transition in this WFST is given by
where is the starting state, is the ending state,
is the alignment link and is the weight. For
an English sentence of length and a French sen-
tence of length , we define as
.
A complete path through the WFST is a sequence
of transitions given by
such that and . Each complete
path defines an alignment link set .
When we write , we mean that is derived
from a complete path through . This allows us to
use alignment models in which the probability of an
alignment can be written as a sum over alignment
link weights, i.e. .
4.2 Alignment Link Posterior Probability
We first introduce the lattice transition posterior
probability of each transition in the
lattice
(6)
where is if and otherwise. The
lattice transition posterior probability is the sum of
the posterior probabilities of all lattice paths pass-
ing through the transition . This can be com-
puted very efficiently with a forward-backward al-
gorithm on the alignment lattice (Wessel et al,
1998). is the posterior probability of an
alignment link set which can be written as
(7)
We now define the alignment link posterior prob-
ability for a link
(8)
where . This is the probability
that any two words are aligned given all the
alignments in the lattice .
4.3 MBR Alignment Under
In this section we derive MBR alignment under the
Alignment Error loss function (Equation 2). The op-
timal decoder has the form (Equation 5)
(9)
The summation is equal to
If is the subset of transitions ( )
that do not contain links with the NULL word, we
can simplify the bracketed term as
For an alignment link we note that
. Therefore, the
MBR alignment (Equation 9) can be found in terms
of the modified link weight for each alignment link
(10)
We can rewrite the above equation as
(11)
4.4 MBR Alignment Under
We now derive MBR alignment under the Gener-
alized Alignment Error loss function (Equation 3).
The optimal decoder has the form (Equation 5)
(12)
The summation can be rewritten as
where and .
We can simplify the bracketed term as
where and .
The MBR alignment (Equation 12) can be found
in terms of the modified link weight for each align-
ment link
(13)
4.5 MBR Alignment Using WFST Techniques
The MBR alignment procedures under the and
loss functions begin with a WFST that con-
tains the alignment probabilities as de-
scribed in Section 4.1. To build the MBR decoder
for each loss function the weights on the transitions
( ) of the WFST are modified ac-
cording to either Equation 11 ( ) or Equa-
tion 13 ( ). Once the weights are modified,
the search procedure for the MBR alignment is the
same in each case. The search is carried out using a
shortest-path algorithm (Mohri et al, 2000).
5 Word Alignment Experiments
We present here examples of Generalized Align-
ment Error loss functions based on three types of
linguistic features and show how they can be incor-
porated into a statistical MT system to obtain auto-
matic alignments.
5.1 Syntactic Distances From Parse-Trees
Suppose a parser is available that generates a parse-
tree for the English sentence. Our goal is to con-
struct an alignment loss function that incorporates
features from the parse. One way to do this is to
define a graph distance
(14)
Here and are the parse-tree leaf nodes cor-
responding to the English words and . This
quantity is computed as the sum of the distances
from each node to their closest common ancestor.
It gives a syntactic distance between any pair of
English words based on the parse-tree. This dis-
tance has been used to measure word association for
information retrieval (Mittendorfer and Winiwarter,
2001). It reflects how strongly the words and
are bound together by the syntactic structure of the
English sentence as determined by the parser. Fig-
ure 1 shows the parse tree for an English sentence
in the test data with the pairwise syntactic distances
between the English words corresponding to the leaf
nodes.
TOP
S
NP
PRP i
VP
VBP think SBAR
S
NP
DT that
VP
VBZ is ADJP
JJ good . .
Pairwise Distances
g("i","think") = 4
g("i", "that") = 7
g("i","is") = 7
g("i" , "good") = 8
g("i" , ".") = 8
Figure 1: Parse tree for a English sentence with the
pairwise syntactic distances between words.
To obtain these distances, Ratnaparkhi?s part-
of-speech (POS) tagger (Ratnaparkhi, 1996) and
Collins? parser (Collins, 1999) were used to obtain
parse trees for the English side of the test corpus.
With defined as in Equation 14, the Generalized
Alignment Error loss function (Equation 3) is called
the Parse-Tree Syntactic Distance ( ).
5.2 Distances Derived From Part-of-Speech
Labels
Suppose a Part-of-Speech(POS) tagger is available
to tag each word in the English sentence. If POS
denotes the POS of the English word , we can de-
fine the word-to-word distance measure (Equa-
tion 4) as
POS POS (15)
Ratnaparkhi?s POS tagger (Ratnaparkhi, 1996)
was used to obtain POS tags for each word in
the English sentence. With specified by Equa-
tion 15, the Generalized Alignment Error loss func-
tion (Equation 3) is called the Part-Of-Speech Dis-
tance ( ).
5.3 Automatic Word Cluster Distances
Suppose we are working in a language for which
parsers and POS taggers are not available. In this
situation we might wish to construct the loss func-
tions based on word classes determined by auto-
matic clustering procedures. If specifies the
word cluster for the English word , then we define
the distance
(16)
In our experiments we obtained word clusters
for English words using a statistical learning proce-
dure (Kneser and Ney, 1991) where the total number
of word classes is restricted to be 100. With as
defined in Equation 16, the Generalized Alignment
Error loss function (Equation 3) is called the Auto-
matic Word Class Distance ( ).
5.4 IBM-3 Word Alignment Models
Since the true distribution over alignments is not
known, we used the IBM-3 statistical transla-
tion model (Brown et al, 1993) to approximate
. This model is specified through four
components: Fertility probabilities for words; Fer-
tility probabilities for NULL; Word Translation
probabilities; and Distortion probabilities. We
used a modified version of the IBM-3 distortion
model (Knight and Al-Onaizan, 1998) in which
each of the possible permutations of the French
sentence is equally likely. The IBM-3 models
were trained on a subset of the Canadian Hansards
French-English data which consisted of 50,000 par-
allel sentences (Och and Ney, 2000b). The vocab-
ulary size was 18,499 for English and 24,198 for
French. The GIZA++ toolkit (Och and Ney, 2000a)
was used for training the IBM-3 models (as in (Och
and Ney, 2000b)).
5.5 Word Alignment Lattice Generation
We obtained word alignments under the
modified IBM-3 models using the finite
state translation framework introduced by
Knight and Al-Onaizan (1998). The finite state
operations were carried out using the AT&T Finite
State Machine Toolkit (Mohri et al, 2001; Mohri et
al., 2000).
The WFST framework involves building a trans-
ducer for each constituent of the IBM-3 Alignment
Models: the word fertility model ; the NULL fer-
tility model ; and the word translation model
(Section 5.4). For each sentence pair we also built a
finite state acceptor that accepts the English sen-
tence and another acceptor which accepts all legal
permutations of the French sentence. The alignment
lattice for the sentence pair was then obtained
by the following weighted finite state composition
. In practice, the WFST ob-
tained by the composition was pruned to a maximum
of 10,000 states using a likelihood based pruning op-
eration. In terms of AT&T Finite State Toolkit shell
commands, these operations are given as:
fsmcompose E M fsmcompose - N
fsmcompose - T fsmcompose - F
fsmprune -n 10000
The finite state composition and pruning were per-
formed using lazy implementations of algorithms
provided in AT&T Finite State libraries (Mohri et
al., 2000). This made the computation efficient be-
cause even though five WFSTs are composed into
a potentially huge transducer, only a small portion
of it is actually searched during the pruning used to
generate the final lattice.
A heavily pruned alignment lattice for a
sentence-pair from the test data is shown in Fig-
ure 2. For clarity of presentation, each alignment
link in the lattice is shown as an ordered
pair where and are
the English and French words on the link. For each
sentence, we also computed the lattice path with the
highest probability . This gives the ML
alignment under the statistical MT models that will
give our baseline performance under the various loss
functions.
5.6 Performance Under The Alignment Error
Rates
Our unseen test data consisted of 207 French-
English sentence pairs from the Hansards cor-
pus (Och and Ney, 2000b). These sentence pairs had
at most 16 words in the French sentence; this restric-
tion on the sentence length was necessary to control
the memory requirements of the composition.
5.6.1 MBR Consensus Alignments
In the previous sections we introduced a total
of four loss functions: , , and
. Using either Equation 11 or 13, an MBR
decoder can be constructed for each. These decoders
are called MBR-AE, MBR-PTSD, MBR-POSD, and
MBR-AWCD, respectively.
5.6.2 Evaluation Metrics
The performance of the four decoders was mea-
sured with respect to the alignments provided by hu-
man experts (Och and Ney, 2000b). The first eval-
uation metric used was the Alignment Error Rate
(Equation 1). We also evaluated each decoder un-
der the Generalized Alignment Error Rates (GAER).
These are defined as:
(17)
There are six variants of GAER. These arise
when is specified by ,
or . There are two versions of each
of these: one version is sensitive only to sure
(S) links. The other version considers all (A)
links in the reference alignment. We there-
fore have the following six Generalized Alignment
Error Rates: PTSD-S, POSD-S, AWCD-S, and
PTSD-A, POSD-A, AWCD-A. We say we have a
matched condition when the same loss function is
used in both the error rate and the decoder design.
 0
1NULL_0:a_4/5.348
3
it_1:ce_1/2.344
2it_1:ce_1/1.927
4NULL_0:a_4/5.348
6
is_2:est_2/1.349
5
is_2:est_2/1.349
9
quite_3:tout_3/4.132
8
quite_3:fait_5/4.405is_2:est_2/0.933
7NULL_0:a_4/5.348
10
quite_3:fait_5/2.195
quite_3:tout_3/1.921
quite_3:tout_3/3.715
quite_3:fait_5/3.989
11understandable_4:comprehensible_6/2.161 12/0._5:._7/0.432
Figure 2: A heavily pruned alignment lattice for the English-French sentence pair
e=?it is quite understandable .? f=?ce est tout a fait comprehensible .?.
5.6.3 Decoder Performance
The performance of the decoders under various
loss functions is given in Table 1. We observe that
in none of these experiments was the ML decoder
found to be optimal. In all instances, the MBR
decoder tuned for each loss function was the best
performing decoder under the corresponding error
rate. In particular, we note that alignment perfor-
mance as measured under the AER metric can be
improved by using MBR instead of ML alignment.
This demonstrates the value of finding decoding pro-
cedures matched to the performance criterion of in-
terest.
We observe some affinity among the loss func-
tions. In particular, the ML decoder performs better
under the AER than any of the MBR-GAE decoders.
This is because the loss, for which the ML de-
coder is optimal, is closer to the loss than any
of the loss functions. The NULL symbol is
treated quite differently under and , and
this leads to a large mismatch between the MBR-
GAE decoders and the AER metric. Similarly, the
performance of the MBR-POS decoder degrades
significantly under the AWCD-S and AWCD-A met-
rics. Since there are more word clusters (100) than
POS tags (55), the MBR-POS decoder is therefore
incapable of producing hypotheses that can match
the word clusters used in the AWCD metrics.
6 Discussion And Conclusions
We have presented a Minimum Bayes-Risk decod-
ing strategy for obtaining word alignments of bilin-
gual texts. MBR decoding is a general formulation
that allows the construction of specialized decoders
from general purpose models. The strategy aims at
direct minimization of the expected risk of align-
ment errors under a given alignment loss function.
We have introduced several alignment loss func-
tions to measure the alignment quality. These in-
corporate information from varied analyses, such
as parse trees, POS tags, and automatically derived
word clusters. We have derived and implemented
lattice based MBR consensus decoders under these
loss functions. These decoders rescore the lattices
produced by maximum likelihood decoding to pro-
duce the optimal MBR alignments.
We have chosen to present MBR decoding using
the IBM-3 statistical MT models implemented via
WFSTs. However MBR decoding is not restricted
to this framework. It can be applied more broadly
using other MT model architectures that might be
selected for reasons of modeling fidelity or compu-
tational efficiency.
We have presented these alignment loss functions
to explore how linguistic knowledge might be in-
corporated into machine translation systems without
building detailed statistical models of these linguis-
tic features. However we stress that the MBR decod-
ing procedures described here do not preclude the
construction of complex MT models that incorporate
linguistic features. The application of such mod-
els, which could be trained using conventional max-
imum likelihood estimation techniques, should still
benefit by the application of MBR decoding tech-
niques.
In future work we will investigate loss functions
that incorporate French and English parse-tree infor-
mation into the alignment decoding process. Our ul-
timate goal, towards which this work is the first step,
is to construct loss functions that take advantage of
linguistic structures such as syntactic dependencies
found through monolingual analysis of the sentences
to be aligned. Recent work (Hwa et al, 2002) sug-
gests that translational corresponence of linguistic
structures can indeed be useful in projecting parses
across languages. Our ideal would be to construct
MBR decoders based on loss functions that are sen-
sitive both to word alignment as well as to agreement
in higher level structures such as parse trees. In this
way ambiguity present in word-to-word alignments
will be resolved by the alignment of linguistic struc-
tures.
Generalized Alignment Error Rates
Decoder AER PTSD-S POSD-S AWCD-S PTSD-A POSD-A AWCD-A
ML 18.13 3.13 4.35 4.69 29.39 51.36 54.58
MBR-AE 14.87 1.34 1.89 1.94 19.81 36.42 38.58
MBR-PTSD 23.26 0.62 0.69 0.82 14.45 26.76 28.42
MBR-POSD 28.60 2.43 0.69 3.23 15.70 26.28 29.48
MBR-AWCD 24.71 1.00 0.95 0.86 14.92 26.83 28.39
Table 1: Performance (%) of the MBR decoders under the Alignment Error and Generalized Alignment
Error Rates. For each metric the error rate of the matched decoder is in bold.
MBR alignment is a promising modeling frame-
work for the detailed linguistic annotation of bilin-
gual texts. It is a simple model rescoring formalism
that improves well trained statistical models by tun-
ing them for particular performance criteria. Ideally,
it will be used to produce decoders optimized for
the loss functions that actually measure the qualities
that we wish to see in newly developed automatic
systems.
Acknowledgments
We would like to thank F. J. Och of RWTH, Aachen
for providing us the GIZA++ SMT toolkit, the mk-
cls toolkit to train word classes, the Hansards 50K
training and test data, and the reference word align-
ments and AER metric software. We would also like
to thank P. Resnik, R. Hwa and O. Kolak of the Univ.
of Maryland for useful discussions and help with the
GIZA++ setup. We thank AT&T Labs - Research for
use of the FSM Toolkit. This work was supported by
an ONR MURI grant N00014-01-1-0685.
References
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania, Philadelphia, PA, USA.
V. Goel and W. Byrne. 2000. Minimum Bayes-risk auto-
matic speech recognition. Computer Speech and Lan-
guage, 14(2):115?135.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using annota-
tion projection. In Proceedings of ACL-2002. To ap-
pear.
R. Kneser and H. Ney. 1991. Forming word classes by
statistical clustering for statistical language modelling.
In The 1st Quantititative Linguistics Conference, Trier,
Germany.
K. Knight and Y. Al-Onaizan. 1998. Translation with
finite-state devices. In Proceedings of the AMTA Con-
ference, pages 421?437, Langhorne, PA, USA.
M. Mittendorfer and W. Winiwarter. 2001. Experiments
with the use of syntactic analysis in information re-
trieval. In Proceedings of the 6th International Work-
shop on Applications of Natural Language and Infor-
mation Systems, Bonn, Germany.
M. Mohri, F. C. N. Pereira, and M. Riley. 2000. The
design principles of a weighted finite-state transducer
library. Theoretical Computer Science, 231(1):17?32.
M. Mohri, F. Pereira, and M. Riley, 2001. ATT
General-purpose finite-state machine software tools.
http://www.research.att.com/sw/tools/fsm/.
F. Och and H. Ney. 2000a. A comparison of alignment
models for statistical machine translation. In Proceed-
ings Of 18th Conference On Computational Linguis-
tics, pages 1086?1090, Saarbrucken, Germany.
F. Och and H. Ney. 2000b. Improved statistical align-
ment models. In Proceedings of ACL-2000, pages
440?447, Hong Kong, China.
K. Papineni, S. Roukos, T. Ward, J. Henderson, and
F. Reeder. 2002. Corpus-based comprehensive and di-
agnostic mt evaluation: Initial arabic, chinese, french,
and spanish results. In Proceedings of HLT 2002.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 133?142, Philadelphia, PA, USA.
F. Wessel, K. Macherey, and R. Schlueter. 1998. Us-
ing word probabilities as confidence measures. In Pro-
ceedings of ICASSP-98, pages 225?228, Seattle, WA,
USA.
Coling 2008: Companion volume ? Posters and Demonstrations, pages 19?22
Manchester, August 2008
Phrasal Segmentation Models for Statistical Machine Translation
Graeme Blackwood, Adri
`
a de Gispert, William Byrne
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
Trumpington Street, Cambridge, CB2 1PZ, U.K.
{gwb24|ad465|wjb31}@cam.ac.uk
Abstract
Phrasal segmentation models define a
mapping from the words of a sentence
to sequences of translatable phrases. We
discuss the estimation of these models
from large quantities of monolingual train-
ing text and describe their realization as
weighted finite state transducers for incor-
poration into phrase-based statistical ma-
chine translation systems. Results are re-
ported on the NIST Arabic-English trans-
lation tasks showing significant comple-
mentary gains in BLEU score with large
5-gram and 6-gram language models.
1 Introduction
In phrase-based statistical machine transla-
tion (Koehn et al, 2003) phrases extracted from
word-aligned parallel data are the fundamental
unit of translation. Each phrase is a sequence
of contiguous translatable words and there is no
explicit model of syntax or structure.
Our focus is the process by which a string of
words is segmented as a sequence of such phrases.
Ideally, the segmentation process captures two as-
pects of natural language. Firstly, segmentations
should reflect the underlying grammatical sentence
structure. Secondly, common sequences of words
should be grouped as phrases in order to preserve
context and respect collocations. Although these
aspects of translation are not evaluated explicitly,
phrases have been found very useful in transla-
tion. They have the advantage that, within phrases,
words appear as they were found in fluent text.
However, reordering of phrases in translation can
lead to disfluencies. By defining a distribution over
possible segmentations, we hope to address such
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
disfluencies. A strength of our approach is that it
exploits abundantly available monolingual corpora
that are usually only used for training word lan-
guage models.
Most prior work on phrase-based statistical lan-
guage models concerns the problem of identifying
useful phrasal units. In (Ries et al, 1996) an iter-
ative algorithm selectively merges pairs of words
as phrases with the goal of minimising perplex-
ity. Several criteria including word pair frequen-
cies, unigram and bigram log likelihoods, and a
correlation coefficient related to mutual informa-
tion are compared in (Kuo and Reichl, 1999). The
main difference between these approaches and the
work described here is that we already have a defi-
nition of the phrases of interest (i.e. the phrases of
the phrase table extracted from parallel text) and
we focus instead on estimating a distribution over
the set of possible alternative segmentations of the
sentence.
2 Phrasal Segmentation Models
Under the generative model of phrase-based statis-
tical machine translation, a source sentence s
I
1
gen-
erates sequences u
K
1
= u
1
, . . . , u
K
of source lan-
guage phrases that are to be translated. Sentences
cannot be segmented into phrases arbitrarily: the
space of possible segmentations is constrained by
the contents of the phrase table which consists of
phrases found with translations in the parallel text.
We start initially with a distribution in which seg-
mentations assume the following dependencies:
P (u
K
1
,K|s
I
1
) = P (u
K
1
|K, s
I
1
)P (K|I). (1)
The distribution over the number of phrases K is
chosen to be uniform, i.e. P (K|I) = 1/I, K ?
{1, 2, . . . , I}, and all segmentations are considered
equally likely. The probability of a particular seg-
mentation is therefore
P (u
K
1
|K, s
I
1
) =
{
C(K, s
I
1
) if u
K
1
= s
I
1
0 otherwise
(2)
19
where C(K, s
I
1
) is chosen to ensure normalisation
and the phrases u
1
, . . . , u
K
are found in the phrase
table. This simple model of segmentation has been
found useful in practice (Kumar et al, 2006).
Our goal is to improve upon the uniform seg-
mentation of equation (2) by estimating the phrasal
segmentation model parameters from naturally oc-
curing phrase sequences in a large monolingual
training corpus. An order-n phrasal segmentation
model assigns a probability to a phrase sequence
u
K
1
according to
P (u
K
1
|K, s
I
1
) =
K
?
k=1
P (u
k
|u
k?1
1
,K, s
I
1
) ?
{
C(K, s
I
1
)
?
K
k=1
P (u
k
|u
k?1
k?n+1
) if u
K
1
= s
I
1
0 otherwise
(3)
where the approximation is due to the Markov as-
sumption that only the most recent n ? 1 phrases
are useful when predicting the next phrase. Again,
each u
k
must be a phrase with a known transla-
tion. For a fixed sentence s
I
1
, the normalisation
term C(K, s
I
1
) can be calculated. In translation,
however, calculating this quantity becomes harder
since the s
I
1
are not fixed. We therefore ignore
the normalisation and use the unnormalised like-
lihoods as scores.
2.1 Parameter Estimation
We focus on first-order phrasal segmentation mod-
els. Although we have experimented with higher-
order models we have not yet found them to yield
improved translation.
Let f(u
k?1
, u
k
) be the frequency of occurrence
of a string of words w
j
i
in a very large training
corpus that can be split at position x such that
i < x ? j and the substrings w
x?1
i
and w
j
x
match
precisely the words of two phrases u
k?1
and u
k
in
the phrase table. The maximum likelihood proba-
bility estimate for phrase bigrams is then their rel-
ative frequency:
?
P (u
k
|u
k?1
) =
f(u
k?1
, u
k
)
f(u
k?1
)
. (4)
These maximum likelihood estimates are dis-
counted and smoothed with context-dependent
backoff such that
P (u
k
|u
k?1
) =
{
?(u
k?1
, u
k
)
?
P (u
k
|u
k?1
) if f(u
k?1
, u
k
) > 0
?(u
k?1
)P (u
k
) otherwise
(5)
where ?(u
k?1
, u
k
) discounts the maximum like-
lihood estimates and the context-specific backoff
weights ?(u
k?1
) are chosen to ensure normalisa-
tion.
3 The Transducer Translation Model
The Transducer Translation Model (TTM) (Kumar
and Byrne, 2005; Kumar et al, 2006) is a gener-
ative model of translation that applies a series of
transformations specified by conditional probabil-
ity distributions and encoded as Weighted Finite
State Transducers (Mohri et al, 2002).
The generation of a target language sentence
t
J
1
starts with the generation of a source lan-
guage sentence s
I
1
by the source language model
P
G
(s
I
1
). Next, the source language sentence is
segmented according to the uniform phrasal seg-
mentation model distribution P
W
(u
K
1
,K|s
I
1
) of
equation (2). The phrase translation and reorder-
ing model P
?
(v
R
1
|u
K
1
) generates the reordered se-
quence of target language phrases v
R
1
. Finally,
the reordered target language phrases are trans-
formed to word sequences t
J
1
under the target
segmentation model P
?
(t
J
1
|v
R
1
). These compo-
nent distributions together form a joint distribu-
tion over the source and target language sentences
and their possible intermediate phrase sequences
as P (t
J
1
, v
R
1
, u
K
1
, s
I
1
).
In translation under the generative model, we
start with the target sentence t
J
1
in the foreign lan-
guage and search for the best source sentence s?
I
1
.
Encoding each distribution as a WFST leads to a
model of translation as a series of compositions
L = G ?W ? ? ? ? ? T (6)
in which T is an acceptor for the target language
sentence and L is the word lattice of translations
obtained during decoding. The most likely trans-
lation s?
I
1
is the path in L with least cost.
The above approach generates a word lattice L
under the unweighted phrasal segmentation model
of equation (2). In the initial experiments reported
here, we apply the weighted phrasal segmentation
model via lattice rescoring. We take the word lat-
tice L and compose it with the unweighted trans-
ducer W to obtain a lattice of phrases L ?W ; this
lattice contains phrase sequences and translation
scores consistent with the initial translation. We
also extract the complete list of phrases relevant to
each translation.
20
We then wish to apply the phrasal segmentation
model distribution of equation (3) to this phrase
lattice. The conditional probabilities and backoff
structure defined in equation (5) can be encoded
as a weighted finite state acceptor (Allauzen et al,
2003). In this acceptor, ?, states encode histories
and arcs define the bigram and backed-off unigram
phrase probabilities. We note that the raw counts
of equation (4) are collected prior to translation
and the first-order probabilities are estimated only
for phrases found in the lattice.
The phrasal segmentation model is composed
with the phrase lattice and projected on the in-
put to obtain the rescored word lattice L
?
=
(L ?W ) ??. The most likely translation after ap-
plying the phrasal segmentation model is found as
the path in L
?
with least cost. Apart from likeli-
hood pruning when generating the original word
lattice, the model scores are included correctly in
translation search.
4 System Development
We describe experiments on the NIST Arabic-
English machine translation task and apply phrasal
segmentation models in lattice rescoring.
The development set mt02-05-tune is formed
from the odd numbered sentences of the NIST
MT02?MT05 evaluation sets; the even numbered
sentences form the validation set mt02-05-test.
Test performance is evaluated using the NIST sub-
sets from the MT06 evaluation: mt06-nist-nw for
newswire data and mt06-nist-ng for newsgroup
data. Results are also reported for the MT08 evalu-
ation. Each set contains four references and BLEU
scores are computed for lower-case translations.
The uniformly segmented TTM baseline system
is trained using all of the available Arabic-English
data for the NIST MT08 evaluation
1
. In first-pass
translation, decoding proceeds with a 4-gram lan-
guage model estimated over the parallel text and a
965 million word subset of monolingual data from
the English Gigaword Third Edition. Minimum
error training (Och, 2003) under BLEU optimises
the decoder feature weights using the development
set mt02-05-tune. In the second pass, 5-gram and
6-gram zero-cutoff stupid-backoff (Brants et al,
2007) language models estimated using 4.7 billion
words of English newswire text are used to gener-
ate lattices for phrasal segmentation model rescor-
ing. The phrasal segmentation model parameters
1
http://www.nist.gov/speech/tests/mt/2008/
mt02-05-tune mt02-05-test
TTM+MET 48.9 48.6
+6g 51.9 51.7
+6g+PSM 52.7 52.7
Table 2: BLEU scores for phrasal segmentation
model rescoring of 6-gram rescored lattices.
are trained using a 1.8 billion word subset of the
same monolingual training data used to build the
second-pass word language model. A phrasal seg-
mentation model scale factor and phrase insertion
penalty are tuned using the development set.
5 Results and Analysis
First-pass TTM translation lattices generated with
a uniform segmentation obtain baseline BLEU
scores of 48.9 for mt02-05-tune and 48.6 for
mt02-05-test. In our experiments we demon-
strate that phrasal segmentation models continue
to improve translation even for second-pass lat-
tices rescored with very large zero-cutoff higher-
order language models. Table 1 shows phrasal seg-
mentation model rescoring of 5-gram lattices. The
phrasal segmentation models consistently improve
the BLEU score: +1.1 for both the development
and validation sets, and +1.4 and +0.4 for the in-
domain newswire and out-of-domain newsgroup
test sets. Rescoring MT08 gives gains of +0.9 on
mt08-nist-nw and +0.3 on mt08-nist-ng.
For a limited quantity of training data it is not
always possible to improve translation quality sim-
ply by increasing the order of the language model.
Comparing tables 1 and 2 shows that the gains in
moving from a 5-gram to a 6-gram are small. Even
setting aside the practical difficulty of estimating
and applying such higher-order language models,
it is doubtful that further gains could be had simply
by increasing the order. That the phrasal segmenta-
tion models continue to improve upon the 6-gram
lattice scores suggests they capture more than just
a longer context and that they are complementary
to word-based language models.
The role of the phrase insertion penalty is to
encourage longer phrases in translation. Table 3
shows the effect of tuning this parameter. The
upper part of the table shows the BLEU score,
brevity penalty and individual n-gram precisions.
The lower part shows the total number of words
in the output, the number of words translated as
a phrase of the specified length, and the average
number of words per phrase. When the insertion
21
mt02-05-tune mt02-05-test mt06-nist-nw mt06-nist-ng mt08-nist-nw mt08-nist-ng
TTM+MET 48.9 48.6 46.1 35.2 48.4 33.7
+5g 51.5 51.5 48.4 36.7 49.1 36.4
+5g+PSM 52.6 52.6 49.8 37.1 50.0 36.7
Table 1: BLEU scores for phrasal segmentation model rescoring of 5-gram rescored lattices.
PIP -4.0 -2.0 0.0 2.0 4.0
BLEU 48.6 50.1 51.1 49.9 48.7
BP 0.000 0.000 0.000 -0.034 -0.072
1g 82.0 83.7 84.9 85.7 86.2
2g 57.3 58.9 59.9 60.5 61.1
3g 40.8 42.2 43.1 43.6 44.2
4g 29.1 30.3 31.1 31.5 32.0
words 70550 66964 63505 60847 58676
1 58840 46936 25040 15439 11744
2 7606 12388 18890 19978 18886
3 2691 4890 11532 13920 14295
4 860 1820 5016 6940 8008
5 240 450 1820 2860 3500
6+ 313 480 1207 1710 2243
w/p 1.10 1.21 1.58 1.86 2.02
Table 3: Effect of phrase insertion penalty (PIP)
on BLEU score, brevity penalty (BP), individual
n-gram precisions, phrase length distribution, and
average words per phrase (w/p) for mt02-05-tune.
penalty is too low, single word phrases dominate
the output and any benefits from longer context or
phrase-internal fluency are lost. As the phrase in-
sertion penalty increases, there are large gains in
precision at each order and many longer phrases
appear in the output. At the optimal phrase in-
sertion penalty, the average phrase length is 1.58
words and over 60% of the translation output is
generated from multi-word phrases.
6 Discussion
We have defined a simple model of the phrasal seg-
mentation process for phrase-based SMT and esti-
mated the model parameters from naturally occur-
ring phrase sequence examples in a large training
corpus. Applying first-order models to the NIST
Arabic-English machine translation task, we have
demonstrated complementary improved transla-
tion quality through exploitation of the same abun-
dantly available monolingual data used for training
regular word-based language models.
Comparing the in-domain newswire and out-
of-domain newsgroup test set performance shows
the importance of choosing appropriate data for
training the phrasal segmentation model param-
eters. When in-domain data is of limited avail-
ability, count mixing or other adaptation strategies
may lead to improved performance.
Acknowledgements
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-06-
C-0022.
References
Allauzen, Cyril, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 557?564.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on EMNLP and CoNLL,
pages 858?867.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference for Computational
Linguistics on Human Language Technology, pages
48?54, Morristown, NJ, USA.
Kumar, Shankar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of the conference on HLT
and EMNLP, pages 161?168.
Kumar, Shankar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer translation
template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Kuo, Hong-Kwang Jeff and Wolfgang Reichl. 1999.
Phrase-based language models for speech recogni-
tion. In Sixth European Conference on Speech Com-
munication and Technology, pages 1595?1598.
Mohri, Mehryar, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language,
volume 16, pages 69?88.
Och, Franz Josef. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 160?167, Morristown, NJ, USA.
Ries, Klaus, Finn Dag Bu, and Alex Waibel. 1996.
Class phrase models for language modeling. In Pro-
ceedings of the 4th International Conference on Spo-
ken Language Processing.
22
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380?388,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Rule Filtering by Pattern for Efficient Hierarchical Translation
Gonzalo Iglesias? Adria` de Gispert?
? University of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain
{giglesia,erbanga}@gts.tsc.uvigo.es
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
Eduardo R. Banga? William Byrne?
Abstract
We describe refinements to hierarchical
translation search procedures intended to
reduce both search errors and memory us-
age through modifications to hypothesis
expansion in cube pruning and reductions
in the size of the rule sets used in transla-
tion. Rules are put into syntactic classes
based on the number of non-terminals and
the pattern, and various filtering strate-
gies are then applied to assess the impact
on translation speed and quality. Results
are reported on the 2008 NIST Arabic-to-
English evaluation task.
1 Introduction
Hierarchical phrase-based translation (Chiang,
2005) has emerged as one of the dominant cur-
rent approaches to statistical machine translation.
Hiero translation systems incorporate many of
the strengths of phrase-based translation systems,
such as feature-based translation and strong tar-
get language models, while also allowing flexi-
ble translation and movement based on hierarchi-
cal rules extracted from aligned parallel text. The
approach has been widely adopted and reported to
be competitive with other large-scale data driven
approaches, e.g. (Zollmann et al, 2008).
Large-scale hierarchical SMT involves auto-
matic rule extraction from aligned parallel text,
model parameter estimation, and the use of cube
pruning k-best list generation in hierarchical trans-
lation. The number of hierarchical rules extracted
far exceeds the number of phrase translations typ-
ically found in aligned text. While this may lead
to improved translation quality, there is also the
risk of lengthened translation times and increased
memory usage, along with possible search errors
due to the pruning procedures needed in search.
We describe several techniques to reduce mem-
ory usage and search errors in hierarchical trans-
lation. Memory usage can be reduced in cube
pruning (Chiang, 2007) through smart memoiza-
tion, and spreading neighborhood exploration can
be used to reduce search errors. However, search
errors can still remain even when implementing
simple phrase-based translation. We describe a
?shallow? search through hierarchical rules which
greatly speeds translation without any effect on
quality. We then describe techniques to analyze
and reduce the set of hierarchical rules. We do
this based on the structural properties of rules and
develop strategies to identify and remove redun-
dant or harmful rules. We identify groupings of
rules based on non-terminals and their patterns and
assess the impact on translation quality and com-
putational requirements for each given rule group.
We find that with appropriate filtering strategies
rule sets can be greatly reduced in size without im-
pact on translation performance.
1.1 Related Work
The search and rule pruning techniques described
in the following sections add to a growing lit-
erature of refinements to the hierarchical phrase-
based SMT systems originally described by Chi-
ang (2005; 2007). Subsequent work has addressed
improvements and extensions to the search proce-
dure itself, the extraction of the hierarchical rules
needed for translation, and has also reported con-
trastive experiments with other SMT architectures.
Hiero Search Refinements Huang and Chiang
(2007) offer several refinements to cube pruning
to improve translation speed. Venugopal et al
(2007) introduce a Hiero variant with relaxed con-
straints for hypothesis recombination during pars-
ing; speed and results are comparable to those of
cube pruning, as described by Chiang (2007). Li
and Khudanpur (2008) report significant improve-
ments in translation speed by taking unseen n-
grams into account within cube pruning to mini-
mize language model requests. Dyer et al (2008)
380
extend the translation of source sentences to trans-
lation of input lattices following Chappelier et al
(1999).
Extensions to Hiero Blunsom et al (2008)
discuss procedures to combine discriminative la-
tent models with hierarchical SMT. The Syntax-
Augmented Machine Translation system (Zoll-
mann and Venugopal, 2006) incorporates target
language syntactic constituents in addition to the
synchronous grammars used in translation. Shen
at al. (2008) make use of target dependency trees
and a target dependency language model during
decoding. Marton and Resnik (2008) exploit shal-
low correspondences of hierarchical rules with
source syntactic constituents extracted from par-
allel text, an approach also investigated by Chiang
(2005). Zhang and Gildea (2006) propose bina-
rization for synchronous grammars as a means to
control search complexity arising from more com-
plex, syntactic, hierarchical rules sets.
Hierarchical rule extraction Zhang et al (2008)
describe a linear algorithm, a modified version of
shift-reduce, to extract phrase pairs organized into
a tree from which hierarchical rules can be directly
extracted. Lopez (2007) extracts rules on-the-fly
from the training bitext during decoding, search-
ing efficiently for rule patterns using suffix arrays.
Analysis and Contrastive Experiments Zollman
et al (2008) compare phrase-based, hierarchical
and syntax-augmented decoders for translation of
Arabic, Chinese, and Urdu into English, and they
find that attempts to expedite translation by simple
schemes which discard rules also degrade transla-
tion performance. Lopez (2008) explores whether
lexical reordering or the phrase discontiguity in-
herent in hierarchical rules explains improvements
over phrase-based systems. Hierarchical transla-
tion has also been used to great effect in combina-
tion with other translation architectures (e.g. (Sim
et al, 2007; Rosti et al, 2007)).
1.2 Outline
The paper proceeds as follows. Section 2 de-
scribes memoization and spreading neighborhood
exploration in cube pruning intended to reduce
memory usage and search errors, respectively. A
detailed comparison with a simple phrase-based
system is presented. Section 3 describes pattern-
based rule filtering and various procedures to se-
lect rule sets for use in translation with an aim
to improving translation quality while minimizing
rule set size. Finally, Section 4 concludes.
2 Two Refinements in Cube Pruning
Chiang (2007) introduced cube pruning to apply
language models in pruning during the generation
of k-best translation hypotheses via the application
of hierarchical rules in the CYK algorithm. In the
implementation of Hiero described here, there is
the parser itself, for which we use a variant of the
CYK algorithm closely related to CYK+ (Chap-
pelier and Rajman, 1998); it employs hypothesis
recombination, without pruning, while maintain-
ing back pointers. Before k-best list generation
with cube pruning, we apply a smart memoiza-
tion procedure intended to reduce memory con-
sumption during k-best list expansion. Within the
cube pruning algorithm we use spreading neigh-
borhood exploration to improve robustness in the
face of search errors.
2.1 Smart Memoization
Each cell in the chart built by the CYK algorithm
contains all possible derivations of a span of the
source sentence being translated. After the parsing
stage is completed, it is possible to make a very ef-
ficient sweep through the backpointers of the CYK
grid to count how many times each cell will be ac-
cessed by the k-best generation algorithm. When
k-best list generation is running, the number of
times each cell is visited is logged so that, as each
cell is visited for the last time, the k-best list as-
sociated with each cell is deleted. This continues
until the one k-best list remaining at the top of the
chart spans the entire sentence. Memory reduc-
tions are substantial for longer sentences: for the
longest sentence in the tuning set described later
(105 words in length), smart memoization reduces
memory usage during the cube pruning stage from
2.1GB to 0.7GB. For average length sentences of
approx. 30 words, memory reductions of 30% are
typical.
2.2 Spreading Neighborhood Exploration
In generation of a k-best list of translations for
a source sentence span, every derivation is trans-
formed into a cube containing the possible trans-
lations arising from that derivation, along with
their translation and language model scores (Chi-
ang, 2007). These derivations may contain non-
terminals which must be expanded based on hy-
potheses generated by lower cells, which them-
381
HIERO MJ1 HIERO HIERO SHALLOW
X ? ?V2V1,V1V2? X ? ??,?? X ? ??s,?s?
X ? ?V ,V ? ?, ? ? ({X} ?T)+ X ? ?V ,V ?
V ? ?s,t? V ? ?s,t?
s, t ? T+ s, t ? T+; ?s, ?s ? ({V } ? T)+
Table 1: Hierarchical grammars (not including glue rules). T is the set of terminals.
selves may contain non-terminals. For efficiency
each cube maintains a queue of hypotheses, called
here the frontier queue, ranked by translation and
language model score; it is from these frontier
queues that hypotheses are removed to create the
k-best list for each cell. When a hypothesis is ex-
tracted from a frontier queue, that queue is updated
by searching through the neighborhood of the ex-
tracted item to find novel hypotheses to add; if no
novel hypotheses are found, that queue necessar-
ily shrinks. This shrinkage can lead to search er-
rors. We therefore require that, when a hypothe-
sis is removed, new candidates must be added by
exploring a neighborhood which spreads from the
last extracted hypothesis. Each axis of the cube
is searched (here, to a depth of 20) until a novel
hypothesis is found. In this way, up to three new
candidates are added for each entry extracted from
a frontier queue.
Chiang (2007) describes an initialization pro-
cedure in which these frontier queues are seeded
with a single candidate per axis; we initialize each
frontier queue to a depth of bNnt+1, where Nnt is
the number of non-terminals in the derivation and
b is a search parameter set throughout to 10. By
starting with deep frontier queues and by forcing
them to grow during search we attempt to avoid
search errors by ensuring that the universe of items
within the frontier queues does not decrease as the
k-best lists are filled.
2.3 A Study of Hiero Search Errors in
Phrase-Based Translation
Experiments reported in this paper are based
on the NIST MT08 Arabic-to-English transla-
tion task. Alignments are generated over all al-
lowed parallel data, (?150M words per language).
Features extracted from the alignments and used
in translation are in common use: target lan-
guage model, source-to-target and target-to-source
phrase translation models, word and rule penalties,
number of usages of the glue rule, source-to-target
and target-to-source lexical models, and three rule
Figure 1: Spreading neighborhood exploration
within a cube, just before and after extraction
of the item C. Grey squares represent the fron-
tier queue; black squares are candidates already
extracted. Chiang (2007) would only consider
adding items X to the frontier queue, so the queue
would shrink. Spreading neighborhood explo-
ration adds candidates S to the frontier queue.
count features inspired by Bender et al (2007).
MET (Och, 2003) iterative parameter estimation
under IBM BLEU is performed on the develop-
ment set. The English language used model is a
4-gram estimated over the parallel text and a 965
million word subset of monolingual data from the
English Gigaword Third Edition. In addition to the
MT08 set itself, we use a development set mt02-
05-tune formed from the odd numbered sentences
of the NIST MT02 through MT05 evaluation sets;
the even numbered sentences form the validation
set mt02-05-test. The mt02-05-tune set has 2,075
sentences.
We first compare the cube pruning decoder to
the TTM (Kumar et al, 2006), a phrase-based
SMT system implemented with Weighted Finite-
State Tansducers (Allauzen et al, 2007). The sys-
tem implements either a monotone phrase order
translation, or an MJ1 (maximum phrase jump of
1) reordering model (Kumar and Byrne, 2005).
Relative to the complex movement and translation
allowed by Hiero and other models, MJ1 is clearly
inferior (Dreyer et al, 2007); MJ1 was developed
with efficiency in mind so as to run with a mini-
mum of search errors in translation and to be eas-
ily and exactly realized via WFSTs. Even for the
382
large models used in an evaluation task, the TTM
system is reported to run largely without pruning
(Blackwood et al, 2008).
The Hiero decoder can easily be made to
implement MJ1 reordering by allowing only a
restricted set of reordering rules in addition to
the usual glue rule, as shown in left-hand column
of Table 1, where T is the set of terminals.
Constraining Hiero in this way makes it possible
to compare its performance to the exact WFST
TTM implementation and to identify any search
errors made by Hiero.
Table 2 shows the lowercased IBM BLEU
scores obtained by the systems for mt02-05-tune
with monotone and reordered search, and with
MET-optimised parameters for MJ1 reordering.
For Hiero, an N-best list depth of 10,000 is used
throughout. In the monotone case, all phrase-
based systems perform similarly although Hiero
does make search errors. For simple MJ1 re-
ordering, the basic Hiero search procedure makes
many search errors and these lead to degradations
in BLEU. Spreading neighborhood expansion re-
duces the search errors and improves BLEU score
significantly but search errors remain a problem.
Search errors are even more apparent after MET.
This is not surprising, given that mt02-05-tune is
the set over which MET is run: MET drives up the
likelihood of good hypotheses at the expense of
poor hypotheses, but search errors often increase
due to the expanded dynamic range of the hypoth-
esis scores.
Our aim in these experiments was to demon-
strate that spreading neighborhood exploration can
aid in avoiding search errors. We emphasize that
we are not proposing that Hiero should be used to
implement reordering models such as MJ1 which
were created for completely different search pro-
cedures (e.g. WFST composition). However these
experiments do suggest that search errors may be
an issue, particularly as the search space grows
to include the complex long-range movement al-
lowed by the hierarchical rules. We next study
various filtering procedures to reduce hierarchi-
cal rule sets to find a balance between translation
speed, memory usage, and performance.
3 Rule Filtering by Pattern
Hierarchical rules X ? ??,?? are composed of
sequences of terminals and non-terminals, which
Monotone MJ1 MJ1+MET
BLEU SE BLEU SE BLEU SE
a 44.7 - 47.2 - 49.1 -
b 44.5 342 46.7 555 48.4 822
c 44.7 77 47.1 191 48.9 360
Table 2: Phrase-based TTM and Hiero perfor-
mance on mt02-05-tune for TTM (a), Hiero (b),
Hiero with spreading neighborhood exploration
(c). SE is the number of Hiero hypotheses with
search errors.
we call elements. In the source, a maximum of
two non-adjacent non-terminals is allowed (Chi-
ang, 2007). Leaving aside rules without non-
terminals (i.e. phrase pairs as used in phrase-
based translation), rules can be classed by their
number of non-terminals, Nnt, and their number
of elements, Ne. There are 5 possible classes:
Nnt.Ne= 1.2, 1.3, 2.3, 2.4, 2.5.
During rule extraction we search each class sep-
arately to control memory usage. Furthermore, we
extract from alignments only those rules which are
relevant to our given test set; for computation of
backward translation probabilities we log general
counts of target-side rules but discard unneeded
rules. Even with this restriction, our initial ruleset
for mt02-05-tune exceeds 175M rules, of which
only 0.62M are simple phrase pairs.
The question is whether all these rules are
needed for translation. If the rule set can be re-
duced without reducing translation quality, both
memory efficiency and translation speed can be
increased. Previously published approaches to re-
ducing the rule set include: enforcing a mini-
mum span of two words per non-terminal (Lopez,
2008), which would reduce our set to 115M rules;
or a minimum count (mincount) threshold (Zoll-
mann et al, 2008), which would reduce our set
to 78M (mincount=2) or 57M (mincount=3) rules.
Shen et al (2008) describe the result of filter-
ing rules by insisting that target-side rules are
well-formed dependency trees. This reduces their
rule set from 140M to 26M rules. This filtering
leads to a degradation in translation performance
(see Table 2 of Shen et al (2008)), which they
counter by adding a dependency LM in translation.
As another reference point, Chiang (2007) reports
Chinese-to-English translation experiments based
on 5.5M rules.
Zollmann et al (2008) report that filtering rules
383
en masse leads to degradation in translation per-
formance. Rather than apply a coarse filtering,
such as a mincount for all rules, we follow a more
syntactic approach and further classify our rules
according to their pattern and apply different fil-
ters to each pattern depending on its value in trans-
lation. The premise is that some patterns are more
important than others.
3.1 Rule Patterns
Class Rule Pattern
Nnt.Ne ?source , target? Types
?wX1 , wX1? 1185028
1.2 ?wX1 , wX1w? 153130
?wX1 , X1w? 97889
1.3 ?wX1w , wX1w? 32903522
?wX1w , wX1? 989540
2.3 ?X1wX2 , X1wX2? 1554656
?X2wX1 , X1wX2? 39163
?wX1wX2 , wX1wX2? 26901823
?X1wX2w , X1wX2w? 26053969
2.4 ?wX1wX2 , wX1wX2w? 2534510
?wX2wX1 , wX1wX2? 349176
?X2wX1w , X1wX2w? 259459
?wX1wX2w , wX1wX2w? 61704299
?wX1wX2w , wX1X2w? 3149516
2.5 ?wX1wX2w , X1wX2w? 2330797
?wX2wX1w , wX1wX2w? 275810
?wX2wX1w , wX1X2w? 205801
Table 3: Hierarchical rule patterns classed by
number of non-terminals, Nnt, number of ele-
ments Ne, source and target patterns, and types in
the rule set extracted for mt02-05-tune.
Given a rule set, we define source patterns and
target patterns by replacing every sequence of
non-terminals by a single symbol ?w? (indicating
word, i.e. terminal string, w ? T+). Each hierar-
chical rule has a unique source and target pattern
which together define the rule pattern.
By ignoring the identity and the number of ad-
jacent terminals, the rule pattern represents a nat-
ural generalization of any rule, capturing its struc-
ture and the type of reordering it encodes. In to-
tal, there are 66 possible rule patterns. Table 3
presents a few examples extracted for mt02-05-
tune, showing that some patterns are much more
diverse than others. For example, patterns with
two non-terminals (Nnt=2) are richer than pat-
terns with Nnt=1, as they cover many more dis-
tinct rules. Additionally, patterns with two non-
terminals which also have a monotonic relation-
ship between source and target non-terminals are
much more diverse than their reordered counter-
parts.
Some examples of extracted rules and their cor-
responding pattern follow, where Arabic is shown
in Buckwalter encoding.
Pattern ?wX1 , wX1w? :
?w+ qAl X1 , the X1said?
Pattern ?wX1w , wX1? :
?fy X1kAnwn Al>wl , on december X1?
Pattern ?wX1wX2 , wX1wX2w? :
?Hl X1lAzmp X2 , a X1solution to the X2crisis?
3.2 Building an Initial Rule Set
We describe a greedy approach to building a rule
set in which rules belonging to a pattern are added
to the rule set guided by the improvements they
yield on mt02-05-tune relative to the monotone
Hiero system described in the previous section.
We find that certain patterns seem not to con-
tribute to any improvement. This is particularly
significant as these patterns often encompass large
numbers of rules, as with patterns with match-
ing source and target patterns. For instance, we
found no improvement when adding the pattern
?X1w,X1w?, of which there were 1.2M instances
(Table 3). Since concatenation is already possible
under the general glue rule, rules with this pattern
are redundant. By contrast, the much less frequent
reordered counterpart, i.e. the ?wX1,X1w? pat-
tern (0.01M instances), provides substantial gains.
The situation is analogous for rules with two non-
terminals (Nnt=2).
Based on exploratory analyses (not reported
here, for space) an initial rule set was built by
excluding patterns reported in Table 4. In to-
tal, 171.5M rules are excluded, for a remaining
set of 4.2M rules, 3.5M of which are hierarchi-
cal. We acknowledge that adding rules in this way,
by greedy search, is less than ideal and inevitably
raises questions with respect to generality and re-
peatability. However in our experience this is a
robust approach, mainly because the initial trans-
lation system runs very fast; it is possible to run
many exploratory experiments in a short time.
384
Excluded Rules Types
a ?X1w,X1w? , ?wX1,wX1? 2332604
b ?X1wX2,?? 2121594
?X1wX2w,X1wX2w? ,c ?wX1wX2,wX1wX2?
52955792
d ?wX1wX2w,?? 69437146
e Nnt.Ne= 1.3 w mincount=5 32394578
f Nnt.Ne= 2.3 w mincount=5 166969
g Nnt.Ne= 2.4 w mincount=10 11465410
h Nnt.Ne= 2.5 w mincount=5 688804
Table 4: Rules excluded from the initial rule set.
3.3 Shallow versus Fully Hierarchical
Translation
In measuring the effectiveness of rules in transla-
tion, we also investigate whether a ?fully hierarchi-
cal? search is needed or whether a shallow search
is also effective. In constrast to full Hiero, in the
shallow search, only phrases are allowed to be sub-
stituted into non-terminals. The rules used in each
case can be expressed as shown in the 2nd and 3rd
columns of Table 1. Shallow search can be con-
sidered (loosely) to be a form of rule filtering.
As can be seen in Table 5 there is no impact on
BLEU, while translation speed increases by a fac-
tor of 7. Of course, these results are specific to this
Arabic-to-English translation task, and need not
be expected to carry over to other language pairs,
such as Chinese-to-English translation. However,
the impact of this search simplification is easy to
measure, and the gains can be significant enough,
that it may be worth investigation even for lan-
guages with complex long distance movement.
mt02-05- -tune -test
System Time BLEU BLEU
HIERO 14.0 52.1 51.5
HIERO - shallow 2.0 52.1 51.4
Table 5: Translation performance and time (in sec-
onds per word) for full vs. shallow Hiero.
3.4 Individual Rule Filters
We now filter rules individually (not by class) ac-
cording to their number of translations. For each
fixed ? /? T+ (i.e. with at least 1 non-terminal),
we define the following filters over rules X ?
??,??:
? Number of translations (NT). We keep the
NT most frequent ?, i.e. each ? is allowed to
have at most NT rules.
? Number of reordered translations (NRT).
We keep the NRT most frequent ? with
monotonic non-terminals and the NRT most
frequent ? with reordered non-terminals.
? Count percentage (CP). We keep the most
frequent ? until their aggregated number of
counts reaches a certain percentage CP of the
total counts of X ? ??,??. Some ??s are al-
lowed to have more ??s than others, depend-
ing on their count distribution.
Results applying these filters with various
thresholds are given in Table 6, including num-
ber of rules and decoding time. As shown, all
filters achieve at least a 50% speed-up in decod-
ing time by discarding 15% to 25% of the base-
line rules. Remarkably, performance is unaffected
when applying the simple NT and NRT filters
with a threshold of 20 translations. Finally, the
CM filter behaves slightly worse for thresholds of
90% for the same decoding time. For this reason,
we select NRT=20 as our general filter.
mt02-05- -tune -test
Filter Time Rules BLEU BLEU
baseline 2.0 4.20 52.1 51.4
NT=10 0.8 3.25 52.0 51.3
NT=15 0.8 3.43 52.0 51.3
NT=20 0.8 3.56 52.1 51.4
NRT=10 0.9 3.29 52.0 51.3
NRT=15 1.0 3.48 52.0 51.4
NRT=20 1.0 3.59 52.1 51.4
CP=50 0.7 2.56 51.4 50.9
CP=90 1.0 3.60 52.0 51.3
Table 6: Impact of general rule filters on transla-
tion (IBM BLEU), time (in seconds per word) and
number of rules (in millions).
3.5 Pattern-based Rule Filters
In this section we first reconsider whether reintro-
ducing the monotonic rules (originally excluded as
described in rows ?b?, ?c?, ?d? in Table 4) affects
performance. Results are given in the upper rows
of Table 7. For all classes, we find that reintroduc-
ing these rules increases the total number of rules
385
mt02-05- -tune -test
Nnt.Ne Filter Time Rules BLEU BLEU
baseline NRT=20 1.0 3.59 52.1 51.4
2.3 +monotone 1.1 4.08 51.5 51.1
2.4 +monotone 2.0 11.52 51.6 51.0
2.5 +monotone 1.8 6.66 51.7 51.2
1.3 mincount=3 1.0 5.61 52.1 51.3
2.3 mincount=1 1.2 3.70 52.1 51.4
2.4 mincount=5 1.8 4.62 52.0 51.3
2.4 mincount=15 1.0 3.37 52.0 51.4
2.5 mincount=1 1.1 4.27 52.2 51.5
1.2 mincount=5 1.0 3.51 51.8 51.3
1.2 mincount=10 1.0 3.50 51.7 51.2
Table 7: Effect of pattern-based rule filters. Time in seconds per word. Rules in millions.
substantially, despite the NRT=20 filter, but leads
to degradation in translation performance.
We next reconsider the mincount threshold val-
ues for Nnt.Ne classes 1.3, 2.3, 2.4 and 2.5 origi-
nally described in Table 4 (rows ?e? to ?h?). Results
under various mincount cutoffs for each class are
given in Table 7 (middle five rows). For classes
2.3 and 2.5, the mincount cutoff can be reduced
to 1 (i.e. all rules are kept) with slight translation
improvements. In contrast, reducing the cutoff for
classes 1.3 and 2.4 to 3 and 5, respectively, adds
many more rules with no increase in performance.
We also find that increasing the cutoff to 15 for
class 2.4 yields the same results with a smaller rule
set. Finally, we consider further filtering applied to
class 1.2 with mincount 5 and 10 (final two rows
in Table 7). The number of rules is largely un-
changed, but translation performance drops con-
sistently as more rules are removed.
Based on these experiments, we conclude that it
is better to apply separate mincount thresholds to
the classes to obtain optimal performance with a
minimum size rule set.
3.6 Large Language Models and Evaluation
Finally, in this section we report results of our
shallow hierarchical system with the 2.5 min-
count=1 configuration from Table 7, after includ-
ing the following N-best list rescoring steps.
? Large-LM rescoring. We build sentence-
specific zero-cutoff stupid-backoff (Brants et
al., 2007) 5-gram language models, estimated
using ?4.7B words of English newswire text,
and apply them to rescore each 10000-best
list.
? Minimum Bayes Risk (MBR). We then rescore
the first 1000-best hypotheses with MBR,
taking the negative sentence level BLEU
score as the loss function to minimise (Ku-
mar and Byrne, 2004).
Table 8 shows results for mt02-05-tune, mt02-
05-test, the NIST subsets from the MT06 evalu-
ation (mt06-nist-nw for newswire data and mt06-
nist-ng for newsgroup) and mt08, as measured by
lowercased IBM BLEU and TER (Snover et al,
2006). Mixed case NIST BLEU for this system on
mt08 is 42.5. This is directly comparable to offi-
cial MT08 evaluation results1.
4 Conclusions
This paper focuses on efficient large-scale hierar-
chical translation while maintaining good trans-
lation quality. Smart memoization and spreading
neighborhood exploration during cube pruning are
described and shown to reduce memory consump-
tion and Hiero search errors using a simple phrase-
based system as a contrast.
We then define a general classification of hi-
erarchical rules, based on their number of non-
terminals, elements and their patterns, for refined
extraction and filtering.
For a large-scale Arabic-to-English task, we
show that shallow hierarchical decoding is as good
1Full MT08 results are available at
http://www.nist.gov/speech/tests/mt/2008/. It is worth
noting that many of the top entries make use of system
combination; the results reported here are for single system
translation.
386
mt02-05-tune mt02-05-test mt06-nist-nw mt06-nist-ng mt08
HIERO+MET 52.2 / 41.6 51.5 / 42.2 48.4 / 43.6 35.3 / 53.2 42.5 / 48.6
+rescoring 53.2 / 40.8 52.6 / 41.4 49.4 / 42.9 36.6 / 53.5 43.4 / 48.1
Table 8: Arabic-to-English translation results (lower-cased IBM BLEU / TER) with large language mod-
els and MBR decoding.
as fully hierarchical search and that decoding time
is dramatically decreased. In addition, we describe
individual rule filters based on the distribution of
translations with further time reductions at no cost
in translation scores. This is in direct contrast
to recent reported results in which other filtering
strategies lead to degraded performance (Shen et
al., 2008; Zollmann et al, 2008).
We find that certain patterns are of much greater
value in translation than others and that separate
minimum count filters should be applied accord-
ingly. Some patterns were found to be redundant
or harmful, in particular those with two monotonic
non-terminals. Moreover, we show that the value
of a pattern is not directly related to the number of
rules it encompasses, which can lead to discarding
large numbers of rules as well as to dramatic speed
improvements.
Although reported experiments are only for
Arabic-to-English translation, we believe the ap-
proach will prove to be general. Pattern relevance
will vary for other language pairs, but we expect
filtering strategies to be equally worth pursuing.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
Agency, Contract No. HR0011- 06-C-0022. G.
Iglesias supported by Spanish Government re-
search grant BES-2007-15956 (project TEC2006-
13694-C03-03).
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood, Adria` de Gispert, Jamie Brunning,
and William Byrne. 2008. Large-scale statistical
machine translation with weighted finite state trans-
ducers. In Proceedings of FSMNLP, pages 27?35.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HLT,
pages 200?208.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
Jean-Ce?dric Chappelier, Martin Rajman, Ramo?n
Aragu?e?s, and Antoine Rozenknop. 1999. Lattice
parsing for speech recognition. In Proceedings of
TALN, pages 95?104.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing reordering constraints for SMT
using efficient BLEU oracle computation. In Pro-
ceedings of SSST, NAACL-HLT 2007 / AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-HLT, pages 1012?1020.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144?151.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of HLT-EMNLP, pages
161?168.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer translation
template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
387
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine translation
with equivalent language model state maintenance.
In Proceedings of the ACL-HLT Second Workshop
on Syntax and Structure in Statistical Translation,
pages 10?18.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CONLL, pages 976?985.
Adam Lopez. 2008. Tera-scale translation models
via pattern matching. In Proceedings of COLING,
pages 505?512.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of ACL-HLT, pages 1003?
1011.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, pages 160?167.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of HLT-
NAACL, pages 228?235.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation
system combination. In Proceedings of ICASSP,
volume 4, pages 105?108.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human an-
notation. In Proceedings of AMTA, pages 223?231.
Ashish Venugopal, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Pro-
ceedings of HLT-NAACL, pages 500?507.
Hao Zhang and Daniel Gildea. 2006. Synchronous
binarization for machine translation. In Proceedings
of HLT-NAACL, pages 256?263.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings of
COLING, pages 1081?1088.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of NAACL Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann, Ashish Venugopal, Franz Och,
and Jay Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and syntax-augmented
statistical MT. In Proceedings of COLING, pages
1145?1152.
388
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 161?168, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Local Phrase Reordering Models for Statistical Machine Translation
Shankar Kumar, William Byrne?
Center for Language and Speech Processing, Johns Hopkins University,
3400 North Charles Street, Baltimore, MD 21218, U.S.A.
Machine Intelligence Lab, Cambridge University Engineering Department,
Trumpington Street, Cambridge CB2 1PZ, U.K.
skumar@jhu.edu , wjb31@cam.ac.uk
Abstract
We describe stochastic models of local
phrase movement that can be incorpo-
rated into a Statistical Machine Transla-
tion (SMT) system. These models pro-
vide properly formulated, non-deficient,
probability distributions over reordered
phrase sequences. They are imple-
mented by Weighted Finite State Trans-
ducers. We describe EM-style parameter
re-estimation procedures based on phrase
alignment under the complete translation
model incorporating reordering. Our ex-
periments show that the reordering model
yields substantial improvements in trans-
lation performance on Arabic-to-English
and Chinese-to-English MT tasks. We
also show that the procedure scales as the
bitext size is increased.
1 Introduction
Word and Phrase Reordering is a crucial component
of Statistical Machine Translation (SMT) systems.
However allowing reordering in translation is com-
putationally expensive and in some cases even prov-
ably NP-complete (Knight, 1999). Therefore any
translation scheme that incorporates reordering must
necessarily balance model complexity against the
ability to realize the model without approximation.
In this paper our goal is to formulate models of lo-
cal phrase reordering in such a way that they can be
embedded inside a generative phrase-based model
? This work was supported by an ONR MURI Grant
N00014-01-1-0685.
of translation (Kumar et al, 2005). Although this
model of reordering is somewhat limited and can-
not capture all possible phrase movement, it forms
a proper parameterized probability distribution over
reorderings of phrase sequences. We show that with
this model it is possible to perform Maximum A
Posteriori (MAP) decoding (with pruning) and Ex-
pectation Maximization (EM) style re-estimation of
model parameters over large bitext collections.
We now discuss prior work on word and phrase
reordering in translation. We focus on SMT systems
that do not require phrases to form syntactic con-
stituents.
The IBM translation models (Brown et al, 1993)
describe word reordering via a distortion model de-
fined over word positions within sentence pairs. The
Alignment Template Model (Och et al, 1999) uses
phrases rather than words as the basis for transla-
tion, and defines movement at the level of phrases.
Phrase reordering is modeled as a first order Markov
process with a single parameter that controls the de-
gree of movement.
Our current work is inspired by the block
(phrase-pair) orientation model introduced by Till-
mann (2004) in which reordering allows neighbor-
ing blocks to swap. This is described as a sequence
of orientations (left, right, neutral) relative to the
monotone block order. Model parameters are block-
specific and estimated over word aligned trained bi-
text using simple heuristics.
Other researchers (Vogel, 2003; Zens and Ney,
2003; Zens et al, 2004) have reported performance
gains in translation by allowing deviations from
monotone word and phrase order. In these cases,
161
0c 4c 5c
0d 1d
1v 2v 3v 4v 5v 6v 7v
1f 2f 3f 4f 5f 6f 7f 8f 9f
2d 3d 4d 5d
2c 3c1c
x 1 x 2 x 3 x 4 x 5
1e 5e 7e2e 3e 4e 6e 9e8e
u 1 u 2 u 3 u 4 u 5
y 1 y 5y 4y 3y 2
doivent de_25_%exportationsgrains fl?chir
exportations grains de_25_%doivent fl?chir
1 
les exportations de 
les  exportations  de  grains  doivent  fl?chir de  25  %
grains doivent fl?chir de_25_%
1.exportations doiventgrains fl?chir de_25_%
grain exports are_projected_to by_25_%
grain  exports  are  projected  to  fall  by  25  %Sentence
fall
Source Language
Target Language Sentence
Figure 1: TTM generative translation process; here,
I = 9,K = 5, R = 7, J = 9.
reordering is not governed by an explicit probabilis-
tic model over reordered phrases; a language model
is employed to select the translation hypothesis. We
also note the prior work of Wu (1996), closely re-
lated to Tillmann?s model.
2 The WFST Reordering Model
The Translation Template Model (TTM) is a genera-
tive model of phrase-based translation (Brown et al,
1993). Bitext is described via a stochastic process
that generates source (English) sentences and trans-
forms them into target (French) sentences (Fig 1 and
Eqn 1).
P (fJ1 , v
R
1 , d
K
0 , c
K
0 , y
K
1 , x
K
1 , u
K
1 ,K, e
I
1) =
P (eI1)?
Source Language Model G
P (uK1 ,K|e
I
1)?
Source Phrase Segmentation W
P (xK1 |u
K
1 ,K, e
I
1)?
Phrase Translation and Reordering R
P (vR1 , d
K
0 , c
K
0 , y
K
1 |x
K
1 , u
K
1 ,K, e
I
1)?
Target Phrase Insertion ?
P (fJ1 |v
R
1 , d
K
0 , c
K
0 , y
K
1 , x
K
1 , u
K
1 ,K, e
I
1)
Target Phrase Segmentation ?
(1)
The TTM relies on a Phrase-Pair Inventory (PPI)
consisting of target language phrases and their
source language translations. Translation is mod-
eled via component distributions realized as WFSTs
(Fig 1 and Eqn 1) : Source Language Model (G),
Source Phrase Segmentation (W ), Phrase Transla-
tion and Reordering (R), Target Phrase Insertion
(?), and Target Phrase Segmentation (?) (Kumar et
al., 2005).
TTM Reordering Previously, the TTM was for-
mulated with reordering prior to translation; here,
we perform reordering of phrase sequences follow-
ing translation. Reordering prior to translation was
found to be memory intensive and unwieldy (Kumar
et al, 2005). In contrast, we will show that the cur-
rent model can be used for both phrase alignment
and translation.
2.1 The Phrase Reordering Model
We now describe two WFSTs that allow local re-
ordering within phrase sequences. The simplest al-
lows swapping of adjacent phrases. The second al-
lows phrase movement within a three phrase win-
dow. Our formulation ensures that the overall model
provides a proper parameterized probability distri-
bution over reordered phrase sequences; we empha-
size that the resulting distribution is not degenerate.
Phrase reordering (Fig 2) takes as its input a
French phrase sequence in English phrase order
x1, x2, ..., xK . This is then reordered into French
phrase order y1, y2, ..., yK . Note that words within
phrases are not affected.
We make the following conditional independence
assumption:
P (yK1 |x
K
1 , u
K
1 ,K, e
I
1) = P (y
K
1 |x
K
1 , u
K
1 ). (2)
Given an input phrase sequence xK1 we now as-
sociate a unique jump sequence bK1 with each per-
missible output phrase sequence yK1 . The jump bk
measures the displacement of the kth phrase xk, i.e.
xk ? yk+bk , k ? {1, 2, ...,K}. (3)
The jump sequence bK1 is constructed such that yK1
is a permutation of xK1 . This is enforced by con-
structing all models so that
?K
k=1 bk = 0.
We now redefine the model in terms of the jump
sequence
P (yK1 |x
K
1 , u
K
1 ) (4)
=
{
P (bK1 |x
K
1 , u
K
1 ) yk+bk = xk ?k
0 otherwise,
162
x 2 x 3 x 4 x 5x 1
y 2 y 3 y 4 y 5y 1
3b = 01b = +12b = ?1 4b = 0 5b = 0
doivent de_25_%exportations fl?chir
exportations grains de_25_%doivent fl?chir
grains
Figure 2: Phrase reordering and jump sequence.
-
where yK1 is determined by xK1 and bK1 .
Each jump bk depends on the phrase-pair (xk, uk)
and preceding jumps bk?11
P (bK1 |x
K
1 , u
K
1 ) =
K?
k=1
P (bk|xk, uk, ?k?1), (5)
where ?k?1 is an equivalence classification (state)
of the jump sequence bk?11 .
The jump sequence bK1 can be described by a
deterministic finite state machine. ?(bk?11 ) is the
state arrived at by bk?11 ; we will use ?k?1 to denote
?(bk?11 ).
We will investigate phrase reordering by restrict-
ing the maximum allowable jump to 1 phrase and
to 2 phrases; we will refer to these reordering
models as MJ-1 and MJ-2. In the first case,
bk ? {0,+1,?1} while in the second case, bk ?
{0,+1,?1,+2,?2}.
2.2 Reordering WFST for MJ-1
We first present the Finite State Machine of the
phrase reordering process (Fig 3) which has two
equivalence classes (FSM states) for any given his-
tory bk?11 ; ?(b
k?1
1 ) ? {1, 2}. A jump of +1 has to
be followed by a jump of ?1, and 1 is the start and
end state; this ensures
?K
k=1 bk = 0.
1 b=+1
b=?1
b=0 
2
Figure 3: Phrase reordering process for MJ-1.
Under this restriction, the probability of the jump
bk (Eqn 5) can be simplified as
P (bk|xk, uk, ?(b
k?1
1 )) = (6)?
??
??
?1(xk, uk) bk = +1, ?k?1 = 1
1 ? ?1(xk, uk) bk = 0, ?k?1 = 1
1 bk = ?1, ?k?1 = 2.
There is a single parameter jump probability
?1(x, u) = P (b = +1|x, u) associated with each
phrase-pair (x, u) in the phrase-pair inventory. This
is the probability that the phrase-pair (x, u) appears
out of order in the transformed phrase sequence.
We now describe the MJ-1 WFST. In the presen-
tation, we use upper-case letters to denote the En-
glish phrases (uk) and lower-case letters to denote
the French phrases (xk and yk).
The PPI for this example is given in Table 1.
English French Parameters
u x P (x|u) ?1(x, u)
A a 0.5 0.2
A d 0.5 0.2
B b 1.0 0.4
C c 1.0 0.3
D d 1.0 0.8
Table 1: Example phrase-pair inventory with trans-
lation and reordering probabilities.
The input to the WFST (Fig 4) is a lattice of
French phrase sequences derived from the French
sentence to be translated. The outputs are the cor-
responding English phrase sequences. Note that the
reordering is performed on the English side.
The WFST is constructed by adding a self-loop
for each French phrase in the input lattice, and
a 2-arc path for every pair of adjacent French
phrases in the lattice. The WFST incorporates the
translation model P (x|u) and the reordering model
P (b|x, u). The score on a self-loop with labels
(u, x) is P (x|u) ? (1 ? ?1(x, u)); on a 2-arc path
with labels (u1, x1) and (u2, x2), the score on the
1st arc is P (x2|u1) ? ?1(x2, u1) and on the 2nd arc
is P (x1|u2).
In this example, the input to this transducer is a
single French phrase sequence V : a, b, c. We per-
form the WFST composition R?V , project the result
on the input labels, and remove the epsilons to form
the acceptor (R?V )1 which contains the six English
phrase sequences (Fig 4).
Translation Given a French sentence, a lattice of
translations is obtained using the weighted finite
state composition: T = G ? W ? R ? ? ? ? ? T .
The most-likely translation is obtained as the path
with the highest probability in T .
Alignment Given a sentence-pair (E,F ), a lattice
of phrase alignments is obtained by the finite state
composition: B = S ? W ? R ? ? ? ? ? T , where
163
A : b / 0.1
A B D 0.4 x 0.6 x 0.2 = 0.480
B A D 0.4 x 0.5 x 0.2 = 0.040
A D B 0.4 x 0.8 x 0.4 = 0.128
A A B 0.4 x 0.1 x 0.4 = 0.016
A B A 0.4 x 0.6 x 0.4 = 0.096
B A A 0.4 x 0.5 x 0.4 = 0.080
VR 1( )
A : b / 0.5
R
V a b d
B : b / 0.6D : d / 0.2
A : d / 0.4A : a / 0.4
B : a / 0.4
B : d / 0.4
D : b / 0.8
Figure 4: WFST for the MJ-1 model.
S is an acceptor for the English sentence E, and
T is an acceptor for the French sentence F . The
Viterbi alignment is found as the path with the high-
est probability in B. The WFST composition gives
the word-to-word alignments between the sentences.
However, to obtain the phrase alignments, we need
to construct additional FSTs not described here.
2.3 Reordering WFST for MJ-2
MJ-2 reordering restricts the maximum allowable
jump to 2 phrases and also insists that the reorder-
ing take place within a window of 3 phrases. This
latter condition implies that for an input sequence
{a, b, c, d}, we disallow the three output sequences:
{b, d, a, c; c, a, d, b; c, d, a, b; }. In the MJ-2 finite
state machine, a given history bk?11 can lead to one
of the six states in Fig 5.
b=0
1
23
45
6
b=?1
b=+1b=?1
b=+2
b=0
b=?2
b=?1
b=+1 b=?2
Figure 5: Phrase reordering process for MJ-2.
The jump probability of Eqn 5 becomes
P (bk|xk, uk, ?k?1) =
?
????
????
?1(xk, uk) bk = 1, ?k?1 = 1
?2(xk, uk) bk = 2, ?k?1 = 1{
1 ? ?1(xk, uk)
??2(xk, uk)
bk = 0, ?k?1 = 1
(7)
{
?1(xk, uk) bk = 1, ?k?1 = 2
1 ? ?1(xk, uk) bk = ?1, ?k?1 = 2
(8)
{
0.5 bk = 0, ?k?1 = 3
0.5 bk = ?1, ?k?1 = 3.
(9)
{
1 bk = ?2, ?k?1 = 4 (10)
{
1 bk = ?2, ?k?1 = 5 (11)
{
1 bk = ?1, ?k?1 = 6 (12)
We note that the distributions (Eqns 7 and 8) are
based on two parameters ?1(x, u) and ?2(x, u) for
each phrase-pair (x, u).
Suppose the input is a phrase sequence a, b, c, the
MJ-2 model (Fig 5) allows 6 possible reorderings:
a, b, c; a, c, b; b, a, c; b, c, a; c, a, b; c, b, a. The distri-
bution Eqn 9 ensures that the sequences b, c, a and
c, b, a are assigned equal probability. The distribu-
tions in Eqns 10-12 ensure that the maximum jump
is 2 phrases and the reordering happens within a
window of 3 phrases. By insisting that the pro-
cess start and end at state 1 (Fig 5), we ensure that
the model is not deficient. A WFST implementing
the MJ-2 model can be easily constructed for both
phrase alignment and translation, following the con-
struction described for the MJ-1 model.
3 Estimation of the Reordering Models
The Translation Template Model relies on an in-
ventory of target language phrases and their source
language translations. Our goal is to estimate the
reordering model parameters P (b|x, u) for each
phrase-pair (x, u) in this inventory. However, when
translating a given test set, only a subset of the
phrase-pairs is needed. Although there may be an
advantage in estimating the model parameters under
an inventory that covers all the training bitext, we fix
the phrase-pair inventory to cover only the phrases
on the test set. Estimation of the reordering model
parameters over the training bitext is then performed
under this test-set specific inventory.
164
We employ the EM algorithm to obtain Maximum
Likelihood (ML) estimates of the reordering model
parameters. Applying EM to the MJ-1 reordering
model gives the following ML parameter estimates
for each phrase-pair (u, x).
??1(x, u) =
Cx,u(0,+1)
Cx,u(0,+1) + Cx,u(0, 0)
. (13)
Cx,u(?, b) is defined for ? = 1, 2 and b =
?1, 0,+1. Any permissible phrase alignment of a
sentence pair corresponds to a bK1 sequence, which
in turn specifies a ?K1 sequence. Cx,u(?, b) is the
expected number of times the phrase-pair x, u is
aligned with a jump of b phrases when the jump his-
tory is ?. We do not use full EM but a Viterbi train-
ing procedure that obtains the counts for the best
(Viterbi) alignments. If a phrase-pair (x, u) is never
seen in the Viterbi alignments, we back-off to a flat
parameter ?1(x, u) = 0.05.
The ML parameter estimates for the MJ-2 model
are given in Table 2, with Cx,u(?, b) defined sim-
ilarly. In our training scenario, we use WFST op-
erations to obtain Viterbi phrase alignments of the
training bitext where the initial reordering model
parameters (?0(x, u)) are set to a uniform value of
0.05. The counts Cx,u(s, b) are then obtained over
the phrase alignments. Finally the ML estimates of
the parameters are computed using Eqn 13 (MJ-1) or
Eqn 14 (MJ-2). We will refer to the Viterbi trained
models as MJ-1 VT and MJ-2 VT. Table 3 shows the
MJ-1 VT parameters for some example phrase-pairs
in the Arabic-English (A-E) task.
u x ?1(x, u)
which is the closest Aqrb 1.0
international trade tjArp EAlmyp 0.8
the foreign ministry wzArp xArjyp 0.6
arab league jAmEp dwl Erbyp 0.4
Table 3: MJ-1 parameters for A-E phrase-pairs.
To validate alignment under a PPI, we mea-
sure performance of the TTM word alignments
on French-English (500 sent-pairs) and Chinese-
English (124 sent-pairs) (Table 4). As desired, the
Alignment Recall (AR) and Alignment Error Rate
(AER) improve modestly while Alignment Preci-
sion (AP) remains constant. This suggests that the
models allow more words to be aligned and thus im-
prove the recall; MJ-2 gives a further improvement
in AR and AER relative to MJ-1. Alignment preci-
Reordering Metrics (%)
Frn-Eng Chn-Eng
AP AR AER AP AR AER
None 94.2 84.8 10.0 85.1 47.1 39.3
MJ-1 VT 94.1 86.8 9.1 85.3 49.4 37.5
MJ-2 VT 93.9 87.4 8.9 85.3 50.9 36.3
Table 4: Alignment Performance with Reordering.
sion depends on the quality of the word alignments
within the phrase-pairs and does not change much
by allowing phrase reordering. This experiment val-
idates the estimation procedure based on the phrase
alignments; however, we do not advocate the use of
TTM as an alternate word alignment technique.
4 Translation Experiments
We perform our translation experiments on the large
data track of the NIST Arabic-to-English (A-E) and
Chinese-to-English (C-E) MT tasks; we report re-
sults on the NIST 2002, 2003, and 2004 evaluation
test sets 1.
4.1 Exploratory Experiments
In these experiments the training data is restricted to
FBIS bitext in C-E and the news bitexts in A-E. The
bitext consists of chunk pairs aligned at sentence
and sub-sentence level (Deng et al, 2004). In A-E,
the training bitext consists of 3.8M English words,
3.2M Arabic words and 137K chunk pairs. In C-E,
the training bitext consists of 11.7M English words,
8.9M Chinese words and 674K chunk pairs.
Our Chinese text processing consists of word seg-
mentation (using the LDC segmenter) followed by
grouping of numbers. For Arabic our text pro-
cessing consisted of a modified Buckwalter analysis
(LDC2002L49) followed by post processing to sep-
arate conjunctions, prepostions and pronouns, and
Al-/w- deletion. The English text is processed us-
ing a simple tokenizer based on the text processing
utility available in the the NIST MT-eval toolkit.
The Language Model (LM) training data consists
of approximately 400M words of English text de-
rived from Xinhua and AFP (English Gigaword), the
English side of FBIS, the UN and A-E News texts,
and the online archives of The People?s Daily.
Table 5 gives the performance of the MJ-1 and
MJ-2 reordering models when translation is per-
formed using a 4-gram LM. We report performance
on the 02, 03, 04 test sets and the combined test set
1http://www.nist.gov/speech/tests/mt/
165
??1(x, u) =
Cx,u(1,+1) + Cx,u(2,+1)
Cx,u(1,+1) + Cx,u(1, 0) + Cx,u(1,+2) + Cx,u(2,+1) + Cx,u(2,?1)
??2(x, u) =
(Cx,u(1, 0) + Cx,u(2,?1) + Cx,u(1,+2))Cx,u(1,+2)
(Cx,u(1,+1) + Cx,u(1, 0) + Cx,u(1,+2) + Cx,u(2,+1) + Cx,u(2,?1))(Cx,u(1,+2) + Cx,u(1, 0))
Table 2: ML parameter estimates for MJ-2 model.
Reordering BLEU (%)
Arabic-English Chinese-English
02 03 04 ALL 02 03 04 ALL
None 37.5 40.3 36.8 37.8 ? 0.6 24.2 23.7 26.0 25.0 ? 0.5
MJ-1 flat 40.4 43.9 39.4 40.7 ? 0.6 25.7 24.5 27.4 26.2 ? 0.5
MJ-1 VT 41.3 44.8 40.3 41.6 ? 0.6 25.8 24.5 27.8 26.5 ? 0.5
MJ-2 flat 41.0 44.4 39.7 41.1 ? 0.6 26.4 24.9 27.7 26.7 ? 0.5
MJ-2 VT 41.7 45.3 40.6 42.0 ? 0.6 26.5 24.9 27.9 26.8 ? 0.5
Table 5: Performance of MJ-1 and MJ-2 reordering models with a 4-gram LM.
(ALL=02+03+04). For the combined set (ALL), we
also show the 95% BLEU confidence interval com-
puted using bootstrap resampling (Och, 2003).
Row 1 gives the performance when no reorder-
ing model is used. The next two rows show the in-
fluence of the MJ-1 reordering model; in row 2, a
flat probability of ?1(x, u) = 0.05 is used for all
phrase-pairs; in row 3, a reordering probability is
estimated for each phrase-pair using Viterbi Train-
ing (Eqn 13). The last two rows show the effect of
the MJ-2 reordering model; row 4 uses flat proba-
bilities (?1(x, u) = 0.05, ?2(x, u) = 0.01) for all
phrase-pairs; row 5 applies reordering probabilities
estimating with Viterbi Training for each phrase-pair
(Table 2).
On both language-pairs, we observe that reorder-
ing yields significant improvements. The gains from
phrase reordering are much higher on A-E relative
to C-E; this could be related to the fact that the word
order differences between English and Arabic are
much higher than the differences between English
and Chinese. MJ-1 VT outperforms flat MJ-1 show-
ing that there is value in estimating the reordering
parameters from bitext. Finally, the MJ-2 VT model
performs better than the flat MJ-2 model, but only
marginally better than the MJ-1 VT model. There-
fore estimation does improve the MJ-2 model but
allowing reordering beyond a window of 1 phrase is
not useful when translating either Arabic or Chinese
into English in this framework.
The flat MJ-1 model outperforms the no-
reordering case and the flat MJ-2 model is better
than the flat MJ-1 model; we hypothesize that phrase
reordering increases search space of translations that
allows the language model to select a higher qual-
ity hypothesis. This suggests that these models of
phrase reordering actually require strong language
models to be effective. We now investigate the inter-
action between language models and reordering.
Our goal here is to measure translation perfor-
mance of reordering models over variable span n-
gram LMs (Table 6). We observe that both MJ-1
and MJ-2 models yield higher improvements under
higher order LMs: e.g. on A-E, gains under 3g
(3.6 BLEU points on MJ-1, 0.2 points on MJ-2) are
higher than the gains with 2g (2.4 BLEU points on
MJ-1, 0.1 points on MJ-2).
Reordering BLEU (%)
A-E C-E
2g 3g 4g 2g 3g 4g
None 21.0 36.8 37.8 16.1 24.8 25.0
MJ-1 VT 23.4 40.4 41.6 16.2 25.9 26.5
MJ-2 VT 23.5 40.6 42.0 16.0 26.1 26.8
Table 6: Reordering with variable span n-gram LMs
on Eval02+03+04 set.
We now measure performance of the reorder-
ing models across the three test set genres used in
the NIST 2004 evaluation: news, editorials, and
speeches. On A-E, MJ-1 and MJ-2 yield larger im-
provements on News relative to the other genres;
on C-E, the gains are larger on Speeches and Ed-
itorials relative to News. We hypothesize that the
Phrase-Pair Inventory, reordering models and lan-
guage models could all have been biased away from
the test set due to the training data. There may also
be less movement across these other genres.
166
Reordering BLEU (%)
A-E C-E
News Eds Sphs News Eds Sphs
None 41.1 30.8 33.3 23.6 25.9 30.8
MJ-1 VT 45.6 32.6 35.7 24.8 27.8 33.3
MJ-2 VT 46.2 32.7 35.5 24.8 27.8 33.7
Table 7: Performance across Eval 04 test genres.
BLEU (%)
Arabic-English Chinese-English
Reordering 02 03 04n 02 03 04n
None 40.2 42.3 43.3 28.9 27.4 27.3
MJ-1 VT 43.1 45.0 45.6 30.2 28.2 28.9
MET-Basic 44.8 47.2 48.2 31.3 30.3 30.3
MET-IBM1 45.2 48.2 49.7 31.8 30.7 31.0
Table 8: Translation Performance on Large Bitexts.
4.2 Scaling to Large Bitext Training Sets
We here describe the integration of the phrase re-
ordering model in an MT system trained on large
bitexts. The text processing and language mod-
els have been described in ? 4.1. Alignment Mod-
els are trained on all available bitext (7.6M chunk
pairs/207.4M English words/175.7M Chinese words
on C-E and 5.1M chunk pairs/132.6M English
words/123.0M Arabic words on A-E), and word
alignments are obtained over the bitext. Phrase-pairs
are then extracted from the word alignments (Koehn
et al, 2003). MJ-1 model parameters are estimated
over all bitext on A-E and over the non-UN bitext
on C-E. Finally we use Minimum Error Training
(MET) (Och, 2003) to train log-linear scaling fac-
tors that are applied to the WFSTs in Equation 1.
04news (04n) is used as the MET training set.
Table 8 reports the performance of the system.
Row 1 gives the performance without phrase re-
ordering and Row 2 shows the effect of the MJ-1
VT model. The MJ-1 VT model is used in an initial
decoding pass with the four-gram LM to generate
translation lattices. These lattices are then rescored
under parameters obtained using MET (MET-basic),
and 1000-best lists are generated. The 1000-best
lists are augmented with IBM Model-1 (Brown et
al., 1993) scores and then rescored with a second set
of MET parameters. Rows 3 and 4 show the perfor-
mance of the MET-basic and MET-IBM1 models.
We observe that the maximum likelihood phrase
reordering model (MJ-1 VT) yields significantly im-
proved translation performance relative to the mono-
tone phrase order translation baseline. This confirms
the translation performance improvements found
over smaller training bitexts.
We also find additional gains by applying MET to
optimize the scaling parameters that are applied to
the WFST component distributions within the TTM
(Equation 1). In this procedure, the scale factor ap-
plied to the MJ-1 VT Phrase Translation and Re-
ordering component is estimated along with scale
factors applied to the other model components; in
other words, the ML-estimated phrase reordering
model itself is not affected by MET, but the likeli-
hood that it assigns to a phrase sequence is scaled
by a single, discriminatively optimized weight. The
improvements from MET (see rows MET-Basic and
MET- IBM1) demonstrate that the MJ-1 VT reorder-
ing models can be incorporated within a discrimi-
native optimized translation system incorporating a
variety of models and estimation procedures.
5 Discussion
In this paper we have described local phrase reorder-
ing models developed for use in statistical machine
translation. The models are carefully formulated
so that they can be implemented as WFSTs, and
we show how the models can be incorporated into
the Translation Template Model to perform phrase
alignment and translation using standard WFST op-
erations. Previous approaches to WFST-based re-
ordering (Knight and Al-Onaizan, 1998; Kumar
and Byrne, 2003; Tsukada and Nagata, 2004) con-
structed permutation acceptors whose state spaces
grow exponentially with the length of the sentence to
be translated. As a result, these acceptors have to be
pruned heavily for use in translation. In contrast, our
models of local phrase movement do not grow ex-
plosively and do not require any pruning or approx-
imation in their construction. In other related work,
Bangalore and Ricardi (2001) have trained WF-
STs for modeling reordering within translation; their
WFST parses word sequences into trees containing
reordering information, which are then checked for
well-formed brackets. Unlike this approach, our
model formulation does not use a tree representation
and also ensures that the output sequences are valid
permutations of input phrase sequences; we empha-
size again that the probability distribution induced
over reordered phrase sequences is not degenerate.
Our reordering models do resemble those of (Till-
mann, 2004; Tillmann and Zhang, 2005) in that we
167
treat the reordering as a sequence of jumps relative
to the original phrase sequence, and that the likeli-
hood of the reordering is assigned through phrase-
pair specific parameterized models. We note that
our implementation allows phrase reordering be-
yond simply a 1-phrase window, as was done by Till-
mann. More importantly, our model implements a
generative model of phrase reordering which can be
incorporated directly into a generative model of the
overall translation process. This allows us to per-
form ?embedded? EM-style parameter estimation,
in which the parameters of the phrase reordering
model are estimated using statistics gathered under
the complete model that will actually be used in
translation. We believe that this estimation of model
parameters directly from phrase alignments obtained
under the phrase translation model is a novel contri-
bution; prior approaches derived the parameters of
the reordering models from word aligned bitext, e.g.
within the phrase pair extraction procedure.
We have shown that these models yield improve-
ments in alignment and translation performance on
Arabic-English and Chinese-English tasks, and that
the reordering model can be integrated into large
evaluation systems. Our experiments show that dis-
criminative training procedures such Minimum Er-
ror Training also yield additive improvements by
tuning TTM systems which incorporate ML-trained
reordering models. This is essential for integrating
our reordering model inside an evaluation system,
where a variety of techniques are applied simultane-
ously.
The MJ-1 and MJ-2 models are extremely sim-
ple models of phrase reordering. Despite their sim-
plicity, these models provide large improvements
in BLEU score when incorporated into a monotone
phrase order translation system. Moreover, they
can be used to produced translation lattices for use
by more sophisticated reordering models that allow
longer phrase order movement. Future work will
build on these simple structures to produce more
powerful models of word and phrase movement in
translation.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Y. Deng, S. Kumar, and W. Byrne. 2004. Bitext chunk
alignment for statistical machine translation. In Re-
search Note, Center for Language and Speech Pro-
cessing, Johns Hopkins University.
K. Knight and Y. Al-Onaizan. 1998. Translation
with finite-state devices. In AMTA, pages 421?437,
Langhorne, PA, USA.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, Squibs & Discussion, 25(4).
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In HLT-NAACL, pages 127?133,
Edmonton, Canada.
S. Kumar and W. Byrne. 2003. A weighted finite state
transducer implementation of the alignment template
model for statistical machine translation. In HLT-
NAACL, pages 142?149, Edmonton, Canada.
S. Kumar, Y. Deng, and W. Byrne. 2005. A weighted fi-
nite state transducer translation template model for sta-
tistical machine translation. Journal of Natural Lan-
guage Engineering, 11(4).
F. Och, C. Tillmann, and H. Ney. 1999. Improved align-
ment models for statistical machine translation. In
EMNLP-VLC, pages 20?28, College Park, MD, USA.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL, Sapporo, Japan.
C. Tillmann and T. Zhang. 2005. A localized prediction
model for statistical machine translation. In ACL, Ann
Arbor, Michigan, USA.
C. Tillmann. 2004. A block orientation model for sta-
tistical machine translation. In HLT-NAACL, Boston,
MA, USA.
H. Tsukada and M. Nagata. 2004. Efficient decoding for
statistical machine translation with a fully expanded
WFST model. In EMNLP, Barcelona, Spain.
S. Vogel. 2003. SMT Decoder Dissected: Word Reorder-
ing. In NLPKE, Beijing, China.
D. Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In ACL, pages 152?158,
Santa Cruz, CA, USA.
R. Zens and H. Ney. 2003. A comparative study on re-
ordering constraints in statistical machine translation.
In ACL, pages 144?151, Sapporo, Japan.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004.
Reordering constraints for phrase-based statistical ma-
chine translation. In COLING, pages 205?211,
Boston, MA, USA.
168
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 110?118,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Context-Dependent Alignment Models for Statistical Machine Translation
Jamie Brunning, Adria` de Gispert and William Byrne
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
Trumpington Street, Cambridge, CB2 1PZ, U.K.
{jjjb2,ad465,wjb31}@eng.cam.ac.uk
Abstract
We introduce alignment models for Machine Trans-
lation that take into account the context of a source
word when determining its translation. Since the use
of these contexts alone causes data sparsity prob-
lems, we develop a decision tree algorithm for clus-
tering the contexts based on optimisation of the
EM auxiliary function. We show that our context-
dependent models lead to an improvement in align-
ment quality, and an increase in translation quality
when the alignments are used in Arabic-English and
Chinese-English translation.
1 Introduction
Alignment modelling for Statistical Machine Translation
(SMT) is the task of determining translational correspon-
dences between the words in pairs of sentences in parallel
text. Given a source language word sequence f J1 and a
target language word sequence eI1, we model the transla-
tion probability as P(eI1|fJ1 ) and introduce a hidden vari-
able aI1 representing a mapping from the target word po-
sitions to source word positions such that ei is aligned to
fai . Then P(eI1|f j1 ) =
?
aI1 P(eI1, aI1|f
j
1 ) (Brown et al,
1993).
Previous work on statistical alignment modelling has
not taken into account the source word context when de-
termining translations of that word. It is intuitive that a
word in one context, with a particular part-of-speech and
particular words surrounding it, may translate differently
when in a different context. We aim to take advantage
of this information to provide a better estimate of the
word?s translation. The challenge of incorporating con-
text information is maintaining computational tractability
of estimation and alignment, and we develop algorithms
to overcome this.
The development of efficient estimation procedures
for context-dependent acoustic models revolutionised the
field of Automatic Speech Recognition (ASR) (Young et
al., 1994). Clustering is used extensively for improv-
ing parameter estimation of triphone (and higher order)
acoustic models, enabling robust estimation of param-
eters and reducing the computation required for recog-
nition. Kannan et al (1994) introduce a binary tree-
growing procedure for clustering Gaussian models for
triphone contexts based on the value of a likelihood ra-
tio. We adopt a similar approach to estimate context-
dependent translation probabilities.
We focus on alignment with IBM Model 1 and HMMs.
HMMs are commonly used to generate alignments from
which state of the art SMT systems are built. Model 1 is
used as an intermediate step in the creation of more pow-
erful alignment models, such as HMMs and further IBM
models. In addition, it is used in SMT as a feature in Min-
imum Error Training (Och et al, 2004) and for rescor-
ing lattices of translation hypotheses (Blackwood et al,
2008). It is also used for lexically-weighted phrase ex-
traction (Costa-jussa` and Fonollosa, 2005) and sentence
segmentation of parallel text (Deng et al, 2007) prior to
machine translation.
1.1 Overview
We first develop an extension to Model 1 that allows the
use of arbitrary context information about a source word
to estimate context-dependent word-to-word translation
probabilities. Since there is insufficient training data to
accurately estimate translation probabilities for less fre-
quently occurring contexts, we develop a decision tree
clustering algorithm to form context classes. We go on to
develop a context-dependent HMM model for alignment.
In Section 3, we evaluate our context-dependent mod-
els on Arabic-English parallel text, comparing them to
our baseline context-independent models. We perform
morphological decomposition of the Arabic text using
MADA, and use part-of-speech taggers on both lan-
guages. Alignment quality is measured using Alignment
Error Rate (AER) measured against a manually-aligned
parallel text. Section 4 uses alignments produced by
110
our improved alignment models to initialise a statistical
machine translation system and evaluate the quality of
translation on several data sets. We also apply part-of-
speech tagging and decision tree clustering of contexts to
Chinese-English parallel text; translation results for these
languages are presented in Section 4.2.
1.2 Previous and related work
Brown et al (1993) introduce IBM Models 1-5 for align-
ment modelling; Vogel et al (1996) propose a Hidden
Markov Model (HMM) model for word-to-word align-
ment, where the words of the source sentence are viewed
as states of an HMM and emit target sentence words;
Deng and Byrne (2005a) extend this to an HMM word-to-
phrase model which allows many-to-one alignments and
can capture dependencies within target phrases.
Habash and Sadat (2006) perform morphological de-
composition of Arabic words, such as splitting of pre-
fixes and suffixes. This leads to gains in machine trans-
lation quality when systems are trained on parallel text
containing the modified Arabic and processing of Arabic
text is carried out prior to translation. Nie?en and Ney
(2001a) perform pre-processing of German and English
text before translation; Nie?en and Ney (2001b) use mor-
phological information of the current word to estimate
hierarchical translation probabilities.
Berger et al (1996) introduce maximum entropy mod-
els for machine translation, and use a window either side
of the target word as context information. Varea et al
(2002) test for the presence of specific words within a
window of the current source word to form features for
use inside a maximum entropy model of alignment.
Toutanova et al (2002) use part-of-speech informa-
tion in both the source and target languages to estimate
alignment probabilities, but this information is not in-
corporated into translation probabilities. Popovic? and
Ney (2004) use the base form of a word and its part-of-
speech tag during the estimation of word-to-word transla-
tion probabilities for IBM models and HMMs, but do not
defined context-dependent estimates of translation prob-
abilities.
Stroppa et al (2007) consider context-informed fea-
tures of phrases as components of the log-linear model
during phrase-based translation, but do not address align-
ment.
2 Use of source language context in
alignment modelling
Consider the alignment of the target sentence e = eI1 with
the source sentence f = fJ1 . Let a = aI1 be the align-
ments of the target words to the source words. Let cj be
the context information of fj for j = 1, . . . , J . This con-
text information can be any information about the word,
e.g. part-of-speech, previous and next words, part-of-
speech of previous and next words, or longer range con-
text information.
We follow Brown et al (1993), but extend their mod-
elling framework to include information about the source
word from which a target word is emitted. We model the
alignment process as:
P(eI1, aI1, I |fJ1 , cJ1 ) =
P(I |fJ1 , cJ1 )
I?
i=1
[P(ei|ai1, ei?11 , fJ1 , cJ1 , I)
? P(ai|ei?11 , ai?11 , fJ1 , cJ1 , I)
] (1)
We introduce word-to-word translation tables that depend
on the source language context for each word, i.e. the
probability that f translates to e given f has context c is
t(e|f, c). We assume that the context sequence is given
for a source word sequence. This assumption can be
relaxed to allow for multiple tag sequences as hidden
processes, but we assume here that a tagger generates
a single context sequence cJ1 for a word sequence fJ1 .
This corresponds to the assumption that, for a context se-
quence c?J1 , P(c?J1 |fJ1 ) = ?cJ1 (c?J1 ); hence
P(eI1, aI1|fJ1 ) =
?
c?J1
P(eI1, aI1, c?J1 |fJ1 ) = P(eI1, aI1|cJ1 , fJ1 )
For Model 1, ignoring the sentence length distribution,
PM1(eI1, aI1|fJ1 , cJ1 ) = 1(J + 1)I
I?
i=1
t(ei|fai , cai). (2)
Estimating translation probabilities separately for ev-
ery possible context of a source word individually leads
to problems with data sparsity and rapid growth of the
translation table. We therefore wish to cluster source con-
texts which lead to similar probability distributions. Let
Cf denote the set of all observed contexts of source word
f . A particular clustering is denoted
Kf = {Kf,1, . . . ,Kf,Nf},
where Kf is a partition of Cf . We define a class mem-
bership function ?f such that for any context c, ?f (c)
is the cluster containing c. We assume that all contexts
in a cluster give rise to the same translation probability
distribution for that source word, i.e. for a cluster K,
t(e|f, c) = t(e|f, c?) for all contexts c, c? ? K and all
target words e; we write this shared translation probabil-
ity as t(e|f,K).
The Model 1 sentence translation probability for a
given alignment (Equation 2) becomes
PM1(eI1, aI1|fJ1 , cJ1 ) = 1(J + 1)I
I?
i=1
t(ei|fai , ?f (cai)).
(3)
111
For HMM alignment, we assume that the transition prob-
abilities a(ai|ai?1) are independent of the word contexts
and the sentence translation probability is
PH(eI1, aI1|fJ1 , cJ1 ) =
I?
i=1
a(ai|ai?1, J)t(ei|fai , ?f (cai)).
(4)
Section 2.1.1 describes how the context classes are deter-
mined by optimisation of the EM auxiliary function. Al-
though the translation model is significantly more com-
plex than that of context-independent models, once class
membership is fixed, alignment and parameter estimation
use the standard algorithms.
2.1 EM parameter estimation
We train using Expectation Maximisation (EM), optimis-
ing the log probability of the training set {e(s), f (s)}Ss=1
(Brown et al, 1993). Given model parameters ??, we es-
timate new parameters ? by maximisation of the EM aux-
iliary function
?
s,a
P??(a|f (s), c(s), e(s)) log P?(e(s), a, I(s)|f (s), c(s)).
We assume the sentence length distribution and align-
ment probabilities do not depend on the contexts of the
source words; hence the relevant part of the auxiliary
function is
?
e
?
f
?
c?Cf
??(e|f, c) log t(e|f, c), (5)
where
??(e|f, c) = ?
s
I(s)?
i=1
J(s)?
j=1
[
?c(c(s)j )?e(e(s)i )?f (f (s)j )
? P??(ai = j|e(s), f (s), c(s))
]
Here ?? can be computed under Model 1 or the HMM,
and is calculated using the forward-backward algorithm
for the HMM.
2.1.1 Parameter estimation with clustered contexts
We can re-write the EM auxiliary function (Equation
5) in terms of the cluster-specific translation probabilities:
?
e
?
f
|Kf |?
l=1
?
c?Kf,l
??(e|f, c) log t(e|f, c)
= ?
e
?
f
?
K?Kf
??(e|f,K) log t(e|f,K) (6)
where ??(e|f,K) = ?
c?K
??(e|f, c)
Following the usual derivation, the EM update for the
class-specific translation probabilities becomes
t?(e|f,K) = ?
?(e|f,K)?
e? ??(e?|f,K)
. (7)
Standard EM training can be viewed a special case of this,
with every context of a source word grouped into a sin-
gle cluster. Another way to view these clustered context-
dependent models is that contexts belonging to the same
cluster are tied and share a common translation proba-
bility distribution, which is estimated from all training
examples in which any of the contexts occur.
2.2 Decision trees for context clustering
The objective for each source word is to split the contexts
into classes to maximise the likelihood of the training
data. Since it is not feasible to maximise the likelihood
of the observations directly, we maximise the expected
log likelihood by considering the EM auxiliary function,
in a similar manner to that used for modelling contextual
variations of phones for ASR (Young et al, 1994; Singer
and Ostendorf, 1996). We perform divisive clustering in-
dependently for each source word f , by building a binary
decision tree which forms classes of contexts which max-
imise the EM auxiliary function. Questions for the tree
are drawn from a set of questions Q = {q1, . . . , q|Q|}
concerning the context information of f .
Let K be any set of contexts of f , and define
L(K) = ?
e
?
c?K
??(e|f, c) log t(e|f, c)
= ?
e
?
c?K
??(e|f, c) log
?
c?K ??(e|f, c)?
e?
?
c?K ??(e?|f, c)
.
This is the contribution to the EM auxiliary function of
source word f occurring in the contexts of K. Let q be
a binary question about the context of f , and consider
the effect on the partial auxiliary function (Equation 6)
of splitting K into two clusters using question q. Define
Kq be the set of contexts in K which answer ?yes? to q
and Kq? be the contexts which answer ?no?. Define the
objective function
Qf,q(K) =
?
e
?
c?Kq
??(e|f, c) log t(e|f, c)
+?
e
?
c?Kq?
??(e|f, c) log t(e|f, c)
= L(Kq) + L(Kq?)
When the node is split using question q, the increase in
objective function is given by
Qf,q(K)? L(K) = L(Kq?) + L(Kq)? L(K).
112
We choose q to maximise this.
In order to build the decision tree for f , we take the set
of all contexts Cf as the initial cluster at the root node.
We then find the question q? such that Qf,q(Cf ) is maxi-
mal, i.e.
q? = arg max
q?Q
Qf,q(Cf )
This splits Cf , so our decision tree now has two nodes.
We iterate this process, at each iteration splitting (into
two further nodes) the leaf node that leads to the great-
est increase in objective function. This leads to a greedy
search to optimise the log likelihood over possible state
clusterings.
In order to control the growth of the tree, we put in
place two thresholds:
? Timp is the minimum improvement in objective func-
tion required for a node to be split; without it, we
would continue splitting nodes until each contained
only one context, even though doing so would cause
data sparsity problems.
? Tocc is the minimum occupancy of a node, based on
how often the contexts at that node occur in the train-
ing data; we want to ensure that there are enough ex-
amples of a context in the training data to estimate
accurately the translation probability distribution for
that cluster.
For each leaf node l and set of contextsKl at that node,
we find the question ql that, when used to split Kl, pro-
duces the largest gain in objective function:
ql = arg max
q?Q
[L(Kl,q) + L(Kl,q?)? L(Kl)]
= arg max
q?Q
[L(Kl,q) + L(Kl,q?)]
We then find the leaf node for which splitting gives the
largest improvement:
l? = arg max
l
[L(Kl,ql) + L(Kl,q?l)? L(Kl)]
If the following criteria are both satisfied at that node, we
split the node into two parts, creating two leaf nodes in
its place:
? The objective function increases sufficiently
L(Kl,ql) + L(Kl,q?l)? L(Kl) > Timp
? The occupancy threshold is exceeded for both child
nodes:
?
e
?
c?Kl,x
??(e|f, c) > Tocc for x = q, q?
We perform such clustering for every source word in the
parallel text.
shares NNS ? ? ? ? ? ? ?
bank NN ? ? ? ? ? ? ?
the DT ? ? ? ? ? ? ?
of IN ? ? ? ? ? ? ?
% PUNC ? ? ? ? ? ? ?
12 NN ? ? ? ? ? ? ?
selling VBG ? ? ? ? ? ? ?
of IN ? ? ? ? ? ? ?
deal NN ? ? ? ? ? ? ?
the DT ? ? ? ? ? ? ?
Sfq
pN
N
by
EN
N
12
NN
%
PU
NC
mn
IN
>
shm
NN
Al
bn
kN
N
city NN ? ? ? ? ? ? ? ?
the DT ? ? ? ? ? ? ? ?
in IN ? ? ? ? ? ? ? ?
liquor NN ? ? ? ? ? ? ? ?
selling VBG ? ? ? ? ? ? ? ?
were VBD ? ? ? ? ? ? ? ?
owners NNS ? ? ? ? ? ? ? ?
whose WP$ ? ? ? ? ? ? ? ?
houses NNS ? ? ? ? ? ? ? ?
several JJ ? ? ? ? ? ? ? ?
and CC ? ? ? ? ? ? ? ?
w+
CC
mn
Az
lN
N
Ed
pJ
J
>
SH
Ab
hA
NN
yb
yE
wn
VB
P
Al
xm
wr
NN
fy
IN
Al
md
yn
pN
N
Figure 1: Alignment of the English selling in different contexts.
In the first, it is preceded by of and links to the infinitive of the
Arabic verb byE; in the second, it is preceded by were and links
to an inflected form of the same Arabic verb, ybyEwn.
3 Evaluation of alignment quality
Our models were built using the MTTK toolkit (Deng
and Byrne, 2005b). Decision tree clustering was imple-
mented and the process parallelised to enable thousands
of decision trees to be built. Our context-dependent (CD)
Model 1 models trained on context-annotated data were
compared to the baseline context-independent (CI) mod-
els trained on untagged data.
The models were trained using data allowed for the
NIST 08 Arabic-English evaluation1, excluding the UN
collections, comprising 300k parallel sentence pairs, a to-
tal of 8.4M words of Arabic and 9.5M words of English.
The Arabic language incorporates into its words sev-
eral prefixes and suffixes which determine grammatical
features such as gender, number, person and voice. The
MADA toolkit (Habash and Sadat, 2006) was used to
perform Arabic morphological word decomposition and
part-of-speech tagging. It determines the best analysis
for each word in a sentence and splits word prefixes and
suffixes, based on the alternative analyses provided by
BAMA (Buckwalter, 2002). We use tokenisation scheme
1http://nist.gov/speech/tests/mt/2008
113
?D2?, which splits certain prefixes and has been reported
to improve machine translation performance (Habash and
Sadat, 2006). The alignment models are trained on this
processed data, and the prefixes and suffixes are treated
as words in their own right; in particular their contexts
are examined and clustered.
The TnT tagger (Brants, 2000), used as distributed
with its model trained on the Wall Street Journal portion
of the Penn treebank, was used to obtain part-of-speech
tags for the English side of the parallel text. Marcus et al
(1993) gives a complete list of part-of-speech tags pro-
duced. No morphological analysis is performed for En-
glish.
Automatic word alignments were compared to a
manually-aligned corpus made up of the IBM Arabic-
English Word Alignment Corpus (Ittycheriah et al,
2006) and the word alignment corpora LDC2006E86 and
LDC2006E93. This contains 28k parallel text sentences
pairs: 724k words of Arabic and 847k words of English.
The alignment links were modified to reflect the MADA
tokenisation; after modification, there are 946k word-to-
word alignment links.
Alignment quality was evaluated by computing Align-
ment Error Rate (AER) (Och and Ney, 2000) relative to
the manual alignments. Since the links supplied con-
tain only ?sure? links and no ?possible? links, we use the
following formula for computing AER given reference
alignment links S and hypothesised alignment links A:
AER = 1? 2|S?A||S|+|A| .
3.1 Questions about contexts
The algorithm presented in Section 2 allows for any infor-
mation about the context of the source word to be consid-
ered. We could consider general questions of the form ?Is
the previous word x?? and ?Does word y occur within n
words of this one??. To maintain computational tractabil-
ity, we restrict the questions to those concerning the part-
of-speech tag assigned to the current, previous and next
words. We do not ask questions about the identities of the
words themselves. For each part-of-speech tag T , we ask
the question ?Does w have tag T??. In addition, we group
part-of-speech tags to ask more general questions: e.g.
the set of contexts which satisfies ?Is w a noun?? contains
those that satisfy ?Is w a proper noun?? and ?Is w a sin-
gular or mass noun??. We also ask the same questions
of the previous and next words in the source sentence.
In English, this gives a total of 152 distinct questions,
each of which is considered when splitting a leaf node.
The MADA part-of-speech tagger uses a reduced tag set,
which produces a total of 68 distinct questions.
Figure 1 shows the links of the English source word
selling in two different contexts where it links to different
words in Arabic, which are both forms of the same verb.
The part-of-speech of the previous word is useful for dis-
-4.22e+07
-4.2e+07
-4.18e+07
-4.16e+07
-4.14e+07
-4.12e+07
-4.1e+07
-4.08e+07
-4.06e+07
-4.04e+07
 11  12  13  14  15  16  17  18  19  20
Lo
g 
pr
ob
ab
ilit
y 
of
 tr
ai
ni
ng
 d
at
a
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
-3.1e+06
-3.05e+06
-3e+06
-2.95e+06
-2.9e+06
-2.85e+06
-2.8e+06
-2.75e+06
 11  12  13  14  15  16  17  18  19  20
Lo
g 
pr
ob
ab
ilit
y 
of
 tr
ai
ni
ng
 d
at
a
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
Figure 2: Increase in log probability of training data during
training for varying Timp, with Model 1, for Arabic to English
(top) and English to Arabic (bottom)
criminating between the two cases, whereas a context-
independent model would assign the same probability to
both Arabic words.
3.2 Training Model 1
Training is carried out in both translation directions. For
Arabic to English, the Arabic side of the parallel text is
tagged and the English side remains untagged; we view
the English words as being generated from the Arabic
words and questions are asked about the context of the
Arabic words to determine clusters for the translation ta-
ble. For English to Arabic, the situation is reversed: we
used tagged English text as the source language and un-
tagged Arabic text, with morphological decomposition,
as the target language.
Standard CI Model 1 training, initialised with a uni-
form translation table so that t(e|f) is constant for all
source/target word pairs (f, e), was run on untagged data
for 10 iterations in each direction (Brown et al, 1993;
Deng and Byrne, 2005b). A decision tree was built to
cluster the contexts and a further 10 iterations of training
were carried out using the tagged words-with-context to
produce context-dependent models (CD Model 1). The
114
English question Frequency
Is Next Preposition 1523
Is Prev Determiner 1444
Is Prev Preposition 1209
Is Prev Adjective 864
Is Next Noun Singular Mass 772
Is Prev Noun Singular Mass 690
Is Next Noun Plural 597
Is Next Noun 549
Arabic question Frequency
Is Prev Preposition 1110
Is Next Preposition 993
Is Prev Noun 981
Is Next Noun 912
Is Prev Coordinating Conjunction 627
Is Prev Noun SingularMass 607
Is Next Punctuation 603
Is Next Adjective Adverb 559
Table 1: Most frequent root node context questions
models were then evaluated using AER at each train-
ing iteration. A number of improvement thresholds Timp
were tested, and performance compared to that of models
found after further iterations of CI Model 1 training on
the untagged data. In both alignment directions, the log
probability of the training data increases during training
(see Figure 2). As expected, the training set likelihood
increases as the threshold Timp is reduced, allowing more
clusters and closer fitting to the data.
3.2.1 Analysis of frequently used questions
Table 1 shows the questions used most frequently at
the root node of the decision tree when clustering con-
texts in English and Arabic. Because they are used first,
these are the questions that individually give the great-
est ability to discriminate between the different contexts
of a word. The list shows the importance of the left and
right contexts of the word in predicting its translation: of
the most common 50 questions, 25 concern the previous
word, 19 concern the next, and only 6 concern the part-
of-speech of the current word. For Arabic, of the most
frequent 50 questions, 21 concern the previous word, 20
concern the next and 9 the current word.
3.2.2 Alignment Error Rate
Since MT systems are usually built on the union of the
two sets of alignments (Koehn et al, 2003), we consider
the union of alignments in the two directions as well as
those in each direction. Figure 3 shows the change in
AER of the alignments in each direction, as well as the
alignment formed by taking their union at corresponding
thresholds and training iterations.
Timp Arabic-English (%) English-Arabic (%)
10 30601 (25.33) 26011 (39.87)
20 11193 (9.27) 18365 (28.15)
40 1874 (1.55) 9104 (13.96)
100 307 (0.25) 1128 (1.73)
Table 2: Words [number (percentage)] with context-dependent
translation for varying Timp
3.2.3 Variation of improvement threshold Timp
There is a trade-off between modelling the data accu-
rately, which requires more clusters, and eliminating data
sparsity problems, which requires each cluster to contain
contexts that occur frequently enough in the training data
to estimate the translation probabilities accurately. Use of
a smaller threshold Timp leads to more clusters per word
and an improved ability to fit to the data, but this can lead
to reduced alignment quality if there is insufficient data
to estimate the translation probability distribution accu-
rately for each cluster. For lower thresholds, we observe
over-fitting and the AER rises after the second iteration of
CD training, similar to the behaviour seen in Och (2002).
Setting Timp = 0 results in each context of a word having
its own cluster, which leads to data sparsity problems.
Table 2 shows the percentage of words for which the
contexts are split into multiple clusters for CD Model 1
with varying improvement thresholds. This occurs when
there are enough training data examples and sufficient
variability between the contexts of a word that splitting
the contexts into more than one cluster increases the EM
auxiliary function. For words where the contexts are not
split, all the contexts remain in the same cluster and pa-
rameter estimation is exactly the same as for the unclus-
tered context-independent models.
3.3 Training HMMs
Adding source word context to translation has so far led
to improvements in AER for Model 1, but the perfor-
mance does not match that of HMMs trained on untagged
data; we therefore train HMMs on tagged data.
We proceed with Model 1 and Model 2 trained in the
usual way, and context-independent (CI) HMMs were
trained for 5 iterations on the untagged data. Statistics
were then gathered for clustering at various thresholds,
after which 5 further EM iterations were performed with
tagged data to produce context-dependent (CD) HMMs.
The HMMs were trained in both the Arabic to English
and the English to Arabic directions. The log likelihood
of the training set varies with Timp in much the same
way as for Model 1, increasing at each iteration, with
greater likelihood at lower thresholds. Figure 4 shows
how the AER of the union alignment varies with Timp
during training. As with Model 1, the clustered HMM
115
 49.2
 49.4
 49.6
 49.8
 50
 50.2
 50.4
 50.6
 50.8
 10  11  12  13  14  15  16  17  18  19  20
AE
R
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
49.6
49.8
50.0
50.2
50.4
50.6
50.8
51.0
51.2
 10  11  12  13  14  15  16  17  18  19  20
AE
R
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
Threshold 100
49.0
49.2
49.4
49.6
49.8
50.0
50.2
50.4
50.6
50.8
51.0
 10  11  12  13  14  15  16  17  18  19  20
AE
R
Iteration
CI Model 1
Threshold 10
Threshold 20
Threshold 60
Figure 3: Variation of AER during Model 1 training for varying
Timp, for Arabic to English (top), English to Arabic (middle)
and their union (bottom)
34.4
34.5
34.6
34.7
34.8
34.9
35.0
35.1
35.2
35.3
 5  6  7  8  9  10
AE
R
Iteration
CI HMM
Threshold 10
Threshold 20
Threshold 60
Figure 4: AER of the union alignment for varying Timp with the
HMM model
 72
 74
 76
 78
 80
 40  45  50  55  60
Pr
ec
is
io
n
Recall
p0=0.95
p0=0.00
p0=0.95
p0=0.00
English-Arabic CD HMM
English-Arabic CI HMM
Arabic-English CD HMM
Arabic-English CI HMM
Figure 5: Precision/recall curves for the context-dependent
HMM and the baseline context-independent HMM, for Arabic
to English and English to Arabic. p0 varies from 0.00 to 0.95 in
steps of 0.05.
models produce alignments with a lower AER than the
baseline model, and there is evidence of over-fitting to
the training data.
3.3.1 Alignment precision and recall
The HMM models include a null transition probability,
p0, which can be modified to adjust the number of align-
ments to the null token (Deng and Byrne, 2005a). Where
a target word is emitted from null, it is not included in
the alignment links, so this target word is viewed as not
being aligned to any source word; this affects the preci-
sion and recall. The results reported above use p0 = 0.2
for English-Arabic and p0 = 0.4 for Arabic-English; we
can tune these values to produce alignments with the low-
est AER. Figure 5 shows precision-recall curves for the
CD HMMs compared to the CI HMMs for both transla-
tion directions. For a given value of precision, the CD
HMM has higher recall; for a given value of recall, the
CD HMM has higher precision.
We do not report F-score (Fraser and Marcu, 2006)
since in our experiments we have not found strong cor-
relation with translation performance, but we note that
these results for precision and recall should lead to im-
proved F-scores as well.
4 Evaluation of translation quality
We have shown that the context-dependent models pro-
duce a decrease in AER measured on manually-aligned
data; we wish to show this improved model performance
leads to an increase in translation quality, measured by
BLEU score (Papineni et al, 2001). In addition to the
Arabic systems already evaluated by AER, we also report
results for a Chinese-English translation system.
Alignment models were evaluated by aligning the
training data using the models in each translation direc-
116
tion. HiFST, a WFST-based hierarchical translation sys-
tem described in (Iglesias et al, 2009), was trained on
the union of these alignments. MET (Och, 2003) was
carried out using a development set, and the BLEU score
evaluated on two test sets. Decoding used a 4-gram lan-
guage model estimated from the English side of the entire
MT08 parallel text, and a 965M word subset of monolin-
gual data from the English Gigaword Third Edition.
For both Arabic and English, the CD HMM models
were evaluated as follows. Iteration 5 of the CI HMM
was used to produce alignments for the parallel text train-
ing data: these were used to train the baseline system.
The same data is aligned using CD HMMs after two
further iterations of training and a second WFST-based
translation system built from these alignments. The mod-
els are evaluated by comparing BLEU scores with those
of the baseline model.
4.1 Arabic to English translation
Alignment models were trained on the NIST MT08
Arabic-English parallel text, excluding the UN portion.
The null alignment probability was chosen based on the
AER, resulting in values of p0 = 0.05 for Arabic to
English and p0 = 0.10 for English to Arabic. We per-
form experiments on the NIST Arabic-English transla-
tion task. The mt02 05 tune and mt02 05 test data sets
are formed from the odd and even numbered sentences
of the NIST MT02 to MT05 evaluation sets respectively;
each contains 2k sentences and 60k words. We use
mt02 05 tune as a development set and evaluate the sys-
tem on mt02 05 test and the newswire portion of the
MT08 set, MT08-nw. Table 3 shows a comparison of the
system trained using CD HMMs with the baseline sys-
tem, which was trained using CI HMM models on un-
tagged data. The context-dependent models result in a
gain in BLEU score of 0.3 for mt02 05 test and 0.6 for
MT08-nw.
4.2 Chinese to English translation
The Chinese training set was 600k random parallel text
sentences of the newswire LDC collection allowed for
NIST MT08, a total of 15.2M words of Chinese and
16.6M words of English. The Chinese text was tagged us-
ing the MXPOST maximum-entropy part of speech tag-
ging tool (Ratnaparkhi, 1996) trained on the Penn Chi-
nese Treebank 5.1; the English text was tagged using the
TnT part of speech tagger (Brants, 2000) trained on the
Wall Street Journal portion of the English Penn treebank.
The development set tune-nw and validation set test-nw
contain a mix of the newswire portions of MT02 through
MT05 and additional developments sets created by trans-
lation within the GALE program. We also report results
on the newswire portion of the MT08 set. Again we see
an increase in BLEU score for both test sets: 0.5 for test-
Arabic-English
Alignments tune mt02 05 test MT08-nw
CI HMM 50.0 49.4 46.3
CD HMM 50.0 49.7 46.9
Chinese-English
Alignments tune test-nw MT08-nw
CI HMM 28.1 28.5 26.9
CD HMM 28.5 29.0 27.7
Table 3: Comparison, using BLEU score, of the CD HMM with
the baseline CI HMM
nw and 0.8 for MT08-nw.
5 Conclusions and future work
We have introduced context-dependent Model 1 and
HMM alignment models, which use context information
in the source language to improve estimates of word-
to-word translation probabilities. Estimation of parame-
ters using these contexts without smoothing leads to data
sparsity problems; therefore we have developed decision
tree clustering algorithms to cluster source word contexts
based on optimisation of the EM auxiliary function. Con-
text information is incorporated by the use of part-of-
speech tags in both languages of the parallel text, and the
EM algorithm is used for parameter estimation.
We have shown that these improvements to the model
lead to decreased AER compared to context-independent
models. Finally, we compare machine translation sys-
tems built using our context-dependent alignments. For
both Arabic- and Chinese-to-English translation, we
report an increase in translation quality measured by
BLEU score compared to a system built using context-
independent alignments.
This paper describes an initial investigation into
context-sensitive alignment models, and there are many
possible directions for future research. Clustering the
probability distributions of infrequently occurring may
produce improvements in alignment quality, different
model training schemes and extensions of the context-
dependence to more sophisticated alignment models will
be investigated. Further translation experiments will be
carried out.
Acknowledgements
This work was supported in part by the GALE program of
the Defense Advanced Research Projects Agency, Con-
tract No. HR0011-06-C-0022. J. Brunning is supported
by a Schiff Foundation graduate studentship. Thanks to
Yanjun Ma, Dublin City University, for training the Chi-
nese part of speech tagger.
117
References
A. L. Berger, S. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
Graeme Blackwood, Adria` de Gispert, Jamie Brunning, and
William Byrne. 2008. European language translation with
weighted finite state transducers: The CUED MT system for
the 2008 ACL workshop on SMT. In Proceedings of the
Third Workshop on Statistical Machine Translation, pages
131?134, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proceedings of the 6th Applied Natural Language
Processing Conference: ANLP-2000, Seattle, USA.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computational
Linguistics, 19(2):263?311.
T. Buckwalter. 2002. Buckwalter Arabic morphological ana-
lyzer.
Marta Ruiz Costa-jussa` and Jos?e A. R. Fonollosa. 2005.
Improving phrase-based statistical translation by modifying
phrase extraction and including several features. In Proceed-
ings of the ACL Workshop on Building and Using Parallel
Texts, pages 149?154, June.
Yonggang Deng and William Byrne. 2005a. HMM word and
phrase alignment for statistical machine translation. In Proc.
of HLT-EMNLP.
Yonggang Deng and William Byrne. 2005b. JHU-Cambridge
statistical machine translation toolkit (MTTK) user manual.
Yonggang Deng, Shankhar Kumar, and William Byrne. 2007.
Segmentation and alignment of parallel text for statistical
machine translation. Journal of Natural Language Engineer-
ing, 13:3:235?260.
Alexander Fraser and Daniel Marcu. 2006. Measuring word
alignment quality for statistical machine translation. Tech-
nical Report ISI-TR-616, ISI/University of Southern Califor-
nia, May.
Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In HLT-NAACL.
G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne. 2009.
Hierarchical phrase-based translation with weighted finite
state transducers. In Procedings of NAACL-HLT, 2009,
Boulder, Colorado.
Abraham Ittycheriah, Yaser Al-Onaizan, and Salim Roukos.
2006. The IBM Arabic-English word alignment corpus, Au-
gust.
A. Kannan, M. Ostendorf, and J. R. Rohlicek. 1994. Maxi-
mum likelihood clustering of Gaussians for speech recogni-
tion. Speech and Audio Processing, IEEE Transactions on,
2(3):453?455, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on Human
Language Technology, pages 48?54.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
English: the Penn Treebank. Computational Linguistics,
19(2):313?330.
Sonja Nie?en and Hermann Ney. 2001a. Morpho-syntactic
analysis for reordering in statistical machine translation. In
Proceedings of MT Summit VIII, pages 247?252, September.
Sonja Nie?en and Hermann Ney. 2001b. Toward hierarchical
models for statistical machine translation of inflected lan-
guages. In Proceedings of the workshop on Data-driven
methods in machine translation, pages 1?8, Morristown, NJ,
USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. A comparison of
alignment models for statistical machine translation. In Pro-
ceedings of the 18th conference on Computational Linguis-
tics, pages 1086?1090.
F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, and D. Radev. 2004. A smorgasbord of features for
statistical machine translation. In Proceedings of NAACL.
Franz Josef Och. 2002. Statistical Machine Translation: From
Single Word Models to Alignment Templates. Ph.D. thesis,
Franz Josef Och.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In ACL ?03: Proceedings of the 41st
Annual Meeting on Association for Computational Linguis-
tics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL, pages 311?318.
Maja Popovic? and Hermann Ney. 2004. Improving word align-
ment quality using morpho-syntactic information. In In Pro-
ceedings of COLING, page 310.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Processing,
pages 133?142.
H. Singer and M. Ostendorf. 1996. Maximum likelihood suc-
cessive state splitting. Proceedings of ICASSP, 2:601?604.
Nicolas Stroppa, Antal van den Bosch, and Andy Way. 2007.
Exploiting source similarity for SMT using context-informed
features. In Proceedings of the 11th Conference on Theoreti-
cal and Methodological Issues in Machine Translation (TMI
2007), pages 231 ? 240.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Man-
ning. 2002. Extensions to HMM-based statistical word
alignment models. In Proceedings of EMNLP, pages 87?94.
Ismael Garc??a Varea, Franz J. Och, Hermann Ney, and Fran-
cisco Casacuberta. 2002. Improving alignment quality in
statistical machine translation using context-dependent max-
imum entropy models. In Proceedings of COLING, pages
1?7.
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
Proceedings of COLING, pages 836?841.
S. J. Young, J. J. Odell, and P. C. Woodland. 1994. Tree-based
state tying for high accuracy acoustic modelling. In HLT ?94:
Proceedings of the workshop on Human Language Technol-
ogy, pages 307?312.
118
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433?441,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hierarchical Phrase-Based Translation with
Weighted Finite State Transducers
Gonzalo Iglesias? Adria` de Gispert?
? University of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain
{giglesia,erbanga}@gts.tsc.uvigo.es
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
Eduardo R. Banga? William Byrne?
Abstract
This paper describes a lattice-based decoder
for hierarchical phrase-based translation. The
decoder is implemented with standard WFST
operations as an alternative to the well-known
cube pruning procedure. We find that the
use of WFSTs rather than k-best lists requires
less pruning in translation search, resulting
in fewer search errors, direct generation of
translation lattices in the target language,
better parameter optimization, and improved
translation performance when rescoring with
long-span language models and MBR decod-
ing. We report translation experiments for
the Arabic-to-English and Chinese-to-English
NIST translation tasks and contrast the WFST-
based hierarchical decoder with hierarchical
translation under cube pruning.
1 Introduction
Hierarchical phrase-based translation generates
translation hypotheses via the application of hierar-
chical rules in CYK parsing (Chiang, 2005). Cube
pruning is used to apply language models at each
cell of the CYK grid as part of the search for a
k-best list of translation candidates (Chiang, 2005;
Chiang, 2007). While this approach is very effective
and has been shown to produce very good quality
translation, the reliance on k-best lists is a limita-
tion. We take an alternative approach and describe a
lattice-based hierarchical decoder implemented with
Weighted Finite State Transducers (WFSTs). In ev-
ery CYK cell we build a single, minimal word lattice
containing all possible translations of the source sen-
tence span covered by that cell. When derivations
contain non-terminals, we use pointers to lower-
level lattices for memory efficiency. The pointers
are only expanded to the actual translations if prun-
ing is required during search; expansion is otherwise
only carried out at the upper-most cell, after the full
CYK grid has been traversed.
We describe how this decoder can be easily im-
plemented with WFSTs. For this we employ the
OpenFST libraries (Allauzen et al, 2007). Using
standard FST operations such as composition, ep-
silon removal, determinization, minimization and
shortest-path, we find this search procedure to be
simpler to implement than cube pruning. The main
modeling advantages are a significant reduction in
search errors, a simpler implementation, direct gen-
eration of target language word lattices, and better
integration with other statistical MT procedures. We
report translation results in Arabic-to-English and
Chinese-to-English translation and contrast the per-
formance of lattice-based and cube pruning hierar-
chical decoding.
1.1 Related Work
Hierarchical phrase-based translation has emerged
as one of the dominant current approaches to statis-
tical machine translation. Hiero translation systems
incorporate many of the strengths of phrase-based
translation systems, such as feature-based transla-
tion and strong target language models, while also
allowing flexible translation and movement based
on hierarchical rules extracted from aligned paral-
lel text. We summarize some extensions to the basic
approach to put our work in context.
433
Hiero Search Refinements Huang and Chiang
(2007) offer several refinements to cube pruning to
improve translation speed. Venugopal et al (2007)
introduce a Hiero variant with relaxed constraints
for hypothesis recombination during parsing; speed
and results are comparable to those of cube prun-
ing, as described by Chiang (2007). Li and Khudan-
pur (2008) report significant improvements in trans-
lation speed by taking unseen n-grams into account
within cube pruning to minimize language model re-
quests. Dyer et al (2008) extend the translation of
source sentences to translation of input lattices fol-
lowing Chappelier et al (1999).
Extensions to Hiero Several authors describe ex-
tensions to Hiero, to incorporate additional syntactic
information (Zollmann and Venugopal, 2006; Zhang
and Gildea, 2006; Shen et al, 2008; Marton and
Resnik, 2008), or to combine it with discriminative
latent models (Blunsom et al, 2008).
Analysis and Contrastive Experiments Zollman et
al. (2008) compare phrase-based, hierarchical and
syntax-augmented decoders for translation of Ara-
bic, Chinese, and Urdu into English. Lopez (2008)
explores whether lexical reordering or the phrase
discontiguity inherent in hierarchical rules explains
improvements over phrase-based systems. Hierar-
chical translation has also been used to great effect
in combination with other translation architectures,
e.g. (Sim et al, 2007; Rosti et al, 2007).
WFSTs for Translation There is extensive work in
using Weighted Finite State Transducer for machine
translation (Bangalore and Riccardi, 2001; Casacu-
berta, 2001; Kumar and Byrne, 2005; Mathias and
Byrne, 2006; Graehl et al, 2008).
To our knowledge, this paper presents the first de-
scription of hierarchical phrase-based translation in
terms of lattices rather than k-best lists. The next
section describes hierarchical phrase-based transla-
tion with WFSTs, including the lattice construction
over the CYK grid and pruning strategies. Sec-
tion 3 reports translation experiments for Arabic-to-
English and Chinese-to-English, and Section 4 con-
cludes.
2 Hierarchical Translation with WFSTs
The translation system is based on a variant of the
CYK algorithm closely related to CYK+ (Chappe-
lier and Rajman, 1998). Parsing follows the de-
scription of Chiang (2005; 2007), maintaining back-
pointers and employing hypothesis recombination
without pruning. The underlying model is a syn-
chronous context-free grammar consisting of a set
R = {Rr} of rules Rr : N ? ??r,?r? / pr, with
?glue? rules, S ? ?X,X? and S ? ?S X,S X?. If a
rule has probability pr, it is transformed to a cost cr;
here we use the tropical semiring, so cr = ? log pr.
N denotes a non-terminal; in this paper, N can be
either S, X, or V (see section 3.2). T denotes the
terminals (words), and the grammar builds parses
based on strings ?, ? ? {{S,X, V } ? T}+. Each
cell in the CYK grid is specified by a non-terminal
symbol and position in the CYK grid: (N,x, y),
which spans sx+y?1x on the source sentence.
In effect, the source language sentence is parsed
using a context-free grammar with rules N ? ?.
The generation of translations is a second step that
follows parsing. For this second step, we describe
a method to construct word lattices with all possible
translations that can be produced by the hierarchical
rules. Construction proceeds by traversing the CYK
grid along the backpointers established in parsing.
In each cell (N,x, y) in the CYK grid, we build a
target language word lattice L(N,x, y). This lat-
tice contains every translation of sx+y?1x from every
derivation headed by N . These lattices also contain
the translation scores on their arc weights.
The ultimate objective is the word lattice
L(S, 1, J) which corresponds to all the analyses that
cover the source sentence sJ1 . Once this is built,
we can apply a target language model to L(S, 1, J)
to obtain the final target language translation lattice
(Allauzen et al, 2003).
We use the approach of Mohri (2002) in applying
WFSTs to statistical NLP. This fits well with the use
of the OpenFST toolkit (Allauzen et al, 2007) to
implement our decoder.
2.1 Lattice Construction Over the CYK Grid
In each cell (N,x, y), the set of rule indices used
by the parser is denoted R(N,x, y), i.e. for r ?
R(N,x, y), N ? ??r,?r? was used in at least one
derivation involving that cell.
For each rule Rr, r ? R(N,x, y), we build a lat-
tice L(N,x, y, r). This lattice is derived from the
target side of the rule ?r by concatenating lattices
434
R1: X ? ?s1 s2 s3,t1 t2?
R2: X ? ?s1 s2,t7 t8?
R3: X ? ?s3,t9?
R4: S ? ?X,X?
R5: S ? ?S X,S X?
L(S, 1, 3) = L(S, 1, 3, 4) ? L(S, 1, 3, 5)
L(S, 1, 3, 4) = L(X, 1, 3) = L(X, 1, 3, 1) =
= A(t1)?A(t2)
L(S, 1, 3, 5) = L(S, 1, 2)? L(X, 3, 1)
L(S, 1, 2) = L(S, 1, 2, 4) = L(X, 1, 2) =
= L(X, 1, 2, 2) = A(t7)?A(t8)
L(X, 3, 1) = L(X, 3, 1, 3) = A(t9)
L(S, 1, 3, 5) = A(t7)?A(t8)?A(t9)
L(S, 1, 3) = (A(t1)?A(t2))? (A(t7)?A(t8)?A(t9))
Figure 1: Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3. The grid
is represented here in two dimensions (x, y). In practice only the first column accepts both non-terminals (S,X). For
this reason it is divided in two subcolumns.
corresponding to the elements of ?r = ?r1...?r|?r |.
If an ?ri is a terminal, creating its lattice is straight-
forward. If ?ri is a non-terminal, it refers to a cell
(N ?, x?, y?) lower in the grid identified by the back-
pointer BP (N,x, y, r, i); in this case, the lattice
used is L(N ?, x?, y?). Taken together,
L(N,x, y, r) = ?
i=1..|?r|
L(N,x, y, r, i) (1)
L(N,x, y, r, i) =
{
A(?i) if ?i ? T
L(N ?, x?, y?) else
(2)
where A(t), t ? T returns a single-arc accep-
tor which accepts only the symbol t. The lattice
L(N,x, y) is then built as the union of lattices cor-
responding to the rules in R(N,x, y):
L(N,x, y) = ?
r?R(N,x,y)
L(N,x, y, r) (3)
Lattice union and concatenation are performed
using the ? and ? WFST operations respectively, as
described by Allauzen et al(2007). If a rule Rr has
a cost cr, it is applied to the exit state of the lattice
L(N,x, y, r) prior to the operation of Equation 3.
2.1.1 An Example of Phrase-based Translation
Figure 1 illustrates this process for a three word
source sentence s1s2s3 under monotone phrase-
based translation. The left-hand side shows the state
of the CYK grid after parsing using the rules R1 to
R5. These include 3 rules with only terminals (R1,
R2, R3) and the glue rules (R4, R5). Arrows repre-
sent backpointers to lower-level cells. We are inter-
ested in the upper-most S cell (S, 1, 3), as it repre-
sents the search space of translation hypotheses cov-
ering the whole source sentence. Two rules (R4, R5)
are in this cell, so the lattice L(S, 1, 3) will be ob-
tained by the union of the two lattices found by the
backpointers of these two rules. This process is ex-
plicitly derived in the right-hand side of Figure 1.
2.1.2 An Example of Hierarchical Translation
Figure 2 shows a hierarchical scenario for the
same sentence. Three rules, R6, R7, R8, are added
to the example of Figure 1, thus providing two ad-
ditional derivations. This makes use of sublattices
already produced in the creation of L(S, 1, 3, 5) and
L(X, 1, 3, 1) in Figure 1; these are within {}.
2.2 A Procedure for Lattice Construction
Figure 3 presents an algorithm to build the lattice
for every cell. The algorithm uses memoization: if
a lattice for a requested cell already exists, it is re-
turned (line 2); otherwise it is constructed via equa-
tions 1,2,3. For every rule, each element of the tar-
get side (lines 3,4) is checked as terminal or non-
terminal (equation 2). If it is a terminal element
(line 5), a simple acceptor is built. If it is a non-
terminal (line 6), the lattice associated to its back-
pointer is returned (lines 7 and 8). The complete
lattice L(N,x, y, r) for each rule is built by equa-
tion 1 (line 9). The lattice L(N,x, y) for this cell
is then found by union of all the component rules
(line 10, equation 3); this lattice is then reduced by
435
R6: X ? ?s1,t20?
R7: X ? ?X1 s2 X2,X1 t10 X2?
R8: X ? ?X1 s2 X2,X2 t10 X1?
L(S, 1, 3) = L(S, 1, 3, 4) ?{L(S, 1, 3, 5)}
L(S, 1, 3, 4) = L(X, 1, 3) =
={L(X, 1, 3, 1)} ?L(X, 1, 3, 7)? L(X, 1, 3, 8)
L(X, 1, 3, 7) = L(X, 1, 1, 6)?A(t10)?L(X, 3, 1, 3) =
= A(t20)?A(t10)?A(t9)
L(X, 1, 3, 8) = A(t9)?A(t10)?A(t20)
L(S, 1, 3) = {(A(t1)?A(t2))} ?
?(A(t20)?A(t10)?A(t9))? (A(t9)?A(t10)?A(t20))?
?{(A(t7)?A(t8)?A(t9))}
Figure 2: Translation as in Figure 1 but with additional rules R6,R7,R8. Lattices previously derived appear within {}.
standard WFST operations (lines 11,12,13). It is
important at this point to remove any epsilon arcs
which may have been introduced by the various
WFST union, concatenation, and replacement oper-
ations (Allauzen et al, 2007).
1 function buildFst(N,x,y)
2 if ? L(N,x, y) return L(N,x, y)
3 for r ? R(N,x, y), Rr : N ? ??,??
4 for i = 1...|?|
5 if ?i ? T, L(N,x, y, r, i) = A(?i)
6 else
7 (N ?, x?, y?) = BP (?i)
8 L(N,x, y, r, i) = buildFst(N ?, x?, y?)
9 L(N,x, y, r)=?i=1..|?| L(N,x, y, r, i)
10 L(N,x, y) =?r?R(N,x,y) L(N,x, y, r)
11 fstRmEpsilon L(N,x, y)
12 fstDeterminize L(N,x, y)
13 fstMinimize L(N,x, y)
14 return L(N,x, y)
Figure 3: Recursive Lattice Construction.
2.3 Delayed Translation
Equation 2 leads to the recursive construction of lat-
tices in upper-levels of the grid through the union
and concatenation of lattices from lower levels. If
equations 1 and 3 are actually carried out over fully
expanded word lattices, the memory required by the
upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as
pointers to the low-level lattices. This effectively
builds a skeleton of the desired lattice and delays
the creation of the final word lattice until a single
replacement operation is carried out in the top cell
(S, 1, J). To make this exact, we define a function
g(N,x, y) which returns a unique tag for each lattice
in each cell, and use it to redefine equation 2. With
the backpointer (N ?, x?, y?) = BP (N,x, y, r, i),
these special arcs are introduced as:
L(N,x, y, r, i) =
{
A(?i) if ?i ? T
A(g(N ?, x?, y?)) else
(4)
The resulting lattices L(N,x, y) are a mix of tar-
get language words and lattice pointers (Figure 4,
top). However each still represents the entire search
space of all translation hypotheses covering the
span. Importantly, operations on these lattices ?
such as lossless size reduction via determinization
and minimization ? can still be performed. Owing
to the existence of multiple hierarchical rules which
share the same low-level dependencies, these opera-
tions can greatly reduce the size of the skeleton lat-
tice; Figure 4 shows the effect on the translation ex-
ample. This process is carried out for the lattice at
every cell, even at the lowest level where there are
only sequences of word terminals. As stated, size
reductions can be significant. However not all redu-
dancy is removed, since duplicate paths may arise
through the concatenation and union of sublattices
with different spans.
At the upper-most cell, the lattice L(S, 1, J) con-
tains pointers to lower-level lattices. A single FST
replace operation (Allauzen et al, 2007) recursively
substitutes all pointers by their lower-level lattices
until no pointers are left, thus producing the com-
plete target word lattice for the whole source sen-
tence. The use of the lattice pointer arc was in-
spired by the ?lazy evaluation? techniques developed
by Mohri et al(2000). Its implementation uses the
infrastructure provided by the OpenFST libraries for
436
01
t1
2g(X,1,2)
3
g(X,1,1)
5
g(X,3,1)
7
t2
g(X,3,1)
4
t10
6t10
g(X,3,1)
g(X,1,1)
0
3g(X,1,1)
2g(X,1,2)
1
t1
4
g(X,3,1)
t10
6
g(X,3,1)
t2
5
t10
g(X,1,1)
Figure 4: Delayed translation WFST with derivations
from Figure 1 and Figure 2 before [t] and after minimiza-
tion [b].
delayed composition, etc.
2.4 Pruning in Lattice Construction
The final translation lattice L(S, 1, J) can grow very
large after the pointer arcs are expanded. We there-
fore apply a word-based language model, via WFST
composition, and perform likelihood-based prun-
ing (Allauzen et al, 2007) based on the combined
translation and language model scores.
Pruning can also be performed on sublattices
during search. One simple strategy is to monitor
the number of states in the determinized lattices
L(N,x, y). If this number is above a threshold, we
expand any pointer arcs and apply a word-based lan-
guage model via composition. The resulting lattice
is then reduced by likelihood-based pruning, after
which the LM scores are removed. This search prun-
ing can be very selective. For example, the pruning
threshold can depend on the height of the cell in the
grid. In this way the risk of search errors can be
controlled.
3 Translation Experiments
We report experiments on the NIST MT08 Arabic-
to-English and Chinese-to-English translation tasks.
We contrast two hierarchical phrase-based decoders.
The first decoder, Hiero Cube Pruning (HCP), is a k-
best decoder using cube pruning implemented as de-
scribed by Chiang (2007). In our implementation, k-
best lists contain unique hypotheses. The second de-
coder, Hiero FST (HiFST), is a lattice-based decoder
implemented with Weighted Finite State Transduc-
ers as described in the previous section. Hypotheses
are generated after determinization under the trop-
ical semiring so that scores assigned to hypotheses
arise from single minimum cost / maximum likeli-
hood derivations. We also use a variant of the k-best
decoder which works in alignment mode: given an
input k-best list, it outputs the feature scores of each
hypothesis in the list without applying any pruning.
This is used for Minimum Error Training (MET)
with the HiFST system.
These two language pairs pose very different
translation challenges. For example, Chinese-
to-English translation requires much greater word
movement than Arabic-to-English. In the frame-
work of hierarchical translation systems, we have
found that shallow decoding (see section 3.2) is
as good as full hierarchical decoding in Arabic-
to-English (Iglesias et al, 2009). In Chinese-to-
English, we have not found this to be the case.
Therefore, we contrast the performance of HiFST
and HCP under shallow hierarchical decoding for
Arabic-to-English, while for Chinese-to-English we
perform full hierarchical decoding.
Both hierarchical translation systems share a
common architecture. For both language pairs,
alignments are generated over the parallel data. The
following features are extracted and used in trans-
lation: target language model, source-to-target and
target-to-source phrase translation models, word and
rule penalties, number of usages of the glue rule,
source-to-target and target-to-source lexical models,
and three rule count features inspired by Bender et
al. (2007). The initial English language model is
a 4-gram estimated over the parallel text and a 965
million word subset of monolingual data from the
English Gigaword Third Edition. Details of the par-
allel corpus and development sets used for each lan-
guage pair are given in their respective section.
Standard MET (Och, 2003) iterative parameter
estimation under IBM BLEU (Papineni et al, 2001)
is performed on the corresponding development set.
For the HCP system, MET is done following Chi-
ang (2007). For the HiFST system, we obtain a k-
437
best list from the translation lattice and extract each
feature score with the aligner variant of the k-best
decoder. After translation with optimized feature
weights, we carry out the two following rescoring
steps.
? Large-LM rescoring. We build sentence-
specific zero-cutoff stupid-backoff (Brants et
al., 2007) 5-gram language models, estimated
using ?4.7B words of English newswire text,
and apply them to rescore either 10000-best
lists generated by HCP or word lattices gener-
ated by HiFST. Lattices provide a vast search
space relative to k-best lists, with translation
lattice sizes of 1081 hypotheses reported in the
literature (Tromble et al, 2008).
? Minimum Bayes Risk (MBR). We rescore the
first 1000-best hypotheses with MBR, taking
the negative sentence level BLEU score as the
loss function (Kumar and Byrne, 2004).
3.1 Building the Rule Sets
We extract hierarchical phrases from word align-
ments, applying the same restrictions as introduced
by Chiang (2005). Additionally, following Iglesias
et al (2009) we carry out two rule filtering strate-
gies:
? we exclude rules with two non-terminals with
the same order on the source and target side
? we consider only the 20 most frequent transla-
tions for each rule
For each development set, this produces approx-
imately 4.3M rules in Arabic-to-English and 2.0M
rules in Chinese-to-English.
3.2 Arabic-to-English Translation
We translate Arabic-to-English with shallow hierar-
chical decoding, i.e. only phrases are allowed to be
substituted into non-terminals. The rules used in this
case are, in addition to the glue rules:
X ? ??s,?s?
X ? ?V ,V ?
V ? ?s,t?
s, t ? T+; ?s, ?s ? ({V } ?T)+
For translation model training, we use all allowed
parallel corpora in the NIST MT08 Arabic track
(?150M words per language). In addition to the
MT08 set itself, we use a development set mt02-05-
tune formed from the odd numbered sentences of the
NIST MT02 through MT05 evaluation sets; the even
numbered sentences form the validation set mt02-
05-test. The mt02-05-tune set has 2,075 sentences.
The cube pruning decoder, HCP, employs k-best
lists of depth k=10000 (unique). Using deeper lists
results in excessive memory and time requirements.
In contrast, the WFST-based decoder, HiFST, re-
quires no local pruning during lattice construction
for this task and the language model is not applied
until the lattice is fully built at the upper-most cell of
the CYK grid.
Table 1 shows results for mt02-05-tune, mt02-
05-test and mt08, as measured by lowercased IBM
BLEU and TER (Snover et al, 2006). MET param-
eters are optimized for the HCP decoder. As shown
in rows ?a? and ?b?, results after MET are compara-
ble.
Search Errors Since both decoders use exactly the
same features, we can measure their search errors on
a sentence-by-sentence basis. A search error is as-
signed to one of the decoders if the other has found
a hypothesis with lower cost. For mt02-05-tune, we
find that in 18.5% of the sentences HiFST finds a hy-
pothesis with lower cost than HCP. In contrast, HCP
never finds any hypothesis with lower cost for any
sentence. This is as expected: the HiFST decoder
requires no pruning prior to applying the language
model, so search is exact.
Lattice/k-best Quality Rescoring results are dif-
ferent for cube pruning and WFST-based decoders.
Whereas HCP improves by 0.9 BLEU, HiFST im-
proves over 1.5 BLEU. Clearly, search errors in HCP
not only affect the 1-best output but also the quality
of the resulting k-best lists. For HCP, this limits the
possible gain from subsequent rescoring steps such
as large LMs and MBR.
Translation Speed HCP requires an average of 1.1
seconds per input word. HiFST cuts this time by
half, producing output at a rate of 0.5 seconds per
word. It proves much more efficient to process com-
pact lattices contaning many hypotheses rather than
to independently processing each one of them in k-
best form.
438
decoder mt02-05-tune mt02-05-test mt08
BLEU TER BLEU TER BLEU TER
a HCP 52.2 41.6 51.5 42.2 42.5 48.6
+5gram 53.1 41.0 52.5 41.5 43.3 48.3
+MBR 53.2 40.8 52.6 41.4 43.4 48.1
b HiFST 52.2 41.5 51.6 42.1 42.4 48.7
+5gram 53.3 40.6 52.7 41.3 43.7 48.1
+MBR 53.7 40.4 53.3 40.9 44.0 48.0
Decoding time in secs/word: 1.1 for HCP; 0.5 for HiFST.
Table 1: Constrative Arabic-to-English translation results (lower-cased IBM BLEU | TER) after MET and subsequent
rescoring steps. Decoding time reported for mt02-05-tune.
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 42.9. This is directly comparable to
the official MT08 Constrained Training Track eval-
uation results1.
3.3 Chinese-to-English Translation
We translate Chinese-to-English with full hierarchi-
cal decoding, i.e. hierarchical rules are allowed to be
substituted into non-terminals. We consider a maxi-
mum span of 10 words for the application of hierar-
chical rules and only glue rules are allowed at upper
levels of the CYK grid.
For translation model training, we use all avail-
able data for the GALE 2008 evaluation2, approx.
250M words per language. In addition to the MT08
set itself, we use a development set tune-nw and
a validation set test-nw. These contain a mix of
the newswire portions of MT02 through MT05 and
additional developments sets created by translation
within the GALE program. The tune-nw set has
1,755 sentences.
Again, the HCP decoder employs k-best lists of
depth k=10000. The HiFST decoder applies prun-
ing in search as described in Section 2.4, so that any
lattice in the CYK grid is pruned if it covers at least
3 source words and contains more than 10k states.
The likelihood pruning threshold relative to the best
path in the lattice is 9. This is a very broad threshold
so that very few paths are discarded.
1Full MT08 results are available at http://www.nist.gov/
speech/tests/mt/2008/doc/mt08 official results v0.html. It is
worth noting that many of the top entries make use of system
combination; the results reported here are for single system
translation.
2See http://projects.ldc.upenn.edu/gale/data/catalog.html.
Improved Optimization Table 2 shows results for
tune-nw, test-nw and mt08, as measured by lower-
cased IBM BLEU and TER. The first two rows show
results for HCP when using MET parameters opti-
mized over k-best lists produced by HCP (row ?a?)
and by HiFST (row ?b?). We find that using the k-
best list obtained by the HiFST decoder yields bet-
ter parameters during optimization. Tuning on the
HiFST k-best lists improves the HCP BLEU score,
as well. We find consistent improvements in BLEU;
TER also improves overall, although less consis-
tently.
Search Errors Measured over the tune-nw devel-
opment set, HiFST finds a hypothesis with lower
cost in 48.4% of the sentences. In contrast, HCP
never finds any hypothesis with a lower cost for any
sentence, indicating that the described pruning strat-
egy for HiFST is much broader than that of HCP.
Note that HCP search errors are more frequent for
this language pair. This is due to the larger search
space required in fully hierarchical translation; the
larger the search space, the more search errors will
be produced by the cube pruning k-best implemen-
tation.
Lattice/k-best Quality The lattices produced by
HiFST yield greater gains in LM rescoring than the
k-best lists produced by HCP. Including the subse-
quent MBR rescoring, translation improves as much
as 1.2 BLEU, compared to 0.7 BLEU with HCP.
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 27.8, comparable to official results
in the UnConstrained Training Track of the NIST
2008 evaluation.
439
decoder MET k-best tune-nw test-nw mt08
BLEU TER BLEU TER BLEU TER
a HCP HCP 31.6 59.7 31.9 59.7 ? ?
b HCP 31.7 60.0 32.2 59.9 27.2 60.2
+5gram HiFST 32.2 59.3 32.6 59.4 27.8 59.3
+MBR 32.4 59.2 32.7 59.4 28.1 59.3
c HiFST 32.0 60.1 32.2 60.0 27.1 60.5
+5gram HiFST 32.7 58.3 33.1 58.4 28.1 59.1
+MBR 32.9 58.4 33.4 58.5 28.9 58.9
Table 2: Contrastive Chinese-to-English translation results (lower-cased IBM BLEU|TER) after MET and subsequent
rescoring steps. The MET k-best column indicates which decoder generated the k-best lists used in MET optimization.
4 Conclusions
The lattice-based decoder for hierarchical phrase-
based translation described in this paper can be eas-
ily implemented using Weighted Finite State Trans-
ducers. We find many benefits in this approach
to translation. From a practical perspective, the
computational operations required can be easily car-
ried out using standard operations already imple-
mented in general purpose libraries. From a model-
ing perspective, the compact representation of mul-
tiple translation hypotheses in lattice form requires
less pruning in hierarchical search. The result is
fewer search errors and reduced overall memory use
relative to cube pruning over k-best lists. We also
find improved performance of subsequent rescor-
ing procedures which rely on the translation scores.
In direct comparison to k-best lists generated un-
der cube pruning, we find that MET parameter opti-
mization, rescoring with large language models, and
MBR decoding, are all improved when applied to
translations generated by the lattice-based hierarchi-
cal decoder.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
Agency, Contract No. HR0011- 06-C-0022. G.
Iglesias supported by Spanish Government research
grant BES-2007-15956 (project TEC2006-13694-
C03-03).
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of ACL, pages 557?
564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Srinivas Bangalore and Giuseppe Riccardi. 2001. A
finite-state approach to machine translation. In Pro-
ceedings of NAACL.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language trans-
lation system. In Proceedings of ASRU, pages 396?
401.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HLT,
pages 200?208.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
Francisco Casacuberta. 2001. Finite-state transducers
for speech-input translation. In Proceedings of ASRU.
Jean-Ce?dric Chappelier and Martin Rajman. 1998.
A generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
Jean-Ce?dric Chappelier, Martin Rajman, Ramo?n
Aragu?e?s, and Antoine Rozenknop. 1999. Lattice
parsing for speech recognition. In Proceedings of
TALN, pages 95?104.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270.
440
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-HLT, pages 1012?1020.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391?427.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL, pages 144?151.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?176.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of HLT-EMNLP, pages 161?168.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the ACL-HLT Second Workshop on Syntax
and Structure in Statistical Translation, pages 10?18.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of COLING, pages
505?512.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT, pages 1003?1011.
Lambert Mathias and William Byrne. 2006. Statisti-
cal phrase-based speech translation. In Proceedings
of ICASSP.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231:17?32.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language, vol-
ume 16, pages 69?88.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311?318.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining outputs from multiple machine
translation systems. In Proceedings of HLT-NAACL,
pages 228?235.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation sys-
tem combination. In Proceedings of ICASSP, vol-
ume 4, pages 105?108.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA, pages 223?231.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk
decoding for statistical machine translation. In Pro-
ceedings of EMNLP, pages 620?629.
Ashish Venugopal, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Proceed-
ings of HLT-NAACL, pages 500?507.
Hao Zhang and Daniel Gildea. 2006. Synchronous bi-
narization for machine translation. In Proceedings of
HLT-NAACL, pages 256?263.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL Workshop on Statistical Ma-
chine Translation, pages 138?141.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of COLING, pages 1145?1152.
441
Proceedings of NAACL HLT 2009: Short Papers, pages 73?76,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Minimum Bayes Risk Combination of Translation Hypotheses from
Alternative Morphological Decompositions
Adria` de Gispert? Sami Virpioja?
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
? Helsinki University of Technology. Adaptive Informatics Research Centre
P.O.Box 5400, 02015 TKK, Finland
{sami.virpioja,mikko.kurimo}@tkk.fi
Mikko Kurimo? William Byrne?
Abstract
We describe a simple strategy to achieve trans-
lation performance improvements by combin-
ing output from identical statistical machine
translation systems trained on alternative mor-
phological decompositions of the source lan-
guage. Combination is done by means of Min-
imum Bayes Risk decoding over a shared N-
best list. When translating into English from
two highly inflected languages such as Ara-
bic and Finnish we obtain significant improve-
ments over simply selecting the best morpho-
logical decomposition.
1 Introduction
Morphologically rich languages pose significant
challenges for natural language processing. The ex-
tensive use of inflection, derivation, and composi-
tion leads to a huge vocabulary, and sparsity in mod-
els estimated from data. Statistical machine transla-
tion (SMT) systems estimated from parallel text are
affected by this. This is particularly acute when ei-
ther the source or the target language, or both, are
morphologically complex.
Owing to these difficulties and to the natural in-
terest researchers take in complex linguistic phe-
nomena, many approaches to morphological anal-
ysis have been developed and evaluated. We fo-
cus on applications to SMT in Section 1.1, but we
note the recent general survey (Roark and Sproat,
2007) and the Morpho Challenge competitive evalu-
ations1. Prior evaluations of morphological analyz-
ers have focused on determining which analyzer was
1See http://www.cis.hut.fi/morphochallenge2009/ and links
best suited for some particular task. For translation,
we take a different approach and investigate whether
competing analyzers might have complementary in-
formation. Our method is straightforward. We train
two identical SMT systems with two versions of
the same parallel corpus, each with a different mor-
phological decomposition of the source language.
We combine their translation hypotheses perform-
ing Minimum Bayes Risk decoding over merged N-
best lists. Results are reported in the NIST 2008
Arabic-to-English MT task and an European Parlia-
ment Finnish-to-English task, with significant gains
over each individual system.
1.1 Prior Work
Several earlier works investigate word segmenta-
tion and transformation schemes, which may include
Part-Of-Speech or other information, to alleviate
the effect of morphological variation on translation
models. With different training corpus sizes, they
focus on translation into English from Arabic (Lee,
2004; Habash and Sadat, 2006; Zollmann et al,
2006), Czech (Goldwater and McClosky, 2005; Tal-
bot and Osborne, 2006), German (Nie?en and Ney,
2004) or Catalan, Spanish and Serbian (Popovic
and Ney, 2004). Some address the generation
challenge when translating from English into Span-
ish (Ueffing and Ney, 2003; de Gispert and Marin?o,
2008). Unsupervised morphology learning is pro-
posed as a language-independent solution to reduce
the problems of rich morphology in (Virpioja et al,
there to earlier workshops. The combination scheme described
in this paper will be one of the evaluation tracks in the upcoming
workshop.
73
Arabic wqrrt An tn$A ljnp tHDyryp jAmEp lljmEyp AlEAmp fY dwrthA AlvAnyp wAlxmsyn
MADA D2 w+ qrrt >n tn$A ljnp tHDyryp jAmEp l+ AljmEyp AlEAmp fy dwrthA AlvAnyp w+ Alxmsyn
SAKHR w+ qrrt An tn$A ljnp tHDyryp jAmEp l*l+ jmEyp Al+ EAmp fY dwrt +hA Al+ vAnyp w*Al+ xmsyn
English a preparatory committee of the whole of the general assembly is to be established at its fifty-second session
Table 1: Example of alternative segmentation schemes for a given Arabic sentence, in Buckwalter transliteration.
2007). Factored models are introduced in (Koehn
and Hoang, 2007) for better integration of morpho-
syntactic information.
Gime?nez and Ma`rquez (2005) merge mul-
tiple word alignments obtained from several
linguistically-tagged versions of a Spanish-English
corpus, but only standard tokens are used in decod-
ing. Dyer et al (2008) report improvements from
multiple Arabic segmentations in translation to En-
glish translation, but their goal was to demonstrate
the value of lattice-based translation. From a model-
ing perspective their approach is unwieldy: multiple
analyses of the parallel text collections are merged
to create a large, heterogeneous training set; a sin-
gle set of models and alignments is produced; lattice
translation is then performed using a single system
to translate all morphological analyses. We find that
similar gains can be obtained much more easily.
The approach we take is Minimum Bayes Risk
(MBR) System Combination (Sim et al, 2007). N-
best lists from multiple SMT systems are merged;
the posterior distributions over the individual lists
are interpolated to form a new distribution over the
merged list. MBR hypotheses selection is then per-
formed using sentence-level BLEU score (Kumar
and Byrne, 2004). It is very likely that even greater
gains can be achieved by more complicated combi-
nation schemes (Rosti et al, 2007), although signif-
icantly more effort in tuning would be required.
2 Arabic-to-English Translation
For Arabic-to-English translation, we consider two
alternative segmentations of the Arabic words. We
first use the MADA toolkit (Habash and Rambow,
2005). After tagging, we split word prefixes and suf-
fixes according to scheme ?D2? (Habash and Sadat,
2006). Secondly, we take the segmentation gener-
ated by Sakhr Software in Egypt using their Arabic
Morphological Tagger, as an alternative segmenta-
tion into subword units. This scheme generates more
tokens as it segments all Arabic articles which other-
wise remain attached in the MADA D2 scheme (Ta-
ble 1).
Translation experiments are based on the NIST
MT08 Arabic-to-English translation task, includ-
ing all allowed parallel data as training material
(?150M English words, and 153M or 178M Arabic
words for MADA-segmented and Sakhr-segmented
text, respectively). In addition to the MT08 set itself,
we take the NIST MT02 through MT05 evaluation
sets and divide them into a development set (odd-
numbered sentences) and a test set (even-numbered
sentences), each containing ?2k sentences.
The SMT system used is HiFST, a hierarchical
phrase-based system implemented with Weighted
Finite-State Transducers (Iglesias et al, 2009). Two
identical systems are trained from each parallel cor-
pus, i.e. MADA-based and SAKHR-based. Both
systems use the same standard features and share
the first-pass English language model, a 4-gram es-
timated over the parallel text and a 965 million word
subset of monolingual data from the English Giga-
word Third Edition. Minimum Error Training pa-
rameter estimation under IBM BLEU is performed
on the development set (mt02-05-tune), and the out-
put translation lattice is rescored with large language
models estimated using ?4.7B words of English
newswire text, in the same fashion as (Iglesias et
al., 2009). Finally, the first 1000-best hypotheses
are rescored with MBR, taking the negative sentence
level BLEU score as the loss function to minimise.
For system combination, we obtain two sets of N-
best lists of depth N=500, one from each system.
Both lists are obtained after large-LM lattice rescor-
ing, i.e. prior to individual MBR. A joint MBR de-
coding is then carried out on the aggregated 1000-
best list with equal weight assigned to the posterior
distribution assigned to the hypotheses by each sys-
tem. Results are shown in Table 2.
As shown, the scores obtained via MBR combi-
nation outperform significantly those achieved via
MBR for the best-performing system (MADA). The
74
mt02-05-
-tune -test mt08
MADA-based 53.3 52.7 43.7
+MBR 53.7 53.3 44.0
SAKHR-based 52.7 52.8 43.3
+MBR 53.2 53.2 43.8
MBR-combined 54.6 54.6 45.6
Table 2: Arabic-to-English translation results. Lower-
cased IBM BLEU reported.
mixed case BLEU-4 for the MBR-combined system
on mt08 is 44.1. This is directly comparable to the
official MT08 Constrained Training Track evalua-
tion results.2
3 Finnish-to-English Translation
Finnish is a highly-inflecting, agglutinative lan-
guage. It has dozens of both inflectional and
derivational suffixes, that are concatenated together
with only moderately small changes in the sur-
face forms. For instance, one can inflect the
word ?kauppa? (shop) into ?kaupa+ssa+mme+kin?
(also in our shop) by glueing the suffixes to the
end. In addition, Finnish has many compound
words, sometimes consisting of several parts, such
as ?ulko+maa+n+kauppa+politiikka? (foreign trade
policy). Due to these properties, the number of dif-
ferent word forms that can be observed is enormous.
Morfessor (Creutz and Lagus, 2007) is a method
for modeling concatenative morphology in an un-
supervised manner. It tries to find morpheme-like
units, morphs, that are segments of the words. In-
spired by the minimum description length principle,
Morfessor tries to find a concise lexicon of morphs
that can effectively code the words in the train-
ing data. Unlike other unsupervised methods (e.g.,
Goldsmith (2001)), there is no restrictions on how
many morphs a word can have. After training the
model, the most likely segmentation of new words
to morphs can be found using the Viterbi algorithm.
There exist a few different versions of Morfessor.
The baseline algorithm has been found to be very
useful in automatic speech recognition of agglutina-
tive languages (Kurimo et al, 2006). However, it
2Full MT08 results are available at http://www.nist.gov/
speech/tests/mt/2008/doc/mt08 official results v0.html
often oversegments morphemes that are rare or not
seen at all in the training data. Following the ap-
proach in (Virpioja et al, 2007), we use the Morfes-
sor Categories-MAP algorithm (Creutz and Lagus,
2005). It applies a hierarchical model with three sur-
face categories (prefix, stem and suffix), that allow
the algorithm to treat out-of-vocabulary words in a
convenient manner. For instance, if we encounter a
new name with a known suffix, it can usually sepa-
rate the suffix and leave the actual name intact.
Similarly to the Arabic-to-English task, we train
two identical HiFST systems. In this case, whereas
one is trained on Finnish morphs decomposed by
Morfessor (morph-based), the other is trained on
standard, unprocessed Finnish (word-based). For
this task we use the EuParl parallel corpus . Portions
from Q4/2000 was reserved for testing and Septem-
ber 2000 for development, both containing around
3,000 sentences. The training data comprised 23M
English words, and 17M or 27M Finnish tokens for
word-based or morph-based text, respectively.
The training set was also used to train the mor-
phological segmentation. The quality of the seg-
mentation is evaluated in (Virpioja et al, 2007). A
precision of 78.72% and recall of 52.29% was mea-
sured for the segmentation boundaries with respect
to a linguistic reference segmentation. As the recall
is not very high, the segmentation is more conserva-
tive than the linguistic reference. Table 4 shows an
example for a phrase in the training data.
Results are shown in Table 3, where again signifi-
cant gains are achieved when simply combining out-
put N-best lists via MBR. Only one reference was
available for scoring. In this case we did not ap-
ply large-LM rescoring, as no large additional par-
liamentary data was available. Individual MBR did
not yield gains for each of the systems.
devel test
Word-based 30.2 27.9
Morph-based 29.4 27.4
MBR-combined 30.5 28.9
Table 3: Finnish-to-English translation results. Lower-
cased IBM BLEU reported.
75
Finnish vaarallisten aineiden kuljetusten turvallisuusneuvonantaja
Morfessor vaaraSTM llistenSTM aineSTM idenSUF kuljetusPRE tenSTM turvallisuusPRE neuvoSTM nSUF antajaSTM
Linguistic vaara llis t en aine i den kuljet us t en turva llis uus neuvo n anta ja
English safety adviser for the transport of dangerous goods
Table 4: Example of Morfessor Categories-MAP segmentation and linguistic segmentation for a Finnish phrase. Sub-
scripts show the morph categories given by Morfessor: stem (STM), prefix (PRE) and suffix (SUF).
4 Conclusions
We demonstrated that multiple morphological anal-
yses can be the basis for SMT system combination.
These results will be of interest to researchers devel-
oping morphological analyzers, as it provides a new,
and potentially profitable way to evaluate compet-
ing analysers. The results should also interest SMT
researchers. SMT system combination is an active
area of research, but good gains from combination
usually require very different system architectures;
this can be a barrier to developing competitive sys-
tems. We find that the same architecture trained on
two different analyses is adequate to generate the di-
verse hypotheses needed for system combination.
Acknowledgments. This work was supported by the GALE
program of DARPA (HR0011-06-C-0022), the GSLT and AIRC
in the Academy of Finland, and the EMIME project and PAS-
CAL2 NoE in the EC?s FP7.
References
M. Creutz and K. Lagus. 2005. Inducing the morpho-
logical lexicon of a natural language from unannotated
text. In Conf. on Adaptive Knowledge Representation
and Reasoning (AKRR).
M. Creutz and K. Lagus. 2007. Unsupervised models
for morpheme segmentation and morphology learning.
ACM Trans. Speech and Language Processing, 4(1).
A. de Gispert and J.B. Marin?o. 2008. On the impact
of morphology in English to Spanish statistical MT.
Speech Communication, 50.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In ACL-HLT.
J. Gime?nez and Ll. Ma`rquez. 2005. Combining linguis-
tic data views for phrase-based SMT. In ACL Work-
shop on Building and Using Parallel Texts.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2).
S. Goldwater and D. McClosky. 2005. Improving sta-
tistical MT through morphological analysis. In HLT-
EMNLP.
N. Habash and O. Rambow. 2005. Arabic tokeniza-
tion, part-of-speech tagging and morphological disam-
biguation in one fell swoop. In ACL.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In HLT-
NAACL: Short Papers.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Hierarchical phrase-based translation with
weighted finite state transducers. In HLT-NAACL.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In EMNLP.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
NAACL.
M. Kurimo, A. Puurula, E. Arisoy, V. Siivola, T. Hir-
sima?ki, J. Pylkko?nen, T. Aluma?e, and M. Saraclar.
2006. Unlimited vocabulary speech recognition for
agglutinative languages. In HLT-NAACL.
Y.-S. Lee. 2004. Morphological analysis for statistical
machine translation. In HLT-NAACL: Short Papers.
S. Nie?en and H. Ney. 2004. Statistical machine transla-
tion with scarce resources using morpho-syntactic in-
formation. Computational Linguistics, 30(2).
M. Popovic and H. Ney. 2004. Towards the use of word
stems and suffixes for statistical machine translation.
In LREC.
B. Roark and R. Sproat. 2007. Computational Ap-
proaches to Morphology and Syntax. Oxford Univer-
sity Press.
A.V. Rosti, S. Matsoukas, and R. Schwartz. 2007. Im-
proved word-level system combination for machine
translation. In ACL.
K.C. Sim, W. Byrne, M. Gales, H. Sahbi, and P. C. Wood-
land. 2007. Consensus network decoding for sta-
tistical machine translation system combination. In
ICASSP, volume 4.
D. Talbot and M. Osborne. 2006. Modelling lexical re-
dundancy for machine translation. In ACL.
N. Ueffing and H. Ney. 2003. Using POS information for
SMT into morphologically rich languages. In EACL.
S. Virpioja, J.J. Va?yrynen, M. Creutz, and M. Sadeniemi.
2007. Morphology-aware statistical machine transla-
tion based on morphs induced in an unsupervised man-
ner. In MT Summit XI.
A. Zollmann, A. Venugopal, and S. Vogel. 2006. Bridg-
ing the inflection morphology gap for Arabic statistical
machine translation. In HLT-NAACL: Short Papers.
76
Proceedings of the Third Workshop on Statistical Machine Translation, pages 131?134,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
European Language Translation with Weighted Finite State Transducers:
The CUED MT System for the 2008 ACL Workshop on SMT
Graeme Blackwood, Adria` de Gispert, Jamie Brunning, William Byrne
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
Trumpington Street, Cambridge, CB2 1PZ, U.K.
{gwb24|ad465|jjjb2|wjb31}@cam.ac.uk
Abstract
We describe the Cambridge University En-
gineering Department phrase-based statisti-
cal machine translation system for Spanish-
English and French-English translation in the
ACL 2008 Third Workshop on Statistical Ma-
chine Translation Shared Task. The CUED
system follows a generative model of trans-
lation and is implemented by composition of
component models realised as Weighted Fi-
nite State Transducers, without the use of a
special-purpose decoder. Details of system
tuning for both Europarl and News translation
tasks are provided.
1 Introduction
The Cambridge University Engineering Department
statistical machine translation system follows the
Transducer Translation Model (Kumar and Byrne,
2005; Kumar et al, 2006), a phrase-based generative
model of translation that applies a series of transfor-
mations specified by conditional probability distri-
butions and encoded as Weighted Finite State Trans-
ducers (Mohri et al, 2002).
The main advantages of this approach are its mod-
ularity, which facilitates the development and eval-
uation of each component individually, and its im-
plementation simplicity which allows us to focus on
modeling issues rather than complex decoding and
search algorithms. In addition, no special-purpose
decoder is required since standard WFST operations
can be used to obtain the 1-best translation or a lat-
tice of alternative hypotheses. Finally, the system
architecture readily extends to speech translation, in
which input ASR lattices can be translated in the
same way as for text (Mathias and Byrne, 2006).
This paper reviews the first participation of CUED
in the ACL Workshop on Statistical Machine Trans-
lation in 2008. It is organised as follows. Firstly,
section 2 describes the system architecture and its
main components. Section 3 gives details of the de-
velopment work conducted for this shared task and
results are reported and discussed in section 4. Fi-
nally, in section 5 we summarise our participation in
the task and outline directions for future work.
2 The Transducer Translation Model
Under the Transducer Translation Model, the gen-
eration of a target language sentence tJ1 starts with
the generation of a source language sentence sI1 by
the source language model PG(sI1). Next, the source
language sentence is segmented into phrases accord-
ing to the unweighted uniform phrasal segmenta-
tion model PW (uK1 ,K|sI1). This source phrase se-
quence generates a reordered target language phrase
sequence according to the phrase translation and re-
ordering model PR(xK1 |uK1 ). Next, target language
phrases are inserted into this sequence according to
the insertion model P?(vR1 |xK1 , uK1 ). Finally, the
sequence of reordered and inserted target language
phrases are transformed to word sequences tJ1 under
the target phrasal segmentation model P?(tJ1 |vR1 ).
These component distributions together form a joint
distribution over the source and target language sen-
tences and their possible intermediate phrase se-
quences as P (tJ1 , vR1 , xK1 , uK1 , sI1).
In translation under the generative model, we start
with the target sentence tJ1 in the foreign language
131
and search for the best source sentence s?I1. Encod-
ing each distribution as a WFST leads to a model of
translation as the series of compositions
L = G ? W ? R ? ? ?? ? T (1)
in which T is an acceptor for the target language
sentence and L is the word lattice of translations ob-
tained during decoding. The most likely translation
s?I1 is the path in L with least cost.
2.1 TTM Reordering Model
The TTM reordering model associates a jump se-
quence with each phrase pair. For the experi-
ments described in this paper, the jump sequence
is restricted such that only adjacent phrases can be
swapped; this is the MJ1 reordering model of (Ku-
mar and Byrne, 2005). Although the reordering
probability for each pair of phrases could be esti-
mated from word-aligned parallel data, we here as-
sume a uniform reordering probability p tuned as de-
scribed in section 3.1. Figure 1 shows how the MJ1
reordering model for a pair of phrases x1 and x2 is
implemented as a WFST.
0 1
x : x
x2 : x1
x1 : x2
p / b=+1
1 / b=?1
1?p / b=0
Figure 1: The uniform MJ1 reordering transducer.
3 System Development
CUED participated in two of the WMT shared task
tracks: French?English and Spanish?English. For
both tracks, primary and contrast systems were sub-
mitted. The primary submission was restricted
to only the parallel and language model data dis-
tributed for the shared task. The contrast submission
incorporates large additional quantities of English
monolingual training text for building the second-
pass language model described in section 3.2.
Table 1 summarises the parallel training data, in-
cluding the total number of sentences, total num-
ber of words, and lower-cased vocabulary size. The
Spanish and French parallel texts each contain ap-
proximately 5% News Commentary data; the rest
is Europarl data. Various single-reference develop-
ment and test sets were provided for each of the
tracks. However, the 2008 evaluation included a new
News task, for which no corresponding development
set was available.
sentences words vocab
FR 39.9M 124k
EN
1.33M 36.4M 106k
ES 38.2M 140k
EN 1.30M 35.7M 106k
Table 1: Parallel corpora statistics.
All of the training and system tuning was per-
formed using lower-cased data. Word alignments
were generated using GIZA++ (Och and Ney, 2003)
over a stemmed version of the parallel text. Stems
for each language were obtained using the Snowball
stemmer1. After unioning the Viterbi alignments,
the stems were replaced with their original words,
and phrase-pairs of up to five foreign words in length
were extracted in the usual fashion (Koehn et al,
2003).
3.1 System Tuning
Minimum error training (Och, 2003) under
BLEU (Papineni et al, 2001) was used to optimise
the feature weights of the decoder with respect
to the dev2006 development set. The following
features are optimized:
? Language model scale factor
? Word and phrase insertion penalties
? Reordering scale factor
? Insertion scale factor
? Translation model scale factor: u-to-v
? Translation model scale factor: v-to-u
? Three phrase pair count features
The phrase-pair count features track whether each
phrase-pair occurred once, twice, or more than twice
1Available at http://snowball.tartarus.org
132
in the parallel text (Bender et al, 2007). All de-
coding and minimum error training operations are
performed with WFSTs and implemented using the
OpenFST libraries (Allauzen et al, 2007).
3.2 English Language Models
Separate language models are used when translating
the Europarl and News sets. The models are esti-
mated using SRILM (Stolcke, 2002) and converted
to WFSTs for use in TTM translation. We use the of-
fline approximation in which failure transitions are
replaced with epsilons (Allauzen et al, 2003).
The Europarl language model is a Kneser-
Ney (Kneser and Ney, 1995) smoothed default-
cutoff 5-gram back-off language model estimated
over the concatenation of the Europarl and News
language model training data. The News language
model is created by optimising the interpolation
weights of two component models with respect to
the News Commentary development sets since we
believe these more closely match the newstest2008
domain. The optimised interpolation weights were
0.44 for the Europarl corpus and 0.56 for the much
smaller News Commentary corpus. For our contrast
submission, we rescore the first-pass translation lat-
tices with a large zero-cutoff stupid-backoff (Brants
et al, 2007) language model estimated over approx-
imately five billion words of newswire text.
4 Results and Discussion
Table 2 reports lower-cased BLEU scores for the
French?English and Spanish?English Europarl
and News translation tasks. The NIST scores are
also provided in parentheses. The row labelled
?TTM+MET? shows results obtained after TTM
translation and minimum error training, i.e. our pri-
mary submission constrained to use only the data
distributed for the task. The row labelled ?+5gram?
shows translation results obtained after rescoring
with the large zero-cutoff 5-gram language model
described in section 3.2. Since this includes addi-
tional language model data, it represents the CUED
contrast submission.
Translation quality for the ES?EN task is
slightly higher than that of FR?EN. For Europarl
translation, most of the additional English language
model training data incorporated into the 5-gram
rescoring step is out-of-domain and so does not sub-
stantially improve the scores. Rescoring yields an
average gain of just +0.5 BLEU points.
Translation quality is significantly lower in both
language pairs for the new news2008 set. Two fac-
tors may account for this. The first is the change
in domain and the fact that no training or devel-
opment set was available for the News translation
task. Secondly, the use of a much freer translation
in the single News reference, which makes it dif-
ficult to obtain a good BLEU score. However, the
second-pass 5-gram language model rescoring gains
are larger than those observed in the Europarl sets,
with approximately +1.7 BLEU points for each lan-
guage pair. The additional in-domain newswire data
clearly helps to improve translation quality.
Finally, we use a simple 3-gram casing model
trained on the true-case workshop distributed
language model data, and apply the SRILM
disambig tool to restore true-case for our final
submissions. With respect to the lower-cased scores,
true-casing drops around 1.0 BLEU in the Europarl
task, and around 1.7 BLEU in the News Commen-
tary and News tasks.
5 Summary
We have reviewed the Cambridge University Engi-
neering Department first participation in the work-
shop on machine translation using a phrase-based
SMT system implemented with a simple WFST ar-
chitecture. Results are largely competitive with the
state-of-the-art in this task.
Future work will examine whether further im-
provements can be obtained by incorporating addi-
tional features into MET, such as the word-to-word
Model 1 scores or phrasal segmentation models. The
MJ1 reordering model could also be extended to al-
low for longer-span phrase movement. Minimum
Bayes Risk decoding, which has been applied suc-
cessfully in other tasks, could also be included.
The difference in the gains from 5-gram lattice
rescoring suggests that, particularly for Europarl
translation, it is important to ensure the language
model data is in-domain. Some form of count mix-
ing or alternative language model adaptation tech-
niques may prove useful for unconstrained Europarl
translation.
133
Task dev2006 devtest2006 test2007 test2008 newstest2008
FR?EN TTM+MET 31.92 (7.650) 32.51 (7.719) 32.94 (7.805) 32.83 (7.799) 19.58 (6.108)
+5gram 32.51 (7.744) 32.96 (7.797) 33.33 (7.880) 33.03 (7.856) 21.22 (6.311)
ES?EN TTM+MET 33.11 (7.799) 32.25 (7.649) 32.90 (7.766) 33.11 (7.859) 20.99 (6.308)
+5gram 33.30 (7.835) 32.96 (7.740) 33.55 (7.857) 33.47 (7.893) 22.83 (6.513)
Table 2: Translation results for the Europarl and News tasks for various dev sets and the 2008 test sets.
Acknowledgements
This work was supported in part under the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of the 41st Meeting of
the Association for Computational Linguistics, pages
557?564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFST: a
general and efficient weighted finite-state transducer
library. In Proceedings of the 9th International Con-
ference on Implementation and Application of Au-
tomata, pages 11?23. Springer.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language trans-
lation system. In Proceedings of the 2007 Automatic
Speech Understanding Workshop, pages 396?401.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858?867.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech, and
Signal Processing, pages 181?184.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference for Computational Lin-
guistics on Human Language Technology, pages 48?
54, Morristown, NJ, USA.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 161?168.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lambert Mathias and William Byrne. 2006. Statistical
phrase-based speech translation. In 2006 IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language, vol-
ume 16, pages 69?88.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Meeting of the Association for Computational
Linguistics, pages 160?167, Morristown, NJ, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Meeting of the Association for Computational
Linguistics, pages 311?318, Morristown, NJ, USA.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
134
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 169?176, Vancouver, October 2005. c?2005 Association for Computational Linguistics
HMM Word and Phrase Alignment for Statistical Machine Translation
Yonggang Deng1 , William Byrne1,2
Center for Language and Speech Processing, Johns Hopkins University
Baltimore, MD 21210, USA 1
Machine Intelligence Lab, Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK 2
dengyg@jhu.edu , wjb31@cam.ac.uk
Abstract
HMM-based models are developed for the
alignment of words and phrases in bitext.
The models are formulated so that align-
ment and parameter estimation can be per-
formed efficiently. We find that Chinese-
English word alignment performance is
comparable to that of IBM Model-4 even
over large training bitexts. Phrase pairs
extracted from word alignments generated
under the model can also be used for
phrase-based translation, and in Chinese
to English and Arabic to English transla-
tion, performance is comparable to sys-
tems based on Model-4 alignments. Di-
rect phrase pair induction under the model
is described and shown to improve trans-
lation performance.
1 Introduction
Describing word alignment is one of the fundamen-
tal goals of Statistical Machine Translation (SMT).
Alignment specifies how word order changes when
a sentence is translated into another language, and
given a sentence and its translation, alignment spec-
ifies translation at the word level. It is straightfor-
ward to extend word alignment to phrase alignment:
two phrases align if their words align.
Deriving phrase pairs from word alignments is
now widely used in phrase-based SMT. Parameters
of a statistical word alignment model are estimated
from bitext, and the model is used to generate word
alignments over the same bitext. Phrase pairs are ex-
tracted from the aligned bitext and used in the SMT
system. With this approach the quality of the under-
lying word alignments can have a strong influence
on phrase-based SMT system performance. The
common practice therefore is to extract phrase pairs
from the best attainable word alignments. Currently,
Model-4 alignments (Brown and others, 1993) as
produced by GIZA++ (Och and Ney, 2000) are often
the best that can be obtained, especially with large
bitexts.
Despite its modeling power and widespread use,
Model-4 has shortcomings. Its formulation is such
that maximum likelihood parameter estimation and
bitext alignment are implemented by approximate,
hill-climbing, methods. Consequently parameter es-
timation can be slow, memory intensive, and diffi-
cult to parallelize. It is also difficult to compute
statistics under Model-4. This limits its usefulness
for modeling tasks other than the generation of word
alignments.
We describe an HMM alignment model devel-
oped as an alternative to Model-4. In the word align-
ment and phrase-based translation experiments to
be presented, its performance is comparable or im-
proved relative to Model-4. Practically, we can train
the model by the Forward-Backward algorithm, and
by parallelizing estimation, we can control memory
usage, reduce the time needed for training, and in-
crease the bitext used for training. We can also com-
pute statistics under the model in ways not practical
with Model-4, and we show the value of this in the
extraction of phrase pairs from bitext.
2 HMM Word and Phrase Alignment
Our goal is to develop a generative probabilistic
model of Word-to-Phrase (WtoP) alignment. We
start with an l-word source sentence e = el1, and an
169
m-word target sentence f = fm1 , which is realized
as a sequence of K phrases: f = vK1 .
Each phrase is generated as a translation of one
source word, which is determined by the alignment
sequence aK1 : eak ? vk . The length of each phrase
is specified by the process ?K1 , which is constrained
so that
?K
k=1 ?k = m.
We also allow target phrases to be inserted, i.e. to
be generated by a NULL source word. For this, we
define a binary hallucination sequence hK1 : if hk =
0, then NULL ? vk ; if hk = 1 then eak ? vk.
With all these quantities gathered into an align-
ment a = (?K1 , aK1 , hK1 ,K), the modeling objective
is to realize the conditional distribution P (f ,a|e).
With the assumption that P (f ,a|e) = 0 if f 6= vK1 ,
we write P (f ,a|e) = P (vK1 ,K, aK1 , hK1 , ?K1 |e) and
P (vK1 ,K, aK1 , hK1 , ?K1 |e)
= ?(m|l) ? P (K|m, e)
? P (aK1 , ?K1 , hK1 |K,m, e)
? P (vK1 |aK1 , hK1 , ?K1 ,K,m, e)
We now describe the component distributions.
Sentence Length ?(m|l) determines the target
sentence length. It is not needed during alignment,
where sentence lengths are known, and is ignored.
Phrase Count P (K|m, e) specifies the number of
target phrases. We use a simple, single parameter
distribution, with ? = 8.0 throughout
P (K|m, e) = P (K|m, l) ? ?K
Word-to-Phrase Alignment Alignment is a
Markov process that specifies the lengths of phrases
and their alignment with source words
P (aK1 , hK1 , ?K1 |K,m, e)
=
K
?
k=1
P (ak, hk, ?k|ak?1, ?k?1, e)
=
K
?
k=1
p(ak|ak?1, hk; l) d(hk)n(?k; eak )
The actual word-to-phrase alignment (ak) is a first-
order Markov process, as in HMM-based word-to-
word alignment (Vogel et al, 1996). It necessarily
depends on the hallucination variable
p(aj |aj?1, hj ; l)
=
?
?
?
1 aj = aj?1, hj = 0
0 aj 6= aj?1, hj = 0
a(aj |aj?1; l) hj = 1
This formulation allows target phrases to be in-
serted without disrupting the Markov dependencies
of phrases aligned to actual source words.
The phrase length model n(?; e) gives the proba-
bility that a word e produces a phrase with ? words
in the target language; n(?; e) is defined for ? =
1, ? ? ? , N . The hallucination process is a simple
i.i.d. process, where d(0) = p0, and d(1) = 1 ? p0.
Word-to-Phrase Translation The translation of
words to phrases is given as
P (vK1 |aK1 , hK1 , ?K1 ,K,m, e) =
K
?
k=1
p(vk|eak , hk, ?k)
We introduce the notation vk = vk[1], . . . , vk[?k]
and a dummy variable xk (for phrase insertion) :
xk =
{
eak hk = 1
NULL hk = 0
We define two models of word-to-phrase translation.
This simplest is based on context-independent word-
to-word translation
p(vk|eak , hk, ?k) =
?k
?
j=1
t(vk[j] |xk)
We also define a model that captures foreign word
context with bigram translation probabilities
p(vk|eak , hk, ?k)
= t(vk[1] |xk)
?k
?
j=2
t2(vk[j] | vk [j ? 1], xk)
Here, t(f |e) is the usual context independent word-
to-word translation probability. The bigram trans-
lation probability t2(f |f ?, e) specifies the likelihood
that target word f is to follow f ? in a phrase gener-
ated by source word e.
170
2.1 Properties of the Model and Prior Work
The formulation of the WtoP alignment model
was motivated by both the HMM word alignment
model (Vogel et al, 1996) and IBM Model-4 with
the goal of building on the strengths of each.
The relationship with the word-to-word HMM
alignment model is straightforward. For example,
constraining the phrase length component n(?; e)
to permit only phrases of one word would give a
word-to-word HMM alignment model. The exten-
sions introduced are the phrase count, and the phrase
length models, and the bigram translation distribu-
tion. The hallucination process is motivated by the
use of NULL alignments into Markov alignment
models as done by (Och and Ney, 2003).
The phrase length model is motivated by
Toutanova et al (2002) who introduced ?stay? prob-
abilities in HMM alignment as an alternative to word
fertility. By comparison, Word-to-Phrase HMM
alignment models contain detailed models of state
occupancy, motivated by the IBM fertility model,
which are more powerful than a single staying pa-
rameter. In fact, the WtoP model is a segmental
Hidden Markov Model (Ostendorf et al, 1996), in
which states emit observation sequences.
Comparison with Model-4 is less straightforward.
The main features of Model-4 are NULL source
words, source word fertility, and the distortion
model. The WtoP alignment model includes the
first two of these. However distortion, which al-
lows hypothesized words to be distributed through-
out the target sentence, is difficult to incorporate into
a model that supports efficient DP-based search. We
preserve efficiency in the WtoP model by insisting
that target words form connected phrases; this is not
as general as Model-4 distortion. This weakness
is somewhat offset by a more powerful (Markov)
alignment process as well as by the phrase count
distribution. Despite these differences, the WtoP
alignment model and Model-4 allow similar align-
ments. For example, in Fig. 1, Model-4 would allow
f
e
f
e
f f
1 2
1 2 3 4
Figure 1: Word-to-Word and Word-to-Phrase Links
f1, f3, and f4 to be generated by e1 with a fertility
of 3. Under the WtoP model, e1 could generate f1
and f3f4 with phrase lengths 1 and 2, respectively:
source words can generate more than one phrase.
This alignment could also be generated via four
single word foreign phrases. The balance between
word-to-word and word-to-phrase alignments is set
by the phrase count distribution parameter ?. As
? increases, alignments with shorter phrases are
favored, and for very large ? the model allows
only word-to-word alignments (see Fig. 2). Al-
though the WtoP alignment model is more com-
plex than the word-to-word HMM alignment model,
the Baum-Welch and Viterbi algorithms can still be
used. Word-to-word alignments are generated by
the Viterbi algorithm: a? = argmaxa P (f ,a|e); if
eak ? vk , eak is linked to all the words in vk.
The bigram translation probability relies on word
context, known to be helpful in translation (Berger
et al, 1996), to improve the identification of tar-
get phrases. As an example, f is the Chinese word
for ?world trade center?. Table 1 shows how the
likelihood of the correct English phrase is improved
with bigram translation probabilities; this example
is from the C?E, N=4 system of Table 2.
Model unigram bigram
P (world|f) 0.06 0.06
P (trade|world, f) 0.06 0.99
P (center|trade, f) 0.06 0.99
P (world trade center|f, 3) 0.0002 0.0588
Table 1: Context in Bigram Phrase Translation.
There are of course much prior work in translation
that incorporates phrases. Sumita et al (2004) de-
velop a model of phrase-to-phrase alignment, which
while based on HMM alignment process, appears
to be deficient. Marcu and Wong (2002) propose a
model to learn lexical correspondences at the phrase
level. To our knowledge, ours is the first non-
syntactic model of bitext alignment (as opposed to
translation) that links words and phrases.
3 Embedded Alignment Model Estimation
We now discuss estimation of the WtoP model pa-
rameters by the EM algorithm. Since the WtoP
model can be treated as an HMM with a very com-
plex state space, it is straightforward to apply Baum-
171
Welch parameter estimation. We show the forward
recursion as an example.
Given a sentence pair (el1, fm1 ), the forward prob-
ability ?j(i, ?) is defined as the probability of gen-
erating the first j target words with the added con-
dition that the target words f jj??+1 form a phrase
aligned to source word ei. It can be calculated recur-
sively (omitting the hallucination process, for sim-
plicity) as
?j(i, ?) =
{
?
i?,??
?j??(i?, ??)a(i|i?, l)
}
? ?
? n(?; ei) ? t(fj??+1|ei) ?
j
?
j?=j??+2
t2(fj? |ei)
This recursion is over a trellis of l(N + 1)m nodes.
Models are trained from a flat-start. We begin
with 10 iterations of EM to train Model-1, followed
by 5 EM iterations to train Model-2 (Brown and oth-
ers, 1993). We initialize the parameters of the word-
to-word HMM alignment model by collecting word
alignment counts from the Model-2 Viterbi align-
ments, and refine the word-to-word HMM alignment
model by 5 iterations of the Baum-Welch algorithm.
We increase the order of the WtoP model (N ) from
2 to the final value in increments of 1, by perform-
ing 5 Baum Welch iterations at each step. At the fi-
nal value of N , we introduce the bigram translation
probability; we use Witten-Bell smoothing (1991)
as a backoff strategy for t2, and other strategies are
possible.
4 Bitext Word Alignment
We now investigate bitext word alignment perfor-
mance. We start with the FBIS Chinese/English
parallel corpus which consists of approx. 10M En-
glish/7.5M Chinese words. The Chinese side of the
corpus is segmented into words by the LDC seg-
menter1. The alignment test set consists of 124 sen-
tences from the NIST 2001 dry-run MT-eval2 set that
are manually word aligned.
We first analyze the distribution of word links
within these manual alignments. Of the Chinese
words which are aligned to more than one English
words, 82% of these words align with consecutive
1http://www.ldc.upenn.edu/Projects/Chinese
2http://www.nist.gov/speech/tests/mt
Model AER1?1 AER1?N AER
C??E
Model-4 37.9 68.3 37.3
HMM, N=1 42.8 72.9 42.0
HMM, N=2 38.3 71.2 38.1
HMM, N=3 37.4 69.5 37.8
HMM, N=4 37.1 69.1 37.8
+ bigram t-table 37.5 65.8 37.1
E??C
Model-4 42.3 87.2 45.0
HMM, N=1 45.0 90.6 47.2
HMM, N=2 42.7 87.5 44.5
+ bigram t-table 44.2 85.5 45.1
Table 2: FBIS Bitext Alignment Error Rate.
2 4 6 8 10 121500
1850
2200
2550
2900
3250
3600
3950
?
# o
f hy
pot
hes
ize
d li
nks
0 1426
28
30
32
34
36
38
40
Ov
era
ll A
ER1?1 Links
1?N Links
Total Links
Overall AER
Figure 2: Balancing Word and Phrase Alignments
English words (phrases). In the other direction,
among all English words which are aligned to mul-
tiple Chinese words, 88% of these align to Chinese
phrases. In this collection, at least, word-to-phrase
alignments are plentiful.
Alignment performance is measured by the
Alignment Error Rate (AER) (Och and Ney, 2003)
AER(B;B?) = 1? 2 ? |B ?B?|/(|B?| + |B|)
where B is a set reference word links, and B? are the
word links generated automatically.
AER gives a general measure of word alignment
quality. We are also interested in how the model
performs over the word-to-word and word-to-phrase
alignments it supports. We split the reference align-
ments into two subsets: B1?1 contains word-to-
word reference links (e.g. 1?1 in Fig 1); and
B1?N contains word-to-phrase reference links (e.g.
1?3, 1?4 in Fig 1); The automatic alignment B?
is partitioned similarly. We define additional AERs:
AER1?1 = AER(B1?1, B?1?1), and AER1?N =
AER(B1?N , B?1?N ), which measure word-to-word
and word-to-phrase alignment, separately.
Table 2 presents the three AER measurements for
172
the WtoP alignment models trained as described in
Section 3. GIZA++ Model 4 alignment performance
is also presented for comparison. We note first that
the word-to-word HMM (N=1) alignment model is
worse than Model 4, as expected. For the WtoP
models in the C?E direction, we see reduced AER
for phrases lengths up to 4, although in the E?C di-
rection, AER is reduced only for phrases of length
2; performance for N > 2 is not reported.
In introducing the bigram phrase translation (the
bigram t-table), there is a tradeoff between word-
to-word and word-to-phrase alignment quality. As
mentioned, the bigram t-table increases the likeli-
hood of word-to-phrase alignments. In both transla-
tion directions, this reduces the AER1?N . However,
it also causes increases in AER1?1, primarily due to
a drop in recall: fewer word-to-word alignments are
produced. For C?E, this is not severe enough to
cause an overall AER increase; however, in E?C,
AER does increase.
Fig. 2 (C?E, N=4) shows how the 1-1 and 1-
N alignment behavior is balanced by the phrase
count parameter. As ? increases, the model favors
alignments with more word-to-word links and fewer
word-to-phrase links; the overall Alignment Error
Rate (AER) suggests a good balance at ? = 8.0.
After observing that the WtoP model performs as
well as Model-4 over the FBIS C-E bitext, we inves-
tigated performance over these large bitexts :
- ?NEWS? containing non-UN parallel Chi-
nese/English corpora from LDC (mainly FBIS, Xin-
hua, Hong Kong, Sinorama, and Chinese Treebank).
- ?NEWS+UN01-02? also including UN parallel
corpora from the years 2001 and 2002.
- ?ALL C-E? refers to all the C-E bitext available
from LDC as of his submission; this consists of the
NEWS corpora with the UN bitext from all years.
Over all these collections, WtoP alignment per-
formance (Table 3) is comparable to that of Model-
4. We do note a small degradation in the E?C WtoP
alignments. It is quite possible that this one-to-many
model suffers slightly with English as the source and
Chinese as the target, since English sentences tend to
be longer. Notably, simply increasing the amount of
bitext used in training need not improve AER. How-
ever, larger aligned bitexts can give improved phrase
pair coverage of the test set.
One of the desirable features of HMMs is that the
Bitext English Words Model C?E E?C
M-4 37.1 45.3NEWS 71M
WtoP 36.1 44.8
NEWS+ M-4 36.1 43.4
UN01-02 96M WtoP 36.4 44.2
ALL C-E 200M WtoP 36.8 44.7
Table 3: AER Over Large C-E Bitexts.
Forward-Backward steps can be run in parallel: bi-
text is partitioned; the Forward-Backward algorithm
is run over the subsets on different CPUs; statistics
are merged to reestimate model parameters. Parti-
tioning the bitext also reduces the memory usage,
since different cooccurrence tables can be kept for
each partition. With the ?ALL C-E? bitext collec-
tion, a single set of WtoP models (C?E, N=4, bi-
gram t-table) can be trained over 200M words of
Chinese-English bitext by splitting training over 40
CPUs; each Forward-Backward process takes less
than 2GB of memory and the training run finishes
in five days. By contrast, the 96M English word
NEWS+UN01-02 is about the largest C-E bitext
over which we can train Model-4 with our GIZA++
configuration and computing infrastructure.
Based on these and other experiments, in this pa-
per we set a maximum value of N = 4 for F?E; in
E?F, we set N=2 and omit the bigram phrase trans-
lation probability; ? is set to 8.0. We do not claim
that this is optimal, however.
5 Phrase Pair Induction
A common approach to phrase-based translation is
to extract an inventory of phrase pairs (PPI) from bi-
text (Koehn et al, 2003), For example, in the phrase-
extract algorithm (Och, 2002), a word alignment
a?m1 is generated over the bitext, and all word sub-
sequences ei2i1 and f
j2
j1 are found that satisfy :
a?m1 : a?j ? [i1, i2] iff j ? [j1, j2] . (1)
The PPI comprises all such phrase pairs (ei2i1 , f
j2
j1 ).
The process can be stated slightly differently.
First, we define a set of alignments :
A(i1, i2; j1, j2) = {am1 : aj ? [i1, i2] iff j ? [j1, j2]} .
If a?m1 ? A(i1, i2; j1, j2) then (ei2i1 , f
j2
j1 ) form a
phrase pair.
Viewed in this way, there are many possible align-
ments under which phrases might be paired, and
173
the selection of phrase pairs need not be based on
a single alignment. Rather than simply accepting a
phrase pair (ei2i1 , f
j2
j1 ) if the unique MAP alignment
satisfies Equation 1, we can assign a probability to
phrases occurring as translation pairs :
P (f , A(i1, i2; j1, j2 ) | e) =
?
a : am1 ?A(i1,i2;j1,j2 )
P (f ,a|e)
For a fixed set of indices i1, i2, j1, j2, the quan-
tity P (f , A(i1, i2; j1, j2 ) | e) can be computed effi-
ciently using a modified Forward algorithm. Since
P (f |e) can also be computed by the Forward al-
gorithm, the phrase-to-phrase posterior distribution
P (A(i1, i2; j1, j2 ) | f , e) is easily found.
PPI Induction Strategies In the phrase-extract
algorithm (Och, 2002), the alignment a? is gener-
ated as follows: Model-4 is trained in both directions
(e.g. F?E and E?F); two sets of word alignments
are generated by the Viterbi algorithm for each set
of models; and the two alignments are merged. This
forms a static aligned bitext. Next, all foreign word
sequences up to a given length (here, 5 words) are
extracted from the test set. For each of these, a
phrase pair is added to the PPI if the foreign phrase
can be found aligned to an English phrase under
Eq 1. We refer to the result as the Model-4 Viterbi
Phrase-Extract PPI.
Constructed in this way, the PPI is limited to
phrase pairs which can be found in the Viterbi align-
ments. Some foreign phrases which do appear in
the training bitext will not be included in the PPI
because suitable English phrases cannot be found.
To add these to the PPI we can use the phrase-to-
phrase posterior distribution to find English phrases
as candidate translations. This adds phrases to the
Viterbi Phrase-Extract PPI and increase the test set
coverage. A somewhat ad hoc PPI Augmentation
algorithm is given to the right.
Condition (A) extracts phrase pairs based on the
geometric mean of the E?F and F?E posteriors
(Tg = 0.01 throughout). The threshold Tp selects
additional phrase pairs under a more forgiving crite-
rion: as Tp decreases, more phrase pairs are added
and PPI coverage increases. Note that this algorithm
is constructed specifically to improve a Viterbi PPI;
it is certainly not the only way to extract phrase pairs
under the phrase-to-phrase posterior distribution.
Once the PPI phrase pairs are set, the phrase trans-
lation probabilities are set based on the number of
times each phrase pair is extracted from a sentence
pair, i.e. from relative frequencies.
For each foreign phrase v not in the Viterbi PPI :
For all pairs (fm1 , el1) and j1, j2 s.t. f
j2
j1 = v :
For 1 ? i1 ? i2 ? l, find
f(i1, i2) = PF?E(A(i1, i2; j1, j2) | el1, fm1 )
b(i1, i2) = PE?F (A(i1, i2; j1, j2) | el1, fm1 )
g(i1, i2) =
?
f(11, i2) b(i1, i2)
(?i1, i?2) = argmax
1?i1,i2?l
g(i1, i2) , and set u = ei?2i?1
Add (u, v) to the PPI if any of A, B, or C hold :
b(?i1, i?2) ? Tg and f (?i1, i?2) ? Tg (A)
b(?i1, i?2) < Tg and f (?i1, i?2) > Tp (B)
f (?i1, i?2) < Tg and b(?i1, i?2) > Tp (C)
PPI Augmentation via Phrase-Posterior Induction
HMM-based models are often used if posterior
distributions are needed. Model-1 can also be used
in this way (Venugopal et al, 2003), although it is
a relatively weak alignment model. By comparison,
finding posterior distributions under Model-4 is dif-
ficult. The Word-to-Phrase alignment model appears
not to suffer this tradeoff: it is a good model of word
alignment under which statistics such as the phrase-
to-phrase posterior can be calculated.
6 Translation Experiments
We evaluate the quality of phrase pairs extracted
from the bitext through the translation performance
of the Translation Template Model (TTM) (Kumar
et al, 2005), which is a phrase-based translation sys-
tem implemented using weighted finite state trans-
ducers. Performance is measured by BLEU (Pap-
ineni and others, 2001).
Chinese?English Translation We report perfor-
mance on the NIST Chinese/English 2002, 2003 and
2004 (News only) MT evaluation sets. These consist
of 878, 919, and 901 sentences, respectively. Each
Chinese sentence has 4 reference translations.
We evaluate two C?E translation systems. The
smaller system is built on the FBIS C-E bitext col-
lection. The language model used for this system is
a trigram word language model estimated with 21M
174
V-PE WtoP eval02 eval03 eval04 eval02 eval03 eval04
Model Tp cvg BLEU cvg BLEU cvg BLEU cvg BLEU cvg BLEU cvg BLEU
FBIS C?E System News A?E System
1 M-4 - 20.1 23.8 17.7 22.8 20.2 23.0 19.5 36.9 21.5 39.1 18.5 40.0
2 0.7 24.6 24.6 21.4 23.7 24.6 23.7 23.8 37.6 26.6 40.2 22.4 40.3
3 WtoP - 19.7 23.9 17.4 23.3 19.8 23.3 18.4 36.2 20.6 38.6 17.4 39.2
4 1.0 23.1 24.0 20.0 23.7 23.2 23.5 21.8 36.7 24.3 39.3 20.4 39.7
5 0.9 24.0 24.8 20.9 23.9 24.0 23.8 23.2 37.2 25.8 39.7 21.8 40.1
6 0.7 24.6 24.9 21.3 24.0 24.7 23.9 23.7 37.2 26.5 39.7 22.4 39.9
7 0.5 24.9 24.9 21.6 24.1 24.8 23.9 24.0 37.2 26.9 39.7 22.7 39.8
Large C?E System Large A?E System
8 M-4 - 32.5 27.7 29.3 27.1 32.5 26.6 26.4 38.1 28.1 40.1 28.2 39.9
9 WtoP - 30.6 27.9 27.5 27.0 30.6 26.4 24.8 38.1 26.6 40.1 26.7 40.6
10 0.7 38.2 28.2 32.3 27.3 37.1 26.8 30.7 39.3 32.9 41.6 32.5 41.9
Table 4: Translation Analysis and Performance of PPI Extraction Procedures
words taken from the English side of the bitext; all
language models are built with the SRILM toolkit
using Kneser-Ney smoothing (Stolcke, 2002).
The larger system is based on alignments gener-
ated over all available C-E bitext (the ?ALL C-E?
collection of Section 4). The language model is
an equal-weight interpolated trigram model trained
over 373M English words taken from the English
side of the bitext and the LDC Gigaword corpus.
Arabic?English Translation We also evaluate our
WtoP alignment models in Arabic-English transla-
tion. We report results on a small and a large system.
In each, Arabic text is tokenized by the Buckwalter
analyzer provided by LDC. We test our models on
NIST Arabic/English 2002, 2003 and 2004 (News
only) MT evaluation sets that consists of 1043, 663
and 707 Arabic sentences, respectively. Each Arabic
sentence has 4 reference translations.
In the small system, the training bitext is from
A-E News parallel text, with ?3.5M words on the
English side. We follow the same training proce-
dure and configurations as in Chinese/English sys-
tem in both translation directions. The language
model is an equal-weight interpolated trigram built
over ?400M words from the English side of the bi-
text, including UN text, and the LDC English Gi-
gaword collection. The large Arabic/English system
employs the same language model. Alignments are
generated over all A-E bitext available from LDC as
of this submission; this consists of approx. 130M
words on the English side.
WtoP Model and Model-4 Comparison We first
look at translation performance of the small A?E
and C?E systems, where alignment models are
trained over the smaller bitext collections. The base-
line systems (Table 4, line 1) are based on Model-4
Viterbi Phrase-Extract PPIs.
We compare WtoP alignments directly to Model-
4 alignments by extracting PPIs from the WtoP
alignments using the Viterbi Phrase-Extract proce-
dure (Table 4, line 3). In C?E translation, perfor-
mance is comparable to that of Model-4; in A?E
translation, performance lags slightly. As we add
phrase pairs to the WtoP Viterbi Phrase-Extract PPI
via the Phrase-Posterior Augmentation procedure
(Table 4, lines 4-7), we obtain a ?1% improvement
in BLEU; the value of Tp = 0.7 gives improvements
across all sets. In C?E translation, this yields good
gains relative to Model-4, while in A?E we match
or improve the Model-4 performance.
The performance gains through PPI augmentation
are consistent with increased PPI coverage of the test
set. We tabulate the percentage of test set phrases
that appear in each of the PPIs (the ?cvg? values
in Table 4). The augmentation scheme is designed
specifically to increase coverage, and we find that
BLEU score improvements track the phrase cover-
age of the test set. This is further confirmed by the
experiment of Table 4, line 2 in which we take the
PPI extracted from Model-4 Viterbi alignments, and
add phrase pairs to it using the Phrase-Posterior aug-
mentation scheme with Tp = 0.7. We find that the
augmentation scheme under the WtoP models can
be used to improve the Model-4 PPI itself.
We also investigate C?E and A?E translation
performance with PPIs extracted from large bitexts.
175
Performance of systems based on Model-4 Viterbi
Phrase-Extract PPIs is shown in Table 4, line 8.
To train Model-4 using GIZA++, we split the bi-
texts into two (A-E) or three (C-E) partitions, and
train models for each division separately; we find
that memory usage is otherwise too great. These
serve as a single set of alignments for the bitext,
as if they had been generated under a single align-
ment model. When we translate with Viterbi Phrase-
Extract PPIs taken from WtoP alignments created
over all available bitext, we find comparable perfor-
mance to the Model-4 baseline (Table 4, line 9). Us-
ing the Phrase-Posterior augmentation scheme with
Tp = 0.7 yields further improvement (Table 4, line
10). Pooling the sets to form two large C?E and
A?E test sets, the A?E system improvements are
significant at a 95% level (Och, 2003); the C?E sys-
tems are only equivalent.
7 Conclusion
We have described word-to-phrase alignment mod-
els capable of good quality bitext word alignment.
In Arabic-English and Chinese-English translation
and alignment they compare well to Model-4, even
with large bitexts. The model architecture was in-
spired by features of Model-4, such as fertility and
distortion, but care was taken to ensure that dy-
namic programming procedures, such as EM and
Viterbi alignment, could still be performed. There
is practical value in this: training and alignment
are easily parallelized. Working with HMMs also
makes it straightforward to explore new modeling
approaches. We show an augmentation scheme that
adds to phrases extracted from Viterbi alignments;
this improves translation with both the WtoP and the
Model-4 phrase pairs, even though it would be infea-
sible to implement the scheme under Model-4 itself.
We note that these models are still relatively simple,
and we anticipate further alignment and translation
improvement as the models are refined.
Acknowledgments The TTM translation system was provided
by Shankar Kumar. This work was funded by ONR MURI
Grant N00014-01-1-0685.
References
A. L. Berger, S. Della Pietra, and V. J. Della Pietra. 1996.
A maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
P. F. Brown et al 1993. The mathematics of machine
translation: Parameter estimation. Computational Lin-
guistics, 19:263?312.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL.
S. Kumar, Y. Deng, and W. Byrne. 2005. A weighted fi-
nite state transducer translation template model for sta-
tistical machine translation. Journal of Natural Lan-
guage Engineering, 11(3).
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP.
F. Och and H. Ney. 2000. Improved statistical alignment
models. In Proc. of ACL, Hong Kong, China.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. Och. 2002. Statistical Machine Translation: From
Single Word Models to Alignment Templates. Ph.D.
thesis, RWTH Aachen, Germany.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
M. Ostendorf, V. Digalakis, and O. Kimball. 1996. From
HMMs to segment models: a unified view of stochas-
tic modeling for speech recognition. IEEE Trans.
Acoustics, Speech and Signal Processing, 4:360?378.
K. Papineni et al 2001. BLEU: a method for automatic
evaluation of machine translation. Technical Report
RC22176 (W0109-022), IBM Research Division.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. ICSLP.
E. Sumita et al 2004. EBMT, SMT, Hybrid and More:
ATR spoken language translation system. In Proc.
of the International Workshop on Spoken Language
Translation, Kyoto, Japan.
K. Toutanova, H. T. Ilhan, and C. Manning. 2002. Exten-
tions to HMM-based statistical word alignment mod-
els. In Proc. of EMNLP.
A. Venugopal, S. Vogel, and A. Waibel. 2003. Effective
phrase translation extraction from alignment models.
In Proc. of ACL.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based
word alignment in statistical translation. In Proc. of
the COLING.
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. In IEEE Trans. Inform
Theory, volume 37, pages 1085?1094, July.
176
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 265?268,
New York City, June 2006. c?2006 Association for Computational Linguistics
MTTK: An Alignment Toolkit for Statistical Machine Translation
Yonggang Deng 1
Center for Language and Speech Processing1
Johns Hopkins University
Baltimore, MD 21218
dengyg@jhu.edu
William Byrne1,2
Machine Intelligence Lab 2
Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK
wjb31@cam.ac.uk
Abstract
The MTTK alignment toolkit for statisti-
cal machine translation can be used for
word, phrase, and sentence alignment of
parallel documents. It is designed mainly
for building statistical machine translation
systems, but can be exploited in other
multi-lingual applications. It provides
computationally efficient alignment and
estimation procedures that can be used
for the unsupervised alignment of parallel
text collections in a language independent
fashion. MTTK Version 1.0 is available
under the Open Source Educational Com-
munity License.
1 Introduction
Parallel text alignment procedures attempt to iden-
tify translation equivalences within collections of
translated documents. This can be be done at various
levels. At the finest level, this involves the alignment
of words and phrases within two sentences that are
known to be translations (Brown et al, 1993; Och
and Ney, 2003; Vogel et al, 1996; Deng and Byrne,
2005). Another task is the identification and align-
ment of sentence-level segments within document
pairs that are known to be translations (Gale and
Church, 1991); this is referred to as sentence-level
alignment, although it may also involve the align-
ment of sub-sentential segments (Deng et al, ) as
well as the identification of long segments in either
document which are not translations. There is also
document level translation which involves the iden-
tification of translated document pairs in a collection
of documents in multiple languages. As an example,
Figure 1 shows parallel Chinese/English text that is
aligned at the sentence, word, and phrase levels.
Parallel text plays a crucial role in multi-lingual
natural language processing research. In particu-
lar, statistical machine translation systems require
collections of sentence pairs (or sentence fragment
pairs) as the basic ingredients for building statistical
word and phrase alignment models. However, with
the increasing availability of parallel text, human-
created alignments are expensive and often unaf-
fordable for practical systems, even at a small scale.
High quality automatic alignment of parallel text has
therefore become indispensable. In addition to good
alignment quality, several other properties are also
desirable in automatic alignment systems. Ideally,
these should be general-purpose and language in-
dependent, capable of aligning very different lan-
guages, such as English, French, Chinese, German
and Arabic, to give a few examples of current in-
terest. If the alignment system is based on statis-
tical models, the model parameters should be esti-
mated from scratch, in an unsupervised manner from
whatever parallel text is available. To process mil-
lions of sentence pairs, these models need to be ca-
pable of generalization and the alignment and esti-
mation algorithms should be computationally effi-
cient. Finally, since noisy mismatched text is often
found in real data, such as parallel text mined from
web pages, automatic alignment needs to be robust.
There are systems available for these purposes, no-
tably the GIZA++ (Och and Ney, 2003) toolkit and
265
! " # $ % & ' () , * +,
$ % '- , ./ 01 &2 .
It is necessary to resolutely remove obstacles in
rivers and lakes .
3 4 56 78 9: , ;< => .
4 . It is necessary to strengthen monitoring and
forecast work and scientifically dispatch people and
materials .
! ?@ AB CD , EFGH IJ 9
: K> .
It is necessary to take effective measures and try by
every possible means to provide precision forecast .
L M ! NO PQ RS 9: FT ,
A U*V W XY() .
Before the flood season comes , it is necessary to
seize the time to formulate plans for forecasting
floods and to carry out work with clear
 	
Figure 1: Chinese/English Parallel Corpus Aligned at the Sentence, Word, and Phrase Levels: horizontal
lines denote the segmentations of a sentence alignment and arrows denote a word-level mapping.
the Champollion Toolkit (Ma et al, 2004).
This demo introduces MTTK, the Machine Trans-
lation Toolkit. The toolkit can be used to train statis-
tical models and perform parallel text alignment at
different levels. Target applications include not only
machine translation, but also bilingual lexicon in-
duction, cross lingual information retrieval and other
multi-lingual applications.
2 MTTK Components
MTTK is a collection of C++ programs and Perl
and shell scripts that can be used to build statisti-
cal alignment models from parallel text. Respective
of the text to be aligned, MTTK?s functions are cat-
egorized into the following two main parts.
2.1 Chunk Alignment
Chunk alignment aims to extract sentence or sub-
sentence pairs from parallel corpora. A chunk
can be multiple sentences, a sentence or a sub-
sentence, as required by the application. Two align-
ment procedures are implemented: one is the widely
used dynamic programming procedure that derives
monotone alignment of sentence segments (Gale
and Church, 1991); the other is divisive clustering
procedure that begins by finding coarse alignments
that are then iteratively refined by successive binary
splitting (Deng et al, ). These two types of align-
ment procedures complement each other. They can
be used together to improve the overall sentence
alignment quality.
When translation lexicons are not available,
chunk alignment can be performed using length-
based statistics. This usually can serve as a start-
ing point of sentence alignment. Alignment qual-
ity can be further improved when the chunking pro-
cedure is based on translation lexicons from IBM
Model-1 alignment model (Brown et al, 1993). The
MTTK toolkit also generates alignment score for
each chunk pair, that can be utilized in post process-
ing, for example in filtering out aligned segments of
dubious quality.
2.2 Word and Phrase Alignment
After a collection of sentence or sub-sentence pairs
are extracted via chunk alignment procedures, sta-
tistical word and phrase alignment models can be
estimated with EM algorithms. MTTK provides im-
plementations of various alignment, models includ-
ing IBM Model-1, Model-2 (Brown et al, 1993),
HMM-based word-to-word alignment model (Vogel
et al, 1996; Och and Ney, 2003) and HMM-based
word-to-phrase alignment model (Deng and Byrne,
2005). After model parameters are estimated, the
Viterbi word alignments can be derived. A novel
computation performed by MTTK is the genera-
266
tion of model-based phrase pair posterior distribu-
tions (Deng and Byrne, 2005), which plays an im-
portant role in extracting a phrase-to-phrase transla-
tion probabilities.
3 MTTK Features
MTTK is designed to process huge amounts of par-
allel text. Model parameter estimation can be car-
ried out parallel during EM training using multiple
CPUs. The entire parallel text is split into parts.
During each E-step , statistics are collected paral-
lel over each part, while in the M-steps, these statis-
tics are merged together to update model parame-
ters for next iteration. This parallel implementation
not only reduces model training time significantly,
it also avoids memory usage issues that arise in pro-
cessing millions of sentence pairs, since each E-Step
need only save and process co-occurrence that ap-
pears in its part of the parallel text. This enables
building a single model from many millions of sen-
tence pairs.
Another feature of MTTK is language indepen-
dence. Linguistic knowledge is not required during
model training, although when it is available, per-
formance can be improved. Statistical parameters
are estimated and learned automatically from data
in an unsupervised way. To accommodate language
diversity, there are several parameters in MTTK that
can be tuned for individual applications to optimize
performance.
4 A Typical Application of MTTK in
Parallel Text Alignment
A typical example of using MTTK is give in Fig-
ure 2. It starts with a collection of document pairs.
During pre-processing, documents are normalized
and tokenized into token sequences. This prepro-
cessing is carried out before using the MTTK, and
is usually language dependent, requiring, for exam-
ple, segmenting Chinese characters into words or ap-
plying morphological analyzing to Arabic word se-
quences.
Statistical models are then built from scratch.
Chunk alignment begins with length statistics that
can be simply obtained by counting the number of
tokens on in each language. The chunk aligning
procedure then applies dynamic programming to de-
rive a sentence alignment. After sorting the gener-
ated sentence pairs by their probabilities, high qual-
ity sentence pairs are then selected and used to train
a translation lexicon. As an input for next round
chunk alignment, more and better sentence pairs can
be extracted and serve as training material for a bet-
ter translation lexicon. This bootstrapping procedure
identifies high quality sentence pairs in an iterative
fashion.
To maximize the number of training words for
building word and phrase alignment models, long
sentence pairs are then processed further using a di-
visive clustering chunk procedure that derives chunk
pairs at the sub-sentence level. This provides addi-
tional translation training pairs that would otherwise
be discarded as being overly long.
Once all usable chunk pairs are identified in the
chunk alignment procedure, word alignment model
training starts with IBM Model-1. Model com-
plexity increases gradually to Model-2, and then
HMM-based word-to-word alignment model, and
finally to HMM-based word-to-phrase alignment
model (Deng and Byrne, 2005). With these models,
word alignments can be obtained using the Viterbi
algorithm, and phrase pair posterior distributions
can be computed in building a phrase translation ta-
ble.
In published experiments we have found that
MTTK generates alignments of quality comparable
to those generated by GIZA++, where alignment
quality is measured both directly in terms of Align-
ment Error Rate relative to human word alignments
and indirectly through the translation performance
of systems constructed from the alignments (Deng
and Byrne, 2005). We have used MTTK as the basis
of translation systems entered into the recent NIST
Arabic-English and Chinese-English MT Evalua-
tions as well as the TC-STAR Chinese-English MT
evaluation (NIST, 2005; TC-STAR, 2005).
5 Availability
MTTK Version 1.0 is released under the Open
Source Educational Community License1.
The tools and documentation are available at
http://mi.eng.cam.ac.uk/?wjb31/distrib/mttkv1/ .
1http://www.opensource.org/licenses/ecl1.php
267
{ t(f|e), a(i|j;l,m) }
WtW HMM Training
{ t(f|e), P(i|i?;l) }
AlignSHmm
{ t(f|e), P(i|i?;l), n(phi;e), t2(f|f?,e) }
Model?1 Training
{ t(f|e) }
Model?2 Training
WtP HMM Training w/ Bigram t?table
{ t(f|e), P(i|i?;l), n(phi;e) }
WtP HMM Training N=2,3...
Length Statistics
AlignHmm
AlignM2
AlignM1
AlignSHmm
PPEM
PPEM
PPEHmm
PPEHmm
PPEHmm
High Quality Pairs
Model?1 Training{ t(f|e) }
FilterChunk Alignments
W
ord A
lignm
ents
Phrase A
lignm
ents
Document Alignments
Bitext Chunking
Figure 2: A Typical Unsupervised Translation Alignment Procedure with MTTK.
6 Acknowledgements
Funded by ONR MURI Grant N00014-01-1-0685.
References
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of machine transla-
tion: Parameter estimation. Computational Linguis-
tics, 19:263?312.
Y. Deng and W. Byrne. 2005. Hmm word and phrase
alignment for statistical machine translation. In Proc.
of HLT-EMNLP.
Y. Deng, S. Kumar, and W. Byrne. Segmentation and
alignment of parallel text for statistical machine trans-
lation. Journal of Natural Language Engineering. to
appear.
W. A. Gale and K. W. Church. 1991. A program for
aligning sentences in bilingual corpora. In Meeting of
the Association for Computational Linguistics, pages
177?184.
X. Ma, C. Cieri, and D. Miller. 2004. Corpora & tools
for machine translation. In Machine Translation Eval-
uation Workshop, Alexandria, VA. NIST.
NIST, 2005. The NIST Machine Translation Eval-
uations Workshop. North Bethesda, MD, June.
http://www.nist.gov/speech/tests/summaries/2005/mt05.htm.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
TC-STAR, 2005. TC-STAR Speech-to-Speech Trans-
lation Evaluation Meeting. Trento, Italy, April.
http://www.tc-star.org/.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based
word alignment in statistical translation. In Proc. of
the COLING.
268
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 71?79,
Beijing, August 2010
Fluency Constraints for Minimum Bayes-Risk Decoding
of Statistical Machine Translation Lattices
Graeme Blackwood and Adria` de Gispert and William Byrne
Machine Intelligence Laboratory, Department of Engineering, Cambridge University
{gwb24|ad465|wjb31}@cam.ac.uk
Abstract
A novel and robust approach to improv-
ing statistical machine translation fluency
is developed within a minimum Bayes-
risk decoding framework. By segment-
ing translation lattices according to con-
fidence measures over the maximum like-
lihood translation hypothesis we are able
to focus on regions with potential transla-
tion errors. Hypothesis space constraints
based on monolingual coverage are ap-
plied to the low confidence regions to im-
prove overall translation fluency.
1 Introduction and Motivation
Translation quality is often described in terms of
fluency and adequacy. Fluency reflects the ?na-
tiveness? of the translation while adequacy indi-
cates how well a translation captures the meaning
of the original text (Ma and Cieri, 2006).
From a purely utilitarian view, adequacy should
be more important than fluency. But fluency and
adequacy are subjective and not easy to tease apart
(Callison-Burch et al, 2009; Vilar et al, 2007).
There is a human tendency to rate less fluent trans-
lations as less adequate. One explanation is that
errors in grammar cause readers to be more crit-
ical. A related phenomenon is that the nature of
translation errors changes as fluency improves so
that any errors in fluent translations must be rel-
atively subtle. It is therefore not enough to fo-
cus solely on adequacy. SMT systems must also
be fluent if they are to be accepted and trusted.
It is possible that the reliance on automatic met-
rics may have led SMT researchers to pay insuffi-
cient attention to fluency: BLEU (Papineni et al,
2002), TER (Snover et al, 2006), and METEOR
(Lavie and Denkowski, 2009) show broad corre-
lation with human rankings of MT quality, but are
incapable of fine distinctions between fluency and
adequacy.
There is concern that the fluency of current
SMT is inadequate (Knight, 2007b). SMT is ro-
bust, in that a translation is nearly always pro-
duced. But unlike translators who should be
skilled in at least one of the languages, SMT sys-
tems are limited in both source and target lan-
guage competence. Fluency and accuracy there-
fore tend to suffer together as translation quality
degrades. This should not be the case. Ideally, an
SMT system should never be any less fluent than
the best stochastic text generation system avail-
able in the target language (Oberlander and Brew,
2000). What is needed is a good way to enhance
the fluency of SMT hypotheses.
The maximum likelihood (ML) formulation
(Brown et al, 1990) of translation of source lan-
guage sentence F to target language sentence E?
E? = argmax
E
P (F |E)P (E) (1)
makes it clear why improving SMT fluency is a
difficult modelling problem. The language model
P (E), the closest thing to a ?fluency component?
in the original formulation, only affects candidates
likely under the translation model P (F |E). Given
the weakness of current translation models this is
a severe limitation. It often happens that SMT sys-
tems assign P (F |E?) = 0 to a correct reference
translation E? of F (see the discussion in Section
9). The problem is that in ML decoding the lan-
guage model can only encourage the production
of fluent translations; it cannot easily enforce con-
straints on fluency or introduce new hypotheses.
In Hiero (Chiang, 2007) and syntax-based SMT
(Knight and Graehl, 2005; Knight, 2007a), the
primary role of syntax is to drive the translation
process. Translations produced by these systems
respect the syntax of their translation models, but
71
this does not force them to be grammatical in the
way that a typical human sentence is grammati-
cal; they produce many translations which are not
fluent. The problem is robustness. Generating
fluent translations demands a tightly constraining
target language grammar but such a grammar is at
odds with broad-coverage parsing needed for ro-
bust translation.
We have described two problems in transla-
tion fluency: (1) SMT may fail to generate flu-
ent hypotheses and there is no simple way to in-
troduce them into the search; (2) SMT produces
many translations which are not fluent but enforc-
ing constraints to improve fluency can hurt robust-
ness. Both problems are rooted in the ML decod-
ing framework in which robustness and fluency
are conflicting objectives.
We propose a novel framework to improve the
fluency of any SMT system, whether syntactic or
phrase-based. We will perform Minimum Bayes-
risk search (Kumar and Byrne, 2004) over a space
of fluent hypotheses H:
E?MBR = argmin
E??H
?
E?E
L(E,E?)P (E|F ) (2)
In this approach the MBR evidence space E is
generated by an SMT system as a k-best list or lat-
tice. The system runs in its best possible config-
uration, ensuring both translation robustness and
good baselines. Rather than decoding in the out-
put of the baseline SMT system, translations will
be sought among a collection of fluent sentences
that are close to the top SMT hypotheses as deter-
mined by the loss function L(E,E?).
Decoupling the MBR hypothesis space from
first-pass translation offers great flexibility. Hy-
potheses in H may be arbitrarily constrained ac-
cording to lexical, syntactic, semantic, or other
considerations, with no effect on translation ro-
bustness. This is because constraints on fluency
do not affect the production of the evidence space
by the baseline system. Robustness and fluency
are no longer conflicting objectives. This frame-
work also allows the MBR hypothesis space to be
augmented with hypotheses produced by an NLG
system, although this is beyond the scope of the
present paper.
This paper focuses on searching out fluent
strings amongst the vast number of hypotheses en-
coded in SMT lattices. Oracle BLEU scores com-
puted over k-best lists (Och et al, 2004) show
that many high quality hypotheses are produced
by first-pass SMT decoding. We propose reducing
the difficulty of enhancing the fluency of complete
hypotheses by first identifying regions of high-
confidence in the ML translations and using these
to guide the fluency refinement process. This has
two advantages: (1) we keep portions of the base-
line hypotheses that we trust and search for alter-
natives elsewhere, and (2) the task is made much
easier since the fluency of sentence fragments can
be refined in context.
In what follows, we use posterior probabilities
over SMT lattices to identify useful subsequences
in the ML translations (Sections 2 & 3). These
subsequences drive the segmentation and transfor-
mation of lattices into smaller subproblems (Sec-
tions 4 & 5). Subproblems are mined for fluent
strings (Section 6), resulting in improved transla-
tion fluency (Sections 7 & 8). Our results show
that, when guided by the careful selection of sub-
problems, fluency can be improved with no real
degradation of the BLEU score.
2 Lattice MBR Decoding
The formulation of the MBR decoder in Equation
(2) separates the hypothesis space from the evi-
dence space. We apply the linearised lattice MBR
decision rule (Tromble et al, 2008)
E?LMBR = argmax
E??H
{
?0|E?|+
?
u?N
?u#u(E?)p(u|E)
}
,
(3)
whereH is the hypothesis space, E is the evidence
space, N is the set of all n-grams in H (typically,
n = 1 . . . 4), and ? are constants estimated on
held-out data. The quantity p(u|E) is the path pos-
terior probability of n-gram u
p(u|E) =
?
E?Eu
P (E|F ), (4)
where Eu = {E ? E : #u(E) > 0} is the sub-
set of paths containing n-gram u at least once.
The path posterior probabilities p(u|E) of Equa-
tion (4) can be efficiently calculated (Blackwood
et al, 2010) using general purpose WFST opera-
tions (Mohri et al, 2002).
72
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Posterior probability threshold ?
A
v
e
r
a
g
e
 
p
e
r
?
s
e
n
t
e
n
c
e
 
p
r
e
c
is
io
n 
p n
,
?
 
 
1?gram
2?gram
3?gram
4?gram
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
5
10
15
20
25
30
Posterior probability threshold ?
A
v
e
r
a
g
e
 
p
e
r
?
s
e
n
t
e
n
c
e
 
n
?
g
r
a
m
 
c
o
u
n
t
s
 
 
1?grams
2?grams
3?grams
4?grams
Figure 1: Average n-gram precisions (left) and counts (right) for 2075 sentences of NIST
Arabic?English ML translations at a range of posterior probability thresholds 0 ? ? ? 1. The left
plot shows at ? = 0 the n-gram precisions used in the BLEU score of the ML baseline system.
3 Posterior Probability Confidence
Measures
In the formulation of Equations (3) and (4) the
path posterior n-gram probabilities play a crucial
role. MBR decoding under the linear approxima-
tion to BLEU is driven mainly by the presence
of high posterior n-grams in the lattice; the low
posterior n-grams contribute relatively little to the
MBR decision criterion. Here we investigate the
predictive power of these statistics. We will show
that the n-gram posterior is a good predictor as to
whether or not an n-gram is to be found in a set of
reference translations.
Let Nn denote the set of n-grams of order n
in the ML hypothesis E?, and let Rn denote the
set of n-grams of order n in the union of the ref-
erences. For confidence threshold ?, let Nn,? =
{u?Nn : p(u|E) ? ?} denote the n-grams inNn
with posterior probability greater than or equal to
?, where p(u|E) is computed using Equation (4).
This is equivalent to identifying all substrings of
length n in the translation hypotheses for which
the system assigns a posterior probability of ? or
higher. The precision at order n for threshold ? is
the proportion of n-grams in Nn,? also present in
the references:
Pn,? =
|Rn ? Nn,?|
|Nn,?|
(5)
The left plot in Figure 1 shows average per-
sentence n-gram precisions Pn,? at orders 1. . .4
for an Arabic?English translation task at a range
of thresholds 0 ? ? ? 1. Sentence start and end
tokens are ignored when computing unigram pre-
cisions. We note that precision at all orders im-
proves as the threshold ? increases. This confirms
that these intrinsic measures of translation confi-
dence have strong predictive power.
The right-hand side of the figure shows the av-
erage number of n-grams per sentence for the
same range of ?. We see that for high ?, there are
few n-grams with p(u|E) ? ?; this is as expected.
However, even at a high threshold of ? = 0.9
there are still on average three 4-grams per sen-
tence with posterior probabilities that exceed ?.
Even at this very high confidence level, high pos-
terior n-grams occur frequently enough that we
can expect them to be useful.
These precision results motivate our use of path
posterior n-gram probabilities as a confidence
measure. We assign confidence p(E?ji |E) to sub-
sequences E?i . . . E?j of the ML hypothesis.
Prior work focuses on word-level confidence
extracted from k-best lists and lattices (Ueffing
and Ney, 2007), while Zens and Ney (2006)
rescore k-best lists with n-gram posterior proba-
bilities. Similar experiments with a slightly dif-
ferent motivation are reported by DeNero et al
(2009); they show that expected n-gram counts in
a lattice can be used to predict which n-grams ap-
pear in the references.
4 Lattice Segmentation
We have shown that current SMT systems, al-
though flawed, can identify with confidence par-
73
the newspaper ? constitution ? quoted brigadier abdullah krishan , the chief of police in karak governorate ( 521 km
south @-@ west of amman ) as saying that the seizure took place after police received information that there were
attempts by the group to sell for more than $ 100 thousand dollars , the police rushed to the arrest in possession .
0 11/313.69
2
3
3
7/4.3574
4
23/0.068359
53580
63
7
23/0.0097656
81300
93/4.6934
104/5.7402
11
5/4.7529
12
23
38
1300/2.9102
13
23
14
3580/2.248
15
1300
18
3580
16
4/4.0488
1723/2.5654
1300
19
23/1.2598
3/0.66016
2023
23/1.5156
32
3
231300
21
3/2.8027
225/6.2256
4423
24
1300
23
4/2.5117 25
23
30
23
2623
29
3580
27
3/2.1895
2810/3.9199
1300
1300
3/1.1123
1300
23
33
4/4.4336
31
1300
3
4/1.6416
23
3580
3/2.1895
3
365614
34
3580
353580
375614
4/1.8594
23
39
23
415614
425614
4312316
5512316
575614
56
5614
69
12316
49
12316
50
12/2.9502
52
7359
53
12272/1.9209
51
42/1.04
4512/2.9502
46
42/1.3105
47
7359
48
12272/1.9209
54
8615/6.5205
58
12316
40
12/2.9502
42/1.3105
7359
12272/1.9209
5942
60
7359
618615/3.6523
62
12272/0.66504
112
58332/5.0586
63
4/0.5752
11036/6.5117
64
309/6.1533
65
755/6.3652
66
58332
68
58332
67
4/0.12109
7359
12272/1.9209
42/1.04
707359
7142
58332
72
4/0.5752
58332
1234/0.12109
7359
12272/0.66504
12316
58332/3.2891
747359
758615/3.9463
76
12272/0.91699
78
58332
774/0.69727
79
12
58332
80
4/0.52637
4
58332/4.6484
132
3
81
515/4.1318
82
755/2.9941
58332
83
515
84
5
4
85
3/4.2812
86
309/3.6953 87
515/3.8184
88
755/3.9072
4
755/3.9072
89
3/4.2812
90309/3.6953
91
515/3.8184
58332/3.1895
755/4.7812
3
515/3.9678
58332
7359
12272/0.91699
3
755/2.9941
3
94
12
10565
12272/1.9209
42/1.3105
73
7359
4/0.5752
58332
9358332
92
4/0.69727
58332/0.086914
12
58332
4/0.52637
4
755/3.9072
3/4.2812
309/3.6953
96515/3.8184
58332/4.7764
3
755/3.123
95
515/4.2607
97
10565
58332/3.1895
3
515/3.9678
139755
985
99
5
100
5
101
309
755
102
515/1.4756
103
515
104
755/1.0029
309/2.4023
105
5
106
5
755
755/1.0029
107
515
108
5
4
755/3.9072
515/3.8184 109
309/3.6953
3
755/3.123
111
5
5
58332
4/1.5498
113
3
114
309
115
3
116
95850/0.21289
138
9
1175
5
4/2.9111
95850/0.44141
118
95850309
1193/0.63672
120
95850/1.582
121
309
5
95850/0.44141
309
3/0.63672
515
122
3
95850/0.75391
124
309
9
95850/1.751
95850
125
8716
1263/0.24902
127
95850
309/0.16016
128
3
1298716
95850/0.39355
130
309
1311008
9
95850/0.91602
309
58332/0.84375
95850/0.12402
133
9
134
15/2.5664
153
16
309
95850/0.9082
16
95850/0.12402
9
136309
95850/0.074219
141
3
14213267
15713267
1358716
1505
143
16/6.207
1378716
2035/4.6826
15/2.5664
16
144
13267/5.418
16
1405
146
8716
1475108
14895850
13267
1495108
145
309
9
95850/6.04
5
168
8/2.209
151
368
154368
1703
171
12
1761001/1.9658
15/1.5693
16
13267/3.3818
12 1591001
160
5
152 156
5 1627375
1635108
155 5
161
7375/4.4355
166
7375
181
15
16715
8/4.0586
14226/5.498
169
368
158 16412 1721001
177
5
1657375/5.1436
1807375
17315
174
15
18320
175
20
12
1001/2.3379
5
4/0.94141 20
4/0.9873
20 1791976
368
1861976 189
13
1843/4.5869
16/6.084
178
15
4/0.9873
20
1821976/5.3818 18513
1923
1874854
1883
13
190714
1914854
196
336
195714
193
31/1.3125
197
57/1.459
1984854
205
185
204336
199
4/5.2324
200
185/1.6523
201626
202236
714
31/1.3125
57/1.459
194
63/6.7617
206
185
207
3
20813/3.3818
20924/1.8857
626/0.54102
210
90/0.18164
211
309/1.875
212505
203
626
214
505
215
185
217309
216
3/0.79297
213336
218
309
220309
219
90/3.8145
2213
222
505
22331
225505
224
90/1.124
226
90
227185
288
90
309
3/0.79297
228
24/5.6748
626/4.7695
229
90/4.7461
233
505
232
90/4.0713
231
309
230
90/3.8027
505
505
90/2.3574
234
57
235
90
236
90
237505
239
90
23831
240
5/4.6641
2418
2423
2978
243
505
24431
505
90/3.7266
2455
8/0.95117
266
57/1.7461
24790
24631
24850
5
8
5/2.1689
8
249
3/3.0986
250
505
25113
2523
2533
255
309
254
90/4.8203
25690
257505
2583
278
50
259
505
26013
261505
309 26213
263
83
264309
265309
505
57
267
8
8
2685/2.1689
269309
270
13
271
83
272
3029/2.8291
273
8
27483
275
29
337
63/1.9453
27613
27713
279
3
3083
2803
2814713
28283 63
28329/0.66406
28429
3
29
2853029
3563029
28683
83
2873029/2.8857
505
289309
290309
29113
63/0.8457
29
2923029
293
194
36217
29
63/1.5664
29529
296
63/0.29004
298
13
29913
30083
30117
302
87
303
3
30410/0.21484
306
194
307194
30983
31083 29
63/1.7354
3
305140/5.4287
312
10/0.21484
313
17
294 3
10/0.65234
140/6.0684
314
140
315
1188/5.5186
316
140
317
1188/3.1943
318
5
31987
32087
321309
63
29/1.043
63
140
1188/2.3926
323
3
32410/0.125
326
8
3255/0.27246
327
11/2.5967
328
1188/0.80762
329140
330
5
331
11/5.2559
332
1188/2.5459
333
140
334
1188
33517
33617
31113 32283 29/0.15527
63
400
140
1188/2.4033
338140
341
3/2.9023
342
1188/6.0127
343
3996
339
1188
340
2710/4.4717
401
3
344
5/4.9404
345
8/1.1172
346
11
347
3848/3.9785
348
8
2710/4.2734
349
1188
350
3
3848/3.3076
402
11
351
8/0.44629
8
352
11/2.667
354
8
353
4/0.53027
3
355
10/0.125
3
10/0.20996
11/2.8037
357
5
358
8/2.1064
359
1188/1.2041
360
5
3611188/2.4062
4/1.6289
365
8
363
3/5.5244
364
5/3.4756
366
11/1.3096
367
1579/6.1328
368
3848/3.5664
8
369
5/1.5518
370
11/1.7031
3711579
3848 374
11
372
3/3.1328
373
9/3.7803
375
81/4.0898
376
1188/2.4893
377
1704/4.7871
378
2710/5.6816
379
1579
1579
3848/0.19922
3/0.019531
380
3996
3996/4.1592
3821579
381
3/2.3301
383
1649/4.8389
384
3848/1.6953
385
11
3996
3512/3.4209
3996/2.5566
4/3.2705
388
8
3863/3.7451
387
5/2.584
389
11/3.0176
390
1579/1.8506
391
1649/5.4414
392
3848/0.073242
393
1579
3/2.3301
395
1579
3963848/1.6953
3943996
1579/0.65039
397
3
399
3996
398
8
140
17
1188
11
8/0.44629
1579/6.5
3848/6.1123
3996
406
3/5.6807
403
1579
1579/1.1162
3848/1.3164
404
3
405
1649/2.5039
3996/4.21391649/4.8936
407
3
408
1579/0.054688
409
3848/1.75
410
11
411
11
3
1579
412
5
413
9/2.7051
414
11/2.5723
415
1188/3.8848
416
1704/4.084
81
421
38/4.04
422
143/4.9678
423
188/6.5078
417
1188
4182710/0.94434
419
569
420
775/0.1084
714
163
775/2.3652
425
11
424
9/0.68457
426
20/4.0977
427
130/1.0742
4281704/0.60352
429
3431/4.0811
430
8/0.40527
431
21
432
9
4331704/1.2285
9/3.7803
438
11
439
1704/4.7871
775/5.4775
443
11441
5/4.5166
442
9/1.3047
444
20/5.0928
445
130/3.2822
446
1704/1.4395
440
1579
447
1704
81/3.7598
449
11
448
9/2.5488
450
130/3.375
451
1704/1.54
452
8246/3.0615
81
3848/6.1123
3996
3/5.6807
453
1579
1579/2.0078
3848/2.208
454
3
3/0.21191 3848/1.9619
455
1579
775/4.3457
5/3.6152
11
456
9/1.7539
457
1704/1.4248
458
1704
81/5.1875
11
459
9/4.9961
4601704/4.542
461
5
11
462
1579
463
3996
9/3.7803
81/5.3926
1704/4.7871
20/6.6523
464
11
1188
3996
8/1. 172
11
1704/1.0518
465
11
3848/2.8662
4661579
1704
467
1579
468
1579
5/4.5166
9/1.3047
20/5.0928
1704/1.4395
469
11
470
130/3.2822
11
9/2.5488
1704/1.54
471
130/3.375
472
8246/3.0615
81
473
38/3.3545
474
143/3.3359
81
475
38/4.7412
476
143/4.5361
81/0.95898
542
1188
477
2710/2.8506
478
569/0.63574
479
775
81
38/1.4326
481
1704
480
9/0.081055
21
775/1.7617
9/0.25488
20/3.3711
1704
482
11/1.3154
483
130/0.80176
11
1704/2.5254
484
9/0.31934
485
11
48611
487
561
488
74
489
188/1.9863
490
3250/0.032227
491
3745/1.8066
492
4816/0.54199
493
143
494
143
81
38/1.3213
497
3/3.3994
496
775
495
569/0.53906
498
3431
81
4993/1.8145
50038/1.2832
501
143/0.49512
502
3745/0.70996
503
8
504
21/0.38281
505
8
81
3/1.2666
50638/0.38281
81
507
3/1.6318
775/0.014648
508
569
8
4345
435
9/4.2754
436
11/3.8105
437
1188/3.6299
81/2.2656
1188
2710/2.8506
775
81/0.10449
509
38
130/0.4707
511
1704
510
9/0.081055
81
38/4.04
512
143/4.9678
21
513
5
5149/1.4326
515
11/0.9668
516
1704/2.2139
81
38/2.5791
3/5.5254
519
143/3.0596
775/0.79395
81
517
1704/0.10059
775/1.0488
518
569
520
3431
81/1.75
522
38
521
3/2.3906
523
143/0.81934
524
8
525
21/1.2314
526
8
81
38/3.6436
528
775
527
569/0.38184
81/0.98145
38
3/0.7793
21/1.4082
529
8
81
38/1.166
530
3/0.90234
531
143/0.37305
532
561/0.8584
5/1.2695
9/0.99316
11
533
1704/1.4629
3848/3.7578
534
1579
536
11
535
9/1.3047
537
1704/1.7061
569
775/0.43359
539
21
5388/0.4082
21
5408/0.33887
775
569/0.38184
21/0.69336
541
8
1188
11
9/1.3047
130/3.2822
1704/1.4395
11
1704/1.54
5
54311
81
38/4.04
188/6.5078
143/4.9678
38
5/2.417
9/1.4287
544
11
545
1704/2.7559
5
11/0.92969
20/4.6992
5
9/1.4326
11/0.9668
1704/2.2139
546
130/5.1309
81
38/2.5791
143/3.0596
81/1.75
38
143/0.81934
38
81
143/0.37305
547
561
74/0.45117
3250
4816/0.50977
548
561
3250
4816/0.50977
549
74/0.71582
775/2.4541
20/3.7178
1704/0.69238
3431/3.6172
550
9
551
11/0.39551
552
130/0.37207
553
9
55411
555
11
556
8
775
81
38/0.22363
3/2.1543
81
38/1.2832
143/0.49512
3745/0.70996
557
3/0.82227
775/0.99414
569
81/0.38965
558
10
81
559
38/1.5928
560
5/2.4922
561
3745
562
188
563
1191
565
1191
564
143/0.625
566143
567
1191/0.20703
1191
568
1191
57111
57011
572
561
8
81/0.56348
573
561
574
561
188/1.9551
3250
3745/1.7744
4816/0.50977
575143
81
577
38/0.38281
576
3/2.0371
81
578
3/1.6318
81/0.09082
38
579
561
580
561
581
11
582
561
21/0.38281
8
775
569/0.53906
74
3250/0.032227
4816/0.54199
775/2.917
81
583
11/3.3545
584569/3.9336
585
1704/2.4365
775
586
569/0.63574
81/0.10449
3/3.4795
38
21/0.51562
587
8
5888
589
11
188/2.8662
3250
3745/0.82031
4816/1.4219
590
74/0.55664
5918
593561
592
561
3250
3745/0.82031
4816/1.4219
81/1.4834
3/3.4424
38
81
594
3/0.89746
38
596
11
595
11
81/0.71484
38
597
561
3250
4816/0.50977
3745
598
5/0.037109
599
4816/0.43164
21
8/0.6377
5/2.0967
9/1.4287
11
1704/2.7559
81
38/2.8457
775/1.0488
600569
21/0.96484
6018
81
3/1.5
81
3/2.3506
38/0.38281
81
38/0.15137
81
38/0.38281
81
38/4.5273
81
38/1.9385
21
8/0.6377
38
602
3745
3745
188
603
81/0.34082
775/0.014648
610
569
81/0.25586
10
81
3/2.1406
38/1.5928
81
38/0.38281
81/0.56348
611
561
612
2339
613
561
614
188
615
143
6161191
617
3356
6193356
6181191
6201191
621
3356
622
3356
5691191
623
3356
81/0.28613
10
81
3/3.3311
38/1.5928
5
6245
625
4816/0.8916
3745
5/1.5254
6261191
628
561
627
561
629561
630
3745
631
5
10
5/4.0293
3745
81/0.56543
38
632
11
21/1.7168
633
8
634
11
81/1.1543
38
81/1.4834
38
81/0.25586
635
10
636
38/3.7988
188
637
3250/1.4678 638
3745/0.75
81/0.84473
38
5
639
3745
640
561
81
10/0.13965
81
3/1.7939
38/1.5928
5
4816/0.048828
641
188
642
1191
64311
81/1.2168
38
775
604569/0.53906
81
38/1.6504
3/3.9385
81
38/1.2832
3745/0.70996
606
143/0.49512
605
3/1.5205
607
561/3.8506
608
3250/3.5908
609
4593/3.8887
1191/3.2842
143
163
644
11
188/1.9551
3745/1.7744
646
3250
647
4816/0.50977
81/0.56348
645
561
3745
5/0.037109
648143
5
649
11
5
4816/0.8916
6509/1.6494
737
5
5
3745/0.52344
652
1191
651
143/2.3555
653
1191
654
3356
655
1032
656
3356
657
1032
6583356
659
1032
6601032
661
1032
662
2295/6.8135
663
188
1191/0.25391
664
143
6653356
5
6664816/1.8076
5/3.1465
3745
5
667
3745/1.3447
1191/3.7744
143
668
188
81
10/1.3828
81/1.2715
38
81/0.25586
669
10
670
2339
671
561
1191/0.5625 672
143
143/1.8193
1191
1191/2.8828
143
4816/2.0596
5
3745/1.3447
1191
6733356
81/0.25586
674
10
81/0.28613
10
38/3.8291
3745/2.3125
5
4816/0.8916
9/1.6494
1191
143/0.625
675
4376/3.2324
1191
676
143/2.5205
677
4376/2.5254
678
1191
81/0.52148
10
6794593
680
81/2.3428
681116/4.3838
682
188
6843356
6831191
6853356
686
1032
687
4
689
4
688
3/2.4316
690
71/1.3447
692
4
691
3/1.8115
693
4
3/1.8115
694
4
4
71/3.583
3/1.8115
695
4
696
6/6.2324
697
7/6.3906
698
9/5.1299
699
24/3.6904
700
77/4.2246
701
119/3.668
702
1032
143/1.918
1191
7031191
704
1032
143
1191/0.83887
143
1191
143/1.918
705
2339
5
706
9/4.5615
740
3745/1.1836
5
3745/0.57617
707
1191
7081032
709
2339
710
1032
7111191
712
1032
7133356
715
5
7173356
7181032
3/3.6035
71/2.5166
4
4
3/1.8115
7193
720
309/4.5137
721
9331/2.3145
309/2.1367
723
9331
724
22194/2.8037
722309
7259331
3
9331/2.3145
726
309
309/2.1367
9331
22194/2.8037
727
3/2.9238
3
71/4.9521
524/5.0078
119/4.9316
3
309/4.5137
9331/2.3145
22194/5.1182
728
24/5.543
48 7299331
730
3
731
9331
732
22194/1.9717
22194/1.3242
733
9331
734
3
735
3
736
3356
3/1.8115
4
5
3745/1.5127
738
4593
1191/1.6035
143
739
3356
4
5
3745/1.4502
741
4
742
3356 3/0.46582
743
4
744
1032
1191
143/0.66602
716
163
1191
143/1.6924
8
1191
143/1.5361
3/2.4316
71/1.3447
4
4
71/3.583
3/1.8115
119/3.668
745
24/3.6904
746
77/4.2246
747
309
749
9331
748
1899/1.582
750
8
751309/4.4062
7529331
8
309/4.4062
753
3/4.7422
754
8
7558
757
9331
756
1899/4.4609
758309
22194/0.54492
759
9331
760
8
761
309
762
8
3/0.42383
8
763
8
764
309
765
309
71/1.3447
8044
766
5
4
854
3
767
9331/0.080078
4
524/0.36816
3
9331/0.080078
4
9331
9331
1899/4.3027
803
9331
768
2925/6.7461
770
8
7692925
309/1.9824
771
3
772
8
7738
309
309
309/2.1914
7743
776
8
7752925
777
9331
778
8
779
3
780
9331
309/0.55273
3
309/0.96387
3
781
8/1.3115
78221215
9331
9331
783
4376
784
309
785
8
7878
786
1716
7888
1716/2.3975 789
3
790
2318/1.6875
791
309
792
2318
795
3
793309
794
8
1716/2.4541
3
7962318/1.7441
797
8
309/1.1377
3
798
309
799
8
8002318
8018
802
1032
3
805
9
8072318
8092318
808
1716/0.81934
810
7/0.6084
811
161
8/3.6729
8121716
813
11070/2.5137
161
11070/3.251
8162318
815
1716/0.81934
814
124/3.165
817
5763/4.4824
818
7235/5.5088
819
11744/5.4629
1716
11070/2.5137
820
2318
7/0.6084
823161
821
3/4.7285
879
4/3.4551
822
9/0.92676
3
2318/1.7441
1716
3
2318/1.7441
824
7
826
2318
825
3/2.9131
827
4
9331
828
3005
1716/2.4541
3
2318/1.7441
806
45/5.8838
829
2318
161
830
7/0.77637
833
9
832
7/0.30078
831
9
57/3.4434
834
9
835
109/2.0273
8363005/0.50586
837
9
9
7/4.2412
838
161/4.2617
839
9
161
7/0.98242
845
9/0.92676
9
843
7/0.30078
842
4/3.3135
4/5.543
9 841
7/4.8145
8405
844
9
9
9/0.125
846
7
9
848
4/3.5527
8497/2.3818
880
3005
881
9
45/2.6279
847
3005
85021215
9/0.92676
852
7/0.6084
853
161
8512318
3
5
8846/0.25
7/0.55762
9
9
45/4.8506
3005
45/2.6006
3005
57/2.3096
9
855
3/1.5703
856
5/0.34961
857
3005/0.019531
45
858
3005/0.84277859
63
5/2.6631
6
45/0.20508
860
3005
9
45/1.249
3005
861
45
45/0.48828
8629
21/3.7686
45/5.5586
57/2.3096
9
109/4.7979
3/3.0703
5/1.8926
3005/0.019531
864
63/5.4697
7/3.0781
863
9
865
3005
3005
9
6
5/0.94043
6
7/0.5791
9
3005/2.1006
866
9
86799
868
7/0.30078
9
109/2.0273
9
869
7/2.3818
8822
870
3005
3
5/3.1211
6
6
8835/0.24609
871
9
5/0.89355
6
872
2318
45
873
3005/0.84277
45
874
3005/2.4316
875
9
5/0.99414
6
45
45
3005/1.2275
45
3005/1.5234
9
3005/0.019531
9
5/2.0693
6
161/2.6025 6
8763005
877
7
5/0.24609
6
5
878
3005
5/0.28711
6
3/2.0576
57/1.9053
9
5
6/0.25098
H1 H2 H3 H4 H5 H6 H7 H8 H9
0 11
23
3
7
423
5
3580
63
7
23 8
1300
9
3
10
4
115
1223
13
1300
23
143580
25
1300
15
4
1623
17
3580
1300
18
23
3
1923
23
203
21
3
22
5 23
1300 24
23
23
4
2623
27
23
28
3
2910
30
3580
31
23
1300
1300
3
1300
321300
3
23
33
4
345614
3 35
3580
363580
37
5614
385614
23
4
3923
4012316
3
3580
41
5614
425614
43
12316
4412316
45
5614
46
12
47
42
48
7359
49
12272
50
12316
51
12316
52
12
53
42
64
7359
12272
12
42
7359
12272
548615
55
12316
56
42
7359
12272
57
8615
63
58332
58332
58
4
36
58332
4
42
7359
12272
7359
5942
7359
12272
58332
6012
7359
12272
42
12272
7359
58332
61
8615 62
12
58332
7359
12272
10565
5833212
10565
0 14 23 3755 45 5309 69 0
13
2
95850
38716
52035
8716
45 95850
0 116 213267 35108 4368 512 61001 75 87375 915 1020 111976 1213 133 144854
0 131
2
57
3
714
44
5
185
6236
7336
10185
3
8336
185
9185
3
0 1309 2505 390 413
0
183
2
3029
329
63
4
29
53029
6
194
717
887
9
3
1010
11
140
1217
13
140
14
1188
15140
16
1188
17
5
18
3
1910
205
21
8
2211
23
1188
24
140
255
2611
27
1188
28140
29
1188
30
140
1188
31
140
32
1188
332710
343
35
1188
36
3996
373
38
5
39
8
40
11
41
3848
42
8
2710
43
1188
443
3848
45
8
46
11
8
47
11
48
4
49
8
11
505
51
8
52
1188
535
54
1188
4
55
3
56
5
57
8
58
11
59
1579
603848
8
61
5
62
11
631579
3848
64
3
65
9
66
11
67
81
68
1188
69
1704
70
2710
711579
1579
3848
3
72
3996
3996
73
3
74
1579
751649
76
3848
77
11
3996
3996
3512
4
78
3
79
5
808
81
11
82
1579
83
1649
84
3848
851579
86
3996
3
87
1579
883848
1579
89
3
90
8
91
3996
1188
3996
8
11
1188
8
11
92
1579
1579
3848
93
3
94
1649
1579
3848
3996
95
3
3996
1649
96
3
97
1579
98
3848
99
11
100
11
3
1579
101
5
102
9
103
11
1041188
105
1704
1061188
1072710
108569
109
775
81
110
38
111
143
112
188
113
163775
9
114
11
115
20
116
130
117
1704
118
3431
1198
120
21
121
9
122
1704
5
123
9
124
11
125
1188
9
126
11
127
1704
128
1579
775
129
5
130
9
131
11
132
20
133
130
134
1704
1351704
81
9
136
11
137
130
138
1704
1398246
81
140
1579
1579
3848
3
3848
3996
3
3
3848
141
1579
775
5
11
9
1704
1704
81
11
1704
142
9
1435
11
9
11
130
1704
11
1704
144
1579
3996
9
81
1704
20
145
11
1704
146
11
3848
147
1579
1704
148
1579
149
1579
5
9
20
1704
150
11
151
130
9
11
1704
152
130
153
8246
81
154
38
155
143
81
156
38
157
143
81
158
1188
159
2710
569
160
775
81
38
1619
162
1704
21
775
9
20
1704
163
11
164
130
11
1704
165
9
166
11
167
11
168
561
169
74
170
188
171
3250
172
3745
173
4816
143
174
143
81
38
175
3
176
3431
81
177
3
178
38
179
143
180
3745
181
8
182
21
8
81
38
3
81
183
3
775
184
569
8
775
81
185
38
130
9
186
1704
81
38
143
21
187
5
188
9
189
11
190
1704
775
81
191
1704
775
192
569
81
38
3
193
143
3431
81
38
194
3
195
143
8
196
21
8
81
38
81
38
3
21
8
81
38
197
3
198
143
199
561
5
9
11
1704
11
9
1704
775
569
1188
5
81
38
188
143
38
5
9
11
200
1704
5
11
20
130
5
9
11
1704
81
38
143
81
38
143
38
81
143
201
561
74
3250
4816
202
561
3250
4816
203
74
775
20
1704
3431
11
2049
205
130
9
11
775
206
8
81
38
3
81
38
143
3745
207
3
775
569
81
208
10
81
38
209
5
2103745
188
211
1191
212
143
213
1191
1191
214
143
1191
2151191
216
561
8
81
217
561
218
561
188
3250
3745
4816
143
81
3
21938
81
3
220
561
22111
222
561
21
8
775
81
569
11
1704
775
223
569
81
3
38
8
21
8
22411
188
3250
3745
4816
225
74
226
561
3250
3745
4816
81
227
3
228
561
3250
4816
5
4816
3745
8
21
229
3745
3745
188
230
81
775
231
569
81
3250
38
3745
561
232
3
233
143
234
4593
81
38
81
235
561
236
2339
237
188
238
143
266
3356
239
1191
240
3356
2411191
2423356
5
4816
243
5
5
3745
244
561
5
10
5
3745
245
11
81
38
246
10
3250
3745
188
5
247
561
5
4816
1191
143
163
248
11
249
561
188
3745
250
3250
251
4816
5
48165
2529
253
5 1191
143
254
1191
267
1032
268
3356
255
1032
3356
256
1032
2572295
188
5
3745
81
25810
259
2339
4816
5
3745
81
38
10
4816
3745
5
9
143
1191
4376
1191
4376
2604593
81
188
261
116
3356
4
4
4
9
1032
262
2339
3745
5
263
9
5
8
3745
5
2644593 2655
4376
0 13 2309 39331 48
0
1
3
2
45
30
1716
32318
5763
7235
4
124
51716
6
2318
7
11070
811744
92318
35
9
10
3
11
4
12
7
13
9
14
161
155
4
9
16
7
9
174
18
7
9
7
9
7
9
19
3005
20
9
57
21
9
22
109
37
3005
45
3005
9
23
4
24
7
2545
45
9
7
26
9
63
21
45
57
3
9
109
3005
27
5
3005
28
6
45
45
29
3005
63
5
6
7
9
9
3005
312318
45
32
3005
3
332
6
34
5
36
7
5
161
6
3
57
9
433 1 4 1 6 1 6860 1 76
Figure 2: ML translation E?, word lattice E , and decomposition as a sequence of four string and five
sublattice regions H1 . . .H9 using n-gram posterior probability threshold p(u|E)?0.8.
tial hypotheses that can be trusted. We wish to
constrain MBR decoding to include these trusted
partial hypotheses but allow decoding to consider
alternatives in regions of low confidence. In this
way we aim to improve the best possible output of
the best available systems.
We use the path posterior n-gram probabilities
of Equation (4) to segment lattice E into regions of
high and low confidence. As shown in the exam-
ple of Figure 2, the lattice segmentation process
is performed relative to the ML hypothesis E?, i.e.
relative to the best path through E .
For confidence threshold ?, we find all 4-grams
u = E?i, . . . , E?i+3 in the ML translation for which
p(u|E) > ?. We then segment E? into regions
of high and low confidence where the high confi-
dence regions are identified by consecutive, over-
lapping high confidence 4-grams. The high confi-
dence regions are contiguous strings of words for
which there is consensus amongst the translations
in the lattice. If we trust the path posterior n-gram
probabilities, any hypothesised translation should
include these high confidence substrings. This ap-
proach differs from simple posterior-based prun-
ing in that we discard paths, rather than words
or n-grams, which are not consistent with high-
confidence regions of the ML hypothesis.
The hypothesis string E? is in this way seg-
mented into R alternating subsequences of high
and low confidence. The segment boundaries are
ir and jr so that E?jrir is either a high confidence
or a low confidence subsequence. Each subse-
quence is associated with an unweighted subspace
Hr; this subspace has the form of a string for high
confidence regions and the form of a lattice for
low confidence regions.
If the rth segment is a high confidence region
then Hr accepts only the string E?jrir . If the rth
segment is a region of low confidence, then Hr
is built to accept relevant substrings from E . It is
constructed as follows. The rth low confidence
region E?jrir has a high confidence left context e?r?1
and a high confidence right context e?r+1 formed
from subsequences of the ML translation hypoth-
esis E? as
e?r?1 = E?jr?1ir?1 , e?r+1 = E?
jr+1
ir+1
Note that when r = 1 the left context e?r?1 is the
empty string and when r = R the right context
e?r+1 is the empty string. We build a transducer
74
Tr for the regular expression /. ? e?r?1(.?)e?r+1. ?
/\1/.1 Composition with E yieldsHr = E?Tr , so
that Hr contains all the reasonable alternatives to
E?jrir in E consistent with the high confidence left
and right contexts e?r?1 and e?r+1. IfHr is aligned
to a high confidence subsequence of E?, we call
it a string region since it contains a single path;
if it is aligned to a low confidence region it is a
lattice and we call it a sublattice region. The se-
ries of high and low confidence subspace regions
H1, . . . ,HR defines the lattice segmentation.
5 Hypothesis Space Construction
We now describe a general framework for improv-
ing the fluency of the MBR hypothesis space.
The segmentation of the lattice described in
Section 4 considerably simplifies the problem of
improving the fluency of its hypotheses since each
region of low confidence may be considered in-
dependently. The low confidence regions can be
transformed one-by-one and then reassembled to
form a new MBR hypothesis space.
In order to transform the hypothesis region Hr
it is important to know the context in which it oc-
curs, i.e. the sequences of words that form its pre-
fix and suffix. Some transformations might need
only a short context; others may need a sentence-
level context, i.e. the full sequence of ML words
E?jr?11 and E?Nir+1 to the left and right of the region
Hr that is to be transformed.
To put this formally, each low confidence sub-
lattice region is transformed by the application of
some function ?:
Hr ? ?(E?jr?11 , Hr, E?Nir+1) (6)
The hypothesis space is then constructed from the
concatenation of high confidence string and trans-
formed low confidence sublattice regions
H = E ?
?
1?r?R
Hr (7)
The composition with the original lattice E dis-
cards any new hypotheses that might be created
via the unconstrained concatenation of strings
from theHr. It may be that in some circumstances
1In this notation parentheses indicate string matches so
that /. ? y(a?)w. ? /\1/ applied to xyaaawzz yields aaa.
the introduction of new paths is good, but in what
follows we test the ability to improve fluency by
searching among existing hypotheses, and this en-
sures that nothing new is introduced.
Size of the Hypothesis Space If no new hy-
potheses are introduced by the operations ?, the
size of the hypothesis space H is determined by
the posterior probability threshold ?. Only the
ML hypothesis remains at ? = 0, since all its
subsequences are of high confidence, i.e. can be
covered by n-grams with non-zero path posterior
probability. At the other extreme, for ? = 1, it
follows that H = E and no paths are removed,
since any string regions created are formed from
subsequences that occur on every path in E .
We can therefore use ? to tighten or relax
constraints on the LMBR hypothesis space. At
? = 0, LMBR returns only the ML hypothesis;
at ? = 1, LMBR is done over the full transla-
tion lattice. This is shown in Table 1, where the
BLEU score approaches the BLEU score of un-
constrained LMBR as ? increases.
Note also that the size of the resulting hypoth-
esis space is the product of the number of se-
quences in the sublattice regions. For Figure 2 at
? = 0.8, this product is ?5.4 billion hypotheses.
Even for fairly aggressive constraints on the hy-
pothesis space, many hypotheses remain.
6 Monolingual Coverage Constraints
This section describes one implementation of the
transformation function ? that we will show leads
to improved fluency of machine translation out-
put. This transformation is based on n-gram cov-
erage in a large target language text collection:
where possible, we filter the sublattice regions
so that they contain only long-span n-grams ob-
served in the text. Our motivation is that large
monolingual text collections are good guides to
fluency. If a hypothesis is composed entirely of
previously seen high order n-grams, it is likely to
be fluent and should be favoured.
Initial attempts to identify fluent hypotheses in
sublattice regions by ranking according to n-gram
LM scores were ineffective. Figure 3 shows the
difficulties. We see that both the 4-gram Kneser-
Ney and 5-gram stupid-backoff language models
75
LM Translation hypothesis E and n-gram orders used by the LM to score each word Score
4g <s>1 the2 reactor3 produces3 plutonium2 needed2 to3 manufacture4 atomic3 bomb2 .3 </s>4 -22.59
<s>1 the2 reactor3 produces3 plutonium2 needed2 to3 manufacture4 the4 atomic2 bomb3 .4 </s>4 -23.61
5g <s>1 the2 reactor3 produces4 plutonium5 needed3 to3 manufacture4 atomic5 bomb2 .3 </s>4 -16.04
<s>1 the2 reactor3 produces4 plutonium5 needed3 to3 manufacture4 the4 atomic4 bomb5 .4 </s>5 -17.96
Figure 3: Scores and n-gram orders for hypotheses using 4-gram Kneser-Ney and 5-gram stupid-
backoff (estimated from 1.1B and 6.6B tokens, resp.) LMs. Low confidence regions are in italics.
favour the shorter but disfluent hypothesis; nor-
malising by length was not effective. However,
the stupid-backoff LM has better coverage and the
backing-off behaviour is a clue to the presence
of disfluency. Similar cues have been observed
in ASR analysis (Chase, 1997). The shorter hy-
pothesis backs off to a bigram for ?atomic bomb?,
whereas the longer hypothesis covers the same
words with 4-grams and 5-grams. We therefore
disregard the language model scores and focus on
n-gram coverage. This is an example where ro-
bustness and fluency are at odds. The n-gram
models are robust, but often favour less fluent hy-
potheses.
Let S denote the set of all n-grams in the mono-
lingual training data. To identify partial hypothe-
ses in sublattice regions that have complete mono-
lingual coverage at the maximum order n, we
build a coverage acceptor Cn with a similar form
to the WFST representation of an n-gram backoff
language model (Allauzen et al, 2003). Cn as-
signs a penalty to every n-gram not found in S .
In Cn word arcs have no cost and backoff arcs are
assigned a fixed cost of 1. Firstly, arcs from the
start state are added for each unigram w ? N1:
w
w/0?
Then for n-grams u ? S ? {?ni=2 Ni}, where
u = wn1 consisting of history h = wn?11 and target
word wn, arcs are added
wn/0h h+
where h+ = wn?12 if u has order n and h+ = wn1
if u has order less than n. Backoff arcs are added
for each u as
?/1h h?
where h? = wn?12 if u has order > 2, and bi-
grams backoff to the null history start state ?.
For each sublattice region Hr, we wish to pe-
nalise each path proportionally to the number of
its n-grams not found in the monolingual text col-
lection S . We wish to do this in context, so that
we include the effect of the neighbouring high
confidence regions Hr?1 and Hr+1. Given that
we are counting n-grams at order n we form the
left context machine Lr which accepts the last
n ? 1 words in Hr?1; similarly, Rr accepts the
first n ? 1 words of Hr+1. The concatenation
Xr = Lr?Hr?Rr represents the partial transla-
tion hypotheses inHr padded with n?1 words of
left and right context from the neighbouring high
confidence regions. Composing Xr ? Cn assigns
each partial hypothesis a cost equal to the number
of times it was necessary to back off to lower order
n-grams while reading the string. Partial hypothe-
ses with cost 0 did not back off at all and contain
only maximum order n-grams.
In the following experiments, we look at each
Xn ? Cn and if there are paths with cost 0, only
these are kept and all others discarded. We intro-
duce this as a constraint on the hypothesis space
which we will evaluate for improvement on flu-
ency. Here the transformation function ? returns
Hr as Xr ?Cn after pruning. If Xr ?Cn has no zero
cost paths, the transformation function ? returns
Hr as we find it, since there is not enough mono-
lingual coverage to guide the selection of fluent
hypotheses. After applying monolingual coverage
constraints to each region, the modified hypothe-
sis space used for MBR search is formed by con-
catenation using Equation (7).
We note that Cn is a simplistic NLG system. It
generates strings by concatenating n-grams found
in S . We do not allow it to run ?open loop? in these
experiments, but instead use it to find the strings
in Xr with good n-gram coverage.
7 LMBR Over Segmented Lattices
The effect of fluency constraints on LMBR de-
coding is evaluated in the context of the NIST
Arabic?English MT task. The set tune consists
76
ML ... view , especially with the open chinese economy to the world and ...
+LMBR ... view , especially with the open chinese economy to the world and ...
+LMBR+CC ... view , especially with the opening of the chinese economy to the world and ...
ML ... revision of the constitution of the japanese public , which dates back ...
+LMBR ... revision of the constitution of the japanese public , which dates back ...
+LMBR+CC ... revision of the constitution of japan , which dates back ...
Figure 4: Improved fluency through the application of monolingual coverage constraints to the hypoth-
esis space in MBR decoding of NIST MT 08 Arabic?English newswire lattices.
of the odd numbered sentences of the MT02?
MT05 testsets; the even numbered sentences form
test. MT08 performance on nw08 (newswire) and
ng08 (newsgroup) data is also reported.
First-pass translation is performed using HiFST
(Iglesias et al, 2009), a hierarchical phrase-based
decoder. The first-pass LM is a modified Kneser-
Ney (Kneser and Ney, 1995) 4-gram estimated
over the English side of the parallel text and an
881M word subset of the English GigaWord 3rd
Edition. Prior to LMBR, the first-pass lattices are
rescored with zero-cutoff stupid-backoff 5-gram
language models (Brants et al, 2007) estimated
over more than 6B words of English text. The
LMBR factors ?0, . . . , ?4 are set as in Tromble et
al. (2008) using unigram precision p = 0.85 and
recall ratio r = 0.74.
The effect of performing LMBR over the seg-
mented hypothesis space is shown in Table 1. The
hypothesis subspaces Hr are constructed at var-
ious confidence thresholds as described in Sec-
tion 4 with H formed via Equation (7); no cover-
age constraints are applied yet. Constraining the
search space using ? = 0.6 leads to little degra-
dation in LMBR performance under BLEU. This
shows lattice segmentation works as intended.
We next investigate the effect of monolingual
coverage constraints on BLEU. We build accep-
tors Cn as described in Section 6 with S con-
sisting of all n-grams in the English GigaWord.
At ? = 0.6 we found 181 sentences with sub-
lattices Hr spanned by maximum order n-grams
from S , i.e. for which Xr ? Cn have paths with
cost 0; these are filtered as described. LMBR
over these coverage-constrained sublattices is de-
noted LMBR+CC. On nw08 the BLEU score for
LMBR+CC is 52.0 which is +0.7 over the ML de-
coder and only -0.2 BLEU below unconstrained
LMBR decoding. Done in this way, constraining
hypotheses to have 5-grams from the GigaWord
tune test nw08 ng08
ML 54.2 53.8 51.3 36.3
?
0.0 54.2 53.8 51.3 36.3
0.2 54.3 53.8 51.3 36.3
0.4 54.6 54.2 51.6 36.7
0.6 54.9 54.4 52.1 36.6
0.8 54.9 54.4 52.1 36.6
1.0 54.9 54.4 52.2 36.7
LMBR 54.9 54.4 52.2 36.8
Table 1: BLEU scores for ML hypotheses and
LMBR decoding inH over 0 ? ? ? 1.
has little impact on BLEU.
At this value of ?, 116 of the 813 nw08 sen-
tences have a low confidence region (1) com-
pletely covered by 5-grams, and (2) within which
the ML hypothesis and the LMBR+CC hypothe-
sis differ. It is these regions which we will inspect
for improved fluency.
8 Human Fluency Evaluation
We asked 17 native speakers to judge the fluency
of sentence fragments from nw08. We compared
hypotheses from the ML and the LMBR+CC de-
coders. Each fragment consisted of the partial
translation hypothesis from a low confidence re-
gion together with its left and right high confi-
dence contexts (examples given in Figure 4). For
each sample, judges were asked: ?Could this frag-
ment occur in a fluent sentence??
The results are shown in Table 2. Most of the
time, the ML and LMBR+CC sentence fragments
were both judged to be fluent; it often happened
that they differed by only a single noun or verb
substitution which didn?t affect fluency. In a small
number of cases, both ML and LMBR+CC were
judged to be disfluent. We are most interested in
the ?off-diagonal? cases. In cases when one sys-
tem was judged to be fluent and the other was not,
LMBR+CC was preferred about twice as often as
the ML baseline (26.9% to 9.7%). In other words,
the monolingual fluency constraints were judged
77
LMBR+CC
Fluent Not Fluent
ML Fluent 1175 (59.6%) 192 (9.7%)Not Fluent 530 (26.9%) 75 (3.8%)
Table 2: Partial hypothesis fluency judgements.
to have improved the fluency of the low confi-
dence region more than twice as often as a fluent
hypothesis was made disfluent.
Some examples of improved fluency are shown
in Figure 4. Although both the ML and un-
constrained LMBR hypotheses might satisfy ad-
equacy, they lack the fluency of the LMBR+CC
hypotheses generated using monolingual fluency
constraints.
9 Summary and Discussion
We have described a general framework for im-
proving SMT fluency. Decoupling the hypothesis
space from the evidence space allows for much
greater flexibility in lattice MBR search.
We have shown that high path posterior proba-
bility n-grams in the ML translation can be used to
guide the segmentation of a lattice into regions of
high and low confidence. Segmenting the lattice
simplifies the process of refining the hypothesis
space since low confidence regions can be refined
in the context of their high confidence neighbours.
This can be done independently before reassem-
bling the refined regions. Lattice segmentation
facilitates the application of post-processing and
rescoring techniques targeted to address particu-
lar deficiencies in ML decoding.
The techniques we presented are related to con-
sensus decoding and system combination for SMT
(Matusov et al, 2006; Sim et al, 2007), and to
segmental MBR for automatic speech recognition
(Goel et al, 2004). Mohit et al (2009) describe
an alternative approach to improving specific por-
tions of translation hypotheses. They use an SVM
classifier to identify a single phrase in each source
language sentence that is ?difficult to translate?;
such phrases are then translated using an adapted
language model estimated from parallel data. In
contrast to their approach, our approach is able
to exploit large collections of monolingual data to
refine multiple low confidence regions using pos-
terior probabilities obtained from a high-quality
evidence space of first-pass translations.
Testset Sentences Reachability
tune 2075 15%
test 2040 14%
nw08 813 11%
ng08 547 9%
Table 3: Arabic?English reference reachability.
We applied hypothesis space constraints based
on monolingual coverage to low confidence re-
gions resulting in improved fluency with no real
degradation in BLEU score relative to uncon-
strained LMBR decoding. This approach is lim-
ited by the coverage of sublattices using monolin-
gual text. We expect this to improve with larger
text collections or in tightly focused scenarios
where in-domain text is less diverse.
However, fluency will be best improved by inte-
grating more sophisticated natural language gen-
eration. NLG systems capable of generating sen-
tence fragments in context can be incorporated di-
rectly into this framework. If the MBR hypothe-
sis spaceH contains a generated hypothesis E? for
which P (F |E?) = 0, E? could still be produced as
a translation, since it can be ?voted for? by nearby
hypotheses produced by the underlying system.
Table 3 shows the proportion of NIST testset
sentences that can be aligned to any of the ref-
erence translations using our high quality base-
line hierarchical decoder with a powerful gram-
mar. The low level of reachability suggests that
NLG may be required to achieve high levels of
translation quality and fluency. Other rescoring
approaches (Kumar et al, 2009; Li et al, 2009)
may also benefit from NLG when the baseline is
incapable of generating the reference.
We note that our approach could also be used to
improve the fluency of ASR, OCR and other lan-
guage processing tasks where the goal is to pro-
duce fluent natural language output.
Acknowledgments
We would like to thank Matt Gibson and the
human judges who participated in the evalua-
tion. This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022 and the European Union Seventh
Framework Programme (FP7-ICT-2009-4) under
Grant Agreement No. 247762.
78
References
Allauzen, Cyril, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of ACL 2003.
Blackwood, Graeme, Adria` de Gispert, and William Byrne.
2010. Efficient path counting transducers for minimum
Bayes-risk decoding of statistical machine translation lat-
tices. In Proceedings of ACL 2010.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in ma-
chine translation. In Proceedings of the EMNLP 2007.
Brown, Peter F., John Cocke, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, Fredrick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin. 1990. A sta-
tistical approach to machine translation. Computational
Linguistics, 16(2):79?85.
Callison-Burch, Chris, Philipp Koehn, Christof Monz, and
Josh Schroeder. 2009. Findings of the 2009 Workshop
on Statistical Machine Translation. In WMT 2009.
Chase, Lin Lawrance. 1997. Error-responsive feed-
back mechanisms for speech recognizers, Ph.D. Thesis,
Carnegie Mellon University.
Chiang, David. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
DeNero, John, David Chiang, and Kevin Knight. 2009. Fast
consensus decoding over translation forests. In Proceed-
ings of ACL-IJCNLP 2009.
Goel, V., S. Kumar, and W. Byrne. 2004. Segmental mini-
mum Bayes-risk decoding for automatic speech recogni-
tion. IEEE Transactions on Speech and Audio Process-
ing, 12:234?249.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo R. Banga, and
William Byrne. 2009. Hierarchical phrase-based trans-
lation with weighted finite state transducers. In Proceed-
ings of the 2009 Annual Conference of the NAACL.
Kneser, R. and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech, and
Signal Processing.
Knight, K and J Graehl. 2005. An overview of probabilis-
tic tree transducers for natural language processing. In
Proceedings of CICLING 2005.
Knight, K. 2007a. Capturing practical natural language
transformations. Machine Translation, 21(2).
Knight, Kevin. 2007b. Automatic language translation gen-
eration help needs badly. In MT Summit XI Workshop on
Using Corpora for NLG: Keynote Address.
Kumar, Shankar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine translation. In
NAACL 2004.
Kumar, Shankar, Wolfgang Macherey, Chris Dyer, and Franz
Och. 2009. Efficient minimum error rate training and
minimum bayes-risk decoding for translation hypergraphs
and lattices. In Proceedings of ACL-IJCNLP 2009.
Lavie, Alon and Michael J. Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine trans-
lation. Machine Translation Journal.
Li, Zhifei, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine translation.
In Proceedings of ACL-IJCNLP 2009.
Ma, Xiaoyi and Christopher Cieri. 2006. Corpus support for
machine translation at LDC. In LREC 2006.
Matusov, Evgeny, Nicola Ueffing, and Hermann Ney. 2006.
Computing consensus translation from multiple machine
translation systems using enhanced hypotheses align-
ment. In 11th Conference of the EACL.
Mohit, B., F. Liberato, and R. Hwa. 2009. Language model
adaptation for difficult-to-translate phrases. In Proceed-
ings of the 13th Annual Conference of the EAMT.
Mohri, Mehryar, Fernando Pereira, and Michael Riley. 2002.
Weighted finite-state transducers in speech recognition.
In CSL, volume 16, pages 69?88.
Oberlander, Jon and Chris Brew. 2000. Stochastic text gen-
eration. In Philosophical Transactions of the Royal Soci-
ety.
Och, F., D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, and D. Radev. 2004. A smorgasbord of features
for statistical machine translation. In Proceedings of the
HLT Conference of the NAACL.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a method for automatic evaluation of
machine translation. In Proceedings of ACL 2002.
Sim, K.-C., W. Byrne, M. Gales, H. Sahbi, and P.C. Wood-
land. 2007. Consensus network decoding for statisti-
cal machine translation system combination. In ICASSP
2007.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, , and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Pro-
ceedings of AMTA.
Tromble, Roy, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum Bayes-risk decoding
for statistical machine translation. In Proceedings of the
2008 Conference on EMNLP.
Ueffing, Nicola and Hermann Ney. 2007. Word-level confi-
dence estimation for machine translation. Computational
Linguistics, 33(1):9?40.
Vilar, D, G Leusch, H Ney, and R Banchs. 2007. Human
evaluation of machine translation through binary system
comparisons. In Proceedings of WMT 2007.
Zens, Richard and Hermann Ney. 2006. N -gram posterior
probabilities for statistical machine translation. In Pro-
ceedings of WMT 2006.
79
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 545?554,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Hierarchical Phrase-based Translation Grammars Extracted from
Alignment Posterior Probabilities
Adria` de Gispert, Juan Pino, William Byrne
Machine Intelligence Laboratory
Department of Engineering, University of Cambridge
Trumpington Street, CB2 1PZ, U.K.
{ad465|jmp84|wjb31}@eng.cam.ac.uk
Abstract
We report on investigations into hierarchi-
cal phrase-based translation grammars based
on rules extracted from posterior distributions
over alignments of the parallel text. Rather
than restrict rule extraction to a single align-
ment, such as Viterbi, we instead extract rules
based on posterior distributions provided by
the HMM word-to-word alignment model. We
define translation grammars progressively by
adding classes of rules to a basic phrase-based
system. We assess these grammars in terms
of their expressive power, measured by their
ability to align the parallel text from which
their rules are extracted, and the quality of the
translations they yield. In Chinese-to-English
translation, we find that rule extraction from
posteriors gives translation improvements. We
also find that grammars with rules with only
one nonterminal, when extracted from posteri-
ors, can outperform more complex grammars
extracted from Viterbi alignments. Finally, we
show that the best way to exploit source-to-
target and target-to-source alignment models
is to build two separate systems and combine
their output translation lattices.
1 Introduction
Current practice in hierarchical phrase-based trans-
lation extracts regular phrases and hierarchical rules
from word-aligned parallel text. Alignment models
estimated over the parallel text are used to generate
these alignments, but these models are then typically
used no further in rule extraction. This is less than
ideal because these alignment models, even if they
are not suitable for direct use in translation, can still
provide a great deal of useful information beyond a
single best estimate of the alignment of the parallel
text. Our aim is to use alignment models to generate
the statistics needed to build translation grammars.
The challenge in doing so is to extend the current
procedures, which are geared towards the use of a
single alignment, to make more of what can be pro-
vided by alignment models. The goal is to extract a
richer and more robust set of translation rules.
There are two aspects to hierarchical phrase-based
translation grammars which concern us. The first
is expressive power, which we take as the ability
to generate known reference translations from sen-
tences in the source language. This is determined
by the degree of phrase movements and the trans-
lations allowed by the rules of the grammar. For a
grammar with given types of rules, larger rule sets
will yield greater expressive power. This motivates
studies of grammars based on the rules which are ex-
tracted and the movement the grammar allows. The
second aspect is of course translation accuracy. If
the expressive power is adequate, then the desire is
that the grammar assigns a high score to a correct
translation.
We use posterior probabilities over parallel data to
address both of these aspects. These posteriors allow
us to build larger rule sets with improved transla-
tion accuracy. Ideally, for a sentence pair we wish to
consider all possible alignments between all possi-
ble source and target phrases within these sentences.
Given a grammar allowing certain types of move-
ment, we would then extract all possible parses that
are consistent with any alignments of these phrases.
545
To make this approach feasible, we consider only
phrase-to-phrase alignments with a high posterior
probability under the alignment models. In this way,
the alignment model probabilities guide rule extrac-
tion.
The paper is organized as follows. Section 2 re-
views related work on using posteriors to extract
phrases, as well as other approaches that tightly in-
tegrate word alignment and rule extraction. Sec-
tion 3 describes rule extraction based on word and
phrase posterior distributions provided by the HMM
word-to-word alignment model. In Section 4 we de-
fine translation grammars progressively by adding
classes of rules to a basic phrase-based system, mo-
tivating each rule type by the phrase movement it is
intended to achieve. In Section 5 we assess these
grammars in terms of their expressive power and the
quality of the translations they yield in Chinese-to-
English, showing that rule extraction from posteriors
gives translation improvements. We also find that
the best way to exploit source-to-target and target-
to-source alignment models is to build two sepa-
rate systems and combine their output translation
lattices. Section 6 presents the main conclusions of
this work.
2 Related Work
Some authors have previously addressed the limita-
tion caused by decoupling word alignment models
from grammar extraction. For instance Venugopal
et al (2008) extract rules from n-best lists of align-
ments for a syntax-augmented hierarchical system.
Alignment n-best lists are also used in Liu et al
(2009) to create a structure called weighted align-
ment matrices that approximates word-to-word link
posterior probabilities, from which phrases are ex-
tracted for a phrase-based system. Alignment pos-
teriors have been used before for extracting phrases
in non-hierarchical phrase-based translation (Venu-
gopal et al, 2003; Kumar et al, 2007; Deng and
Byrne, 2008).
In order to simplify hierarchical phrase-based
grammars and make translation feasible with rela-
tively large parallel corpora, some authors discuss
the need for various filters during rule extraction
(Chiang, 2007). In particular Lopez (2008) enforces
a minimum span of two words per nonterminal,
Zollmann et al (2008) use a minimum count thresh-
old for all rules, and Iglesias et al (2009) propose
a finer-grained filtering strategy based on rule pat-
terns. Other approaches include insisting that target-
side rules are well-formed dependency trees (Shen et
al., 2008).
We also note approaches to tighter coupling be-
tween translation grammars and alignments. Marcu
and Wong (2002) describe a joint-probability
phrase-based model for alignment, but the approach
is limited due to excessive complexity as Viterbi
inference becomes NP-hard (DeNero and Klein,
2008). More recently, Saers et al (2009) report
improvement on a phrase-based system where word
alignment has been trained with an inversion trans-
duction grammar (ITG) rather than IBM models.
Pauls et al (2010) also use an ITG to directly align
phrases to nodes in a string-to-tree model. Bayesian
methods have been recently developed to induce a
grammar directly from an unaligned parallel corpus
(Blunsom et al, 2008; Blunsom et al, 2009). Fi-
nally, Cmejrek et al (2009) extract rules directly
from bilingual chart parses of the parallel corpus
without using word alignments. We take a differ-
ent approach in that we aim to start with very strong
word alignment models and use them to guide gram-
mar extraction.
3 Rule Extraction from Alignment
Posteriors
The goal of rule extraction is to generate a set of
good-quality translation rules from a parallel cor-
pus. Rules are of the form X???,?,?? , where
?, ? ? {X ? T}+ are the source and target sides of
the rule, T denotes the set of terminals (words) and
? is a bijective function1 relating source and target
nonterminals X of each rule (Chiang, 2007). For
each ?, the probability over translations ? is set by
relative frequency over the extracted examples from
the corpus.
We take a general approach to rule extraction, as
described by the following procedure. For simplic-
ity we discuss the extraction of regular phrases, that
is, rules of the form X??w,w?, where w ? {T}+.
Section 3.3 extends this procedure to rules with non-
1This function is defined if there are at least two nontermi-
nals, and for clarity of presentation will be omitted in this paper
546
terminal symbols.
Given a sentence pair (fJ1 , eI1), the extraction al-
gorithm traverses the source sentence and, for each
sequence of terminals f j2j1 , it considers all possible
target-side sequences ei2i1 as translation candidates.
Each target-side sequence that satisfies the align-
ment constraints CA is ranked by the function fR.
For practical reasons, a set of selection criteria CS is
then applied to these ranked candidates and defines
the set of translations of the source sequence that are
extracted as rules. Each extracted rule is assigned a
count fC .
In this section we will explore variations of this
rule extraction procedure involving alternative def-
initions of the ranking and counting functions, fR
and fC , based on probabilities over alignment mod-
els.
Common practice (Koehn et al, 2003) takes a set
of word alignment links L and defines the alignment
constraints CA so that there is a consistency between
the links in the (f j2j1 , e
i2
i1) phrase pair. This is ex-
pressed by ?(j, i) ? L : (j ? [j1, j2]? i ? [i1, i2])?
(j 6? [j1, j2] ? i 6? [i1, i2]). If these constraints
are met, then alignment probabilities are ignored and
fR = fC = 1. We call this extraction Viterbi-based,
as the set of alignment links is generally obtained
after applying a symmetrization heuristic to source-
to-target and target-to-source Viterbi alignments.
In the following section we depart from this ap-
proach and apply novel functions to rank and count
target-side translations according to their quality in
the context of each parallel sentence, as defined by
the word alignment models. We also depart from
common practice in that we do not use a set of links
as alignment constraints. We thus find an increase
in the number of extracted rules, and consequently
better relative frequency estimates over translations.
3.1 Ranking and Counting Functions
We describe two alternative approaches to modify
the functions fR and fC so that they incorporate the
probabilities provided by the alignment models.
3.1.1 Word-to-word Alignment Posterior
Probabilities
Word-to-word alignment posterior probabilities
p(lji|fJ1 , eI1) express how likely it is that the words
in source position j and target position i are aligned
given a sentence pair. These posteriors can be effi-
ciently computed for Model 1, Model 2 and HMM,
as described in (Brown et al, 1993; Venugopal et al,
2003; Deng and Byrne, 2008).
We will use these posteriors in functions to
score phrase pairs. For a simple non-disjoint case
(f j2j1 , e
i2
i1) we use:
fR(f j2j1 , e
i2
i1) =
j2
?
j=j1
i2
?
i=i1
p(lji|fJ1 , eI1)
i2 ? i1 + 1
(1)
which is very similar to the score used for lexical
features in many systems (Koehn, 2010), with the
link posteriors for the sentence pair playing the role
of the Model 1 translation table.
For a particular source phrase, Equation 1 is not
a proper conditional probability distribution over all
phrases in the target sentence. Therefore it cannot be
used as such without further normalization. Indeed
we find that this distribution is too sharp and over-
emphasises short phrases, so we use fC = 1. How-
ever, it does allow us to rank target phrases as pos-
sible translations. In contrast to the common extrac-
tion procedure described in the previous section, the
ranking approach described here can lead to a much
more exhaustive extraction unless selection criteria
are applied. These we describe in Section 3.2.
We note that Equation 1 can be computed us-
ing link posteriors provided by alignment models
trained on either source-to-target or target-to-source
translation directions.
3.1.2 Phrase-to-phrase Alignment Posterior
Probabilities
Rather than limit ourselves to word-to-word
link posteriors we can define alignment proba-
bility distributions over phrase alignments. We
do this by defining the set of alignments A as
A(j1, j2; i1, i2) = {aJ1 : aj ? [i1, i2] iff j ?
[j1, j2]}, where aj is the random process that de-
scribes word-to-word alignments. These are the
alignments from which the phrase pair (f j2j1 , e
i2
i1)
would be extracted.
The posterior probability of these alignments
given the sentence pair is defined as follows:
p(A|eI1, fJ1 ) =
?
aJ1?A
p(fJ1 , aJ1 |eI1)
?
aJ1
p(fJ1 , aJ1 |eI1)
(2)
547
G0 G1 G2 G3
S??X,X? X??w X,X w? X??w X,X w? X??w X,X w?
S??S X,S X? X??X w,w X? X??X w,w X? X??X w,w X?
X??w,w? X??w X,w X? X??w X,w X?
X??w X w,w X w?
Table 1: Hierarchical phrase-based grammars containing different types of rules. The grammar expressivity is greater
as more types of rules are included. In addition to the rules shown in the respective columns, G1, G2 and G3 also
contain the rules of G0.
With IBM models 1 and 2, the numerator and de-
nominator in Equation 2 can be computed in terms
of posterior link probabilities (Deng, 2005). With
the HMM model, the denominator is computed us-
ing the forward algorithm while the numerator can
be computed using a modified forward algorithm
(Deng, 2005).
These phrase posteriors directly define a proba-
bility distribution over the alignments of translation
candidates, so we use them both for ranking and
scoring extracted rules, that is fR = fC = p. This
approach assigns a fractional count to each extracted
rule, which allows finer estimation of the forward
and backward translation probability distributions.
3.2 Alignment Constraints and Selection
Criteria
In order to keep this process computationally
tractable, some extraction constraints are needed. In
order to extract a phrase pair (f j2j1 , e
i2
i1), we define
the following:
? CA requires at least one pair of positions (j, i) :
(j ? [j1, j2] ? i ? [i1, i2]) with word-to-word
link posterior probability p(lji|fJ1 , eI1) > 0.5,
and that there is no pair of positions (j, i) : (j ?
[j1, j2]?i 6? [i1, i2])?(j 6? [j1, j2]?i ? [i1, i2])
with p(lji|fJ1 , eI1) > 0.5
? CS allows only the k best translation candidates
to be extracted. We use k = 3 for regular
phrases, and k = 2 for hierarchical rules.
Note that we do not discard rules according to
their scores fC at this point (unlike Liu et al
(2009)), since we prefer to add all phrases from
all sentence pairs before carrying out such filtering
steps.
Once all rules over the entire collection of paral-
lel sentences have been extracted, we require each
rule to occur at least nobs times and with a forward
translation probability p(?|?) > 0.01 to be used for
translation.
3.3 Extraction of Rules with Nonterminals
Extending the procedure previously described to
the case of more complex hierarchical rules includ-
ing one or even two nonterminals is conceptually
straightforward. It merely requires that we traverse
the source and target sentences and consider possi-
bly disjoint phrase pairs. Optionally, the alignment
constraints can also be extended to apply on the non-
terminal X.
Equation 1 is then only modified in the limits
of the product and summation, whereas Equation
2 remains unchanged, as long as the set of valid
alignments A is redefined. For example, for a rule
of the form X??w X w,w X w?, we use A ?
A(j1, j2; j3, j4; i1, i2; i3, i4).
4 Hierarchical Translation Grammar
Definition
In this section we define the hierarchical phrase-
based synchronous grammars we use for translation
experiments. Each grammar is defined by the type of
hierarchical rules it contains. The rule type can be
obtained by replacing every sequence of terminals
by a single symbol ?w?, thus ignoring the identity of
the words, but capturing its generalized structure and
the kind of reordering it encodes (this was defined as
rule pattern in Iglesias et al (2009)).
A monotonic phrase-based translation grammar
G0 can be defined as shown in the left-most col-
umn of Table 1; it includes all regular phrases, repre-
sented by the rule type X??w,w?, and the two glue
548
(G0) R1: S??X,X?
(G0) R2: X??s2 s3,t2?
(G1) R3: X??s1 X,X t3?
(G1) R4: X??X s4,t1 X?
(G2) R5: X??s1 X,t7 X?
(G3) R6: X??s1 X s4,t5 X t6?
Figure 1: Example of a hierarchical translation grammar and two parsing trees following alternative rule derivations
for the input sentence s1s2s3s4.
rules that allow concatenation. Our approach is now
simple: we extend this grammar by successively in-
corporating sets of hierarchical rules. The goal is to
obtain a grammar with few rule types but which is
capable of generating a rich set of translation candi-
dates for a given input sentence.
With this in mind, we define the following three
grammars, also summarized in Table 1:
? G1 := G0
?
{ X??w X,X w? , X??X w,w X? }. This
incorporates reordering capabilities with two
rule types that place the unique nonterminal
in an opposite position in each language; we
call these ?phrase swap rules?. Since all non-
terminals are of the same category X, nested
reordering is possible. However, this needs to
happen consecutively, i.e. a swap must apply
after a swap, or the rule is concatenated with
the glue rule.
? G2 := G1
?{ X??w X,w X? }. This
adds monotonic concatenation capabilities to
the previous translation grammar. The glue rule
already allows rule concatenation. However, it
does so at the S category, that is, it concate-
nates phrases and rules after they have been re-
ordered, in order to complete a sentence. With
this new rule type, G2 allows phrase/rule con-
catenation before reordering with another hier-
archical rule. Therefore, nested reordering does
not require successive swaps anymore.
? G3 := G2
?{ X??w X w,w X w? }. This
adds single nonterminal rules with disjoint ter-
minal sequences, which can encode a mono-
tonic or reordered relationship between them,
depending on what their alignment was in the
parallel corpus. Although one could expect the
movement captured by this phrase-disjoint rule
type to be also present in G2 (via two swaps or
one concatenation plus one swap), the terminal
sequences w may differ.
Figure 1 shows an example set of rules indicat-
ing to which of the previous grammars each rule be-
longs, and shows three translation candidates as gen-
erated by grammars G1 (left-most tree), G2 (mid-
dle tree) and G3 (right-most tree). Note that the
middle tree cannot be generated with G1 as it re-
quires monotonic concatenation before reordering
with rule R4.
The more rule types a hierarchical grammar con-
tains, the more different rule derivations and the
greater the search space of alternative translation
candidates. This is also connected to how many
rules are extracted per rule type. Ideally we would
like the grammar to be able to generate the correct
translation of a given input sentence, without over-
generating too many other candidates, as that makes
the translation task more difficult.
We will make use of the parallel data in measuring
the ability of a grammar to generate correct transla-
tions. By extracting rules from a parallel sentence,
we translate them and observe whether the transla-
tion grammar is able to produce the parallel target
translation. In Section 5.1 we evaluate this for a
Chinese-to-English task.
549
4.1 Reducing Grammar Redundancy
Let us discuss grammar G2 in more detail. As de-
scribed in the previous section, the motivation for in-
cluding rule type X??w X,w X? is that the gram-
mar be able to carry out monotonic concatenation
before applying another hierarchical rule with re-
ordering. This movement is permitted by this rule
type, but the use of a single nonterminal category X
also allows the grammar to apply the concatenation
after reordering, that is, immediately before the glue
rule is applied. This creates significant redundancy
in rule derivations, as this rule type is allowed to act
as a glue rule. For example, given an input sentence
s1s2 and the following simple grammar:
R0: S??X,X?
R1: S??S X,S X?
R2: X??s1,t1?
R3: X??s2,t2?
R4: X??s1 X,t1 X?
two derivations are possible: R2,R0,R3,R1 and
R3,R4,R0, and the translation result is identical.
To avoid this situation we introduce a nonterminal
M in the left-hand side of monotonic concatenation
rules of G2. All rules are allowed to use nontermi-
nals X and M in their right-hand side, except the
glue rules, which can only take X. In the context of
our example, R4 is substituted by:
R4a: M??s1 X,t1 X?
R4b: M??s1 M ,t1 M?
so that only the first derivation is possible:
R2,R0,R3,R1, because applying R3,R4a yields a non-
terminal M that cannot be taken by the glue rule R0.
5 Experiments
We report experiments in Chinese-to-English trans-
lation. Our system is trained on a subset of the
GALE 2008 evaluation parallel text;2 this is approx-
imately 50M words per language. We report trans-
lation results on a development set tune-nw and a
test set test-nw1. These contain translations pro-
duced by the GALE program and portions of the
newswire sections of MT02 through MT06. They
contain 1,755 sentences and 1,671 sentences respec-
tively. Results are also reported on a smaller held-
2See http://projects.ldc.upenn.edu/gale/data/catalog.html.
We excluded the UN material and the LDC2002E18,
LDC2004T08, LDC2007E08 and CUDonga collections.
 30
 40
 50
 60
 70
 80
V-st
V-ts
V-union
V-gdf
V-merge
WP-st
WP-ts
WP-merge
G0
G1
G2
G3
Figure 2: Percentage of parallel sentences successfully
aligned for various extraction methods and grammars.
out test set test-nw2, containing 60% of the NIST
newswire portion of MT06, that is, 369 sentences.
The parallel texts for both language pairs are
aligned using MTTK (Deng and Byrne, 2008). For
decoding we use HiFST, a lattice-based decoder im-
plemented with Weighted Finite State Transducers
(de Gispert et al, 2010). Likelihood-based search
pruning is applied if the number of states in the
lattice associated with each CYK grid cell exceeds
10,000, otherwise the entire search space is ex-
plored. The language model is a 4-gram language
model estimated over the English side of the paral-
lel text and the AFP and Xinhua portions of the En-
glish Gigaword Fourth Edition (LDC2009T13), in-
terpolated with a zero-cutoff stupid-backoff (Brants
et al, 2007) 5-gram estimated using 6.6B words of
English newswire text. In tuning the systems, stan-
dard MERT (Och, 2003) iterative parameter estima-
tion under IBM BLEU3 is performed on the devel-
opment sets.
5.1 Measuring Expressive Power
We measure the expressive power of the grammars
described in the previous section by running the
translation system in alignment mode (de Gispert
et al, 2010) over the parallel corpus. Conceptually,
this is equivalent to replacing the language model by
the target sentence and seeing if the system is able to
find any candidate. Here the weights assigned to the
3See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
550
Grammar Extraction # Rules tune-nw test-nw1 test-nw2
time prune BLEU BLEU BLEU
GH V-union 979149 3.7 0.3 35.1 35.6 37.6
V-union 613962 0.4 0.0 33.6 34.6 36.4
G1 WP-st 920183 0.9 0.0 34.3 34.8 37.5
PP-st 893542 1.4 0.0 34.4 35.1 37.7
V-union 734994 1.0 0.0 34.5 35.4 37.2
G2 WP-st 1132386 5.8 0.5 35.1 36.0 37.7
PP-st 1238235 7.8 0.7 35.5 36.4 38.2
V-union 966828 1.2 0.0 34.9 35.3 37.0
G3 WP-st 2680712 8.3 1.1 35.1 36.2 37.9
PP-st 5002168 10.7 2.6 35.5 36.4 38.5
Table 2: Chinese-to-English translation results with alternative grammars and extraction methods (lower-cased BLEU
shown). Time (secs/word) and prune (times/word) measurements done on tune-nw set.
rules are irrelevant, as only the ability of the gram-
mar to create a desired hypothesis is important.
We compare the percentage of target sentences
that can be successfully produced by grammars G0,
G1, G2 and G3 for the following extraction meth-
ods:
? Viterbi (V). This is the standard extraction
method based on a set of alignment links. We
distinguish four cases, depending on the model
used to obtain the set of links: source-to-
target (V-st), target-to-source (V-ts), and two
common symmetrization strategies: union (V-
union) and grow-diag-final (V-gdf), described
in (Koehn et al, 2003).
? Word Posteriors (WP). The extraction method
is based on word alignment posteriors de-
scribed in Section 3.1.1. These rules can be ob-
tained either from the posteriors of the source-
to-target (WP-st) or the target-to-source (WP-
ts) alignment models. We apply the alignment
constraints and selection criteria described in
Section 3.2. We do not report alignment per-
centages when using phrase posteriors (as de-
scribed in Section 3.1.2) as they are roughly
identical to the WP case.
? Finally, in both cases, we also report results
when merging the extracted rules in both direc-
tions into a single rule set (V-merge and WP-
merge).
Figure 2 shows the results obtained for a random
selection of 10,000 parallel corpus sentences. As ex-
pected, we can see that for any extraction method,
the percentage of aligned sentences increases when
switching from G0 to G1, G2 and G3. Posterior-
based extraction is shown to outperform standard
methods based on a Viterbi set of alignments for
nearly all grammars. The highest alignment percent-
ages are obtained when merging rules obtained un-
der models trained in each direction (WP-merge),
approximately reaching 80% for grammar G3.
The maximum rule span in alignment was al-
lowed to be 15 words, so as to be similar to transla-
tion, where the maximum rule span is 10 words. Re-
laxing this in alignment to 30 words yields approxi-
mately 90% coverage for WP-merge under G3.
We note that if alignment constraints CA and se-
lection criteria CS were not applied, that is k = ?,
then alignment percentages would be 100% even
for G0, but the extracted grammar would include
many noisy rules with poor generalization power
and would suffer from overgeneration.
5.2 Translation Results
In this section we investigate the translation perfor-
mance of each hierarchical grammar, as defined by
rules obtained from three rule extraction methods:
? Viterbi union (V-union). Standard rule extrac-
tion from the union of the source-to-target and
target-to-source alignment link sets.
551
? Word Posteriors (WP-st). Extraction based
on word posteriors as described in Section
3.1.1. The posteriors are provided by the
source-to-target algnment model. Alignment
constraints and selection criteria of Section 3.2
are applied, with nobs = 2.
? Phrase Posteriors (PP-st). Extraction based
on phrase alignment posteriors, as described
in Section 3.1.2, with fractional counts pro-
portional to the phrase probability under the
source-to-target algnment model. Alignment
constraints and selection criteria of Section 3.2
are applied, with nobs = 0.2.
Table 2 reports the translation results, as well as
the number of extracted rules in each case. It also
shows the following decoding statistics as measured
on the tune-nw set: decoding time in seconds per in-
put word, and number of instances of search pruning
(described in Section 5) per input word.
As a contrast, we extract rules according to the
heuristics introduced in (Chiang, 2007) and apply
the filters described in (Iglesias et al, 2009) to gen-
erate a standard hierarchical phrase-based grammar
GH . This uses rules with up to two nonadjacent non-
terminals, but excludes identical rule types such as
X??w X,w X? or X??w X1 w X2,w X1 w X2?,
which were reported to cause computational difficul-
ties without a clear improvement in translation (Igle-
sias et al, 2009).
Grammar expressivity. As expected, for the stan-
dard extraction method (see rows entitled V-union),
grammar G1 is shown to underperform all other
grammars due to its structural limitations. On the
other hand, grammar G2 obtains much better scores,
nearly generating the same translation quality as
the baseline grammar GH . Finally, G3 does not
prove able to outperform G2, which suggests that
the phrase-disjoint rules with one nonterminal are
redundant for the translation grammar.
Rule extraction method. For all grammars, we
find that the proposed extraction methods based on
alignment posteriors outperform standard Viterbi-
based extraction, with improvements ranging from
0.5 to 1.1 BLEU points for test-nw1 (depending on
the grammar) and from 1.0 to 1.5 for test-nw2. In
all cases, the use of phrase posteriors PP is the best
option. Interestingly, we find that G2 extracted with
WP and PP methods outperforms the more complex
GH grammar as obtained from Viterbi alignments.
Rule set statistics. For grammar G2 evaluated
on the tune-nw set, standard Viterbi-based extrac-
tion produces 0.7M rules, whereas the WP and PP
extraction methods yield 1.1M and 1.2M rules re-
spectively. We further analyse the sets of rules
X???,?,?? in terms of the number of distinct
source and target sequences ? and ? which are ex-
tracted. Viterbi extraction yields 82k distinct source
sequences whereas the WP and PP methods yield
116k and 146k sequences, respectively. In terms
of the average number of target sequences for each
source sequence, Viterbi extraction yields an aver-
age of 8.7 while WP and PP yield 9.7 and 8.4 rules
on average. This shows that method PP yields wider
coverage but with sharper forward rule translation
probability distributions than method WP, as the av-
erage number of translations per rule is determined
by the p(?|?) > 0.01 threshold mentioned in Sec-
tion 3.2.
Decoding time and pruning in search. In connec-
tion to the previous comments, we find an increased
need for search pruning, and subsequently slower
decoding speed, as the search space grows larger
with methods WP and PP. A larger search space is
created by the larger rule sets, which allows the sys-
tem to generate new hypotheses of better quality.
5.3 Rule Concatenation in Grammar G2
In Section 4.1 we described a strategy to reduce
grammar redundancy by introducing an additional
nonterminal M for monotonic concatenation rules.
We find that without this distinction among nonter-
minals, search pruning and decoding time are in-
creased by a factor of 1.5, and there is a slight degra-
dation in BLEU (?0.2) as more search errors are in-
troduced.
Another relevant aspect of this grammar is the ac-
tual rule type selected for monotonic concatenation.
We described using type X??w X,w X? (con-
catenation on the right), but one could also include
X??X w,X w? (concatenation on the left), or both,
for the same purpose. We evaluated the three alter-
natives and found that scores are identical when ei-
ther including right or left concatenation types, but
including both is harmful for performance, as the
need to prune and decoding time increase by a fac-
552
tor of 5 and 4, respectively, and we observe again a
slight degradation in performance.
Rule Extraction tune-nw test-nw1 test-nw2
V-st 34.7 35.6 37.5
V-ts 34.0 34.8 36.6
V-union 34.5 35.4 37.2
V-gdf 34.4 35.3 37.1
WP-st 35.1 36.0 37.7
WP-ts 34.5 35.0 37.0
PP-st 35.5 36.4 38.2
PP-ts 34.8 35.3 37.2
PP-merge 35.5 36.4 38.4
PP-merge-MERT 35.5 36.4 38.3
LMBR(V-st) 35.0 35.8 38.4
LMBR(V-st,V-ts) 35.5 36.3 38.9
LMBR(PP-st) 36.1 36.8 38.8
LMBR(PP-st,PP-ts) 36.4 36.9 39.3
Table 3: Translation results under grammar G2 with indi-
vidual rule sets, merged rule sets, and rescoring and sys-
tem combination with lattice-based MBR (lower-cased
BLEU shown)
5.4 Symmetrizing Alignments of Parallel Text
In this section we investigate extraction from align-
ments (and posterior distributions) over parallel text
which are generated using alignment models trained
in the source-to-target (st) and target-to-source (ts)
directions. Our motivation is that symmetrization
strategies have been reported to be beneficial for
Viterbi extraction methods (Och and Ney, 2003;
Koehn et al, 2003).
Results are shown in Table 3 for grammar G2. We
find that rules extracted under the source-to-target
alignment models (V-st, WP-st and PP-st) consis-
tently perform better than the V-ts, WP-ts and PP-
ts cases. Also, for Viterbi extraction we find that the
source-to-target V-st case performs better than any
of the symmetrization strategies, which contradicts
previous findings for non-hierarchical phrase-based
systems(Koehn et al, 2003).
We use the PP rule extraction method to extract
two sets of rules, under the st and ts alignment mod-
els respectively. We now investigate two ways of
merging these sets into a single grammar for trans-
lation. The first strategy is PP-merge and merges
both rule sets by assigning to each rule the maximum
count assigned by either alignment model. We then
extend the previous strategy by adding three binary
feature functions to the system, indicating whether
the rule was extracted under the ?st? model, the ?ts?
model or both. The motivation is that MERT can
weight rules differently according to the alignment
model they were extracted from. However, we do
not find any improvement with either strategy.
Finally, we use linearised lattice minimum Bayes-
risk decoding (Tromble et al, 2008; Blackwood et
al., 2010) to combine translation lattices (de Gis-
pert et al, 2010) as produced by rules extracted
under each alignment direction (see rows named
LMBR(V-st,V-ts) and LMBR(PP-st,PP-ts)). Gains
are consistent when comparing this to applying
LMBR to each of the best individual systems (rows
named LMBR(V-st) and LMBR(PP-st)). Overall,
the best-performing strategy is to extract two sets of
translation rules under the phrase pair posteriors in
each translation direction, and then to perform trans-
lation twice and merge the results.
6 Conclusion
Rule extraction based on alignment posterior proba-
bilities can generate larger rule sets. This results in
grammars with more expressive power, as measured
by the ability to align parallel sentences. Assign-
ing counts equal to phrase posteriors produces bet-
ter estimation of rule translation probabilities. This
results in improved translation scores as the search
space grows.
This more exhaustive rule extraction method per-
mits a grammar simplification, as expressed by the
phrase movement allowed by its rules. In particular
a simple grammar with rules of only one nontermi-
nal is shown to outperform a more complex gram-
mar built on rules extracted from Viterbi alignments.
Finally, we find that the best way to exploit align-
ment models trained in each translation direction is
to extract two rule sets based on alignment posteri-
ors, translate the input independently with each rule
set and combine translation output lattices.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
553
Agency, Contract No. HR0011- 06-C-0022.
References
Graeme Blackwood, Adria` de Gispert, and William
Byrne. 2010. Efficient Path Counting Transducers
for Minimum Bayes-Risk Decoding of Statistical Ma-
chine Translation Lattices. In Proceedings of ACL,
Short Papers, pages 27?32.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian Synchronous Grammar Induction. In Ad-
vances in Neural Information Processing Systems, vol-
ume 21, pages 161?168.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of the ACL, pages
782?790.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguis-
tics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Martin Cmejrek, Bowen Zhou, and Bing Xiang. 2009.
Enriching SCFG Rules Directly From Efficient Bilin-
gual Chart Parsing. In Proceedings of IWSLT, pages
136?143.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3).
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
HLT, Short Papers, pages 25?28.
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494?507.
Yonggang Deng. 2005. Bitext Alignment for Statisti-
cal Machine Translation. Ph.D. thesis, Johns Hopkins
University.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
the EACL, pages 380?388.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT/NAACL, pages 48?54.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar, Franz J. Och, and Wolfgang Macherey.
2007. Improving word alignment with bridge lan-
guages. In Proceedings of EMNLP-CoNLL, pages 42?
50.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of EMNLP, pages 1017?
1026.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of COLING, pages
505?512.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP, pages 133?139.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In Proceedings of
the HLT-NAACL, pages 118?126.
Markus Saers and Dekai Wu. 2009. Improving phrase-
based translation via word alignments from stochastic
inversion transduction grammars. In Proceedings of
the HLT-NAACL Workshop on Syntax and Structure in
Statistical Translation, pages 28?36.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk
decoding for statistical machine translation. In Pro-
ceedings of EMNLP, pages 620?629.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL, pages 319?
326.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: N-best
alignments and parses in mt training. In Proceedings
of AMTA, pages 192?201.
Andreas Zollmann, Ashish Venugopal, Franz J. Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of COLING, pages 1145?1152.
554
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373?1383,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Hierarchical Phrase-Based Translation Representations
Gonzalo Iglesias? Cyril Allauzen? William Byrne?
Adria` de Gispert? Michael Riley?
?Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
{gi212,wjb31,ad465}@eng.cam.ac.uk
? Google Research, 76 Ninth Avenue, New York, NY 10011
{allauzen,riley}@google.com
Abstract
This paper compares several translation rep-
resentations for a synchronous context-free
grammar parse including CFGs/hypergraphs,
finite-state automata (FSA), and pushdown
automata (PDA). The representation choice is
shown to determine the form and complex-
ity of target LM intersection and shortest-path
algorithms that follow. Intersection, shortest
path, FSA expansion and RTN replacement al-
gorithms are presented for PDAs. Chinese-to-
English translation experiments using HiFST
and HiPDT, FSA and PDA-based decoders,
are presented using admissible (or exact)
search, possible for HiFST with compact
SCFG rulesets and HiPDT with compact LMs.
For large rulesets with large LMs, we intro-
duce a two-pass search strategy which we then
analyze in terms of search errors and transla-
tion performance.
1 Introduction
Hierarchical phrase-based translation, using a syn-
chronous context-free translation grammar (SCFG)
together with an n-gram target language model
(LM), is a popular approach in machine transla-
tion (Chiang, 2007). Given a SCFG G and an n-
gram language model M , this paper focuses on how
to decode with them, i.e. how to apply them to the
source text to generate a target translation. Decod-
ing has three basic steps, which we first describe
in terms of the formal languages and relations in-
volved, with data representations and algorithms to
follow.
1. Translating the source sentence s with G
to give target translations: T = {s} ? G,
a (weighted) context-free language resulting
from the composition of a finite language and
the algebraic relation G for SCFG G.
2. Applying the language model to these target
translations: L=T ?M, a (weighted) context-
free language resulting from the intersection
of a context-free language and the regular lan-
guage M for M .
3. Searching for the translation and language
model combination with the highest-probablity
path: L?=argmaxl?LL
Of course, decoding requires explicit data represen-
tations and algorithms for combining and searching
them. In common to the approaches we will con-
sider here, s is applied to G by using the CYK algo-
rithm in Step 1 and M is represented by a finite au-
tomaton in Step 2. The choice of the representation
of T in many ways determines the remaining de-
coder representations and algorithms needed. Since
{s} is a finite language and we assume through-
out that G does not allow unbounded insertions,
T and L are, in fact, regular languages. As such,
T and L have finite automaton representations Tf
and Lf . In this case, weighted finite-state intersec-
tion and single-source shortest path algorithms (us-
ing negative log probabilities) can be used to solve
Steps 2 and 3 (Mohri, 2009). This is the approach
taken in (Iglesias et al, 2009a; de Gispert et al,
2010). Instead T and L can be represented by hy-
pergraphs Th and Lh (or very similarly context-free
rules, and-or trees, or deductive systems). In this
case, hypergraph intersection with a finite automa-
ton and hypergraph shortest path algorithms can be
used to solve Steps 2 and 3 (Huang, 2008). This
is the approach taken by Chiang (2007). In this
paper, we will consider another representation for
context-free languages T and L as well, pushdown
automata (PDA) Tp and Lp, familiar from formal
1373
language theory (Aho and Ullman, 1972). We will
describe PDA intersection with a finite automaton
and PDA shortest-path algorithms in Section 2 that
can be used to solve Steps 2 and 3. It cannot be
over-emphasized that the CFG, hypergraph and PDA
representations of T are used for their compactness
rather than for expressing non-regular languages.
As presented so far, the search performed in Step
3 is admissible (or exact) ? the true shortest path
is found. However, the search space in MT can be
quite large. Many systems employ aggressive prun-
ing during the shortest-path computation with little
theoretical or empirical guarantees of correctness.
Further, such pruning can greatly complicate any
complexity analysis of the underlying representa-
tions and algorithms. In this paper, we will exclude
any inadmissible pruning in the shortest-path algo-
rithm itself. This allows us in Section 3 to compare
the computational complexity of using these differ-
ent representations. We show that the PDA represen-
tation is particularly suited for decoding with large
SCFGs and compact LMs.
We present Chinese-English translation results
under the FSA and PDA translation representations.
We describe a two-pass translation strategy which
we have developed to allow use of the PDA repre-
sentation in large-scale translation. In the first pass,
translation is done using a lattice-generating version
of the shortest path algorithm. The full translation
grammar is used but with a compact, entropy-pruned
version (Stolcke, 1998) of the full language model.
This first-step uses admissible pruning and lattice
generation under the compact language model. In
the second pass, the original, unpruned LM is simply
applied to the lattices produced in the first pass. We
find that entropy-pruning and first-pass translation
can be done so as to introduce very few search errors
in the overall process; we can identify search errors
in this experiment by comparison to exact transla-
tion under the full translation grammar and language
model using the FSA representation. We then inves-
tigate a translation grammar which is large enough
that exact translation under the FSA representation
is not possible. We find that translation is possible
using the two-pass strategy with the PDA translation
representation and that gains in BLEU score result
from using the larger translation grammar.
1.1 Related Work
There is extensive prior work on computational ef-
ficiency and algorithmic complexity in hierarchical
phrase-based translation. The challenge is to find al-
gorithms that can be made to work with large trans-
lation grammars and large language models.
Following the original algorithms and analysis of
Chiang (2007), Huang and Chiang (2007) devel-
oped the cube-growing algorithm, and more recently
Huang and Mi (2010) developed an incremental de-
coding approach that exploits left-to-right nature of
the language models.
Search errors in hierarchical translation, and in
translation more generally, have not been as exten-
sively studied; this is undoubtedly due to the diffi-
culties inherent in finding exact translations for use
in comparison. Using a relatively simple phrase-
based translation grammar, Iglesias et al (2009b)
compared search via cube-pruning to an exact FST
implementation (Kumar et al, 2006) and found that
cube-pruning suffered significant search errors. For
Hiero translation, an extensive comparison of search
errors between the cube pruning and FSA imple-
mentation was presented by Iglesias et al (2009a)
and de Gispert et al (2010). Relaxation techniques
have also recently been shown to finding exact so-
lutions in parsing (Koo et al, 2010) and in SMT
with tree-to-string translation grammars and trigram
language models (Rush and Collins, 2011), much
smaller models compared to the work presented in
this paper.
Although entropy-pruned language models have
been used to produce real-time translation sys-
tems (Prasad et al, 2007), we believe our use of
entropy-pruned language models in two-pass trans-
lation to be novel. This is an approach that is widely-
used in automatic speech recognition (Ljolje et al,
1999) and we note that it relies on efficient represen-
tation of very large search spaces T for subsequent
rescoring, as is possible with FSAs and PDAs.
2 Pushdown Automata
In this section, we formally define pushdown au-
tomata and give intersection, shortest-path and re-
lated algorithms that will be needed later.
Informally, pushdown automata are finite au-
tomata that have been augmented with a stack. Typ-
1374
0
1a
2
?
(
3)b
0
1
a
2
?
(
?
3
)
?
b
(a) (b)
0
1
(
3
?
2
a
4(
)
5
b
)
0,?
1,(
?
3,?
?
2,(a
4,(?
?
5,(
b
?
(c) (d)
Figure 1: PDA Examples: (a) Non-regular PDA accept-
ing {anbn|n ? N}. (b) Regular (but not bounded-stack)
PDA accepting a?b?. (c) Bounded-stack PDA accepting
a?b? and (d) its expansion as an FSA.
ically this is done by adding a stack alphabet and la-
beling each transition with a stack operation (a stack
symbol to be pushed onto, popped or read from the
stack) in additon to the usual input label (Aho and
Ullman, 1972; Berstel, 1979) and weight (Kuich
and Salomaa, 1986; Petre and Salomaa, 2009). Our
equivalent representation allows a transition to be la-
beled by a stack operation or a regular input symbol
but not both. Stack operations are represented by
pairs of open and close parentheses (pushing a sym-
bol on and popping it from the stack). The advantage
of this representation is that is identical to the finite
automaton representation except that certain sym-
bols (the parentheses) have special semantics. As
such, several finite-state algorithms either immedi-
ately generalize to this PDA representation or do so
with minimal changes. The algorithms described in
this section have been implemented in the PDT ex-
tension (Allauzen and Riley, 2011) of the OpenFst
library (Allauzen et al, 2007).
2.1 Definitions
A (restricted) Dyck language consist of ?well-
formed? or ?balanced? strings over a finite num-
ber of pairs of parentheses. Thus the string
( [ ( ) ( ) ] { } [ ] ) ( ) is in the Dyck language over 3
pairs of parentheses.
More formally, let A and A be two finite alpha-
bets such that there exists a bijection f from A to
A. Intuitively, f maps an open parenthesis to its cor-
responding close parenthesis. Let a? denote f(a) if
a ? A and f?1(a) if a ? A. The Dyck language
DA over the alphabet A? = A ? A is then the lan-
guage defined by the following context-free gram-
mar: S ? , S ? SS and S ? aSa? for all a?A.
We define the mapping cA : A?? ? A?? as follow.
cA(x) is the string obtained by iteratively deleting
from x all factors of the form aa? with a ? A. Ob-
serve that DA=c?1A ().
Let A and B be two finite alphabets such that
B ? A, we define the mapping rB : A? ? B?
by rB(x1 . . . xn) = y1 . . . yn with yi = xi if xi ?B
and yi= otherwise.
A weighted pushdown automaton (PDA) T over
the tropical semiring (R ? {?},min,+,?, 0) is
a 9-tuple (?,?,?, Q,E, I, F, ?) where ? is the fi-
nite input alphabet, ? and ? are the finite open and
close parenthesis alphabets, Q is a finite set of states,
I ?Q the initial state, F ? Q the set of final states,
E ? Q ? (? ? ?? ? {}) ? (R ? {?}) ? Q a fi-
nite set of transitions, and ? : F ? R ? {?} the
final weight function. Let e= (p[e], i[e], w[e], n[e])
denote a transition in E.
A path pi is a sequence of transitions pi=e1 . . . en
such that n[ei]=p[ei+1] for 1 ? i < n. We then de-
fine p[pi] = p[e1], n[pi] = n[en], i[pi] = i[e1] ? ? ? i[en],
and w[pi]=w[e1] + . . . + w[en].
A path pi is accepting if p[pi] = I and n[pi] ? F .
A path pi is balanced if r??(i[pi]) ?D?. A balanced
path pi accepts the string x ? ?? if it is a balanced
accepting path such that r?(i[pi])=x.
The weight associated by T to a string x ? ??
is T (x) = minpi?P (x) w[pi] + ?(n[pi]) where P (x)
denotes the set of balanced paths accepting x. A
weighted language is recognizable by a weighted
pushdown automaton iff it is context-free. We de-
fine the size of T as |T |= |Q|+|E|.
A PDA T has a bounded stack if there exists K ?
N such that for any sub-path pi of any balanced path
in T : |c?(r??(i[pi]))| ? K . If T has a bounded stack,
then it represents a regular language. Figure 1 shows
non-regular, regular and bounded-stack PDAs.
A weighted finite automaton (FSA) can be viewed
as a PDA where the open and close parentheses al-
phabets are empty, see (Mohri, 2009) for a stand-
alone definition.
1375
2.2 Expansion Algorithm
Given a bounded-stack PDA T , the expansion of T
is the FSA T ? equivalent to T defined as follows.
A state in T ? is a pair (q, z) where q is a state in T
and z ???. A transition (q, a, w, q?) in T results in
a transition ((q, z), a?, w, (q?, z?)) in T ? only when:
(a) a?? ? {}, z? =z and a? =a, (b) a??, z? =za
and a? = , or (c) a ? ?, z? is such that z = z?a
and a? = . The initial state of T ? is I ? = (I, ). A
state (q, z) in T ? is final if q is final in T and z = 
(??((q, ))=?(q)). The set of states of T ? is the set of
pairs (q, z) that can be reached from an initial state
by transitions defined as above. The condition that
T has a bounded stack ensures that this set is finite
(since it implies that for any (q, z), |z| ? K).
The complexity of the algorithm is linear in
O(|T ?|)=O(e|T |). Figure 1d show the result of the
algorithm when applied to the PDA of Figure 1c.
2.3 Intersection Algorithm
The class of weighted pushdown automata is closed
under intersection with weighted finite automata
(Bar-Hillel et al, 1964; Nederhof and Satta, 2003).
Considering a pair (T1, T2) where one element is an
FSA and the other element a PDA, then there exists
a PDA T1?T2, the intersection of T1 and T2, such
that for all x ? ??: (T1?T2)(x) = T1(x)+T2(x).
We assume in the following that T2 is an FSA. We
also assume that T2 has no input- transitions. When
T2 has input- transitions, an epsilon filter (Mohri,
2009; Allauzen et al, 2011) generalized to handle
parentheses can be used.
A state in T =T1?T2 is a pair (q1, q2) where q1 is
a state of T1 and q2 a state of T2. The initial state is
I=(I1, I2). Given a transition e1=(q1, a, w1, q?1) in
T1, transitions out of (q1, q2) in T are obtained using
the following rules.
If a ? ?, then e1 can be matched with a tran-
sition (q2, a, w2, q?2) in T2 resulting a transition
((q1, q2), a, w1+w2, (q?1, q?2)) in T .
If a = , then e1 is matched with staying in q2
resulting in a transition ((q1, q2), , w1, (q?1, q2)).
Finally, if a ? ??, e1 is also matched
with staying in q2, resulting in a transition
((q1, q2), a, w1, (q?1, q2)) in T .
A state (q1, q2) in T is final when both q1 and q2
are final, and then ?((q1, q2))=?1(q1)+?2(q2).
SHORTESTDISTANCE(T )
1 for each q ? Q and a ? ? do
2 B[q, a]? ?
3 GETDISTANCE(T, I)
4 return d[f, I ]
RELAX(q, s, w,S)
1 if d[q, s] > w then
2 d[q, s]? w
3 if q 6? S then
4 ENQUEUE(S , q)
GETDISTANCE(T,s)
1 for each q ? Q do
2 d[q, s]??
3 d[s, s]? 0
4 Ss ? s
5 while Ss 6=? do
6 q ? HEAD(Ss)
7 DEQUEUE(Ss)
8 for each e ? E[q] do
9 if i[e] ? ? ? {} then
10 RELAX(n[e], s, d[q, s] + w[e],Ss)
11 elseif i[e] ? ? then
12 B[s, i[e]]? B[s, i[e]] ? {e}
13 elseif i[e] ? ? then
14 if d[n[e], n[e]] is undefined then
15 GETDISTANCE(T, n[e])
16 for each e? ? B[n[e], i[e]] do
17 w? d[q, s] +w[e] + d[p[e?], n[e]] + w[e?]
18 RELAX(n[e?], s, w,Ss)
Figure 2: PDA shortest distance algorithm. We assume
that F ={f} and ?(f)=0 to simplify the presentation.
The complexity of the algorithm is in O(|T1||T2|).
2.4 Shortest Distance and Path Algorithms
A shortest path in a PDA T is a balanced accepting
path with minimal weight and the shortest distance
in T is the weight of such a path. We show that when
T has a bounded stack, shortest distance and short-
est path can be computed in O(|T |3 log |T |) time
(assuming T has no negative weights) and O(|T |2)
space.
Given a state s in T with at least one incoming
open parenthesis transition, we denote by Cs the set
of states that can be reached from s by a balanced
path. If s has several incoming open parenthesis
transitions, a naive implementation might lead to the
states in Cs to be visited up to exponentially many
times. The basic idea of the algorithm is to memo-
ize the shortest distance from s to states in Cs. The
1376
pseudo-code is given in Figure 2.
GETDISTANCE(T, s) starts a new instance of the
shortest-distance algorithm from s using the queue
Ss, initially containing s. While the queue is not
empty, a state is dequeued and its outgoing transi-
tions examined (line 5-9). Transitions labeled by
non-parenthesis are treated as in Mohri (2009) (line
9-10). When the considered transition e is labeled by
a close parenthesis, it is remembered that it balances
all incoming open parentheses in s labeled by i[e]
by adding e to B[s, i[e]] (line 11-12). Finally, when
e is labeled with an open parenthesis, if its destina-
tion has not already been visited, a new instance is
started from n[e] (line 14-15). The destination states
of all transitions balancing e are then relaxed (line
16-18).
The space complexity of the algorithm is
quadratic for two reasons. First, the number of
non-infinity d[q, s] is |Q|2. Second, the space re-
quired for storing B is at most in O(|E|2) since
for each open parenthesis transition e, the size of
|B[n[e], i[e]]| is O(|E|) in the worst case. This
last observation also implies that the cumulated
number of transitions examined at line 16 is in
O(N |Q| |E|2) in the worst case, where N denotes
the maximal number of times a state is inserted in
the queue for a given call of GETDISTANCE. As-
suming the cost of a queue operation is ?(n) for a
queue containing n elements, the worst-case time
complexity of the algorithm can then be expressed
as O(N |T |3 ?(|T |)). When T contains no negative
weights, using a shortest-first queue discipline leads
to a time complexity in O(|T |3 log |T |). When all
the Cs?s are acyclic, using a topological order queue
discipline leads to a O(|T |3) time complexity.
In effect, we are solving a k-sources shortest-
path problem with k single-source solutions. A po-
tentially better approach might be to solve the k-
sources or k-pairs problem directly (Hershberger et
al., 2003).
When T has been obtained by converting an RTN
or an hypergraph into a PDA (Section 2.5), the poly-
nomial dependency in |T | becomes a linear depen-
dency both for the time and space complexities. In-
deed, for each q in T , there exists a unique s such
that d[q, s] is non-infinity. Moreover, for each close
parenthesis transistion e, there exists a unique open
parenthesis transition e? such that e?B[n[e?], i[e?]].
When each component of the RTN is acyclic, the
complexity of the algorithm is hence in O(|T |) in
time and space.
The algorithm can be modified to compute the
shortest path by keeping track of parent pointers.
2.5 Replacement Algorithm
A recursive transition network (RTN) can be speci-
fied by (N,?, (T?)??N , S) where N is an alphabet
of nonterminals, ? is the input alphabet, (T?)??N is
a family of FSAs with input alphabet ? ? N , and
S?N is the root nonterminal.
A string x ? ?? is accepted by R if there exists
an accepting path pi in TS such that recursively re-
placing any transition with input label ? ?N by an
accepting path in T? leads to a path pi? with input x.
The weight associated by R is the minimum over all
such pi? of w[pi?]+?S(n[pi?]).
Given an RTN R, the replacement of R is the
PDA T equivalent to R defined by the 9-tuple
(?,?,?, Q,E, I, F, ?, ?) with ?=Q=???N Q? ,
I = IS , F = FS , ?= ?S , and E =
?
??N
?
e?E? Ee
where Ee = {e} if i[e] 6? N and Ee =
{(p[e], n[e], w[e], I?), (f, n[e], ??(f), n[e])|f ?F?}
with ?= i[e]?N otherwise.
The complexity of the construction is in O(|T |).
If |F? | = 1, then |T | = O(
?
??N |T? |) = O(|R|).
Creating a superfinal state for each T? would lead to
a T whose size is always linear in the size of R.
3 Hierarchical Phrase-Based Translation
Representation
In this section, we compare several different repre-
sentations for the target translations T of the source
sentence s by synchronous CFG G prior to language
model M application. As discussed in the introduc-
tion, T is a context-free language. For example, sup-
pose it corresponds to:
S?abXdg, S?acXfg, and X?bc.
Figure 3 shows several alternative representations of
T : Figure 3a shows the hypergraph representation of
this grammar; there is a 1:1 correspondence between
each production in the CFG and each hyperedge in
the hypergraph. Figure 3b shows the RTN represen-
tation of this grammar with a 1:1 correspondence be-
tween each production in the CFG and each path in
the RTN; this is the translation representation pro-
1377
SX
3 3
a
1 1
b
2
1
c
2
2
d
4
f
4
g
5 5
(a) Hypergraph
0
1a
6
a
2b
7c
3X 4d 5g
8X 9f 10g 11 12b 13c
S X
(b) RTN
0
1a
6
a
2b
7c
11
(
12b
3 4d 5g
[
8 9f 10g
13c
)
]
(c) PDA
0,?
1,?a
6,?
a
2,?b
7,?c
11,(?
11,[?
12,(b
12,[b
13,(c
13,[c
3,??
8,??
4,?d
9,?f
5,?g
10,?g
(d) FSA
Figure 3: Alternative translation representations
duced by the HiFST decoder (Iglesias et al, 2009a;
de Gispert et al, 2010). Figure 3c shows the push-
down automaton representation generated from the
RTN with the replacement algorithm of Section 2.5.
Since s is a finite language and G does not allow
unbounded insertion, Tp has a bounded stack and
T is, in fact, a regular language. Figure 3d shows
the finite-state automaton representation of T gen-
erated by the PDA using the expansion algorithm
of Section 2.2. The HiFST decoder converts its
RTN translation representation immediately into the
finite-state representation using an algorithm equiv-
alent to converting the RTN into a PDA followed by
PDA expansion.
As shown in Figure 4, an advantage of the RTN,
PDA, and FSA representations is that they can bene-
fit from FSA epsilon removal, determinization and
minimization algorithms applied to their compo-
nents (for RTNs and PDAs) or their entirety (for
FSAs). For the complexity discussion below, how-
ever, we disregard these optimizations. Instead we
focus on the complexity of each MT step described
in the introduction:
1. SCFG Translation: Assuming that the parsing
of the input is performed by a CYK parse, then
the CFG, hypergraph, RTN and PDA represen-
0 1a
2b
3
c
4X
5X
6
d
f 7
g
0 1b 2c
S X
(a) RTN
0 1a
2b
3
c 8
(
[ 9
b
4
6
d
7g
5
f1 0c
)
]
(b) PDA
0 1a
2b
3
c
4b
5b
6c
7c
8
d
f 9
g
(c) FSA
Figure 4: Optimized translation representations
tations can be generated in O(|s|3|G|) time and
space (Aho and Ullman, 1972). The FSA rep-
resentation can require an additional O(e|s|3|G|)
time and space since the PDA expansion can be
exponential.
2. Intersection: The intersection of a CFG Th
with a finite automaton M can be performed by
the classical Bar-Hillel algorithm (Bar-Hillel
et al, 1964) with time and space complex-
ity O(|Th||M |3).1 The PDA intersection algo-
rithm from Section 2.3 has time and space com-
plexity O(|Tp||M |). Finally, the FSA intersec-
tion algorithm has time and space complexity
O(|Tf ||M |) (Mohri, 2009).
3. Shortest Path: The shortest path algorithm on
the hypergraph, RTN, and FSA representations
requires linear time and space (given the under-
lying acyclicity) (Huang, 2008; Mohri, 2009).
As presented in Section 2.4, the PDA rep-
resentation can require time cubic and space
quadratic in |M |.2
Table 1 summarizes the complexity results. Note
the PDA representation is equivalent in time and su-
perior in space to the CFG/hypergraph representa-
tion, in general, and it can be superior in both space
1The modified Bar-Hillel construction described by Chi-
ang (2007) has time and space complexity O(|Th||M |4); the
modifications were introduced presumably to benefit the subse-
quent pruning method employed (but see Huang et al (2005)).
2The time (resp. space) complexity is not cubic (resp.
quadratic) in |Tp||M |. Given a state q in Tp, there exists a
unique sq such that q belongs to Csq . Given a state (q1, q2)
in Tp ?M , (q1, q2) ? C(s1,s2) only if s1 = sq1 , and hence
(q1, q2) belongs to at most |M | components.
1378
Representation Time Complexity Space Complexity
CFG/hypergraph O(|s|3 |G| |M |3) O(|s|3 |G| |M |3)
PDA O(|s|3 |G| |M |3) O(|s|3 |G| |M |2)
FSA O(e|s|3|G| |M |) O(e|s|3|G| |M |)
Table 1: Complexity using various target translation rep-
resentations.
and time to the FSA representation depending on the
relative SCFG and LM sizes. The FSA representa-
tion favors smaller target translation sets and larger
language models. Should a better complexity PDA
shortest path algorithm be found, this conclusion
could change. In practice, the PDA and FSA rep-
resentations benefit hugely from the optimizations
mentioned above, these optimizations improve the
time and space usage by one order of magnitude.
4 Experimental Framework
We use two hierarchical phrase-based SMT de-
coders. The first one is a lattice-based decoder im-
plemented with weighted finite-state transducers (de
Gispert et al, 2010) and described in Section 3. The
second decoder is a modified version using PDAs
as described in Section 2. In order to distinguish
both decoders we call them HiFST and HiPDT, re-
spectively. The principal difference between the two
decoders is where the finite-state expansion step is
done. In HiFST, the RTN representation is immedi-
ately expanded to an FSA. In HiPDT, this expansion
is delayed as late as possible - in the output of the
shortest path algorithm. Another possible configu-
ration is to expand after the LM intersection step but
before the shortest path algorithm; in practice this is
quite similar to HiFST.
In the following sections we report experiments
in Chinese-to-English translation. For translation
model training, we use a subset of the GALE 2008
evaluation parallel text;3 this is 2.1M sentences and
approximately 45M words per language. We re-
port translation results on a development set tune-nw
(1,755 sentences) and a test set test-nw (1,671 sen-
tences). These contain translations produced by the
GALE program and portions of the newswire sec-
tions of MT02 through MT06. In tuning the sys-
3See http://projects.ldc.upenn.edu/gale/data/catalog.html.
We excluded the UN material and the LDC2002E18,
LDC2004T08, LDC2007E08 and CUDonga collections.
0 7.5? 10?9 7.5? 10?8 7.5? 10?7
207.5 20.2 4.1 0.9
Table 2: Number of ngrams (in millions) in the 1st pass 4-gram
language models obtained with different ? values (top row).
tems, standard MERT (Och, 2003) iterative param-
eter estimation under IBM BLEU4 is performed on
the development set.
The parallel corpus is aligned using MTTK (Deng
and Byrne, 2008) in both source-to-target and
target-to-source directions. We then follow stan-
dard heuristics (Chiang, 2007) and filtering strate-
gies (Iglesias et al, 2009b) to extract hierarchical
phrases from the union of the directional word align-
ments. We call a translation grammar the set of rules
extracted from this process. We extract two transla-
tion grammars:
? A restricted grammar where we apply the fol-
lowing additional constraint: rules are only
considered if they have a forward translation
probability p > 0.01. We call this G1. As will
be discussed later, the interest of this grammar
is that decoding under it can be exact, that is,
without any pruning in search.
? An unrestricted one without the previous con-
straint. We call this G2. This is a superset of
the previous grammar, and exact search under
it is not feasible for HiFST: pruning is required
in search.
The initial English language model is a Kneser-
Ney 4-gram estimated over the target side of the par-
allel text and the AFP and Xinhua portions of mono-
lingual data from the English Gigaword Fourth Edi-
tion (LDC2009T13). This is a total of 1.3B words.
We will call this language model M1. For large lan-
guage model rescoring we also use the LM M2 ob-
tained by interpolating M1 with a zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram estimated using
6.6B words of English newswire text.
We next describe how we build translation sys-
tems using entropy-pruned language models.
1. We build a baseline HiFST system that uses M1
and a hierarchical grammar G, parameters be-
ing optimized with MERT under BLEU.
4See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
1379
2. We then use entropy-based pruning of the lan-
guage model (Stolcke, 1998) under a relative
perplexity threshold of ? to reduce the size of
M1. We will call the resulting language model
as M?1 . Table 2 shows the number of n-grams
(in millions) obtained for different ? values.
3. We translate with M?1 using the same param-
eters obtained in MERT in step 1, except for
the word penalty, tuned over the lattices under
BLEU performance. This produces a transla-
tion lattice in the topmost cell that contains hy-
potheses with exact scores under the translation
grammar and M?1 .
4. Translation lattices in the topmost cell are
pruned with a likelihood-based beam width ?.
5. We remove the M?1 scores from the pruned
translation lattices and reapply M1, moving the
word penalty back to the original value ob-
tained in MERT. These operations can be car-
ried out efficiently via standard FSA opera-
tions.
6. Additionally, we can rescore the translation lat-
tices obtained in steps 1 or 5 with the larger
language model M2. Again, this can be done
via standard FSA operations.
Note that if ?=? or if ?=0, the translation lattices
obtained in step 1 should be identical to the ones of
step 5. While the goal is to increase ? to reduce the
size of the language model used at Step 3, ? will
have to increase accordingly so as to avoid pruning
away desirable hypotheses in Step 4. If ? defines
a sufficiently wide beam to contain the hypotheses
which would be favoured by M1, faster decoding
with M?1 would be possible without incurring search
errors M1. This is investigated next.
5 Entropy-Pruned LM in Rescoring
In Table 3 we show translation performance under
grammar G1 for different values of ?. Performance
is reported after first-pass decoding with M?1 (see
step 3 in Section 4), after rescoring with M1 (see
step 5) and after rescoring with M2 (see step 6). The
baseline (experiment number 1) uses ? = 0 (that is,
M1) for decoding.
Under translation grammar G1, HiFST is able to
generate an FSA with the entire space of possible
candidate hypotheses. Therefore, any degradation
in performance is only due to the M?1 involved in
decoding and the ? applied prior to rescoring.
As shown in row number 2, for ? ? 10?9 the
system provides the same performance to the base-
line when ? > 8, while decoding time is reduced
by roughly 40%. This is because M?1 is 10% of the
size of the original language model M1, as shown
in Table 2. As M?1 is further reduced by increas-
ing ? (see rows number 3 and 4), decoding time is
also reduced. However, the beam width ? required
in order to recover the good hypotheses in rescoring
increases, reaching 12 for experiment 3 and 15 for
experiment 4.
Regarding rescoring with the larger M2 (step 6
in Section 4), the system is also able to match the
baseline performance as long as ? is wide enough,
given the particular M?1 used in first-pass decoding.
Interestingly, results show that a similar ? value is
needed when rescoring either with M1 or M2.
The usage of entropy-pruned language models in-
crements speed at the risk of search errors. For in-
stance, comparing the outputs of systems 1 and 2
with ?=10 in Table 3 we find 45 different 1-best hy-
potheses, even though the BLEU score is identical.
In other words, we have 45 cases in which system 2
is not able to recover the baseline output because the
1st-pass likelihood beam ? is not wide enough. Sim-
ilarly, system 3 fails in 101 cases (? =12) and sys-
tem 4 fails in 95 cases. Interestingly, some of these
sentences would require impractically huge beams.
This might be due to the Kneser-Ney smoothing,
which interacts badly with entropy pruning (Chelba
et al, 2010).
6 Hiero with PDAs and FSAs
In this section we contrast HiFST with HiPDT under
the same translation grammar and entropy-pruned
language models. Under the constrained grammar
G1 their performance is identical as both decoders
can generate the entire search space which can then
be rescored with M1 or M2 as shown in the previous
section.
Therefore, we now focus on the unconstrained
grammar G2, where exact search is not feasible for
HiFST. In order to evaluate this problem, we run
both decoders over tune-nw, restricting memory us-
age to 10 gigabytes. If this limit is reached in decod-
1380
HiFST (G1 + M?1 ) +M1 +M2
# ? tune-nw test-nw time ? tune-nw test-nw tune-nw test-nw
1 0 (M1) 34.3 34.5 0.68 - - - 34.8 35.6
2 7.5? 10?9 32.0 32.8 0.38 10 34.8 35.6
9 34.3 34.5 34.9 35.5
8
3 7.5? 10?8 29.5 30.0 0.28 12 34.2 34.5 34.7 35.6
9 34.3 34.4 34.8 35.2
8 34.2 35.1
4 7.5? 10?7 26.0 26.4 0.20 15 34.2 34.5 34.7 35.6
12 34.4 35.5
Table 3: Results (lowercase IBM BLEU scores) under G1 with various M?1 as obtained with several values of ?.
Performance in subsequent rescoring with M1 and M2 after likelihood-based pruning of the translation lattices for
various ? is also reported. Decoding time, in seconds/word over test-nw, refers strictly to first-pass decoding.
Exact search for G2 + M?1 with memory usage under 10 GB
# ? HiFST HiPDT
Success Failure Success Failure
Expand Compose Compose Expand
2 7.5? 10?9 12 51 37 40 8 52
3 7.5? 10?8 16 53 31 76 1 23
4 7.5? 10?7 18 53 29 99.8 0 0.2
Table 4: Percentage of success in producing the 1-best translation under G2 with various M?1 when applying a hard
memory limitation of 10 GB, as measured over tune-nw (1755 sentences). If decoder fails, we report what step was
being done when the limit was reached. HiFST could be expanding into an FSA or composing the FSA with M?1 ;
HiPDT could be PDA composing with M?1 or PDA expanding into an FSA.
HiPDT (G2 + M?1 ) +M1 +M2
? tune-nw test-nw ? tune-nw test-nw tune-nw test-nw
7.5? 10?7 25.7 26.3 15 34.6 34.8 35.2 36.1
Table 5: HiPDT performance on grammar G2 with ? = 7.5 ? 10?7. Exact search with HiFST is not possible under
these conditions: pruning during search would be required.
ing, the process is killed5. We report what internal
decoding operation caused the system to crash. For
HiFST, these include expansion into an FSA (Ex-
pand) and subsequent intersection with the language
model (Compose). For HiPDT, these include PDA
intersection with the language model (Compose) and
subsequent expansion into an FSA (Expand), using
algorithms described in Section 2.
Table 4 shows the number of times each decoder
succeeds in finding a hypothesis given the memory
limit, and the operations being carried out when they
fail to do so, when decoding with various M?1 . With
?=7.5? 10?9 (row 2), HiFST can only decode 218
sentences, while HiPDT succeeds in 703 cases. The
5We used ulimit command. The experiment was carried out
over machines with different configurations and load. There-
fore, these numbers must be considered as approximate values.
differences between both decoders increase as the
M?1 is more reduced, and for ?=7.5?10?7 (row 4),
HiPDT is able to perform exact search over all but
three sentences.
Table 5 shows performance using the latter con-
figuration (Table 4, row 4). After large language
model rescoring, HiPDT improves 0.5 BLEU over
baseline with G1 (Table 3, row 1).
7 Discussion and Conclusion
HiFST fails to decode mainly because the expansion
into an FST leads to far too big search spaces (e.g.
fails 938 times under ? = 7.5 ? 10?8). If it suc-
ceeds in expanding the search space into an FST,
the decoder still has to compose with the language
model, which is also critical in terms of memory us-
1381
age (fails 536 times). In contrast, HiPDT creates a
PDA, which is a more compact representation of the
search space and allows efficient intersection with
the language model before expansion into an FST.
Therefore, the memory usage is considerably lower.
Nevertheless, the complexity of the language model
is critical for the PDA intersection and very specially
the PDA expansion into an FST (fails 403 times for
?=7.5? 10?8).
With the algorithms presented in this paper, de-
coding with PDAs is possible for any translation
grammar as long as an entropy pruned LM is used.
While this allows exact decoding, it comes at the
cost of making decisions based on less complex
LMs, although this has been shown to be an ad-
equate strategy when applying compact CFG rule-
sets. On the other hand, HiFST cannot decode under
large translation grammars, thus requiring pruning
during lattice construction, but it can apply an un-
pruned LM in this process. We find that with care-
fully designed pruning strategies, HiFST can match
the performance of HiPDT reported in Table 5. But
without pruning in search, expansion directly into an
FST would lead to an explosion in terms of memory
usage. Of course, without memory constraints both
strategies would reach the same performance.
Overall, these results suggest that HiPDT is more
robust than HiFST when using complex hierarchi-
cal grammars. Conversely, FSTs might be more
efficient for search spaces described by more con-
strained hierarchical grammars. This suggests that
a hybrid solution could be effective: we could use
PDAs or FSTs e.g. depending on the number of
states of the FST representing the expanded search
space, or other conditions.
8 Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Advanced
Research Projects Agency, Contract No.HR0011-
06-C-0022, and a Google Faculty Research Award,
May 2010.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation and Compiling, volume 1-2.
Prentice-Hall.
Cyril Allauzen and Michael Riley, 2011. Pushdown
Transducers. http://pdt.openfst.org.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of CIAA, pages 11?23.
http://www.openfst.org.
Cyril Allauzen, Michael Riley, and Johan Schalkwyk.
2011. Filters for efficient composition of weighted
finite-state transducers. In Proceedings of CIAA, vol-
ume 6482 of LNCS, pages 28?38. Springer.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Y. Bar-Hillel, editor, Language and Information: Se-
lected Essays on their Theory and Application, pages
116?150. Addison-Wesley.
Jean Berstel. 1979. Transductions and Context-Free
Languages. Teubner.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng
Xu. 2010. Study on interaction between entropy prun-
ing and kneser-ney smoothing. In Proceedings of In-
terspeech, pages 2242?2245.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3).
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494?507.
Manfred Drosde, Werner Kuick, and Heiko Vogler, ed-
itors. 2009. Handbook of Weighted Automata.
Springer.
John Hershberger, Subhash Suri, and Amit Bhosle. 2003.
On the difficulty of some shortest path problems. In
Proceedings of STACS, volume 2607 of LNCS, pages
343?354. Springer.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL, pages 144?151.
1382
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with hooks.
In Proceedings of the Ninth International Workshop
on Parsing Technology, Parsing ?05, pages 65?73,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Liang Huang. 2008. Advanced dynamic programming in
semiring and hypergraph frameworks. In Proceedings
of COLING, pages 1?18.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-based
translation with weighted finite state transducers. In
Proceedings of NAACL-HLT, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL, pages 380?388.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP, pages 1288?1298.
Werner Kuich and Arto Salomaa. 1986. Semirings, au-
tomata, languages. Springer.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Andrej Ljolje, Fernando Pereira, and Michael Riley.
1999. Efficient general lattice generation and rescor-
ing. In Proceedings of Eurospeech, pages 1251?1254.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Drosde et al (Drosde et al, 2009), chapter 6, pages
213?254.
Mark-Jan Nederhof and Giorgio Satta. 2003. Probabilis-
tic parsing as intersection. In Proceedings of 8th In-
ternational Workshop on Parsing Technologies, pages
137?148.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Ion Petre and Arto Salomaa. 2009. Algebraic systems
and pushdown automata. In Drosde et al (Drosde et
al., 2009), chapter 7, pages 257?289.
R. Prasad, K. Krstovski, F. Choi, S. Saleem, P. Natarajan,
M. Decerbo, and D. Stallard. 2007. Real-time speech-
to-speech translation for pdas. In Proceedings of IEEE
International Conference on Portable Information De-
vices, pages 1 ?5.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
lagrangian relaxation. In Proceedings of ACL-HLT,
pages 72?82.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270?274.
1383
Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 85?95,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Graph-Based Approach to String Regeneration
Matic Horvat
Computer Laboratory
University of Cambridge
15 JJ Thomson Avenue, CB3 0FD, U.K.
mh693@cam.ac.uk
William Byrne
Department of Engineering
University of Cambridge
Trumpington Street, CB2 1PZ, U.K.
wjb31@cam.ac.uk
Abstract
The string regeneration problem is the
problem of generating a fluent sentence
from a bag of words. We explore the N-
gram language model approach to string
regeneration. The approach computes the
highest probability permutation of the in-
put bag of words under an N-gram lan-
guage model. We describe a graph-based
approach for finding the optimal permuta-
tion. The evaluation of the approach on a
number of datasets yielded promising re-
sults, which were confirmed by conduct-
ing a manual evaluation study.
1 Introduction
The string regeneration problem can be stated as:
given a bag of words taken from a fluent grammat-
ical sentence, recover the original sentence. As it
is often difficult to recover the exact original sen-
tence based solely on a bag of words, the problem
is relaxed to generating a fluent version of the orig-
inal sentence (Zhang and Clark, 2011).
The string regeneration problem can generally
be considered a difficult problem even for humans.
Consider the following bag of words:
{ Iraq, list, in, a, third, joins, the, ., of,
Bush?s, of, critics, policy, senator, re-
publican }
and try to recover the original sentence or at least
a fluent grammatical sentence. The original sen-
tence was:
a third republican senator joins the list
of critics of Bush?s policy in Iraq.
The purpose of investigating and developing ap-
proaches to solving the string regeneration prob-
lem is grammaticality and fluency improvement
of machine generated text. The output of sys-
tems generating text, including SMT, abstract-like
text summarisation, question answering, and dia-
logue systems, often lacks grammaticality and flu-
ency (Knight, 2007; Soricut and Marcu, 2005).
The string regeneration problem is used as an
application-independent method of evaluating ap-
proaches for improving grammaticality and flu-
ency of such systems.
The string regeneration can also be viewed as
a natural language realization problem. The basic
task of all realization approaches is to take a mean-
ing representation as input and generate human-
readable output. The approaches differ on how
much information is required from the meaning
representation, ranging from semantically anno-
tated dependency graphs to shallow syntactic de-
pendency trees. A simple bag of words can then be
considered as the least constrained input provided
to a natural language realization system. The bag
of words can be combined with partial constraints
to form a more realistic meaning representation.
Wan et al. (2009) proposed an algorithm for
grammaticality improvement based on depen-
dency spanning trees and evaluated it on the string
regeneration task. They compared its performance
against a baseline N-gram language model genera-
tor. They found that their approach performs better
with regards to BLEU score. The latter approach
does well at a local level but nonetheless often pro-
duces ungrammatical sentences.
We argue that the authors have not fully ex-
plored the N-gram language model approach to
string regeneration. They used a Viterbi-like gen-
erator with a 4-gram language model and beam
pruning to find approximate solutions. Addition-
ally, the 4-gram language model was trained on
a relatively small dataset of around 20 million
words.
The N-gram language model approach finds the
highest probability permutation of the input bag
85
of words under an N-gram language model as the
solution to the string regeneration problem. In
this paper we describe a graph-based approach to
computing the highest probability permutation of
a bag of words. The graph-based approach models
the problem as a set of vertices containing words
and a set of edges between the vertices, whose
cost equals language model probabilities. Finding
the permutation with the highest probability in the
graph formulation is equal to finding the shortest
tour in the graph or, equally, solving the Travelling
Salesman Problem (TSP). Despite the TSP being
an NP-hard problem, state-of-the-art approaches
exist to solving large problem instances. An in-
troduction to TSP and its variants discussed in this
paper can be found in Applegate et al. (2006b).
In contrast to the baseline N-gram approach by
Wan et al. (2009), our approach finds optimal so-
lutions. We built several models based on 2-gram,
3-gram, and 4-gram language models. We exper-
imentally evaluated the graph-based approach on
several datasets. The BLEU scores and example
output indicated that our approach is successful
in constructing a fairly fluent version of the orig-
inal sentence. We confirmed the results of auto-
matic evaluation by conducting a manual evalua-
tion. The human judges were asked to compare the
outputs of two systems and decide which is more
fluent. The results are statistically significant and
confirm the ranking of the systems obtained us-
ing the BLEU scores. Additionally, we explored
computing approximate solutions with time con-
straints. We found that approximate solutions sig-
nificantly decrease the quality of the output com-
pared to optimal ones.
This paper describes work conducted in the
MPhil thesis by Horvat (2013).
2 Graph-Based Approach to String
Regeneration
The underlying idea of the approach discussed in
this paper is to use an N-gram language model to
compute the probabilities of permutations of a bag
of words and pick the permutation with the highest
probability as our solution.
The probability of a sequence of words under an
N-gram language model is computed as:
logP (w
n
1
) =
n
?
k=1
logP (w
k
|w
k?1
k?N+1
)
(1)
2.1 Naive Approach
A naive approach to finding the permutation with
the highest probability is to enumerate all permu-
tations, compute their probabilities using Equa-
tion 1, and choose the permutation with the highest
probability as the solution.
The time complexity of the naive approach is
O(n ? n!) as we are enumerating all permuta-
tions of n words and multiplying n conditional
probabilities for each permutation. This means
that the naive approach is not viable for sen-
tences of even moderate length. For example,
there are 3,628,800 permutations of 10 words and
355,687,428,096,000 of 17 words.
2.2 Bigram Graph-Based Approach
In this section we define the graph-based approach
to finding the highest probability permutation and
consequently our solution to the string regenera-
tion problem. For a bag of words S we define a
set of symbols X , X = S ? {<s>,</s>}, which
contains all the words in S (with indexes appended
to distinguish repeated words) and the start and
end of sentence symbols. For a bigram language
model, N = 2, we define a directed weighted
graph G = (V,E), with the set of vertices defined
as V = {w
i
|w
i
? X}. Therefore, each symbol
in X is represented by a single vertex. Let the set
of edges E be a set of ordered pairs of vertices
(w
i
, w
j
), such that E = {(w
i
, w
j
)|w
i
, w
j
? V }.
The edge cost is then defined as:
c
ij
=
?
?
?
?
?
?
?
?
?
0
if w
i
= </s>
and w
j
= <s>,
? logP (w
j
|w
i
) if w
i
6= w
j
,
? otherwise.
(2)
The conditional log probabilities of the form
logP (w
j
|w
i
) are computed by a bigram language
model. Consequently, finding the sentence permu-
tation with the highest probability under the bi-
gram language model equals finding the shortest
tour in graph G or, equally, solving the Asymmet-
ric Travelling Salesman Problem (ATSP). A gen-
eral example graph for a sentence of length 3 is
shown in Figure 1a.
The individual cases of the edge cost function
presented in Equation 2 ensure that the solution
tour is a valid sentence permutation. The nega-
tion of log probabilities transforms the problem of
86
 -
 
l
o
g
 
P
 
(
 
w
1
 
|
 
<
s
>
)
- log P (</s> | w
3 
)
- log P ( w
2 
| <s>)
-
 
l
o
g
 
P
 
(
 
w
1
 
|
 
w
2
 
)
-
 
l
o
g
 
P
 
(
 
w
2
 
|
w
1
 
)
-
 
l
o
g
 
P
 
(
w
3
 
|
 
w
2
 
)
-
 
l
o
g
 
P
 
(
w
2
 
|
 
w
3
 
)
-
 
l
o
g
 
P
 
(
 
w
3
 
|
 
w
1
 
)
-
 
l
o
g
 
P
 
(
 
w
1
 
|
 
w
3
 
)
-
 
l
o
g
 
P
 
(
<
/
s
>
 
|
 
w
1
 
)
-
 
l
o
g
 
P
 
(
 
w
3
 
|
<
s
>
)
-
 
l
o
g
 
P
 
(
<
/
s
>
 
|
 
w
2
 
)
0
<s>
  w
1
  w
2
 w
3
</s>
(a) A general graph. The edge cost equals the negated bigram
conditional log probability of the destination vertex given the
origin vertex. Only edges with non-infinite edge cost are
shown in the graph. Finding the shortest tour in the graph
equals finding the sentence permutation with the highest prob-
ability.
 
0
3

2
3
0
0
2
Hierarchical Phrase-Based Translation with
Weighted Finite-State Transducers and
Shallow-n Grammars
Adria` de Gispert?
University of Cambridge
Gonzalo Iglesias??
University of Vigo
Graeme Blackwood?
University of Cambridge
Eduardo R. Banga??
University of Vigo
William Byrne?
University of Cambridge
In this article we describe HiFST, a lattice-based decoder for hierarchical phrase-based translation
and alignment. The decoder is implemented with standard Weighted Finite-State Transducer
(WFST) operations as an alternative to the well-known cube pruning procedure. We find that
the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in
fewer search errors, better parameter optimization, and improved translation performance. The
direct generation of translation lattices in the target language can improve subsequent rescoring
procedures, yielding further gains when applying long-span language models and Minimum
Bayes Risk decoding. We also provide insights as to how to control the size of the search space
defined by hierarchical rules. We show that shallow-n grammars, low-level rule catenation,
and other search constraints can help to match the power of the translation system to specific
language pairs.
1. Introduction
Hierarchical phrase-based translation (Chiang 2005) is one of the current promising
approaches to statistical machine translation (SMT). Hiero SMT systems are based
on probabilistic synchronous context-free grammars (SCFGs) whose translation rules
? University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K.
E-mail: {ad465,gwb24,wjb31}@eng.cam.ac.uk.
?? University of Vigo, Department of Signal Processing and Communications, 36310 Vigo, Spain.
E-mail: {giglesia,erbanga}@gts.tsc.uvigo.es.
Submission received: 30 October 2009; revised submission received: 24 February 2010; accepted for
publication: 10 April 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
can be extracted automatically from word-aligned parallel text. These grammars can
produce a very rich space of candidate translations and, relative to simpler phrase-
based systems (Koehn, Och, and Marcu 2003), the power of Hiero is most evident in
translation between dissimilar languages, such as English and Chinese (Chiang 2005,
2007). Hiero is able to learn and apply complex patterns in movement and translation
that are not possible with simpler systems. Hiero can also be used to good effect on
?simpler? problems, such as translation between English and Spanish (Iglesias et al
2009c), even though there is not the same need for the full complexity of movement and
translation. If gains in using Hiero are small, however, the computational and modeling
complexity involved are difficult to justify. Such concerns would vanish if there were
reliable methods to match Hiero complexity for specific translation problems. Loosely
put, it would be a good thing if the complexity of a system was somehow proportional
to the improvement in translation quality the system delivers.
Another notable current trend in SMT is system combination. Minimum Bayes
Risk decoding is widely used to rescore and improve hypotheses produced by indi-
vidual systems (Kumar and Byrne 2004; Tromble et al 2008; de Gispert et al 2009),
and more aggressive system combination techniques which synthesize entirely new
hypotheses from those of contributing systems can give even greater translation im-
provements (Rosti et al 2007; Sim et al 2007). It is now commonplace to note that even
the best available individual SMT system can be significantly improved upon by such
techniques. This puts a burden on the underlying SMT systems which is somewhat
unusual in NLP. The requirement is not merely to produce a single hypothesis that
is as good as possible. Ideally, the SMT systems should generate large collections of
candidate hypotheses that are simultaneously diverse and of good quality.
Relative to these concerns, previously published descriptions of Hiero have noted
certain limitations. Spurious ambiguity (Chiang 2005) was described as
a situation where the decoder produces many derivations that are distinct yet have the
same model feature vectors and give the same translation. This can result in n-best lists
with very few different translations which is problematic for the minimum-error-rate
training algorithm ...
This is due in part to the cube pruning procedure (Chiang 2007), which enumerates all
distinct hypotheses to a fixed depth by means of k-best hypothesis lists. If enumeration
was not necessary, or if the lists could be arbitrarily deep, there might still be many
duplicate derivations, but at least the hypothesis space would not be impoverished.
Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu
1997; Setiawan et al 2009). For our purposes we say that overgeneration occurs when
different derivations based on the same set of rules give rise to different translations.
An example is given in Figure 1.
This process is not necessarily a bad thing in that it allows new translations to be
synthesized from rules extracted from training data; a strong target language model,
such as a high order n-gram, is typically relied upon to discard unsuitable hypotheses.
Overgeneration does complicate translation, however, in that many hypotheses are
introduced only to be subsequently discarded. The situation is further complicated by
search errors. Any search procedure which relies on pruning during search is at risk of
search errors and the risk is made worse if the grammars tend to introduce many similar
scoring hypotheses. In particular we have found that cube pruning is very prone to
search errors, that is, the hypotheses produced by cube pruning are not the top scoring
hypotheses which should be found under the Hiero grammar (Iglesias et al 2009b).
506
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 1
Example of multiple translation sequences from a simple grammar fragment showing variability
in reordering in translation of the source sequence abc.
These limitations are clearly related to each other. Moreover, they become more
problematic as the amount of parallel text grows. As the number of rules in the grammar
increases, the grammars become more expressive, but the ability to search them does not
improve. This leads to a widening gap between the expressive power of the grammar
and the ability to search it to find good and diverse hypotheses.
In this article we describe the following two refinements to Hiero which are in-
tended to address some of the limitations in its original formulation.
Lattice-based hierarchical translation We describe how the cube pruning procedure
can be replaced by standard operations with Weighted Finite State Transducers
(WFSTs) so that Hiero uses translation lattices rather than n-best lists in search.
We find that keeping partial translation hypotheses in lattice form greatly reduces
search errors. In some instances it is possible to perform translation without
any pruning at all so that search errors are completely eliminated. Consistent
with the observation by Chiang (2005), this leads to improvements in minimum
error rate training. Furthermore, the direct generation of translation lattices can
improve gains from subsequent language model and Minimum Bayes Risk (MBR)
rescoring.
Shallow-n grammars and additional nonterminal categories Nonterminals can be in-
corporated into hierarchical translation rules for the purpose of tuning the size
of the Hiero search space for individual language pairs. Shallow-n grammars are
described and shown to control the level of rule nesting, low-level rule catenation,
and the minimum and maximum spans of individual translation rules. In trans-
lation experiments we find that a shallow-1 grammar (one level of rule nesting)
is sufficiently expressive for Arabic-to-English translation, but that a shallow-3
grammar is required in Chinese-to-English translation to match the performance
of a full Hiero system that allows arbitrary rule nesting. These nonterminals are
introduced to control the Hiero search space and do not require estimation from
annotated?or parsed?parallel text, as can be required by translation systems
based on linguistically motivated grammars. We use this approach as the basis
of a general approach to SMT modeling. To control overgeneration, we revisit
the synchronous context-free grammar defined by hierarchical rules and take a
shallow-1 grammar as a starting point. We then increase the complexity of the
rules until the desired translation quality is found.
507
Computational Linguistics Volume 36, Number 3
With these refinements we find that hierarchical phrase-based translation can be effi-
ciently carried out with no (or minimal) search errors in large-data tasks and can achieve
state-of-the-art translation performance.
There are many benefits to formulating Hiero translation in terms of WFSTs. Fol-
lowing the manner in which Knight and Al-Onaizan (1998), Kumar, Deng, and Byrne
(2006), and Graehl, Knight, and May (2008) elucidate other machine translation models,
we can use WFST operations to make the operations of the Hiero decoder very clear. The
simplicity of the analysis makes it possible to focus on the underlying grammars and
avoid the complexities of heuristic search procedures. Once the decoder is formulated,
implementation is mostly straightforward using standard WFST techniques developed
for language processing (Mohri, Pereira, and Riley 2002). What difficulties arise are due
to using finite state techniques with grammars which are not themselves finite state.
We will show, however, that the basic operations which need to be performed, such as
extracting sufficient statistics for minimum error rate training, can be done relatively
easily and naturally.
1.1 Overview
In Section 2 we describe HiFST, which is a hierarchical phrase-based translation system
based on the OpenFST WFST libraries (Allauzen et al 2007). We describe how trans-
lation lattices can be generated over the Cocke-Younger-Kasami (CYK) grid used for
parsing under Hiero. We also review some modeling issues needed for practical trans-
lation, such as the efficient handling of source language deletions and the extraction of
statistics for minimum error rate training. This requires running HiFST in ?alignment
mode? (Section 2.3) to find all the rule derivations that generate a given set of translation
hypotheses.
In Section 3 we investigate parameters that control the size and nature of the
hierarchical phrase-based search space as defined by hierarchical translation rules. To
efficiently explore the largest possible space and avoid pruning in search, we introduce
ways to easily adapt the grammar to the reordering needs of each language pair. We
describe the use of additional nonterminal categories to limit the degree of rule nesting,
and can directly control the minimum or maximum span each translation rule can cover.
In Section 4 we report detailed translation results for Arabic-to-English and
Chinese-to-English, and review translation results for Spanish-to-English and Finnish-
to-English translation. In these experiments we contrast the performance of lattice-based
and cube pruning hierarchical decoding and we measure the impact on processing
time and translation performance due to changes in search parameters and grammar
configurations. We demonstrate that it is easy and feasible to compute the marginal
instead of the Viterbi probabilities when using WFSTs, and that this yields gains in
translation performance. And finally, we show that lattice-based translation performs
significantly better than k-best lists for the task of combining translation hypotheses
generated from alternative morphological segmentations of the data via lattice-based
MBR decoding.
2. Hierarchical Translation and Alignment with WFSTs
Hierarchical phrase-based rules define a synchronous context-free grammar (CFG) and
a particular search space of translation candidates. Table 1 shows the type of rules in-
cluded in a standard hierarchical phrase-based grammar, where T denotes the terminals
(words) and ? is a bijective function that relates the source and target nonterminals of
508
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 1
Rules contained in the standard hierarchical grammar.
standard hierarchical grammar
S??X,X? glue rule 1
S??S X,S X? glue rule 2
X???,?,?? , ?,? ? {X ? T}+ hiero rules
each rule (Chiang 2007). This function is defined if there are at least two nonterminals,
and for clarity of presentation may be omitted in general rule discussions. When ?,? ?
{T}+, that is, the rule contains no nonterminals, the rule is a simple lexical phrase pair.
The HiFST translation system is based on a variant of the CYK algorithm closely
related to CYK+ (Chappelier and Rajman 1998). Parsing follows the description of
Chiang (2005, 2007); it maintains back-pointers and employs hypothesis recombination
without pruning. The underlying model is a probabilisitic synchronous CFG consisting
of a set R = {Rr} of rules Rr : Nr ? ??r,?r? / pr, with ?glue? rules, S ? ?X,X? and
S ? ?S X,S X?. N denotes the set of nonterminal categories (examples are given in
Section 3), and pr denotes the rule probability, typically transformed to a cost cr; unless
otherwise noted we use the tropical semiring, so cr = ? log pr. T denotes the terminals
(words), and the grammar builds parses based on strings ?,? ? {N ? T}+. Each cell
in the CYK grid is specified by a nonterminal symbol and position in the CYK grid:
(N, x, y), which spans sx+y?1x on the source sentence.
In effect, the source language sentence is parsed using a CFG with rules N ? ?. The
generation of translations is a second step that follows parsing. For this second step, we
describe a method to construct word lattices with all possible translations that can be
produced by the hierarchical rules. Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In each cell (N, x, y) in the CYK grid, we
build a target language word lattice L(N, x, y). This lattice contains every translation of
sx+y?1x from every derivation headed by N. These lattices also contain the translation
scores on their arc weights.
The ultimate objective is the word lattice L(S, 1, J) which corresponds to all the
analyses that cover the source sentence sJ1. Once this is built, we can apply a target lan-
guage model to L(S, 1, J) to obtain the final target language translation lattice (Allauzen,
Mohri, and Roark 2003).
2.1 Lattice Construction over the CYK Grid
In each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), that
is, for r ? R(N, x, y), the rule N ? ??r,?r? was used in at least one derivation involving
that cell.
For each rule Rr, r ? R(N, x, y), we build a lattice L(N, x, y, r). This lattice is derived
from the target side of the rule ?r by concatenating lattices corresponding to the ele-
ments of ?r = ?r1...?
r
|?r|. If an ?
r
i is a terminal, creating its lattice is straightforward. If
?ri is a nonterminal, it refers to a cell (N
?, x?, y?) lower in the grid identified by the back-
pointer BP(N, x, y, r, i); in this case, the lattice used is L(N?, x?, y?). Taken together,
L(N, x, y, r) =
?
i=1..|?r|
L(N, x, y, r, i) (1)
509
Computational Linguistics Volume 36, Number 3
Figure 2
Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3.
The grid is represented here in two dimensions (x, y). In practice only the first column accepts
both nonterminals (S,X). For this reason it is divided into two subcolumns.
L(N, x, y, r, i) =
{
A(?i) if ?i ? T
L(N?, x?, y?) otherwise (2)
where A(t), t ? T returns a single-arc acceptor which accepts only the symbol t. The
lattice L(N, x, y) is then built as the union of lattices corresponding to the rules in
R(N, x, y):
L(N, x, y) =
?
r?R(N,x,y)
L(N, x, y, r) (3)
Lattice union and concatenation are performed using the? and?WFST operations,
respectively, as described by Allauzen et al (2007). If a rule Rr has a cost cr, it is applied
to the exit state of the lattice L(N, x, y, r) prior to the operation of Equation (3).
2.1.1 An Example of Phrase-based Translation. Figure 2 illustrates this process for a three-
word source sentence s1s2s3 under monotonic phrase-based translation. The left-hand
side shows the state of the CYK grid after parsing using the rules R1 to R5. These include
three standard phrases, that is, rules with only terminals (R1, R2, R3), and the glue rules
(R4, R5). Arrows represent back-pointers to lower-level cells. We are interested in the
uppermost S cell (S, 1, 3), as it represents the search space of translation hypotheses
covering the whole source sentence. Two rules (R4, R5) are in this cell, so the lattice
L(S, 1, 3) will be obtained by the union of the two lattices found by the back-pointers of
these two rules. This process is explicitly derived in the right-hand side of Figure 2.
2.1.2 An Example of Hierarchical Translation. Figure 3 shows a hierarchical scenario for
the same sentence. Three rules, R6,R7,R8, are added to the example of Figure 2, thus
providing two additional derivations. This makes use of sublattices already produced
in the creation of L(S, 1, 3, 5) and L(X, 1, 3, 1) in Figure 2; these are shown within {}.
510
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 3
Translation as in Figure 2 but with additional rules R6,R7,R8. Lattices previously derived appear
within {}.
2.2 A Procedure for Lattice Construction
Figure 4 presents an algorithm to build the lattice for every cell. The algorithm uses
memoization: If a lattice for a requested cell already exists, it is returned (line 2);
otherwise it is constructed via Equations (1)?(3). For every rule, each element of the
target side (lines 3,4) is checked as terminal or nonterminal (Equation (2)). If it is a
terminal element (line 5), a simple acceptor is built. If it is a nonterminal (line 6), the
lattice associated to its back-pointer is returned (lines 7 and 8). The complete lattice
L(N, x, y, r) for each rule is built by Equation (1) (line 9). The lattice L(N, x, y) for this
cell is then found by union of all the component rules (line 10, Equation (3)); this lattice
is then reduced by standard WFST operations (lines 11, 12, 13). It is important at this
point to remove any epsilon arcs which may have been introduced by the various WFST
union, concatenation, and replacement operations (Allauzen et al 2007).
We now address several important aspects of efficient implementation.
Figure 4
Recursive lattice construction from a CYK grid.
511
Computational Linguistics Volume 36, Number 3
Figure 5
Delayed translation WFST with derivations from Figures 2 and 3 before (left) and after
minimization (right).
2.2.1 Delayed Translation. Equation (2) leads to the recursive construction of lattices in
upper levels of the grid through the union and concatenation of lattices from lower
levels. If Equations (1) and (3) are actually carried out over fully expanded word lattices,
the memory required by the upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as pointers to the low-level lattices. This
effectively builds a skeleton for the desired lattice and delays the creation of the final
word lattice until a single replacement operation is carried out in the top cell (S, 1, J).
To make this exact, we define a function g(N, x, y) which returns a unique tag for each
lattice in each cell, and use it to redefine Equation (2). With the back-pointer (N?, x?, y?) =
BP(N, x, y, r, i), these special arcs are introduced as
L(N, x, y, r, i) =
{
A(?i) if ?i ? T
A(g(N?, x?, y?)) otherwise (4)
The resulting lattices L(N, x, y) are a mix of target language words and lattice
pointers (Figure 5, left). Each still represents the entire search space of all translation
hypotheses covering the span, however. Importantly, operations on these lattices?such
as lossless size reduction via determinization and minimization (Mohri, Pereira, and
Riley 2002)?can still be performed. Owing to the existence of multiple hierarchical rules
which share the same low-level dependencies, these operations can greatly reduce the
size of the skeleton lattice; Figure 5 shows the effect on the translation example. This
process is carried out for the lattice at every cell, even at the lowest level where there
are only sequences of word terminals. As stated, size reductions can be significant. Not
all redundancy is removed, however, because duplicate paths may arise through the
concatenation and union of sublattices with different spans.
At the upper-most cell, the lattice L(S, 1, J) contains pointers to lower-level lattices.
A single FST replace operation (Allauzen et al 2007) recursively substitutes all pointers
by their lower-level lattices until no pointers are left, thus producing the complete
target word lattice for the whole source sentence. The use of the lattice pointer arc was
inspired by the ?lazy evaluation? techniques developed by Mohri, Pereira, and Riley
(2000), closely related to Recursive Transition Networks (Woods 1970; Mohri 1997). Its
implementation uses the infrastructure provided by the OpenFST libraries for delayed
composition.
2.2.2 Top-level Pruning and Search Pruning. The final translation lattice L(S, 1, J) can
be quite large after the pointer arcs are expanded. We therefore apply a word-based
512
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 6
Transducers for filtering up to one (left) or two (right) consecutive deletions.
language model via WFST composition (Allauzen et al 2007) and perform likelihood-
based pruning based on the combined translation and language model scores. We call
this top-level pruning because it is performed over the topmost lattice.
Pruning can also be performed on the sublattices in each cell during search. One
simple strategy is to monitor the number of states in the determinized lattices L(N, x, y).
If this number is above a threshold, we expand any pointer arcs and apply a word-based
language model via composition. The resulting lattice is then reduced by likelihood-
based pruning, after which the LM scores are removed. These pruning strategies can be
very selective, for example allowing the pruning threshold to depend on the height of
the cell in the grid. In this way the risk of search errors can be controlled.
The same n-gram language model can be used for top-level pruning and search
pruning, although different WFST realizations are required. For top-level pruning, a
standard implementation as described by Allauzen et al (2007) is appropriate. For
search pruning, the WFST must allow for incomplete language model histories, because
many sublattice paths are incomplete translation hypotheses which do not begin with
a sentence-start marker. The language model acceptor is constructed so that initial
substrings of length less than the language model order are assigned no weight under
the language model.
2.2.3 Constrained Source Word Deletion. As a practical matter it can be useful to allow SMT
systems to delete some source words rather than to enforce their translation. Deletions
can be allowed in Hiero by including in the grammar a set of special deletion rules
of the type: X??s,NULL?. Unconstrained application of these rules can lead to overly
large and complex search spaces, however. We therefore limit the number of consecutive
source word deletions as we explore each cell of the CYK grid. This is done by standard
composition with an unweighted transducer that maps any word to itself, and up to k
NULL tokens to  arcs. In Figure 6 this simple transducer for k = 1 and k = 2 is drawn.
Composition of the lattice in each cell with this transducer filters out all translations
with more than k consecutive deleted words.
2.3 Hierarchical Phrase-Based Alignment with WFSTs
We now describe a method to apply our decoder in alignment mode. The objective in
alignment is to recover all the derivations which can produce a given translation. We do
this rather than keep track of the rules used during translation, because we find it faster
and more efficient first to generate translations and then, by running the system as an
aligner with a constrained target space, to extract all the relevant derivations with their
costs. As will be discussed in Section 2.3.1, this is useful for minimum error training,
where the contribution of each feature to the overall hypothesis cost is required for
system optimization.
513
Computational Linguistics Volume 36, Number 3
Figure 7
Transducer encoding simultaneously rule derivations R2R1R3R4 and R1R5R6, and the translation
t5t8. The input sentence is s1s2s3 and the grammar considered here contains the following rules:
R1: S??X,X?, R2: S??S X,S X? , R3: X??s1,t5?, R4: X??s2 s3,t8?, R5: X??s1 X s3,X t8? and R6:
X??s2,t5?.
Conceptually, we would like to create a transducer that represents the mapping
from all possible rule derivations to all possible translations, and then compose this
transducer with an acceptor for the translations of interest. Creating this transducer
which maps derivations to translations is not feasible for large translation grammars,
so we instead keep track of rules as they are used to generate a particular translation
output. We introduce two modifications into lattice construction over the CYK grid
described in Section 2.2:
1. In each cell transducers are constructed which map rule sequences to the
target language translation sequences they produce. In each transducer the
output strings are all possible translations of the source sentence span
covered by that cell; the input strings are all the rule derivations that
generate those translations. The rule derivations are expressed as
sequences of rule indices r given the set of rules R = {Rr}.
2. As these transducers are built they are composed with acceptors for
subsequences of the reference translations so that any translations not
present in the given set of reference translations are removed. In effect, this
replaces the general target language model used in translation with an
unweighted acceptor which accepts only specific sentences.
For alignment, Equations (1) and (2) are redefined as
L(N, x, y, r) = AT(r,)
?
i=1..|?r|
L(N, x, y, r, i) (5)
L(N, x, y, r, i) =
{
AT(,?i) if ?i ? T
L(N?, x?, y?) otherwise (6)
where AT(r, t), Rr ? R, t ? T returns a single-arc transducer which accepts the symbol
r in the input language (rule indices) and the symbol t in the output language (target
words). The weight assigned to each arc is the same in alignment as in translation. With
these definitions the goal lattice L(S, 1, J) is now a transducer with rule indices in the
input symbols and target words in the output symbols. A simple example is given
in Figure 7 where two rule derivations for the translation t5t8 are represented by the
transducer.
As we are only interested in those rule derivations that generate the given target
references, we can discard non-desired translations via standard FST composition of
514
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 8
Construction of a substring acceptor. An acceptor for the strings t1t2t4 and t3t4 (left) and its
substring acceptor (right). In alignment the substring acceptor can be used to filter out undesired
partial translations via standard FST composition operations.
the lattice transducer with the given reference acceptor. In principle, this would be done
in the uppermost cell of the CYK, once the complete source sentence has been covered.
However, keeping track of all possible rule derivations and all possible translations until
the last cell may not be computationally feasible for many sentences. It is more desirable
to carry out this filtering composition in lower-level cells while constructing the lattice
over the CYK grid so as to avoid storing an increasing number of undesired translations
and derivations in the lattice. The lattice in each cell should contain translations formed
only from substrings of the references.
To achieve this we build an unweighted substring acceptor that accepts all sub-
strings of each target reference string. For instance, given the reference string t1t2 . . .
tJ, we build an acceptor for all substrings ti . . . tj, where 1 ? i ? j ? J. This is applied
to lattices in all cells (x, y) that do not span the whole sentence. In the uppermost
cell we compose with the reference acceptor which accepts only complete reference
strings. Given a lattice of target references, the unweighted substring acceptor is
built as:
1. change all non-initial states into final states
2. add one initial state and add  arcs from it to all other states
Figure 8 shows an example of a substring acceptor for the two references t1t2t4 and
t3t4. The substring acceptor also accepts an empty string, accounting for those rules
that delete source words, which in other words translate into NULL. In some instances
the final composition with the reference acceptor might return an empty lattice. If this
happens there is no rule sequence in the grammar that can generate the given source
and target sentences simultaneously.
We have described the use of transducers to encode mappings from rule deriva-
tions to translations. These transducers are somewhat impoverished relative to parse
trees and parse forests, which are more commonly used to encode this relationship. It
is easy to map from a parse tree to one of these transducers but the reverse essentially
requires re-parsing to recreate the tree structure. The structures of the parse trees asso-
ciated with a translation are not needed by many algorithms, however. In particular,
parameter optimization by MERT requires only the rules involved in translation. Our
approach keeps only what is needed by such algorithms. This approach also has prac-
tical advantages such as the ability to align directly to k-best lists or lattices of reference
translations.
515
Computational Linguistics Volume 36, Number 3
Figure 9
One arc from a rule acceptor that assigns a vector of K feature weights to each rule (top) and the
result of composition with the transducer of Figure 7 (after weight-pushing) (bottom). The
components of the final K-dimensional weight vector agree with the feature weights of the rule
sequence, for example ck = c2,k + c1,k + c3,k + c4,k for k = 1 . . .K.
2.3.1 Extracting Feature Values from Alignments. As described by Chiang (2007), scores are
associated with hierarchical translation rules through a factoring into features within a
log-linear model (Och and Ney 2002). We assume that we have a collection of K features
and that the cost cr of each rule Rr is cr =
?K
k=1 ?kc
r,k, where cr,k is the value of the kth
feature value for the rth rule and ?k is the weight assigned to the kth feature for all rules.
For a parse which makes use of the rules Rr1 . . .RrN , its cost
?N
n=1 c
rn can therefore
be written as
?K
k=1 ?k
?N
n=1 c
rn,k. The quantity
?N
n=1 c
rn,k is the contribution by the kth
feature to the overall translation score for that parse. These are the quantities which
need to be extracted from alignment lattices for use in procedures such as minimum
error rate training for estimation of the feature weights ?k.
The procedure described in Section 2.3 produces alignment lattices with scores
consistent with the total parse score. Further steps must be taken to factor this over-
all score to identify the contribution due to individual features or translation rules.
We introduce a rule acceptor which accepts sequences of rule indices, such as the
input sequences of the alignment transducer, and assigns weights in the form of
K-dimensional vectors. Each component of the weight vector corresponds to the feature
value for that rule. Arcs have the form 0
Rr/wr?? 0 where wr = [cr,1, . . . , cr,K]. An example
of composition with this rule acceptor is given in Figure 9 to illustrate how feature scores
are mapped to components of the weight vector. The same operations can be applied to
the (unweighted) alignment transducer on a much larger scale to extract the statistics
needed for minimum error rate training.
We typically apply this procedure in the tropical semiring (Viterbi likelihoods), so
that only the best rule derivation that generated each translation candidate is taken
into account when extracting feature contributions for MERT. However, given the
alignment transducer L, this could also be performed in the log semiring (marginal
likelihoods), taking into account the feature contributions from all rule derivations, for
each translation candidate. This would be adequate if the translation system also car-
ried out decoding in the log semiring, an experiment which is partially explored in
Section 4.4.
We note that the contribution of the language model to the overall translation score
cannot be calculated in this scheme, since the language model score cannot be factored
in terms of rules. To obtain the language model contribution, we simply carry out
WFST composition (Allauzen et al 2007) between an unweighted acceptor of the target
516
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 10
Hierarchical translation grammar example and two parse trees with different levels of rule
nesting for the input sentence s1s2s3s4.
sentences and the n-gram language model used in translation. After determinization,
the cost of each path in the acceptor is then the desired LM feature contribution.
3. Shallow-n Translation Grammars: Translation Search Space Refinements
In this section we describe shallow-n grammars in order to reduce Hiero overgeneration
and adapt the grammar complexity to specific language pairs; the ultimate goal is to de-
fine the most constrained grammar that is capable of generating the desired movement
and translation, so that decoding can be performed without search errors.
Hiero can provide varying degrees of complexity in movement and translation.
Consider the example shown in Figure 10, which shows a hierarchical grammar defined
by six rules. For the input sentence s1s2s3s4, there are two possible parse trees as shown;
the rule derivations for each tree are R1R4R3R5 and R2R1R3R5R6. Along with each tree
is shown the translation generated and the phrase-level alignment. Comparing the two
trees and alignments, the leftmost tree makes use of more reordering when translating
from source to target through the nested application of the hierarchical rules R3 and R4.
For some language pairs this level of reordering may be required in translation, but for
other language pairs it may lead to overgeneration of unwanted hypotheses. Suppose
the grammar in this example is modified as follows:
1. A nonterminal X0 is introduced into hierarchical translation rules
R3:X??X0 s3,t5 X0?
R4:X??X0 s4,t3 X0?
2. Rules for lexical phrases are applied to X0
R5:X0??s1 s2,t1 t2?
R6:X0??s4,t7?
These modifications exclude parses in which hierarchical translation rules generate
other hierarchical rules, except at the 0th level which generate lexical phrases. Con-
sequently the left most tree of Figure 10 cannot be generated and t5t1t2t7 is the only
allowable translation of s1s2s3s4. We call this a ?shallow-1? grammar: The maximum
517
Computational Linguistics Volume 36, Number 3
degree of rule nesting allowed is 1 and only the glue rule can be applied above this
level.
The use of additional nonterminal categories is an elegant way to easily control
important aspects that can have a strong impact on the search space. A shallow-n
translation grammar can be formally defined as:
1. the usual nonterminal S
2. a set of nonterminals {X0, . . . ,XN}
3. two glue rules: S ? ?XN,XN? and S ? ?S XN,S XN?
4. hierarchical translation rules for levels n = 1, . . . ,N:
R: Xn???,?,?? , ?,? ? {{Xn?1} ? T}+
with the requirement that ? and ? contain at least one Xn?1
5. translation rules which generate lexical phrases:
R: X0???,?? , ?,? ? T+
3.1 Avoiding Some Spurious Ambiguity
The added requirement in condition (4) in the definition of shallow-n grammars is
included to avoid some instances in which multiple parses lead to the same translation.
It is not absolutely necessary but it can be added without any loss in representational
capability. To see the effect of this constraint, consider the following example with a
source sentence s1 s2 and a shallow-1 grammar defined by these four rules:
R1: S??X1,X1?
R2: X1??s1 s2,t2 t1?
R3: X1??s1 X0,X0 t1?
R4: X0??s2,t2?
There are two derivations R1R2 and R1R3R4 which yield identical translations. However
R2 would not be allowed under the constraint introduced here because it does not
rewrite an X1 to an X0.
3.2 Structured Long-Distance Movement
The basic formulation of shallow-n grammars allows only the upper-level nonterminal
category S to act within the glue rule. This can prevent some useful long-distance
movement, as might be needed to translate Arabic sentences in Verb-Subject-Object
order into English. It often happens that the initial Arabic verb requires long-distance
movement, but the subject which follows can be translated in monotonic order. For
instance, consider the following Romanized Arabic sentence:
TAlb AlwzrA? AlmjtmEyn Alywm fy dm$q <lY ...
(CALLED) (the ministers) (gathered) (today) (in Damascus) (FOR) ...
where the verb ?TAlb? must be translated into English so that it follows the translations
of the five subsequent Arabic words ?AlwzrA? AlmjtmEyn Alywm fy dm$q?, which
518
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
are themselves translated monotonically. A shallow-1 grammar cannot generate this
movement except in the relatively unlikely case that the five words following the verb
can be translated as a single phrase.
A more powerful approach is to define grammars which allow low-level rules to
form movable groups of phrases. Additional nonterminals {Mk} are introduced to allow
successive generation of k nonterminals XN?1 in monotonic order for both languages,
where K1 ? k ? K2. These act in the same manner as the glue rule does in the uppermost
level. Applying Mk nonterminals at the N?1 level allows one hierarchical rule to perform
a long-distance movement over the tree headed by Mk.
We further refine the definition of shallow-n grammars by specifying the allowable
values of k for the successive productions of nonterminals XN?1. There are many pos-
sible ways to formulate and constrain these grammars. If K2 = 1, then the grammar
is equivalent to the previous definition of shallow-n grammars, because monotonic
production is only allowed by the glue rule of level N. If K1 = 1 and K2 > 1, then the
search space defined by the grammar is greater than the standard shallow-n grammar
as it includes structured long-distance movement. Finally, if K1 > 1 then the search
space is different from standard shallow-n as the n level is only used for long-distance
movement.
Introduction of Mk nonterminals redefines shallow-n grammars as:
1. the usual nonterminal S
2. a set of nonterminals {X0, . . . ,XN}
3. a set of nonterminals {MK1 , . . . ,MK2} for K1 = 1, 2; K1 ? K2
4. two glue rules: S ? ?XN,XN? and S ? ?S XN,S XN?
5. hierarchical translation rules for level N:
R: XN???,?,?? , ?,? ? {{MK1 , . . . ,MK2} ? T}+
with the requirement that ? and ? contain at least one Mk
6. hierarchical translation rules for levels n = 1, . . . ,N ? 1:
R: Xn???,?,?? , ?,? ? {{Xn?1} ? T}+
with the requirement that ? and ? contain at least one Xn?1
7. translation rules which generate lexical phrases:
R: X0???,?? , ?,? ? T+
8. rules which generate k nonterminals XN?1:
if K1 == 2 :
R: Mk??XN?1 Mk?1,XN?1 Mk?1? , for k = 3, . . . ,K2
R: M2??XN?1 XN?1,XN?1 XN?1,I?
if K1 == 1 :
R: Mk??XN?1 Mk?1,XN?1 Mk?1? , for k = 2, . . . ,K2
R: M1??XN?1,XN?1?
where I denotes the identity function that enforces monotonocity in the nonterminals.
For example, with a shallow-1 grammar, M3 leads to the monotonic production of three
nonterminals X0, which leads to the production of three lexical phrase pairs; these can be
moved with a hierarchical rule of level 1. This is graphically represented by the leftmost
tree in Figure 11. With a shallow-2 grammar, M2 leads to the monotonic production of
519
Computational Linguistics Volume 36, Number 3
Figure 11
Movement allowed by two grammars: shallow-1, with K1 = 1, K2 = 3 (left), and shallow-2, with
K1 = 1, K2 = 3 (right). Both grammars allow movement of the bracketed term as a unit.
Shallow-1 requires that translation within the object moved be monotonic whereas shallow-2
allows up to two levels of reordering.
two nonterminals X1, a movement represented by the rightmost tree in Figure 11. This
movement cannot be achieved with a shallow-1 grammar.
3.3 Minimum and Maximum Rule Span
It is useful to define two parameters which further control the application of hierarchical
translation rules in generating the search space. Parameters hmax and hmin specify
the maximum and minimum height at which any hierarchical translation rule can be
applied in the CYK grid. In other words, a hierarchical rule can only be applied in cell
(x, y) if hmin? y ?hmax. Note that these parameters can also be set independently for
each nonterminal category.
3.4 Verb Movement Grammars for Arabic-to-English Translation
Following the discussion which motivated this section, we wish to model movement of
Arabic verbs when translating into English. We add to the shallow-n grammars a verb
restriction so that the hierarchical translation rules (5) apply only if the source language
string ? contains a verb. This encourages translations in which the Arabic verb is moved
at the uppermost level N.
3.5 Grammars Used for SMT Experiments
We now define the hierarchical grammars for the translation experiments which we
describe next.
Shallow-1,2,3 : Shallow-n grammars for n = 1, 2, 3. These grammars do not incorporate
any monotonicity constraints, that is K1 = K2 = 1.
Shallow-1, K1 = 1, K2 = 3 : hierarchical rules with one nonterminal can reorder a
monotonic production of up to three target language phrases of level 0.
520
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Shallow-1, K1 = 1, K2 = 3, vo : hierarchical rules with one nonterminal can reorder a
monotonic catenation of up to three target language phrases of level 0, but only if
one of the source terminals is tagged as a verb.
Shallow-2, K1 = 2,K2 = 3, vo : two levels of reordering with monotonic production
of up to three target language phrases of level 1, but only if one of the source
terminals is tagged as a verb.
4. Translation Experiments
In this section we report on hierarchical phrase-based translation experiments with
WFSTs. We focus mainly on the NIST Arabic-to-English and Chinese-to-English trans-
lation tasks; some results for other language pairs are summarized in Section 4.6.
Translation performance is evaluated using the BLEU score (Papineni et al 2001) as
implemented for the NIST 2009 evaluation.1 The experiments are organized as follows:
- Lattice-based and cube pruning hierarchical decoding (Section 4.2)
- Grammar configurations and search parameters and their effect on
translation performance and processing time (Section 4.3)
- Marginalization over translation derivations (Section 4.4)
- Combining translation lattices obtained from alternative morphological
decompositions of the input (Section 4.5)
4.1 Experimental Framework
For Arabic-to-English translation we use all allowed parallel corpora in the NIST MT08
(and MT09) Arabic Constrained Data track (?150M words per language). In addition to
reporting results on the MT08 set itself, we make use of a development set mt02-05-tune
formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation
sets; the even numbered sentences form a validation set mt02-05-test. The mt02-05-tune
set has 2,075 sentences.
For Chinese-to-English translation we use all available parallel text for the GALE
2008 evaluation;2 this is approximately 250M words per language. We report translation
results on the NIST MT08 set, a development set tune-nw, and a validation set test-nw.
These tuning and test sets contain translations produced by the GALE program and
portions of the newswire sections of MT02 through MT05. The tune-nw set has 1,755
sentences, and test-nw set is similar.
The parallel texts for both language pairs are aligned using MTTK (Deng and Byrne
2008). We extract hierarchical rules from the aligned parallel texts using the constraints
developed by Chiang (2007). We further filter the extracted rules by count and pattern
as described by Iglesias et al(2009a). The following features are extracted from the
parallel data and used to assign scores to translation rules: source-to-target and target-
to-source phrase translation models, word and rule penalties, number of usages of the
glue rule, source-to-target and target-to-source lexical models, and three rule count
features inspired by Bender et al (2007).
1 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
2 See http://projects.ldc.upenn.edu/gale/data/catalog.html
521
Computational Linguistics Volume 36, Number 3
We use two types of language model in translation. In first-pass translation we use
4-gram language models estimated over the English side of the parallel text (for each
language pair) and a 965 million word subset of monolingual data from the English
Gigaword Third Edition (LDC2007T07). These are the language models used if pruning
is needed during search. The main language model is a zero-cutoff stupid-backoff
(Brants et al 2007) 5-gram language model, estimated using 6.6B words of English text
from the English Gigaword corpus. These language models are converted to WFSTs
as needed (Allauzen, Mohri, and Roark 2003); failure transitions are used for correct
application of back-off weights. In tuning the systems, standard MERT (Och 2003)
iterative parameter estimation under IBM BLEU is performed on the development sets.
4.2 Contrast between HiFST and Cube Pruning
We contrast two hierarchical phrase-based decoders. The first decoder, HCP, is a k-best
decoder using cube pruning following the description by Chiang (2007); in our im-
plementation, these k-best lists contain only unique hypotheses (Iglesias et al 2009a),
which are obtained by extracting the 10,000 best candidates from each cell (including
the language model cost), using a priority queue to explore the cross-product of the
k-best lists from the cells pointed by nonterminals. We find that deeper k-best lists
(i.e., k = 100, 000) results in impractical decoding times and that fixed k-best list depths
yields better performance than use of a likelihood threshold parameter. The second
decoder, HiFST, is a lattice-based decoder implemented with WFSTs as described earlier.
Hypotheses are generated after determinization under the tropical semiring so that
scores assigned to hypotheses arise from a single minimum cost/maximum likelihood
derivation.
Translation proceeds as follows. After Hiero translation with optimized feature
weights and the first-pass language model, hypotheses are written to disk. For HCP we
save translations as 10,000-best lists, whereas HiFST generates word lattices. The first-
pass results are then rescored with the main 5-gram language model. In this operation
the first-pass language model scores are removed before the main language model
scores are applied. We then perform MBR rescoring. For the n-best lists we rescore
the top 1,000 hypotheses using the negative sentence-level BLEU score as the loss
function (Kumar and Byrne 2004); we have found that using a deeper k-best list is
impractically slow. For the HiFST lattices we use lattice-based MBR search procedures
described by Tromble et al (2008) in an implementation based on standard WFST
operations (Allauzen et al 2007).
4.2.1 Shallow-1 Arabic-to-English Translation. We translate Arabic-to-English with
shallow-1 hierarchical decoding: as described in Section 3, nonterminals are allowed
only to generate target language phrases. Table 2 shows results for mt02-05-tune, mt02-
05-test, and mt08. In this experiment we use MERT to find optimized parameters for
HCP and we use these parameter values in HiFST as well. This allows for a close
comparison of decoder behavior, independent of parameter optimization.
In these experiments, the first-pass translation quality of the two systems (Table 2
a vs. b) is nearly identical. The most notable difference in the first-pass behavior of the
decoders is their memory use. For example, for an input sentence of 105 words, HCP
uses 1.2Gb memory whereas HiFST takes only 900Mb under the same conditions. To
run HCP successfully requires cube pruning with the first-pass 4-gram language model.
By contrast, HiFST requires no pruning during lattice construction and the first pass
language model is not applied until the lattice is fully built at the upper most cell of the
522
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 2
Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) after first-pass
decoding and subsequent rescoring steps. Decoding time reported for mt02-05-tune is in seconds
per word. Both systems are optimized using MERT over the k-best lists generated by HCP.
decoder time mt02-05-tune mt02-05-test mt08
a HCP 1.1 52.5 51.9 42.8
+5g - 53.4 52.9 43.5
+5g+MBR - 53.6 53.0 43.6
b HiFST 0.5 52.5 51.9 42.8
+5g - 53.6 53.2 43.9
+5g+LMBR - 54.3 53.7 44.8
CYK grid. For this grammar, HiFST is able to produce exact translations without any
search errors.
Search Errors Because both decoders are constrained to use exactly the same features,
we can compare their search errors on a sentence-by-sentence basis. A search error is
assigned to one of the decoders if the other has found a hypothesis with lower cost. For
mt02-05-tune, we find that in 18.5% of the sentences HiFST finds a hypothesis with lower
cost than HCP. In contrast, HCP never finds any hypothesis with lower cost for any
sentence. This is as expected: The HiFST decoder requires no pruning prior to applying
the first-pass language model, so search is exact.
Lattice/k-best Quality It is clear from the results that the lattices produced by HiFST
yield better rescoring results than the k-best lists produced by HCP. This is the case for
both 5-gram language model rescoring and MBR. In MT08 rescoring, HCP k-best lists
yield an improvement of 0.8 BLEU relative to the first-pass baseline, whereas rescoring
HiFST lattices yield an improvement of 2.0 BLEU. The advantage of maintaining a large
search space in lattice form during decoding is clear. The use of k-best lists in HCP limits
the gains from subsequent rescoring procedures.
Translation Speed HCP requires an average of 1.1 seconds per input word. HiFST
cuts this time by half, producing output at a rate of 0.5 seconds per word. It proves
much more efficient to process compact lattices containing many hypotheses rather than
independently processing each distinct hypothesis in k-best form.
4.2.2 Fully Hierarchical Chinese-to-English Translation. We translate Chinese-to-English
with full hierarchical decoding: nonterminals are allowed to generate other hierarchical
rules in recursion.
We apply the constraint hmax=10 for nonterminal category X, as described in Sec-
tion 2.2.2, so that only glue rules are allowed at upper levels of the CYK grid; this is
applied in both HCP and HiFST.
In HiFST any lattice in the CYK grid is pruned if it covers at least three source words
and contains more than 10,000 states. The log-likelihood pruning threshold relative to
the best path in the sublattices is 9.0.
Improved Optimization and Generalization Table 3 shows results for tune-nw, test-nw,
and mt08. The first two rows show results for HCP when using MERT parameters
optimized over k-best lists produced by HCP (row a) and by HiFST (row b); in the latter
case, we are tuning HCP parameters over the hypothesis list generated by HiFST. When
measured over test-nw this gives a 0.3 BLEU improvement. HCP benefits from tuning
523
Computational Linguistics Volume 36, Number 3
Table 3
Contrastive Chinese-to-English translation results (lower-cased IBM BLEU) after first-pass
decoding and subsequent rescoring steps. The MERT k-best column indicates which decoder
generated the k-best lists used in MERT optimization. The mt08 set contains 691 sentences of
newswire and 666 sentences of Web text.
decoder MERT k-best tune-nw test-nw mt08
a HCP HCP 32.8 33.1 ?
b HCP 32.9 33.4 28.2
+5g HiFST 33.4 33.8 28.7
+5g+MBR 33.6 34.0 28.9
c HiFST 33.1 33.4 28.1
+5g HiFST 33.8 34.3 29.0
+5g+LMBR 34.5 34.9 30.2
over the HiFST hypotheses and we conclude that using the k-best list obtained by the
HiFST decoder yields better parameters in optimization.
Search Errors Measured over the tune-nw development set, HiFST finds a hypothesis
with lower cost in 48.4% of the sentences. In contrast, HCP never finds any hypothesis
with a lower cost for any sentence, indicating that the described pruning strategy for
HiFST is much broader than that of HCP. Note that HCP search errors are more frequent
for this language pair. This is due to the larger search space required for full hierarchical
translation; the larger the search space, the more likely it is that search errors will be
introduced by the cube pruning algorithm.
Lattice/k-best Quality Large LMs and MBR both benefit from the richer space of
translation hypotheses contained in the lattices. Relative to the first-pass baseline in
MT08, rescoring HiFST lattices yields a gain of 2.1 BLEU, compared to a gain of 0.7
BLEU with HCP k-best lists.
4.2.3 Reliability of n-gram Posterior Distributions. MBR decoding under linear BLEU
(Tromble et al 2008) is driven mainly by the presence of high posterior n-grams in
the lattice; the low posterior n-grams have poor discriminatory power. In the following
experiment, we show that high posterior n-grams are more likely to be found in the
references, and that using the full evidence space of the lattice is much better than even
very large k-best lists for computing posterior probabilities. Let Ni = {w1, . . .,w|Ni|}
denote the set of n-grams of order i in the first-pass translation 1-best, and let Ri =
{w1, . . .,w|Ri|} denote the set of n-grams of order i in the union of the references.
For confidence threshold ?, let Ni,? = {w ? Ni : p(w|L) ? ?} denote the set of all
n-grams in Ni with posterior probability greater than or equal to ?, where p(w|L) is
the posterior probability of the n-gram w, that is, the sum of the posterior probabilities
of all translations containing at least one occurrence of w. The precision at order i for
threshold ? is the proportion of n-grams in Ni,? found in the references:
pi,? =
|Ri ?Ni,?|
|Ni,?|
. (7)
The average per-sentence 4-gram precision at a range of posterior probability thresholds
? is shown in Figure 12. The posterior probabilities are computed using either the full
524
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 12
4-gram precisions for Arabic-to-English mt02-05-tune first-pass 1-best translations computed
using the full evidence space of the lattice and k-best lists of various sizes.
lattice L or a k-best list of the specified size. The 4-gram precision of the 1-best trans-
lations is approximately 0.35. At higher values of ?, the reference precision increases
considerably. Expanding the k-best list size from 1,000 to 10,000 hypotheses only slightly
improves the precision but much higher precisions are observed when the full evidence
space of the lattice is used. The improved precision results from more accurate estimates
of n-gram posterior probabilites and emphasizes once more the advantage of lattice-
based decoding and rescoring techniques.
4.3 Grammar Configurations and Search Parameters
We report translation performance and decoding speed as we vary hierarchical gram-
mar depth and the constraints on low-level rule concatenation (see Section 3). Unless
otherwise noted, hmin = 1 and hmax = 10 throughout (except for the ?S? nonterminal
category, where these constraints are not relevant).
4.3.1 Grammars for Arabic-to-English Translation. Table 4 reports Arabic-to-English trans-
lation results using the alternative grammar configurations described in Section 3.5.
Results are shown in first-pass decoding (HiFST rows), and in rescoring with a larger
5-gram language model for the most promising configurations (+5g rows). Decoding
time is reported for first-pass decoding only; rescoring time is negligible by comparison.
As shown in the upper part of Table 4, translation under a shallow-2 grammar does
not improve relative to a shallow-1 grammar, although decoding is much slower. This
indicates that the additional hypotheses generated when allowing a hierarchical depth
of two are not useful in Arabic-to-English translation. By contrast the shallow gram-
mars that allow long-distance movement for verbs only (shallow-1+K1,K2 = 1, 3, vo and
shallow-2+K1,K2 = 2, 3, vo), perform slightly better than shallow-1 grammar at a similar
decoding time. Performance differences increase when the larger 5-gram is applied
525
Computational Linguistics Volume 36, Number 3
Table 4
Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) with various
grammar configurations. Decoding time reported in seconds per word for mt02-05-tune.
grammar time mt02-05-tune mt02-05-test mt08
HiFST shallow-1 0.8 52.7 52.0 42.9
+K1,K2 = 1, 3 1.3 52.6 51.9 42.8
+K1,K2 = 1, 3, vo 0.9 52.7 52.1 42.9
shallow-2 4.2 52.7 51.9 42.6
+K1,K2 = 2, 3, vo 1.8 52.8 52.2 43.0
+5g shallow-1 - 53.9 53.4 44.9
+K1,K2 = 1, 3, vo - 54.1 53.6 45.0
shallow-2
+K1,K2 = 2, 3, vo
- 54.2 53.8 45.0
(Table 4, bottom). This is expected given that these grammars add valid translation
candidates to the search space with similar costs; a language model is needed to select
the good hypotheses among all those introduced.
4.3.2 Grammars for Chinese-to-English Translation. Table 5 shows contrastive results in
Chinese-to-English translation for full hierarchical and shallow-n (n = 1, 2, 3) gram-
mars.3 Unlike Arabic-to-English translation, Chinese-to-English translation improves
as the hierarchical depth of the grammar is increased (i.e., for larger n). Decoding time
also increases significantly. The shallow-1 grammar constraints which worked well for
Arabic-to-English translation are clearly inadequate for this task; performance degrades
by approximately 1.0 BLEU relative to the full hierarchical grammar.
However, we find that translation under the shallow-3 grammar yields performance
nearly as good as that of the full hiero grammars; translation times are shorter and
yield degradations of only 0.1 to 0.3 BLEU. Translation can be made significantly faster
by constraining the shallow-3 search space with hmin = 9, 5, 2 for X2,X1, and X0, respec-
tively; translation speed is reduced from 10.8 sec/word to 3.8 sec/word at a degradation
of 0.2 to 0.3 BLEU relative to full Hiero.
Shallow-3 grammars describe a restricted search-space but appear to have expressive
power in Chinese-to-English translation which is very similar to that of a full Hiero
grammar. Each cell (x, y) is represented by a bigger set of nonterminals; this allows for
more effective pruning strategies during lattice construction. We note also that hmax
values greater than 10 yield little improvement. As shown in the five bottom rows of
Table 5, differences between grammar configurations tend to carry through after 5-gram
rescoring. In summary, a shallow-3 grammar and filtering with hmin = 9, 5, 2 lead to a
0.4 degradation in BLEU relative to full Hiero. As a final contrast, the mixed-case NIST
BLEU-4 for the HiFST system on mt08 is 28.6. This result is obtained under the same
evaluation conditions as the official NIST MT08 Constrained Training Track.4
3 We note that the scores in full hiero row do not match those of row c in Table 3 which were obtained with
a slightly simplified version of HiFST and optimized according to the 2008 NIST implementation of IBM
BLEU; here we use the 2009 implementation by NIST.
4 Full MT08 results are available at
www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
526
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 5
Contrastive Chinese-to-English translation results (lower-cased IBM BLEU ) with various
grammar configurations and search parameters. Decoding time is reported in sec/word for
tune-nw.
grammar time tune-nw test-nw mt08 (nw)
HiFST shallow-1 0.7 33.6 33.4 32.6
shallow-2 5.9 33.8 34.2 32.7
+hmin=5 5.6 33.8 34.1 32.9
+hmin=7 4.0 33.8 34.3 33.0
shallow-3 8.8 34.0 34.3 33.0
+hmin=7 7.7 34.0 34.4 33.1
+hmin=9 5.9 33.9 34.3 33.1
+hmin=9,5,2 3.8 34.0 34.3 33.0
+hmin=9,5,2+hmax=11 6.1 33.8 34.4 33.0
+hmin=9,5,2+hmax=13 9.8 34.0 34.4 33.1
full hiero 10.8 34.0 34.4 33.3
+5g shallow-1 - 34.1 34.5 33.4
shallow-2 - 34.3 35.1 34.0
shallow-3 - 34.6 35.2 34.4
+hmin=9,5,2 - 34.5 34.8 34.2
full hiero - 34.5 35.2 34.6
4.4 Marginalization Over Translation Derivations
As has been discussed earlier, the translation model in hierarchical phrase-based ma-
chine translation allows for multiple derivations of a target language sentence. Each
derivation corresponds to a particular combination of hierarchical rules and it has been
argued that the correct approach to translation is to accumulate translation probability
by summing over the scores of all derivations (Blunsom, Cohn, and Osborne 2008).
Computing this sum for each of the many translation candidates explored during de-
coding is computationally difficult, however. For this reason the translation probability
is commonly computed using the Viterbi max-derivation approximation. This is the
approach taken in the previous sections in which translations scores were accumulated
under the tropical semiring.
The use of WFSTs allows the sum over alternative derivations of a target string
to be computed efficiently. HiFST generates a translation lattice realized as a weighted
transducer with output labels encoding words and input labels encoding the sequence
of rules corresponding to a particular derivation, and the cost of each path in the lattice
is the negative log probability of the derivation that generated the hypothesis.
Determinization applies the ? operator to all paths with the same word se-
quence (Mohri 1997). When applied in the log semiring, this operator computes the
sum of two paths with the same word sequence as x ? y = ?log(e?x + e?y) so that the
probabilities of alternative derivations can be summed.
Currently this operation is only performed in the top cell of the hierarchical decoder
so it is still an approximation to the true translation probability. Computing the true
translation probability would require the same operation to be repeated in every cell
during decoding, which is very time consuming. Note that the translation lattice was
generated with a language model and so the language model costs must be removed
527
Computational Linguistics Volume 36, Number 3
Table 6
Arabic-to-English results (lower-cased IBM BLEU) when determinizing the lattice at the
upper-most CYK cell with alternative semirings.
semiring mt02-05-tune mt02-05-test mt08
tropical HiFST 52.8 52.2 43.0
+5g 54.2 53.8 44.9
+5g+LMBR 55.0 54.6 45.5
log HiFST 53.1 52.6 43.2
+5g 54.6 54.2 45.2
+5g+LMBR 55.0 54.6 45.5
before determinization to ensure that only the derivation probabilities are included
in the sum. After determinization, the language model is reapplied and the 1-best
translation hypothesis can be extracted from the logarc determinized lattices.
Table 6 compares translation results obtained using the tropical semiring (Viterbi
likelihoods) and the log semiring (marginal likelihoods). First-pass translation shows
small gains in all sets: +0.3 and +0.4 BLEU for mt02-05-tune and mt02-05-test, and +0.2
for mt08. These gains show that the sum over alternative derivations can be easily
obtained in HiFST simply by changing semiring and that these alternative derivations
are beneficial to translation. The gains carry through to the large language model 5-gram
rescoring stage but after LMBR the final BLEU scores are unchanged. The hypotheses
selected by LMBR are in almost all cases exactly the same regardless of the choice of
semiring. This may be due to the fact that our current marginalization procedure is only
an approximation to the true marginal likelihoods, since the log semiring determiniza-
tion operation is applied only in the uppermost cell of the CYK grid and MERT training
is performed using regular Viterbi likelihoods.
We note that a close study of the interaction between LMBR and marginalization
over derivations is beyond the scope of this paper. Our purpose here is to show how
easily these operations can be done using WFSTs.
4.5 Combining Lattices Obtained from Alternative Morphological Decompositions
It has been shown that MBR decoding is a very effective way of combining translation
hypotheses obtained from alternative morphological decompositions of the same source
data. In particular, de Gispert et al (2009) show gains for Arabic-to-English and Finnish-
to-English when taking k-best lists obtained from two morphological decompositions
of the source language. Here we extend this approach to the case of translation lat-
tices and experiment with more than two alternative decompositions. We will show
that working with translation lattices gives significant improvements relative to k-best
lists.
In lattice-based MBR system combination, first-pass decoding results in a set of I
distinct translation lattices L(i), i = 1. . .I for each foreign sentence, with each lattice
produced by translating one of the alternative morphological decompositions. The
evidence space for MBR decoding is formed as the union of these lattices L =
?I
i=1 L(i).
The posterior probability of n-gram w in the union of lattices is computed as a simple
528
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 7
Arabic-to-English results (lower-cased IBM BLEU) when using alternative Arabic
decompositions, and their combination with k-best-based and lattice-based MBR.
configuration mt02-05-tune mt02-05-test mt08
a HiFST+5g 54.2 53.8 44.9
b HiFST+5g 53.8 53.6 45.0
c HiFST+5g 54.1 53.8 44.7
a+b +MBR 55.1 54.7 46.1
+LMBR 55.7 55.4 46.7
a+c +MBR 55.4 54.9 46.5
+LMBR 56.0 55.9 46.9
a+b+c +MBR 55.3 54.9 46.5
+LMBR 56.0 55.7 47.3
linear interpolation of the posterior probabilities according to the evidence space of each
individual lattice so that
p(w|L) =
I
?
i=1
?i pi(w|L(i) ), (8)
where the interpolation parameters 0 ? ?i ? 1 such that
?I
i=1 ?i = 1 specify the weight
associated with each system in the combination and are optimized with respect to
the tuning set. The system-specific posteriors required for the interpolation are com-
puted as
pi(w|L(i) ) =
?
E?L(i)w
Pi(E|F), (9)
where Pi(E|F) is the posterior probability of translation E given source sentence F and
the sum is taken over the subset L(i)w = {E ? L(i) : #w(E) > 0} of the lattice L(i) containing
paths with at least one occurrence of the n-gram w. These posterior probabilities are
used in MBR decoding under the linear approximation to the BLEU score described
in Tromble et al (2008). We find that for system combination, decoding often produces
output that is slightly shorter than required. A fixed per-word factor optimized on the
tuning set is applied when computing the gain and this results in output with improved
BLEU score and reduced brevity penalty.
Table 7 shows translation results in Arabic-to-English using three alternative mor-
phological decompositions of the Arabic text (upper rows a, b, and c). For each decom-
position an independent set of hierarchical rules is obtained from the respective parallel
corpus alignments. The decompositions were generated by the MADA toolkit (Habash
and Rambow 2005) with two alternative tokenization schemes, and by the Sakhr Arabic
Morphological Tagger, developed by Sakhr Software in Egypt.
The following rows show the results when combining with MBR the translation
hypotheses obtained from two or three decompositions. The table also shows a contrast
529
Computational Linguistics Volume 36, Number 3
Figure 13
Average per-sentence 4-gram reference precisions for Arabic-to-English mt02-05-tune
single-system MBR 1-best translations and the 1-best obtained through MBR system
combination.
between decoding the joint k-best lists (rows named MBR, with k = 1, 000) and decod-
ing the unioned translation lattices (rows named LMBR). In line with the findings of
de Gispert et al (2009), we find significant gains from combining k-best lists with respect
to using any one segmentation alone. Interestingly, here we find further gains when
applying lattice-based MBR instead of a k-best approach, obtaining consistent gains of
0.6?0.8 BLEU across all sets.
The results reported in Table 7 are very competitive. The mixed-case NIST BLEU-4
score for a+b+c LMBR system in MT08 is 44.9. This result is obtained under the same
evaluation conditions as the official NIST MT08 Constrained Training Track.5 For MT09,
the mixed-case BLEU-4 is 48.3, which ranks first in the Arabic-to-English NIST 2009
Constrained Data Track.6
4.5.1 System Combination and Reference Precision. We have demonstrated that MBR de-
coding of multiple lattices generated from alternative morphological segmentations
leads to significant improvements in BLEU score. We now show that one reason for
the improved performance is that lattice combination leads to better n-gram posterior
probability estimates. To combine two equally weighted lattices L(1) and L(2), the in-
terpolation weights are ?1 = ?2 = 12 ; Equation (8) simplifies as p(w|L) = 12 (p1(w|L(1) )+
p2(w|L(2))). Figure 13 plots average per-sentence reference precisions for the 4-grams in
the MBR 1-best of systems a and b and their combination (labeled a+b) at a range of
posterior probability thresholds 0 ? ? ? 1. Systems a and b have similar precisions at
all values of ?, confirming that the optimal interpolation weights for this combination
should be equal. The precision obtained using n-gram posterior probabilities computed
5 Full MT08 results are available at
www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
6 Full MT09 results are available at www.itl.nist.gov/iad/mig/tests/mt/2009/ResultsRelease
530
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
from the combined lattices is higher than that of the individual systems. A higher
proportion of the n-grams assigned high posterior probability under the interpolated
distribution are found in the references and this is one of the reasons for the large gains
in BLEU in lattice-based MBR system combination.
4.6 European Language Translation
The HiFST described here has also been found to achieve competitive performance for
other language pairs, such as Spanish-to-English and Finnish-to-English.
For Spanish-to-English we carried out experiments on the shared task of the ACL
2008 Workshop on Statistical Machine Translation (Callison-Burch et al 2008) based on
the Europarl corpus. For the official test2008 evaluation set we obtain a BLEU score of
34.2 using a shallow-1 grammar. Similarly to the Arabic case, deeper grammars are not
found to improve scores for this task.
In Finnish-to-English, we conducted translation experiments based on the Europarl
corpus using 3,000 sentences from the Q4/2000 period for testing with a single ref-
erence. In this case, the shallow-1 grammar obtained a BLEU score of 28.0, whereas
the full hierarchical grammar only achieved 27.6. This is further evidence that full-
hierarchical grammars are not appropriate in all instances. In this case we suspect that
the use of Finnish words without morphological decomposition leads to data sparsity
problems and complicates the task of learning complex translation rules. The lack of
a large English language model suitable for this domain may also make it harder to
select the right hypothesis when the translation grammar produces many more English
alternatives.
5. Conclusions
We have described two linked investigations into hierarchical phrase-based translation.
We investigate the use of weighted finite state transducers rather than k-best lists to
represent the space of translation hypotheses. We describe a lattice-based Hiero de-
coder, with which we find reductions in search errors, better parameter optimization,
and improved translation performance. Relative to these reductions in search errors,
direct generation of target language translation lattices also leads to further translation
improvements through subsequent rescoring steps, such as MBR decoding and the
application of large n-gram language models. These steps can be carried out easily via
standard WFST operations.
As part of the machinery needed for our experiments we develop WFST procedures
for alignment and feature extraction so that statistics needed for system optimization
can be easily obtained and represented as transducers. In particular, we make use of
a lattice-based representation of sequences of rule applications, which proves useful
for minimum error rate training. In all instances we find that using lattices as compact
representations of translation hypotheses offers clear modeling advantages.
We also investigate refinements in translation search space through shallow-n gram-
mars, structured long-distance movement, and constrained word deletion. We find
that these techniques can be used to fit the complexity of Hiero translation systems to
individual language pairs. In translation from Arabic into English, shallow grammars
make it possible to explore the entire search space and to do so more quickly but with the
same translation quality as the full Hiero grammar. Even in complex translation tasks,
such as Chinese to English, we find significant speed improvements with minimal loss
531
Computational Linguistics Volume 36, Number 3
in performance using these methods. We take the view that it is better to perform exact
search of a constrained space than to risk search errors in translation.
We note finally that Chiang introduced Hiero as a model ?based on a synchronous
CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns
1968).? We have taken this formulation as a starting point for the development of
novel realizations of Hiero. Our motivation has mainly been practical in that we seek
improved translation quality and efficiency through better models and algorithms.
Our approach suggests close links between Hiero and Recursive Transition Net-
works (Woods 1970; Mohri 1997). Although this connection is beyond the scope of this
paper, we do note that Hiero translation requires keeping track of two grammars, one
based on the Hiero translation rules and the other based on n-gram language model
probabilities. These two grammars have very different dependencies which suggests
that a full implementation of Hiero translation such as we have addressed does not
have a simple expression as an RTN.
Acknowledgments
This work was supported in part by the
GALE program of the Defense Advanced
Research Projects Agency, Contract No.
HR0011- 06-C-0022, and in part by the
Spanish government and the ERDF under
projects TEC2006-13694-C03-03 and
TEC2009-14094-C04-04.
References
Allauzen, Cyril, Mehryar Mohri, and Brian
Roark. 2003. Generalized algorithms for
constructing statistical language models.
In Proceedings of ACL, pages 557?564,
Sapporo.
Allauzen, Cyril, Michael Riley, Johan
Schalkwyk, Wojciech Skut, and Mehryar
Mohri. 2007. OpenFst: A general and
efficient weighted finite-state transducer
library. In Proceedings of CIAA,
pages 11?23, Prague.
Bender, Oliver, Evgeny Matusov, Stefan
Hahn, Sasa Hasan, Shahram Khadivi,
and Hermann Ney. 2007. The RWTH
Arabic-to-English spoken language
translation system. In Proceedings of
ASRU, pages 396?401, Kyoto.
Blunsom, Phil, Trevor Cohn, and Miles
Osborne. 2008. A discriminative latent
variable model for statistical machine
translation. In Proceedings of ACL-HLT,
pages 200?208, Columbus, OH.
Brants, Thorsten, Ashok C. Popat, Peng Xu,
Franz J. Och, and Jeffrey Dean. 2007.
Large language models in machine
translation. In Proceedings of EMNLP-ACL,
pages 858?867, Prague.
Callison-Burch, Chris, Cameron Fordyce,
Philipp Koehn, Christof Monz, and Josh
Schroeder. 2008. Further meta-evaluation
of machine translation. In Proceedings
of the ACL Workshop on Statistical
Machine Translation, pages 70?106,
Columbus, OH.
Chappelier, Jean-Ce?dric and Martin Rajman.
1998. A generalized CYK algorithm for
parsing stochastic CFG. In Proceedings of
TAPD, pages 133?137, Paris.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of ACL,
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
de Gispert, Adria`, Sami Virpioja, Mikko
Kurimo, and William Byrne. 2009.
Minimum Bayes-Risk combination of
translation hypotheses from alternative
morphological decompositions. In
Proceedings of HLT/NAACL, Companion
Volume: Short Papers, pages 73?76,
Boulder, CO.
Deng, Yonggang and William Byrne. 2008.
HMM word and phrase alignment for
statistical machine translation. IEEE
Transactions on Audio, Speech, and Language
Processing, 16(3):494?507.
Graehl, Jonathan, Kevin Knight, and
Jonathan May. 2008. Training tree
transducers. Computational Linguistics,
34(3):391?427.
Habash, Nizar and Owen Rambow. 2005.
Arabic tokenization, part-of-speech
tagging and morphological
disambiguation in one fell swoop. In
Proceedings of the ACL, pages 573?580,
Ann Arbor, MI.
Iglesias, Gonzalo, Adria` de Gispert,
Eduardo R. Banga, and William Byrne.
2009a. Rule filtering by pattern for
532
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
efficient hierarchical translation. In
Proceedings of the EACL, pages 380?388,
Athens.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo
R. Banga, and William Byrne. 2009b.
Hierarchical phrase-based translation with
weighted finite state transducers. In
Proceedings of HLT/NAACL, pages 433?441,
Boulder, CO.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo
R. Banga, and William Byrne. 2009c. The
HiFST system for the Europarl
Spanish-to-English task. In Proceedings of
SEPLN, pages 207?214, Donosti.
Knight, Kevin and Yaser Al-Onaizan. 1998.
Translation with finite-state devices. In
Proceedings of the Third Conference of the
AMTA on Machine Translation and the
Information Soup, pages 421?437,
Langhorne, PA.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL,
pages 127?133, Edmonton.
Kumar, Shankar and William Byrne. 2004.
Minimum Bayes-risk decoding for
statistical machine translation. In
Proceedings of HLT-NAACL, pages 169?176,
Boston, MA.
Kumar, Shankar, Yonggang Deng, and
William Byrne. 2006. A weighted finite
state transducer translation template
model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lewis, P. M., II, and R. E. Stearns. 1968.
Syntax-directed transduction. Journal of the
ACM, 15(3):465?488.
Mohri, Mehryar. 1997. Finite-state
transducers in language and speech
processing. Computational Linguistics,
23:269?311.
Mohri, Mehryar, Fernando Pereira, and
Michael Riley. 2000. The design principles
of a weighted finite-state transducer
library. Theoretical Computer Science,
231:17?32.
Mohri, Mehryar, Fernando Pereira, and
Michael Riley. 2002. Weighted finite-state
transducers in speech recognition.
Computer Speech and Language, 16:69?88.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL, pages 160?167,
Sapporo.
Och, Franz Josef and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the ACL,
pages 295?302, Philadelphia, PA.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. BLEU: A
method for automatic evaluation of
machine translation. In Proceedings of ACL,
pages 311?318, Toulouse.
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing
Xiang, Spyros Matsoukas, Richard
Schwartz, and Bonnie Dorr. 2007.
Combining outputs from multiple
machine translation systems. In
Proceedings of HLT-NAACL, pages 228?235,
Rochester, NY.
Setiawan, Hendra, Min Yen Kan, Haizhou Li,
and Philip Resnik. 2009. Topological
ordering of function words in hierarchical
phrase-based translation. In Proceedings
of the ACL-IJCNLP, pages 324?332,
Singapore.
Sim, Khe Chai, William Byrne, Mark Gales,
Hichem Sahbi, and Phil Woodland. 2007.
Consensus network decoding for statistical
machine translation system combination.
In Proceedings of ICASSP, pages 105?108,
Honolulu, HI.
Tromble, Roy, Shankar Kumar, Franz J. Och,
and Wolfgang Macherey. 2008. Lattice
Minimum Bayes-Risk decoding for
statistical machine translation. In
Proceedings of EMNLP, pages 620?629,
Honolulu, HI.
Varile, Giovanni B. and Peter Lau. 1988.
Eurotra practical experience with a
multilingual machine translation system
under development. In Proceedings of the
Second Conference on Applied Natural
Language Processing, pages 160?167,
Austin, TX.
Woods, W. A. 1970. Transition network
grammars for natural language analysis.
Communications of the ACM,
13(10):591?606.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
533

Proceedings of the ACL 2010 Conference Short Papers, pages 27?32,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding
of Statistical Machine Translation Lattices
Graeme Blackwood, Adria` de Gispert, William Byrne
Machine Intelligence Laboratory
Cambridge University Engineering Department
Trumpington Street, CB2 1PZ, U.K.
{gwb24|ad465|wjb31}@cam.ac.uk
Abstract
This paper presents an efficient imple-
mentation of linearised lattice minimum
Bayes-risk decoding using weighted finite
state transducers. We introduce transduc-
ers to efficiently count lattice paths con-
taining n-grams and use these to gather
the required statistics. We show that these
procedures can be implemented exactly
through simple transformations of word
sequences to sequences of n-grams. This
yields a novel implementation of lattice
minimum Bayes-risk decoding which is
fast and exact even for very large lattices.
1 Introduction
This paper focuses on an exact implementation
of the linearised form of lattice minimum Bayes-
risk (LMBR) decoding using general purpose
weighted finite state transducer (WFST) opera-
tions1. The LMBR decision rule in Tromble et al
(2008) has the form
E? = argmax
E??E
{
?0|E?|+
?
u?N
?u#u(E?)p(u|E)
}
(1)
where E is a lattice of translation hypotheses, N
is the set of all n-grams in the lattice (typically,
n = 1 . . . 4), and the parameters ? are constants
estimated on held-out data. The quantity p(u|E)
we refer to as the path posterior probability of the
n-gram u. This particular posterior is defined as
p(u|E) = p(Eu|E) =
?
E?Eu
P (E|F ), (2)
where Eu = {E ? E : #u(E) > 0} is the sub-
set of lattice paths containing the n-gram u at least
1We omit an introduction to WFSTs for space reasons.
See Mohri et al (2008) for details of the general purpose
WFST operations used in this paper.
once. It is the efficient computation of these path
posterior n-gram probabilities that is the primary
focus of this paper. We will show how general
purpose WFST algorithms can be employed to ef-
ficiently compute p(u|E) for all u ? N .
Tromble et al (2008) use Equation (1) as an
approximation to the general form of statistical
machine translation MBR decoder (Kumar and
Byrne, 2004):
E? = argmin
E??E
?
E?E
L(E,E?)P (E|F ) (3)
The approximation replaces the sum over all paths
in the lattice by a sum over lattice n-grams. Even
though a lattice may have many n-grams, it is
possible to extract and enumerate them exactly
whereas this is often impossible for individual
paths. Therefore, while the Tromble et al (2008)
linearisation of the gain function in the decision
rule is an approximation, Equation (1) can be com-
puted exactly even over very large lattices. The
challenge is to do so efficiently.
If the quantity p(u|E) had the form of a condi-
tional expected count
c(u|E) =
?
E?E
#u(E)P (E|F ), (4)
it could be computed efficiently using counting
transducers (Allauzen et al, 2003). The statis-
tic c(u|E) counts the number of times an n-gram
occurs on each path, accumulating the weighted
count over all paths. By contrast, what is needed
by the approximation in Equation (1) is to iden-
tify all paths containing an n-gram and accumulate
their probabilities. The accumulation of probabil-
ities at the path level, rather than the n-gram level,
makes the exact computation of p(u|E) hard.
Tromble et al (2008) approach this problem by
building a separate word sequence acceptor for
each n-gram in N and intersecting this acceptor
27
with the lattice to discard all paths that do not con-
tain the n-gram; they then sum the probabilities of
all paths in the filtered lattice. We refer to this as
the sequential method, since p(u|E) is calculated
separately for each u in sequence.
Allauzen et al (2010) introduce a transducer
for simultaneous calculation of p(u|E) for all un-
igrams u ? N1 in a lattice. This transducer is
effective for finding path posterior probabilities of
unigrams because there are relatively few unique
unigrams in the lattice. As we will show, however,
it is less efficient for higher-order n-grams.
Allauzen et al (2010) use exact statistics for
the unigram path posterior probabilities in Equa-
tion (1), but use the conditional expected counts
of Equation (4) for higher-order n-grams. Their
hybrid MBR decoder has the form
E? = argmax
E??E
{
?0|E?|
+
?
u?N :1?|u|?k
?u#u(E?)p(u|E)
+
?
u?N :k<|u|?4
?u#u(E?)c(u|E)
}
, (5)
where k determines the range of n-gram orders
at which the path posterior probabilities p(u|E)
of Equation (2) and conditional expected counts
c(u|E) of Equation (4) are used to compute the
expected gain. For k < 4, Equation (5) is thus
an approximation to the approximation. In many
cases it will be perfectly fine, depending on how
closely p(u|E) and c(u|E) agree for higher-order
n-grams. Experimentally, Allauzen et al (2010)
find this approximation works well at k = 1 for
MBR decoding of statistical machine translation
lattices. However, there may be scenarios in which
p(u|E) and c(u|E) differ so that Equation (5) is no
longer useful in place of the original Tromble et
al. (2008) approximation.
In the following sections, we present an efficient
method for simultaneous calculation of p(u|E) for
n-grams of a fixed order. While other fast MBR
approximations are possible (Kumar et al, 2009),
we show how the exact path posterior probabilities
can be calculated and applied in the implementa-
tion of Equation (1) for efficient MBR decoding
over lattices.
2 N-gram Mapping Transducer
We make use of a trick to count higher-order n-
grams. We build transducer ?n to map word se-
quences to n-gram sequences of order n. ?n has a
similar form to the WFST implementation of an n-
gram language model (Allauzen et al, 2003). ?n
includes for each n-gram u = wn1 arcs of the form:
wn-11 wn2wn:u
The n-gram lattice of order n is called En and is
found by composing E ??n, projecting on the out-
put, removing ?-arcs, determinizing, and minimis-
ing. The construction of En is fast even for large
lattices and is memory efficient. En itself may
have more states than E due to the association of
distinct n-gram histories with states. However, the
counting transducer for unigrams is simpler than
the corresponding counting transducer for higher-
order n-grams. As a result, counting unigrams in
En is easier than counting n-grams in E .
3 Efficient Path Counting
Associated with each En we have a transducer ?n
which can be used to calculate the path posterior
probabilities p(u|E) for all u ? Nn. In Figures
1 and 2 we give two possible forms2 of ?n that
can be used to compute path posterior probabilities
over n-grams u1,2 ? Nn for some n. No modifica-
tion to the ?-arc matching mechanism is required
even in counting higher-order n-grams since all n-
grams are represented as individual symbols after
application of the mapping transducer ?n.
Transducer ?Ln is used by Allauzen et al (2010)
to compute the exact unigram contribution to the
conditional expected gain in Equation (5). For ex-
ample, in counting paths that contain u1, ?Ln re-
tains the first occurrence of u1 and maps every
other symbol to ?. This ensures that in any path
containing a given u, only the first u is counted,
avoiding multiple counting of paths.
We introduce an alternative path counting trans-
ducer ?Rn that effectively deletes all symbols ex-
cept the last occurrence of u on any path by en-
suring that any paths in composition which count
earlier instances of u do not end in a final state.
Multiple counting is avoided by counting only the
last occurrence of each symbol u on a path.
We note that initial ?:? arcs in ?Ln effectively
create |Nn| copies of En in composition while
searching for the first occurrence of each u. Com-
2The special composition symbol ? matches any arc; ?
matches any arc other than those with an explicit transition.
See the OpenFst documentation: http://openfst.org
28
01
2
3
u1:u1
u2:u2?:?
?:?
?:?
?:?
?:?
Figure 1: Path counting transducer ?Ln matching
first (left-most) occurrence of each u ? Nn.
0
1
3
2
4
u1:u1
u2:u2
u1:?
u2:?
?:?
?:?
?:?
Figure 2: Path counting transducer ?Rn matching
last (right-most) occurrence of each u ? Nn.
posing with ?Rn creates a single copy of En while
searching for the last occurrence of u; we find this
to be much more efficient for large Nn.
Path posterior probabilities are calculated over
each En by composing with ?n in the log semir-
ing, projecting on the output, removing ?-arcs, de-
terminizing, minimising, and pushing weights to
the initial state (Allauzen et al, 2010). Using ei-
ther ?Ln or ?Rn , the resulting counts acceptor is Xn.
It has a compact form with one arc from the start
state for each ui ? Nn:
0 iui/- log p(ui|E)
3.1 Efficient Path Posterior Calculation
Although Xn has a convenient and elegant form,
it can be difficult to build for large Nn because
the composition En ? ?n results in millions of
states and arcs. The log semiring ?-removal and
determinization required to sum the probabilities
of paths labelled with each u can be slow.
However, if we use the proposed ?Rn , then each
path in En ? ?Rn has only one non-? output la-
bel u and all paths leading to a given final state
share the same u. A modified forward algorithm
can be used to calculate p(u|E) without the costly
?-removal and determinization. The modification
simply requires keeping track of which symbol
u is encountered along each path to a final state.
More than one final state may gather probabilities
for the same u; to compute p(u|E) these proba-
bilities are added. The forward algorithm requires
that En??Rn be topologically sorted; although sort-
ing can be slow, it is still quicker than log semiring
?-removal and determinization.
The statistics gathered by the forward algo-
rithm could also be gathered under the expectation
semiring (Eisner, 2002) with suitably defined fea-
tures. We take the view that the full complexity of
that approach is not needed here, since only one
symbol is introduced per path and per exit state.
Unlike En ??Rn , the composition En ??Ln does
not segregate paths by u such that there is a di-
rect association between final states and symbols.
The forward algorithm does not readily yield the
per-symbol probabilities, although an arc weight
vector indexed by symbols could be used to cor-
rectly aggregate the required statistics (Riley et al,
2009). For large Nn this would be memory in-
tensive. The association between final states and
symbols could also be found by label pushing, but
we find this slow for large En ??n.
4 Efficient Decoder Implementation
In contrast to Equation (5), we use the exact values
of p(u|E) for all u ? Nn at orders n = 1 . . . 4 to
compute
E? = argmin
E??E
{
?0|E?|+
4
?
n=1
gn(E,E?)
}
, (6)
where gn(E,E?) =
?
u?Nn ?u#u(E
?)p(u|E) us-
ing the exact path posterior probabilities at each
order. We make acceptors ?n such that E ? ?n
assigns order n partial gain gn(E,E?) to all paths
E ? E . ?n is derived from ?n directly by assign-
ing arc weight ?u?p(u|E) to arcs with output label
u and then projecting on the input labels. For each
n-gram u = wn1 in Nn arcs of ?n have the form:
wn-11 wn2wn/?u ? p(u|E)
To apply ?0 we make a copy of E , called E0,
with fixed weight ?0 on all arcs. The decoder is
formed as the composition E0 ??1 ??2 ??3 ??4
and E? is extracted as the maximum cost string.
5 Lattice Generation for LMBR
Lattice MBR decoding performance and effi-
ciency is evaluated in the context of the NIST
29
mt0205tune mt0205test mt08nw mt08ng
ML 54.2 53.8 51.4 36.3
k
0 52.6 52.3 49.8 34.5
1 54.8 54.4 52.2 36.6
2 54.9 54.5 52.4 36.8
3 54.9 54.5 52.4 36.8
LMBR 55.0 54.6 52.4 36.8
Table 1: BLEU scores for Arabic?English maximum likelihood translation (ML), MBR decoding using
the hybrid decision rule of Equation (5) at 0 ? k ? 3, and regular linearised lattice MBR (LMBR).
mt0205tune mt0205test mt08nw mt08ng
Posteriors
sequential 3160 3306 2090 3791
?Ln 6880 7387 4201 8796
?Rn 1746 1789 1182 2787
Decoding sequential 4340 4530 2225 4104
?n 284 319 118 197
Total
sequential 7711 8065 4437 8085
?Ln 7458 8075 4495 9199
?Rn 2321 2348 1468 3149
Table 2: Time in seconds required for path posterior n-gram probability calculation and LMBR decoding
using sequential method and left-most (?Ln) or right-most (?Rn ) counting transducer implementations.
Arabic?English machine translation task3. The
development set mt0205tune is formed from the
odd numbered sentences of the NIST MT02?
MT05 testsets; the even numbered sentences form
the validation set mt0205test. Performance on
NIST MT08 newswire (mt08nw) and newsgroup
(mt08ng) data is also reported.
First-pass translation is performed using HiFST
(Iglesias et al, 2009), a hierarchical phrase-based
decoder. Word alignments are generated using
MTTK (Deng and Byrne, 2008) over 150M words
of parallel text for the constrained NIST MT08
Arabic?English track. In decoding, a Shallow-
1 grammar with a single level of rule nesting is
used and no pruning is performed in generating
first-pass lattices (Iglesias et al, 2009).
The first-pass language model is a modified
Kneser-Ney (Kneser and Ney, 1995) 4-gram esti-
mated over the English parallel text and an 881M
word subset of the GigaWord Third Edition (Graff
et al, 2007). Prior to LMBR, the lattices are
rescored with large stupid-backoff 5-gram lan-
guage models (Brants et al, 2007) estimated over
more than 6 billion words of English text.
The n-gram factors ?0, . . . , ?4 are set according
to Tromble et al (2008) using unigram precision
3http://www.itl.nist.gov/iad/mig/tests/mt
p = 0.85 and average recall ratio r = 0.74. Our
translation decoder and MBR procedures are im-
plemented using OpenFst (Allauzen et al, 2007).
6 LMBR Speed and Performance
Lattice MBR decoding performance is shown in
Table 1. Compared to the maximum likelihood
translation hypotheses (row ML), LMBR gives
gains of +0.8 to +1.0 BLEU for newswire data and
+0.5 BLEU for newsgroup data (row LMBR).
The other rows of Table 1 show the performance
of LMBR decoding using the hybrid decision rule
of Equation (5) for 0 ? k ? 3. When the condi-
tional expected counts c(u|E) are used at all orders
(i.e. k = 0), the hybrid decoder BLEU scores are
considerably lower than even the ML scores. This
poor performance is because there are many un-
igrams u for which c(u|E) is much greater than
p(u|E). The consensus translation maximising the
conditional expected gain is then dominated by
unigram matches, significantly degrading LMBR
decoding performance. Table 1 shows that for
these lattices the hybrid decision rule is an ac-
curate approximation to Equation (1) only when
k ? 2 and the exact contribution to the gain func-
tion is computed using the path posterior probabil-
ities at orders n = 1 and n = 2.
30
We now analyse the efficiency of lattice MBR
decoding using the exact path posterior probabil-
ities of Equation (2) at all orders. We note that
the sequential method and both simultaneous im-
plementations using path counting transducers ?Ln
and ?Rn yield the same hypotheses (allowing for
numerical accuracy); they differ only in speed and
memory usage.
Posteriors Efficiency Computation times for
the steps in LMBR are given in Table 2. In calcu-
lating path posterior n-gram probabilities p(u|E),
we find that the use of ?Ln is more than twice
as slow as the sequential method. This is due to
the difficulty of counting higher-order n-grams in
large lattices. ?Ln is effective for counting uni-
grams, however, since there are far fewer of them.
Using ?Rn is almost twice as fast as the sequential
method. This speed difference is due to the sim-
ple forward algorithm. We also observe that for
higher-order n, the composition En ? ?Rn requires
less memory and produces a smaller machine than
En ? ?Ln . It is easier to count paths by the final
occurrence of a symbol than by the first.
Decoding Efficiency Decoding times are signif-
icantly faster using ?n than the sequential method;
average decoding time is around 0.1 seconds per
sentence. The total time required for lattice MBR
is dominated by the calculation of the path pos-
terior n-gram probabilities, and this is a func-
tion of the number of n-grams in the lattice |N |.
For each sentence in mt0205tune, Figure 3 plots
the total LMBR time for the sequential method
(marked ?o?) and for probabilities computed using
?Rn (marked ?+?). This compares the two tech-
niques on a sentence-by-sentence basis. As |N |
grows, the simultaneous path counting transducer
is found to be much more efficient.
7 Conclusion
We have described an efficient and exact imple-
mentation of the linear approximation to LMBR
using general WFST operations. A simple trans-
ducer was used to map words to sequences of n-
grams in order to simplify the extraction of higher-
order statistics. We presented a counting trans-
ducer ?Rn that extracts the statistics required for
all n-grams of order n in a single composition and
allows path posterior probabilities to be computed
efficiently using a modified forward procedure.
We take the view that even approximate search
0 1000 2000 3000 4000 5000 6000
0
10
20
30
40
50
60
70
 
 
sequential
simultaneous ?Rn
to
ta
lt
im
e
(se
co
n
ds
)
lattice n-grams
Figure 3: Total time in seconds versus |N |.
criteria should be implemented exactly where pos-
sible, so that it is clear exactly what the system is
doing. For machine translation lattices, conflat-
ing the values of p(u|E) and c(u|E) for higher-
order n-grams might not be a serious problem, but
in other scenarios ? especially where symbol se-
quences are repeated multiple times on the same
path ? it may be a poor approximation.
We note that since much of the time in calcula-
tion is spent dealing with ?-arcs that are ultimately
removed, an optimised composition algorithm that
skips over such redundant structure may lead to
further improvements in time efficiency.
Acknowledgments
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 557?564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: a
general and efficient weighted finite-state transducer
library. In Proceedings of the 9th International Con-
ference on Implementation and Application of Au-
tomata, pages 11?23. Springer.
Cyril Allauzen, Shankar Kumar, Wolfgang Macherey,
Mehryar Mohri, and Michael Riley. 2010. Expected
31
sequence similarity maximization. In Human Lan-
guage Technologies 2010: The 11th Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, Los Angeles,
California, June.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 858?867.
Yonggang Deng and William Byrne. 2008. HMM
word and phrase alignment for statistical machine
translation. IEEE Transactions on Audio, Speech,
and Language Processing, 16(3):494?507.
Jason Eisner. 2002. Parameter estimation for prob-
abilistic finite-state transducers. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 1?8, Philadel-
phia, July.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edition.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Hierarchical phrase-
based translation with weighted finite state trans-
ducers. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 433?441, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech,
and Signal Processing, pages 181?184.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2004 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 169?176.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the Association for Computational Linguistics and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 163?
171, Suntec, Singapore, August. Association for
Computational Linguistics.
M. Mohri, F.C.N. Pereira, and M. Riley. 2008. Speech
recognition with weighted finite-state transducers.
Handbook on Speech Processing and Speech Com-
munication.
Michael Riley, Cyril Allauzen, and Martin Jansche.
2009. OpenFst: An Open-Source, Weighted Finite-
State Transducer Library and its Applications to
Speech and Language. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Tutorial Abstracts, pages 9?10, Boulder, Col-
orado, May. Association for Computational Linguis-
tics.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
620?629, Honolulu, Hawaii, October. Association
for Computational Linguistics.
32
Proceedings of the ACL 2010 System Demonstrations, pages 48?53,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Personalising speech-to-speech translation in the EMIME project
Mikko Kurimo1?, William Byrne6, John Dines3, Philip N. Garner3, Matthew Gibson6,
Yong Guan5, Teemu Hirsima?ki1, Reima Karhila1, Simon King2, Hui Liang3, Keiichiro
Oura4, Lakshmi Saheer3, Matt Shannon6, Sayaka Shiota4, Jilei Tian5, Keiichi Tokuda4,
Mirjam Wester2, Yi-Jian Wu4, Junichi Yamagishi2
1 Aalto University, Finland, 2 University of Edinburgh, UK, 3 Idiap Research Institute,
Switzerland, 4 Nagoya Institute of Technology, Japan, 5 Nokia Research Center Beijing, China,
6 University of Cambridge, UK
?Corresponding author: Mikko.Kurimo@tkk.fi
Abstract
In the EMIME project we have studied un-
supervised cross-lingual speaker adapta-
tion. We have employed an HMM statisti-
cal framework for both speech recognition
and synthesis which provides transfor-
mation mechanisms to adapt the synthe-
sized voice in TTS (text-to-speech) using
the recognized voice in ASR (automatic
speech recognition). An important ap-
plication for this research is personalised
speech-to-speech translation that will use
the voice of the speaker in the input lan-
guage to utter the translated sentences in
the output language. In mobile environ-
ments this enhances the users? interaction
across language barriers by making the
output speech sound more like the origi-
nal speaker?s way of speaking, even if she
or he could not speak the output language.
1 Introduction
A mobile real-time speech-to-speech translation
(S2ST) device is one of the grand challenges in
natural language processing (NLP). It involves
several important NLP research areas: auto-
matic speech recognition (ASR), statistical ma-
chine translation (SMT) and speech synthesis, also
known as text-to-speech (TTS). In recent years
significant advance have also been made in rele-
vant technological devices: the size of powerful
computers has decreased to fit in a mobile phone
and fast WiFi and 3G networks have spread widely
to connect them to even more powerful computa-
tion servers. Several hand-held S2ST applications
and devices have already become available, for ex-
ample by IBM, Google or Jibbigo1, but there are
still serious limitations in vocabulary and language
selection and performance.
When an S2ST device is used in practical hu-
man interaction across a language barrier, one fea-
ture that is often missed is the personalization of
the output voice. Whoever speaks to the device in
what ever manner, the output voice always sounds
the same. Producing high-quality synthesis voices
is expensive and even if the system had many out-
put voices, it is hard to select one that would sound
like the input voice. There are many features in the
output voice that could raise the interaction expe-
rience to a much more natural level, for example,
emotions, speaking rate, loudness and the speaker
identity.
After the recent development in hidden Markov
model (HMM) based TTS, it has become possi-
ble to adapt the output voice using model trans-
formations that can be estimated from a small
number of speech samples. These techniques, for
instance the maximum likelihood linear regres-
sion (MLLR), are adopted from HMM-based ASR
where they are very powerful in fast adaptation of
speaker and recording environment characteristics
(Gales, 1998). Using hierarchical regression trees,
the TTS and ASR models can further be coupled
in a way that enables unsupervised TTS adaptation
(King et al, 2008). In unsupervised adaptation
samples are annotated by applying ASR. By elimi-
nating the need for human intervention it becomes
possible to perform voice adaptation for TTS in
almost real-time.
The target in the EMIME project2 is to study
unsupervised cross-lingual speaker adaptation for
S2ST systems. The first results of the project have
1http://www.jibbigo.com
2http://emime.org
48
been, for example, to bridge the gap between the
ASR and TTS (Dines et al, 2009), to improve
the baseline ASR (Hirsima?ki et al, 2009) and
SMT (de Gispert et al, 2009) systems for mor-
phologically rich languages, and to develop robust
TTS (Yamagishi et al, 2010). The next step has
been preliminary experiments in intra-lingual and
cross-lingual speaker adaptation (Wu et al, 2008).
For cross-lingual adaptation several new methods
have been proposed for mapping the HMM states,
adaptation data and model transformations (Wu et
al., 2009).
In this presentation we can demonstrate the var-
ious new results in ASR, SMT and TTS. Even
though the project is still ongoing, we have an
initial version of mobile S2ST system and cross-
lingual speaker adaptation to show.
2 Baseline ASR, TTS and SMT systems
The baseline ASR systems in the project are devel-
oped using the HTK toolkit (Young et al, 2001)
for Finnish, English, Mandarin and Japanese. The
systems can also utilize various real-time decoders
such as Julius (Kawahara et al, 2000), Juicer at
IDIAP and the TKK decoder (Hirsima?ki et al,
2006). The main structure of the baseline sys-
tems for each of the four languages is similar and
fairly standard and in line with most other state-of-
the-art large vocabulary ASR systems. Some spe-
cial flavors for have been added, such as the mor-
phological analysis for Finnish (Hirsima?ki et al,
2009). For speaker adaptation, the MLLR trans-
formation based on hierarchical regression classes
is included for all languages.
The baseline TTS systems in the project utilize
the HTS toolkit (Yamagishi et al, 2009) which
is built on top of the HTK framework. The
HMM-based TTS systems have been developed
for Finnish, English, Mandarin and Japanese. The
systems include an average voice model for each
language trained over hundreds of speakers taken
from standard ASR corpora, such as Speecon
(Iskra et al, 2002). Using speaker adaptation
transforms, thousands of new voices have been
created (Yamagishi et al, 2010) and new voices
can be added using a small number of either su-
pervised or unsupervised speech samples. Cross-
lingual adaptation is possible by creating a map-
ping between the HMM states in the input and the
output language (Wu et al, 2009).
Because the resources of the EMIME project
have been focused on ASR, TTS and speaker
adaptation, we aim at relying on existing solu-
tions for SMT as far as possible. New methods
have been studied concerning the morphologically
rich languages (de Gispert et al, 2009), but for the
S2ST system we are currently using Google trans-
late3.
3 Demonstrations to show
3.1 Monolingual systems
In robust speech synthesis, a computer can learn
to speak in the desired way after processing only a
relatively small amount of training speech. The
training speech can even be a normal quality
recording outside the studio environment, where
the target speaker is speaking to a standard micro-
phone and the speech is not annotated. This differs
dramatically from conventional TTS, where build-
ing a new voice requires an hour or more careful
repetition of specially selected prompts recorded
in an anechoic chamber with high quality equip-
ment.
Robust TTS has recently become possible us-
ing the statistical HMM framework for both ASR
and TTS. This framework enables the use of ef-
ficient speaker adaptation transformations devel-
oped for ASR to be used also for the TTS mod-
els. Using large corpora collected for ASR, we can
train average voice models for both ASR and TTS.
The training data may include a small amount of
speech with poor coverage of phonetic contexts
from each single speaker, but by summing the ma-
terial over hundreds of speakers, we can obtain
sufficient models for an average speaker. Only a
small amount of adaptation data is then required to
create transformations for tuning the average voice
closer to the target voice.
In addition to the supervised adaptation us-
ing annotated speech, it is also possible to em-
ploy ASR to create annotations. This unsu-
pervised adaptation enables the system to use a
much broader selection of sources, for example,
recorded samples from the internet, to learn a new
voice.
The following systems will demonstrate the re-
sults of monolingual adaptation:
1. In EMIME Voice cloning in Finnish and En-
glish the goal is that the users can clone their
own voice. The user will dictate for about
3http://translate.google.com
49
Figure 1: Geographical representation of HTS voices trained on ASR corpora for EMIME projects.
Blue markers show male speakers and red markers show female speakers. Available online via
http://www.emime.org/learn/speech-synthesis/listen/Examples-for-D2.1
10 minutes and then after half an hour of
processing time, the TTS system has trans-
formed the average model towards the user?s
voice and can speak with this voice. The
cloned voices may become especially valu-
able, for example, if a person?s voice is later
damaged in an accident or by a disease.
2. In EMIME Thousand voices map the goal is
to browse the world?s largest collection of
synthetic voices by using a world map in-
terface (Yamagishi et al, 2010). The user
can zoom in the world map and select any
voice, which are organized according to the
place of living of the adapted speaker, to ut-
ter the given sentence. This interactive ge-
ographical representation is shown in Figure
1. Each marker corresponds to an individual
speaker. Blue markers show male speakers
and red markers show female speakers. Some
markers are in arbitrary locations (in the cor-
rect country) because precise location infor-
mation is not available for all speakers. This
geographical representation, which includes
an interactive TTS demonstration of many of
the voices, is available from the URL pro-
vided. Clicking on a marker will play syn-
thetic speech from that speaker4. As well as
4Currently the interactive mode supports English and
Spanish only. For other languages this only provides pre-
being a convenient interface to compare the
many voices, the interactive map is an attrac-
tive and easy-to-understand demonstration of
the technology being developed in EMIME.
3. The models developed in the HMM frame-
work can be demonstrated also in adapta-
tion of an ASR system for large-vocabulary
continuous speech recognition. By utilizing
morpheme-based language models instead of
word-based models the Finnish ASR system
is able to cover practically an unlimited vo-
cabulary (Hirsima?ki et al, 2006). This is
necessary for morphologically rich languages
where, due to inflection, derivation and com-
position, there exists so many different word
forms that word based language modeling be-
comes impractical.
3.2 Cross-lingual systems
In the EMIME project the goal is to learn cross-
lingual speaker adaptation. Here the output lan-
guage ASR or TTS system is adapted from speech
samples in the input language. The results so far
are encouraging, especially for TTS: Even though
the cross-lingual adaptation may somewhat de-
grade the synthesis quality, the adapted speech
now sounds more like the target speaker. Sev-
eral recent evaluations of the cross-lingual speaker
synthesised examples, but we plan to add an interactive type-
in text-to-speech feature in the near future.
50
Figure 2: All English HTS voices can be used as online TTS on the geographical map.
adaptation methods can be found in (Gibson et al,
2010; Oura et al, 2010; Liang et al, 2010; Oura
et al, 2009).
The following systems have been created to
demonstrate cross-lingual adaptation:
1. In EMIME Cross-lingual Finnish/English
and Mandarin/English TTS adaptation the
input language sentences dictated by the user
will be used to learn the characteristics of her
or his voice. The adapted cross-lingual model
will be used to speak output language (En-
glish) sentences in the user?s voice. The user
does not need to be bilingual and only reads
sentences in their native language.
2. In EMIME Real-time speech-to-speech mo-
bile translation demo two users will interact
using a pair of mobile N97 devices (see Fig-
ure 3). The system will recognize the phrase
the other user is speaking in his native lan-
guage and translate and speak it in the native
language of the other user. After a few sen-
tences the system will have the speaker adap-
tation transformations ready and can apply
them in the synthesized voices to make them
sound more like the original speaker instead
of a standard voice. The first real-time demo
version is available for the Mandarin/English
language pair.
3. The morpheme-based translation system for
Finnish/English and English/Finnish can be
compared to a word based translation for
arbitrary sentences. The morpheme-based
approach is particularly useful for language
pairs where one or both languages are mor-
phologically rich ones where the amount and
complexity of different word forms severely
limits the performance for word-based trans-
lation. The morpheme-based systems can
learn translation models for phrases where
morphemes are used instead of words (de
Gispert et al, 2009). Recent evaluations (Ku-
rimo et al, 2009) have shown that the perfor-
mance of the unsupervised data-driven mor-
pheme segmentation can rival the conven-
tional rule-based ones. This is very useful if
hand-crafted morphological analyzers are not
available or their coverage is not sufficient for
all languages.
Acknowledgments
The research leading to these results was partly
funded from the European Communitys Seventh
51
  
ASR SMT TTS
Cross-lingualSpeaker adaptation
Speakeradaptation
input outputspeech
Figure 3: EMIME Real-time speech-to-speech
mobile translation demo
Framework Programme (FP7/2007-2013) under
grant agreement 213845 (the EMIME project).
References
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum Bayes risk combination of transla-
tion hypotheses from alternative morphological de-
compositions. In Proc. NAACL-HLT.
J. Dines, J. Yamagishi, and S. King. 2009. Measur-
ing the gap between HMM-based ASR and TTS. In
Proc. Interspeech ?09, Brighton, UK.
M. Gales. 1998. Maximum likelihood linear transfor-
mations for HMM-based speech recognition. Com-
puter Speech and Language, 12(2):75?98.
M. Gibson, T. Hirsima?ki, R. Karhila, M. Kurimo,
and W. Byrne. 2010. Unsupervised cross-lingual
speaker adaptation for HMM-based speech synthe-
sis using two-pass decision tree construction. In
Proc. of ICASSP, page to appear, March.
T. Hirsima?ki, M. Creutz, V. Siivola, M. Kurimo, S.
Virpioja, and J. Pylkko?nen. 2006. Unlimited vo-
cabulary speech recognition with morph language
models applied to finnish. Computer Speech & Lan-
guage, 20(4):515?541, October.
T. Hirsima?ki, J. Pylkko?nen, and M Kurimo. 2009.
Importance of high-order n-gram models in morph-
based speech recognition. IEEE Trans. Audio,
Speech, and Language Process., 17:724?732.
D. Iskra, B. Grosskopf, K. Marasek, H. van den
Heuvel, F. Diehl, and A. Kiessling. 2002.
SPEECON speech databases for consumer devices:
Database specification and validation. In Proc.
LREC, pages 329?333.
T. Kawahara, A. Lee, T. Kobayashi, K. Takeda,
N. Minematsu, S. Sagayama, K. Itou, A. Ito, M. Ya-
mamoto, A. Yamada, T. Utsuro, and K. Shikano.
2000. Free software toolkit for japanese large vo-
cabulary continuous speech recognition. In Proc.
ICSLP-2000, volume 4, pages 476?479.
S. King, K. Tokuda, H. Zen, and J. Yamagishi. 2008.
Unsupervised adaptation for HMM-based speech
synthesis. In Proc. Interspeech 2008, pages 1869?
1872, September.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
H. Liang, J. Dines, and L. Saheer. 2010. A
comparison of supervised and unsupervised cross-
lingual speaker adaptation approaches for HMM-
based speech synthesis. In Proc. of ICASSP, page
to appear, March.
Keiichiro Oura, Junichi Yamagishi, Simon King, Mir-
jam Wester, and Keiichi Tokuda. 2009. Unsuper-
vised speaker adaptation for speech-to-speech trans-
lation system. In Proc. SLP (Spoken Language Pro-
cessing), number 356 in 109, pages 13?18.
K. Oura, K. Tokuda, J. Yamagishi, S. King, and
M. Wester. 2010. Unsupervised cross-lingual
speaker adaptation for HMM-based speech synthe-
sis. In Proc. of ICASSP, page to appear, March.
Y.-J. Wu, S. King, and K. Tokuda. 2008. Cross-lingual
speaker adaptation for HMM-based speech synthe-
sis. In Proc. of ISCSLP, pages 1?4, December.
Y.-J. Wu, Y. Nankaku, and K. Tokuda. 2009. State
mapping based method for cross-lingual speaker
adaptation in HMM-based speech synthesis. In
Proc. of Interspeech, pages 528?531, September.
J. Yamagishi, T. Nose, H. Zen, Z.-H. Ling, T. Toda,
K. Tokuda, S. King, and S. Renals. 2009. Robust
speaker-adaptive HMM-based text-to-speech syn-
thesis. IEEE Trans. Audio, Speech and Language
Process., 17(6):1208?1230. (in press).
J. Yamagishi, B. Usabaev, S. King, O. Watts, J. Dines,
J. Tian, R. Hu, K. Oura, K. Tokuda, R. Karhila, and
M. Kurimo. 2010. Thousands of voices for hmm-
based speech synthesis. IEEE Trans. Speech, Audio
& Language Process. (in press).
52
S. Young, G. Everman, D. Kershaw, G. Moore, J.
Odell, D. Ollason, V. Valtchev, and P. Woodland,
2001. The HTK Book Version 3.1, December.
53
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 155?160,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The CUED HiFST System for the WMT10 Translation Shared Task
Juan Pino Gonzalo Iglesias?1 Adria` de Gispert
Graeme Blackwood Jamie Brunning William Byrne
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
{jmp84,gi212,ad465,gwb24,jjjb2,wjb31}@eng.cam.ac.uk
? Department of Signal Processing and Communications, University of Vigo, Vigo, Spain
Abstract
This paper describes the Cambridge Uni-
versity Engineering Department submis-
sion to the Fifth Workshop on Statistical
Machine Translation. We report results for
the French-English and Spanish-English
shared translation tasks in both directions.
The CUED system is based on HiFST, a
hierarchical phrase-based decoder imple-
mented using weighted finite-state trans-
ducers. In the French-English task, we
investigate the use of context-dependent
alignment models. We also show that
lattice minimum Bayes-risk decoding is
an effective framework for multi-source
translation, leading to large gains in BLEU
score.
1 Introduction
This paper describes the Cambridge University
Engineering Department (CUED) system submis-
sion to the ACL 2010 Fifth Workshop on Statis-
tical Machine Translation (WMT10). Our trans-
lation system is HiFST (Iglesias et al, 2009a), a
hierarchical phrase-based decoder that generates
translation lattices directly. Decoding is guided
by a CYK parser based on a synchronous context-
free grammar induced from automatic word align-
ments (Chiang, 2007). The decoder is imple-
mented with Weighted Finite State Transducers
(WFSTs) using standard operations available in
the OpenFst libraries (Allauzen et al, 2007). The
use of WFSTs allows fast and efficient exploration
of a vast translation search space, avoiding search
errors in decoding. It also allows better integration
with other steps in our translation pipeline such as
5-gram language model (LM) rescoring and lattice
minimum Bayes-risk (LMBR) decoding.
1Now a member of the Department of Engineering, Uni-
versity of Cambridge, Cambridge, CB2 1PZ, U.K.
# Sentences # Tokens # Types
(A)Europarl+News-Commentary
FR 1.7 M 52.4M 139.7kEN 47.6M 121.6k
(B)Europarl+News-Commentary+UN
FR 8.7 M 277.9M 421.0kEN 241.4M 482.1k
(C)Europarl+News-Commentary+UN+Giga
FR 30.2 M 962.4M 2.4MEN 815.3M 2.7M
Table 1: Parallel data sets used for French-to-
English experiments.
We participated in the French-English and
Spanish-English translation shared tasks in each
translation direction. This paper describes the de-
velopment of these systems. Additionally, we re-
port multi-source translation experiments that lead
to very large gains in BLEU score.
The paper is organised as follows. Section 2
describes each step in the development of our sys-
tem for submission, from pre-processing to post-
processing. Section 3 presents and discusses re-
sults and Section 4 describes an additional experi-
ment on multi-source translation.
2 System Development
We built three French-English and two Spanish-
English systems, trained on different portions of
the parallel data sets available for this shared task.
Statistics for the different parallel sets are sum-
marised in Tables 1 and 2. No additional parallel
data was used. As will be shown, the largest paral-
lel corpus gave the best results in French, but this
was not the case in Spanish.
2.1 Pre-processing
The data was minimally cleaned by replacing
HTML-related metatags by their corresponding
155
# Sentences # Tokens # Types
(A) Europarl + News-Commentary
SP 1.7M 49.4M 167.2kEN 47.0M 122.7k
(B) Europarl + News-Commentary + UN
SP 6.5M 205.6M 420.8kEN 192.0M 402.8k
Table 2: Parallel data sets used for Spanish-to-
English experiments.
UTF8 token (e.g., replacing ?&amp? by ?&?) as
this interacts with tokenization. Data was then to-
kenized and lowercased, so mixed case is added as
post-processing.
2.2 Alignments
Parallel data was aligned using the MTTK toolkit
(Deng and Byrne, 2005). In the English-to-French
and English-to-Spanish directions, we trained
a word-to-phrase HMM model with maximum
phrase length of 2. In the French to English and
Spanish to English directions, we trained a word-
to-phrase HMM Model with a bigram translation
table and maximum phrase length of 4.
We also trained context-dependent alignment
models (Brunning et al, 2009) for the French-
English medium-size (B) dataset. The context of
a word is based on its part-of-speech and the part-
of-speech tags of the surrounding words. These
tags were obtained by applying the TnT Tagger
(Brants, 2000) for English and the TreeTagger
(Schmid, 1994) for French. Decision tree clus-
tering based on optimisation of the EM auxiliary
function was used to group contexts that trans-
late similarly. Unfortunately, time constraints pre-
vented us from training context-dependent models
for the larger (C) dataset.
2.3 Language Model
For each target language, we used the SRILM
Toolkit (Stolcke, 2002) to estimate separate 4-
gram LMs with Kneser-Ney smoothing (Kneser
and Ney, 1995), for each of the corpora listed in
Tables 3, 4 and 5. The LM vocabulary was ad-
justed to the parallel data set used. The compo-
nent models of each language pair were then in-
terpolated to form a single LM for use in first-pass
translation decoding. For French-to-English trans-
lation, the interpolation weights were optimised
for perplexity on a development set.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 246.4M
CNA 1.3M 34.8M
LTW 12.9M 298.7M
XIN 16.0M 352.5M
AFP 30.4M 710.6M
APW 62.1M 1268.6M
NYT 73.6M 1622.5M
Giga 21.4M 573.8M
News 48.7M 1128.4M
Total 275.4M 6236.4M
Table 3: English monolingual training corpora.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 282.8
AFP 25.2M 696.0M
APW 12.7M 300.6M
News 15.2M 373.5M
Giga 21.4M 684.4M
Total 83.5 M 2337.3M
Table 4: French monolingual training corpora.
Corpus # Sentences # Tokens
NC + News 4.0M 110.8M
EU + Gigaword (5g) 249.4M 1351.5M
Total 253.4 M 1462.3M
Table 5: Spanish monolingual training corpora.
The Spanish-English first pass LM was trained
directly on the NC+News portion of monolingual
data, as we did not find improvements by using
Europarl. The second pass rescoring LM used all
available data.
2.4 Grammar Extraction and Decoding
After unioning the Viterbi alignments, phrase-
based rules of up to five source words in length
were extracted, hierarchical rules with up to two
non-contiguous non-terminals in the source side
were then extracted applying the restrictions de-
scribed in (Chiang, 2007). For Spanish-English
and French-English tasks, we used a shallow-1
grammar where hierarchical rules are allowed to
be applied only once on top of phrase-based rules.
This has been shown to perform as well as a
fully hierarchical grammar for a Europarl Spanish-
English task (Iglesias et al, 2009b).
For translation, we used the HiFST de-
156
coder (Iglesias et al, 2009a). HiFST is a hierarchi-
cal decoder that builds target word lattices guided
by a probabilistic synchronous context-free gram-
mar. Assuming N to be the set of non-terminals
and T the set of terminals or words, then we can
define the grammar as a set R = {Rr} of rules
Rr : N ? ??r,?r? / pr, where N ? N; and
?, ? ? {N ? T}+.
HiFST translates in three steps. The first step
is a variant of the CYK algorithm (Chappelier and
Rajman, 1998), in which we apply hypothesis re-
combination without pruning. Only the source
language sentence is parsed using the correspond-
ing source-side context-free grammar with rules
N ? ?. Each cell in the CYK grid is specified
by a non-terminal symbol and position: (N,x, y),
spanning sx+y?1x on the source sentence s1...sJ .
For the second step, we use a recursive algo-
rithm to construct word lattices with all possi-
ble translations produced by the hierarchical rules.
Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In
each cell (N,x, y) of the CYK grid, we build a
target language word lattice L(N,x, y) containing
every translation of sx+y?1x from every derivation
headed by N . For efficiency, this lattice can use
pointers to lattices on other cells of the grid.
In the third step, we apply the word-based LM
via standard WFST composition with failure tran-
sitions, and perform likelihood-based pruning (Al-
lauzen et al, 2007) based on the combined trans-
lation and LM scores.
As explained before, we are using shallow-1 hi-
erarchical grammars (de Gispert et al, 2010) in
our experiments for WMT2010. One very inter-
esting aspect is that HiFST is able to build ex-
act search spaces with this model, i.e. there is no
pruning in search that may lead to spurious under-
generation errors.
2.5 Parameter Optimisation
Minimum error rate training (MERT) (Och, 2003)
under the BLEU score (Papineni et al, 2001) opti-
mises the weights of the following decoder fea-
tures with respect to the newstest2008 develop-
ment set: target LM, number of usages of the
glue rule, word and rule insertion penalties, word
deletion scale factor, source-to-target and target-
to-source translation models, source-to-target and
target-to-source lexical models, and three binary
rule count features inspired by Bender et al (2007)
indicating whether a rule occurs once, twice, or
more than twice in the parallel training data.
2.6 Lattice Rescoring
One of the advantages of HiFST is direct gener-
ation of large translation lattices encoding many
alternative translation hypotheses. These first-pass
lattices are rescored with second-pass higher-order
LMs prior to LMBR.
2.6.1 5-gram LM Lattice Rescoring
We build sentence-specific, zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram LMs esti-
mated over approximately 6.2 billion words for
English, 2.3 billion words for French, and 1.4 bil-
lion words for Spanish. For the English-French
task, the second-pass LM training data is the same
monolingual data used for the first-pass LMs (as
summarised in Tables 3, 4). The Spanish second-
pass 5-gram LM includes an additional 1.4 billion
words of monolingual data from the Spanish Giga-
Word Second Edition (Mendonca et al, 2009) and
Europarl, which were not included in the first-pass
LM (see Table 5).
2.6.2 LMBR Decoding
Minimum Bayes-risk (MBR) decoding (Kumar
and Byrne, 2004) over the full evidence space
of the 5-gram rescored lattices was applied to
select the translation hypothesis that maximises
the conditional expected gain under the linearised
sentence-level BLEU score (Tromble et al, 2008;
Blackwood and Byrne, 2010). The unigram preci-
sion p and average recall ratio r were set as de-
scribed in Tromble et al (2008) using the new-
stest2008 development set.
2.7 Hypothesis Combination
Linearised lattice minimum Bayes-risk decoding
(Tromble et al, 2008) can also be used as an ef-
fective framework for multiple lattice combination
(de Gispert et al, 2010). For the French-English
language pair, we used LMBR to combine transla-
tion lattices produced by systems trained on alter-
native data sets.
2.8 Post-processing
For both Spanish-English and French-English sys-
tems, the recasing procedure was performed with
the SRILM toolkit. For the Spanish-English sys-
tem, we created models from the GigaWord set
corresponding to each system output language.
157
Task Configuration newstest2008 newstest2009 newstest2010
FR ? EN
HiFST (A) 23.4 26.4 ?
HiFST (B) 24.0 27.3 ?
HiFST (B)CD 24.2 27.6 28.0
+5g+LMBR 24.6 28.4 28.9
HiFST (C) 24.7 28.4 28.5
+5g+LMBR 25.3 29.1 29.3
LMBR (B)CD+(C) 25.6 29.3 29.6
EN ? FR
HiFST (A) 22.5 24.2 ?
HiFST (B) 23.4 24.8 ?
HiFST (B)CD 23.3 24.8 26.7
+5g+LMBR 23.7 25.3 27.1
HiFST (C) 23.6 25.6 27.4
+5g+LMBR 23.9 25.8 27.8
LMBR (B)CD+(C) 24.2 26.1 28.2
Table 6: Translation Results for the French-English (FR-EN) language pair, shown in single-reference
lowercase IBM BLEU. Bold results correspond to submitted systems.
For the French-English system, the English model
was trained using the monolingual News corpus
and the target side of the News-Commentary cor-
pus, whereas the French model was trained using
all available constrained French data.
English, Spanish and French outputs were also
detokenized before submission. In French, words
separated by apostrophes were joined.
3 Results and Discussion
French?English Language Pair
Results are reported in Table 6. We can see
that using more parallel data consistently improves
performance. In the French-to-English direction,
the system HiFST (B) improves over HiFST (A)
by +0.9 BLEU and HiFST (C) improves over
HiFST (B) by +1.1 BLEU on the newstest2009
development set prior to any rescoring. The
same trend can be observed in the English-to-
French direction (+0.6 BLEU and +0.8 BLEU im-
provement). The use of context dependent align-
ment models gives a small improvement in the
French-to-English direction: system (B)CD im-
proves by +0.3 BLEU over system (B) on new-
stest2009. In the English-to-French direction,
there is no improvement nor degradation in per-
formance. 5-gram and LMBR rescoring also give
consistent improvement throughout the datasets.
Finally, combination between the medium-size
system (B)CD and the full-size system (C) gives
further small gains in BLEU over LMBR on each
individual system.
Spanish?English Language Pair
Results are reported in Table 7. We report experi-
mental results on two systems. The HiFST(A) sys-
tem is built on the Europarl + News-Commentary
training set. Systems HiFST (B),(B2) and (B3)
use UN data in different ways. System (B) simply
uses all the data for the standard rule extraction
procedure. System HiFST (B2) includes UN data
to build alignment models and therefore reinforce
alignments obtained from smaller dataset (A), but
extracts rules only from dataset (A). HiFST (B3)
combines hierarchical phrases extracted for sys-
tem (A) with phrases extracted from system (B).
Unfortunately, these three larger data strategies
lead to degradation over using only the smaller
dataset (A). For this reason, our best systems only
use the Euparl + News-Commentary parallel data.
This is surprising given that additional data was
helpful for the French-English task. Solving this
issue is left for future work.
4 Multi-Source Translation Experiments
Multi-source translation (Och and Ney, 2001;
Schroeder et al, 2009) is possible whenever mul-
tiple translations of the source language input sen-
tence are available. The motivation for multi-
source translation is that some of the ambiguity
that must be resolved in translating between one
pair of languages may not be present in a differ-
ent pair. In the following experiments, multiple
LMBR is applied for the first time to the task of
multi-source translation.
158
Task Configuration newstest2008 newstest2009 newstest2010
SP ? EN
HiFST (A) 24.6 26.0 29.1
+5g+LMBR 25.4 27.0 30.5
HiFST (B) 23.7 25.4 ?
HiFST (B2) 24.3 25.7 ?
HiFST (B3) 24.2 25.6 ?
EN ? SP HiFST (A) 23.9 24.5 28.0
+5g+LMBR 24.7 25.5 29.1
Table 7: Translation Results for the Spanish-English (SP-EN) language pair, shown in lowercase IBM
BLEU. Bold results correspond to submitted systems.
Configuration newstest2008 newstest2009 newstest2010
FR?EN HiFST+5g 24.8 28.5 28.8
+LMBR 25.3 29.0 29.2
ES?EN HiFST+5g 25.2 26.8 30.1
+LMBR 25.4 26.9 30.3
FR?EN + ES?EN LMBR 27.2 30.4 32.0
Table 8: Lowercase IBM BLEU for single-system LMBR and multiple LMBR multi-source translation
of French (FR) and Spanish (ES) into English (EN).
Separate second-pass 5-gram rescored lattices
EFR and EES are generated for each test set sen-
tence using the French-to-English and Spanish-to-
English HiFST translation systems. The MBR hy-
pothesis space is formed as the union of these lat-
tices. In a similar manner to MBR decoding over
multiple k-best lists in de Gispert et al (2009),
the path posterior probability of each n-gram u re-
quired for linearised LMBR is computed as a lin-
ear interpolation of the posterior probabilities ac-
cording to each individual lattice so that p(u|E) =
?FR p(u|EFR) + ?ES p(u|EES), where p(u|E) is the
sum of the posterior probabilities of all paths con-
taining the n-gram u. The interpolation weights
?FR + ?ES = 1 are optimised for BLEU score on
the development set newstest2008.
The results of single-system and multi-source
LMBR decoding are shown in Table 8. The opti-
mised interpolation weights were ?FR = 0.55 and
?ES = 0.45. Single-system LMBR gives relatively
small gains on these test sets. Much larger gains
are obtained through multi-source MBR combina-
tion. Compared to the best of the single-system 5-
gram rescored lattices, the BLEU score improves
by +2.0 for newstest2008, +1.9 for newstest2009,
and +1.9 for newstest2010. For scoring with re-
spect to a single reference, these are very large
gains indeed.
5 Summary
We have described the CUED submission to
WMT10 using HiFST, a hierarchical phrase-based
translation system. Results are very competitive in
terms of automatic metric for both English-French
and English-Spanish tasks in both directions. In
the French-English task, we have seen that the UN
and Giga additional parallel data are helpful. It
is surprising that UN data did not help for the
Spanish-English language pair.
Future work includes investigating this issue,
developing detokenization tailored to each output
language and applying context dependent align-
ment models to larger parallel datasets.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022. Gonzalo Iglesias was sup-
ported by the Spanish Government research grant
BES-2007-15956 (projects TEC2006-13694-C03-
03 and TEC2009-14094-C04-04).
159
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood and William Byrne. 2010. Ef-
ficient Path Counting Transducers for Minimum
Bayes-Risk Decoding of Statistical Machine Trans-
lation Lattices (to appear). In Proceedings of the
ACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of ANLP, pages 224?
231, April.
Jamie Brunning, Adria` de Gispert, and William Byrne.
2009. Context-dependent alignment models for
statistical machine translation. In Proceedings of
HLT/NAACL, pages 110?118.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alterna-
tive Morphological Decompositions. In Proceed-
ings of HLT/NAACL, Companion Volume: Short Pa-
pers, pages 73?76.
Adria` de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite state transducers and shallow-n grammars (to
appear). In Computational Linguistics.
Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. In Proceedings of HLT/EMNLP, pages
169?176.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-
based translation with weighted finite state transduc-
ers. In Proceedings of NAACL, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. The HiFST System for
the EuroParl Spanish-to-English Task. In Proceed-
ings of SEPLN, pages 207?214.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of ICASSP, volume 1, pages 181?184.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Angelo Mendonca, David Graff, and Denise DiPersio.
2009. Spanish Gigaword Second Edition, Linguistic
Data Consortium.
Franz Josef Och and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Machine Translation
Summit 2001, pages 253?258.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12. Manchester, UK.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word Lattices for Multi-Source Translation.
In Proceedings of EACL, pages 719?727.
Andreas Stolcke. 2002. SRILM?An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP,
volume 3, pages 901?904.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk decoding for statistical machine trans-
lation. In Proceedings of EMNLP, pages 620?629.
160
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 116?125,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Lattice-Based Minimum Error Rate Training using Weighted Finite-State
Transducers with Tropical Polynomial Weights
Aurelien Waite? Graeme Blackwood?
? Department of Engineering, University of Cambridge, Trumpington Street, CB2 1PZ, U.K.
{aaw35|wjb31}@cam.ac.uk
? IBM T.J. Watson Research, Yorktown Heights, NY-10598
blackwood@us.ibm.com
William Byrne?
Abstract
Minimum Error Rate Training (MERT) is a
method for training the parameters of a log-
linear model. One advantage of this method
of training is that it can use the large num-
ber of hypotheses encoded in a translation lat-
tice as training data. We demonstrate that the
MERT line optimisation can be modelled as
computing the shortest distance in a weighted
finite-state transducer using a tropical polyno-
mial semiring.
1 Introduction
Minimum Error Rate Training (MERT) (Och, 2003)
is an iterative procedure for training a log-linear sta-
tistical machine translation (SMT) model (Och and
Ney, 2002). MERT optimises model parameters
directly against a criterion based on an automated
translation quality metric, such as BLEU (Papineni
et al, 2002). Koehn (2010) provides a full descrip-
tion of the SMT task and MERT.
MERT uses a line optimisation procedure (Press
et al, 2002) to identify a range of points along a line
in parameter space that maximise an objective func-
tion based on the BLEU score. A key property of the
line optimisation is that it can consider a large set of
hypotheses encoded as a weighted directed acyclic
graph (Macherey et al, 2008), which is called a lat-
tice. The line optimisation procedure can also be ap-
plied to a hypergraph representation of the hypothe-
ses (Kumar et al, 2009).
?The work reported in this paper was carried out while the
author was at the University of Cambridge.
It has been noted that line optimisation over a lat-
tice can be implemented as a semiring of sets of lin-
ear functions (Dyer et al, 2010). Sokolov and Yvon
(2011) provide a formal description of such a semir-
ing, which they denote the MERT semiring. The dif-
ference between the various algorithms derives from
the differences in their formulation and implemen-
tation, but not in the objective they attempt to opti-
mise.
Instead of an algebra defined in terms of trans-
formations of sets of linear functions, we propose
an alternative formulation using the tropical polyno-
mial semiring (Speyer and Sturmfels, 2009). This
semiring provides a concise formalism for describ-
ing line optimisation, an intuitive explanation of the
MERT shortest distance, and draws on techniques
in the currently active field of Tropical Geometry
(Richter-Gebert et al, 2005) 1.
We begin with a review of the line optimisation
procedure, lattice-based MERT, and the weighted
finite-state transducer formulation in Section 2. In
Section 3, we introduce our novel formulation
of lattice-based MERT using tropical polynomial
weights. Section 4 compares the performance of our
approach with k-best and lattice-based MERT.
2 Minimum Error Rate Training
Following Och and Ney (2002), we assume that
we are given a tuning set of parallel sentences
{(r1, f1), ..., (rS , fS)}, where rs is the reference
translation of the source sentence fs. We also as-
sume that sets of hypotheses Cs = {es,1, ..., es,K}
1An associated technical report contains an extended discus-
sion of our approach (Waite et al, 2011)
116
are available for each source sentence fs.
Under the log-linear model formulation with fea-
ture functions hM1 and model parameters ?M1 , the
most probable translation in a set Cs is selected as
e?(fs;?M1 ) = argmax
e?Cs
{ M?
m=1
?mhm(e, fs)
}
. (1)
With an error function of the form E(rS1 , eS1 ) =?S
s=1E(rs, es), MERT attempts to find model pa-
rameters to minimise the following objective:
??M1 = argmin
?M1
{ S?
s=1
E(rs, e?(fs;?M1 ))
}
. (2)
Note that for MERT the hypotheses set Cs is
a k-best list of explicitly enumerated hypotheses,
whereas lattice-based MERT uses a larger space.
2.1 Line Optimisation
Although the objective function in Eq. (2) cannot be
solved analytically, the line optimisation procedure
of Och (2003) can be used to find an approxima-
tion of the optimal model parameters. Rather than
evaluating the decision rule in Eq. (1) over all pos-
sible points in parameter space, the line optimisa-
tion considers a subset of points defined by the line
?M1 +?dM1 , where ?M1 corresponds to an initial point
in parameter space and dM1 is the direction along
which to optimise. Eq. (1) can be rewritten as:
e?(fs; ?) = argmax
e?Cs
{
(?M1 + ?dM1 )ThM1 (e, f s)
}
= argmax
e?Cs
{?
m
?mhm(e, f s)
? ?? ?
a(e,fs)
+?
?
m
dmhm(e, f s)
? ?? ?
b(e,fs)
}
= argmax
e?Cs
{a(e, f s) + ?b(e, f s)? ?? ?
`e(?)
} (3)
This decision rule shows that each hypothesis
e ? Cs is associated with a linear function of ?:
`e(?) = a(e, f s) + ?b(e, f s), where a(e, f s) is the
y-intercept and b(e, f s) is the gradient. The opti-
misation problem is further simplified by defining a
subspace over which optimisation is performed. The
subspace is found by considering a form of the func-
tion in Eq. (3) defined with a range of real numbers
(Macherey et al, 2008; Och, 2003):
Env(f) = max
e?C
{a(e, f) + ?b(e, f)? ?? ?
`e(?)
: ? ? R} (4)
?
Env(fs; ?)
`e1
`e2 `e3
`e4
?
E(rs, e?(fs; ?))
e4
e3
e1
?1 ?2
Figure 1: An upper envelope and projected error. Note
that the upper envelope is completely defined by hypothe-
ses e4, e3, and e1, together with the intersection points ?1
and ?2 (after Macherey et al (2008), Fig. 1).
For any value of ? the linear functions `e(?) associ-
ated with Cs take (up to) K values. The function in
Eq. (4) defines the ?upper envelope? of these values
over all ?. The upper envelope has the form of a con-
tinuous piecewise linear function in ?. The piece-
wise linear function can be compactly described by
the linear functions which form line segments and
the values of ? at which they intersect. The example
in the upper part of Figure 1 shows how the upper
envelope associated with a set of four hypotheses
can be represented by three associated linear func-
tions and two values of ?. The first step of line op-
timisation is to compute this compact representation
of the upper envelope.
Macherey et al (2008) use methods from com-
putational geometry to compute the upper envelope.
The SweepLine algorithm (Bentley and Ottmann,
1979) computes the upper envelope from a set of lin-
ear functions with a complexity of O(K log(K)).
Computing the upper envelope reduces the run-
time cost of line optimisation as the error function
need only be evaluated for the subset of hypotheses
in Cs that contribute to the upper envelope. These
errors are projected onto intervals of ?, as shown in
the lower part of Figure 1, so that Eq. (2) can be
readily solved.
2.2 Incorporation of Line Optimisation into
MERT
The previous algorithm finds the upper envelope
along a particular direction in parameter space over
117
a hypothesis set Cs. The line optimisation algorithm
is then embedded within a general optimisation pro-
cedure. A common approach to MERT is to select
the directions using Powell?s method (Press et al,
2002). A line optimisation is performed on each co-
ordinate axis. The axis giving the largest decrease
in error is replaced with a vector between the initial
parameters and the optimised parameters. Powell?s
method halts when there is no decrease in error.
Instead of using Powell?s method, the Downhill
Simplex algorithm (Press et al, 2002) can be used
to explore the criterion in Eq. (2). This is done by
defining a simplex in parameter space. Directions
where the error count decreases can be identified by
considering the change in error count at the points
of the simplex. This has been applied to parameter
searching over k-best lists (Zens et al, 2007).
Both Powell?s method and the Downhill Simplex
algorithms are approaches based on heuristics to se-
lect lines ?M1 + ?dM1 . It is difficult to find theoret-
ically sound reasons why one approach is superior.
Therefore Cer et al (2008) instead choose the di-
rection vectors dM1 at random. They report that this
method can find parameters that are as good as the
parameters produced by more complex algorithms.
2.3 Lattice Line Optimisation
Macherey et al (2008) describe a procedure for con-
ducting line optimisation directly over a word lattice
encoding the hypotheses in Cs. Each lattice edge is
labelled with a word e and has a weight defined by
the vector of word specific feature function values
hM1 (e, f) so that the weight of a path in the lattice
is found by summing over the word specific feature
function values on that path. Given a line through
parameter space, the goal is to extract from a lattice
its upper envelope and the associated hypotheses.
Their algorithm proceeds node by node through
the lattice. Suppose that for a state q the upper enve-
lope is known for all the partial hypotheses on all
paths leading to q. The upper envelope defines a
set of functions {`e?1(?), ..., `e?N (?)} over the partial
hypotheses e?n. Two operations propagate the upper
envelope to other lattice nodes.
We refer to the first operation as the ?extend? op-
eration. Consider a single edge from state q to state
q?. This edge defines a linear function associated
with a single word `e(?). A path following this edge
transforms all the partial hypotheses leading to q by
concatenating the word e. The upper envelope as-
sociated with the edge from q to q? is changed by
adding `e(?) to the set of linear functions. The in-
tersection points are not changed by this operation.
The second operation is a union. Suppose q?
has another incoming edge from a state q?? where
q 6= q??. There are now two upper envelopes rep-
resenting two sets of linear functions. The first up-
per envelope is associated with the paths from the
initial state to state q? via the state q. Similarly the
second upper envelope is associated with paths from
the initial state to state q? via the state q??. The upper
envelope that is associated with all paths from the
initial state to state q? via both q and q?? is the union
of the two sets of linear functions. This union is no
longer a compact representation of the upper enve-
lope as there may be functions which never achieve
a maximum for any value of ?. The SweepLine al-
gorithm (Bentley and Ottmann, 1979) is applied to
the union to discard redundant linear functions and
their associated hypotheses (Macherey et al, 2008).
The union and extend operations are applied to
states in topological order until the final state is
reached. The upper envelope computed at the final
state compactly encodes all the hypotheses that max-
imise Eq. (1) along the line ?M1 + ?dM1 . Macherey?s
theorem (Macherey et al, 2008) states that an upper
bound for the number of linear functions in the up-
per envelope at the final state is equal to the number
of edges in the lattice.
2.4 Line Optimisation using WFSTs
Formally, a weighted finite-state transducer (WFST)
T = (?,?, Q, I, F,E, ?, ?) over a semiring
(K,?,?, 0?, 1?) is defined by an input alphabet ?, an
output alphabet ?, a set of states Q, a set of initial
states I ? Q, a set of final states F ? Q, a set
of weighted transitions E, an initial state weight as-
signment ? : I ? K, and a final state weight assign-
ment ? : F ? K (Mohri et al, 2008). The weighted
transitions of T form the setE ? Q?????K?Q,
where each transition includes a source state fromQ,
input symbol from ?, output symbol from ?, cost
from the weight set K, and target state from Q.
For each state q ? Q, let E[q] denote the set of
edges leaving state q. For each transition e ? E[q],
let p[e] denote its source state, n[e] its target state,
118
and w[e] its weight. Let pi = e1 ? ? ? eK denote a
path in T from state p[e1] to state n[eK ], so that
n[ek?1] = p[ek] for k = 2, . . . ,K. The weight as-
sociated by T to path pi is the generalised product ?
of the weights of the individual transitions:
w[pi] =
K?
k=1
w[ek] = w[e1]? ? ? ? ? w[eK ] (5)
If P(q) denotes the set of all paths in T start-
ing from an initial state in I and ending in state q,
then the shortest distance d[q] is defined as the gen-
eralised sum ? of the weights of all paths leading to
q (Mohri, 2002):
d[q] = ?pi?P(q)w[pi] (6)
For some semirings, such as the tropical semir-
ing, the shortest distance is the weight of the short-
est path. For other semirings, the shortest distance
is associated with multiple paths (Mohri, 2002); for
these semirings there are shortest distances but need
not any be shortest paths. That will be the case in
what follows. However, the shortest distance algo-
rithms rely only on general properties of semirings,
and once the semiring is specified, the general short-
est distance algorithms can be directly employed.
Sokolov and Yvon (2011) define the MERT
semiring based on operations described in the pre-
vious section. The extend operation is used for the
generalised product ?. The union operation fol-
lowed by an application of the SweepLine algorithm
becomes the generalised sum ?. The word lattice
is then transformed for an initial parameter ?M1 and
direction dM1 . The weight of edge is mapped from
a word specific feature function hM1 (e, f) to a word
specific linear function `e(?). The weight of each
path is the generalised product ? of the word spe-
cific feature linear functions. The upper envelope is
the shortest distance of all the paths in the WFST.
3 The Tropical Polynomial Semiring
In this section we introduce the tropical polynomial
semiring (Speyer and Sturmfels, 2009) as a replace-
ment for the MERT semiring (Sokolov and Yvon,
2011). We then provide a full description and a
worked example of our MERT algorithm.
3.1 Tropical Polynomials
A polynomial is a linear combination of a finite
number of non-zero monomials. A monomial con-
sists of a real valued coefficient multiplied by one or
more variables, and these variables may have expo-
nents that are non-negative integers. In this section
we limit ourselves to a description of a polynomial
in a single variable. A polynomial function is de-
fined by evaluating a polynomial:
f(?) = an?n + an?1?n?1 + ? ? ?+ a2?2 + a1?+ a0
A useful property of these polynomials is that they
form a ring2 (Cox et al, 2007) and therefore are can-
didates for use as weights in WFSTs.
Speyer and Sturmfels (2009) apply the defini-
tion of a classical polynomial to the formulation of
a tropical polynomial. The tropical semiring uses
summation for the generalised product ? and a min
operation for the generalised sum ?. In this form,
let ? be a variable that represents an element in the
tropical semiring weight set R ? {??,+?}. We
can write a monomial of ? raised to an integer expo-
nent as
?i = ? ? ? ? ? ? ?? ?? ?
i
where i is a non-negative integer. The monomial
can also have a constant coefficient: a? ?i, a ? R.
We can define a function that evaluates a tropical
monomial for a particular value of ?. For example,
the tropical monomial a? ?i is evaluated as:
f(?) = a? ?i = a+ i?
This shows that a tropical monomial is a linear
function with the coefficient a as its y-intercept and
the integer exponent i as its gradient. A tropical
polynomial is the generalised sum of tropical mono-
mials where the generalised sum is evaluated using
the min operation. For example:
f(?) = (a? ?i)? (b? ?j) = min(a+ i?, b+ j?)
Evaluating tropical polynomials in classical arith-
metic gives the minimum of a finite collection of
linear functions.
Tropical polynomials can also be multiplied by a
monomial to form another tropical polynomial. For
example:
f(?) = [(a? ?i)? (b? ?j)]? (c? ?k)
= [(a+ c)? ?i+k]? [(b+ c)? ?j+k]
= min((a+ c) + (i+ k)?, (b+ c) + (j + k)?)
2A ring is a semiring that includes negation.
119
Our re-formulation of Eq. (4) negates the feature
function weights and replaces the argmax by an
argmin. This allows us to keep the usual formu-
lation of tropical polynomials in terms of the min
operation when converting Eq. (4) to a tropical rep-
resentation. What remains to be addressed is the role
of integer exponents in the tropical polynomial.
3.2 Integer Realisations for Tropical
Monomials
In the previous section we noted that the function
defined by the upper envelope in Eq. (4) is simi-
lar to the function represented by a tropical poly-
nomial. A significant difference is that the formal
definition of a polynomial only allows integer expo-
nents, whereas the gradients in Eq. (4) are real num-
bers. The upper envelope therefore encodes a larger
set of model parameters than a tropical polynomial.
To create an equivalence between the upper enve-
lope and tropical polynomials we can approximate
the linear functions {`e(?) = a(e, f s)+? ? b(e, f s)}
that compose segments of the upper envelope. We
define a?(e, f s) = [a(e, f s) ? 10n]int and b?(e, f s) =
[b(e, f s)?10n]int where [x]int denotes the integer part
of x. The approximation to `e(?) is:
`e(?) ? ?`e(?) =
a?(e, f s)
10n + ? ?
b?(e, f s)
10n (7)
The result of this operation is to approximate
the y-intercept and gradient of `e(?) to n decimal
places. We can now represent the linear function
?`e(?) as the tropical monomial?a?(e, fs)???b?(e,fs).
Note that a?(e, fs) and b?(e, fs) are negated since trop-
ical polynomials define the lower envelope as op-
posed to the upper envelope defined by Eq. (4).
The linear function represented by the tropical
monomial is a scaled version of `e(?), but the up-
per envelope is unchanged (to the accuracy allowed
by n). If for a particular value of ?, `ei(?) > `ej (?),
then ?`ei(?) > ?`ej (?). Similarly, the boundary
points are unchanged: if `ei(?) = `ej (?), then
?`ei(?) = ?`ej (?). Setting n to a very large value re-
moves numerical differences between the upper en-
velope and the tropical polynomial representation,
as shown by the identical results in Table 1.
Using a scaled version of `e(?) as the basis for a
tropical monomial may cause negative exponents to
be created. Following Speyer and Sturmfels (2009),
?
f(?)
0
a? ?i
(a? ?i)? (b? ?j)? (c? ?k)
b? ?j
c? ?k
Figure 2: Redundant terms in a tropical polynomial. In
this case (a??i)?(b??j)?(c??k) = (a??i)?(c??k).
we widen the definition of a tropical polynomial to
allow for these negative exponents.
3.3 Canonical Form of a Tropical Polynomial
We noted in Section 2.1 that linear functions induced
by some hypotheses do not contribute to the upper
envelope and can be discarded. Terms in a tropi-
cal polynomial can have similar behaviour. Figure
2 plots the lines associated with the three terms of
the example polynomial function f(?) = (a??i)?
(b??j)?(c??k). We note that the piecewise linear
function can also be described with the polynomial
f(?) = (a??i)?(c??k). The latter representation
is simpler but equivalent.
Having multiple representations of the same poly-
nomial causes problems when implementing the
shortest distance algorithm defined by Mohri (2002).
This algorithm performs an equality test between
values in the semiring used to weight the WFST. The
behaviour of the equality test is ambiguous when
there are multiple polynomial representations of the
same piecewise linear function. We therefore re-
quire a canonical form of a tropical polynomial so
that a single polynomial represents a single function.
We define the canonical form of a tropical polyno-
mial to be the tropical polynomial that contains only
the monomial terms necessary to describe the piece-
wise linear function it represents.
We remove redundant terms from a tropical poly-
nomial after computing the generalised sum. For a
tropical polynomial of one variable we can take ad-
vantage of the equivalence with Lattice MERT and
compute the canonical form using the SweepLine al-
gorithm (Bentley and Ottmann, 1979). Each term
120
corresponds to a linear function; linear functions
that do not contribute to the upper envelope are dis-
carded. Only monomials which correspond to the
remaining linear functions are kept in the canonical
form. The canonical form of a tropical polynomial
thus corresponds to a unique and minimal represen-
tation of the upper envelope.
3.4 Relationship to the Tropical Semiring
Tropical monomial weights can be transformed into
regular tropical weights by evaluating the tropical
monomial for a specific value of ?. For example, a
tropical polynomial evaluated at ? = 1 corresponds
to the tropical weight:
f(1) = ?a?(e, fs)? 1?b?(e,fs) = ?a?(e, fs)? b?(e, fs)
Each monomial term in the tropical polynomial
shortest distance represents a linear function. The
intersection points of these linear functions define
intervals of ? (as in Fig. 1). This suggests an alter-
nate explanation for what the shortest distance com-
puted using the tropical polynomial semiring rep-
resents. Conceptually, there is a continuum of lat-
tices which have identical edges and vertices but
with varying, real-valued edge weights determined
by values of ? ? R, so that each lattice in the contin-
uum is indexed by ?. The tropical polynomial short-
est distance agrees with the shortest distance through
each lattice in the continuum.
Our alternate explanation is consistent with the
Theorem of Macherey (Section 2.3), as there could
never be more paths than edges in the lattice. There-
fore the upper bound for the number of monomial
terms in the tropical polynomial shortest distance is
the number of edges in the input lattice.
We can use the mapping to the tropical semiring
to compute the error surface. Let us assume we have
n + 1 intervals separated by n interval boundaries.
We use the midpoint of each interval to transform the
lattice of tropical monomial weights into a lattice of
tropical weights. The sequence of words that label
the shortest path through the transformed lattice is
the MAP hypothesis for the interval. The shortest
path can be extracted using the WFST shortest path
algorithm (Mohri and Riley, 2002). As a technical
matter, the midpoints of the first interval [??, ?1)
and last interval [?n,?) are not defined. We there-
fore evaluate the tropical polynomial at ? = ?1 ? 1
and ? = ?n + 1 to find the MAP hypothesis in the
first and last intervals, respectively.
3.5 The TGMERT Algorithm
We now describe an alternative algorithm to Lat-
tice MERT that is formulated using the tropical
polynomial shortest distance in one variable. We
call the algorithm TGMERT, for Tropical Geome-
try MERT. As input to this procedure we use a word
lattice weighted with word specific feature functions
hM1 (e, f), a starting point ?M1 , and a direction dM1 in
parameter space.
1. Convert the word specific feature functions
hM1 (e, f) to a linear function `e(?) using ?M1
and dM1 , as in Eq. (3).
2. Convert `e(?) to ?`e(?) by approximating y-
intercepts and gradients to n decimal places, as
in Eq. (7).
3. Convert ?`e(?) in Eq. (7) to the tropical mono-
mial ?a?(e, fs)? ??b?(e,fs).
4. Compute the WFST shortest distance to the exit
states (Mohri, 2002) with generalised sum ?
and generalised product ? defined by the trop-
ical polynomial semiring. The resulting trop-
ical polynomial represents the upper envelope
of the lattice.
5. Compute the intersection points of the linear
functions corresponding to the monomial terms
of the tropical polynomial shortest distance.
These intersection points define intervals of ?
in which the MAP hypothesis does not change.
6. Using the midpoint of each interval convert the
tropical monomial?a?(e, fs)???b?(e,fs) to a reg-
ular tropical weight. Find the MAP hypothesis
for this interval by extracting the shortest path
using the WFST shortest path algorithm (Mohri
and Riley, 2002).
3.6 TGMERT Worked Example
This section presents a worked example showing
how we can use the TGMERT algorithm to compute
the upper envelope of a lattice. We start with a three
state lattice with a two dimensional feature vector
shown in the upper part of Figure 3.
We want to optimise the parameters along a line
in two-dimensional feature space. Suppose the ini-
tial parameters are ?21 = [0.7, 0.4] and the direction
121
0 1 2
z/[?0.2, 0.7]?
x/[?1.4, 0.3]?
y/[?0.9,?0.8]?
z/[?0.2,?0.6]?
0 1 2
z/?14? ??29
x/86? ?27
y/95? ?67
z/38? ?36
Figure 3: The upper part is a translation lattice with 2-
dimensional log feature vector weights hM1 (e, f) where
M = 2. The lower part is the lattice from the upper part
with weights transformed into tropical monomials.
is d21 = [0.3, 0.5]. Step 1 of the TGMERT algorithm
(Section 3.5) maps each edge weight to a word spe-
cific linear function. For example, the weight of the
edge labelled ?x? between states 0 and 1 is trans-
formed as follows:
`e(?) =
2?
m=1
?mhM1 (e, f)
? ?? ?
a(e,f)
+?
2?
m=1
dmhM1 (e,fs)
? ?? ?
b(e,f)
= 0.7 ? ?1.4 + 0.4 ? 0.3? ?? ?
a(e,f)
+? ? 0.3 ? ?1.4 + 0.5 ? 0.3? ?? ?
b(e,f)
= ?0.86? 0.27?
Step 2 of the TGMERT algorithm converts the
word specific linear functions into tropical mono-
mial weights. Since all y-intercepts and gradients
have a precision of two decimal places, we scale the
linear functions `e(?) by 102 and negate them to cre-
ate tropical monomials (Step 3). The edge labelled
?x? now has the monomial weight of 86? ?27. The
transformed lattice with weights mapped to the trop-
ical polynomial semiring is shown in the lower part
of Figure 3.
We can now compute the shortest distance
(Mohri, 2002) from the transformed example lattice
with tropical monomial weights. There are three
unique paths through the lattice corresponding to
three distinct hypotheses. The weights associated
with these hypotheses are:
?14? ??29 ? 38? ?36 = 24? ?7 z z
86? ?27 ? 38? ?36 = 122? ?63 x z
95? ?67 ? 38? ?36 = 133? ?103 y z
0 1 2
z/-2.4
x/75.2
y/68.2
z/23.6
0 1 2
z/55.6
x/21.2
y/-65.8
z/-48.4
Figure 4: The lattice in the lower part of Figure 3 trans-
formed to regular tropical weights: ? = ?0.4 (top) and
? = ?1.4 (bottom).
The shortest distance from initial to final state is
the generalised sum of the path weights: (24??7)?
(133? ?103). The monomial term 122? ?63 corre-
sponding to ?x z? can be dropped because it is not
part of the canonical form of the polynomial (Sec-
tion 3.3). The shortest distance to the exit state can
be represented as the minimum of two linear func-
tions: min(24 + 7?, 133 + 103?).
We now wish to find the hypotheses that define
the error surface by performing Steps 5 and 6 of the
TGMERT algorithm. These two linear functions de-
fine two intervals of ?. The linear functions intersect
at ? ? ?1.4; at this value of ? the MAP hypothesis
changes. Two lattices with regular tropical weights
are created using ? = ?0.4 and ? = ?2.4. These
are shown in Figure 4. For the lattice shown in the
upper part the value for the edge labelled ?x? is com-
puted as 86??0.427 = 86 + 0.4 ? 27 = 75.2.
When ? = ?0.4 the lattice in the upper part in
Figure 4 shows that the shortest path is associated
with the hypothesis ?z z?, which is the MAP hy-
pothesis for the range ? < 1.4. The lattice in the
lower part of Figure 4 shows that when ? = ?2.4
the shortest path is associated with the hypothesis
?y z?, which is the MAP hypothesis when ? > 1.4.
3.7 TGMERT Implementation
TGMERT is implemented using the OpenFst Toolkit
(Allauzen et al, 2007). A weight class is added
for tropical polynomials which maintains them in
canonical form. The ? and ? operations are im-
plemented for piece-wise linear functions, with the
SweepLine algorithm included as discussed.
122
Iteration Arabic-to-English
MERT LMERT TGMERT
Tune Test Tune Test Tune Test
1 36.2 36.2 36.242.1 40.9 39.7 38.9 39.7 38.9
2 42.0 44.5 44.545.1 43.2 45.8 44.3 45.8 44.3
3 44.545.5 44.1
4 45.645.7 44.0
Iteration Chinese-to-English
MERT LMERT TGMERT
Tune Test Tune Test Tune Test
1 19.5 19.5 19.525.3 16.7 29.3 22.6 29.3 22.6
2 16.4 22.5 22.518.9 23.9 31.4 32.1 31.4 32.1
3 23.6 31.6 31.628.2 29.1 32.2 32.5 32.2 32.5
4 29.2 32.2 32.231.3 31.5 32.2 32.5 32.2 32.5
5 31.331.8 32.1
6 32.132.4 32.3
7 32.432.4 32.3
Table 1: GALE AR?EN and ZH?EN BLEU scores
by MERT iteration. BLEU scores at the initial and final
points of each iteration are shown for the Tune sets.
4 Experiments
We compare feature weight optimisation using k-
best MERT (Och, 2003), lattice MERT (Macherey
et al, 2008), and tropical geometry MERT. We refer
to these as MERT, LMERT, and TGMERT, resp.
We investigate MERT performance in the context
of the Arabic-to-English GALE P4 and Chinese-
to-English GALE P3 evaluations3. For Arabic-to-
English translation, word alignments are generated
over around 9M sentences of GALE P4 parallel text.
Following de Gispert et al (2010b), word align-
ments for Chinese-to-English translation are trained
from a subset of 2M sentences of GALE P3 paral-
lel text. Hierarchical rules are extracted from align-
ments using the constraints described in (Chiang,
2007) with additional count and pattern filters (Igle-
3See http://projects.ldc.upenn.edu/gale/data/catalog.html
sias et al, 2009b). We use a hierarchical phrase-
based decoder (Iglesias et al, 2009a; de Gispert et
al., 2010a) which directly generates word lattices
from recursive translation networks without any in-
termediate hypergraph representation (Iglesias et al,
2011). The LMERT and TGMERT optimisation al-
gorithms are particularly suitable for this realisation
of hiero in that the lattice representation avoids the
need to use the hypergraph formulation of MERT
given by Kumar et al (2009).
MERT optimises the weights of the following fea-
tures: target language model, source-to-target and
target-to-source translation models, word and rule
penalties, number of usages of the glue rule, word
deletion scale factor, source-to-target and target-to-
source lexical models, and three count-based fea-
tures that track the frequency of rules in the parallel
data (Bender et al, 2007). In both Arabic-to-English
and Chinese-to-English experiments all MERT im-
plementations start from a flat feature weight initial-
ization. At each iteration new lattices and k-best lists
are generated from the best parameters at the previ-
ous iteration, and each subsequent iteration includes
100 hypotheses from the previous iteration. For
Arabic-to-English we consider an additional twenty
random starting parameters at every iteration. All
translation scores are reported for the IBM imple-
mentation of BLEU using case-insensitive match-
ing. We report BLEU scores for the Tune set at the
start and end of each iteration.
The results for Arabic-to-English and Chinese-
to-English are shown in Table 1. Both TGMERT
and LMERT converge to a small gain over MERT
in fewer iterations, consistent with previous re-
ports (Macherey et al, 2008).
5 Discussion
We have described a lattice-based line optimisation
algorithm which can be incorporated into MERT
for parameter tuning of SMT systems and systems
based on log-linear models. Our approach recasts
the optimisation procedure used in MERT in terms
of Tropical Geometry; given this formulation imple-
mentation is relatively straightforward using stan-
dard WFST operations and algorithms.
123
References
C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and
M. Mohri. 2007. OpenFst: A general and efficient
weighted finite-state transducer library. In Proceed-
ings of the Ninth International Conference on Imple-
mentation and Application of Automata, pages 11?23.
O. Bender, E. Matusov, S. Hahn, S. Hasan, S. Khadivi,
and H. Ney. 2007. The RWTH Arabic-to-English spo-
ken language translation system. In Automatic Speech
Recognition Understanding, pages 396 ?401.
J.L. Bentley and T.A. Ottmann. 1979. Algorithms for
reporting and counting geometric intersections. Com-
puters, IEEE Transactions on, C-28(9):643 ?647.
Daniel Cer, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Regularization and search for minimum
error rate training. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 26?34.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
David A. Cox, John Little, and Donal O?Shea. 2007.
Ideals, Varieties, and Algorithms: An Introduction to
Computational Algebraic Geometry and Commutative
Algebra, 3/e (Undergraduate Texts in Mathematics).
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010a. Hier-
archical phrase-based translation with weighted finite-
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3):505?533.
Adria` de Gispert, Juan Pino, and William Byrne. 2010b.
Hierarchical phrase-based translation grammars ex-
tracted from alignment posterior probabilities. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 545?554.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7?12, July.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-based
translation with weighted finite state transducers. In
Proceedings of HLT: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, pages
380?388.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adria`
de Gispert, and Michael Riley. 2011. Hierarchical
phrase-based translation representations. In Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 1373?1383. As-
sociation for Computational Linguistics.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 725?734.
Mehryar Mohri and Michael Riley. 2002. An efficient
algorithm for the n-best-strings problem. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing 2002.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Ri-
ley. 2008. Speech recognition with weighted finite-
state transducers. Handbook on Speech Processing
and Speech Communication.
Mehryar Mohri. 2002. Semiring frameworks and algo-
rithms for shortest-distance problems. J. Autom. Lang.
Comb., 7(3):321?350.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
K. A. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318.
W. H. Press, W. T. Vetterling, S. A. Teukolsky, and B. P.
Flannery. 2002. Numerical Recipes in C++: the art
of scientific computing. Cambridge University Press.
J. Richter-Gebert, B. Sturmfels, and T. Theobald. 2005.
First steps in tropical geometry. In Idempotent mathe-
matics and mathematical physics.
Artem Sokolov and Franc?ois Yvon. 2011. Minimum er-
ror rate training semiring. In Proceedings of the Euro-
pean Association for Machine Translation.
124
David Speyer and Bernd Sturmfels. 2009. Tropical
mathematics. Mathematics Magazine.
Aurelien Waite, Graeme Blackwood, and William Byrne.
2011. Lattice-based minimum error rate training using
weighted finite-state transducers with tropical polyno-
mial weights. Technical report, Department of Engi-
neering, University of Cambridge.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 524?532.
125
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 200?205,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The University of Cambridge Russian-English System at WMT13
Juan Pino Aurelien Waite Tong Xiao
Adria` de Gispert Federico Flego William Byrne
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, UK
{jmp84,aaw35,tx212,ad465,ff257,wjb31}@eng.cam.ac.uk
Abstract
This paper describes the University of
Cambridge submission to the Eighth
Workshop on Statistical Machine Transla-
tion. We report results for the Russian-
English translation task. We use mul-
tiple segmentations for the Russian in-
put language. We employ the Hadoop
framework to extract rules. The decoder
is HiFST, a hierarchical phrase-based de-
coder implemented using weighted finite-
state transducers. Lattices are rescored
with a higher order language model and
minimum Bayes-risk objective.
1 Introduction
This paper describes the University of Cam-
bridge system submission to the ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion (WMT13). Our translation system is HiFST
(Iglesias et al, 2009), a hierarchical phrase-based
decoder that generates translation lattices directly.
Decoding is guided by a CYK parser based on a
synchronous context-free grammar induced from
automatic word alignments (Chiang, 2007). The
decoder is implemented with Weighted Finite
State Transducers (WFSTs) using standard op-
erations available in the OpenFst libraries (Al-
lauzen et al, 2007). The use of WFSTs allows
fast and efficient exploration of a vast translation
search space, avoiding search errors in decoding.
It also allows better integration with other steps
in our translation pipeline such as 5-gram lan-
guage model (LM) rescoring and lattice minimum
Bayes-risk (LMBR) decoding (Blackwood, 2010).
We participate in the Russian-English transla-
tion shared task in the Russian-English direction.
This is the first time we train and evaluate a sys-
tem on this language pair. This paper describes the
development of the system.
The paper is organised as follows. Section 2
describes each step in the development of our sys-
tem for submission, from pre-processing to post-
processing and Section 3 presents and discusses
results.
2 System Development
2.1 Pre-processing
We use all the Russian-English parallel data avail-
able in the constraint track. We filter out non
Russian-English sentence pairs with the language-
detection library.2 A sentence pair is filtered out if
the language detector detects a different language
with probability more than 0.999995 in either the
source or the target. This discards 78543 sen-
tence pairs. In addition, sentence pairs where the
source sentence has no Russian character, defined
by the Perl regular expression [\x0400-\x04ff],
are discarded. This further discards 19000 sen-
tence pairs.
The Russian side of the parallel corpus is to-
kenised with the Stanford CoreNLP toolkit.3 The
Stanford CoreNLP tokenised text is additionally
segmented with Morfessor (Creutz and Lagus,
2007) and with the TreeTagger (Schmid, 1995).
In the latter case, we replace each token by its
stem followed by its part-of-speech. This of-
fers various segmentations that can be taken ad-
vantage of in hypothesis combination: CoreNLP,
CoreNLP+Morfessor and CoreNLP+TreeTagger.
The English side of the parallel corpus is tokenised
with a standard in-house tokeniser. Both sides of
the parallel corpus are then lowercased, so mixed
case is restored in post-processing.
Corpus statistics after filtering and for various
segmentations are summarised in Table 1.
2http://code.google.com/p/language-detection/
3http://nlp.stanford.edu/software/corenlp.shtml
200
Lang Segmentation # Tokens # Types
RU CoreNLP 47.4M 1.2M
RU Morfessor 50.0M 0.4M
RU TreeTagger 47.4M 1.5M
EN Cambridge 50.4M 0.7M
Table 1: Russian-English parallel corpus statistics
for various segmentations.
2.2 Alignments
Parallel data is aligned using the MTTK toolkit
(Deng and Byrne, 2008). We train a word-
to-phrase HMM model with a maximum phrase
length of 4 in both source-to-target and target-to-
source directions. The final alignments are ob-
tained by taking the union of alignments obtained
in both directions.
2.3 Rule Extraction and Retrieval
A synchronous context-free grammar (Chiang,
2007) is extracted from the alignments. The con-
straints are set as in the original publication with
the following exceptions:
? phrase-based rule maximum number of
source words: 9
? maximum number of source element (termi-
nal or nonterminal): 5
? maximum span for nonterminals: 10
Maximum likelihood estimates for the transla-
tion probabilities are computed using MapReduce.
We use a custom Hadoop-based toolkit which im-
plements method 3 of Dyer et al (2008). Once
computed, the model parameters are stored on disk
in the HFile format (Pino et al, 2012) for fast
querying. Rule extraction and feature computa-
tion takes about 2h30. The HFile format requires
data to be stored in a key-value structure. For the
key, we use shared source side of many rules. The
value is a list of tuples containing the possible tar-
gets for the source key and the associated param-
eters of the full rule. The query set of keys for
the test set is all possible source phrases (includ-
ing nonterminals) found in the test set.
During HFile querying we add other features.
These include IBM Model 1 (Brown et al, 1993)
lexical probabilities. Loading these models in
memory doesn?t fit well with the MapReduce
model so lexical features are computed for each
test set rather than for the entire parallel corpus.
The model parameters are stored in a client-server
based architecture. The client process computes
the probability of the rule by querying the server
process for the Model 1 parameters. The server
process stores the model parameters completely
in memory so that parameters are served quickly.
This architecture allows for many low-memory
client processes across many machines.
2.4 Language Model
We used the KenLM toolkit (Heafield et al, 2013)
to estimate separate 4-gram LMs with Kneser-Ney
smoothing (Kneser and Ney, 1995), for each of the
corpora listed in Tables 2 (self-explanatory abbre-
viations). The component models were then in-
terpolated with the SRILM toolkit (Stolcke, 2002)
to form a single LM for use in first-pass trans-
lation decoding. The interpolation weights were
optimised for perplexity on the news-test2008,
newstest2009 and newssyscomb2009 development
sets. The weights reflect both the size of the com-
ponent models and the genre of the corpus the
component models are trained on, e.g. weights are
larger for larger corpora in the news genre.
Corpus # Tokens
EU + NC + UN + CzEng + Yx 652.5M
Giga + CC + Wiki 654.1M
News Crawl 1594.3M
afp 874.1M
apw 1429.3M
cna + wpb 66.4M
ltw 326.5M
nyt 1744.3M
xin 425.3M
Total 7766.9M
Table 2: Statistics for English monolingual cor-
pora.
2.5 Decoding
For translation, we use the HiFST decoder (Igle-
sias et al, 2009). HiFST is a hierarchical decoder
that builds target word lattices guided by a prob-
abilistic synchronous context-free grammar. As-
suming N to be the set of non-terminals and T the
set of terminals or words, then we can define the
grammar as a set R = {R} of rules R : N ?
??,?? / p, where N ? N, ?, ? ? {N ?T}+ and p
the rule score.
201
HiFST translates in three steps. The first step
is a variant of the CYK algorithm (Chappelier and
Rajman, 1998), in which we apply hypothesis re-
combination without pruning. Only the source
language sentence is parsed using the correspond-
ing source-side context-free grammar with rules
N ? ?. Each cell in the CYK grid is specified
by a non-terminal symbol and position: (N, x, y),
spanning sx+y?1x on the source sentence s1...sJ .
For the second step, we use a recursive algo-
rithm to construct word lattices with all possi-
ble translations produced by the hierarchical rules.
Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In
each cell (N, x, y) of the CYK grid, we build a
target language word lattice L(N, x, y) containing
every translation of sx+y?1x from every derivation
headed by N . For efficiency, this lattice can use
pointers to lattices on other cells of the grid.
In the third step, we apply the word-based LM
via standard WFST composition with failure tran-
sitions, and perform likelihood-based pruning (Al-
lauzen et al, 2007) based on the combined trans-
lation and LM scores.
We are using shallow-1 hierarchical gram-
mars (de Gispert et al, 2010) in our experiments.
This model is constrained enough that the decoder
can build exact search spaces, i.e. there is no prun-
ing in search that may lead to spurious undergen-
eration errors.
2.6 Features and Parameter Optimisation
We use the following standard features:
? language model
? source-to-target and target-to-source transla-
tion scores
? source-to-target and target-to-source lexical
scores
? target word count
? rule count
? glue rule count
? deletion rule count (each source unigram, ex-
cept for OOVs, is allowed to be deleted)
? binary feature indicating whether a rule is ex-
tracted once, twice or more than twice (Ben-
der et al, 2007)
No alignment information is used when com-
puting lexical scores as done in Equation (4) in
(Koehn et al, 2005). Instead, the source-to-target
lexical score is computed in Equation 1:
s(ru, en) = 1(E + 1)R
R?
r=1
E?
e=0
pM1(ene|rur)
(1)
where ru are the terminals in the Russian side of
a rule, en are the terminals in the English side of
a rule, including the null word, R is the number
of Russian terminals, E is the number of English
terminals and pM1 is the IBM Model 1 probability.
In addition to these standard features, we also
use provenance features (Chiang et al, 2011). The
parallel data is divided into four subcorpora: the
Common Crawl (CC) corpus, the News Commen-
tary (NC) corpus, the Yandex (Yx) corpus and the
Wiki Headlines (Wiki) corpus. For each of these
subcorpora, source-to-target and target-to-source
translation and lexical scores are computed. This
requires computing IBM Model 1 for each sub-
corpus. In total, there are 28 features, 12 standard
features and 16 provenance features.
When retrieving relevant rules for a particular
test set, various thresholds are applied, such as
number of targets per source or translation prob-
ability cutoffs. Thresholds involving source-to-
target translation scores are applied separately for
each provenance and the union of all surviving
rules for each provenance is kept. This strategy
gives slight gains over using thresholds only for
the general translation table.
We use an implementation of lattice minimum
error rate training (Macherey et al, 2008) to op-
timise under the BLEU score (Papineni et al,
2001) the feature weights with respect to the odd
sentences of the newstest2012 development set
(newstest2012.tune). The weights obtained match
our expectation, for example, the source-to-target
translation feature weight is higher for the NC cor-
pus than for other corpora since we are translating
news.
2.7 Lattice Rescoring
The HiFST decoder is set to directly generate
large translation lattices encoding many alterna-
tive translation hypotheses. These first-pass lat-
tices are rescored with second-pass higher-order
LMs prior to LMBR.
202
2.7.1 5-gram LM Lattice Rescoring
We build a sentence-specific, zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram LMs esti-
mated over the data described in section 2.4. Lat-
tices obtained by first-pass decoding are rescored
with this 5-gram LM (Blackwood, 2010).
2.7.2 LMBR Decoding
Minimum Bayes-risk decoding (Kumar and
Byrne, 2004) over the full evidence space of the 5-
gram rescored lattices is applied to select the trans-
lation hypothesis that maximises the conditional
expected gain under the linearised sentence-level
BLEU score (Tromble et al, 2008; Blackwood,
2010). The unigram precision p and average re-
call ratio r are set as described in Tromble et al
(2008) using the newstest2012.tune development
set.
2.8 Hypothesis Combination
LMBR decoding (Tromble et al, 2008) can also be
used as an effective framework for multiple lattice
combination (Blackwood, 2010). We used LMBR
to combine translation lattices produced by sys-
tems trained on alternative segmentations.
2.9 Post-processing
Training data is lowercased, so we apply true-
casing as post-processing. We used the disam-
big tool provided by the SRILM toolkit (Stolcke,
2002). The word mapping model which contains
the probability of mapping a lower-cased word
to its mixed-cased form is trained on all avail-
able data. A Kneser-Ney smoothed 4-gram lan-
guage model is also trained on the following cor-
pora: NC, News Crawl, Wiki, afp, apw, cna, ltw,
nyt, wpb, xin, giga. In addition, several rules are
manually designed to improve upon the output of
the disambig tool. First, casing information from
pass-through translation rules (for OOV source
words) is used to modify the casing of the output.
For example, this allows us to get the correct cas-
ing for the word Bundesrechnungshof. Other rules
are post-editing rules which force some words
to their upper-case forms, such as euro ? Euro.
Post-editing rules are developed based on high-
frequency errors on the newstest2012.tune devel-
opment set. These rules give an improvement of
0.2 mixed-cased NIST BLEU on the development
set.
Finally, the output is detokenised before sub-
mission and Cyrillic characters are transliterated.
We assume for human judgment purposes that it
is better to have a non English word in Latin al-
phabet than in Cyrillic (e.g. uprazdnyayushchie);
sometimes, transliteration can also give a correct
output (e.g. Movember), especially in the case of
proper nouns.
3 Results and Discussion
Results are reported in Table 3. We use the inter-
nationalisation switch for the NIST BLEU scor-
ing script in order to properly lowercase the hy-
pothesis and the reference. This introduces a
slight discrepancy with official results going into
the English language. The newstest2012.test de-
velopment set consists of even sentences from
newstest2012. We observe that the CoreNLP
system (A) outperforms the other two systems.
The CoreNLP+Morfessor system (B) has a much
smaller vocabulary but the model size is compa-
rable to the system A?s model size. Translation
did not benefit from source side morphological de-
composition. We also observe that the gain from
LMBR hypothesis combination (A+B+C) is mini-
mal. Unlike other language pairs, such as Arabic-
English (de Gispert et al, 2009), we have not yet
found any great advantage in multiple morpho-
logical decomposition or preprocessing analyses
of the source text. 5-gram and LMBR rescoring
give consistent improvements. 5-gram rescoring
improvements are very modest, probably because
the first pass 4-gram model is trained on the same
data. As noted, hypothesis combination using the
various segmentations gives consistent but modest
gains over each individual system.
Two systems were submitted to the evalua-
tion. System A+B+C achieved a mixed-cased
NIST BLEU score of 24.6, which was the top
score achieved under this measure. System A sys-
tem achieved a mixed-cased NIST BLEU score of
24.5, which was the second highest score.
4 Summary
We have successfully trained a Russian-English
system for the first time. Lessons learned include
that simple tokenisation is enough to process the
Russian side, very modest gains come from com-
bining alternative segmentations (it could also be
that the Morfessor segmentation should not be per-
formed after CoreNLP but directly on untokenised
data), and reordering between Russian and En-
glish is such that a shallow-1 grammar performs
203
Configuration newstest2012.tune newstest2012.test newstest2013
CoreNLP(A) 33.65 32.36 25.55
+5g 33.67 32.58 25.63
+5g+LMBR 33.98 32.89 25.89
CoreNLP+Morfessor(B) 33.21 31.91 25.33
+5g 33.28 32.12 25.44
+5g+LMBR 33.58 32.43 25.78
CoreNLP+TreeTagger(C) 32.92 31.54 24.78
+5g 32.94 31.85 24.97
+5g+LMBR 33.12 32.12 25.05
A+B+C 34.32 33.13 26.00
Table 3: Translation results, shown in lowercase NIST BLEU. Bold results correspond to submitted
systems.
competitively.
Future work could include exploring alterna-
tive grammars, applying a 5-gram Kneser-Ney
smoothed language model directly in first-pass de-
coding, and combining alternative segmentations
that are more diverse from each other.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762. Tong Xiao was sup-
ported in part by the National Natural Science
Foundation of China (Grant 61073140 and Grant
61272376) and the China Postdoctoral Science
Foundation (Grant 2013M530131).
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood. 2010. Lattice rescoring meth-
ods for statistical machine translation. Ph.D. thesis,
Cambridge University Engineering Department and
Clare College.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263?311.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
David Chiang, Steve DeNeefe, and Michael Pust.
2011. Two easy improvements to lexical weighting.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 455?460, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alterna-
tive Morphological Decompositions. In Proceed-
ings of HLT/NAACL, Companion Volume: Short Pa-
pers, pages 73?76.
Adria` de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite state transducers and shallow-n grammars. In
Computational Linguistics.
Yonggang Deng and William Byrne. 2008. Hmm word
and phrase alignment for statistical machine trans-
lation. IEEE Transactions on Audio, Speech, and
Language Processing, 16(3):494?507.
Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy
Lin. 2008. Fast, easy, and cheap: Construc-
tion of statistical machine translation models with
204
MapReduce. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 199?207,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, Sofia, Bulgaria,
August.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Hierarchical phrase-
based translation with weighted finite state transduc-
ers. In Proceedings of NAACL, pages 433?441.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of ICASSP, volume 1, pages 181?184.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 iwslt speech translation evaluation. In
International Workshop on Spoken Language Trans-
lation, volume 8.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
725?734, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Juan Pino, Aurelien Waite, and William Byrne. 2012.
Simple and efficient model filtering in statistical ma-
chine translation. The Prague Bulletin of Mathemat-
ical Linguistics, 98(1):5?24.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Andreas Stolcke. 2002. SRILM?An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP,
volume 3, pages 901?904.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk decoding for statistical machine trans-
lation. In Proceedings of EMNLP, pages 620?629.
205
