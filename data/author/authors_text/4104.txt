Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 625?632
Manchester, August 2008
Exploring Domain Differences for the Design of Pronoun Resolution
Systems for Biomedical Text
Ngan L.T. Nguyen Jin-Dong Kim
Department of Computer Science, University of Tokyo, Hongo 7-3-1, Tokyo, Japan
{nltngan, jdkim}@is.s.u-tokyo.ac.jp
Abstract
Much effort in the research community has
been spent on solving the anaphora resolu-
tion or pronoun resolution problem, and in
particular for news texts. In order to selec-
tively inherit the previous works and solve
the same problem for a new domain, we
carried out a comparative study with three
different corpora: MUC, ACE for the news
texts, and GENIA for bio-medical papers.
Our corpus analysis and experimental re-
sults show the significant differences in the
use of pronouns in the two domains, thus
by properly considering the characteristics
of a domain, we can improve the perfor-
mance of pronoun resolution for that do-
main.
1 Introduction
Pronoun resolution is the task of determining the
antecedent of an anaphoric pronoun, or a pro-
noun pointing back to some previously mentioned
item in a text. For example, in the sentence, ?The
IL-2 gene displays both T cell specific and in-
ducible expression: it is only expressed in CD4+ T
cells after antigenic or mitogenic stimulation,? the
pronoun ?it? should be resolved to refer to ?the
IL-2 gene,? and thus, we have an anaphora link.
Pronoun resolution is an important task in the
family of reference resolution tasks, including
anaphora resolution and co-reference resolution,
which are known as significant parts of text un-
derstanding systems. Recently the need to have
more powerful information extraction systems for
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
biomedical technical papers has motivated re-
searchers to solve the same task for the biomed-
ical domain. Castano (Castano et al, 2002) re-
solved the sortal and pronominal anaphora, by us-
ing a salience measure, which is the sum of all fea-
ture scores. Kim and Park (Kim and C.Park, 2004)
introduced BioAR, a biomedical anaphora resolu-
tion system that relates entity mentions in text with
their corresponding Swiss-Prot entries. This sys-
tem resolves anaphoric pronouns by using heuris-
tic rules and seven patterns for parallelism. How-
ever, the sizes of the data sets used in their exper-
iments were small. In the former system, 46 and
54 MEDLINE abstracts were used for the devel-
opment set, and the test set respectively, and the
test set in the latter work contained only sixteen
anaphoric pronouns. Contrary to their work, in this
work we made use of GENIA, a large co-reference
annotated corpus for the bio domain, containing
1999 MEDLINE abstracts.
While there are quite a few works on this task
for the bio-medical domain, for other domains, and
especially for the news domain, a myriad of works
on pronoun resolution has been carried out by
the NLP researchers (Mitkov, 2002). Since Soon
(Soon et al, 2001) started the trend of using the
machine learning approach by using a binary clas-
sifier in a pairwise manner for solving co-reference
resolution problem, many machine learning-based
systems have been built, using both supervised
and, unsupervised learning methods (Haghighi and
Klein, 2007). Such methods were claimed to be
comparable with traditional methods. However,
the problems caused by domain differences, which
strongly affect a deep-semantics related task like
pronoun resolution, have not yet been studied well
enough.
In order to recognize the important factors in
625
building an effective machine learning-based pro-
noun resolution system, and in particular for the
bio-domain, we have built a machine learning-
based pronoun resolver and observed the contribu-
tions of different features in the pronoun resolution
process. In our experiments for the news domain,
we used the MUC-7 and ACE corpora, and for the
biomedical domain, we employed the GENIA co-
reference corpus.
Section 2 describes the noticeable issues related
to the corpora, and their preprocessing. Section 3
describes the implementation of our pronoun reso-
lution system, including the resolution model and
the features used. Our experiment settings, eval-
uation scheme, and experimental results are pre-
sented in Section 4. Finally, we conclude our paper
in Section 5.
2 Corpora
In this section, we briefly introduce three corpora
used in our experiments: MUC-7, ACE, and GE-
NIA, and discuss the differences in their annota-
tion schemes. Afterwards, we analyzed the major
differences in the distributions of anaphoric pro-
nouns in these data sets, which provide important
information for the design of features used in the
pronoun resolution process.
The MUC-7 co-reference corpus is a collection
of news wire articles from the source for North
American News Text Corpora. It contains the
training, dry run test, and formal run test sets. The
dry run and formal run have different domains; the
dry run (and training) consists of aircrash scenar-
ios, while the formal run consists of missile launch
scenarios. The ACE (phase 2) corpus for named
entity detection contains three data sets: news wire
(NWIRE), broadcast news (BNEWS), and news-
paper (NPAPER). Each data set is divided into
2 parts for training (train), and for development
testing (devtest). For the bio-domain, we use the
GENIA co-reference corpus, containing 1999 ab-
stracts selected from MEDLINE: a huge source of
bio-domain scientific papers.
These three corpora are all manually annotated
with co-reference information; i.e., the informa-
tion where mentions refer to the same entities.
However, since the annotation schemes used are
not the same, these corpora contain some signif-
icant differences, which may affect our reference
resolution systems.
Figure 1: The symmetric and asymmetric annota-
tion schemes. The dotted lines represent implicit
links between the elements.
2.1 Variations in co-reference annotation
schemes
We started by introducing some important ter-
minologies together with some noticeable issues
related to the common co-reference annotation
scheme. Later, we mention the differences among
the annotation schemes of the three corpora used
in our experiments.
There are three main elements in the co-
reference corpus annotation: the anaphoric expres-
sions, which are anaphoric pronouns in the case
of the pronominal anaphora, their antecedents, and
the referred concepts. Depending on either the
asymmetric scheme employed in MUC (Lynette,
1997) and GENIA (Hong, 2004) or the symmetric
scheme in ACE (NIST, 2003), the annotation task
is defined as either an anaphor-antecedent linking,
or mention-concept linking task, correspondingly
(See Figure 1). Moreover, each annotation scheme
provides its own guidelines for recognizing and an-
notating these three elements, causing the varia-
tions across different co-reference annotated cor-
pora.
In the annotation schemes, mentions which may
join in the co-reference relationship are called
markable. All of the three annotation schemes
record both a maximal and a minimal boundary of
markables, in concerning the evaluation schemes.
However, the types of markables to be annotated,
and the ways to decide their maximal boundary,
are not the same in every annotation scheme.
Table 1 shows the concepts annotated for each
corpora according to the annotation schemes.
While the number of concepts in the ACE corpus
is limited to only 5 entity types, the GENIA and
MUC annotation schemes do not clearly specify
the concept types. This means that every possible
concept in the text domains can join the anaphora
relations; i.e., can be annotated as markables. This
in turn makes the resolution task become more dif-
ficult.
626
Table 1: Possible concepts according to the anno-
tation schemes
GENIA ACE MUC
(Not specified
explicitly)
-Bio entities
5 types of enti-
ties
-Person
-Organization
-Facility
-Location
-GPE(Geo-
political Entity)
(Not specified
explicitly)
-Person
-Organization
-Location
-Date
-Time
-Money
-Percent
Table 2: Possible types of anaphor according to the
annotation schemes (O: allowed, X: not allowed,
U: unspecified)
TYPE GENIA ACE MUC
Personal pronoun O O O
Demonstrative pronoun O O O
Possessive pronoun O O O
Reflexive pronoun O O U
Indefinite pronoun (e.g.,
both)
O U U
Pleonastic pronoun it X U U
Bound anaphor X U O
Mention with empty head
(e.g., five of)
X U U
here, there U O U
The possible types of annotated anaphoric pro-
nouns are given in Table 2. O denotes the type of
pronoun, which may be annotated as markable, in
contrast to X, which denotes the type of pronoun,
which is not allowed to be annotated as markable.
The notation U represents the annotation scheme
that does not state how a type should be treated
because that type is not popular in the domain, or
the scheme does not allow the annotation of such a
type implicitly.
Using the similar notations as in Table 2, Ta-
ble 3 shows the possible syntactic structures of
antecedents according to the annotation schemes,
which are also the structures of markables in real
annotations. In practice, such structural varia-
tions may cause troubles for automatically mark-
able recognition, so in the experiments with pro-
noun resolution, gold markables are often used to
eliminate error-prone problems.
2.2 Corpus preprocessing
Our objective anaphoric pronouns are limited to
the following types: personal pronouns (all cases),
possessive pronouns, and demonstrative pronouns,
which have a nominal antecedent. In addition
Table 3: Possible types of antecedent according
to the annotation schemes (O: allowed, X: not al-
lowed, U: unspecified)
TYPE GENIA ACE MUC
Pronominal X O O
Noun used as a modifier (em-
bedded in NP)
X O O
Name, named entity (embedded
in NP)
X O O
Gerund U U X
NP with a head noun (definite
and indefinite)
O O O
Conjoint NP (with more than
one head)
O X O
Coordinated NP O O O
Predicate nominal X O O
NP with a restrictive appositive
phrase
X O O
NP with a non-restrictive ap-
positive phrase
X O O
NP with a restrictive preposi-
tional phrase
O O O
NP with a non-restrictive
prepositional phrase
X O O
NP with a restrictive relative
clause
O O O
NP with a non-restrictive rela-
tive clause
O O O
Infinitive clause O U U
Date, Currency expressions,
and percentages
U U O
Proper adjective (e.g.,French) U O U
here, there X O U
to these types of pronouns, the annotated cor-
pora contain other types of pronominal anaphora,
including ?both,? ?one,? numeric mentions (GE-
NIA), and bound anaphora (ACE). However, anal-
ysis statistics show that such pronouns occupy less
than 5% of the total pronouns in the GENIA cor-
pus, thus we have ignored them.
In the preprocessing step, for each corpus, we
extract the gold pronominal anaphora links, which
link the anaphoric pronouns with their antecedents.
Although MUC and GENIA used the same asym-
metric annotation schemes, picking one gold an-
tecedent in a set of co-referenced mentions is not
straightforward, since pronouns in GENIA are not
allowed to be linked with a pronomial antecedent,
while in the MUC corpus, this kind of link is al-
lowed. In order to achieve the fairest compara-
tive experimental results, we uniformly choose the
nearest item in the co-reference chain of a pro-
noun, and make a gold anaphora link. This pol-
icy is best suited for ACE, thanks to the symmetric
scheme used.
627
Table 4: Sizes of the data sets (number of
anaphoric pronoun)
GENIA ACE MUC
Training set 1442 2427 371
Test set 357 633 240
Figure 2: Analysis of anaphoric pronoun in differ-
ent data sets
2.3 Statistics
In the following step, we analyze the extracted
anaphora links for the three corpora. The analy-
sis statistics in Figure 2 show the differences of the
distributions of pronoun types and pronoun prop-
erties in three data sets: MUC-7, GENIA, and
BNEWS from ACE. Note that only four major
types out of the nine types of anaphoric pronouns
mentioned in the previous section are counted. In
particular, the chosen types correspond to those
rows in Table 2 that contain at least two O.
We can see that all of the anaphoric pronouns in
GENIA are neutral-gender and third-person pro-
nouns. Another difference is that the number of
demonstrative pronouns in GENIA comes to about
20%, which is much more than in other data sets.
As each type of pronoun has its own referen-
tial characteristics, such differences in the distribu-
tions of pronouns can significantly affect the pro-
noun resolution. This will be shown in our experi-
ments, and analysis of the experimental results will
be given in the following section.
3 Implementation
3.1 Pronoun resolution model
We built a machine learning based pronoun res-
olution engine using a Maximum Entropy ranker
model (Berger et al, 1996), similar with Denis and
Baldridge?s model (Denis and Baldridge, 2007).
For every anaphoric pronoun ?, the ranker selects
the most likely antecedent candidate ?, from a set
of k candidate markables.
P
r
(?
j
|?) =
exp (
?
n
i=1
?
i
f
i
(?, ?
j
))
?
k
exp (
?
n
i=1
?
i
f
i
(?, ?
k
))
(1)
We constructed the training examples in the fol-
lowing way: for each gold anaphora link in the
training corpus, we created a positive instance, and
negative training instances are created by pairing
the pronoun with all of the other markables ap-
pearing in a window of w preceding sentences. In
all the experiments on ACE and MUC, we set w
to 10 sentences, while for GENIA, w is set to 5.
This setting is based on our corpus analysis show-
ing that many of the gold antecedents in the bio-
domain texts are in at most three sentences from
their anaphors. In the resolution phase, the same
method for collecting instances was also applied.
3.2 Features
Table 5 shows the primitive features used in our
system, which are grouped into feature groups ac-
cording to the type of information that they carry.
Note that the actual features used by the ranker are
distance features (sdist, and tdist), and not only the
primitive features themselves, but also the combi-
nations of these primitive features. The pronoun
resolution model makes use of the discriminative
power of these combinatory features. For exam-
ple, the combination of P num and C num tests the
agreement in number between the anaphoric pro-
noun and its candidate. Such agreements in num-
ber and gender are one of the constraints in the
anaphora phenomenon, and have been exploited in
almost all machine learning-based pronoun resolu-
tion frameworks (Soon et al, 2001).
Each primitive feature is from a layer of text
analysis (see Layer), which can be morphological
(mor.), syntactic (syn.), or semantic (sem.). The
second column represents the feature sets that are
used in our experiments. The explanation column
in the table shows the way we extract feature val-
ues from texts, with the exception of the primitive
628
feature P semw, reflecting the context information
of the anaphoric pronoun. This feature value is de-
termined in the following way. If the pronoun is
a subject, then P semw is its governing head verb,
and if it is a possessive adjective, then P semw is
the head noun of the noun phrase containing that
pronoun. A default value is used if the pronoun
belongs to neither of the above cases.
The last column of this table shows an exam-
ple of the feature characterization for the anaphora
link PMA-its in this discourse: ?By comparison,
PMA is a very inefficient inducer of the jun gene
family in Jurkat cells. Similar to its effect on the
induction of AP1 by okadaic acid, PMA inhibits
the induction of c-jun mRNA by okadaic acid.?
We divided the feature groups into 3 feature sets:
fundamental, baseline and additional. The funda-
mental feature set contains the indispensable fea-
tures for solving pronoun resolution. The base-
line feature set mostly includes morphological fea-
tures, reflecting the properties of text mentions,
and in particular the pronoun properties such as
gender, number, etc. The features in the addi-
tional feature set are used to exploit higher levels
of knowledge through more semantic and syntactic
features. We also include in this feature set the fea-
tures that have been used in some previous work in
order to clarify their contributions in our system.
4 Experiments
4.1 Experiment setting and evaluation
scoring
For each corpus, we trained our resolver on the
training set, and then applied it to the develop-
ment test set. For the case of the ACE corpus, we
only used the train part of the BNEWS data set for
training, and applied on the corresponding devtest
data set. We randomly splitted the GENIA cor-
pus it into 2 parts: the train, and the heldout data
sets, which contain 1599 and 400 abstracts, respec-
tively. For the MUC corpus, we used the dryrun
part for training, and the formal part for testing.
Similar to previous works, all of the experimen-
tal results in this paper are reported in success rate
(Mitkov, 2002), calculated using the following for-
mula.
Success rate = Number of successfully resolved anaphors
Number of all anaphors
(2)
The input of our resolver are the gold mentions
annotated in the corpora. The output anaphora
links of a pronoun resolution system are evalu-
ated following two criteria. In criterion 1, the
recognized antecedent of an anaphoric pronoun is
considered correct only when it matches the an-
tecedent in the gold anaphora link of that pro-
noun. Criterion 2 is a bit looser when the recog-
nized antecedent just needs to match one of the
antecedents of a pronoun in its co-reference chain.
This criterion has been used by most of the pre-
vious works, including Denis and Baldridge?s sys-
tem (Denis and Baldridge, 2007).
4.2 Baseline resolver
In this experiment, we use the baseline feature set
presented in section 3.2. One of the reasons in
choosing these features for the baseline system, is
that they are basic features that have been used by
most of the previous reference resolution systems.
Moreover, we wanted to see how these features
contribute to the resolution process for different
corpora, presented in the next section.
Our baseline system achieved a 71.41% success
rate on the BNEWS data set (Table 6, criterion
2), which is comparable to the result of Denis and
Baldridge?s system on the same data set (Denis and
Baldridge, 2007). Moreover, we can see that the
differences caused by the two criteria are not the
same for every data set. For the news domain data
sets, the differences vary from 4.17% (MUC) to
6.8% (ACE), which is high in comparison with the
percentages of GENIA, which were less than two
percent. This can be explained by the fact that pro-
nouns in news texts are used more repeatedly than
those in bio-medical texts. Because bio-entities are
neutral-gender mentions, and are referred by the
neutral gender and third person pronouns, the re-
peated use of pronouns may increase the ambiguity
of the text, and confuse the readers.
To prevent the confusion of the readers, we
chose just one data set BNEWS to represent the
ACE corpus and present our further analysis ex-
periments on these three data sets: GENIA, ACE
(BNEWS), and MUC (MUC-7).
4.3 Contributions of the features in the
baseline resolver
In order to observe the effects of the features in the
baseline pronoun resolver, we omitted each fea-
ture group from the whole feature set, retrained
our resolution models with the new feature set,
and applied them to the three data sets: GENIA,
629
Table 5: Features used in the pronoun resolver
Layer Feature set Group Primitive Feature Explanation Example
mor.
fundamental mention type P type pronoun type possessive pronounC type candidate mention type proper name
baseline
sdist CP sdis distance in sentence 1
tdist CP tdis normalized distance in token 17
numb P numb number of p singularC numb number of c unknown
pers P pers person of p third personC pers person of c third person
gend P gend gender of p neutralC gend gender of c neutral
pfam P pfam family of p itC pfam family of c null
string P word pronoun string its
syn.
C head candidate head string PMA
additional
pos
P lpos POS of the left word of p TO
P rpos POS of the right word of p NN
C lpos POS of the left word of c COMMA
C rpos POS of the right word of c VBZ (is)
parg P parg argument role of p nullC parg argument role of c arg1
sem. netype C netype entity type of c null
mor. last3c C last3c the last 3 characters of c pma
syn.
comb P semw see Section 3.2 effect
other C 1stnp first NP in a sentence or not false
Table 6: Baseline system evalutation (C1: criterion
1, C2: criterion 2, D: difference between criterion
1 and 2)
GENIA ACE MUC
C1 70.31 64.61 57.08
C2 71.43 71.41 61.25
Diff 1.12 6.80 4.17
BNEWS, and MUC. Pronoun type and mention
type are the most significant features, and thus, are
not omitted in this experiment.
Table 7 shows the experimental results: the first
column is the feature group name, and the follow-
ing three columns show the resolution accuracy of
the three corpora. The figures in the parentheses
show the degradation, when we exclude the corre-
sponding group from the baseline feature set. Our
data analysis show some noticeable issues:
Number features (numb) :
The number-combination features are the most
significant features in bio-texts while they are not
so effective on ACE, and even perform negatively
on MUC. One of the reasons behind this, is that
in the bio-texts, all of the anaphoric pronouns
have a deterministic number; i.e., either singular
or plural (Section 2.3), while the news texts con-
tain first- and second-person pronouns whose num-
bers are unspecified. Another reason emerges from
the non-pronominal types of mentions, which play
a role as antecedents. The number property of
these mentions is characterized in the markable
detection phase based on the part-of-speech tag,
the head noun, and the phrase structure of those
mentions. In particular, the MUC corpus con-
tains many coordinated-structured mentions (Sec-
tion 2.1), which are difficult for markable charac-
terization.
Person features and pronoun family (pers
and pfam) :
The absence of the pers features caused the
biggest loss for the resolution success rate on the
ACE corpus, because the co-reference chains in
this corpus contain a lot of pronouns, and it is
easier for the pronoun resolver to determine a
pronominal antecedent than to determine a non-
pronominal antecedent. The same phenomena can
be observed with pfam features. The bio-text only
contains third-person anaphoric pronouns (Section
2.3), so the person features do not have any profits.
Distance features (sdist and tdist) :
Our baseline resolver again confirmed that the
sentence distance is an indispensable feature in
pronoun resolution. However, the token-based dis-
tance did not show any improvements on the MUC
corpus. Analyzing the MUC anaphora links, we
found that these tdist features resulted in 10 cor-
rect anaphora links, but also mis-recognized 10 an-
tecedents.
630
Table 7: Feature contributions in the baseline sys-
tem (evaluation criterion 1)
Excluded GENIA ACE MUC
none 70.31 64.61 57.08
-sdist 67.23(?3.08) 63.51(?1.10) 51.67(?5.41)
-tdist 70.03(?0.28) 59.56(?5.05) 57.08(+0.00)
-numb 65.83(?4.48) 61.77(?2.84) 58.33(+1.25)
-pers 70.31(+0.00) 57.19(?7.42) 55.42(?1.66)
-gend 69.75(?0.56) 64.45(?0.16) 56.67(?0.41)
-pfam 71.15(+0.84) 63.51(?1.10) 57.92(+0.84)
-string 68.07(?2.24) 61.93(?2.68) 55.83(?1.25)
4.4 Contributions of additional features to
the baseline feature set
In addition to the baseline feature set, we enhanced
our resolver with more features. Among them,
there are two noticeable features: the grammati-
cal role of pronouns or antecedent candidates, and
the named entity type of the candidates. The other
feature groups are used in Denis and Baldridge?s
system, which we also want to test in our system.
Table 8 shows the resolution results and the
increase when adding the corresponding feature
group. With the exception of the last3c features,
the others significantly improved the resolution
success rate on bio-texts, although they did not
have clear contributions to the news domain data
sets. The following is our further analysis to see
the way that these features can contribute to the
pronoun resolution process.
Semantic features (netype)
The first feature we would like to observe is
the combination of C netype and P semw features,
which contributed to the increase by 3.64 points.
We further conducted a small test by excluding this
combination from the netype feature group, but the
success rate remained unchanged from the baseline
result. This signifies that this combination con-
tributed the most to the above increase.
The combination of C netype and P semw fea-
tures exploits the co-ocurrence of the semantic
type of the candidate antecedent and the context
word, which appears in some relationship with the
pronoun. This combination feature uses the infor-
mation similar to the semantic compatibility fea-
tures proposed by Yang (Yang et al, 2005) and
Bergsma (Bergsma and Lin, 2006). Depending
on the pronoun type, the feature extractor decides
which relationship is used. For example, the re-
solver successfully recognizes the antecedent of
the pronoun its in this discourse: ?HSF3 is con-
stitutively expressed in the erythroblast cell line
HD6 , the lymphoblast cell line MSB , and em-
bryo fibroblasts , and yet its DNA-binding activ-
ity is induced only upon exposure of HD6 cells to
heat shock ,? because HSF3 was detected as a Pro-
tein entity, which has a strong association with the
governing head noun activity of the pronoun.
Another example is the correct anaphora link
between ?it? and ?the viral protein? in the fol-
lowing sentence, which the other features failed to
detect. ?Tax , the viral protein , is thought to be
crucial in the development of the disease , since
it transforms healthy T cells in vitro and induces
tumors in transgenic animals.? The correct an-
tecedent was recognized due to the bias given to
the association of the Protein entity type, and the
governing verb, ?transform? of the pronoun. The
experimental results show the contribution of the
domain knowledge to the pronoun resolution, and
the potential combination use of such knowledge
with the syntactic features.
Parse features (parg)
The combinations of the primitive features of
grammatical roles significantly improved the per-
formance of our resolver. The following examples
show the correct anaphora links resulting from us-
ing the parse features:
? ?By comparison, PMA is a very inefficient in-
ducer of the jun gene family in Jurkat cells.
Similar to its effect on the induction of AP1
by okadaic acid, PMA inhibits the induction
of c-jun mRNA by okadaic acid.?
In this example, the possessive pronoun ?its? in
the second sentence corefers to ?PMA?, the sub-
ject of the preceding sentence.
Among the combination features in this group,
one noticeable feature is the combination of
C parg, Sdist, and P type which contains the as-
sociation of the grammatical role of the candidate,
the sentence-based distance, and the pronoun type.
The idea of adding this combination is based on
the Centering theory (Walker et al, 1998), a the-
ory of discourse successfully used in pronoun res-
olution. This simple feature shows the potential of
encoding centering theory in the machine learning
features, based on the parse information.
Feature integration
Finally, we integrated all of the positive fea-
ture groups for each data set in the above experi-
ments, and tested this combining feature set. Table
631
Table 8: Additional features and their contribu-
tions (evaluation criterion 1)
Added GENIA ACE MUC
none 70.31 64.61 57.08
+pos 75.63(+5.32) 62.88(?1.73) 57.50(+0.42)
+parg 73.67(+3.36) 63.82(?0.79) 58.75(+1.67)
+netype 73.95(+3.64) 64.30(?0.31) 58.33(+1.25)
+last3c 67.51(?2.80) 62.09(?2.52) 56.67(?0.41)
+comb 72.83(+2.52) 63.82(?0.79) 56.25(?0.83)
Table 9: Feature integration
GENIA ACE MUC
C1 79.55 (+9.24) 64.61 (+0.00) 60.42 (+3.34)
C2 80.95 (+9.52) 71.41 (+0.00) 66.25 (+5.00)
9 shows a significant increase in the performance
of the resolver on GENIA and MUC.
5 Conclusion and future work
Through the differences in the corpus annotation
schemes, in the corpora themselves, and in contri-
butions of resolution factors to the pronoun resolu-
tion process, we can see that adapting pronoun res-
olution for a different domain is not an easy task. A
good study on the types of anaphoric pronouns and
entity mention structures beforehand can help de-
sign a better feature set for our machine learning-
based pronoun resolution system and thus, can
save much time and labor.
As shown in this paper, for the news do-
main, the properties of anaphoric pronouns contain
rich information about their antecedents, which is
very useful in the resolution process. While in
biomedical text, it is more important to capture
the information to connect a pronoun and its an-
tecedent from their surrounding context, because
the anaphoric pronouns themselves contain almost
no information of their antecedents with the excep-
tion of the numbers.
As a future work, it would be interesting to see
how the system performs in other domains. More
experiments should be designed to make the influ-
ences of annotation schemes on the pronoun reso-
lution process clearer.
References
Berger, Adam L., Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Bergsma, Shane and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL,
pages 33?40.
Castano, Jose, Jason Zhang, and James Pusterjovsky.
2002. Anaphora resolution in biomedical literature.
In Int?l Symposium Reference Resolution in NLP.
Denis, Pascal and J. Baldridge. 2007. A ranking ap-
proach to pronoun resolution. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence (IJCAI07).
Haghighi, Aria and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 848?855.
Hong, Huaqing. 2004. Coreference annotation scheme
for medco corpus.
Kim, Jung-Jae and Jong C.Park. 2004. Bioar:
Anaphora resolution for relating protein names to
proteome database entries. In Proceedings of the
ACL 2004: Workshop on Reference Resolution and
its Applications, pages 79?86.
Lynette, Hirschman. 1997. Muc-7 coreference task
definition.
Mitkov, Ruslan. 2002. Anaphora resolution. Pearson
Education, London, Great Britain.
NIST. 2003. Entity detection and tracking - phrase
1 edt and metonymy annotation guidelines version
2.5.1 20030502.
Soon, W., H. Ng, and D. Lim. 2001. A machine
learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
Walker, Marilyn A., Aravind K. Joshi, and Ellen F.
Prince. 1998. Centering Theory in Discourse.
Clarendon Press, Oxford.
Yang, Xiaofeng, Jian Su, and Chew-Lim Tan. 2005.
Improving pronoun resolution using statistics-based
semantic compatibility information. In Proceedings
of the 43rd Annual meeting of the Association for
Computational Linguistics (ACL05), pages 427?434.
632
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Self-Organizing Markov Models and
Their Application to Part-of-Speech Tagging
Jin-Dong Kim
Dept. of Computer Science
University of Tokyo
jdkim@is.s.u-tokyo.ac.jp
Hae-Chang Rim
Dept. of Computer Science
Korea University
rim@nlp.korea.ac.kr
Jun?ich Tsujii
Dept. of Computer Science
University of Tokyo, and
CREST, JST
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper presents a method to de-
velop a class of variable memory Markov
models that have higher memory capac-
ity than traditional (uniform memory)
Markov models. The structure of the vari-
able memory models is induced from a
manually annotated corpus through a de-
cision tree learning algorithm. A series of
comparative experiments show the result-
ing models outperform uniform memory
Markov models in a part-of-speech tag-
ging task.
1 Introduction
Many major NLP tasks can be regarded as prob-
lems of finding an optimal valuation for random
processes. For example, for a given word se-
quence, part-of-speech (POS) tagging involves find-
ing an optimal sequence of syntactic classes, and NP
chunking involves finding IOB tag sequences (each
of which represents the inside, outside and begin-
ning of noun phrases respectively).
Many machine learning techniques have been de-
veloped to tackle such random process tasks, which
include Hidden Markov Models (HMMs) (Rabiner,
1989), Maximum Entropy Models (MEs) (Rat-
naparkhi, 1996), Support Vector Machines
(SVMs) (Vapnik, 1998), etc. Among them,
SVMs have high memory capacity and show high
performance, especially when the target classifica-
tion requires the consideration of various features.
On the other hand, HMMs have low memory
capacity but they work very well, especially when
the target task involves a series of classifications that
are tightly related to each other and requires global
optimization of them. As for POS tagging, recent
comparisons (Brants, 2000; Schro?der, 2001) show
that HMMs work better than other models when
they are combined with good smoothing techniques
and with handling of unknown words.
While global optimization is the strong point of
HMMs, developers often complain that it is difficult
to make HMMs incorporate various features and to
improve them beyond given performances. For ex-
ample, we often find that in some cases a certain
lexical context can improve the performance of an
HMM-based POS tagger, but incorporating such ad-
ditional features is not easy and it may even degrade
the overall performance. Because Markov models
have the structure of tightly coupled states, an ar-
bitrary change without elaborate consideration can
spoil the overall structure.
This paper presents a way of utilizing statistical
decision trees to systematically raise the memory
capacity of Markov models and effectively to make
Markov models be able to accommodate various fea-
tures.
2 Underlying Model
The tagging model is probabilistically defined as
finding the most probable tag sequence when a word
sequence is given (equation (1)).
T (w1,k) = arg maxt1,k P (t1,k|w1,k) (1)
= arg max
t1,k
P (t1,k)P (w1,k|t1,k) (2)
? arg max
t1,k
k
?
i=1
P (ti|ti?1)P (wi|ti) (3)
By applying Bayes? formula and eliminating a re-
dundant term not affecting the argument maximiza-
tion, we can obtain equation (2) which is a combi-
nation of two separate models: the tag language
model, P (t1,k) and the tag-to-word translation
model, P (w1,k|t1,k). Because the number of word
sequences, w1,k and tag sequences, t1,k is infinite,
the model of equation (2) is not computationally
tractable. Introduction of Markov assumption re-
duces the complexity of the tag language model and
independent assumption between words makes the
tag-to-word translation model simple, which result
in equation (3) representing the well-known Hidden
Markov Model.
3 Effect of Context Classification
Let?s focus on the Markov assumption which is
made to reduce the complexity of the original tag-
ging problem and to make the tagging problem
tractable. We can imagine the following process
through which the Markov assumption can be intro-
duced in terms of context classification:
P (T = t1,k) =
k
?
i=1
P (ti|t1,i?1) (4)
?
k
?
i=1
P (ti|?(t1,i?1)) (5)
?
k
?
i=1
P (ti|ti?1) (6)
In equation (5), a classification function ?(t1,i?1) is
introduced, which is a mapping of infinite contextual
patterns into a set of finite equivalence classes. By
defining the function as follows we can get equation
(6) which represents a widely-used bi-gram model:
?(t1,i?1) ? ti?1 (7)
Equation (7) classifies all the contextual patterns
ending in same tags into the same classes, and is
equivalent to the Markov assumption.
The assumption or the definition of the above
classification function is based on human intuition.
( )conjP |?
( )conjfwP ,|?
( )conjvbP ,|?
( )conjvbpP ,|?
vbvb
vbpvbp
Figure 1: Effect of 1?st and 2?nd order context
atat
prepprep
nnnn( )prepP |?
( )in'',| prepP ?
( )with'',| prepP ?
( )out'',| prepP ?
Figure 2: Effect of context with and without lexical
information
Although this simple definition works well mostly,
because it is not based on any intensive analysis of
real data, there is room for improvement. Figure 1
and 2 illustrate the effect of context classification on
the compiled distribution of syntactic classes, which
we believe provides the clue to the improvement.
Among the four distributions showed in Figure 1,
the top one illustrates the distribution of syntactic
classes in the Brown corpus that appear after all the
conjunctions. In this case, we can say that we are
considering the first order context (the immediately
preceding words in terms of part-of-speech). The
following three ones illustrates the distributions col-
lected after taking the second order context into con-
sideration. In these cases, we can say that we have
extended the context into second order or we have
classified the first order context classes again into
second order context classes. It shows that distri-
butions like P (?|vb, conj) and P (?|vbp, conj) are
very different from the first order ones, while distri-
butions like P (?|fw, conj) are not.
Figure 2 shows another way of context extension,
so called lexicalization. Here, the initial first order
context class (the top one) is classified again by re-
ferring the lexical information (the following three
ones). We see that the distribution after the prepo-
sition, out is quite different from distribution after
other prepositions.
From the above observations, we can see that by
applying Markov assumptions we may miss much
useful contextual information, or by getting a better
context classification we can build a better context
model.
4 Related Works
One of the straightforward ways of context exten-
sion is extending context uniformly. Tri-gram tag-
ging models can be thought of as a result of the
uniform extension of context from bi-gram tagging
models. TnT (Brants, 2000) based on a second or-
der HMM, is an example of this class of models and
is accepted as one of the best part-of-speech taggers
used around.
The uniform extension can be achieved (rela-
tively) easily, but due to the exponential growth of
the model size, it can only be performed in restric-
tive a way.
Another way of context extension is the selective
extension of context. In the case of context exten-
sion from lower context to higher like the examples
in figure 1, the extension involves taking more infor-
mation about the same type of contextual features.
We call this kind of extension homogeneous con-
text extension. (Brants, 1998) presents this type of
context extension method through model merging
and splitting, and also prediction suffix tree learn-
ing (Schu?tze and Singer, 1994; D. Ron et. al, 1996)
is another well-known method that can perform ho-
mogeneous context extension.
On the other hand, figure 2 illustrates heteroge-
neous context extension, in other words, this type
of extension involves taking more information about
other types of contextual features. (Kim et. al, 1999)
and (Pla and Molina, 2001) present this type of con-
text extension method, so called selective lexicaliza-
tion.
The selective extension can be a good alternative
to the uniform extension, because the growth rate
of the model size is much smaller, and thus various
contextual features can be exploited. In the follow-
VP
N C
$
$ C N P V
P-1-
$ C N P V
Figure 3: a Markov model and its equivalent deci-
sion tree
ing sections, we describe a novel method of selective
extension of context which performs both homoge-
neous and heterogeneous extension simultaneously.
5 Self-Organizing Markov Models
Our approach to the selective context extension is
making use of the statistical decision tree frame-
work. The states of Markov models are represented
in statistical decision trees, and by growing the trees
the context can be extended (or the states can be
split).
We have named the resulting models Self-
Organizing Markov Models to reflect their ability to
automatically organize the structure.
5.1 Statistical Decision Tree Representation of
Markov Models
The decision tree is a well known structure that is
widely used for classification tasks. When there are
several contextual features relating to the classifi-
cation of a target feature, a decision tree organizes
the features as the internal nodes in a manner where
more informative features will take higher levels, so
the most informative feature will be the root node.
Each path from the root node to a leaf node repre-
sents a context class and the classification informa-
tion for the target feature in the context class will be
contained in the leaf node1 .
In the case of part-of-speech tagging, a classifi-
cation will be made at each position (or time) of a
word sequence, where the target feature is the syn-
tactic class of the word at current position (or time)
and the contextual features may include the syntactic
1While ordinary decision trees store deterministic classifi-
cation information in their leaves, statistical decision trees store
probabilistic distribution of possible decisions.
V
P,*,
N C
$
$ C N W-1- V
P-1-
$ C N P V
P,out, t
P,*,P,out, t
Figure 4: a selectively lexicalized Markov model
and its equivalent decision tree
V
P,*,
N
(N)C( )$
$ P-2- N W-1- V
P-1-
$ C N P V
P,out, t
P,*,P,out, t
(V)C( )
(*)C( )
(*)C( )(N)C( ) (V)C( )
Figure 5: a selectively extended Markov model and
its equivalent decision tree
classes or the lexical form of preceding words. Fig-
ure 3 shows an example of Markov model for a sim-
ple language having nouns (N), conjunctions (C),
prepositions (P) and verbs (V). The dollar sign ($)
represents sentence initialization. On the left hand
side is the graph representation of the Markov model
and on the right hand side is the decision tree repre-
sentation, where the test for the immediately preced-
ing syntactic class (represented by P-1) is placed on
the root, each branch represents a result of the test
(which is labeled on the arc), and the correspond-
ing leaf node contains the probabilistic distribution
of the syntactic classes for the current position2 .
The example shown in figure 4 involves a further
classification of context. On the left hand side, it is
represented in terms of state splitting, while on the
right hand side in terms of context extension (lexi-
calization), where a context class representing con-
textual patterns ending in P (a preposition) is ex-
tended by referring the lexical form and is classi-
fied again into the preposition, out and other prepo-
sitions.
Figure 5 shows another further classification of
2The distribution doesn?t appear in the figure explicitly. Just
imagine each leaf node has the distribution for the target feature
in the corresponding context.
context. It involves a homogeneous extension of
context while the previous one involves a hetero-
geneous extension. Unlike prediction suffix trees
which grow along an implicitly fixed order, decision
trees don?t presume any implicit order between con-
textual features and thus naturally can accommodate
various features having no underlying order.
In order for a statistical decision tree to be a
Markov model, it must meet the following restric-
tions:
? There must exist at least one contextual feature
that is homogeneous with the target feature.
? When the target feature at a certain time is clas-
sified, all the requiring context features must be
visible
The first restriction states that in order to be a
Markov model, there must be inter-relations be-
tween the target features at different time. The sec-
ond restriction explicitly states that in order for the
decision tree to be able to classify contextual pat-
terns, all the context features must be visible, and
implicitly states that homogeneous context features
that appear later than the current target feature can-
not be contextual features. Due to the second re-
striction, the Viterbi algorithm can be used with the
self-organizing Markov models to find an optimal
sequence of tags for a given word sequence.
5.2 Learning Self-Organizing Markov Models
Self-organizing Markov models can be induced
from manually annotated corpora through the SDTL
algorithm (algorithm 1) we have designed. It is a
variation of ID3 algorithm (Quinlan, 1986). SDTL
is a greedy algorithm where at each time of the node
making phase the most informative feature is se-
lected (line 2), and it is a recursive algorithm in the
sense that the algorithm is called recursively to make
child nodes (line 3),
Though theoretically any statistical decision tree
growing algorithms can be used to train self-
organizing Markov models, there are practical prob-
lems we face when we try to apply the algorithms to
language learning problems. One of the main obsta-
cles is the fact that features used for language learn-
ing often have huge sets of values, which cause in-
tensive fragmentation of the training corpus along
with the growing process and eventually raise the
sparse data problem.
To deal with this problem, the algorithm incor-
porates a value selection mechanism (line 1) where
only meaningful values are selected into a reduced
value set. The meaningful values are statistically
defined as follows: if the distribution of the target
feature varies significantly by referring to the value
v, v is accepted as a meaningful value. We adopted
the ?2-test to determine the difference between the
distributions of the target feature before and after re-
ferring to the value v. The use of ?2-test enables
us to make a principled decision about the threshold
based on a certain confidence level3.
To evaluate the contribution of contextual features
to the target classification (line 2), we adopted Lopez
distance (Lo?pez, 1991). While other measures in-
cluding Information Gain or Gain Ratio (Quinlan,
1986) also can be used for this purpose, the Lopez
distance has been reported to yield slightly better re-
sults (Lo?pez, 1998).
The probabilistic distribution of the target fea-
ture estimated on a node making phase (line 4) is
smoothed by using Jelinek and Mercer?s interpola-
tion method (Jelinek and Mercer, 1980) along the
ancestor nodes. The interpolation parameters are
estimated by deleted interpolation algorithm intro-
duced in (Brants, 2000).
6 Experiments
We performed a series of experiments to compare
the performance of self-organizing Markov models
with traditional Markov models. Wall Street Jour-
nal as contained in Penn Treebank II is used as the
reference material. As the experimental task is part-
of-speech tagging, all other annotations like syntac-
tic bracketing have been removed from the corpus.
Every figure (digit) in the corpus has been changed
into a special symbol.
From the whole corpus, every 10?th sentence from
the first is selected into the test corpus, and the re-
maining ones constitute the training corpus. Table 6
shows some basic statistics of the corpora.
We implemented several tagging models based on
equation (3). For the tag language model, we used
3We used 95% of confidence level to extend context. In
other words, only when there are enough evidences for improve-
ment at 95% of confidence level, a context is extended.
Algorithm 1: SDTL(E, t, F )
Data : E: set of examples,
t: target feature,
F : set of contextual features
Result : Statistical Decision Tree predicting t
initialize a null node;
for each element f in the set F do
1 sort meaningful value set V for f ;
if |V | > 1 then
2 measure the contribution of f to t;
if f contributes the most then
select f as the best feature b;
end
end
end
if there is b selected then
set the current node to an internal node;
set b as the test feature of the current node;
3 for each v in |V | for b do
make SDTL(Eb=v, t, F ? {b}) as the
subtree for the branch corresponding to
v;
end
end
else
set the current node to a leaf node;
4 store the probability distribution of t over
E ;
end
return current node;
1,289,20168,590Total
129,1006,859Test
1,160,10161,731Training
 	 
  	 
   	 
  	 
  	 
  	 
  	 
  	 
       	 
  	 
  	 
  	 
  	 
  	 
  	 
  	 
     
Figure 6: Basic statistics of corpora
the following 6 approximations:
P (t1,k) ?
k
?
i=1
P (ti|ti?1) (8)
?
k
?
i=1
P (ti|ti?2,i?1) (9)
?
k
?
i=1
P (ti|?(ti?2,i?1)) (10)
?
k
?
i=1
P (ti|?(ti?1, wi?1)) (11)
?
k
?
i=1
P (ti|?(ti?2,i?1, wi?1)) (12)
?
k
?
i=1
P (ti|?(ti?2,i?1, wi?2,i?1))(13)
Equation (8) and (9) represent first- and second-
order Markov models respectively. Equation (10)
? (13) represent self-organizing Markov models at
various settings where the classification functions
?(?) are intended to be induced from the training
corpus.
For the estimation of the tag-to-word translation
model we used the following model:
P (wi|ti)
= ki ? P (ki|ti) ? P? (wi|ti)
+(1 ? ki) ? P (?ki|ti) ? P? (ei|ti) (14)
Equation (14) uses two different models to estimate
the translation model. If the word, wi is a known
word, ki is set to 1 so the second model is ig-
nored. P? means the maximum likelihood probabil-
ity. P (ki|ti) is the probability of knownness gener-
ated from ti and is estimated by using Good-Turing
estimation (Gale and Samson, 1995). If the word, wi
is an unknown word, ki is set to 0 and the first term
is ignored. ei represents suffix of wi and we used the
last two letters for it.
With the 6 tag language models and the 1 tag-to-
word translation model, we construct 6 HMM mod-
els, among them 2 are traditional first- and second-
hidden Markov models, and 4 are self-organizing
hidden Markov models. Additionally, we used T3,
a tri-gram-based POS tagger in ICOPOST release
1.8.3 for comparison.
The overall performances of the resulting models
estimated from the test corpus are listed in figure 7.
From the leftmost column, it shows the model name,
the contextual features, the target features, the per-
formance and the model size of our 6 implementa-
tions of Markov models and additionally the perfor-
mance of T3 is shown.
Our implementation of the second-order hid-
den Markov model (HMM-P2) achieved a slightly
worse performance than T3, which, we are in-
terpreting, is due to the relatively simple imple-
mentation of our unknown word guessing module4.
While HMM-P2 is a uniformly extended model
from HMM-P1, SOHMM-P2 has been selectively
extended using the same contextual feature. It is
encouraging that the self-organizing model suppress
the increase of the model size in half (2,099Kbyte vs
5,630Kbyte) without loss of performance (96.5%).
In a sense, the results of incorporating word
features (SOHMM-P1W1, SOHMM-P2W1 and
SOHMM-P2W2) are disappointing. The improve-
ments of performances are very small compared to
the increase of the model size. Our interpretation
for the results is that because the distribution of
words is huge, no matter how many words the mod-
els incorporate into context modeling, only a few of
them may actually contribute during test phase. We
are planning to use more general features like word
class, suffix, etc.
Another positive observation is that a homo-
geneous context extension (SOHMM-P2) and a
heterogeneous context extension (SOHMM-P1W1)
yielded significant improvements respectively, and
the combination (SOHMM-P2W1) yielded even
more improvement. This is a strong point of using
decision trees rather than prediction suffix trees.
7 Conclusion
Through this paper, we have presented a framework
of self-organizing Markov model learning. The
experimental results showed some encouraging as-
pects of the framework and at the same time showed
the direction towards further improvements. Be-
cause all the Markov models are represented as de-
cision trees in the framework, the models are hu-
4T3 uses a suffix trie for unknown word guessing, while our
implementations use just last two letters.
?96.6??T3
96.9
96.8
96.3
96.5
96.5
95.6
 	
                 	

24,628KT0P-2, W-1, P-1SOHMM-P2W1
W-2, P-2, W-1, P-1
W-1, P-1
P-2, P-1
P-2, P-1
P-1
T0
T0
T0
T0
T0
14,247KSOHMM-P1W1
35,494K
2,099K
5,630K
123K
SOHMM-P2
SOHMM-P2W2 
HMM-P2
HMM-P1
                
Figure 7: Estimated Performance of Various Models
man readable and we are planning to develop editing
tools for self-organizing Markov models that help
experts to put human knowledge about language into
the models. By adopting ?2-test as the criterion for
potential improvement, we can control the degree of
context extension based on the confidence level.
Acknowledgement
The research is partially supported by Information
Mobility Project (CREST, JST, Japan) and Genome
Information Science Project (MEXT, Japan).
References
L. Rabiner. 1989. A tutorial on Hidden Markov Mod-
els and selected applications in speech recognition. in
Proceedings of the IEEE, 77(2):257?285
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
V. Vapnik. 1998. Statistical Learning Theory. Wiley,
Chichester, UK.
I. Schr o?der. 2001. ICOPOST - Ingo?s Collection
Of POS Taggers. In http://nats-www.informatik.uni-
hamburg.de/?ingo/icopost/.
T. Brants. 1998 Estimating HMM Topologies. In The
Tbilisi Symposium on Logic, Language and Computa-
tion: Selected Papers.
T. Brants. 2000 TnT - A Statistical Part-of-Speech Tag-
ger. In 6?th Applied Natural Language Processing.
H. Sch u?tze and Y. Singer. 1994. Part-of-speech tagging
using a variable memory Markov model. In Proceed-
ings of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
D. Ron, Y. Singer and N. Tishby. 1996 The Power of
Amnesia: Learning Probabilistic Automata with Vari-
able Memory Length. In Machine Learning, 25(2-
3):117?149.
J.-D. Kim, S.-Z. Lee and H.-C. Rim. 1999 HMM
Specialization with Selective Lexicalization. In
Proceedings of the Joint SIGDAT Conference on
Empirical Methods in NLP and Very Large Cor-
pora(EMNLP/VLC99).
F. Pla and A. Molina. 2001 Part-of-Speech Tagging
with Lexicalized HMM. In Proceedings of the Inter-
national Conference on Recent Advances in Natural
Language Processing(RANLP2001).
R. Quinlan. 1986 Induction of decision trees. In Ma-
chine Learning, 1(1):81?106.
R. L o?pez de M a?ntaras. 1991. A Distance-Based At-
tribute Selection Measure for Decision Tree Induction.
In Machine Learning, 6(1):81?92.
R. L o?pez de M a?ntaras, J. Cerquides and P. Garcia. 1998.
Comparing Information-theoretic Attribute Selection
Measures: A statistical approach. In Artificial Intel-
ligence Communications, 11(2):91?100.
F. Jelinek and R. Mercer. 1980. Interpolated estimation
of Markov source parameters from sparse data. In Pro-
ceedings of the Workshop on Pattern Recognition in
Practice.
W. Gale and G. Sampson. 1995. Good-Turing frequency
estimatin without tears. In Jounal of Quantitative Lin-
guistics, 2:217?237
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17?20,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An intelligent search engine and GUI-based efficient MEDLINE search
tool based on deep syntactic parsing
Tomoko Ohta
Yoshimasa Tsuruoka??
Jumpei Takeuchi
Jin-Dong Kim
Yusuke Miyao
Akane Yakushiji?
Kazuhiro Yoshida
Yuka Tateisi?
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{okap, yusuke, ninomi, tsuruoka, akane, kmasuda, tj jug,
kyoshida, harasan, jdkim, yucca, tsujii}@is.s.u-tokyo.ac.jp
Takashi Ninomiya?
Katsuya Masuda
Tadayoshi Hara
Jun?ichi Tsujii
Abstract
We present a practical HPSG parser for
English, an intelligent search engine to re-
trieve MEDLINE abstracts that represent
biomedical events and an efficient MED-
LINE search tool helping users to find in-
formation about biomedical entities such
as genes, proteins, and the interactions be-
tween them.
1 Introduction
Recently, biomedical researchers have been fac-
ing the vast repository of research papers, e.g.
MEDLINE. These researchers are eager to search
biomedical correlations such as protein-protein or
gene-disease associations. The use of natural lan-
guage processing technology is expected to re-
duce their burden, and various attempts of infor-
mation extraction using NLP has been being made
(Blaschke and Valencia, 2002; Hao et al, 2005;
Chun et al, 2006). However, the framework of
traditional information retrieval (IR) has difficulty
with the accurate retrieval of such relational con-
cepts. This is because relational concepts are
essentially determined by semantic relations of
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
This paper proposes a practical HPSG parser
for English, Enju, an intelligent search engine for
the accurate retrieval of relational concepts from
?Current Affiliation:
?School of Informatics, University of Manchester
?Knowledge Research Center, Fujitsu Laboratories LTD.
?Faculty of Informatics, Kogakuin University
?Information Technology Center, University of Tokyo
F-Score
GENIA treebank Penn Treebank
HPSG-PTB 85.10% 87.16%
HPSG-GENIA 86.87% 86.81%
Table 1: Performance for Penn Treebank and the
GENIA corpus
MEDLINE, MEDIE, and a GUI-based efficient
MEDLINE search tool, Info-PubMed.
2 Enju: An English HPSG Parser
We developed an English HPSG parser, Enju 1
(Miyao and Tsujii, 2005; Hara et al, 2005; Ni-
nomiya et al, 2005). Table 1 shows the perfor-
mance. The F-score in the table was accuracy
of the predicate-argument relations output by the
parser. A predicate-argument relation is defined
as a tuple ??,wh, a, wa?, where ? is the predi-
cate type (e.g., adjective, intransitive verb), wh
is the head word of the predicate, a is the argu-
ment label (MOD, ARG1, ..., ARG4), and wa is
the head word of the argument. Precision/recall
is the ratio of tuples correctly identified by the
parser. The lexicon of the grammar was extracted
from Sections 02-21 of Penn Treebank (39,832
sentences). In the table, ?HPSG-PTB? means that
the statistical model was trained on Penn Tree-
bank. ?HPSG-GENIA? means that the statistical
model was trained on both Penn Treebank and GE-
NIA treebank as described in (Hara et al, 2005).
The GENIA treebank (Tateisi et al, 2005) consists
of 500 abstracts (4,446 sentences) extracted from
MEDLINE.
Figure 1 shows a part of the parse tree and fea-
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
17
Figure 1: Snapshot of Enju
ture structure for the sentence ?NASA officials
vowed to land Discovery early Tuesday at one
of three locations after weather conditions forced
them to scrub Monday?s scheduled return.?
3 MEDIE: a search engine for
MEDLINE
Figure 2 shows the top page of the MEDIE. ME-
DIE is an intelligent search engine for the accu-
rate retrieval of relational concepts from MED-
LINE 2 (Miyao et al, 2006). Prior to retrieval, all
sentences are annotated with predicate argument
structures and ontological identifiers by applying
Enju and a term recognizer.
3.1 Automatically Annotated Corpus
First, we applied a POS analyzer and then Enju.
The POS analyzer and HPSG parser are trained
by using the GENIA corpus (Tsuruoka et al,
2005; Hara et al, 2005), which comprises around
2,000 MEDLINE abstracts annotated with POS
and Penn Treebank style syntactic parse trees
(Tateisi et al, 2005). The HPSG parser generates
parse trees in a stand-off format that can be con-
verted to XML by combining it with the original
text.
We also annotated technical terms of genes and
diseases in our developed corpus. Technical terms
are annotated simply by exact matching of dictio-
2http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
nary entries and the terms separated by space, tab,
period, comma, hat, colon, semi-colon, brackets,
square brackets and slash in MEDLINE.
The entire dictionary was generated by apply-
ing the automatic generation method of name vari-
ations (Tsuruoka and Tsujii, 2004) to the GENA
dictionary for the gene names (Koike and Takagi,
2004) and the UMLS (Unified Medical Language
System) meta-thesaurus for the disease names
(Lindberg et al, 1993). It was generated by ap-
plying the name-variation generation method, and
we obtained 4,467,855 entries of a gene and dis-
ease dictionary.
3.2 Functions of MEDIE
MEDIE provides three types of search, seman-
tic search, keyword search, GCL search. GCL
search provides us the most fundamental and pow-
erful functions in which users can specify the
boolean relations, linear order relation and struc-
tural relations with variables. Trained users can
enjoy all functions in MEDIE by the GCL search,
but it is not easy for general users to write ap-
propriate queries for the parsed corpus. The se-
mantic search enables us to specify an event verb
with its subject and object easily. MEDIE auto-
matically generates the GCL query from the se-
mantic query, and runs the GCL search. Figure 3
shows the output of semantic search for the query
?What disease does dystrophin cause??. This ex-
ample will give us the most intuitive understand-
ings of the proximal and structural retrieval with a
richly annotated parsed corpus. MEDIE retrieves
sentences which include event verbs of ?cause?
and noun ?dystrophin? such that ?dystrophin? is the
subject of the event verbs. The event verb and its
subject and object are highlighted with designated
colors. As seen in the figure, small sentences in
relative clauses, passive forms or coordination are
retrieved. As the objects of the event verbs are
highlighted, we can easily see what disease dys-
trophin caused. As the target corpus is already
annotated with diseases entities, MEDIE can ef-
ficiently retrieve the disease expressions.
4 Info-PubMed: a GUI-based
MEDLINE search tool
Info-PubMed is a MEDLINE search tool with
GUI, helping users to find information about
biomedical entities such as genes, proteins, and
18
Figure 2: Snapshot of MEDIE: top page?
Figure 3: Snapshot of MEDIE: ?What disease does
dystrophin cause??
the interactions between them 3.
Info-PubMed provides information from MED-
LINE on protein-protein interactions. Given the
name of a gene or protein, it shows a list of the
names of other genes/proteins which co-occur in
sentences from MEDLINE, along with the fre-
quency of co-occurrence.
Co-occurrence of two proteins/genes in the
same sentence does not always imply that they in-
teract. For more accurate extraction of sentences
that indicate interactions, it is necessary to iden-
tify relations between the two substances. We
adopted PASs derived by Enju and constructed ex-
traction patterns on specific verbs and their argu-
ments based on the derived PASs (Yakusiji, 2006).
Figure 4: Snapshot of Info-PubMed (1)
Figure 5: Snapshot of Info-PubMed (2)
Figure 6: Snapshot of Info-PubMed (3)
4.1 Functions of Info-PubMed
In the ?Gene Searcher? window, enter the name
of a gene or protein that you are interested in.
For example, if you are interested in Raf1, type
?raf1? in the ?Gene Searcher? (Figure 4). You
will see a list of genes whose description in our
dictionary contains ?raf1? (Figure 5). Then, drag
3http://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/
19
one of the GeneBoxes from the ?Gene Searcher?
to the ?Interaction Viewer.? You will see a list
of genes/proteins which co-occur in the same
sentences, along with co-occurrence frequency.
The GeneBox in the leftmost column is the one
you have moved to ?Interaction Viewer.? The
GeneBoxes in the second column correspond to
gene/proteins which co-occur in the same sen-
tences, followed by the boxes in the third column,
InteractionBoxes.
Drag an InteractionBox to ?ContentViewer? to
see the content of the box (Figure 6). An In-
teractionBox is a set of SentenceBoxes. A Sen-
tenceBox corresponds to a sentence in MEDLINE
in which the two gene/proteins co-occur. A Sen-
tenceBox indicates whether the co-occurrence in
the sentence is direct evidence of interaction or
not. If it is judged as direct evidence of interac-
tion, it is indicated as Interaction. Otherwise, it is
indicated as Co-occurrence.
5 Conclusion
We presented an English HPSG parser, Enju, a
search engine for relational concepts from MED-
LINE, MEDIE, and a GUI-based MEDLINE
search tool, Info-PubMed.
MEDIE and Info-PubMed demonstrate how the
results of deep parsing can be used for intelligent
text mining and semantic information retrieval in
the biomedical domain.
6 Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Proc.
of IJCNLP 2005.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid parsing
in probabilistic HPSG parsing. In Proc. of IWPT
2005, pages 103?114.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the
GENIA corpus. In Proc. of the IJCNLP 2005, Com-
panion volume, pp. 222?227.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proc. of ACL ?06,
to appear.
Yoshimasa Tsuruoka, Yuka Tateisi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Part-of-speech tagger for
biomedical text. In Proc. of the 10th Panhellenic
Conference on Informatics.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Asako Koike and Toshihisa Takagi. 2004.
Gene/protein/family name recognition in biomed-
ical literature. In Proc. of HLT-NAACL 2004
Workshop: Biolink 2004, pages 9?16.
D.A. Lindberg, B.L. Humphreys, and A.T. McCray.
1993. The unified medical language system. Meth-
ods in Inf. Med., 32(4):281?291.
Akane Yakushiji. 2006. Relation Information Extrac-
tion Using Deep Syntactic Analysis. Ph.D. Thesis,
University of Tokyo.
20
Encoding Biomedical Resources in TEI: the Case of the GENIA Corpus
Tomaz? Erjavec
Dept. of Intelligent Systems
Joz?ef Stefan Institute, Ljubljana
Yuka Tateisi
CREST
Japan Science and
Technology Corporation
Jin-Dong Kim
Dept. of Information Science
University of Tokyo
Tomoko Ohta
CREST
Japan Science and
Technology Corporation
Jun-ichi Tsujii
CREST JST &
Dept. of Information Science
University of Tokyo
Abstract
It is well known that standardising the
annotation of language resources signifi-
cantly raises their potential, as it enables
re-use and spurs the development of com-
mon technologies. Despite the fact that
increasingly complex linguistic informa-
tion is being added to biomedical texts,
no standard solutions have so far been
proposed for their encoding. This pa-
per describes a standardised XML tagset
(DTD) for annotated biomedical corpora
and other resources, which is based on
the Text Encoding Initiative Guidelines
P4, a general and parameterisable stan-
dard for encoding language resources. We
ground the discussion in the encoding of
the GENIA corpus, which currently con-
tains 2,000 abstracts taken from the MED-
LINE database, and has almost 100,000
hand-annotated terms marked for seman-
tic class from the accompanying ontol-
ogy. The paper introduces GENIA and
TEI and implements a TEI parametrisa-
tion and conversion for the GENIA cor-
pus. A number of aspects of biomedi-
cal language are discussed, such as com-
plex tokenisation, prevalence of contrac-
tions and complex terms, and the linkage
and encoding of ontologies.
1 Introduction
With the growing research on processing texts from
the biomedical domain, the number of resources,
esp. corpora, is increasing rapidly. Such corpora can
be heavily annotated, e.g., with meta-data, words
and part-of-speech tags, named entities, phrases,
terms, concepts, translation equivalents, etc. Cor-
pora are invaluable to the further development of
technologies for utilising the information in biomed-
ical texts, as they provide them with training and
testing data. Given the value of such resources, it
is important to ensure their reusability and increase
their interchange potential ? a step in this direc-
tion is developing common encodings for biomedi-
cal corpora.
Standardisation of resource encoding practices
has now, for some time, been in the forefront of at-
tention. Most of these advances are Web-driven, and
include XML and related recommendations, such as
XSLT, XML Schemas, XPointer, SAX, etc. The
higher level standards, of meta-data (RDF) and on-
tologies (OWL) have been especially influential in
encoding biomedical resources. However, there re-
mains the question how to best encode the structure
of the text themselves, how to mark-up added lin-
guistic analyses, and how to implement linkages be-
tween the text and and further resources, such as lex-
ica, thesauri and ontologies. As discussed in (Ide
and Brew, 2000), in order to qualify as a ?good?
annotated corpus, its encoding should provide for
reusabilty and extensibily.
In this paper we build on previous work (Erjavec
et al, 2003) and show how to develop a standard-
ised encoding for biomedical corpora. We base
our discussion on the case of the GENIA corpus
(Ohta et al, 2002), which is originaly encoded in
GPML, the GENIA Project Markup Language, an
XML DTD. We re-encode the corpus into a stan-
dardised annotation scheme, based on the Text En-
coding Initiative Guidelines P4 (Sperberg-McQueen
and Burnard, 2002), and specify a constructive map-
ping from the original DTD to the developed encod-
ing via a XSLT transformation.
One of the motivations for such an re-encoding
is that TEI is well-designed and widely accepted ar-
chitecture, which has been often used for annotating
language corpora, and by porting to it, GENIA, and
other projects, can gain new insights into possible
encoding practices and maybe make the corpus bet-
ter suited for interchange. As the transformation to
TEI is fully automatic, there is also no need to aban-
don the original markup format (in this case GPML),
which, as it has been crafted specially for the corpus,
provides a tighter encoding than can be possible with
the more general TEI.
The paper thus proposes the creation of a prac-
tical annotation scheme for linguistically annotated
(biomedical) corpora, the conversion to which is
automatic and supports consistency checking and
validation. The paper also serves as a guide to
parametrising TEI and draws attention to certain as-
pects of biomedical corpora which are likely to face
all that wish to process such texts.
The paper is structured as follows: Section 2 in-
troduces the GENIA corpus; Section 3 introduces
the TEI, gives some pros and cons of using it,
and the method of parametrising TEI for particular
projects; Section 4 discusses such a parametrisation
for biomedical corpora and explains the conversion
of the GENIA corpus to TEI; Section 5 discusses
some challenging properties of biomedical text an-
notations; finally, Section 6 offers some conclusions
and directions for further work.
2 The GENIA Corpus
The GENIA corpus (Ohta et al, 2002) is be-
ing developed in the scope of the GENIA project,
which seeks to develop information extraction tech-
niques for scientific texts using NLP technol-
ogy. The corpus consists of semantically anno-
tated published abstracts from the biomedical do-
main. The corpus is a collection of articles ex-
tracted from the on-line MEDLINE abstracts (U.S.
National Center for Biotechnology Information,
http://www.ncbi.nlm.nih.gov/, PubMed database).
Since the focus of the corpus is on biological re-
actions concerning transcription factors in human
blood cells, articles were selected that contain the
MeSH terms human, blood cell and transcription
factor.
As usual for the field, the articles are composed
largely of structurally very complex technical terms,
and are almost incomprehensible to a layperson. A
typical heading e.g., reads IL-2 gene expression and
NF-kappa B activation through CD28 requires reac-
tive oxygen production by 5-lipoxygenase.
The main value of the GENIA corpus comes from
its annotation: all the abstracts and their titles have
been marked-up by two domain experts for bio-
logically meaningful terms, and these terms have
been semantically annotated with descriptors from
the GENIA ontology.
The GENIA ontology is a taxonomy of, currently,
47 biologically relevant nominal categories, such as
body part, virus, or RNA domain or region; the tax-
onomy has 35 terminal categories.
The terms of the corpus are semantically de-
fined as those sentence constituents that can be cate-
gorised using the terminal categories from the ontol-
ogy. Syntactically such constituents are quite varied:
they include qualifiers and can be recursive.
The GENIA corpus is encoded in the Genia
Project Markup Language. The GPML is an XML
DTD (Kim et al, 2001) where each article con-
tains its MEDLINE ID, title and abstract. The texts
of the abstracts are segmented into sentences, and
these contain the constituents with their semantic
classification. The GENIA ontology is provided to-
gether with the GENIA corpus and is encoded in
DAML+OIL (http://www.daml.org/ ), the standard
XML-based ontology description language. This
structure and its annotation will be further discussed
below.
A suite of supporting tools has been developed or
tuned for the GENIA corpus and GPML: the term
annotation is performed with the XMLMind editor;
an XPath-based concordancer has been developed
for searching the corpus; and CSS stylesheets are
available for browsing it.
At the time of writing, the latest version of the
GENIA corpus is 3.01, which has been released
in April 2003. It consists of 2,000 abstracts with
over 400,000 words and more than 90,000 marked-
up terms. This version has not yet been marked-
up with tokens or PoS information, although an
earlier version (Genia-V3.0p) has been. The GE-
NIA corpus is available free of charge from the GE-
NIA project homepage, at http://www-tsujii.is.s.u-
tokyo.ac.jp/GENIA/.
3 The Text Encoding Initiative
The Text Encoding Initiative was established in
1987 as a systematised attempt to develop a fully
general text encoding model and set of encoding
conventions based upon it, suitable for processing
and analysis of any type of text, in any language,
and intended to serve the increasing range of ex-
isting (and potential) applications and uses. The
TEI Guidelines for Electronic Text Encoding and
Interchange were first published in April 1994 in
two substantial green volumes, known as TEI P3.
In May 1999, a revised edition of TEI P3 was
produced, correcting several typographic and other
errors. In December 2000 the TEI Consortium
(http://www.tei-c.org/ ) was set up to maintain and
develop the TEI standard. In 2002, the Consortium
announced the availability of a major revision of TEI
P3, the TEI P4 (Sperberg-McQueen and Burnard,
2002) the object of which is to provide equal sup-
port for XML and SGML applications using the TEI
scheme. The revisions needed to make TEI P4 have
been deliberately restricted to error correction only,
with a view to ensuring that documents conforming
to TEI P3 will not become illegal when processed
with TEI P4. For GENIA, we are using the XML-
compatible version of TEI P4.
In producing P4, many possibilities for other,
more fundamental changes have been identified.
With the establishment of the TEI Council, it be-
came possible to agree on a programme of work to
enhance and modify the Guidelines more fundamen-
tally over the coming years. TEI P5 will be the next
full revision of the Guidelines. The work on P5 has
started, and the date of its appearance will likely be
in 2004 and there are currently several TEI Working
Groups addressing various parts of the Guidelines
that need attention.
More than 80 projects spanning over 30 languages
have so far made use of the TEI guidelines, pro-
ducing diverse resources, e.g., text-critical editions
of classical works. TEI has also been influential
in corpus encoding, where the best known exam-
ple is probably the British National Corpus. How-
ever, while the TEI has been extensively used for
annotating PoS tagged corpora, it been less popu-
lar for encoding texts used by the the Information
Retrieval/Extraction community; here, a number of
other initiatives have taken the lead in encoding, say,
ontologies or inter-document linking.
3.1 Pros and cons of using TEI
Why, if a corpus is already encoded in XML using
a home-grown DTD, to re-encoded it in TEI at all?
One reasons is certainly the validation aspect of the
exercise: re-coding a corpus, or any other resource,
reveals hidden (and in practice incorrect) assump-
tions about its structure. Re-coding to a standard
recommendation also forces the corpus designers to
face issues which might have been overlooked in the
original design.
There are also other advantages of using TEI as
the interchange format: (1) it is a wide-coverage,
well-designed (modular and extensible), widely ac-
cepted and well-maintained architecture; (2) it pro-
vides extensive documentation, which comprises not
only the Guidelines but also papers and documen-
tation (best practices) of various projects; (3) it of-
fers community support via the tei-l public discus-
sion list; (4) various TEI-dedicated software already
exists, and more is likely to become available; and
(5) using it contributes to the adoption of open stan-
dards and recommendations.
However, using a very general recommendation
which tries to cater for any possible situation brings
with it also several disadvantages:
Tag abuse TEI might not have elements / attributes
with the exact meaning we require. This re-
sults in a tendency to misuse tags for purposes
they were not meant for; however, it is a case
of individual judgement to decide whether to
(slightly) abuse a tag, or to implement a lo-
cal extension to add the attribute or element re-
quired.
Tag bloat Being a general purpose recommenda-
tion, TEI can ? almost by definition ? never
be optimal for a specific application. Thus a
custom developed DTD will be leaner, have
less (redundant) tags and simpler content mod-
els.
TEI for humanities While the Guidelines cover a
vast range of text types and annotations, they
are maybe the least developed for ?high level?
NLP applications or have failed to keep abreast
of ?cutting-edge? initiatives. As will be seen,
critical areas are the encoding of ontologies, of
lexical databases and of feature structures.
3.2 Building the TEI DTD
The TEI Guidelines (Sperberg-McQueen and
Burnard, 2002) consist of the formal part, which
is a set of SGML/XML DTD fragments, and the
documentation, which explains the rationale behind
the elements available in these fragments, as well as
giving overall information about the structure of the
TEI.
The formal SGML/XML part of TEI comes as a
set of DTD fragments or tagsets. A TEI DTD for a
particular application is then constructed by select-
ing an appropriate combination of such tagsets. TEI
distinguishes the following types of tagsets:
Core tagset : standard components of the TEI main
DTD in all its forms; these are always included
without any special action by the encoder.
Base tagsets : basic building blocks for specific text
types; exactly one base must be selected by the
encoder, unless one of the combined bases is
used.
Additional tagsets : extra tags useful for particular
purposes. All additional tagsets are compatible
with all bases and with each other; an encoder
may therefore add them to the selected base in
any combination desired.
User defined tagsets : these extra tags give the pos-
sibility of extending and overriding the defi-
nitions provided in the TEI tagset. Further-
more, they give the option of explicitly includ-
<!DOCTYPE teiCorpus.2 SYSTEM
"http://www.tei-c.org/P4X/DTD/tei2.dtd"
[<!ENTITY % TEI.XML "INCLUDE">
<!ENTITY % TEI.prose "INCLUDE">
<!ENTITY % TEI.linking "INCLUDE">
<!ENTITY % TEI.analysis "INCLUDE">
<!ENTITY % TEI.corpus "INCLUDE">
<!ENTITY % TEI.extensions.ent SYSTEM
?geniaex.ent?>
<!ENTITY % TEI.extensions.dtd SYSTEM
?geniaex.dtd?>
]>
Figure 1: The XML TEI prolog for GENIA
ing or ignoring (disallowing) each particular el-
ement licensed by the chosen base and addi-
tional tagsets.
While a project-particular XML DTD can be con-
structed by including and ignoring the TEI DTD
fragments directly (as exemplified in Figure 1), it is
also possible to build ? for easier processing ? a
one-file DTD with the help of the on-line TEI Pizza
Chef service, available from the TEI web site.
4 Parametrising TEI for biomedical
corpora
In previous work (Erjavec et al, 2003) we have al-
ready proposed a TEI parametrisation of GENIA
which was quite broad in its scope. Because a num-
ber of tagsets could prove useful in the long term
this parametrisation collected not only those that we
considered necessary for the current version of GE-
NIA, but also some that might prove of service in the
future. Furthermore, we supported the encoding of
both version 2.1 and 3.0 of the corpus. The resulting
DTD was thus very generous in what kinds of data it
caters for. To focus the discussion we, in the current
paper, only address tagset that are immediately rele-
vant to annotating biomedical texts. In Figure 1 we
define the XML DTD that can be used for encoding
biomedical resources, and that we used for GENIA
V3.01. The XML prolog given in this Figure defines
that ?teiCorpus.2? is the root element of the corpus,
that the external DTD resides at the given URL be-
longing to the TEI Consortium, and that a number
of TEI modules, detailed below, are being used to
parametrise the TEI to arrive at our particular DTD.
4.1 TEI.XML
TEI P4 allows both standard SGML and XML en-
codings. Including the TEI.XML option indicates
that the target DTD is to be expressed in XML.
4.2 TEI.prose
The base tagset does not declare many elements but
rather inherits all of the TEI core, which includes the
TEI header, and text elements. A TEI document will
typically have as its root element ?TEI.2? which is
composed of the ?teiHeader?, followed by the ?text?;
c.f. right hand side of Figure 2, but note that the root
element from the TEI.corpus module is used for the
complete corpus.
The TEI header describes an encoded work so that
the text (corpus) itself, its source, its encoding, and
its revisions are all thoroughly documented.
TEI.prose also contains elements and attributes
for describing text structure, e.g. ?div? for text divi-
sion, ?p? for paragraph, ?head? for text header, etc.
The tagset is therefore useful for encoding the gross
structure of the corpus texts; for an illustration again
see Figure 2.
4.3 TEI.linking
This additional tagset provides mechanisms for link-
ing, segmentation, and alignment. The elements
provided here enable links to be made e.g., between
the articles and their source URLs, or between con-
cepts and their hypernyms.
It should be noted that while the TEI treatment
of external pointers had been very influential, it was
overtaken and made obsolete by newer recommen-
dations. However, the TEI does have a Working
Group on Stand-Off Markup, XLink and XPointer,
which should produce new TEI encoding recom-
mendations for this area in 2003.
4.4 TEI.analysis
This additional tagset is used for associating sim-
ple linguistic analyses and interpretations with text
elements. It can be used to annotate words, ?w?,
clauses, ?cl?, and sentences, ?s? with dedicated tags,
as well as arbitrary and possibly nested segments
with the ?seg?. Such elements can be, via at-
tributes, associated with their analyses. This tagset
has proved very popular for PoS-annotated corpora;
for an illustration see Figure 3.
4.5 TEI.corpus
This additional tagset introduces a new root element,
?teiCorpus.2?, which comprises a (corpus) header
and a series of ?TEI.2? elements. The TEI.corpus
tagset alo extends the certain header elements to
provide more detailed descriptions of the corpus ma-
terial.
4.6 TEI.extensions.ent
The file gives, for each element sanctioned by the
chosen modules, whether we include or ignore it in
our parametrisation. While this is not strictly neces-
sary (without any such specification, all the elements
would be included) we thought it wise to constrain
the content models somewhat, to reduce the bewil-
dering variety of choices that the TEI otherwise of-
fers. Also, such an entity extension file gives the
complete list of all the TEI elements that are allowed
(and disallowed) in GENIA, which might prove use-
ful for documentation purposes.
4.7 TEI.extensions.dtd
This file specifies the changes we have made to TEI
elements. We have e.g., added the url attribute to
?xptr? and ?xref ? and tagging attributes to word and
punctuation elements.
4.8 Conversion of GPML to TEI
Because the source format of GENIA will remain
the simpler GPML, it is imperative to have an au-
tomatic procedure for converting to the TEI inter-
change format. The translation process takes advan-
tage of the fact that both the input and output are
encoded in XML, which makes it possible to use the
XSL Transformation Language, XSLT that defines a
standard declarative specification of transformations
between XML documents. There also exist a num-
ber of free XSLT processors; we used Daniel Veil-
lard?s xsltproc.
The transformation is written as a XSLT
stylesheet, which makes reference to two docu-
ments: the GENIA ontology in TEI and the template
for the corpus header. The stylesheet then resolves
the GPML encoded corpus into TEI. The translation
of the corpus is thus fully automatic, except for the
taxonomy, which was translated by hand.
Figure 2 illustrates the top level structure of the
corpus, and how it differs between the GPML and
TEI encodings. The most noticeable difference is,
apart from the renaming of elements, the addition
of headers to the corpus and texts. In the GENIA
?teiHeader? we give e.g., the name, address, avail-
ability, sampling description, and, for each abstract?s
?sourceDesc?, two ?xptr?s: the first gives the URL of
the HTML article in the MEDLINE database, while
the second is the URL of the article in the origi-
nal XML. It should be noted that we use a locally
defined url attribute for specifying the value of the
pointer.
5 Characteristics of biomedical texts
In this section we review some challenges that
biomedical texts present to the processing and en-
coding of linguistic information, and the manner of
their encoding in our DTD.
5.1 Tokens
Tokenisation, i.e., the identification of words and
punctuation marks, is the lowest level of linguistic
analysis, yet is, in spite (or because) of this of con-
siderable importance. As all other levels of linguis-
tic markup make direct or direct reference to the to-
ken stream of the text, so if this is incorrect, errors
will propagate to all other annotations.
It is also interesting to note that current annota-
tion practice is more and more leaning toward stand-
off markup, i.e., annotations that are separated from
the primary data (text) and make reference to it only
via pointers. However, it is beneficial to have some
markup in the primary data to which it is possible to
refer, and this markup is, almost exclusivelly, that of
tokens; see e.g., (Freese et al, 2003).
Version V1.1 of GENIA has been also annotated
with LTG tools (Grover et al, 2002). In short, the
corpus is tokenised, and then part-of-speech tagged
with two taggers, each one using a different tagset,
and the nouns and verbs lemmatised. Additionally,
the deverbal nominalisations are assigned their ver-
bal stems.
The conversion to TEI is also able to handle this
additional markup, by using the TEI.analysis mod-
ule. The word and punctuation tokens are encoded
as ?w? and ?c? elements respectively, which are fur-
ther marked with type and lemma and the locally de-
fined c1, c2 and vstem. An example of such markup
<s>
<w c1="DT" c2="DB">All</w>
<c type="HYPH" c1=":" c2="-">-</c>
<w c1="VBZ" c2="JJ">trans</w>
<w c1="JJ" c2="JJ">retinoic</w>
<w lemma="acid" c1="NN" c2="NN1">acid</w>
<c type="BR" c1="(" c2="(">(</c>
<w lemma="Ra" c1="NN" c2="NP1">RA</w>
<c type="BR" c1=")" c2=")">)</c>
<w lemma="be" c1="VBZ" c2="VBZ">is</w>
<w c1="DT" c2="AT1">an</w>
<w c1="JJ" c2="JJ">important</w>
...
Figure 3: TEI encoding of annotated tokens
is given in Figure 3.
Given the high density of technical terms,
biomedical texts are rife with various types of con-
tractions, such as abbreviations, acronyms, prefixes,
etc. As seen already in Figure 3, one of the
more problematic apects of tokenisaton are paren-
theses. Almost all tokenisers (e.g., the LT one, or
the UPENN tokeniser) take these as separate tokens,
but many are in biomedical texts parts of terms. So,
out of almost 35,000 distinct terms that have been
marked up in the GENIA corpus, over 1,700 con-
tain parentheses. Some examples: (+)-pentazocine,
(3H)-E2 binding, (gamma(c))-like molecule.
Correct tokenisation of the biomedical texts is
thus a challenging tasks, and it is fair to say that,
from a linguistic processing perspective, complex
tokenisation is one of the defining characteristics of
such corpora.
5.2 Terms
Annotation of terms is a prerequisite for meaningful
processing of biomedical texts, yet it is often diffi-
cult to decide what constitutes a term in a text, and
how to abstract away from local variations. Biomed-
ical texts are largerly (one could almost say excu-
sivelly) composed of terms, and, as mentioned, this
brings with it complex abbreviatory mechanisms.
Even though TEI offers a ?term? element, we
chose, in line with the original GPML encoding, to
rather use the TEI.analysis clause (?cl?) element to
encode terms. In GENIA, the terms have been hand-
annotated, and marked up with concepts from the
GENIA ontology; this was also the defining factor
of term-hood, namely that the term could be linked
<!DOCTYPE set SYSTEM "gpml.dtd"> <!DOCTYPE teiCorpus.2 SYSTEM "genia-tei.dtd">
<set> <TEIcorpus.2>
<article> <teiHeader type="corpus">
<articleinfo><bibliomisc> *Corpus_header*</teiHeader>
*MEDLINE_ID* <TEI.2 id="*MEDLINE_ID*">
</bibliomisc></articleinfo> <teiHeader type="text">
<title> *Article_header*</teiHeader>
*Title_of_article* <text><body>
</title> <div type="abstract">
<abstract> <head>*Title_of_article*</head>
*Abstract_of_article* <p>*Abstract_of_article*</p>
</abstract> </div>
</article> </body></text></TEI.2>
*More_articles* *More_articles*
</set> </TEIcorpus.2>
Figure 2: The GPML and TEI structure of the corpus
to a terminal concept of the GENIA ontology.
In spite of the simple semantic definition, the syn-
tactic structure of the terms in the corpus varies
dramatically. Biomedical terms are in some ways
similar to named entities (names of people, orga-
nizations, etc.) but from the linguistic perspective,
they are different in that named entities are mostly
proper nouns, while terms mostly contain common
nouns, and the two differ in their syntactic proper-
ties. Terms in the corpus can also be nested, where
complex terms are composed out of simpler ones,
e.g., ?cl??cl?IL-2 gene?/cl? transcription?/cl?.
This nesting, and the reference to ontology con-
cepts is often far from simple, as (partial) terms can
appear in coordinated clauses involving ellipsis. For
example, ?CD2 and CD 25 receptors? refers to two
terms, CD2 receptors and CD25 receptors, but only
the latter actually appears in the text.
In such cases by parsing the coordination
all the terms can be identified and annotated;
the TEI encoding achieves this by specifyng
the propositional formula involving the par-
ticipating concepts in the function attribute;
for example, ?cl function=?(AND G.tissue
G.tissue)? ana=?G.tissue???cl?normal?/cl? and
?cl?hypopigmented?/cl? ?cl?skin samples?/cl??/cl?.
The ana attribute encodes the IDREF of the con-
cept; currently, only same valued concepts are either
conjoined or disjoined.
The number of ?cl? elements in the GENIA cor-
pus is 96,582, among which 89,682 are simple terms
and 1,583 are nested terms that are contain 3,431
terms. 5,137 terms do not yet have the ana attribute
for concept identification, so the total number of
ontology-linked terms is 93,293.
5.3 Ontologies
One of the more interesting questions in recoding
GENIA in TEI was how to encode the ontology. The
ontology is in GENIA GPML encoded in a separate
document, conforming to the OIL+DAML specifi-
cation. This, inter alia, means that that XML file
heavily relies on XML Namespaces and the RDF
recommendation. An illustrative fragment is given
on the left side of Figure 4.
Currently the GENIA ontology has a simple tree-
like structure, i.e., it corresponds to a taxonomy,
so we translated it to the TEI ?taxonomy? element,
which is contained in the ?classDecl? of the header
?encodingDesc?. The TEI defines this element
as ?[the classification declaration] contains one or
more taxonomies defining any classificatory codes
used elsewhere in the text?, i.e., is exactly suited for
our purposes.
There are quite substantial differences between
the two encodings: the DAML+OIL models class
inclusion with links, while the TEI does it as XML
element inclusion. This is certainly the simpler and
more robust solution, but requires that the ontol-
ogy is a taxonomy, i.e., tree structured. The sec-
ond difference is in the status of the identifiers: in
DAML+OIL they are general #CDATA links, which
need a separate (XLink/XPointer) mechanisms for
their resolution. In TEI they are XML ID attributes,
<daml:Class rdf:ID="source"></daml:Class> <taxonomy id="G.taxonomy">
<daml:Class rdf:ID="natural"> <category id="G.source">
<rdfs:subClassOf rdf:resource="#source"/> <catDesc>biological source</catDesc>
</daml:Class> <category id="G.natural">
<daml:Class rdf:ID="organism"> <catDesc>natural</catDesc>
<rdfs:subClassOf rdf:resource="#natural"/> <category id="G.organism">
</daml:Class> <catDesc>organism</catDesc>
<daml:Class rdf:ID="multi_cell"> <category id="G.multi_cell">
<rdfs:subClassOf rdf:resource="#organism"/> <catDesc>multi-cellular</catDesc>
</daml:Class> </category>
... ...
Figure 4: The GENIA DAML+OIL and TEI ontology
and can rely on the XML parser to resolve them.
While this is a simpler solution, it does support
document-internal reference only.
6 Conclusions
The paper proposed an XML paramterisation of TEI
P4 developed for linguistically annotated biomedi-
cal corpora, and applied it to the GENIA corpus.
The conversion from the Genia Project Markup Lan-
guage to this encoding has been implemented in
XSLT and both the TEI-conformant parametrisation
(TEI extension file and one-file DTD) and the XSLT
stylesheets are, together with a report documenting
them, available at http://nl.ijs.si/et/genia/, while the
GENIA corpus is freely available from http://www-
tsujii.is.s.u-tokyo.ac.jp/GENIA/.
The paper gave a survey of the TEI modules that
can be useful for encoding a wide variety of linguis-
tically annotated corpora. This contribution, it is
hoped, can thus serve as a blueprint for parametris-
ing TEI for diverse corpus resources.
Further work involves the inclusion of other
knowledge sources into the corpus, say of Medi-
cal Subject Headings (MeSH), Unified Medical Lan-
guage System (UMLS), International Classification
of Disease (ICD), etc. The place of these annota-
tions in the corpus will have to be considered, and
their linking to the existing information determined.
References
Tomaz? Erjavec, Jin-Dong Kim, Tomoko Ohta, Yuka
Tateisi, and Jun ichi Tsujii. 2003. Stretching the TEI:
Converting the GENIA corpus. In Proceedings of the
EACL-03 Workshop on Linguistically Interpreted Cor-
pora (LINC-03), pages 117?124, Budapest. ACL.
Marion Freese, Ulrich Heid, and Martin Emele. 2003.
Enhancing XCES to XCOMFORT: An Extensible
Modular Architecture for Manipulation of Text Re-
sources. In Proceedings of the EACL-03 Workshop
on Language Technology and the Semantic Web: 3rd
Workshop on NLP and XML (NLPXML-2003), pages
33?40, Budapest. ACL.
Claire Grover, Ewan Klein, Alex Lascarides, and Maria
Lapata. 2002. XML-based NLP Tools for Analysing
and Annotating Medical Language. In 2nd Workshop
on NLP and XML (CoLing Workshop NLPXML-2002).
http://www.ltg.ed.ac.uk/software/ttt/.
Nancy Ide and Chris Brew. 2000. Requrements, Tools
and Architectures for Annotated Corpora. In Proceed-
ings of Data Architectures and Software Support for
Large Corpora, pages 1?5, Budapest. ELRA.
Jin-Dong Kim, Tomoko Ohta, and Jun-ichi Tsujii. 2001.
XML-based Linguistic Annotation of Corpus. In Pro-
ceedings of the first NLP and XML Workshop, pages
44?53.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.
The GENIA Corpus: an Annotated Research Abstract
Corpus in Molecular Biology Domain. In Proceedings
of the Human Language Technology Conference, page
To appear.
C. M. Sperberg-McQueen and Lou Burnard, editors.
2002. Guidelines for Electronic Text Encoding and
Interchange, The XML Version of the TEI Guidelines.
The TEI Consortium. http://www.tei-c.org/.
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 90?91,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Prediction of Protein Sub-cellular Localization
using Information from Texts and Sequences
Hong-Woo Chun1,2,3 Chisato Yamasaki2,3 Naomi Saichi2,3 Masayuki Tanaka2,3
chun@dbcls.rois.ac.jp, {chisato-yamasaki, nao-saichi, masa-tanaka}@aist.go.jp
Teruyoshi Hishiki3 Tadashi Imanishi3,5 Takashi Gojobori3,6
{t-hishiki, t.imanishi, t-gojobori}@aist.go.jp
Jin-Dong Kim4 Jun?ichi Tsujii4,7,8 Toshihisa Takagi1,9
{jdkim, tsujii}@is.s.u-tokyo.ac.jp, takagi@dbcls.rois.ac.jp
1 Database Center for Life Science, Research Organization of Information and System,
Engineering 12th Bldg., University of Tokyo, 2-11-16 Yayoi, Bunkyo-ku, Tokyo, 113-0032, Japan
2 Japan Biological Information Research Center, Japan Biological Informatics Consortium
3 Biological Information Research Center,
National Institute of Advanced Industrial Science and Technology, Japan
4 Department of Computer Science, University of Tokyo, Japan
5 Graduate School of Information Science and Technology, Hokkaido University, Japan
6 Center for Information Biology and DNA Data Bank of Japan, National Institute of Genetics
7 School of Informatics, University of Manchester, UK
8 National Centre for Text Mining, UK
9 Department of Computational Biology, University of Tokyo, Japan
Abstract
This paper presents a novel prediction ap-
proach for protein sub-cellular localization. We
have incorporated text and sequence-based ap-
proaches.
1 Introduction
Natural Language Processing (NLP) has tackled and
solved a lot of prediction problems in Biology. One
practical research issue is Protein Sub-Cellular Lo-
calization (PSL) Prediction. Many previous ap-
proaches have combined information from both texts
and sequences by a machine learning (ML) technique
(Shatkay et al, 2007). All of them have not used tra-
ditional NLP techniques such as parsing. Our aim
is to develop a novel PSL prediction system using
information from texts and sequences. At the same
time, we demonstrated the effectiveness of the tra-
ditional NLP and the sequence-based features in the
viewpoint of the text-based approach.
2 Methodology
A Maximum Entropy-based ML technique has been
used to combine information from both texts and se-
quences. To develop a supervised ML-based predic-
tion system, an annotated corpus is needed to train
the system. However, there is no publicly available
corpus that contains the PSL. Therefore, we have
constructed a corpus using GENIA corpus as an ini-
tial data, because the annotation of Protein and Cel-
lular component in GENIA corpus is already done
by human experts. The new types of annotation con-
tain two tasks. The first annotation is to classify
1,117 cellular components in GENIA corpus into 11
locations, and the second annotation is to catego-
rize a relation between a protein and a location into
positive, negative, and neutral. Biologists selected
11 locations based on Gene Ontology: Cytoplasm,
Cytoskeleton, Endoplasmic reticulum, Extracellular,
Golgi apparatus, Granule, Lysosome, Mitochondria,
Nucleus, Peroxisome, and Plasma membrane. The
number of co-occurrences in GENIA corpus is 864.
1 Three human experts annotated with 79.49% of
inter-annotator agreement. For calculating the inter-
annotator agreement, all annotators annotated 117
1The co-occurrence in the proposed approach is a sentence
that contains at least one pair of protein and cellular component
names.
90
# Relevant Performance : F-score (Precision, Recall)
Location relations Baseline Text Sequence Text + Sequence
Nucleus 173 0.282 (0.164, 1.0) 0.764 (0.736, 0.794) 0.725 (0.569, 1.000) 0.778 (0.758, 0.798)
Cytoplasm 94 0.163 (0.089, 1.0) 0.828 (0.804, 0.852) 0.788 (0.657, 0.984) 0.828 (0.804, 0.852)
Plasma membrane 23 0.043 (0.022, 1.0) 0.875 (0.814, 0.946) 0.857 (0.766, 0.973) 0.885 (0.841, 0.932)
Table 1: Performance of protein sub-cellular localization prediction for each location.
co-occurrences. From the texts, we used eight fea-
tures: (1) protein and cellular component names an-
notated by human experts, (2) adjacent one and two
words of names, (3) bag of words, (4) order of names,
(5) distance between names, (6) syntactic category
of names, (7) predicates of names, and (8) part-of-
speech of predicates. To analyze the syntactic struc-
ture, we used the ENJU full parser whose output is
predicate-argument structures of a sentence.
To combine the information from sequences, we
attempted to predict PSL for all proteins in GE-
NIA corpus by two existing sequence-based meth-
ods: WoLF PSORT (Horton et al, 2006) and SOSUI
(Hirokawa et al, 1998). Approximately 14% of pro-
tein names in GENIA corpus obtained results. From
the sequences, we used two features: (1) existence
of the sequence-based results, and (2) the number of
sequence-based results.
3 Experimental results and Conclusion
The proposed approach has integrated text and
sequence-based approaches. To evaluate the system,
we performed 10-fold cross validation using 864 co-
occurrences including positive, negative, and neutral
relations. We measured the precision, recall, and
F-score of the system for all experiments. Among
864 co-occurrences in GENIA corpus, 301 positive
or negative co-occurrences have been considered as
relevant relations, and the remaining 563 neutral re-
lations have been considered as irrelevant relations.
Four approaches have been compared based on
three locations in Table 1. The four approaches are
baseline, text-based approach, sequence-based ap-
proach, and integration of the text and sequence-
based approaches. Baseline experiment used an as-
sumption: there is a relevant relation if a protein and
a cellular component names occur together in a co-
occurrence. The three locations selected when there
are the sequence-based results and the number of rel-
evant relations is more than one. All experiments
showed that the integration of text and sequence-
based approaches is the best, even though the exper-
iments for Cytoplasm showed the best performance
at both the text-based approach and the integration
approach.
A new prediction method has been developed for
protein sub-cellular localization, and it has integrated
text and sequence-based approach using an ML tech-
nique. The traditional NLP techniques contributed
to improve performance of the text-based approach,
and the text and sequence-based approaches recipro-
cally contributed to obtain a improved PSL predic-
tion method. The newly constructed corpus will be
included in the next version of GENIA corpus. There
are weak points in the proposed approach. The cur-
rent evaluation method has been focusing on eval-
uating the text-based approach, and the results of
the sequence-based approach were obtained for only
14% of proteins in GENIA corpus, so these situations
might be the reason that the sequence-based approach
did contribute a little. Thus, we need to evaluate the
proposed approach with a more reasonable method.
Acknowledgments
We acknowledge Fusano Todokoro for her technical
assistance.
References
Paul Horton, Keun-Joon Park, Takeshi Obayashi and
Kenta Nakai. 2006. Protein Subcellular Localization
Prediction with WoLF PSORT. Asia Pacific Bioinfor-
matics Conference (APBC), pp. 39?48.
Takatsugu Hirokawa, Seah Boon-Chieng and Shigeki Mi-
taku. 1998. SOSUI: classification and secondary
structure prediction system for membrane proteins.
Bioinformatics, 14(4): pp. 378?379.
Hagit Shatkay, Annette Ho?glund, Scott Brady, Torsten
Blum, Pierre Do?nnes and Oliver Kohlbacher. 2007.
SherLoc: high-accuracy prediction of protein subcellu-
lar localization by integrating text and protein sequence
data. Bioinformatics., 23(11): pp. 1410?1417
91
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 118?119,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Raising the Compatibility of Heterogeneous Annotations:
A Case Study on Protein Mention Recognition
Yue Wang? Kazuhiro Yoshida? Jin-Dong Kim? Rune S?tre? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?National Center for Text Mining
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{wangyue, kyoshida, jdkim, rune.saetre, tsujii}@is.s.u-tokyo.ac.jp
Abstract
While there are several corpora which claim
to have annotations for protein references,
the heterogeneity between the annotations is
recognized as an obstacle to develop expen-
sive resources in a synergistic way. Here we
present a series of experimental results which
show the differences of protein mention an-
notations made to two corpora, GENIA and
AImed.
1 Introduction
There are several well-known corpora with protein
mention annotations. It is a natural request to bene-
fit from the existing annotations, but the heterogene-
ity of the annotations remains an obstacle. The het-
erogeneity is caused by different definitions of ?pro-
tein?, annotation conventions, and so on.
It is clear that by raising the compatibility of an-
notations, we can reduce the performance degrada-
tion caused by the heterogeneity of annotations.
In this work, we design several experiments to
observe the effect of removing or relaxing the het-
erogeneity between the annotations in two corpora.
The experimental results show that if we understand
where the difference is, we can raise the compati-
bility of the heterogeneous annotations by removing
the difference.
2 Corpora and protein mention recognizer
We used two corpora: the GENIA corpus (Kim
et al, 2003), and the AImed corpus (Bunescu and
Mooney, 2006). There are 2,000 MEDLINE ab-
stracts and 93,293 entities in the GENIA corpus.
?
??
??
??
??
??
??
??
??
??
?? ?? ?? ?? ??? ??? ??? ???????
???????????????????
???
????
Proceedings of the Workshop on BioNLP, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Static Relations: a Piece in the Biomedical Information Extraction Puzzle
Sampo Pyysalo? Tomoko Ohta? Jin-Dong Kim? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{smp,okap,jdkim,tsujii}@is.s.u-tokyo.ac.jp
Abstract
We propose a static relation extraction task to
complement biomedical information extrac-
tion approaches. We argue that static re-
lations such as part-whole are implicitly in-
volved in many common extraction settings,
define a task setting making them explicit, and
discuss their integration into previously pro-
posed tasks and extraction methods. We fur-
ther identify a specific static relation extrac-
tion task motivated by the BioNLP?09 shared
task on event extraction, introduce an anno-
tated corpus for the task, and demonstrate the
feasibility of the task by experiments showing
that the defined relations can be reliably ex-
tracted. The task setting and corpus can serve
to support several forms of domain informa-
tion extraction.
1 Introduction
Relation Extraction (RE) is a key task in biomedi-
cal Information Extraction (IE). The automatic de-
tection of relevant types of relations ? for various
definitions of relevant ? between entities has been
one of the primary focus points for significant do-
main research efforts over the past decade, and a
substantial number of biomedical RE methods and
annotated corpora have been published (Zweigen-
baum et al, 2007). Motivated by the needs of biolo-
gists and e.g. database curation efforts, most domain
RE efforts target relations involving biologically rel-
evant changes in the involved entities, commonly to
the complete exclusion of static relations. However,
static relations such as entity membership in a fam-
ily and one entity being a part of another are not only
relevant IE targets in themselves but can also play an
important supporting role in IE systems not primar-
ily targeting them.
In this paper, we investigate the role of static re-
lations in causal RE and event extraction. Here,
we use relation extraction in the MUC and ACE
(Sundheim, 1995; Doddington et al, 2004) sense to
refer to the task of extracting binary relations, or-
dered pairs of entities, where both participating enti-
ties must be specified and their roles (agent, patient,
etc.) are fixed by the relation. By contrast, event ex-
traction is understood to involve events (things that
happen) and representations where the number and
roles of participants may vary more freely. We re-
fer to relations where one one entity causes another
to change as causal relations; typical domain exam-
ples are phosphorylation and activation. Static rela-
tions, by contrast, hold between two entities without
implication of change or causality: examples from
the ACE IE task include Physical.Located and Part-
Whole.Artifact.
2 Task definition
In the following, we argue that static relations are
relevant to much of current biomedical IE work,
present a task setting making these relations explicit,
and discuss applications of static relation annotation
and extraction methods.
2.1 Named entity-driven IE and static relations
Named entities (NEs) provide a simple anchor con-
necting text to entities in the real world and thus a
natural starting point for IE. Named entity recog-
nition (NER) is well studied and several biomed-
1
ical NER systems are available (see e.g. (Wilbur
et al, 2007; Leaman and Gonzalez, 2008)), and
most domain IE approaches are NE-driven: a typi-
cal way to cast the RE task is as deciding for each
pair of co-occurring NEs whether a relevant rela-
tion is stated for them in context. Like the previ-
ous LLL and BioCreative2-PPI relation extraction
tasks (Ne?dellec, 2005; Krallinger et al, 2007), the
BioNLP?09 shared task on event extraction (Kim et
al., 2009) similarly proceeds from NEs, requiring
participants to detect events and determine the roles
given NEs play in them.
Any domain IE approach targeting nontrivial
causal NE relations or events necessarily involves
decisions relating to static relations. Consider, for
example, the decision whether to extract a relation
between NE1 and NE2 in the following cases (affects
should here be understood as a placeholder for any
relevant statement of causal relation):
1) NE1 affects NE2 gene
2) NE1 affects NE2 promoter
3) NE1 affects NE2 mutant
4) NE1 affects NE2 antibody
5) NE1 affects NE2 activator
The decision here depends on the interpretation of
the noun compounds (NCs) NE2 gene, NE2 pro-
moter, etc. Depending on the IE setting, one might,
for example, judge that statements (1)?(3) justify the
extraction of an (NE1, NE2) relation, while (4) and
(5) do not. This question is rarely formalized as
a separate (sub)task in domain studies, and meth-
ods targeting e.g. the LLL, BioCreative2-PPI and
BioNLP?09 shared task relations and events must
learn to resolve this question together with the sep-
arate issue of which words and syntactic structures
express relevant causal relations.
2.2 Task setting
The relation extraction problems represented by ex-
amples (1)?(5) above are closely related to the well-
studied issue of NC semantics. However, the prob-
lem extends past simple binary NCs to include judg-
ments on the relations of arbitrary base NPs (nouns
with premodifiers) to contained NEs,
NE1 affects truncated NE2
NE1 affects NE2/NE3 complexes
NE1 affects NE2-dependent phosphatase
and further to relations of NPs with NEs that are syn-
tactically less immediately attached:
NE1 affects first exon of NE2
NE1 affects an element in the NE2 promoter
NE1 affects members of the immediate-early acti-
vation genes family such as NE2
The problem thus encompasses also more general
relations between nominals.
While these different cases could also be studied
as separate tasks, in the current IE context they can
be seen as presenting a continuum of different syn-
tactic realizations of similar relations that also carry
the same implications for further processing. We
propose to treat them together, formulating the spe-
cific task studied in this paper as follows:
Given: named entity NE and another entity E
with their context in text,
Determine: whether there is a relevant static re-
lation R(NE, E) and its type.
Here, relevant relations are defined as those that jus-
tify an inference of some role for the NE in causal re-
lations/events involving E. Additionally, the level of
granularity chosen for typing is chosen according to
the need to determine the role of the NE in the rela-
tions/events. These choices are intentionally depen-
dent on the IE context: we do not expect to be able
to formulate a universally accepted set of relevance
criteria or relations. Our choice of relation scope
and types here follows the perspective of a currently
highly relevant IE problem, the BioNLP?09 shared
task on event extraction. We aim to recognize a set
of relations sufficient to capture the relevant rela-
tionships of the NEs provided as given information
in the shared task (all of protein/gene/RNA type)
and the terms annotated in the GENIA Event corpus
(Kim et al, 2008) as participants in events.
We note that this task setting excludes the recog-
nition of candidate NEs and other entities. The as-
sumption that they are given is analogous to the
common NE-NE causal relation extraction setting.
Further, requiring their recognition would, in our
view, unnecessarily complicate the task with aspects
of NER and NP chunking, well-studied separate
tasks.
We next sketch a formulation of an causal rela-
tion/event extraction task incorporating static rela-
tions and briefly present one possible way in which
2
static relation extraction could be applied in IE set-
tings not explicitly targeting such relations.
2.3 Applications of static relations
In the following, we assume that NEs are detected in
a prior processing step. Consider, then, the task of
extracting relevant information from the following
sentence:
NE1 is a subunit of the complex that inhibits the
expression of mutant forms of NE2
An example causal relation extraction target here
could be
Inhibit(NE1,NE2)
while an event extraction task might aim to recog-
nize the events
E1:Expression(NE2)
E2:Inhibit(NE1, E1)
An IE system directly targeting either representa-
tion will need to simultaneously address issues re-
lating to the causal statements and static relations.
Static relation annotation makes this explicit (square
brackets are used to mark non-NE entities):
Part-Whole.Component-Object(NE1, [complex])
Variant(NE2, [mutant forms])
This type of static relation detection as prior step to
causal relation or event extraction could be applied
in at least two different ways: primarily augment-
ing the extracted information, or alternatively assist-
ing in the extraction of the information considered
above. Assuming the successful extraction of the
above static relations, the input can be reformulated
as
NE1 is a subunit of the [complex] that inhibits the
expression of [mutant forms] of NE2
Then, under the augmented extraction model, the
causal relation and event extraction targets would be,
respectively,
Inhibit([complex],[mutant forms])
and
E1:Expression([mutant forms])
E2:Inhibit([complex], E1)
Taken together with the static relations, this provides
a more detailed representation of the information
stated in the example sentence. Further, simple rules
would suffice to derive the simplified representations
involving only the NEs, and such rules would have
the further benefit of making explicit which inter-
vening static relations are taken to support the infer-
ence that an NE is involved in a stated causal relation
or event.
Alternatively, under the assisted extraction model,
with the assumption that the static relations are taken
to allow the inference that any relation or event hold-
ing of the other entities holds for the NEs, the input
to the causal relation or event extraction system can
be recast as
NE1 is a subunit of the NE?1 that inhibits the ex-
pression of NE?2 of NE2
where NE?1 and NE?2 should be understood as
aliases for NE1 and NE2, respectively. Now, un-
der the causal relation extraction model, each of
the (NE1,NE2), (NE?1, NE2), (NE1,NE?2), (NE?1,NE?2)
pairs can serve as an example of the desired rela-
tion, both for the purposes of training and actual
extraction (the event extraction case can be treated
analogously). By increasing the number of positive
cases, this application of information on static rela-
tions would be expected to have a positive effect on
the performance of the primary causal relation/event
extraction method.
While these two alternatives are only rough
sketches of possible uses of static relation annota-
tion, we expect either could be developed into a
practical implementation. Further, these examples
by no means exhaust the possibilities of this class
of annotation. As static relation extraction can thus
be seen to have multiple potential benefits for both
causal relation and event extraction, we believe the
efforts to pursue static relations as a separate task
and to develop resources specific to this task are jus-
tified.
3 Relations
Based on an analysis of the shared task data (see
Section 4.1), we recognize the static relations illus-
trated in Table 1. In the following, we briefly discuss
the types and their selection.
3
Name Examples
Variant Bcl-6 gene, IL-1 mRNA, wild-type SHP1, TRADD mutant, human IL-1beta,
[cell-surface isoforms] of CD43, phosphorylated CREB protein
PW.Object-Component IL-6 promoter, GR N-terminal transactivation domain, SAA promoter sequence,
proximal IL-2 promoter-enhancer, [transcriptional enhancers] including IFNB
PW.Component-Object NF-kappa B1/RelA heterodimer, p65 homodimer, p50-p65 complex,
STAT1-containing [DNA-binding complex], [heterodimer] of p50 and p65
PW.Member-Collection CREB/ATF family, p21ras small GTP binding proteins,
[non-heat shock genes] such as IL1B, [cellular genes] including GM-CSF
PW.Place-Area beta-globin locus
Table 1: Relations. In examples, NEs are underlined and square brackets are used to mark the extent of non-NE entities
that do not span the entire example text.
3.1 Selection criteria
Relations could be recognized and split into differ-
ent types at a number of different granularities. Mo-
tivated by practical IE applications, we aimed to de-
fine a static relation extraction subtask that fits natu-
rally into existing IE frameworks and to create an-
notation that supplements existing annotation and
avoids overlap in annotated information. The practi-
cal goals also motivate our aim to recognize a min-
imal set of different relation types that can satisfy
other goals, fewer distinctions implying an easier
task and more reliable extraction.
To decide whether to use a single relation type or
introduce several subtypes to annotate a given set of
cases, we aimed to introduce coherent relation types,
each implying consistent further processing. More
specifically, we required that each relation R(NE,
entity) must uniquely and consistently define the re-
lation and roles of the participants, and that in the
relevant IE context the relation alone is sufficient to
decide how to interpret the role of the NE in other
relations/events. Specific examples are given in the
introduction of the chosen relation types below.
In the following, we follow in part the relation
taxonomy and relation definitions of (Winston et al,
1987). However, we recognize that there is no clear
agreement on how to subdivide these relations and
do not suggest this to be the only appropriate choice.
3.2 Part-whole relations
Part-whole, or meronymic, relations are, not surpris-
ingly, the most common class of static relations in
our data: a single generic Part-Whole relation could
capture more than half of the relevant relations in
the corpus. However, although the relations be-
tween the NE and entity in, for example, [complex]
containing NE and [site] in NE are both types of
Part-Whole (below PW) relations, the roles of par-
ticipants are not consistently defined: in PW(NE,
[site]) the entity is a component of the NE, while
in PW(NE, [complex]) the roles are reversed. We
thus recognize separate PW.Object-Component and
PW.Component-Object relations. By contrast, while
the relation between a NE representing a gene and a
site on that gene is is arguably different from the re-
lation between a protein NE and a site on the protein,
we do not distinguish these relations as the annota-
tion would duplicate information available in as part
of the entity typing in the corpus and would further
imply a static relation extraction task that incorpo-
rates aspects of NE recognition.
Also frequent in the data are relations such as
that between a protein and a protein family it be-
longs to. While many cases are clearly identifiable
as PW.Member-Collection relations, others could al-
ternatively be analysed as Class-Member. As in our
context the relations in e.g. P, a member of the [type
F protein family] and P, a [type F protein] imply
the same processing, we will apply the PW.Member-
Collection label to both, as well as to ad hoc col-
lections such as [cellular genes] such as NE, even
if this requires a somewhat relaxed interpretation of
the relation label. Finally, there are a few cases in
our data (e.g. NE locus) that we view as instances of
the PW.Place-Area relation.
3.3 Variant relations
To avoid unnecessary division of relations that im-
ply in our context similar interpretation and process-
ing, we define a task-specific Variant relation that
4
encompasses a set of possible relation types holding
between an NE and its variants along multiple dif-
ferent axes. One significant class of cases annotated
as Variant includes expressions such as NE gene and
NE protein, under the interpretation that NE refers
to the abstract information that is ?realized? as ei-
ther DNA, RNA or protein form, and the entity to
one of these realizations (for alternative interpreta-
tions, see e.g. (Rosario and Hearst, 2001; Heimonen
et al, 2008)).
The Variant relation is also used to annotate NE-
entity relations where the entity expresses a different
state of the NE, such as a phosphorylated or mutated
state. While each possible post-translational modifi-
cation, for example, could alternatively be assigned
a specific relation type, in the present IE context
these would only increase the difficulty of the task
without increasing the applicability of the resulting
annotation.
3.4 Other/Out annotation
We apply a catch-all category, Other/Out, for anno-
tating candidate (NE, entity) pairs between which
there is no relevant static relation. This label is thus
applied to a number of quite different cases: causal
relations, both implied (e.g. NE receptors, NE re-
sponse element) and explicitly stated (NE binds the
[site]), relations where the entity is considered too
far removed from the NE to support reliable infer-
ence of a role for the NE in causal relations/events
involving the entity (e.g. [antibodies] for NE), and
cases where no relation is stated (e.g. NE and other
[proteins]). The diversity of this generic category
of irrelevant cases is a necessary consequence of the
aim to avoid annotation involving decisions directly
relating to other tasks by creating distinctions be-
tween e.g. causal and no relation.
3.5 Sufficiency of the setting and relation types
We have cast the static relation extraction task as al-
ways involving an NE, which in the present context
is further always of a protein, gene or RNA type.
This restriction considerably simplifies the task con-
ceptually and reduces annotation effort as well as ex-
pected extraction difficulty, as the type of only one
of the entities involved in the relation can vary sig-
nificantly. However, it is not obvious that the restric-
tion allows coherent relations types to be defined. If
the corpus contained frequent cases where the stated
relationship of the NE to the entity involved different
types of relevant relations (e.g. collections of parts
of an NE), it would be necessary to either recog-
nized ?mixed? or combined relations or extend the
task to include general entity-entity relations.
Interestingly, during annotation we encountered
only two cases (less than 0.1% of those annotated)
involving two of the recognized relation types at
once: mutant NE promoter and 5? truncation mu-
tants of the NE promoter1. While this result is likely
affected by a number of complex factors (annota-
tion criteria, NE and entity types, granularity of re-
lations, etc.), we find the outcome ? which was nei-
ther planned for nor forced on the data ? a very en-
couraging sign of the sufficiency of the task setting
for this and related domain IE tasks.
4 Data
We created the data set by building on the annota-
tion of the GENIA Event corpus (Kim et al, 2008),
making use of the rich set of annotations already
contained in the corpus: term annotation for NEs
and other entities (Ohta et al, 2002), annotation of
events between these terms, and treebank structure
closely following the Penn Treebank scheme (Tateisi
et al, 2005).
4.1 Annotation
The existing GENIA annotations served as the basis
of the new annotation. We initially selected as can-
didates entities annotated as participating in events
considered in the BioNLP?09 shared task.
As the term annotation includes nesting of en-
tities, NEs contained within these relevant entities
were used as the starting point for the annotation.
We first performed a preliminary study of the rele-
vant static relations occurring between the entities
and NEs occurring within them to determine the
set of relations to annotate. Next, all unique cases
where a selected entity contained an NE were anno-
tated with the appropriate relation based on the con-
tained text of the entity, with the text of the contained
NE normalized away. For the present study, we ex-
cluded from consideration cases where the annota-
1To resolve these cases, we simply ignored the implied Vari-
ant relation.
5
tion indicated simple aliasing (e.g. [CREB/ATF]), a
relation irrelevant to our purpose and found in the
selected data only due to the annotation specifying
one entity but two NEs in these cases. In this step,
830 unique cases representing a total of 1601 entities
containing NEs were annotated.
The nesting structure of the term annotation does
not, however, capture all relevant static relations:
the term annotation scheme disallows discontinuous
terms and annotation of terms with structure more
complex than base NPs. Thus, the possible relations
of NEs to entities to which they were connected e.g.
by a prepositional phrase cannot be directly derived
from the existing annotation. As an example, the
nesting in [NE region] directly suggest the existence
of a relation, while no such connection appears in
[region] of NE. To annotate relations for entities for
which the term annotation does not identify a can-
didate related NE, it is necessary to form (NE, en-
tity) pairs with co-occurring NEs. Even when the
candidate NEs were restricted to those occurring in
the same sentence, the number of such pairs in the
corpus was over 17,000, beyond the scope of what
could be annotated as part of this effort. Further, as
the great majority of co-occurring (NE, entity) pairs
will have no relevant static relation, we used heuris-
tics to increase the proportion of relevant and near-
miss cases in the annotated data.
We first converted the gold standard annotation of
the GENIA treebank (Tateisi et al, 2005) into a de-
pendency representation using the Stanford parser
tools (de Marneffe et al, 2006) and then deter-
mined the shortest paths in the dependency analy-
ses connecting each relevant entity with each NE.
The (NE, entity) pairs were then ordered according
to the length of these paths, on the assumption that
syntactically more closely related entities are more
likely to have a relevant static relation. Annotation
then proceeded on the ordered list of pairs. Dur-
ing the annotation, we further developed more or-
dering heuristics, such as giving higher ranking to
candidate pairs connected by a path that contains
a subpath known to connect pairs with relevant re-
lations. Such known paths were first derived from
the BioInfer static relation annotation (Pyysalo et al,
2007) and later extracted from previously annotated
cases. In this annotation process, judgments were
performed with reference to the full sentence con-
Annotated instances
Relation cont. nonc. total
PW.Object-Component 394 133 527
PW.Component-Object 299 44 343
Variant 253 20 273
PW.Member-Collection 25 124 149
PW.Place-Area 4 1 5
Other/Out 626 778 1404
total 1601 1100 2701
Table 2: Statistics for annotated data. Number of in-
stances given separately for relations annotated between
entities with contained (cont.) and non-contained (nonc.)
NEs.
text. In total, 1100 cases were annotated in this way.
All stages of the annotation process involved only
lists formatted as simple text files for markup and
custom-written software for processing.
Table 2 contains statistics for the annotated data,
showing separately the number of annotated re-
lations of entities to contained and non-contained
NEs. There are interesting differences in the rela-
tion type distribution between these two categories,
reflecting the different ways in which relations are
typically stated. This difference in distribution sug-
gests that it may be beneficial to give the two cases
different treatment in extraction.
4.2 Representation
For simplicitly of use, we provide the annotated data
in two equivalent representations: a simple inline
XML format and a standoff format. The XML for-
mat closely resembles the representation used for the
SemEval-2007 Semantic Relations between Nomi-
nals task (Girju et al, 2007). Here, each NE-Entity
pair is given its own entry with its sentence con-
text in which only the pair is marked. In the alter-
nate standoff representation, all entities appearing in
each sentence are tagged, and the annotated relations
given separately. These representations are easily
processed and should be usable with little modifica-
tion with many existing relation extraction methods.
We further split the data into training,
development-test and test sets according to the
same division applied in the BioNLP?09 shared
task on event extraction. This division allows the
dataset to be easily integrated into settings using the
shared task data, combining static relation and event
extraction approaches.
6
5 Experiments
The selected task setting and representation form a
natural basis for two alternative classification prob-
lems: a binary classification problem for detecting
the presence of any relevant relation, and a multi-
class classification problem where the correct rela-
tion type must also be determined. In the following,
we describe experiments using the dataset in these
two settings. While we apply a state-of-the-art ma-
chine learning method and a fairly expressive repre-
sentation, the aim of the experiments is only to de-
termine the relative difficulty of the relation extrac-
tion task and to establish a moderately competitive
baseline result for the newly created dataset.
We use a linear Support Vector Machine (SVM)
classifier (Chang and Lin, 2001) with N-gram fea-
tures defined over token sequences delimited by the
beginning and end of the entity and the position of
the NE. The NE is treated as a single token and
its text content blinded from the classifier to avoid
overfitting on specific names. Features are gener-
ated from two sequences of tokens: those inside
the entity and, when the NE is not contained in the
entity, those between the entity and the NE (inclu-
sive of the entity and NE at the sequence bound-
aries). In preliminary experiments on the develop-
ment test set we found no clear benefit from includ-
ing N-gram features extracted from a broader con-
text, supporting an assumption that the problem can
be mostly addressed on the basis of local features.
By contrast, preliminary experiments supported the
use of the simple Porter algorithm (Porter, 1980) for
stemming, the inclusion of uni-, bi- and trigram fea-
tures, and normalization of the feature vectors to unit
length; these were adopted for the final experiment.
The SVM regularization parameter was optimized
using a sparse search with evaluation on the devel-
opment test set.
We first reduced the annotated data into a binary
classification problem with the Other/Out class rep-
resenting negative (irrelevant) and the other rela-
tions positive (relevant) cases. The results for this
experiment were very encouraging, giving both a
high classification accuracy of 86.8% and an F-score
of 84.1%. The test set contains 179 positive and
269 negative cases, giving a majority baseline ac-
curacy of 60.0% and an all-true baseline F-score of
P R F
Relevant 81.2 87.2 84.1
PW.Object-Component 94.2 75.4 83.8
PW.Component-Object 60.0 71.2 65.1
Variant 88.0 57.9 69.8
PW.Member-Collection 54.5 37.5 44.4
Table 3: Classification results with (P)recision, (R)ecall
and (F)-score for the binary Relevant/Irrelevant exper-
iment and classwise results for the relevant classes
(PW.Place-Area excluded for lack of data).
57.1%. The classifier notably and statistically sig-
nificantly (McNemar?s test, p < 0.01) outperforms
these simple baselines. We then performed a sep-
arate multiclass classification experiment, predict-
ing the specific type of the relation, also including
the Other/Out type. In this experiment, accuracy re-
mained relatively high at 81.9%, while per-class pre-
cision and recall results (considering each class in
turn positive and all others negative, see Table 3) in-
dicate some remaining challenges. The results vary
somewhat predictably with the number of exam-
ples per relation type (Table 2): while PW.Object-
Component relations can be predicted at high pre-
cision and fair recall, performance for PW.Member-
Collection relations falls behind expectations for a
local relation extraction problem.
To briefly relate these results to domain causal RE
results, we note that the recently proposed state-of-
the-art method of (Airola et al, 2008) was reported
to achieve F-scores ranging between 56.4?76.8% on
five different causal RE corpora in a binary classi-
fication setting. As our relatively simple method
achieves a notably higher 84.1% F-score at the bi-
nary static RE task, we can conclude that this static
RE task is not as difficult as the causal RE tasks.
This is encouraging for the prospects of static RE in
support of domain causal RE and event extraction.
6 Related work
Relations of types that we have here termed static
have figured prominently in the MUC and ACE se-
ries of events that have largely defined the ?gen-
eral domain? IE research program (Sundheim, 1995;
Doddington et al, 2004). In this line of research,
event-type annotation is used (as the name implies)
to capture events, defined as ?[...] something that
happens [...] [that] can frequently be described as a
7
change of state? (LDC, 2005) and relation-type an-
notation is applied for relevant non-causal relation-
ships. General static relations have been studied ex-
tensively also in broader, non-IE contexts (see e.g.
(Girju et al, 2007)).
In the biomedical domain, static relations have re-
ceived relatively little attention. Domain noun com-
pound semantics, including static relations, have
been considered in studies by (Rosario and Hearst,
2001) and (Nakov et al, 2005), but in IE settings
static relations tend to appear only implicitly, as in
the RelEx causal RE system of (Fundel et al, 2007),
or through the causal relations they imply: for ex-
ample, in the AIMed corpus (Bunescu et al, 2005)
statements such as NE1/NE2 complex are annotated
as a binding relation between the two NEs, not Part-
Whole relations with the broader entity. By contrast,
there has been considerable focus on the extraction
of ?things that happen,? dominantly making use of
relation-type corpus annotation and extraction ap-
proaches: a study of five corpora containing primar-
ily causal relation annotation is found in (Pyysalo et
al., 2008); more complete lists of domain corpora
are maintained by Kevin Cohen2 and Jo?rg Haken-
berg3. For a thorough review of recent work in do-
main RE, we refer to (Zweigenbaum et al, 2007).
BioInfer (Pyysalo et al, 2007), to the best of our
knowledge the first domain corpus to include event-
type annotation, also includes annotation for a set
of static relation types. The design of the BioIn-
fer corpus and relationship type ontology as well as
work applying the corpus in jointly targeting event
extraction and static relation extraction (Heimonen
et al, 2008; Bjo?rne et al, 2008) have considerably
influenced the present study. A key difference in fo-
cus is that BioInfer primarily targets NE-NE rela-
tions, while our concern here has been the relations
of NEs with other, non-NE entities, specifically fo-
cusing on the requirements of the BioNLP?09 shared
task. A class of static relations, connecting Mu-
tants and Fragments with their parent proteins, is
annotated in the recently introduced ITI TXM cor-
pora (Alex et al, 2008). While somewhat limited
in the scope of static relations, this annotation cov-
ers an extensive number of instances, over 20,000,
2http://compbio.uchsc.edu/ccp/corpora/obtaining.shtml
3http://www2.informatik.hu-
berlin.de/?hakenber/links/benchmarks.html
and could likely support the development of high-
reliability methods for the extraction extraction of
these specific static relations. As discussed in detail
in Section 4.1, previously published versions of the
GENIA corpus (Kim et al, 2008) contain NE, term
and event annotation, but no static relations have
been annotated in GENIA prior to this effort.
While previously introduced corpora thus cover
aspects of the annotation required to address the
static relation extraction task considered in this pa-
per, we are not aware of previously published re-
sources that would address this task specifically or
contain annotation supporting the entire task as en-
visioned here.
7 Conclusions and future work
In this paper, we have argued for a position for static
relations in biomedical domain IE, specifically
advancing the subtask of extracting static relations
between named entities and other entities appearing
in their context. We explored this subtask in the
specific IE context of the BioNLP?09 shared task on
event extraction, identifying possible instances of
static relations relevant to the task setting. We then
studied these instances of detail, defining a minimal
set of basic static relations argued to be sufficient
to support the type of IE envisioned in the shared
task. We annotated 2701 instances of candidate
static relations, creating the first domain corpus
of static relations explicitly designed to support
IE, and performed experiments demonstrating that
the static relation extraction task can be performed
accurately, yet retains challenges for future work.
The newly annotated corpus is publicly available at
www-tsujii.is.s.u-tokyo.ac.jp/GENIA
to encourage further research on this task.
Acknowledgments
Discussions with members of the BioInfer group
were central for developing many of the ideas pre-
sented here. We are grateful for the efforts of Maki
Niihori in producing supporting annotation applied
in this work. This work was partially supported
by Grant-in-Aid for Specially Promoted Research
(Ministry of Education, Culture, Sports, Science and
Technology (MEXT), Japan), and Genome Network
Project (MEXT, Japan).
8
References
Antti Airola, Sampo Pyysalo, Jari Bjorne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein interaction
extraction with evaluation of cross-corpus learning.
BMC Bioinformatics, 9(Suppl 11):S2.
Bea Alex, Claire Grover, Barry Haddow, Mijail Kabad-
jov, Ewan Klein, Michael Matthews, Stuart Roebuck,
Richard Tobin, and Xinglong Wang. 2008. The ITI
TXM corpora: Tissue expressions and protein-protein
interactions. In Proceedings of LREC?08.
Jari Bjo?rne, Sampo Pyysalo, Filip Ginter, and Tapio
Salakoski. 2008. How complex are complex protein-
protein interactions? In Proceedings SMBM?08.
Razvan C Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun Kumar
Ramani, and Yuk Wah Wong. 2005. Comparative ex-
periments on learning information extractors for pro-
teins and their interactions. Artificial Intelligence in
Medicine, 33(2):139?155.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program: Tasks, data, and evaluation. In
Proceedings of LREC?04, pages 837?840.
Katrin Fundel, Robert Kuffner, and Ralf Zimmer. 2007.
RelEx?Relation extraction using dependency parse
trees. Bioinformatics, 23(3):365?371.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic re-
lations between nominals. In Proceedings of Se-
mEval?07, pages 13?18.
Juho Heimonen, Sampo Pyysalo, Filip Ginter, and Tapio
Salakoski. 2008. Complex-to-pairwise mapping of
biological relationships using a semantic network rep-
resentation. In Proceedings of SMBM?08.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
bionlp?09 shared task on event extraction. In Proceed-
ings of BioNLP?09.
Martin Krallinger, Florian Leitner, and Alfonso Valen-
cia. 2007. Assessment of the second BioCreative PPI
task: Automatic extraction of protein-protein interac-
tions. In Proceedings of BioCreative II, pages 41?54.
LDC. 2005. ACE (automatic content extraction) en-
glish annotation guidelines for events. Technical re-
port, Linguistic Data Consortium.
R. Leaman and G. Gonzalez. 2008. Banner: An exe-
cutable survey of advances in biomedical named entity
recognition. In Proceedings of PSB?08, pages 652?
663.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC?06, pages 449?454.
Preslav Nakov, Ariel Schwartz, Brian Wolf, and Marti
Hearst. 2005. Scaling up bionlp: Application of a text
annotation architecture to noun compound bracketing.
In Proceedings of BioLINK?05.
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceedings
of LLL?05.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichi
Tsujii. 2002. GENIA corpus: An annotated research
abstract corpus in molecular biology domain. In Pro-
ceedings of the Human Language Technology Confer-
ence (HLT?02), pages 73?77.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(2):130?137.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Antti Airola, Juho Heimonen, and Jari
Bjo?rne. 2008. Comparative analysis of five protein-
protein interaction corpora. BMC Bioinformatics,
9(Suppl. 3):S6.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
EMLNP?01, pages 82?90.
Beth M. Sundheim. 1995. Overview of results of the
MUC-6 evaluation. In Proceedings of MUC-6, pages
13?31.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax annotation for the GE-
NIA corpus. In Proceedings of IJCNLP?05, pages
222?227.
John Wilbur, Larry Smith, and Lorrie Tanabe. 2007.
Biocreative 2 gene mention task. In Proceedings of
BioCreative 2, pages 7?16.
Morton E. Winston, Roger Chaffin, and Douglas Her-
rmann. 1987. A taxonomy of part-whole relations.
Cognitive Science, 11.
Pierre Zweigenbaum, Dina Demner-Fushman, Hong Yu,
and Kevin B. Cohen. 2007. Frontiers of biomedical
text mining: Current progress. Briefings in Bioinfor-
matics.
9
Proceedings of the Workshop on BioNLP, pages 106?107,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Incorporating GENETAG-style annotation to GENIA corpus
Tomoko Ohta? and Jin-Dong Kim? and Sampo Pyysalo? and Yue Wang? and Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{okap,jdkim,smp,wangyue,tsujii}@is.s.u-tokyo.ac.jp
1 Introduction
Proteins and genes are the most important entities in
molecular biology, and their automated recognition
in text is the most widely studied task in biomed-
ical information extraction (IE). Several corpora
containing annotation for these entities have been
introduced, GENIA (Kim et al, 2003; Kim et al,
2008) and GENETAG (Tanabe et al, 2005) being
the most prominent and widely applied. While both
aim to address protein/gene annotation, their an-
notation principles differ notably. One key differ-
ence is that GENETAG annotates the conceptual en-
tity, gene, which is often associated with a function,
while GENIA concentrates on the physical forms of
gene, i.e. protein, DNA and RNA. The difference
has caused serious problems relating to the compat-
ibility and comparability of the annotations. In this
work, we present an extension of GENIA annotation
which integrates GENETAG-style gene annotation.
The new version of the GENIA corpus is the first to
bring together these two types of entity annotation.
2 GGP Annotation
Gene is the basic unit of heredity, which is encoded
in the coding region of DNA. Its physical manifes-
tations as RNA and Protein are often called its prod-
ucts. In our view of these four entity types, gene is
taken as an abstract entity whereas protein, DNA and
RNA are physical entities. While the three physical
entity types are disjoint, the abstract concept, gene,
is defined from a different perspective and is realized
in, not disjoint from, the physical entity types.
The latest public version of GENIA corpus (here-
after ?old corpus?) contains annotations for gene-
Protein DNA RNA GGP
Old Annotation 21,489 8,653 876 N/A
New Annotation 15,452 7,872 863 12,272
Table 1: Statistics on annotation for gene-related entities
related entities, but they are classified into only
physical entity types: Protein, DNA and RNA. The
corpus revisions described in this work are two-fold.
First, annotation for the abstract entity, gene, were
added (Table 1, GGP). To emphasize the character-
istics of the new entity type, which does not dis-
tinguish a gene and its products, we call it GGP
(gene or gene product). Second, the addition of GGP
annotation triggered large-scale removal of Protein,
DNA and RNA annotation instances for cases where
the physical form of the gene was not referred to
(Due to space limitations, we omit RNA from now
on). The time cost involved with this revision was
approximately 500 person-hours.
3 Quality Assessment
To measure the effect of revision, we performed
NER experiments with old and new annotation (Ta-
bles 2 and 3). We split the corpus into disjoint 90%
and 10% parts for use in training and test, respec-
tively. We used the BANNER (Leaman and Gonza-
lez, 2008) NE tagger and created a separate single-
class NER problem for each entity type.
In the old annotation, consistency is moderate
for protein (77.70%), while DNA is problematic
(58.03%). The new GGP annotation has been
achieved in a fairly consistent way (81.44%). How-
ever, the removal of annotation for entities previ-
ously marked as protein or DNA had opposite effects
on the two: better performance for DNA (64.06%),
106
Precision Recall F-score
Protein 80.78 74.84 77.70
DNA 64.90 52.48 58.03
Table 2: NER performance before GGP annotation
Precision Recall F-score
Protein 71.20 56.61 63.08
DNA 69.59 59.35 64.06
GGP 86.86 76.65 81.44
Protein+ 83.22 78.20 80.63
Table 3: NER performance after GGP annotation
Phosphorylation Gene expression
GGP in protein 70% GGP abstract 34%
Protein 25% Protein 24%
GGP abstract 3% GGP in Protein 17%
Peptide 1% GGP in DNA 9%
Table 4: Distribution of theme entity types in GENIA
implying annotation consistency improved with the
removals, but worse for Protein (63.08%).
We find the primary explanation for this effect in
the statistics in Table 1: in the revision, a large num-
ber of protein annotations (6,037) but only a small
number of DNA annotations (780) were replaced
with GGP. To distinguish such GGPs from those em-
bedded in Protein or DNA annotations, we call them
?abstract? GGPs, as they appear in text without in-
formation on their physical form. Nevertheless, in
the old annotation, they had to be annotated as either
protein or DNA, which might have caused inconsis-
tent annotation. However, the statistics show a clear
preference for choosing Protein over DNA. The rad-
ical drop of performance in protein recognition can
then be explained in part as a result of removing this
systematic preference.
Aside from the discussion on whether the pref-
erence is general or specific, we interpret the pref-
erence as a need for ?potential? proteins to be re-
trieved together with ?real? proteins, which was an-
swered by the old protein annotation. To reproduce
this class in the new annotation, we added abstract
GGPs to the Protein annotation and performed an
NER experiment. The result (Table 3, Protein+)
shows a clear improvement over the comparable re-
sult for the old protein annotation.
In conclusion, we argue, the revision of the GE-
NIA annotation, in addition to introducing a new en-
tity class, has led to a significant improvement of
overall consistency.
4 Discussion
Although there are already corpora such as GENE-
TAG with annotation similar to GGPs, we expect
this newly introduced class of annotation to support
existing annotations of GENIA, such as event and
co-reference annotation, opening up new possibili-
ties for application. The quality of entity annota-
tion should be closely related to that of other seman-
tic annotation, e.g. events. For example, the event
type Phosphorylation is about a change on physi-
cal entities, e.g. proteins and peptides, and as such,
it is expected that themes of these events would be
physical entities. On the other hand, the event type
Gene expression is about the manifestation of an ab-
stract entity (gene) as a physical entity (protein) and
would thus be expected to involve both abstract and
physical entities. Statistics from GENIA (Table 4)
show that the theme selection made in event anno-
tation well reflects these characteristics of the two
event types. The observation suggests that there is a
good likelihood that improvement of the entity an-
notation can be further transferred to other semantic
annotation, which is open for future work.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Genome Network Project (MEXT, Japan).
References
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. GENIA corpus - a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19(suppl. 1):i180?i182.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
R. Leaman and G. Gonzalez. 2008. Banner: an exe-
cutable survey of advances in biomedical named en-
tity recognition. Pacific Symposium on Biocomputing,
pages 652?663.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-
ten, and W John Wilbur. 2005. Genetag: a tagged cor-
pus for gene/protein named entity recognition. BMC
Bioinformatics, 6(Suppl 1):S3.
107
Proceedings of the Workshop on BioNLP, pages 162?170,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Bridging the Gap between Domain-Oriented and
Linguistically-Oriented Semantics
Sumire Uematsu Jin-Dong Kim Jun?ich Tsujii
Department of Computer Science
Graduate School of Information Science and Technology
University of Tokyo
7-3-1 Hongo Bunkyo-ku Tokyo 113-0033 Japan
{uematsu,jdkim,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper compares domain-oriented and
linguistically-oriented semantics, based on the
GENIA event corpus and FrameNet. While
the domain-oriented semantic structures are
direct targets of Text Mining (TM), their ex-
traction from text is not straghtforward due
to the diversity of linguistic expressions. The
extraction of linguistically-oriented semactics
is more straghtforward, and has been studied
independentely of specific domains. In or-
der to find a use of the domain-independent
research achievements for TM, we aim at
linking classes of the two types of seman-
tics. The classes were connected by analyz-
ing linguistically-oriented semantics of the ex-
pressions that mention one biological class.
With the obtained relationship between the
classes, we discuss a link between TM and
linguistically-oriented semantics.
1 Introduction
This paper compares the linguistically-oriented and
domain-oriented semantics of the GENIA event cor-
pus, and suggests a factor for utilizing NLP tech-
niques for Text Mining (TM) in the bio-medical do-
main.
The increasing number of scientific articles in the
bio-medical domain has contributed in drawing con-
siderable attention to NLP-based TM. An impor-
tant step in NLP-based TM is obtaining the domain-
oriented semantics of sentences, as shown at the bot-
tom of figure 1. The BioInfer (Pyysalo et al, 2007)
and the GENIA event corpus (Kim et al, 2008) pro-
vide annotations of such semantic structures on col-
lections of bio-medical articles. Domain-oriented
semantic structures are valuable assets because their
representation suits information needs in the do-
main; however, the extraction of such structures is
difficult due to the large gap between the text and
these structures.
On the other hand, the extraction of linguistically-
oriented semantics from text has long been studied
in computational linguistics, and has recently been
formalized as Semantic Role Labeling (Gildea and
Jurafsky, 2002), and semantic structure extraction
(Baker et al, 2007)(Surdeanu et al, 2008). Seman-
tic structures in such tasks are exemplified in the
middle of figure 1. The linguistically-oriented se-
mantic structures are easier to extract, although the
information is not practical to the domain.
We aim at relating linguistically-oriented frames
of semantics with domain-oriented classes, thus
making a step forward in utilizing the computa-
tional linguistic resources for the bio-medical TM.
Of all the differences in the two type of seman-
tics, we focused on the fact that the former frames
are more sensitive to the perspective imposed by
the sentence writer. In the right hand-side exam-
ple of figure 1, the linguistically-oriented structure
treats PBMC, a cell entity, as an agent; however the
bio-medical structure reflects the scientific view that
there are no agents, objects acting with intention, in
bio-molecular phenomena.
As a preliminary investigation, we selected
four representative classes of bio-molecular phe-
nomena; Localization, Binding, Cell adhesion,
and Gene expression, and investigated domain-
oriented annotations for the classes in the GENIA
162
?, whereas in many other cell types, NF-kappa B TRANSLOCATES from cytosol to nucleus as a result of ?
?, both C3a and C3a(desArg) were found to enhance IL-6 RELEASE by PBMC in a dose-dependent manner.
Natural?language
FrameNet?expression?(Linguis?ally?oriented?seman?s) 
Class:?????Mo?n?Theme:?NF?kappa?B?Source:?from?cytosol?Goal:?????to?nucleus?
Class:????Releasing?Theme:?IL?6?Agent:???PBMC?
GENIA?expression?(Biologically?oriented?seman?s) 
Class:???????Localiza?n?Theme:????NF?kappa?B?FromLoc:?cytosol?ToLoc:??????nucleus?
Theme:????IL?6?FromLoc:?(inside?of)?PMBC?ToLoc:??????(outside?of)?PMBC?
Figure 1: A comparison of the linguistically-oriented and biologically-
oriented structure of semantics
event corpus. Expressions mentioning the four
classes were examined and manually classified into
linguistically-oriented frames, represented by those
defined in FrameNet (Baker et al, 1998). FN frames
associated to a bio-molecular event class constitute a
list of possible perspectives in mentioning phenom-
ena of the class.
The rest of this paper is structured in the fol-
lowing way: Section 2 reviews the existing work
on semantic structures and expression varieties in
the bio-medical domain, and provides a compari-
son to our work. In section 3, we describe the GE-
NIA event corpus, and the FrameNet frames used as
linguistically-oriented classes in our investigation.
Sections 4 and 5 explain the methods and results of
the corpus investigation; in particular the sections in-
vestigate how the linguistic frames were associated
to the domain-oriented classes of semantics. Finally,
we provide discussion and conclusion in section 6
and 7.
2 Related Work
Existing work on semantics approached domain-
oriented semantic structures from linguistically-
oriented semantics. In contrast, our approach uses
domain-oriented semantics to find the linguistic se-
mantics that represent them. We believe that the two
different approaches could complement each other.
The PASbio(Wattarujeekrit et al, 2004) pro-
poses Predicate Argument Structures (PASs), a type
of linguistically-oriented semantic structures, for
domain-specific lexical items, based on PASs de-
fined in PropBank(Wattarujeekrit et al, 2004) and
NomBank(Meyers et al, 2004). The PASs are de-
fined per lexical item, and is therefore distinct from a
biologically-oriented representation of events. (Co-
hen et al, 2008) investigated syntactic alternations
of verbs and their nominalized forms which oc-
curred in the PennBioIE corpus(Kulick et al, 2004),
whilst keeping PASs of the PASBio in their minds.
The BioFrameNet(Dolbey et al, 2006) is an at-
tempt to extend the FrameNet with specific frames
to the bio-medical domain, and to apply the frames
to corpus annotation. Our attempts were similar, in
that both were: 1) utilizing the FN frames or their
extensions to classify mentions of biological events,
and 2) relating the frames and the FEs (roles of par-
ticipants) with classes in domain ontologies; e.g. the
Gene Ontology(Ashburner et al, 2000).
As far as the authors know, it is the first at-
tempt to explicitly address the problem of linking
linguistically-oriented and domain-oriented frames
of semantics. However, it has been indirectly stud-
ied through works on TM or Relation Extraction
using linguistically-oriented semantic structures as
features, such as in the case with (Harabagiu et al,
2005).
3 Corpora
?We used domain-oriented annotations of the GE-
NIA event corpus and linguistically-oriented frames
defined in FrameNet (FN), to link domain-oriented
and linguistically-oriented frames of semantics. We
briefly describe these resources next.
163
Mo?n 
Releasing
Ge?g 
A?ching 
Being_located
Becoming
Event StateGENIA?event
Biological_process
Viral_life_cycle
Cellular_process
Physiological_process
Cell_adhesion
Cell_communica?n 
Localiza?n 
Binding
Metabolism
DNA_metabolism
Gene_expression
Crea?g 
Being_a?ched 
Figure 2: The resulting relationship between linguistically-oriented and
biologically-oriented frames.
The GENIA event corpus consists of 1,000 Med-
line abstracts; that is, 9,372 sentences annotated
with domain-oriented semantic structures. The an-
notation was completed for all mentions of biolog-
ical events, and resulted in 6,114 identified events.
Examples of annotated event structures are shown at
the bottom of figure 1. Each structure has attributes
type and themes, which respectively show the bio-
logical class of the mentioned event and phrases ex-
pressing the event participants. The event classes are
defined based on the terms in the Gene Ontology.
For example, the Localization class in the GENIA
event corpus is defined as an equivalent of the GO
term Localization (GO0051179). The event classi-
fications used in the corpus are depicted in the left
hand-side of figure 2. Arrows in the figure depict
the inheritance relations defined in the GENIA event
ontology. For instance, the Localization class is de-
fined as a type of Physiological process. Each of
the annotated structures has additional attributes that
point phrases that the annotator of the structure used
as a clue. Among the attributes, the clueType at-
tribute shows a clue phrase to the event class. In our
investigation, the attribute was treated as a predicate,
or an equivalent of the lexical unit in the FN.
FN is a network of frames that are are
linguistically-oriented classifications of semantics.
A FN frame is defined as ?a script-like conceptual
structure that describes a particular type of situation,
object, or event and the participants and proposi-
tions involved in it,? and is associated with words,
or lexical units, evoking the frame. For instance, the
verbs move, go and fly are lexical units of the Mo-
tion frame, and they share the same semantic struc-
ture. Each FN frame has annotation examples form-
ing an attestation of semantic overlap between the
lexical units. Additionally, FN defines several types
of frame-frame relations; e.g. inheritance, prece-
dence, subframe, etc. The right hand-side of figure
2 shows some FN frames and inheritance relation-
ships between them. The FN provides linguistically-
oriented classifications of event mentions based on
surface expressions, and also shows abstract rela-
tions between the frames.
4 Additional Annotation
Our aim is to link linguistically-oriented and
domain-oriented frames of the bio-medical text?s se-
mantics. A major problem in this task was that there
were no annotated corpora with both types of se-
mantic structures. Therefore, we decided to concen-
trate on the mentions of a few classes of biological
phenomena, and to annotate samples of the mentions
with linguistically-oriented structures conforming to
164
Freq. Keyword Frame
693 binding Attaching
247 bind Attaching
125 interaction Attaching, Being attached
120 complex ?
99 bound Attaching, Being attached
91 interact Attaching, Being attached
61 form Becoming
52 crosslink Attaching
46 formation Becoming
Table 1: The most frequent keywords of the Binding class,
mentioned 2,006 times in total.
Freq. Keyword Frame
131 translocation Motion
81 secretion Releasing
75 release Releasing
32 secrete Releasing
25 mobilization Motion
23 localization Being located
20 uptake Getting
18 translocate Motion
15 expression Creating
9 present Being located
Table 2: The most frequent keywords of the Localization
class, mentioned 582 times in total.
the FrameNet annotations.
The following provides the annotation proce-
dures. First, we collected linguistic expressions that
mention each of the selected GENIA event classes
from the GENIA event corpus. We then sampled
and annotated them with their linguistically-oriented
semantics which conformed to the FrameNet.
4.1 Target Classes and Keywords
We concentrated mainly on the mentions of four GE-
NIA classes; Localization, Binding, Cell adhesion,
and Gene expression. Gene expression, Binding,
and Localization are three of the most frequent four
classes in the GENIA event corpus.1 Binding and
Localization are the two most primitive molecular
events. The Cell adhesion class was included as a
comparison for the Binding class.
Counting keywords for mentioning events was
close to automatic. We extracted phrases pointed
by a clueType attribute from each event structure.
We then tokenized the phrases, performed a simple
stemming on the tokens, and counted the resulting
words. The stemming process simply replaced each
inflected word to its stem by consulting a small list
of inflected words with their stems. Manual work
was only used in making the small list.
4.2 FN Annotation
A major challenge encountered in annotating a sam-
pled expression with a semantic structure conform-
ing to FN, was in the assignment of a FN frame to
1Except correlation and regulation classes which express re-
lational information rather than events.
the mention. Our decision was based on the follow-
ing four points: 1) keywords used in the mention, 2)
description of FN frames, 3) syntactic positions of
the event participants, and 4) frame-frame relations.
The first indicates that a FN frame became a can-
didate frame for the mention, if the keyword in the
mention is a lexical unit of the FN frame. FN frames
and their lexical units could be easily checked by
consulting the FN dictionary. If there were no en-
tries for the keyword in the dictionary, synonyms or
words in the keyword?s definition were used. For ex-
ample, the verb translocate has no entries in the FN
dictionary, and the frames for verbs such as move
were used instead.
For the second point, we discarded FN frames that
are either evoked by a completely different sense of
the keyword, or too specific of a non-biological sit-
uations.
Before we assigned a FN frame to each mention,
we manually examined the syntactic positions of all
event participants present in the sampled GENIA
mentions. Combinations of the syntactic position
and event participants observed for a keyword were
compared with sample annotations of the candidate
FN frames.
We checked frame-frame relations between the
candidate frames, because they can be regarded
as evidence that shows that the conception of the
frames is related. For our aim, it was sufficient to
choose a set of frames that best describes the differ-
ent perspectives for mentioning one type of molecu-
lar phenomena. Even when some keywords seemed
to be dissimilar in the three points mentioned above,
165
Freq. Keyword Frame
98 adhesion Being attached
19 adherence Being attached
16 interaction Being attached, Attaching
15 binding Attaching
8 adherent Being attached
Table 3: The most frequent keywords of the Cell adhesion
class, mentioned 193 times in total.
Freq. Keyword Frame
1513 expression Creating
357 express Creating
239 production Creating
71 overexpression Creating
69 produce Creating
62 synthesis Creating
Table 4: The most frequent keywords of the
Gene expression class, mentioned 2,769 times in
total.
a single frame could be assigned to them if it was
quite clear that they shared a similar perspective.
The frame-frame relations provided in the FN were
treated as clues to the similarity.
Keywords frequently used in each event class are
listed in tables 1, 2, 3, and 4, with the final assign-
ment of FN frames to each keyword.
5 Analysis
After the linguistic annotation was performed, we
compared the GENIA event structure and the frame
structure of each sampled expression, and obtained
relations of the GENIA class-FN frame and GE-
NIA slot-FN participant. The resulting relationships
between FN frames and the four GENIA classes
demonstrate a gap between linguistically-oriented
and domain-oriented classification of events, as
shown in figure 2.
The relations can be explained by decomposing it
into two cases: 1) 1-to-n mappings, and 2) n-to-1
mappings. The n-to-n mapping from GENIA to FN
can then be regarded as a mix of the two cases. In
the following sections, the two cases are described
in detail. Further, we show conversion examples of
a FN structure to a GENIA event structure, which
were supported by the obtained GENIA participant-
FN participant relations.
5.1 1-to-N Mapping: Different Perspectives on
the Same Phenomena
A 1-to-n mapping from GENIA to FN can be ex-
plained as the case where the same molecular phe-
nomena are expressed from different perspectives.
5.1.1 Binding Expressed in Multiple frames
The Binding class in GENIA is defined as
?the selective, often stoichiometric interaction of a
molecule with one or more specific sites on an-
other molecule.? We associated the class with three
frames, and two frames of the three, Attaching and
Becoming frames, represent different perspectives
for mentioning the class. The Being attached frame
shares the same conception as Attaching, but ex-
presses states instead of events. See table 1 for key-
words of the class, and the frames assigned to the
words.
Attaching: In the perspective represented by this
frame, a binding phenomenon was recognized as a
event in which protein molecules were simply at-
tached to one another.
[The 3?-CAGGTG E-boxItem] could BIND
[USF proteinsGoal], ? ? ?
(PubMed ID 10037751, Event IDs E11, E12, E13)
Becoming: In the perspective represented by this
frame, a product of a binding event was treated, on
the surface, as a different entity from the original
parts.
When activated, [glucocorticoid recep-
torsEntity] FORM [a dimerFinal category] ? ? ?
(PubMed ID 10191934, Event ID E5)
This type of expression was possible because a prod-
uct of a binding often obtains a different function-
ality, and can be treated as a different type of en-
tity. Note that this frame was not associated with the
Cell adhesion class described in section 5.2.
166
A CB?
Figure 3: A schematic figure of translocation.
Being attached: Annotators recognized a protein
binding event from the sentence below, which basi-
cally mentions a state of the NF-kB.
In T cells and T cell lines, [NF-kBItem]
is BOUND [to a cytoplasmic proteic in-
hibitor, the IkBGoal].
(PubMed ID 1958222, Event ID E2, E102)
Although this type of expression shares a similar
point of view with the Attaching frame, we classi-
fied these expressions into the Being attached frame
in order to demonstrate cases in which a prerequisite
Binding event was inferred from a state.
5.1.2 Translocation Expressed in Multiple
Frames
The Localization class in the GENIA corpus is de-
fined as a class for ?any process by which a cell, a
substance, or a cellular entity, such as a protein com-
plex or organelle, is transported to, and/or main-
tained in a specific location.? Sampled expressions
of the class separated into mentions of a process, by
which an entity was transported to a specific loca-
tion, and those of the process in which an entity was
maintained in a specific location. We concentrate on
the former in this section, and describe the latter in
section 5.1.3.
We associated the frames: Motion, Releasing and
Getting with what we call translocation events, or
Localization events in which an entity was trans-
ported to a specific location. Figure 3 provides a
schematic representation of a translocation event.
Each of the three frames had a different perspective
in expressing the translocations. See table 2 for key-
words of the frames.
Motion: This group consists of expressions cen-
tered on the translocated entities of the translocation
- namely, B in the figure 3.
[NK cell NFATTheme] ? ? ? MIGRATES [to
the nucleusGoal] upon stimulation,? ? ?
(PubMed ID 7650486, Event ID E33)
Activation of T lymphocytes ? ? ? results
in TRANSLOCATION [of the transcrip-
tion factors NF-kappa B, AP-1, NFAT, and
STATTheme] [from the cytoplasmSource] [into
the nucleusGoal].
(PubMed ID 9834092, Event ID E67)
These expressions are similar to those of the Motion
frame in the FN.
[Her footTheme] MOVED [from the
brakeSource] [to the acceleratorGoal] and the
car glided forward.
Releasing: This group consists of expressions
centered on a starting point of the translocation -
namely, A in the figure 3.
In [unstimulated cells whichAgent] do not
SECRETE [IL-2Theme], only Sp1 binds to
this region, ? ? ?
(PubMed ID 7673240, Event ID E13)
Activation of NF-kappaB is thought to
be required for [cytokineTheme] RELEASE
[from LPS-responsive cellsAgent], ? ? ?
(PubMed ID 1007564, Event ID E14)
The verbal keywords occurred as a transitive in
most cases, and had subjects and objects that ex-
pressed starting points and entities in the transloca-
tions. This is a typical syntactic pattern of the Re-
leasing frame, if we regarded an Agent in the FN as
a starting point of the movement of a Theme.
[The policeAgent] RELEASED [the sus-
pectTheme].
Getting: This group consists of expressions cen-
tered on a goal point of the translocation - namely,
C in figure 3. We assumed that this group has an
opposite point of view from the Releasing frame.
The noun uptake was found to be a keyword in this
group.
The integral membrane ? ? ? appears to play
a physiological role in binding and UP-
TAKE [of Ox LDLTheme] [by monocyte-
macrophagesRecipient], ? ? ?
(PubMed ID 9285527, Event ID E10)
167
To summarize, we observed three groups of ex-
pressions that mention translocation events, and
each group represented different perspectives to
mention the events. Each of the groups and the as-
sociated frame seemed similar, in that they shared
similar keywords and possible syntactic positions to
express the event participant.
5.1.3 Localization excluding Translocation
Expressed in Multiple Frames
Localization events excluding translocations were
expressed in the Being located and Creating frames.
Being located: This group consists of expressions
that simply mention an entity in a specific location.
? ? ? [recombinant NFAT1Theme] LOCAL-
IZES [in the cytoplasm of transiently
transfected T cellsLocation] ? ? ?
(PubMed ID 8668213, Event ID E23)
Creating: A noun expression was observed to be
used by instances mentioning the presence of pro-
teins.
horbol esters are required to induce
[AIM/CD69Created entity] Cell-surface EX-
PRESSION as well as ? ? ?
(PubMed ID 1545132, Event ID E12)
Expressions in these cases indicate an abbrevi-
ation for gene expression, which is a event of
Gene expression class. This type of overlap be-
tween the Localization and Gene expression is ex-
plained in section 5.2.2
5.2 N-to-1 Mapping: Same Conception for
Different Molecular Phenomenon
In contrast to the cases described in section 5.1, the
same conception could be applied to different bio-
logical phenomena.
5.2.1 Shared Conception for Binding and
Cell adhesion
Molecular events classified into Binding and
Cell adhesion shared the conception that two enti-
ties were attached to each other. However, types of
the entities involved are different. They are: the pro-
tein molecule in Binding, and cell in Cell adhesion.
CD36 is a cell surface glycoprotein
? ? ?, which INTERACTS with throm-
bospondin, ? ? ?, and erythrocytes para-
sitized with Plasmodium falciparum.
In the sentence above, an event involving a cell sur-
face glycoprotein and thrombospondin was recog-
nized as a Binding, whereas an event involving a cell
surface glycoprotein and erythrocytes was classified
as a Cell adhesion event.
5.2.2 Shared Expressions of Localization and
Gene expression
Both Localization and Gene expression classes
are connected with the Creating frame. Some
Localization events have a dependency on the
Gene expression event. Protein molecules are made
in events classified into the Gene expression class.
[Th1 cellsCreator] PRODUCE [IL-2 and
IFN-gammaCreated entity], ? ? ?
(PubMed ID 10226884, Event ID E11, E12)
The molecules are then translocated somewhere.
Consequently, localized protein molecules might in-
dicate a Gene expression event, and a phrase ?pro-
tein expression? was occasionally recognized as
mentioning a Localization.
horbol esters are required to induce
[AIM/CD69Created entity] cell-surface EX-
PRESSION as well as ? ? ?
(PubMed ID 1545132, Event ID E12)
5.3 Conversion of FN Structures to GENIA
Events
During the investigation, we compared participant
slots of GENIA and FN structures, in addition to the
structures themselves. Figures 4 and 5 depict con-
version examples from a FN structure and its par-
ticipants to a GENIA structure, with the domain-
oriented type of each participant entity. The conver-
sions were supported by samples, and need quanti-
tative evaluation.
6 Discussion
By annotating sentences of the GENIA event corpus
with semantic structures conforming to FrameNet,
we explicitly compared linguistically-oriented and
168
Class:????Releasing?Theme:?Protein?Agent:???Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(inside?of)?Cell?ToLoc:??????(outside?of)?Cell?
Class:?A?ching?Item:?Protein?A?Goal:?Protein?B?
Class:??Binding?Theme:?Protein?A,?protein?B?
FrameNet?expression
Class:????Mo?n?Theme:?Protein?Source:?Cell?loca?n?A?Goal:?????Cell?loca?n?B?
GENIA?expression
Class:???????Localiza?n?Theme:????Protein?FromLoc:?Cell?loca?n?A?ToLoc:??????Cell?loca?n?B?
Class:????Ge?g?Theme:?Protein?Recipient:?Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(outside?of)?Cell?ToLoc:??????(inside?of)?Cell?
FrameNet?expression
GENIA?expression
Class:?Becoming?En?y:????????????????? Proteins?Final_category:?Pro?n_complex ?
Class:??Binding?Theme:?Proteins?
Figure 4: FN-to-GENIA conversions for Binding
Class:????Releasing?Theme:?Protein?Agent:???Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(inside?of)?Cell?ToLoc:??????(outside?of)?Cell?
Class:?A?ching?Item:?Protein?A?Goal:?Protein?B?
Class:??Binding?Theme:?Protein?A,?protein?B?
FrameNet?expression
Class:????Mo?n?Theme:?Protein?Source:?Cell?loca?n?A?Goal:?????Cell?loca?n?B?
GENIA?expression
Class:???????Localiza?n?Theme:????Protein?FromLoc:?Cell?loca?n?A?ToLoc:??????Cell?loca?n?B?
Class:????Ge?g?Theme:?Protein?Recipient:?Cell?
Class:???????Localiza?n?Theme:????Protein?FromLoc:?(outside?of)?Cell?ToLoc:??????(inside?of)?Cell?
FrameNet?expression
GENIA?expression
Class:?Becoming?En?y:????????????????? Proteins?Final_category:?Pro?n_complex ?
Class:??Binding?Theme:?Proteins?
Figure 5: FN-to-GENIA conversions for Localization.
domain-oriented semantics of the bio-molecular ar-
ticles. Our preliminary result illustrates the gap be-
tween the two type of semantics, and a relationship
between them. We discuss development of a Text
Mining (TM) system, in association with the extrac-
tion of linguistically-oriented semantics, which has
been studied independently of TM.
First, our result would show that TM involves at
least two qualitatively different tasks. One task is
related to our results; that is, recognizing equiva-
lent events which are expressed from different per-
spectives, and hence expressed by using different
linguistic frames, and at the same time distinguish-
ing event mentions which share the same linguistic
frame but belong to different domain classes. Our
investigation indicates that this task is mainly depen-
dent on domain knowledge and how a phenomenon
can be conceptualized. Another task of TM is the ex-
traction of linguistically-oriented semantics, which
basically maps various syntactic realizations to the
shared structures. In order to develop a TM system,
we need to solve the two difficult tasks.
Second, TM could benefit from linguistically-
oriented frames by using them as an intermediat-
ing layer between text and domain-oriented infor-
mation. The domain-oriented semantic structures,
which is a target of TM, are inevitably dependent
on the domain. On the other hand, the extraction of
linguistically-oriented semantics from text is less de-
pendent. Therefore, using the linguistically-oriented
structure could be favorable to domain portability of
a TM system.
Our aim was explicitly linking linguistically-
oriented and domain-oriented semantics of the bio-
molecular articles, and the preliminary result show
the possibility of the extraction of linguistically-
oriented semantics contributing to TM. Further in-
v tigation of the relationship would be a important
step forward for TM in the bio-molecular domain.
Our investigation was preliminary. For exam-
ple, conversions from FN structures to GENIA event
structures, depicted in figures 4 and 5, were based
on manual investigation. Further, they were attested
by limited samples in the corpus. For our results to
contribute to a TM system, evaluation of the conver-
sions and automatic extraction of such conversions
must be considered.
7 Conclusion
This paper presents a relationship of domain-
oriented and linguistically-oriented frames of se-
mantics, obtained by an investigation of the GE-
NIA event corpus. In the investigation, we anno-
tated sample sentences from the GENIA event cor-
pus with linguistically-oriented semantic structures
as those of FrameNet, and compared them with
domain-oriented semantic annotations that the cor-
pus originally possesses. The resulting relations
between the domain-oriented and linguistically-
oriented frames suggest that mentions of a bio-
logical phenomenon could be realized in a num-
ber of linguistically-oriented frames, and that
the linguistically-oriented frames represent possible
perspectives in mentioning the phenomenon. The
resulting relations would illustrate a challenge in
developing a Text Mining system, and would indi-
cate importance of linguistically-oriented frames as
an intermediating layer between text and domain-
oriented information. Our future plan includes
evaluation of our conversions from a linguistically-
oriented to a domain-oriented structure, and auto-
matic extraction of such conversions.
169
References
M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein,
H. Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. S.
Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. Issel-
Tarver, A. Kasarskis, S. Lewis, J. C. Matese, J. E.
Richardson, M. Ringwald, G. M. Rubin, and G. Sher-
lock. 2000. Gene ontology: tool for the unification of
biology. The Gene Ontology Consortium. Nat Genet,
25(1):25?29, May.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
Semeval-2007 task 19: Frame semantic structure ex-
traction. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 99?104, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
K. Bretonnel Cohen, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9):e3158, 09.
Andrew Dolbey, Michael Ellsworth, and Jan Scheffczyk.
2006. Bioframenet: A domain-specific framenet
extension with links to biomedical ontologies. In
Proceedings of the Second International Workshop
on Formal Biomedical Knowledge Representation:
?Biomedical Ontology in Action? (KR-MED 2006),
volume 222 of CEUR Workshop Proceedings. CEUR-
WS.org, Nov.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Sanda M. Harabagiu, Cosmin Adrian Bejan, and Paul
Morarescu. 2005. Shallow semantics for relation
extraction. In IJCAI-05, Proceedings of the Nine-
teenth International Joint Conference on Artificial In-
telligence, pages 1061?1066.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(1):10.
Seth Kulick, Ann Bies, Mark Liberman, Mark Man-
del, Ryan McDonald, Martha Palmer, Andrew Schein,
Lyle Ungar, Scott Winters, and Pete White. 2004.
Integrated annotation for biomedical information ex-
traction. In Lynette Hirschman and James Puste-
jovsky, editors, HLT-NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, Ontolo-
gies and Databases, pages 61?68, Boston, Mas-
sachusetts, USA, May 6. Association for Computa-
tional Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The nombank
project: An interim report. In A. Meyers, editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annota-
tion, pages 24?31, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjorne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. Bioinfer: a corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(1):50.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August. Coling 2008 Organizing Committee.
Tuangthong Wattarujeekrit, Parantu Shah, and Nigel Col-
lier. 2004. Pasbio: predicate-argument structures for
event extraction in molecular biology. BMC Bioinfor-
matics, 5(1):155.
170
Proceedings of the Workshop on BioNLP: Shared Task, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Overview of BioNLP?09 Shared Task on Event Extraction
Jin-Dong Kim? Tomoko Ohta? Sampo Pyysalo? Yoshinobu Kano? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{jdkim,okap,smp,kano,tsujii}@is.s.u-tokyo.ac.jp
Abstract
The paper presents the design and implemen-
tation of the BioNLP?09 Shared Task, and
reports the final results with analysis. The
shared task consists of three sub-tasks, each of
which addresses bio-molecular event extrac-
tion at a different level of specificity. The data
was developed based on the GENIA event cor-
pus. The shared task was run over 12 weeks,
drawing initial interest from 42 teams. Of
these teams, 24 submitted final results. The
evaluation results are encouraging, indicating
that state-of-the-art performance is approach-
ing a practically applicable level and revealing
some remaining challenges.
1 Introduction
The history of text mining (TM) shows that shared
tasks based on carefully curated resources, such
as those organized in the MUC (Chinchor, 1998),
TREC (Voorhees, 2007) and ACE (Strassel et al,
2008) events, have significantly contributed to the
progress of their respective fields. This has also been
the case in bio-TM. Examples include the TREC Ge-
nomics track (Hersh et al, 2007), JNLPBA (Kim et
al., 2004), LLL (Ne?dellec, 2005), and BioCreative
(Hirschman et al, 2007). While the first two ad-
dressed bio-IR (information retrieval) and bio-NER
(named entity recognition), respectively, the last two
focused on bio-IE (information extraction), seeking
relations between bio-molecules. With the emer-
gence of NER systems with performance capable of
supporting practical applications, the recent interest
of the bio-TM community is shifting toward IE.
Similarly to LLL and BioCreative, the
BioNLP?09 Shared Task (the BioNLP task, here-
after) also addresses bio-IE, but takes a definitive
step further toward finer-grained IE. While LLL and
BioCreative focus on a rather simple representation
of relations of bio-molecules, i.e. protein-protein
interactions (PPI), the BioNLP task concerns the
detailed behavior of bio-molecules, characterized as
bio-molecular events (bio-events). The difference in
focus is motivated in part by different applications
envisioned as being supported by the IE methods.
For example, BioCreative aims to support curation
of PPI databases such as MINT (Chatr-aryamontri
et al, 2007), for a long time one of the primary tasks
of bioinformatics. The BioNLP task aims to support
the development of more detailed and structured
databases, e.g. pathway (Bader et al, 2006) or Gene
Ontology Annotation (GOA) (Camon et al, 2004)
databases, which are gaining increasing interest
in bioinformatics research in response to recent
advances in molecular biology.
As the first shared task of its type, the BioNLP
task aimed to define a bounded, well-defined bio-
event extraction task, considering both the actual
needs and the state of the art in bio-TM technology
and to pursue it as a community-wide effort. The
key challenge was in finding a good balance between
the utility and the feasibility of the task, which was
also limited by the resources available. Special con-
sideration was given to providing evaluation at di-
verse levels and aspects, so that the results can drive
continuous efforts in relevant directions. The pa-
per discusses the design and implementation of the
BioNLP task, and reports the results with analysis.
1
Type Primary Args. Second. Args.
Gene expression T(P)
Transcription T(P)
Protein catabolism T(P)
Phosphorylation T(P) Site
Localization T(P) AtLoc, ToLoc
Binding T(P)+ Site+
Regulation T(P/Ev), C(P/Ev) Site, CSite
Positive regulation T(P/Ev), C(P/Ev) Site, CSite
Negative regulation T(P/Ev), C(P/Ev) Site, CSite
Table 1: Event types and their arguments. The type of the
filler entity is specified in parenthesis. The filler entity
of the secondary arguments are all of Entity type which
represents any entity but proteins: T=Theme, C=Cause,
P=Protein, Ev=Event.
2 Task setting
To focus efforts on the novel aspects of the event
extraction task, is was assumed that named entity
recognition has already been performed and the task
was begun with a given set of gold protein anno-
tation. This is the only feature of the task setting
that notably detracts from its realism. However,
given that state-of-the-art protein annotation meth-
ods show a practically applicable level of perfor-
mance, i.e. 88% F-score (Wilbur et al, 2007), we
believe the choice is reasonable and has several ad-
vantages, including focus on event extraction and ef-
fective evaluation and analysis.
2.1 Target event types
Table 1 shows the event types addressed in the
BioNLP task. The event types were selected from
the GENIA ontology, with consideration given to
their importance and the number of annotated in-
stances in the GENIA corpus. The selected event
types all concern protein biology, implying that they
take proteins as their theme. The first three types
concern protein metabolism, i.e. protein production
and breakdown. Phosphorylation is a representa-
tive protein modification event, and Localization and
Binding are representative fundamental molecular
events. Regulation (including its sub-types, Posi-
tive and Negative regulation) represents regulatory
events and causal relations. The last five are uni-
versal but frequently occur on proteins. For the bio-
logical interpretation of the event types, readers are
referred to Gene Ontology (GO) and the GENIA on-
tology.
The failure of p65 translocation to the nucleus . . .
T3 (Protein, 40-46)
T2 (Localization, 19-32)
E1 (Type:T2, Theme:T3, ToLoc:T1)
T1 (Entity, 15-18)
M1 (Negation E1)
Figure 1: Example event annotation. The protein an-
notation T3 is given as a starting point. The extraction
of annotation in bold is required for Task 1, T1 and the
ToLoc:T1 argument for Task 2, and M1 for Task 3.
As shown in Table 1, the theme or themes of all
events are considered primary arguments, that is, ar-
guments that are critical to identifying the event. For
regulation events, the entity or event stated as the
cause of the regulation is also regarded as a primary
argument. For some event types, other arguments
detailing of the events are also defined (Secondary
Args. in Table 1).
From a computational point of view, the event
types represent different levels of complexity. When
only primary arguments are considered, the first five
event types require only unary arguments, and the
task can be cast as relation extraction between a
predicate (event trigger) and an argument (Protein).
The Binding type is more complex in requiring the
detection of an arbitrary number of arguments. Reg-
ulation events always take a Theme argument and,
when expressed, also a Cause argument. Note that a
Regulation event may take another event as its theme
or cause, a unique feature of the BioNLP task com-
pared to other event extraction tasks, e.g. ACE.
2.2 Representation
In the BioNLP task, events are expressed using three
different types of entities. Text-bound entities (t-
entities hereafter) are represented as text spans with
associated class information. The t-entities include
event triggers (Localization, Binding, etc), protein
references (Protein) and references to other entities
(Entity). A t-entity is represented by a pair, (entity-
type, text-span), and assigned an id with the pre-
fix ?T?, e.g. T1?T3 in Figure 1. An event is ex-
pressed as an n-tuple of typed t-entities, and has
a id with prefix ?E?, e.g. E1. An event modifi-
cation is expressed by a pair, (predicate-negation-
or-speculation, event-id), and has an id with prefix
?M?, e.g. M1.
2
Item Training Devel. Test
Abstract 800 150 260
Sentence 7,449 1,450 2,447
Word 176,146 33,937 57,367
Event 8,597 / 8,615 1,809 / 1,815 3,182 / 3,193
Table 2: Statistics of the data sets. For events,
Task1/Task2 shown separately as secondary arguments
may introduce additional differentiation of events.
2.3 Subtasks
The BioNLP task targets semantically rich event ex-
traction, involving the extraction of several different
classes of information. To facilitate evaluation on
different aspects of the overall task, the task is di-
vided to three sub-tasks addressing event extraction
at different levels of specificity.
Task 1. Core event detection detection of typed,
text-bound events and assignment of given pro-
teins as their primary arguments.
Task 2. Event enrichment recognition of sec-
ondary arguments that further specify the
events extracted in Task 1.
Task 3. Negation/Speculation detection detection
of negations and speculation statements
concerning extracted events.
Task 1 serves as the backbone of the shared task and
is mandatory for all participants. Task 2 involves the
recognition of Entity type t-entities and assignment
of those as secondary event arguments. Task 3 ad-
dresses the recognition of negated or speculatively
expressed events without specific binding to text. An
example is given in Fig. 1.
3 Data preparation
The BioNLP task data were prepared based on the
GENIA event corpus. The data for the training and
development sets were derived from the publicly
available event corpus (Kim et al, 2008), and the
data for the test set from an unpublished portion of
the corpus. Table 2 shows statistics of the data sets.
For data preparation, in addition to filtering out
irrelevant annotations from the original GENIA cor-
pus, some new types of annotation were added to
make the event annotation more appropriate for the
purposes of the shared task. The following sections
describe the key changes to the corpus.
3.1 Gene-or-gene-product annotation
The named entity (NE) annotation of the GENIA
corpus has been somewhat controversial due to dif-
ferences in annotation principles compared to other
biomedical NE corpora. For instance, the NE an-
notation in the widely applied GENETAG corpus
(Tanabe et al, 2005) does not differentiate proteins
from genes, while GENIA annotation does. Such
differences have caused significant inconsistency in
methods and resources following different annota-
tion schemes. To remove or reduce the inconsis-
tency, GENETAG-style NE annotation, which we
term gene-or-gene-product (GGP) annotation, has
been added to the GENIA corpus, with appropriate
revision of the original annotation. For details, we
refer to (Ohta et al, 2009). The NE annotation used
in the BioNLP task data is based on this annotation.
3.2 Argument revision
The GENIA event annotation was made based on
the GENIA event ontology, which uses a loose typ-
ing system for the arguments of each event class.
For example, in Figure 2(a), it is expressed that
the binding event involves two proteins, TRAF2
and CD40, and that, in the case of CD40, its cy-
toplasmic domain takes part in the binding. With-
out constraints on the type of theme arguments,
the following two annotations are both legitimate:
(Type:Binding, Theme:TRAF2, Theme:CD40)
(Type:Binding, Theme:TRAF2,
Theme:CD40 cytoplasmic domain)
The two can be seen as specifying the same event
at different levels of specificity1. Although both al-
ternatives are reasonable, the need to have consis-
tent training and evaluation data requires a consis-
tent choice to be made for the shared task.
Thus, we fix the types of all non-event
primary arguments to be proteins (specifically
GGPs). For GENIA event annotations involving
themes other than proteins, additional argument
types were introduced, for example, as follows:
1In the GENIA event annotation guidelines, annotators are
instructed to choose the more specific alternative, thus the sec-
ond alternative for the example case in Fig. 2(a).
3
(a)
TRAF2 is a ? which binds to the CD40 cytoplasmic domain
GGP GGP PDR
(b)
HMG-I binds to GATA motifs
GGP DDR
(c)
alpha B2 bound the PEBP2 site within the GM-CSF promoter
GGP GGPDDR DDR
Figure 2: Entity annotation to example sentences
from (a) PMID10080948, (b) PMID7575565, and (c)
PMID7605990 (simplified).
(a)
Ah receptor recognizes the B cell transcription factor, BSAP
(b)
Grf40 binds to linker for activation of T cells (LAT)
(c)
expression of p21(WAF1/CIP1) and p27(KIP1)
(d)
included both p50/p50 and p50/p65 dimers
(e)
IL-4 Stat, also known as Stat6
Figure 3: Equivalent entities in example sentences from
(a) PMID7541987 (simplified), (b) PMID10224278, (c)
PMID10090931, (d) PMID9243743, (e) PMID7635985.
(Type:Binding, Theme1:TRAF2, Theme2:CD40,
Site2:cytoplasmic domain)
Note that the protein, CD40, and its domain, cyto-
plasmic domain, are associated by argument num-
bering. To resolve issues related to the mapping
between proteins and related entities systematically,
we introduced partial static relation annotation for
relations such as Part-Whole, drawing in part on
similar annotation of the BioInfer corpus (Pyysalo
et al, 2007). For details of this part of the revision
process, we refer to (Pyysalo et al, 2009).
Figure 2 shows some challenging cases. In (b),
the site GATA motifs is not identified as an argument
of the binding event, because the protein containing
it is not stated. In (c), among the two sites (PEBP2
site and promoter) of the gene GM-CSF, only the
more specific one, PEBP2, is annotated.
3.3 Equivalent entity references
Alternative names for the same object are fre-
quently introduced in biomedical texts, typically
through apposition. This is illustrated in Figure 3(a),
where the two expressions B cell transcription fac-
tor and BSAP are in apposition and refer to the
same protein. Consequently, in this case the fol-
lowing two annotations represent the same event:
(Type:Binding, Theme:Ah receptor,
Theme:B cell transcription factor)
(Type:Binding, Theme:Ah receptor, Theme:BSAP)
In the GENIA event corpus only one of these is an-
notated, with preference given to shorter names over
longer descriptive ones. Thus of the above exam-
ple events, the latter would be annotated. How-
ever, as both express the same event, in the shared
task evaluation either alternative was accepted as
correct extraction of the event. In order to im-
plement this aspect of the evaluation, expressions
of equivalent entities were annotated as follows:
Eq (B cell transcription factor, BSAP)
The equivalent entity annotation in the revised GE-
NIA corpus covers also cases other than simple ap-
position, illustrated in Figure 3. A frequent case in
biomedical literature involves use of the slash sym-
bol (?/?) to state synonyms. The slash symbol is
ambiguous as it is used also to indicate dimerized
proteins. In the case of p50/p50, the two p50 are
annotated as equivalent because they represent the
same proteins at the same state. Note that although
rare, also explicitly introduced aliases are annotated,
as in Figure 3(e).
4 Evaluation
For the evaluation, the participants were given the
test data with gold annotation only for proteins. The
evaluation was then carried out by comparing the
annotation predicted by each participant to the gold
annotation. For the comparison, equality of anno-
tations is defined as described in Section 4.1. The
evaluation results are reported using the standard
recall/precision/f-score metrics, under different cri-
teria defined through the equalities.
4.1 Equalities and Strict matching
Equality of events is defined as follows:
Event Equality equality holds between any two
events when (1) the event types are the same,
(2) the event triggers are the same, and (3) the
arguments are fully matched.
4
A full matching of arguments between two events
means there is a perfect 1-to-1 mapping between the
two sets of arguments. Equality of individual argu-
ments is defined as follows:
Argument Equality equality holds between any
two arguments when (1) the role types are the
same, and (2-1) both are t-entities and equality
holds between them, or (2-2) both are events
and equality holds between them.
Due to the condition (2-2), event equality is defined
recursively for events referring to events. Equality
of t-entities is defined as follows:
T-entity Equality equality holds between any two
t-entities when (1) the entity types are the same,
and (2) the spans are the same.
Any two text spans (beg1, end1) and (beg2, end2),
are the same iff beg1 = beg2 and end1 = end2.
Note that the event triggers are also t-entities thus
their equality is defined by the t-entity equality.
4.2 Evaluation modes
Various evaluation modes can be defined by varying
equivalence criteria. In the following, we describe
three fundamental variants applied in the evaluation.
Strict matching The strict matching mode requires
exact equality, as defined in section 4.1. As some
of its requirements may be viewed as unnecessarily
precise, practically motivated relaxed variants, de-
scribed in the following, are also applied.
Approximate span matching The approximate
span matching mode is defined by relaxing the
requirement for text span matching for t-entities.
Specifically, a given span is equivalent to a gold
span if it is entirely contained within an extension
of the gold span by one word both to the left and
to the right, that is, beg1 ? ebeg2 and end1 ?
eend2, where (beg1, end1) is the given span and
(ebeg2, eend2) is the extended gold span.
Approximate recursive matching In strict match-
ing, for a regulation event to be correct, the events it
refers to as theme or cause must also be be strictly
correct. The approximate recursive matching mode
is defined by relaxing the requirement for recursive
event matching, so that an event can match even
if the events it refers to are only partially correct.
Event Release date
Announcement Dec 8
Sample data Dec 15
Training data Jan 19 ? 21, Feb 2 (rev1), Feb 10 (rev2)
Devel. data Feb 7
Test data Feb 22 ? Mar 2
Submission Mar 2 ? Mar 9
Table 3: Shared task schedule. The arrows indicate a
change of schedule.
Specifically, for partial matching, only Theme argu-
ments are considered: events can match even if re-
ferred events differ in non-Theme arguments.
5 Schedule
The BioNLP task was held for 12 weeks, from the
sample data release to the final submission. It in-
cluded 5 weeks of system design period with sam-
ple data, 6 weeks of system development period with
training and development data, and a 1 week test pe-
riod. The system development period was originally
planned for 5 weeks but extended by 1 week due to
the delay of the training data release and the revi-
sion. Table 3 shows key dates of the schedule.
6 Supporting Resources
To allow participants to focus development efforts
on novel aspects of event extraction, we prepared
publicly available BioNLP resources readily avail-
able for the shared task. Several fundamental
BioNLP tools were provided through U-Compare
(Kano et al, 2009)2, which included tools for to-
kenization, sentence segmentation, part-of-speech
tagging, chunking and syntactic parsing.
Participants were also provided with the syntactic
analyses created by a selection of parsers. We ap-
plied two mainstream Penn Treebank (PTB) phrase
structure parsers: the Bikel parser3, implementing
Collins? parsing model (Bikel, 2004) and trained
on PTB, and the reranking parser of (Charniak
and Johnson, 2005) with the self-trained biomed-
ical parsing model of (McClosky and Charniak,
2008)4. We also applied the GDep5, native de-
pendency parser trained on the GENIA Treebank
2http://u-compare.org/
3http://www.cis.upenn.edu/?dbikel/software.html
4http://www.cs.brown.edu/?dmcc/biomedical.html
5http://www.cs.cmu.edu/?sagae/parser/gdep/
5
NLP Task
Team Task Org Word Chunking Parsing Trigger Argument Ext. Resources
UTurku 1-- 3C+2BI Porter MC SVM SVM (SVMlight)
JULIELab 1-- 1C+2L+2B OpenNLP OpenNLP GDep Dict+Stat SVM(libSVM) UniProt, Mesh,
Porter ME(Mallet) GOA, UMLS
ConcordU 1-3 3C Stanford Stanford Dict+Stat Rules WordNet, VerbNet,
UMLS
UT+DBCLS 12- 2C Porter MC Dict MLN(thebeast)
CCG
VIBGhent 1-3 2C+1B Porter, Stanford Dict SVM(libSVM)
UTokyo 1-- 3C GTag GDep, Dict ME(liblinear) UIMA
Enju
UNSW 1-- 1C+1B GDep CRF Rules WordNet, MetaMap
UZurich 1-- 3C LingPipe, LTChunk Pro3Gres Dict Rules
Morpha
ASU+HU+BU 123 6C+2BI Porter BioLG, Dict Rules Lucene
Charniak Rules
Cam 1-- 3C Porter RASP Dict Rules
UAntwerp 12- 3C GTag GDep MBL MBL(TiMBL)
Rules
UNIMAN 1-- 4C+2BI Porter GDep Dict, CRF SVM MeSH, GO
GTag Rules
SCAI 1-- 1C Rules
UAveiro 1-- 1C+1L NooJ NooJ Rules BioLexicon
USzeged 1-3 3C+1B GTag Dict, VSM C4.5(WEKA) BioScope
Rules
NICTA 1-3 4C GTag ERG CRF(CRF++) Rules JULIE
CNBMadrid 12- 2C+1B Porter, GTag CBR
GTag Rules
CCP-BTMG 123 7C LingPipe LingPipe OpenDMAP LingPipe, CM Rules GO, SO, MIO,
UIMA
CIPS-ASU 1-- 3C MontyTagger Custom Stanford CRF(ABNER) Rules,
NB(WEKA)
UMich 1-- 2C Stanford MC Dict SVM(SVMlight)
PIKB 1-- 5C+2B MIRA MIRA
KoreaU 1-- 5C GTag GDep Rules, ME ME WSJ
Table 4: Profiles of the participants: GTag=GENIAtagger, MLN=Markov Logic Network, UMLS=UMLS SPE-
CIALIST Lexicon/tools, MC=McClosky-Charniak, GDep=Genia Dependency Parser, Stanford=Stanford Parser,
CBR=Case-Based Reasoning, CM=ConceptMapper.
(Tateisi et al, 2005), and a version of the C&C CCG
deep parser6 adapted to biomedical text (Rimell and
Clark, 2008).
The text of all documents was segmented and to-
kenized using the GENIA Sentence Splitter and the
GENIA Tagger, provided by U-Compare. The same
segmentation was enforced for all parsers, which
were run using default settings. Both the native out-
put of each parser and a representation in the popular
Stanford Dependency (SD) format (de Marneffe et
al., 2006) were provided. The SD representation was
created using the Stanford tools7 to convert from the
PTB scheme, the custom conversion introduced by
(Rimell and Clark, 2008) for the C&C CCG parser,
and a simple format-only conversion for GDep.
7 Results and Discussion
7.1 Participation
In total, 42 teams showed interest in the shared task
and registered for participation, and 24 teams sub-
6http://svn.ask.it.usyd.edu.au/trac/candc/wiki
7http://nlp.stanford.edu/software/lex-parser.shtml
mitted final results. All 24 teams participated in the
obligatory Task 1, six in each of Tasks 2 and 3, and
two teams completed all the three tasks.
Table 4 shows a profile of the 22 final teams,
excepting two who wished to remain anonymous.
A brief examination on the team organization (the
Org column) shows a computer science background
(C) to be most frequent among participants, with
less frequent participation from bioinformaticians
(BI), biologists (B) and liguists (L). This may be
attributed in part to the fact that the event extrac-
tion task required complex computational modeling.
The role of computer scientists may be emphasized
in part due to the fact that the task was novel to most
participants, requiring particular efforts in frame-
work design and implementation and computational
resources. This also suggests there is room for im-
provement from more input from biologists.
7.2 Evaluation results
The final evaluation results of Task 1 are shown in
Table 5. The results on the five event types involv-
6
Team Simple Event Binding Regulation All
UTurku 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95
JULIELab 59.81 / 79.80 / 68.38 49.57 / 35.25 / 41.20 35.03 / 34.18 / 34.60 45.82 / 47.52 / 46.66
ConcordU 49.75 / 81.44 / 61.76 20.46 / 40.57 / 27.20 27.47 / 49.89 / 35.43 34.98 / 61.59 / 44.62
UT+DBCLS 55.75 / 72.74 / 63.12 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35
VIBGhent 54.48 / 79.31 / 64.59 38.04 / 38.60 / 38.32 17.36 / 31.61 / 22.41 33.41 / 51.55 / 40.54
UTokyo 45.69 / 72.19 / 55.96 34.58 / 50.63 / 41.10 14.22 / 34.26 / 20.09 28.13 / 53.56 / 36.88
UNSW 45.85 / 69.94 / 55.39 23.63 / 37.27 / 28.92 16.58 / 28.27 / 20.90 28.22 / 45.78 / 34.92
UZurich 44.92 / 66.62 / 53.66 30.84 / 37.28 / 33.75 14.82 / 30.21 / 19.89 27.75 / 46.60 / 34.78
ASU+HU+BU 45.09 / 76.80 / 56.82 19.88 / 44.52 / 27.49 05.20 / 33.46 / 09.01 21.62 / 62.21 / 32.09
Cam 39.17 / 76.40 / 51.79 12.68 / 31.88 / 18.14 09.98 / 37.76 / 15.79 21.12 / 56.90 / 30.80
UAntwerp 41.29 / 65.68 / 50.70 12.97 / 31.03 / 18.29 11.07 / 29.85 / 16.15 22.50 / 47.70 / 30.58
UNIMAN 50.00 / 63.21 / 55.83 12.68 / 40.37 / 19.30 04.05 / 16.75 / 06.53 22.06 / 48.61 / 30.35
SCAI 43.74 / 70.73 / 54.05 28.82 / 35.21 / 31.70 12.64 / 16.55 / 14.33 25.96 / 36.26 / 30.26
UAveiro 43.57 / 71.63 / 54.18 13.54 / 34.06 / 19.38 06.29 / 21.05 / 09.69 20.93 / 49.30 / 29.38
Team 24 41.29 / 64.72 / 50.41 22.77 / 35.43 / 27.72 09.38 / 19.23 / 12.61 22.69 / 40.55 / 29.10
USzeged 47.63 / 44.44 / 45.98 15.27 / 25.73 / 19.17 04.17 / 18.21 / 06.79 21.53 / 36.99 / 27.21
NICTA 31.13 / 77.31 / 44.39 16.71 / 29.00 / 21.21 07.80 / 18.12 / 10.91 17.44 / 39.99 / 24.29
CNBMadrid 50.25 / 46.59 / 48.35 33.14 / 20.54 / 25.36 12.22 / 07.99 / 09.67 28.63 / 20.88 / 24.15
CCP-BTMG 28.17 / 87.63 / 42.64 12.68 / 40.00 / 19.26 03.09 / 48.11 / 05.80 13.45 / 71.81 / 22.66
CIPS-ASU 39.68 / 38.60 / 39.13 17.29 / 31.58 / 22.35 11.86 / 08.15 / 09.66 22.78 / 19.03 / 20.74
UMich 52.71 / 25.89 / 34.73 31.70 / 12.61 / 18.05 14.22 / 06.56 / 08.98 30.42 / 14.11 / 19.28
PIKB 26.65 / 75.72 / 39.42 07.20 / 39.68 / 12.20 01.09 / 30.51 / 02.10 11.25 / 66.54 / 19.25
Team 09 27.16 / 43.61 / 33.47 03.17 / 09.82 / 04.79 02.42 / 11.90 / 04.02 11.69 / 31.42 / 17.04
KoreaU 20.56 / 66.39 / 31.40 12.97 / 50.00 / 20.59 00.67 / 37.93 / 01.31 09.40 / 61.65 / 16.31
Table 5: Evaluation results of Task 1 (recall / precision / f-score).
Team All Site for Phospho.(56) AtLoc & ToLoc (65) All Second Args.
UT+DBCLS 35.86 / 54.08 / 43.12 71.43 / 71.43 / 71.43 23.08 / 88.24 / 36.59 32.14 / 72.41 / 44.52
UAntwerp 21.52 / 45.77 / 29.27 00.00 / 00.00 / 00.00 01.54 /100.00 / 03.03 06.63 / 52.00 / 11.76
ASU+HU+BU 19.70 / 56.87 / 29.26 00.00 / 00.00 / 00.00 00.00 / 00.00 / 00.00 00.00 / 00.00 / 00.00
Team 24 22.08 / 38.28 / 28.01 55.36 / 93.94 / 69.66 21.54 / 66.67 / 32.56 30.10 / 76.62 / 43.22
CCP-BTMG 13.25 / 70.97 / 22.33 30.36 /100.00 / 46.58 00.00 / 00.00 / 00.00 08.67 /100.00 / 15.96
CNBMadrid 25.02 / 18.32 / 21.15 85.71 / 57.14 / 68.57 32.31 / 47.73 / 38.53 50.00 / 09.71 / 16.27
Table 6: Evaluation results for Task 2.
ing only a single primary theme argument are shown
in one merged class, ?Simple Event?. The broad per-
formance range (31% ? 70%) indicates even the ex-
traction of simple events is not a trivial task. How-
ever, the top-ranked systems show encouraging per-
formance, achieving or approaching 70% f-score.
The performance ranges for Binding (5% ? 44%)
and Regulation (1% ? 40%) events show their ex-
traction to be clearly more challenging. It is in-
teresting that while most systems show better per-
formance for binding over regulation events, the
systems [ConcordU] and [UT+DBCLS] are better
for regulation, showing somewhat reduced perfor-
mance for Binding events. This is in particular con-
trast to the following two systems, [ViBGhent] and
[UTokyo], which show far better performance for
Binding than Regulation events. As one possible
explanation, we find that the latter two differentiate
binding events by their number of themes, while the
former two give no specific treatment to multi-theme
binding events. Such observations and comparisons
are a clear benefit of a community-wide shared task.
Table 6 shows the evaluation results for the teams
who participated in Task 2. The ?All? column shows
the overall performance of the systems for Task 2,
while the ?All Second Args.? column shows the
performance of finding only the secondary argu-
ments. The evaluation results show considerable
differences between the criteria. For example, the
system [Team 24] shows performance comparable
to the top ranked system in finding secondary argu-
ments, although its overall performance for Task 2
is more limited. Table 6 also shows the three sys-
tems, [UT+DBCLS], [Team 24] and [CNBMadrid],
7
Team Negation Speculation
ConcordU 14.98 / 50.75 / 23.13 16.83 / 50.72 / 25.27
VIBGhent 10.57 / 45.10 / 17.13 08.65 / 15.79 / 11.18
ASU+HU+BU 03.96 / 27.27 / 06.92 06.25 / 28.26 / 10.24
NICTA 05.29 / 34.48 / 09.17 04.81 / 30.30 / 08.30
USzeged 05.29 / 01.94 / 02.84 12.02 / 03.88 / 05.87
CCP-BTMG 01.76 / 05.26 / 02.64 06.73 / 13.33 / 08.95
Table 7: Evaluation results for Task 3.
 0
 10
 20
 30
 40
 50
 60
02/18 02/21 02/24 02/27 03/02 03/05 03/08
daily average
Figure 4: Scatterplot of the evaluation results on the de-
velopment data during the system development period.
show performance at a practical level in particular in
finding specific sites of phosphorylation.
As shown in Table 7, the performance range for
Task 3 is very low although the representation of the
task is as simple as the simple events. We attribute
the reason to the fact that Task 3 is the only task of
which the annotation is not bound to textual clue,
thus no text-bound annotation was provided.
Figure 4 shows a scatter plot of the performance
of the participating systems during the system devel-
opment period. The performance evaluation comes
from the log of the online evaluation system on the
development data. It shows the best performance
and the average performance of the participating
systems were trending upwards up until the dead-
line of final submission, which indicates there is still
much potential for improvement.
7.3 Ensemble
Table 8 shows experimental results of a system en-
semble using the final submissions. For the ex-
periments, the top 3?10 systems were chosen, and
the output of each system treated as a weighted
vote8. Three weighting schemes were used; ?Equal?
weights each vote equally; ?Averaged? weights each
8We used the ?ensemble? function of U-Compare.
Ensemble Equal Averaged Event Type
Top 3 53.19 53.19 54.08
Top 4 54.34 54.34 55.21
Top 5 54.77 55.03 55.10
Top 6 55.13 55.77 55.96
Top 7 54.33 55.45 55.73
Top 10 52.79 54.63 55.18
Table 8: Experimental results of system ensemble.
vote by the overall f-score of the system; ?Event
Type? weights each vote by the f-score of the sys-
tem for the specific event type. The best score,
55.96%, was obtained by the ?Event Type? weight-
ing scheme, showing a 4% unit improvement over
the best individual system. While using the final
scores for weighting uses data that would not be
available in practice, similar weighting could likely
be obtained e.g. using performance on the devel-
opment data. The experiment demonstrates that an
f-score better than 55% can be achieved simply by
combining the strengths of the systems.
8 Conclusion
Meeting with the community-wide participation, the
BioNLP Shared Task was successful in introducing
fine-grained event extraction to the domain. The
evaluation results of the final submissions from the
participants are both promising and encouraging for
the future of this approach to IE. It has been revealed
that state-of-the-art performance in event extraction
is approaching a practically applicable level for sim-
ple events, and also that there are many remain-
ing challenges in the extraction of complex events.
A brief analysis suggests that the submitted data
together with the system descriptions are rich re-
sources for finding directions for improvements. Fi-
nally, the experience of the shared task participants
provides an invaluable basis for cooperation in fac-
ing further challenges.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Genome Network Project (MEXT, Japan).
8
References
Gary D. Bader, Michael P. Cary, and Chris Sander. 2006.
Pathguide: a Pathway Resource List. Nucleic Acids
Research., 34(suppl 1):D504?506.
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
Evelyn Camon, Michele Magrane, Daniel Barrell, Vi-
vian Lee, Emily Dimmer, John Maslen, David Binns,
Nicola Harte, Rodrigo Lopez, and Rolf Apweiler.
2004. The Gene Ontology Annotation (GOA)
Database: sharing knowledge in Uniprot with Gene
Ontology. Nucl. Acids Res., 32(suppl 1):D262?266.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Andrew Chatr-aryamontri, Arnaud Ceol, Luisa Montec-
chi Palazzi, Giuliano Nardelli, Maria Victoria Schnei-
der, Luisa Castagnoli, and Gianni Cesareni. 2007.
MINT: the Molecular INTeraction database. Nucleic
Acids Research, 35(suppl 1):D572?574.
Nancy Chinchor. 1998. Overview of MUC-7/MET-2.
In Message Understanding Conference (MUC-7) Pro-
ceedings.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
William Hersh, Aaron Cohen, Ruslenm Lynn, , and
Phoebe Roberts. 2007. TREC 2007 Genomics track
overview. In Proceeding of the Sixteenth Text RE-
trieval Conference.
Lynette Hirschman, Martin Krallinger, and Alfonso Va-
lencia, editors. 2007. Proceedings of the Second
BioCreative Challenge Evaluation Workshop. CNIO
Centro Nacional de Investigaciones Oncolo?gicas.
Yoshinobu Kano, William Baumgartner, Luke McCro-
hon, Sophia Ananiadou, Kevin Cohen, Larry Hunter,
and Jun?ichi Tsujii. 2009. U-Compare: share and
compare text mining tools with UIMA. Bioinformat-
ics. To appear.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction
to the bio-entity recognition task at JNLPBA. In Pro-
ceedings of the International Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA), pages 70?75.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (ACL-HLT?08), pages 101?104.
Claire Ne?dellec. 2005. Learning Language in Logic -
Genic Interaction Extraction Challenge. In J. Cussens
and C. Ne?dellec, editors, Proceedings of the 4th Learn-
ing Language in Logic Workshop (LLL05), pages 31?
37.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, and
Jun?ichi Tsujii. 2009. Incorporating GENETAG-style
annotation to GENIA corpus. In Proceedings of Nat-
ural Language Processing in Biomedicine (BioNLP)
NAACL 2009 Workshop. To appear.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static Relations: a Piece
in the Biomedical Information Extraction Puzzle.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop. To
appear.
Laura Rimell and Stephen Clark. 2008. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, To Appear.
Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi
Song, and Kazuaki Maeda. 2008. Linguistic Re-
sources and Evaluation Techniques for Evaluation of
Cross-Document Automatic Content Extraction. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-
ten, and John Wilbur. 2005. Genetag: a tagged cor-
pus for gene/protein named entity recognition. BMC
Bioinformatics, 6(Suppl 1):S3.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the IJCNLP 2005,
Companion volume, pages 222?227.
Ellen Voorhees. 2007. Overview of TREC 2007. In
The Sixteenth Text REtrieval Conference (TREC 2007)
Proceedings.
John Wilbur, Lawrence Smith, and Lorraine Tanabe.
2007. BioCreative 2. Gene Mention Task. In
L. Hirschman, M. Krallinger, and A. Valencia, editors,
Proceedings of Second BioCreative Challenge Evalu-
ation Workshop, pages 7?16.
9
KCAT : A Korean Corpus Annotating Tool Minimizing Human 
Intervention 
Won-He Ryu, Jin-Dong Kim, l ine-Chang Rim 
Dept. of Computer Science & Engineering, 
Natural Language Processing Lab, 
Korea University 
Anam-dong 5-ga, Seongbuk-gu, Seoul, Korea 
whryu, jin, rim @nlp.korea.ac.kr 
Abstract 
While large POS(part-of-speech) annotated 
corpora play an important role in natural 
language processing, the annotated corpus 
requires very high accuracy and consistency. 
To build such an accurate and consistent 
corpus, we often use a manual tagging 
method. But the manual tagging is very 
labor intensive and expensive. Furthernaore, 
it is not easy to get consistent results from 
the humari experts. In this paper, we present 
an efficient tool lbr building large accurate 
and consistent corpora with minimal human 
labor. The proposed tool supports semi- 
automatic tagging. Using disambiguation 
rules acquired from human experts, it 
minimizes the human intervention in both 
the manual tagging and post-editing steps. 
1. Introduction 
The POS annotated corpora are very 
important as a resource of usefiil information tbr 
natural language processing. A problem for 
corpus annotation is tile trade-off between 
efficiency and accuracy. 
Although manual POS ta,,<,in,,==  is very 
reliable, it is labor intcnsive and hard to make a 
consistent POS tagged corpus. On the other hand, 
automatic ta,-,in,,>~  is prone to erroi-s Ibr 
infrequently occurring words duo to tile lack el" 
overall linguistic information. At present, it is 
ahnost impossible to construct a highly accurate 
corptls by usin<,~ an automatic taggcr~ alone. 
/ks a consequence, a semi-autonmtic ta,,,,in,~== 
method is proposed IBi corpus annotation. In 
Heui-.Seok Lira 
Information Communications Department, 
Natural Language Processing Lab, 
Chonan University 
85-1, Anseo-Dong, Chonan City, 
ChungChong-NamDo Province, Korea 
timhs@inli~com.chonan.ac.kr 
ordiriary semi-automatic tagging, an automatic 
tagger tags each word and human experts correct 
the rots-tagged words in the post-editing step. 
But, in the post-editing step, as the human expert 
cannot know which word has been annotated 
incorrectly, he must check every word in the 
whole corpus. And he lnust do the same work 
again and again for the same words in the same 
context. This situation causes as Inuch 
labor-intensive work as in manual ta<+<qlw 
In this paper, we propose a semi-automatic 
tagging method that can reduce the human labor 
and guarantee the consistent tagging. 
2o System Requivemer~ts 
To develop ari efficient tool that attempts to 
build a large accurately armotated corpus with 
minimal human labor~ we must consider the 
following requirements: 
? In order to minimize human labor, the same 
human intervention to tag and to correct the 
same word in tile same context should not be 
repeated. 
* There may be a word which was tagged 
inconsistently in the same context becatlse it 
was tagged by different human experts or at a 
different ask time. As an elticient tool, it can 
prevent tile inconsistency of tile annotated 
( I  results and ~uarantec the consistency of the 
annotated results. 
* It must provide an effective annotating 
capability lbr many unknown words in the 
whole corpus. 
1096 
3. Proposed POS Tagging ToohKCAT 
The proposed POG tagging tool is used to 
combine the manual tagging method and the 
automatic tagging method. They are integrated 
to increase the accuracy o\[" the automatic tagging 
method and to minimize the amount of tile 
human labor of thc manual tagging method. 
Figure 1 shows the overall architecture of the 
proposed tagging tool :KCAT. 
I . . . . . . . . .  I I I ~ I P I Raw (..rpus ILI 
Pos t -Fn Jcess  ~t  I re - l rocess  
( ' c J r rec t  an  ~ . . . . . . . . . . .  :~  ; . . . .  ,R  s " " . . . . . . . . . .  
- -7 - -~ i - -g  
~____~ " : . 
i ~'f::: 2aa' :ii,:n~ ...... 
Figure 1. System Architecture of KCAT 
As shown in figm'e 1, KCAT consists of 
three modules: the pre-processing module, the 
automatic tagging module, and the 
post-processing module. In the prcoprocessing 
module, the disambiguation rules are acquired 
I%m human experts. The candidate words are 
Ihe target words whose disambiguation rules are 
acquired. The candidate words can be unknown 
words and also very frequent words. In addition, 
the words with problematic ambiguity for tlle 
automatic tagger can become candidates. 
l)lsamblguation rules are acquired with minimal 
human labor using tile tool t:n'oposed in 
(Lee, 1996). In the automatic tagging naodule, the 
disambiguation rules resolve the ambiguity of 
{,'very word to which they can be applied. 
I lowever, tile rules are certainly not sufficient o 
resolve all the ambiguity of the whole words in 
file corpus. The proper tags are assigned to the 
remaining ambiguous words by a stochastic 
< t~"  c, hL l l l l an  lagger. After the automatic t, m~, a 
expert corrects tile onors o\[ the stochastic ta,me, 
The system presents the expert with the results 
of the stochastic tagger. If the result is incorrect, 
tile hulllan expel1 corrects the error and 
generates a disambiguation rule ~br the word. 
The rule is also saved in the role base in order to 
bc used later. 
3. I. l.exical Rules for Disambiguation 
There are many ambiguous words that are 
extremely difficult to resolve alnbiguities by 
using a stochastic tagger. Due to the problematic 
words, manual tagging and manual correction 
must be done to build a correct coqms. Such 
human intervention may be repeated again and 
again to tag or to correct tile same word in the 
same context. 
For example, a human expert should assign 
'Nal(flying)/Verb+Neun/Ending' to every 
'NaNemf repeatedly in the following sentences: 
" Keu-Nyeo-Neun Ha-Neul-Eul Na-Neun 
Pi-Haeng-Ki-Reul Port Ceok-i Iss-Ta." (she has 
seen a flying plane) 
"Keu-Netm lht-Nc'ul-Eul NaoNeun 
t'i-Itaeng--Ki-Reul Port Ceok-i Eops-Ta." (he has 
never seen a flying phme) 
"Keu-Netm tta-Ne,tl-Eul Na-Neun 
Pi--ttaeng--Ki-Reul Pal-Myeong-tlaess- Ta." (he 
invented a flying plane) 
In the above sentences, human experts can 
resolve the word, 'Na-Nemf with only the 
previous and ttle next lexical information: 
'fla-Neul-Eul' and 'Pi-tlaeng- Ki-Reul'. In other 
words, tile human expert has to waste time on 
tagging the same word in tile same context 
repeatedly. This inefficiency can also be 
happened in the manual correction of the 
ntis-tagged words. So, if the human expert can 
make a rule with his disambiguation knowledge 
and use it for tile same words in tile same 
context, such inefficiency can be minimized. We 
define the disambiguation rule as a lexical rule. 
Its template is as follows. 
\[P:N\] \[Current Word\] \[Context\] = \[Tagging 
P, esuh\] 
Context ? Previous words?p * Next Words?,, 
Ill tile above template, p and n mean tile 
previous and the next context size respectively. 
For the present, p and n are limited to 3. '*' 
1097 
represents the separating mark between the 
previous and next context. For example, tile rule 
\[1:1\] \[Na-,'\:lten\] \[Ha-Neul-Eld * Pi-Haeng-Ki- 
Reul\] = \[Na/(flying)/Verb i- Neun/Ending \]says 
the tag 'Nal(flying)/Verb +Neun/Ending' should 
be assigned to the word 'Na-Neun' when the 
previous word and the next word is 
'Ha-Neul-Eul' and 'Pi-Haeng-Ki-Reul'. 
Although these lexical rules cannot always 
correctly disambiguate all Korean words, they 
are enough to cover many problematic 
ambignous words. We can gain some advantages 
of using the lexical rule. First, it is very accurate 
because it refers to the very specific lexical 
information. Second, the possibility of rule 
conflict is very little even though the number of 
the rules is increased. Third, it can resolve 
problematic ambiguity that cannot be resolved 
without semantic inf'onnation(Lim, 1996). 
3.2. Lexicai Rule Acquisition 
Lexical rules are acquired for the unknown 
words and the problematic words that are likely 
to be tagged erroneously by an automatic tagger. 
Lexical rule acquisition is perlbrmed by 
following steps: 
1. The system builds a candidate list of 
words li)r which the lexical rules would be 
acquired. The candidate list is the collection 
of all examples of unknown words and 
problematic words for an automatic tagger. 
2. A human expert selects a word from the 
list and makes a lexical rule for the word. 
3. The system applies tile lexical rule to all 
examples of the selected word with same 
context and also saves the lexical rule in the 
rule base. 
4. P, epeat tile steps 2 and 3 until all 
examples of the candidate words can be 
tagged by the acquired lexical rules. 
3.3. Automatic Ta,,, in,,  
In the automatic ta,,~dn-oo ~ phase, words are 
disambiguated by using the lexical rules and a 
stochastic tagger. To armotate a word in a raw 
corpus, the rule-based tagger first searches the 
lexical rule base to find a lexical rule that can be 
nlatched with tile given context. If a matching 
rnle is found, the system assigns the result of the 
rule to the word. According to the corresponding 
rule, a proper tag is assigned to a word. With tile 
lexical rules~ a very precise tag can be assigned 
to a word. However, because the lexical rules do 
not  resolve all the ambiguity of the whole corpus, 
we must make use of a stochastic tagger. We 
employ an HMM--based POS tagger for this 
purpose(Kim,1998). The stochastic tagger 
assigns the proper tags to the ambiguous words 
afier the rule application. 
Alter disambiguating the raw corpus using 
the lexical rules and the atttomatic tagger, we 
arrive at the frilly disambiguated result. But the 
word tagged by the stochastic tagger may have a 
chance to be mis-tagged. Therefore, the 
post-processing for error correction is required 
for the words tagged by the stochastic tagger. 
3.4. Error Correction 
The human expert carries out the error 
correction task for the words tagged by a 
stochastic tagger. This error correction also 
requires tile repeatecl human labor as in the 
manual tagging. We employ the similar way of 
the rule acquisition to reduce the human labor 
needed for manual error cmTection. The results 
of the automatic tagger are marked to be 
distinguished from tile results of the rule-based 
tagger. The human expert checks the marked 
words only. If an error is found, the ht/man 
expert assigns a correct tag to the word. When 
tile expert corrects the erroneous word, tile 
system automatically generates a lexicat rule and 
stores it in tile rnle base. File newly acquired 
rule is autoinatically applied to the rest of tile 
corpus. Thus, the expert does not need to correct 
the repeated errors. 
1098 
B ........ 
A . . . J  
:~ ,?  "~; ~ ~'J ~'Y,I .:'l~ll,k! G'~(~ ~:)':'fl,q ! ! !~l))L" , l ' ) l  ,q';'.%ll.q !ll~ ~. "?1 )~:d~ 
:':} 'k L '  ~i~ tl ? r31 ,31 ?2 :~ '2'.' :~ ,:,i\[ .~ YZ. "?! :'J q l :'112j X,"~ ?t ) I -@ ~! ? I 
".'.20 t~'~tJ 2: .c Ul I '3t3b!:! I ~ :~ '~ (IM{3tl *,1 N ~ :31 ,'q ~?i ::i ; '  ?,,3 ~ :~ ~J g~ "JH G 
r.NwO}.lx* I '?v?et~a5 : !~31 W~'gf f l l  Y '~a!  t l t~0 l  adlTll ' , lLr'9~. 
r ,~ wU,t.l:<t E : '?t:S eft ~1:" 
E(';'.hTF ~ I '~ t 
,iH,'.t CH.q',~'~ Ncrs icTc~ 
, : ' I~L . IOF ' , I~qEA ~x~.'qlOL }~M~?-"-- '?I I~ "~. 
=i~I  'q 
:" Gt~i!~} 5"3~d~/t,\]HP*~:}IJ'2 
J, kt21 ~t X}el/tlt'l!3 ? N,,'JF O 
:" h!T;UIHOII !,~T?~I'I/f'g'dG.OII/JC 
:. *.;9 E ,~;.,V~.,*?.L.'EP* /EF -  GF  
~.~n ~ 7;/I, iN p .  ~,,d X 
:, ~la~ At ~I"~INhII\]-.Lt/JFB 
> @~(~,~ .. @~iNNP- ( /SS ,~ ' I /SH.  
g~ ~ 11 t,'(I ~J L~ .~,!/N N P ? Ilt Xl ~I/ JK G 
> ZI~01 M XI~/NN,-~*OtlM/Ji:B 
?H ;~?I  01 ~/NN,5 .  ~I,/.JK6 
G' Xtl ~ 7? .~tl / r,~ N 6 ? ~ ,/o K o " - . _ "  
> ~'\[?8101 gd,l> ~t' *~alDN G-0~/J  
> ~.~.  $t lVV*92/EP-~niEF - zSP 
i'~ 92XI g, ol  S'!gMtaNG.XI%VNN,3.?I/. 
> ~Xl2 J~ ~XI? J /NN? '~ I JF .O  
~ )I a~ IJ}:?_k ~ ) I /NNG -8 }/XSV-OHjLnt 
; ~.E}. 8t/VV*?)\[EP.E~/EF*/SF 
\] ~,J '_'6&}; .3} .~4j~q <?Jr 't ~r~? j  ~.II?41~.L ~> k12I > 3~t?ItG'?3 )ldlt ~"/q'41'lG.?l ', KG 
. . . . . . . . .  21 x oH,~t'~011:,11 ~,,ki~_~ ~HYj9 : ; ' .3o  x ~,-a4 ~,i1,,'i~bJ 3* /SP  ? 
. . . .  2\[_ '. f~ l _  ___ -~-~A Xll, ) lge.  131Ct1~21 2N.~gJ ~,{~0"IIM)~- . . . . . . . . . . . . .  I . . . . . . . . . .  ;111~ J 
::,~2!;,7~,-~ -~. ,~, ,~.~.  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ ,~_  . . . . . .  
Figure 2. Bu i ld ing  Annotated  Corpus Usiug KCAT 
4. Application to Build Large Corpora 
Based on the proposed method~ we have 
imrdemented, a corpus--annotating tool for 
Koreart which is named as KCAT(Korean 
Corpus Annotating 'Fool). The process of 
building large corpora with KCAT is as lbllows: 
1. The lexical roles in the rule base are 
applied to a raw corpu::,. If the rule base i!; 
empty, nothing will be done. 
2. The sy,~;tem akes a candidate li';t. 
3. Ilunmn expert produces the lexical 1.ules 
for the words in the candidate list. 
4. The .~;ystem tags the corpus by using the 
lexical rHles and a stochastic t,l~,~.c~. 
5. Hunmn manually con?cots errors caused by 
the stochastic tagger, and lexical rules for 
those errors are also stored in the 
role--base. 
6. For other corpus, repeat the steps 1 
through 5. 
Figure 2 shows a screenshot f KCAT. In this 
figure, "A' window represents the list of raw 
corpus arm a "B' window contains the contcnt of 
the selected raw corpus in the window A. The 
tagging result is displayed in the window 'C'. 
Words beginning with ">' are tagged by a 
stocha,,;tic la-<,e, and the other words are ta~Eed 
by lexical rules. 
We can -et the more lexical rules as the 
ta,,,,itw process is prom-esscd. Therefore, we can 
expect that the aecunu-y and the reduction rate 
C 
of human htbor are increased a~ long as the 
tagging process is corltilmed. 
5. Experimental Results 
In order to estimate tim experimental results 
of our system, we collected the highly 
ambiguous words and frequently occurring 
words in our test corpus with 50,004 words. 
\]able I shows reductions in human intervention 
required to armotate the raw coums when we use 
lexical rules lbr the highly ambiguous words and 
the frequently occurring words respectively. The 
second colurnn shows that we examined the 
4,081 OCCLirrences of 2,088 words with tag 
choices above 7 and produced 4,081 lexical 
rules covering 4,832 occurrences of the corpl_lS. 
In this case, the reduction rate of human 
intervention is 1.5%. ~ The third column shows 
that we exalnined thc 6,845 occurrences of 511 
words with ficqucncy above 10 and produced 
6,845 lexical rules covering 15,4 l 8 occurrences 
of the corpus. In tiffs case, the reduction rate of 
human intervention is 17%. 2 
The last row in the table shows how 
intbrnmtive the rules are. We measured it by the 
inq-~iovement rate of stochastic tagging ;_!.l'l.el- the 
rules arc applied. From these experimental 
result.~;, wc can judge that rule-acquisition from 
flcquelatly occurring words is preferable. 
i (4,~., _4,(),v; l ) / 50,004 
~. ( 15,41 x-6,g~b ) / 50,004 
1099 
Table 1. Reduction in human Intervention 
I Type of word 
lbr rule 
acquisition 
Number of 
words 
Ambiguous 
words (_>7) 
Frequently 
occurring 
words (_>10) 
4832(9.6?/,,) 15418(30%) 
Number of 408 l 6845 
lexical rules 
Decrement of 1.5% 17% 
h u lll a 11 
intervention 
hnprovement 1.6% 3.7% 
of tagging 
accttracy (94.1-92.5%) (95.2-92.5%) 
Table 2 shows the results of our experiments on 
tile applicability of lexical rules. We measure it 
by the improyement rate of stochastic tagging 
alter the rules acquired from other corpus are 
applied. 
The third row shows that we annotate a training 
corpus with 10,032 words and produce 631 
lexieal rules, which can be applied to another 
test corpus to reduce tile number of the 
stochastic ta-,,in,, errors frorn 697 to 623. 3 
The ~brth and fifth row show that as the number 
of lexical rules is increased, the number of the 
errors of the tagger is decreased on the test 
corpus. 
These experilnental results demonstrate tile 
promise of gradual decrement of human 
intervention and improvement of tagging 
accuracy in annotating corpora. 
Table 2. Applicability of Lexical Rules 
Size of tile The nunaber The number of 
corpus of lexical stochastic 
roles errors 
0 0 697 
10,032 631 62.3 
20,047 136l 565 
_~( ,049 2091 538 
6. Conclusion 
The main goal of our work is to dcvelop an 
efficiclat tool which supports to build a very 
3 Our test corpus includes 10,015 words 
accurately and consistently POS annotated 
corpus with nlinilnal hunmn labor. To achieve 
the goal, we have proposed a POS ta,,-in- tool 
named KCAT which can use human linguistic 
knowledge as a lexical rule form. Once a lexical 
role is acquired, the hutnan expert doesn't need 
to spend titne in tagging the same word in the 
same context. By using the lexical roles, we 
could have very accurate and consistent results 
as well its reducing the amount of the hurnan 
labor. 
It is obvious that the more lexical roles the 
tool acquires the higher accuracy and 
consistency it achieves. But it still requires a lot 
of human labor and cost to acquire many lexical 
rules. And, as the number of the lexical rules is 
increased, the speed of rule application is 
decreased. To overcome the barriers, we try to 
find a way of rule generalization and a more 
efficient way of rule encoding scheme like the 
finite-state atttomata(Roche, 1995). 
Furthermore, we will use the distance of the 
best and second tag's probabilities to classify 
reliable automatic tagging result and unreliable 
ta,,,,in,, result(Brants, 1999). 
Refere\[Ices 
Brants~ T. Skut, W. and Uszkoreit, H. (1999) 
A),ntac'tic /hmotatio/1 of  a German N~.-*lri'Spal)e\]" 
Coums. In "Jourrlees ATALA", pp.69?76. 
Kim, J. D. Lira, H. S. and Rim, H. C. (1998) 
Morl)henle-Unit POS Tagging Mode/ 
Considering Eojeol-Spacing. In "Proc. of the 
10th ttangul and Korean Information 
Processing Conference", pp.3-8. 
Lee, J. K. (1996) Eojeol-tmit rule Based POS 
tag~in?~ with minimal human intervention. M. 
S dissertation, Dept. of Computer Science and 
Engineering, Korea Univ. 
Lira, H. S. Kim, J. D. and Rim, H. C. (1996) .4 
Korean 1)'an.@)rmation-I~axed POS Tagger 
with Lexical h!fi)rmation ojmi.vtag,4ed Eojeo\[. 
In "Proc. of the 2nd Korea-China Joint 
Symposium on Oriental Language 
Computing", pp. 119-124. 
Roche, E. and Schabes, Y. (1995)Determini.s'tic 
Part-o.f-St)eect~ Taggi/Ig with Fi//te-State 
7?aHsduc'er. Computational Linguistics, 21/2, 
pp. 227-253. 
1100 
Introduction to the Bio-Entity Recognition Task at JNLPBA
Jin-Dong KIM, Tomoko OHTA, Yoshimasa TSURUOKA, Yuka TATEISI
CREST, Japan Science and Technology Agency, and
Department of Computer Science, University of Tokyo,
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan?
Nigel COLLIER
National Institute of Informatics,
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan?
Abstract
We describe here the JNLPBA shared task of
bio-entity recognition using an extended version
of the GENIA version 3 named entity corpus of
MEDLINE abstracts. We provide background
information on the task and present a general
discussion of the approaches taken by partici-
pating systems.
1 Introduction
Bio-entity recognition aims to identify and clas-
sify technical terms in the domain of molecu-
lar biology that correspond to instances of con-
cepts that are of interest to biologists. Exam-
ples of such entities include the names of pro-
teins, genes and their locations of activity such
as cells or organism names as shown in Figure 1.
Entity recognition is a core component tech-
nology in several higher level information access
tasks such as information extraction (template
filling), summarization and question answering.
These tasks aim to help users find structure in
unstructured text data and aid in finding rele-
vant factual information. This is becoming in-
creasingly important with the massive increase
in reported results due to high throughput ex-
perimental methods.
Bio-entity recognition by computers remains
a significantly challenging task. Despite good
progress in newswire entity recognition (e.g.
(MUC, 1995; Tjong Kim Sang and De Meul-
der, 2003)) that has led to ?near human? levels
of performance, measured in the high 90s for F-
score (van Rijsbergen, 1979), similar methods
have not performed so well in the bio-domain
leaving an accuracy gap of some 30 points of F-
score. Challenges occur for example due to am-
biguity in the left boundary of entities caused
by descriptive naming, shortened forms due to
abbreviation and aliasing, the difficulty of creat-
? {jdkim,yucca,okap,tsuruoka}@is.s.u-tokyo.ac.jp
? collier@nii.ac.jp
We have shown that <cons
sem=?G#protein?>interleukin-1</cons>
(<cons sem=?G#protein?>IL-1</cons>)
and <cons sem=?G#protein?>IL-2</cons>
control <cons sem=?G#DNA?>IL-2 receptor
alpha (IL-2R alpha) gene</cons> transcription
in <cons sem=?G#cell line?>CD4-CD8-
murine T lymphocyte precursors</cons>.
Figure 1: Example MEDLINE sentence marked
up in XML for molecular biology named-
entities.
ing consistently annotated human training data
with a large number of classes, etc. In or-
der to make progress it is becoming clear that
several points need to be considered: (1) ex-
tension of feature sets beyond the lexical level
(part of speech, orthography etc.) and use of
higher-levels of linguistic knowledge such as de-
pendency relations, (2) potential for re-use of
external domain knowledge resources such as
gazetteers and ontologies, (3) improved quality
control methods for building annotation collec-
tions, (4) fine grained error analysis beyond the
F-score statistics.
The JNLPBA shared task 1 is an open chal-
lenge task and as such we allowed participants
to use whatever methodology and knowledge
sources they liked in the bio-entity task. The
systems were evaluated on a common bench-
mark data set using a common evaluation
method. Although it is not directly possible
to compare systems due to the diversity of re-
sources used the F-score results provide an ap-
proximate indication of how useful each method
is.
2 Data
The training data used in the task came from
the GENIA version 3.02 corpus (Kim et al,
1http://research.nii.ac.jp/
?collier/workshops/JNLPBA04st.htm
70
2003). This was formed from a controlled search
on MEDLINE using the MeSH terms ?human?,
?blood cells? and ?transcription factors?. From
this search 2,000 abstracts were selected and
hand annotated according to a small taxon-
omy of 48 classes based on a chemical classi-
fication. Among the classes, 36 terminal classes
were used to annotate the GENIA corpus.
The GENIA corpus is important for two ma-
jor reasons: the first is that it provides the
largest single source of annotated training data
for the NE task in molecular biology and the
second is in the breadth of classification. Al-
though 36 classes is a fraction of the classes con-
tained in major taxonomies it is still the largest
class set that has been attempted so far for the
NE task. In this respect it is an important test
of the limits of human and machine annotation
capability. For the shared task we decided how-
ever to simplify the 36 classes and used only the
classes protein, DNA, RNA, cell line and cell
type. The first three incorporate several sub-
classes from the original taxonomy while the
last two are interesting in order to make the
task realistic for post-processing by a potential
template filling application.
For testing purposes we used a newly anno-
tated collection of MEDLINE abstracts from
the GENIA project. 404 abstracts were used
that were annotated for the same classes of en-
tities: Half of them were from the same domain
as the training data and the other half of them
were from the super-domain of ?blood cells? and
?transcription factors?. Our hope was that this
should provide an important test of generaliz-
ability in the methods used.
3 Evaluation
The 2,000 abstracts of the GENIA corpus ver-
sion 3.02 which had already been made publicly
available were formatted for IOB2 notation and
made available as training materials. For test-
ing, additional 404 abstracts were randomly se-
lected from an unpublished set of the GENIA
corpus and the annotations were re-checked by a
biologist. The training set consists of abstracts
retrieved from the MEDLINE database with
MeSH terms ?human?, ?blood cells? and ?tran-
scription factors?, and their publication year
ranges over 1990?1999. Most parts of the test
set include abstracts retrieved with the same
set of MeSH terms, and their publication year
ranges over 1978?2001. To see the effect of pub-
lication year, the test set was roughly divided
into four subsets: 1978-1989 set (which rep-
resents an old age from the viewpoint of the
models that will be trained using the training
set), 1990-1999 set (which represents the same
age as the training set), 2000-2001 set (which
represents a new age compared to the training
set) and S/1998-2001 set (which represents
roughly a new age in a super domain). The last
subset represents a super domain and the ab-
stracts was retrieved with MeSH terms, ?blood
cells? and ?transcription factors? (without ?hu-
man?)2. Table 1 illustrates the size of the data
sets
Table 2 shows the number of entities anno-
tated in each data set3. As seen in the ta-
ble, the annotation density of proteins increases
over the ages significantly, whereas the anno-
tation density of DNAs and RNAs increases in
the 1990-1999 set and slightly decreases in the
2000-2001 set. This tendency roughly corre-
sponds to the expansion in the subject area as a
whole that can be estimated from statistics on
the MeSH terms introduced in each age shown
in Table 3. This observation suggests that the
density of mention of a class of entities in aca-
demic papers is affected by the amount of inter-
est the entity receives in each age.
Figure 2 shows the ratio of annotated struc-
tures in each set. In accordance with our expec-
tation, the 1990-1999 set has the most simi-
lar annotation trait with the training set. The
2000-2001 set is also similar to the training
set, but the 1978-1989 set had quite a differ-
ent distribution of entity classes. The variation
of domain does not seem to make any signif-
icant difference to the distribution of entities
mentioned. One reason may be the large frac-
tion of abstracts from the same domain in the
super domain set. In fact, among 206 abstracts
in the super domain set, 140 abstracts (69%)
are also from the same domain. It also corre-
sponds to the fraction in the whole MEDLINE
database: among 9,362 abstracts that can be
retrieved with MeSH terms, ?blood cells? and
?transcription factors?, 6,297 abstracts (67%)
can also be retrieved with MeSH terms ?human?,
?blood cells? and ?transcription factors?.
To simplify the annotation task to a simple
linear sequential analysis problem, embedded
structures have been removed leaving only the
2The S/1998-2001 set includes the whole 2000-
2001 set.
3The figures in the parenthesis are the average num-
ber of entities per an abstract in each set.
71
# abs # sentences # words
Training Set 2,000 20,546 (10.27/abs) 472,006 (236.00/abs) (22.97/sen)
Test Set 404 4,260 (10.54/abs) 96,780 (239.55/abs) (22.72/sen)
1978-1989 104 991 ( 9.53/abs) 22,320 (214.62/abs) (22.52/sen)
1990-1999 106 1,115 (10.52/abs) 25,080 (236.60/abs) (22.49/sen)
2000-2001 130 1,452 (11.17/abs) 33,380 (256.77/abs) (22.99/sen)
S/1998-2001 206 2,270 (11.02/abs) 51,957 (252.22/abs) (22.89/sen)
Table 1: Basic statistics for the data sets
protein DNA RNA cell type cell line ALL
Training Set 30,269 (15.1) 9,533 (4.8) 951 (0.5) 6,718 (3.4) 3,830 (1.9) 51,301 (25.7)
Test Set 5,067 (12.5) 1,056 (2.6) 118 (0.3) 1,921 (4.8) 500 (1.2) 8,662 (21.4)
1978-1989 609 ( 5.9) 112 (1.1) 1 (0.0) 392 (3.8) 176 (1.7) 1,290 (12.4)
1990-1999 1,420 (13.4) 385 (3.6) 49 (0.5) 459 (4.3) 168 (1.6) 2,481 (23.4)
2000-2001 2,180 (16.8) 411 (3.2) 52 (0.4) 714 (5.5) 144 (1.1) 3,501 (26.9)
S/1998-2001 3,186 (15.5) 588 (2.9) 70 (0.3) 1,138 (5.5) 170 (0.8) 5,152 (25.0)
Table 2: Absolute (and relative) frequencies for NEs in each data set. Figures for the test set are
broken down according to the age of the data.
Figure 2: Ratio of annotated NEs
          
 	 
         
       
         
        
    
         
    Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 19?27,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Event Extraction for Post-Translational Modifications
Tomoko Ohta? Sampo Pyysalo? Makoto Miwa? Jin-Dong Kim? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{okap,smp,mmiwa,jdkim,tsujii}@is.s.u-tokyo.ac.jp
Abstract
We consider the task of automatically
extracting post-translational modification
events from biomedical scientific publica-
tions. Building on the success of event
extraction for phosphorylation events in
the BioNLP?09 shared task, we extend the
event annotation approach to four major
new post-transitional modification event
types. We present a new targeted corpus of
157 PubMed abstracts annotated for over
1000 proteins and 400 post-translational
modification events identifying the modi-
fied proteins and sites. Experiments with
a state-of-the-art event extraction system
show that the events can be extracted with
52% precision and 36% recall (42% F-
score), suggesting remaining challenges
in the extraction of the events. The an-
notated corpus is freely available in the
BioNLP?09 shared task format at the GE-
NIA project homepage.1
1 Introduction
Post-translational-modifications (PTM), amino
acid modifications of proteins after translation, are
one of the posterior processes of protein biosyn-
thesis for many proteins, and they are critical
for determining protein function such as its ac-
tivity state, localization, turnover and interac-
tions with other biomolecules (Mann and Jensen,
2003). Since PTM alter the properties of a pro-
tein by attaching one or more biochemical func-
tional groups to amino acids, understanding of
the mechanism and effects of PTM are a major
goal in the recent molecular biology, biomedicine
and pharmacology fields. In particular, epige-
netic (?outside conventional genetics?) regulation
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA
of gene expression has a crucial role in these fields
and PTM-like modifications of biomolecules are a
burning issue. For instance, tissue specific or con-
text dependent expression of many proteins is now
known to be controlled by specific PTM of his-
tone proteins, such as Methylation and Acetylation
(Jaenisch and Bird, 2003). This Methylation and
Acetylation of specific amino acid residues in his-
tone proteins are strongly implicated in unwinding
the nucleosomes and exposing genes to transcrip-
tion, replication and DNA repairing machinery.
The recent BioNLP?09 Shared Task on Event
Extraction (Kim et al, 2009a) (below, BioNLP
shared task) represented the first community-wide
step toward the extraction of fine-grained event
representations of information from biomolecular
domain publications (Ananiadou et al, 2010). The
nine event types targeted in the task included one
PTM type, Phosphorylation, whose extraction in-
volved identifying the modified protein and, when
stated, the specific phosphorylated site. The re-
sults of the shared task showed this PTM event to
be single most reliably extracted event type in the
data, with the best-performing system for the event
type achieving 91% precision and 76% recall
(83% F-score) in the extraction of phosphorylation
events (Buyko et al, 2009). The results suggest
both that the event representation is well applica-
ble to PTM and that current extraction methods are
capable of reliable PTM extraction. Most of the
proposed state-of-the-art methods for event extrac-
tion are further largely machine-learning based.
This suggest that the coverage of many existing
methods could be straightforwardly extended to
new event types and domains by extending the
scope of available PTM annotations and retrain-
ing the methods on newly annotated data. In this
study, we take such an annotation-based approach
to extend the extraction capabilities of state of the
art event extraction methods for PTM.
19
Term Count
Phosphorylation 172875 50.90%
Methylation 49780 14.66%
Glycosylation 36407 10.72%
Hydroxylation 20141 5.93%
Acetylation 18726 5.51%
Esterification 7836 2.31%
Ubiquitination 6747 1.99%
ADP-ribosylation 5259 1.55%
Biotinylation 4369 1.29%
Sulfation 3722 1.10%
. . .
TOTAL 339646 100%
Table 1: PTM mentions in PubMed. The number
of citations returned by the PubMed search engine
for each PTM term shown together with the frac-
tion of the total returned for all searches. Searches
were performed with the terms as shown, allow-
ing MeSH term expansion and other optimizations
provided by the Entrez search.
2 Corpus Annotation
We next discuss the selection of the annotated
PTM types and source texts and present the rep-
resentation and criteria used in annotation.
2.1 Event Types
A central challenge in the automatic extraction
of PTMs following the relatively data-intensive
BioNLP shared task model is the sheer number
of different modifications: the number of known
PTM types is as high as 300 and constantly grow-
ing (Witze et al, 2007). Clearly, the creation of
a manually annotated resource with even mod-
est coverage of statements of each of the types
would be a formidable undertaking. We next
present an analysis of PTM statement occurrences
in PubMed as the first step toward resolving this
challenge.
We estimated the frequency of mentions of
prominent PTM types by combining MeSH
ontology2 PTM terms with terms occurring
in the post-translational protein
modification branch of the Gene Ontology
(The Gene Ontology Consortium, 2000). After
removing variants (e.g. polyamination for amina-
tion or dephosphorylation for phosphorylation)
and two cases judged likely to occur frequently
2http://www.nlm.nih.gov/mesh/meshhome.
html
in non-PTM contexts (hydration and oxidation),
we searched PubMed for the remaining 31 PTM
types. The results for the most frequent types
are shown in Table 1. We find a power-law
- like distribution with phosphorylation alone
accounting for over 50% of the total, and the top
6 types together for over 90%. By contrast, the
bottom ten types together represent less than a
percent of total occurrences.
This result implies that fair coverage of individ-
ual PTM event mentions can be achieved without
considering even dozens of different PTM event
types, let alne hundreds. Thus, as a step toward
extending the coverage of event extraction systems
for PTM, we chose to focus limited resources on
annotating a small selection of types so that a num-
ber of annotations sufficient for supervised learn-
ing and stable evaluation can be provided. To
maximize the utility of the created annotation, the
types were selected based on their frequency of oc-
currence.
2.2 Text Selection
Biomedical domain corpora are frequently anno-
tated from selections of texts chosen as a sample
of publications in a particular subdomain of inter-
est. While several areas in present-day molecu-
lar biology are likely to provide ample source data
for PTM statements, a sample of articles from any
subdomain is unlikely to provide a well-balanced
distribution of event types: for example, the most
frequent PTM event type annotated in the GENIA
event corpus occurs more than 10 times as often
as the second most frequent (Kim et al, 2008).
Further, avoiding explicit subdomain restrictions
is not alone sufficient to assure a balanced distri-
bution of event types: in the BioInfer corpus, for
which sentences were selected on the basis of their
containing mentions of protein pairs known to in-
teract, the most frequent PTM type is again anno-
tated nearly four times as often as the second most
frequent (Pyysalo et al, 2007).
To focus annotation efforts on texts relevant to
PTM and to guarantee that the annotation results
in relatively balanced numbers of PTM events of
each targeted type, we decided to annotate a tar-
geted set of source texts instead of a random sam-
ple of texts for a particular subdomain. This type
of targeted annotation involves a risk of introduc-
ing bias: a badly performed selection could pro-
duce a corpus that is not representative of the
20
PTM type AB FT
Acetylation 103 128
Glycosylation 226 336
Methylation 72 69
Phosphorylation 186 76
Hydroxylation 71 133
Table 2: Number of abstracts (AB) and full-text ar-
ticles (FT) tagged in PIR as containing PTM state-
ments.
statements expressing PTMs in text and thus poor
material for either meaningful evaluation or for
training methods with good generalization perfor-
mance.3 To avoid such bias, we decided to base
our selection of the source texts on an indepen-
dently annotated PTM resource with biological (as
opposed to textual) criteria for inclusion. Owing
in part to the recent interest in PTMs, there are
currently a wealth of resources providing different
levels of annotation for PTMs.
Here, we have chosen to base initial annotation
on corpora provided by the Protein Information
Resource4 (PIR) (Wu et al, 2003). These corpora
contain annotation for spans with evidence for five
different PTM types (Table 2), corresponding to
the five PTMs found above to occur in PubMed
with the highest frequency. A key feature setting
this resource apart from others we are aware of is
that it provides text-bound annotations identifying
the statement by which a PTM record was made in
the context of the full publication abstracts. While
this annotation is less specific and detailed than
the full BioNLP shared task markup, it could both
serve as an initial seed for annotation and assure
that the annotation agrees with relevant database
curation criteria. The PIR corpora have also been
applied in previous PTM extraction studies (e.g.
(Hu et al, 2005; Narayanaswamy et al, 2005)).
We judged that the annotated Phosphorylation
events in the BioNLP shared task data provide
sufficient coverage for the extraction of this PTM
type, and chose to focus on producing annota-
tion for the four other PTM types in the PIR data.
As the high extraction performance for phospho-
rylation events in the BioNLP shared task was
3One could easily gather PTM-rich texts by performing
protein name tagging and searching for known patterns such
as ?[PROTEIN] methylates [PROTEIN]?, but a corpus cre-
ated in this way would not necessarily provide significant
novelty over the original search patterns.
4http://pir.georgetown.edu
Protein Site PTM Count
collagen lysine Hydroxylate 44
myelin arginine Methylate 17
M protein N-terminal Glycosylate 2
EF-Tu lysine Methylate 1
Actobindin NH2 terminus Acetylate 0
Table 3: Example queried triples and match counts
from Medie.
achieved with annotated training data containing
215 PTM events, in view of the available resources
we set as an initial goal the annotation of 100
events of each of the four PTM types. To assure
that the annotated resource can be made publicly
available, we chose to use only the part of the PIR
annotations that identified sections of PubMed ab-
stracts, excluding full-text references and non-
PubMed abstracts. Together with the elimination
of duplicates and entries judged to fall outside of
the event annotation criteria (see Section 2.4), this
reduced the number of source texts below our tar-
get, necessitating a further selection strategy.
For further annotation, we aimed to select ab-
stracts that contain specific PTM statements iden-
tifying both the name of a modified protein and the
modified site. As for the initial selection, we fur-
ther wished to avoid limiting the search by search-
ing for any specific PTM expressions. To imple-
ment this selection, we used the Medie system5
(Ohta et al, 2006; Miyao et al, 2006) to search
PubMed for sentences where a specific protein and
a known modified site were found together in a
sentence occurring in an abstract annotated with a
specific MeSH term. The (protein name, modified
site, MeSH term) triples were extracted from PIR
records, substituting the appropriate MeSH term
for each PTM type. Some examples with the num-
ber of matching documents are shown in Table 3.
As most queries returned either no documents or a
small number of hits, we gave priority to responses
to queries that returned a small number of docu-
ments to avoid biasing the corpus toward proteins
whose modifications are frequently discussed.
We note that while the PIR annotations typically
identified focused text spans considerably shorter
than a single sentence and sentence-level search
was used in the Medie-based search to increase the
likelihood of identifying relevant statements, after
selection all annotation was performed to full ab-
stracts.
5http://www-tsujii.is.s.u-tokyo.ac.jp/
medie/
21
Event type Count
Protein modification 38
Phosphorylation 546
Dephosphorylation 28
Acetylation 7
Deacetylation 1
Ubiquitination 6
Deubiquitination 0
Table 4: GENIA PTM-related event types and
number of events in the GENIA event corpus.
Type names are simplified: the full form of e.g.
the Phosphorylation type in the GENIA event on-
tology is Protein amino acid phosphorylation.
Event type Arguments Count
Protein modification Theme 31
Phosphorylation Theme 261
Phosphorylation Theme, Site 230
Phosphorylation Site 20
Phosphorylation Theme, Cause 14
Dephosphorylation Theme 16
Table 5: GENIA PTM-related event arguments.
Only argument combinations appearing more than
10 times in the corpus shown.
2.3 Representation
The employed event representation can capture
the association of varying numbers of participants
in different roles. To apply an event extraction
approach to PTM, we must first define the tar-
geted representation, specifying the event types,
the mandatory and optional arguments, and the ar-
gument types ? the roles that the participants play
in the events. In the following, we discuss alterna-
tives and present the representation applied in this
work.
The GENIA Event ontology, applied in the
annotation of the GENIA Event corpus (Kim
et al, 2008) that served as the basis of the
BioNLP shared task data, defines a general Pro-
tein modification event type and six more specific
modification subtypes, shown in Table 4. While
the existing Acetylation type could thus be applied
together with the generic Protein modification
type to capture all the annotated PTMs, we be-
lieve that identification of the specific PTM type
is not only important to users of extracted PTM
events but also a relatively modest additional bur-
den for automatic extraction, owing to the unam-
biguous nature of typical expressions used to state
Figure 1: Alternative representations for PTM
statements including a catalyst in GENIA Event
corpus. PTM events can be annotated with a di-
rect Cause argument (top, PMID 9374467) or us-
ing an additional Regulation event (middle, PMID
10074432). The latter annotation can be applied
also in cases where there is no expression directly
?triggering? the secondary event (bottom, PMID
7613138).
PTMs in text. We thus chose to introduce three
additional specific modification types, Glycosyla-
tion, Hydroxylation and Methylation for use in the
annotation.
The GENIA Event corpus annotation allows
PTM events to take Theme, Site and Cause argu-
ments specifying the event participants, where the
Theme identifies the entity undergoing the mod-
ification, Site the specific region being modified,
and Cause an entity or event leading to the modi-
fication. Table 5 shows frequent argument combi-
nations appearing in the annotated data. We note
that while Theme is specified in the great majority
of events and Site in almost half, Cause is anno-
tated for less than 5% of the events. However, the
relative sparsity of Cause arguments in modifica-
tion events does not imply that e.g. catalysts of the
events are stated only very rarely, but instead re-
flects also the use of an alternative representation
for capturing such statements without a Cause ar-
gument for the PTM event. The GENIA event an-
notation specifies a Regulation event (with Posi-
tive regulation and Negative regulation subtypes),
used to annotate not only regulation in the biolog-
ical sense but also statements of general causality
between events: Regulation events are used gen-
erally to connect entities or events stated to other
events that they are stated to cause. Thus, PTM
22
events with a stated cause (e.g. a catalyst) can be
alternatively represented with a Cause argument
on the PTM event or using a separate Regulation
event (Figure 1). The interpretation of these event
structures is identical, and from an annotation per-
spective there are advantages to both. However,
for the purpose of automatic extraction it is impor-
tant to establish a consistent representation, and
thus only one should be used.
In this work, we follow the latter representation,
disallowing Cause arguments for annotated PTM
events and applying separate Regulation events
to capture e.g. catalyst associations. This choice
has the benefits of providing an uniform repre-
sentation for catalysis and inhibition (one involv-
ing a Positive regulation and the other a Nega-
tive regulation event), reducing the sparseness of
specific event structures in the data, and matching
the representation chosen in the BioNLP shared
task, thus maintaining compatibility with exist-
ing event extraction methods. Finally, we note
that while we initially expected that glycosylation
statements might frequently identify specific at-
tached side chains, necessitating the introduction
of an additional argument type to accurately cap-
ture all the stated information regarding Glycosy-
lation events, the data contained too few examples
for either training material or to justify the mod-
ification of the event model. We adopt the con-
straints applied in the BioNLP shared task regard-
ing the entity types allowed as specific arguments.
Thus, the representation we apply here annotated
PTM events with specific types, taking as Theme
argument a gene/gene product type entity and as
Site argument a physical (non-event) entity that
does not need to be assigned a specific type.
2.4 Annotation criteria
To create PTM annotation compatible with the
event extraction systems introduced for the
BioNLP shared task, we created annotation fol-
lowing the GENIA Event corpus annotation cri-
teria (Kim et al, 2008), as adapted for the shared
task. The criteria specify that annotation should be
applied to statements that involve the occurrence
of a change in the state of an entity ? even if stated
as having occurred in the past, or only hypotheti-
cally ? but not in cases merely discussing the state
or properties of entities, even if these can serve as
the basis for inference that a specific change has
occurred. We found that many of the spans an-
notated in PIR as evidence for PTM did not ful-
fill the criteria for event annotation. The most fre-
quent class consisted of cases where the only evi-
dence for a PTM was in the form of a sequence of
residues, for example
Characterization [. . . ] gave the follow-
ing sequence, Gly-Cys-Hyp-D-Trp-Glu-
Pro-Trp-Cys-NH2 where Hyp = 4-trans-
hydroxyproline. (PMID 8910408)
Here, the occurrence of hydroxyproline in the se-
quence implies that the protein has been hydrox-
ylated, but as the hydroxylation event is only im-
plied by the protein state, no event is annotated.
Candidates drawn from PIR but not fulfilling
the criteria were excluded from annotation. While
this implies that the general class of event extrac-
tion approaches considered here will not recover
all statements providing evidence of PTM to bi-
ologists (per the PIR criteria), several factors mit-
igate this limitation of their utility. First, while
PTMs implied by sequence only are relatively fre-
quent in PIR, its selection criteria give emphasis
to publications initially reporting the existence of a
PTM, and further publications discussing the PTM
are not expected to state it as sequence only. Thus,
it should be possible to extract the correspond-
ing PTMs from later sources. Similarly, one of
the promises of event extraction approaches is the
potential to extract associations of multiple enti-
ties and extract causal chains connecting events
with others (e.g. E catalyzes the hydroxylation of
P, leading to . . . ), and the data indicates that the
sequence-only statements typically provide little
information on the biological context of the modi-
fication beyond identifying the entity and site. As
such non-contextual PTM information is already
available in multiple databases, this class of state-
ments may not be of primary interest for event ex-
traction.
2.5 Annotation results
The new PTM annotation covers 157 PubMed
abstracts. Following the model of the BioNLP
shared task, all mentions of specific gene or gene
product names in the abstracts were annotated, ap-
plying the annotation criteria of (Ohta et al, 2009).
This new named entity annotation covers 1031
gene/gene product mentions, thus averaging more
than six mentions per annotated abstract. In to-
tal, 422 events of which 405 are of the novel PTM
23
Event type Count
Glycosylation 122
Hydroxylation 103
Methylation 90
Acetylation 90
Positive reg. 12
Phosphorylation 3
Protein modification 2
TOTAL 422
Table 6: Statistics of the introduced event annota-
tion.
Arguments Count
Theme, Site 363
Theme 36
Site 6
Table 7: Statistics for the arguments of the anno-
tated PTM events.
types were annotated, matching the initial annota-
tion target in number and giving a well-balanced
distribution of the specific PTM types (Table 6).
Reflecting the selection of the source texts, the
argument structures of the annotated PTM events
(Table 7) show a different distribution from those
annotated in the GENIA event corpus (Table 5):
whereas less than half of the GENIA event corpus
PTM events include a Site argument, almost 90%
of the PTM events in the new data include a Site.
PTM events identifying both the modified protein
and the specific modified site are expected to be
of more practical interest. However, we note that
the greater number of multi-argument events is ex-
pected to make the dataset more challenging as an
extraction target.
3 Evaluation
To estimate the capacity of the newly annotated
resource to support the extraction of the targeted
PTM events and the performance of current event
extraction methods at open-domain PTM extrac-
tion, we performed a set of experiments using an
event extraction method competitive with the state
of the art, as established in the BioNLP shared task
on event extraction (Kim et al, 2009a; Bjo?rne et
al., 2009).
3.1 Methods
We adopted the recently introduced event extrac-
tion system of Miwa et al (2010). The system
applies a pipeline architecture consisting of three
supervised classification-based modules: a trig-
ger detector, an event edge detector, and an event
detector. In evaluation on the BioNLP shared
task test data, the system extracted phosphory-
lation events at 75.7% precision and 85.2% re-
call (80.1% F-score) for Task 1, and 75.7% preci-
sion and 83.3% recall (79.3% F-score) for Task 2,
showing performance comparable to the best re-
sults reported in the literature for this event class
(Buyko et al, 2009). We assume three precondi-
tions for the PTM extraction: proteins are given,
all PTMs have Sites, and all arguments in a PTM
co-occur in sentence scope. The first of these is
per the BioNLP shared task setup, the second fixed
based the corpus statistics, and the third a property
intrinsic to the extraction method, which builds on
analysis of sentence structure.6 In the experiments
reported here, only the four novel PTM event types
with Sites in the corpus are regarded as a target for
the extraction.
The system extracted PTMs as follows: the
trigger detector detected the entities (triggers and
sites) of the PTMs, the event edge detector de-
tected the edges in the PTMs, and the event de-
tector detected the PTMs. The evaluation setting
was the same as the evaluation in (Miwa et al,
2010) except for the threshold. The thresholds in
the three modules were tuned with the develop-
ment data set.
Performance evaluation is performed using the
BioNLP shared task primary evaluation criteria,
termed the ?Approximate Span Matching? crite-
rion. This criterion relaxes the requirements of
strict matching in accepting extracted event trig-
gers and entities as correct if their span is inside
the region of the corresponding region in the gold
standard annotation.
3.2 Data Preparation
The corpus data was split into training and test sets
on the document level with a sampling strategy
that aimed to preserve a roughly 3:1 ratio of oc-
currences of each event type between training and
test data. The test data was held out during sys-
tem development and parameter selection and only
applied in a single final experiment. The event ex-
traction system was trained using the 112 abstracts
of the training set, further using 24 of the abstracts
6We note that in the BioNLP shared task data, all argu-
ments were contained within single sentences for 95% of
events.
24
Figure 2: Performance of PTM extraction on the
development data set.
Event type Prec Rec F
Acetylation 69.6% 36.7% 48.1%
Methylation 50.0% 34.2% 40.6%
Glycosylation 36.7% 42.5% 39.4%
Hydroxylation 57.1% 29.3% 38.7%
Overall 52.1% 35.7% 42.4%
Table 8: Event extraction results on the test set.
as a development test set.
3.3 Results
We first performed parameter selection, setting the
machine learning method parameter by estimating
performance on the development data set. Figure 2
shows the performance of PTM extraction on the
development data set with different values of pa-
rameter. The threshold value corresponding to the
best performance (0.3) was then applied for an ex-
periment on the held-out test set.
Performance on the test set was evaluated as
52% precision and 36% recall (42% F-score),
matching estimates on the development data. A
breakdown by event type (Table 8) shows that
Acetylation is most reliably extracted with extrac-
tion for the other three PTM types showing sim-
ilar F-scores despite some variance in the preci-
sion/recall balance. We note that while these re-
sults fall notably below the best result reported
for Phosphorylation events in the BioNLP shared
task, they are comparable to the best results re-
ported in the task for Regulation and Binding
events (Kim et al, 2009a), suggesting that the
dataset alows the extraction of the novel PTM
events with Theme and Site arguments at levels
comparable to multi-argument shared task events.
Figure 3: Learning curve of PTM extraction on the
development data set.
Further, a learning curve (Figure 3) plotted on
the development data suggests roughly linearly
increasing performance over most of the curve.
While the increase appears to be leveling off to
an extent when using all of the available data, the
learning curve indicates that performance can be
further improved by increasing the size of the an-
notated dataset.
4 Discussion
Post-translational modifications have been a fo-
cus of interest in the biomedical text mining com-
munity, and a number of resources and systems
targeting PTM have been proposed. The GE-
NIES and GeneWays systems (Friedman et al,
2001; Rzhetsky et al, 2004) targeted PTM events
such as phosphorylation and dephosphorylation
under the more general createbond and breakbond
types. Hu et al (2005) introduce the RLIMS-P
rule-based system for mining the substrates and
sites for phosphorylation, which is extended with
the capacity to extract intra-clausal statements by
Narayanaswamy et al (2005). Saric et al (2006)
present an extension of their rule-based STRING-
IE system for extracting regulatory networks to
capture phosphorylation and dephosphorylation
events. Lee et al (2008) present E3Miner, a tool
for automatically extracting information related to
ubiquitination, and Kim et al (2009b) present a
preliminary study adapting the E3Miner approach
to the mining of acetylation events.
It should be noted that while studies target-
ing single specific PTM types report better re-
sults than found in the initial evaluation presented
here (in many cases dramatically so), different
25
extraction targets and evaluation criteria compli-
cate direct comparison. Perhaps more importantly,
our aim here is to extend the capabilities of gen-
eral event extraction systems targeting multiple
types of structured events. Pursuing this broader
goal necessarily involves some compromise in the
ability to focus on the extraction of individual
event types, and it is expected that highly focused
systems will provide better performance than re-
trained general systems.
The approach to PTM extraction adopted here
relies extensively on the availability of annotated
resources, the creation of which requires consider-
able effort and expertise in understanding the tar-
get domain as well as the annotation methodology
and tools. The annotation created in this study,
performed largely on the basis of partial existing
annotations drawn from PIR data, involved an es-
timated three weeks of full-time effort from an ex-
perienced annotator. As experiments further in-
dicated that a larger corpus may be necessary for
reliable annotation, we can estimate that extending
the approach to sufficient coverage of each of hun-
dreds of PTM types without a partial initial anno-
tation would easily require several person-years of
annotation efforts. We thus see a clear need for the
development of unsupervised or semisupervised
methods for PTM extraction to extend the cover-
age of event extraction systems to the full scale of
different PTM types. Nevertheless, even if reliable
methods for PTM extraction that entirely avoid the
need for annotated training data become available,
a manually curated reference standard will still be
necessary for reliable estimation of their perfor-
mance. To efficiently support the development of
event extraction systems capable of capturing the
full variety of PTM events, it may be beneficial to
reverse the approach taken here: instead of anno-
tating hundreds of examples of a small number of
PTM types, annotate a small number of each of
hundreds of PTM types, thus providing both seed
data for semisupervised approaches as well as ref-
erence data for the evaluation of broad-coverage
PTM event extraction systems.
5 Conclusions and Future Work
We have presented an event extraction approach
to automatic PTM recognition, building on the
model introduced in the BioNLP shared task on
event extraction. By annotating a targeted cor-
pus for four prominent PTM types not considered
in the BioNLP shared task data, we have created
a resource that can be straightforwardly used to
extend the capability of event extraction systems
for PTM extraction. We estimated that while sys-
tems trained on the original shared task dataset
could not recognize more than 50% of PTM men-
tions due to their types, the introduced annotation
increases this theoretical upper bound to nearly
90%. An initial experiment on the newly intro-
duced dataset using a state-of-the-art method indi-
cated that straightforward adoption of the dataset
as training data to extend coverage of PTM events
without specific adaptations of the method is feasi-
ble, although the measured performance indicates
remaining challenges for reliable extraction. Fur-
ther, while the experiments were performed on a
dataset selected to avoid bias toward e.g. a partic-
ular subdomain or specific forms of event expres-
sions, it remains an open question how extraction
performance generalizes to biomedical literature
beyond the selected sample. As experiments in-
dicated clear remaining potential for the improve-
ment of extraction performance from more train-
ing data, the extension of the annotated dataset is
a natural direction for future work. We considered
also the possiblity of extending annotation to cover
small numbers of each of a large variety of PTM
types, which would place focus on the challenges
of event extraction with little or no training data
for specific event types.
The annotated corpus covering over 1000 gene
and gene product entities and over 400 events is
freely available in the widely adopted BioNLP
shared task format at the GENIA project home-
page.7
Acknowledgments
We would like to thank Goran Topic for automat-
ing Medie queries to identify target abstracts.
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan) and Japan-Slovenia Research Cooperative
Program (JSPS, Japan and MHEST, Slovenia).
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology. (to appear).
7http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA
26
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed
dependency graphs. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 19?27, Boulder, Colorado, June. Association
for Computational Linguistics.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. GE-
NIES: A natural-language processing system for the
extraction of molecular pathways from journal arti-
cles. Bioinformatics, 17(Suppl. 1):S74?S82.
Z. Z. Hu, M. Narayanaswamy, K. E. Ravikumar,
K. Vijay-Shanker, and C. H. Wu. 2005. Literature
mining and database annotation of protein phospho-
rylation using a rule-based system. Bioinformatics,
21(11):2759?2765.
Rudolf Jaenisch and Adrian Bird. 2003. Epigenetic
regulation of gene expression: how the genome in-
tegrates intrinsic and environmental signals. Nature
Genetics, 33:245?254.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009a. Overview
of bionlp?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Youngrae Kim, Hodong Lee, and Gwan-Su Yi. 2009b.
Literature mining for protein acetylation. In Pro-
ceedings of LBM?09.
Hodong Lee, Gwan-Su Yi, and Jong C. Park. 2008.
E3Miner: a text mining tool for ubiquitin-protein
ligases. Nucl. Acids Res., 36(suppl.2):W416?422.
Matthias Mann and Ole N. Jensen. 2003. Proteomic
analysis of post-translational modifications. Nature
Biotechnology, 21:255?261.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010. Event extraction with com-
plex event classification using rich features. Jour-
nal of Bioinformatics and Computational Biology
(JBCB), 8(1):131?146, February.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya, and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proceedings of
COLING-ACL 2006, pages 1017?1024.
M. Narayanaswamy, K. E. Ravikumar, and K. Vijay-
Shanker. 2005. Beyond the clause: extraction
of phosphorylation information from medline ab-
stracts. Bioinformatics, 21(suppl.1):i319?327.
Tomoko Ohta, Yusuke Miyao, Takashi Ninomiya,
Yoshimasa Tsuruoka, Akane Yakushiji, Katsuya
Masuda, Jumpei Takeuchi, Kazuhiro Yoshida, Ta-
dayoshi Hara, Jin-Dong Kim, Yuka Tateisi, and
Jun?ichi Tsujii. 2006. An Intelligent Search Engine
and GUI-based Efficient MEDLINE Search Tool
Based on Deep Syntactic Parsing. In Proceedings
of the COLING/ACL 2006 Interactive Presentation
Sessions, pages 17?20.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, and
Jun?ichi Tsujii. 2009. Incorporating GENETAG-
style annotation to GENIA corpus. In Proceedings
of Natural Language Processing in Biomedicine
(BioNLP) NAACL 2009 Workshop, pages 106?107,
Boulder, Colorado. Association for Computational
Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8(50).
Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,
Michael Krauthammer, Pauline Kra, Mitzi Mor-
ris, Hong Yu, Pablo Ariel Duboue?, Wubin Weng,
W. John Wilbur, Vasileios Hatzivassiloglou, and
Carol Friedman. 2004. GeneWays: A system for
extracting, analyzing, visualizing, and integrating
molecular pathway data. Journal of Biomedical In-
formatics, 37(1):43?53.
Jasmin Saric, Lars Juhl Jensen, Rossitza Ouzounova,
Isabel Rojas, and Peer Bork. 2006. Extraction
of regulatory gene/protein networks from Medline.
Bioinformatics, 22(6):645?650.
The Gene Ontology Consortium. 2000. Gene ontol-
ogy: tool for the unification of biology. Nature Ge-
netics, 25:25?29.
Eric S Witze, William M Old, Katheryn A Resing,
and Natalie G Ahn. 2007. Mapping protein post-
translational modifications with mass spectrometry.
Nature Methods, 4:798?806.
Cathy H. Wu, Lai-Su L. Yeh, Hongzhan Huang, Leslie
Arminski, Jorge Castro-Alvear, Yongxing Chen,
Zhangzhi Hu, Panagiotis Kourtesis, Robert S. Led-
ley, Baris E. Suzek, C.R. Vinayaka, Jian Zhang, and
Winona C. Barker. 2003. The Protein Information
Resource. Nucl. Acids Res., 31(1):345?347.
27
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 164?173,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Parsing Natural Language Queries for Life Science Knowledge
Tadayoshi Hara
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, JAPAN
harasan@nii.ac.jp
Yuka Tateisi
Faculty of Informatics, Kogakuin University
1-24-2 Nishi-shinjuku, Shinjuku-ku,
Tokyo 163-8677, JAPAN
yucca@cc.kogakuin.ac.jp
Jin-Dong Kim
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku,
Tokyo 113-0032, JAPAN
jdkim@dbcls.rois.ac.jp
Yusuke Miyao
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, JAPAN
yusuke@nii.ac.jp
Abstract
This paper presents our preliminary work on
adaptation of parsing technology toward natu-
ral language query processing for biomedical
domain. We built a small treebank of natu-
ral language queries, and tested a state-of-the-
art parser, the results of which revealed that
a parser trained on Wall-Street-Journal arti-
cles and Medline abstracts did not work well
on query sentences. We then experimented
an adaptive learning technique, to seek the
chance to improve the parsing performance on
query sentences. Despite the small scale of the
experiments, the results are encouraging, en-
lightening the direction for effective improve-
ment.
1 Introduction
Recent rapid progress of life science resulted in a
greatly increased amount of life science knowledge,
e.g. genomics, proteomics, pathology, therapeutics,
diagnostics, etc. The knowledge is however scat-
tered in pieces in diverse forms over a large number
of databases (DBs), e.g. PubMed, Drugs.com, Ther-
apy database, etc. As more and more knowledge is
discovered and accumulated in DBs, the need for
their integration is growing, and corresponding ef-
forts are emerging (BioMoby1, BioRDF2, etc.).
Meanwhile, the need for a query language with
high expressive power is also growing, to cope with
1http://www.biomoby.org/
2http://esw.w3.org/HCLSIG BioRDF Subgroup
the complexity of accumulated knowledge. For ex-
ample, SPARQL3 is becoming an important query
language, as RDF4 is recognized as a standard in-
teroperable encoding of information in databases.
SPARQL queries are however not easy for human
users to compose, due to its complex vocabulary,
syntax and semantics. We propose natural language
(NL) query as a potential solution to the problem.
Natural language, e.g. English, is the most straight-
forward language for human beings. Extra training
is not required for it, yet the expressive power is
very high. If NL queries can be automatically trans-
lated into SPARQL queries, human users can access
their desired knowledge without learning the com-
plex query language of SPARQL.
This paper presents our preliminary work for
NL query processing, with focus on syntactic pars-
ing. We first build a small treebank of natural
language queries, which are from Genomics track
(Hersh et al, 2004; Hersh et al, 2005; Hersh et al,
2006; Hersh et al, 2007) topics (Section 2 and 3).
The small treebank is then used to test the perfor-
mance of a state-of-the-art parser, Enju (Ninomiya
et al, 2007; Hara et al, 2007) (Section 4). The
results show that a parser trained on Wall-Street-
Journal (WSJ) articles and Medline abstracts will
not work well on query sentences. Next, we ex-
periment an adaptive learning technique, to seek the
chance to improve the parsing performance on query
sentences. Despite the small scale of the experi-
ments, the results enlighten directions for effective
3http://www.w3.org/TR/rdf-sparql-query/
4http://www.w3.org/RDF/
164
GTREC
04 05 06 07
Declarative 1 0 0 0
Imperative 22 60 0 0
Infinitive 1 0 0 0
Interrogative
- WP/WRB/WDT 3 / 1 / 11 0 / 0 / 0 6 / 22 / 0 0 / 0 / 50
- Non-wh 5 0 0 0
NP 14 0 0 0
Total 58 60 28 50
Table 1: Distribution of sentence constructions
improvement (Section 5).
2 Syntactic Features of Query Sentences
While it is reported that the state-of-art NLP tech-
nology shows reasonable performance for IR or
IE applications (Ohta et al, 2006), NLP technol-
ogy has long been developed mostly for declara-
tive sentences. On the other hand, NL queries in-
clude wide variety of sentence constructions such
as interrogative sentences, imperative sentences, and
noun phrases. Table 1 shows the distribution of the
constructions of the 196 query sentences from the
topics of the ad hoc task of Genomics track 2004
(GTREC04) and 2005 (GTREC05) in their narra-
tive forms, and the queries for the passage retrieval
task of Genomics track 2006 (GTREC06) and 2007
(GTREC07).
GTREC04 set has a variety of sentence construc-
tions, including noun phrases and infinitives, which
are not usually considered as full sentences. In the
2004 track, the queries were derived from interviews
eliciting information needs of real biologists, with-
out any control on the sentence constructions.
GTREC05 consists only of imperative sentences.
In the 2005 track, a set of templates were derived
from an analysis of the 2004 track and other known
biologist information needs. The derived templates
were used as the commands to find articles describ-
ing biological interests such as methods or roles of
genes. Although the templates were in the form
?Find articles describing ...?, actual obtained imper-
atives begin with ?Describe the procedure or method
for? (12 sentences), ?Provide information about?
(36 sentences) or ?Provide information on? (12 sen-
tences).
GTREC06 consists only of wh-questions where a
wh-word constitutes a noun phrase by itself (i.e. its
 
S  
 
  
 
VP  
 
  
 
NP  
 
  
 
PP  
 
  
 
NP  
 
  
 
PP  
 
  
NP NP NP NP 
        VB NNS IN NN IN NN 
[ ] Find articles abut function of FancD2 
 
Figure 1: The tree structure for an imperative sentence
part-of-speech is the WP in Penn Treebank (Marcus
et al, 1994) POS tag set) or is an adverb (WRB). In
the 2006 track, the templates for the 2005 track were
reformulated into the constructions of questions and
were then utilized for deriving the questions. For ex-
ample, the templates to find articles describing the
role of a gene involved in a given disease is refor-
mulated into the question ?What is the role of gene
in disease??
GTREC07 consists only of wh-questions where a
wh-word serves as a pre-nominal modifier (WDT).
In the 2007 track, unlike in those of last two years,
questions were not categorized by the templates, but
were based on biologists? information needs where
the answers were lists of named entities of a given
type. The obtained questions begin with ?what +
entity type? (45 sentences), ?which + entity type? (4
sentences), or ?In what + entity type? (1 sentence).
In contrast, the GENIA Treebank Corpus (Tateisi
et al, 2005)5 is estimated to have no imperative sen-
tences and only seven interrogative sentences (see
Section 5.2.2). Thus, the sentence constructions in
GTREC04?07 are very different from those in the
GENIA treebank.
3 Treebanking GTREC query sentences
We built a treebank (with POS) on 196 query sen-
tences following the guidelines of the GENIA Tree-
bank (Tateisi and Tsujii, 2006). The queries were
first parsed using the Stanford Parser (Klein and
Manning, 2003), and manual correction was made
5http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/
wiki.cgi?page=GENIA+Treebank
165
 SBARQ  
 
  
 
SQ  
 
   
 VP    
WHNP[i168]   NP[i169?i168]  NP[?i169] PP               
WDT NNS VBP   VBN   IN NN 
What toxicities are [ ] associated [ ] with cytarabine 
 
Figure 2: The tree structure for an interrogative sentence
by the second author. We tried to follow the guide-
line of the GENIA Treebank as closely as possible,
but for the constructions that are rare in GENIA, we
used the ATIS corpus in Penn Treebank (Bies et al,
1995), which is also a collection of query sentences,
for reference.
Figure 1 shows the tree for an imperative sen-
tence. A leaf node with [ ] corresponds to a null
constituent. Figure 2 shows the tree for an inter-
rogative sentence. Coindexing is represented by
assigning an ID to a node and a reference to the
ID to the node which is coindexed. In Figure 2,
WHNP[i168] means that the WHNP node is indexed
as i168, NP[i169?i168] means that the NP node is
indexed as i169 and coindexed to the i168 node, and
NP[?i169] means that the node is coindexed to the
i169 node. In this sentence, which is a passive wh-
question, it is assumed that the logical object (what
toxicities) of the verb (associate) is moved to the
subject position (the place of i169) and then moved
to the sentence-initial position (the place of i168).
As most of the query sentences are either impera-
tive or interrogative, there are more null constituents
compared to the GENIA Corpus. In the GTREC
query treebank, 184 / 196 (93.9%) sentences con-
tained one or more null constituents, whereas in GE-
NIA, 12,222 / 18,541 (65.9%) sentences did. We ex-
pected there are more sentences with multiple null
constituents in GTREC compared to GENIA, due to
the frequency of passive interrogative sentences, but
on the contrary the number of sentences containing
more than one null constituents are 65 (33.1%) in
GTREC, and 6,367 (34.5%) in GENIA. This may be
due to the frequency of relative clauses in GENIA.
4 Parsing system and extraction of
imperative and question sentences
We introduce the parser and the POS tagger whose
performances are examined, and the extraction of
imperative or question sentences from GTREC tree-
bank on which the performances are measured.
4.1 HPSG parser
The Enju parser (Ninomiya et al, 2007)6 is a deep
parser based on the HPSG formalism. It produces
an analysis of a sentence that includes the syntac-
tic structure (i.e., parse tree) and the semantic struc-
ture represented as a set of predicate-argument de-
pendencies. The grammar is based on the standard
HPSG analysis of English (Pollard and Sag, 1994).
The parser finds a best parse tree scored by a max-
ent disambiguation model using a Cocke-Kasami-
Younger (CKY) style algorithm.
We used a toolkit distributed with the Enju parser
for training the parser with a Penn Treebank style
(PTB-style) treebank. The toolkit initially converts
the PTB-style treebank into an HPSG treebank and
then trains the parser on it. We used a toolkit dis-
tributed with the Enju parser for extracting a HPSG
lexicon from a PTB-style treebank. The toolkit ini-
tially converts the PTB-style treebank into an HPSG
treebank and then extracts the lexicon from it.
The HPSG treebank converted from the test sec-
tion was used as the gold-standard in the evaluation.
As the evaluation metrics of the Enju parser, we used
labeled and unlabeled precision/recall/F-score of the
predicate-argument dependencies produced by the
parser. A predicate-argument dependency is repre-
sented as a tuple of ?wp, wa, r?, where wp is the
predicate word, wa is the argument word, and r is
the label of the predicate-argument relation, such
as verb-ARG1 (semantic subject of a verb) and
prep-ARG1 (modifiee of a prepositional phrase).
4.2 POS tagger
The Enju parser assumes that the input is already
POS-tagged. We use a tagger in (Tsuruoka et al,
2005). It has been shown to give a state-of-the-art
accuracy on the standard Penn WSJ data set and also
on a different text genre (biomedical literature) when
trained on the combined data set of the WSJ data and
6http://www-tsujii.is.s.u-tokyo.ac.jp/enju
166
the target genre (Tsuruoka et al, 2005). Since our
target is biomedical domain, we utilize the tagger
adapted to the domain as a baseline, which we call
?the GENIA tagger?.
4.3 Extracting imperative and question
sentences from GTREC treebank
In GTREC sentences, two major constructions of
sentences can be observed: imperative and question
sentences. These two types of sentences have differ-
ent sentence constructions and we will observe the
impact of each or both of these constructions on the
performances of parsing or POS-tagging. In order
to do so, we collected imperative and question sen-
tences from our GTREC treebank as follows:
? GTREC imperatives - Most of the impera-
tive sentences in GTREC treebank begin with
empty subjects ?(NP-SBJ */-NONE-)?. We ex-
tracted such 82 imperative sentences.
? GTREC questions - Interrogative sentences
are annotated with the phrase label ?SBARQ?
or ?SQ?, where ?SBARQ? and ?SQ? respec-
tively denote a wh-question and an yes/no ques-
tion. We extracted 98 interrogative sentences
whose top phrase labels were either of them.
5 Experiments
We examine the POS-tagger and the parser for the
sentences in the GTREC corpus. They are adapted
to each of GTREC overall, imperatives, and ques-
tions. We then observe how the parsing or POS-
tagging accuracies are improved and analyze what
is critical for parsing query sentences.
5.1 Experimental settings
5.1.1 Dividing corpora
We prepared experimental datasets for the follow-
ing four domains:
? GENIA Corpus (GENIA) (18,541 sentences)
Divided into three parts for training (14,849
sentences), development test (1,850 sentences),
and final test (1,842 sentences).
? GTREC overall (196 sentences)
Divided into two parts: one for ten-folds cross
validation test (17-18 ? 10 sentences) and the
other for error analysis (17 sentences)
Target GENIA tagger Adapted tagger
GENIA 99.04% -
GTREC (overall) 89.98% 96.54%
GTREC (imperatives) 90.32% 97.30%
GRREC (questions) 89.25% 94.77%
Table 2: Accuracy of the POS tagger for each domain
? GTREC imperatives (82 sentences)
Divided into two parts: one for ten-folds cross
validation test (7-8 ? 10 sentences) and the
other for error analysis (7 sentences)
? GTREC questions (98 sentences)
Divided into two parts: one for ten-folds cross
validation test (9 ? 10 sentences) and the other
for error analysis (8 sentences)
5.1.2 Adaptation of POS tagger and parser
In order to adapt the POS tagger and the parser to
a target domain, we took the following methods.
? POS tagger - For the GTREC overall / impera-
tives / questions, we replicated the training data
for 100,000 times and utilized the concatenated
replicas and GENIA training data in (Tsuruoka
et al, 2005) for training. For POS tagger, the
number of replicas of training data was deter-
mined among 10n(n = 0, . . . , 5) by testing
these numbers on development test sets in three
of ten datasets of cross validation.
? Enju parser - We used a toolkit in the Enju
parser (Hara et al, 2007). As a baseline model,
we utilized the model adapted to the GENIA
Corpus. We then attempted to further adapt the
model to each domain. In this paper, the base-
line model is called ?the GENIA parser?.
5.2 POS tagger and parser performances
Table 2 and 3 respectively show the POS tagging and
the parsing accuracies for the target domains, and
Figure 3 and 4 respectively show the POS tagging
and the parsing accuracies for the target domains
given by changing the size of the target training data.
The POS tagger could output for each word either
of one-best POS or POS candidates with probabili-
ties, and the Enju parser could take either of the two
output types. The bracketed numbers in Table 3 and
167
Parser GENIA Adapted
POS Gold GENIA tagger Adapted tagger Gold GENIA tagger Adapted tagger
For GENIA 88.54 88.07 (88.00) - - - -
For GTREC overall 84.37 76.81 (72.43) 83.46 (81.96) 89.00 76.98 (74.44) 86.98 (85.42)
For GTREC imperatives 85.19 78.54 (77.75) 85.71 (85.48) 89.42 74.40 (74.84) 88.97 (88.67)
For GTREC questions 85.45 76.25 (67.27) 83.55 (80.46) 87.33 81.41 (71.90) 84.87 (82.70)
[ using POS candidates with probabilities (using only one best POS) ]
Table 3: Accuracy of the Enju parser for GTREC
70
75
80
85
90
0 20 40 60 80 100 120 140
F
-
s
c
o
r
e
Corpus size (sentences)
70
75
80
85
90
0 20 40 60
F
-
s
c
o
r
e
Corpus size (sentences)
65
70
75
80
85
90
95
0 20 40 60 80
F
-
s
c
o
r
e
Corpus size (sentences)
Adapted parser, gold POS
Adapted parser, adapted tagger (prob.)
GENIA parser, adapted tagger (prob.)
Adapted parser, GENIA tagger (prob.)
Adapted parser, adapted tagger (1best)
GENIA parser, adapted tagger (1best)
Adapted parser, GENIA tagger (1best)
For GTREC imperatives For GTREC questionsFor GTREC overall
Figure 4: Parsing accuracy vs. corpus size
88
90
92
94
96
98
0 50 100 150
A
c
c
u
r
a
c
y
 (
%
)
Corpus size (sentences)
GTREC overall
GTREC imperatives
GTREC questions
Figure 3: POS tagging accuracy vs. corpus size
the dashed lines in Figure 4 show the parsing accu-
racies when we utilized one-best POS given by the
POS tagger, and the other numbers and lines show
the accuracies given by POS candidates with proba-
bilities. In the rest of this section, when we just say
?POS tagger?, the tagger?s output is POS candidates
with probabilities.
Table 4 and 5 respectively compare the types of
POS tagging and parsing errors for each domain
between before and after adapting the POS tagger,
and Table 6 compares the types of parsing errors for
Correct ? Error GENIA tagger Adapted tagger
For GTREC overall (17 sentences)
NN ? NNP 4 0.6
VB ? NN 4 0
WDT ? WP 4 0
NN ? JJ 1 1.9
For GTREC imperative (seven sentences)
FW ? NNP / NN / JJ 7 4
VB ? NN 4 0
NN ? NNP 2 0
For GTREC question (eight sentences)
WDT ? WP 3 0
VB ? VBP 2 1
NNS ? VBZ 2 0
(The table shows only error types observed more than
once for either of the taggers)
Table 4: Tagging errors for each of the GTREC corpora
each domain between before and after adapting the
parser. The numbers of errors for the rightmost col-
umn in each of the tables were given by the average
of the ten-folds cross validation results.
In the following sections, we examine the im-
pact of the performances of the POS taggers or the
parsers on parsing the GTREC documents.
168
GENIA parserError types GENIA tagger Adapted tagger
For GTREC overall (17 sentences)
Failure in detecting verb 12 0.2
Root selection 6 0
Range of NP 5 5
PP-attachment 4 3
Determiner / pronoun 4 1
Range of verb subject 4 4
Range of verb object 3 3
Adjective / modifier noun 2 3
For GTREC imperatives (seven sentences)
Failure in detecting verb 8 0
Root selection 4 0
Range of NP 3 4
PP-attachment 3 1.8
Range of PP 2 2
For GTREC questions (eight sentences)
Range of coordination 5 3
Determiner / pronoun 3 0
PP-attachment 3 1
Range of PP 2 2
Subject for verb 2 1
(The table shows only the types of parsing errors observed more
than once for either of the parsers)
Table 5: Impact of adapting POS tagger on parsing errors
5.2.1 Impact of POS tagger on parsing
In Table 2, for each of the GTREC corpora,
the GENIA tagger dropped its tagging accuracy by
around nine points, and then recovered five to seven
points by the adaptation. According to this behav-
ior of the tagger, Table 3 shows that the GENIA and
the adapted parsers with the GENIA tagger dropped
their parsing accuracies by 6?15 points in F-score
from the accuracies with the gold POS, and then re-
covered the accuracies within two points below the
accuracies with the gold POS. The performance of
the POS tagger would thus critically affect the pars-
ing accuracies.
In Figure 3, we can observe that the POS tagging
accuracy for each corpus rapidly increased only for
first 20?30 sentences, and after that the improvement
speed drastically declined. Accordingly, in Figure 4,
the line for the adapted parser with the adapted tag-
ger (the line with triangle plots) rose rapidly for the
first 20?30 sentences, and after that slowed down.
We explored the tagging and parsing errors, and
analyze the cause of the initial accuracy jump and
the successive improvement depression.
Gold POSError types GENIA parser Adapted parser
For GTREC overall (17 sentences)
Range of NP 5 1.3
Range of verb subject 3 2.6
PP-attachment 3 2.7
Whether verb takes
object & complement 3 2.9
Range of verb object 2 1
For GTREC imperatives (seven sentences)
Range of NP 4 1.1
PP-attachment 2 1.6
Range of PP 2 0.3
Preposition / modifier 2 2
For GTREC questions (eight sentences)
Coordination / conjunction 2 2.2
Auxiliary / normal verb 2 2.6
Failure in detecting verb 2 2.6
(The table shows only the types of parsing errors observed more
than once for either of the parsers)
Table 6: Impact of adapting parser on parsing errors
Cause of initial accuracy jump
In Table 4, ?VB ? NN? tagging errors were
observed only in imperative sentences and drasti-
cally decreased by the adaptation. In a impera-
tive sentence, a verb (VB) usually appears as the
first word. On the other hand, the GENIA tagger
was trained mainly on the declarative sentences and
therefore would often take the first word in a sen-
tence as the subject of the sentence, that is, noun
(NN). When the parser received a wrong NN-tag for
a verb, the parser would attempt to believe the infor-
mation (?failure in detecting verb? in Table 6) and
could then hardly choose the NN-tagged word as a
main verb (?root selection? in Table 6). By adapting
the tagger, the correct tag was given to the verb and
the parser could choose the verb as a main verb.
?WDT ? WP? tagging errors were observed only
in the question sentences and also drastically de-
creased. For example, in the sentence ?What toxici-
ties are associated with cytarabine??, ?What? works
as a determiner (WDT) which takes ?toxicities?,
while the GENIA tagger often took this ?What? as a
pronoun (WP) making a phrase by itself. This would
be because the training data for the GENIA tagger
would contain 682 WP ?what? and only 27 WDT
?what?. WP ?what? could not make a noun phrase
by taking a next noun, and then the parsing of the
parsing would corrupt (?determiner / pronoun? in
Table 5). By adapting the tagger, ?WDT? tag was
169
given to ?What?, and the parser correctly made a
phrase ?What toxicities?.
Since the variation of main verbs in GTREC im-
peratives is very small (see Section 2) and that of
interrogatives is also very small, in order to cor-
rect the above two types of errors, we would require
only small training data. In addition, these types of
errors widely occurred among imperatives or ques-
tions, the accuracy improvement by correcting the
errors was very large. The initial rapid improvement
would thus occur.
Cause of improvement depression
?NN ? NNP? tagging errors would come from
the description style of words. In the GTREC
queries, technical terms, such as the names of dis-
eases or proteins, sometimes begin with capital char-
acters. The GENIA tagger would take the capi-
talized words not as a normal noun (NN) but as a
proper noun (NNP). By adaptation, the tagger would
have learned the capital usage for terms and the er-
rors then decreased.
However, in order to achieve such improvement,
we would have to wait until a target capitalized term
is added to the training corpus. ?FW ? NNP / NN
/ JJ?, ?NN ? JJ?, and several other errors would be
similar to this type of errors in the point that, they
would be caused by the difference in annotation pol-
icy or description style between the training data for
the GENIA tagger and the GTREC queries.
?VB ? VBP? errors were found in questions. For
example, ?affect? in the question ?How do muta-
tions in Sonic Hedgehog genes affect developmen-
tal disorders?? was base form (VB), while the GE-
NIA tagger took it as a present tense (VBP) since
the GENIA tagger would be unfamiliar with such
verb behavior in questions. By adaptation, the tag-
ger would learn that verbs in the domain tend to take
base forms and the errors then decreased.
However, the tagger model based on local context
features could not substantially solve the problem.
VBP of course could appear in question sentences.
We observed that a verb to be VBP was tagged with
VB by the adapted tagger. In order to distinguish
VB from VBP, we should capture longer distance
dependencies between auxiliary and main verbs.
In tagging, the fact that the above two types of
errors occupied most of the errors other than the er-
rors involved in the initial jump, would be related
to why the accuracy improvement got so slowly,
which would lead to the improvement depression of
the parsing performances. With the POS candidates
with probabilities, the possibilities of correct POSs
would increase, and therefore the parser would give
higher parsing performances than using only one-
best POSs (see Table 3 and Figure 4).
Anyway, the problems were not substantially
solved. For these tagging problems, just adding the
training data would not work. We might need re-
construct the tagging system or re-consider the fea-
ture designs of the model.
5.2.2 Impact of parser itself on parsing
For the GTREC corpora, the GENIA parser with
gold POSs lowered the parsing accuracy by more
than three points than for the GENIA Corpus, while
the adaptation of the parser recovered a few points
for each domain (second and fifth column in Table
3). Figure 4 would also show that we could improve
the parser?s performance with more training data for
each domain. For GTREC questions, the parsing ac-
curacy dropped given the maximum size of the train-
ing data. Our training data is small and therefore
small irregular might easily make accuracies drop or
rise. 7 We might have to prepare more corpora for
confirming our observation.
Table 6 would imply that the major errors for all
of these three corpora seem not straightforwardly as-
sociated with the properties specific to imperative or
question sentences. Actually, when we explored the
parse results, errors on the sentence constructions
specific to the two types of sentences would hardly
be observed. (?Failure in detecting verb? errors in
GTREC questions came from other causes.) This
would mean that the GENIA parser itself has poten-
tial to parse the imperative or question sentences.
The training data of the GENIA parser consists
of the WSJ Penn Treebank and the GENIA Corpus.
As long as we searched with our extraction method
in Section 4.3, the WSJ and GENIA Corpus seem
respectively contain 115 and 0 imperative, and 432
7This time we could not analyze which training data affected
the decrease, because through the cross validation experiments
each sentence was forced to be once final test data. However,
we would like to find the reason for this accuracy decrease in
some way.
170
and seven question sentences. Unlike the POS tag-
ger, the parser could convey more global sentence
constructions from these sentences.
Although the GENIA parser might understand the
basic constructions of imperative or question sen-
tences, by adaptation of the parser to the GTREC
corpora, we could further learn more local construc-
tion features specific to GTREC, such as word se-
quence constructing a noun phrase, attachment pref-
erence of prepositions or other modifiers. The error
reduction in Table 6 would thus be observed.
However, we also observed that several types of
errors were still mostly unsolved after the adapta-
tion. Choosing whether to add complements for
verbs or not, and distinguishing coordinations from
conjunctions seems to be difficult for the parser. If
two question sentences were concatenated by con-
junctions into one sentence, the parser would tend to
fail to analyze the sentence construction for the lat-
ter sentence. The remaining errors in Table 6 would
imply that we should also re-consider the model de-
signs or the framework itself for the parser in addi-
tion to just increasing the training data.
6 Related work
Since domain adaptation has been an extensive re-
search area in parsing research (Nivre et al, 2007),
a lot of ideas have been proposed, including un-
/semi-supervised approaches (Roark and Bacchiani,
2003; Blitzer et al, 2006; Steedman et al, 2003;
McClosky et al, 2006; Clegg and Shepherd, 2005;
McClosky et al, 2010) and supervised approaches
(Titov and Henderson, 2006; Hara et al, 2007).
Their main focus was on adapting parsing models
trained with a specific genre of text (in most cases
PTB-WSJ) to other genres of text, such as biomed-
ical research papers. A major problem tackled in
such a task setting is the handling of unknown words
and domain-specific ways of expressions. However,
as we explored, parsing NL queries involves a sig-
nificantly different problem; even when all words in
a sentence are known, the sentence has a very differ-
ent construction from declarative sentences.
Although sentence constructions have gained lit-
tle attention, a notable exception is (Judge et al,
2006). They pointed out low accuracy of state-of-
the-art parsers on questions, and proposed super-
vised parser adaptation by manually creating a tree-
bank of questions. The question sentences are anno-
tated with phrase structure trees in the PTB scheme,
although function tags and empty categories are
omitted. An LFG parser trained on the treebank then
achieved a significant improvement in parsing ac-
curacy. (Rimell and Clark, 2008) also worked on
question parsing. They collected question sentences
from TREC 9-12, and annotated the sentences with
POSs and CCG (Steedman, 2000) lexical categories.
They reported a significant improvement in CCG
parsing without phrase structure annotations.
On the other hand, (Judge et al, 2006) also im-
plied that just increasing the training data would not
be enough. We went further from their work, built
a small but complete treebank for NL queries, and
explored what really occurred in HPSG parsing.
7 Conclusion
In this paper, we explored the problem in parsing
queries. We first attempted to build a treebank on
queries for biological knowledge and successfully
obtained 196 annotated GTREC queries. We next
examined the performances of the POS tagger and
the HPSG parser on the treebank. In the experi-
ments, we focused on the two dominant sentence
constructions in our corpus: imperatives and ques-
tions, extracted them from our corpus, and then also
examined the parser and tagger for them.
The experimental results showed that the POS
tagger?s mis-tagging to main verbs in imperatives
and wh-interrogatives in questions critically de-
creased the parsing performances, and that our
small corpus could drastically decrease such mis-
tagging and consequently improve the parsing per-
formances. The experimental results also showed
that the parser itself could improve its own perfor-
mance by increasing the training data. On the other
hand, the experimental results suggested that the
POS tagger or the parser performance would stag-
nate just by increasing the training data.
In our future research, on the basis of our findings,
we would like both to build more training data for
queries and to reconstruct the model or reconsider
the feature design for the POS tagger and the parser.
We would then incorporate the optimized parser and
tagger into NL query processing applications.
171
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
style ? Penn Treebank project. Technical report, De-
partment of Linguistics, University of Pennsylvania.
John Blitzer, Ryan Mcdonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia.
A. B. Clegg and A. Shepherd. 2005. Evaluating and in-
tegrating treebank parsers on a biomedical corpus. In
Proceedings of the ACL 2005 Workshop on Software,
Ann Arbor, Michigan.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of 10th International Confer-
ence on Parsing Technologies (IWPT 2007), pages 11?
22.
William R. Hersh, Ravi Teja Bhupatiraju, L. Ross,
Aaron M. Cohen, Dale Kraemer, and Phoebe Johnson.
2004. TREC 2004 Genomics Track Overview. In Pro-
ceedings of the Thirteenth Text REtrieval Conference,
TREC 2004.
William R. Hersh, Aaron M. Cohen, Jianji Yang,
Ravi Teja Bhupatiraju, Phoebe M. Roberts, and
Marti A. Hearst. 2005. TREC 2005 Genomics Track
Overview. In Proceedings of the Fourteenth Text RE-
trieval Conference, TREC 2005.
William R. Hersh, Aaron M. Cohen, Phoebe M. Roberts,
and Hari Krishna Rekapalli. 2006. TREC 2006 Ge-
nomics Track Overview. In Proceedings of the Fif-
teenth Text REtrieval Conference, TREC 2006.
William R. Hersh, Aaron M. Cohen, Lynn Ruslen, and
Phoebe M. Roberts. 2007. TREC 2007 Genomics
Track Overview. In Proceedings of The Sixteenth Text
REtrieval Conference, TREC 2007.
John Judge, Aoife Cahill, and Josef van Genabith.
2006. Questionbank: Creating a Corpus of Parsing-
Annotated Questions. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the ACL, pages 497?504.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert Macintyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of ARPA Human Language Technology
Workshop.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 337?344, Sydney, Australia.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic Domain Adaptation for Parsing. In
Proceedings of the 2010 Annual Conference of the
North American Chapter of the ACL, pages 28?36, Los
Angeles, California.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao,
and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate hpsg pars-
ing. In Proceedings of 10th International Conference
on Parsing Technologies (IWPT 2007), pages 60?68.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Laura Rimell and Stephen Clark. 2008. Adapting a
Lexicalized-Grammar Parser to Contrasting Domains.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 475?
584.
Brian Roark and Michiel Bacchiani. 2003. Supervised
and unsupervised PCFG adaptation to novel domains.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 126?133, Edmonton, Canada.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the tenth conference on European chap-
ter of the Association for Computational Linguistics,
pages 331?338, Budapest, Hungary.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
Yuka Tateisi and Jun?ichi Tsujii. 2006. GENIA Anno-
tation Guidelines for Treebanking. Technical Report
TR-NLP-UT-2006-5, Tsujii Laboratory, University of
Tokyo.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the Second Interna-
tional Joint Conference on Natural Language Process-
172
ing (IJCNLP 2005), Companion volume, pages 222?
227.
Ivan Titov and James Henderson. 2006. Porting statis-
tical parsers with data-defined kernels. In Proceed-
ings of the Tenth Conference on Computational Natu-
ral Language Learning, pages 6?13, New York City.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics - 10th Panhellenic Conference on Infor-
matics, volume LNCS 3746, pages 382?392, Volos,
Greece, November. ISSN 0302-9743.
173
Proceedings of BioNLP Shared Task 2011 Workshop, pages 1?6,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of BioNLP Shared Task 2011
Jin-Dong Kim
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
jdkim@dbcls.rois.ac.jp
Sampo Pyysalo
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
smp@is.s.u-tokyo.ac.jp
Tomoko Ohta
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
okap@is.s.u-tokyo.ac.jp
Robert Bossy
National Institute for Agricultural Research
78352 Jouy en Josas, Cedex
Robert.Bossy@jouy.inra.fr
Ngan Nguyen
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
nltngan@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Microsoft Research Asia
5 Dan Ling Street, Haiian District, Beijing
jtsujii@microsoft.com
Abstract
The BioNLP Shared Task 2011, an informa-
tion extraction task held over 6 months up to
March 2011, met with community-wide par-
ticipation, receiving 46 final submissions from
24 teams. Five main tasks and three support-
ing tasks were arranged, and their results show
advances in the state of the art in fine-grained
biomedical domain information extraction and
demonstrate that extraction methods success-
fully generalize in various aspects.
1 Introduction
The BioNLP Shared Task (BioNLP-ST, hereafter)
series represents a community-wide move toward
fine-grained information extraction (IE), in particu-
lar biomolecular event extraction (Kim et al, 2009;
Ananiadou et al, 2010). The series is complemen-
tary to BioCreative (Hirschman et al, 2007); while
BioCreative emphasizes the short-term applicability
of introduced IE methods for tasks such as database
curation, BioNLP-ST places more emphasis on the
measurability of the state-of-the-art and traceabil-
ity of challenges in extraction through an approach
more closely tied to text.
These goals were pursued in the first event,
BioNLP-ST 2009 (Kim et al, 2009), through high
quality benchmark data provided for system devel-
opment and detailed evaluation performed to iden-
tify remaining problems hindering extraction perfor-
mance. Also, as the complexity of the task was high
and system development time limited, we encour-
aged focus on fine-grained IE by providing gold an-
notation for named entities as well as various sup-
porting resources. BioNLP-ST 2009 attracted wide
attention, with 24 teams submitting final results. The
task setup and data since have served as the basis
for numerous studies (Miwa et al, 2010b; Poon and
Vanderwende, 2010; Vlachos, 2010; Miwa et al,
2010a; Bjo?rne et al, 2010).
As the second event of the series, BioNLP-ST
2011 preserves the general design and goals of the
previous event, but adds a new focus on variabil-
ity to address a limitation of BioNLP-ST 2009: the
benchmark data sets were based on the Genia corpus
(Kim et al, 2008), restricting the community-wide
effort to resources developed by a single group for
a small subdomain of molecular biology. BioNLP-
ST 2011 is organized as a joint effort of several
groups preparing various tasks and resources, in
which variability is pursued in three primary direc-
tions: text types, event types, and subject domains.
Consequently, generalization of fine grained bio-IE
in these directions is emphasized as the main theme
of the second event.
This paper summarizes the entire BioNLP-ST
2011, covering the relationships between tasks and
similar broad issues. Each task is presented in detail
in separate overview papers and extraction systems
in papers by participants.
1
2 Main tasks
BioNLP-ST 2011 includes four main tracks (with
five tasks) representing fine-grained bio-IE.
2.1 Genia task (GE)
The GE task (Kim et al, 2011) preserves the task
definition of BioNLP-ST 2009, arranged based on
the Genia corpus (Kim et al, 2008). The data repre-
sents a focused domain of molecular biology: tran-
scription factors in human blood cells. The purpose
of the GE task is two-fold: to measure the progress
of the community since the last event, and to eval-
uate generalization of the technology to full papers.
For the second purpose, the provided data is com-
posed of two collections: the abstract collection,
identical to the BioNLP-ST 2009 data, and the new
full paper collection. Progress on the task is mea-
sured through the unchanged task definition and the
abstract collection, while generalization to full pa-
pers is measured on the full paper collection. In this
way, the GE task is intended to connect the entire
event to the previous one.
2.2 Epigenetics and post-translational
modification task (EPI)
The EPI task (Ohta et al, 2011) focuses on IE for
protein and DNA modifications, with particular em-
phasis on events of epigenetics interest. While the
basic task setup and entity definitions follow those of
the GE task, EPI extends on the extraction targets by
defining 14 new event types relevant to task topics,
including major protein modification types and their
reverse reactions. For capturing the ways in which
different entities participate in these events, the task
extends the GE argument roles with two new roles
specific to the domain, Sidechain and Contextgene.
The task design and setup are oriented toward the
needs of pathway extraction and curation for domain
databases (Wu et al, 2003; Ongenaert et al, 2008)
and are informed by previous studies on extraction
of the target events (Ohta et al, 2010b; Ohta et al,
2010c).
2.3 Infectious diseases task (ID)
The ID task (Pyysalo et al, 2011a) concerns the ex-
traction of events relevant to biomolecular mecha-
nisms of infectious diseases from full-text publica-
tions. The task follows the basic design of BioNLP-
ST 2009, and the ID entities and extraction targets
are a superset of the GE ones. The task extends
considerably on core entities, adding to PROTEIN
four new entity types, including CHEMICAL and
ORGANISM. The events extend on the GE defini-
tions in allowing arguments of the new entity types
as well as in introducing a new event category for
high-level biological processes. The task was im-
plemented in collaboration with domain experts and
informed by prior studies on domain information ex-
traction requirements (Pyysalo et al, 2010; Anani-
adou et al, 2011), including the support of systems
such as PATRIC (http://patricbrc.org).
2.4 Bacteria track
The bacteria track consists of two tasks, BB and BI.
2.4.1 Bacteria biotope task (BB)
The aim of the BB task (Bossy et al, 2011) is to ex-
tract the habitats of bacteria mentioned in textbook-
level texts written for non-experts. The texts are
Web pages about the state of the art knowledge about
bacterial species. BB targets general relations, Lo-
calization and PartOf , and is challenging in that
texts contain more coreferences than usual, habitat
references are not necessarily named entities, and,
unlike in other BioNLP-ST 2011 tasks, all entities
need to be recognized by participants. BB is the first
task to target phenotypic information and, as habi-
tats are yet to be normalized by the field community,
presents an opportunity for the BioNLP community
to contribute to the standardization effort.
2.4.2 Bacteria interaction task (BI)
The BI task (Jourde et al, 2011) is devoted to the ex-
traction of bacterial molecular interactions and reg-
ulations from publication abstracts. Mainly focused
on gene transcriptional regulation in Bacillus sub-
tilis, the BI corpus is provided to participants with
rich semantic annotation derived from a recently
proposed ontology (Manine et al, 2009) defining
ten entity types such as gene, protein and deriva-
tives as well as DNA sites/motifs. Their interactions
are described through ten relation types. The BI
corpus consists of the sentences of the LLL corpus
(Ne?dellec, 2005), provided with manually checked
linguistic annotations.
2
Task Text Focus #
GE abstracts, full papers domain (HT) 9
EPI abstracts event types 15
ID full papers domain (TCS) 10
BB web pages domain (BB) 2
BI abstracts domain (BS) 10
Table 1: Characteristics of BioNLP-ST 2011 main tasks.
?#?: number of event/relation types targeted. Domains:
HT = human transcription factors in blood cells, TCS
= two-component systems, BB = bacteria biology, BS =
Bacillus subtilis
2.5 Characteristics of main tasks
The main tasks are characterized in Table 1. From
the text type perspective, BioNLP-ST 2011 gener-
alizes from abstracts in 2009 to full papers (GE and
ID) and web pages (BB). It also includes data collec-
tions for a variety of specific subject domains (GE,
ID, BB an BI) and a task (EPI) whose scope is not
defined through a domain but rather event types. In
terms of the target event types, ID targets a superset
of GE events and EPI extends on the representation
for PHOSPHORYLATION events of GE. The two bac-
teria track tasks represent an independent perspec-
tive relatively far from other tasks in terms of their
target information.
3 Supporting tasks
BioNLP-ST 2011 includes three supporting tasks
designed to assist in primary the extraction tasks.
Other supporting resources made available to par-
ticipants are presented in (Stenetorp et al, 2011).
3.1 Protein coreference task (CO)
The CO task (Nguyen et al, 2011) concerns the
recognition of coreferences to protein references. It
is motivated from a finding from BioNLP-ST 2009
result analysis: coreference structures in biomedical
text hinder the extraction results of fine-grained IE
systems. While finding connections between event
triggers and protein references is a major part of
event extraction, it becomes much harder if one is
replaced with a coreferencing expression. The CO
task seeks to address this problem. The data sets for
the task were produced based on MedCO annotation
(Su et al, 2008) and other Genia resources (Tateisi
et al, 2005; Kim et al, 2008).
Event Date Note
Sample Data 31 Aug. 2010
Support. Tasks
Train. Data 27 Sep. 2010 7 weeks for development
Test Data 15 Nov. 2010 4 days for submission
Submission 19 Nov. 2010
Evaluation 22 Nov. 2010
Main Tasks
Train. Data 1 Dec. 2010 3 months for development
Test Data 1 Mar. 2011 9 days for submission
Submission 10 Mar. 2011 extended from 8 Mar.
Evaluation 11 Mar. 2011 extended from 10 Mar.
Table 2: Schedule of BioNLP-ST 2011
3.2 Entity relations task (REL)
The REL task (Pyysalo et al, 2011b) involves the
recognition of two binary part-of relations between
entities: PROTEIN-COMPONENT and SUBUNIT-
COMPLEX. The task is motivated by specific chal-
lenges: the identification of the components of pro-
teins in text is relevant e.g. to the recognition of
Site arguments (cf. GE, EPI and ID tasks), and re-
lations between proteins and their complexes rele-
vant to any task involving them. REL setup is in-
formed by recent semantic relation tasks (Hendrickx
et al, 2010). The task data, consisting of new anno-
tations for GE data, extends a previously introduced
resource (Pyysalo et al, 2009; Ohta et al, 2010a).
3.3 Gene renaming task (REN)
The REN task (Jourde et al, 2011) objective is to ex-
tract renaming pairs of Bacillus subtilis gene/protein
names from PubMed abstracts, motivated by dis-
crepancies between nomenclature databases that in-
terfere with search and complicate normalization.
REN relations partially overlap several concepts:
explicit renaming mentions, synonymy, and renam-
ing deduced from biological proof. While the task
is related to synonymy relation extraction (Yu and
Agichtein, 2003), it has a novel definition of renam-
ing, one name permanently replacing the other.
4 Schedule
Table 2 shows the task schedule, split into two
phases to allow the use of supporting task results in
addressing the main tasks. In recognition of their
higher complexity, a longer development period was
arranged for the main tasks (3 months vs 7 weeks).
3
Team GE EPI ID BB BI CO REL REN
UTurku 1 1 1 1 1 1 1 1
ConcordU 1 1 1 1 1 1
UMass 1 1 1
Stanford 1 1 1
FAUST 1 1 1
MSR-NLP 1 1
CCP-BTMG 1 1
Others 8 0 2 2 0 4 2 1
SUM 15 7 7 3 1 6 4 3
Table 3: Final submissions to BioNLP-ST 2011 tasks.
5 Participation
BioNLP-ST 2011 received 46 submissions from 24
teams (Table 3). While seven teams participated in
multiple tasks, only one team, UTurku, submitted fi-
nal results to all the tasks. The remaining 17 teams
participated in only single tasks. Disappointingly,
only two teams (UTurku, and ConcordU) performed
both supporting and main tasks, and neither used
supporting task analyses for the main tasks.
6 Results
Detailed evaluation results and analyses are pre-
sented in individual task papers, but interesting ob-
servations can be obtained also by comparisons over
the tasks. Table 4 summarizes best results for vari-
ous criteria (Note that the results shown for e.g. GEa,
GEf and GEp may be from different teams).
The community has made a significant improve-
ment in the repeated GE task, with an over 10%
reduction in error from ?09 to GEa. Three teams
achieved better results than M10, the best previously
reported individual result on the ?09 data. This in-
dicates a beneficial role from focused efforts like
BioNLP-ST. The GEf and ID results show that
generalization to full papers is feasible, with very
modest loss in performance compared to abstracts
(GEa). The results for PHOSPHORYLATION events
in GE and EPI are comparable (GEp vs EPIp), with
the small drop for the EPI result, suggesting that
the removal of the GE domain specificity does not
compromise extraction performance. EPIc results
indicate some challenges in generalization to simi-
lar event types, and EPIf suggest substantial further
challenges in additional argument extraction. The
complexity of ID is comparable to GE, also reflected
to their final results, which further indicate success-
Task Evaluation Results
BioNLP-ST 2009 (?09) 46.73 / 58.48 / 51.95
Miwa et al (2010b) (M10) 48.62 / 58.96 / 53.29
LLL 2005 (LLL) 53.00 / 55.60 / 54.30
GE abstracts (GEa) 50.00 / 67.53 / 57.46
GE full texts (GEf) 47.84 / 59.76 / 53.14
GE PHOSPHORYLATION (GEp) 79.26 / 86.99 / 82.95
GE LOCALIZATION (GEl) 37.88 / 77.42 / 50.87
EPI full task (EPIf) 52.69 / 53.98 / 53.33
EPI core task (EPIc) 68.51 / 69.20 / 68.86
EPI PHOSPHORYLATION (EPIp) 86.15 / 74.67 / 80.00
ID full task (IDf) 48.03 / 65.97 / 55.59
ID core task (IDc) 50.62 / 66.06 / 57.32
BB 45.00 / 45.00 / 45.00
BB PartOf (BBp) 32.00 / 83.00 / 46.00
BI 71.00 / 85.00 / 77.00
CO 22.18 / 73.26 / 34.05
REL 50.10 / 68.00 / 57.70
REN 79.60 / 95.90 / 87.00
Table 4: Best results for various (sub)tasks (recall / preci-
sion / f-score (%)). GEl: task 2 without trigger detection.
ful generalization to a new subject domain as well
as to new argument (entity) types. The BB task is
in part comparable to GEl and involves a represen-
tation similar to REL, with lower results likely in
part because BB requires entity recognition. The BI
task is comparable to LLL Challenge, though BI in-
volves more entity and event types. The BI result
is 20 points above the LLL best result, indicating a
substantial progress of the community in five years.
7 Discussion and Conclusions
Meeting with wide participation from the commu-
nity, BioNLP-ST 2011 produced a wealth of valu-
able resources for the advancement of fine-grained
IE in biology and biomedicine, and demonstrated
that event extraction methods can successfully gen-
eralize to new text types, event types, and domains.
However, the goal to observe the capacity of sup-
porting tasks to assist the main tasks was not met.
The entire shared task period was very long, more
than 6 months, and the complexity of the task was
high, which could be an excessive burden for partic-
ipants, limiting the application of novel resources.
There have been ongoing efforts since BioNLP-ST
2009 to develop IE systems based on the task re-
sources, and we hope to see continued efforts also
following BioNLP-ST 2011, especially exploring
the use of supporting task resources for main tasks.
4
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology.
Sophia Ananiadou, Dan Sullivan, William Black, Gina-
Anne Levow, Joseph J. Gillespie, Chunhong Mao,
Sampo Pyysalo, BalaKrishna Kolluru, Junichi Tsujii,
and Bruno Sobral. 2011. Named entity recognition
for bacterial type IV secretion systems. PLoS ONE,
6(3):e14780.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Complex event extraction
at PubMed scale. Bioinformatics, 26(12):i382?390.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid ?O. Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. Semeval-2010 task 8: Multi-way classi-
fication of semantic relations between pairs of nom-
inals. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ?10, pages 33?
38, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Lynette Hirschman, Martin Krallinger, and Alfonso Va-
lencia, editors. 2007. Proceedings of the Second
BioCreative Challenge Evaluation Workshop. CNIO
Centro Nacional de Investigaciones Oncolo?gicas.
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task 2011
- Bacteria Gene Interactions and Renaming. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
A.P. Manine, E. Alphonse, and Bessie`res P. 2009. Learn-
ing ontological rules to extract multiple relations of
genic interactions from text. International Journal of
Medical Informatics, 78(12):e31?38.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In Proceedings of
BioNLP?10, pages 37?45.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010b. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146, February.
Ne?dellec. 2005. Learning Language in Logic ? Genic
Interaction Extraction Challenge. In Proceedings of
4th Learning Language in Logic Workshop (LLL?05),
pages 31?37.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Overview of the Protein Coreference task in BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010a. A re-evaluation of biomedical
named entity-term relations. Journal of Bioinformat-
ics and Computational Biology (JBCB), 8(5):917?928.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, Jin-Dong
Kim, and Jun?ichi Tsujii. 2010b. Event extraction
for post-translational modifications. In Proceedings of
BioNLP?10, pages 19?27.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and
Jun?ichi Tsujii. 2010c. Event extraction for dna
methylation. In Proceedings of SMBM?10.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Mate? Ongenaert, Leander Van Neste, Tim De Meyer,
Gerben Menschaert, Sofie Bekaert, and Wim
Van Criekinge. 2008. PubMeth: a cancer methylation
database combining text-mining and expert annota-
tion. Nucleic Acids Research, 36(suppl 1):D842?846.
Hoifung Poon and Lucy Vanderwende. 2010. Joint infer-
ence for knowledge extraction from biomedical litera-
ture. In Proceedings of NAACL-HLT?10, pages 813?
821.
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static Relations: a Piece
5
in the Biomedical Information Extraction Puzzle.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9, Boulder, Colorado. Association for Computa-
tional Linguistics.
Sampo Pyysalo, Tomoko Ohta, Han-Cheol Cho, Dan Sul-
livan, Chunhong Mao, Bruno Sobral, Jun?ichi Tsujii,
and Sophia Ananiadou. 2010. Towards event extrac-
tion from full texts on infectious diseases. In Proceed-
ings of BioNLP?10, pages 132?140.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the Entity Relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
Jian Su, Xiaofeng Yang, Huaqing Hong, Yuka Tateisi,
and Jun?ichi Tsujii. 2008. Coreference Resolution in
Biomedical Texts: a Machine Learning Approach. In
Ontologies and Text Mining for Life Sciences?08.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the IJCNLP 2005,
Companion volume, pages 222?227.
Andreas Vlachos. 2010. Two strong baselines for the
bionlp 2009 event extraction task. In Proceedings of
BioNLP?10, pages 1?9.
Cathy H. Wu, Lai-Su L. Yeh, Hongzhan Huang, Leslie
Arminski, Jorge Castro-Alvear, Yongxing Chen,
Zhangzhi Hu, Panagiotis Kourtesis, Robert S. Led-
ley, Baris E. Suzek, C.R. Vinayaka, Jian Zhang, and
Winona C. Barker. 2003. The Protein Information
Resource. Nucleic Acids Research, 31(1):345?347.
H. Yu and E. Agichtein. 2003. Extracting synony-
mous gene and protein terms from biological litera-
ture. Bioinformatics, 19(suppl 1):i340.
6
Proceedings of BioNLP Shared Task 2011 Workshop, pages 7?15,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of Genia Event Task in BioNLP Shared Task 2011
Jin-Dong Kim
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
jdkim@dbcls.rois.ac.jp
Yue Wang
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
wang@dbcls.rois.ac.jp
Toshihisa Takagi
University of Tokyo
5-1-5 Kashiwa-no-ha, Kashiwa, Chiba
tt@k.u-tokyo.ac.jp
Akinori Yonezawa
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
yonezawa@dbcls.rois.ac.jp
Abstract
The Genia event task, a bio-molecular event
extraction task, is arranged as one of the main
tasks of BioNLP Shared Task 2011. As its sec-
ond time to be arranged for community-wide
focused efforts, it aimed to measure the ad-
vance of the community since 2009, and to
evaluate generalization of the technology to
full text papers. After a 3-month system de-
velopment period, 15 teams submitted their
performance results on test cases. The re-
sults show the community has made a sig-
nificant advancement in terms of both perfor-
mance improvement and generalization.
1 Introduction
The BioNLP Shared Task (BioNLP-ST, hereafter)
is a series of efforts to promote a community-
wide collaboration towards fine-grained informa-
tion extraction (IE) in biomedical domain. The
first event, BioNLP-ST 2009, introducing a bio-
molecular event (bio-event) extraction task to the
community, attracted a wide attention, with 42 teams
being registered for participation and 24 teams sub-
mitting final results (Kim et al, 2009).
To establish a community effort, the organizers
provided the task definition, benchmark data, and
evaluations, and the participants competed in devel-
oping systems to perform the task. Meanwhile, par-
ticipants and organizers communicated to develop a
better setup of evaluation, and some provided their
tools and resources for other participants, making it
a collaborative competition.
The final results enabled to observe the state-of-
the-art performance of the community on the bio-
event extraction task, which showed that the auto-
matic extraction of simple events - those with unary
arguments, e.g. gene expression, localization, phos-
phorylation - could be achieved at the performance
level of 70% in F-score, but the extraction of com-
plex events, e.g. binding and regulation, was a lot
more challenging, having achieved 40% of perfor-
mance level.
After BioNLP-ST 2009, all the resources from the
event were released to the public, to encourage con-
tinuous efforts for further advancement. Since then,
several improvements have been reported (Miwa et
al., 2010b; Poon and Vanderwende, 2010; Vlachos,
2010; Miwa et al, 2010a; Bjo?rne et al, 2010).
For example, Miwa et al (Miwa et al, 2010b)
reported a significant improvement with binding
events, achieving 50% of performance level.
The task introduced in BioNLP-ST 2009 was re-
named to Genia event (GE) task, and was hosted
again in BioNLP-ST 2011, which also hosted four
other IE tasks and three supporting tasks (Kim et al,
2011). As the sole task that was repeated in the two
events, the GE task was referenced during the devel-
opment of other tasks, and took the role of connect-
ing the results of the 2009 event to the main tasks of
2011. The GE task in 2011 received final submis-
sions from 15 teams. The results show the commu-
nity made a significant progress with the task, and
also show the technology can be generalized to full
papers at moderate cost of performance.
This paper presents the task setup, preparation,
and discusses the results.
7
Event Type Primary Argument Secondary Argument
Gene expression Theme(Protein)
Transcription Theme(Protein)
Protein catabolism Theme(Protein)
Phosphorylation Theme(Protein) Site(Entity)
Localization Theme(Protein) AtLoc(Entity), ToLoc(Entity)
Binding Theme(Protein)+ Site(Entity)+
Regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)
Positive regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)
Negative regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)
Table 1: Event types and their arguments for Genia event task. The type of each filler entity is specified in parenthesis.
Arguments that may be filled more than once per event are marked with ?+?.
2 Task Definition
The GE task follows the task definition of BioNLP-
ST 2009, which is briefly described in this section.
For more detail, please refer to (Kim et al, 2009).
Table 1 shows the event types to be addressed in
the task. For each event type, the primary and sec-
ondary arguments to be extracted with an event are
defined. For example, a Phosphorylation event is
primarily extracted with the protein to be phospho-
rylated. As secondary information, the specific site
to be phosphorylated may be extracted.
From a computational point of view, the event
types represent different levels of complexity. When
only primary arguments are considered, the first five
event types in Table 1 are classified as simple event
types, requiring only unary arguments. The Bind-
ing and Regulation types are more complex: Bind-
ing requires detection of an arbitrary number of ar-
guments, and Regulation requires detection of recur-
sive event structure.
Based on the definition of event types, the entire
task is divided to three sub-tasks addressing event
extraction at different levels of specificity:
Task 1. Core event extraction addresses the ex-
traction of typed events together with their pri-
mary arguments.
Task 2. Event enrichment addresses the extrac-
tion of secondary arguments that further spec-
ify the events extracted in Task 1.
Task 3. Negation/Speculation detection
addresses the detection of negations and
speculations over the extracted events.
Task 1 serves as the backbone of the GE task and is
mandatory for all participants, while the other two
are optional.
The failure of p65 translocation to the nucleus ?
Protein Localization Location
theme ToLoc
Negated
Figure 1: Event annotation example
Figure 1 shows an example of event annotation.
The event encoded in the text is represented in a
standoff-style annotation as follows:
T1 Protein 15 18
T2 Localization 19 32
T3 Entity 40 46
E1 Localization:T2 Theme:T1 ToLoc:T1
M1 Negation E1
The annotation T1 identifies the entity referred
to by the string (p65) between the character offsets,
15 and 18 to be a Protein. T2 identifies the string,
translocation, to refer to a Localization event. Enti-
ties other than proteins or event type references are
classified into a default class Entity, as in T3. E1
then represents the event defined by the three enti-
ties, as defined in Table 1. Note that for Task 1, the
entity, T3, does not need to be identified, and the
event, E1, may be identified without specification of
the secondary argument, ToLoc:T1:
E1? Localization:T2 Theme:T1
Finding the full representation of E1 is the goal of
Task 2. In the example, the localization event, E1,
is negated as expressed in the failure of . Finding the
negation, M1 is the goal of Task 3.
8
Training Devel Test
Item Abs. Full Abs. Full Abs. Full
Articles 800 5 150 5 260 4
Words 176146 29583 33827 30305 57256 21791
Proteins 9300 2325 2080 2610 3589 1712
Events 8615 1695 1795 1455 3193 1294
Gene expression 1738 527 356 393 722 280
Transcription 576 91 82 76 137 37
Protein catabolism 110 0 21 2 14 1
Phosphorylation 169 23 47 64 139 50
Localization 265 16 53 14 174 17
Binding 887 101 249 126 349 153
Regulation 961 152 173 123 292 96
Positive regulation 2847 538 618 382 987 466
Negative regulation 1062 247 196 275 379 194
Table 2: Statistics of annotations in training, development, and test sets
3 Data preparation
The data sets are prepared in two collections: the
abstract and the full text collections. The abstract
collection includes the same data used for BioNLP-
ST 2009, and is meant to be used to measure the
progress of the community. The full text collection
includes full papers which are newly annotated, and
is meant to be used to measure the generalization
of the technology to full papers. Table 2 shows the
statistics of the annotations in the GE task data sets.
Since the training data from the full text collection is
relatively small despite of the expected rich variety
of expressions in full text, it is expected that ?gener-
alization? of a model from the abstract collection to
full papers would be a key technique to get a reason-
able performance.
A full paper consists of several sections includ-
ing the title, abstract, introduction, results, conclu-
sion, methods, and so on. Different sections would
be written with different purposes, which may af-
fect the type of information that are found in the sec-
tions. Table 3 shows the distribution of annotations
in different sections. It indicates that event men-
tions, according to the event definition in Table 1, in
Methods and Captions are much less frequent than
in the other TIAB, Intro. and R/D/C sections. Fig-
ure 2 illustrates the different distribution of anno-
tated event types in the five sections. It is notable
that the Methods section (depicted in blue) shows
very different distribution compared to others: while
Gene_expression
Transcrip.
Binding
Regulation
Pos_regul.
Neg_regul.
TIAB Intro. R/D/C Methods Caption
Figure 2: Event distribution in different sections
Regulation and Positive regulation events are not as
frequent as in other sections, Negative regulation is
relatively much more frequent. It may agree with
an intuition that experimental devices, which will be
explained in Methods sections, often consists of ar-
tificial processes that are designed to cause a nega-
tive regulatory effect, e.g. mutation, addition of in-
hibitor proteins, etc. This observation suggests a dif-
ferent event annotation scheme, or a different event
extraction strategy would be required for Methods
sections.
9
Full Paper
Item Abstract Whole TIAB Intro. R/D/C Methods Caption
Words 267229 80962 3538 7878 43420 19406 6720
Proteins 14969 6580 336 597 3980 916 751
(Density: P / W) (5.60%) (8.13%) (9.50%) (7.58%) (9.17%) (4.72%) (11.18%)
Events 13603 4436 272 427 3234 198 278
(Density: E / W) (5.09%) (5.48%) (7.69%) (5.42%) (7.51%) (1.02%) (4.14%)
(Density: E / P) (90.87%) (67.42%) (80.95%) (71.52%) (81.93%) (21.62%) (37.02%)
Gene expression 2816 1193 62 98 841 80 112
Transcription 795 204 7 7 140 30 20
Protein catabolism 145 3 0 0 3 0 0
Phosphorylation 355 137 12 12 101 10 2
Localization 492 47 3 15 22 7 0
Binding 1485 380 16 74 266 6 18
Regulation 1426 371 35 30 281 4 21
Positive regulation 4452 1385 98 131 1087 15 54
Negative regulation 1637 716 39 60 520 46 51
Table 3: Statistics of annotations in different sections of text: the Abstract column is of the abstraction collection
(1210 titles and abstracts), and the following columns are of full paper collection (14 full papers). TIAB = title and
abstract, Intro. = introduction and background, R/D/C = results, discussions, and conclusions, Methods = methods,
materials, and experimental procedures. Some minor sections, supporting information, supplementary material, and
synopsis, are ignored. Density = relative density of annotation (P/W = Protein/Word, E/W = Event/Word, and E/P =
Event/Protein).
4 Participation
In total, 15 teams submitted final results. All 15
teams participated in the mandatory Task 1, four
teams in Task 2, and two teams in Task 3. Only one
team, UTurku, completed all the three tasks.
Table 4 shows the profile of the teams, except-
ing three who chose to remain anonymous. A brief
examination on the team organization (the People
column) suggests the importance of a computer sci-
ence background, C and BI, to perform the GE task,
which agrees with the same observation made in
2009. It is interpreted as follows: the role of com-
puter scientists may be emphasized in part due to
the fact that the task requires complex computational
modeling, demanding particular efforts in frame-
work design and implementation and computational
resources. The ?09 column suggests that previous
experience in the task may have affected to the per-
formance of the teams, especially in a complex task
like the GE task.
Table 5 shows the profile of the systems. A
notable observation is that four teams developed
their systems based on the model of UTurku09
(Bjo?rne et al, 2009) which was the winning sys-
tem of BioNLP-ST 2009. It may show an influence
of the BioNLP-ST series in the task. For syntac-
tic analyses, the prevailing use of Charniak John-
son re-ranking parser (Charniak and Johnson, 2005)
using the self-trained biomedical model from Mc-
Closky (2008) (McCCJ) which is converted to Stan-
ford Dependency (de Marneffe et al, 2006) is no-
table, which may also be an influence from the re-
sults of BioNLP-ST 2009. The last two teams,
XABioNLP and HCMUS, who did not use syntactic
analyses could not get a performance comparable to
the others, which may suggest the importance of us-
ing syntactic analyses for a complex IE task like GE
task.
5 Results
5.1 Task 1
Table 6 shows the final evaluation results of Task 1.
For reference, the reported performance of the two
systems, UTurku09 and Miwa10 is listed in the
top. UTurku09 was the winning system of Task 1
in 2009 (Bjo?rne et al, 2009), and Miwa10 was
the best system reported after BioNLP-ST 2009
(Miwa et al, 2010b). Particularly, the latter made
10
Team ?09 Task People reference
FAUST
?
12- 3C (Riedel et al, 2011)
UMASS
?
12- 1C (Riedel and McCallum, 2011)
UTurku
?
123 1BI (Bjrne and Salakoski, 2011)
MSR-NLP 1-- 4C (Quirk et al, 2011)
ConcordU
?
1-3 2C (Kilicoglu and Bergler, 2011)
UWMadison
?
1-- 2C (Vlachos and Craven, 2011)
Stanford 1-- 3C+1.5L (McClosky et al, 2011)
BMI@ASU
?
12- 3C (Emadzadeh et al, 2011)
CCP-BTMG
?
1-- 3BI (Liu et al, 2011)
TM-SCS 1-- 1C (Bui and Sloot, 2011)
XABioNLP 1-- 4C (Casillas et al, 2011)
HCMUS 1-- 6L (Minh et al, 2011)
Table 4: Team profiles: The ?09 column indicates whether at least one team member participated in BioNLP-ST 2009.
In People column, C=Computer Scientist, BI=Bioinformatician, B=Biologist, L=Linguist
NLP Task Other resources
Team Lexical Proc. Syntactic Proc. Trig. Arg. group Dictionary Other
FAUST SnowBall, CNLP McCCJ+SD Stacking (UMASS + Stanford)
UMASS SnowBall, CNLP McCCJ+SD Joint infer., Dual Decomposition
UTurku Porter McCCJ+SD SVM SVM SVM S. cues
MSR-NLP Porter McCCJ+SD, Enju SVM MaxEnt rules Coref(Hobbs)
ConcordU - McCCJ+SD dic rules rules S./N. cues
UWMadison Morpha, Porter MCCCJ+SD Joint infer., SEARN
Stanford Morpha, CNLP McCCJ+SD MaxEnt MSTParser word clusters
BMI@ASU Porter, WordNet Stanford+SD SVM SVM - MeSH
CCP-BTMG Porter, WordNet Stanford+SD Subgraph Isomorphism
TM-SCS Stanford Stanford dic rules rules
XABioNLP KAF - rules
HCMUS OpenNLP - dic, rules rules UIMA
Table 5: System profiles: SnowBall=SnowBall Stemmer, CNLP=Stanford CoreNLP (tokenization), KAF=Kyoto An-
notation Format McCCJ=McClosky-Charniak-Johnson Parser, Stanford=Stanford Parser, SD=Stanford Dependency
Conversion, S.=Speculation, N.=Negation
an impressive improvement with Binding events
(44.41%?52.62%).
The best performance in Task 1 this time is
achieved by the FAUST system, which adopts a
combination model of UMass and Stanford. Its
performance on the abstract collection, 56.04%,
demonstrates a significant improvement of the com-
munity in the repeated GE task, when compared to
both UTurku09, 51.95% and Miwa10, 53.29%.
The biggest improvement is made to the Regulation
events (40.11%?46.97%) which requires a com-
plex modeling for recursive event structure - an
event may become an argument of another event.
The second ranked system, UMass, shows the best
performance on the full paper collection. It suggests
that what FAUST obtained from the model combi-
nation might be a better optimization to abstracts.
The ConcordU system is notable as it is the sole
rule-based system that is ranked above the average.
It shows a performance optimized for precision with
relatively low recall. The same tendency is roughly
replicated by other rule-based systems, CCP-BTMG,
TM-SCS, XABioNLP, and HCMUS. It suggests that
a rule-based system might not be a good choice if a
high coverage is desired. However, the performance
of ConcordU for simple events suggests that a high
precision can be achieved by a rule based system
with a modest loss of recall. It might be more true
when the task is less complex.
This time, three teams achieved better results than
Miwa10, which indicates some role of focused ef-
forts like BioNLP-ST. The comparison between the
11
performance on abstract and full paper collections
shows that generalization to full papers is feasible
with very modest loss in performance.
5.2 Task 2
Tables 7 shows final evaluation results of Task 2.
For reference, the reported performance of the task-
winning system in 2009, UT+DBCLS09 (Riedel et
al., 2009), is shown in the top. The first and second
ranked system, FAUST and UMass, which share a
same author with Riedel09, made a significant
improvement over Riedel09 in the abstract col-
lection. UTurku achieved the best performance in
finding sites arguments but did not produce location
arguments. In table 7, the performance of all the
systems in full text collection suggests that finding
secondary arguments in full text is much more chal-
lenging.
In detail, a significant improvement was made for
Location arguments (36.59%?50.00%). A further
breakdown of the results of site extraction, shown
in table 8, shows that finding site arguments for
Phosphorylation, Binding and Regulation events are
all significantly improved, but in different ways.
The extraction of protein sites to be phosphory-
lated is approaching a practical level of performance
(84.21%), while protein sites to be bound or to be
regulated remains challenging to be extracted.
5.3 Task 3
Table 9 shows final evaluation results of Task 3.
For reference, the reported performance of the task-
winning system in 2009, Kilicoglu09(Kilicoglu
and Bergler, 2009), is shown in the top. Among the
two teams participated in the task, UTurku showed
a better performance in extracting negated events,
while ConcordU showed a better performance in
extracting speculated events.
6 Conclusions
The Genia event task which was repeated for
BioNLP-ST 2009 and 2011 took a role of measur-
ing the progress of the community and generaliza-
tion IE technology to full papers. The results from
15 teams who made their final submissions to the
task show that a clear advance of the community in
terms of the performance on a focused domain and
also generalization to full papers. To our disappoint-
ment, however, an effective use of supporting task
results was not observed, which thus remains as fu-
ture work for further improvement.
Acknowledgments
This work is supported by the ?Integrated Database
Project? funded by the Ministry of Education, Cul-
ture, Sports, Science and Technology of Japan.
References
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 10?
18, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Complex event extraction
at PubMed scale. Bioinformatics, 26(12):i382?390.
Jari Bjrne and Tapio Salakoski. 2011. Generaliz-
ing Biomedical Event Extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Quoc-Chinh Bui and Peter. M.A. Sloot. 2011. Extracting
biological events from text using simple syntactic pat-
terns. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Arantza Casillas, Arantza Daz de Ilarraza, Koldo Go-
jenola, Maite Oronoz, and German Rigau. 2011. Us-
ing Kybots for Extracting Events in Biomedical Texts.
In Proceedings of the BioNLP 2011 Workshop Com-
panion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double Layered Learning for Bi-
ological Event Extraction from Text. In Proceedings
12
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2009. Syntactic de-
pendency based heuristics for biological event extrac-
tion. In Proceedings of the BioNLP 2009 Workshop
Companion Volume for Shared Task, pages 119?127,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
General Semantic Interpretation Approach to Biolog-
ical Event Extraction. In Proceedings of the BioNLP
2011 Workshop Companion Volume for Shared Task,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011. Overview of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching
approach for information extraction from biomedical
text. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (ACL-HLT?08), pages 101?104.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event Extraction as Dependency Parsing
for BioNLP 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Quang Le Minh, Son Nguyen Truong, and Quoc Ho Bao.
2011. A pattern approach for Biomedical Event Anno-
tation . In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In Proceedings of
BioNLP?10, pages 37?45.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010b. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146, February.
Hoifung Poon and Lucy Vanderwende. 2010. Joint infer-
ence for knowledge extraction from biomedical litera-
ture. In Proceedings of NAACL-HLT?10, pages 813?
821.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwend. 2011. MSR-NLP Entry in
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel and Andrew McCallum. 2011. Robust
Biomedical Event Extraction with Dual Decomposi-
tion and Minimal Domain Adaptation. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
Shared Task, pages 41?49, Boulder, Colorado, June.
Association for Computational Linguistics.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher Manning. 2011.
Model Combination for Event Extraction in BioNLP
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Andreas Vlachos and Mark Craven. 2011. Biomedical
Event Extraction from Abstracts and Full Papers using
Search-based Structured Prediction. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Andreas Vlachos. 2010. Two strong baselines for the
bionlp 2009 event extraction task. In Proceedings of
BioNLP?10, pages 1?9.
13
Team Simple Event Binding Regulation All
UTurku09 A 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95
Miwa10 A 70.44 52.62 40.60 48.62 / 58.96 / 53.29
W 68.47 / 80.25 / 73.90 44.20 / 53.71 / 48.49 38.02 / 54.94 / 44.94 49.41 / 64.75 / 56.04
FAUST A 66.16 / 81.04 / 72.85 45.53 / 58.09 / 51.05 39.38 / 58.18 / 46.97 50.00 / 67.53 / 57.46
F 75.58 / 78.23 / 76.88 40.97 / 44.70 / 42.75 34.99 / 48.24 / 40.56 47.92 / 58.47 / 52.67
W 67.01 / 81.40 / 73.50 42.97 / 56.42 / 48.79 37.52 / 52.67 / 43.82 48.49 / 64.08 / 55.20
UMass A 64.21 / 80.74 / 71.54 43.52 / 60.89 / 50.76 38.78 / 55.07 / 45.51 48.74 / 65.94 / 56.05
F 75.58 / 83.14 / 79.18 41.67 / 47.62 / 44.44 34.72 / 47.51 / 40.12 47.84 / 59.76 / 53.14
W 68.22 / 76.47 / 72.11 42.97 / 43.60 / 43.28 38.72 / 47.64 / 42.72 49.56 / 57.65 / 53.30
UTurku A 64.97 / 76.72 / 70.36 45.24 / 50.00 / 47.50 40.41 / 49.01 / 44.30 50.06 / 59.48 / 54.37
F 78.18 / 75.82 / 76.98 37.50 / 31.76 / 34.39 34.99 / 44.46 / 39.16 48.31 / 53.38 / 50.72
W 68.99 / 74.30 / 71.54 42.36 / 40.47 / 41.39 36.64 / 44.08 / 40.02 48.64 / 54.71 / 51.50
MSR-NLP A 65.99 / 74.71 / 70.08 43.23 / 44.51 / 43.86 37.14 / 45.38 / 40.85 48.52 / 56.47 / 52.20
F 78.18 / 73.24 / 75.63 40.28 / 32.77 / 36.14 35.52 / 41.34 / 38.21 48.94 / 50.77 / 49.84
W 59.99 / 85.53 / 70.52 29.33 / 49.66 / 36.88 35.72 / 45.85 / 40.16 43.55 / 59.58 / 50.32
ConcordU A 56.51 / 84.56 / 67.75 29.97 / 49.76 / 37.41 36.24 / 47.09 / 40.96 43.09 / 60.37 / 50.28
F 70.65 / 88.03 / 78.39 27.78 / 49.38 / 35.56 34.58 / 43.22 / 38.42 44.71 / 57.75 / 50.40
W 59.67 / 80.95 / 68.70 29.33 / 49.66 / 36.88 34.10 / 49.46 / 40.37 42.56 / 61.21 / 50.21
UWMadison A 54.99 / 79.85 / 65.13 34.87 / 56.81 / 43.21 34.54 / 50.67 / 41.08 42.17 / 62.30 / 50.30
F 74.03 / 83.58 / 78.51 15.97 / 29.87 / 20.81 33.11 / 46.87 / 38.81 43.53 / 58.73 / 50.00
W 65.79 / 76.83 / 70.88 39.92 / 49.87 / 44.34 27.55 / 48.75 / 35.21 42.36 / 61.08 / 50.03
Stanford A 62.61 / 77.57 / 69.29 42.36 / 54.24 / 47.57 28.25 / 49.95 / 36.09 42.55 / 62.69 / 50.69
F 75.58 / 75.00 / 75.29 34.03 / 40.16 / 36.84 26.01 / 46.08 / 33.25 41.88 / 57.36 / 48.41
W 62.09 / 76.55 / 68.57 27.90 / 44.92 / 34.42 22.30 / 40.26 / 28.70 36.91 / 56.63 / 44.69
BMI@ASU A 58.71 / 78.51 / 67.18 26.22 / 47.40 / 33.77 22.99 / 40.47 / 29.32 36.61 / 57.82 / 44.83
F 72.47 / 72.09 / 72.28 31.94 / 40.71 / 35.80 20.78 / 39.74 / 27.29 37.65 / 53.93 / 44.34
W 53.61 / 75.13 / 62.57 22.61 / 49.12 / 30.96 19.01 / 43.80 / 26.51 31.57 / 58.99 / 41.13
CCP-BTMG A 50.93 / 74.50 / 60.50 25.65 / 53.29 / 34.63 19.54 / 43.47 / 26.96 31.87 / 59.02 / 41.39
F 61.82 / 76.77 / 68.49 15.28 / 37.29 / 21.67 17.83 / 44.63 / 25.48 30.82 / 58.92 / 40.47
W 57.33 / 71.34 / 63.57 34.01 / 44.77 / 38.66 16.39 / 25.37 / 19.91 32.73 / 45.84 / 38.19
TM-SCS A 53.65 / 71.66 / 61.36 36.02 / 49.41 / 41.67 18.29 / 27.07 / 21.83 33.36 / 47.09 / 39.06
F 68.57 / 70.59 / 69.57 29.17 / 35.00 / 31.82 12.20 / 21.02 / 15.44 31.14 / 42.83 / 36.06
W 43.71 / 47.18 / 45.38 05.30 / 50.00 / 09.58 05.79 / 26.94 / 09.54 19.07 / 42.08 / 26.25
XABioNLP A 39.76 / 45.90 / 42.61 06.34 / 56.41 / 11.40 04.72 / 23.21 / 07.84 17.91 / 40.74 / 24.89
F 55.84 / 50.23 / 52.89 02.78 / 30.77 / 05.10 08.18 / 33.89 / 13.17 21.96 / 45.09 / 29.54
W 24.82 / 35.14 / 29.09 04.68 / 12.92 / 06.88 01.63 / 10.40 / 02.81 10.12 / 27.17 / 14.75
HCMUS A 22.42 / 37.38 / 28.03 04.61 / 10.46 / 06.40 01.69 / 10.37 / 02.91 09.71 / 27.30 / 14.33
F 32.21 / 31.16 / 31.67 04.86 / 28.00 / 08.28 01.47 / 10.48 / 02.59 11.14 / 26.89 / 15.75
Table 6: Evaluation results (recall / precision / f-score) of Task 1 in (W)hole data set, (A)bstracts only, and (F)ull
papers only. Some notable figures are emphasized in bold.
14
Team Sites (222) Locations (66) All (288)
UT+DBCLS09 A 23.08 / 88.24 / 36.59 32.14 / 72.41 / 44.52
W 32.88 / 70.87 / 44.92 36.36 / 75.00 / 48.98 33.68 / 71.85 / 45.86
FAUST A 43.51 / 71.25 / 54.03 36.92 / 77.42 / 50.00 41.33 / 72.97 / 52.77
F 17.58 / 69.57 / 28.07 - 17.39 / 66.67 / 27.59
W 31.98 / 71.00 / 44.10 36.36 / 77.42 / 49.48 32.99 / 72.52 / 45.35
UMass A 42.75 / 70.00 / 53.08 36.92 / 77.42 / 50.00 40.82 / 72.07 / 52.12
F 16.48 / 75.00 / 27.03 - 16.30 / 75.00 / 26.79
W 32.88 / 62.93 / 43.20 22.73 / 83.33 / 35.71 30.56 / 65.67 / 41.71
BMI@ASU A 37.40 / 67.12 / 48.04 23.08 / 83.33 / 36.14 32.65 / 70.33 / 44.60
F 26.37 / 55.81 / 35.82 - 26.09 / 55.81 / 35.56
W 40.09 / 65.44 / 49.72 00.00 / 00.00 / 00.00 30.90 / 65.44 / 41.98
UTurku A 48.09 / 69.23 / 56.76 00.00 / 00.00 / 00.00 32.14 / 69.23 / 43.90
F 28.57 / 57.78 / 38.24 - 28.26 / 57.78 / 37.96
Table 7: Evaluation results of Task 2 in (W)hole data set, (A)bstracts only, and (F)ull papers only
Team Phospho. (67) Binding (84) Reg. (71)
Riedel?09 A 71.43 / 71.43 / 71.43 04.76 / 50.00 / 08.70 12.96 / 58.33 / 21.21
W 71.64 / 84.21 / 77.42 05.95 / 38.46 / 10.31 28.17 / 60.61 / 38.46
FAUST A 71.43 / 81.63 / 76.19 04.76 / 14.29 / 07.14 29.63 / 66.67 / 41.03
F 72.73 / 100.0 / 84.21 06.35 / 66.67 / 11.59 23.53 / 44.44 / 30.77
W 76.12 / 79.69 / 77.86 04.76 / 36.36 / 08.42 22.54 / 64.00 / 33.33
UMass A 76.79 / 76.79 / 76.79 04.76 / 14.29 / 07.14 22.22 / 70.59 / 33.80
F 72.73 / 100.0 / 84.21 04.76 / 75.00 / 08.96 23.53 / 50.00 / 32.00
W 52.24 / 97.22 / 67.96 20.24 / 53.12 / 29.31 29.58 / 43.75 / 35.29
BMI@ASU A 53.57 / 96.77 / 68.97 09.52 / 22.22 / 13.33 31.48 / 51.52 / 39.08
F 45.45 / 100.0 / 62.50 23.81 / 65.22 / 34.88 23.53 / 26.67 / 25.00
W 76.12 / 91.07 / 82.93 21.43 / 51.43 / 30.25 28.17 / 44.44 / 34.48
UTurku A 78.57 / 89.80 / 83.81 09.52 / 18.18 / 12.50 31.48 / 54.84 / 40.00
F 63.64 / 100.0 / 77.78 25.40 / 66.67 / 36.78 17.65 / 21.43 / 19.35
Table 8: Evaluation results of Site information for different event types in (A)bstracts
Team Negation Speculation All
Kilicoglu09 A 14.98 / 50.75 / 23.13 16.83 / 50.72 / 25.27 15.86 / 50.74 / 24.17
W 22.87 / 48.85 / 31.15 17.86 / 32.54 / 23.06 20.30 / 39.67 / 26.86
UTurku A 22.03 / 49.02 / 30.40 19.23 / 38.46 / 25.64 20.69 / 43.69 / 28.08
F 25.76 / 48.28 / 33.59 15.00 / 23.08 / 18.18 19.28 / 30.85 / 23.73
W 18.77 / 44.26 / 26.36 21.10 / 38.46 / 27.25 19.97 / 40.89 / 26.83
ConcordU A 18.06 / 46.59 / 26.03 23.08 / 40.00 / 29.27 20.46 / 42.79 / 27.68
F 21.21 / 38.24 / 27.29 17.00 / 34.69 / 22.82 18.67 / 36.14 / 24.63
Table 9: Evaluation results of Task 3 in (W)hole data set, (A)bstracts only, and (F)ull papers only
15
Proceedings of BioNLP Shared Task 2011 Workshop, pages 74?82,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of the Protein Coreference task in BioNLP Shared Task 2011
Ngan Nguyen
University of Tokyo
Hongo 7-3-1, Bunkyoku, Tokyo
nltngan@is.s.u-tokyo.ac.jp
Jin-Dong Kim
Database Center for Life Science
Yayoi 2-11-16, Bunkyo-ku, Tokyo
jdkim@dbcls.rois.ac.jp
Jun?ichi Tsujii
Microsoft Research Asia
5 Dan Ling Street, Haiian District, Beijing
jtsujii@microsoft.com
Abstract
This paper summarizes the Protein Coref-
erence Resolution task of BioNLP Shared
Task 2011. After 7 weeks of system devel-
opment period, the task received final sub-
missions from 6 teams. Evaluation results
show that state-of-the-art performance on the
task can find 22.18% of protein coreferences
with the precision of 73.26%. Analysis of
the submissions shows that several types of
anaphoric expressions including definite ex-
pressions, which occupies a significant part of
the problem, have not yet been solved.
1 Introduction
While named entity recognition (NER) and relation
or event extraction are regarded as standard tasks
of information extraction (IE), coreference resolu-
tion (Ng, 2010; Bejan and Harabagiu, 2010) is more
and more recognized as an important component of
IE for a higher performance. Without coreference
resolution, the performance of IE is often substan-
tially limited due to an abundance of coreference
structures in natural language text, i.e. information
pieces written in text with involvement of a corefer-
ence structure are hard to be captured (Miwa et al,
2010). There have been several attempts for coref-
erence resolution, particularly for newswire texts
(Strassel et al, 2008; Chinchor, 1998). It is also one
of the lessons from BioNLP Shared Task (BioNLP-
ST, hereafter) 2009 that coreference structures in
biomedical text substantially hinder the progress of
fine-grained IE (Kim et al, 2009).
To address the problem of coreference resolution
in molecular biology literature, the Protein Corefer-
ence (COREF) task is arranged in BioNLP-ST 2011
as a supporting task. While the task itself is not
an IE task, it is expected to be a useful compo-
nent in performing the main IE tasks more effec-
tively. To establish a stable evaluation and to observe
the effect of the results of the task to the main IE
tasks, the COREF task particularly focuses on find-
ing anaphoric protein references.
The benchmark data sets for developing and test-
ing coreference resolution system were developed
based on various manual annotations made to the
Genia corpus (Ohta et al, 2002). After 7 weeks of
system development phase, for which training and
development data sets with coreference annotation
were given, six teams submitted their prediction of
coreferences for the test data. The best system ac-
cording to our primary evaluation criteria is evalu-
ated to find 22.18% of anaphoric protein references
at the precision of 73.26%.
This paper presents overall explanation of the
COREF task, which includes task definition (Sec-
tion 2), data preparation (Section 4), evaluation
methods (Section 5), results (Section 7), and thor-
ough analyses (Section 8) to figure out what are
remaining problems for coreference resolution in
biomedical text.
2 Problem Definition
This section provides an explanation of the corefer-
ence resolution task in our focus, through examples.
Figure 1 shows an example text segmented into
four sentences, S2 - S5, where anaphoric corefer-
ences are illustrated with colored extends and ar-
rows. In the figure, protein names are highlighted in
purple, T4 - T10, and anaphoric protein references,
e.g. pronouns and definite noun phrases, are high-
lighted in red, T27, T29, T30, T32, of which the an-
74
Figure 1: Protein coreference annotation
tecedents are indicated by arrows if found in the text.
In the example, the definite noun phrase (NP), this
transcription factor (T32), is a coreference to p65
(T10). Without knowing the coreference structure,
it becomes hard to capture the information written
in the phrase, nuclear exclusion of this transcription
factor, which is localization of p65 (out of nucleus)
according to the framework of BioNLP-ST.
A standard approach would include a step to find
candidate anaphoric expressions that may refer to
proteins. In this task, pronouns, e.g. it or they, and
definite NPs that may refer to proteins, e.g. the tran-
scription factor or the inhibitor are regarded as can-
didates of anaphoric protein references. This step
corresponds to markable detection and anaphoric-
ity determination steps in the jargon of MUC. The
next step would be to find the antecedents of the
anaphoric expressions. This step corresponds to
anaphora resolution in the jargon of MUC.
3 Task Setting
In the task, the training, development and test data
sets are provided in three types of files: the text, the
protein annotation, and the coreference annotation
files. The text files contain plain texts which are tar-
get of annotation. The protein annotation files pro-
vide gold annotation for protein names in the texts,
and the coreference annotation files provide gold an-
notation for anaphoric references to those protein
names. The protein annotation files are given to the
participants, together with all the training, develop-
ment and test data sets. The coreference annotation
files are not given with the test data set, and the task
for the participants is to produce them automatically.
In protein annotation files, annotations for protein
names are given in a stand-off style encoding. For
example, those highlighted in purple in Figure 1 are
protein names, which are given in protein annotation
files as follows:
T4 Protein 275 278 p65
T5 Protein 294 297 p50
T6 Protein 367 372 v-rel
T7 Protein 406 409 p65
T8 Protein 597 600 p50
T9 Protein 843 848 MAD-3
T10 Protein 879 882 p65
The first line indicates there is a protein reference
in the span that begins at 275th character and ends
before 278th character, of which the text is ?p65?,
and the annotation is identified by the id, ?T4?
The coreference annotation files include three sort
of annotations. First, annotations for anaphoric pro-
tein references are given. For example, those in red
in Figure 1 are anaphoric protein references:
T27 Exp 179 222 the N.. 215 222 complex
T29 Exp 307 312 which
T30 Exp 459 471 this .. 464 471 complex
T32 Exp 1022 1047 this .. 1027 1047 tra..
The first line indicates that there is an anaphoric
protein reference in the specified span, of which the
text is ?the NF-kappa B transcription factor com-
plex? (truncated due to limit of space), and that its
minimal expression is ?complex?. Second, noun
phrases that are antecedents of the anaphoric refer-
ences are also given in the coreference annotation
file. For example, T28 and T31 (highlighted in blue)
are antecedents of T29 and T32, respectively, and
thus given in the file:
T28 Exp 264 297 NF-ka..
T31 Exp 868 882 NF-ka..
Third, the coreference relation between the
anaphoric expressions and their antecedents are
given in predicate-argument expressions1:
R1 Coref Ana:T29 Ant:T28 [T5, T4]
R2 Coref Ana:T30 Ant:T27
R3 Coref Ana:T32 Ant:T31 [T10]
The first line indicates there is a coreference rela-
tion, R1, of which the anaphor is T29 and the an-
tecedent is T28, and the relation involves two protein
names, T5 and T4.
Note that, sometimes, an anaphoric expression,
e.g. which (T29), is connected to more than one
protein names, e.g. p65 (T4) and p50 (T5). Some-
times, coreference structures do not involve any spe-
cific protein names, e.g. T30 and T27. In order
1Due to limitation of space, argument names are abbrevi-
ated, e.g. ?Ana? for ?Anaphora?, and ?Ant? for ?Antecedent?
75
to establish a stable evaluation, our primary evalu-
ation will focus only on coreference structures that
involve specific protein names, e.g. T29 and T28,
and T32 and T31. Among the three, only two, R1
and R3, involves specific protein references, T4 and
T5, and T10. Thus, finding of R2 will be ignored
in the primary evaluation. However, those not in-
volving specific protein references are also provided
in the training data to help system development,
and will be considered in the secondary evaluation
mode. See section 5 for more detail.
4 Data Preparation
The data sets for the COREF task are produced
based on three resources: MedCO coreference an-
notation (Su et al, 2008), Genia event annotation
(Kim et al, 2008), and Genia Treebank (Tateisi et
al., 2005). Although the three have been developed
independently from each other, they are annotations
made to the same corpus, the Genia corpus (Kim et
al., 2008). Since COREF was focused on finding
anaphoric references to proteins (or genes), only rel-
evant annotations were extracted from the MedCO
corpus though the following process:
1. From MedCo annotation, coreference entities that
were pronouns and definite base NPs were ex-
tracted, which became candidate anaphoric expres-
sions. The base NPs were determined by consulting
Genia Tree Bank.
2. Among the candidate anaphoric expressions, those
that could not be protein references were filtered
out. This process was done by checking the head
noun of NPs. For example, definite NPs with ?cell?
as their head noun were filtered out. The remaining
ones became candidate protein coreferences.
3. The candidate protein coreferences and their an-
tecedents according to MedCo annotation were in-
cluded in the data files for COREF task.
4. The protein name annotations from Genia event
annotation were added to the data files to deter-
mine which coreference expressions involve protein
name references.
Table 1 summarizes the coreference entities in the
training, development, and test sets for COREF task.
In the table, the anaphoric entities are classified into
four types as follows:
RELAT indicates relative pronouns or relative adjec-
tives, e.g. that, which, or whose.
PRON indicates pronouns, e.g. it.
Type Train Dev Test
RELAT 1193 254 349
PRON 738 149 269
Anaphora DNP 296 58 91
APPOS 9 1 3
N/C 11 1 2
Antecedent 2116 451 674
TOTAL 4363 914 1388
Table 1: Statistics of coreference entities in COREF data
sets: N/C = not-classified.
DNP indicates definite NPs or demonstrative NPs, e.g.
NPs that begin with the, this, etc.
APPOS indicates coreferences in apposition.
5 Evaluation
The coreference resolution performance is evaluated
in two modes.
The Surface coreference mode evaluates the per-
formance of finding anaphoric protein references
and their antecedents, regardless whether the an-
tecedents actually embed protein names or not. In
other words, it evaluates the ability to predict the
coreference relations as provided in the gold coref-
erence annotation file, which we call surface coref-
erence links.
The protein coreference mode evaluates the per-
formance of finding anaphoric protein references
with their links to actual protein names (protein
coreference links). In the implementation of the
evaluation, the chain of surface coreference linkes
is traced until an antecedent embedding a protein
name is found. If a protein-name-embedding an-
tecedent is connected to an anaphora through only
one surfs link, we call the antecedent a direct pro-
tein antecedent. If a protein-name-embedding an-
teceden is connected to an anaphora through more
than one surface link, we call it an indirect protein
antecedent, and the antecedents in the middle of the
chain intermediate antecedents. The performance
evaluated in this mode may be directly connected
to the potential performance in main IE tasks: the
more the (anaphoric) protein references are found,
the more the protein-related events may be found.
For this reason, the protein coreference mode is cho-
sen as the primary evaluation mode.
Evaluation results for both evaluation modes are
76
given in traditional precision, recall and f-score,
which are similar to (Baldwin, 1997).
5.1 Surface coreference
A response expression is matched with a gold ex-
pression following partial match criterion. In par-
ticular, a response expression is considered cor-
rect when it covers the minimal boundary, and is
included in the maximal boundary of expression.
Maximal boundary is the span of expression anno-
tation, and minimal boundary is the head of ex-
pression, as defined in MUC annotation schemes
(Chinchor, 1998). A response link is correct when
its two argument expressions are correctly matched
with those of a gold link.
5.2 Protein coreference
This is the primary evaluation perspective of the pro-
tein coreference task. In this mode, we ignore coref-
erence links that do not reference to proteins. Inter-
mediate antecedents are also ignored.
Protein coreference links are generated from the
surface coreference links. A protein coreference link
is composed of an anaphoric expression and a pro-
tein reference that appears in its direct or indirect
antecedent. Below is an example.
Example:
R1 Coref Ana:T29 Ant:T28 [T5, T4]
R2 Coref Ana:T30 Ant:T27
R3 Coref Ana:T32 Ant:T31 [T10]
R4 Coref Ana:T33 Ant:T32
In this example, supposing that there are four surface
links in the coreference annotation file (T29,T28),
(T30,T27), (T32,T31), and (T33, T32), in which
T28 contains two protein mentions T5, T4, and T31
contains one protein mention T10; thus, the protein
coreference links generated from these surface links
are (T29,T4), (T29,T5), (T32,T10), and (T33, T10).
Notice that T33 is connected with T10 through the
intermediate expression T32.
Response expressions and generated response re-
sult links are matched with gold expressions and
links correspondingly in a way similar to the surface
coreference evaluation mode.
6 Participation
We received submissions from six teams. Each team
was requested to submit a brief description of their
team, which was summarized in Table 2.
Team Member Approach & Tools
UU 1 NLP ML (Yamcha SVM,
Reconcile)
UZ 5 NLP RB (-)
CU 2 NLP RB (-)
UT 1 biochemist ML (SVM-Light)
US 2 AI ML (SVM-Light)
UC 3 NLP, 1 BioNLP ML (Weka SVM)
Table 2: Participation. UU = UofU, UZ = UZH,
CU=ConcordU, UT = UTurku, UZ = UZH, US =
Uszeged, UC = UCD SCI, RB = Rule-based, ML = Ma-
chine learning-based.
TEAM RESP C P R F
UU 86 63 73.26 22.18 34.05
UZ 110 61 55.45 21.48 30.96
CU 87 55 63.22 19.37 29.65
UT 61 41 67.21 14.44 23.77
US 259 9 3.47 3.17 3.31
UC 794 2 0.25 0.70 0.37
Table 3: Protein coreference results. Total num-
ber of gold link = 284. RESP=response, C=correct,
P=precision, R=recall, F=fscore
The tool column shows the external tools used
in resolution processing. Among these tools,
there is only one team used an external coref-
erence resolution framework, Reconcile, which
achieved the state-of-the-art performance for super-
vised learning-based coreference resolution (Stoy-
anov et al, 2010b).
7 Results
7.1 Protein coreference results
Evaluation results in the protein coreference mode
are shown in Table 3. The UU team got the high-
est f-score 34.05%. The UZ and CU teams are
the second- and third-best teams with 30.96% and
29.65% f-score correspondingly, which are compa-
rable to each other. Unfortunately, two teams, US
and UC could not produce meaningful results, and
the other four teams show performance optimized
for high precision. It was expected that the 22.18%
of protein coreferences may contribute to improve
the performance on main task, which was not ob-
served this time, unfortunately.
The first ranked system by UU utilized Recon-
77
TEAM RESP C P R F
UU 360 43 11.94 20.48 15.09
UZ 736 51 6.93 24.29 10.78
CU 365 36 9.86 17.14 12.52
UT 452 50 11.06 23.81 15.11
US 259 4 1.54 1.90 1.71
UC 797 1 0.13 0.48 0.20
Table 4: Surface coreference results. Total num-
ber of gold link = 210. RESP=response, C=correct,
P=precision, R=recall, F=fscore
UU UT
S-correct & P-missing 8 29
S-missing & P-correct 16 5
Table 5: Count of anaphors that have different status in
different evaluation modes. S = surface coreference eval-
uation mode, P = protein coreference evaluation mode
cile which was originally developed for newswire
domain. It supports the hypothesis that machine
learning-based coreference resolution tool trained
on different domains can be helpful for the bio med-
ical domain; however, it still requires some adapta-
tions.
7.2 Surface coreference results
Table 4 shows the evaluation results in the surface
link mode. The overall performances of all the sys-
tems are low, in which recalls are much higher than
the precisions. One possible reason of the low re-
sults is because most of the teams focus on resolv-
ing pronominal coreference; however, they failed to
solve some difficult types of pronoun such as ?it?,
?its?, ?these?, ?them?, and ?which?, which occupy
the majority of anaphoric pronominal expressions
(Table 1). Definite anaphoric expressions were ig-
nored by almost all of the systems (except one sub-
mission).
The results show that the protein coreference res-
olution is not a trivial task; and many parts remains
challenging. In next section, we analyze about po-
tential reason of the low results, and discuss possible
directions for further improvement.
Ex 1 GOLD
T5 DQalpha and DQbeta trans heterodimeric
HLA-DQ molecules
T6 such trans-dimers
T7 which
R1 T6 T5 [T3, T4]
R2 T7 T6
RESP
T5 such trans-dimers
T6 which
R1 T6 T5
Ex 2 GOLD
T18 Five members of this family
(MYC, SCL, TAL-2, LYL-1 and E2A)
T20 their
R3 T20 T18 [T3, T2, T5, T4]
RESP
T19 Five members
T20 their
R2 T20 T19
Table 6: Example of surface-correct & protein-missing
cases. Protein names are underlined, and the min-values
are in italic.
8 Analysis
8.1 Why the rankings based on the two
evaluation methods are not the same?
Comparing with the protein coreference mode, we
can see the rankings based on two evaluation meth-
ods are different. In order to find out what led to
this interesting difference, we further analyzed the
submissions from the two teams UT and UU. The
UT team achieved the highest f-score in the surface
evaluation mode, but was in the fourth rank in the
protein evaluation mode. Meanwhile, the score of
UU team was slightly less than the UT team in the
former mode, but got the highest in the later (Table
3 and Table 4). In other words, there is no clear cor-
relation between the two evaluation results.
Because the two precisions in surface evaluation
mode are not much different, the recalls were the
main contribution in the difference of f-score. An-
alyzing the correct and missing examples in both
evaluation modes, we found that there are anaphors
whose surface links are correct, while the protein
links with the same anaphors are evaluated as miss-
ing; and vice versa with missing surface links and
correct protein links. Counts of anaphors of each
78
type are shown in Table 5. In this table, the cell
at column UT and row S-correct and P-missing can
be interpreted as following. There are 29 anaphors
in the UT response whose surface links are correct
but protein links are missing, which contributes pos-
itively to the recall in surface coreference mode, and
negatively to that in protein coreference mode.
Table 6 shows two examples of S-correct and
P-missing. In the first example, we can see that
the gold antecedent proteins are contained in an in-
direct antecedent. Therefore, when the interme-
diate antecedent is correctly detected by the sur-
face link R1, but the indirect antecedent is not de-
tected, the anaphor is not linked to it antecedent
proteins ?DQalpha? and ?DQbeta?. Another reason
is because response antecedents do not include an-
tecedent proteins. This is actually the problem of
expression boundary detection. An example of this
is example 2 (Table 6), in which the response sur-
face link R2 is correct, but the protein links to the
four proteins are not detected, because the response
antecedent ?five members? does not include the pro-
tein mentions ?SCL, TAL-2, LYL-1 and E2A?. How-
ever, the response antecedent expression is correct
because it contains the minimal boundary ?mem-
bers?.
For S-missing and P-correct, we found that
anaphors are normally directly linked to antecedent
proteins. In other words, expression boundary is
same as protein boundary. Another case is that re-
sponse antecedents contain the antecedent proteins,
but are evaluated as incorrect because the expres-
sion boundary of the response expression is larger
than the gold expression. An example is shown in
Table 7 where the response expression ?a second
GCR, termed GCRbeta? includes the gold expres-
sion ?GCRbeta?. Therefore, although the surface
link is incorrect because the response expression is
evaluated as incorrect, the protein coreference link
receives a full score .
The difference reflects the characteristics of the
two evaluation methods. The analysis result also
shows the affect of markable detection or expression
detection on the resolution evaluation result.
8.2 Protein coreference analysis
We want to see how well each system performs on
each type of anaphor. However, the type information
Ex 3 GOLD
T17 GCRbeta
T18 which
R2 T18 T17 [T4]
RESP
T16 a second GCR, termed GCRbeta
T19 which
R2 T19 T16
Table 7: Examples of S-missing and P-correct
is not explicitly included in the response, so it has
to be induced automatically. We done this by find-
ing the first word of anaphoric expression; then, we
combine it with 1 if the expression is a single-word
expression, or 2 if the expression is multi-word, to
create a sub type value for each anaphor of both
gold and response anaphors. After that, subtypes are
mapped with the anaphor types specified in Section
4 using the mapping in Table 10.
Protein coreference resolution results by sub type
are given in Table 9 and 8. It can be easily seen in
Table 9 which team performed well on which type
of anaphor. In particular, the CU system was good at
resolving the RELAT, APPOS and other types. The
UU team performed well on the DNP type. And for
the PRON type, UZ was the best team. In theory,
knowing this, we can combine strengths of the teams
to tackle all the types.
We analyzed false positive protein anaphora links
to see what types of anaphora are solved by each
system. The recalls in Table 11 are calculated based
on the anaphor type information manually annotated
in the gold data. Comparing with those in Table 9,
there is a small difference due to the automatic in-
duction of anaphoric types based on sub types. It
can be seen in the table 11 that only 77.5 percent of
RELAT-typed anaphora links were resolved (by CU
team), although this type is supposed to be the eas-
iest type. Examining the output data, we found that
the system tends to choose the nearest expression
as the antecedent of a relative pronoun; however,
this is not always correct, as in the following exam-
ples from the UofU submission: ?We also identified
functional Aiolos-binding sites1a in the Bcl-2 pro-
moter1b, which1 are able to activate the luciferase
reporter gene.?, and ?Furthermore, the analysis of
IkappaBalpha turnover demonstrated an increased
79
PRON P- P- P- P- P- P- DNP D- RELAT R-
both-2 it-1 its-1 one-2 that-1 their-1 these-2 this-2 those-1 which-1 whose-1 N/C
UU 36.4 64.4 2 13.3 18.2 62 5 30.8
UZ 46.2 35.7 53.3 7.1 12.5 5.4 59 66.7 15.4
CU 62 70.9 5 42.1
UT 9.5 36.8 10 34.6 9.5 5 30.8
US 13.9 22.9
UC 28.6 9.1
Table 8: Fine-grained results (f-score, %)
Team PRON P- P- DNP D- D- RELAT R- R- Others O- O-
P R F P R F P R F P R F
UU 79.0 11.5 20.1 66.7 5.9 10.8 71.3 56.0 62.7 100.0 18.3 30.8
UZ 62.9 16.9 26.7 12.5 4.4 6.5 71.4 46.7 56.5 50.0 9.1 15.4
CU 64.6 68.0 66.2 50.0 36.4 42.1
UT 72.7 12.3 21.1 14.3 1.5 2.7 73.3 29.3 41.9 100.0 18.2 30.8
US 27.3 6.9 11.0
UC 9.1 1.5 2.6
Table 9: Protein coreference results by coreference type (fscore, %). P = precision, R = recall, F = f-score. O = Others.
TEAM A R D P O
UU 0.0 62.0 5.7 11.1 0.0
UZ 0.0 49.3 4.3 17.0 0.0
CU 0.0 77.5 0.0 0.0 0.0
UT 0.0 32.4 1.4 11.9 14.3
US 0.0 0.0 0.0 6.7 0.0
UC 0.0 0.0 1.4 0.7 0.0
Table 11: Exact recalls by anaphor type, based on man-
ual type annotation. A=APPOS, R=RELAT, D=DNP,
P=PRON, O=OTHER
degradation of IkappaBalpha2a in HIV-1-infected
cells2b that2 may account for the constitutive DNA
binding activity.?. Expressions with the same index
are coreferential expressions. The a subscript indi-
cates correct antecedent, and b subscript indicates
the wrong one. In these examples, the relative pro-
noun that and which are incorrectly linked with the
nearest expression, which is actually part of post-
modifier or the correct antecedent expression.
For the DNP type, recall of the best system is less
than 6 percent (Table 11), although it is an impor-
tant type which occupies almost one fifth of all pro-
tein links (Table 1). There is only one team, the UC
team, attempted to tackle the anaphor; however, it
resulted in many spurious links. The other teams
did not make any prediction on this type. A possi-
ble reason of this is because there are much more
non-anaphoric definite noun phrases than anaphoric
ones, which making it difficult to train an effective
classier for anaphoricity determination. We have to
seek for a better method for solving the DNP links,
in order to significantly improve protein coreference
resolution system.
Concerning the PRON type, Table 8 shows that
except for that-1, no other figures are higher than
50 percent f-score. This is an interesting obser-
vation because pronominal anaphora problem has
been reported with much higher results on other
domains(Raghunathan et al, 2010), and also on
other bio data (hsiang Lin and Liang, 2004). One
of the reasons for the low recall is because target
anaphoric pronouns in the bio domain are neutral-
gender and third-person pronouns(Nguyen and Kim,
2008), which are difficult to resolve than other types
of pronouns(Stoyanov et al, 2010a).
8.3 Protein coreference analysis - Intermediate
antecedent
As mentioned in the task setting, anaphors can di-
rectly link to their antecedent, or indirectly link via
one or more intermediate antecedents. We counted
the numbers of correct direct and indirect protein
coreference links in each submission (Table 12).
80
Sub type Type Count Sub type Type Count Sub type Type Count
both 1 PRON 2 both 2 PRON 4 either 1 PRON 0
it 1 PRON 17 its 1 PRON 61 one 2 PRON 1
such 2 DNP 2 that 1 RELAT 37 the 2 DNP 20
their 1 PRON 27 them 1 PRON 1 these 1 PRON 1
these 2 DNP 26 they 1 PRON 5 this 1 PRON 1
this 2 DNP 20 those 1 PRON 9 which 1 RELAT 37
whose 1 RELAT 1 whose 2 RELAT 0 (others) N/C 11
Table 10: Mapping from sub type to coreference type. Count = number of anaphors
TEAM A R R D D P P O
Di Di In Di In Di In Di
UU 44 4 15
UZ 35 2 1 23
CU 54 1
UT 22 1 1 16 1
US 8 1
UC 1 1
Total 1 64 7 65 5 126 9 7
Table 12: Numbers of correct protein coreference links
by anaphor type and by number of antecedents, based on
manual type annotation. A=APPOS, R=RELAT, D=DNP,
P=PRON, O=Others. Di=direct, In=indirect.
APPOS and Others types do not have any intermedi-
ate antecedent, thus there is only one column marked
with D (direct protein coreference link). We can
see in this table that very few indirect links were
detected. Therefore, there is place to improve our
resolution system by focusing on detection of such
links.
8.4 Surface coreference results
Because inclusion of all expressions was not a re-
quirement of shared task submission, the submitted
results may not contain expressions that do not in-
volve in any coreference links. Therefore, it is un-
fair to evaluate expression detection based on the re-
sponse expressions.
Evaluation results for anaphoricity determination
are shown in Table 13. The calculation is performed
as following. Supposing that every anaphor has a
response link, the number of anaphors is number
of distinct anaphoric expressions inferred from the
response links, which is given in the first column.
The total number of gold anaphors are also calcu-
lated in similar way. Since response expressions
are lined with gold expressions before evaluation,
Team Resp Align P R F
UU 360 94.2 19.4 33.3 24.6
UZ 736 75.8 22.0 77.1 34.2
CU 365 89.6 15.3 26.7 19.5
UT 452 92.0 18.1 39.0 24.8
US 259 9.3 6.2 7.6 6.8
UC 797 6.8 1.1 4.3 1.8
Table 13: Anaphoricity determination results. Total num-
ber of gold anaphors = 210. Resp = number of response
anchors, Align = alignment rate(%), P = precision (%), R
= recall (%), F = f-score (%)
we provided the alignment rate for reference in the
second column of the table. The third and forth
columns show the precisions and recalls. In theory,
low anaphoricity determination precision results in
many spurious response links, while low recall be-
comes the bottle neck for the overall coreference
resolution recall. Therefore, we can conclude that
the low performance of anaphoricity determination
contribute to the low coreference evaluation results
(Table 4, Table 3).
9 Conclusion
The coreference resolution supporting task of
BioNLP Shared Task 2011 has drawn attention from
researchers of different interests. Although the over-
all results are not good enough to be helpful for the
main shared tasks as expected, the analysis results in
this paper shows the coreference types which have
and have not yet been successfully solved. Tack-
ling the remained problems in expression bound-
ary detection, anaphoricity determination and reso-
lution algorithms for difficult types of anaphors such
as definite noun phrases should be the future work.
Then, it would be interesting to see how much coref-
erence can contribute to event extraction.
81
References
B. Baldwin. 1997. Cogniac: High precision with limited
knowledge and linguistic resources. In Proceedings of
the ACL?97/EACL?97 Workshop on Operational Fac-
tors in Practical, Robust Anaphora Resolution, pages
38?45, Madrid, Spain.
Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised event coreference resolution with rich linguis-
tic features. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1412?1422, Uppsala, Sweden, July. Association
for Computational Linguistics.
Nancy Chinchor. 1998. Overview of MUC-7/MET-2.
In Message Understanding Conference (MUC-7) Pro-
ceedings.
Yu hsiang Lin and Tyne Liang. 2004. Pronominal and
sortal anaphora resolution for biomedical literature.
In In Proceedings of ROCLING XVI: Conference on
Computational Linguistics and Speech Processing.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146, February.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
ACL, pages 1396?1411.
Ngan Nguyen and Jin-Dong Kim. 2008. Exploring do-
main differences for the design of a pronoun resolution
system for biomedical texts. In Proceedings of 22nd
International Conference on Computational Linguis-
tics (COLING-2008).
T Ohta, Y Tateisi, H Mima, and J Tsujii. 2002. Ge-
nia corpus: an annotated research abstract corpus in
molecular biology domain. Proceedings of the Hu-
man Language Technology Conference (HLT 2002),
San Diego, California, pages 73?77.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-
sky, and Christopher Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 492?501, October.
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler,
and D. Hysom. 2010a. Coreference resolution with
reconcile. In Proceedings of the Conference of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2010).
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler,
and D. Hysom. 2010b. Reconcile: A coreference res-
olution platform. In Tech Report - Cornell University.
Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi
Song, and Kazuaki Maeda. 2008. Linguistic Re-
sources and Evaluation Techniques for Evaluation of
Cross-Document Automatic Content Extraction. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Jian Su, Xiaofeng Yang, Huaqing Hong, Yuka Tateisi,
and Jun?ichi Tsujii. 2008. Coreference Resolution in
Biomedical Texts: a Machine Learning Approach. In
Ontologies and Text Mining for Life Sciences?08.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax annotation for the genia
corpus. In International Joint Conference on Natu-
ral Language Processing, pages 222?227, Jeju Island,
Korea, October.
82
Proceedings of BioNLP Shared Task 2011 Workshop, pages 112?120,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP Shared Task 2011: Supporting Resources
Pontus Stenetorp: Goran Topic? Sampo Pyysalo
Tomoko Ohta Jin-Dong Kim; and Jun?ichi Tsujii$
Tsujii Laboratory, Department of Computer Science, University of Tokyo, Tokyo, Japan
:Aizawa Laboratory, Department of Computer Science, University of Tokyo, Tokyo, Japan
; Database Center for Life Science,
Research Organization of Information and Systems, Tokyo, Japan
$Microsoft Research Asia, Beijing, People?s Republic of China
{pontus,goran,smp,okap}@is.s.u-tokyo.ac.jp
jdkim@dbcls.rois.ac.jp
jtsujii@microsoft.com
Abstract
This paper describes the supporting resources
provided for the BioNLP Shared Task 2011.
These resources were constructed with the
goal to alleviate some of the burden of sys-
tem development from the participants and al-
low them to focus on the novel aspects of con-
structing their event extraction systems. With
the availability of these resources we also seek
to enable the evaluation of the applicability of
specific tools and representations towards im-
proving the performance of event extraction
systems. Additionally we supplied evaluation
software and services and constructed a vi-
sualisation tool, stav, which visualises event
extraction results and annotations. These re-
sources helped the participants make sure that
their final submissions and research efforts
were on track during the development stages
and evaluate their progress throughout the du-
ration of the shared task. The visualisation
software was also employed to show the dif-
ferences between the gold annotations and
those of the submitted results, allowing the
participants to better understand the perfor-
mance of their system. The resources, evalu-
ation tools and visualisation tool are provided
freely for research purposes and can be found
at http://sites.google.com/site/bionlpst/
1 Introduction
For the BioNLP?09 Shared Task (Kim et al, 2009),
the first in the ongoing series, the organisers pro-
vided the participants with automatically generated
syntactic analyses for the sentences from the anno-
tated data. For evaluation purposes, tools were made
publicly available as both distributed software and
online services. These resources were well received.
A majority of the participants made use of one or
more of the syntactic analyses, which have remained
available after the shared task ended and have been
employed in at least two independent efforts study-
ing the contribution of different tools and forms of
syntactic representation to the domain of informa-
tion extraction (Miwa et al, 2010; Buyko and Hahn,
2010). The evaluation software for the BioNLP?09
Shared Task has also been widely adopted in subse-
quent studies (Miwa et al, 2010; Poon and Vander-
wende, 2010; Bjo?rne et al, 2010).
The reception and research contribution from pro-
viding these resources encouraged us to continue
providing similar resources for the BioNLP Shared
Task 2011 (Kim et al, 2011a). Along with the
parses we also encouraged the participants and ex-
ternal groups to process the data with any NLP (Nat-
ural Language Processing) tools of their choice and
make the results available to the participants.
We provided continuous verification and evalua-
tion of the participating systems using a suite of in-
house evaluation tools. Lastly, we provided a tool
for visualising the annotated data to enable the par-
ticipants to better grasp the results of their experi-
ments and to help gain a deeper understanding of
the underlying concepts and the annotated data. This
paper presents these supporting resources.
2 Data
This section introduces the data resources provided
by the organisers, participants and external groups
for the shared task.
112
Task Provider Tool
CO University of Utah Reconcile
CO University of Zu?rich UZCRS
CO University of Turku TEES
REL University of Turku TEES
Table 1: Supporting task analyses provided, TEES
is the Turku Event Extraction System and UZCRS
is the University of Zu?rich Coreference Resolution
System
2.1 Supporting task analyses
The shared task included three Supporting Tasks:
Coreference (CO) (Nguyen et al, 2011), Entity re-
lations (REL) (Pyysalo et al, 2011b) and Gene re-
naming (REN) (Jourde et al, 2011). In the shared
task schedule, the supporting tasks were carried out
before the main tasks (Kim et al, 2011b; Pyysalo
et al, 2011a; Ohta et al, 2011; Bossy et al, 2011)
in order to allow participants to make use of analy-
ses from the systems participating in the Supporting
Tasks for their main task event extraction systems.
Error analysis of BioNLP?09 shared task sub-
missions indicated that coreference was the most
frequent feature of events that could not be cor-
rectly extracted by any participating system. Fur-
ther, events involving statements of non-trivial rela-
tions between participating entities were a frequent
cause of extraction errors. Thus, the CO and REL
tasks were explicitly designed to support parts of
the main event extraction tasks where it had been
suggested that they could improve the system per-
formance.
Table 1 shows the supporting task analyses pro-
vided to the participants. For the main tasks, we
are currently aware of one group (Emadzadeh et al,
2011) that made use of the REL task analyses in their
system. However, while a number of systems in-
volved coreference resolution in some form, we are
not aware of any teams using the CO task analyses
specifically, perhaps due in part to the tight sched-
ule and the somewhat limited results of the CO task.
These data will remain available to allow future re-
search into the benefits of these resources for event
extraction.
2.2 Syntactic analyses
For syntactic analyses we provided parses for all
the task data in various formats from a wide range
of parsers (see Table 2). With the exception of
the Pro3Gres1 parser (Schneider et al, 2007), the
parsers were set up and run by the task organisers.
The emphasis was put on availability for research
purposes and variety of parsing models and frame-
works to allow evaluation of their applicability for
different tasks.
In part following up on the results of Miwa et al
(2010) and Buyko and Hahn (2010) regarding the
impact on performance of event extraction systems
depending on the dependency parse representation,
we aimed to provide several dependency parse for-
mats. Stanford Dependencies (SD) and Collapsed
Stanford Dependencies (SDC), as described by de
Marneffe et al (2006), were generated by convert-
ing Penn Treebank (PTB)-style (Marcus et al, 1993)
output using the Stanford CoreNLP Tools2 into the
two dependency formats. We also provided Confer-
ence on Computational Natural Language Learning
style dependency parses (CoNLL-X) (Buchholz and
Marsi, 2006) which were also converted from PTB-
style output, but for this we used the conversion
tool3 from Johansson and Nugues (2007). While
this conversion tool was not designed with convert-
ing the output from statistical parsers in mind (but
rather to convert between treebanks), it has previ-
ously been applied successfully for this task (Miyao
et al, 2008; Miwa et al, 2010).
The text from all documents provided were split
into sentences using the Genia Sentence Splitter4
(S?tre et al, 2007) and then postprocessed using a
set of heuristics to correct frequently occurring er-
rors. The sentences were then tokenised using a to-
kenisation script created by the organisers intended
to replicate the tokenisation of the Genia Tree Bank
(GTB) (Tateisi et al, 2005). This tokenised and
sentence-split data was then used as input for all
parsers.
We used two deep parsers that provide phrase
structure analysis enriched with deep sentence struc-
1https://files.ifi.uzh.ch/cl/gschneid/parser/
2http://nlp.stanford.edu/software/corenlp.shtml
3http://nlp.cs.lth.se/software/treebank converter/
4http://www-tsujii.is.s.u-tokyo.ac.jp/y-matsu/geniass/
113
Name Format(s) Model Availability BioNLP?09
Berkeley PTB, SD, SDC, CoNLL-X News Binary, Source No
C&C CCG, SD Biomedical Binary, Source Yes
Enju HPSG, PTB, SD, SDC, CoNLL-X Biomedical Binary No
GDep CoNLL-X Biomedical Binary, Source Yes
McCCJ PTB, SD, SDC, CoNLL-X Biomedical Source Yes
Pro3Gres Pro3Gres Combination ? No
Stanford PTB, SD, SDC, CoNLL-X Combination Binary, Source Yes
Table 2: Parsers, the formats for which their output was provided and which type of model that was used. The
availability column signifies public availability (without making an explicit request) for research purposes
tures, for example predicate-argument structure for
Head-Driven Phrase Structure Grammar (HPSG).
First we used the C&C Combinatory Categorial
Grammar (CCG) parser5 (C&C) by Clark and Cur-
ran (2004) using the biomedical model described in
Rimell and Clark (2009) which was trained on GTB.
Unlike all other parsers for which we supplied SD
and SDC dependency parses, the C&C output was
converted from its native format using a separate
conversion script provided by the C&C authors. Re-
grettably we were unable to provide CoNLL-X for-
mat output for this parser due to the lack of PTB-
style output. The other deep parser used was the
HPSG parser Enju6 by Miyao and Tsujii (2008), also
trained on GTB.
We also applied the frequently adopted Stanford
Parser7 (Klein and Manning, 2003) using a mixed
model which includes data from the biomedical do-
main, and the Charniak Johnson re-ranking parser8
(Charniak and Johnson, 2005) using the self-trained
biomedical model from McClosky (2009) (McCCJ).
For the BioNLP?09 shared task it was observed
that the Bikel parser9 (Bikel, 2004), which used a
non-biomedical model and can be argued that it uses
the somewhat dated Collins? parsing model (Collins,
1996), did not contribute towards event extraction
performance as strongly as other parses supplied for
the same data. We therefore wanted to supply a
parser that can compete with the ones above in a do-
main which is different from the biomedical domain
to see whether conclusions could be drawn as to the
5http://svn.ask.it.usyd.edu.au/trac/candc/
6http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
7http://nlp.stanford.edu/software/lex-parser.shtml
8ftp://ftp.cs.brown.edu/pub/nlparser/
9http://www.cis.upenn.edu/dbikel/software.html
importance of using a biomedical model. For this
we used the Berkeley parser10 (Petrov et al, 2006).
Lastly we used a native dependency parser, the GE-
NIA Dependency parser (GDep) by Sagae and Tsujii
(2007).
At least one team (Choudhury et al, 2011) per-
formed experiments on some of the provided lexi-
cal analyses and among the 14 submissions for the
EPI and ID tasks, 13 submissions utilised tools for
which resources were provided by the organisers of
the shared task. We intend to follow up on whether
or not the majority of the teams ran the tools them-
selves or used the provided analyses.
2.3 Other analyses
The call for analyses was open to all interested par-
ties and all forms of analysis. In addition to the Sup-
porting Task analyses (CO and REL) and syntactic
analyses provided by various groups, the University
of Antwerp CLiPS center (Morante et al, 2010) re-
sponded to the call providing negation/speculation
analyses in the BioScope corpus format (Szarvas et
al., 2008).
Although this resource was not utilised by the par-
ticipants for the main task, possibly due to a lack of
time, it is our hope that by keeping the data available
it can lead to further development of the participat-
ing systems and analysis of BioScope and BioNLP
ST-style hedging annotations.
3 Tools
This section presents the tools produced by the or-
ganisers for the purpose of the shared task.
10http://code.google.com/p/berkeleyparser/
114
1 10411007-E1 Regulation <Exp>regulate[26-34] <Theme>TNF-alpha[79-88] ?
?<Excerpt>[regulate] an enhancer activity in the third intron of [TNF-alpha]
2 10411007-E2 Gene_expression <Exp>activity[282-290] <Theme>TNF-alpha[252-261] ?
?<Excerpt>[TNF-alpha] gene displayed weak [activity]
3 10411007-E3 +Regulation <Exp>when[291-295] <Theme>E2 <Excerpt>[when]
Figure 1: Text output from the BioNLP?09 Shared Event Viewer with line numbering and newline markings
Figure 2: An illustration of collective (sentence 1)
and distributive reading (sentence 2). ?Theme? is
abbreviated as ?Th? and ?Protein? as ?Pro? when
there is a lack of space
3.1 Visualisation
The annotation data in the format specified by the
shared task is not intended to be human-readable ?
yet researchers need to be able to visualise the data
in order to understand the results of their experi-
ments. However, there is a scarcity of tools that can
be used for this purpose. There are three available
for event annotations in the BioNLP ST format that
we are aware of.
One is the BioNLP?09 Shared Task Event
Viewer11, a simple text-based annotation viewer: it
aggregates data from the annotations, and outputs it
in a format (Figure 1) that is meant to be further pro-
cessed by a utility such as grep.
Another is What?s Wrong with My NLP12, which
visualises relation annotations (see Figure 3a) ? but
is unable to display some of the information con-
tained in the Shared Task data. Notably, the distribu-
tive and collective readings of an event are not dis-
tinguished (Figure 2). It also displays all annotations
on a single line, which makes reading and analysing
longer sentences, let alne whole documents, some-
what difficult.
The last one is U-Compare13 (Kano et al, 2009),
11http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/
downloads.shtml
12http://code.google.com/p/whatswrong/
13http://u-compare.org/bionlp2009.html
which is a comprehensive suite of tools designed for
managing NLP workflows, integrating many avail-
able services. However, the annotation visualisation
component, illustrated in Figure 3b, is not optimised
for displaying complex event structures. Each anno-
tation is marked by underlining its text segment us-
ing a different colour per annotation type, and a role
in an event is represented by a similarly coloured arc
between the related underlined text segments. The
implementation leaves some things to be desired:
there is no detailed information added in the display
unless the user explicitly requests it, and then it is
displayed in a separate panel, away from the text it
annotates. The text spacing makes no allowance for
the annotations, with opaque lines crossing over it,
with the effect of making both the annotations and
the text hard to read if the annotations are above a
certain degree of complexity.
As a result of the difficulties of these existing
tools, in order to extract a piece of annotated text
and rework it into a graph that could be embedded
into a publication, users usually read off the annota-
tions, then create a graph from scratch using vector
drawing or image editing software.
To address these issues, we created a visualisa-
tion tool named stav (stav Text Annotation Visual-
izer), that can read the data formatted according to
the Shared Task specification and aims to present it
to the user in a form that can be grasped at a glance.
Events and entities are annotated immediately above
the text, and the roles within an event by labelled
arcs between them (Figure 3c). In a very complex
graph, users can highlight the object or association
of interest to follow it even more easily. Special fea-
tures of annotations, such as negation or speculation,
are shown by unique visual cues, and more in-depth,
technical information that is usually not required can
be requested by floating the mouse cursor over the
annotation (as seen in Figure 5).
We took care to minimise arc crossovers, and to
115
(a) Visualisation using What?s Wrong with My NLP
(b) Visualisation using U-Compare
(c) Visualisation using stav
Figure 3: Different visualisations of complex textual annotations of Dickensheets et al (1999)
116
Figure 4: A screenshot of the stav file-browser
keep them away from the text itself, in order to main-
tain text readability. The text is spaced to accommo-
date the annotations between the rows. While this
does end up using more screen real-estate, it keeps
the text legible, and annotations adjacent to the text.
The text is broken up into lines, and each sentence
is also forced into a new line, and given a numer-
ical identifier. The effect of this is that the text is
laid out vertically, like an article would be, but with
large spacing to accomodate the annotations. The
arcs are similarly continued on successive lines, and
can easily be traced ? even in case of them spanning
multiple lines, by the use of mouseover highlight-
ing. To preserve the distributionality information of
the annotation, any event annotations are duplicated
for each event, as demonstrated in the example in
Figure 2.
stav is not limited to the Shared Task datasets with
appropriate configuration settings, it could also vi-
sualise other kinds of relational annotations such as:
frame structures (Fillmore, 1976) and dependency
parses (de Marneffe et al, 2006).
To achieve our objectives above, we use the Dy-
namic Scalable Vector Graphics (SVG) functional-
ity (i.e. SVG manipulated by JavaScript) provided
by most modern browsers to render the WYSIWYG
(What You See Is What You Get) representation of
the annotated document. An added benefit from
this technique is that the installation process, if any,
is very simple: although not all browsers are cur-
rently supported, the two that we specifically tested
against are Safari14 and Google Chrome15; the for-
mer comes preinstalled with the Mac OS X oper-
ating system, while the latter can be installed even
by relatively non-technical users. The design is kept
modular using a dispatcher pattern, in order to al-
low the inclusion of the visualiser tool into other
JavaScript-based projects. The client-server archi-
tecture also allows centralisation of data, so that ev-
ery user can inspect an uploaded dataset without the
hassle of downloading and importing into a desktop
application, simply by opening an URL which can
uniquely identify a document, or even a single an-
notation. A screenshot of the stav file browser can
be seen in Figure 4.
3.2 Evaluation Tools
The tasks of BioNLP-ST 2011 exhibit very high
complexity, including multiple non-trivial subprob-
lems that are partially, but not entirely, independent
of each other. With such tasks, the evaluation of par-
ticipating systems itself becomes a major challenge.
Clearly defined evaluation criteria and their precise
implementation is critical not only for the compari-
son of submissions, but also to help participants fol-
low the status of their development and to identify
the specific strengths and weaknesses of their ap-
proach.
A further challenge arising from the complexity
of the tasks is the need to process the relatively in-
tricate format in which annotations are represented,
which in turn carries a risk of errors in submissions.
To reduce the risk of submissions being rejected or
the evaluation showing poor results due to format-
ting errors, tools for checking the validity of the file
format and annotation semantics are indispensable.
For these reasons, we placed emphasis in the or-
ganisation of the BioNLP-ST?11 on making tools for
format checking, validation and evaluation available
to the participants already during the early stages of
system development. The tools were made avail-
able in two ways: as downloads, and as online ser-
vices. With downloaded tools, participants can per-
form format checking and evaluation at any time
without online access, allowing more efficient op-
timisation processes. Each task in BioNLP-ST also
14http://www.apple.com/safari
15http://www.google.com/chrome
117
Figure 5: An example of a false negative illustrated by the evaluation tools in co-ordination with stav
maintained an online evaluation tool for the develop-
ment set during the development period. The online
evaluation is intended to provide an identical inter-
face and criteria for submitted data as the final on-
line submission system, allowing participants to be
better prepared for the final submission. With on-
line evaluation, the organisers could also monitor
submissions to ensure that there were no problems
in, for example, the evaluation software implemen-
tations.
The system logs of online evaluation systems
show that the majority of the participants submit-
ted at least one package with formatting errors, con-
firming the importance of tools for format checking.
Further, most of the participants made use of the on-
line development set evaluation at least once before
their final submission.
To enhance the evaluation tools we drew upon the
stav visualiser to provide a view of the submitted re-
sults. This was done by comparing the submitted
results and the gold data to produce a visualisation
where errors are highlighted, as illustrated in Fig-
ure 5. This experimental feature was available for
the EPI and ID tasks and we believe that by doing so
it enables participants to better understand the per-
formance of their system and work on remedies for
current shortcomings.
4 Discussion and Conclusions
Among the teams participating in the EPI and ID
tasks, a great majority utilised tools for which re-
sources were made available by the organisers. We
hope that the continued availability of the parses will
encourage further investigation into the applicability
of these and similar tools and representations.
As for the analysis of the supporting analyses pro-
vided by external groups and the participants, we are
so far aware of only limited use of these resources
among the participants, but the resources will re-
main available and we are looking forward to see
future work using them.
To enable reproducibility of our resources, we
provide a publicly accessible repository containing
the automated procedure and our processing scripts
used to produce the released data. This repository
also contains detailed instructions on the options and
versions used for each parser and, if the software li-
cense permits it, includes the source code or binary
that was used to produce the processed data. For the
cases where the license restricts redistribution, in-
structions and links are provided on how to obtain
the same version that was used. We propose that us-
ing a multitude of parses and formats can benefit not
just the task of event extraction but other NLP tasks
as well.
We have also made our evaluation tools and visu-
alisation tool stav available along with instructions
on how to run it and use it in coordination with the
shared task resources. The responses from the par-
ticipants in relation to the visualisation tool were
very positive, and we see this as encouragement to
advance the application of visualisation as a way to
better reach a wider understanding and unification
of the concept of events for biomedical event extrac-
tion.
All of the resources described in this paper are
available at http://sites.google.com/site/bionlpst/.
118
Acknowledgements
We would like to thank Jari Bjo?rne of the Uni-
versity of Turku BioNLP group; Gerold Schneider,
Fabio Rinaldi, Simon Clematide and Don Tuggener
of the Univerity of Zurich Computational Linguis-
tics group; Roser Morante of University of Antwerp
CLiPS center; and Youngjun Kim of the Univer-
sity of Utah Natural Language Processing Research
Group for their generosity with their time and exper-
tise in providing us with supporting analyses.
This work was supported by Grant-in-Aid for
Specially Promoted Research (MEXT, Japan) and
the Royal Swedish Academy of Sciences.
References
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
J. Bjo?rne, F. Ginter, S. Pyysalo, J. Tsujii, and
T. Salakoski. 2010. Complex event extraction at
PubMed scale. Bioinformatics, 26(12):i382.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, pages 149?164. Association
for Computational Linguistics.
E. Buyko and U. Hahn. 2010. Evaluating the impact
of alternative dependency graph encodings on solv-
ing event extraction tasks. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 982?992. Association for
Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Pallavi Choudhury, Michael Gamon, Chris Quirk, and
Lucy Vanderwende. 2011. MSR-NLP entry in
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
S. Clark and J.R. Curran. 2004. Parsing the WSJ us-
ing CCG and log-linear models. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 103. Association for Com-
putational Linguistics.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 184?191, Santa
Cruz, California, USA, June. Association for Compu-
tational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
H.L. Dickensheets, C. Venkataraman, U. Schindler, and
R.P. Donnelly. 1999. Interferons inhibit activation of
STAT6 by interleukin 4 in human monocytes by in-
ducing SOCS-1 gene expression. Proceedings of the
National Academy of Sciences of the United States of
America, 96(19):10800.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. A generalizable and efficient ma-
chine learning approach for biological event extraction
from text. In Proceedings of the BioNLP 2011 Work-
shop Companion Volume for Shared Task, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Charles J. Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy of
Sciences, 280(1):20?32.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task 2011
- Bacteria Gene Interactions and Renaming. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Yoshinobu Kano, William Baumgartner, Luke McCro-
hon, Sophia Ananiadou, Kevin Cohen, Larry Hunter,
and Jun?ichi Tsujii. 2009. U-Compare: share and
compare text mining tools with UIMA. Bioinformat-
ics, 25(15):1997?1998, May.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
119
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
D. Klein and C.D. Manning. 2003. Fast exact infer-
ence with a factored model for natural language pars-
ing. Advances in neural information processing sys-
tems, pages 3?10.
M.P Marcus, B. Santorini, and M.A Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Tree Bank. Computational Linguistics,
pages 313?318.
D. McClosky. 2009. Any Domain Parsing: Automatic
Domain Adaptation for Natural Language Parsing.
Ph.D. thesis, Ph. D. thesis, Department of Computer
Science, Brown University.
M. Miwa, S. Pyysalo, T. Hara, and J. Tsujii. 2010. Eval-
uating Dependency Representation for Event Extrac-
tion. In In the 23rd International Conference on Com-
putational Linguistics (COLING 2010), pages 779?
787.
Y. Miyao and J. Tsujii. 2008. Feature forest models for
probabilistic HPSG parsing. Computational Linguis-
tics, 34(1):35?80.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
Proceedings of ACL-08: HLT, pages 46?54, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
R. Morante, V. Van Asch, and W. Daelemans. 2010.
Memory-based resolution of in-sentence scopes of
hedge cues. CoNLL-2010: Shared Task, page 40.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Overview of the Protein Coreference task in BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 433?440. Association for Computa-
tional Linguistics.
H. Poon and L. Vanderwende. 2010. Joint inference
for knowledge extraction from biomedical literature.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 813?
821. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the Entity Relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Laura Rimell and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, 42(5):852 ? 865.
Biomedical Natural Language Processing.
R. S?tre, K. Yoshida, A. Yakushiji, Y. Miyao, Y. Matsub-
yashi, and T. Ohta. 2007. AKANE system: protein-
protein interaction pairs in BioCreAtIvE2 challenge,
PPI-IPS subtask. In Proceedings of the Second
BioCreative Challenge Workshop, pages 209?212.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL 2007 Shared
Task.
G. Schneider, M. Hess, and P. Merlo. 2007. Hybrid
long-distance functional dependency parsing. Unpub-
lished PhD thesis, Institute of Computational Linguis-
tics, University of Zurich.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope corpus: annotation for negation, uncer-
tainty and their scope in biomedical texts. In Proceed-
ings of the Workshop on Current Trends in Biomedical
Natural Language Processing, pages 38?45. Associa-
tion for Computational Linguistics.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005.
Syntax Annotation for the GENIA corpus. In Proceed-
ings of the IJCNLP, pages 222?227.
120
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 100?108,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
New Resources and Perspectives for Biomedical Event Extraction
Sampo Pyysalo1, Pontus Stenetorp2, Tomoko Ohta1, Jin-Dong Kim3 and Sophia Ananiadou1
1National Centre for Text Mining and University of Manchester,
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester, UK
2Tokyo University, 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan
3Database Center for Life Science, 2-11-16 Yayoi, Bunkyo-ku, Tokyo, Japan
Abstract
Event extraction is a major focus of re-
cent work in biomedical information extrac-
tion. Despite substantial advances, many chal-
lenges still remain for reliable automatic ex-
traction of events from text. We introduce a
new biomedical event extraction resource con-
sisting of analyses automatically created by
systems participating in the recent BioNLP
Shared Task (ST) 2011. In providing for the
first time the outputs of a broad set of state-of-
the-art event extraction systems, this resource
opens many new opportunities for studying
aspects of event extraction, from the identifi-
cation of common errors to the study of ef-
fective approaches to combining the strengths
of systems. We demonstrate these opportuni-
ties through a multi-system analysis on three
BioNLP ST 2011 main tasks, focusing on
events that none of the systems can success-
fully extract. We further argue for new per-
spectives to the performance evaluation of do-
main event extraction systems, considering a
document-level, ?off-the-page? representation
and evaluation to complement the mention-
level evaluations pursued in most recent work.
1 Introduction
Biomedical information extraction efforts are in-
creasingly focusing on event extraction using struc-
tured representations that allow associations of arbi-
trary numbers of participants in specific roles (e.g.
Theme, Cause) to be captured (Ananiadou et al,
2010). Domain event extraction has been advanced
in particular by the BioNLP Shared Task (ST) events
(Kim et al, 2011a; Kim et al, 2011b), which have
introduced common task settings, datasets, and eval-
uation criteria for event extraction. Participants in
these shared tasks have introduced dozens of sys-
tems for event extraction, and the resulting methods
have been applied to automatically analyse the entire
available domain literature (Bjo?rne et al, 2010) and
applied in support of applications such as semantic
literature search (Ohta et al, 2010; Van Landeghem
et al, 2011b) and pathway curation support (Kemper
et al, 2010).
It is possible to assess recent advances in event ex-
traction through results for a task considered both in
the BioNLP ST 2009 and 2011. By the primary eval-
uation criteria, the highest performance achieved in
the 2009 task was 51.95% F-score, and a 57.46% F-
score was reached in the comparable 2011 task (Kim
et al, 2011b). These results demonstrate significant
advances in event extraction methods, but also indi-
cate that the task continues to hold substantial chal-
lenges. This has led to a call from task participants
for further analysis of the data and results, accompa-
nied by a proposal to release analyses from individ-
ual systems to facilitate such analysis (Quirk et al,
2011).
In this study, we explore new perspectives into the
analyses and performance of event extraction meth-
ods. We build primarily on a new resource compiled
with the support of the majority of groups participat-
ing in the BioNLP ST 2011, consisting of analyses
from systems for the three main tasks sharing the
text-bound event representation. We demonstrate
the use of this resource through an evaluation fo-
cusing on events that cannot be extracted even by
the union of combined systems, identifying partic-
ular remaining challenges for event extraction. We
further propose and evaluate an alternate, document-
level perspective to event extraction, demonstrat-
ing that when only unique events are considered for
100
Figure 1: Example event annotations. The ?crossed-out? event type identifies an event marked as negated. Event
illustrations created using the STAV visualization tool (Stenetorp et al, 2011).
each document, the measured performance and even
ranking of systems participating in the shared task is
notably altered.
2 Background
In this work, we focus on the definition of the
event extraction task first introduced in the BioNLP
Shared Task 2009.1 The task targets the extrac-
tion of events, represented as n-ary associations of
participants (entities or other events), each marked
as playing a specific role such as Theme or Cause
in the event. Each event is assigned a type such
as BINDING or PHOSPHORYLATION from a fixed,
task-specific set. Events are further typically associ-
ated with specific trigger expressions that state their
occurrence in text. As physical entities such as pro-
teins are also identified in the setting with specific
spans referring to the real-world entities in text, the
overall task is ?text-bound? in the sense of requiring
not only the extraction of targeted statements from
text, but also the identification of specific regions of
text expressing each piece of extracted information.
Events can further be marked with modifiers iden-
tifying additional features such as being explicitly
negated or stated in a speculative context. Figure 1
shows an illustration of event annotations.
This BioNLP ST 2009 formulation of the event
extraction task was followed also in three 2011 main
tasks: the GE (Kim et al, 2011c), ID (Pyysalo et al,
2011a) and EPI (Ohta et al, 2011) tasks. A vari-
ant of this representation that omits event triggers
was applied in the BioNLP ST 2011 bacteria track
(Bossy et al, 2011), and simpler, binary relation-
type representations were applied in three support-
ing tasks (Nguyen et al, 2011; Pyysalo et al, 2011b;
Jourde et al, 2011). Due to the challenges of con-
sistent evaluation and processing for tasks involv-
1While far from the only formulation proposed in the litera-
ture, this specific task setting is the most frequently considered
and arguably a de facto standard for domain event extraction.
ing different representations, we focus in this work
specifically on the three 2011 main tasks sharing a
uniform representation: GE, ID and EPI.
3 New Resources for Event Extraction
In this section, we present the new collection of au-
tomatically created event analyses and demonstrate
one use of the data through an evaluation of events
that no system could successfully extract.
3.1 Data Compilation
Following the BioNLP ST 2011, the MSR-NLP
group called for the release of outputs from various
participating systems (Quirk et al, 2011) and made
analyses of their system available.2 Despite the ob-
vious benefits of the availability of these resources,
we are not aware of other groups following this ex-
ample prior to the time of this publication.
To create the combined resource, we approached
each group that participated in the three targeted
BioNLP ST 2011 main tasks to ask for their support
to the creation of a dataset including analyses from
their event extraction systems. This suggestion met
with the support of all but a few groups that were
approached.3 The groups providing analyses from
their systems into this merged resource are summa-
rized in Table 1, with references to descriptions of
the systems used to create the included analyses. We
compiled for each participant and each task both the
final test set submission and a comparable submis-
sion for the separate development set.
As the gold annotations for the test set are only
available for evaluation through an online interface
(in order to avoid overfitting and assure the compa-
rability of results), it is important to provide also de-
velopment set analyses to permit direct comparison
2http://research.microsoft.com/bionlp/
3We have yet to hear back from a few groups, but none has
yet explicitly denied the release of their data. Should any re-
maining group accept the release of their data, we will release a
new, extended version of the resource.
101
Task System
Team GE EPI ID BB BI CO REL REN description
UTurku 1 1 1 1 1 1 1 1 Bjo?rne and Salakoski (2011)
ConcordU 1 1 1 1 1 1 Kilicoglu and Bergler (2011)
UMass 1 1 1 Riedel and McCallum (2011)
Stanford 1 1 1 McClosky et al (2011)
FAUST 1 1 1 Riedel et al (2011)
MSR-NLP 1 1 Quirk et al (2011)
CCP-BTMG 1 1 Liu et al (2011)
BMI@ASU 1 Emadzadeh et al (2011)
TM-SCS 1 Bui and Sloot (2011)
UWMadison 1 Vlachos and Craven (2011)
HCMUS 1 1 Le Minh et al (2011)
PredX 1 -
VIBGhent 1 Van Landeghem et al (2011a)
Table 1: BioNLP ST 2011 participants contributing to the combined resource.
Events
Task Gold FN Recall
GE (task 1) 3250 1006 69.05%
EPI (CORE task) 601 129 78.54%
ID (CORE task) 691 183 73.52%
Table 2: Recall for the union of analyses from systems
included in the combined dataset.
against gold annotations. The inclusion of both de-
velopment and test set annotations also allows e.g.
the study of system combination approaches where
the combination parameters are estimated on devel-
opment data for final testing on the test set (Kim et
al., 2011a).
3.2 Evaluation
We demonstrate the use of the newly compiled
dataset through a manual evaluation of GE, EPI and
ID main task development set gold standard events
that are not extracted by any of the systems for
which analyses were available.4 We perform eval-
uation on the GE subtask 1 and the EPI and ID
task CORE subtasks, as all participating systems ad-
dressed the extraction targets of these subtasks.
We first evaluated each of the analyses against the
development set of the respective task using the of-
ficial shared task evaluation software, using options
for the evaluation tools to list the sets of true posi-
tive (TP), false positive (FP) and false negative (FN)
4The final collection includes analyses from the systems of
two groups that agreed to the release of their data after the com-
pletion of this analysis, but we expect the results to largely hold
also for the final collection.
events. We then selected for each of the three tasks
the set of events that were included in the FN list
for all systems. This gives the results for the re-
call of the union of all systems shown in Table 2.
The recall of the system union is approximately 30%
points higher than that of any individual GE system
(Kim et al, 2011c) and 25% points higher for EPI
and ID (Ohta et al, 2011; Pyysalo et al, 2011a),
suggesting potential remaining benefits from system
combination. Nevertheless, a substantial fraction of
the total set of gold events remains inaccessible also
to this system union.
We then selected a random set of 100 events from
each of the three sets of events that were not re-
covered by any system (i.e. 300 events in total) and
performed a manual evaluation to identify frequent
properties of these events that could contribute to
extraction failures. In brief, we first performed a
brief manual evaluation to identify common charac-
teristics of these events, and then evaluated the 300
events individually to identify the set of these char-
acteristics that apply to each event.
The results of the evaluation for common cases
are shown in Table 3. We find that the most fre-
quent property of the unrecoverable events is that
they involve implicit arguments (Gerber and Chai,
2010), a difficult challenge that has not been ex-
tensively considered in domain event extraction. A
closely related issue are events involving arguments
in a sentence different from that containing the trig-
ger (?cross-sentence?), connected either implicitly
or through explicit coreference (?coreference?). Al-
102
Type GE EPI ID Total
Implicit argument 18 33 15 66
Cross-sentence 14 40 4 58
Weak trigger 28 14 11 53
Coreference 12 20 18 50
Static Relation 6 28 6 40
Error in gold 17 4 9 30
Ambiguous type 2 9 11 22
Shared trigger 2 12 1 15
Table 3: Manual evaluation results for features of events
that could not be recovered by any system.
though coreference was considered as as separate
task in BioNLP ST 2011 (Nguyen et al, 2011), it is
clear that it involves many remaining challenges for
event extraction systems. Similarly, events where
explicit arguments are connected to other arguments
through ?static? relations such as part-of (e.g. ?A
binds the X domain of B?) represent a known chal-
lenge (Pyysalo et al, 2011b). These results sug-
gest that further advances in event extraction perfor-
mance could be gained by the integration of systems
for the analysis of coreference and static relations,
approaches for which some success has already been
demonstrated in recent efforts (Van Landeghem et
al., 2010; Yoshikawa et al, 2011; Miwa et al, 2012).
?Weak? trigger expressions that must be inter-
preted in context to determine whether they express
an event, as well as a related class of events whose
type must be disambiguated with reference to con-
text (?ambiguous type?) are comparatively frequent
in the three tasks, while EPI in particular involves
many cases where a trigger is shared between mul-
tiple events ? an issue for approaches that assume
each token can be assigned at most a single class.
Finally, we noted a number of cases that we judged
to be errors in the gold annotation; the number
is broadly in line with the reported inter-annotator
agreement for the data (see e.g. Ohta et al (2011)).
While there is an unavoidable subjective com-
ponent to evaluations such as this, we note that a
similar evaluation performed following the BioNLP
Shared Task 2009 using test set data reached broadly
comparable results (Kim et al, 2011a). The newly
compiled dataset represents the first opportunity for
those without direct access to the test set data and
submissions to directly assess the task results, as
demonstrated here. We hope that this resource will
encourage further exploration of both the data, the
system analyses and remaining challenges in event
extraction.
4 New Perspectives to Event Extraction
As discussed in Section 2, the BioNLP ST event ex-
traction task is ?text-bound?: each entity and event
annotation is associated with a specific span of text.
Contrasted to the alternative approach where anno-
tations are document-level only, this approach has
a number of important benefits, such as allowing
machine learning methods for event extraction to
be directly trained on fully and specifically anno-
tated data without the need to apply frequently error-
prone heuristics (Mintz et al, 2009) or develop ma-
chine learning methods addressing the mapping be-
tween text expressions and document-level annota-
tions (Riedel et al, 2010). Many of the most suc-
cessful event extraction approaches involve direct
training of machine learning methods using the text-
bound annotations (Riedel and McCallum, 2011;
Bjo?rne and Salakoski, 2011; McClosky et al, 2011).
However, while the availability of text-bound anno-
tations in data provided to task participants is clearly
a benefit, there are drawbacks to the choice of ex-
clusive focus on text-bound annotations in system
output, including issues relating to evaluation and
the applicability of methods to the task. In the fol-
lowing section, we discuss some of these issues and
propose alternatives to representation and evaluation
addressing them.
4.1 Evaluation
The evaluation of the BioNLP ST is instance-based
and text-bound: each event in gold annotation and
each event extracted by a system is considered in-
dependently, separating different mentions of the
?same? real-world event. This is the most detailed
(sensitive) evaluation setting permitted by the data,
and from a technical perspective a reasonable choice
for ranking systems performing the task.
However, from a practical perspective, this eval-
uation setting arguably places excessively strict de-
mands on systems, and may result in poor correla-
tion between measured performance and the practi-
cal value of systems. Our motivating observations
are that specific real-world events tend to be men-
103
tioned multiple times in a single publication ? espe-
cially the events that are of particular importance in
the study ? and that there are few practical applica-
tions for which it is necessary to find each such re-
peated mention. For example, in literature search for
e.g. pathway or database curation support, one typi-
cal information need is to identify biomolecular re-
actions involving a specific protein. Event extraction
can support such needs either by summarizing all
events involving the protein that could be extracted
from the literature (Van Landeghem et al, 2011b), or
by retrieving documents (perhaps showing relevant
text snippets) containing such events (Ohta et al,
2010). For the former to meet the information need,
it may be sufficient that each different event is ex-
tracted once from the entire literature; for the latter,
once from each relevant document. For uses such
as these, there is no obvious need for, or, indeed,
no very obvious benefit from the ability of extrac-
tion systems to separately enumerate every mention
of every event in every publication. It is easy to en-
vision other practical use cases where instance-level
extraction performance is at best secondary and, we
argue, difficult to identify ones where it is of critical
importance.
For applications such as these, the important
question is the reliability of the system at identify-
ing events either on the level of documents or on the
level of (a relevant subset of) the literature, rather
than on the level of individual mentions. For a more
complete and realistic picture of the practical value
of event extraction methods, measures other than
instance-level should thus also be considered.
4.2 Task setting
While applications can benefit from the ability of
IE systems to identify a specific span of text sup-
porting extracted information,5 the requirement of
the BioNLP ST setting that the output of event ex-
traction systems must identify specific text spans for
each entity and event makes it complex or impossi-
ble to address the task using a number of IE methods
that might otherwise represent feasible approaches
to event extraction.
5For example, for curation support tasks, this allows the hu-
man curator to easily check the correctness of extracted infor-
mation and helps to select ?evidence sentences?, as included in
many databases.
For example, Patwardhan and Riloff (2007) and
Chambers and Jurafsky (2011) consider an IE ap-
proach where the extraction targets are MUC-4 style
document-level templates (Sundheim, 1991), the
former a supervised system and the latter fully un-
supervised. These methods and many like them for
tasks such as ACE (Doddington et al, 2004) work
on the document level, and can thus not be readily
applied or evaluated against the existing annotations
for the BioNLP shared tasks. Enabling the appli-
cation of such approaches to the BioNLP ST could
bring valuable new perspectives to event extraction.
4.3 Alternative evaluation
We propose a new mode of evaluation that otherwise
follows the primary BioNLP ST evaluation criteria,
but incorporates the following two exceptions:
1. remove the requirement to match trigger spans
2. only require entity texts, not spans, to match
The first alternative criterion has also been previ-
ously considered in the GE task evaluation (Kim et
al., 2011c); the latter has, to the best of our knowl-
edge, not been previously considered in domain
event extraction. We additionally propose to con-
sider only the minimal set of events that are unique
on the document level (under the evaluation criteria),
thus eliminating effects from repeated mentions of a
single event on evaluated performance. We created
tools implementing this mode of evaluation with ref-
erence to the BioNLP ST 2011 evaluation tools.
While this type of evaluation has, to the best of
our knowledge, not been previously applied specif-
ically in biomedical event extraction, it is closely
related (though not identical) to evaluation criteria
applied in MUC, ACE, and the in-domain PPI re-
lation extraction tasks in BioCreative (Krallinger et
al., 2008).
4.4 Alternative representation
A true conversion to a document-level, ?off the
page? representation would require manual anno-
tation efforts to identify the real-world entities and
events referred to in text (Doddington et al, 2004).
However, it is possible to reasonably approximate
such a representation through an automatic heuristic
conversion.
104
BioNLP Shared Task
T1 Protein 0 5 CIITA
T2 Protein 21 28 TAFII32
T3 Binding 6 15 interacts
E1 Binding:T3 Theme:T1 Theme2:T2
T4 Protein 54 61 TAFII32
T5 Protein 66 71 CIITA
T6 Binding 33 45 interactions
E2 Binding:T6 Theme:T4 Theme2:T5
Document level
T1 Protein CIITA
T2 Protein TAFII32 
E1 Binding Theme:T1 Theme2:T2
CIITA interacts with TAFII32 ... interactions between TAFII32 and CIITA are
Pro Binding Protein Binding Protein ProTh Th2 Theme
Theme2
...
Figure 2: Illustration of BioNLP Shared Task annotation format and the proposed document-level (?off-the-page?)
format.
We first introduce a non-textbound annotation for-
mat that normalizes over differences in e.g. argu-
ment order and eliminates duplicate events. The for-
mat largely follows that of the shared task but re-
moves any dependencies and references to text off-
sets (see Figure 2). The conversion process into this
representation involves a number of steps. First, we
merge duplicate pairs of surface strings and types,
as different mentions of the same entity in different
parts of the text are no longer distinguishable in the
representation. In the original format, equivalence
relations (Kim et al, 2011a) are annotated only for
specific mentions. When ?raising? the annotations
to the document level, equivalence relations are rein-
terpreted to cover the full document by extending
the equivalence to all mentions that share the surface
form and type with members of existing equivalence
classes. Finally, we implemented an event equiv-
alence comparison to remove duplicate annotations
from each document. The result of the conversion
to this alternate representation is thus an ?off-the-
page? summary of the unique set of events in the
document.
This data can then be used for training and com-
parison of methods analogously to the original anno-
tations, but without the requirement that all analyses
include text-bound annotations.
4.5 Experimental Results
We next present an evaluation using the alternative
document-level event representation and evaluation,
comparing its results to those for the primary shared
task evaluation criteria. As comparatively few of the
Primary criteria New criteria
Group Rec. Prec. F Rec. Prec. F
FAUST 49.41 64.75 56.04 53.10 67.56 59.46
UMass 48.49 64.08 55.20 52.55 66.57 58.74
UTurku 49.56 57.65 53.30 54.23 60.11 57.02
MSR-NLP 48.64 54.71 51.50 53.55 58.24 55.80
ConcordU 43.55 59.58 50.32 47.42 60.85 53.30
UWMadison 42.56 61.21 50.21 46.09 62.50 53.06
Stanford 42.36 61.08 50.03 46.48 63.22 53.57
BMI@ASU 36.91 56.63 44.69 41.15 61.44 49.29
CCP-BTMG 31.57 58.99 41.13 34.82 66.89 45.80
TM-SCS 32.73 45.84 38.19 38.02 50.87 43.51
HCMUS 10.12 27.17 14.75 14.50 40.05 21.29
Table 4: Comparison of BioNLP ST 2011 GE task 1 re-
sults.
shared task participants attempted subtasks 2 and 3
for GE or the FULL task setting for EPI and ID, we
consider only GE subtask 1 and the EPI and ID task
CORE extraction targets in these experiments. We
refer to the task overviews for the details of the sub-
tasks and the primary evaluation criteria (Kim et al,
2011c; Pyysalo et al, 2011a; Ohta et al, 2011).
Tables 4, 5 and 6 present the results for the
GE, EPI and ID tasks, respectively. For GE, we
see consistently higher F-scores for the new crite-
ria, in most cases reflecting primarily an increase
in recall, but also involving increases in precision.
The F-score differences range between 3-4% for
most high-ranking systems, with more substantial
increases for lower-ranking systems. Notable in-
creases in precision are seen for some systems (e.g.
HCMUS), indicating that the systems comparatively
frequently extract correct information, but associ-
ated with the wrong spans of text.
105
Primary criteria New criteria
Group Rec. Prec. F Rec. Prec. F
UTurku 68.51 69.20 68.86 74.20 69.14 71.58
FAUST 59.88 80.25 68.59 67.04 76.82 71.60
MSR-NLP 55.70 77.60 64.85 59.24 77.66 67.21
UMass 57.04 73.30 64.15 65.76 69.65 67.65
Stanford 56.87 70.22 62.84 62.74 67.12 64.86
CCP-BTMG 45.06 63.37 52.67 54.62 63.17 58.58
ConcordU 40.28 76.71 52.83 48.41 76.57 59.32
Table 5: Comparison of BioNLP ST 2011 EPI CORE
task results.
For EPI (Table 5), we find comparable differences
in F-score to those for GE, but there is a signifi-
cant difference in the precision-recall balance: the
majority of systems show over 5% points higher re-
call under the new criteria, but many show substan-
tial losses in precision, while for GE precision was
also systematically increased. This effect was not
unexpected: we judge this to reflect primarily the
increased number of opportunities to extract each
unique event (higher recall) combined with the com-
paratively higher effect from errors from the reduc-
tion in the total number of unique correct extraction
targets (lower precision). It is not clear from our
analysis why a comparable effect was not seen for
GE. Interestingly, most systems show a better pre-
cision/recall balance under the new criteria than the
old, despite not optimizing for these criteria.
For ID (Table 6), we find a different effect also on
F-score, with all but one system showing reduced
performance under the new criteria, with some very
clear drops in performance; the only system to ben-
efit is UTurku. Analysis suggests that this effect
traces primarily to a notable reduction in the number
of simple PROCESS events that take no arguments6
when considering unique events on the document
level instead of each event mention independently.7
Conversely, the Stanford system, which showed the
highest instance-level performance in the extraction
of PROCESS type events (see Pyysalo et al (2011a)),
shows a clear loss in precision.
6The ID task annotation criteria call for mentions of some
high-level biological processes such as ?infection? to be anno-
tated as PROCESS even if no explicit participants are mentioned
(Pyysalo et al, 2011a).
7It is interesting to note that there was an error in the
UTurku system implementation causing it to fail to output any
events without arguments (Jari Bjo?rne, personal communica-
tion), likely contributing to the effect seen here.
Primary criteria New criteria
Group Rec. Prec. F Rec. Prec. F
FAUST 50.84 66.35 57.57 50.11 65.33 56.72
UMass 49.67 62.39 55.31 49.34 60.98 54.55
Stanford 49.16 56.37 52.52 42.00 50.80 45.98
ConcordU 50.91 43.37 46.84 43.42 37.18 40.06
UTurku 39.23 49.91 43.93 48.03 51.84 49.86
PredX 23.67 35.18 28.30 20.94 30.69 24.90
Table 6: Comparison of BioNLP ST 2011 ID CORE task
results.
The clear differences in performance and the
many cases in which the system rankings under the
two criteria differ demonstrate that the new evalua-
tion criteria can have a decisive effect in which ap-
proaches to event extraction appear preferred. While
there may be cases for which the original shared task
criteria are preferred, there is at the very minimum
a reasonable argument to be made that the emphasis
these criteria place on the extraction of each instance
of simple events is unlikely to reflect the needs of
many practical applications of event extraction.
While these experimental results demonstrate that
the new evaluation criteria emphasize substantially
different aspects of the performance of the systems
than the original criteria, they cannot per se serve
as an argument in favor of one set of criteria over
another. We hope that these results and the accom-
panying tools will encourage increased study and
discussion of evaluation criteria for event extraction
and more careful consideration of the needs of spe-
cific applications of the technology.
5 Discussion and Conclusions
We have presented a new resource combining analy-
ses from the systems participating in the GE, ID and
EPI main tasks of the BioNLP Shared Task 2011,
compiled with the collaboration of groups partic-
ipating in these tasks. We demonstrated one use
of the resource through an evaluation of develop-
ment set events that none of the participating sys-
tems could recover, finding that events involving
implicit arguments, coreference and participants in
more than once sentence continue to represent chal-
lenges to the event extraction systems that partici-
pated in these tasks.
We further argued in favor of new perspectives to
the evaluation of domain event extraction systems,
106
emphasizing in particular the need for document-
level, ?off-the-page? representations and evaluation
to complement the text-bound, instance-level eval-
uation criteria that have so far been applied in the
shared task evaluation. We proposed a variant of
the shared task standoff representation for support-
ing such evaluation, and introduced evaluation tools
implementing the proposed criteria. An evaluation
supported by the introduced resources demonstrated
that the new criteria can in cases provide substan-
tially different results and rankings of the systems,
confirming that the proposed evaluation can serve
as an informative complementary perspective into
event extraction performance.
In future work, we hope to further extend the cov-
erage of the provided system outputs as well as their
analysis to cover all participants of all tasks in the
BioNLP Shared Task 2011. We also aim to use the
compiled resource in further study of appropriate
criteria for the evaluation of event extraction meth-
ods and deeper analysis of the remaining challenges
in event extraction.
To encourage further study of all aspects of event
extraction, all resources and tools introduced in this
study are provided freely to the community from
http://2011.bionlp-st.org.
Acknowledgments
We wish to thank the members of all groups con-
tributing to the combined resource, and in particular
the members of the MSR-NLP group for providing
both the initial suggestion for its creation as well as
the first publicly released analyses from their sys-
tem. We would also like to thank the anonymous
reviewers for their many insightful comments.
This work was funded in part by UK Biotechnol-
ogy and Biological Sciences Research Council (BB-
SRC) under project Automated Biological Event Ex-
traction from the Literature for Drug Discovery (ref-
erence number: BB/G013160/1), by the Ministry of
Education, Culture, Sports, Science and Technology
of Japan under the Integrated Database Project and
by the Swedish Royal Academy of Sciences.
References
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381?390.
Jari Bjo?rne and Tapio Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of the
BioNLP Shared Task 2011 Workshop.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Complex event extraction
at PubMed scale. Bioinformatics, 26(12):i382?390.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Maarten
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 56?64.
Quoc-Chinh Bui and Peter. M.A. Sloot. 2011. Extract-
ing biological events from text using simple syntactic
patterns. In Proceedings of BioNLP Shared Task 2011
Workshop, pages 143?146.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the ACL-HLT 2011, pages 976?986.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ACE) program?tasks, data, and evaluation. In Pro-
ceedings of LREC, volume 4, pages 837?840.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double layered learning for bio-
logical event extraction from text. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 153?154.
Matthew Gerber and Joyce Chai. 2010. Beyond nom-
bank: A study of implicit arguments for nominal predi-
cates. In Proceedings of ACL 2010, pages 1583?1592.
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task
2011 ? Bacteria gene interactions and renaming. In
Proceedings of BioNLP Shared Task 2011 Workshop,
pages 65?73.
Brian Kemper, Takuya Matsuzaki, Yukiko Matsuoka,
Yoshimasa Tsuruoka, Hiroaki Kitano, Sophia Anani-
adou, and Jun?ichi Tsujii. 2010. PathText: a text min-
ing integrator for biological pathway visualizations.
Bioinformatics, 26(12):i374?i381.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
general semantic interpretation approach to biologi-
cal event extraction. In Proceedings of the BioNLP
Shared Task 2011 Workshop.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2011a. Extracting
bio-molecular events from literature - the BioNLP?09
shared task. Computational Intelligence, 27(4):513?
540.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011b.
107
Overview of BioNLP Shared Task 2011. In Proceed-
ings of BioNLP Shared Task, pages 1?6.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori
Yonezawa. 2011c. Overview of the Genia Event task
in BioNLP Shared Task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop.
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, Alfonso Valencia, et al 2008. Overview
of the protein-protein interaction annotation extrac-
tion task of BioCreative II. Genome Biology, 9(Suppl
2):S4.
Quang Le Minh, Son Nguyen Truong, and Quoc Ho Bao.
2011. A pattern approach for biomedical event anno-
tation. In Proceedings of BioNLP Shared Task 2011
Workshop, pages 149?150.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching
approach for information extraction from biomedical
text. In Proceedings of the BioNLP Shared Task 2011
Workshop.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event extraction as dependency parsing.
In Proceedings of ACL-HLT 2011, pages 1626?1635.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-IJCNLP
2009, pages 1003?1011.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012. Boosting automatic event extraction from the
literature using domain adaptation and coreference
resolution. Bioinformatics.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii.
2011. Overview of BioNLP 2011 Protein Coreference
Shared Task. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 74?82.
Tomoko Ohta, Takuya Matsuzaki, Naoaki Okazaki,
Makoto Miwa, Rune S?tre, Sampo Pyysalo, and
Jun?ichi Tsujii. 2010. Medie and info-pubmed: 2010
update. BMC Bioinformatics, 11(Suppl 5):P7.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP Shared Task 2011
Workshop.
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of EMNLP-
CoNLL 2007, pages 717?727.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the entity relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of BioNLP Shared Task 2011 Workshop,
pages 83?88.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP entry in
BioNLP Shared Task 2011. In Proceedings of BioNLP
Shared Task 2011 Workshop, pages 155?163.
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction. In
Proceedings of EMNLP 2011, pages 1?12.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. Machine Learning and Knowledge Dis-
covery in Databases, pages 148?163.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Chris Manning. 2011. Model
combination for event extraction in BioNLP 2011. In
Proceedings of the BioNLP Shared Task 2011 Work-
shop.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP Shared Task 2011 Work-
shop.
Beth M. Sundheim. 1991. Third message understanding
evaluation and conference (MUC-3): Phase 1 status
report. In Proceedings of the Speech and Natural Lan-
guage Workshop, pages 301?305.
Sofie Van Landeghem, Sampo Pyysalo, Tomoko Ohta,
and Yves Van de Peer. 2010. Integration of static re-
lations to enhance event extraction from text. In Pro-
ceedings of BioNLP 2010, pages 144?152.
Sofie Van Landeghem, Thomas Abeel, Bernard De Baets,
and Yves Van de Peer. 2011a. Detecting entity rela-
tions as a supporting task for bio-molecular event ex-
traction. In Proceedings of BioNLP Shared Task 2011
Workshop, pages 147?148.
Sofie Van Landeghem, Filip Ginter, Yves Van de Peer,
and Tapio Salakoski. 2011b. Evex: a pubmed-scale
resource for homology-based generalization of text
mining predictions. In Proceedings of BioNLP 2011
Workshop, pages 28?37.
Andreas Vlachos and Mark Craven. 2011. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 36?40.
Katsumasa Yoshikawa, Sebastian Riedel, Tsutomu Hi-
rao, Masayuki Asahara, and Yuji Matsumoto. 2011.
Coreference based event-argument relation extraction
on biomedical text. Journal of Biomedical Semantics,
2(Suppl 5):S6.
108
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 202?205,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
PubAnnotation - a persistent and sharable corpus and annotation repository
Jin-Dong Kim and Yue Wang
Database Center for Life Science (DBCLS),
Research Organization of Information and Systems (ROIS),
2-11-16, Yayoi, Bunkyo-ku, Tokyo, 113-0032, Japan
{jdkim|wang}@dbcls.rois.ac.jp
Abstract
There has been an active development of cor-
pora and annotations in the BioNLP commu-
nity. As those resources accumulate, a new
issue arises about the reusability. As a solu-
tion to improve the reusability of corpora and
annotations, we present PubAnnotation, a per-
sistent and sharable repository, where various
corpora and annotations can be stored together
in a stable and comparable way. As a position
paper, it explains the motivation and the core
concepts of the repository and presents a pro-
totype repository as a proof-of-concept.
1 Introduction
Corpora with high-quality annotation is regarded in-
dispensable for the development of language pro-
cessing technology (LT), e.g. natural language pro-
cessing (NLP) or textmining. Biology is one of the
fields which have strong needs for LT, due to the
high productivity of new information, most of which
is published in literature. There have been thus an
active development of corpora and annotations for
the NLP for biology (BioNLP). Those resources are
certainly an invaluable asset of the community.
As those resources accumulate, however, a new
issue arises about the reusability: the corpora and
annotations need to be sharable and comparable. For
example, there are a number of corpora that claim to
have annotations for protein or gene names, e.g, Ge-
nia (Kim et al, 2003), Aimed (Bunescu et al, 2004),
and Yapex (Franze?n et al, 2002). To reuse them, a
user needs to be able to compare them so that they
can devise a strategy on how to use them. It is how-
ever known that often the annotations in different
corpora are incompatible to each other (Wang et al,
2010): while one is considered as a protein name in
a corpus, it may not be the case in another.
A comparison of annotations in different corpora
could be made directly or indirectly. If there is an
overlap between two corpora, a direct comparison
of them would be possible. For example, there are
one1, two2 and three3 PubMed abstracts overlapped
between Genia - Yapex, Genia - Aimed, and Yapex
- Aimed corpora, respectively. When there is no or
insufficient overlap, an indirect comparison could be
tried (Wang et al, 2010). In any case, there are a
number of problems that make it costly and trouble-
some, though not impossible, e.g. different formats,
different ways of character encoding, and so on.
While there have been a few discussions about
the reusability of corpora and annotations (Cohen et
al., 2005; Johnson et al, 2007; Wang et al, 2010;
Campos et al, 2012), as a new approach, we present
PubAnnotation, a persistent and sharable storage or
repository, where various corpora and annotations
can be stored together in a stable and comparable
way. In this position paper, after the motivation and
background are explained in section 1, the initial de-
sign and a prototype implementation of the storage
are presented in section 2 and 3, respectively and fu-
ture works are discussed in section 4.
2 Design
Figure 1 illustrates the current situation of cor-
pus annotation in the BioNLP community, which
we consider problematic. In the community, there
1PMID-10357818
2PMID-8493578, PMID-8910398
3PMID-9144171, PMID-10318834, PMID-10713102
202
Figure 1: Usual setup of PubMed text annotation
are several central sources of texts, e.g. PubMed,
PubMed Central (PMC), and so on. In this work,
we consider only PubMed as the source of texts for
brevity, but the same concept should be applicable
to other sources. Texts from PubMed are mostly the
title and abstract of literature indexed in PubMed.
For an annotation project, text pieces from a source
database (DB) are often copied in a local storage and
annotations are attached to them.
Among others, the problem we focus on in this
situation is the variations that are made to the texts.
Suppose that there are two groups who happen to
produce annotations to a same PubMed abstract.
The abstract will be copied to the local storages of
the two groups (illustrated as the local storage 1 and
2 in the figure). There are however at least two rea-
sons that may cause the local copies to be different
from the abstract in PubMed, and also to be different
from each other even though they are copies of the
same PubMed abstract:
Versioning This variation is made by PubMed. The
text in PubMed is changed from time to time
for correction, change of policy, and so on. For
example, Greek letters, e.g., ?, are spelled out,
e.g., alpha, in old entries, but in recent entries
they are encoded as they are in Unicode. For
the reason, there is a chance that copies of the
same entry made at different times (snapshots,
hereafter) may be different from each other.
Conversion This variation is made by individual
groups. The texts in a local storage are some-
times changed for local processing. For exam-
ple, most of the currently available NLP tools
(for English), e.g., POS taggers and parsers that
Figure 2: Persistent text/annotation repository
Figure 3: Text/annotation alignment for integration
are developed based on Penn Treebank, can-
not treat Unicode characters appropriately. For
such NLP tools to be used, all the Unicode
characters need to be converted to ASCII char-
acter sequences in local copies. Sometimes, the
result of some pre-processing, e.g. tokeniza-
tion, also remains in local copies.
The problem of text variation may not be such a
problem that makes the reuse of corpora and anno-
tations extremely difficult, but a problem that makes
it troublesome, raising the cost of the entire commu-
nity substantially.
To remedy the problem, we present, a persistent
and sharable storage of corpora and annotations,
which we call PubAnnotation. Figure 2 illustrates
an improved situation we aim at with PubAnnota-
tion. The key idea is to maintain all the texts in
PubAnnotation in their canonical form, to which all
the corresponding annotations are to be aligned. For
texts from PubMed, the canonical form is defined to
be exactly the same as in PubMed. With the defini-
tion, a text entry in PubAnnotation needs to be up-
dated (uptodate in the figure) as the corresponding
text in PubMed changes (versioning). Accordingly,
the annotations belonging to the entry also need to
be re-aligned (alignment).
There also would be a situation where a variation
of a text entry is required for some reason, e.g. for
203
Figure 4: Text/annotation alignment example
application of an NLP tool that cannot handle Uni-
code characters. Figure 3 illustrates a required pro-
cess to cope with such a situation: first, the text is
exported in a desired form (conversion in the fig-
ure); second, annotations are made to the text; and
third, the annotations are aligned back to the text in
its canonical form in the repository.
Figure 4 shows an example of text conversion
and annotation alignment that are required when the
Enju parser (Miyao and Tsujii, 2008) needs to be
used for the annotation of protein names. The ex-
ample text includes a Greek letter, ?, which Enju
cannot properly handle. As Enju expects Greek
letters to be spelled out with double equal signs
on both sides, the example text is converted as so
when it is exported into a local storage. Based
on the pre-processing by Enju, the two text spans,
CD==epsilon== and CD4, are annotated as pro-
tein names. When they are imported back to PubAn-
notation, the annotations are re-aligned to the canon-
ical text in the repository. In this way, the texts
and annotations can be maintained in their canon-
ical form and in alignment respectively in PubAn-
notation. In the same way, existing annotations, e.g.
Genia, Aimed, Yapex, may be imported in the repos-
itory, as far as their base texts are sufficiently similar
to the canonical entries so that they can be aligned
reliably. In this way, various existing annotations
may be integrated in the repository,
To enable all the processes described so far, any
two versions of the same text need to be aligned, so
that the places of change can be detected. Text align-
ment is therefore a key technology of PubAnnota-
tion. In our implementation of the prototype repos-
itory, the Hunt-McIlroy?s longest common subse-
quence (LCS) algorithm (Hunt and McIlroy, 1976),
as implemented in the diff-lcs ruby gem pack-
age, is used for the alignment.
Figure 5: DB schema of persistent annotation repository
3 Prototype implementation
As a proof-of-concept, a prototype repository has
been implemented. One aspect considered seriously
is the scalability, as repository is intended to be ?per-
sistent?. Therefore it is implemented on a relational
database (Ruby on Rails with PostgreSQL 9.1.3), in-
stead of relying on a plain file system.
Figure 5 shows the database schema of the reposi-
tory.4 Three tables are created for documents, anno-
tations, and (annotation) contexts, respectively. The
annotations are stored in a stand-off style, each of
which belongs to a document and also to an anno-
tation context (context, hereafter). A context rep-
resents a set of annotations sharing the same set of
meta-data, e.g., the type of annotation and the an-
notator. For brevity, we only considered PubMed as
the source DB, and named entity recognition (NER)-
type annotations, which may be simply represented
by the attributes, begin, end, and label.
The prototype repository provides a RESTful in-
terface. Table 1 shows some example which can be
accessed with the standard HTTP GET method. A
new entry can be created in the repository using a
HTTP POST method with data in JSON format. Fig-
ure 6 shows an example of JSON data for the cre-
ation of annotations in the repository. Note that, the
base text of the annotations needs to be passed to-
gether with the annotations, so that the text can be
compared to the canonical one in the repository. If a
difference is detected, the repository will try to align
the annotations to the text in the repository.
4Although not shown in the figure, all the records are stored
with the date of creation.
204
http://server url/pmid/8493578
to retrieve the document record of a specific PMID
http://server url/pmid/8493578.ascii
same as above, but in US-ASCII encoding (Unicode characters are converted to HTML entities).
http://server url/pmid/8493578/annotations
to retrieve all the annotations to the specific document.
http://server url/pmid/8493578/contexts
to retrieve all the annotation contexts created to the specific document.
http://server url/pmid/8493578/annotations?context=genia-protein
to retrieve all the annotations that belong to genia-protein context.
http://server url/pmid/8493578/annotations.json?context=genia-protein
the same as above, but in JSON format.
Table 1: Examples of RESTful interface of the prototype repository
{
"document":
{"pmid":"8493578",
"text":"Regulation ..."},
"context":
{"name":"genia-protein"},
"annotations":
[
{"begin":51,"end":56,
"label":"Protein",
{"begin":75,"end":97,
"label":"Protein",
]
}
Figure 6: The JSON-encoded data for the creation of two
protein annotations to the document of PMID:8493578.
4 Discussions and conclusions
The current state of the design and the prototype
implementation are largely incomplete, and there is
a much room for improvement. For example, the
database schema has to be further developed to store
texts from various source DBs, e.g., PMC, and to
represent various types of annotations, e.g., relations
and events. The issue of governance is yet to be
discussed. We, however, hope the core concepts
presented in this position paper to facilitate discus-
sions and collaborations of the community and the
remaining issues to be addressed in near future.
Acknowledgments
This work was supported by the ?Integrated
Database Project? funded by the Ministry of Edu-
cation, Culture, Sports, Science and Technology of
Japan.
References
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M.
Marcotte, Raymond J. Mooney, Arun K. Ramani, and
Yuk Wah Wong. 2004. Comparative experiments
on learning information extractors for proteins and
their interactions. Artificial Intelligence in Medicine,
33(2):139?155.
David Campos, Srgio Matos, Ian Lewin, Jos Lus Oliveira,
and Dietrich Rebholz-Schuhmann. 2012. Harmoniza-
tion of gene/protein annotations: towards a gold stan-
dard medline. Bioinformatics, 28(9):1253?1261.
K. Bretonnel Cohen, Philip V Ogren, Lynne Fox, and
Lawrence Hunter. 2005. Empirical data on corpus de-
sign and usage in biomedical natural language process-
ing. In AMIA annual symposium proceedings, pages
156?160.
Kristofer Franze?n, Gunnar Eriksson, Fredrik Olsson, Lars
Asker, Per Lide?n, and Joakim Co?ster. 2002. Protein
names and how to find them. International Journal of
Medical Informatics, 67(13):49 ? 61.
James W. Hunt and M. Douglas McIlroy. 1976. An Al-
gorithm for Differential File Comparison. Technical
Report 41, Bell Laboratories Computing Science, July.
Helen Johnson, William Baumgartner, Martin Krallinger,
K Bretonnel Cohen, and Lawrence Hunter. 2007.
Corpus refactoring: a feasibility study. Journal of
Biomedical Discovery and Collaboration, 2(1):4.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. GENIA corpus - a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19(suppl. 1):i180?i182.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35?80, March.
Yue Wang, Jin-Dong Kim, Rune S?tre, Sampo Pyysalo,
Tomoko Ohta, and Jun?ichi Tsujii. 2010. Improving
the inter-corpora compatibility for protein annotations.
Journal of Bioinformatics and Computational Biology,
8(5):901?916.
205
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 240?243,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Boosting the protein name recognition performance
by bootstrapping on selected text
Yue Wang and Jin-Dong Kim
Database Center for Life Science,
Research Organization of Information and Systems
2-11-16 Yayoi, Bunkyo-ku, Tokyo, Japan 113-0032
{wang,jdkim}@dbcls.rois.ac.jp
Abstract
When only a small amount of manually anno-
tated data is available, application of a boot-
strapping method is often considered to com-
pensate for the lack of sufcient training ma-
terial for a machine-learning method. The
paper reports a series of experimental results
of bootstrapping for protein name recogni-
tion. The results show that the performance
changes signicantly according to the choice
of text collection where the training samples
to bootstrap, and that an improvement can be
obtained only with a well chosen text collec-
tion.
1 Introduction
While machine learning-based approaches are be-
coming more and more popular for the development
of natural language processing (NLP) systems, cor-
pora with annotation are regarded as a critical re-
source for the training process. Nonetheless, the cre-
ation of corpus annotation is an expensive and time-
consuming work (Cohen et al, 2005), and it is of-
ten the case that lack of sufcient annotation hinders
the development of NLP systems. Bootstrapping
method (Becker et al, 2005; Vlachos and Gasperin,
2006) can be considered as a way to automatically
inate the amount of corpus annotation to comple-
ment the lack of sufcient annotation.
In this study, we report the experimental results on
the effect of bootstrapping for the training of protein
name recognizers, particularly in the situation when
we have only a small amount of corpus annotations.
In summary, we begin with a small corpus with
manual annotation for protein names. A named en-
tity tagger trained on the small corpus is applied to
a big collection of text, to obtain more annotation.
We hope the newly created annotation to be precise
enough so that the training of a protein tagger can
benet from the increased training material.
We assume that the accuracy of a bootstrapping
method (Ng, 2004) depends on two factors: the ac-
curacy of the bootstrap tagger itself and the similar-
ity of the text to the original corpus. While accuracy
of the bootstrap tagger may be maximized by nd-
ing the optimal parameters of the applied machine
learning method, the choice of text where the origi-
nal annotations will bootstrap may also be a critical
factor for the success of the bootstrapping method.
Experimental results presented in this paper con-
rm that we can get a improvement by using a boot-
strapping method with a well chosen collection of
texts.
The paper is organized as follows. Section 2 intro-
duces the two datasets used in this paper. Following
that, in Section 3, we briey introduce the experi-
ments performed in our research. The experimental
results are demonstrated in Section 4. The research
is concluded in Section 5 and in the meanwhile, fu-
ture work is discussed.
2 Datasets
2.1 The cyanobacteria genome database
Cyanobacteria are prokaryotic organisms that have
served as important model organisms for studying
oxygenic photosynthesis and have played a signi-
240
cant role in the Earthfs history as primary producers
of atmospheric oxygen (Nakao et al, 2010).
The cyanobacteria genome database (abbreviated
to CyanoBase
1
) includes the annotations to the
PubMed text. In total, 39 species of the cyanobacte-
ria are covered in the CyanoBase.
In our cyanobacteria data (henceforth, the Kazusa
data for short), 270 abstracts were annotated by two
independent annotators. We take the entities, about
which both of the annotators agreed with each other.
In total, there are 1,101 entities in 2,630 sentences.
The Kazusa data was split equally into three sub-
sets and the subsets were used in turn as the training,
development and testing sets in the experiments.
2.2 The BioCreative data
The BioCreative data, which was used for the
BioCreative II gene mention task
2
, is described as
the tagged gene/protein names in the PubMed text.
The training set is used in the research, and totally
there are 15,000 sentences in the dataset.
Unlike other datasets, the BioCreative data was
designed to contain sentences both with and without
protein names, in a variety of contexts. Since the
collection is made to explicitly compile positive and
negative examples for protein recognition, there is a
chance that the sample of text is not comprehensive,
and gray-zone expressions may be missed.
The reason that we chose the BioCreative data
for the bootstrapping is that, the BioCreative data
(henceforth, the BC2 data for short) is the collection
for the purpose of training and evaluation of protein
name taggers.
3 Experiment summary
In the following experiments, the NERSuite
3
, a
named entity tagger based on Conditional Random
Fields (CRFs) (Lafferty et al, 2001; Sutton and Mc-
Callum, 2007), is used. The NERSuite is executable
open-source and serves as a machine learning sys-
tem for named entity recognition (NER). The sigma
value for the L2-regularization is optimizable and in
our experiments, we tune the sigma value between
10?1 to 104.
1
http://genome.kazusa.or.jp/cyanobase
2
http://www.biocreative.org/
3
http://nersuite.nlplab.org/
As mentioned in Section 2.1, the three subsets of
Kazusa data are used for training, tuning and testing
purposes, in turn. We experimented with all the six
combinations.
Experiments were performed to compare three
different strategies. First, with the baseline strat-
egy, the protein tagger is trained only on the Kazusa
training set. The sigma value is optimized on the
tuning set, and the performance is evaluated on the
test set. It is the most typical strategy particularly
when it is believed there is a sufcient training ma-
terial.
Second, with the bootstrapping strategy, the
Kazusa training set is used as the seed data. A tag-
ger for bootstrapping (bootstrap tagger, hereafter) is
trained on the seed data, and applied to the BC2 data
to bootstrap the training examples. Another pro-
tein tagger (application tagger) is then trained on the
bootstrapped BC2 data together with the seed data.
The Kazusa tuning set is used to optimize the two
sigma values for the two protein taggers, and the
performance is evaluated on the test set. With this
strategy, we wish the bootstrapped examples com-
plement the lack of sufcient training examples.
Experiment Seed BT BT+SS
E1 368 647 647 (1,103)
E2 368 647 647 (1,103)
E3 366 759 759 (1,200)
E4 366 769 590 (1,056)
E5 367 882 558 (1,068)
E6 367 558 558 (1,068)
Table 1: The number of positive examples used in each
experiment. The ?BT? column shows the number of posi-
tive examples obtained by the bootstrapping in the 15,000
BC2 sentences. In the last column, the gures in paren-
theses are the number of the selected sentences.
Third, the bootstrapping with sentence selection
strategy is almost the same with the bootstrapping
strategy, except that the second tagger is trained after
the non-relevant sentences are ltered out from the
BC2 data. Here, non-relevant sentences mean those
that are not tagged by the the bootstrap tagger. With
this strategy, we wish an improvement with the boot-
strapping by removing noisy data. Table 1 shows the
number of the seed and bootstrapped examples used
for the three strategies. It is observed that the seed
241
Training Tuning Testing Baseline BT BT+SS
E1 A B C 63.7/29.2/40.0 [102] 61.3/25.9/36.4 [104-101] 61.7/38.2/47.1 [104-104]
E2 A C B 65.2/36.9/47.1 [103] 67.7/35.0/46.1 [104-101] 61.7/46.7/53.2 [104-104]
E3 B C A 75.3/36.4/49.1 [102] 75.2/31.3/44.2 [102-101] 67.1/40.0/50.1 [102-101]
E4 B A C 68.5/33.8/45.3 [102] 70.2/28.9/40.9 [104-101] 66.7/36.5/47.2 [101-102]
E5 C B A 77.7/35.1/48.3 [101] 71.8/27.7/40.0 [104-102] 70.9/38.3/49.7 [100-101]
E6 C A B 73.0/39.1/50.9 [101] 76.1/32.2/45.3 [100-102] 67.7/41.8/51.7 [100-102]
Table 2: Experimental results of using the Kazusa and BC2 data (Precision/Recall/F-score). ?BT? and ?SS? represent
the bootstrapping and sentence selection strategies, respectively. The gures in square brackets are the sigma values
optimized in the experiments.
annotation bootstrap only on a small portion of the
BC2 data set, e.g., 1,103 vs. 15,000 sentences in the
case of E1 (less than 10%), suggesting that a large
portion of the data set may be irrelevant to the origi-
nal data set.
4 Experimental results
The experimental results of all the six combinations
are shown in Table 2. The use of the three subsets,
denoted by A, B, C, of the Kazusa data set for train-
ing, tuning and testing in each experiment is spec-
ied in ?training?, ?tuning? and ?testing? columns.
The results of the baseline strategy that uses only
the Kazusa data are shown in the ?baseline? column,
whereas the results with the bootstrapping methods
with and without sentence selection are shown in the
last two columns. As explained in Section 3, the
sigma values are optimized using the tuning set for
each experiment. Note that for bootstrapping, we
need two sigma values for the bootstrapping tagger
and the application tagger. See section 3.
The performance of named entity recognition is
measured in terms of precision, recall and F-score.
For matching criterion, in order to avoid underesti-
mation, instead of the exact matching, system per-
formance is evaluated under a soft matching, the
overlapping matching criterion. That is, if any part
of the annotated protein/gene names is recognized
by the NER tagger, we will regard that as a correct
answer.
4.1 Results with the bootstrapping strategy
Comparing the two columns, ?baseline? and ?BT?,
we observe that the use of bootstrapping may lead
to a degradation of the performance. Note that the
sigma values are optimized on the development set
for each experiment, and the text for bootstrapping
is BC2 corpus which is expected to be similar to the
Kazusa corpus, but still it is observed that the boot-
strapping does not work, suggesting that the text col-
lection may not yet similar enough.
4.2 Results with bootstrapping with sentence
selection
Comparing the last column (the ?BT+SS? column)
to the ?baseline? column, we observe that the appli-
cation of the bootstrapping method with sentence se-
lection consistently improves the performance. The
improvement is sometimes signicant, e.g., 7.1% of
difference in F-score in the case of E1, but some-
times not, e.g., only 0.8% in the case of E6, but the
performance is improved in the every experiments.
The results conrm our assumption that the choice
of text for bootstrapping is important, and that the
sentence selection is a stable method for the choice
of text.
5 Conclusion and future work
In order to compensate for the lack of sufcient
training data for a CRF-based protein name recog-
nizer, the potential of a bootstrapping method has
been explored through a series of experiments. The
BC2 data was chosen for the bootstrapping as the
data set was one collected for protein name recogni-
tion.
Our initial experiment showed that the seed anno-
tations bootstrapped only on a very small portion of
the BC2 data set, suggesting that a big portion of the
data set might be less relevant to the seed corpus.
From a series of experiments, it was observed that
the performance of protein name recognition was al-
ways improved with bootstrapping by selecting only
242
the sentences where the seed annotations bootstrap,
and by using them as an additional training data.
The goal was to be able to predict more possible
protein mentions (recall) at a relatively satisfactory
level of the quality (precision). The experimental
results suggest us, in order to achieve the goal, the
choice of text collection is important for the success
of the use of a bootstrapping method.
For the future work, we would like to take use of
the original annotations in the BC2 data. A ltering
strategy (Wang, 2010) will be performed. Instead of
completely using the output of the Kazusa-trained
tagger, we compare the output of the Kazusa-trained
tagger with the BioCreative annotations. If the en-
tity is recognized by the tagger and also annotated
in the BioCreative data, then the annotation to this
entity will be kept. The entity will be regarded as
a true positive according to the BioCreative annota-
tions. Otherwise, we will remove the annotation to
the entity from the BioCreative annotations.
Further, we also would like to combine the boot-
strapping with the ltering. Besides keeping the true
positives, we also want to include some false pos-
itives from the bootstrapping. Because these false
positives helps in improving the recall, when the tag-
ger is applied to the Kazusa testing subset. To dis-
criminate this strategy from the bootstrapping and
ltering strategies, different sigma value should be
used.
Acknowledgement
We thank Shinobu Okamoto for providing the
Kazusa data and for many useful discussion. This
work was supported by the ?Integrated Database
Project? funded by the Ministry of Education, Cul-
ture, Sports, Science and Technology (MEXT) of
Japan.
References
K. Bretonnel Cohen, Lynne Fox, Philip Ogren and
Lawrence Hunter. 2005. Empirical data on corpus
design and usage in biomedical natural language pro-
cessing. Proceedings of the AMIA Annual Symposium,
38?45.
Markus Becker, Ben Hachey, Beatrice Alex, Claire
Grover. 2005. Optimising Selective Sampling for
Bootstrapping Named Entity Recognition. Proceed-
ings of the Workshop on Learning with Multiple Views,
5?11.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and Evaluating Named Entity Recognition
in the Biomedical domain. Proceedings of the BioNLP
Workshop, 138?145.
Andrew Ng. 2004. Feature selection, L1 vs. L2 regu-
larization, and rotational invariance. Proceedings of
the 21st International Conference on Machine Learn-
ing (ICML).
Mitsuteru Nakao, Shinobu Okamoto, Mitsuyo Kohara,
Tsunakazu Fujishiro, Takatomo Fujisawa, Shusei
Sato, Satoshi Tabata, Takakazu Kaneko and Yasukazu
Nakamura. 2010. CyanoBase: the cyanobacteria
genome database update 2010. Nucleic Acids Re-
search, 38:D379?D381.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, 282?289.
Charles Sutton and Andrew McCallum. 2007. An Intro-
duction to Conditional Random Fields for Relational
Learning. Introduction to Statistical Relational Learn-
ing, MIT Press.
Yue Wang. 2010. Developing Robust Protein Name
Recognizers Based on a Comparative Analysis of Pro-
tein Annotations in Different Corpora. University of
Tokyo, Japan, PhD Thesis.
243
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 1?7,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Overview of BioNLP Shared Task 2013 
 Claire N?dellec MIG INRA UR1077  F-78352 Jouy-en-Josas cedex claire.nedellec@jouy.inra.fr  
Robert Bossy MIG INRA UR1077  F-78352 Jouy-en-Josas cedex robert.bossy@jouy.inra.fr   Jin-Dong Kim Database Center for Life Science  2-11-16 Yayoi, Bunkyo-ku, Tokyo  jdkim@dbcls.rois.ac.jp  
Jung-jae Kim Nanyang Technological University Singapore  jungjae.kim@ntu.edu.sg  Tomoko Ohta National Centre for Text Mining and School of Computer Science University of Manchester tomoko.ohta@manchester.ac.uk  
Sampo Pyysalo National Centre for Text Mining and School of Computer Science University of Manchester sampo.pyysalo@gmail.com    Pierre Zweigenbaum LIMSI-CNRS F-91403 Orsay  pz@limsi.fr 
 
    Abstract The BioNLP Shared Task 2013 is the third edition of the BioNLP Shared Task series that is a community-wide effort to address fine-grained, structural information extraction from biomedical literature. The BioNLP Shared Task 2013 was held from January to April 2013. Six main tasks were proposed. 38 final submissions were received, from 22 teams. The results show advances in the state of the art and demonstrate that extraction methods can be successfully generalized in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST hereafter) series is a community-wide effort toward fine-grained biomolecular event extraction, from scientific documents. BioNLP-ST 2013 follows the general outline and goals of the previous tasks, namely BioNLP-ST?09 (Kim  et al, 2009) and BioNLP-ST?11 (Kim et al, 
2011). BioNLP-ST aims to provide a common framework for the comparative evaluation of information extraction (IE) methods in the biomedical domain. It shares this common goal with other tasks, namely BioCreative (Critical Assessment of Information Extraction in Biology) (Arighi  et al, 2011), DDIExtraction (Extraction of Drug-Drug Interactions from biomedical texts) (Segura-Bedmar et al, 2011) and i2b2 (Informatics for Integrating Biology and the Bedside) Shared-Tasks (Sun et al, 2013).  The biological questions addressed by the BioNLP-ST series belong to the molecular biology domain and its related fields. With the three editions, the series gathers several groups that prepared various tasks and resources, which represent diverse themes in biology. As the two previous editions, this one measures the progress accomplished by the community on complex text-bound event extraction. Compared to the other initiatives, the BioNLP-ST series proposes a linguistically motivated approach to event representation that enables the evaluation of the participating methods in a unifying computer science framework. Each edition has attracted an 
1
increasing number of teams with 22 teams submitting 38 final results this year. The task setup and the data serve as a basis for numerous further studies, released event extraction systems, and published datasets.  The first event in 2009 triggered active research in the community on a specific fine-grained IE task called Genia event extraction  task. Expanding on this, the second BioNLP-ST was organized under the theme Generalization, where the participants introduced numerous systems that could be straightforwardly applied to different tasks. This time, the BioNLP-ST goes a step further and pursues the grand theme of Knowledge base construction. There were five tasks in 2011, and this year there are 6.  - [GE] Genia Event Extraction for NFkB knowledge base  - [CG] Cancer Genetics  - [PC] Pathway Curation  - [GRO] Corpus Annotation with Gene Regulation Ontology  - [GRN] Gene Regulation Network in Bacteria  - [BB] Bacteria Biotopes  The grand theme of Knowledge base construction is addressed in various ways: semantic web (GE, GRO), pathway (PC), molecular mechanism of cancer (CG), regulation network (GRN) and ontology population (GRO, BB).  In the biology domain, BioNLP-ST 2013 covers many new hot topics that reflect the evolving needs of biologists. BioNLP-ST 2013 broadens the scope of the text-mining application domains in biology by introducing new issues on cancer genetics and pathway curation. It also builds on the well-known previous datasets GENIA, LLL/BI and BB to propose tasks closer to the actual needs of biological data integration.  As in previous events, manually annotated data are provided to the participants for training, development and evaluation of the information extraction methods. According to their relevance for biological studies, the annotations are either bound to specific expressions in the text or represented as structured knowledge. Linguistic processing support was provided to the participants in the form of analyses of the dataset texts produced by state-of-the art tools. This paper summarizes the BioNLP-ST 2013 organization, the task characteristics and their relationships. It gives synthetic figures on the participants and discusses the participating system advances. 
2 Tasks The BioNLP-ST?13 includes six tasks from four groups: DBCLS, NaCTeM, NTU and INRA. As opposed to the last edition, all tasks were main extraction tasks. There were no supporting tasks designed to assist the extraction tasks. All tasks share the same event-based representation and file format, which is similar to the previous editions. This makes it easier to reuse the systems across tasks. Five kinds of annotation types are defined: ? T: text-bound annotation (entity/event trigger) ? Equiv: entity aliases ? E: event ? M: event modification ? R: relation ? N: normalization (external reference) The normalization type has been introduced this year to represent the references to external resources such as dictionaries for GRN or ontologies for GRO and BB. The annotations are stand-off: the texts of the documents are kept separate from the annotations that refer to specific spans of texts through character offsets. More detail and examples can be found on the BioNLP-ST?13 web site. 2.1     Genia Event Extraction (GE) Originally the design and implementation of the GE task was based on the Genia event corpus (Kim et al, 2008) that represents domain knowledge of NF?B proteins. It was first organized as the sole task of the initial 2009 edition of BioNLP-ST (Kim et al, 2009). While in 2009 the data sets consisted only of Medline abstracts, in its second edition in 2011 (Kim et al, 2011b), it was extended to include full text articles to measure the generalization of the technology to full text papers. For its third edition this year, the GE task is organized with the goal of making it a more ?real? task useful for knowledge base construction. The first design choice is to construct the data sets with recent full papers only, so that the extracted pieces of information could represent up-to-date knowledge of the domain. Second, the co-reference annotations are integrated into the event annotations, to encourage the use of these co-reference features in the solution of the event extraction. 
2
2.2     Cancer Genetics (CG) The CG task concerns the extraction of events relevant to cancer, covering molecular foundations, cellular, tissue, and organ-level effects, and organism-level outcomes. In addition to the domain, the task is novel in particular in extending event extraction to upper levels of biological organization. The CG task involves the extraction of 40 event types involving 18 types of entities, defined with respect to community-standard ontologies (Pyysalo et al, 2011a; Ohta et al, 2012). The newly introduced CG task corpus, prepared as an extension of a previously introduced corpus of 250 abstracts (Pyysalo et al, 2012), consists of 600 PubMed abstracts annotated for over 17,000 events. 2.3     Pathway Curation (PC) The PC task focuses on the automatic extraction of biomolecular reactions from text with the aim of supporting the development, evaluation and maintenance of biomolecular pathway models. The PC task setting and its document selection protocol account for both signaling and metabolic pathways. The 23 event types, including chemical modifications (Pyysalo et al, 2011b), are defined primarily with respect to the Systems Biology Ontology (SBO) (Ohta et al, 2011b; Ohta et al, 2011c), involving 4 SBO entity types. The PC task corpus was newly annotated for the task and consists of 525 PubMed abstracts, chosen for the relevance to specific pathway reactions selected from SBML models registered in BioModels and PANTHER DB repositories (Mi and Thomas, 2009). The corpus was manually annotated for over 12,000 events on top of close to 16,000 entities.  2.4     Gene Regulation Ontology (GRO) The GRO task aims to populate the Gene Regulation Ontology (GRO) (Beisswanger et al, 2008) with events and relations identified from text. The large size and the complex semantic representation of the underlying ontology are the main challenges of the task. Those issues, to a greater extent, should be addressed to support full-fledged semantic search over the biomedical literature, which is the ultimate goal of this work.   The corpus consists of 300 MEDLINE abstracts, prepared as an extension of (Kim et al, 2011c). The analysis of the inter-annotator agreement between the two annotators shows 
Kappa values of 43%-56%, which might indicate the difficulty of the task.  2.5     Gene Regulation Network in Bacteria           (GRN) The Gene Regulation Network task consists of the extraction of the regulatory network of a set of genes involved in the sporulation phenomenon of the model organism Bacillus subtilis. Participant system predictions are evaluated with respect to the target regulation network, rather than the text-bound relations. The aim is to assess the IE methods with regards to the needs of systems biology and predictive biology studies. The GRN corpus is a set of sentences from PubMed abstracts that extends the BioNLP-ST 2011 BI (Jourde et al, 2011) and LLL (Nedellec, 2005) corpora. The additional sentences cover a wider range of publication dates and complement the regulation network of the sporulation phenomenon. It has been thoroughly annotated with different levels of biological abstraction: entities, biochemical events, genic interactions and the corresponding regulation network. The network prediction submissions have been evaluated against the reference network using an original metric, the Slot Error Rate (Makhoul et al, 1999) that is more adapted to graph comparison than the usual Recall, Precision and F-score measures.  2.6     Bacteria Biotopes (BB) The Bacteria Biotope (BB) task concerns the extraction of locations in which bacteria live and the categorization of these habitats with concepts from OntoBiotope,1 a large ontology of 1,700 concepts and 2,000 synonyms. The association between bacteria and their habitats is essential information for environmental biology studies, metagenomics and phylogeny. In the previous edition of the BB task, participants had to recognize bacteria and habitat entities, to categorize habitat entities among eight broad types and to extract localization relations between bacteria and their habitats (Bossy et al, 2011). The BioNLP-ST 2013 edition has been split into 3 sub-tasks in order to better assess the performance of the predictive systems for each step. The novelty of this task is mainly the more comprehensive and fine-grained categorization. It addresses the critical problem of habitat normalization necessary for the                                                            1 http://bibliome.jouy.inra.fr/MEM-OntoBiotope 
3
automatic exploitation of bacteria-habitat databases.  2.7     Task characteristics Task features are given in Table 1. Three different types of text were considered: the abstracts of scientific papers taken from PubMed (CG, PC, GRO and GRN), full-text scientific papers (GE) and scientific web pages (BB).   Task Documents # types # events GE 34 Full papers  2 13 CG 600 Abstracts 18 40 PC 525 Abstracts 4 23 GRO 300 Abstracts 174  126 GRN 201 Abstracts 6 12 BB 124 Web pages 563 2 Table 1. Characteristics of the BioNLP-ST 2013 tasks. The number of relations or events targeted greatly varies with the tasks as shown in column 3. The high number of types and events reflect the increasing complexity of the biological knowledge to be extracted. The grand theme of Knowledge base construction in this edition has been translated into rich knowledge representations with the goal of integrating textual data with data from sources other than text. These figures illustrate the shared ambition of the organizers to promote fine-grained information extraction together with an increasing biological plausibility. Beyond gene and protein interactions, they include many complex biological phenomena and environmental factors. 3 BioNLP-ST?13 organization  BioNLP-ST?13 was split in three main periods. During thirteen weeks from mid-January to the first week of April, the participants prepared their systems with the training data. Supporting resources were delivered to participants during this period. Supporting resources were provided by the organizers and by three external providers after a public call for contribution. They range from tokenizers to entity detection tools, mostly focusing on syntactic parsing (Enju (Miyao and Tsujii, 2008), Stanford (Klein and Manning, 2002), McCCJ (Charniak and Johnson, 2005)). The test data were made available for 10 days before the participants had to submit their final results using on-line services. The evaluation results were 
communicated shortly after and published on the ST site. The descriptions of the tasks and representative sample data have been available since October 2012 so that the participants could become acquainted with the task goals and data formats in advance. Table 2 shows the task schedule.  Date Event 23 Oct. 2012 Release of sample data sets 17 Jan 2013 Release of the training data sets 06 Apr. 2013 Release of the test data sets 16 Apr. 2013 Result submission 17 Apr. 2013 Notification of the evaluation results Table 2: Schedule of BioNLP-ST 2013. The BioNLP-ST?13 web site and a dedicated mailing-list have kept the participant informed about the whole process.  4 Participation  GE 1-2-3 CG PC GRO GRN BB 1 - 2-3 EVEX ? ? ?    ?    TEES-2.1 ? ? ? ? ? ? ?  ? ? BioSEM ?          NCBI ?          DlutNLP ?          HDS 4NLP ?          NICTA  ?  ?        USheff ?          UZH  ?          HCMUS ?          NaCTeM     ? ?      NCBI     ?       RelAgent     ?       UET-NII     ?       ISI    ?       OSEE      ?     U. of Ljubljana       ?    K.U. Leuven       ?    IRISA-TexMex       ? ? ?  Boun        ? ?  LIPN        ?   LIMSI        ? ? ? Table 3: Participating teams per task. BioNLP-ST 2013 received 38 submissions from 22 teams (Table 3). One third, or seven teams, participated in multiple tasks. Only one team, UTurku, submitted final results with TEES-2.1 to 
4
all the tasks except one ? entity categorization. This broad participation resulted from the growing capability of the systems to be applied to various tasks without manual tuning. The remaining 15 teams participated in one single task. 5 Results  Table 4 summarizes the best results and the participating systems for each task and sub-task. They are all measured using F-scores, except when it is not relevant, in which case SER is used instead. It is noticeable that the TEES-2.1 system that participated in 9 of the 10 tasks and sub-tasks achieved the best result in 6 cases. Most of the participating systems applied a combination of machine learning algorithms and linguistic features, mainly syntactic parses, with some noticeable exceptions.   Tasks Evaluation results  GE Core event extraction TEES-2.1, EVEX, BioSEM:  0.51 GE 2 Event enrichment TEES2.1:  0.32 GE 3 Negation/Speculation TEES-2.1, EVEX:   0.25 CG TEES-2.1:  0.55 PC NaCTeM:  0.53 
GRO TEES-2.1: 0.22 (events),   0.63 (relations) 
GRN U. of Ljubljana:   0.73 (SER) BB 1 Entity detection and categorization IRISA: 0.46 (SER) BB 2 Relation extraction IRISA:  0.40 BB 3 Full event extraction TEES-2.1:  0.14 Table 4. Best results and team per task  (F-score, except when SER). Twelve teams submitted final results to the GE task. The performance of highly ranked systems shows that the event extraction technology is applicable to the most recent full papers without drop of performance. Six teams submitted final results to the CG task. The highest-performing systems achieved 
results comparable to those for established molecular level extraction tasks (Kim et al, 2011). The results indicate that event extraction methods generalize well to higher levels of biological organization and are applicable to the construction of knowledge bases on cancer. Two teams successfully completed the PC task, and the highest F-score reached 52.8%, indicating that event extraction is a promising approach to support pathway curation efforts. The GRN task attracted five participants. The best SER score was 0,73 (the higher, the worse), which shows their capability of designing regulatory network, but handling modalities remains an issue. Five teams participated to the 3 BB subtasks with 10 final submissions. Not surprisingly, the systems achieved better results in relation extraction than habitat categorization, which remains a major challenge in IE. One team participated in the GRO task, and their results were compared with those of a preliminary system prepared by the task organizers. An analysis of the evaluation results leads us to study issues such as the need to consider the ontology structure and the need for semantic analysis, which are not seriously dealt with by current approaches to event extraction. 6 Organization of the workshop The BioNLP Shared Task 2013 (BioNLP-ST) workshop was organized as part of the ACL BioNLP 2013 workshop. After submission of their system results, participants were invited to submit a paper on their systems to the workshop.  Task organizers were also invited to present overviews of each task, with analyses of the participant system features and results. The workshop was held in August 2013 in Sofia (Bulgaria). It included overview presentations on tasks, as well as oral and poster presentations by Shared Task participants.  7 Discussion and Conclusion This year, the tasks has significantly gained in complexity to face the increasing need for Systems Biology knowledge from various textual sources. The high level of participation and the quality of the results show that the maturity of the field is such that it can meet this challenge. The innovative and various solutions applied this year will without doubt be extended in the future. As for previous editions of BioNLP-ST, all tasks maintain an online evaluation service that is 
5
publicly available. This on-going challenge will contribute to the assessment of the evolving information extraction field in the biomedical domain. References  Auhors. 2013. Title. In Proceedings of the BioNLP 2013 Workshop Companion Volume for Shared Task, Sofia, Bulgaria. Association for Computational Linguistics. Arighi, C., Lu, Z., Krallinger, M., Cohen, K., Wilbur, W., Valencia, A., Hirschman, L. and Wu, C. 2011. Overview of the BioCreative III Workshop. BMC Bioinformatics, 12, S1. E Beisswanger, V Lee, JJ Kim, D Rebholz-Schuhmann, A Splendiani, O Dameron, S Schulz, U Hahn. Gene Regulation Ontology (GRO): Design principles and use cases. Studies in Health Technology and Informatics, 136:9-14, 2008. BioNLP-ST?13 web site: https://2013.bionlp-st.org Robert Bossy, Julien Jourde, Philippe Bessi?res, Maarten van de Guchte, Claire N?dellec. 2011. BioNLP shared Tasks 2011 - Bacteria Biotope. In Proceedings of BioNLP 2011 Workshop, pages 65-73. Association for Computational Linguistics, Portland, USA, 2011. Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173?180. Association for Computational Linguistics. Julien Jourde, Alain-Pierre Manine, Philippe Veber, Karen Fort, Robert Bossy, Erick Alphonse, Philippe Bessi?res. 2011. BioNLP Shared Task 2011 - Bacteria Gene Interactions and Renaming. In Proceedings of BioNLP 2011 Workshop, pages 65-73. Association for Computational Linguistics, Portland. Jin-Dong Kim, Tomoko Ohta and Jun'ichi Tsujii, 2008, Corpus annotation for mining biomedical events from literature, BMC Bioinformatics, 9(1): 10. Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano and Jun'ichi Tsujii. 2009. Overview of BioNLP'09 Shared Task on Event Extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 1-9. Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, Ngan Nguyen and Jun?ichi Tsujii. 2011. Overview of BioNLP Shared Task 2011. In Proceedings of BioNLP 2011 Workshop, pages 1-6. Association for Computational Linguistics. 
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori Yonezawa. 2011b. Overview of the Genia Event task in BioNLP Shared Task 2011. In Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task, Portland, Oregon, June. Association for Computational Linguistics. Jung-Jae Kim, Xu Han and Watson Wei Khong Chua. 2011c. Annotation of biomedical text with Gene Regulation Ontology: Towards Semantic Web for biomedical literature. Proceedings of LBM 2011, pp. 63-70. Dan Klein and Christopher D Manning. 2002. Fast ex act inference with a factored model for natural language parsing. Advances in neural information processing systems, 15(2003):3?10. John Makhoul, Francis Kubala, Richard Schwartz and Ralph Weischedel. 1999.  Performance measures for information extraction. In Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February. Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35?80. Huaiyu Mi and Paul Thomas. 2009. PANTHER path- way: an ontology-based pathway database coupled with data analysis tools. In Protein Networks and Pathway Analysis, pages 123?140. Springer. Claire N?dellec. 2005. Learning Language in Logic - Genic Interaction Extraction Challenge. In Proceedings of the Learning Language in Logic (LLL05) workshop joint to ICML'05. Cussens J. and Nedellec C. (eds). Bonn, August. Tomoko Ohta, Sampo Pyysalo, Sophia Ananiadou, and Jun'ichi Tsujii. 2011b. Pathway curation support as an information extraction task. Proceedings of LBM 2011. Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011c. From pathways to biomolecular events: opportunities and challenges. In Proceedings of BioNLP 2011 Workshop, pages 105?113. Association for Computational Linguistics. Tomoko Ohta, Sampo Pyysalo, Jun?ichi Tsujii, and Sophia Ananiadou. 2012. Open-domain anatomical entity mention detection. In Proceedings of DSSD 2012, pages 27?36. Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-Cheol Cho, Jun?ichi Tsujii, and Sophia Ananiadou. 2012. Event extraction across multiple levels of biological organization. Bioinformatics, 28(18):i575-i581. Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, and Jun'ichi Tsujii. 2011b. Towards exhaustive event extraction for protein modifications. In Proceedings of the BioNLP 2011 Workshop, 
6
pp.114-123, Association for Computational Linguistics. Sampo Pyysalo, Tomoko Ohta, Jun'ichi Tsujii and Sophia Ananiadou. 2011a. Anatomical Entity Recognition with Open Biomedical Ontologies. In proceedings of LBM 2011. Isabel Segura-Bedmar, Paloma Martinez, and Daniel Sanchez-Cisneros. 2011. The 1st DDIExtraction-2011 challenge task: Extraction of Drug-Drug Interactions from biomedical texts. In Proceedings of the 1st Challenge Task on Drug-Drug Interaction Extraction 2011, SEPLN 2011 satellite workshop. Huelva, Spain, September 7. Weiyi Sun, Anna Rumshisky, Ozlem Uzuner. 2013. Evaluating temporal relations in clinical text: 2012 i2b2 Challenge. J Am Med Inform Assoc.
7
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 8?15,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
The Genia Event Extraction Shared Task, 2013 Edition - Overview
Jin-Dong Kim and Yue Wang and Yamamoto Yasunori
Database Center for Life Science (DBCLS)
Research Organization of Information and Systems (ROIS)
{jdkim|wang|yy}@dbcls.rois.ac.jp
Abstract
The Genia Event Extraction task is orga-
nized for the third time, in BioNLP Shared
Task 2013. Toward knowledge based con-
struction, the task is modified in a num-
ber of points. As the final results, it re-
ceived 12 submissions, among which 2
were withdrawn from the final report. This
paper presents the task setting, data sets,
and the final results with discussion for
possible future directions.
1 Introduction
Among various resources of life science, litera-
ture is regarded as one of the most important types
of knowledge base. Nevertheless, lack of explicit
structure in natural language texts prevents com-
puter systems from accessing fine-grained infor-
mation written in literature. BioNLP Shared Task
(ST) series (Kim et al, 2009; Kim et al, 2011a)
is one of the community-wide efforts to address
the problem. Since its initial organization in 2009,
BioNLP-ST series has published a number of fine-
grained information extraction (IE) tasks moti-
vated for bioinformatics projects. Having solicited
wide participation from the community of natural
language processing, machine learning, and bioin-
formatics, it has contributed to the production of
rich resources for fine-grained BioIE, e.g., TEES1
(Bjo?rne and Salakoski, 2011), SBEP2 (McClosky
et al, 2011) and EVEX3 (Van Landeghem et al,
2011).
The Genia Event Extraction (GE) task is a sem-
inal task of BioNLP-ST. It was first organized as
the sole task of the initial 2009 edition of BioNLP-
ST. The task was originally designed and imple-
mented based on the Genia event corpus (Kim et
1https://github.com/jbjorne/TEES/wiki
2http://nlp.stanford.edu/software/eventparser.shtml
3http://www.evexdb.org/
al., 2008b) which represented domain knowledge
around NF?B proteins. There were also some ef-
forts to explore the possibility of literature mining
for pathway construction (Kim et al, 2008a; Oda
et al, 2008). The GE task was designed to make
such an effort a community-driven one by sharing
available resources, e.g., benchmark data sets, and
evaluation tools, with the community.
In its second edition (Kim et al, 2011b) orga-
nized in BioNLP-ST 2011 (Kim et al, 2011a), the
data sets were extended to include full text articles.
The data sets consisted of two collections. The ab-
stract collection, that had come from the first edi-
tion, was used again to measure the progress of the
community between 2009 and 2011 editions, and
the full text collection, that was newly created, was
used to measure the generalization of the technol-
ogy to full text papers.
In its third edition this year, while succeeding
the fundamental characteristics from its previous
editions, the GE task tries to evolve with the goal
to make it a more ?real? task toward knowledge
base construction. The first design choice to ad-
dress the goal is to construct the data sets fully
with recent full papers, so that the extracted pieces
of information can represent up-to-date knowl-
edge of the domain. The abstract collection, that
had been already used twice (in 2009 and 2011), is
removed from official evaluation this time4. Sec-
ond, GE task subsumes the coreference task which
has long been considered critical for improvement
of event extraction performance. It is implemented
by providing coreference annotation in integration
with event annotation in the data sets.
The paper explains the task setting and data sets,
presents the final results of participating systems,
and discusses notable observations with conclu-
sions.
4However, if necessary, the online evaluation for the pre-
vious editions of GE task may be used, which is available at
http://bionlp-st.dbcls.jp/GE/.
8
Event Type Primary Argument Secondary Argument
Gene expression Theme(Protein)
Transcription Theme(Protein)
Localization Theme(Protein) Loc(Entity)?
Protein catabolism Theme(Protein)
Binding Theme(Protein)+ Site(Entity)*
Protein modification Theme(Protein), Cause(Protein/Event)? Site(Entity)?
Phosphorylation Theme(Protein), Cause(Protein/Event)? Site(Entity)?
Ubiquitination Theme(Protein), Cause(Protein/Event)? Site(Entity)?
Acetylation Theme(Protein), Cause(Protein/Event)? Site(Entity)?
Deacetylation Theme(Protein), Cause(Protein/Event)? Site(Entity)?
Regulation Theme(Protein/Event), Cause(Protein/Event)? Site(Entity)?, CSite(Entity)?
Positive regulation Theme(Protein/Event), Cause(Protein/Event)? Site(Entity)?, CSite(Entity)?
Negative regulation Theme(Protein/Event), Cause(Protein/Event)? Site(Entity)?, CSite(Entity)?
Table 1: Event types and their arguments for Genia Event Extraction task. The type of each filler entity
is specified in parenthesis. Arguments that may be filled more than once per event are marked with ?+?,
and optional arguments are with ???.
2 Task setting
This section explains the task setting of the 2013
edition of the GE task with a focus on changes to
previous editions. For comprehensive explanation,
readers are referred to Kim et al (2009).
The changes made to the task setting are three-
folds, among which two are about event types
to be extracted. Table 1 shows the event types
and their arguments targeted in the 2013 edition.
First, four new event types are added to the target
of extraction; the Protein modification
type and its three sub-types, Ubiquitination,
Acetylation, Deacetylation. Second,
The Protein modification types are modi-
fied to be directly linked to causal entities, which
was only possible through Regulation events
in previous editions.
The modifications were made based on analy-
sis on preliminary annotation during preparation
of the data sets: in recent papers on NF?B, dis-
cussions on protein modification were observed
with non-trivial frequency. However, in the end,
it turned out that the influence of the above modi-
fications was trivial in terms of the number of an-
notated instances in the final data sets, as shown
in section 3, after filtering out events on non-
individual proteins, e.g., protein families, protein
complexes.
Third change made to the task setting is addition
of coreference and part-of annotations to the data
sets. It is to address the observation from 2009
edition that coreference structures and entity rela-
tions often hide the syntactic paths between event
triggers and their arguments, restricting the perfor-
mance of event extraction. In 2011, the Protein
coreference task and Entity Relation were orga-
nized as sub-tasks, to explicitly address the prob-
lem, but this time, coreference and part-of anno-
tations are integrated in the GE task, to encour-
age an integrative use of them for event extrac-
tion. Figure 1 shows an example of annotation
with coreference and part-of annotations5. Note
that the event representation in the figure is re-
lation centric6, which is different from the event
centric representation of the default BioNLP-ST
format. The two representations are interchange-
able, and the GE task provides data sets in both
formats, together with an automatic converter be-
tween them. Below is the corresponding annota-
tion in the BioNLP-ST format:
T8 Protein 933 938 TRAF1
T9 Protein 940 945 TRAF2
T10 Protein 947 952 TRAF3
T11 Protein 958 963 TRAF6
T12 Protein 1038 1042 CD40
T41 Anaphora 1058 1072 These proteins
T48 Binding 1112 1119 binding
T49 Entity 1127 1143 cytoplasmic tail
T13 Protein 1147 1151 CD40
R1 Coreference Subject:T41 Object:T8
R2 Coreference Subject:T41 Object:T9
R3 Coreference Subject:T41 Object:T10
R4 Coreference Subject:T41 Object:T11
E4 Binding:T48 Theme:T8 Theme2:T13 Site2:T49
E5 Binding:T48 Theme:T9 Theme2:T13 Site2:T49
E6 Binding:T48 Theme:T10 Theme2:T13 Site2:T49
E7 Binding:T48 Theme:T11 Theme2:T13 Site2:T49
In the example, the event trigger, binding, de-
notes four binding events, in which the four pro-
teins, TRAF1, TRAF2, TRAF3, and TRAF6, bind
to the protein, CD40, respectively, through the
site, cytoplasmic tail. The links between the four
5The example is taken from the file, PMC-3148254-01-
Introduction.
6PubAnnotation (http://pubannotation.org) format.
9
Figure 1: Annotation example with coreferences and part-of relationship
proteins and the event trigger are however very
hard to find, without being bridged by the demon-
strative noun phrase (NP), These proteins. In the
case, if the link between the demonstrative NP,
These proteins and its four antecedents, TRAF1,
TRAF2, TRAF3, and TRAF6, can be somehow de-
tected, the remaining link, between the demonstra-
tive NP and the trigger, may be detected by their
syntactic connection. A key point here is the dif-
ferent characteristics of the two step links: de-
tecting the former is rather semantic or discour-
sal while the latter may be a more syntactic prob-
lem. Then, solving them using different processes
would make a sense. To encourage an exploration
into the hypothesis, the coreference annotation is
provided in the training and development data sets.
Based on the definition of event types, the en-
tire task is divided into three sub-tasks addressing
event extraction at different levels of specificity:
Task 1. Core event extraction addresses the ex-
traction of typed events together with their
primary arguments.
Task 2. Event enrichment addresses the extrac-
tion of secondary arguments that further
specify the events extracted in Task 1.
Task 3. Negation/Speculation detection
addresses the detection of negations and
speculations over the extracted events.
For more detail of the subtasks, readers are re-
ferred to Kim et al (2011b).
Item Training Devel Test
Articles 10 10 14
Words 54938 57907 75144
Proteins 3571 4138 4359
Entities 121 314 327
Events 2817 3199 3348
Gene expression 729 591 619
Transcription 122 98 101
Localization 44 197 99
Protein catabolism 23 30 14
Binding 195 376 342
Protein modification 8 1 1
Phosphorylation 117 197 161
Ubiquitination 4 2 30
Acetylation 0 3 0
Deacetylation 0 5 0
Regulation 299 284 299
Positive regulation 780 883 1144
Negative regulation 496 532 538
Coreferences 178 160 197
to Protein 152 123 169
to Entity 5 6 6
to Event 18 27 13
to Anaphora 3 4 9
Table 2: Statistics of annotations in training, de-
velopment, and test sets
3 Data Preparation
As discussed in section 1, for the 2013 edition, the
data sets are constructed fully with full text pa-
pers. Table 2 shows statistics of three data sets for
training, development and test. The data sets con-
sist of 34 full text papers from the Open Access
subset of PubMed Central. The papers were re-
trieved using lexical variants of the term, ?NF?B?
as primary keyword, and ?pathway? and ?regula-
tion? as secondary keywords. The retrieved papers
were given to the annotators with higher priority
10
Item TIAB Intro. R/D/C Methods Caption all
Words 10483 25543 125172 59612 29085 263133
Proteins 816 1507 9060 1797 2169 16427
(Density: P / W) (7.78%) (5.90%) (7.24%) (3.01%) (7.46%) (6.24%)
Prot. Coreferences 18 89 267 5 33 445
(Density: C / P) (2.21%) (5.91%) (2.95%) (0.28%) (1.52%) (2.71%)
Events 510 902 6391 311 892 9364
(Density: E / W) (4.87%) (3.53%) (5.11%) (0.52%) (3.07%) (3.56%)
(Density: E / P) (62.50%) (59.85%) (70.54%) (17.31%) (41.12%) (57.00%)
Gene expression 101 152 1265 125 220 1939
Transcription 10 18 209 36 47 321
Localization 19 47 191 8 41 340
Protein catabolism 0 3 49 0 8 67
Binding 29 158 572 15 92 913
Protein modification 1 1 7 0 0 10
Phosphorylation 27 38 347 19 35 475
Ubiquitination 0 2 8 0 10 36
Acetylation 0 3 0 0 0 3
Deacetylation 0 5 0 0 0 5
Regulation 67 76 625 7 66 882
Positive regulation 167 286 2045 19 203 2807
Negative regulation 89 113 1073 69 170 1566
Table 3: Statistics of annotations in different sections of text: the Abstract column is of the abstraction
collection (1210 titles and abstracts), and the following columns are of full paper collection (14 full
papers). TIAB = title and abstract, Intro. = introduction and background, R/D/C = results, discussions,
and conclusions, Methods = methods, materials, and experimental procedures. Some minor sections,
supporting information, supplementary material, and synopsis, are ignored. Density = relative density of
annotation (P/W = Protein/Word, E/W = Event/Word, and E/P = Event/Protein).
Figure 2: Event distribution in different sections
to newer ones. Note that among 34 papers, 14
were from the full text collection of 2011 edition
data sets, and 20 were newly collected this time.
The annotation to the all 34 papers were produced
by the same annotators who also produced anno-
tations for the previous editions of GE task.
The annotated papers are divided into the train-
ing, development, and test data sets; 10, 10, and
14, respectively. Note that the size of the training
data set is much smaller than previous editions,
in terms of number of words and events, while
the size of the development and test data sets are
comparable to previous editions. It is the conse-
quence of a design choice of the organizers with
the notion that (1) relevant resources are substan-
tially accumulated through last two editions, and
that (2) therefore the importance of training data
set may be reduced while the importance of devel-
opment and test data sets needs to be kept. Instead,
participants may utilize, for example, the abstract
collection of the 2011 edition, of which the anno-
tation was produced by the same annotators with
almost same principles. As another example, the
data sets of the EPI task (Ohta et al, 2011) also
may be utilized for the newly added protein modi-
fication events.
Table 3 shows the statistics of annotated event
types in different sections of the full papers in the
data sets. For the analysis, the sections are classi-
fied to five groups as follows:
? The TIAB group includes the titles and
abstracts. In the GE-2011 data sets,
the corresponding files match the pattern,
PMC-*TIAB*.txt.
? The Intro group includes sections
for introduction, and background. The
corresponding files match the pattern,
PMC-*@(-|._)@(I|Back)*.txt.
11
Team ?09 ?11 Task Expertise
EVEX UTurku 123 2C+2BI+1B
TEES-2.1 UTurku 123 2BI
BioSEM TM-SCS 1-- 1C+1BI
NCBI CCP-BTMG 1-- 3BI
DlutNLP 1-- 3C
HDS4NLP 1-- 3C
NICTANLM CCP-BTMG 1-3 6C
USheff 1-- 2C
UZH UZurich 1-- 6C
HCMUS HCMUS 1-- 4C
Table 4: Team profiles: The ?09 and ?11 columns
show the predecessors in 2009 and 2011 editions.
In Expertise column, C=Computer Scientist,
BI=Bioinformatician, B=Biologist, L=Linguist
? The R/D/C group includes sections
on results, discussions, and conclu-
sions. The files match the pattern,
PMC-*@(-|._)@(R|D|Conc)*.txt
? The Methods group includes sections on
methods, materials, and experimental pro-
cedures. The files match the pattern,
PMC-*@(-|._)@(Met|Mat|MAT|E)*.txt
? The Caption group includes the captions of
tables and figures. The corresponding files
math the pattern, PMC-*aption*.txt.
Figure 2 illustrates the different distribution of
annotated event types in the five section groups.
It shows that the Methods group has signifi-
cantly different distribution of annotated events,
confirming a similar observation reported in Kim
et al (2011b).
4 Participation
The GE task received final submissions from 12
teams, among which 2 were withdrawn from final
report. Table 4 summarizes the teams. Unfortu-
nately, the subtasks 2 and 3 did not met a large
participation.
Table 5 profiles the participating systems. The
systems are roughly grouped into SVM-based
pipeline (EVEX, TEES-2.1, and DlutNLP),
rule-based pipeline (BioSEM and UZH), mixed
pipeline (USheff and HCMUS), joint pattern
matching (NCBI and NICTANLM), and joint SVM
(HDS4NLP) systems. In terms of use of ex-
ternal resources, 5 teams (EVEX, TEES-2.1,
NCBI, DlutNLP, and USheff) utilized data sets
from 2011 edition, and two teams (HDS4NLP and
NICTANLM) utilized independent resources, e.g.,
UniProt (Bairoch et al, 2005), IntAct (Kerrien et
al., 2012), and CRAFT (Verspoor et al, 2012).
5 Results and Discussions
Table 6 shows the final results of subtask 1. Over-
all EVEX, TEES-2.1, and BioSEM show the
best performance only with marginal difference
between them. In detail, the performance of
BioSEM is significantly different from EVEX and
TEES-2.1: (1) while BioSEM show the best per-
formance with Binding and Protein modification
events, EVEX and TEES-2.1 show the best per-
formance with Regulation events which takes the
largest portion of annotation in data sets; and (2)
while the performance of EVEX and TEES-2.1
is balanced over recall and precision, BioSEM is
biased for precision, which is a typical feature of
rule-based systems. It is also notable that BioSEM
has achieved a near best performance using only
shallow parsing. Although it is not shown in the
table, NCBI is the only system which produced
Ubiquitination events, which is interpreted
as a result of utilizing 2011-EPI data sets (Ohta et
al., 2011) for the system development.
Table 7 shows subtask 1 final results only within
TIAB sections. It shows that the systems de-
veloped utilizing previous resources, e.g., 2011
data sets, and EVEX, perform better for titles and
abstracts, which makes sense because those re-
sources are title and abstract-centric.
Tables 8 and 9 show evaluation results within
Methods and Captions section groups, respec-
tively. All the systems show their worst per-
formance in the two section groups. Especially
the drop of performance with regulation events is
huge. Note the two section groups also show sig-
nificantly different event distribution compared to
other section groups (see section 3). It suggests
that language expression in the two section groups
may be quite different from other sections, and an
extensive examination is required to get a reason-
able performance in the sections.
Table 10 and 11 show final results of Task 2
(Event enrichment) and 3 (Negation/Speculation
detection), respectively, which unfortunately did
not meet a large participation.
6 Conclusions
In its third edition, the GE task is fully changed
to a full text paper centric task, while the online
evaluation service on the abstract-centric data sets
12
NLP Task Other resources
Team Lexical Proc. Syntactic Proc. Trig. Arg. group Dic. Other
EVEX Porter McCCJ SVM SVM SVM S. cues EVEX
TEES-2.1 Porter McCCJ SVM SVM SVM S. cues
BioSEM OpenNLP, LingPipe OpenNLP(shallow) dic rules rules
NCBI MedPost, BioLemm McCCJ Subgraph Isomorphism rules 2011 GE / EPI
DlutNLP Porter, GTB-tok McCCJ SVM SVM rules 2011 GE
HDS4NLP CNLP, Morpha McCCJ SVM SVM UniProt, IntAct
NICTANLM ClearParser Subgraph Isomorphism rules CRAFT, EVEX
USheff Porter, LingPipe Stanford dic SVM SVM, rules 2011 GE
UZH Porter, Morpha, LingPipe LTT2, Pro3Gres dic. MaxEnt rules rules
HCMUS SnowBall McCCJ dic, SVM rules, SVM rules
Table 5: System profiles: SnowBall=SnowBall Stemmer, CNLP=Stanford CoreNLP (tokenization),
McCCJ=McClosky-Charniak-Johnson Parser, Stanford=Stanford Parser, S.=Speculation, N.=Negation
Team Simple Event Binding Prot-Mod. Regulation All
EVEX 73.83 / 79.56 / 76.59 41.14 / 44.77 / 42.88 61.78 / 69.41 / 65.37 32.41 / 47.16 / 38.41 45.44 / 58.03 / 50.97
TEES-2.1 74.19 / 79.64 / 76.82 42.34 / 44.34 / 43.32 63.87 / 69.32 / 66.49 33.08 / 44.78 / 38.05 46.17 / 56.32 / 50.74
BioSEM 67.71 / 86.90 / 76.11 47.45 / 52.32 / 49.76 69.11 / 80.49 / 74.37 28.19 / 49.06 / 35.80 42.47 / 62.83 / 50.68
NCBI 72.99 / 72.12 / 72.55 37.54 / 41.81 / 39.56 64.92 / 77.02 / 70.45 24.74 / 55.61 / 34.25 40.53 / 61.72 / 48.93
DlutNLP 69.15 / 80.56 / 74.42 40.84 / 44.16 / 42.43 62.83 / 77.42 / 69.36 26.49 / 43.46 / 32.92 40.81 / 57.00 / 47.56
HDS4NLP 75.27 / 83.27 / 79.07 41.74 / 33.74 / 37.32 70.68 / 75.84 / 73.17 16.67 / 30.86 / 21.64 37.11 / 51.19 / 43.03
NICTANLM 73.59 / 57.67 / 64.66 32.13 / 31.10 / 31.61 42.41 / 72.97 / 53.64 21.60 / 47.14 / 29.63 36.99 / 50.68 / 42.77
USheff 54.50 / 80.07 / 64.86 31.53 / 46.88 / 37.70 39.79 / 92.68 / 55.68 21.14 / 52.69 / 30.18 31.69 / 63.28 / 42.23
UZH 60.26 / 77.47 / 67.79 22.22 / 28.03 / 24.79 62.30 / 70.83 / 66.30 11.06 / 31.02 / 16.31 27.57 / 51.33 / 35.87
HCMUS 67.47 / 60.24 / 63.65 38.74 / 26.99 / 31.81 64.92 / 57.67 / 61.08 19.60 / 19.93 / 19.76 36.23 / 33.80 / 34.98
Table 6: Evaluation results (recall / precision / f-score) of Task 1. Some notable figures are emphasized
in bold.
is kept maintained. Unfortunately, the corefer-
ence annotation, which has been integrated in the
event annotation in the data sets, was not exploited
by the participants, during the official shared task
period. An analysis shows that the performance
of systems significantly drops in the Methods and
Captions sections, suggesting for an extensive ex-
amination in the sections.
As usual, after the official shared task period,
the GE task is maintaining an online evaluation
that can be freely accessed by anyone but with
a time limitation; once in 24 hours per a per-
son. With a few new features that are introduced
in 2013 editions but are not fully exploited by
the participants, the organizers solicit participants
to continuously explore the task using the online
evaluation. The organizers are also planning to
provide more resources to the participants, based
on the understanding that interactive communica-
tion between organizers and participants is impor-
tant for progress of the participating systems and
also the task itself.
References
Amos Bairoch, Rolf Apweiler, Cathy H. Wu,
Winona C. Barker, Brigitte Boeckmann, Serenella
Ferro, Elisabeth Gasteiger, Hongzhan Huang, Ro-
drigo Lopez, Michele Magrane, Maria J. Mar-
tin, Darren A. Natale, Claire O?Donovan, Nicole
Redaschi, and Lai-Su L. Yeh. 2005. The universal
protein resource (uniprot). Nucleic Acids Research,
33(suppl 1):D154?D159.
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 183?
191, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Samuel Kerrien, Bruno Aranda, Lionel Breuza, Alan
Bridge, Fiona Broackes-Carter, Carol Chen, Mar-
garet Duesbury, Marine Dumousseau, Marc Feuer-
mann, Ursula Hinz, Christine Jandrasits, Rafael C.
Jimenez, Jyoti Khadake, Usha Mahadevan, Patrick
Masson, Ivo Pedruzzi, Eric Pfeiffenberger, Pablo
Porras, Arathi Raghunath, Bernd Roechert, Sandra
Orchard, and Henning Hermjakob. 2012. The in-
tact molecular interaction database in 2012. Nucleic
Acids Research, 40(D1):D841?D846.
Jin-Dong Kim, Tomoko Ohta, Kanae Oda, and Jun?ichi
Tsujii. 2008a. From text to pathway: corpus annota-
tion for knowledge acquisition from biomedical lit-
erature. In Proceedings of the 6th Asia Pacific Bioin-
formatics Conference, Series on Advances in Bioin-
13
formatics and Computational Biology, pages 165?
176. Imperial College Press.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008b. Corpus annotation for mining biomedical
events from lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing
in Biomedicine (BioNLP) NAACL 2009 Workshop,
pages 1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association
for Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia
Event task in BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1626?
1635, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Kanae Oda, Jin-Dong Kim, Tomoko Ohta, Daisuke
Okanohara, Takuya Matsuzaki, Yuka Tateisi, and
Jun?ichi Tsujii. 2008. New challenges for text min-
ing: Mapping between text and manually curated
pathways.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsu-
jii. 2011. Overview of the Epigenetics and Post-
translational Modifications (EPI) task of BioNLP
Shared Task 2011. In Proceedings of the BioNLP
2011 Workshop Companion Volume for Shared Task,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Sofie Van Landeghem, Filip Ginter, Yves Van de Peer,
and Tapio Salakoski. 2011. Evex: A pubmed-scale
resource for homology-based generalization of text
mining predictions. In Proceedings of BioNLP 2011
Workshop, pages 28?37, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Karin Verspoor, Kevin Cohen, Arrick Lanfranchi,
Colin Warner, Helen Johnson, Christophe Roeder,
Jinho Choi, Christopher Funk, Yuriy Malenkiy,
Miriam Eckert, Nianwen Xue, William Baumgart-
ner, Michael Bada, Martha Palmer, and Lawrence
Hunter. 2012. A corpus of full-text journal articles
is a robust evaluation tool for revealing differences
in performance of biomedical natural language pro-
cessing tools. BMC Bioinformatics, 13(1):207.
14
Team Simple Event Binding Prot-Mod. Regulation All
EVEX 91.67 / 88.00 / 89.80 55.56 / 62.50 / 58.82 85.71 / 75.00 / 80.00 51.18 / 59.09 / 54.85 62.83 / 68.18 / 65.40
TEES-2.1 91.67 / 88.00 / 89.80 55.56 / 62.50 / 58.82 85.71 / 75.00 / 80.00 51.18 / 57.02 / 53.94 62.83 / 66.67 / 64.69
NCBI 81.25 / 79.59 / 80.41 55.56 / 45.45 / 50.00 85.71 / 66.67 / 75.00 37.01 / 67.14 / 47.72 50.79 / 69.78 / 58.79
BioSEM 83.33 / 88.89 / 86.02 66.67 / 66.67 / 66.67 85.71 / 75.00 / 80.00 35.43 / 54.22 / 42.86 50.79 / 66.90 / 57.74
DlutNLP 87.50 / 93.33 / 90.32 44.44 / 50.00 / 47.06 85.71 / 85.71 / 85.71 37.01 / 51.09 / 42.92 51.83 / 65.13 / 57.73
USheff 81.25 / 88.64 / 84.78 44.44 / 57.14 / 50.00 71.43 / 71.43 / 71.43 29.13 / 56.06 / 38.34 44.50 / 68.55 / 53.97
NICTANLM 93.75 / 57.69 / 71.43 22.22 / 25.00 / 23.53 42.86 /100.00 / 60.00 29.92 / 49.35 / 37.25 46.07 / 53.01 / 49.30
HDS4NLP 93.75 / 90.00 / 91.84 66.67 / 54.55 / 60.00 85.71 / 85.71 / 85.71 19.69 / 31.65 / 24.27 42.93 / 55.78 / 48.52
HCMUS 93.75 / 69.23 / 79.65 33.33 / 27.27 / 30.00 71.43 / 41.67 / 52.63 27.56 / 25.36 / 26.42 46.07 / 38.94 / 42.21
UZH 72.92 / 79.55 / 76.09 44.44 / 57.14 / 50.00 71.43 / 71.43 / 71.43 11.02 / 32.56 / 16.47 30.37 / 57.43 / 39.73
Table 7: Evaluation results (recall / precision / f-score) of Task 1 in titles and abstracts. Some notable
figures are emphasized in bold.
Team Simple Event Binding Prot-Mod. Regulation All
BioSEM 70.83 / 90.44 / 79.44 48.24 / 53.93 / 50.93 74.17 / 82.41 / 78.07 28.74 / 51.25 / 36.83 42.97 / 64.90 / 51.70
EVEX 73.51 / 83.26 / 78.08 43.72 / 47.80 / 45.67 66.67 / 66.12 / 66.39 32.79 / 46.79 / 38.56 45.29 / 58.05 / 50.88
TEES-2.1 74.09 / 83.37 / 78.46 43.72 / 47.80 / 45.67 66.67 / 65.04 / 65.84 33.24 / 44.48 / 38.04 45.70 / 56.34 / 50.46
NCBI 74.28 / 75.59 / 74.93 38.19 / 45.24 / 41.42 67.50 / 81.82 / 73.97 24.69 / 55.46 / 34.17 40.01 / 63.56 / 49.11
DlutNLP 70.06 / 84.49 / 76.60 39.20 / 44.32 / 41.60 67.50 / 74.31 / 70.74 27.78 / 43.23 / 33.83 41.01 / 56.70 / 47.60
NICTANLM 75.24 / 57.14 / 64.95 35.68 / 41.76 / 38.48 52.50 / 76.83 / 62.38 22.33 / 46.83 / 30.24 37.73 / 52.30 / 43.84
USheff 56.81 / 80.43 / 66.59 32.66 / 48.15 / 38.92 45.00 / 94.74 / 61.02 21.67 / 53.55 / 30.85 32.27 / 63.93 / 42.89
HDS4NLP 76.20 / 84.65 / 80.20 41.21 / 38.14 / 39.61 75.83 / 75.21 / 75.52 16.58 / 30.16 / 21.40 36.19 / 51.26 / 42.42
UZH 63.53 / 78.25 / 70.13 23.12 / 28.75 / 25.63 66.67 / 74.07 / 70.18 10.61 / 29.39 / 15.59 27.36 / 50.89 / 35.58
HCMUS 67.18 / 62.84 / 64.94 38.19 / 28.15 / 32.41 67.50 / 61.83 / 64.54 19.45 / 20.11 / 19.78 35.09 / 33.95 / 34.51
Table 8: Evaluation results (recall / precision / f-score) of Task 1 in Methods section group. Some notable
figures are emphasized in bold.
Team Simple Event Binding Prot-Mod. Regulation All
TEES-2.1 76.67 / 67.65 / 71.88 53.19 / 46.30 / 49.50 60.61 / 76.92 / 67.80 22.68 / 39.29 / 28.76 43.41/53.74 / 48.02
BioSEM 60.00 / 78.26 / 67.92 68.09 / 58.18 / 62.75 69.70 / 82.14 / 75.41 23.20 / 34.35 / 27.69 42.31/54.42 / 47.60
EVEX 76.67 / 67.65 / 71.88 53.19 / 46.30 / 49.50 48.48 / 72.73 / 58.18 21.13 / 39.81 / 27.61 41.48/53.74 / 46.82
DlutNLP 70.00 / 67.02 / 68.48 55.32 / 48.15 / 51.49 57.58 / 79.17 / 66.67 18.04 / 46.67 / 26.02 39.29/57.89 / 46.81
NCBI 80.00 / 58.54 / 67.61 40.43 / 41.30 / 40.86 66.67 / 70.97 / 68.75 14.95 / 44.62 / 22.39 39.01/53.58 / 45.15
HDS4NLP 78.89 / 78.02 / 78.45 48.94 / 29.49 / 36.80 66.67 / 68.75 / 67.69 06.19 / 14.63 / 08.70 35.16/45.23 / 39.57
UZH 57.78 / 68.42 / 62.65 23.40 / 26.19 / 24.72 69.70 / 74.19 / 71.88 12.89 / 43.10 / 19.84 30.49/53.62 / 38.88
USheff 47.78 / 74.14 / 58.11 36.17 / 45.95 / 40.48 30.30 /100.00 / 46.51 13.40 / 45.61 / 20.72 26.37/59.26 / 36.50
NICTANLM 75.56 / 53.12 / 62.39 40.43 / 27.94 / 33.04 18.18 / 54.55 / 27.27 11.34 / 36.67 / 17.32 31.59/43.07 / 36.45
HCMUS 73.33 / 52.80 / 61.40 53.19 / 25.51 / 34.48 63.64 / 53.85 / 58.33 15.46 / 17.96 / 16.62 39.01/33.10 / 35.81
Table 9: Evaluation results (recall / precision / f-score) of Task 1 in Captions section group. Some notable
figures are emphasized in bold.
Team Site-Binding Site-Phosphorylation Loc-Localization Total
TEES-2.1 31.37 / 56.14 / 40.25 37.21 / 82.05 / 51.20 36.67 / 78.57 / 50.00 22.03 / 61.90 / 32.50
EVEX 31.37 / 56.14 / 40.25 32.56 / 80.00 / 46.28 36.67 / 78.57 / 50.00 20.90 / 61.67 / 31.22
Table 10: Evaluation results (recall / precision / f-score) of Task 2
Team Negation Speculation Total
TEES-2.1 21.68 / 36.84 / 27.30 18.46 / 33.96 / 23.92 19.53 / 35.59 / 25.22
EVEX 20.98 / 38.03 / 27.04 18.46 / 32.73 / 23.61 19.82 / 34.41 / 25.15
NICTANLM 15.38 / 32.76 / 20.94 14.36 / 34.15 / 20.22 14.79 / 33.57 / 20.54
Table 11: Evaluation results (recall / precision / f-score) of Task 3
15
