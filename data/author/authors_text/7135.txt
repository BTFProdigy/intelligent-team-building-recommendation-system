Experiments with Corpus-based LFG Specialization 
Nico la  Cancedda and  Chr i s te r  Samuelsson  
Xerox Research Centre Europe, 
6, chemin de Maupertuis  
38240 Meylan, France 
{cancedda.lsamuelsson}@xrce.xerox.com 
Abst rac t  
Sophisticated grammar formalisms, uch as LFG, al- 
low concisely capturing complex linguistic phenom- 
ena. The powerful operators provided by such for- 
malisms can however introduce spurious ambigu- 
ity, making parsing inefficient. A simple form of 
corpus-based grammar pruning is evaluated experi- 
mentally on two wide-coverage grammars, one En- 
giish and one French. Speedups of up to a factor 6 
were obtained, at a cost in grammatical coverage of 
about 13%. A two-stage architecture allows achiev- 
ing significant speedups without introducing addi- 
tional parse failures. 
1 In t roduct ion  
Expressive grammar formalisms allow grammar de- 
velopers to capture complex linguistic generaliza- 
tions concisely and elegantly, thus greatly facilitat- 
ing grammar development and maintenance. (Car- 
rol, 1994) found that the empirical performance 
when parsing with unification-based grammars is 
nowhere near the theoretical worst-case complexity. 
Nonetheless, directly parsing with such grammars, 
in the form they were developed, can be very ineffi- 
cient. For this reason, grammars are typically com- 
piled into representations that allow faster parsing. 
This does however not solve the potential problem 
of the grammars overgenerating considerably, thus 
allowing large amounts of spurious ambiguity. In- 
deed, a current rend in high-coverage parsing, es- 
pecially when employing a statistical model of lan- 
guage, see, e.g., (Collins 97), is to allow the grammar 
to massively overgenerate and instead isambiguate 
by statistical means during or after parsing. If the 
benefits resulting from more concise grammatical de- 
scriptions are to outweigh the costs of spurious am- 
biguity, the latter must be brought down. 
In such a situation, corpus-based compilation 
techniques can drastically improve parsing perfor- 
mance without burdening the grammar developer. 
The initial, and much seminal work in this area 
was been carried out by Rayner and coworkers, see 
(Rayner 1988), (Samuelsson and Rayner 91) and 
(Rayner and Carter 1996). In the current article, 
we apply similar ideas to Lexical Functional Gram- 
mar (LFG) in the incarnation of the Xerox Linguis- 
tic Environment (XLE). The goal is to investigate 
to what extent corpus-based compilation techniques 
can reduce overgeneration a d spurious ambiguity, 
and increase parsing efficiency, without jeopardiz- 
ing coverage. The rest of the article is organized 
as follows: Section 2 presents the relevant aspects 
of the LFG formalism and the pruning strategy em- 
ployed, Section 3 describes the experimental setup, 
Section 4 reports the experimental results and Sec- 
tion 5 relates this to other work. 
2 LFG and  Grammar  Prun ing  
The LFG formalism (Kaplan and Bresnan, 1982) al- 
lows the right-hand sides (RHS) of grammar rules to 
consist of a regular expression over grammar sym- 
bols. This makes it more appropriate to refer to 
the grammar rules as rule schemata, since each RHS 
can potentially be expanded into a (possibly infinite) 
number of distinct sequences of grammar symbols, 
each corresponding to a traditional phrase-structure 
rule. As can easily be imagined, the use of regular- 
expression operators uch as Kleene-star and com- 
plementation may introduce a considerable amount 
of spurious ambiguity. Moreover, the LFG formal- 
ism provides operators which - -  although not in- 
creasing its theoretical expressive power - -  allow 
rules to be written more concisely. Examples of such 
operators are the ignore operator, which allows skip- 
ping any sequence of grammar symbols that matches 
a given pattern; the shuffle operator, which allows 
a set of grammar symbols to occur in any order; 
and the linear precedence operator, which allows par- 
tially specifying the order of grammar symbols. 
The pruning method we propose consists in elim- 
inating complex operators from the grammar de- 
scription by considering how they were actually in- 
stantiated when parsing a corpus. In LFGs, each 
rule scheme corresponds to a particular grammar 
symbol, since different expansions of the same sym- 
bol are expressed as alternatives in the regular ex- 
pression on its RHS. We can define a specific path 
through the RHS of a rule scheme by the choices 
~tf~ 211 204
made when matching it against some sequence of 
grammar symbols. Our training data allows us to 
derive, for each training example, the choices made 
at each rule expansion. By applying these choices to 
the rule scheme in isolation, we can derive a phrase- 
structure rule from it,. 
The grammar is specialized, or pruned, by retain- 
ing all and only those phrase-structure ules that 
correspond to a path taken through a rule scheme 
when expanding some node in some training exam- 
ple. Since the grammar formalism requires that each 
LHS occur only in one rule scheme in the gram- 
mar, extracted rules with the same LHS symbol are 
merged into a single rule scheme with a disjunction 
operator at its top level. For instance, if a rule 
scheme with the structure 
A ~ B*{CI D} 
is expanded in the training data only in the following 
ways 
A -> C 
A --+ BC 
A -+ BD 
then it will be replaced by a rule scheme with the 
following structure 
A --+ {C IBC\ ]BD} 
The same approach is taken to replace all regular- 
expression operators, other than concatenation, with 
the actual sequences of grammar symbols that are 
matched against them. A more realistic example, 
taken from the actual data, is shown in Figure 1: 
none of the optional alternative portions following 
the V is ever used in any correct parse in the corpus. 
Moreover, the ADVP preceding the V occurs only 
0 or 1 times in correct parses. 
Like other unification-based formalisms, lexical 
functional grammars allow grammar ules to be an- 
notated with sets of feature-based constraints, here 
called "functional descriptions", whose purpose is 
both to enforce additional constraints on rule appli- 
cability and to build an enriched predicate-argument 
structure called "f-structure", which, together with 
the parse tree, constitutes the output of the parsing 
process. As these constraints are maintained verba- 
tim in the specialized version of the rule scheme, this 
poses no problem for this form of grammar pruning. 
3 Exper imenta l  Setup  
The experiments carried out to determine the ef- 
fectiveness of corpus-based specialization were per- 
formed as illustrated in Figure 2. Two broad- 
coverage LFG grammars were used, one for French 
and one for English, both of which were developed 
within the Pargram project (Butt et al, 1999) dur- 
ing several years time. The French grammar consists 
of 133 rule schemata, the English grammar of 8.5 rule 
schemata. 
Each gralmnar is equipped with a treebank, which 
was developed for other purposes than grammar spe- 
cialization. Each treebank was produced by letting 
the system parse a corpus of technical documenta- 
tion. Any sentence that did not obtain any parse 
was discarded. At this point, the French corpus 
was reduced to 960 sentences, and the English cor- 
pus to 970. The average sentence length was 9 for 
French and 8 for English. For each sentence, a hu- 
man expert then selected the most appropriate anal- 
ysis among those returned by the parser. 
In the current experiments, each treebank was 
used to specialize the grammar it had been devel- 
oped with. A set of 10-fold cross-validation experi- 
ments was carried out to measure several interesting 
quantities under different conditions. This means 
that, for each language, the corpus was randomly 
split into ten equal parts, and one tenth at a time 
was held out for testing while the remaining nine 
tenths were used to specialize the grammar, and the 
results were averaged over the ten runs.. For each 
grammar the average number of parses per sentence, 
the fraction of sentences which still received at least 
one parse (angparse) and the fraction of sentences for 
which the parse selected by the expert was still de- 
rived (coverage) were measured 1. The average CPU 
time required by parsing was also measured, and this 
was used to compute the speedup with respect o the 
original grammar. 
The thus established results constitute one data 
point in the trade-off between ambiguity reduction 
on one side, which is in turn related to parsing speed, 
and loss in coverage on the other. In order to deter- 
mine other points of this trade-off, the same set. of 
experiments was performed where speciMization was 
inhibited for certain rule schemata. In particular, for 
each grammar, the two rule schemata that received 
the largest number of distinct expansions in the cor- 
pora were determined. These proved to be those 
associated with the LHS symbols 'VPverb\[main\]' 
and 'NP'  for the French grammar, and 'VPv'  and 
'NPadj'  for the English one. 2 The experiments were 
repeated while inhibiting specialization of first the 
scheme with the most expansions, and then the two 
most expanded schemata. 
Measures of coverage and speedup are important 
1 As long  as we are  in teres ted  in preserv ing  the  f - s t ructure  
ass igned to  sentences ,  th i s  not ion  of  coverage  is s t r i c te r  than  
necessary .  The  same f - s t ructure  can  in fac t  be  ass igned by  
more  than  one  parse ,  so that  in  some cases  a sentence  is con-  
s idered  out  of  coverage  ven if  the  spec ia l i zed  grammar  ass igns  
to  it  the  cor rect  f - s t ruc ture .  
2 'VPv '  and  'VPverb \ [main \ ] '  cover  VPs  headed by  a main  
verb .  'NPad j '  covers  NPs  w i th  ad jec t ives  a t tached.  
205
The original rule: 
l/Pperfp --+ 
ADVP* 
SE (t ADJUNCT) 
($ ADV_TYPE) = t,padv 
~/r 
{ @M_Head_Perfp I@M_Head_Passp } 
@( Anaph_Ctrl $) 
{ AD VP+ 
SE ('~ ADJUNCT) 
($ ADV_TYPE) = vpadv 
is replaced by the following: 
ADVP 
,\[.E (~ ADJUNCT) 
(.l. ADV_TYPE)  = vpadv 
l/'Pperfp --+ 
@PPadjunct @PPcase_obl 
{@M.Head_Pevfp \[@M..Head_Passp} 
@( Anaph_Ctrl ~ )
V 
{ @M_Head_Perfp I@M_Head_Passp } 
@( Anaph_Ctrl ~) 
Figure 1: The pruning of a rule from the actual French grammar. The "*" and the "+" signs have the usual 
interpretation as in regular expressions. A sub-expression enclosed in parenthesis optional. Alternative 
sub-expressions are enclosed in curly brackets and separated by the "\[" sign. An "@" followed by an identifier 
is a macro expansion operator, and is eventually replaced by further functional descriptions. 
Corpus 
--..,, 
0.1\[ 
Disambiguated 
Treebank treebank 
Human 
expert 
Grammar 
specialization 
Specialized 
grammar 
Figure 2: The setting for our experiments on grammar specialization. 
indicators of what can be achieved with this form of 
grammar pruning. However, they could potentially 
be misleading, since failure times for uncovered sen- 
tences might be considerably ower than their pars- 
ing times, had they not been out of coverage. If 
the pruned grammar fails more frequently on sen- 
tences which take longer to parse, the measured 
speedup might be artificiMly high. This is easily 
realized, as simply removing the hardest sentences 
froln the corpus would cause a decrease ill the av- 
erage parsing time, and thus result in a speedup, 
without any pruning at all. To factor out the contri- 
bution of uncovered sentences fi'om the results, the 
performance of a two-stage architecture analogous 
to that of (Samuelsson and Rayner, 1991) was siln- 
ulated, in which the pruned grammar is attempted 
206
"A Sentence" 
Parser with specialized 
grammar 
Fails 
1 
Succeeds 
L_ 
Time = Timespecialize d 
Parser with original 
grammar 
Time = Timespecialize d + Time original 
Figure 3: A schematic representation of the simu- 
lated two-stage coverage-preserving architecture. 
first, and the sentence is passed on to the original 
unpruned grammar whenever the pruned grammar 
fails to return a parse (see Figure 3). The mea- 
sured speedup of this simulated architecture, which 
preserves the anyparse measure of the original gram- 
mar, takes into account the contribution of uncov- 
ered sentences, as it penalizes weeping difficult sen- 
tences under the carpet. 
4 Experimental Results 
The results of the experiments described in the sec- 
tion above are summarized in the table in Figure 4. 
The upper part of the table refers to experiments 
with the French grammar, the lower part to exper- 
iments with the English grammar. For each lan- 
guage, the first line presents data gathered for the 
original grammar for comparison with the pruned 
grammars. The figures in the second line were col- 
lected by pruning the grammar based on the whole 
corpus, and then testing on the corpus itself. The 
grammars obtained in this way contain 516 and 388 
disjuncts - -  corresponding to purely concatenative 
rules - -  for French and English respectively. Any- 
parse and coverage are not, of course, relevant in 
this case, but the statistics on parsing time are, es- 
pecially the one on the maximum parsing time. For 
each iteration in the 10-fold cross-validation experi- 
ment, the maximum parsing time was retained, and 
those ten times were eventually averaged. If pruning 
tended to leave sentences which take long to parse 
uncovered, then we would observe a significant dif- 
ference between the average over ma.ximum times on 
the grammar trained and tested on the same corpus 
(which parses all sentences, including the hardest), 
and the average over maximum times for grammars 
trained and tested on different sets. The fact that 
this does not seem to be the case indicates that prun- 
ing does not penalize difficult sentences. Note also 
that the average number of parses per sentence is 
significantly smaller than with the full grammar, of 
almost a factor of 9 in the case of the French gram- 
inar. 
The third line contains results for the fully pruned 
grammar .  In the case of the French grammar  a 
speedup of about 6 is obtained with a loss in cov- 
erage of 13%. The smaller speedup gained with 
the English grammar can be explained by the fact 
that here, the parsing times are lower in general, 
and that a non-negligible part of this time, espe- 
cially that needed for morphological nalysis, is un- 
affected by pruning. Even in the case of the English 
grammar, though, speedup is substantial (2.67). For 
both grammars, the reduction in the average max- 
inmm parsing time is particularly good, confirming 
our hypothesis that tr imming the grammar by re- 
moving heavy constructs makes it considerably more 
efficient. A partially negative note comes from the 
average number of disjuncts in the prun.ed grain- 
mars, which is 501 for French and 374 for English. 
Comparing this figures to the number of disjuncts in 
grammars pruned on the full corpus (516 and 388), 
we find that after training on nine tenths of the cor- 
pus, adding the last tenth still leads to an increase 
of 3-4% in the size of the resulting grammars. In 
other words, the marginal gain of further training 
examples is still significant after considering about 
900 sentences, indicating that the training corpora 
are somewhat too small. 
The last two lines for each language show figures 
for grammars with pruning inhibited on the most 
variable and the two most variable symbols respec- 
tively. For both languages, inhibiting pruning on the 
most variable symbol has the expected effect of in- 
creasing both parsing time and coverage. Inhibiting 
pruning also on the second most variable symbol has 
ahnost no effect for French, and only a small effect 
for English. 
The table in Figure 5 summarizes the measures 
on the simulated two-stage architecture. For both 
languages the best trade-off, once the distribution 
of uncovered sentences has been taken into account, 
is achieved by the fully pruned grammars. 
5 Re la ted  Work  
The work presented in the current article is related 
to previous work on corpus-based grammar spe- 
cialization as presented in (Rayner, 1988; Salnuels- 
son and Rayner, 1991; Rayner and Carter, 1996; 
Samuelsson, 1994; Srinivas a.nd Joshi, 1995; Neu- 
mann, 1997). 
207 
Parses/sentence 
French  
original grammar 1941 
test = training 219 
Anyparse Coverage Avg. time 
(secs.) 
Max. time 
(secs.) 
Speedup 
1.00 1.OO 1.52 78.5 1 
1.00 1.00 0.28 5.62 5.43 
0.91 0.25 First-order pruning 164 0.87 5.69 6.08 
no pruning on 
'VPverb\[main\]' 1000 0.94 0.91 0.42 8.70 3.62 
no pruning on 
'Vpverb\[main\]' and 'NP'  
First-order pruning 
0.94 0.92 0.42 8.42 1279 3.62 
0.88 
1.00 1.00 0.56 31.73 1 
1.00 1.00 0.23 3.92 2.43 
0.94 0.21 
0.91 
Eng l i sh  
original grammar 58 
test = training 24 
21 
0.96 0.32 
0.35 
no pruning on 
'VPv'  
0.96 
25 
no pruning on 
'Vpv' and 'NPadj '  31 0.93 
3.92 
11.06 
11.16 
Figure 4: The results of the experiments on LFG specialization. 
Avg. CPU time (secs.) Speedup 
French  
0.570 2.67 
0.616 2.47 
0.614 2.48 
First-order pruning 
no pruning on 'VPverb\[main\]' 
no pruning on 'VPverb\[main\]' and 'NP'  
Eng l i sh  
First-order pruning 0.311 1.81 
no pruning on 'VPv'  0.380 1.47 
no pruning on 'VPv'  and 'NPadj'  0.397 1.40 
2.67 
1.75 
1.60 
Figure 5: Results for the simulated two-stage architecture. 
The line of work described in (Rayner, 1988; 
Samuelsson and Rayner, 1991; Rayner and Carter, 
1996; Samuelsson, 1994) deals with unification- 
based grammars that already have a purely- 
concatenative context-fi'ee backbone, and is more 
concerned with a different t~orm of specialization, 
consisting in the application of explanation-based 
learning (EBL). Here, the central idea is to collect 
the most frequently occurring subtrees in a treebank 
and use them as atomic units for parsing. The cited 
works differ mainly in the criteria adopted for select- 
ing subtrees fi'om the treebank. In (Rayner, 1988; 
Samuelsson and Rayner, 1991; Rayner and Carter, 
1996) these criteria are handcoded: all subtrees at- 
isfying some properties are selected, and a new gram- 
mar rule is created by flattening each such subtree, 
i.e., by taking the root as lefl.-hand side and the yield 
as right-hand side, and in the process performing all 
unifications corresponding to the thus removed in- 
ternal nodes. Experiments carried out on a corpus 
of 15,000 trees from the ATIS domain using a ver- 
sion of the SRI Core Language Engine resulted in a 
speedup of about 3.4 at a cost of 5% in gralmnati- 
cal coverage, which however was compensated by an 
increase in parsing accuracy. 
Finding suitable tree-cutting criteria requires a 
considerable amount of work, and must be repeated 
for each new grammar and for each new domain to 
which the grammar is to be specialized. Samuelsson 
(Samuelsson, 1994) proposes a technique to auto- 
matically selects what subtrees to retain. The se- 
lection of appropriate subtrees is done by choosing 
a subset of nodes at which to cut trees. Cutnodes 
are determined by computing the entropy of each 
node, and selecting only those nodes whose entropy 
exceeds a given threshold. Intuitively, nodes with 
low entropy indicate locations in the trees where a 
given symbol was expanded using a predictable set 
of rules, at least most of the times, so that the loss 
of coverage that derives from ignoring the remain- 
ing cases is low. Nodes with high entropy, on the 
other hand, indicate positions in which there is a 
high uncertainty in what rule was used to expand 
the symbol, so that it is better to preserve all alter- 
natives. Several schemas are proposed to compute 
entropies, each leading to a different trade-off be- 
~fllR 
tween coverage reduction and speedup. In general, 
results are not quite as good as those obtained using 
handcoded criteria, though of course the specialized 
grammar is obtained fully automatically, and thus 
with much less effort. 
When ignoring issues related to the elimination of 
complex operators t"1"o111 the RHS of rule schemata, 
the grammar-pruning strategy described in the cur- 
rent article is equivalent to explanation-based l arn- 
ing where all nodes have been selected,as eutnodes. 
Conversely, EBL can be viewed as higher-order 
grammar pruning, removing not grammar ules, but 
gramlnar-rule combinations. 
Some of the work done on data-oriented parsing 
(DOP) (Bod, 1993; Bod and Scha, 1996; Bod and 
Kaplan, 1998; Sima'an, 1999) can also be considered 
related to our work, as it can be seen as a way to 
specialize in an gBL-like way the (initially unknown) 
grammar implicitly underlying a treebank. 
(Srinivas and aoshi, 1995) and (Neumann, 1997) 
apply EBL to speed up parsing with tree-adjoining 
grammars and sentence generation with HPSGs re- 
spectively, though they do so by introducing new 
components in their systems rather then by modify- 
ing the grammars they use. 
6 Conclus ions 
Sophisticated grammar formalisms are very useful 
and convenient when designing high-coverage ram- 
mars for natural languages. Very expressive gram- 
matical constructs can make the task of develop- 
ing and maintaining such a large resource consid- 
erably easier. On the other hand, their use can re- 
sult in a considerable increase in grammatical am- 
biguity. Gramnaar-compilation techniques based on 
grammar structure alone are insufficient remedies in 
those cases, as they cannot access the information 
required to determine which alternatives to retain 
and which alternatives to discard. 
The current article demonstrates that a relatively 
simple pruning technique, employing the kind of ref- 
erence corpus that is typically used for grammar de- 
velopment and thus often already available, can sig- 
nificantly improve parsing performance. On large 
lexical functional grammars, speedups of up to a 
factor 6 were observed, at the price of a. reduction 
in grammatical coverage of about 13%. A simple 
two-stage architecture was also proposed that pre- 
serves the anyparse measure of the original gram- 
mar, demonstrating that significant speedups can be 
obtained without increasing the number of parsing 
failures. 
Future work includes extending the study of 
corpus-based grammar specialization from first- 
order grammar pruning to higher-order gram- 
mar pruning, thus extending previous work on 
explanation-based learning for parsing, aad apply- 
ing it to the LFG fornaalism. 
References  
Rens Bod and Ronald Kaplan. 1998. A probabilistic 
corpus-driven model for lexical-functional naly- 
sis. In Proceedings of Coling-ACL-98, Montreal, 
Canada. 
R. Bod and R. Scha. 1996. Data-oriented lan- 
guage processing: An overview. Technical report, 
ILLC, University of Amsterdam, Alnsterdam, The 
Netherlands. 
Rens Bod. 1993. Using an annotated corpus as a 
stochastic grammar. In Proceedings of EACL-93, 
Utrecht, The Netherlands. 
M. Butt, T.H. King, M.E. Nifio, and F. Segond. 
1999. A Grammar Writer's Cookbook. CSLI Pub- 
lications, Stanford, CA. 
John Carrol. 1994. Relating complexity to practical 
performance in parsing with wide-coverage uni- 
fication grammars. In Proceedings of (ACL '94), 
Las Cruces, New Mexico, June. 
Ronald Kaplan and Joan Bresnan. 1982. Lexical- 
functional grammar: A formal system for gram- 
matical representation. In Joan Bresnan, editor, 
The Mental Representation f Grammatical Rela- 
tions, pages 173-281. MIT Press. 
G/inter Neumann. 1997. Applying explanation- 
based learning to control and speeding-up natu- 
ral language generation. In Proceedings of A CL- 
EACL-97, Madrid, Spain. 
Manny Rayner and David Carter. 1996. Fast pars- 
ing using pruning and grammar specialization. In 
Proceedings of the ACL-96, Santa. Cruz, CA. 
Manny Rayner. 1988. Applying explanation-based 
generalization to natural-language processing. 
In Proceedings of the International Conference 
on Fifth Generation Computer Systems, Tokyo, 
Japan. 
Christer Samuelsson and Manny Rayner. 1991. 
Quantitative evaluation of explanation-based 
learning as an optimization tool for a large-scale 
natural language system. In Proceedings of the 
IJCAI-91, Sydney, Oz. 
Christer Samuelsson. 1994. Grammar specialization 
through entropy thresholds. In Proceedings of the 
ACL-94, Las Cruces, New Mexico. Available as 
cmp-lg/9405022. 
Khalil Sima'an. 1999. Learning Efficient Dis- 
ambiguation. Ph.D. thesis, Institute for Logic, 
Language and Computation, Amsterdam, The 
Netherlands. 
B. Srinivas and A. Joshi. 1995. Some novel appli- 
cations of explanation-based learning to parsing 
lexicalized tree-adjoining ramlnars. In Proceed- 
ings of the ACL-95, Cambridge, MA. 
209 
A Statistical Theory of Dependency Syntax 
Chr i s ter  Sa lnue lsson 
Xerox  Resem:ch Cent re  Europe  
6, chemin  de Mauper tu is  
38240 Mey lan ,  FRANCE 
Chr i s ' cer .  Samue: l . sson?xrce .  x rox ,  com 
Abst ract  
A generative statistical model of dependency syntax 
is proposed based on 'l'csniSre's classical theory. It 
provides a stochastic formalization of the original 
model of syntactic structure and augments it with 
a model of the string realization process, the latter 
which is lacking in TesniSre's original work. The 
resulting theory models crossing dependency links, 
discontinuous nuclei and string merging, and it has 
been given an etIicient computational rendering. 
1 I n t roduct ion  
The theory of dependency grammar culminated in 
the seminal book by Lncien TesniSre, (Tesnihre, 
1959), to which also today's leading scholars pay 
homage, see, e.g., (Mel'enk, 1987). Unfortunately, 
Tesnibre's book is only available in French, with a 
partial translation into German, and subsequent de- 
scriptions of his work reported in English, (Hays, 
196/1), (Gaifinan, 1965), (Robinson, 1970), ,etc., 
stray increasingly t'urther fi:om the original, see (En- 
gel, 1996) or (,15~rvinen, 1998) for an account of this. 
The first step when assigning a dependency de- 
scription to an input string is to segment he input 
string into nuclei. A nucleus can he a word, a part 
of a word, or a sequence of words and subwords, 
and these need not appear eontiguonsly in the input 
string. The best way to visualize this is perhaps the 
following: the string is tokenized into a sequence of 
tokens and each lmcleus consists of a subsequence of
these tokens. Alternative readings may imply differ- 
ent ways of dividing the token sequence into nuclei, 
and segmenting the input string into nuclei is there- 
fore in general a nondeterministic process. 
The next step is to relate the nuclei to each other 
through dependency links, which are directed and 
typed. If there is a dependency link froln one nu- 
cleus to another, the former is called a dependent 
of the latter, and the latter a regent of the former. 
Theories of dependency syntax typically require that 
each nucleus, save a single root nucleus, is assigned 
a unique regent, and that there is no chain of de- 
pendency links that constitutes a cycle. This means 
that the dependency links establish a tree structure, 
main main 
ate ate 
~bj~d~j  ~bj~su~i 
John beans beans John 
Fignre 1: Dependency trees for John ate beans. 
where each node is labeled by a nucleus. Thus, the 
label assigned to a node is a dependent of the label 
assigned to its parent node, and conversely, the label 
assigned to a node is the regent of the labels assigned 
to its child nodes. Figure 1 shows two dependency 
trees tbr the sentence John ate beans. 
In Tesni~re's dependency syntax, only the depen- 
dency strncture, not the order of the dependents, is 
represented by a dependency tree. This means that 
dependency trees are unordered, a.nd thus that the 
two trees of Figure 1 are equivalent. This also mea.ns 
that specitying the sm'face-string realization of a de- 
pendency description becomes a separate issue. 
We will model dependency desc,:iptions as two 
separate stochastic processes: one top-down process 
generating the tree structure T and one bottom-up 
process generating the surt3.ce string(s) S given the 
tree structure: 
1)(7, s) = \] '(7). p(s 1 7) 
This can be viewed as a variant of Shannon's noisy 
channel model, consisting of a language model of 
tree structures and a signal model converting trees 
to surface strings. In Section 2 we describe the top- 
down process generating tree structures and in Sec- 
tion 3 we propose a series of increasingly more so- 
phisticated bottom-up roccsses generating surl~ce 
strings, which resnlt in grammars with increasingly 
greater expressive power. Section el describes how 
the proposed stochastic model of dependency syn- 
tax was realized as a probabilistic hart parser. 
2 Generat ing  Dependency  Trees 
To describe a tree structure T, we will use a string 
notation, introduced in (Gorn, 1962), for the nodes 
684 
?/e 
mainl 
ate/I 
~bv d0bl 
John/ll beans/12 
l!'igure 2: Corn's tree notation tbr John ate beans. 
H ? S S 
e \[main\] s (1) .  
\] ate \ [subj ,dobj \ ]  s ( l l )  ate s(12) 
1:1 John ~ John 
12 beans 0 beans 
I,'igure 3: l)ependency encoding of John ale beans. 
of Clio tree, where the node name sl)ecifi0s the path 
fi'om the root no(le ~ to the node in question, 
I f  (Jj is a node of the tree T, 
with j C N+ and (/J E N~, 
then  q5 is also a node of the trc'e T 
and ()j is a child of 4. 
ltere, N+ denotes the set of positive integers 
{1,2,. . .} and N~_ is the set of strings over N+. 'l'his 
lncans that the label assigned to node ()j is a de- 
pendent of the label assigned to node (J. The first 
dependency tree of Figure 1 is shown in l!'igure 2 
using this notation. 
We introduce three basic random variables, which 
incrementally generate the tree strucl;ure: 
? ?(4)) = l assigns the \]al)el l to node 4), where 
l is a i it lc|etis, i.e., it; is drawn frol,-I the  set of  
s t r ings  over  t, he so.t of  i, okens.  
? "D(Oj) = d indicates t.h(~ dep(:ndency t pe d link- 
ing the label of node OJ to its regent, the label 
Of node 4'" 
? V(?,) = v indica.tes that node 7, has exactly v 
child nodes. 
Note the use of ~(~/J) = 0, rather than a partitioning 
of the labels into terlninal and nonterminal syml)ols, 
to indicate that ~ is a leaf node. 
l,et D be the (finite) set of l)ossible dependency 
types. We next introduce the composite variables 
.T(()) ranging over the power bag* N D, indicating 
the bag of dependency t pes of dJ's children: 
m(4,) = f = \[d,,...,dl,\] *> 
e~O P(c/,) = v A V:ic{i ..... v} D(6 j )  = dj 
Figure 3 encodes the dependency ti;ee ()1' Figure 2 
accordingly. We will ignore the last cohunn \['or now. 
1 A bag (mull 'set) can contain several tokens of the Smlm 
type. We denote sets {...}, \]Jags \[...\] and ordered tuples {...), 
\]Jill, over\]o+'id O~ (~> etc, 
We introduce the probabilities 
s'~(~) - 
: P (~(0  : l) 
P~ if, j )  = 
= l ' (10 j )  = (~ I ~(4,) = t,~D(4,J) = 4d  
PT(0  = 
= P( f (0  = f l~(0  = l) 
Pm(~) = { tbr4J~-e } 
= s,(m(4,) = S I ?(4>) -- l, 9(40 -- d) 
These l~robabilities are tyl)ically model parameters, 
o1' further decomposed into such. lJ~(@j) is the prob- 
ability of the label ?(4~J) of a node given the label 
?(4') of its regent and the dependency type "D(0j) 
linking them. l{eh~ting Eft, j) and ?(0) yiekls lexical 
collocation statistics and including D((~j) makes the 
collocation statistics lexical-fimetional. Pm(~0 is the 
probability of the bag of' dependency types Y(0) of 
a ,,ode Rive,, its label ?(4J) and its relation D(#)) to 
its regent. This retleets the probability of the label's 
vM oncy, or lexieal-fimctional eoml)lement , and of op- 
1.ional adjuncts. Including D(q)) makes this proba- 
bility situa.ted in taking its current role into accounl.. 
These allow us to define the tree probal)ility 
~,(m) = I I  ~ '~0/ , ) - s>(4 , )  
,',,EAr 
wiiere the 1)roduct is taken over the set. of nodes .A/" 
of the tree. 
\?e generate the random variables ? and S using 
a top-down s tochast i c  process,  where  ?(( ) )  is goner-  
ated I)efore Y(O). The probal)ility of the condition- 
ing material of l~(Oj) is then known from Pc(O) and 
19((,), and that of Sg(4,j) is known froln \]'?(OJ) 
and lJ:n(O). F'igure 3 shows the process generating 
the dependency tree' of Figure 2 by reading the ? 
and .7:- colunms downwards in parallel, ? before Y: 
~,(~) =. ,  m(O = r~,~\ ] ,  
?(1) = ate, Y(1)= \ [subj ,dobj \ ] ,  
~;(~)  = Job,,, ro l l : i )=  ~, 
~(~)  = /,~,,,,~, m(12) = 0 
Consider calculating the l)robabilities at node 1: 
IJ~ (1) = 
= s'(/_;(l) = <,t~ I? (0  = .,'D(1) = main) 
P~(~)  = 
= P lY ( l )=  \[subj,dobj\] \[ 
I C(I) = .t~,'D(~) = m~n)  
3 St r ing  Rea l i za t ion  
'\]'he string realization cannot be uniquely deter- 
mined from the tree structure. 'lb model the string- 
realization process, we introduce another fundamen- 
tal random w~riable $(()), which denotes the string 
685 
associated with node 0 and which should not be con- 
fused with the node label ?(()). We will introduce 
yet another fundamental randoln variable \]v4(~)) in 
Section 3.2, when we accommodate crossing depen- 
dency links. In Section 3.1, we present a projectivc 
stochastic dependency gralnlnar with an expressive 
power not exceeding that of stochastic ontext-free 
grammars. 
3.1 P ro jec t ive  Dependency  Grammars  
We let the stochastic process generating the ? and .7- 
vtu'iM)les be as described above. We then define tile 
stochastic string-realization process by letting tile 
8(~5) variables, given ?'s label 1(40 and the bag of 
strings s(()j) of ~5's child nodes, randomly permute 
and concatenate them according to the probability 
distributions of the modeh 
Ps(0 = 
= s,(s(<.) = 401  c(O, 7 (0 ,  c(O) 
~'~(4,) = { fo r??~ } 
= s'(s(?,) = 4?') I >frO, eft,), 7(?) ,  c(?,)) 
where 
c(4,) = 0 \[~(<bJ)\] 
j= l  
8(~$) = adjoin(C(g,),l(?)) 
adjoin(A,/3) = eoncat(permute(A U \[/3\])) 
The latter equations hould be interpreted as defin- 
ing the randorn variable 8, rather than specifying its 
probability distribution or some possible outcome. 
This means that each dependent is realized adjacent 
to its regent, where wc allow intervening siblings, 
and that we thus stay within the expressive power 
of stochastic ontext-free grammars. 
We define the string-realization probability 
~beAr 
and the tree-string probability as 
P(7, s) = I"(7-)./,(s I 7) 
The stochastic process generating the tree struc- 
ture is as described above. We then generate the 
string variables S using a bottom-up stochastic pro- 
cess. Figure 3 also shows the process realizing the 
surface string John ate beans fl-om the dependency 
tree of Figure 2 by reading the S column upwards: 
,9(12) = bca,,s, S (11)  = Job,,, 
,9(1) = s(11)ate s(12), S(e) = s ( l ) .  
Consider cMeulating tile striug probability at node 
1. Ps is the probability of the particular permut~t- 
tion observed of the strings of the children and the 
?le 
did say/1 
~bj/'%sc0nj 
Mao,/l 1 that ate/l 2 
subj/"~ol0bj  
Johnll21 Whatbeans/122 
Figure 4: Del)endency tree for What beans did Mary 
say lhat John ate? 
1M)el of the node. To overcome the sparse-data prob- 
lem, we will generalize over the actual strings of tile 
children to their dependency types. For example, 
s(subj) denotes the string of the subject child, re- 
gardless of what it actually might be. 
J'~(1) = P(S(1) = s(subj) ate s(dobj) I 
I "D(1) = m~n,  s:(1) = <,re, 
C(I) = \[s(subj), s(dobj)\]) 
This is the probability of the permutation 
(s(subj), ate, s(dobj)) 
of the bag 
\[s(subj), aic, s(dobj)\] 
given this bag and the fact that we wish to tbrm a 
main, declarative clause. This example highlights 
the relationship between the node strings and both 
Sallssure's notion of constituency and tile l)ositiolml 
schemata of, amongst others, l)idrichsen. 
3.2 Cross ing Dependency  Links 
To accommodate long-distance dependencies, we al- 
low a dependent to be realized adjacent o the la- 
bel of rely node that dominates it, immediately or 
not. For example, consider the dependency tree of 
Figure 4 tbr the sentence l/Vhat beans did Ma'Jw say 
that John ate? as encoded in Figure 5. Ilere, What 
beans is a dependent of that arc, which in turn is a 
dependent of did say, and What beans is realized be- 
tween did and sag. This phenomenon is called move- 
ment in conjunction with phrase-structure gram- 
m~rs. It makes the dependency grammar non- 
projective, since it creates crossing dependency links 
if the dependency trees also depict the word order. 
We introduce variables A//(~) that randomly se- 
lect from C(4)) a, subbag CM(4,) of strings passed up 
to ()'s regent: 
C(?) = O(\[s(4)J)\] UCM(?j)) 
j= l  
Gd</,) c_ c(4)) 
s'~(4,) = P(M(4)) = c'~,,(e) I 
I ~(<t,), s:(?), f i fO, c(6)) 
686 
N" ? be 
c v \[whq\] 
1 did say \ [subj ,  sconj \ ]  
11 Mary (~ 
:12 thai ate \[subj ,dobj\] 
1.21 John 
122 What beans 0 
Figure 5: l)ependency encoding of What beans did 
Mary say that John ate? 
A/ M S 
di(/411) 
11 ~ Mary 
12 \[.s(122)\] that s(121) ate 
121 ~J John 
122 0 W/tat bem~s 
Fi.?;ure 6: Process generating What beans did Mary 
.sag that dohn ate? 
q'he rest of the strings, Cs(?), are realized here: 
c:,;(?) = c(?)  \ 
= = I be(C), c;,(?)) 
s'(?) = 
3,3 D iscont inuous Nuch;i  
We generalize the scheme to discontinuous nuclei by 
allowing 8(?) to i,mert the strings of C~.(~5) any- 
where in 1(?): e 
adjoin(A, fl) = 
= V @ 
m 
j= l  
fl = b~ ...b,,, 
Tllis means that strings can only l)e inserted into an- 
cestor labels, ,lot into other strings, which enforces 
a. type of reverse islaml constraint. Note how in Fig- 
ure 6 John is inserted between that and ate to form 
the subordina, te clause that John atc. 
We define tile string-realization probability 
,b6 ar 
and again define the tree-string prol)ability 
~ ' ( r ,S )  = l ' (T ) . l ' (a  I T) 
2x -~ y indicates that x precedes y in the resulting per- 
mutation, q~snihre's original implicit definition of a nucleus 
actually does not require that the order be preserved when 
realizing it; if has catch is a nucleus, so is eaten h.as. This is 
obviously a useflfl feature for nlodeling verb chains in G erln&n 
subordinate clauses. 
'lb avoid derivational ambiguity when generating a 
tree-string pair, i.e., have more than one derivation 
generate tile same tree-string pair, we require that 
no string be realized adjacent o the string of any 
node it was passed u 1) through. This introduces the 
l)raetica.l problem of ensuring that zero probability 
mass is assigned to all derivations violating this con- 
straint. Otherwise, the result will be approxima.ting 
the parse probabi\]ity with a derivation probability, 
as described in detail in (Samuelsson, 2000) based on 
the seminal work of (Sima'an, 1996). Schemes like 
(Alshawi, 1996) tacitly make this approximation. 
The tree-structure variables ? and be are gener- 
ated just as before. ~?e then generate the string vari- 
ables 8 and Ad using a bottom-up stochastic process, 
where M(?) i s  generated before 8(?). 'l.'he proba- 
bility of the eonditkming material of \]o~ (?) is then 
known either from the top-down process or from 
I 'M(?j)  and Pa(?j) ,  and that of INTO)is known 
either from the top-down process, or from 15v4(?), 
\[)dgq(4)j) and 1~(?j).  The coherence of S(~) a.nd 
f14(~/)) is enforced by explicit conditioning. 
Figure 5 shows a top-down process generating the 
dependency tree of Figure <1; the columns ? and 
be should be read downwards in parallel, L; before 
b e. Figure 6 shows a bottom-up rocess generating 
the string l/Vhat beans did Mary say that dohn at(:? 
from the dependency description of Figure 5. The 
colltlll,lS d~v4 and S should be read upwards in paral- 
lel, 2t4 before $. 
3.4 St r ing Merg ing  
We have increased the expressive power of our de- 
pendency gramma.rs by nlodifying tile S variables, 
i.e., by extending the adjoin opera.lion. In tile first 
version, the adjoin operation randomly permutes the 
node label and the strings of the child nodes, and 
concatenates the result. In the second version, it 
randondy inserts the strings of the child nodes, and 
any moved strings to be rea.lized at tile current node, 
into the node label. 
The adjoin operation can be fln:ther refined to al- 
low handling an even wider range of phenomena, 
such as negation in French. Here, the dependent 
string is merged with the label of the regent, as ne . . .  
pas is wrapped around portions of the verb phrase, 
e.g., Ne me quitte pas!, see (Brel, 195.(t). Figure 7 
shows a dependency tree h)r this. In addition to this, 
the node labels may be linguistic abstractions, e.g. 
"negation", calling on the S variables also for their 
surface-string realization. 
Note that the expressive power of the grammar 
depends on the possible distributions of the string 
probabilities IN. Since each node label can be moved 
and realized at the root node, any language can be 
recognized to which the string probabilities allow as- 
signing the entire probablity mass, and the gralnmar 
will possess at least this expressive power. 
687 
!le 
imnl 
quitte/l 
~g>~do~ 
Ne pas/l l me~12 
Figure 7: Dependency tree for Ne me quitte pas t 
4 A Computational Rendering 
A close approximation of the described stochastic 
model of dependency syntax has been realized as a 
type of prohabilistic bottom-up chart parser. 
4.1 Model Specialization 
The following modifications, which are really just 
specializations, were made to the proposed model 
for efficiency reasons and to cope with sparse data. 
According to Tesni6re, a nucleus is a unit that 
contains both tile syntactic and semantic head and 
that does not exhihit any internal syntactic struc- 
ture. We take the view that a nucleus consists of a 
content word, i.e., an open-class word, and all flmc- 
tion words adding information to it that could just as 
well have been realized morphologically. For exam- 
ple, the definite article associates definiteness with a. 
word, which conld just has well have been manifested 
in the word form, as it is done in North-Germanic 
languages; a preposition could be realized as a loca.- 
tional or temporal inflection, as is done in Finnish. 
The longest nuclei we currently allow are verb chains 
of the form that have been eoten, as in John knows 
that lhe beans have been eaten. 
The 5 r variables were decomposed into generating 
the set of obligatory arguments, i.e., the valency or 
lexical complement, atonce, as in the original model. 
Optional modifiers (adjuncts) are attached through 
one memory-less process tbr each modifier type, re- 
suiting in geometric distributions for these. This is 
the same separation of arguments and adjuncts as 
that employed by (Collins, 1997). However, the ? 
variables remained as described above, thus leaving 
the lexieal collocation statistics intact. 
The movement probability was divided into three 
parts: the probability of moving the string of a par- 
ticular argument dependent from its regent, that of 
a moved dependency type passing through a par- 
ticular other dependency type, and that of a de- 
pendency type landing beneath a particular other 
dependency type. The one type of movement that 
is not yet properly handled is assigning arguments 
and adjuncts to dislocated heads, as in What book 
did John read by Chomsky? 
The string-realization probability is a straight- 
forward generalization of that given at the end of 
Section 3.1, and they m:e defined through regu- 
lar expressions. Basically, each unmoved depen- 
dent string, each moved string landed at. the cur- 
?/e ?/e 
ynql ynql 
Did xxx/I Did eat/1 
s/uu b j /~ 'do .~ s/ubj/ '~dobi 
John/l I beaus~12 Johull I xxx/12 
Figure 8: Dependency trees for Did John xxm beans? 
and Did John eat xxm? 
rent node, and each token of the nucleus labeling the 
current node are treated as units that are randomly 
permuted. Whenever possible, strings are general- 
ized to their dependency t pes, but accurately mod- 
elling dependent order in French requires inspecting 
tile actual strings of dependent clitics. Open-class 
words are typically generalized to their word class. 
String merging only applies to a small class of nuclei, 
where we treat tile individual tokens of the depen- 
dent string, which is typically its label, as separate 
units when perfornfing tile permutation. 
4.2 The  Char t  Parser  
The parsing algorithm, which draws on the Co&e- 
t(asanli-Younger (CI(Y) algorithm, see (Younger, 
1967), is formulated as a prohabilistic deduction 
scheme, which in turn is realized as an agenda-driven 
chart-pa.rser. The top-level control is similar to that 
of (Pereira and Shieher, 1987), pp. 196-210. The 
parser is implemented in Prolog, and it relies heav- 
ily on using set and bag operations as primitives, 
utilizing and extending existing SICStus libraries. 
The parser first nondeterministically segments the 
input string into nuclei, using a lexicon, and each 
possible lmcleus spawns edges tbr the initial chart. 
Due to discontinuous nuclei, each edge spans not a 
single pair of string positions, indicating its start 
and end position, \])tit a set of such string-position 
pairs, and we call this set an index. If the index 
is a singleton set, then it is continuous. We extend 
the notion of adjacent indices to be any two non- 
overlapping indices where one has a start position 
that equals an end position of the other. 
The lexicon contains intbrmation about the roles 
(dependency t pes linking it to its regent) and va- 
lencies (sets of types of argument dependents) that 
are possible for each nucleus. These are hard con- 
straints. Unknown words are included in nuclei in a 
judicious way and the resulting nuclei are assigned 
all reasonable role/valency pairs in the lexicon. For 
example, the parser "correctly" analyzes tile sen- 
tences Did John xxx beans? and Did John eat xxx? 
as shown in Figure 8, where xxx' is not in the lexicon. 
For each edge added to the initial chart, the lexi- 
con predicts a single valency, but a set of alternative 
roles. Edges arc added to cover all possible valen- 
al)ue to the uniqueness principle of arguments, these are 
sets, rather than bags. 
688 
ties for each nucleus. The roles correspond to tim 
"goal" of dotted items used ill traditional cha.rt pars- 
ing, and the unfilled valency slots play the part of 
the "l)ody", i.e., the i)ortion of the \]{IlS \['ol\]owing 
the dot that renlailis to I)e found. If an argunl_ent is 
attached to the edge, the corresponding valency slot 
is filled in the resulting new odg(;; no arg~llnlont ea.ll 
be atta.ched to a.n edge llnless tllere is a (;orrespon(l- 
ing unfilled witency slot for it, or it is licensed by a 
lnoved arguln0nt, l,'or obvions reasons, the lexicon 
ca.nnot predict all possible combinations of adjuncts 
for each nuehms, and in fact predicts none at all. 
There will in general be nmltiple derivations of any 
edge with more than ()no del)endent , but the parser 
avoids adding dul)licate edges to tlt(? chart in the 
same way as a. traditional chart l)arser does. 
The l>arser enll)loys a. l)a(:ked l)arse foresl. (PI)I! ') 
to represent he set of all possible analyses and the 
i)robalfility of each analysis is recoverable I\]:om the 
PPI!' entries. Since optional inodifiers are not 1)re - 
dieted by the lexicon, the chart does not (:onl, a.ii~ 
any edges that ('orrespon(t directly to passive edges 
ill traditional chart parsing; at any point, an ad.lun('t 
C~ll always be added to an existing edge to form a 
new edge. In sonic sense, though, tile 1)1)1 '' nodes 
play tlie role all' passive edges, since the l)arser never 
attempts to combine two edges, only Olle ('xlgc and 
one I)l)l! ' lio(le, and the la.tter will a.lways 1)e a. de- 
pendent el'the fornier, directly, or indirectly tlirough 
the lists of n:iovcd dependents. 'l'he edge and l)l)l i' 
node to be Colnl)ined ai'e required to \]lave adjacent 
indices, and their lnlion is the index of tile now edge. 
The lnain point in using a l)acked parse forest, is to 
po'rI'orni local ~tiiil)iguity packing, which lneans a.b 
stracting over difl);ren(-es ill intc'rnal stlFlletlllye that 
do not lllalL, t(;r for fllrth(~,r \])arsilig. \?hen attching a 
I)PF no(-l(~' to SOlllo edgc ;_is a direct or indirect de- 
pendent, the only relewuit teatnres are its index, its 
nucleus, its role a.nd its moved dependents. Oilier 
features necessary for recovering the comph;tc" anal- 
ysis are recorded in the P1)F entries of the node, bnt 
arc not used for parsing. 
To indicate the alternative that no more depen- 
dents are added to an edge, it is converted into a 
set of PPF updates, where each alternative role of 
the edge adds or updates one PPF entry. When 
doing this, any unfilled valency slots are added to 
the edge's set of moved arguments, which in turn is 
inherited by the resulting PPF update. '.\['lie edges 
are actually not assigned probabilities, since they 
contain enough information to derive the appropri- 
ate l)robabilities once they are converted into I)I)F 
entries. '1'o avoid the combinator ia l  explosion el' un- 
restricted string merging, we only allow edges with 
continuous indices to be converted into PI)I! ' 0ntries, 
with the exception of a very limited class of lexically 
signMed nnelei, snch as the nc pas, nc jamais, etc., 
scheme of French negation. 
4.3 P r in t ing  
As Ot)l)osed to traditional chart parsing, meaningful 
upper and lower 1)ounds of the supply and demand 
for the dependency types C" the "goal" (roles) and 
"body" (wdency) of each edge can 1)e determined 
From the initial chart, which allows performing so- 
phis(,icated pruning. The basic idea is that if some 
edge is proposed with a role that is not sought out- 
side its index, this role can safely be removed. For 
example, the word me could potentially be an in- 
direct object, but if there is no other word in the 
inl)ut string that can have a.n indirect object as an 
argument, this alternative can be discarded. 
'Phis idea is generalized to a varia.nt of pigeonhole 
reasoning, in the v(;in of 
If wc select this role or edge, then ~here are 
by necessity too few or too many of some 
del)endcncy tyl)e sought or Cl'ered in the 
chart. 
or alternatively 
If wc select this nucleus or edge, then we 
cannot span the entire input string. 
Pruning is currently only al)plied to the initial chart 
to remove logically inq>ossible alternatives and used 
to filter out impossible dges produced in the predic- 
tion step. Nonetheless, it reduces parsing times by 
an order of magnitude or more tbr many of the test 
examples. \]t would however be possible to apply 
similar ideas to interniittently reinove alternatives 
that are known 1:o be suboptimal, or to \]leuristically 
prtllie unlik(;ly searcll branches. 
Discussion 
We have proposed a generative, statistical t.iieory 
of dependency syntax, based on TesniSrc's classical 
theory, that models crossing dependency links, dis- 
continuous nuclei and string merging. The key in- 
sight was to separate the tree-generation a d string- 
realization processes. The model has been real- 
ized as a type of probabilistie chart parser. The 
only other high-fidelity computational rendering of 
Tesnitre's dependency syntax that we are aware of 
is that of (rl.'apanainen and J fi.rvinen, 1997), which is 
neither generative nor statistical. 
The stochastic model generating dependency trees 
is very similar to other statistical dependency mod-  
els, e.g., to that of (Alshawi, 1996). Formulating it 
using Gorn's notation and the L; and 2" variables, 
though, is concise, elegant; and novel. Nothing pre- 
vents conditioning the random variables on arbitrary 
portions of Clio 1)artial tree generated this far, using, 
e.g., maximum-entrol)y or decision-tree models to 
extract relevant ~atnres of it; there is no difference 
689 
in principle between our model and history-based 
parsing, see (Black el; al., 1993; Magerman, 1995). 
The proposed treatment of string realization 
through the use of the ,5 and A4 variables is also both 
truly novel and important. While phrase-structure 
grammars overemphasize word order by making the 
processes generating the S variables deterministic, 
Tesni6re treats string realization as a secondary is- 
sue. We tind a middle ground by nsing stochastic 
processes to generate the S and Ad variables, thus 
reinstating word order as a parameter of equal im- 
portance as, say, lexical collocation statistics. It is 
however not elevated to the hard-constraint s atus 
it enjoys in phrase-structure grammars. 
Due to the subordinate role of string realization in 
classical dependency grammar, the technical prob- 
lems related to incorporating movement into the 
string-realization process have not been investigated 
in great detail. Our use of the 54 variables is moti- 
vated partly by practical considerations, and partly 
by linguistic ones. The former in the sense that 
this allows designing efficient parsing algorithms for 
handling also crossing dependency links. The lat- 
ter as this gives us a quantitative handle on the 
empirically observed resistance against crossing de- 
pendency links. As TesniSre points out, there is 
locality in string realization in the sense that de- 
pendents tend to be realized adjacent o their re- 
gents. This fact is reflected by the model parame- 
ters, which also model, probabilistically, barrier con- 
straints, constraints on landing sites, etc. It is note- 
worthy that treating movelnent as in GPSG, with 
the use of the "slash" l~ature, see (Gazdar et al, 
1985), pp. 137-168, or as is done in (Collins, \]997), 
is the converse of that proposed here for dependency 
grammars: the tbrmer pass constituents down the 
tree, the 54 variables pass strings up the tree. 
The relationship between the proposed stochastic 
model of dependency syntax and a number of other 
prominent stochastic grammars i explored in detail 
in (Samuelsson, 2000). 
References 
Itiyan Alshawi. 1996. IIead automata nd bilingual 
tiling: Translation with minimal representations. 
Procs. 3~th Annual Meeting of the Association for 
Computational Linguistics, pages 167-176. 
Ezra Black, Fred Jelinek, John Lafferty, David 
Magerman, l~obert Mercer, and Salim Roukos. 
1993. Towards history-based grammars: Using 
richer models tbr probabilistic parsing. Proes. 
28th Annual Meeting of the Association for Com- 
putational Linguistics, pages 31 37. 
Jacques Brel. 1959. Ne mc quitte pas. La Valse h 
Mille Temps (PIII 6325.205). 
Micha.el Collins. 1997. Three generative, lexical- 
ized models for statistical parsing. Procs. 35th 
Annual Meeting of the Association fog" Computa- 
tional Linguistics, pages 16-23. 
Ulrich Engel, 1996. Tcsni~rc Miflvcrstanden: Lu- 
cicn Tcsni~re - Syntaxc Structuralc et Opera- 
tion Mcntalcs. Aktcn des deulscl,-franzSsischen 
Kolloquiums anliifllich dcr 100 Wiedcrkchr seines 
Gebursttagcs, 5'trasbourg 1993, volume 348 of Lin- 
guistische Arbeiten, pages 53-61. Niedermcyer, 
Tiibingen. 
ltaim Gaiflnan. 1965. l)ependency systems and 
phrase-structure systems. Information and Con- 
trol, 8:304-337. 
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullnm, 
and Ivan A. Sag. 1985. Generalized Phrase Struc- 
turc Grammar. Basil Blackwell l?ul)lishing, Ox- 
ford, England. Also published by Harvard Uni- 
versity Press, Cambridge, MA. 
Saul Gorn. 1962. Processors for infinite codes of 
shannon-fa.no type. Syrup. Math. Theory of Au- 
tomata. 
David Ilays. 1964. l)ependency theory: A formal- 
ism and some observations. Languagc, 40(4):511- 
525. 
Timo J'a.rvinen. 1998. Tcsnidre's 5'tg'uctural Syntax 
Rcworl?ed. University of llelsinki, Itelsinki. 
David Ma.german. 1995. Statistical decision-tree 
models for parsing. Ib'ocs. 33rd Annual Meeting 
of the Association fog-" Computational Linguistics, 
pages 276 283. 
Igor Mel'Snk. 1987. Dependency 5'ynta.r:. State Uni- 
versity of New York Press, All)any. 
l?crnando Pereira and Stuart Shieber. 1987. Pro- 
log and Natural-Language Analysis. CSLI Lecture 
Note 10. 
Jane Robinson. 1970. l)ependency structures and 
transfornm.tional rules. Lcmguage, 46:259 285. 
Christer Samuelsson. 2000. A theory of stochastic 
grammars. In Proceedings oJNLI)-200(\], l)ages 92 
105. Springer Verlag. 
Khalil Sima'an. 1996. Computational complexity of 
probabilistic disambigua.tions by means of tree- 
grammars. Procs. 16lh International Confcrencc 
on Computational Linguistics, at the very end. 
Pasi Tapanainen and Timo JSrvinen. 1997. A non- 
projective dependency parser. Pgvcs. 5th Con- 
fercnce on Applied Natural Language Processing, 
pages 6zl 71. 
Lucien Tesnihre. 1959. lOldmcnts de Syntaxc Sh'uc- 
turalc. Libraire C. Klincksieck, Paris. 
l)avid 1I. Younger. 1967. R.ecognition and parsing 
of context-fi'ee languages in time n a. Information 
and Control, 10(2):189 208. 
690 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 7-12, Lisbon, Portugal, 2000. 
Corpus-Based Grammar Specialization 
Nico la  Cancedda and Chr i s te r  Samuelsson  
Xerox Research Centre Europe 
6, chemin de Maupertuis 
38240, Meylan, France 
{Nicola. Cancedda, Christer. Samuelsson}@xrce. xerox, com 
Abst rac t  
Broad-coverage rammars tend to be highly am- 
biguous. When such grammars are used in a 
restricted omain, it may be desirable to spe- 
cialize them, in effect trading some coverage for 
a reduction in ambiguity. Grammar specializa- 
tion is here given a novel formulation as an opti- 
mization problem, in which the search is guided 
by a global measure combining coverage, ambi- 
guity and grammar size. The method, applica- 
ble to any unification grammar with a phrase- 
structure backbone, is shown to be effective in 
specializing a broad-coverage LFG for French. 
1 In t roduct ion  
Expressive grammar formalisms allow grammar 
developers to capture complex linguistic gener- 
alizations concisely and elegantly, thus greatly 
facilitating grammar development and main- 
tenance. Broad-coverage grammars, however, 
tend to overgenerate considerably, thus allowing 
large amounts of spurious ambiguity. If the ben- 
efits resulting from more concise grammatical 
descriptions are to outweigh the costs of spuri- 
ous ambiguity, the latter must be brought down. 
We here investigate a corpus-based compilation 
technique that reduces overgeneration a d spu- 
rious ambiguity without jeopardizing coverage 
or burdening the grammar developer. 
The current work extends previous work 
on corpus-based grammar specialization, which 
applies variants of explanation-based learning 
(EBL) to grammars of natural anguages. The 
earliest work (Rayner, 1988; Samuelsson and 
Rayner, 1991) builds a specialized grammar by 
chunking together grammar ule combinations 
while parsing training examples. What rules to 
combine is specified by hand-coded criteria. 
Subsequent work (Rayner and Carter, 1996; 
Samuelsson, 1994) views the problem as that 
of cutting up each tree in a treebank of cor- 
rect parse trees into subtrees, after which the 
rule combinations corresponding to the subtrees 
determine the rules of the specialized gram- 
mar. This approach reports experimental re- 
sults, using the SRI Core Language Engine, 
(Alshawi, 1992), in the ATIS domain, of more 
than a 3-fold speedup at a cost of 5% in gram- 
matical coverage, the latter which is compen- 
sated by an increase in parsing accuracy. Later 
work (Samuelsson, 1994; Sima'an, 1999) at- 
tempts to automatically determine appropriate 
tree-cutting criteria, the former using local mea- 
sures, the latter using global ones. 
The current work reverts to the view of EBL 
as chunking grammar rules. It extends the 
latter work by formulating rammar special- 
ization as a global optimization problem over 
the space of all possible specialized grammars 
with an objective function based on the cover- 
age, ambiguity and size of the resulting gram- 
mar. The method was evaluated on the LFG 
grammar for French developed within the PAR- 
GRAM project (Butt et al, 1999), but it is 
applicable to any unification grammar with a 
phrase-structure backbone where the reference 
treebank contains all possible analyses for each 
training example, along with an indication of 
which one is the correct one. 
To explore the space of possible grammars, a 
special treebank representation was developed, 
called a \]folded treebank, which allows the ob- 
jective function to be computed very efficiently 
for each candidate grammar. This representa- 
tion relies on the fact that all possible parses 
returned by the original grammar for each train- 
ing sentence axe available and the fact that the 
grammar specialization ever introduces new 
parses; it only removes existing ones. 
The rest of this paper is organized as follows: 
Section 2 describes the initial candidate gram- 
mar and the operators used to generate new 
candidate grammars from any given one. The 
function to be maximized is introduced and mo- 
tivated in Section 3. The folded treebank repre- 
sentation is described in Section 4, while Sec- 
tion 5 presents the experimental results. 
2 Unfo ld ing  and  Spec ia l i za t ion  
The initial grammar is the grammar underly- 
ing the subset of correct parses in the training 
set. This is in itself a specialization of the gram- 
mar which was used to parse the treebank, since 
some rules may not show up in any correct parse 
in the training set; experimental results for this 
first-order specialization are reported in (Can- 
cedda and Samuelsson, 2000). This grammar 
is further specialized by inhibiting rule combi- 
nations that show up in incorrect parses much 
more often than in correct parses. 
In more detail, we considered ownward un- 
folding of grammar ules (see Fig. l)3 A gram- 
mar rule is unfolded downwards on one of the 
symbols in its right-hand side if it is replaced 
by a set of rules, each corresponding to the ex- 
pansion of the chosen symbol by means of an- 
other grammar ule. More formally, let G = 
(E, EN, S, R) be a context-free grammar, and 
let r , r '  C R, k E .M + such that rhs(r) = aAfl, 
lal = k - 1, lhs(r') = A, rhs(r') = V. The rule 
adjunction of r I in the k th position of r is defined 
as a new rule RA(r, k, r ~) = r ' ,  such that: 
lhs(r") = lhs(r) 
rhs(r") = aVfl 
For unification grammars, we instead require 
lhs(r') U rhs(r)(k) 
lhs(r 1') = O(lhs(r)) 
rhs(r") = O(oLTfl )
where rhs(r)(k) is the kth symbol of rhs(r), 
where X t3 Y indicates that X and Y unify, and 
where 0 is the most general unifier of lhs(r ~) and 
rhs(r)(k). 
The downward rule unfolding of rule r on its 
k th position is then defined as: 
DRU(r, k) = 
1The converse operation, upward unfolding, was not 
used in the current experiments. 
{r'\[3r"\[r' = RA(r, k, r")\]} if ? 0 
= {r} otherwise 
It is easy to see that if all r I E DRU(r, k) are 
retained then the new grammar has exactly the 
same coverage as the old one. Once the rule 
has been unfolded, however, the grammar can 
be specialized. This involves inhibiting some 
rule combinations by simply removing the cor- 
responding newly created rules. Any subset 
X C_ DRU(r,k) is called a downward special- 
ization of rule r on the k th element of its rhs. 
Given a grammar, all possible (downward) 
unfoldings of its rules are considered and, for 
each unfolding, the specialization leading to the 
best increase in the objective function is deter- 
mined. The set of all such best specializations 
defines the set of candidate successor grammars. 
In the experiments, a simple hill-climbing algo- 
r ithm was adopted. Other iterative-refinement 
schemes, such as simulated annealing, could eas- 
ily be implemented. 
3 The  Ob ject ive  Funct ion  
Previous research approached the task of de- 
termining which rule combinations to allow ei- 
ther by a process of manual trial and error or 
by statistical measures based on a collection of 
positive examples only: if the original grammar 
produces more than a single parse of a sentence, 
only the "correct" parse was stored in the tree- 
bank. However, we here also have access to all 
incorrect parses assigned by the original gram- 
mar. This in turn means that we do not need 
to estimate ambiguity through some correlated 
statistical indicator, since we can measure it di- 
rectly simply by checking which parse trees are 
licensed by every new candidate grammar G. 
There are many possible ways of combining the 
counts of correct and incorrect parses in a suit- 
able objective function. For the sake of sire- 
plicity we opted for a linear combination. How- 
ever, simply maximizing correct parses and min- 
imizing incorrect ones would most likely lead to 
overfitting. In fact, a grammar with one large 
flat rule for each correct parse in the treebank 
would achieve a very high score during training, 
but most likely perform poorly on unseen data. 
A way to avoid overfitting consists in penalizing 
large grammars by introducing an appropriate 
term in the linear combination. The objective 
8 
A C  A-oc  A_oc 
B "-'~" D A "-'P" E F C A ---~" E F C 
B-----P- E F A ~  
B ~ G Specialization 
Downward unfolding 
of A -> B C on "B" 
A  C  j 
B -----~ A D B " '~"B  C C " 'D"E  B C 
B ' ' '~  A C " " -~E B C 
C ' ' '~  E A Specialization 
Upward unfolding 
of  A -> B C 
Figure 1: Schematic examples of upward and downward unfolding of rules. 
function Score was thus formulated as follows: 
Scorea = Acorr Corra -- Aine InCa - ~size S i zea  
where Corr and Inc are the number of correct 
and incorrect parses allowed by the grammar, 
and Size is the size of the grammar measured 
as the total number of symbol occurrences in the 
right-hand sides of its rules. Acorr and Ainc are 
weights controlling the pruning aggressiveness: 
their ratio Acorr/Ainc intuitively corresponds to 
the number of incorrect trees a specialization 
must disallow for each disallowed correct tree, 
if the specialization is to lead to an improve- 
ment over the current grammar. The lower this 
ratio is, the more aggressive the pruning is. The 
relative value of ;~size with respect o the other 
As also controls the depth to which the search is 
conducted: most specializations result in an in- 
crease in grammar size, which tends to be more 
and more significant as the number and the size 
of rules grows; a larger Asize thus has the effect 
of stopping the search earlier. Note that only 
two of the three weights are independent. 
4 T reebank  Representat ion  
A folded treebank is a representation of a set 
of parse trees which allows an immediate as- 
sessment of the effects of inhibiting specific rule 
combinations. It is based on the idea of "fold- 
ing" each tree onto a representation f the gram- 
mar itself. Any phrase-structure grammar can 
be represented as a concatenation/or g aph - -  
a directed bipartite multigraph with an or-node 
for each symbol and a concatenation-node for
each rule in the grammar. The present de- 
scription covers context-free grammars, but the 
scheme can easily be extended to any unifica- 
tion grammar with a context-free backbone by 
replacing symbol eqality with unification. 
Given a grammar G = (E, EN, S,R), we can 
define a relation ~ and a (partial) function ~?n: 
? ~/~ C EN ? R s.t. (A , r  / E r/~ iffA = lhs(r) 
? r~R : R ? Af + ~ E s.t. r lR ( r  , i )  = X iff 
rhs(r) = f iX% \]/3\[ = i - 1 
Figure 2 shows the correspondence between a 
simple grammar fragment and its concatena- 
t ion/or graph. 
Each tree can be represented by folding it 
onto the concatenation/or graph representing 
the grammar it was derived with, or, in other 
words, by appropriately annotating the graph 
itself. If N is the set of nodes of a parse tree 
obtained using grammar G, the corresponding 
folded tree is a partial function f
f :NxN- - -~RxAf  + 
such that f (n ,n ' )  = (r, k) implies that node n 
was expanded using rule r, and that node n' is 
its k th daughter in the tree (Fig.3). In the fol- 
lowing, we will use the inverse image of (r, k) un- 
der f ,  which we denote ?(r, k) : f - l ( ( r ,  k)) : 
{(n ,n ' ) l f (n ,n '  ) : ( r ,k)} C g ? N. This can in 
turn be seen as a partial function 
? : R ? Jkf+ ~ 2 N?N 
Disallowing the expansion of the k th element 
in the right-hand side of rule r by means of rule 
r' (assuming symbols match, i.e., (r/R(r, k), r') E 
rl2 ) results in suppressing a tree where: 
3n, n', n" E N, k' E .hf + 
\[<n, e ?(r, k) A <n', n"> ? ?(r', k')\] 
This check can be performed very efficiently 
once the tree is represented by the ? function, 
i.e., once it is folded, as all this requires is to 
compare the entries for (r, k) and (r', k') 2 with a 
procedure linear in the size of the entries them- 
selves. If we used a more traditional represen- 
tation, the same check would require traversing 
2In fact, it suffices to check the entries for (r', 1). 
A 
? nO 
nl ~ B 
? n2 ? 
c ~ n4 
? 
f n3 
c On5 
? n7 ? n8 f e 
?(rl ,  1) = {<no,n1), <rt4,rt5>} 
?(r l ,  2) = {<no, n2), (n4, n6)} 
?(r2, 1) = 0 
?(r2, 2) = 0 
?(r3, 1) = {(n2, n3)} 
?(r3,  2) = (<n2, n4>} 
?(r4, I) = {<n6, nT>} 
?(r4,  2) -~ {(n6, n8) } 
A 
r 1 --% ~ r2 
O^'~ " O 
C x \  
e .  ~ ~;, .e  
r ~ ~o" 
~ ; !  '~r4 
Figure 3: A tree and its folded representation. 
the whole tree. The worst-case complexity is 
still linear in the size of the tree, but in prac- 
tice, the number of nodes expanded using any 
given rule is much smaller than the total num- 
ber of nodes. 
Whenever a specialization is performed, all 
folded trees that are no longer licensed are 
removed; the concatenation/or graph for the 
grammar is updated to reflect the introduction 
and the elimination of rules; and the annota- 
tions on the affected edges are appropriately re- 
combined and distributed. If the performed spe- 
cialization is X C_ DRU(r, k) ~ {r}, 3 then the 
concatenation/or g aph is updated as follows 
= nux\{~) 
~r = ~r U {(lhs(r),~)l~ E X} \ {(lhs(r),r)} 
~(r" ,  i) = 
{ VR(r",i), r" ? X 
~\]R(r,i), r u E X , i  < k 
= VR(r' , i  -- k + 1), r" = aA(r ,  k, r') E X, 
k<i<k+m-1,  
~R(r, i -- m + 1), r" = RA(r, k, r') E X, 
i>k+m-1,  
where m = arity(r') = Irhs(r')l is the number 
of right-hand-side symbols of rule r ~. For each 
tree that is not eliminated as a consequence of
3If X = DRU(r, k) = {r}, then no update is needed. 
the specialization we have 
~(~",~) =
?(r",i), 
if r" ?~ X ,  r" ? r' 
?(~",/) k(<n',n">13n\[<n,n'> e ?(~, k)\]), 
if r" = r ~ 
{(n, n">13n', n", k'\[<n, n'> ~ ?(~, k) 
A(n ' ,n" )  E ?(r ' ,k ' )  A (n,n'")  E ?(r,i)\]} 
if r" = RA( r ,k , r  ~) E X , i  < k 
{(n, n")13n'\[(n, n') E ?(r, k)A 
(n', n") E ?(r', i -- k + 1)\]} 
i f r "=RA(r ,k , r  ~) EX ,  k<i<k+m-1,  
{ (n ,n" )13n ' ,n" ,k ' \ [ (n ,n '  ) E ?(r,k)A 
<n', n"> e ?(~', k')A 
(n, n'") E ?(r, i - m + 1)\]} 
if r" = RA(r ,  k, r ~) E X ,  i > k + m - 1, 
where again m = arity(r'). These updates can 
be implemented efficiently, requiring neither a 
traversal of the tree nor of the grammar. 
5 Exper imenta l  Resu l t s  
We specialized a broad-coverage LFG grammar 
for French on a corpus of technical documen- 
tation using the method described above. The 
treebank consisted of 960 sentences which were 
all known to be covered by the original gram- 
mar. For each sentence, all the trees returned by 
10 
R --  { r l ,  r2, r3, r4} s.t.  
rl: A -~cB 
r2: A --+ e A 
ra: B -+fA  
r4: B -arE  
~2 = {(A, r l) ,  (A, r2}, {B, r3), (B, r4)} 
C 
fiR: (rl, 1) ~ c 
(rl, 2) ~ B 
(r2,1) -+ e 
(r2, 2) ~ A 
(r3, 1) ~ f 
(r3, 2) ~ A 
(r4,1) I 
@4, 2) --~ e 
A 
r l  r 2 
? ? 
? ? / ? e / 
r 3 \ / r4  
f 
Figure 2: A tiny grammar and the correspond- 
ing concatenation/or graph. 
the original grammar were available, together 
with a manually assigned indication of which 
was the correct one. The environment used, 
the Xerox Linguistic Environment (Kaplan and 
Maxwell, 1996) implements a version of opti- 
mality theory where parses are assigned "opti- 
mality marks" based on a number of criteria, 
and are ranked according to these marks. The 
set of parses with the best marks are called the 
optimal parses for a sentence. The correct parse 
was also an optimal parse for 913 out of 960 sen- 
tences. Given this, the specialization was aimed 
at reducing the number of optimal parses per 
sentence. 
We ran a series of ten-fold cross-validation ex- 
periments; the results are summarized in the ta- 
ble in Fig.4. The first line contains values for 
the original grammar. The second line contains 
measures for the first-order pruning grammar, 
i.e., the grammar with all and only those rules 
actually used in correct parses in the training 
set, with no combination inhibited. Lines 3 and 
4 list results for fully specialized grammars. Re- 
sults in the third line were obtained with a value 
for ~corr equal to 15 times the value of Ainc in 
the objective function: in other words, during 
training we were willing to lose a correct parse 
only if at least 15 incorrect parses were canceled 
as well. Results in the fourth line were obtained 
when this ratio was reduced to 10. The average 
number of parses per sentence is reported in the 
first column, whereas the second lists the av- 
erage number of optimal parses. Coverage was 
measured as the fraction of sentences which still 
receive the correct parse with the specialized 
grammar. To assess the trade off between cover- 
age and ambiguity reduction, we computed the 
F-score 4 considering only optimal parses when 
computing precision. This measure should not 
be confused with the F-score on labelled brack- 
eting reported for many stochastic parsers; here 
precision and recall concern perfect matching of 
whole trees. Recall is the same as coverage: the 
ratio between the number of correct parses pro- 
duced by the specialized grammar and the to- 
tal number of correct parses (equalling the total 
number of sentences in the test set). Precision is 
the ratio between the number of correct parses 
produced by the specialized grammar and the 
total number of parses produced by the same 
grammar. The fourth column lists values for the 
F-score when equal weight is given to precision 
and recall. Intuitively, however, in many cases 
missing the correct parse is more of a problem 
than returning spurious parses, so we also com- 
puted the F-score with a much larger emphasis 
on recall, i.e., with a = 0.1. The corresponding 
values are listed in the last column. 
The average number of parses per sentence, 
both optimal and non-optimal, decreases signif- 
icantly as more and more aggressive specializa- 
tion.is carried out, and consequently, more cov- 
erage is lost. The most aggressive form of spe- 
4The F-score is the harmonic mean of recall and pre- 
cision, where precision is weighted a and recall 1 - a. 
11 
Avg.p/s Avg. o.p./s. Coverage (%) F(a-- 0.5) F(c~-- 0.1) 
orig. 1941 4.69 100 35.15 73.05 
f.o.pruning 184 3.38 89 40.64 71.89 
Acorr=lh)~inc 82 2.23 86 53.25 76.58 
~corr----lO)kin c 63 2.03 82.5 54.46 74.80 
Figure 4: Results of the 
cialization gives the highest F-score for c~ = 0.5, 
whereas omewhat more conservative parame- 
ter settings lead to a better F-score when re- 
call is valued more. A speedup of a factor 4 
is achieved already by first-order pruning and 
remains approximately the same after further 
specialization. 
6 Conclusions 
Broad-coverage rammars tend to be highly am- 
biguous, which may constitute a serious prob- 
lem when using them for natural-language pro- 
cessing. Corpus-independent compilation tech- 
niques, although useful for increasing efficiency, 
do little in terms of reducing ambiguity. 
In this paper we proposed a corpus-based 
technique for specializing a grammar on a do- 
main for which a treebank exists containing all 
trees returned for each sentence. This tech- 
nique, which builds extensively on previous 
work on explanation-based learning for NLP, 
consists in casting the problem as an optimiza- 
tion problem in the space of all possible spe- 
cializations of the original grammar. As initial 
candidate grammar, the first-order pruning of 
the original grammar is considered. Candidate 
successor grammars are obtained through the 
downward rule unfolding and specialization op- 
erator, that has the desirable property of never 
causing previously unseen parses to become 
available for sentences in the training set. Can- 
didate grammars are then assessed according to 
an objecting function combining rammar am- 
biguity and coverage, adapted to avoid overfit- 
ting. In order to ensure efficient computability 
of the objective function, the treebank is pre- 
viously folded onto the grammar itself. Exper- 
imental results using a broad-coverage lexical- 
functional grammar of French show that the 
technique allows effectively trading coverage for 
ambiguity reduction. Moreover, the parameters 
of the objective function can be used to control 
the trade off. 
specialization experiments. 
Acknowledgements  
We would like to thank the members of the 
MLTT group at the Xerox Research Centre Eu- 
rope in Grenoble, France, and the three anony- 
mous reviewers for valuable discussions and 
comments. This research was funded by the Eu- 
ropean TMR network Learning Computational 
Grammars. 
Re ferences  
Hiyan Alshawi, editor. 1992. The Core Language 
Engine. MIT Press. 
M. Butt, T.H. King, M.E. Nifio, and F. Segond. 
1999. A Grammar Writer's Cookbook. CSLI Pub- 
lications, Stanford, CA. 
Nicola Cancedda nd Christer Samuelsson. 2000. 
Experiments with corpus-based lfgspecialization. 
In Proceedings o/ the NAACL-ANLP 2000 Con- 
ference, Seattle, WA. 
Ronald Kaplan and John T. Maxwell. 1996. 
LFG grammar writer's workbench. Technical 
report, Xerox PARC. Available on-line as 
ftp://ftp.parc.xerox.com/pub/lfg/lfgmanual.ps. 
Manny Rayner and David Carter. 1996. Fast pars- 
ing using pruning and grammar specialization. In 
Proceedings of the ACL-96, Santa Cruz, CA. 
Manny Rayner. 1988. Applying explanation-based 
generalization to natural-language processing. 
In Proceedings o/ the International Con/erence 
on Fifth Generation Computer Systems, Tokyo, 
Japan. 
Christer Samuelsson and Manny Rayner. 1991. 
Quantitative evaluation of explanation-based 
learning as an optimization tool for a large-scale 
natural anguage system. In Proceedings o/the 
IJCAI-91, Sydney, Australia. 
Christer Samuelsson. 1994. Grammar specialization 
through entropy thresholds. In Proceedings ofthe 
ACL-94, Las Cruces, New Mexico. Available as 
cmp-lg/9405022. 
Khalil Sima'an. 1999. Learning Efficient Dis- 
ambiguation. Ph.D. thesis, Institute for Logic, 
Language and Computation, Amsterdam, The 
Netherlands. 
12 
