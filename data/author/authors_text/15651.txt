Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1865?1874, Dublin, Ireland, August 23-29 2014.
Synchronous Constituent Context Model for Inducing Bilingual
Synchronous Structures
Xiangyu Duan Min Zhang
?
Qiaoming Zhu
School of Computer Science & Technology, Soochow University
{xiangyuduan;minzhang;qmzhu}@suda.edu.cn
Abstract
Traditional Statistical Machine Translation (SMT) systems heuristically extract synchronous
structures from word alignments, while synchronous grammar induction provides better so-
lutions that can discard heuristic method and directly obtain statistically sound bilingual syn-
chronous structures. This paper proposes Synchronous Constituent Context Model (SCCM) for
synchronous grammar induction. The SCCM is different to all previous synchronous grammar
induction systems in that the SCCM does not use the Context Free Grammars to model the bilin-
gual parallel corpus, but models bilingual constituents and contexts directly. The experiments
show that valuable synchronous structures can be found by the SCCM, and the end-to-end ma-
chine translation experiment shows that the SCCM improves the quality of SMT results.
1 Introduction
Traditional Statistical Machine Translation (SMT) learns translation model from bilingual corpus that
is sentence aligned. No large-scale hand aligned structures inside the parallel sentences are usually
available to the SMT community, while the aligned structures are essential for training the translation
model. Thus, various unsupervised methods had been explored to automatically obtain aligned structures
inside the parallel sentences. Currently, the dominant method is a two step pipeline that obtains word
alignments by unsupervised learning (Brown et al., 1993) at the first step, then obtains aligned structures
at the second step by heuristically extracting all bilingual structures that are consistent with the word
alignments.
The second step in this two step pipeline is problematic due to its obtained aligned structures, whose
counts are heuristically collected and violate valid translation derivations, while most SMT decoders
perform translation via valid translation derivations. This problem leads to researches on synchronous
grammar induction that discards the heuristic method and the two separate steps pipeline.
Synchronous grammar induction aims to directly obtain aligned structures by using one statistically
sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level
(Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free
Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and
Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al.,
2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or
discriminative modeling.
This paper departs from using the above two traditional CFGs-based grammars, and proposes Syn-
chronous Constituent Context Model (SCCM) which models synchronous constituents and contexts
directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is
inspired by researches on monolingual grammar induction, whose experience is valuable to the syn-
chronous grammar induction community due to its standard evaluation on released monolingual tree-
banks, while no hand annotated bilingual synchronous treebank is available for evaluating synchronous
?
Corresponding Author
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1865
grammar induction. According to the evaluation results, the state-of-the-art monolingual grammar induc-
tion was achieved by Bayesian modeling of the Constituent Context Model (CCM) (Duan et al., 2013;
Klein and Manning, 2002), while traditional CFGs based monolingual grammar induction methods per-
form well below the CCM.
In view of the significant achievements of the CCM in monolingual grammar induction, we propose
the SCCM to apply the CCM to the bilingual case. The tremendous possible constituents and contexts
incurred in this bilingual case put a challenge for the SCCM to model such kind of sparse variables. We
further propose a non-parametric Bayesian Modeling of the SCCM to cope with the sparse variables.
Experiments on Chinese-English machine translation show that meaningful synchronous phrases can be
detected by our SCCM, and the performance of the end-to-end SMT is significantly improved.
The rest of the paper is structured as follows: we propose the SCCM in Section 2. The non-parametric
Bayesian modeling of the SCCM is presented in Section 3, followed by the presentation of posterior
inference for the Bayesian SCCM. Then experiments and results are presented. Conclusion are presented
in the final section.
2 Synchronous Constituent Context Model (SCCM)
We propose the SCCM to model synchronous structures explicitly. Unlike Synchronous Context Free
Grammars (SCFGs) which are defined on latent production rules of parallel corpus, the SCCM deals
with both synchronous tree spans (syn spans) and non-synchronous spans (non-syn spans). All spans
are represented by two kinds of strings: bilingual constituents and bilingual contexts. The SCCM is a
generative model defined over such representations.
2.1 Bilingual Constituents and Contexts
By extending the concept of constituents and contexts introduced in (Klein and Manning, 2002), we
define bilingual constituents and contexts as follows. Bilingual constituents are pairs of contiguous
surface strings of sentence spans (bilingual subsequences), bilingual contexts are tokens preceding and
following the bilingual constituents. In the SCCM, each bi-span in a sentence pair, either a syn span or
a non-syn span, is represented by a bilingual constituent and a bilingual context.
Fig. 1 gives an illustration of the bilingual constituents and contexts. In Fig. 1-(a), a latent syn-
chronous tree over the example sentence pair is illustrated. With the word alignments shown in the
sentence pair, the latent tree over the target sentence ?e
1
e
2
e
3
? can be inferred. For the ease of presen-
tation, the latent target side tree is neglected in Fig. 1-(a).
Given the synchronous tree, two sets of bilingual constituents and contexts can be extracted as shown
in the two tables of Fig. 1. One is about syn spans, the other is about non-syn spans. 3 appearing in
the contexts denotes a sentence boundary. nil appearing in the constituents of the non-tree spans denotes
an empty span, which is actually a space between two terminals (or between a terminal and 3).
2.2 Generative Model
The SCCM computes the joint probability of a sentence pair S and its synchronous tree T as below:
P (S, T ) = P (S|T )P (T ) = P (S|T )P (B)P (T |B) (1)
= P (S|T )P (B)
?
0?i?j?m
0?p?q?n
P (?
ij,pq
|B
ij,pq
)P (?
ij,pq
|B
ij,pq
)
where B denotes a synchronous bracketing skeleton, in which no words are populated. Fig. 1-(b) shows
the skeleton of Fig. 1-(a). The skeleton B is considered being filled by the synchronous tree T , and
P (T |B) is decomposed into conditional probabilities of bilingual constituents ? and contexts ? condi-
tioning on B
ij,pq
, a Boolean variable indicating whether the under-consideration bi-span <i, j><p, q>
is a syn span or not. In particular, ?
ij,pq
denotes the bilingual constituent spanning from i to j on source
side sentence, and spanning from p to q on target side sentence. ?
ij,pq
denotes the context of ?
ij,pq
.
1866
  
 
 
 
 
 
 
f1 f2 f3 0 1 2 3 
e1 e2 e3 0 1 2 3 
syn span <0,1><2,3> <1,2> <1,2> <2,3><0,1> <0,2><1,3> <0,3><0,3> 
constituent (f1)(e3) (f2)(e2) (f3)(e1) (f1 f2)(e2 e3) (f1 f2 f3)(e1 e2 e3) 
context (-f2)(e2-) (f 1-f3)(e1- e3) (f2-)(- e2) (-f3)(e1-) (-)(-) 
 
non-syn span <1,3><0,1> <1,1><1,2>       ? 
constituent (f2 f3)(e1) (nil)(e2)         ? 
context (f1-)(-e2) (f 1-f2)(e1- e3)      ? 
 
 
 
 
 
 
 
 
 
 
 
 
(a) (b)
Figure 1: Illustration of bilingual constituents and contexts over a sentence pair which consists of a
source side sentence ?f
1
f
2
f
3
? and a target side sentence ?e
1
e
2
e
3
?. In (a), the bottom numbers around
each word are indexes for denoting spans. A synchronous tree is illustrated in (a), based on which two
sets of bilingual constituents and contexts are extracted as shown in the two tables below the tree. Take a
syn span <1,2><1,2> for example, the source side span <1,2> is ?f
2
? and the target side span <1,2>
is ?e
2
?. They constitutes a bilingual constituent ?(f
2
)(e
2
)?, whose context is ?(f
1
-f
3
)(e
1
-e
3
)? that is
preceding and following the bilingual constituent. Figure (b) shows the skeleton of figure (a).
B
ij,pq
is defined as below:
B
ij,pq
=
{
1 if bispan < i, j >< p, q > is a syn span
0 otherwise
In the SCCM, skeletons Bs are restricted to be binary branching and are distributed uniformly. Fur-
thermore, since T and S are consistent, P (S|T ) is always equal to 1 in Eq. (1). Therefore, we can infer
(with the expansion of the continued multiplication operator of Eq. (1) ):
P (S, T ) ?
?
<i,j><p,q>?T
(P (?
ij,pq
|B
ij,pq
= 1)P (?
ij,pq
|B
ij,pq
= 1)) (2)
?
<i,j><p,q> ??T
(P (?
ij,pq
|B
ij,pq
= 0)P (?
ij,pq
|B
ij,pq
= 0))
where <i, j><p, q> ? T indicates that bi-span <i, j><p, q> is a syn span contained in T ,
<i, j><p, q> ?? T indicates otherwise case. Formula (2) is the basis for Bayesian modeling of the
SCCM and the posterior inference that are proposed in the following sections.
3 Bayesian Modeling for the SCCM
For the SCCM, the posterior of a synchronous tree T given the observation of a sentence pair S is:
P (T |S) ? P (S, T ). As shown in formula (2), it turns out that the posterior P (T |S) depends on the four
kinds of distributions:
P (?
ij,pq
|B
ij,pq
= 1) P (?
ij,pq
|B
ij,pq
= 1)
P (?
ij,pq
|B
ij,pq
= 0) P (?
ij,pq
|B
ij,pq
= 0)
1867
We propose to define two kinds of Bayesian priors over the constituents related variables ?
ij,pq
|B
ij,pq
and the contexts related variables ?
ij,pq
|B
ij,pq
respectively. Since constituents exhibits richer appear-
ances than contexts, the proposed Bayesian prior over ?
ij,pq
|B
ij,pq
is more complicate than that over
?
ij,pq
|B
ij,pq
.
Specifically, one of the non-parametric Bayesian priors, the Pitman-Yor-Process (PYP) prior, is defined
on ?
ij,pq
|B
ij,pq
. The PYP prior can produce the power-law distribution (Goldwater et al., 2009) that is
commonly observed in natural languages, and can flexibly model distributions on layer structures due to
its defined distribution on distribution hierarchy. The PYP prior had been successfully applied on many
NLP tasks such as language modeling (YeeWhye, 2006), word segmentation (Johnson et al., 2007b;
Goldwater et al., 2011), dependency grammar induction (Cohen et al., 2008; Cohn et al., 2010), grammar
refinement (Liang et al., 2007; Finkel et al., 2007) and Tree-Substitution Grammar induction (Cohn et
al., 2010). We use the PYP to model the constituents? layered structure by using the PYP?s distribution
hierarchy. On ?
ij,pq
|B
ij,pq
, we use the Dirichlet distribution for its simplicity because contexts appear in
much fewer kinds of surface strings than those of constituents.
3.1 The PYP Prior over Bilingual Constituents
Constituents consist of both words and POS tags. Though in much monolingual grammar induction
works, only POS tag sequences were used as the observed constituents for their significant hints of
phrases (Klein and Manning, 2002; Cohn et al., 2010), our work needs considering raw words as obser-
vation data too because word alignments encode the important translation correspondence and contribute
to synchronous bi-spans. But it causes severe data sparse problem due to the quite large number of unique
constituents consisting of both words and POS tags. Besides, constituents can be extremely long which
intensify the data sparse problem. So, solely using the surface strings of constituents is impractical.
In this section, we propose a hierarchical representation of constituents to overcome the data sparse
problem and use the PYP prior on this kind of representation. From top to bottom, the hierarchical rep-
resentation encodes the information of a bilingual constituent from fine-grained level to coarse-grained
levels. The probability of a fine-grained level can be backed-off to the probabilities of coarse-grained
levels.
The first (top) level of the hierarchical representation is the bilingual constituent itself. The second
level is composed of two sequences: one is word sequence, the other is POS tags sequence. The third
level mainly decomposes the second level into boundaries and middle words/POSs. Since the target of
inducing synchronous structures in this paper is to induce the latent phrasal equivalences of a parallel
sentence, boundaries of bilingual constituents play the key role of identifying phrasal equivalences. The
third level is the function to make use of boundaries. Fig. 2 gives an illustration of the hierarchical
representation.
w1p1 w2p2 w3p3 w4p4
w1 w2 w3 w4 p1 p2 p3 p4
w1 w2 w4 w3 p1 p2 p4 p3
Figure 2: Illustration of the hierarchical representation of a bilingual constituent ?w
1
p
1
w
2
p
2
w
3
p
3
w
4
p
4
?. Here w and p denote word and POS respectively, and the suffixes denote positions. Note that
both w and p are composite, w denotes a source side word and a target side word, and p denotes the
POS case. The second level decomposes the first level into a word sequence and a POS sequence, and
the third level decomposes further into boundaries and middle words/POSs. The boundary width in this
figure is two for left side boundary and one for right side boundary.
1868
The PYP prior encodes distribution on distribution. Recursively using the PYP prior can create a
distribution hierarchy, which is appropriate for modeling the distribution over the hierarchical repre-
sentations of constituents. Smoothing is fulfilled through backing-off fine-grained level distributions to
coarse-grained level distributions.
3.1.1 The PYP Hierarchy
We define the PYP hierarchy over the hierarchical representation of bilingual constituents in a top-down
manner. For the topmost (first) level:
?
ij,pq
|B
ij,pq
= b ? G
first
b
G
first
b
? PY P (d
first
b
, ?
first
b
, P
word?pos
(.|B
ij,pq
= b))
The PYP has three parameters: (d
first
b
, ?
first
b
, P
word?pos
). P
word?pos
(.|B
ij,pq
= b) is a base
distribution over infinite space of bilingual constituents conditioned on span type b, which provides
the back-off probability of P (?
ij,pq
|B
ij,pq
= b). The remaining parameters d
first
b
and ?
first
b
control the
strength of the base distribution.
The back-off probability P
word?pos
(?
ij,pq
= x|B
ij,pq
= b) is defined as below:
P
word?pos
(?
ij,pq
= x|B
ij,pq
= b)) = P
word
(Rw(x)|b)? P
pos
(Rp(x)|b)
where Rw(x) is the function returning a word sequence of a bilingual constituent x, Rp(x) returning
the correspondent POS sequence. This is the second level of the hierarchical representation of bilingual
constituents as illustrated in Fig. 2. Further, Rw(x) and Rp(x) are decomposed into the third level of
the hierarchy. Taking Rw(x) for example:
P
word
(Rw(x)|B
ij,pq
= b)) = P
word?bound
(Rwb(x)|b)?
1
|W |
|Rw(x)|?|Rwb(x)|
where Rwb is a function returning a word sequence?s boundary representation, |W | is the vocabulary
size, |Rw(x)| ? |Rwb(x)| is the number of the words in Rw(x) excluding those in the boundary rep-
resentation. The above equation models the generation of a word sequence with surface string Rw(x)
(given b) by first generating its boundary representation Rwb(x), then generating its middle words from
a uniform distribution over the vocabulary. P
pos
(Rp(x)|B
ij,pq
= b)) is defined similarly.
We put the Dirichlet prior over P
word?bound
(Rwb(x)|b):
Rwb(x)|b ? Discrete(G
Rwb
b
)
G
Rwb
b
? Dirichlet(?
b
)
For P
pos?bound
(Rpb(x)|b), similar definition to P
word?bound
(Rwb(x)|b) is applied.
3.2 The Dirichlet Prior over Bilingual Contexts
The Dirichlet prior is defined as below:
?
ij,pq
|B
ij,pq
= b ? Discrete(G
Dir
b
)
G
Dir
b
? Dirichlet(?
b
)
A context ?
ij,pq
(given the specific span type b) is drawn i.i.d according to a multinomial parameter
G
Dir
b
, which is drawn from the Dirichlet distribution with a real value parameter ?
b
.
1869
4 MCMC Sampling for Inferring the Latent Synchronous Trees
We approximate the distribution over latent synchronous trees by sampling them from the posterior
P (T |S), where T is a latent synchronous tree of a sentence pair S. As presented in the beginning of
section 3, the posterior depends on P (?
ij,pq
|B
ij,pq
= b) and P (?
ij,pq
|B
ij,pq
= b), on which we put
the PYP prior and the Dirichlet prior respectively. Because of integrating out all Gs in all of the priors,
interdependency between samples of ?
ij,pq
|B
ij,pq
= b or ?
ij,pq
|B
ij,pq
= b is introduced, resulting in
simultaneously obtaining multiple samples impractical. On the other hand, blocked sampling, which ob-
tains sentence-level samples simultaneously (Blunsom and Cohn, 2010; Cohn et al., 2010; Johnson et al.,
2007a) is attractive for the fast mixing speed and the easy application of standard dynamic programming
algorithms.
4.1 Metropolis-Hastings (MH) Sampler
We apply a MH sampler similar to (Johnson et al., 2007a) to overcome the difficulty of obtaining multi-
ple samples simultaneously from posterior. The MH sampler is a MCMC technique that draws samples
from a true distribution by first drawing samples simultaneously from a proposal distribution, and then
correcting the samples to the true distribution by using an accept/reject test. In practical, the proposal
distribution is designed to facilitate the use of blocked sampling that applies standard dynamic program-
ming, and the resulting samples are corrected by the accept/reject test to the true distribution.
In our case, the proposal distribution is theMaximum-a-Posteriori (MAP) estimate of P (?
i,j
|B
i,j
= b)
and P (?
i,j
|B
i,j
= b), and the blocked sampling of T applies a dynamic programming algorithm that is
based on the inside chart derived from a transformation of Eq. (1):
P (S, T ) = K(S)
?
<i,j><p,q>?T
?(ij, pq)
where ?(ij, pq) =
P (?
ij,pq
|B
ij,pq
= 1)P (?
ij,pq
|B
ij,pq
= 1)
P (?
ij,pq
|B
ij,pq
= 0)P (?
ij,pq
|B
ij,pq
= 0)
K(S) is a constant given S. The inside chart I can be constructed recursively as below:
I
ij,pq
=
?
?
?
?
?
?
?
?
?
?
?
?(ij, pq) if j ? i ? 1 and q ? p ? 1
?(ij, pq)
?
i?u?j
p?v?q
(I
iu,pv
I
uj,vq
+ I
iu,vq
I
uj,pv
) otherwise
Based on this inside chart, a synchronous tree can be top-down sampled (Johnson et al., 2007a), then
is accepted or rejected by the MH-test to correct to the true distribution.
5 Experiments
The experiments were conducted on both a pilot word alignment task and an end-to-end Chinese-to-
English machine translation task to test the quality of the learned synchronous structures by the SCCM.
The bi-side monolingual gold bracketings contained in Penn treebanks were not used for evaluating the
quality of the learned synchronous structures because of great syntactic divergence between source tree
and target tree, which results in that gold monolingual syntactic trees on both sides are asynchronous
(large number of tree nodes can not be aligned). The synchronous grammar induction community as-
sumes the existence of synchronous grammar for MT, and do not evaluate synchronous grammar induc-
tion on monolingual gold treebanks because of their asynchronous property. The synchronous grammar
induction community is not the same with the multilingual grammar induction community, which targets
at inducing bi-side monolingual syntactic trees. Due to the same reason, our synchronous bracketing
induction method was not evaluated on bi-side monolingual bracketing trees which are asynchronous.
1870
5.1 Sampler Configuration
Our sampler was initialised with trees through a random split process. Firstly, we used GIZA++ mod-
el 4 to get source-to-target and target-to-source word alignments, and used grow-diag-final-and (gdfa)
heuristic to extract reliable word alignments for each sentence pair. Secondly, we randomly split each
sentence pair in a top-down manner, and make sure that each split is consistent with the GIZA++ gdfa
word alignments. For example, given a sentence pair of m source words and n target words, we random-
ly choose a split point at each side and the alignment type (straight alignment or inverted alignment),
then recursively build bi-spans further on each new split. Finally, a synchronous binary tree is built at
the end of this process
1
. Note that all splits must be consistent with the GIZA++ gdfa word alignments.
When a piece of word alignments (such as non-ITG alignment structure) do not permit binary split, we
keep this structure unsplitted and continue split only on its sub-structures that are ITG derivable.
Our sampler ran 200 iterations for all data. After each sampling iteration, we resample all the hyper-
parameters using slice-sampling, with the following priors: d ? Beta(1, 1), ? ? Gamma(10, 0.1).
The time complexity of our inference algorithm is O(n
6
), which is not practical in applications. We
reduce the time complexity by only considering bi-spans that do not violate GIZA++ intersection word
alignments (intersection of source-to-target and target-to-source word alignments) (Cohn and Haffari,
2013).
5.2 Word Alignment Task
5.2.1 Experimental Setting
Since there are no annotated synchronous treebanks, we evaluate the SCCM indirectly by evaluating its
output word alignments on a gold standard English Chinese parallel tree bank with hand aligned word
alignments referred as HIT corpus
2
. The HIT corpus, which was collected from English learning text
books in China as well as example sentences in dictionaries, was originally designed for annotating
bilingual tree node alignments. The annotation strictly reserves the semantic equivalence of the aligned
sub-tree pair. The byproduct of this corpus is the hand aligned word alignments, which was utilized
to evaluate word alignment performance
3
. The word segmentation, tokenization and parse-tree in the
corpus were manually constructed or checked. The statistics of the HIT corpus are shown in table 1.
Table 1: Corpus statistics of the HIT corpus.
ch en
sent 16131
word 210k 209k
avg. len. 13.06 13.0
5.2.2 Results
We adopt the commonly used metric: the alignment error rate (AER) to evaluate our proposed align-
ments (a) against hand-annotated alignments, which are marked with sure (s) and possible (p) align-
ments. The AER is given by (the lower the better):
AER(a, s, p) = 1?
|a ? s|+ |a ? p|
|a|+ |s|
In the HIT corpus, only sure alignments were annotated, possible alignments were bypassed because
of the strict annotation standard of semantic equivalence.
The word alignments evaluation results are reported in Table 2. The baseline was GIZA++ model
4 in both directions with symmetrization by the grow-diag-final-and heuristic (Koehn et al., 2003). A
1
The initialization with different random split bi-trees results in marginal variance of performances.
2
HIT corpus is designed and constructed by HIT-MITLAB. http://mitlab.hit.edu.cn/index.php/resources.html
3
We did not use annotated tree node alignments for synchronous structure evaluation because the coverage of tree nodes
that can be aligned is quite low. The reason of low coverage is that Chinese and English exhibit great syntax divergences from
monolingual treebank point of view.
1871
released induction system - PIALIGN (Neubig et al., 2011)
4
was also experimented to compare with our
proposed induction system - SCCM.
PIALIGN is a model that generalizes adaptor grammars for machine translation (MT), while our mod-
el is to generalize CCM for MT. Adaptor grammars has been successfully applied on shallow unsuper-
vised tasks such as morphlogical/word analysis, while CCM has obtained state-of-the-art performance
on the more complex unsupervised task - inducing syntactic trees. In view of CCM?s successful mono-
lingual application, we generalize it to bilingual case. In depth comparison: our SCCM deals with both
consituents and distituents, and contexts of them, while PIALIGN only deals with constituents. Fur-
thermore, SCCM does not model non-terminal rewriting rules, while PIALIGN model those rules which
can rewrite a non-terminal into a complete subtree as adaptor grammars does. In addition, PIALIGN
adopts a beam search algorithm of (Saers et al., 2009). Through setting small beam size, PIALIGN?s
time complexity is almost O(n
3
). But as critisized by (Cohn and Haffari, 2013), their heuristic beam
search algorithm does not meet either of the Markov Chain Monte Carlo (MCMC) criteria of ergodic-
ity or detailed balance. Our method adopts MCMC sampling (Johnson et al., 2007a) which meets the
MCMC criteria.
We can see that the two induction systems perform significantly better than GIZA++, and our proposed
SCCM performs better than PIALIGN. Manual evaluation for the quality of the phrase pairs generated
from word alignments is also reported in Table 2. We considered the top-100 high frequency phrase pairs
that are beyond word level and less than six words on both sides, and report the proportion of reasonably
well phrase pairs through manual check. We found that more good phrase pairs can be derived from the
SCCM?s word alignments than from others.
Table 2: Quality of word alignments and their generated phrase pairs.
AER good phrase pairs proportion
GIZA++ 0.322 0.493
PIALIGN 0.263 0.531
SCCM 0.255 0.534
5.3 Machine Translation Task
5.3.1 Experimental Setting
A released tourism-related domain machine translation data was used in our experiment. It consists of a
parallel corpus extracted from the Basic Travel Expression Corpus (BTEC), which had been used in
evaluation campaigns of the yearly International Workshop on Spoken Language Translation (IWSLT).
Table 3 lists statistics of the corpus used in the experiment.
Table 3: Statistics of the corpus used by IWSLT
ch en
sent 23k
word 190k 213k
avg. len. 8.3 9.2
We used CSTAR03 as development set, used IWSLT04 and IWSLT05 official test set for test. A
4-gram language model with modified Kneser-Ney smoothing was trained on English side of parallel
corpus. We use minimum error rate training (Och, 2003) with nbest list size 100 to optimize the fea-
ture weights for maximum development BLEU. Experimental results were evaluated by case-insensitive
BLEU-4 (Papineni et al., 2001). Closest reference sentence length was used for brevity penalty.
5.3.2 Results
Following (Levenberg et al., 2012; Neubig et al., 2011; Cohn and Haffari, 2013), we evaluate our model
by using the SCCM?s output word alignments to construct a phrase table. As a baseline, we train a
phrase-based model using the moses toolkit
5
based on the word alignments obtained using GIZA++
4
http://www.phontron.com/pialign/
5
http://www.statmt.org/moses
1872
model 4 in both directions and symmetrized using the grow-diag-final-and heuristic (Koehn et al., 2003).
For comparison with CFG-based induction systems, word alignments generated by the PIALIGN were
also used to train a phrase-based model.
In the end-to-end MT evaluation, we used the standard set of features: relative-frequency and lexical
translation model probabilities in both directions; distance-based distortion model; language model and
word count. The evaluation results are reported in table 4. Word alignments derived by the two induction
systems can be more helpful to obtain better translations than GIZA++ derived word alignments. The
SCCM, while departing from traditional CFG-based methods, achieves comparable translation perfor-
mance to the PIALIGN.
Table 4: BLEU on both the development set: CSTAR03, and the two test sets: IWSLT04 and IWSLT05.
CSTAR03 IWSLT04 IWSLT05
GIZA++ 0.4304 0.4190 0.4866
PIALIGN 0.4661 0.4556 0.5248
SCCM 0.4560 0.4469 0.5193
6 Conclusion
A new model for synchronous structure induction is proposed in this paper. Different to all the previous
works that are based on Context Free Grammars, our proposed SCCM deals with bilingual constituents
and contexts explicitely so that bilingual translational equivalences can be directly modeled. A non-
parametric Bayesian modeling of the SCCM is applied to cope with the sparse representations of bilin-
gual constituents and contexts. Both intrinsic evaluation on word alignments and extrinsic evaluation on
end-to-end machine translation were conducted. The intrinsic evaluation show that the highest quality
word alignments were obtained by our proposed SCCM. Such statistically sound word alignments of
the SCCM were used in the extrinsic evaluation on machine translation, showing that significantly better
translations were achieved than those obtained by using the word alignments of GIZA++, the widely
used word aligner in the two-step pipeline.
Acknowledgments
This work was supported by the National Natural Science Foundation of China under grant No.
61273319, and grant No. 61373095. Thanks for the helpful advices of anonymous reviewers.
References
Phil Blunsom and Trevor Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency
parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages
1204?1213. Association for Computational Linguistics.
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Shay B Cohen, Kevin Gimpel, and Noah A Smith. 2008. Logistic normal priors for unsupervised probabilistic
grammar induction. In Proceedings of the Advances in Neural Information Processing Systems.
Trevor Cohn and Phil Blunsom. 2009. A bayesian model of syntax-directed tree to string grammar induction. In
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume
1, pages 352?361. Association for Computational Linguistics.
Trevor Cohn and Gholamreza Haffari. 2013. An infinite hierarchical bayesian model of phrasal translation. In
Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of
Machine Learning Research, 11:3053?3096.
Xiangyu Duan, Zhang Min, and Chen Wenliang. 2013. Smoothing for bracketing induction. In Proceedings of
23rd International Joint Conference on Artificial Intelligence. AAAI Press/International Joint Conferences on
Artificial Intelligence.
1873
Jenny Rose Finkel, Trond Grenager, and Christopher D Manning. 2007. The infinite tree. In ANNUAL MEETING-
ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 272.
Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2011. Producing power-law distributions and damping
word frequencies with two-stage language models. Journal of Machine Learning Research, 12:2335?2382.
Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007a. Bayesian inference for pcfgs via markov chain
monte carlo. In Proceedings of Human Language Technologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139?146.
Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007b. Adaptor grammars: A framework for specify-
ing compositional nonparametric bayesian models. Proceedings of Advances in neural information processing
systems, 19:641.
Dan Klein and Christopher Manning. 2002. A generative constituent-context model for improved grammar induc-
tion. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 128?135.
Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 48?54. Association for Computational Linguistics.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012. A bayesian model for learning scfgs with discontiguous
rules. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and
computational natural language learning, pages 223?232. Association for Computational Linguistics.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007. The infinite PCFG using hierarchical Dirichlet process-
es. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning
(EMNLP/CoNLL).
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised
model for joint phrase alignment and extraction. In Proceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language Technologies-Volume 1, pages 632?641. Association for
Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for
Computational Linguistics.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009. Learning stochastic bracketing inversion transduction gram-
mars with a cubic time biparsing algorithm. In Proceedings of the 11th International Conference on Parsing
Technologies, pages 29?32. Association for Computational Linguistics.
Xinyan Xiao and Deyi Xiong. 2013. Max-margin synchronous grammar induction for machine translation. In
EMNLP.
Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, and Shouxun Lin. 2012. Unsupervised discriminative induction of
synchronous grammar for machine translation. In COLING, pages 2883?2898.
Teh YeeWhye. 2006. A bayesian interpretation of interpolated kneser-ney. In Technical Report TRA2/06. School
of Computing, National University of Singapore.
1874
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2161?2171, Dublin, Ireland, August 23-29 2014.
Employing Event Inference to Improve Semi-Supervised Chinese 
Event Extraction 
 
 
Peifeng Li, Qiaoming Zhu, Guodong Zhou 
School of Computer Science & Technology 
Soochow University, Suzhou, 215006, China 
{pfli, qmzhu, gdzhou}@suda.edu.cn 
 
 
 
Abstract 
Although semi-supervised model can extract the event mentions matching frequent event patterns, it suf-
fers much from those event mentions, which match infrequent patterns or have no matching pattern. To 
solve this issue, this paper introduces various kinds of linguistic knowledge-driven event inference 
mechanisms to semi-supervised Chinese event extraction. These event inference mechanisms can capture 
linguistic knowledge from four aspects, i.e. semantics of argument role, compositional semantics of trig-
ger, consistency on coreference events and relevant events, to further recover missing event mentions 
from unlabeled texts. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mech-
anisms significantly outperform the refined state-of-the-art semi-supervised Chinese event extraction 
system in F1-score by 8.5%. 
1 Introduction 
An event is a specific occurrence involving arguments (participants and attributes) of the specific roles. 
In an event, trigger is the main word which most clearly expresses its occurrence, so recognizing an 
event can be recast as identifying a corresponding trigger. An event may have several arguments, 
which are entity mentions (e.g., person name, time, location, etc.) and must fulfill the corresponding 
roles. Take the following sentence as an example: 
S1: On the 25th Dec. (A1: Artifact), peacekeepers (A2: Artifact) returned (E1: Transport) to Am-
man (A3: Place) by flight (A4: Vehicle). 
For this example, an event extraction system should identify one event mention E1, which is trig-
gered by verb ?returned? whose event type is Transport, with four arguments, ?peacekeepers?, ?25th 
Dec.?, ?flight?, and ?Amman?, fulfilling the roles of Artifact, Time, Vehicle, and Place, respectively. 
Automatically extracting events from free texts is a higher-level Information Extraction (IE) task, 
which is still a challenge due to the complexity of natural language and the domain-specific nature, 
especially in Chinese for its specific characteristics. In particular, most of previous studies have fo-
cused on English event extraction, while only a few concern Chinese. 
Currently, supervised learning models have dominated event extraction. To reduce the labeled data 
required, a few semi-supervised models have been applied to English event extraction (e.g., Riloff 
1996; Yangarber et al., 2000; Stevenson and Greenwood, 2005; Huang and Riloff, 2012). Since classi-
fier-based model needs dozens of annotated documents to train model, most of previous semi-
supervised models focused on pattern-based approach, which only needed a few seed (event) patterns. 
In those pattern-based approaches, frequent event patterns, which occur in many documents, were 
chosen as relevant patterns to match event mentions in unlabeled texts. However, the order of words in 
a Chinese sentence is rather agile for its open and flexible structure, and different orders might express 
the same meaning due to the semantics-driven nature of the Chinese language. This results in the di-
versity of Chinese event patterns and numerous infrequent patterns, even some event mentions having 
no matching patterns. Hence, it is an issue to extract the event mentions with infrequent patterns. 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 
2161
In this paper, we first implement a pattern-based semi-supervised model for Chinese event extrac-
tion as a baseline, following the state-of-the-art system as described in (Liao and Grishman, 2010a) 
and then refine this model to suit Chinese event extraction. Moreover, we propose various kinds of 
novel linguistic knowledge-driven event inference mechanisms to address the above issue and recover 
missing event mentions. These event inference mechanisms can capture the linguistic knowledge from 
semantics of argument role, compositional semantics of trigger, consistency on coreference events and 
relevant events. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mecha-
nisms dramatically outperform the baseline. 
The rest of this paper is organized as follows. Section 2 overviews related work. Section 3 presents 
the refined semi-supervised model for Chinese event extraction. Section 4 proposes several linguistic 
knowledge-driven event inference mechanisms. Section 5 reports and analyzes the experimental re-
sults. Finally, we conclude our work in Section 6. 
2 Related Work 
Almost all previous semi-supervised models focus on English event extraction, which can be subdi-
vided into pattern-based models (e.g., Riloff, 1996; Yangrber et al., 2000; Liao and Grishman, 2010a; 
Chambers and Jurafsky, 2011; Balasubramanian et al., 2013) and classifier-based models (e.g., Chieu 
et al., 2003; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009; Liu and Strzalkowski, 2012; 
Wang et al., 2013). Classifier-based models normally require a small set of annotated data (e.g., 100 
annotated documents), while pattern-based models need dozens of high quality seed patterns. 
Riloff (1996) first divided unlabeled documents into irrelevant and relevant documents, and the lat-
ter was much likely to contain further relevant patterns. Then event patterns from relevant documents 
were generated by using an annotated data and a set of heuristic rules. Yangarber et al. (2000) pro-
posed a document-centric view to boost a semi-supervised event extraction system, which assumes 
relevant documents always contain some shared patterns. Yangarber (2003) further introduced multi-
ple learners into the bootstrapping procedure to make the final decision on the combination of multiple 
learners on distinct event types. Huang and Riloff (2012) employed role-identifying nouns, which pro-
posed by Phillips and Riloff (2007), as seed terms to extract patterns from relevant documents and 
then generated the labeled instances to train three classifiers in their event extraction system. 
As an alternative, Stevenson and Greenwood (2005) proposed a pattern similarity-centric view and 
selected relevant patterns on similarity scores. Normally, bootstrapping on the document-centric view 
tends to accept the irrelevant patterns with a high occurrence frequency in relevant documents. To ad-
dress this problem, Liao and Grishman (2010a) introduced a pattern similarity metric into the docu-
ment-centric view as a filter to eliminate those irrelevant patterns. Liao and Grishman (2011) further 
applied an information retrieval mechanism to detect relevant documents and proposed a self-training 
strategy for bootstrapping. 
In addition, several studies focused on the event pattern representation, such as pairwise (e.g., Sub-
ject-Verb, Verb-Object) (Chambers and Jurafsky, 2008, 2009), SVO (Subject-Verb-Object) 
(Yangarber, 2000; Balasubramanian et al., 2013), chain (Sudo et al., 2001), subtree (Sudo et al., 2003) 
and complex pattern (Liu and Strzalkowski, 2012). 
In the literature, only one paper concerns semi-supervised Chinese event extraction. Chen and Ji 
(2009a) applied various kinds of cross-lingual features in the bootstrapping procedure to extract Chi-
nese event. With the help of over 500 annotated seed event mentions in 100 documents, they only 
achieved 35% in F1-score. This indicates the critical challenge in semi-supervised Chinese event ex-
traction. 
Only a few studies concern event inference mechanisms. Ji and Grishman (2008) employed a rule-
based approach to propagate consistent triggers and arguments across topic-related documents. Liao 
and Grishman (2010b) employed cross-event consistent information to improve sentence-level event 
extraction. Hong et al. (2011) regarded entity type consistency as a key feature to predict event men-
tions and adopted an information retrieval mechanism to promote event extraction. Li et al. (2013) 
proposed a global argument inference model on Chinese argument extraction to explore specific rela-
tionships among relevant event mentions to recover those inter-sentence arguments in the sentence, 
discourse and document layers. Li et al. (2014) also introduced Markov Logic Network (MLN) to cap-
ture the discourse-level consistency between Chinese trigger mentions to further recover those poor-
2162
context event mentions. In a word, all of above mechanisms focus on supervised event extraction and 
no literature involves in the event inference of semi-supervised event extraction. 
3 Semi-supervised Model for Chinese Event Extraction 
In this section, we refine a semi-supervised model for Chinese event extraction as a baseline, which 
includes two views, the document-centric view and pattern similarity-centric view. 
3.1 Semi-supervised Model 
Liao and Grishman (2010a) proposed a state-of-the-art semi-supervised event extraction system, 
which was a pattern-based approach and adopted bootstrapping mechanism to extract relevant patterns. 
Besides, two distinct views, the document-centric view and the pattern similarity-centric view as de-
scribed in Subsection 3.2 and 3.3, are incorporated in the bootstrapping procedure to rank event pat-
terns on different metrics. In each iteration, the candidate patterns, which extracted from unlabeled 
texts as the candidates of relevant patterns, are ranked following the document-centric view, then the 
candidate patterns with pattern similarity scores below a similarity threshold (0.9 in (Liao and Grish-
man, 2010a)) will be removed; only top 3 candidate patterns in the ranking scores of the document-
centric view will be accepted as relevant patterns. In addition, if no pattern is found in the current iter-
ation, the threshold will be reduced by 0.1 until new relevant patterns are extracted. 
As we mentioned earlier, the open and flexible structure of Chinese sentences results in the diversi-
ty of Chinese event patterns. Moreover, the syntax or semantic path is often used to represent event 
patterns, but the performance in Chinese syntactic parsers and Semantic Role Labeling (SRL) tools is 
lower than that in English. Therefore, we refine this semi-supervised model to suit Chinese event ex-
traction in three aspects as follows, due to the above characteristics of Chinese language. 
Firstly, we construct a refined event pattern representation of Chinese events. Liao and Grishman 
(2010a) used semantic roles to represent the relationship between the trigger and its arguments. Due to 
the wide spread of ellipsis (especially entities) and the relatively low performance of Chinese SRL, 
pairwise (trigger-entity) representation and dependency path are introduced to represent Chinese event 
pattern in our refined model. Hence, the event pattern in this paper is a triple-style template as follows. 
<trigger, entity type, their dependency path > 
A pattern is formed by a trigger, the entity type of its argument1 and the dependency path from the 
trigger to the argument. For example, trigger ?returned? and its argument ?peacekeepers? (entity type: 
PER) in sentence S1 can be described as a pattern <returned, PER, nsubj>. 
Secondly, we introduce a novel mechanism to extract candidate patterns. Since verb and noun dom-
inate in triggering an event in Chinese and they are chosen as candidate triggers to create candidate 
patterns. Besides, since different event types may have different roles and different roles are fulfilled 
by entities with different types, the entities whose types can fulfil the core roles of a specific event are 
chosen as candidate entities. For example, Attacker and Target are the core roles of event Attack and 
entity types PER/ORG/GPE2 can fulfil above two roles, so we only accept those entities, whose types 
belong to PER/ORG/GPE, to form candidate patterns. For each sentence in the unlabeled data, all can-
didate trigger-entity pairs and their dependency path are enumerated as candidate patterns. 
Finally, we present a new mechanism to generate seed patterns based on seed triggers. Considering 
the relatively large number of Chinese triggers and the flexibility of Chinese sentences, an instance-
based approach is adopted by enumerating a few high-quality seed triggers with explicit meaning and 
high probability to trigger a specific event. Instead of dozens of predefined patterns required in previ-
ous studies, only one seed trigger is given to each event type or subtype without any predefined pat-
terns. Hence, all patterns consisting of a seed trigger in the candidate patterns are accepted as seed pat-
terns for their high probability to trigger a specific event. 
3.2 Document-centric View 
The document-centric view regards those documents containing the patterns always identified as rele-
vant to a specific event as relevant documents and concludes that they are likely to contain additional 
                                                 
1 All event arguments must be entity mentions following the ACE 2005 annotation guidelines of events. 
2 PER/ORG/GPE refers to person, organization and geo-political entity respectively, which are annotated in the ACE 2005 
corpus. These helpful information can be seen as ontological classes. 
2163
relevant patterns. Hence, those candidate patterns occurring in the relevant documents frequently will 
be extracted as relevant ones. Following Yangarber et al. (2000) and Liao and Grishman (2010a), we 
also employ the disjunctive voting scheme to calculate the ranking scores Rscore(p) of pattern p as fol-
lows. 
 
?
?
?
?
)p(Ld
)p(Ld
Score )d(lRelog*)p(L
)d(lRe
)p(R =
                                                   (1) 
 
where L(p) is the set of documents, which contain candidate pattern p, and Rel(d) is the relevance 
score of document d as follows. 
 
?
?
?
?--
Pp
)p(Ld
'
)
)p(L
)d(lRe
1(1)d(lRe =                                                          (2) 
 
where Rel?(d) is the relevance score of document d in the previous iteration. Initially the relevance 
score of document d is set to n if document d has n relevant patterns in the set of extracted patterns P. 
3.3 Pattern Similarity-centric View 
The similarity-centric view tries to find the candidate patterns who are similar to those seed patterns. 
The similarity scores derive from two aspects, lexical similarity and syntactic similarity, while the 
former is based on the trigger and entity type in a pattern and the latter is based on the relation be-
tween the trigger and the entity. Especially, we realize the pattern similarity view following the lexical 
and syntactic similarity, and refine the similarity ranking score Iscore(p) of candidate pattern p as fol-
lows: 
 
)d,d(DSim)e,e(ESim)t,t(WSim(Max)p(I spspspPsscore ??= ?
                      (3) 
 
where t, e and d represent the trigger, entity type and dependency path in candidate pattern p(tp, ep, dp) 
or seed pattern s(ts, es, ds) in the set of extracted patterns P, respectively; ESim identifies whether two 
entities have the same type, and assigned 1 if two entities have the same entity type and otherwise a 
small number 0.1; DSim calculates the similarity between two dependency paths in edit distance. Fi-
nally, WSim is to obtain the trigger similarity in lexical semantics, using Hownet (Dong and Dong, 
2006) following Liu and Li (2002): 
 
?
?
?? ),(),( spsp ttDisttWSim
                                                                (4) 
 
where Dis(tp,ts) is the distance between the sememes of triggers tp and ts, in HowNet?s sememe hierar-
chical architecture, with parameter ? assigned 0.75 following Liu and Li (2002). 
4 Event Inference 
The pattern-based semi-supervised model cannot extract those event mentions matching infrequent 
patterns or without matching patterns. The knowledge from linguistic aspect (e.g., definition of events, 
compositional semantics of Chinese words, coreference events and relevant events, etc.) is helpful to 
further recover missing event mentions or filter pseudo event mentions. In this section, various kinds 
of event inference mechanisms based on linguistic knowledge are proposed to improve the perfor-
mance of semi-supervised Chinese event extraction. 
We unify the semi-supervised model and the event inference mechanisms into one model as follows: 
In each iteration, after the top 3 patterns have been chosen following the document-centric view and 
event mentions in the unlabeled data have been extracted by pattern matching, all event inference 
2164
mechanisms are applied to recover missing event mentions,. Due to our inference mechanisms are 
trigger-based and each inferred event mention may have more than one pattern while most of them are 
noisy, we do not add those patterns in the set of relevant patterns for bootstrapping. 
4.1 Event Inference on Role Semantics 
The core of an event can be expressed as ?Who do What to Whom? in which ?Who? and ?Whom? are 
the core roles3 to participate in an event, while ?What? often refers to event trigger. The relationship 
between the verbal trigger and its core roles are the key clues to express event semantics. Since the 
subject or object always play the core roles in an event mention, SVO (Sbject-Verb-Object) is a better 
representation of event pattern. However, ellipsis is a widespread phenomenon in Chinese language 
and many sentences do not have an overt subject or object, so lots of event mentions cannot be repre-
sented as SVO pattern. In this paper, we only use the trigger-entity pair to represent event pattern and 
one of the disadvantages of this representation is its loose constraint on events, which will extract lots 
of pseudo event mentions. 
In most cases in Chinese, the object is often the most important core role to identify a specific event 
and it is more helpful than the subject to distinguish true event mentions from pseudo ones. Take fol-
lowing two sentences as examples: 
S2: ??(PER) ?(hit)? ????(PER)?(The teacher hit this student.) 
S3: ??(PER) ?(call)? ?? ? ????(PER)?(The teacher made a phone call to this stu-
dent.) 
The relation between verb ? (hit) and object???? (this student) is clear to indicate sentence 
S2 is an Attack event mention since the object is a person, while object ?? (phone) in sentence S3 is 
not a person and it indicates this sentence is not an Attack event mention following the sense of verb 
? (call). Therefore, the object is an effective evidence to indicate event mentions and it is incorpo-
rated in our model to remove pseudo event mentions as follows. 
Role Semantics: If the object of a candidate verbal trigger mention is not an entity or its entity type 
cannot fulfil the object roles (e.g., Victim in events Injure and Die) in a specific event, this candidate 
trigger mention4 will be inferred as pseudo one. 
For example, core role Target of event Attack often acts as the object of a verbal trigger and entity 
types PER, ORG and GPE can fulfill this role according to be definition of event Attack in the ACE 
2005 corpus. Hence, a candidate trigger mention of event Attack will be regarded as pseudo one when 
this mention has an object which is not an entity or whose entity type is not PER, ORG or GPE. 
4.2 Event Inference on Compositional Semantics 
In Chinese language, a word is composed of one or more characters. Almost all Chinese characters 
have their own meanings and are morpheme (or single-morpheme word), the minimal meaningful unit. 
If a Chinese word contains more than one character, its meaning can often be derived from its compo-
site morphemes. This more fine-grained semantics is compositional semantics of Chinese words. Ac-
tually, it is also a normal way for a native Chinese speaker to understand a new Chinese word. 
Two-morpheme words are used widely in Chinese language and almost all Chinese triggers contain 
one or two morphemes. The compositional semantics of a two-morpheme word comes from both its 
morphemes and morphological structure. Besides morphological structure Coordination, all other 
morphological structures (e.g., Modifier-Head, Predicate-Object, Predicate-Complement (Li and zhou, 
2012)) always have one head morpheme, the morpheme as the governing semantic element, to express 
the meaning of a word. Commonly, there are two head morphemes in a two-morpheme word of Coor-
dination structure. In particular, a two-morpheme word triggers an event if its two head morphemes 
are homogeneous (e.g., ?(attack)?(attack), ?(die)?(die)). Otherwise, it may refer to more than one 
event and this means that two triggers are within a word whose morphological structure is Coordina-
tion. Take the following sentence as an example: 
                                                 
3 We select core roles following the ACE Chinese annotation guidelines of events. Agent/Victim are the core roles of events 
Die/Injure while Attacker/Target are the core roles of event Attack. 
4 Recognizing a trigger mention can be recast as identifying a corresponding event mention, since trigger is the main word 
which most clearly expresses the occurrence of an event. 
2165
S4: ?????(E2: Attack)?(E3: Die)?????(A younger stabbed (E2: Attack) a woman to 
death (E3: Die).) 
In S4, two-morpheme word ?? (stab a person to death) is a trigger with the Coordination struc-
ture. There are two event mentions in sentence S4, one Attack (E2) and one Die (E3), while morpheme
? (stab) triggers an Attack event and ? (die) refers to a Die one.  
Almost all event extraction systems assigned only one event type to a trigger and this will lead to 
that the other event type does not have any patterns to match and then cannot be identified. To address 
this issue, we first identify those triggers who refers to two distinct events as follows: for each two-
morpheme candidate trigger in the candidate patterns whose morphemes are m1 and m2, it will be iden-
tified as candidate trigger with two event types and split into two single-morpheme word to generate 
two candidate trigger mentions when the following three conditions are satisfied: 
1) )m(POSverb)m(POSverb 21 ???  
2) 
)s(Etype)s(Etype))s,m(Wsim())s,m(Wsim( MaxMax seedssseedss 212211 11 21 ??? ?? ??
 
3) Morph(m1 m2)= Coordination 
where POS(m) returns all possible parts of speech of morpheme m in Hownet and Etype(s) is to obtain 
the event type of seed trigger s; WSim(m,s) is defined in Subsection 3.3 and returns 1 when one word 
m is the synonym of the other word s; Morph(w) is to obtain the morphological structure of word w 
following Li and Zhou (2012). 
Since there is a strong trigger consistency in those two-morpheme words of Coordination structure 
which refers to two distinct events, we propose an event inference mechanism as follows. 
Compositional semantics: For each two-morpheme word identified by the above three conditions, 
if one of its morphemes has been extracted as an trigger mention of a specific event type, the other 
morpheme in the same word will refer to an a relevant event type. 
4.3 Event Inference on Coreference Events 
To mine more event mentions, we use the simple trigger-entity pair to represent event pattern in this 
paper. However, lots of event mentions still cannot be extracted due to the ellipsis of arguments. Take 
following sentences as examples: 
S5: ?????????????(E4: Meeting)?(The US and DPRK finished talking (E4: 
Meeting) in Kuala Lumpur.) 
S6: ??(E5: Meeting)??????(The talks (E5: Meeting) are serious.) 
Obviously, more than one pattern of event mention E4 can be generated from sentence S5, since it 
contains more than one entity. On the contrary, no pattern can be extracted from S6 and this leads to 
event mention E5 cannot be extracted in our pattern-based semi-supervised model. 
Within a document, almost all event mentions are around a topic and there is a strong trigger con-
sistency: if one mention of a word triggers a specific event, its other mentions in the same document 
will refer to the same event type. Besides, similar words (e.g., ? (bomb), ?? (bomb), ?? (bomb)), 
which contains the same head morpheme, always express the same or similar meaning following the 
principle of compositional semantics. Similarly, there is a strong trigger consistency on those similar 
words: If one mention of a word refers to a specific event, the mentions of its similar words in the 
same document will trigger events of the same type. 
Since the mentions of the same word or similar words are often coreference ones and always refer 
to the same event type, we propose an event inference mechanism on coreference events to recover 
missing event mentions based on head morpheme as follows. In particular, head morphemes are also 
identified following Li and Zhou (2012). 
Coreference events: 1) if a mention of a candidate trigger refers to a specific event, all its other 
mentions in the same document will trigger the same type event; 2) if one mention of a candidate trig-
ger refers to a specific event, all the mentions of its similar words in the same document will trigger 
the same type event too. 
4.4 Event Inference on Relevant Events 
The bootstrapping procedure of the document-centric view selects frequent patterns in relevant docu-
2166
ments and ignores those infrequent patterns both in relevant or irrelevant documents. However, the 
number of infrequent patterns in Chinese is larger than that in English, due to its open and flexible 
sentence structure, as mentioned in Subsection 3.1. 
Besides the pattern-based semi-supervised model, we propose a trigger-based mechanism as a sup-
plement to recover those missing event mentions concerning infrequent patterns following this as-
sumption: if a trigger mention refers a specific event in a document, there is a high probability that its 
relevant events occur in the same document. Take the following sentence as an example: 
S7: ???(E6: Attack)??? 1???????(E7: Die)?(An Arabian was dying (E7: Die) in 
this conflict (E6: Attack).) 
In sentence S7, there is an extracted Die event mention E7 triggered by ?? (die) and ?? (con-
flict) is a candidate trigger mention. If there is an evidence that ?? (conflict) triggers an Attack event 
in the other documents, it is possible to identify ?? (conflict) as a trigger mention of Attack event in 
S7 for the high probability that events Die and Attack occur in the same document. We propose an in-
ference mechanism on relevant events as follows. 
Relevant Events: If a trigger mention is identified in a document, each candidate trigger mention in 
the same document will be recognized as true ones when it satisfies the following condition: this can-
didate trigger occurs in the other documents as an event trigger and refers to the relevant events of this 
identified trigger mentions.  
Since the seed triggers have a high probability to trigger a specific event, to further explore those 
missing event mentions, we expand this inference mechanism following compositional semantics in 
Chinese and expand the condition as follows: This candidate trigger occurs in the other documents as 
an event trigger or contains one of the seed triggers, which refers to the relevant events of this identi-
fied trigger mentions. 
5 Experimentation 
In this section, we systematically evaluate our event inference mechanisms on the ACE 2005 Chinese 
corpus and provide the analysis. 
5.1 Experimental Setting 
The ACE 2005 Chinese corpus is the only available corpus in Chinese event extraction and it is used 
in all our experiments. This corpus contains 633 documents annotated with 33 predefined types. Due 
to evaluation on all 33 types is a hard work for the time-consuming bootstrapping procedure and the 
diversity of distinct event types, most of previous works selected part of event types for evaluation. In 
this paper, 3 event types (i.e. Die, Injure and Attack) are selected for evaluation, because they reflect 
the relevance of different event types and occur at different frequencies in the corpus. While events 
Die and Injure are easy to define, event Attack is rather complicated and can be divided into several 
subtypes. In the ACE 2005 Chinese corpus, almost one third of the annotated event mentions belong to 
the above three event types. Moreover, we report the experimental results on all 33 event types to fur-
ther verify the effectiveness of our inference mechanisms in Subsection 5.2. 
Unlike MUC shared task, which only distinguishes whether a sentence contains a specific event 
mention or not, we follow previous studies on the ACE 2005 corpus and report the performance of 
trigger-based event extraction: a trigger is correctly identified if its position and event type match a 
reference trigger. As for evaluation, we use the ground truth entities, time and values annotated in the 
ACE 2005 Chinese corpus, and report the micro-average Precision (P), Recall (R) and F1-score (F1). 
Table 1 shows the seed triggers for the three event types. For example, only one seed trigger is pro-
vided for either the Die or Injure event, while three seed triggers are given for event Attack. Since the 
Attack event contains several distinct event subtypes, we assign one seed trigger to each of its major 
subtypes. Thus, all patterns whose triggers belong to the set of seed triggers are accepted as seed pat-
terns automatically. 
 
Type Die Injure Attack 
Seed triggers ?(die) ?(injure) ??(attack), ??(conflict), ?(hit) 
Table 1. Seed triggers of Die, Injure and Attack event types 
2167
Besides, all the sentences in the corpus are divided into words using a Chinese word segmentation 
tool (ICTCLAS) with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford 
Parser to create the constituent and dependency parse trees. 
5.2 Experimental Results 
To verify the performance of our event inference mechanisms, it is compared with the refined baseline, 
a supervised model for Chinese event extraction. Table 2 shows the results of our event inference 
mechanisms with peak recall, precision and F1-score, following Liao and Grishman (2010a). Com-
pared with the baseline, Table 2 shows that our event inference mechanisms improve the F1-score of 
Chinese event extraction by 8.5%, largely due to the improvement of 11.8% in recall. These results 
confirm the effectiveness of our event inference mechanisms in recovering missing event mentions. 
The disadvantage of our event inference mechanisms is the fact that it will also introduce some pseudo 
event mentions into our model and harm the precision. Additionally, there is still a big performance 
gap between our model and the supervised model and this leaves much room for future research. 
 
Approach Attack Injure Die All (micro-average) 
P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 
Baseline 71.4 36.6 48.4 93.2 41.7 57.6 90.1 44.0 59.3 79.7 39.4 52.7 
+Event inference 70.9 47.5 56.9 83.2 54.6 65.9 80.8 57.2 67.0 75.5 51.2 61.2 
Supervised model 70.4 72.5 71.4 85.3 78.4 81.7 83.9 92.9 88.1 77.2 78.4 77.8 
Table 2. Performance of event inference mechanisms in Chinese event extraction (Attack/Injure/Die). 
 
Table 2 also indicates the performance difference of our inference mechanisms for distinct event 
types. Among all event types, event Attack achieves the highest improvement (8.5%) in F1-score, with 
a dramatic improvement of 10.9% in recall and a less loss of 0.5% in precision. Event Die and Injure 
also gain a significant improvement of 7.7% and 8.3% in F1-score respectively, largely due to the in-
crease in recall, while their precisions reduce rapidly due to those pseudo event mentions inferred by 
our inference mechanisms. However, the loss of precision of event Attack is much less than these of 
events Die and Injure. The reason is that the inference on role semantics mainly impacts on Attack 
events to remove pseudo event mentions. 
To well evaluate different approaches, it is better to compare them on different corpora. Since the 
ACE 2005 Chinese corpus is the only available corpus in Chinese event extraction, we divide it into 
three sub-corpora according to data sources, i.e. Broadcast News, Newswire and WebLog, which are 
much different in various aspects, such as quality, length and style. Figure 1 compares the perfor-
mance of different models on different sub-corpora. It indicates that our event inference mechanisms 
perfect better than the baseline in all three sub-corpora and that results confirm the huge influence of 
the event inference mechanisms. It also shows that the WebLog sub-corpus reports the worst F1-score 
due to the low document quality and the low percentage of relevant documents, and that the Newswire 
sub-corpus reports significantly better performance than the Broadcast News sub-corpus due to its 
spoken nature. 
 
Figure 1. Performance comparison (F1-score) on different data sources. 
To further verify the effectiveness of our event inference mechanisms, we evaluate them on all 33 
event types. Due to event extraction is a domain-specific task, distinct event types have the different 
seed triggers and different pro-process procedures. In this paper, we just report the final results for the 
50.9 57.8 
36.9 
59.6 65.7 
44.8 
0
50
100
Broadcast news Newwise WebLog
Baseline Baseline+Event Inference
2168
sake of brevity. Table 3 shows the experimental results on all 33 event types and it ensures that our 
mechanisms are effective on extracting all event types. Compared with the baseline, our approach im-
proves the F1-score by 7.6%, which is less than that reported in Table 2. Among all 33 event types, the 
performances of almost all event types associated with justice are higher than other event types for 
their unambiguous definitions and high coverage of seed triggers while event Transport achieves the 
lowest performance for its complexity and low coverage of seed triggers. Besides, the performance on 
all event types is lower than that on 3 event types and this result comes from the low performance of 
the Transport event which occupies almost 20% of all annotated event mentions in the ACE 2005 
Chinese corpus. 
Approach P(%) R(%) F1 
Baseline 70.7 34.2 46.1 
+Event inference 65.2 45.7 53.7 
Table 3. Performance of event inference mechanisms in Chinese event extraction (All 33 event types). 
5.3 Analysis on Event Inference Mechanisms 
Table 4 shows the contributions of the different event inference mechanisms. It is worthy to mention 
that an event mentions may be identified by both the semi-supervised model and the event inference 
mechanisms. In this paper, we attribute those extracted event mentions to the former and the contribu-
tion of our inference mechanisms is greater than those in Table 4. 
 
Inference P(%) R(%) F1 
Baseline 79.7 39.4 52.7 
+Inference on role semantics (RS) 87.5(+7.8) 39.1(-0.3) 54.1(+1.4) 
+Inference on compositional semantic (CS) 85.7(+6.0) 43.7(+4.3) 57.8(+3.7) 
+Inference on coreference events (CE) 83.0(+3.3) 45.8(+6.4) 59.0(+1.2) 
+Inference on relevant events (RE) 75.7(-4.0) 51.3(+11.9) 61.2(+2.2) 
Table 4. The contribution of event inference on Chinese event extraction. 
 
Actually, inference mechanism RS is a filter to remove those pseudo event mentions and it can im-
prove the precision (+7.8%), with a less lost (-0.3%) in recall. Moreover, it can also help the seed pat-
tern generation to generate high quality seed patterns. Table 5 shows the contribution of RS on seed 
pattern generation and we report the result of Chinese event extraction which only uses the seed pat-
terns5. It improves the accuracy from 75.8% to 82.5%, largely due to the decline (-30) in the set of 
pseudo event mentions. These results indicate that the object is a key clue to identify event mentions. 
 
Method #True event mentions #Pseudo event mentions 
w/o RS 273 87 
w/ RS 269 57 
Table 5. The contribution of RS on seed pattern generation. 
 
Chen and Ji (2009b) have reported that almost 13% of Chinese triggers are in-word or cross-words 
and this figure ensures it is an important issue. Inference mechanism CS gains the highest improve-
ment (+3.7%) in F1-score and this result indicates that compositional semantics is an effective way to 
solve such issue. The accuracy of this inference mechanism is very high (~92%) and most of the ex-
ceptions need the help of deep semantics since these instances are also hard to be distinguished by 
humans without the context. 
Inference mechanisms CE and RE improve the F1-scores by 1.2% and 2.2% respectively. CE as-
sumes all mentions of a word in a document only have one sense and it will introduce lots of pseudo 
event mentions to reduce precision. The experimental results also show that RE is an effective sup-
plement of the document-centric view to mine event mentions. Although they derive from the similar 
                                                 
5 Since sometimes a pattern can infer both true event mentions and pseudo event mentions, it is hard to identify whether a 
pattern is relevant or irrelevant without the test data. Hence, we compare their extracted event mentions in this paper. 
2169
principle of occurrence of relevant events, they focus on different perspectives where RE is trigger-
based and the document-centric view is pattern-based. RE ignores the difference on patterns and iden-
tifies event mentions on the occurrence of their relevant event mentions. In addition, sense shifting of 
Chinese words in different contexts is the main factor to extract lots of pseudo event mentions and 
then reduce the precision rapidly. 
It?s obvious that these inference mechanisms interact with others. In particular, almost 20% event 
mentions can be inferred by both CE and RE for the transitivity of event inference on coreference and 
relevant events. Besides, RS is not only beneficial to the semi-supervised model, but also helpful to 
the other inference mechanisms to further remove pseudo event mentions. 
6 Conclusion 
This paper proposes various kinds of novel linguistic knowledge-driven event inference mechanisms 
as a supplement of the semi-supervised Chinese event extraction to recover missing event mentions. 
The experimental results verify their effectiveness to extract the event mentions with infrequent pat-
terns or without matching pattern. Although this paper focuses on Chinese language, most of the event 
inference mechanisms are language-independent and can be applied to other languages. Our future 
work will focus on how to apply our event inference mechanisms to other languages and introduce 
more effective inference mechanisms to further improve the performance of semi-supervised event 
extraction. 
Acknowledgments 
The authors would like to thank three anonymous reviewers for their comments on this paper. This 
research was supported by the National Natural Science Foundation of China under Grant No. 
61331011 and No. 61272260, the National 863 Project of China under Grant No. 2012AA011102. 
Reference 
Niranjan Balasubramanian, Stephen Soderland, Mausam and Oren Etzioni. 2013. Generating Coherent Event 
Schemas at Scale. In Proc. EMNLP 2013, pages 1721-1731, Seatle, WA. 
Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised Learning of Narrative Event Chains. In Proc. ACL-
HLT 2008, pages 787-797, Hawaii. 
Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised Learning of Narrative Schemas and Their Partici-
pants. In Proc. ACL 2009, pages 602-610, Columbus, OH. 
Nathanael Chambers and Dan Jurafsky. 2011. Template-Based Information Extraction without the Templates. In 
Proc. ACL 2011, pages 976-986, Portland, OR. 
Hai Leong Chieu, Hwee Tou Ng and Yoong Keok Lee. 2003. Closing the Gap: Learning-based Information 
Extraction Rivaling Knowledge-Engineering Methods. In Proc. ACL 2003, pages 216-230, Sapporo, Japan. 
Zheng Chen and Heng Ji. 2009a. Can One Language Bootstrap the Other: A Case Study on Event Extraction. In 
Proc. NAACL-HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing, pages 
66-74, Boulder, CO. 
Zheng Chen and Heng Ji. 2009b. Language Specific Issue and Feature Exploration in Chinese Event Extraction. 
In Proc. NAACL-HLT 2009, pages 209-212, Boulder, CO. 
Zhengdong Dong and Qiang Dong. 2006. HowNet and the Computation of Meaning. World Scientific Pub Co. 
Inc. 
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, Guodong Zhou and Qiaoming Zhu. 2011. Using Cross-Entity 
Inference to Improve Event Extraction. In Proc. ACL 2011, pages 1127-1136, Portland, OR. 
Ruihong Huang and Ellen Riloff. 2012. Bootstrpped Training of Event Extraction Classifiers. In Proc. EACL 
2012, pages 286-295, Avignon, France. 
Heng Ji and Ralph Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proc. 
ACL-HLT 2008, pages 254-262, Columbus, OH. 
2170
Peifeng Li and Guodong Zhou. 2012. Employing Morphological Structures and Sememes for Chinese Event Ex-
traction. In Proc. COLING 2012, pages 1619-1634, Mumbai, India. 
Peifeng Li, Qiaoming Zhu, and Guodong Zhou. 2013. Argument Inference from Relevant Event Mentions in 
Chinese Argument Extraction. In Proc. ACL 2013, pages 1477-1487, Sofia, Bugaria. 
Peifeng Li, Qiaoming Zhu, Guodong Zhou. 2014. Using Compositional Semantics and Discourse Consistency to 
Improve Chinese Trigger Identification. Information Processing and Management, 50: 399?415.  
Shasha Liao and Ralph Grishman. 2010a. Filtered Ranking for Bootstrapping in Event Extraction. In Proc. COL-
ING 2010, pages 680-688, Beijing, China. 
Shasha Liao and Ralph Grishman. 2010b. Using Document Level Cross-Event Inference to Improve Event Ex-
traction. In Proc. ACL 2010, pages 789-797, Uppsala, Sweden. 
Shasha Liao and Ralph Grishman. 2011. Can Document Selection Help Semi-supervised Learning? A Case 
Study On Event Extraction. In Proc. ACL 2011, pages 260-265, Portland, OR. 
Qun Liu and Sujian Li. 2002. Word Similarity Computing Based on How-net. In Proc. 3th Chinese Lexical Se-
mantic Workshop, Taibei, Taiwan. 
Ting Liu and Tomek Strzalkowski. 2012. Bootstrapping Events and Relations from Text. In Proc. EACL 2012, 
pages 296-305, Avignon, France. 
Mstislav Maslennikov and Tat-Seng Chua. 2007. A Multi-resolution Framework for Information Extraction from 
Free Text. In Proc. ACL 2007, pages 592-599, Prague, Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Unified Model of Phrasal and Sentential Evidence for Infor-
mation Extraction. In Proc. EMNLP 2009, pages 151-160, Singapore. 
William Phillips and Ellen Riloff. 2007. Exploiting Role-Identifying Nouns and Expressions for Information Ex-
traction. In Proc. RANLP 2007, pages 468-473, Borovets, Bulgaria. 
Ellen Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proc. AAAI 1996, 
pages 1044-1049, Portland, OR. 
Mark Stevenson and Mark Greenwood. 2005. A Semantic Approach to IE Pattern Induction. In Proc. ACL 2005, 
pages 379-386, Ann Arbor, MI. 
Kiyoshi Sudo, Satoshi Sekine, Ralph Grishman. 2001. Automatic Pattern Acquisition for Japanese Information 
Extraction. In Proc. HLT 2001, pages 1-7, San Diego, CA.  
Kiyoshi Sudo, Satoshi Sekine, Ralph Grishman. 2003. An Improved Extraction Pattern Representation Model 
for Automatic IE Pattern Acquisition. In Proc. ACL 2003, pages 224-231, Tokyo, Japan. 
Roman Yangarber, Ralph Grishman, Pasi Tapanainen and Silja Huttunen. 2000. Automatic Acquisition of Do-
main Knowledge for Information Extraction. In Proc. COLING 2000, pages 940-946, Hong Kong. 
Roman Yangarber. 2003. Counter-Training in Discovery of Semantic Patterns. In Proc. ACL 2003, pages 343-
350, Sapporo, Japan. 
Jian Wang, Qian Xu, Hongfei Lin, Zhihao Yang, Yanpeng Li. 2013. Semi-supervised Method for Biomedical 
Event Extraction. Proteome Science, 11(Suppl 1): S17. 
2171
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 714?724,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Unified Framework for Scope Learning via Simplified Shallow Seman-
tic Parsing 
 
Qiaoming Zhu    Junhui Li    Hongling Wang    Guodong Zhou?  
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
{qmzhu, lijunhui, hlwang, gdzhou}@suda.edu.cn 
 
 
                                                          
? Corresponding author 
Abstract 
This paper approaches the scope learning 
problem via simplified shallow semantic pars-
ing. This is done by regarding the cue as the 
predicate and mapping its scope into several 
constituents as the arguments of the cue. 
Evaluation on the BioScope corpus shows that 
the structural information plays a critical role 
in capturing the relationship between a cue 
and its dominated arguments. It also shows 
that our parsing approach significantly outper-
forms the state-of-the-art chunking ones. Al-
though our parsing approach is only evaluated 
on negation and speculation scope learning 
here, it is portable to other kinds of scope 
learning.  
1 Introduction 
Recent years have witnessed an increasing interest 
in the analysis of linguistic scope in natural lan-
guage. The task of scope learning deals with the 
syntactic analysis of what part of a given sentence 
is under user?s special interest. For example, of 
negation assertion concerned, a negation cue (e.g., 
not, no) usually dominates a fragment of the given 
sentence, rather than the whole sentence, especially 
when the sentence is long. Generally, scope learn-
ing involves two subtasks: cue recognition and its 
scope identification. The former decides whether a 
word or phrase in a sentence is a cue of a special 
interest, where the semantic information of the 
word or phrase, rather than the syntactic informa-
tion, plays a critical role. The latter determines the 
sequences of words in the sentence which are 
dominated by the given cue.  
Recognizing the scope of a special interest (e.g., 
negative assertion and speculative assertion) is es-
sential in information extraction (IE), whose aim is 
to derive factual knowledge from free text. For 
example, Vincze et al (2008) pointed out that the 
extracted information within the scope of a nega-
tion or speculation cue should either be discarded 
or presented separately from factual information. 
This is especially important in the biomedical and 
scientific domains, where various linguistic forms 
are used extensively to express impressions, hy-
pothesized explanations of experimental results or 
negative findings. Besides, Vincez et al (2008) 
reported that 13.45% and 17.70% of the sentences 
in the abstracts subcorpus of the BioScope corpus 
contain negative and speculative assertions, respec-
tively, while 12.70% and 19.44% of the sentences 
in the full papers subcorpus contain negative and 
speculative assertions, respectively. In addition to 
the IE tasks in the biomedical domain, negation 
scope learning has attracted increasing attention in 
some natural language processing (NLP) tasks, 
such as sentiment classification (Turney, 2002). 
For example, in the sentence ?The chair is not 
comfortable but cheap?, although both the polari-
ties of the words ?comfortable? and ?cheap? are 
positive, the polarity of ?the chair? regarding the 
attribute ?cheap? keeps positive while the polarity 
of ?the chair? regarding the attribute ?comfortable? 
is reversed due to the negation cue ?not?. Similarly, 
seeing the increasing interest in speculation scope 
learning, the CoNLL?2010 shared task (Farkas et 
al., 2010) aims to detect uncertain information in 
resolving the scopes of speculation cues. 
Most of the initial research in this literature fo-
cused on either recognizing negated terms or iden-
tifying speculative sentences, using some heuristic 
714
rules (Chapman et al, 2001; Light et al, 2004), 
and machine learning methods (Goldin and Chap-
man, 2003; Medlock and Briscoe, 2007). However, 
scope learning has been largely ignored until the 
recent release of the BioScope corpus (Szarvas et 
al., 2008), where negation/speculation cues and 
their scopes are annotated explicitly. Morante et al 
(2008) and Morante and Daelemans (2009a & 
2009b) pioneered the research on scope learning 
by formulating it as a chunking problem, which 
classifies the words of a sentence as being inside or 
outside the scope of a cue. Alternatively, ?zg?r 
and Radev (2009) and ?vrelid et al (2010) defined 
heuristic rules for speculation scope learning from 
constituency and dependency parse tree perspec-
tives, respectively. 
Although the chunking approach has been 
evaluated on negation and speculation scope learn-
ing and can be easily ported to other scope learning 
tasks, it ignores syntactic information and suffers 
from low performance. Alternatively, even if the 
rule-based methods may be effective for a special 
scope learning task (e.g., speculation scope learn-
ing), it is not readily adoptable to other scope 
learning tasks (e.g., negation scope learning). In-
stead, this paper explores scope learning from 
parse tree perspective and formulates it as a simpli-
fied shallow semantic parsing problem, which has 
been extensively studied in the past few years 
(Carreras and M?rquez, 2005). In particular, the 
cue is recast as the predicate and the scope is recast 
as the arguments of the cue. The motivation behind 
is that the structured syntactic information plays a 
critical role in scope learning and should be paid 
much more attention, as indicated by previous 
studies in shallow semantic parsing (Gildea and 
Palmer, 2002; Punyakanok et al, 2005). Although 
our approach is evaluated only on negation and 
speculation scope learning here, it is portable to 
other kinds of scope learning. 
The rest of this paper is organized as follows. 
Section 2 reviews related work. Section 3 intro-
duces the Bioscope corpus on which our approach 
is evaluated. Section 4 describes our parsing ap-
proach by formulating scope learning as a simpli-
fied shallow semantic parsing problem. Section 5 
presents the experimental results. Finally, Section 
6 concludes the work. 
 
 
2 Related Work  
Most of the previous research on scope learning 
falls into negation scope learning and speculation 
scope learning.  
Negation Scope Learning 
Morante et al (2008) pioneered the research on 
negation scope learning, largely due to the avail-
ability of a large-scale annotated corpus, the Bio-
scope corpus. They approached negation cue 
recognition as a classification problem and formu-
lated negation scope identification as a chunking 
problem which predicts whether a word in the sen-
tence is inside or outside of the negation scope, 
with proper post-processing to ensure consecutive-
ness of the negation scope. Morante and Daele-
mans (2009a) further improved the performance by 
combing several classifiers and achieved the accu-
racy of ~98% for negation cue recognition and the 
PCS (Percentage of Correct Scope) of ~74% for 
negation scope identification on the abstracts sub-
corpus. However, this chunking approach suffers 
from low performance, in particular on long sen-
tences. For example, given golden negation cues 
on the Bioscope corpus, Morante and Daelemans 
(2009a) only got the performance of 50.26% in 
PCS on the full papers subcorpus (22.8 words per 
sentence on average), compared to 87.27% in PCS 
on the clinical reports subcorpus (6.6 words per 
sentence on average). 
Speculation Scope Learning 
Similar to Morante and Daelemans (2009a), 
Morante and Daelemans (2009b) formulated 
speculation scope identification as a chunking 
problem which predicts whether a word in the sen-
tence is inside or outside of the speculation scope, 
with proper post-processing to ensure consecutive-
ness of the speculation scope. They concluded that 
their method for negation scope identification is 
portable to speculation scope identification. How-
ever, of speculation scope identification concerned, 
it also suffers from low performance, with only 
60.59% in PCS for the clinical reports subcorpus 
of short sentences. 
Alternatively, ?zg?r and Radev (2009) em-
ployed some heuristic rules from constituency 
parse tree perspective on speculation scope identi-
fication. Given golden speculation cues, their rule-
based method achieves the accuracies of 79.89% 
715
and 61.13% on the abstracts and the full papers 
subcorpora, respectively. The more recent 
CoNLL?2010 shared task was dedicated to the de-
tection of speculation cues and their linguistic 
scope in natural language processing (Farkas et al, 
2010). As a representative, ?vrelid et al (2010) 
adopted some heuristic rules from dependency 
parse tree perspective to identify their speculation 
scopes. 
3 Cues and Scopes in the BioScope Cor-
pus 
This paper employs the BioScope corpus (Szarvas 
et al, 2008; Vincze et al, 2008) 1 , a freely 
downloadable resource from the biomedical do-
main, as the benchmark corpus. In this corpus, 
every sentence is annotated with negation cues and 
speculation cues (if it has), as well as their linguis-
tic scopes. Figure 1 shows a self-explainable ex-
ample. It is possible that a negation/speculation cue 
consists of multiple words, i.e., ?can not?/?indicate 
that? in Figure 1. 
 
The Bioscope corpus consists of three sub-
corpora: biological full papers from FlyBase and 
from BMC Bioinformatics, biological paper ab-
stracts from the GENIA corpus (Collier et al, 
1999), and clinical (radiology) reports. Among 
them, the full papers subcorpus and the abstracts 
subcorpus come from the same genre, and thus 
share some common characteristics in statistics, 
such as the number of words in the nega-
tion/speculation scope to the right (or left) of the 
negation/speculation cue and the average scope 
length. In comparison, the clinical reports subcor-
pus consists of clinical radiology reports with short 
sentences. For detailed statistics and annotation 
                                                          
                                                          
1 http://www.inf.u-szeged.hu/rgai/bioscope 
guidelines about the three subcorpora, please see 
Morante and Daelemans (2009a & 2009b). 
For preprocessing, all the sentences in the Bio-
scope corpus are tokenized and then parsed using 
the Berkeley parser (Petrov and Klein, 2007) 2  
trained on the GENIA TreeBank (GTB) 1.0 
(Tateisi et al, 2005)3, which is a bracketed corpus 
in (almost) PTB style. 10-fold cross-validation on 
GTB1.0 shows that the parser achieves the per-
formance of 86.57 in F1-measure. It is worth not-
ing that the GTB1.0 corpus includes all the 
sentences in the abstracts subcorpus of the Bio-
scope corpus. 
4 Scope Learning via Simplified Shallow 
Semantic Parsing 
In this section, we first formulate the scope learn-
ing task as a simplified shallow semantic parsing 
problem. Then, we deal with it using a simplified 
shallow semantic parsing framework. 
4.1 Formulating Scope Learning as a Simpli-
fied Shallow Semantic Parsing Problem 
<sentence id="S26.8">These findings <xcope 
id="X26.8.2"><cue type="speculation" 
ref="X26.8.2">indicate that</cue> <xcope 
id="X26.8.1">corticosteroid resistance in bron-
chial asthma <cue type="negation" 
ref="X26.8.1">can not</cue> be explained by 
abnormalities in corticosteroid receptor charac-
teristics</xcope></xcope>.</sentence> 
Figure 1: An annotated sentence in the BioScope 
corpus 
Given a parse tree and a predicate in it, shallow 
semantic parsing recognizes and maps all the con-
stituents in the sentence into their corresponding 
semantic arguments (roles) of the predicate or not. 
As far as scope learning considered, the cue can be 
regarded as the predicate4, while its scope can be 
mapped into several constituents dominated by the 
cue and thus can be regarded as the arguments of 
the cue. In particular, given a cue and its scope 
which covers wordm, ?, wordn, we adopt the fol-
lowing two heuristic rules to map the scope of the 
cue into several constituents which can be deemed 
as its arguments in the given parse tree. 
1) The cue itself and all of its ancestral constituents 
are non-arguments. 
2) If constituent X is an argument of the given cue, 
then X should be the highest constituent domi-
nated by the scope of wordm, ?, wordn. That is 
to say, X?s parent constituent must cross-bracket 
or include the scope of wordm, ?, wordn. 
2 http://code.google.com/p/berkeleyparser/ 
3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA 
4 If a speculation cue consists of multiply words (e.g., whether 
or not), the first word (e.g., whether) is chosen to represent the 
speculation signal. However, the last word (e.g., not) is chosen 
to represent the negation cue if it consists of multiple words 
(e.g., can not). 
716
 Figure 2: Examples of a negation/speculation cue and its arguments in a parse tree 
These findings 
indicate 
that 
corticosteroid resistance
NP0,1
VBP2,2 SBAR3,11
can not
IN3,3
be
explained by abnormalities
NP4,5
MD6,6 RB7,7
VB8,8 VP9,11
VP8,11
VP6,11
S4,11
VP2,11
S0,11
neg-predicate
neg-arguments
spec-predicate
spec-argument
 
The first rule ensures that no argument covers 
the cue while the second rule ensures no overlap 
between any two arguments. These two constraints 
between a cue and its arguments are consistent 
with shallow semantic parsing (Carreras and 
M?rquez, 2005). For example, in the sentence 
?These findings indicate that corticosteroid resis-
tance can not be explained by abnormalities?, the 
negation cue ?can not? has the negation scope 
?corticosteroid resistance can not be explained by 
abnormalities? while the speculation cue ?indicate 
that? has the speculation scope ?indicate that cor-
ticosteroid resistance can not be explained by ab-
normalities?. As shown in Figure 2, the node 
?RB7,7? (i.e., not) represents the negation cue ?can 
not? while its arguments include three constituents 
{NP4,5, MD6,6, and VP8,11}. Similarly, the node 
?VBP2,2? (i.e., indicate) represents the  speculation 
cue ?indicate that? while its arguments include one 
constituent SBAR3,11. It is worth noting that ac-
cording to the above rules, scope learning via shal-
low semantic parsing, i.e. determining the 
arguments of a given cue, is robust to some varia-
tions in the parse trees. This is also empirically 
justified by our later experiments. For example, if 
the VP6,11 in Figure 2 is incorrectly expanded by 
the rule VP6,11?MD6,6+RB7,7+VB8,8+VP9,11, the 
negation scope of the negation cue ?can not? can 
still be correctly detected as long as {NP4,5, MD6,6, 
VB8,8, and VP9,11} are predicated as the arguments 
of the negation cue ?can not?. 
Compared with common shallow semantic pars-
ing which needs to assign an argument with a se-
mantic label, scope identification does not involve 
semantic label classification and thus could be di-
vided into three consequent phases: argument 
pruning, argument identification and post-
processing. 
 
4.2 Argument Pruning 
Similar to the predicate-argument structures in 
common shallow semantic parsing, the cue-scope 
structures in scope learning can be also classified 
into several certain types and argument pruning 
can be done by employing several heuristic rules 
accordingly to filter out constituents, which are 
most likely non-arguments of a given cue. Similar 
to the heuristic algorithm proposed in Xue and 
Palmer (2004) for argument pruning in common 
shallow semantic parsing, the argument pruning 
algorithm adopted here starts from designating the 
cue node as the current node and collects its sib-
lings. It then iteratively moves one level up to the 
parent of the current node and collects its siblings. 
The algorithm ends when it reaches the root of the 
parse tree. To sum up, except the cue node itself 
and its ancestral constituents, any constituent in the 
parse tree whose parent covers the given cue will 
be collected as argument candidates. Taking the 
negation cue node ?RB7,7? in Figure 2 as an exam-
ple, constituents {MD6,6, VP8,11, NP4,5, IN3,3,  
717
 
 
Feature Remarks 
B1 Cue itself: the word of the cue, e.g., not,
rather_than. (can_not) 
B2 Phrase Type: the syntactic category of the
argument candidate. (NP) 
B3 Path: the syntactic path from the argument 
candidate to the cue. (NP<S>VP>RB) 
B4 Position: the positional relationship of the
argument candidate with the cue. ?left? or 
?right?. (left) 
Table 1: Basic features and their instantiations for ar-
gument identification in scope learning, with NP4,5 as 
the focus constituent (i.e., the argument candidate) and 
?can not? as the given cue, regarding Figure 2. 
 
 
Feature Remarks 
Argument Candidate (AC) related 
AC1 The headword (AC1H) and its POS
(AC1P). (resistance, NN) 
AC2 The left word (AC2W) and its POS
(AC2P). (that, IN) 
AC3 The right word (AC3W) and its POS
(AC3P). (can, MD) 
AC4 The phrase type of its left sibling (AC4L)
and its right sibling (AC4R). (NULL, VP)
AC5 The phrase type of its parent node. (S) 
AC6 The subcategory. (S:NP+VP) 
Cue/Predicate (CP) related 
CP1 Its POS. (RB) 
CP2 Its left word (CP2L) and right word
(CP2R). (can, be) 
CP3 The subcategory. (VP:MD+RB+VP) 
CP4 The phrase type of its parent node. (VP) 
Combined Features related with the Argument Candi-
date  (CFAC1-CFAC2) 
b2&AC1H, b2&AC1P 
Combined Features related with the given
Cue/Predicate  (CFCP1-CFCP2) 
B1&CP2L, B1&CP2R 
Combined Features related with both the Argument 
Candidate and the given Cue/Predicate (CFACCP1-
CFACCP7) 
B1&B2, B1&B3, B1&CP1, B3&CFCP1, B3&CFCP2, 
B4&CFCP1, B4&CFCP2 
Table 2: Additional features and their instantiations for 
argument identification in scope identification, with 
NP4,5 as the focus constituent (i.e., the argument candi-
date) and ?can not? as the given cue, regarding Figure 2. 
 
VBP2,2, and NP0,1} are collected as its argument 
candidates consequently. 
4.3 Argument Identification 
Here, a binary classifier is applied to determine the 
argument candidates as either valid arguments or 
non-arguments. Similar to argument identification 
in common shallow semantic parsing, the struc-
tured syntactic information plays a critical role in 
scope learning. 
Basic Features 
Table 1 lists the basic features for argument identi-
fication. These features are also widely used in 
common shallow semantic parsing for both verbal 
and nominal predicates (Xue, 2008; Li et al, 2009). 
Additional Features 
To capture more useful information in the cue-
scope structures, we also explore various kinds of 
additional features. Table 2 shows the features in 
better capturing the details regarding the argument 
candidate and the cue. In particular, we categorize 
the additional features into three groups according 
to their relationship with the argument candidate 
(AC, in short) and the given cue/predicate (CP, in 
short). 
Some features proposed above may not be effec-
tive in argument identification. Therefore, we 
adopt the greedy feature selection algorithm as de-
scribed in Jiang and Ng (2006) to pick up positive 
features incrementally according to their contribu-
tions on the development data. The algorithm re-
peatedly selects one feature each time, which con-
tributes most, and stops when adding any of the 
remaining features fails to improve the perform-
ance. 
4.4 Post-Processing 
Although a cue in the BioScope corpus always has 
only one continuous block as its scope (including 
the cue itself), the scope identifier may result in 
discontinuous scope due to independent predica-
tion in the argument identification phase. Given the 
golden negation/speculation cues, we observe that 
6.2%/9.1% of the negation/speculation scopes pre-
dicted by our scope identifier are discontinuous. 
718
 
Figure 3 demonstrates the projection of all the 
argument candidates into the word level. Accord-
ing to our argument pruning algorithm in Section 
4.2, except the words presented by the cue, the pro-
jection covers the whole sentence and each con-
stituent (LACi or RACj in Figure 3) receives a 
probability distribution of being an argument of the 
given cue in the argument identification phase. 
Since a cue is deemed inside its scope in the 
BioScope corpus, our post-processing algorithm 
first includes the cue in its scope and then starts to 
identify the left and the right scope boundaries, 
respectively. 
As shown in Figure 3, the left boundary has 
m+1 possibilities, namely the cue itself, the left-
most word of constituent LACi (1<=i<=m). Sup-
posing LACi receives probability of Pi being an 
argument, we use the following formula to deter-
mine LACk* whose leftmost word represents the 
boundary of the left scope. If k*=0, then the cue 
itself represents its left boundary. 
( )*
1 1
arg max 1
k m
i i
k i i k
k P
= = +
= ?? ?  P?
Similarly, the right boundary of the given cue 
can be decided. 
4.5 Cue Recognition 
Automatic recognition of cues of a special interest 
is the prerequisite for a scope learning system. The 
approaches to recognizing cues of a special interest 
usually fall into two categories: 1) substring 
matching approaches, which require a set of cue 
words or phrases in advance (e.g., Light et al, 
2004); 2) machine learning approaches, which 
train a classifier with either supervised or semi-
supervised learning methods (e.g., ?zg?r and 
Radev, 2009; Szarvas, 2008). Without loss of gen-
erality, we adopt a machine learning approach and 
train a classifier with supervised learning. In par-
ticular, we make an independent classification for 
each word with a BIO label to indicate whether it 
is the first word of a cue, inside a cue, or outside of 
it, respectively. 
LACm    ?.      LAC1 RAC1      ?.    RACn
m n 
Figure 3: Projecting the left and the right argument 
candidates into the word level. 
Inspired by previous studies on similar tasks 
such as WSD and nominal predicate recognition in 
shallow semantic parsing (Lee and Ng, 2002; Li et 
al., 2009), where various features on the word it-
self, surrounding words and syntactic information 
have been successfully used, we believe that such 
information is also valuable to automatic recogni-
tion of cues. Table 3 shows the features employed 
for cue recognition. In particular, we categorize 
these features into three groups: 1) features about 
the cue candidate itself (CC in short); 2) features 
about surrounding words (SW in short); and 3) 
structural features derived from the syntactic parse 
tree (SF in short).
 
Feature Remarks 
Cue Candidate (CC) related 
CC1 The cue candidate itself. (indicate) 
CC2 The stem of the cue candidate. (indicate)
CC3 The POS tag of the cue candidate. (VBP)
Surrounding Words (SW) related 
SW1 The left surrounding words with the win-
dow size of 3. (these, findings) 
SW2 The right surrounding words with the 
window size of 3. (that, corticosteroid,
resistance) 
Structural Features (SF) 
SF1 The subcategory of the candidate node.  
(VP-->VBP+SBAR) 
SF2 The subcategory of the candidate node?s 
parent. (S-->NP+VP) 
SF3 POS tag of the candidate node + Phrase 
type of its parent node + Phrase type of its 
grandpa node. (VBP + VP + S) 
Table 3: Features and their instantiations for cue recog-
nition, with VBP2,2 as the cue candidate, regarding Fig-
ure 2. 
5 Experimentation 
We have evaluated our simplified shallow seman-
tic parsing approach to negation and speculation 
scope learning on the BioScope corpus. 
5.1 Experimental Settings 
Following the experimental setting in Morante et al 
(2008) and Morante and Daelemans (2009a & 
2009b), the abstracts subcorpus is randomly di-
vided into 10 folds so as to perform 10-fold cross-
validation, while the performance on both the pa-
719
pers and clinical reports subcorpora is evaluated 
using the system trained on the whole abstracts 
subcorpus. In addition, SVMLight  is selected as 
our classifier. 
5
For cue recognition, we report its performance 
using precision/recall/F1-measure. For scope iden-
tification, we report the accuracy in PCS (Percent-
age of Correct Scopes) when the golden cues are 
given, and report precision/recall/F1-measure 
when the cues are automatically recognized. 
5.2 Experimental Results on Golden Parse 
Trees and Golden Cues 
In order to select beneficial features from the addi-
tional features proposed in Section 4.3, we ran-
domly split the abstracts subcorpus into the 
training data and the development data with pro-
portion of 4:1. After performing the greedy feature 
selection algorithm on the development data, 7 
features {CFACCP5, CP2R, CFCP1, AC1P, CP3, 
CFACCP7, AC4R} are selected consecutively for 
negation scope identification while 11 features 
{CFACCP5, AC2W, CFACCP2, CFACCP4, AC5, 
CFCP1, CFACCP7, CFACCP1, CP4, AC3P, 
CFAC2} are selected for speculation scope identi-
fication. Table 4 gives the contribution of addi-
tional features on the development data. It shows 
that the additional features significantly improve 
the performance by 11.66% in accuracy from 
74.93% to 86.59% ( ) for negation scope 
identification and improve the performance by 
11.07% in accuracy from 77.29% to 88.36% 
( ) for speculation scope identification. 
The feature selection experiments suggest that the 
features (e.g., CFACCP5, AC2W, CFCP1) related 
to neighboring words of the cue play a critical role 
for both negation and speculation scope identifica-
tion. This may be due to the fact that neighboring 
words usually imply important sentential informa-
tion. For example, ?can not be? indicates a passive 
clause while ?did not? indicates an active clause. 
2; 0.0p? < 1
1
                                                          
2; 0.0p? <
Since the additional selected features signifi-
cantly improve the performance for both negation 
and speculation scope identification, we will in-
clude those additional selected features in all the 
remaining experiments. 
 
 
5 http://svmlight.joachims.org/ 
Task Features Acc (%) 
Baseline 74.93 Negation scope 
identification +selected features 86.59 
Baseline 77.29 Speculation scope 
identification +selected features 88.36 
Table 4: Contribution of additional selected features on 
the development dataset of the abstracts subcorpus 
 
Since all the sentences in the abstracts subcorpus 
are included in the GTB1.0 corpus while we do not 
have golden parse trees for the sentences in the full 
papers and the clinical reports subcorpora, we only 
evaluate the performance of scope identification on 
the abstracts subcorpus with golden parse trees. 
Table 5 presents the performance on the abstracts 
subcorpus by performing 10-fold cross-validation. 
It shows that given golden parse trees and golden 
cues, speculation scope identification achieves 
higher performance (e.g., ~3.3% higher in accu-
racy) than negation scope identification. This is 
mainly due to the observation on the BioScope 
corpus that the scope of a speculation cue can be 
usually characterized by its POS and the syntactic 
structures of the sentence where it occurs. For ex-
ample, the scope of a verb in active voice usually 
starts at the cue itself and ends at its object (e.g., 
the speculation cue ?indicate that? in Figure 2 
scopes the fragment of ?indicate that corticoster-
oid resistance can not be explained by abnormali-
ties?). Moreover, the statistics on the abstracts 
subcorpus shows that the number of arguments per 
speculation cue is smaller than that of arguments 
per negation cue (e.g., 1.5 vs. 1.8). 
 
Task Acc (%) 
Negation scope identification 83.10 
Speculation scope identification 86.41 
Table 5: Accuracy (%) of scope identification with 
golden parse trees and golden cues on the abstracts sub-
corpus using 10-fold cross-validation 
 
It is worth nothing that we adopted the post-
processing algorithm proposed in Section 4.4 to 
ensure the continuousness of identified scope. As 
to examine the effectiveness of the algorithm, we 
abandon the proposed algorithm by simply taking 
the left and right-most boundaries of any nodes in 
the tree which are classified as in scope. Experi-
ments on the abstracts subcorpus using 10-fold 
cross-validation shows that the simple post-
processing rule gets the performance of 80.59 and 
86.08 in accuracy for negation and speculation 
720
scope identification, respectively, which is lower 
than the performance in Table 5 achieved by our 
post-processing algorithm.  
5.3 Experimental Results on Automatic 
Parse Trees and Golden Cues 
The GTB1.0 corpus contains 18,541 sentences in 
which 11,850 of them (63.91%) overlap with the 
sentences in the abstracts subcorpus6. In order to 
get automatic parse trees, we train the Berkeley 
parser with the remaining 6,691 sentences in 
GTB1.0, which achieves the performance of 85.22 
in F1-measure on the remaining 11,850 sentences 
in GTB1.0. Table 6 shows the performance of 
scope identification on automatic parse trees and 
golden cues. In addition, we also report an oracle 
performance to explore the best possible perform-
ance of our system by assuming that our scope 
finder can always correctly determine whether a 
candidate is an argument or not. That is, if an ar-
gument candidate falls within the golden scope, 
then it is a argument. This is to measure the impact 
of automatic syntactic parsing itself. Table 6 shows 
that: 
1) For both negation and speculaiton scope 
identification, automatic syntactic parsing 
lowers the performance on the abstracts 
subcorpus (e.g., from 83.10% to 81.84% in 
accuracy for negation scope identification and 
from 86.41% to 83.74% in accuracy for 
speculaiton scope identification). However, the 
performance drop shows that both negation and 
speculation scope identification are not as 
senstive to automatic syntactic parsing as 
common shallow semantic parsing, whose 
performance might decrease by about ~10 in F1-
measure (Toutanova et al, 2005). This indicates 
that scope identification via simplified shallow 
semantic parsing is robust to some variations in 
the parse trees.  
2) Although speculation scope identification 
consistently achieves higher performance than 
negaiton scope identification when golden parse 
trees are availabe, speculation scope 
identification achieves comparable performance 
with negation scope identification on the 
abstracts subcorpus and the full papers 
                                                          
6 There are a few cases where two sentences in the abstracts 
subcorpus map into one sentence in GTB1.0. 
subcorpus while speculation scope identification 
even performs ~20% lower in accuracy than 
negation scope identification on the clinical 
report subcorpus. This is largely due to that 
specuaiton scope identification is more sensitive 
to syntactic parsing errors than negation scope 
identification due to the wider scope of a 
speculation cue while the sentences of the 
clinical reports come from a different genre, 
which indicates low performance in syntactic 
parsing.  
3) Given the performance gap between the 
performance of our scope finder and the oracle 
performance, there is still much room for further 
performance improvement. 
 
Task Method Abstracts Papers Clinical
auto 81.84 62.70 85.21 Negation scope 
identification oracle 94.37 83.33 98.39 
auto 83.74 61.29 67.90 Speculation scope
identification oracle 95.69 83.72 83.29 
Table 6: Accuracy (%) of scope identification on the 
three subcorpora using automatic parser trained on 
6,691 sentences in GTB1.0 
 
Task Method Abstracts Papers Clinical
M et al (2008) 57.33 n/a n/a 
M & D (2009a) 73.36 50.26 87.27 
Our baseline 73.42 53.70 88.42 
Negation 
scope 
identification 
Our final  81.84 64.02 89.79 
M & D (2009b) 77.13 47.94 60.59 
? & R (2009) 79.89 61.13 n/a 
Our baseline 77.39 54.55 61.92 
Speculation 
scope 
identification 
Our final  83.74 63.49 68.78 
Table 7: Performance comparison of our system with 
the state-of-the-art ones in accuracy (%). Note that all 
the performances achieved on the full papers subcorpus 
and the clinical subcorpus are achieved using the whole 
GTB1.0 corpus of 18,541 sentences while all the per-
formances achieved on the abstract subcorpus are 
achieved using 6,691 sentences from GTB1.0 due to 
overlap of the abstract subcorpus with GTB1.0. 
 
Table 7 compares our performance with related 
ones. It shows that even our baseline system with 
the four basic features presented in Table 1 
achieves comparable performance with Morante et 
al. (2008) and Morante and Daelemans (2009a & 
2009b). This further indicates the appropriateness 
of our simplified shallow semantic parsing ap-
proach and the effectiveness of structured syntactic 
information on scope identification. It also shows 
that our final system significantly outperforms the 
721
state-of-the-art ones using a chunking approach, 
especially on the abstracts and full papers subcor-
pora. However, the improvement on the clinical 
reports subcorpora for negation scope identifica-
tion is much less apparent, partly due to the fact 
that the sentences in this subcorpus are much sim-
pler (with average length of 6.6 words per sentence) 
and thus a chunking approach can achieve high 
performance. Table 7 also shows that our parsing 
approach to speculation scope identification out-
performs the rule-based method in ?zg?r and 
Radev (2009), where 10-fold cross-validation is 
performed on both the abstracts and the full papers 
subcorpora. 
5.4 Experimental Results with Automatic 
Parse Trees and Automatic Cues 
So far negation/speculation cues are assumed to be 
manually annotated and available. Here we turn to 
a more realistic scenario in which cues are auto-
matically recognized. In the following, we first 
report the results of cue recognition and then the 
results of scope identification with automatic cues. 
Cue Recognition 
Task Features R (%) P (%) F1 
CC + SW 93.80 94.39 94.09Negation cue  
recognition CC+SW+SF 95.50 95.72 95.61
CC + SW 83.77 92.04 87.71Speculation cue  
recognition CC+SW+SF 84.33 93.07 88.49
Table 8: Performance of automatic cue recognition with 
gold parse trees on the abstracts subcorpus using 10-fold 
cross-validation 
 
Table 8 lists the performance of cue recognition on 
the abstracts subcorpus, assuming all words in the 
sentences as candidates. It shows that as a com-
plement to features derived from word/pos infor-
mation (CC+SW features), structural features (SF 
features) derived from the syntactic parse tree sig-
nificantly improve the performance of cue recogni-
tion by about 1.52 and 0.78 in F1-measure for 
negation and speculation cue recognition, respec-
tively, and thus included thereafter. In addition, we 
have also experimented on only these words, 
which happen to be a cue or inside a cue in the 
training data as cue candidates. However, this ex-
perimental setting achieves a lower performance 
than that when all words are considered. 
 
Task Corpus R (%) P (%) F1 
Abstracts 94.99 94.35 94.67 
Papers 90.48 87.47 88.95 
Negation cue 
recognition 
Clinical 86.81 88.54 87.67 
Abstracts 83.74 93.14 88.19 
Papers 73.02 82.31 77.39 
Speculation cue 
recognition 
Clinical 33.33 91.77 48.90 
Table 9: Performance of automatic cue recognition with 
automatic parse trees on the three subcorpora 
 
Table 9 presents the performance of cue recog-
nition achieved with automatic parse trees on the 
three subcorpora. It shows that: 
1) The performance gap of cue recognition 
between golden parse trees and automatic parse 
trees on the abstracts subcorpus is not salient 
(e.g., 95.61 vs. 94.67 in F1-measure for negation 
cues and 88.49 vs. 88.19 for speculation cues), 
largely due to the features defined for cue 
recognition are local and insenstive to syntactic 
variations. 
2) The performance of negation cue recognition is 
higher than that of speculation cue recognition 
on all the three subcorpora. This is prabably due 
to the fact that the collection of negation cue 
words or phrases is limitted while speculation 
cue words or phrases are more open. This is 
illustrated by our statistics that about only 1% 
and 1% of negation cues in the full papers and 
the clinical reports subcorpora are absent from 
the abstracts subcorpus, compared to about 6% 
and 20% for speculation cues. 
3) Unexpected, the recall of speculation cue 
recognition on the clinical reports subcorpus is 
very low (i.e., 33.33% in recall measure). This is 
probably due to the absence of about 20% 
speculation cues from the training data of the 
abstracts subcorpus. Moreover, the speculation 
cue ?or?, which accounts for about 24% of 
specuaiton cues in the clinical reports subcorpus, 
only acheives about 2% in recall largely due to 
the errors caused by the classifier trained on the 
abstracts subcorpus, where only about 11% of 
words ?or? are annotated as speculation cues. 
Scope Identification with Automatic Cue Rec-
ognition 
Table 10 lists the performance of both negation 
and speculation scope identification with automatic 
cues and automatic parse trees. It shows that auto-
matic cue recognition lowers the performance by 
722
3.34, 6.80, and 8.38 in F1-measure for negation 
scope identification on the abstracts, the full papers 
and the clinical reports subcorpora, respectively, 
while it lowers the performance by 6.50, 13.14 and 
31.23 in F1-measures for speculation scope identi-
fication on the three subcorpora, respectively, sug-
gesting the big challenge of cue recognition in the 
two scope learning tasks. 
 
Task Corpus R (%) P (%) F1 
Abstracts 78.77 78.24 78.50
Papers 58.20 56.27 57.22
Negation scope 
identification 
Clinical 80.62 82.22 81.41
Abstracts 73.34 81.58 77.24
Papers 47.51 53.55 50.35
Speculation scope 
identification 
Clinical 25.59 70.46 37.55
Table 10: Performance of both negation and speculation 
scope identification with automatic cues and automatic 
parse trees 
6 Conclusion  
In this paper we have presented a new approach to 
scope learning by formulating it as a simplified 
shallow semantic parsing problem, which has been 
extensively studied in the past few years. In par-
ticular, we regard the cue as the predicate and map 
its scope into several constituents which are 
deemed as arguments of the cue. Evaluation on the 
Bioscope corpus shows the appropriateness of our 
parsing approach and that structured syntactic in-
formation plays a critical role in capturing the 
domination relationship between a cue and its 
dominated arguments. It also shows that our pars-
ing approach outperforms the state-of-the-art 
chunking ones. Although our approach is only 
evaluated on negation and speculation scope learn-
ing here, it is portable to other kinds of scope 
learning. 
For the future work, we will explore tree kernel-
based methods to further improve the performance 
of scope learning in better capturing the structural 
information, and apply our parsing approach to 
other kinds of scope learning. 
Acknowledgments 
This research was supported by Projects 60873150, 
60970056, and 90920004 under the National Natu-
ral Science Foundation of China, Project 
20093201110006 under the Specialized Research 
Fund for the Doctoral Program of Higher Educa-
tion of China. 
References  
Xavier Carreras and Llu?s M?rquez. 2005. Introduction 
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. CoNLL? 2005. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. A 
Simple Algorithm for Identifying Negated Findings 
and Diseases in Discharge Summaries. Journal of 
Biomedical Informatics, 34: 301-310. 
Nigel Collier, Hyun Seok Park, Norihiro Ogata, et al 
1999. The GENIA Project: Corpus-Based Knowl-
edge Acquisition and Information Extraction from 
Genome Research Papers. EACL?1999. 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos 
Csirik, and Gy?rgy Szarvas. 2010. The CoNLL-2010 
Shared Task: Learning to Detect Hedges and their 
Scope in Natural Language Text. CoNLL?2010: 
Shared Task. 
Daniel Gildea and Martha Palmer. 2002. The Necessity 
of Parsing for Predicate Argument Recognition. 
ACL?2002. 
Ilya M. Goldin and Wendy W. Chapman. 2003. Learn-
ing to Detect Negation with ?Not? in Medical Texts. 
SIGIR?2003. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic 
Role Labeling of NomBank: A Maximum Entropy 
Approach. EMNLP? 2006. 
Yoong Keok Lee and Hwee Tou Ng. 2002. An Empiri-
cal Evaluation of Knowledge Sources and Learning 
Algorithms for Word Sense Disambiguation. 
EMNLP?2002. 
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu, 
and Peide Qian. 2009. Improving Nominal SRL in 
Chinese Language with Verbal SRL Information and 
Automatic Predicate Recognition. EMNLP? 2009. 
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The Language of Bioscience: Facts, Specula-
tions, and Statements in Between. BioLink?2004. 
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific 
Literature. ACL?2007. 
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the Scope of Negation in Bio-
medical Texts. EMNLP?2008. 
723
Roser Morante and Walter Daelemans. 2009a. A 
Metalearning Approach to Processing the Scope of 
Negation. CoNLL?2009. 
Roser Morante and Walter Daelemans. 2009b. Learning 
the Scope of Hedge Cues in Biomedical Texts. 
BioNLP?2009. 
Lilja ?vrelid, Erik Velldal, and Stephan Oepen. 2010. 
Syntactic Scope Resolution in Uncertainty Analysis. 
COLING?2010. 
Arzucan ?zg?r, Dragomir R. Radev. 2009. Detecting 
Speculations and their Scopes in Scientific Text. 
EMNLP?2009. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. NAACL?2007. 
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2005. 
The Necessity of Syntactic Parsing for Semantic Role 
Labeling. IJCAI? 2005. 
Gy?rgy Szarvas. 2008. Hedge Classification in Bio-
medical Texts with a Weakly Supervised Selection of 
Keywords. ACL?2008. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, and 
J?nos Csirik. 2008. The BioScope corpus: Annota-
tion for Negation, Uncertainty and their Scope in 
Biomedical Texts. BioNLP?2008. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun?ichi Tsujii. 2005. Syntax Annotation for the 
GENIA Corpus. IJCNLP?2005 (Companion volume). 
Peter D. Turney. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised Clas-
sification of Reviews. ACL?2002. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The BioScope 
corpus: biomedical texts annotated for uncertainty, 
negation and their scopes. BMC Bioinformatics, 
9(Suppl 11):S9. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. EMNLP?2004. 
Nianwen Xue. 2008. Labeling Chinese Predicates with 
Semantic Roles. Computational Linguistics, 
34(2):225-255.  
724
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1006?1016, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Employing Compositional Semantics and Discourse Consistency in 
Chinese Event Extraction 
Peifeng Li, Guodong Zhou, Qiaoming Zhu, Libin Hou 
School of Computer Science & Technology 
Soochow University, Suzhou, 215006, China 
{pfli, gdzhou, qmzhu, 20094227021}@suda.edu.cn 
 
Abstract 
Current Chinese event extraction systems suffer 
much from two problems in trigger 
identification: unknown triggers and word 
segmentation errors to known triggers. To 
resolve these problems, this paper proposes two 
novel inference mechanisms to explore special 
characteristics in Chinese via compositional 
semantics inside Chinese triggers and discourse 
consistency between Chinese trigger mentions. 
Evaluation on the ACE 2005 Chinese corpus 
justifies the effectiveness of our approach over 
a strong baseline. 
1 Introduction 
Event extraction, a classic information extraction 
task, is to identify instances of a predefined event 
type and can be typically divided into four subtasks: 
trigger identification, trigger type determination, 
argument identification and argument role 
determination. In the literature, most studies focus 
on English event extraction and have achieved 
certain success (e.g. Grishman et al 2005; Ahn, 
2006; Hardy et al 2006; Maslennikov and Chua, 
2007; Finkel et al 2005; Ji and Grishman, 2008; 
Patwardhan and Riloff, 2009, 2011; Liao and 
Grishman 2010; Hong et al 2011).  
In comparison, there are few successful stories 
regarding Chinese event extraction due to special 
characteristics in Chinese trigger identification. In 
particular, there are two major reasons for the low 
performance: unknown triggers 1  and word 
segmentation errors to known triggers. Table 1 
gives the statistics of unknown triggers and word 
segmentation errors to known triggers in both the 
                                                          
1 In this paper, a trigger word/phrase occurring in the training 
data is called a known trigger and otherwise, an unknown 
trigger.  
ACE 2005 Chinese and English corpora2 using 10-
fold cross-validation. In each validation, we leave 
10% trigger mentions as the test set and the 
remaining ones as the training set. If a mention in 
the test set doesn?t occurred in the training set, we 
regard it as an unknown trigger. It shows that these 
two cases cover almost 30% of Chinese trigger 
mentions while this figure reduces to only about 
9% in English. It also shows that given the same 
number of event mentions, there are 30% more 
different triggers in Chinese than that in English. 
This justifies the low performance (specifically, 
the recall) of a Chinese event extraction system, 
which normally extracts those known triggers 
occurring in the training data as candidate 
instances and uses a classifier to distinguish correct 
triggers from wrong ones. 
 
Language Chinese English 
%unknown triggers 33.7% 18.5% 
%unknown trigger mentions 20.9% 8.9% 
%word segmentation errors 
to known trigger mentions 
8.7% 0% 
#triggers 763 586 
Table 1. Statistics: a comparison between Chinese and 
English event extraction with regard to unknown 
triggers and word segmentation errors to known triggers. 
Note that word segmentation only applies to Chinese. 
 
In this paper, we propose two novel inference 
mechanisms to Chinese trigger identification by 
employing compositional semantics inside Chinese 
triggers and discourse consistency between 
Chinese trigger mentions.  
The first mechanism is motivated by the 
compositional nature of Chinese words, whose 
semantics can be often determined by the 
component characters. Hence, it is natural to infer 
                                                          
2  The whole Chinese ACE corpus has about 3300 event 
mentions. For the sake of fair comparison, we choose the same 
number of event mentions from the English corpus as the 
cross-validation data. 
1006
unknown triggers by employing compositional 
semantics inside Chinese triggers.  
The second mechanism is enlightened by the 
wide use of discourse consistency in natural 
languages, particularly for Chinese, due to its 
discourse-driven nature (Zhu, 1980). Very often, 
distinguishing true trigger mentions from pseudo 
ones is only possible with contextual information.  
The rest of this paper is organized as follows. 
Section 2 overviews the related work. Section 3 
introduces a state-of-the-art baseline system for 
Chinese event extraction. Sections 4 and 5 describe 
two novel inference mechanisms to Chinese trigger 
identification by employing compositional 
semantics inside Chinese triggers and discourse 
consistency between Chinese trigger mentions. 
Section 6 presents the experimental results. Section 
7 concludes the paper and points out future work. 
2 Related Work 
Almost all the existing studies on event extraction 
concern English. While earlier studies focus on 
sentence-level extraction (Grishman et al 2005; 
Ahn, 2006; Hardy et al 2006), later ones turn to 
employ high-level information, such as document 
(Maslennikov and Chua, 2007; Finkel et al 2005; 
Patwardhan and Riloff, 2009), cross-document (Ji 
and Grishman, 2008), cross-event (Liao and 
Grishman, 2010; Gupta and Ji, 2009) and cross-
entity (Hong et al 2011) information. 
2.1 Chinese Event Extraction  
Compared with tremendous efforts in English 
event extraction, there are only a few studies on 
Chinese event extraction.  
Tan et al(2008) modeled event extraction as a 
pipeline of classification tasks. Specially, they used 
a local feature selection approach to ensure the 
performance of trigger classification (trigger 
identification + trigger type determination) and 
applied multiple levels of patterns to improve the 
coverage of patterns in argument classification 
(argument identification + argument role 
determination). Chen and Ji (2009a) proposed a 
bootstrapping framework, which exploited extra 
information captured by an English event 
extraction system. Chen and Ji (2009b) applied 
various kinds of lexical, syntactic and semantic 
features to address the specific issues in Chinese. 
They also constructed a global errata table to 
record the inconsistency in the training set and 
used it to correct the inconsistency in the test set. Ji 
(2009) extracted cross-lingual predicate clusters 
using bilingual parallel corpora and a cross-lingual 
information extraction system, and then used the 
derived clusters to improve the performance of 
Chinese event extraction. 
2.2 Compositional Semantics 
Almost all the related studies on compositional 
semantics focus on how to combine words together 
to convey complex meanings, such as semantic 
parser (Zettlemoyer and Collins, 2007; Wong and 
Mooney, 2007; Liang et al 2011). However, the 
compositional semantics mentioned in this paper is 
more fined-grained and focuses on how to 
construct Chinese characters into a word and mine 
the semantics of words from the word structures, 
especially of verbs as event triggers.  
To our knowledge, there is only one paper 
associated with compositional semantics inside 
Chinese words. Li (2011) discussed the internal 
structures inside Chinese nouns and used it in word 
segmentation.  
2.3 Discourse Consistency 
Discourse consistency is an important hypothesis 
in natural languages and has been applied to many 
natural language processing applications, such as 
named entity recognition and coreference 
resolution. Specially, several studies have 
successfully incorporated trigger or entity 
consistency constraint into event extraction.  
Yarowsky (1995) and Yangarber et al
(Yangarber and Jokipii, 2005; Yangarber et al 
2007) applied cross-document inference to refine 
local extraction results for disease name, location 
and start/end time. Mann (2007) proposed some 
specific inference rules to improve extraction of 
personal information. Ji and Grishman (2008) 
employed a rule-based approach to propagate 
consistent triggers and arguments across topic-
related documents. Gupta and Ji (2009) used a 
similar approach to recover implicit time 
information for events. Liao and Grishman (2011) 
also used a similar approach and a self-training 
strategy to extract events. Liao and Grishman 
(2010) employed cross-event consistency 
information to improve sentence-level event 
extraction. Hong et al(2011) regarded entity type 
1007
consistency as a key feature to predict event 
mentions and adopted this inference method to 
improve the traditional event extraction system.  
3 Baseline 
As a baseline, we re-implement a state-of-the-art 
system, which consists of four typical components 
(trigger identification, trigger type determination, 
argument identification and argument role 
determination), in a pipeline way and employ the 
same set of features as described in Chen and Ji 
(2009b). 
Besides, the Maximum-Entropy (ME) model is 
employed to train individual component classifiers 
for the above four components. During testing, 
each word in the test set is first scanned for 
instances of known triggers from the training set. 
When an instance is found, the trigger identifier is 
applied to distinguish true trigger mentions from 
pseudo ones. If true, the trigger type determiner is 
then applied to recognize its event type. For any 
entity mentions in the sentence, the argument 
identifier is employed to assign possible arguments 
to them afterwards. Finally, the argument role 
determiner is introduced to assign a role to each 
argument.  
One problem with Chen and Ji?s system is its 
ignoring effective long-distance features. In order 
to resolve this problem and provide a stronger 
baseline, we introduce more refined and 
dependency features in four components:  
? Trigger Identification and Trigger Type 
Determination: 1) syntactic features: path to 
the root of the governing clause, 2) nearest 
entity information: entity type of left 
syntactically/physically nearest entity to the 
trigger + entity, entity type of right 
syntactically/physically nearest entity to the 
trigger mention in the sentence + entity; 3) 
dependency features: the subject and the object 
of the trigger when they are entities. 
? Argument Identification and Argument Role 
Determination: 1) basic features: POS of 
trigger; 2) neighboring words: left neighboring 
word of the entity + its POS, right neighbor 
word of the entity + its POS, left neighbor word 
of the trigger + its POS, right neighbor word of 
the trigger + its POS; 3) dependency feature: 
dependency path from the entity to the trigger; 
4) semantic role features: Arg0 and Arg1 which 
tagged by semantic role labeling tool (Li, et al 
2010). 
3.1 Experimental Setting 
The ACE 2005 Chinese corpus (only the training 
data is available) is used in all our experiments. 
The corpus contains 633 Chinese documents 
annotated with 8 predefined event types and 33 
predefined subtypes. Similar to previous studies, 
we treat these subtypes simply as 33 separate event 
types and do not consider the hierarchical structure 
among them. 
Following Chen and Ji (2009b), we randomly 
select 567 documents as the training set and the 
remaining 66 documents as the test set. Besides, 
we reserve 33 documents in the training set as the 
development set, and follow the setting of ACE 
diagnostic tasks and use the ground truth entities, 
times and values for our training and testing. 
For evaluation, we follow the standards as 
defined in Ji (2009):  
? A trigger is correctly identified if its position in 
the document matches a reference trigger; 
? A trigger type is correctly determined if its 
event type and position in the document match 
a reference trigger; 
? An argument is correctly identified if its 
involved event type and position in the 
document match any of the reference argument 
mentions; 
? An argument role is correctly determined if its 
involved event type, position in the document, 
and role match any of the reference argument 
mentions. 
Finally, all sentences in the corpus are divided 
into words using a word segmentation tool 
ICTCLAS3 with all entities annotated in the corpus 
kept. Besides, we use Stanford Parser (Levy and 
Manning, 2003, Chang, et al 2009) to create the 
constituent and dependency parse trees and employ 
the ME model to train individual component 
classifiers. 
3.2 Experimental Results 
Table 2 and 3 show the Precision (P), Recall (R) 
and F1-Measure (F) on the held-out test set. It 
shows that our baseline system outperforms Chen 
and Ji (2009b) by 1.8, 2.2, 3.9 and 2.3 in F1-
measure on trigger identification, trigger type 
                                                          
3 http://ictclas.org/ 
1008
determination, argument identification and 
argument role determination, respectively, with 
both gains in precision and recall. This is simply 
due to contribution of the newly-added refined and 
dependency features. 
 
Performance 
 
System 
Trigger 
Identification 
Trigger Type 
Determination 
P(%) R(%) F P(%) R(%) F 
Chen and Ji 
(2009b) 
71.5 51.2 59.7 66.5 47.7 55.6 
Our Baseline 75.2 52.0 61.5 70.3 49.0 57.8 
Table 2. Performance of trigger identification and 
trigger type determination  
 
Performance 
 
System 
Argument 
Identification 
Argument Role 
Determination 
P(%) R(%) F P(%) R(%) F 
Chen and Ji 
(2009b) 
56.1 38.2 45.4 53.1 36.2 43.1 
Our Baseline 58.4 42.7 49.3 55.2 38.6 45.4 
Table 3. Performance of argument identification and 
argument role determination 
 
For our baseline system, given the small 
performance gaps between trigger identification 
and trigger type determination (3.7 in F1-measure: 
61.5 vs. 57.8) and between argument identification 
and argument role determination (3.9 in F1-
measure: 49.3 vs. 45.4), the performance 
bottlenecks of our baseline system mainly exist in 
trigger identification and argument identification, 
particularly for the former one. While argument 
identification has the performance gap of 8.5 in 
F1-measure compared to trigger type 
determination (49.3 vs. 57.8), the former one, 
trigger identification, can only achieve the 
performance of 61.5 in F1-measure (in particular 
the recall with only 52.0). In this paper, we will 
focus on trigger identification to improve its 
performance, particularly for the recall, via 
compositional semantics inside Chinese triggers 
and discourse consistency between Chinese trigger 
mentions.  
4 Employing Compositional Semantics 
inside Chinese Triggers  
Language is perhaps the only communicative 
system in nature, which compositionally builds 
structured meanings from smaller pieces, and this 
compositionality is the cognitive mechanism that 
allows for what Humboldt called language?s 
?infinite use of finite means.? As usual, the lexical 
semantics is the smallest piece in most Chinese 
language processing applications. In this section, 
we introduce a more fine-grained semantics - the 
compositional semantics in Chinese verb structure 
- and unveil its effect and usage in Chinese 
language processing by employing it into Chinese 
event extraction. 
4.1 Compositional Semantics inside Chinese 
Triggers 
In English, a component character is just the basic 
unit to form a word instead of a semantics unit. In 
comparison, almost all Chinese characters have 
their own meanings and can be formed as SCWs 
(Single Character Words) themselves. If a Chinese 
word contains more than one character, its 
meaning can be often inferred from the meanings 
of its component characters (Yuan, 1998). Actually, 
it is the normal way of understanding a new 
Chinese word in everyday life of a Chinese native 
speaker. A general method to this problem is to 
systematically explore the morphological 
structures in Chinese words. In this paper, 
compositional semantics provides a simple but 
effective compromise to the general method and 
we leave the general method in the future work. 
Table 4 shows samples of such compositional 
semantics in Chinese words. For example, ???? 
is composed of two characters: ??? and ??? 
which have their own semantics and the semantics 
of ??? ? comes from that of its component 
characters ??? and ???.  
 
Words Characters 
?? (interview4) ? (meet) ?(meet) 
?? (shoot and kill) ?(shoot) ? (kill) 
??(come)  
?? (private letter) 
? (come) ? (to) 
?(private) ?(letter) 
Table 4. Examples of compositional semantics in 
Chinese words 
 
Therefore, it is natural to infer unknown triggers 
by employing compositional semantics inside 
Chinese triggers. Take following two sentences as 
examples: 
(1) 4?????????(Known trigger) 
                                                          
4  Most Chinese words have more than one sense. Here, we 
just give the one when it acts as a trigger. 
1009
(Four students were scratched by the glass.) 
(2)  1???????(Unknown trigger) 
(A passenger was stabbed.) 
where ???? is a known trigger and ???? is an 
unknown one.  
In above examples, the semantics of ???? 
(injure by scratching) can be largely determined 
from those of its component characters ?? ? 
(scratch) and ??? (injure) while the semantics of 
??? ? (injure by stabbing) from those of its 
component characters ??? (stab) and ??? (injure). 
Since these two triggers have similar internal 
structures, we can easily infer that ???? is a 
trigger of injure event if ???? is known as a 
trigger of injure event. Similarly, we can infer 
more triggers for injure event, such as ???? 
(injure by burning), ???? (injure by hitting), ??
? ? (injure by pressing), all with component 
character ??? (injure) as the head and the other 
component character as the way of causing injury.  
Since most triggers in Chinese event extraction 
are verbs 5 , we focus on the compositional 
semantics in the verb structure. Statistics on the 
training set shows that 3.3% triggers (e.g. ???
?? (open letter), ???? (event), ???? (patient's 
condition), etc.) don?t contain a BV and all of them 
are nouns. Normally, almost all verbs contain one 
or more single-character verbs as the basic element 
to construct a verb (we call it basic verb, shorted as 
BV) and the semantics of such a verb thus can be 
inferred from its BV. There are some studies on the 
Chinese verb structure in linguistics. However, 
their structures are much more complex and there 
are no annotated corpora available. We define 
following six main structures from our empirical 
observations: 
(1) BV (e.g. ??? (see), ??? (kill)) 
(2) BV + verb (e.g. ???? (meet)) 
(3) verb + BV (e.g. ???? (fire) ) 
(4) BV + complementation (e.g. ???? (kill) ) 
(5) BV + noun/adj. (e.g. ???? (go to home)) 
(6) noun/adj. +BV (e.g. ???? (shoot using 
gun)). 
                                                          
5 Actually, in the ACE 2005 Chinese (training) corpus, more 
than 90% of triggers are either verbs al or verbal nouns (those 
verbs which act as nouns). For simplicity, we don?t 
differentiate these two types in this paper. 
From above structures, a BV plays an important 
role in the verb structure and most of semantics of 
a verb can be interred from its contained BV and 
two words normally have very similar semantics if 
they have the same BV (e.g. ???? (meet) and 
???? (meet)). Actually, sometime the verb can 
be shortened to its contained BV (e.g. ?????
? ? and ??????? ? have the same 
semantics.).  
4.2 Inferring via Compositional Semantics 
inside Chinese Triggers 
Here a simple rule is employed to infer triggers via 
compositional semantics inside Chinese triggers: a 
verb is a trigger if it contains a BV which occurs 
as a known trigger or is contained in a known 
trigger. Table 5 shows the distribution of the set of 
triggers (contains the same BV 6 ) classified by 
number of triggers.  
From Table 5, we can find out that 85.3% of 
BVs occur in more than one trigger and 56.2% of 
them in more than 4 triggers. As for trigger 
mentions, these percentages become 89.1% and 
65.2% respectively. A extreme example is that 
85.2% (75/88) of triggers of Trial-Hearing event 
mentions contain ??? (trial) and 85.4% (117/138) 
of triggers of injure event mentions contains ??? 
(injure).  
 
Number  Distribution over 
Triggers 
Distribution over 
Trigger Mentions 
1 14.7% 10.9% 
2~4 29.1% 23.9% 
5~9 28.1% 32.9% 
>=10 28.1% 32.3% 
Table 5. Distribution of BVs in the number of 
triggers/trigger mentions  
 
In this paper, the inference is done as follows: 
? Add all single-character triggers into the BV set 
if it?s a verb; 
? Split all other triggers in the training set into a 
set of single characters and include all single 
characters into the BV set if it?s a verb; 
? For each word in the test set, it is identified as a 
trigger if it contains a BV. 
It is worthwhile to note that such inference 
works for unknown triggers and word 
                                                          
6 We didn?t tag BVs in the training set and regards all single-
character verbs contained in triggers as BVs. 
1010
segmentation errors to known triggers since in both 
cases, their BVs will always exist as either a SCW 
or a component of a word. 
4.3 Noise Filtering  
One problem with above inference is that while it 
is able to recover some true triggers and increase 
the recall, it may introduce many pseudo ones and 
harm the precision. To filter out those pseudo 
triggers, we propose following rules according to 
our intuition and statistics over the training set. 
Non-trigger Filtering 
A Chinese word will not be a trigger if it 
appears in the training set but never trigger an 
event. Statistics on the training set shows that this 
rule applies at 99.7% of cases. 
POS filtering 
A Chinese word will not be a trigger if it has a 
different POS from that of the same known 
trigger or similar known triggers 7  in the 
training set. In Chinese, a single-character verb 
has very high probability of composing words (e.g. 
??? (come), ??? (act as), ??? (combine), etc) 
with different POS from the single-character verb 
itself, such as preposition (e.g. ??? ? (for)), 
conjunction (e.g. ???? (and)), etc. Statistics on 
the training set shows that this rule applies at 
97.3% of cases.  
Verb structure filtering 
A Chinese word will not be a trigger if its verb 
structure is different from that of the same 
known trigger or similar known triggers in the 
training set. Figure 1 shows different distributions 
of three BVs over six verb structures as described 
in subsection 4.1. For example, we can find that all 
triggers including ??? (unbind) (e.g. ???? (fire), 
???? (fire), ???? (disband)) just have one verb 
structure (BV + verb) and those of ??? (kill) have 
4 structures. Obviously, we can use such 
distribution information to filter out pseudo 
triggers. For example, although both word ???? 
(console) and ???? (decompose) are constructed 
form verb ???, their verb structure (verb + BV) 
does not appear in the training set. Therefore, they 
will be filtered our via verb structure filtering. 
                                                          
7 Similar triggers are those ones which have the same BV and 
verb structure. 
Statistics on the training set shows that this rule 
applies at 95.5% of cases. 
 
0
0.2
0.4
0.6
0.8
1
BV verb+BV
BV+Verb
N/Adj+BV
BV+Comp
BV+N/Adj
?
?
?
Figure 1. Distribution of three BVs (??? (unbind), ??? 
(trial) and ??? (kill)) over six verb structures in 
constructing triggers 
5 Employing Discourse Consistency 
between Chinese Trigger Mentions  
Chinese event extraction may suffer much from the 
errors propagated from upstream processing such 
as part-of-speech tagging and parsing, especially 
word segmentation. To alleviate word 
segmentation errors to known triggers, Chen and Ji 
(2009b) constructed a global errata table to record 
the inconsistency in the training set and proved its 
effectiveness. In this paper, a merge and split 
method is applied to recover those known triggers. 
In this way, word segmentation errors can be 
alleviated to certain extent.  
For unknown triggers, we can merge two or 
more neighboring short words or single characters 
as a trigger candidate. In this paper, for each 
single-character verb in a document after word 
segmentation, this single-character verb can be 
merged with either previous SCW or next SCW to 
form a trigger candidate if this single-character 
verb has occurred in the training set with the same 
verb structure. 
Given above recovered triggers for both known 
and unknown triggers, the key issue here is how to 
distinguish true triggers from pseudo ones. In this 
paper, we employ discourse consistency between 
Chinese trigger mentions for Chinese event 
extraction. Previous studies on English event 
extraction have proved the effectiveness of both 
cross-entity and cross-document consistency.  
5.1 Discourse Consistency between Chinese 
Trigger Mentions  
As a discourse-driven language, the syntax of 
1011
Chinese is not as strict as English and sometime 
we must infer from the discourse-level information 
to understand the meaning of a sentence. Kim 
(2000) compared the use of overt subjects in 
English and Chinese and he found that overt 
subjects occupy over 96% in English, while this 
percentage drops to only 64% in Chinese. 
Similarly, argument missing is another issue in 
Chinese event extraction and almost 55% of 
arguments are missing in the ACE 2005 Chinese 
corpus. Normally, using a feature-based approach 
to distinguish true triggers from pseudo ones is 
very difficult from the sentence level if some of 
related arguments are missing from the trigger-
occurring sentence. Take following two contingent 
sentences as examples: 
(3) ????? 3????????????  
(The United States and the Democratic 
People's Republic of Korea finished missile 
talks in Kuala Lumpur.) 
(4) ???????? 
(The talks are serious.) 
While it is relatively easy to determine that 
mention ???? in sentence (3) indicates a meet 
event from the contained information in itself 
(there are many entities, such as agents, time and 
place in the sentence) and difficult to determine 
that mention ???? in sentence (4) is a meet event 
from the contained information in itself, we can 
easily infer from sentence (3) that sentence (4) also 
indicates a meet event, using discourse consistency: 
if one instance of a word is a trigger mention, other 
instances in the same discourse will be a trigger 
mention with high probability.  
 
Language Discourse-based Instance-based 
English 70.2% 87.5% 
Chinese 90.5% 95.4% 
Table 6. Comparison of discourse consistency between 
Chinese and English trigger mentions 
 
Table 6 compares the probabilities of discourse 
consistency between Chinese and English trigger 
mentions in the ACE 2005 Chinese and English 
corpora. A trigger may appear many times in a 
discourse. It?s considered discourse-consistent 
when all the appearances of a trigger have the 
same event type while instance-based consistency 
refers to pair-wired cases. It shows that within the 
discourse, there is a strong consistency in both 
Chinese and English between trigger mentions: if 
one instance of a word is a trigger, other instances 
in the same discourse will be a trigger of the same 
event type with very high probability. 
0.85
0.9
0.95
1
?
?
? ?
?
? ?
?
?
?
?
?
?
?
?
?
?
 
Figure 2. Probabilities of discourse-level consistency of 
top 10 frequent triggers 
It also shows that discourse consistency in 
Chinese triggers holds much more likely than the 
English counterpart. Figure 2 give the probabilities 
of discourse-level consistency of top 10 frequent 
triggers, which occupy 18% of event mentions in 
the ACE 2005 Chinese corpus. 
5.2 Inference via Discourse Consistency 
between Chinese Trigger Mentions  
Given a discourse and different mentions of a 
trigger returned by the trigger identifier, we can 
simply accept those mentions with high probability 
as true mentions of the trigger and discard those 
with low probability8. However, for those mentions 
in-between, an additional discourse-level trigger 
identifier is further employed to determine whether 
a trigger mention is true or not from the discourse 
level by augmenting the normal trigger identifier 
with several features to explore the consistency 
information between trigger mentions in the 
discourse (first three features) and the related 
information returned from the trigger type 
identifier (last two features).  
? Probability of the discourse consistency of the 
candidate trigger mention in the training set. If 
it doesn?t exist in the training set, we infer its 
probability from that of all of its similar triggers 
? Number of candidate trigger mentions being a 
trigger in the same discourse via trigger 
identification 
? Number of candidate trigger mentions being a 
non-trigger in the same discourse via trigger 
identification 
                                                          
8 The high and low probability thresholds are fine-tuned to 
95% and 5% respectively, using the development set. 
1012
? Event type of candidate trigger mention via 
trigger type determination 
? Confidence of trigger type determination 
6 Experiments 
In this section, we evaluate our two inference 
mechanisms in Chinese trigger identification and 
its application to overall Chinese event extraction, 
using the same experimental settings as described 
in Subsection 3.1. 
6.1 Chinese Trigger Identification 
Table 7 shows the impact of compositional 
semantics in trigger identification. Here, the 
baseline just extracts those triggers occurring in the 
training data. It justifies the effectiveness of our 
compositional semantics-based inference 
mechanism in recovering true triggers and its three 
filtering rules in removing pseudo triggers.  
 
                    Numbers 
Approaches 
Triggers Non-triggers 
Baseline 266 629 
+Compositional semantics 
without filtering 
334 1885 
+ Non-trigger filtering 328 1062 
+ POS filtering 325 974 
+ Verb structure filtering 302 444 
Gold 367 - 
Table 7. Impact of compositional semantics in trigger 
identification 
 
To reduce those pseudo triggers after above 
inference process, three rules are introduced.  
The first rule, the non-trigger filtering rule, 
filters out those pseudo ones in the test set which 
do not frequently occur as trigger mentions in the 
training set. In particular, to keep true triggers in 
our candidate set as many as possible, we just filter 
out those candidates which occur as non-triggers 
more than 5 times in the training set according to 
our validation on the development set. Table 7 
shows that 43.7% (823) of pseudo triggers are 
filtered out while only 1.8% (6) of true ones is 
wrongly filtered out.  
The second rule, the POS filtering rule, just 
filters out 8.3% (88) of pseudo triggers, due to 
POS errors in word segmentation and constituent 
parsing (e.g. 9.4% of candidate triggers have 
wrong POS tags in the development set.). Manual 
inspection shows that if we correct those wrong 
POS tags, that percentage will be increased to 
14.5%. 
The third rule, the verb structure filtering rule, is 
deployed in following steps: 1) keeping all 
candidates if they act as a trigger in the training set; 
2) if the candidate is a SCW, removing it when it 
does not occur as a BV in any triggers in the 
training set; 3) if the candidate is not a SCW, 
calculating the condition probability of its similar 
trigger words as triggers in the training set9 and 
then deleting all candidates whose conditional 
probabilities are less than a threshold ? , which is 
fine-tuned to 0.5. Figure 3 shows the effect on 
precision, recall and F1-measure of varying the 
threshold?  on the development set. 
0.5
0.6
0.7
0.8
0 0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
6
0
.
7
P
R
F
 
Figure 3. Effect of threshold ?  on the development 
set 
 
                Performance 
System 
Trigger Identification 
P(%) R(%) F 
Baseline 75.2 52.0 61.5 
+Compositional semantics 
without filtering 
34.8 66.8 45.8 
+ Non-trigger filtering 49.4 66.5 56.7 
+ POS filtering 50.2 65.9 57.0 
+ Verb structure filtering 73.5 62.1 67.4 
+Discourse consistency 79.3 63.5 70.5 
Table 8. Contribution to Chinese triggers identification 
(incremental) 
 
Table 8 shows the contribution of employing 
compositional semantics and discourse consistency 
to trigger identification on the held-out test set. We 
can find out that our approach dramatically 
enhances F1-measure by 9.0 units, largely due to a 
dramatic increase of 11.5% in recall, benefiting 
from both compositional semantics and discourse 
consistency mechanisms. We expect that the 
precision will also increase since our filtering 
approach successfully filters out almost 30% more 
                                                          
9 If there are more than one BV in a candidate, we calculate 
the average one. 
1013
non-triggers and the number of non-trigger 
mentions is less than that of the baseline. 
Unfortunately, the resulting set of 444 non-trigger 
mentions (after all filtering) is not a subset of 
original 629 non-trigger ones. Our observation 
shows that our compositional semantics inference 
adds almost 10% new non-triggers into candidates 
which are very hard to distinguish.  
Table 8 also justifies the impact of the discourse 
consistency between trigger mentions in trigger 
identification and the effect of the additional 
discourse-level trigger identifier, with a big gain of 
5.8% in precision and a small gain of 1.4% in 
recall. 
6.2 Chinese Event Extraction 
Table 9 shows the contribution of trigger 
identification with compositional semantics and 
discourse consistency to overall event extraction 
on the held-out test set. In addition, we also report 
the performance of two human annotators (The 
human annotator 1 is a first year postgraduate 
student with no background to Chinese event 
extraction while the human annotator 2 is a third 
year postgraduate student working on Chinese 
event extraction) on 33 texts (a subset of the held-
out test set). From the results presented in Table 9, 
we can find that our approach can improve the F1-
measure for trigger identification by 9.0 units, 
trigger type determination by 9.1 units, argument 
identification by 6.0 units and argument role 
determination (i.e. overall event extraction) by 5.4 
units, largely due to the dramatic increase in recall 
of 11.5%, 11.2%, 7.5% and 7.2%.  
 
                        Performance 
 
System/Human 
Trigger 
Identification 
Trigger Type 
Determination 
Argument 
Identification 
Argument Role 
Determination 
P(%) R(%) F P(%) R(%) F P(%) R(%) F P(%) R(%) F 
Our Baseline 75.2 52.0 61.5 70.3 49.0 57.8 58.4 42.7 49.3 55.2 38.6 45.4 
+Compositional semantics 73.5 62.1 67.4 70.2 59.1 64.2 58.0 48.9 53.0 54.7 44.5 49.1 
+Discourse consistency 79.3 63.5 70.5 75.2 60.2 66.9 61.6 50.2 55.3 56.9 45.8 50.8 
Human annotator1(blind) 63.3 62.9 63.1 61.7 59.5 60.6 64.6 54.1 58.9 60.9 48.2 53.8 
Human annotator2(familiar) 72.6 74.3 73.4 69.1 70.2 69.6 71.5 65.9 68.6 66.4 54.6 59.9 
Inter-Annotator Agreement 45.8 42.9 44.3 45.3 42.5 43.8 60.4 49.7 54.5 55.1 45.9 50.1 
Table 9: Overall contribution to Chinese event extraction  
 
In addition, the results of two annotators show 
that Chinese event extraction is really challenging 
even for a well-educated human being. As shown 
in Table 9, the inter-annotator agreement on trigger 
identification and trigger type determination is 
even less than 45%. Although this figure is very 
low, it is not surprising: the results on the English 
ACE 2005 corpus show that the inter-annotator 
agreement on trigger identification is only about 
40% (Ji and Grishman, 2008). Detailed analysis 
shows that a human annotator tends to make more 
mistakes in trigger identification for two reasons. 
The first reason is that a human annotator always 
misses some event mentions when a sentence 
contains more than one event mention. The second 
reason is that it is hard to identify an event mention 
due to the failure of following specified annotation 
guidelines, as mentioned in Ji and Grishman 
(2008). Table 9 also shows the performance gaps 
of human annotators between trigger identification 
and trigger type determination is very small (2.5% 
and 3.8% in F1-measure). It ensures that trigger 
identification is the most important step in Chinese 
event extraction for a human being. For human 
annotators, it?s much easier to determine the event 
type of a trigger, identify its arguments and 
determine the role of each argument, all with more 
than 90% in accuracy, once a trigger is identified 
correctly.  
6.3 Discussion 
Compared with English, the word structures in 
Chinese are much more complex and diverse, 
causing a lot of troubles in Chinese language 
processing. We ensure that compositional 
semantics in Chinese words is very useful for 
many Chinese language processing applications, 
such as machine translation, semantic parser, etc. 
For example, many actions (e.g. ??? (hack), ??? 
(bite), ??? (kick), etc) can combine with ??? 
(injure) to form words and most of those words 
have similar semantics. The results in table 8 show 
its contribution in Chinese event extraction. 
Although our approach is simple, the result is 
1014
promising enough for further efforts in this 
direction.  
This paper shows that the compositional 
semantics in the verb structure provides an ideal 
way to expand the coverage of triggers. As a 
discourse-driven language, ellipsis is very common 
in Chinese, causing inference from the discourse-
level information is a fundamental requirement to 
understand the meaning of a clause, sentence or 
discourse. 
7 Conclusion 
In this paper we propose two novel inference 
mechanisms to Chinese trigger identification. In 
particular, compositional semantics inside Chinese 
triggers and discourse consistency between 
Chinese trigger mentions are used to resolve two 
critical issues in Chinese trigger identification: 
unknown triggers and word segmentation errors to 
known triggers. We give good reasons why this 
should be done, and present effective methods how 
this could be done. It shows that such novel 
inference mechanisms for Chinese event extraction 
are linguistically justified and pragmatically 
beneficial to real world applications.  
In future work, we will focus on how to 
introduce the discourse information into the 
individual classifiers to capture those long-distance 
features and joint learning of subtasks in Chinese 
event extraction. 
Acknowledgments 
The authors would like to thank three anonymous 
reviewers for their comments on this paper. This 
research was supported by the National Natural 
Science Foundation of China under Grant No. 
61070123 and No. 90920004, the National 863 
Project of China under Grant No. 2012AA011102. 
References 
David Ahn. 2006. The Stages of Event Extraction. In 
Proc. COLING/ACL 2006 Workshop on Annotating 
and Reasoning about Time and Events. Pages 1-8, 
Sydney, Australia. 
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and 
Christopher Manning. 2009. Discriminative 
Reordering with Chinese Grammatical Relations 
Features. In Proc. Third Workshop on Syntax and 
Structure in Statistical Translation, pages 51-59. 
Zheng Chen and Heng Ji. 2009a. Can One Language 
Bootstrap the Other: A Case Study on Event 
Extraction. In Proc. NAACL HLT Workshop on 
Semi-supervised Learning for Natural Language 
Processing, pages 66-74, Boulder, Colorado. 
Zheng Chen and Heng Ji. 2009b. Language Specific 
Issue and Feature Exploration in Chinese Event 
Extraction. In Proc. NAACL HLT 2009, pages 209-
212, Boulder, CO. 
Jenny Rose Finkel, Trond Grenager and Christopher 
Manning. 2005. Incorporating Non-local 
Information into Information Extraction Systems by 
Gibbs Sampling. In Proc. ACL 2005, pages 363-370, 
Ann Arbor, MI. 
Prashant Gupta and Heng Ji. 2009. Predicting Unknown 
Time Arguments based on Cross-Event Propagation. 
In Proc. ACL-IJCNLP 2009, pages 369-272, Suntec, 
Singapore. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System 
Description. In Proc. ACE 2005 Evaluation 
Workshop, Gaithersburg, MD. 
Hilda Hardy, Vika Kanchakouskaya and Tomek 
Strzalkowski. 2006. Automatic Event Classification 
Using Surface Text Features. In Proc. AAAI 2006 
Workshop on Event Extraction and Synthesis, pages 
36-41, Boston, MA. 
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, 
Guodong Zhou and Qiaoming Zhu. 2011. Using 
Cross-Entity Inference to Improve Event Extraction. 
In Proc. ACL 2011, pages 1127-1136, Portland, OR. 
Heng Ji. 2009. Cross-lingual Predicate Cluster 
Acquisition to Improve Bilingual Event Extraction 
by Inductive Learning. In Proc. NAACL HLT 
Workshop on Unsupervised and Minimally 
Supervised Learning of Lexical Semantics, pages 
27-35, Boulder, CO. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proc. ACL-08: HLT, pages 254-262, Columbus, OH. 
Young-Joo Kim. 2000. Subject/object drop in the 
acquisition of Korean: A Cross-linguistic 
Comparison. Journal of East Asian Linguistics, 9(4): 
325-351. 
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank? In 
Proc. ACL 2003, pages 439-446, Sapporo, Japan.  
Shasha Liao and Ralph Grishman. 2010. Using 
Document Level Cross-Event Inference to Improve 
Event Extraction. In Proc. ACL 2010, pages 789-
797, Uppsala, Sweden. 
Zhongguo Li. 2011. Parsing the Internal Structure of 
Words: A New Paradigm for Chinese Word 
Segmentation. In Proc. ACL 2011, pages 1405-1414, 
Portland, OR. 
Percy Liang, Michael I. Joedan and Dan Klein. 2011. 
Learning Dependency-Based Compositional 
1015
Semantics. In Proc. ACL 2011, pages 590-599, 
Portland, OR. 
Gideon Mann. 2007. Multi-document Relationship 
Fusion via Constraints on Probabilistic Databases. In 
Proc. HLT/NAACL 2007, pages 332-229, Rochester, 
NY. 
Mstislav Maslennikov and Tat-Seng Chua. 2007. A 
Multi Resolution Framework for Information 
Extraction from Free Text. In Proc. ACL 2007, 
pages 592-599, Prague, Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2007. Effective 
Information Extraction with Semantic Affinity 
Patterns and Relevant Regions. In Proc. 
EMNLP/CoNLL 2007, pages 717-727, Prague, 
Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Unified 
Model of Phrasal and Sentential Evidence for 
Information Extraction. In Proc. EMNLP 2009, 
pages 151-160, Singapore. 
Hongye Tan, Tiejun Zhao, Jiaheng Zheng. 2008. 
Identification of Chinese Event and Their Argument 
Roles. Proc. of the 2008 IEEE 8th International 
Conference on Computer and Information 
Technology Workshops, pages 14-19, Sydney, 
Australia. 
Yuk Wah Wong and Raymond J. Mooney. 2007. 
Learning Synchronous Grammars for Semantic 
Parsing with Lambda Calculus. In Proc. ACL 2007, 
pages 960-967, Prague, Czech Republic. 
Roman Yangarber, Clive Best, Peter von Etter, Flavio 
Fuart, David Horby and Ralf Steinberger. 2007. 
Combining Information about Epidemic Threats 
from Multiple Sources. In Proc. RANLP 2007 
workshop on Multi-source, Multilingual Information 
Extraction and Summarization. Borovets, pages 41-
48, Borovets, Bulgaria.  
Roman Yangarber and Lauri Jokipii. 2005. 
Redundancy-based Correction of Automatically 
Extracted Facts. In Proc. EMNLP 2005, pages 57-64, 
Vancouver, Canada.  
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proc. ACL 1995, pages 189-196, Cambridge, MA. 
Minglin Yuan. 1998. Studies on Valency in Modern 
Chinese. Chinese Commerce and Trade Press, 
Beijing, China. 
Luke S. Zettlemoyer and Michael Collins. 2007. Online 
Learning of Relaxed CCG Grammars for Parsing to 
Logical Form. In EMNLP/CoNLL 2007, pages 678-
687, Prague, Czech Republic. 
Dexi Zhu. 1980. Research on Chinese Modern 
Grammars. Chinese Commerce and Trade Press, 
Beijing, China.  
1016
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 968?976,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
  Tree Kernel-based Negation and Speculation Scope Detection with 
Structured Syntactic Parse Features 
 
 
Bowei Zou       Guodong Zhou       Qiaoming Zhu* 
Natural Language Processing Lab, School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China 
zoubowei@gmail.com, {gdzhou,qmzhu}@suda.edu.cn 
 
  
 
Abstract 
Scope detection is a key task in information ex-
traction. This paper proposes a new approach for 
tree kernel-based scope detection by using the 
structured syntactic parse information. In addi-
tion, we have explored the way of selecting 
compatible features for different part-of-speech 
cues. Experiments on the BioScope corpus show 
that both constituent and dependency structured 
syntactic parse features have the advantage in 
capturing the potential relationships between 
cues and their scopes. Compared with the state 
of the art scope detection systems, our system 
achieves substantial improvement.* 
1 Introduction 
The task of scope detection is to detect the linguis-
tic scope dominated by a specific cue. Current re-
searches in this field focus on two semantic as-
pects: negation and speculation. The negative 
scope detection is to detect the linguistic scope 
which is repudiated by a negative word (viz., nega-
tive cue, e.g., ?not?). In other side, the speculative 
scope detection is to detect the uncertain part in a 
sentence corresponding to the speculative word 
(viz., speculative cue, e.g., ?seems?). See the sen-
tence 1) below, the negative cue ?not? dominates 
the scope of ?not expensive?. Similarly, the specu-
lative cue ?possible? in sentence 2) dominates the 
uncertain scope ?the possible future scenarios?. 
1) The chair is [not expensive] but comfortable.  
2) Considering all that we have seen, what are now 
[the possible future scenarios]? 
                                                 
*	Corresponding	author	
The negative and speculative scope detection 
task consists of two basic stages. The first one is to 
identify the sentences involving negative or specu-
lative meaning. The second stage is to detect the 
linguistic scope of the cue in sentences (Velldal et 
al, 2012). In this paper, we focus on the second 
stage. That is, by given golden cues, we detect 
their linguistic scopes. 
We propose a tree kernel-based negation and 
speculation scope detection with structured syntac-
tic parse features. In detail, we regard the scope 
detection task as a binary classification issue, 
which is to classify the tokens in a sentence as be-
ing inside or outside the scope. In the basic 
framework, we focus on the analysis and applica-
tion of structured syntactic parse features as fol-
lows: 
Both constituent and dependency syntactic fea-
tures have been proved to be effective in scope 
detection (?zg?r et al 2009; ?vrelid et al 2010). 
However, these flat features are hardly to reflect 
the information implicit in syntactic parse tree 
structures. Our intuition is that the segments of the 
syntactic parse tree around a negative or specula-
tive cue is effective for scope detection. The relat-
ed structures normally underlay the indirect clues 
to identify the relations between cues and their 
scopes, e.g., in sentence 1), ?but something?, as a 
frequently co-occurred syntactic structure with 
?not something?, is an effective clue to determine 
the linguistic scope of ?not?. 
The tree kernel classifier (Moschitti, 2006) 
based on support vector machines uses a kernel 
function between two trees, affording a compari-
son between their substructures. Therefore, a tree 
kernel-based scope detection approach with struc-
tured syntactic parse tree is employed. The tree 
968
kernel has been already proved to be effective in 
semantic role labeling (Che et al 2006) and rela-
tion extraction (Zhou et al 2007). 
In addition, the empirical observation shows 
that features have imbalanced efficiency for scope 
classification, which is normally affected by the 
part-of-speech (abbr., POS) of cues. Hence, we 
build the discriminative classifiers for each kind of 
POS of cues, then explore and select the most 
compatible features for them. 
We construct a scope detection system by using 
the structured syntactic parse features based tree 
kernel classification. Compared with the state of 
the art scope detection systems, our system 
achieves the performance of accuracy 76.90% on 
negation and 84.21% on speculation (on Abstracts 
sub-corpus). Additionally, we test our system on 
different sub-corpus (Clinical Reports and Full 
Papers). The results show that our approach has 
better cross-domain performance. 
The rest of this paper is organized as follows: 
Section 2 reviews related work. Section 3 intro-
duces the corpus and corresponding usage in our 
experiments. Section 4 describes our approach and 
the experiments are presented in Section 5. Finally, 
there is a conclusion in Section 6. 
2 Related Work 
Most of the previous studies on negation and spec-
ulation scope detection task can be divided into 
two main aspects: the heuristic rule based methods 
and the machine learning based methods. We re-
spectively introduce the aspects in below. 
2.1 Heuristic Rule based Methods 
The initial studies for scope detection are to com-
pile effective heuristic rules (Chapman et al 2001; 
Goldin et al 2003). Recently, the heuristic rule 
based methods have further involved the syntactic 
features. 
Huang et al(2007) implemented a hybrid ap-
proach to automated negation scope detection. 
They combined the regular expression matching 
with grammatical parsing: negations are classified 
on the basis of syntactic categories and located in 
parse trees. Their hybrid approach is able to identi-
fy negated concepts in radiology reports even 
when they are located at some distance from the 
negative term. 
?zg?r et al(2009) hypothesized that the scope 
of a speculation cue can be characterized by its 
part-of-speech and the syntactic structure of the 
sentence and developed rules to map the scope of a 
cue to the nodes in the syntactic parse tree. By 
given golden speculation cues, their rule-based 
method achieves the accuracies of 79.89% and 
61.13% on the Abstracts and the Full-Papers sub-
corpus, respectively. 
?vrelid et al(2010) constructed a small set of 
heuristic rules which define the scope for each cue. 
In developing these rules, they made use of the 
information provided by the guidelines for scope 
annotation in the BioScope corpus, combined with 
manual inspection of the training data in order to 
further generalize over the phenomena discussed 
by Vincze et al(2008) and work out interactions of 
constructions for various types of cues. 
Apostolova et al(2011) presented a linguistical-
ly motivated rule-based system for the detection of 
negation and speculation scopes that performs on 
par with state-of-the-art machine learning systems. 
The rules are automatically extracted from the Bi-
oScope corpus and encode lexico-syntactic pat-
terns in a user-friendly format. While their system 
was developed and tested using a biomedical cor-
pus, the rule extraction mechanism is not domain-
specific. 
The heuristic rule based methods have bad ro-
bustness in detecting scopes crossing different 
meaning aspects (e.g., negative vs. speculative) 
and crossing different linguistic resources (e.g., 
Technical Papers vs. Clinical Reports). 
2.2 Machine Learning based Methods 
The machine learning based methods have been 
ignored until the release of the BioScope corpus 
(Szarvas et al 2008), where the large-scale data of 
manually annotated cues and corresponding scopes 
can support machine learning well. 
Morante et al(2008) formulated scope detection 
as a chunk classification problem. It is worth not-
ing that they also proposed an effective proper 
post-processing approach to ensure the consecu-
tiveness of scope. Then, for further improving the 
scope detection, Morante et al(2009a) applied a 
meta-learner that uses the predictions of the three 
classifiers (TiMBL/SVM/CRF) to predict the 
scope. 
For the competitive task in CoNLL?2010 (Far-
kas et al 2010), Morante et al(2010) used a 
969
memory-based classifier based on the k-nearest 
neighbor rule to determine if a token is the first 
token in a scope sequence, the last, or neither. 
Therefore, in order to guarantee that all scopes are 
continuous sequences of tokens they apply a first 
post-processing step that builds the sequence of 
scope. 
The existing machine learning based approaches 
substantially improve the robustness of scope de-
tection, and have nearly 80% accuracy. However, 
the approaches ignore the availability of the struc-
tured syntactic parse information. This information 
involves more clues which can well reflect the re-
lations between cues and scopes. S?nchez et al
(2010) employed a tree kernel based classifier with 
CCG structures to identify speculative sentences 
on Wikipedia dataset. However, in S?nchez?s ap-
proach, not all sentences are covered by the classi-
fier. 
3 Corpus 
We have employed the BioScope corpus (Szarvas 
et al 2008; Vincze et al 2008)1, an open resource 
from the biomedical domain, as the benchmark 
corpus. The corpus contains annotations at the to-
ken level for negative and speculative cues and at 
the sentence level for their linguistic scope (as 
shown in Figure 1). 
 (Note: <Sentence> denotes one sentence and the tag ?id? denotes its 
serial number; <xcope> denotes the scope of a cue; <cue> denotes the 
cue, the tag ?type? denotes the specific kind of cues and the tag ?ref? 
is the cue?s serial number.) 
Figure 1. An annotated sentence in BioScope. 
The BioScope corpus consists of three sub-
corpora: biological Full Papers from FlyBase and 
BMC Bioinformatics, biological paper Abstracts 
from the GENIA corpus (Collier et al 1999), and 
Clinical Reports. Among them, the Full Papers 
sub-corpus and the Abstracts sub-corpus come 
from the same genre. In comparison, the Clinical 
Reports sub-corpus consists of clinical radiology 
reports with short sentences. 
                                                 
1 http://www.inf.u-szeged.hu/rgai/bioscope 
In our experiments, if there is more than one cue 
in a sentence, we treat them as different cue and 
scope (two independent instances). The statistical 
data for our corpus is presented in Table 1 in be-
low. 
The average length of sentences in the negation 
portion is almost as long as that in speculation, 
while the average length of scope in negation is 
shorter than that in speculation. In addition, the 
length of sentence and scope in both Abstracts and 
Full Papers sub-corpora is comparative. But in 
Clinical Reports sub-corpus, it is shorter than that 
in Abstracts and Full Papers. Thus, looking for the 
effective features in short sentences is especially 
important for improving the robustness for scope 
detection. 
(Note: ?Av. Len? stands for average length.) 
Table 1. Statistics for our corpus in BioScope. 
4 Methodology 
We regard the scope detection task as a binary 
classification problem, which is to classify each 
token in sentence as being the element of the scope 
or not. Under this framework, we describe the flat 
syntactic features and employ them in our bench-
mark system. Then, we propose a tree kernel-
based scope detection approach using the struc-
tured syntactic parse features. Finally, we con-
struct the discriminative classifier for each kind of 
POS of cues, and select the most compatible fea-
tures for each classifier. 
4.1 Flat Syntactic Features 
In our benchmark classification system, the fea-
tures relevant to the cues or tokens are selected. 
Then, we have explored the constituent and de-
pendency syntactic features for scope detection. 
These features are all flat ones which reflect the 
characteristic of tokens, cues, scopes, and the rela-
tion between them. 
 Abstract Paper Clinical
Nega-
tion 
Sentences 1594 336 441 
Words 46849 10246 3613 
Scopes 1667 359 442 
Av. Len Sentence 29.39 30.49 8.19 
Av. Len Scope 9.62 9.36 5.28 
Specu-
lation
Sentences 2084 519 854 
Words 62449 16248 10241
Scopes 2693 682 1137 
Av. Len Sentence 29.97 31.31 11.99
Av. Len Scope 17.24 15.58 6.99 
<sentence id=?S26.8?> These findings <xcope id=?X26.8.2?> 
<cue type=?speculation? ref=?X26.8.2?> indicate that </cue> 
<xcope id=?X26.8.1?> corticosteroid resistance in bronchial 
asthma <cue type=?negation? ref=?X26.8.1?> can not </cue> 
be explained by abnormalities in corticosteroid receptor char-
acteristics </xcope></xcope> . </sentence> 
970
Basic Features: Table 2 shows the basic fea-
tures which directly relate to the characteristic of 
cues or tokens in our basic classification. 
Feature Remark 
B1 Cue. 
B2 Candidate token. 
B3 Part-of-speech of candidate token. 
B4 Left token of candidate token. 
B5 Right token of candidate token. 
B6 Positional relation between cue and token.
   Table 2. Basic features. 
Constituent Syntactic Features: For improv-
ing the basic classification, we employ 10 constit-
uent features belonging to two aspects. On the one 
hand, we regard the linguistic information of the 
neighbor locating around the candidate tokens as 
the coherent features (CS1~CS6 in Table 3). These 
features are used for detecting the close coopera-
tion of a candidate token co-occurring with its 
neighbors in a scope. On the other hand, we regard 
the linguistic characteristics of the candidate to-
kens themselves in a syntactic tree as the inherent 
features (CS7~CS10 in Table 3). These features 
are used for determining whether the token has the 
direct relationship with the cue or not. 
Features Remarks 
CS1 POS of left token. 
CS2 POS of right token. 
CS3 Syntactic category of left token. 
CS4 Syntactic category of right token. 
CS5 Syntactic path from left token to the cue. 
CS6 Syntactic path from right token to the cue. 
CS7 Syntactic category of the token. 
CS8 Syntactic path from the token to the cue. 
CS9 Whether the syntactic category of the token is 
the ancestor of the cue. 
CS10 Whether the syntactic category of the cue is the 
ancestor of the token. 
Table 3. Constituent syntactic features. 
Features Remarks 
DS1 Dependency direction (?head?or ?dependent?). 
DS2 Dependency syntactic path from the token to cue. 
DS3 The kind of dependency relation between the token 
and cue. 
DS4 Whether the token is the ancestor of the cue. 
DS5 Whether the cue is the ancestor of the token. 
Table 4. Dependency syntactic features. 
Dependency Syntactic Features: For the effec-
tiveness to obtain the syntactic information far 
apart from cues, we use 5 dependency syntactic 
features which emphasize the dominant relation-
ship between cues and tokens by dependency arcs 
as shown in Table 4. 
The features in Table 2, 3, and 4 have imbal-
anced classification for the scope classification. 
Therefore, we adopt the greedy feature selection 
algorithm as described in Jiang et al(2006) to pick 
up positive features incrementally according to 
their contributions. The algorithm repeatedly se-
lects one feature each time, which contributes most, 
and stops when adding any of the remaining fea-
tures fails to improve the performance. 
4.2 Structured Syntactic Features 
Syntactic trees involve not only the direct bridge 
(e.g., syntactic path) between cue and its scope but 
also the related structures to support the bridge 
(e.g., sub-tree). The related structures normally 
involve implicit clues which underlay the relation 
between cue and its scope. Therefore, we use the 
constituent and dependency syntactic structures as 
the supplementary features to further improve the 
benchmark system. 
Furthermore, we employ the tree kernel-based 
classifier to capture the structured information 
both in constituent and dependency parsing trees. 
The results of the constituent syntactic parser are 
typical trees which always consist of the syntactic 
category nodes and the terminal nodes. Thus, the 
constituent syntactic tree structures could be used 
in tree kernel-based classifier directly, but not for 
the dependency syntactic tree structures. As Figure 
2 shows, in sentence ?The chair is not expensive 
but comfortable.? the tree kernels cannot represent 
the relations on the arcs (e.g., ?CONJ? between 
?expensive? and ?comfortable?). It is hard to use 
the relations between tokens and cues in tree ker-
nels. 
 Figure 2: The dependency tree of sentence ?The 
chair is not expensive but comfortable.? 
971
 Figure 3. Two transformational rules. 
To solve the problem, we transform the depend-
ency tree into other two forms capable of being 
used directly as the compatible features in tree-
kernel based classification. The transformational 
rules are described as below: 
(1) Extracting the dependency relations to gen-
erate a tree of pure relations (named dependency 
relational frame), where the tokens on the nodes of 
original dependency tree are ignored and only the 
relation labels are used. E.g., the tokens ?chair?, 
?is?, etc in Figure 2 are all deleted and replaced by 
the corresponding relation labels. E.g., ?NSUBJ?, 
?COP?, etc are used as nodes in the dependency 
relational frame, see (1a) & (1b) in Figure 3. 
(2) Inserting the tokens which have been deleted 
in step (1) into the dependency relational frame 
and making them follow and link with their origi-
nal dependency relations. E.g., the tokens ?chair?, 
?is?, etc are added below the nodes ?NSUBJ?, 
?COP?, etc, see (2a) & (2b) in Figure 3. 
 
Figure 4. Two transformations for tree-kernel. 
Within the constituent and dependency syntactic 
trees, we have employed both the Completed Sub-
Tree and the Critical Path as the syntactic structure 
features for our classification. The former is a min-
imum sub-tree that involves the cues and the to-
kens, while the latter is the path from the cues to 
the tokens in the completed tree containing the 
primary structural information. Figure 4 shows 
them. 
4.3 Part-of-Speech Based Classification Op-
timization 
Motivating in part by the rule-based approach of 
?zg?r et al(2009), we infer that features have im-
balanced efficiency for scope classification, nor-
mally affected by the part-of-speech (POS) of cues.  
POS of Cues Number POS of Cues Number
CC 157 VB 31 
IN 115 VBD 131 
JJ 238 VBG 225 
MD 733 VBN 112 
NN 43 VBP 561 
RB 137 VBZ 207 
Table 5. Distribution of different POSs of specula-
tive cues in Abstracts sub-corpus. 
Table 5 shows the distribution for different 
POSs of cues in the Abstracts sub-corpus of Bio-
Scope for speculation detection task. The cues of 
different POS usually undertake different syntactic 
roles. Thus, there are different characteristics in 
triggering linguistic scopes. See the two examples 
below: 
3) TCF-1 contained a single DNA box in the [putative 
mammalian sex-determining gene SRY]. 
4) The circadian rhythm of plasma cortisol [either 
disappeared or was inverted]. 
The speculative cue ?putative? in sentence 3) is 
an adjective. The corresponding scope is its modi-
ficatory structure (?putative mammalian sex-
determining gene SRY?). In sentence 4), ?ei-
ther?or?? is a conjunction speculation cue. Its 
scope is the two connected components (?either 
disappeared or was inverted?). Thus, the effective 
features for the adjectival cue are normally the de-
pendency features, e.g., the features of DS1 and 
DS5 in Table 4, while the features for the conjunc-
tion cue are normally the constituent information, 
e.g., the features of CS9 in Table 3.  
In Table 5, considering the different function of 
verb voice, we cannot combine the ?VB(*)? POS. 
For instance, the POS of ?suggest? in sentence 5) 
is ?VBP? (the verb present tense). The correspond-
ing scope does not involve the sentence subject. 
972
The POS of ?suggested? in sentence 6) is ?VBN? 
(the past participle). The scope involves the sub-
ject ?An age-related decrease?. 
5) These results [suggest that the genes might be in-
volved in terminal granulocyte differentiation]. 
6) [An age-related decrease was suggested between 
subjects younger than 20 years]. 
As a result, we have built a discriminative clas-
sifier for each kind of POS of cues, and then ex-
plored and selected the most compatible features 
for each classifier. 
5 Experiments and Results 
5.1 Experimental Setting 
Considering the effectiveness of different features, 
we have split the Abstracts sub-corpus into 5 equal 
parts, within which 2 parts are used for feature 
selection (Feature Selection Data) and the rest for 
the scope detection experiments (Scope Detection 
Data). The Feature Selection Data are divided into 
5 equal parts, within which 4 parts for training and 
the rest for developing. In our scope detection ex-
periments, we divide the Scope Detection Data 
into 10 folds randomly, so as to perform 10-fold 
cross validation. As the experiment data is easily 
confusable, Figure 5 illustrates the allocation. 
Checking the validity of our method, we use the 
Abstracts sub-corpus in Section 5.2, 5.3 and 5.4, 
while in Section 5.5 we use all of the three sub-
corpora (Abstracts, Full Papers, and Clinical Re-
ports) to test the robustness of our system when 
applied to different text types within the same do-
main. 
 Figure 5. The allocation for experiment data. 
The evaluation is made using the precision, re-
call and their harmonic mean, F1-score. Addition-
ally, we report the accuracy in PCS (Percentage of 
Correct Scopes) applied in CoNLL?2010, within 
which a scope is fully correct if all tokens in a sen-
tence have been assigned to the correct scope class 
for a given cue. The evaluation in terms of preci-
sion and recall measures takes a token as a unit, 
whereas the evaluation in terms of PCS takes a 
scope as a unit. The key toolkits for scope classifi-
cation include: 
Constituent and Dependency Parser: All the 
sentences in BioScope corpus are tokenized and 
parsed using the Berkeley Parser (Petrov et al 
2007) 2  which have been trained on the GENIA 
TreeBank 1.0 (Tateisi et al 2005)3, a bracketed 
corpus in PTB style. 10-fold cross-validation on 
GTB1.0 shows that the parser achieves 87.12% in 
F1-score. On the other hand, we obtain the de-
pendency relations by the Stanford Dependencies 
Parser4. 
Support Vector Machine Classifier: SVMLight5 
is selected as our classifier, which provides a way 
to combine the tree kernels with the default and 
custom SVMLight kernels. We use the default pa-
rameter computed by SVMLight. 
Besides, according to the guideline of the Bio-
Scope corpus, scope must be a continuous chunk. 
The scope classifier may result in discontinuous 
blocks, as each token may be classified inside or 
outside the scope. Therefore, we perform the rule 
based post-processing algorithm proposed by Mo-
rante et al(2008) to obtain continuous scopes. 
5.2 Results on Flat Syntactic Features 
Relying on the results of the greedy feature selec-
tion algorithm (described in Section 4.1), we ob-
tain 9 effective features {B1, B3, B6, CS3, CS4, 
CS9, DS1, DS3, DS5} (see Table 2, 3 and 4) for 
negation scope detection and 13 effective features 
{B3, B4, B5, B6, CS1, CS5, CS6, CS8, CS9, CS10, 
DS1, DS4, DS5} for speculation. Table 6 lists the 
performances on the Scope Detection Data by per-
forming 10-fold cross validation. It shows that flat 
constituent and dependency syntactic features sig-
nificantly improve the basic scope detection by 
13.48% PCS for negation and 30.46% for specula-
tion (?2; p < 0.01). It demonstrates that the selected 
syntactic features are effective for scope detection. 
 
 
                                                 
2 http://code.google.com/p/berkeleyparser 
3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://svmlight.joachims.org 
973
Negation 
Features P R F PCS
Basic 89.89 68.72 77.86 39.50
Con.  85.72 67.80 75.66 41.81
Dep.  90.31 69.01 78.19 40.08
Bas.&Con.  88.86 79.07 83.61 51.64
Bas.&Dep. 90.44 73.62 81.17 49.36
All 91.21 76.57 83.25 52.98
Speculation 
Features P R F PCS
Basic 89.67 86.86 88.24 40.09
Con.  96.43 87.46 91.72 66.57
Dep.  90.84 87.04 88.89 44.45
Bas.&Con.  95.66 92.08 93.83 69.59
Bas.&Dep. 92.39 88.27 90.28 67.49
All 95.71 92.09 93.86 70.55
(Note: ?Bas.? denotes basic features; ?Con.? denotes Constituent 
features; ?Dep.? denotes Dependency features; ?All? contains Basic, 
Constituent, and Dependency features being selected.) 
Table 6. Performance of flat syntactic features. 
The results also show that the speculative scope 
detection achieves higher performance (16.98% 
higher in PCS) (?2; p < 0.01) than the negation 
scope detection. The main reason is that although 
the average sentence length of negation and specu-
lation are comparable (29.97 vs. 29.39 words, in 
Table 1), the average length of speculation scopes 
is much longer than the negation (17.24 vs. 9.62 
words, in Table 1) in Abstracts sub-corpus. With 
the shorter scopes in training data, the classifier 
inevitably have more negative samples. Thus, by 
using a token as the basic unit in our classification, 
the imbalanced samples will seriously mislead the 
classifier and result in bias on the negative samples. 
In addition, both constituent and dependency 
flat features can improve the scope classification, 
for the reason that the constituent features usually 
provide the nearer syntactic information of the 
cues, and that the further syntactic information 
between cues and scopes have been obtained by 
the dependency features. 
5.3 Results on Structured Syntactic Parse 
Features 
Table 7 and Table 8 give the scope detection per-
formance using the different structured syntactic 
parse features on negation and speculation respec-
tively. Compared to the optimal system (using all 
of the selected flat features in Table 6) in Section 
5.2, the structured syntactic parse features at best 
improve the scope classification nearly 17.29% on 
negation (PCS=70.27%) and 12.32% on specula-
tion (PCS=82.87%) (?2; p < 0.01). It indicates that 
the structured syntactic parse features can provide 
more implicit linguistic information, as supple-
mentary clues, to support scope classification. 
The improvements also show that both the com-
pleted syntactic sub-trees and critical paths in con-
stituent and dependency parsing trees are effective. 
The reason is that the completed syntactic sub-
trees contain the surrounding information related 
to cues and tokens, while there are more direct 
syntactic information in the critical paths between 
cue and its scope. 
Features P R F PCS 
Con. CT 91.12 83.25 86.89 54.57 
Con. CT&CP 93.31 89.32 91.20 66.58 
Dep. T1 CT 87.29 84.37 85.81 53.07 
Dep. T1 CT&CP 90.03 86.77 88.37 59.53 
Dep. T2 CT 88.17 84.58 86.34 53.76 
Dep. T2 CT&CP 91.09 87.31 89.16 60.11 
All 93.84 91.94 92.88 70.27 
(Note: ?Con.? denotes Constituent features; ?Dep.? denotes Depend-
ency features; ?T1? use the transformational rule (1) in Section 4.2 to 
get the dependency tree; ?T2? use the transformational rule (2) in 
Section 4.2 to get the dependency tree; CT-?Completed syntactic sub-
Tree?; CP-?Critical Path?; ?All? contains Con CT&CP, Dep T1 
CT&CP and Dep T2 CT&CP) 
Table 7. Performance of structured syntactic parse 
features on negation. 
Features P R F PCS 
Con. CT 95.89 93.37 94.61 75.17 
Con. CT&CP 96.05 94.36 95.20 76.73 
Dep. T1 CT 93.24 90.77 91.99 72.31 
Dep. T1 CT&CP 94.28 92.30 93.28 73.75 
Dep. T2 CT 93.76 89.68 91.67 73.06 
Dep. T2 CT&CP 95.29 94.55 94.92 75.69 
All 96.93 96.86 96.89 82.87 
Table 8. Performance of structured syntactic parse 
features on speculation. 
5.4 Results on Part-of-Speech Based Classifi-
cation 
To confirm the assumption in Section 4.3, we have 
built a discriminative classifier for each kind of 
POS of cues. Considering that the features involv-
ing the global structured syntactic parse infor-
mation in Section 4.2 are almost effective to all 
instances, we only use the flat syntactic features in 
Section 4.1. 
Negation
System P R F PCS
All Features 91.21 76.57 83.25 52.98
POS Classifier 91.79 78.29 84.50 56.77
Specula-
tion 
System P R F PCS
All Features  95.71 92.09 93.86 70.55
POS Classifier 95.79 93.13 94.44 71.68
(Note: ?All Features? System is the optimal system in Section 5.2) 
Table 9. Performances of POS based classification. 
Table 9 shows the performance of POS based 
classification. Compared with the system which 
only uses one classifier for all cues in Section 5.2, 
974
the POS based classification improves 1.13% on 
PCS (?2; p < 0.01), as different POS kinds of cues 
involve respectively effective features with more 
related clues between cue and its scope. 
Table 10 lists the performance of each POS kind 
of cues in speculation scope classification. There 
are still some low performances in some kinds of 
POS of cues. We consider it caused by two reasons. 
Firstly, some kinds of POS of cues  (e.g. NN etc.) 
have fewer samples (just 43 samples shown in Ta-
ble 5). For this reason, the training for classifier is 
limited. Then, for these low performance kinds of 
POS of cues, we may have not found the effective 
features for them. Although there are some kinds 
of cues with low performance, the whole perfor-
mance of part-of-speech based classification is 
improved. 
Cue?s 
POS 
B1~B6 
  1   2   3   4   5   6 
CS1~CS10 
1   2   3   4    5   6   7   8   9 10 
DS1~DS5 
1   2   3   4   5 PCS 
CC ?    ? ?    ? ?   ? ? ? ? ? 38.45
IN ?    ? ?    ? ? ?  ? ?  ? ? ? 87.99
JJ ?     ?    ?  ?  ?    ? 31.83
MD    ? ? ?  ?  ?  ? ? ? ? ?  ? ? 79.84
NN ?     ? ? ?   ?   ?  ?  ? 65.83
RB      ? ? ?   ? ?    ?    ? 37.03
VB      ? ?       ? ?   44.29
VBD    ? ?    ? ? ? ? ? ? ? ?  ? 63.57
VBG    ? ?    ? ? ?  ? ? ? ?  ? ? 82.89
VBN    ? ?    ?  ?  ? ? ? ?  ? 66.38
VBP    ? ? ? ?   ? ?  ? ? ? ? ?  ? 81.91
VBZ    ? ? ? ?   ? ?  ? ? ? ? ?  ? 77.16
Table 10. Performance of each POS kind of cues 
in speculation scope classification. 
5.5 Results of Comparison Experiments 
To get the final performance of our approach, we 
train the classifiers respectively by different effec-
tive features in Section 4.1 for POS kinds of cues, 
and use the structured syntactic parse features in 
Section 4.2 on Abstracts sub-corpus by performing 
10-fold cross validation. 
Negation 
System Abstract Paper Clinical
Morante (2008) 57.33 N/A N/A 
Morante (2009a) 73.36 50.26 87.27 
Ours 76.90 61.19 85.31 
Specula-
tion 
System Abstract Paper Clinical
Morante (2009b) 77.13 47.94 60.59 
?zg?r (2009) 79.89 61.13 N/A 
Ours 84.21 67.24 72.92 
Table 11. Performance comparison of our system 
with the state-of-the-art ones in PCS. 
The results in Table 11 show that our system 
outperforms the state of the art ones both on nega-
tion and speculation scope detection. Results also 
show that the system is portable to different types 
of documents, although performance varies de-
pending on the characteristics of the corpus. 
In addition, on both negation and speculation, 
the results on Clinical Reports sub-corpus are bet-
ter than those on Full Papers sub-corpus. It is 
mainly due to that the clinical reports are easier to 
process than full papers and abstracts. The average 
length of sentence for negative clinical reports is 
8.19 tokens, whereas for abstracts it is 29.39 and 
for full papers 30.49. Shorter sentences imply 
shorter scopes. The more unambiguous sentence 
structure of short sentence can make the structured 
constituent and dependency syntactic features eas-
ier to be processed. 
6 Conclusion 
This paper proposes a new approach for tree ker-
nel-based scope detection by using the structured 
syntactic parse information. In particular, we have 
explored the way of selecting compatible features 
for different part-of-speech cues. Experiments 
show substantial improvements of our scope clas-
sification and better robustness. 
However, the results on the Full Papers and the 
Clinical Reports sub-corpora are lower than those 
on the Abstracts sub-corpus for both negation and 
speculation. That is because the structured syntac-
tic parse features contain some complicated and 
lengthy components, and the flat features cross 
corpus are sparse. Our future work will focus on 
the pruning algorithm for the syntactic structures 
and analyzing errors in depth in order to get more 
effective features for the scope detection on differ-
ent corpora. 
Acknowledgments 
This research is supported by the National Natural 
Science Foundation of China, No.61272260, 
No.61373097, No.61003152, the Natural Science 
Foundation of Jiangsu Province, No.BK2011282, 
the Major Project of College Natural Science 
Foundation of Jiangsu Province, No.11KJA520003 
and the Graduates Project of Science and Innova-
tion, No.CXZZ12_0818. Besides, thanks to Yu 
Hong and the three anonymous reviewers for their 
valuable comments on an earlier draft. 
 
975
References  
Emilia Apostolova, Noriko Tomuro and Dina Demner-
Fushman. 2011. Automatic Extraction of Lexico-
Syntactic Patterns for Detection of Negation and 
Speculation Scopes. In Proceedings of ACL-HLT 
short papers, pages 283-287. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. A 
Simple Algorithm for Identifying Negated Findings 
and Diseases in Discharge Summaries. Journal of 
Biomedical Informatics, 34 (5): 301-310. 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A Hybrid Convolution Tree Kernel for Seman-
tic Role Labeling. In Proceedings of ACL, pages 73-
80. 
Nigel Collier, Hyun S. Park, Norihiro Ogata, et al 1999. 
The GENIA Project: Corpus-Based Knowledge Ac-
quisition and Information Extraction from Genome 
Research Papers. In Proceedings of EACL. 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos 
Csirik, and Gy?rgy Szarvas. 2010. The CoNLL-2010 
Shared Task: Learning to Detect Hedges and their 
Scope in Natural Language Text. In Proceedings of 
CoNLL: Shared Task, pages 1-12. 
Ilya M. Goldin and Wendy W. Chapman. 2003. Learn-
ing to Detect Negation with ?Not? in Medical Texts. 
In SIGIR Workshop: Text Analysis and Search for 
Bioinformatics. 
Yang Huang and Henry Lowe. 2007. A Novel Hybrid 
Approach to Automated Negation Detection in Clin-
ical Radiology Reports. Journal of the American 
Medical Informatics Association, 14(3):304-311. 
Zhengping Jiang and Hwee T. Ng. 2006. Semantic Role 
Labeling of NomBank: A Maximum Entropy Ap-
proach. In Proceedings of EMNLP, pages 138-145. 
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the Scope of Negation in Bio-
medical Texts. In Proceedings of EMNLP, pages 
715-724. 
Roser Morante and Walter Daelemans. 2009a. A Met-
alearning Approach to Processing the Scope of Ne-
gation. In Proceedings of CoNLL, pages 21-29. 
Roser Morante and Walter Daelemans. 2009b. Learning 
the Scope of Hedge Cues in Biomedical Texts. In 
Proceedings of the BioNLP Workshop, pages 28-36. 
Roser Morante, Vincent Van Asch and Walter Daele-
mans. 2010. Memory-Based Resolution of In-
Sentence Scopes of Hedge Cues. In Proceedings of 
CoNLL Shared Task, pages 40-47. 
Alessandro Moschitti. 2006. Making tree kernels practi-
cal for natural language learning. In Proceedings of 
the 11th Conference of the European Chapter of the 
Association for Computational Linguistics, pages 
113-120. 
Lilja ?vrelid, Erik Velldal, and Stephan Oepen. 2010. 
Syntactic Scope Resolution in Uncertainty Analysis. 
In Proceedings of COLING, pages 1379-1387. 
Arzucan ?zg?r and Dragomir R. Radev. 2009. Detect-
ing Speculations and their Scopes in Scientific Text. 
In Proceedings of EMNLP, pages 1398-1407. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. In Proceedings of NAACL, 
pages 404-411. 
Liliana M. S?nchez, Baoli Li, Carl Vogel. 2007. Ex-
ploiting CCG Structures with Tree Kernels for Spec-
ulation Detection. In Proceedings of the Fourteenth 
Conference on Computational Natural Language 
Learning: Shared Task, pages 126-131. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, and 
J?nos Csirik. 2008. The BioScope corpus: Annota-
tion for Negation, Uncertainty and their Scope in Bi-
omedical Texts. In Proceedings of BioNLP, pages 
38-45. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun?ichi Tsujii. 2005. Syntax Annotation for the 
GENIA Corpus. In Proceedings of IJCNLP, Com-
panion volume, pages 222-227. 
Erik Velldal, Lilja ?vrelid, Jonathon Read and Stephan 
Oepen. 2012. Speculation and Negation: Rules, 
Rankers, and the Role of Syntax. Computational 
Linguistics, 38(2):369-410. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra and J?nos Csirik. 2008. The BioScope 
corpus: biomedical texts annotated for uncertainty, 
negation and their scopes. BMC Bioinformatics, 
9(Suppl 11):S9. 
Guodong Zhou, Min Zhang, Donghong Ji, and Qi-
aoming Zhu. 2007. Tree Kernel-based Relation Ex-
traction with Context-Sensitive Structured Parse 
Tree Information. In Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages, 728-736. 
 
976
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1477?1487,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Argument Inference from Relevant Event Mentions in Chinese 
Argument Extraction 
 
 
Peifeng Li, Qiaoming Zhu, Guodong Zhou* 
School of Computer Science & Technology 
Soochow University, Suzhou, 215006, China 
{pfli, qmzhu, gdzhou}@suda.edu.cn 
 
 
 
Abstract 
As a paratactic language, sentence-level 
argument extraction in Chinese suffers 
much from the frequent occurrence of 
ellipsis with regard to inter-sentence 
arguments. To resolve such problem, this 
paper proposes a novel global argument 
inference model to explore specific 
relationships, such as Coreference, 
Sequence and Parallel, among relevant 
event mentions to recover those inter-
sentence arguments in the sentence, 
discourse and document layers which 
represent the cohesion of an event or a 
topic. Evaluation on the ACE 2005 
Chinese corpus justifies the effectiveness 
of our global argument inference model 
over a state-of-the-art baseline. 
1 Introduction 
The task of event extraction is to recognize event 
mentions of a predefined event type and their 
arguments (participants and attributes). 
Generally, it can be divided into two subtasks: 
trigger extraction, which aims to identify 
trigger/event mentions and determine their event 
type, and argument extraction, which aims to 
extract various arguments of a specific event and 
assign the roles to them. In this paper, we focus 
on argument extraction in Chinese event 
extraction. While most of previous studies in 
Chinese event extraction deal with Chinese 
trigger extraction (e.g., Chen and Ji, 2009a; Qin 
et al, 2010; Li et al, 2012a, 2012b), there are 
only a few on Chinese argument extraction (e.g., 
Tan et al, 2008; Chen and Ji, 2009b). Following 
previous studies, we divide argument extraction 
into two components, argument identification 
and role determination, where the former 
recognizes the arguments in a specific event 
mention and the latter classifies these arguments 
by roles.  
With regard to methodology, most of previous 
studies on argument extraction recast it as a 
Semantic Role Labeling (SRL) task and focus on 
intra-sentence information to identify the 
arguments and their roles. However, argument 
extraction is much different from SRL in the 
sense that, while the relationship between a 
predicate and its arguments in SRL can be 
mainly decided from the syntactic structure, the 
relationship between an event trigger and its 
arguments are more semantics-based, especially 
in Chinese, as a paratactic (e.g., discourse-driven 
and pro-drop) language with the wide spread of 
ellipsis and the open flexible sentence structure. 
Therefore, some arguments of a specific event 
mention are far away from the trigger and how to 
recover those inter-sentence arguments becomes 
a challenging issue in Chinese argument 
extraction. Consider the following discourse 
(from ACE 2005 Chinese corpus) as a sample: 
D1: ??????????????? 20 ?
????????????(E1)?????
(E2)?????????????(E3)???
???? (The Palestinian National Authority 
denied any involvement in the bomb attack (E2) 
occurred in the Gaza Strip on the morning of the 
20th, which killed (E1) two Israelites. ? They 
claimed that they will be investigating this 
attack (E3).) - From CBS20001120.1000.0823 
In above discourse, there are three event 
mentions, one kill (E1) and two Attack (E2, E3). 
While it is relatively easy to identify 20??? 
(morning of 20th), ???? (Gaza Strip) and ?
?  (bomb) as the Time, Place and Instrument 
roles in E2 by a sentence-based argument 
1477
extractor, it is really challenging to recognize 
these entities as the arguments of its corefered 
mention E3 since to reduce redundancy in a 
Chinese discourse, the later Chinese sentences 
omit many of these entities already mentioned in 
previous sentences. Similarly, it is hard to 
recognize ?????? (two Israelites) as the 
Target role for event mention E2 and identify?
?  (bomb) as the Instrument role for event 
mention E1. An alternative way is to employ 
various relationships among relevant event 
mentions in a discourse to infer those inter-
sentence arguments. 
The contributions of this paper are: 
1) We propose a novel global argument 
inference model, in which various kinds of 
event relations are involved to infer more 
arguments on their semantic relations. 
2) Different from Liao and Grishman (2010) 
and Hong et al (2011), which only consider 
document-level consistency, we propose a 
more fine-gained consistency model to 
enforce the consistency in the sentence, 
discourse and document layers. 
3) We incorporate argument semantics into our 
global argument inference model to unify the 
semantics of the event and its arguments. 
The rest of this paper is organized as follows. 
Section 2 overviews the related work. Section 3 
describes a state-of-the-art Chinese argument 
extraction system as the baseline. Section 4 
introduces our global model in inferring those 
inter-sentence arguments. Section 5 reports 
experimental results and gives deep analysis. 
Finally, we conclude our work in Section 6. 
2 Related Work 
Almost all the existing studies on argument 
extraction concern English. While some apply 
pattern-based approaches (e.g., Riloff, 1996; 
Califf and Mooney, 2003; Patwardhan and Riloff, 
2007; Chambers and Jurafsky, 2011), the others 
use machine learning-based approaches (e.g., 
Grishman et al, 2005; Ahn, 2006; Patwardhan 
and Riloff, 2009; Lu and Roth, 2012), most of 
which rely on various kinds of features in the 
context of a sentence. In comparison, there are 
only a few studies exploring inter-sentence 
information or argument semantics (e.g., Liao 
and Grishman, 2010; Hong et al, 2011; Huang 
and Riloff, 2011, 2012). 
Compared with the tremendous work on 
English event extraction, there are only a few 
studies (e.g., Tan et al, 2008; Chen and Ji, 2009b; 
Fu et al, 2010; Qin et al, 2010; Li et al, 2012) 
on Chinese event extraction with focus on either 
feature engineering or trigger expansion, under 
the same framework as English trigger 
identification. In additional, there are only very 
few of them focusing on Chinese argument 
extraction and almost all aim to feature 
engineering and are based on sentence-level 
information and recast this task as an SRL-style 
task. Tan et al (2008) introduce multiple levels 
of patterns to improve the coverage in Chinese 
argument classification. Chen and Ji (2009b) 
apply various kinds of lexical, syntactic and 
semantic features to address the special issues in 
Chinese argument extraction. Fu et al (2010) use 
a feature weighting scheme to re-weight various 
features for Chinese argument extraction. Li et al 
(2012b) introduce more refined features to the 
system of Chen and Ji (2009b) as their baseline. 
Specially, several studies have successfully 
incorporated cross-document or document-level 
information and argument semantics into event 
extraction, most of them focused on English.  
Yangarber et al (2007) apply a cross-
document inference mechanism to refine local 
extraction results for the disease name, location 
and start/end time. Mann (2007) proposes some 
constraints on relationship rescoring to impose 
the discourse consistency on the CEO?s personal 
information. Chambers and Jurafsky (2008) 
propose a narrative event chain which are 
partially ordered sets of event mentions centered 
around a common protagonist and this chain can 
represent the relationship among the relevant 
event mentions in a document. 
Ji and Grishman (2008) employ a rule-based 
approach to propagate consistent triggers and 
arguments across topic-related documents. Liao 
and Grishman (2010) mainly focus on employing 
the cross-event consistency information to 
improve sentence-level trigger extraction and 
they also propose an inference method to infer 
the arguments following role consistency in a 
document. Hong et al (2011) employ the 
background information to divide an entity type 
into more cohesive subtypes to create the bridge 
between two entities and then infer arguments 
and their roles using cross-entity inference on the 
subtypes of entities. Huang and Rillof (2012) 
propose a sequentially structured sentence 
classifier which uses lexical associations and 
discourse relations across sentences to identify 
event-related document contexts and then apply 
it to recognize arguments and their roles on the 
relation among triggers and arguments. 
1478
3 Baseline 
In the task of event extraction as defined in ACE 
evaluations, an event is defined as a specific 
occurrence involving participants (e.g., Person, 
Attacker, Agent, Defendant) and attributes (e.g., 
Place, Time). Commonly, an event mention is 
triggered via a word (trigger) in a phrase or 
sentence which clearly expresses the occurrence 
of a specific event. The arguments are the entity 
mentions involved in an event mention with a 
specific role, the relation of an argument to an 
event where it participates. Hence, extracting an 
event consists of four basic steps, identifying an 
event trigger, determining its event type, 
identifying involved arguments (participants and 
attributes) and determining their roles. 
As the baseline, we choose a state-of-the-art 
Chinese event extraction system, as described in 
Li et al (2012b), which consists of four typical 
components: trigger identification, event type 
determination, argument identification and role 
determination. In their system, the former two 
components, trigger identification and event type 
determination, are processed in a joint model, 
where the latter two components are run in a 
pipeline way. Besides, the Maximum-Entropy 
(ME) model is employed to train individual 
component classifiers for above four components. 
This paper focuses on argument identification 
and role determination. In order to provide a 
stronger baseline, we introduce more refined 
features in such two components, besides those 
adopted in Li et al (2012b). Following is a list of 
features adopted in our baseline. 
1) Basic features: trigger, POS (Part Of Speech) 
of the trigger, event type, head word of the 
entity, entity type, entity subtype; 
2) Neighbouring features: left neighbouring 
word of the entity + its POS, right neighbour 
word of the entity + its POS, left neighbour 
word of the trigger + its POS, right neighbour 
word of the trigger + its POS;  
3) Dependency features: dependency path from 
the entity to the trigger, depth of the 
dependency path; 
4) Syntactic features: path from the trigger to the 
entity, difference of the depths of the trigger 
and entity, place of the entity (before trigger 
or after trigger), depth of the path from the  
trigger to the entity, siblings of the entity; 
5) Semantic features: semantic role of the entity 
tagged by an SRL tool (e.g., ARG0, ARG1) 
(Li et al, 2010), sememe of trigger in Hownet 
(Dong and Dong, 2006). 
4 Inferring Inter-Sentence Arguments 
on Relevant Event Mentions 
In this paper, a global argument inference model 
is proposed to infer those inter-sentence 
arguments and their roles, incorporating with 
semantic relations between relevant event 
mention pairs and argument semantics. 
4.1 Motivation 
It?s well-known that Chinese is a paratactic 
language, with an open flexible sentence 
structure and often omits the subject or the object, 
while English is a hypotactic language with a 
strict sentence structure and emphasizes on 
cohesion between clauses. Hence, there are two 
issues in Chinese argument extraction, associated 
with its nature of the paratactic language. 
The first is that many arguments of an event 
mention are out of the event mention scope since 
ellipsis is a common phenomenon in Chinese. 
We call them inter-sentence arguments in this 
paper. Table 1 gives the statistics of intra-
sentence and inter-sentence arguments in the 
ACE 2005 Chinese corpus and it shows that 
20.8% of the arguments are inter-sentence ones 
while this figure is less than 1% of the ACE 2005 
English corpus. The main reason of that 
difference is that some Chinese arguments are 
omitted in the same sentence of the trigger since 
Chinese is a paratactic language with the wide 
spread of ellipsis. Besides, a Chinese sentence 
does not always end with a full stop. In particular, 
a comma is used frequently as the stop sign of a 
sentence in Chinese. We detect sentence 
boundaries, relying on both full stop and comma 
signs, since in a Chinese document, comma can 
be also used to sign the end of a sentence. In 
particular, we detect sentence boundaries on full 
stop, exclamatory mark and question mark firstly. 
Then, we identify the sentence boundaries on 
comma, using a binary classifier with a set of 
lexical and constituent-based syntactic features, 
similar to Xue and Yang (2010). 
 
Category Number 
#Arguments 8032 
#Inter-sentence 1673(20.8%) 
#Intra-sentence 6359(79.2%) 
Table 1. Statistics: Chinese argument extraction 
with regard to intra- sentence and inter-sentence 
arguments. 
 
The second issue is that the Chinese word 
order in a sentence is rather agile for the open 
1479
flexible sentence structure. Hence, different word 
orders can often express the same semantics. For 
example, a Die event mention ?Three person 
died in this accident.? can be expressed in many 
different orders in Chinese, such as ??????
?????, ??????????, ??????
?????, etc. 
In a word, above two issues indicate that 
syntactic feature-based approaches are limited in 
identifying Chinese arguments and it will lead to 
low recall in argument identification. Therefore, 
employing those high level information to 
capture the semantic relation, not only the 
syntactic structure, between the trigger and its 
long distance arguments is the key to improve 
the performance of the Chinese argument 
identification. Unfortunately, it is really hard to 
find their direct relations since they always 
appear in different clauses or sentences. An 
alternative way is to link the different event 
mentions with their predicates (triggers) and use 
the trigger as a bridge to connect the arguments 
to the trigger in another event mention indirectly. 
Hence, the semantic relations among event 
mentions are helpful to be a bridge to identify 
those inter-sentence arguments. 
4.2 Relations of Event Mention Pairs 
In a discourse, most event mentions are 
surrounding a specific topic. It?s obvious that 
those mentions have the intrinsic relationships to 
reveal the essential structure of a discourse. 
Those relevant semantics-based relations are 
helpful to infer the arguments for a specific 
trigger mention when the syntactic relations in 
Chinese argument extraction are not as effective 
as that in English. In this paper, we divide the 
relations among relevant event mentions into 
three categories: Coreference, Sequence and 
Parallel. 
An event may have more than one mention in 
a document and coreference event mentions refer 
to the same event, as same as the definition in the 
ACE evaluations. Those coreference event 
mentions always have the same arguments and 
roles. Therefore, employing this relation can 
infer the arguments of an event mention from 
their Coreference ones. For example, we can 
recover the Time, Place and Instrument for E3 
via its Coreference mention E2 in discourse D1, 
mentioned in Section 1. 
Li et al (2012a) find out that sometimes two 
trigger mentions are within a Chinese word 
whose morphological structure is Coordination. 
Take the following sentence as a sample: 
D2: ?? 17 ????????????(E4)
? (E5)????? (A 12-year-old younger 
hijacked a bus and then stabbed (E4) a woman 
to death (E5).) - From ZBN20001218.0400.0005 
In D2, ??  (stab a person to death) is a 
trigger with the Coordination structure and can 
be divided into two single-morpheme words ? 
(stab) and ? (die) while the former triggers an 
Attack event and the latter refers to a Die one. 
It?s interesting that they share all arguments in 
this sentence. The relation between those event 
mentions whose triggers merge a Chinese word 
or share the subject and the object are Parallel. 
For the errors in the syntactic parsing, the second 
single-morpheme trigger is often assigned a 
wrong tag (e.g., NN, JJ) and this leads to the 
errors in the argument extraction. Therefore, 
inferring the arguments of the second single-
morpheme trigger from that of the first one based 
on Parallel relation is also an available way to 
recover arguments. 
Like that the topic is an axis in a discourse, the 
relations among those relevant event mentions 
with the different types is the bone to link them 
into a narration. There are a few studies on using 
the event relations in NLP (e.g., summarization 
(Li et al, 2006), learning narrative event chains 
(Chambers and Jurafsky, 2007)) to ensure its 
effectiveness. In this paper, we define two types 
of Sequence relations of relevant event mentions: 
Cause and Temporal for their high probabilities 
of sharing arguments.  
The Cause relation between the event 
mentions are similar to that in the Penn 
Discourse TreeBank 2.0 (Prasad et al, 2008). 
For example, an Attack event often is the cause 
of an Die or Injure event. Our Temporal relation 
is limited to those mentions with the same or 
relevant event types (e.g., Transport and Arrest) 
for the high probabilities of sharing arguments. 
Take the following discourse as a sample: 
D3: ??????(E6)??????????
????(E7)?????????????
(These prisoners left (E6) Tindouf, a western 
city of Algeria, and went (E7) to Agadir, a 
southwestern city of Morocco.) - From 
Xin20001215.2000.0158 
In D3, there are two Transport mentions and it 
is natural to infer ????  (Agadir) as the 
Destination role of E6 and??? (Tindouf) as 
the Origin role of E7 via their Sequence relation. 
1480
4.3 Identifying Relations of Event Mention 
Pairs 
Currently, there are only few studies focusing on 
such area (e.g., Ahn, 2006; Chamber and 
Jurafsky, 2007; Huang and Rillof, 2012; Do et al, 
2012) and their approaches cannot be introduced 
to our system directly for the language nature 
and the different goal. We try to achieve a higher 
accuracy in this stage so that our argument 
inference can recover more true arguments.  
Inspired by Li and Zhou (2012), we also use 
the morphological structure to identify the 
Parallel relation. Two parallel event mentions 
with the adjacent trigger mentions w1 and w2 must 
satisfy follows two conditions: 
1) Morph(w1,w2) is Coordination 
2) jiTwHMTwHM ji ??? )(,)( 21   
where Morph(w1,w2) is a function to recognize 
the morphological structure of joint word w1w2, 
HM(wi) is to identify the head morpheme 1  in 
word wi and Ti is the set of the head morphemes 
with ith event type. These constraints are 
enlightened by the fact that only Chinese words 
with Coordination structure can be divided into 
two new words and each word can trigger an 
event with the different event type 2 . The 
implementation of Morph(w1,w2) and HM(w) are 
described in Li and Zhou (2012). 
The Coreference relation is divided into two 
types: Noun-based Coreference (NC) and Event-
based Coreference (EC) while the former always 
uses a verbal noun to refer to an event mentioned 
in current or previous sentence and the latter is 
that an event is mentioned twice or more actually. 
For example, the relation between E2 and E3 in 
D1 is NC while the trigger of E3 is only a verbal 
noun without any direct arguments and it refers 
to E2. 
We adopt a simple rule to recognize those NC 
relations: for each event mention whose trigger is 
a noun and doesn?t act as the subject/object, we 
regard their relation as NC if there is another 
event mention with the same trigger in current or 
previous sentence. 
Inspired by Ahn (2006), we use the following 
conditions to infer the EC relations between two 
event mentions with the same event type: 
1) Their trigger mentions refer to the same 
trigger; 
2) They have at least one same or similar 
                                                          
1 It acts as the governing semantic element in a Chinese 
word. 
2 If they have the same event type, they will be regarded as 
a single event mention. 
subject/object; 
3) The score of cosine similarity of two event 
mentions is more than a threshold3. 
Finally, for the Sequence relation, instead of 
identifying and classifying the relations clearly 
and correctly, our goal is to identify whether 
there are relevant event mentions in a long 
sentence or two adjacent short sentences who 
share arguments. Algorithm 1 illustrates a 
knowledge-based approach to identify the 
Sequence event relation in a discourse for any 
two trigger mentions tri1 and tri2 as follows: 
 
Algorithm 1 
1: input: tri1 and tri2 and their type et1 and et2 
2:  output: whether their relation is Sequence 
3:  begin 
4:      hm1 ?HM(tri1);  hm2 ?HM(tri2) 
5:  MP ?FindAllMP(hm1,et1,hm2,et2) 
6:     for any mpi in MP 
7:         if ShareArg(mpi) is true then 
8:             return true   // Sequence 
9:        end if 
10:    end for 
11:    return false 
12:  end 
 
In algorithm 1, HM(tri) is to identify the head 
morpheme in trigger tri and FindAllMP(hm1, et1, 
hm2, et2) is to find all event mention pairs in the 
training set which satisfy the condition that their 
head morphemes are hm1 and hm2, and their 
event types are et1 and et2 respectively. Besides, 
ShareArg(mpi)is used to identify whether the 
event mention pair mpi sharing at least one 
argument. In this algorithm, since the relations 
on the event types are too coarse, we introduce a 
more fine-gained Sequence relation both on the 
event types and the head morphemes of the 
triggers which can divide an event type into 
many subtypes on the head morpheme. Li and 
Zhou (2012) have ensured the effectiveness of 
using head morpheme to infer the triggers and 
our experiment results also show it is helpful for 
identifying relevant event mentions which aims 
to the higher accuracy. 
4.4 Global Argument Inference Model 
Our global argument inference model is 
composed of two steps: 1) training two sentence-
based classifiers: argument identifier (AI) and 
role determiner (RD) that estimate the score of a 
candidate acts as an argument and belongs to a 
                                                          
3 The threshold is tuned to 0.78 on the training set. 
1481
specific role following Section 3. 2) Using the 
scores of two classifiers and the event relations 
in a sentence, a discourse or a document, we 
perform global optimization to infer those 
missing or long distance arguments and their 
roles.  
To incorporate those event relations with our 
global argument inference model, we regard a 
document as a tree and divide it into three layers: 
document, discourse and sentence. A document 
is composed of a set of the discourses while a 
discourse contains three sentences. Since almost 
all arguments (~98%) of a specific event mention 
in the ACE 2005 Chinese corpus appear in the 
sentence containing the specific event mention 
and its two adjacent sentences (previous and next 
sentences), we only consider these three 
sentences as a discourse to simplify the process 
of identifying the scope of a discourse.  
We incorporate different event relations into 
our model on the different layer and the goal of 
our global argument inference model is to 
achieve the maximized scores over a document 
on its three layers and two classifiers: AI and RD. 
The score of document D is defined as 
))1))(,(1(),(
()1(
))1))((1()(
((maxarg
,,
, ,,, ,,
, ,,, ,,,
^
><><
? ?>< ><?>< ><? ?
? ?>< ><?>< ><?
??++
?+
??+
=
? ? ? ? ?
? ? ? ?
mZmZDmZmZD
DiI iIjiS jiSkjiT kjiTZA Rm
ZZIZZI
DiI iIjiS jiSkjiT kjiTZAYX
YREfYREf
XEfXEf
D
?
?
(1) 
}1,0{.. ?ZXts                                          (2) 
}1,0{, ?>< mZY                                  (3) 
RmYX mZZ ??? >< ,                       (4) 
?
?Rm
mZZ YX ><= ,                               (5) 
where Ii is the ith discourses in document D; 
S<i,j> is the jth sentences in discourse Ii; T<i,j,k> is 
the kth event mentions in sentence S<i,j>; A<i,j,k,l> 
is the lth candidate arguments in event mention 
T<i,j,k>; Z is used to denote <i,j,k,l>; fI(EZ) is the 
score of AI identifying entity mention EZ as an 
argument, where EZ is the lth entity of the kth 
event mention of the jth sentence of the ith 
discourse in document D. fD(EZ, Rm) is the score 
of RD assigning role Rm to argument EZ. Finally, 
XZ and Y<Z,m> are the indicators denoting whether 
entity EZ is an argument and whether the role Rm 
is assigned to entity EZ respectively. Besides, Eq. 
4 and Eq. 5 are the inferences to enforce that:  
1) if an entity belongs to a role, it must be an 
argument; 
2) if a entity is an argument of a specific event 
mention, it must have a role. 
Parallel relation: Sentence-based 
optimization is used to incorporate the Parallel 
relation of two event mentions into our model 
and they share all arguments in a sentence. Since 
different event type may have different role set, 
each role in a specific event should be mapped to 
the corresponding role in its Parallel event when 
they have the different event type. For example, 
the argument ??? 17 ????? (A 12-year-
old younger) in D2 acts as the Attacker role in 
the Attack event and the Agent role in the Die 
event. We learn those role-pairs from the training 
set and Table 2 shows part of the role relations 
learning from the training set. 
 
Event type pair Role pair 
Attack-Die Attacker-Agent; Target-
Victim;? 
Injure-Die Agent-Agent; Victim-
Victim;? 
Transport-
Demonstrate 
Artifact-Entity; 
Destination-Place;? 
Table 2. Part of role-pairs for those event 
mention pairs with Parallel relation. 
 
To infer the arguments and their roles on the 
Parallel relation, we enforce the consistency on 
the role-pair as follows: 
><><?
><><><><
><><><><
><><
=?>?<
????
???????
=
',',,,,,'
',,',',,,,,,,
,',',,,,
',',',,,,,,
',
,
lkjilkjihethet
kjilkjikjilkji
jikjikjiijii
mlkjimlkji
EERPmm
TATA
STTISDI
YY
(6) 
where 
'hh etetRP ?  is the set of role-pairs between 
two Parallel event mention eth and eth? and 
><>< = ',',,,,, lkjilkji EE  means they refer to the 
same entity mention. With the transitivity 
between the indicators X and Y, Eq. 6 also 
enforces the consistency on X<i,j,k,l> and X<i,j,k?,l?>. 
Coreference relation: Since the NC and EC 
relcation between two event mentions are 
different in the event expression, we introduce 
the discourse-based optimization for the former 
and document-based optimization for the latter. 
For two NC mentions, we ensure that the 
succeeding mentions can inherit the arguments 
form the previous one. To enforce this 
consistency, we just replace all fI(EZ) and fD(EZ, 
Rm) of the succeeding event mention with that of 
the previous one, since the previous one have the 
more context information. 
As for two EC event mentions, algorithm 2 
shows how to create the constraints for our 
1482
global argument inference model to infer 
arguments and roles. 
 
Algorithm 2 
1: input: two event mentions T, T? and their 
arguments set A and A? 
2:  output: the constraints set C 
3:  begin 
4:       for each argument a in A do 
5:            a??FindSim(a) 
6:    if a??? then 
7:                 ),( 'aa YYyConsistencCC ??  
8:             end if 
9:        end for 
10: end 
 
In algorithm 2, the function FindSim(a) is 
used to find a similar candidate argument a? in 
A? for a. If it?s found, we enforce the consistency 
of argument a and a? in the role by using 
Consistency(Ya,Ya?) where Ya  and Ya? are the 
indicators in Eq. 1. To evaluate the similarity 
between two candidates a and a?, we regard them 
as similar ones when they are the same word or 
in the same entity coreference chain. We use a 
coreference resolution tool to construct the entity 
coreference chains, as described in Kong et al
(2010). 
Sequence relation: For any two event 
mentions in a discourse, we use the event type 
pair with their head morphemes (e.g., Attack:?
(burst) - Die:?(die), Trial-Hearing:?(trial) - 
Sentence:?(sentence)) to search the training set 
and then obtain the probabilities of sharing the 
arguments as mentioned in algorithm 1. We 
denoted Pro<et,et?,HM(tri),HM(tri?),Rm,Rm?> as the 
probability of the trigger mentions tri and tri? 
(their event types are et and et? respectively.) 
sharing an argument whose roles are Rm and Rm? 
respectively. We propose following discourse-
based constraint to enforce the consistency 
between the roles of two arguments, which are 
related semantically, temporally, causally or 
conditionally, based on the probability of sharing 
an argument and the absolute value of the 
difference between the scores of RD: 
?
?
>
>
=?
????
=
><><
><><><><
><><><><
><><
),(),(
),),'(),(,',(Pr
',
,?
'',',',,,,
'
',',',,,,',',',
,,,',,
',',',',,,,,
mlkjiDmlkjiD
mm
lkjilkjijikji
jikjiijijii
mlkjimlkji
REfREf
RRtriHMtriHMeteto
EERmmST
STISSDI
YY
??
????
? (7) 
where ? and ? are the thresholds learned from the 
development set; tri and tri? are triggers of kth 
and k?th event mention whose event types are et 
and et? in S<i,j> and S<i,j?> respectively. 
4.5 Incorporating Argument Semantics into 
Global Argument Inference Model 
We also introduce the argument semantics, 
which represent the semantic relations of 
argument-argument pair, argument-role pair and 
argument-trigger pair, to reflect the cohesion 
inside an event. Hong et al (2011) found out that 
there is a strong argument and role consistency in 
the ACE 2005 English corpus. Those 
consistencies also occur in Chinese and they 
reveal the relation between the trigger and its 
arguments, and also explore the relation between 
the argument and its role. Besides, those entities 
act as non-argument also have the consistency 
with high probabilities.  
To let the global argument inference model 
combine those knowledges of argument 
semantics, we compute the prior probabilities 
P(X<i,j>=1) and P(Y<i,j,m>=1) that entity enj 
occurrs in a specific event type eti as an 
argument and its role is Rm respectively. To 
overcome the sparsity of the entities, we cluster 
those entities into more cohesive subtype 
following Hong et al (2011). Hence, following 
the independence assumptions described by 
Berant et al (2011), we modify the fI(EZ) and 
fD(EZ,Rm)in Eq. 1 as follows: 
)0()|1(1(
)1()|1(
log)( ==?
===
ZZZ
ZZZ
ZI XPFXP
XPFXP
Ef     (8) 
)0()|1(1(
)1()|1(
log),(
,,,
,,,
==?
===
><><><
><><><
mZmZmZ
mZmZmZ
mZD XPFXP
XPFYP
REf (9) 
where )|1( ZZ FXP =  and )|1( ,, ><>< = mZmZ FYP  
are the probabilities from the AI and AD 
respectively while FZ and F<Z,m> are the feature 
vectors. Besides, )1( , =>< mZXP  and )1( =ZXP  
are the prior probabilities learning from the 
training set. 
5 Experimentation 
In this section, we first describe the experimental 
settings and the baseline, and then evaluate our 
global argument inference model incorporating 
with relevant event mentions and argument 
semantics to infer arguments and their roles. 
5.1 Experimental Settings and Baseline 
For fair comparison, we adopt the same 
experimental settings as the state-of-the-art event 
extraction system (Li et al 2012b) and all the 
1483
evaluations are experimented on the ACE 2005 
Chinese corpus. We randomly select 567 
documents as the training set and the remaining 
66 documents as the test set. Besides, we reserve 
33 documents in the training set as the 
development set and use the ground truth entities, 
times and values for our training and testing. As 
for evaluation, we also follow the standards as 
defined in Li et al (2012b). Finally, all the 
sentences in the corpus are divided into words 
using a Chinese word segmentation tool 
(ICTCLAS) 1  with all entities annotated in the 
corpus kept. We use Berkeley Parser 2  and 
Stanford Parser 3  to create the constituent and 
dependency parse trees.  Besides, the ME tool 
(Maxent) 4  is employed to train individual 
component classifiers and lp_solver5 is used to 
construct our global argument inference model. 
Besides, all the experiments on argument 
extraction are done on the output of the trigger 
extraction system as described in Li et al 
(2012b). Table 3 shows the performance of the 
baseline trigger extraction system and Line 1 in 
Table 4 illustrates the results of argument 
identification and role determination based on 
this system. 
 
Trigger 
identification 
Event type 
determination 
P(%) R(%) F1 P(%) R(%) F1 
74.4 71.9 73.1 71.4 68.9 70.2
Table 3. Performance of the baseline on trigger 
identification and event type determination. 
5.2 Inferring Arguments on Relevant Event 
Mentions and Argument Semantics 
We develop a baseline system as mentioned in 
Section 3 and Line 2 in Table 4 shows that it 
slightly improves the F1-measure by 0.9% over 
Li et al (2012b) due to the incorporation of more 
refined features. This result indicates the 
limitation of syntactic-based feature engineering. 
Before evaluating our global argument 
inference model, we should identify the event 
relations between two mentions in a sentence, a 
discourse or a document. The experimental 
results show that the accuracies of identifying 
NC, EC, Parallel and Sequence relation are 
80.0%, 72.4%, 88.5% and 87.7% respectively. 
Those results ensure that our simple methods are 
                                                          
1http://ictclas.org/  
2 http://code.google.com/p/berkeleyparser/ 
3 http://nlp.stanford.edu/software/lex-parser.shtml 
4 http://mallet.cs.umass.edu/ 
5 http://lpsolve.sourceforge.net/5.5/ 
effective. Our statistics on the development set 
shows almost 65% of the event mentions are 
involved in those Correfrence, Parallel and 
Sequence relations, which occupy 63%, 50%, 9% 
respectively6. Most of the exceptions are isolated 
event mentions. 
 
System 
Argument 
identification 
Argument role 
determination
P(%) R(%) F1 P(%) R(%) F1
Li et al(2012b) 59.1 57.2 58.1 55.8 52.1 53.9
Baseline 60.5 57.6 59.0 55.7 53.0 54.4
BIM 59.3 60.1 59.7 54.4 55.2 54.8
BIM+RE 60.2 65.6 62.8 55.0 60.0 57.4
BIM+RE+AS 62.9 66.1 64.4 57.2 60.2 58.7
Table 4. Performance comparison of argument 
extraction on argument identification and role 
determination. 
Once the classifier AI and RD are trained, we 
would like to apply our global argument 
inference model to infer more inter-sentence 
arguments and roles. To achieve an optimal 
solution, we formulate the global inference 
problem as an Integer Linear Program (ILP), 
which leads to maximize the objective function. 
ILP is a mathematical method for constraint-
based inference to find the optimal values for a 
set of variables that maximize an objective 
function in satisfying a certain number of 
constraints. In the literature, ILP has been widely 
used in many NLP applications (e.g., Barzilay 
and Lapata, 2006; Do et al, 2012; Li et al, 
2012b).  
For our systems, we firstly evaluate the 
performance of our basic global argument 
inference model (BIM) with the Eq. 2?5 which 
enforce the consistency on AI and RD and then 
introduce the inference on the relevant event 
mentions (RE) and argument semantics (AS) to 
BIM. Table 4 shows their results and we can find 
out that: 
1) BIM only slightly improves the performance 
in F1-measure, as the result of more increase 
in recall (R) than decrease in precision (P). 
This suggests that those constraints just 
enforcing the consistency on AI and RD is not 
effective enough to infer more arguments. 
2) Compared to the BIM, our model BIM+RE 
enhances the performance of argument 
identification and role determination by 3.1% 
and 2.6% improvement in F1-measure 
respectively. This suggests the effectiveness 
                                                          
6 20% of the mentions belongs to both Coreference and 
Sequence relations. 
1484
of our global argument inference model on 
the relevant event mentions to infer inter-
sentence arguments. Table 5 shows the 
contributions of the different event relations 
while the Sequence relation gains the highest 
improvement of argument identification and 
role determination in F1-measure respectively. 
 
Constraint 
Argument 
identification 
Argument role 
determination 
P(%) R(%) F1 P(%) R(%) F1
BIM 59.3 60.1 59.7 54.4 55.2 54.8
+Parallel +0.6 +0.7 +0.6 +0.4 +0.6 +0.5
+NC +0.0 +0.8 +0.4 -0.2 +0.6 +0.2
+EC +0.6 +1.2 +0.9 +0.5 +1.0 +0.7
+ Sequence -0.3 +2.8 +1.2 -0.2 +2.6 +1.1
Table 5. Contributions of different event 
relations on argument identification and role 
determination. (Incremental) 
3) Our model BIM+ER+AS gains 1.6% 
improvement for argument identification, and 
1.3% for role determination. The results 
ensure that argument semantics not only can 
improve the performance of argument 
identification, but also is helpful to assign a 
correct role to an argument in role 
determination. 
Table 3 shows 25.6% of trigger mentions 
introduced into argument extraction are pseudo 
ones. If we use the golden trigger extraction, our 
exploration shows that the precision and recall of 
argument identification can be up to 78.6% and 
88.3% respectively. Table 6 shows the 
performance comparison of argument extraction 
on AI and RD given golden trigger extraction. 
Compared to the Baseline, our system improves 
the performance of argument identification and 
role determination by 6.4% and 5.8% 
improvement in F1-measure respectively, largely 
due to the dramatic increase in recall of 10.9% 
and 10.4%. 
 
 
System 
Argument 
identification 
Argument role 
determination 
P(%) R(%) F1 P(%) R(%) F1
Baseline 76.2 77.4 76.8 70.4 72.0 71.2
Model2 78.6 88.3 83.2 72.3 82.4 77.0
Table 6. Performance comparison of argument 
identification and type determination. (Golden 
trigger extraction) 
5.3 Discussion 
The initiation of our paper is that syntactic 
features play an important role in current 
machine learning-based approaches for English 
event extraction, however, their effectiveness is 
much reduced in Chinese. So the improvement of 
our model for English event extraction is much 
less than that of Chinese. However, our model 
can be an effective complement of the sentence-
level English argument extraction systems since 
the performance of argument extraction is still 
low in English and using discourse-level 
information is a way to improve its performance, 
especially for those event mentions whose 
arguments spread in complex sentences. 
Moreover, our exploration shows that our 
global argument inference model can mine those 
arguments within a long distance which are un-
annotated as arguments of a special event 
mention in the corpus since the annotators just 
tagged arguments in a narrow scope or omitted a 
few arguments. Actually, they are the true ones 
to our knowledge and  are more than 30.6% of 
those pseudo arguments inferred by our model. 
This ensures that our global argument inference 
model and those relations among event mentions 
is helpful to argument extraction. 
6 Conclusion 
In this paper we propose a global argument 
inference model to extract those inter-sentence 
arguments due to the nature of Chinese that it is a 
discourse-driven pro-drop language with the 
wide spread of ellipsis and the open flexible 
sentence structure. In particular, we incorporate 
various kinds of event relations and the argument 
semantics into the model in the sentence, 
discourse and document layers which represent 
the cohesion of an event or a topic. The 
experimental results ensure that our global 
argument inference model outperforms the state-
of-the-art system. 
In future work, we will focus on introducing 
more semantic information and cross-document 
information into the global argument inference 
model to improve the performance of argument 
extraction. 
Acknowledgments 
The authors would like to thank three 
anonymous reviewers for their comments on this 
paper. This research was supported by the 
National Natural Science Foundation of China 
under Grant No. 61070123, No. 61272260 and 
No. 61273320, the National 863 Project of China 
under Grant No. 2012AA011102. The co-author 
tagged with ?*? is the corresponding author. 
1485
References  
David Ahn. 2006. The Stages of Event Extraction. In 
Proc. COLING/ACL 2006 Workshop on 
Annotating and Reasoning about Time and Events. 
Pages 1-8, Sydney, Australia. 
Regina Barzilay and Miralla Lapata. 2006. 
Aggregation via Set Partitioning for Natural 
Language Generation. In Proc. NAACL 2006, 
pages 359-366, New York City, NY. 
Jonathan Berant, Ido Dagan and Jacob Goldberger. 
2011. Global Learning of Typed Entailment Rules. 
In Proc. ACL 2011, pages 610-619, Portland, OR. 
Mary Elaine Califf and Raymond J. Mooney. 2003. 
Bottom-up Relational Learning of Pattern 
Matching rules for Information Extraction. Journal 
of Machine Learning Research, 4:177?210. 
Nathanael Chambers and Dan Jurafsky. 2008. 
Unsupervised Learning of Narrative Event Chains. 
In Proc. ACL 2008, pages 789-797, Columbus, OH. 
Nathanael Chambers and Dan Jurafsky. 2011. 
Template-based Information Extraction without the 
Templates. In Proc. ACL 2011, pages 976-986, 
Portland, OR. 
Zheng Chen and Heng Ji. 2009a. Can One Language 
Bootstrap the Other: A Case Study on Event 
Extraction. In Proc. NAACL/HLT 2009 Workshop 
on Semi-supervised Learning for Natural Language 
Processing, pages 66-74, Boulder, Colorado. 
Zheng Chen and Heng Ji. 2009b. Language Specific 
Issue and Feature Exploration in Chinese Event 
Extraction. In Proc. NAACL HLT 2009, pages 
209-212, Boulder, Colorado. 
Zhengdong Dong and Qiang Dong. 2006. HowNet 
and the Computation of Meaning. World Scientific 
Pub Co. Inc. 
Quang Xuan Do, Wei Lu and Dan Roth. 2012. Joint 
Inference for Event Timeline Construction. In Proc.  
EMNLP 2012, pages 677-687, Jeju, Korea. 
Jianfeng Fu, Zongtian Liu, Zhaoman Zhong and 
Jianfang Shan. 2010. Chinese Event Extraction 
Based on Feature Weighting. Information 
Technology Journal, 9: 184-187.  
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System 
Description. In Proc. ACE 2005 Evaluation 
Workshop, Gaithersburg, MD. 
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, 
Guodong Zhou and Qiaoming Zhu. 2011. Using 
Cross-Entity Inference to Improve Event Extraction. 
In Proc. ACL 2011, pages 1127-1136, Portland, 
OR. 
Ruihong Huang and Ellen Riloff. 2011. Peeling Back 
the Layers: Detecting Event Role Fillers in 
Secondary Contexts, In Proc. ACL 2011, pages 
1137-1147, Portland, OR. 
Ruihong Huang and Ellen Riloff. 2012. Modeling 
Textual Cohesion for Event Extraction. In Proc. 
AAAI 2012, pages 1664-1770, Toronto, Canada. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proc. ACL 2008, pages 254-262, Columbus, OH. 
Fang Kong, Guodong Zhou, Longhua Qian and 
Qiaoming Zhu. 2010. Dependency-driven 
Anaphoricity Determination for Coreference 
Resolution. In Proc. COLING 2010, pages 599-607, 
Beijing, China. 
Junhui Li, Guodong Zhou and Hwee Tou Ng. 2010. 
Joint Syntactic and Semantic Parsing of Chinese. 
In Proc. ACL 2010, pages 1108-1117, Uppsala, 
Sweden. 
Peifeng Li, Guodong Zhou, Qiaoming Zhu and Libin 
Hou. 2012a. Employing Compositional Semantics 
and Discourse Consistency in Chinese Event 
Extraction. In Proc. EMNLP 2012, pages 1006-
1016, Jeju, Korea. 
Peifeng Li, Qiaoming Zhu, Hongjun Diao and 
Guodong Zhou. 2012b. Joint Modeling of Trigger 
Identification and Event Type Determination in 
Chinese Event Extraction. In Proc. COLING 2012, 
pages 1635-1652, Mumbai, India. 
Peifeng Li and Guodong Zhou. 2012. Employing 
Morphological Structures and Sememes for 
Chinese Event Extraction. In Proc. COLING 2012, 
pages 1619-1634, Mumbai, India. 
Wenjie Li, Mingliu Wu, Qin Lu, Wei Xu and Chunfa 
Yuan. 2006. Extractive Summarization using Inter- 
and Intra- Event Relevance. In Proc. 
COLING/ACL 2006, pages 369-376, Sydney, 
Australia.  
Shasha Liao and Ralph Grishman. 2010. Using 
Document Level Cross-Event Inference to Improve 
Event Extraction. In Proc. ACL 2010, pages 789-
797, Uppsala, Sweden. 
Wei Lu and Dan Roth. 2012. Automatic Event 
Extraction with Structured Preference Modeling. 
In Proc. ACL 2012, pages 835-844, Jeju, Korea. 
Gideon Mann. 2007. Multi-document Relationship 
Fusion via Constraints on Probabilistic Databases. 
In Proc. HLT/NAACL 2007, pages 332-229,  
Rochester, NY. 
Siddharth Patwardhan and Ellen Riloff. 2007. 
Effective Information Extraction with Semantic 
Affinity Patterns and Relevant Regions. In Proc. 
EMNLP/CoNLL 2007, pages 717-727, Prague, 
Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2009. A 
Unified Model of Phrasal and Sentential Evidence 
1486
for Information Extraction. In Proc. EMNLP 2009, 
pages 151-160, Singapore. 
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni 
Miltsakaki, Livio Robaldo, Aravind Joshi and 
Bonnie Webber. 2008. The Penn Discourse 
Treebank 2.0. In Proc. LREC 2008, pages 2961-
2968, Marrakech, Morocco. 
Bing Qin, Yanyan Zhao, Xiao Ding, Ting Liu and 
Guofu Zhai. 2010. Event Type Recognition Based 
on Trigger Expansion. Tsinghua Science and 
Technology, 15(3): 251-258, Beijing, China. 
Ellen Riloff. 1996. Automatically Generating 
Extraction Patterns from Untagged Text. In Proc. 
AAAI 1996, pages 1044?1049, Portland, OR. 
Hongye Tan, Tiejun Zhao, Jiaheng Zheng. 2008. 
Identification of Chinese Event and Their 
Argument Roles. In Proc. 2008 IEEE International 
Conference on Computer and Information 
Technology Workshops, pages 14-19, Sydney, 
Australia. 
Nianwen Xue and Yaqin Yang. 2010. Chinese 
Sentence Segmentation as Comma Classification. 
In Proc. ACL 2010, pages 631-635, Uppsala, 
Sweden. 
Roman Yangarber, Clive Best, Peter von Etter, Flavio 
Fuart, David Horby and Ralf Steinberger. 2007. 
Combining Information about Epidemic Threats 
from Multiple Sources. In Proc. RANLP 2007 
Workshop on Multi-source, Multilingual 
Information Extraction and Summarization, pages 
41-48, Borovets, Bulgaria. 
1487
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 522?530,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Negation Focus Identification with Contextual Discourse Information 
 
 
Bowei Zou        Qiaoming Zhu       Guodong Zhou* 
Natural Language Processing Lab, School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China 
zoubowei@gmail.com, {qmzhu, gdzhou}@suda.edu.cn 
 
  
 
Abstract 
Negative expressions are common in natural 
language text and play a critical role in in-
formation extraction. However, the perfor-
mances of current systems are far from satis-
faction, largely due to its focus on intra-
sentence information and its failure to con-
sider inter-sentence information. In this paper, 
we propose a graph model to enrich intra-
sentence features with inter-sentence features 
from both lexical and topic perspectives. 
Evaluation on the *SEM 2012 shared task 
corpus indicates the usefulness of contextual 
discourse information in negation focus iden-
tification and justifies the effectiveness of our 
graph model in capturing such global infor-
mation. * 
1 Introduction 
Negation is a grammatical category which com-
prises various kinds of devices to reverse the 
truth value of a proposition (Morante and 
Sporleder, 2012). For example, sentence (1) 
could be interpreted as it is not the case that he 
stopped. 
(1) He didn't stop. 
Negation expressions are common in natural 
language text. According to the statistics on bio-
medical literature genre (Vincze et al, 2008), 
19.44% of sentences contain negative expres-
sions. The percentage rises to 22.5% on Conan 
Doyle stories (Morante and Daelemans, 2012). It 
is interesting that a negative sentence may have 
both negative and positive meanings. For exam-
ple, sentence (2) could be interpreted as He 
stopped, but not until he got to Jackson Hole 
with positive part he stopped and negative part 
until he got to Jackson Hole. Moreover, a nega-
                                                 
* Corresponding author 
tive expression normally interacts with some 
special part in the sentence, referred as negation 
focus in linguistics. Formally, negation focus is 
defined as the special part in the sentence, which 
is most prominently or explicitly negated by a 
negative expression. Hereafter, we denote nega-
tive expression in boldface and negation focus 
underlined. 
(2) He didn't stop until he got to Jackson Hole. 
While people tend to employ stress or intona-
tion in speech to emphasize negation focus and 
thus it is easy to identify negation focus in 
speech corpora, such stress or intonation infor-
mation often misses in the dominating text cor-
pora. This poses serious challenges on negation 
focus identification. Current studies (e.g., Blanco 
and Moldovan, 2011; Rosenberg and Bergler, 
2012) sort to various kinds of intra-sentence in-
formation, such as lexical features, syntactic fea-
tures, semantic role features and so on, ignoring 
less-obvious inter-sentence information. This 
largely defers the performance of negation focus 
identification and its wide applications, since 
such contextual discourse information plays a 
critical role on negation focus identification. 
Take following sentence as an example. 
(3) Helen didn?t allow her youngest son to 
play the violin. 
In sentence (3), there are several scenarios on 
identification of negation focus, with regard to 
negation expression n?t, given different contexts: 
Scenario A: Given sentence But her husband did 
as next sentence, the negation focus should be 
Helen, yielding interpretation the person who 
didn?t allow the youngest son to play the violin is 
Helen but not her husband. 
Scenario B: Given sentence She thought that he 
didn?t have the artistic talent like her eldest son 
as next sentence, the negation focus should be 
the youngest son, yielding interpretation Helen 
522
thought that her eldest son had the talent to play 
the violin, but the youngest son didn?t. 
Scenario C: Given sentence Because of her 
neighbors? protests as previous sentence, the ne-
gation focus should be play the violin, yielding 
interpretation Helen didn?t allow her youngest 
son to play the violin, but it didn?t show whether 
he was allowed to do other things. 
In this paper, to well accommodate such con-
textual discourse information in negation focus 
identification, we propose a graph model to en-
rich normal intra-sentence features with various 
kinds of inter-sentence features from both lexical 
and topic perspectives. Besides, the standard 
PageRank algorithm is employed to optimize the 
graph model. Evaluation on the *SEM 2012 
shared task corpus (Morante and Blanco, 2012) 
justifies our approach over several strong base-
lines. 
The rest of this paper is organized as follows. 
Section 2 overviews the related work. Section 3 
presents several strong baselines on negation fo-
cus identification with only intra-sentence fea-
tures. Section 4 introduces our topic-driven 
word-based graph model with contextual dis-
course information. Section 5 reports the exper-
imental results and analysis. Finally, we con-
clude our work in Section 6. 
2 Related Work 
Earlier studies of negation were almost in lin-
guistics (e.g. Horn, 1989; van der Wouden, 
1997), and there were only a few in natural lan-
guage processing with focus on negation recog-
nition in the biomedical domain. For example, 
Chapman et al (2001) developed a rule-based 
negation recognition system, NegEx, to deter-
mine whether a finding mentioned within narra-
tive medical reports is present or absent. Since 
the release of the BioScope corpus (Vincze et al, 
2008), a freely available resource consisting of 
medical and biological texts, machine learning 
approaches begin to dominate the research on 
negation recognition (e.g. Morante et al, 2008; 
Li et al, 2010). 
Generally, negation recognition includes three 
subtasks: cue detection, which detects and identi-
fies possible negative expressions in a sentence, 
scope resolution, which determines the grammat-
ical scope in a sentence affected by a negative 
expression, and focus identification, which iden-
tifies the constituent in a sentence most promi-
nently or explicitly negated by a negative expres-
sion. This paper concentrates on the third subtask, 
negation focus identification. 
Due to the increasing demand on deep under-
standing of natural language text, negation 
recognition has been drawing more and more 
attention in recent years, with a series of shared 
tasks and workshops, however, with focus on cue 
detection and scope resolution, such as the Bi-
oNLP 2009 shared task for negative event detec-
tion (Kim et al, 2009) and the ACL 2010 Work-
shop for scope resolution of negation and specu-
lation (Morante and Sporleder, 2010), followed 
by a special issue of Computational Linguistics 
(Morante and Sporleder, 2012) for modality and 
negation. 
The research on negation focus identification 
was pioneered by Blanco and Moldovan (2011), 
who investigated the negation phenomenon in 
semantic relations and proposed a supervised 
learning approach to identify the focus of a nega-
tion expression. However, although Morante and 
Blanco (2012) proposed negation focus identifi-
cation as one of the *SEM?2012 shared tasks, 
only one team (Rosenberg and Bergler, 2012) 1 
participated in this task. They identified negation 
focus using three kinds of heuristics and 
achieved 58.40 in F1-measure. This indicates 
great expectation in negation focus identification. 
The key problem in current research on nega-
tion focus identification is its focus on intra-
sentence information and large ignorance of in-
ter-sentence information, which plays a critical 
role in the success of negation focus identifica-
tion. For example, Ding (2011) made a qualita-
tive analysis on implied negations in conversa-
tion and attempted to determine whether a sen-
tence was negated by context information, from 
the linguistic perspective. Moreover, a negation 
focus is always associated with authors? intention 
in article. This indicates the great challenges in 
negation focus identification. 
3 Baselines 
Negation focus identification in *SEM?2012 
shared tasks is restricted to verbal negations an-
notated with MNEG in PropBank, with only the 
constituent belonging to a semantic role selected 
as negation focus. Normally, a verbal negation 
expression (not or n?t) is grammatically associat-
ed with its corresponding verb (e.g., He didn?t 
stop). For details on annotation guidelines and 
                                                 
1 In *SEM?2013, the shared task is changed with focus on 
"Semantic Textual Similarity". 
523
examples for verbal negations, please refer to 
Blanco and Moldovan (2011). 
For comparison, we choose the state-of-the-art 
system described in Blanco and Moldovan 
(2011), which employed various kinds of syntac-
tic features and semantic role features, as one of 
our baselines. Since this system adopted C4.5 for 
training, we name it as BaselineC4.5. In order to 
provide a stronger baseline, besides those fea-
tures adopted in BaselineC4.5, we added more re-
fined intra-sentence features and adopted ranking 
Support Vector Machine (SVM) model for train-
ing. We name it as BaselineSVM. 
Following is a list of features adopted in the 
two baselines, for both BaselineC4.5 and Base-
lineSVM, 
? Basic features: first token and its part-of-
speech (POS) tag of the focus candidate; the 
number of tokens in the focus candidate; 
relative position of the focus candidate 
among all the roles present in the sentence; 
negated verb and its POS tag of the negative 
expression;  
? Syntactic features: the sequence of words 
from the beginning of the governing VP to 
the negated verb; the sequence of POS tags 
from the beginning of the governing VP to 
the negated verb; whether the governing VP 
contains a CC; whether the governing VP 
contains a RB. 
? Semantic features: the syntactic label of se-
mantic role A1; whether A1 contains POS 
tag DT, JJ, PRP, CD, RB, VB, and WP, as 
defined in Blanco and Moldovan (2011); 
whether A1 contains token any, anybody, an-
ymore, anyone, anything, anytime, anywhere, 
certain, enough, full, many, much, other, 
some, specifics, too, and until, as defined in 
Blanco and Moldovan (2011); the syntactic 
label of the first semantic role in the sentence; 
the semantic label of the last semantic role in 
the sentence; the thematic role for 
A0/A1/A2/A3/A4 of the negated predicate. 
and for BaselineSVM only, 
? Basic features: the named entity and its type 
in the focus candidate; relative position of the 
focus candidate to the negative expression 
(before or after). 
? Syntactic features: the dependency path and 
its depth from the focus candidate to the neg-
ative expression; the constituent path and its 
depth from the focus candidate to the nega-
tive expression; 
4 Exploring Contextual Discourse In-
formation for Negation Focus Identi-
fication 
While some of negation focuses could be identi-
fied by only intra-sentence information, others 
must be identified by contextual discourse in-
formation. Section 1 illustrates the necessity of 
such contextual discourse information in nega-
tion focus identification by giving three scenarios 
of different discourse contexts for negation ex-
pression n?t in sentence (3). 
For better illustration of the importance of 
contextual discourse information, Table 1 shows 
the statistics of intra- and inter-sentence infor-
mation necessary for manual negation focus 
identification with 100 instances randomly ex-
tracted from the held-out dataset of *SEM'2012 
shared task corpus. It shows that only 17 instanc-
es can be identified by intra-sentence information. 
It is surprising that inter-sentence information is 
indispensable in 77 instances, among which 42 
instances need only inter-sentence information 
and 35 instances need both intra- and inter-
sentence information. This indicates the great 
importance of contextual discourse information 
on negation focus identification. It is also inter-
esting to note 6 instances are hard to determine 
even given both intra- and inter-sentence infor-
mation. 
Info Number
#Intra-Sentence Only 17 
#Inter-Sentence Only 42 
#Both 35 
#Hard to Identify 6 
(Note: "Hard to Identify" means that it is hard for a 
human being to identify the negation focus even 
given both intra- and inter-sentence information.) 
Table 1. Statistics of intra- and inter-sentence 
information on negation focus identification. 
Statistically, we find that negation focus is al-
ways related with what authors repeatedly states 
in discourse context. This explains why contex-
tual discourse information could help identify 
negation focus. While inter-sentence information 
provides the global characteristics from the dis-
course context perspective and intra-sentence 
information provides the local features from lex-
ical, syntactic and semantic perspectives, both 
have their own contributions on negation focus 
identification. 
In this paper, we first propose a graph model 
to gauge the importance of contextual discourse 
524
information. Then, we incorporate both intra- 
and inter-sentence features into a machine learn-
ing-based framework for negation focus identifi-
cation. 
4.1 Graph Model 
Graph models have been proven successful in 
many NLP applications, especially in represent-
ing the link relationships between words or sen-
tences (Wan and Yang, 2008; Li et al, 2009). 
Generally, such models could construct a graph 
to compute the relevance between document 
theme and words. 
In this paper, we propose a graph model to 
represent the contextual discourse information 
from both lexical and topic perspectives. In par-
ticular, a word-based graph model is proposed to 
represent the explicit relatedness among words in 
a discourse from the lexical perspective, while a 
topic-driven word-based model is proposed to 
enrich the implicit relatedness between words, by 
adding one more layer to the word-based graph 
model in representing the global topic distribu-
tion of the whole dataset. Besides, the PageRank 
algorithm (Page et al, 1998) is adopted to opti-
mize the graph model. 
Word-based Graph Model: 
A word-based graph model can be defined as 
Gword (W, E), where W={wi} is the set of words in 
one document and E={eij|wi, wj ?W} is the set of 
directed edges between these words, as shown in 
Figure 1. 
 Figure 1. Word-based graph model. 
In the word-based graph model, word node wi 
is weighted to represent the correlation of the 
word with authors? intention. Since such correla-
tion is more from the semantic perspective than 
the grammatical perspective, only content words 
are considered in our graph model, ignoring 
functional words (e.g., the, to,?). Especially, the 
content words limited to those with part-of-
speech tags of JJ, NN, PRP, and VB. For sim-
plicity, the weight of word node wi is initialized 
to 1. 
In addition, directed edge eij is weighted to 
represent the relatedness between word wi and 
word wj in a document with transition probability 
P(j|i) from i to j, which is normalized as follows: 
???|?? ? ??????,???? ??????,????                    (1) 
where k represents the nodes in discourse, and 
Sim(wi,wj) denotes the similarity between wi and 
wj. In this paper, two kinds of information are 
used to calculate the similarity between words. 
One is word co-occurrence (if word wi and word 
wj occur in the same sentence or in the adjacent 
sentences, Sim(wi,wj) increases 1), and the other 
is WordNet (Miller, 1995) based similarity. 
Please note that Sim(wi,wi) = 0 to avoid self-
transition, and Sim(wi,wj) and Sim(wj,wi) may not 
be equal. 
Finally, the weights of word nodes are calcu-
lated using the PageRank algorithm as follows: 
???????????? ? 1 
?????????????? ? ? ? ???????????? ???? ???|?? ?
																																		?1 ? ??                                       (2) 
where d is the damping factor as in the PageRank 
algorithm. 
Topic-driven Word-based Graph Model 
While the above word-based graph model can 
well capture the relatedness between content 
words, it can only partially model the focus of a 
negation expression since negation focus is more 
directly related with topic than content. In order 
to reduce the gap, we propose a topic-driven 
word-based model by adding one more layer to 
refine the word-based graph model over the 
global topic distribution, as shown in Figure 2.  
 Figure 2. Topic-driven word-based graph model. 
525
Here, the topics are extracted from all the doc-
uments in the *SEM 2012 shared task using the 
LDA Gibbs Sampling algorithm (Griffiths, 2002). 
In the topic-driven word-based graph model, the 
first layer denotes the relatedness among content 
words as captured in the above word-based graph 
model, and the second layer denotes the topic 
distribution, with the dashed lines between these 
two layers indicating the word-topic model re-
turn by LDA. 
Formally, the topic-driven word-based two-
layer graph is defined as Gtopic (W, T, Ew, Et), 
where W={wi} is the set of words in one docu-
ment and T={ti} is the set of topics in all docu-
ments; Ew={ewij|wi, wj ?W} is the set of directed 
edges between words and Et ={etij|wi?W, tj ?T} 
is the set of undirected edges between words and 
topics; transition probability Pw(j|i) of ewij is de-
fined as the same as P(j|i) of the word-based 
graph model. Besides, transition probability Pt 
(i,m) of etij in the word-topic model is defined as: 
????, ?? ? ??????,???? ??????,????                 (3) 
where Rel(wi, tm) is the weight of word wi in top-
ic tm calculated by the LDA Gibbs Sampling al-
gorithm.  On the basis, the transition probability 
Pw (j|i) of ewij is updated by calculating as fol-
lowing: 
?????|?? ? ? ? ????|?? ? ?1 ? ?? ? ????,???????,??? ????,???????,???   
(4) 
where k represents all topics linked to both word 
wi and word wj, and ??[0,1] is the coefficient 
controlling the relative contributions from the 
lexical information in current document and the 
topic information in all documents. 
Finally, the weights of word nodes are calcu-
lated using the PageRank algorithm as follows: 
???????????? ? 1 
?????????????? ? ? ? ???????????? ???? ?????|?? ?
																																		?1 ? ??                                       (5) 
where d is the damping factor as in the PageRank 
algorithm. 
4.2 Negation Focus Identification via 
Graph Model 
Given the graph models and the PageRank opti-
mization algorithm discussed above, four kinds 
of contextual discourse information are extracted 
as inter-sentence features (Table 2). 
In particular, the total weight and the max 
weight of words in the focus candidate are calcu-
lated as follows: 
??????????? ? ? ?????????????????         (6) 
????????? ? max? ????????????????    (7) 
where i represents the content words in the focus 
candidate. These two kinds of weights focus on 
different aspects about the focus candidate with 
the former on the contribution of content words, 
which is more beneficial for a long focus candi-
date, and the latter biased towards the focus can-
didate which contains some critical word in a 
discourse. 
No Feature 
1 Total weight of words in the focus candi-date using the co-occurrence similarity. 
2 Max weight of words in the focus candi-date using the co-occurrence similarity. 
3 Total weight of words in the focus candi-date using the WordNet similarity. 
4 Max weight of words in the focus candi-date using the WordNet similarity. 
Table 2. Inter-sentence features extracted from 
graph model. 
For evaluating the contribution of contextual 
discourse information on negation focus identifi-
cation directly, we incorporate the four inter-
sentence features from the topic-driven word-
based graph model into a negation focus identifi-
er. 
5 Experimentation 
In this section, we describe experimental settings 
and systematically evaluate our negation focus 
identification approach with focus on exploring 
the effectiveness of contextual discourse infor-
mation. 
5.1 Experimental Settings 
Dataset 
In all our experiments, we employ the 
*SEM'2012 shared task corpus (Morante and 
Blanco, 2012)2 . As a freely downloadable re-
source, the *SEM shared task corpus is annotated 
on top of PropBank, which uses the WSJ section 
of the Penn TreeBank. In particular, negation 
focus annotation on this corpus is restricted to 
verbal negations (with corresponding mark 
                                                 
2 http://www.clips.ua.ac.be/sem2012-st-neg/ 
526
MNEG in PropBank). On 50% of the corpus an-
notated by two annotators, the inter-annotator 
agreement was 0.72 (Blanco and Moldovan, 
2011). Along with negation focus annotation, 
this corpus also contains other annotations, such 
as POS tag, named entity, chunk, constituent tree, 
dependency tree, and semantic role. 
In total, this corpus provides 3,544 instances 
of negation focus annotations. For fair compari-
son, we adopt the same partition as *SEM?2012 
shared task in all our experiments, i.e., with 
2,302 for training, 530 for development, and 712 
for testing. Although for each instance, the cor-
pus only provides the current sentence, the pre-
vious and next sentences as its context, we sort to 
the Penn TreeBank3 to obtain the corresponding 
document as its discourse context. 
Evaluation Metrics 
Same as the *SEM'2012 shared task, the evalua-
tion is made using precision, recall, and F1-score. 
Especially, a true positive (TP) requires an exact 
match for the negation focus, a false positive (FP) 
occurs when a system predicts a non-existing 
negation focus, and a false negative (FN) occurs 
when the gold annotations specify a negation 
focus but the system makes no prediction. For 
each instance, the predicted focus is considered 
correct if it is a complete match with a gold an-
notation. 
Beside, to show whether an improvement is 
significant, we conducted significance testing 
using z-test, as described in Blanco and Moldo-
van (2011). 
Toolkits 
In our experiments, we report not only the de-
fault performance with gold additional annotated 
features provided by the *SEM'2012 shared task 
corpus and the Penn TreeBank, but also the per-
formance with various kinds of features extracted 
automatically, using following toolkits: 
? Syntactic Parser: We employ the Stanford 
Parser4 (Klein and Manning, 2003; De Marn-
effe et al, 2006) for tokenization, constituent 
and dependency parsing. 
? Named Entity Recognizer: We employ the 
Stanford NER5 (Finkel et al, 2005) to obtain 
named entities. 
                                                 
3 http://www.cis.upenn.edu/~treebank/ 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://nlp.stanford.edu/ner/ 
? Semantic Role Labeler: We employ the se-
mantic role labeler, as described in Punyaka-
nok et al(2008). 
? Topic Modeler: For estimating transition 
probability Pt(i,m), we employ 
GibbsLDA++6, an LDA model using Gibbs 
Sampling technique for parameter estimation 
and inference. 
? Classifier: We employ SVMLight 7 with default 
parameters as our classifier. 
5.2 Experimental Results 
With Only Intra-sentence Information 
Table 3 shows the performance of the two base-
lines, the decision tree-based classifier as in 
Blanco and Moldovan (2011) and our ranking 
SVM-based classifier. It shows that our ranking 
SVM-based baseline slightly improves the F1-
measure by 2.52% over the decision tree-based 
baseline, largely due to the incorporation of more 
refined features.  
System P(%) R(%) F1 
BaselineC4.5 66.73 49.93 57.12
BaselineSVM 60.22 59.07 59.64
Table 3. Performance of baselines with only 
intra-sentence information. 
Error analysis of the ranking SVM-based 
baseline on development data shows that 72% of 
them are caused by the ignorance of inter-
sentence information. For example, among the 
42 instances listed in the category of ?#Inter-
Sentence Only? in Table 1, only 7 instances can 
be identified correctly by the ranking SVM-
based classifier. With about 4 focus candidates in 
one sentence on average, this percentage is even 
lower than random. 
With Only Inter-sentence Information 
For exploring the usefulness of pure contextual 
discourse information in negation focus identifi-
cation, we only employ inter-sentence features 
into ranking SVM-based classifier. First of all, 
we estimate two parameters for our topic-driven 
word-based graph model: topic number T for 
topic model and coefficient ? between Pw(j|i) and 
Pt (i,m) in Formula 4. 
Given the LDA Gibbs Sampling model with 
parameters ? = 50/T and ? = 0.1, we vary T from 
20 to 100 with an interval of 10 to find the opti-
                                                 
6 http://gibbslda.sourceforge.net/ 
7 http://svmlight.joachims.org 
527
mal T. Figure 3 shows the experiment results of 
varying T (with ? = 0.5) on development data. It 
shows that the best performance is achieved 
when T = 50 with 51.11 in F1). Therefore, we set 
T as 50 in our following experiments. 
 Figure 3. Performance with varying T. 
For parameter ?, a trade-off between the tran-
sition probability Pw(j|i) (word to word) and the 
transition probability Pt (i,m) (word and topic) to 
update P?w(j|i), we vary it from 0 to 1 with an 
interval of 0.1. Figure 4 shows the experiment 
results of varying ? (with T=50) on development 
data. It shows that the best performance is 
achieved when ? = 0.6, which are adopted here-
after in all our experiments. This indicates that 
direct lexical information in current document 
contributes more than indirect topic information 
in all documents on negation focus identification. 
It also shows that direct lexical information in 
current document and indirect topic information 
in all documents are much complementary on 
negation focus identification. 
 Figure 4. Performance with varying ?. 
System P(%) R(%) F1 
using word-based graph 
model  45.62 42.02 43.75
using topic-driven word-
based graph model 54.59 50.76 52.61
Table 4. Performance with only inter-sentence 
information. 
Table 4 shows the performance of negation 
focus identification with only inter-sentence fea-
tures. It also shows that the system with inter-
sentence features from the topic-driven word-
based graph model significantly improves the 
F1-measure by 8.86 over the system with inter-
sentence features from the word-based graph 
model, largely due to the usefulness of topic in-
formation. 
In comparison with Table 3, it shows that the 
system with only intra-sentence features achieves 
better performance than the one with only inter-
sentence features (59.64 vs. 52.61 in F1-
measure). 
With both Intra- and Inter-sentence In-
formation 
Table 5 shows that enriching intra-sentence fea-
tures with inter-sentence features significantly 
(p<0.01) improve the performance by 9.85 in F1-
measure than the better baseline. This indicates 
the usefulness of such contextual discourse in-
formation and the effectiveness of our topic-
driven word-based graph model in negation fo-
cus identification.  
System P(%) R(%) F1 
BaselineC4.5 with intra 
feat. only 66.73 49.93 57.12
BaselineSVM with intra 
feat. only 60.22 59.07 59.64
Ours with Both feat. 
using word-based GM 64.93 62.47 63.68
Ours  with  Both   feat. 
using    topic-driven 
word-based GM
71.67 67.43 69.49
(Note: ?feat.? denotes features; ?GM? denotes graph model.) 
Table 5. Performance comparison of systems on 
negation focus identification. 
System P(%) R(%) F1 
BaselineC4.5 with intra 
feat. only (auto) 60.94 44.62 51.52
BaselineSVM with intra 
feat. Only (auto) 53.81 51.67 52.72
Ours with Both feat. 
using word-based GM 
(auto) 
58.77 57.19 57.97
Ours  with  Both   feat. 
using    topic-driven 
word-based GM (auto) 
66.74 64.53 65.62
Table 6. Performance comparison of systems on 
negation focus identification with automatically 
extracted features. 
528
Besides, Table 6 shows the performance of 
our best system with all features automatically 
extracted using the toolkits as described in Sec-
tion 5.1. Compared with our best system employ-
ing gold additional annotated features (the last 
line in Table 5), the homologous system with 
automatically extracted features (the last line in 
Table 6) only decrease of less than 4 in F1-
measure. This demonstrates the achievability of 
our approach. 
In comparison with the best-reported perfor-
mance on the *SEM?2012 shared task (Rosen-
berg and Bergler, 2012), our system performs 
better by about 11 in F-measure.  
5.3 Discussion 
While this paper verifies the usefulness of con-
textual discourse information on negation focus 
identification, the performance with only inter-
sentence features is still weaker than that with 
only intra-sentence features. There are two main 
reasons. On the one hand, the former employs an 
unsupervised approach without prior knowledge 
for training. On the other hand, the usefulness of 
inter-sentence features depends on the assump-
tion that a negation focus relates to the meaning 
of which is most relevant to authors? intention in 
a discourse. If there lacks relevant information in 
a discourse context, negation focus will become 
difficult to be identified only by inter-sentence 
features.  
Error analysis also shows that some of the ne-
gation focuses are very difficult to be identified, 
even for a human being. Consider the sentence (3) 
in Section 1, if given sentence because of her 
neighbors' protests, but her husband doesn?t 
think so as its following context, both Helen and 
to play the violin can become the negation focus. 
Moreover, the inter-annotator agreement in the 
first round of negation focus annotation can only 
reach 0.72 (Blanco and Moldovan, 2011). This 
indicates inherent difficulty in negation focus 
identification. 
6 Conclusion 
In this paper, we propose a graph model to enrich 
intra-sentence features with inter-sentence fea-
tures from both lexical and topic perspectives. In 
this graph model, the relatedness between words 
is calculated by word co-occurrence, WordNet-
based similarity, and topic-driven similarity. 
Evaluation on the *SEM 2012 shared task corpus 
indicates the usefulness of contextual discourse 
information on negation focus identification and 
our graph model in capturing such global infor-
mation. 
In future work, we will focus on exploring 
more contextual discourse information via the 
graph model and better ways of integrating intra- 
and inter-sentence information on negation focus 
identification. 
Acknowledgments 
This research is supported by the National Natu-
ral Science Foundation of China, No.61272260, 
No.61331011, No.61273320, the Natural Science 
Foundation of Jiangsu Province, No. BK2011282, 
the Major Project of College Natural Science 
Foundation of Jiangsu Province, 
No.11KIJ520003, and the Graduates Project of 
Science and Innovation, No. CXZZ12_0818. The 
authors would like to thank the anonymous re-
viewers for their insightful comments and sug-
gestions. Our sincere thanks are also extended to 
Dr. Zhongqing Wang for his valuable discus-
sions during this study. 
Reference 
Eduardo Blanco and Dan Moldovan. 2011. Semantic 
Representation of Negation Using Focus Detection. 
In Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics, pages 
581-589, Portland, Oregon, June 19-24, 2011. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. 
A simple algorithm for identifying negated find-
ings and diseases in discharge summaries. Journal 
of Biomedical Informatics, 34:301-310. 
Marie-Catherine De Marneffe, Bill MacCartney and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 
In Proceedings of  LREC?2006. 
Yun Ding. 2011. Implied Negation in Discourse. 
Journal of Theory and Practice in Language Stud-
ies, 1(1): 44-51, Jan 2011. 
Jenny Rose Finkel, Trond Grenager, and Christopher 
Manning. 2005. Incorporating non-local infor-
mation into information extraction systems by 
gibbs sampling. In Proceedings of the 43rd Annual 
Meeting on Association for Computational Lin-
guistics, pages 363-370, Stroudsburg, PA, USA. 
Tom Griffiths. 2002. Gibbs sampling in the generative 
model of Latent Dirichlet Allocation. Tech. rep., 
Stanford University. 
Laurence R Horn. 1989. A Natural History of Nega-
tion. Chicago University Press, Chicago, IL. 
529
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan 
Zhu. 2009. Answering Opinion Questions with 
Random Walks on Graphs. In Proceedings of the 
47th Annual Meeting of the ACL and the 4th 
IJCNLP of the AFNLP, pages 737-745, Suntec, 
Singapore, 2-7 Aug 2009. 
Junhui Li, Guodong Zhou, Hongling Wang, and Qi-
aoming Zhu. 2010. Learning the Scope of Negation 
via Shallow Semantic Parsing. In Proceedings of 
the 23rd International Conference on Computa-
tional Linguistics. Stroudsburg, PA, USA: Associa-
tion for Computational Linguistics, 671-679. 
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, 
Yoshinobu Kano, and Jun'ichi Tsujii. 2009. Over-
view of BioNLP'09 Shared Task on Event Extrac-
tion. In Proceedings of the BioNLP'2009 Workshop 
Companion Volume for Shared Task. Stroudsburg, 
PA, USA: Association for Computational Linguis-
tics, 1-9. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 
41st Meeting of the Association for Computational 
Linguistics, pages 423-430. 
George A. Miller. 1995. Wordnet: a lexical database 
for english. Commun. ACM, 38(11):39-41. 
Roser Morante, Anthony Liekens and Walter Daele-
mans. 2008. Learning the Scope of Negation in Bi-
omedical Texts. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 715-724, Honolulu, Oc-
tober 2008. 
Roser Morante and Caroline Sporleder, editors. 2010. 
In Proceedings of the Workshop on Negation and 
Speculation in Natural Language Processing. Uni-
versity of Antwerp, Uppsala, Sweden. 
Roser Morante and Eduardo Blanco. 2012. *SEM 
2012 Shared Task: Resolving the Scope and Focus 
of Negation. In Proceedings of the First Joint Con-
ference on Lexical and Computational Semantics 
(*SEM), pages 265-274, Montreal, Canada, June 7-
8, 2012. 
Roser Morante and Caroline Sporleder. 2012. Modali-
ty and Negation: An Introduction to the Special Is-
sue. Computational Linguistics, 2012, 38(2): 223-
260. 
Roser Morante and Walter Daelemans. 2012. Conan 
Doyle-neg: Annotation of negation cues and their 
scope in Conan Doyle stories. In Proceedings of 
LREC 2012, Istambul. 
Lawrence Page, Sergey Brin, Rajeev Motwani, and 
Terry Winograd. 1998. The pagerank citation rank-
ing: Bringing order to the web. Technical report, 
Stanford University. 
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. 
The importance of syntactic parsing and inference 
in semantic role labeling. Computational Linguis-
tics, 34(2):257-287, June. 
Sabine Rosenberg and Sabine Bergler. 2012. UCon-
cordia: CLaC Negation Focus Detection at *Sem 
2012. In Proceedings of the First Joint Conference 
on Lexical and Computational Semantics (*SEM), 
pages 294-300, Montreal, Canada, June 7-8, 2012. 
Ton van der Wouden. 1997. Negative Contexts: Col-
location, Polarity, and Multiple Negation. 
Routledge, London. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for un-
certainty,negation and their scopes. BMC Bioin-
formatics, 9(Suppl 11):S9. 
Xiaojun Wan and Jianwu Yang. 2008. Multi-
document summarization using cluster-based link 
analysis. In Proceedings of the 31st annual inter-
national ACM SIGIR conference on Research and 
development in information retrieval, pages 299-
306. 
 
530
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 582?592,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Bilingual Active Learning for Relation Classification via Pseudo Paral-
lel Corpora 
 
Longhua Qian    Haotian Hui   Ya?nan Hu   Guodong Zhou*   Qiaoming Zhu 
Natural Language Processing Lab 
School of Computer Science and Technology, Soochow University 
1 Shizi Street, Suzhou, China 215006 
{qianlonghua,20134227019,20114227025,gdzhou,qmzhu}@suda.edu.cn 
 
  
 
Abstract 
Active learning (AL) has been proven ef-
fective to reduce human annotation ef-
forts in NLP. However, previous studies 
on AL are limited to applications in a 
single language. This paper proposes a 
bilingual active learning paradigm for re-
lation classification, where the unlabeled 
instances are first jointly chosen in terms 
of their prediction uncertainty scores in 
two languages and then manually labeled 
by an oracle. Instead of using a parallel 
corpus, labeled and unlabeled instances 
in one language are translated into ones 
in the other language and all instances in 
both languages are then fed into a bilin-
gual active learning engine as pseudo 
parallel corpora. Experimental results on 
the ACE RDC 2005 Chinese and English 
corpora show that bilingual active learn-
ing for relation classification signifi-
cantly outperforms monolingual active 
learning. 
1 Introduction 
Semantic relation extraction between named en-
tities (aka. entity relation extraction or more con-
cisely relation extraction) is an important subtask 
of Information Extraction (IE) as well as Natural 
Language Processing (NLP). With its aim to 
identify and classify the semantic relationship 
between two entities (ACE 2002-2007), relation 
extraction is of great significance to many NLP 
applications, such as question answering, infor-
mation fusion, social network construction, and 
knowledge mining and population etc. 
                                                 
* Corresponding author 
In the literature, the mainstream research on 
relation extraction adopts statistical machine 
learning methods, which can be grouped into 
supervised learning (Zelenko et al, 2003; Culotta 
and Soresen, 2004; Zhou et al, 2005; Zhang et 
al., 2006; Qian et al, 2008; Chan and Roth, 
2011), semi-supervised learning (Zhang et al, 
2004; Chen et al, 2006; Zhou et al, 2008; Qian 
et al, 2010) and unsupervised learning (Hase-
gawa et al, 2004; Zhang et al, 2005) in terms of 
the amount of labeled training data they need. 
Usually the extraction performance depends 
heavily on the quality and quantity of the labeled 
data, however, the manual annotation of a large-
scale corpus is labor-intensive and time-
consuming. In the last decade researchers have 
turned to another effective learning paradigm--
active learning (AL), which, given a small num-
ber of labeled instances and a large number of 
unlabeled instances, selects the most informative 
unlabeled instances to be manually annotated and 
add them into the training data in an iterative 
fashion. Essentially active learning attempts to 
decrease the quantity of labeled instances by en-
hancing their quality, gauged by their informa-
tiveness to the learner. Since its emergence, ac-
tive learning has been successfully applied to 
many tasks in NLP (Engelson and Dagan, 1996; 
Hwa, 2004; Tomanek et al, 2007; Settles and 
Craven, 2008).  
It is trivial to validate, as we will do later in 
this paper, that active learning can also alleviate 
the annotation burden for relation extraction in 
one language while retaining the extraction per-
formance. However, there are cases when we 
may exploit relation extraction in multiple lan-
guages and there are corpora with relation in-
stances annotated for more than one language, 
such as the ACE RDC 2005 English and Chinese 
corpora. Hu et al (2013) shows that supervised 
relation extraction in one language (e.g. Chinese) 
582
can be enhanced by relation instances translated 
from another language (e.g. English). This dem-
onstrates that there is some complementariness 
between relation instances in two languages, par-
ticularly when the training data is scarce. One 
natural question is: Can this characteristic be 
made full use of so that active learning can 
maximally benefit relation extraction in two lan-
guages? To the best of our knowledge, so far the 
issue of joint active learning in two languages 
has yet been addressed. Moreover, the success of 
joint bilingual learning may lend itself to many 
inherent multilingual NLP tasks such as POS 
tagging (Yarowsky and Ngai, 2001), name entity 
recognition (Yarowsky et al, 2001), sentiment 
analysis (Wan, 2009), and semantic role labeling 
(Sebastian and Lapata, 2009) etc. 
This paper proposes a bilingual active learn-
ing (BAL) paradigm to relation classification 
with a small number of labeled relation instances 
and a large number of unlabeled instances in two 
languages (non-parallel). Instead of using a par-
allel corpus which should have entity/relation 
alignment information and is thus difficult to 
obtain, this paper employs an off-the-shelf ma-
chine translator to translate both labeled and 
unlabeled instances from one language into the 
other language, forming pseudo parallel corpora. 
These translated instances along with the original 
instances are then fed into a bilingual active 
learning engine. Findings obtained from experi-
ments with relation classification on the ACE 
2005 corpora show that this kind of pseudo-
parallel corpora can significantly improve the 
classification performance for both languages in 
a BAL framework. 
The rest of the paper is organized as follows. 
Section 2 reviews the previous work on relation 
extraction while Section 3 describes our baseline 
systems. Section 4 elaborates on the bilingual 
active learning paradigm and Section 5 discusses 
the experimental results. Finally conclusions and 
directions for future work are presented in Sec-
tion 6. 
2 Related Work 
While there are many studies in monolingual 
relation extraction, there are only a few on multi-
lingual relation extraction in the literature. 
Monolingual relation extraction: A wide 
range of studies on relation extraction focus on 
monolingual resources. As far as representation 
of relation instances is concerned, there are fea-
ture-based methods (Zhao et al, 2004; Zhou et 
al., 2005; Chan and Roth, 2011) and kernel-
based methods (Zelenko et al, 2003; Zhang et al, 
2006; Qian et al, 2008), mainly for the English 
language. Both methods are also widely used in 
relation extraction in other languages, such as 
those in Chinese relation extraction (Che et al, 
2005; Li et al, 2008; Yu et al, 2010). 
Multilingual relation extraction: There are 
only two studies related to multilingual relation 
extraction. Kim et al (2010) propose a cross-
lingual annotation projection approach which 
uses parallel corpora to acquire a relation detec-
tor on the target language. However, the map-
ping of two entities involved in a relation in-
stance may leads to errors. Therefore, Kim and 
Lee (2012) further employ a graph-based semi-
supervised learning method, namely Label 
Propagation (LP), to indirectly propagate labels 
from the source language to the target language 
in an iterative fashion. Both studies transfer rela-
tion annotations via parallel corpora from the 
resource-rich language (English) to the resource-
poor language (Korean), but not vice versa. 
Based on a small number of labeled instances 
and a large number of unlabeled instances in 
both languages, our method differs from theirs in 
that we adopt a bilingual active learning para-
digm via machine translation and improve the 
performance for both languages simultaneously. 
Active Learning in NLP: Active learning 
has become an active research topic due to its 
potential to significantly reduce the amount of 
labeled training data while achieving comparable 
performance with supervised learning. It has 
been successfully applied to many NLP applica-
tions, such as POS tagging (Engelson and Dagan, 
1996; Ringger et al, 2007), word sense disam-
biguation (Chan and Ng, 2007; Zhu and Hovy, 
2007), sentiment detection (Brew et al, 2010; Li 
et al, 2012), syntactical parsing (Hwa, 2004; 
Osborne and Baldridge, 2004), and named entity 
recognition (Shen et al, 2004; Tomanek et al, 
2007; Tomanek and Hahn, 2009) etc.  
Different from these AL studies on a single 
task, Reichart et al (2008) introduce a multi-task 
active learning (MTAL) paradigm, where unla-
beled instances are selected for two annotation 
tasks (i.e. named entity and syntactic parse tree). 
They demonstrate that MTAL in the same lan-
guage outperforms one-sided and random selec-
tion AL. From a different perspective, we pro-
pose an active learning framework for the same 
task, but across two different languages. 
Another related study (Haffari and Sarkar, 
2009) deals with active learning for multilingual 
583
machine translation, which make use of multilin-
gual corpora to decrease human annotation ef-
forts by selecting highly informative sentences 
for a newly added language in multilingual paral-
lel corpora. While machine translation inherently 
deals with multilingual parallel corpora, our task 
focuses on relation extraction by pseudo parallel 
corpora in two languages. 
3 Baseline Systems 
This section first introduces the fundamental su-
pervised learning method, and then describes a 
baseline active learning algorithm. 
3.1 Supervised Learning 
We adopt the feature-based method for funda-
mental supervised relation classification, rather 
than the tree kernel-based method, since active 
learning needs a large number of iterations and 
the kernel-based method usually performs much 
slower than the feature-based one. Following is a 
list of our used features, much similar to Zhou et 
al. (2005): 
a) Lexical features of entities and their contexts 
WM1: bag-of-words in the 1st entity mention 
HM1: headword of M1 
WM2: bag-of-words in the 2nd entity mention 
HM2: headword of M2 
HM12: combination of HM1 and HM2 
WBNULL: when no word in between 
WBFL: the only one word in between 
WBF: the first word in between when at least 
two words in between 
WBL: the last word in between when at least 
two words in between 
WBO: other words in between except the first 
and last words when at least three words in 
between 
b) Entity type 
ET12: combination of entity types 
EST12: combination of entity subtypes 
EC12: combination of entity classes 
c) Mention level 
ML12: combination of entity mention levels 
MT12: combination of LDC mention types 
d) Overlap 
#WB: number of other mentions in between 
#MB: number of words in between 
M1>M2 or M1<M2: flag indicating whether 
M2/M1 is included in M1/M2. 
3.2 Active Learning Algorithm 
We use a pool-based active learning procedure 
with uncertainty sampling (Scheffer et al, 2001; 
Culotta and McCallum, 2005; Kim et al, 2006) 
for both Chinese and English relation classifica-
tion as illustrated in Fig. 1. During iterations a 
batch of unlabeled instances are chosen in terms 
of their informativeness to the current classifier, 
labeled by an oracle and in turn added into the 
labeled data to retrain the classifier. Due to our 
focus on the effectiveness of bilingual active 
learning on relation classification, we only use 
uncertainty sampling without incorporating more 
complex measures, such as diversity and repre-
sentativeness (Settles and Craven, 2008), and 
leave them for future work. 
Input: 
- L, labeled data set 
- U, unlabeled data set 
- n, batch size
Output:
- SVM, classifier 
Repeat:
    1. Train a single classifier SVM on L
2. Run the classifier on U
3. Find at most n instances in U that the classifier 
has the highest prediction uncertainty
    4. Have these instances labeled by an oracle
5. Add them into L
Until: certain number of instances are labeled or 
certain performance is reached
Algorithm uncertainty-based active learning
Figure 1. Pool-based active learning with uncer-
tainty sampling 
Since the SVMLIB package used in this paper 
can output probabilities assigned to the class la-
bels on an instance, we have three uncertainty 
metrics readily available, i.e., least confidence 
(LC), margin (M) and entropy (E). The NER 
experimental results on multiple corpora (Settles 
and Craven, 2008) show that there is no single 
clear winner among these three metrics. This 
conclusion is also validated by our preliminary 
experiments on the task of active learning rela-
tion extraction, thus we adopt the LC metric for 
simplicity. Specifically, with a sequence of K 
probabilities for a relation instance at some itera-
tion, denoted as {p1,p2,?pK} in the descending 
order, the LC metric of the relation instance can 
be simply picked as the first one, i.e. 
1pH
LC =     (1) 
Where K denotes the total number of relation 
classes. Note that this metric actually reflects 
prediction reliability (i.e. reverse uncertainty) 
rather than uncertainty in order to facilitate joint 
584
confidence calculation for two languages (cf. 
?4.4). Intuitively, the smaller the HLC is, the less 
confident the prediction is. 
4 Bilingual Active Learning for Rela-
tion Classification 
In this section, we elaborate on the bilingual ac-
tive learning for relation extraction. 
4.1 Problem Definition 
With Chinese and English (designated as c and e) 
as two languages used in our study, this paper 
intends to address the task of bilingual relation 
classification, i.e., assigning relation labels to 
candidate instances that have semantic relation-
ships. Suppose we have a small number of la-
beled instances in both languages, denoted as Lc 
and Le (non-parallel) respectively, and a large 
number of unlabeled instances in both languages, 
denoted as Uc and Ue (non-parallel). The test in-
stances in both languages are represented as Tc 
and Te. In order to take full advantage of bilin-
gual resources, we translate both labeled and 
unlabeled instances in one language to ones in 
the other language as follows: 
Lc ? Let 
Uc ? Uet 
Le ? Lct 
Ue ? Lct 
The objective is to learn SVM classifiers in 
both languages, denoted as SVMc and SVMe re-
spectively, in a BAL fashion to improve their 
classification performance. 
4.2 Bilingual Active Learning Framework 
Currently, AL is widely used in NLP tasks in a 
single language, i.e., during iterations unlabeled 
instances least confident only in one language 
are picked and manually labeled to augment the 
training data. The only exception is AL for ma-
chine translation (Haffari et al, 2009; Haffari 
and Sarkar, 2009), whose purpose is to select the 
most informative sentences in the source lan-
guage to be manually translated into the target 
language. Previous studies (Reichart et al, 2008; 
Haffari and Sarkar, 2009) show that multi-task 
active learning (MTAL) can yield promising 
overall results, no matter whether they are two 
different tasks or the task of machine translation 
on multiple language pairs. If a specific NLP 
task on two languages, such as relation classifi-
cation, can be regarded as two tasks, it is reason-
able to argue that these two tasks can benefit 
each other when jointly performed in the BAL 
framework. Yet, to our knowledge, this issue 
remains unexplored. 
An important issue for bilingual learning is 
how to obtain two language views for relation 
instances from multilingual resources. There are 
three solutions to this problem, i.e. parallel cor-
pora (Lu et al, 2011), translated corpora (aka. 
pseudo parallel corpora) (Wan 2009), and bilin-
gual lexicons (Oh et al, 2009). We adopt the one 
with pseudo parallel corpora, using the machine 
translation method to generate instances from 
one language to the other in the BAL paradigm, 
as depicted in Fig. 2. 
English View
Labeled 
Chinese Instances 
(Lc)
Labeled Translated 
English Instances 
(Let)
Labeled 
English Instances (Le)
Labeled Translated 
Chinese Instances 
(Lct)
Machine 
Translation
Machine 
Translation
Unlabeled 
Chinese Instances 
(Uc)
Unlabeled 
Translated Chinese 
Instances (Uct)
Unlabeled Translated
 English Instances (Uet)
Unlabeled 
English Instances 
(Ue)
Machine 
Translation
Machine 
Translation
Chinese View
Bilingual 
active learning
Test
Chinese Instances 
(Tc)
Test
English Instances 
(Te)
 
Figure 2. Framework of bilingual active learning 
In order to make full use of pseudo parallel 
corpora, translated labeled and unlabeled in-
stances are augmented in the following two ways: 
z For labeled Chinese instances (Lc) and Eng-
lish instances (Le), their translated counter-
parts (Let and Lct), along with their labels, are 
directly added into the labeled instances in the 
other language; 
z For unlabeled Chinese instances (Uc) and 
English instances (Ue), during an active learn-
ing iteration the top n unlabeled instances in 
Uc and Uet which are least confidently jointly 
585
predicted by SVMc and SVMe are labeled by 
an oracle and added to Lc and Le respectively. 
(cf. ?4.4) 
4.3 Instance Projection via MT 
Among the several off-the-shelf machine transla-
tion services, we select the Google Translator1 
because of its high quality and easy accessibility. 
Both the mentions of relation instances and the 
mentions of two involved entities are first trans-
lated into the other language via machine transla-
tion. Then, two entities in the original instance 
are aligned with their counterparts in the trans-
lated instance in order to form an aligned bilin-
gual relation instance pair. 
Instance translation 
All the positive instances in the ACE 2005 Chi-
nese and English corpora are translated to an-
other language respectively, i.e. Chinese to Eng-
lish and vice versa. The relation instance is rep-
resented as the word sequence between two enti-
ties. This word sequence, rather than the whole 
sentence, is then translated to another language 
by the Google Translator. The reason is that, al-
though this sequence loses partial contextual in-
formation of the relation instance, its translation 
quality is supposed to be better. Our preliminary 
experiments indicate that the addition of contex-
tual information fail to benefit the task. After 
translation, word segmentation is performed on 
Chinese instances translated from English while 
tokenization is needed for translated English in-
stances. 
Entity alignment 
The objective of entity alignment is to build a 
mapping from the entities in the original in-
stances to the entities in the translated instances. 
Put in another way, entity alignment automati-
cally marks the entity mentions in the translated 
instance, thereby the feature vector correspond-
ing to the translated instance can be constructed. 
Entity alignment is vital in cross-language rela-
tion extraction whose difficulty lies in the fact 
that the same entity mention as an isolated phrase 
and as an integral phrase in the relation instance 
can be translated to different phrases. For exam-
ple, the Chinese entity mention ???? (officer) 
is translated to ?officer? in isolation, it is, how-
ever, translated to ?officials? when in the relation 
instance ???? ??? (Syrian officials). 
                                                 
1 http://translate.google.com 
Input:
- Me, entity mention in English
- Re, relation instance in English
- Mct, translation of Me in Chinese
- Rc, translation of Re in Chinese
- L, a lexicon consisting of entries like (ei, ci, pi), 
where pi is the translation probability from ei to ci
- ?, probability threshold
Output:
- Mc, the counterpart of Me in Rc
Steps:
1. If Mct can be exactly found in Rc, then return 
Mct
2. If the rightmost part of Mct can be found in Rc, 
then this part can be returned
3. For very word we in Me,
a) If there exists a word wc in Rc and (we, wc, p) 
in L and p>?, then (we, wc) is a match of two words
b) Return a successive sequence of matching 
words wc
4. Return null
Algorithm entity alignment
 Figure 3. Entity alignment algorithm 
Therefore, we devise some heuristics to align 
entity mentions between Chinese and English. 
The basic idea is that the word sequence in one 
mention successively matches the word sequence 
in the other mention. Take entity alignment from 
English to Chinese as an example, given entity 
mention Me in relation instance Re in English and 
their respective translations Mct and Rc in Chi-
nese, the objective of entity alignment is to find 
Mc, the counterpart of Me in Rc. The procedure of 
entity alignment algorithm can be described in 
Fig. 3. 
In the algorithm, the probability threshold ? is 
empirically set to 0.002 where the precision and 
recall of entity alignment are balanced. Our lexi-
con is derived from the FBIS parallel corpus 
(#LDC2003E14), which is widely used in ma-
chine translation between English and Chinese. It 
should be noted that the process of relation trans-
lation and entity alignment are far from perfec-
tion, leading to reduction in the number of in-
stances being mapping to the other language, i.e. 
|Lc| > |Let| 
|Uc| > |Uet| 
|Le| > |Lct| 
|Ue| > |Lct| 
4.4 Bilingual Active Learning Algorithm 
The basic idea of our BAL paradigm is that, 
while unlabeled instances uncertain in one lan-
586
guage are informative to the learner in that lan-
guage, unlabeled instances jointly uncertain in 
both languages are informative to the learners in 
both languages, thus potentially improving clas-
sification performance for both languages more 
than their individual active learners do.  This 
idea is embodied in the BAL algorithm in Fig. 4, 
where n is the batch size, i.e., the number of in-
stances selected, labeled and augmented at each 
iteration. 
Figure 4. Bilingual active learning algorithm 
The key point of this algorithm lies in Step 5 
and Step 6, where unlabeled instances from Uc 
and Ue are selected and labeled respectively. 
Take Chinese for an example, when gauging the 
prediction uncertainty for an unlabeled instance 
in Uc, not only its own uncertainty measure Hc 
predicted by SVMc is considered, but also the 
uncertainty measure Het for its translation coun-
terpart in Uet, which is predicted by SVMe, is con-
sidered. Generally, in order to jointly consider 
these two measures, there are three methods to 
compute their means, namely, arithmetic mean, 
geometric mean and harmonic mean. Preliminary 
experiments show that among these three means, 
there is no single winner, so we simply take the 
geometric mean defined as follows:  
etcg HHH *=    (2) 
Considering that we adopt the LC measure as 
the uncertainty score, when an instance in Uc 
can?t find its translation counterpart in Uet due to 
translation error or entity alignment failure, Het is 
set to 1, i.e. the maximum. Since the bigger H is, 
the more confident the prediction is, the less 
likely the instance will be chosen, in this way we 
discourage the unlabeled instances without trans-
lation counterparts. 
5 Experimentation 
We have systematically evaluated our BAL para-
digm on the relation classification task using 
ACE RDC 2005 RDC Chinese and English cor-
pora. 
5.1 Experimental Settings 
Corpora and Preprocessing 
We use the ACE 2005 RDC Chinese and English 
corpora as the benchmark data (hereafter we re-
fer to them as the Chinese corpus (ACE2005c) 
and the English corpus (ACE2005e) respec-
tively). Both corpora have the same en-
tity/relation hierarchies, which define 7 entity 
types, 6 major relation types. However, the Chi-
nese corpus contains 633 documents and 9,147 
positive relation instances while the English cor-
pus only contains 498 files and 6,253 positive 
instances. Therefore, in order to balance the cor-
pus scale to fairly evaluate bilingual active learn-
ing impact on relation classification, we ran-
domly select 458 Chinese files and thus get 
6,268 positive instances, comparable to the Eng-
lish corpus. 
Preprocessing steps for both corpora include 
sentence splitting and tokenization (word seg-
mentation for Chinese using ICTCLAS2). Then, 
positive relation mentions with word sequences 
between two entities and their feature vectors are 
extracted from sentences while negative relation 
mentions are simply discarded because we focus 
on the task of relation classification. After entity 
and relation mentions in one language are trans-
                                                 
2 http://ictclas.org/ 
587
lated into the other language using the Google 
translator, entity alignment is performed between 
relation mentions and their translations. Finally 
4,747 Chinese relation mentions are successfully 
translated and aligned from English and vice 
versa, 4,936 English relation mentions are trans-
lated and aligned from Chinese. 
SVMLIB (Chang and Lin, 2011) is selected as 
our classifier since it supports multi-class classi-
fication. The training parameters C (SVM) is set 
to 2.4 according to our previous work on relation 
extraction (Qian et al, 2010). Relation classifica-
tion performance is evaluated using the standard 
Precision (P), Recall (R) and their harmonic av-
erage (F1) as well as deficiency measure (cf. lat-
ter in this section.). Overall performance scores 
are averaged over 10 runs. For each run, 1/40 
and 1/5 randomly selected instances are used as 
the training and test set respectively while the 
remaining instances are used as the unlabeled set 
for further labeling during active learning itera-
tions. 
Methods for Comparison 
For fair comparison, two baseline methods of 
supervised learning are included to augment their 
training sets with labeled instances during itera-
tions. However, these labeled instances are cho-
sen randomly from the corpus. 
SL-MO (Supervised Learning with monolin-
gual labeled instances): only the monolingual 
labeled instances are fed to the SVM classifiers 
for both Chinese and English relation classifica-
tion respectively. The initial training data only 
contain Lc and Le for Chinese and English respec-
tively.  
SL-CR (Supervised Learning with cross-
lingual labeled instances): in addition to mono-
lingual labeled instances (SL-MO), the training 
data for supervised learning contain labeled in-
stances translated from the other language. That 
is, the initial training data contain Lc and Lct for 
Chinese, or Le and Let for English. More impor-
tant, at each iteration not only the labeled in-
stances are added to the training data of its own 
language, but their translated instances are also 
added to the training data of the other language. 
AL-MO (Active Learning with monolingual 
instances): labeled and unlabeled data for active 
learning only contain monolingual instances. No 
translated instances are involved. That is, the 
data contain Lc and Uc for Chinese, or Le and Ue 
for English respectively. This is the normal ac-
tive learning method applied to a single language. 
AL-CR (Active Learning with cross-lingual 
instances): both the manually labeled instances 
and their translated ones are added to the respec-
tive training data. The initial training data con-
tain Lc and Lct for Chinese, or Le and Let for Eng-
lish. At each iteration, the n least confidently 
classified instances in Uc and Ue are labeled and 
added to the Chinese/English training data re-
spectively. Their translated instances in Uet and 
Uct are also added to the English/Chinese training 
data respectively. 
AL-BI (Active Learning with bilingual la-
beled and unlabeled instances): similar to AL-
CR with the exception that the unlabeled in-
stances are chosen not by uncertainty scores in 
one language, but by the joint uncertainty scores 
in two languages. (cf. ?4.4) 
Evaluation Metric 
Although learning curves are often used to evalu-
ate the performance for active learning, it is pref-
erable to quantitatively compare various active 
learning methods using a statistical metric defi-
ciency (Schein and Ungar, 2007) defined as: 
?
?
=
=
?
?= n
i in
n
i in
n
REFFREFF
ALFREFF
REFALdef
1
1
))()((
))()((
),(     (3) 
Where n is the number of iterations involved in 
active learning and Fi is the F1-score of relation 
classification at the ith iteration. REF is the base-
line active learning method and AL is an im-
proved variant of REF, such as AL-CR or AL-
BI. Essentially this deficiency metric measures 
the degree to which REF outperforms AL. Thus, 
smaller deficiency value (i.e. <1.0) indicates AL 
outperforms REF while a larger value (i.e. >1.0) 
indicates AL underperforms REF. 
5.2 Experimental Results and Analysis 
Comparison of overall deficiency 
Table 1 compares the deficiency scores of rela-
tion classification on the Chinese (ACE2005c) 
and English corpora (ACE2005e) for various 
learning methods, i.e., SL-CR, AL-MO, AL-CR 
and AL-BI. Particularly, SL-MO is used as the 
baseline system against which deficiency scores 
for other methods are computed. The batch size n 
is set to 100 and iterations stop after all the unla-
beled instances have run out of. Deficiency 
scores are averaged over 10 runs and the best 
ones are highlighted in bold font. Each run has a 
different test set and a different seed set. 
588
 
 (a) Chinese      (b) English 
Figure 5. Deficiency comparison for different batch sizes 
 
(a) Chinese      (b) English 
Figure 6. Learning curves for different methods 
 
The table shows that among the three active 
learning methods, bilingual active learning (AL-
BI) achieves the best performance for both Chi-
nese and English relation classification. This 
demonstrates that, bilingual active learning with 
jointly selecting the unlabeled instances can not 
only enhance relation classification for its own 
language, but also help relation classification for 
the other language due to the complementary 
nature of relation instances between Chinese and 
English. 
Corpora SL-CR AL-MO AL-CR AL-BI
ACE2005c 0.934 0.383 0.323 0.254
ACE2005e 0.779 0.405 0.298 0.160
Table 1. Deficiency comparison of different 
methods 
The table also shows the consistent utility of 
cross-lingual information for relation classifica-
tion for both languages. When cross-lingual in-
formation is augmented, SL-CR outperforms 
SL-MO and AL-CR outperforms AL-MO. 
Comparison of different batch sizes 
Figure 5(a) and 5(b) illustrate the deficiency 
scores for four learning methods (SL-CR, AL-
MO, AL-CR and AL-BI) against the SL-MO 
method with different batch sizes (n), where pre-
fixes ?C? and ?E? denote Chinese and English 
respectively. The horizontal axes denote the 
range of n (<=1000) while the vertical ones de-
note the deficiency scores. 
The figures show that the deficiency scores 
for three active learning methods run virtually 
parallel with each other while they increase mo-
notonously with the batch size n. This suggests 
that for both Chinese and English AL-BI consis-
tently performs best against other methods across 
a wide range of batch sizes, though the overall 
advantage of three active learning methods gen-
erally diminish. 
Comparison of learning curves 
In order to gain an intuition into how the per-
formance evolves when the labeled instances are 
added into the training data during iterations, we 
depict the learning curves for various learning 
methods on the Chinese and English corpora in 
Fig. 6(a) and 6(b) respectively. The horizontal 
axes denote learning iterations while the vertical 
ones denote F1-scores. For simplicity of illustra-
tion the F1-scores are collected from one of the 
10 runs. 
75
77
79
81
83
85
87
89
91
93
95
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48
C-SL-MO
C-SL-CR
C-AL-MO
C-AL-CR
C-AL-BI
75
77
79
81
83
85
87
89
91
93
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48
E-SL-MO
E-SL-CR
E-AL-MO
E-AL-CR
E-AL-BI
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
100 200 300 400 500 600 700 800 900 1000
C-SL-CR
C-AL-MO
C-AL-CR
C-AL-BI
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
100 200 300 400 500 600 700 800 900 1000
E-SL-CR
E-AL-MO
E-AL-CR
E-AL-BI
589
The figures clearly demonstrate the perform-
ance difference for both languages among five 
methods at the beginning of iterations while F1-
scores converge at the end of iterations. Particu-
larly at the very outset, AL-BI outperforms other 
methods, quickly jumps to a very high point 
comparable to its best performance. However, 
after the 10th iteration the performance scores for 
the three AL variants tend to show trivial differ-
ence probably because most highly informative 
instances have already been added to the training 
data. 
Comparison of annotation scale 
In order to better compare BAL with other AL 
methods Figure 7 zooms out partial data on three 
AL methods in Fig. 6 and rescale the data for 
AL-MO, where ?C? and ?E? denote Chinese and 
English respectively. Likewise, the vertical axis 
denotes F1-scores while the horizontal axis de-
notes the number of instances labeled for AL-
CR and AL-BI. However, for AL-MO that num-
ber is doubled. This figure tries to answer the 
question: to label n respective instances in both 
languages for BAL or to labeled 2n instances in 
just one language for monolingual AL, can the 
former rival the latter? 
80
82
84
86
88
90
92
94
100 200 300 400 500 600 700 800 900 1000
C-AL-MO (2n)
C-AL-CR
C-AL-BI
E-AL-MO (2n)
E-AL-CR
E-AL-BI
 
Figure 7. Comparison of annotation scale among 
three AL methods 
The figure shows that for both Chinese and 
English, when the number of instances (n) to be 
labeled is no greater than 400, AL-BI with n in-
stances can achieve comparable performance 
with AL-MO with 2n instances. It implies that 
when the labeled instances are limited, labeling 
instances, half in one language and half in the 
other for BAL, is competitive against labeling 
the same total number of instances in just one 
language for monolingual AL, not to mention 
that the former can generate two relation extrac-
tors on two languages. 
6 Conclusion 
This paper proposes a bilingual active learning 
paradigm for Chinese and English relation classi-
fication. Given a small number of relation in-
stances and a large number of unlabeled relation 
instances in both languages, we translate both the 
labeled and unlabeled instances in one language 
to the other as pseudo parallel corpora. After en-
tity alignment, these labeled and unlabeled in-
stances in both languages are fed into a bilingual 
active learning engine. Experiments with the task 
of relation classification on the ACE RDC 2005 
Chinese and English corpora show that bilingual 
active learning can significantly outperforms 
monolingual active learning for both Chinese and 
English simultaneously. Moreover, we demon-
strate that BAL across two languages can com-
pete against monolingual AL when the annota-
tion scale is limited, though the overall number 
of labeled instances remains the same. 
For future work, on one hand, we plan to 
combine uncertainty sampling with diversity and 
informativeness measures; on the other hand, we 
intend to combine BAL with semi-supervised 
learning to further reduce human annotation ef-
forts. 
Acknowledgments 
This research is supported by Grants 61373096, 
61305088, 61273320, and 61331011 under the 
National Natural Science Foundation of China; 
Project 2012AA011102 under the ?863? Na-
tional High-Tech Research and Development of 
China; Grant 11KJA520003 under the Education 
Bureau of Jiangsu, China. We would like to 
thank the excellent and insightful comments 
from the three anonymous reviewers. Thanks 
also go to my colleague Dr. Shoushan Li for his 
helpful suggestions. 
Reference 
ACE. 2002-2007. Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/ 
A. Brew, D. Greene, and P. Cunningham. 2010. Using 
crowdsourcing and active learning to track senti-
ment in online media. ECAI?2010: 145?150. 
Y.S. Chan and D. Roth. 2011. Exploiting Syntactico-
Semantic Structures for Relation Extraction. 
ACL?2011: 551-560 
Y.S. Chan and H.T. Ng. 2007. Domain adaptation 
with active learning for word sense disambiguation. 
ACL?2007. 
590
C.C. Chang and C.J. Lin. 2011. LIBSVM: a library 
for support vector machines. ACM Transactions on 
Intelligent Systems and Technology, 2(27):1-27. 
W.X. Che, T. Liu, and S. Li. 2005. Automatic Extrac-
tion of Entity Relation (in Chinese). Journal of 
Chinese Information Processing, 19(2): 1-6. 
J.X. Chen, D.H. Ji, and C. L. Tan. 2006. Relation Ex-
traction using Label Propagation-based Semi-
supervised Learning. ACL/COLING?2006: 129-136. 
A. Culotta and J. Sorensen. 2004. Dependency tree 
kernels for relation extraction. ACL?2004: 423-439.  
A. Culotta and A. McCallum. 2005. Reducing label-
ing effort for stuctured prediction tasks. AAAI?2005: 
746?751. 
S. P. Engelson and I. Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. ACL?1996: 319?326. 
G. Haffari, M. Roy, and A. Sarkar. 2009. Active 
learning for statistical phrase-based machine trans-
lation. NAACL?2009: 415?423. 
G. Haffari and A. Sarkar. 2009. Active learning for 
multilingual statistical machine translation. 
ACL/IJCNLP?2009: 181?189. 
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Dis-
covering Relations among Named Entities from 
Large Corpora. ACL?2004. 
Y.N. Hu, J.G. Shu, L.H. Qian, and Q.M. Qiao. 2013. 
Cross-lingual Relation Extraction based on Ma-
chine Translation (in Chinese). Journal of Chinese 
Information Processing, 27(5): 191-197. 
R. Hwa. 2004. Sample selection for statistical parsing. 
Computational Linguistics, 30(3): 253?276. 
S. Kim, M. Jeong, J. Lee, and G.G. Lee. 2010. A 
Cross-lingual Annotation Projection Approach for 
Relation Detection. COLING?2010: 564-571. 
S. Kim and G.G. Lee. 2012. A Graph-based Cross-
lingual Projection Approach for Weakly Super-
vised Relation Extraction. ACL?2012: 48-53. 
S. Kim, Y. Song, K. Kim, J.W. Cha, and G.G. Lee. 
2006. MMR-based active machine learning for bio 
named entity recognition. HLT-NAACL?2006: 69?
72. 
W.J. Li, P. Zhang, F.R. Wei, Y.X. Hou, and Q. Lu. 
2008. A Novel Feature-based Approach to Chinese 
Entity Relation Extraction. ACL?2008: 89-92. 
S.S. Li, S.F. Ju, G.D. Zhou, and X.J. Li. 2012. Active 
learning for imbalanced sentiment classifica-
tion. EMNLP-CoNLL?2012: 139-148. 
B. Lu, C.H. Tan, C. Cardie, and B.K. Tsou. 2011. 
Joint Bilingual Sentiment Classification with 
Unlabeled Parallel Corpora. ACL?2011: 320-330. 
J. Oh, K. Uchimoto, and K. Torisawa. 2009.  Bilin-
gual Co-Training for Monolingual Hyponymy-
Relation Acquisition. ACL?2009: 432-440. 
M. Osborne and J. Baldridge. 2004. Ensemble based 
active learning for parse selection. HLT-NAACL? 
2004: 89?96. 
L.H. Qian, G.D. Zhou, F. Kong, and Q.M. Zhu. 2010. 
Clustering-based Stratified Seed Sampling for 
Semi-Supervised Relation Classification. 
EMNLP2010: 346-355. 
L.H. Qian, G.D. Zhou, Q.M. Zhu, and P.D. Qian. 
2008. Exploiting constituent dependencies for tree 
kernel-based semantic relation extraction. COL-
ING?2008: 697-704.  
R. Reichart, K. Tomanek, U. Hahn, and A. Rappoport. 
2008. Multi-task active learning for linguistic an-
notations. ACL?2008: 861-869. 
E. Ringger, P. McClanahan, R. Haertel, G. Busby, M. 
Carmen, J. Carroll, K. Seppi, and D. Lonsdale. 
2007. Active learning for part-of-speech tagging: 
Accelerating corpus annotation. In Proceedings of 
the Linguistic Annotation Workshop at ACL?2007: 
101?108. 
T. Scheffer, C. Decomain, and S. Wrobel. 2001. Ac-
tive hidden Markov models for information extrac-
tion. In Proceedings of the International Confer-
ence on Advances in Intelligent Data Analysis 
(CAIDA), pages 309?318. 
A. I. Schein and L. H. Ungar. 2007. Active learning 
for logistic regression: an evaluation. Machine 
Learning, 68(3): 235-265. 
P. Sebastian and M. Lapata. 2009. Cross-lingual an-
notation projection of semantic roles. Journal of 
Artificial Intelligence Research, 36(1): 307-340. 
B. Settles and M. Craven. 2008. An Analysis of Ac-
tive Learning Strategies for Sequence Labeling 
Tasks. EMNLP?2008: 1070?1079. 
D. Shen, J. Zhang, J. Su, G.D. Zhou and C.-L. Tan. 
2004. Multi-criteria-based active learning for 
named entity recognition. ACL?2004. 
K. Tomanek and U. Hahn. 2009. Semi-Supervised 
Active Learning for Sequence Labeling. ACL-
IJCNLP?2009: 1039-1047. 
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts an-
notation costs and maintains reusability of anno-
tated data. EMNLP-CoNLL?2007: 486?495. 
X.J. Wan. 2009. Co-Training for Cross-Lingual Sen-
timent Classification. ACL-AFNLP?2009: 235-243. 
D. Yarowsky and G. Ngai. 2001. Inducing multilin-
gual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. NAACL?2001: 1-8. 
591
D. Yarowsky, G. Ngai, and R. Wicentorski. 2001. 
Inducing multilingual text analysis tools via robust 
projection across aligned corpora. HLT?2001:1-8. 
H.H. Yu, L.H. Qian, G.D. Zhou, and Q.M. Zhu. 2010. 
Chinese Semantic Relation Extraction based on 
Unified Syntactic and Entity Semantic Tree (in 
Chinese). Journal of Chinese Information Process-
ing, 24(5): 17-23. 
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel Methods for Relation Extraction. Journal of 
Machine Learning Research, 3: 1083-1106. 
Z. Zhang. 2004. Weakly-supervised relation classifi-
cation for Information Extraction. CIKM?2004. 
M. Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L. 
Tan. 2005. Discovering Relations between Named 
Entities from a Large Raw Corpus Using Tree 
Similarity-Based Clustering. IJCNLP?2005: 378-
389.  
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A 
Composite Kernel to Extract Relations between 
Entities with both Flat and Structured Features. 
ACL/COLING?2006: 825-832.  
S.B. Zhao and R. Grishman. 2005. Extracting rela-
tions with integrated information using kernel 
methods.  ACL?2005: 419-426. 
G.D. Zhou, J.H. Li, L.H. Qian, and Q.M. Zhu. 2008. 
Semi-Supervised Learning for Relation Extraction. 
IJCNLP?2008: 32-38. 
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Ex-
ploring various knowledge in relation extraction. 
ACL?2005: 427-434.  
J.B. Zhu and E. Hovy. 2007. Active learning for word 
sense disambiguation with methods for addressing 
the class imbalance problem. EMNLP-
CoNLL?2007: 783-790. 
592
