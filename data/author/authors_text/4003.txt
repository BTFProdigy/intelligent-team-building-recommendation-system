An Approach to Summarizing Short Stories
Anna Kazantseva
The School of Information Technology and Engineering
University of Ottawa
ankazant@site.uottawa.ca
Abstract
This paper describes a system that pro-
duces extractive summaries of short
works of literary fiction. The ultimate
purpose of produced summaries is de-
fined as helping a reader to determine
whether she would be interested in read-
ing a particular story. To this end, the
summary aims to provide a reader with
an idea about the settings of a story (such
as characters, time and place) without re-
vealing the plot. The approach presented
here relies heavily on the notion of as-
pect. Preliminary results show an im-
provement over two na?ve baselines: a
lead baseline and a more sophisticated
variant of it. Although modest, the results
suggest that using aspectual information
may be of help when summarizing fic-
tion. A more thorough evaluation involv-
ing human judges is under way.
1 Introduction
In the course of recent years the scientific
community working on the problem of automatic
text summarization has been experiencing an
upsurge. A multitude of different techniques has
been applied to this end, some of the more
remarkable of them being (Marcu, 1997; Mani et
al. 1998; Teufel and Moens, 2002; Elhadad et al,
2005), to name just a few. These researchers
worked on various text genres: scientific and
popular scientific articles (Marcu, 1997; Mani et
al., 1998), texts in computational linguistics
(Teufel and Moens, 2002), and medical texts
(Elhadad et al, 2002). All these genres are ex-
amples of texts characterized by rigid structure,
relative abundance of surface markers and
straightforwardness. Relatively few attempts
have been made at summarizing less structured
genres, some of them being dialogue and speech
summarization (Zechner, 2002; Koumpis et al
2001). The issue of summarizing fiction remains
largely untouched, since a few very thorough
earlier works (Charniak, 1972; Lehnert, 1982).
The work presented here seeks to fill in this gap.
The ultimate objective of the project is stated
as follows: to produce indicative summaries of
short works of fiction such that they be helpful to
a potential reader in deciding whether she would
be interested in reading a particular story or not.
To this end, revealing the plot was deemed un-
necessary and even undesirable. Instead, the cur-
rent approach relies on the following assumption:
when a reader is presented with an extracted
summary outlining the general settings of a story
(such as time, place and who it is about), she will
have enough information to decide how inter-
ested she would be in reading a story. For exam-
ple, a fragment of such a summary, produced by
an annotator for the story The Cost of Kindness
by Jerome K. Jerome is presented in Figure 1.
The plot, which is a tale of how one local family
decides to bid a warm farewell to Rev. Crackle-
thorpe and causes the vicar to change his mind
and remain in town, is omitted.
The data used in the experiments consisted of
23 short stories, all written in XIX ? early XX
century by main-stream authors such as Kathe-
rine Mansfield, Anton Chekhov, O.Henry, Guy
de Maupassant and others (13 authors in total).
The genre can be vaguely termed social fiction
with the exception of a few fairy-tales. Such
vagueness as far as genre is concerned was de-
liberate, as the author wished to avoid producing
a system relying on cues specific to a particular
genre. Average length of a story in the corpus is
3,333 tokens (approximately 4.5 letter-sized
pages) and the target compression rate is 6%.
In order to separate the background of a story
from events, this project relies heavily on the
notion of aspect (the term is explained in Section
3.1). Each clause of every sentence is described
in terms of aspect-related features. This represen-
tation is then used to select salient descriptive
sentences and to leave out those which describe
events.
55
The organization of the paper follows the
overall architecture of the system. Section 2 pro-
vides a generalized overview of the pre-
processing stage of the project, during which
pronominal and nominal anaphoric references
(the term is explained in Section 2) were re-
solved and main characters were identified. Sec-
tion 3 briefly reviews the concept of aspect,
gives an overview of the system and provides the
linguistic motivation behind it. Section 4 de-
scribes the classification procedures (machine
learning and manual rule creation) used to distin-
guish between descriptive elements of a story
and passages that describe events. It also reports
results. Section 5 draws some conclusions and
outlines possible directions in which this work
may evolve.
2 Data Pre-Processing
Before working on selecting salient descriptive
sentences, the stories of the training set were ana-
lyzed for presence of surface markers denoting
characters, locations and temporal anchors. To
this end, the GATE Gazetteer (Cunningham et
al., 2002) was used, and only entities recognized
by it automatically were considered.
The findings were as follows. Each story con-
tained multiple mentions of characters (an aver-
age of 64 mentions per story). Yet only 22 loca-
tion markers were found, most of these being
street names. The 22 markers were found in 10
out of 14 stories, leaving 4 stories without any
identifiable location markers. Only 4 temporal
anchors were identified in all 14 stories: 2 abso-
lute (such as years) and 2 relative (names of
holidays). These findings support the intuitive
idea that short stories revolve around their char-
acters, even if the ultimate goal is to show a lar-
ger social phenomenon.
Due to this fact, the data was pre-processed in
such a way as to resolve pronominal and nominal
anaphoric references to animate entities. The
term anaphora can be informally explained as a
way of mentioning a previously encountered en-
tity without naming it explicitly. Consider exam-
ples 1a and 1b from The Gift of the Magi by O.
Henri. 1a is an example of pronominal anaphora,
where the noun phrase (further NP) Della is re-
ferred to as an antecedent and both occurrences
of the pronoun her as anaphoric expressions or
referents. Example 1b illustrates the concept of
nominal anaphora. Here the NP Dell is the ante-
cedent and my girl is the anaphoric expression
(in the context of this story Della and the girl are
the same person).
(1a) Della finished her cry and attended to
her cheeks with the powder rag.
(1b) "Don't make any mistake, Dell," he said,
?about me. I don't think there's anything
[?] that could make me like my girl any
less.
The author created a system that resolved 1st
and 3rd person singular pronouns (I, me, my, he,
his etc.) and singular nominal anaphoric expres-
sions (e.g. the man, but not men). The system
was implemented in Java, within the GATE
framework, using Connexor Machinese Syntax
parser (Tapanainen and J?rvinen, 1997).
A generalized overview of the system is pro-
vided below. During the first step, the docu-
ments were parsed using Connexor Machinese
Syntax parser. The parsed data was then for-
warded to the Gazetteer in GATE, which recog-
nized nouns denoting persons. The original ver-
sion of the Gazetteer recognized only named en-
tities and professions, but the Gazetteer was ex-
tended to include common animate nouns such as
man, woman, etc. As the next step, an imple-
mentation based on a classical pronoun resolu-
tion algorithm (Lappin and Leass, 1994) was ap-
plied to the texts. Subsequently, anaphoric noun
phrases were identified using the rules outlined
Figure 1. A fragment of a desired summary for The Cost of Kindness by Jerome K. Jerome.
The Cost of Kindness
Jerome K. Jerome (1859-1927)
Augustus Cracklethorpe would be quitting Wychwood-on-the-Heath the following Monday, never to set
foot--so the Rev. Augustus Cracklethorpe himself and every single member of his congregation hoped sin-
cerely--in the neighbourhood again. [?] The Rev. Augustus Cracklethorpe, M.A., might possibly have been
of service to his Church in, say, some East-end parish of unsavoury reputation, some mission station far
advanced amid the hordes of heathendom. There his inborn instinct of antagonism to everybody and every-
thing surrounding him, his unconquerable disregard for other people's views and feelings, his inspired con-
viction that everybody but himself was bound to be always wrong about everything, combined with deter-
mination to act and speak fearlessly in such belief, might have found their uses. In picturesque little
Wychwood-on-the-Heath [?] these qualities made only for scandal and disunion.
56
in (Poesio and Vieira, 2000). Finally, these ana-
phoric noun phrases were resolved using a modi-
fied version of (Lappin and Leass, 1994), ad-
justed to finding antecedents of nouns.
A small-scale evaluation based on 2 short sto-
ries revealed results shown in Table 1. After re-
solving anaphoric expressions, characters that are
central to the story were selected based on nor-
malized frequency counts.
3 Selecting Descriptive Sentences Using
Aspectual Information
3.1 Linguistic definition of aspect
In order to select salient sentences that set out the
background of a story, this project relied on the
notion of aspect. For the purposes of this paper
the author uses the term aspect to denote the
same concept as what (Huddleston and Pullum,
2002) call the situation type. Informally, it can be
explained as a characteristic of a clause that
gives an idea about the temporal flow of an event
or state being described.
A general hierarchy of aspectual classifi-
cation based on (Huddleston and Pullum, 2002)
is shown in Figure 2 with examples for each
type. In addition, aspectual type of a clause may
be altered by multiplicity, e.g. repetitions. Con-
sider examples 2a and 2b.
(2a) She read a book.
(2b) She usually read a book a day. (e.g. She
used to read a book a day).
Example 2b is referred to as serial situation
(Huddleston and Pullum, 2002). It is considered
to be a state, even though a single act of reading
a book would constitute an event.
Intuitively, stative situations (especially serial
ones) are more likely to be associated with de-
scriptions; that is with things that are, or things
that were happening for an extended period of
time (consider He was a tall man. vs. He opened
the window.).The rest of Section 3 describes the
approach used for identifying single and serial
stative clauses and for using them to construct
summaries.
3.2 Overall system design
Selection of the salient background sentences
was conducted in the following manner. Firstly,
the pre-processed data (as outlined in Section 2)
was parsed using Connexor Machinese Syntax
parser. Then, sentences were recursively split
into clauses. For the purposes of this project a
clause is defined as a main verb with all its com-
plements, including subject, modifiers and their
sub-trees.
Subsequently, two different representations
were constructed for each clause: one fine-
grained and one coarse-grained. The main differ-
ence between these two representations was in
the number of attributes and in the cardinality of
the set of possible values, and not in how much
and what kind of information they carried. For
instance, the fine-grained dataset had 3 different
features with 7 possible values to carry tense-
related information: tense, is_progressive and
is_perfect, while the coarse-grained dataset car-
ried only one binary feature,
is_simple_past_or_present.
Two different approaches for selecting de-
scriptive sentences were tested on each of the
representations. The first approach used machine
learning techniques, namely C5.0 (Quinlan,
1992) implementation of decision trees. The sec-
ond approach consisted of applying a set of
manually created rules that guided the classifica-
tion process. Motivation for features used in each
dataset is given in Section 3.3. Both approaches
and preliminary results are discussed in Sections
4.1 - 4.4.
The part of the system responsible for select-
ing descriptive sentences was implemented in
Python.
3.3 Feature selection: description and moti-
vation
Figure 2. Aspectual hierarchy after (Hud-
dleston and Pullum, 2002).
Table 1. Results of anaphora resolution.
Type of
anaphora
All Correct Incor-
rect
Error
rate, %
Pronominal 597 507 90 15.07
Nominal 152 96 56 36.84
Both 749 603 146 19.49
57
Features for both representations were selected
based on one of the following criteria:
(Criterion 1) a clause should ?talk? about im-
portant things, such as characters or locations
(Criterion 2) a clause should contain back-
ground descriptions rather then events
The number of features providing information
towards each criterion, as well as the number of
possible values, is shown in Table 2 for both
representations.
The attributes contributing towards Criterion 1
can be divided into character-related and loca-
tion-related.
Character-related features were designed so as
to help identify sentences that focused on charac-
ters, not just mentioned them in passing. These
attributes described whether a clause contained a
character mention and what its grammatical
function was (subject, object, etc.), whether such
a mention was modified and what was the posi-
tion of a parent sentence relative to the sentence
where this character was first mentioned (intui-
tively, earlier mentions of characters are more
likely to be descriptive).
Location-related features in both datasets de-
scribed whether a clause contained a location
mention and whether it was embedded in a
prepositional phrase (further PP). The rationale
behind these attributes is that location mentions
are more likely to occur in PPs, such as from the
Arc de Triomphe, to the Place de la Concorde.
In order to meet Criterion 2 (that is, to select
descriptive sentences) a number of aspect-related
features were calculated. These features were
selected so as to model characteristics of a clause
that help determine its aspectual class. The char-
acteristics used were default aspect of the main
verb of a clause, tense, temporal expressions,
semantic category of a verb, voice and some
properties of the direct object. Each of these
characteristics is listed below, along with motiva-
tion for it, and information about how it was
calculated.
It must be mentioned that several researchers
looked into determining automatically various
semantic properties of verbs, such as (Siegel,
1998; Merlo et al, 2002). Yet these approaches
dealt with properties of verbs in general and not
with particular usages in the context of concrete
sentences.
Default verbal aspect. A set of verbs, referred
to as stative verbs, tends to produce mostly sta-
tive clauses. Examples of such verbs include be,
like, feel, love, hate and many others. A common
property of such verbs is that they do not readily
yield a progressive form (Vendler, 1967; Dowty,
1979). Consider examples 3a and 3b.
(3a) She is talking. (a dynamic verb talk)
(3b) *She is liking the book. (a stative verb
like)
The default aspectual category of a verb was ap-
proximated using Longman Dictionary of Con-
temporary English (LDOCE). Verbs marked in
LDOCE as not having a progressive form were
considered stative and all others ? dynamic. This
information was expressed in both datasets as 1
binary feature.
Grammatical tense. Usually, simple tenses
are more likely to be used in stative or habitual
situations than progressive or perfect tenses. In
fact, it is considered to be a property of stative
clauses that they normally do not occur in pro-
gressive (Vendler, 1967; Huddleston and Pullum,
2002). Perfect tenses are feasible with stative
clauses, yet less frequent. Simple present is only
feasible with states and not with events (Huddle-
ston and Pullum, 2002) (see examples 4a and
4b).
(4a) She likes writing.
(4b) *She writes a book. (e.g. now)
In the fine-grained dataset this information was
expressed using 3 features with 7 possible values
Table 2. Description of the features in both datasets
Fine-grained dataset Coarse-grained dataset
Type of features Number of fea-
tures
Number of val-
ues
Number of fea-
tures
Number of values
Character-related 9 16 4 6
Aspect-related 12 92 8 16
Location-related 2 4 2 4
Others 4 9 3 4
All 27 121 17 30
58
(whether a clause is in present, past or future
tense, whether it is progressive and whether it is
perfective). In the coarse-grained dataset, this
information was expressed using 1 binary fea-
ture: whether a clause is in simple past or present
tense.
Temporal expressions. Temporal markers
(often referred to as temporal adverbials), such as
usually, never, suddenly, at that moment and
many others are widely employed to mark the
aspectual type of a sentence (Dowty, 1982;
Harkness, 1987; By, 2002). Such markers pro-
vide a wealth of information and often unambi-
guously signal aspectual type. For example:
(5a) She read a lot tonight.
(5b) She always read a lot. (Or She used to
read a lot.)
Yet, such expressions are not easy to capture
automatically. In order to use the information
expressed in temporal adverbials, the author ana-
lyzed the training data for presence of such ex-
pressions and found 295 occurrences in 10 sto-
ries. It appears that this set could be reduced to
95 templates in the following manner. For exam-
ple, the expressions this year, next year, that long
year could all be reduced to a template
<some_expression> year. Each template is char-
acterized by 3 features: type of the temporal ex-
pression (location, duration, frequency, enact-
ment) (Harkness, 1987); magnitude (year, day,
etc.); and plurality (year vs. years). The fine-
grained dataset contained 3 such features with 14
possible values (type of expression, its magni-
tude and plurality). The coarse-grained dataset
contained 1 binary feature (whether there was an
expression of a long period of time).
Verbal semantics. Inherent meaning of a verb
also influences the aspectual type of a given
clause.
(6a) She memorized that book by heart. (an
event)
(6b) She enjoyed that book. (a state)
Not surprisingly, this information is very difficult
to capture automatically. Hoping to leverage it,
the author used semantic categorization of the
3,000 most common English verbs as described
in (Levin, 1993). The fine-grained dataset con-
tained a feature with 49 possible values that cor-
responded to the top-level categories described in
(Levin, 1993). The coarse-grained dataset con-
tained 1 binary feature that carried this informa-
tion. Verbs that belong to more than one category
were manually assigned to a single category that
best captured their literal meaning.
Voice. Usually, clauses in passive voice only
occur with events (Siegel, 1998). Both datasets
contained 1 binary feature to describe this infor-
mation.
Properties of direct object. For some verbs
properties of direct object help determine
whether a given clause is stative or dynamic.
(7a) She wrote a book. (event)
(7b) She wrote books. (state)
The fine-grained dataset contained 2 binary fea-
tures to describe whether direct object is definite
or indefinite and whether it is plural. The coarse-
grained dataset contained no such information
because it appeared that this information was not
crucial.
Several additional features were present in
both datasets that described overall characteris-
tics of a clause and its parent sentence, such as
whether these were affirmative, their index in the
text, etc. The fine-grained dataset contained 4
such features with 9 possible values and the
coarse-grained dataset contained 3 features with
7 values.
4 Experiments
4.1 Experimental setting
The data used in the experiments consisted of 23
stories split into a training set (14 stories) and a
testing set (9 stories). Each clause of every story
was annotated by the author of this paper as
summary-worthy or not. Therefore, the classifi-
cation process occurred at the clause-level. Yet,
summary construction occurred at the sentence-
level, that is if one clause in a sentence was con-
sidered summary-worthy, the whole sentence
was also considered summary-worthy. Because
of this, results are reported at two levels: clause
and sentence. The results at the clause-level are
more appropriate to judge the accuracy of the
classification process. The results at the sentence
level are better suited for giving an idea about
how close the produced summaries are to their
annotated counterparts.
The training set contained 5,514 clauses and
the testing set contained 4,196 clauses. The target
compression rate was set at 6% expressed in
terms of sentences. This rate was selected be-
cause it approximately corresponds to the aver-
age compression rate achieved by the annotator
59
Table 3. Results obtained using rules (summary-worthy class)
Dataset Level Preci-
sion,%
Recall,
%
F-score
,%
Kappa Overall error rate,%
(both classes)
Baseline LEAD Clause 19.92 23.39 21.52 16.85 8.87
Baseline
LEAD CHAR
Clause 8.93 25.69 13.25 6.01 17.47
Fine-grained Clause 34.77 40.83 37.55 33.84 17.73
Coarse-grained Clause 32.00 47.71 38.31 34.21 7.98
Baseline LEAD Sent. 23.57 24.18 23.87 19.00 9.24
Baseline
LEAD CHAR
Sent. 22.93 23.53 23.23 18.31 9.24
Fine-grained Sent. 41.40 42.48 41.94 38.22 6.99
Coarse-grained Sent. 40.91 41.18 41.04 37.31 7.03
(5.62%). The training set consisted of 310 posi-
tive examples and 5,204 negative examples, and
the testing set included 218 positive and 3,978
negative examples.
Before describing the experiments and dis-
cussing results, it is useful to define baselines.
The author of this paper is not familiar with any
comparable summarization experiments and for
this reason was unable to use existing work for
comparison. Therefore, a baseline needed to be
defined in different terms. To this end, two na?ve
baselines were computed.
Intuitively, when a person wishes to decide
whether to read a certain book or not, he opens it
and flips through several pages at the beginning.
Imitating this process, a simple lead baseline
consisting of the first 6% of the sentences in a
story was computed. It is denoted LEAD in Ta-
bles 3 and 4. The second baseline is a slightly
modified version of the lead baseline and it con-
sists of the first 6% of the sentences that contain
at least one mention of one of the important
characters. It is denoted LEAD CHAR in Tables
3 and 4.
4.2 Experiments with the rules
The first classification procedure consisted of
applying a set of manually designed rules to pro-
duce descriptive summaries. The rules were de-
signed using the same features that were used for
machine learning and that are described in Sec-
tion 3.3.
Two sets of rules were created: one for the
fine-grained dataset and another for the coarse-
grained dataset. Due to space restrictions it is not
possible to reproduce the rules in this paper. Yet,
several examples are given in Figure 4. (If a rule
returns True, then a clause is considered to be
summary-worthy.)
The results obtained using these rules are pre-
sented in Table 3. They are discussed along with
the results obtained using machine learning in
Section 4.4.
4.3 Experiments with machine learning
As an alternative to rule construction, the author
used C5.0 (Quilan, 1992) implementation of de-
cision trees to select descriptive sentences. The
algorithm was chosen mainly because of the
readability of its output. Both training and testing
datasets exhibited a 1:18 class imbalance, which,
given a small size of the datasets, needed to be
compensated. Undersampling (randomly remov-
ing instances of the majority class) was applied
to both datasets in order to correct class imbal-
ance.
This yielded altogether 4 different datasets
(see Table 4). For each dataset, the best model
was selected using 10-fold cross-validation on
the training set. The model was then tested on the
testing set and the results are reported in Table 4.
Figure 4. Examples of manually composed
rules.
Rule 1
if a clause contains a character mention as
subject or object and a temporal expression
of type enactment (ever, never, always)
return True
Rule 2
if a clause contains a character mention as
subject or object and a stative verb
return True
Rule 3
if a clause is in progressive tense
return False
60
4.4 Results
The results displayed in Tables 3 and 4 show
how many clauses (and sentences) selected by
the system corresponded to those chosen by the
annotator. The columns Precision, Recall and F-
score show measures for the minority class (sum-
mary-worthy). The columns Overall error rate
and Kappa show measures for both classes.
Although modest, the results suggest an im-
provement over both baselines. Statistical sig-
nificance of improvements over baselines was
tested for p = 0.001 for each dataset-approach.
The improvements are significant in all cases.
The columns F-score in Tables 3 and 4 show
f-score for the minority class (summary-worthy
sentences), which is a measure combining preci-
sion and recall for this class. Yet, this measure
does not take into account success rate on the
negative class. For this reason, Cohen?s kappa
statistic (Cohen, 1960) was also computed. It
measures the overall agreement between the sys-
tem and the annotator. This measure is shown in
the column named Kappa.
In order to see what features were the most in-
formative in each dataset, a small experiment
was conducted. The author removed one feature
at a time from the training set and used the de-
crease in F-score as a measure of informative-
ness. The experiment revealed that in the coarse-
grained dataset the following features were the
most informative: 1) the position of a sentence
relative to the first mention of a character; 2)
whether a clause contained character mentions;
3) voice and 4) tense. In the fine-grained dataset
the findings were similar: 1) presence of a char-
acter mention; 2) position of a sentence in the
text; 3) voice; and 4) tense were more important
than the other features.
It is not easy to interpret these results in any
conclusive way at this stage. The main weakness,
of course, is that the results are based solely on
the annotations of one person while it is gener-
ally known that human annotators are likely to
exhibit some disagreement. The second issue lies
in the fact that given the compression rate of 6%,
and the objective that the summary be indicative
and not informative, more that one ?good? sum-
mary is possible. It would therefore be desirable
that the results be evaluated not based on overlap
with an annotator (or annotators, for that matter),
but on how well they achieve the stated objec-
tive.
5 Conclusions
In the immediate future the inconclusiveness of
the results will be addressed by means of asking
human judges to evaluate the produced summa-
ries. During this process the author hopes to find
out how informative the produced summaries are
and how well they achieve the stated objective
(help readers decide whether a story is poten-
tially interesting to them). The judges will also
be asked to annotate their own version of a sum-
mary to explore what inter-judge agreement
means in the context of fiction summarization.
More remote plans include possibly tackling
the problem of summarizing the plot and dealing
more closely with the problem of evaluation in
the context of fiction summarization.
Table 4. Results obtained using machine learning (summary-worthy class)
Dataset Training data-
set
Level Preci-
sion, %
Recall,
%
F-score,
%
Kap-
pa
Overall
error rate,
%
Baseline LEAD Clause 19.92 23.39 21.52 16.85 8.87
Baseline
LEAD CHAR
Clause 8.93 25.69 13.25 6.01 17.47
Fine-grained original Clause 28.81 31.19 29.96 25.96 7.58
Fine-grained undersampled Clause 39.06 45.87 42.19 38.76 6.53
Coarse-grained original Clause 34.38 30.28 32.22 28.73 6.63
Coarse-grained undersampled Clause 28.52 33.49 30.80 26.69 7.82
Baseline LEAD Sent. 23.57 24.18 23.87 19.00 9.24
Baseline
LEAD CHAR
Sent. 22.93 23.53 23.23 18.31 9.24
Fine-grained original Sent. 38.93 37.91 38.41 34.57 7.22
Fine-grained undersampled Sent. 41.4 42.48 41.94 38.22 6.99
Coarse-grained original Sent. 42.19 35.29 38.43 34.91 6.72
Coarse-grained undersampled Sent. 37.58 38.56 38.06 34.10 7.46
61
Acknowledgements
The author would like to express her gratitude
to Connexor Oy and especially to Atro Vouti-
lainen for their kind permission to use Connexor
Machinese Syntax parser free of charge for re-
search purposes.
References
Tomas By. 2002. Tears in the Rain. Ph.D. thesis, Uni-
versity of Sheffield.
Eugene Charniak. 1972. Toward a Model of Chil-
dren?s Story Comprehension. Ph.D. thesis, Massa-
chusetts Institute of Technology.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales, Educational and Psychological
Measurement, (20): 37?46.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan. 2002. GATE: A
Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. Pro-
ceedings of the 40th Anniversary Meeting of the
Association for Computational Linguistics
(ACL'02), Philadelphia.
David Dowty. 1982. Tense, Time Adverbs, and Com-
positional Semantic Theory. Linguistics and Phi-
losophy, (5), p. 23-59.
David Dowty. 1979. Word Meaning and Montague
Grammar. D. Reidel Publishing Company,
Dordrecht.
Noemie Elhadad, Min-Yen Kan, Judith Klavans, and
Kathleen McKeown. 2005. Customization in a uni-
fied framework for summarizing medical literature.
Artificial Intelligence in Medicine 33(2): 179-198.
Janet Harkness. 1987. Time Adverbials in English and
Reference Time. In Alfred Schopf (ed.), Essays on
Tensing in English, Vol. I: Reference Time, Tense
and Adverbs, p. 71-110. T?bingen: Max Niemeyer.
Rodney Huddleston and Geoffrey Pullum. 2002. The
Cambridge Grammar of the English Language Us-
age, p. 74-210. Cambridge University Press.
Konstantinos Koumpis, Steve Renals, and Mahesan
Niranjan. 2001. Extractive summarization of
voicemail using lexical and prosodic feature subset
selection. In Proeedings of Eurospeech, p. 2377?
2380, Aalborg, Denmark.
Herbert Leass and Shalom Lappin. 1994. An algo-
rithm for Pronominal Anaphora Resolution. Com-
putational Linguistics, 20(4): 535-561.
Wendy Lehnert. 1982. Plot Units: A Narrative Sum-
marization Strategy. In W. Lehnert and M. Ringle
(eds.). Strategies for Natural Language Processing.
Erlbaum, Hillsdale, NJ.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press.
Longman Dictionary of Contemporary English. 2002.
Pearson Education.
Inderjeet Mani, Eric Bloedorn and Barbara Gates.
1998. Using Cohesion and Coherence Models for
Text Summarization. In Working Notes of the
Workshop on Intelligent Text Summarization, p.
69-76. Menlo Park, California: American Associa-
tion for Artificial Intelligence Spring Simposium
Series.
Daniel Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language
Texts. PhD Thesis, Department of Computer Sci-
ence, University of Toronto.
Paola Merlo, Suzanne Stevenson, Vivian Tsang and
Gianluca Allaria. 2002. A Multilingual Paradigm
for Automatic Verb Classification. Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics.(ACL?02), Philadelphia.
Massimo Poesio and Renata Vieira. 2000 . An Em-
pirically Based System for Processing Definite De-
scriptions. Computational Linguistics, 26(4): 525-
579.
J. Ross Quinlan, 1992: C4.5: Programs for Machine
Learning. Morgan Kaufmann Pub., San Mateo,
CA.
Eric V. Siegel. 1998. Linguistic Indicators for Lan-
guage Understanding: Using machine learning
methods to combine corpus-based indicators for
aspectual classification of clauses. Ph.D. Disserta-
tion. Columbia University.
Pasi Tapanainen and Timo J?rvinen. 1997 A non-
projective dependency parser. In Proceedings of
the 5th Conference on Applied Natural Language
Processing, p. 64-71.
Simone Teufel and Marc Moens. 2002. Summarizing
scientific articles-experiments with relevance and
rhetorical status. Computational Linguistics, 28(4):
409?445.
Zeno Vendler. 1967. Linguistics in Philosophy. Cor-
nell University Press, p. 97- 145.
Klaus Zechner. 2002. Automatic Summarization of
Open-Domain Multiparty Dialogues in Diverse
Genres. Computational Linguistics 28(4):447-485.
62
Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 8?15,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Challenges in Evaluating Summaries of Short Stories 
Anna Kazantseva School of Information Technology and Engineering, University of Ottawa, Ottawa, Canada ankazant@site.uottawa.ca 
Stan Szpakowicz School of Information Technology and Engineering, University of Ottawa, Ottawa, Canada Institute of Computer Science, Polish Academy of Sciences, Warsaw, Poland szpak@site.uottawa.ca  Abstract This paper presents experiments with the evaluation of automatically produced summaries of literary short stories. The summaries are tailored to a particular purpose of helping a reader decide whether she wants to read the story. The evaluation procedure includes extrinsic and intrinsic measures, as well as subjective and factual judgments about the summaries pronounced by human subjects. The experiments confirm the experience of summarizing more conventional genres: sentence overlap between human- and machine-made summaries is not a complete picture of the quality of a summary. In fact, in our case, sentence overlap does not correlate well with human judgment. We explain the evaluation procedures and discuss several challenges of evaluating summaries of works of fiction. 1 Introduction In recent years the automatic text summarization community has increased its focus on reliable evaluation. The much used evaluation methods based on sentence overlap with reference summaries have been called into question (Mani 2001) as they provide only a rough approximation of semantic similarity between summaries. A number of deeper, more semantically-motivated approaches have been proposed, such as the factoid method (van Halteren 
and Teufel, 2003) and the pyramid method (Nenkova and Passonneau 2004). These methods measure similarity between reference and generated summaries more reliably but, unfortunately, have a disadvantage of being very labour-intensive. This paper describes experiments in evaluating automatically produced summaries of literary short stories. It presents an approach that evaluates summaries from two different perspectives: comparing computer-made summaries to those produced by humans based on sentence-overlap and measuring usefulness and informativeness of the summaries by themselves ? a step critical when creating and evaluating summaries of a relatively unexplored genre. The paper also points out several challenges specific to evaluating summaries of fiction such as questionable suitability of traditional metrics (those based on sentence overlap), unavailability of clearly defined criteria to judge ?goodness? of a summary and a higher degree of redundancy in such texts. We achieve these goals by performing a two-step evaluation of our summaries. Initially, for each story in the test set we compare sentence overlap between summaries which the system generates and those produced by three human subjects. These experiments reveal that inter-rater agreement measures tend to be pessimistic where fiction is concerned. This seems due to a higher degree of redundancy and paraphrasing in such texts. The second stage of the evaluation process seeks to measure usefulness of the summaries in a more tangible way. To this end, three subjects answered a number of questions, first after 
8
Figure 1. Example of a summary produced by the system. A MATTER OF MEAN ELEVATION. By O. Henry (1862-1910). On the camino real along the beach the two saddle mules and the four pack mules of Don Se?or Johnny Armstrong stood, patiently awaiting the crack of the whip of the arriero, Luis. These articles Don Johnny traded to the interior Indians for the gold dust that they washed from the Andean streams and stored in quills and bags against his coming. It was a profitable business, and Se?or Armstrong expected soon to be able to purchase the coffee plantation that he coveted. Armstrong stood on the narrow sidewalk, exchanging garbled Spanish with old Peralto, the rich native merchant who had just charged him four prices for half a gross of pot-metal hatchets, and abridged English with Rucker, the little German who was Consul for the United States. [?]  Armstrong, waved a good-bye and took his place at the tail of the procession. Armstrong concurred, and they turned again upward toward Tacuzama. [?] Peering cautiously inside, he saw, within three feet of him, a woman of marvellous, imposing beauty, clothed in a splendid loose robe of leopard skins. The hut was packed close to the small space in which she stood with the squatting figures of Indians. [?] I am an American. If you need assistance tell me how I can render it. [?] The woman was worthy of his boldness. Only by a sudden flush of her pale cheek did she acknowledge understanding of his words. [?] " I am held a prisoner by these Indians. God knows I need help. [?] look, Mr. Armstrong, there is the sea!   reading only the summary and then after reading the complete story. The set included both factual questions (e.g. can you tell where this story takes place?) and subjective questions (e.g. how readable did you find this summary?). Finally, we compare the two types of results with a surprising discovery: overlap-based measures and human judgment do not correlate well in our case. This paper is organized in the following manner. Section 2 briefly describes our summarizer of short stories. Section 3.1 discusses experiments comparing generated summaries to reference ones based on sentence overlap. The experiments involving human judgment of the summaries are presented in Section 3.2 and the two types of experiments are compared in Section 3.3. Section 4 draws conclusions and outlines possible directions for future work. 2 Background: System Description A detailed description of our summarizer of short stories is outside the scope of this paper. For completeness, this section gives an overview of the system?s inner workings. An interested reader is referred to our previous work (Kazantseva 2006) for more information. The system is designed to create a particular type of indicative generic summaries ? namely, summaries that would help readers decide whether they would like to read a given story. Because of this, a summary, as defined here, is not meant to summarize the plot of a story. It is intended 
to raise adequate expectations and to enable a reader to make informed decisions based on a summary only. We achieve this goal by identifying the salient portions of the original texts that lay out the setting of a story, namely, location and main characters. The present prototype of our system creates summaries by extracting sentences from original documents. An example summary produced by the system appears in Figure 1. The system works in two stages. First it attempts to identify important entities in stories (locations and characters). Next, sentences that are descriptive and set out the background of a story are separated from those that relate events of the plot. Finally, the system selects summary-worthy sentences in a way that favours descriptive ones that focus on important entities and occur early in the text. The identification of important entities is achieved by processing the stories using a gazetteer. Pronominal and noun phrase anaphora are very common in fiction, so we resolve anaphoric expressions of these two types. The anaphora resolution module is restricted to resolving singular anaphoric expressions that denote animate entities (people and, sometimes, animals). The main characters are then identified using normalized frequency counts. The next stage of the process attempts to identify sentences that set out the background in each story. The stories are parsed using the Connexor Machinese Syntax Parser (Tapanainen and J?rvinen 1997) and sentences are split into clauses. 
9
Each clause is represented as a vector of features that approximate its aspectual type. The features are designed to help identify state clauses (John was a tall man) and serial situations (John always drops things) (Huddleston and Pullum 2002, p. 123-124). Four groups of features represent each clause: character-related, location-related, aspect-related and others. Character-related features capture such information as the presence of a mention of one of the main characters in a clause, its syntactic function, how early in the text this mention occurs, etc. Location-related features state whether a clause contains a location name and whether this name is embedded in a prepositional phrase. Aspect-related features reflect a number of properties of a clause that influence its aspectual type. They include the main verb?s lexical aspect, the tense, the presence and the type of temporal expressions, voice, and the presence of modal verbs. In our experiments we create two separate representations for each clause: fine-grained and coarse-grained. Both contain features from all four feature groups. The difference between them is only in the number of features and in the cardinality of the set of possible values. Two different procedures achieve the actual selection process. The first procedure performs decision tree induction using C5.0 (Quinlan 1992) to select the most likely candidate sentences. The training data for this process consists of short stories annotated at the clause-level by the first author of this paper. The second procedure applies a set of manually created rules to select summary-worthy sentences. The corpus for the experiments contains 47 short stories from Project Gutenberg (http://www.gutenberg.org) divided into a training set (27 stories) and a test set (20 stories). These are classical works written in English or translated into English by authors including O.Henry, Jerome K. Jerome, Katherine Mansfield and Anton Chekhov. They have on average 3,333 tokens and 244 sentences (4.5 letter-sized pages). The target compression rate was set at 6% counted in 
sentences. This rate was selected because it corresponded to the compression rate achieved by the first author when creating initial training and test data. 3 Evaluation: Experimental Setup We designed our evaluation procedure to have easily interpreted, meaningful results, and keep the amount of labour reasonable. We worked with six subjects (different than the authors of this paper) who performed two separate tasks. In Task 1 each subject was asked to read a story and create its summary by selecting 6% of the sentences. The subjects were explained that their summaries were to raise expectations about the story, but not to reveal what happens in it. In Task 2 the subjects made a number of judgments about the summaries before and after reading the original stories. The subjects read a summary similar to the one shown in Figure 1. Next, they were asked six questions, three of which were factual in nature and three others were subjective. The subjects had to answer these questions using the summary as the only source of information. Subsequently, they read the original story and answered almost the same questions (see Section 4). This process allowed us to understand how informative the summaries were by themselves, without access to the originals, and also whether they were misleading or incomplete. The experiments were performed on a test set of 20 stories and involved six participants divided into two groups of three people. Group 1 performed Task 1 on stories 1-10 of the testing set and Group 2 performed this task on stories 11-20. During Task 2 Group 1 worked on stories 11-20 and Group 2 ? on stories 1-10. By adjusting a number of system parameters, we produced four different summaries per story. All four versions were compared with human-made summaries using sentence overlap-based measures. However, because the experiments are rather time consuming, it was not possible to evaluate more than one set of summaries using human judgments (Task 2). That is 
10
Table 1. Inter-judge agreement. Statistic Group 1 Group 2 Average  Cohen (4) 0.50 0.34 0.42 Cohen (3) 0.51 0.34 0.42 PABAK (4) 0.88 0.85 0.87 PABAK (3) 0.89 0.86 0.87 ICC (4) 0.80 (0.78, 0.82) 0.67 (0.64, 0.70) 0.73 (0.71, 0.76) ICC (3) 0.76 (0.74, 0.80) 0.6 (0.56, 0.64) 0.68 (0.65, 0.72)  
why only summaries generated using the coarse-grained dataset and manually composed rules were evaluated in Task 2. We selected this version because the differences between this set of summaries and gold-standard summaries are easiest to interpret. That is to say, decisions based on a set of rules employing a smaller number of parameters are easier to track than those taken using machine learning or more elaborate rules. On average, the subjects reported that completing both tasks required between 15 and 35 hours of work. Four out of six subjects were native speakers of English. Two others had a near-native and very good levels of English respectively. The participants were given the data in form of files and had four weeks to complete the tasks. 3.1 Creating Gold-Standard Summaries: Task 1 During this task each participant had to create extract-based summaries for 10 different stories. The criteria (making a summary indicative rather than informative) were explained and one example of an annotated story shown. The instructions for these experiments are available at <http://www.site.uottawa.ca/~ankazant/instructions.zip>. Table 1 presents several measures of agreement between judges within each group and with the first author of this paper (included in the agreement figures because this person created the initial training data and test data for the preliminary experiments). The measurement names are displayed in the first column of Table 1. Cohen denotes Cohen?s kappa (Cohen 1960). PABAK denotes Prevalence and Bias Adjusted Kappa (Bland and Altman 1986). ICC denotes Intra-class Correlation Coefficient (Shrout and Fleiss 1979). The numbers 3 and 4 state whether the statistic is computed only for 3 subjects participating in the evaluation or for 4 subjects (including the first author of the paper). 
As can be seen in Table 1, the agreement statistics are computed for each group separately. This is because the sets of stories that they annotated are disjoint. The column Average provides an average of these figures to give a better overall idea. Cohen?s kappa in its original form can only be computed for a pair of raters. For this reason we computed it for each possible pair-wise combination of raters within a group and then the numbers were averaged. The PABAK statistic was computed in the same manner using Cohen?s kappa as its basis. ICC is the statistic that measures inter-rater agreement and can be computed for more than 2 judges. It was computed for all 3 or 4 raters at the same time. ICC was computed for a two-way mixed model and measures the average reliability of ratings taken together. The numbers in parentheses are confidence intervals for 99% confidence.  We compute three different agreement measures because each of these statistics has its weakness and distorts the results in a different manner. Cohen?s kappa is known to be a pessimistic measurement in the presence of a severe class imbalance, as is the case in our setting (Sim and Wright 2005). PABAK is a measure that takes class imbalance into account, but it is too optimistic because it artificially removes class imbalance present in the original setting. ICC has weaknesses similar to Cohen?s kappa (sensitivity to class imbalance). Besides, it assumes that the sample of targets to be rated (sentences in our case) is a random sample of targets drawn from a larger population. This is not 
11
Table 2. Sentence overlap between computer- and human-made summaries. Majority gold-standard. Dataset Prec. Rec. F LEAD  25.09 30.49 27.53 LEAD CHAR  28.14 33.18 30.45 Rules, coarse-grained 34.14 44.39 38.60 Rules, fine-gr. 39.27 50.00 43.99 Machine learning, coarse-gr. 35.55 40.81 38.00 ML, fine-gr. 37.97 50.22 43.22  
Figure 2. Fragments of summaries produced by 3 annotators for The Cost of Kindness by Jerome K Jerome. Annotator A. The Rev. Augustus Cracklethorpe would be quitting Wychwood-on-the-Heaththe the following Monday, never to set foot [?] in the neighbourhood again. The Rev. Augustus Cracklethorpe, M.A., might possibly have been of service to his Church in, say, [?] some mission station far advanced amid the hordes of heathendom. In picturesque little Wychwood-on-the-Heath [?] these qualities made only for scandal and disunion. Churchgoers who had not visited St. Jude's for months had promised themselves the luxury of feeling they were listening to the Rev. Augustus Cracklethorpe for the last time. The Rev. Augustus Cracklethorpe had prepared a sermon that for plain speaking and directness was likely to leave an impression. Annotator B. The Rev. Augustus Cracklethorpe would be quitting Wychwood-on-the-Heaththe the following Monday, never to set foot [?] in the neighbourhood again. The Rev. Augustus Cracklethorpe, M.A., might possibly have been of service to his Church in, say, [..] some mission station far advanced amid the hordes of heathendom. What marred the entire business was the impulsiveness of little Mrs. Pennycoop. Mr. Pennycoop, carried away by his wife's eloquence, added a few halting words of his own. Other ladies felt it their duty to show to Mrs. Pennycoop that she was not the only Christian in Wychwood-on-the-Heath. Annotator C. The Rev. Augustus Cracklethorpe would be quitting Wychwood-on-the-Heath the following Monday, never to set foot [?] in the neighbourhood again. The Rev. Augustus Cracklethorpe, M.A., might possibly have been of service to his Church in, say, [?] some mission station far advanced amid the hordes of heathendom. For the past two years the Rev. Cracklethorpe's parishioners [?] had sought to impress upon him, [..] their cordial and daily-increasing dislike of him, both as a parson and a man. The Rev. Augustus Cracklethorpe had prepared a sermon that for plain speaking and directness was likely to leave an impression. The parishioners of St. Jude's, Wychwood-on-the-Heath, had their failings, as we all have. The Rev. Augustus flattered himself that he had not missed out a single one, and was looking forward with pleasurable anticipation to the sensation that his remarks, from his "firstly" to his "sixthly and lastly," were likely to create.     
necessarily the case as the corpus was not compiled randomly. We hope that these three measures, although insufficient individually, provide an adequate understanding of inter-rater agreement in our evaluation. We note that the average overlap (intersection) between judges in each group is 1.8% out of 6% of summary-worthy sentences. All of these agreement measures and, in fact, all measures based on computing sentence overlap are inherently incomplete where fiction is concerned because any two different sentences are not necessarily ?equally different?. The matter is exemplified in Figure 2. It displays 
segments of summaries produced for the same story by three different annotators. Computing Cohen?s kappa between these fragments gives agreement of 0.521 between annotators A and B and 0.470 between annotators A and C. However, a closer look at these fragments reveals that there are more differences between summaries A and B than between summaries A and C. This is because many of the sentences in summaries A and C describe the same information (personal qualities of Rev. Cracklethorpe) even though they do not overlap. On the other hand, sentences from summaries A and B are not only distinct; they ?talk? about different facts. This problem is not unique to fiction, but in this context it is more acute because literary texts exhibit more redundancy. Tables 2-4 show the results of comparing four different versions of computer-made summaries against gold-standard summaries produced by humans. The tables also display the results of two baseline algorithms. The LEAD baseline refers to the version of summaries produced by selecting the first 6% of sentences in each story. LEAD CHAR baseline is obtained by selecting first 
12
Table 3. Sentence overlap between computer- and human-made summaries. Union gold-standard. Dataset Prec. Rec. F LEAD 36.53 17.97 24.09 LEAD CHAR 44.49 21.23 28.75 Rules, coarse-grained 52.41 30.96 38.92 Rules, fine-gr. 56.77 31.22 40.28 Machine learning, coarse-gr. 51.17 23.76 32.47 ML, fine-gr. 55.59 29.76 38.77  
Table 4. Sentence overlap between computer- and human-made summaries. Intersection gold-standard. Dataset Prec. Rec. F LEAD 12.55 37.36 18.78 LEAD CHAR 15.97 46.14 23.73 Rules, coarse-grained 19.66 62.64 29.92 Rules, fine-gr. 23.10 76.92 35.53 Machine learning, coarse-gr. 19.14 53.85 28.24 ML, fine-gr. 21.36 69.23 32.64  
Table 5. Answers to factual questions. Id Question After summary only After reading the original  Mean Std. dev Mean Std. dev. Q1, Q7 Please list up to 3 main characters in this story, in the order of importance (scale: -1 to 3) 2.28 0.64 2.78 0.45 Q2, Q8 State where this story takes place. Be as specific as possible (scale: -1 to 3) 1.78 1.35 2.60 0.91 Q3, Q9 Select a time period when this story takes place.(scale: 0 or 1) 0.53 0.50 0.70 0.46   
6% of sentences that contain a mention of an important character. The improvements over the baselines are significant with 99% confidence in all cases. By combining summaries created by human annotators in different ways we create three distinct gold-standard summaries. The majority gold-standard summary contains all sentences that were selected by at least two judges. It is the most commonly accepted way of creating gold-standard summaries and it is best suited to give an overall picture of how similar computer-made summaries are to man-made ones. The union gold standard is obtained by considering all sentences that were judged summary-worthy by at least one judge. Union summaries provide a more relaxed measurement. Precision for the union gold standard gives one an idea of how many irrelevant sentences a given summary contains (sentences not selected by any of three judges are more likely to prove irrelevant). The intersection summaries are obtained by combining sentences that all three judges deemed to be important. Intersection gold standard is the strictest way to measure the 
goodness of a summary. Recall for intersection gold standard tells one how many of the most important sentences were included in summaries by the system (sentences selected by all three judges are likely to be the most important ones). It should be noted, however, that the numbers in Tables 2-4 do not give a complete picture of the quality of the summaries for the same reason that the agreement measures do not reveal fully the extent of inter-judge agreement: sentences that are not part of the reference summaries are not necessarily equally unsuitable for inclusion in the summary. 3.2 Human Judgment of Computer-Made Summaries: Task 2 In order to evaluate one summary in Task 2, a participant had to read it and to answer six questions using the summary as the only source of information. The participant was then required to read the original story and to answer another six questions. The questions asked before and after reading the original were the same with one exception: question Q4 was replaced by Q11 (see Table 6.) The subjects were asked not to correct the answers after the fact. 
13
Table 8. ANOVA F-values between sentence overlap measures and human judgments. Question Prec. Rec. F Q1(main characters) 0.60 0.61 0.58 Q2(location) 2.58 1.94 2.36 Q3(time) 1.11 0.67 0.97 Q4(readability) 2.10 0.90 1.60 Q5(relevance) 4.55 3.75 4.28 Q10 (relevance) 6.33 3.46 5.15 Q11(completeness) 3.11 4.22 3.43 Q12(helpfulness) 4.53 2.54 3.72  
Table 7. Spearman rank correlation coefficient between sentence overlap measures and human judgments. Question Prec. Rec. F Q1(main characters) 0.09 0.29 0.17 Q2(location) 0.21 0.18 0.22 Q3(time) 0.38 0.28 0.34 Q4(readability) 0.47 0.31 0.50 Q5(relevance) 0.31 0.19 0.34 Q10(relevance) 0.60 0.40 0.59 Q11(completeness) 0.40 0.29 0.40 Q12(helpfulness) 0.59 0.41 0.61  
Table 6. Answers to subjective questions. Id Question (scale: 1 to 6) After summary only After reading the original   Mean Std. dev Mean Std. dev. Q4 How readable do you find this summary? 4.43 1.39 N/A N/A Q5, Q10 How much irrelevant information does this summary contain? 4.27 1.41 4.51 1.16 Q11 How complete is the summary? N/A N/A 4.53 1.25 Q6, Q12 How helpful was this summary for deciding whether you would like to read the story or not? 4.52 1.37 4.6 1.21    Three of the questions were factual and three others ? subjective. Table 5 displays the factual questions along with the resulting answers. The participants had to answer questions Q1 and Q2 in their own words and question Q3 was a multiple-choice question where a participant selected the century when the story took place. Q1 and Q2 were ranked on a scale from -1 to 3. A score of 3 means that the answer was complete and correct, 2 ? slightly incomplete, 1 ? very incomplete, 0 ? a subject could not find the answer in the text and -1 if the person answered incorrectly. Q3 was ranked on a binary scale (0 or 1). Questions Q3-Q7 asked the participants to pronounce a subjective judgment on a summary. These were multiple-choice questions where a participant needed to select a score from 1 to 6, with 1 indicating a strong negative property and 6 indicating a strong positive property. The questions and results appear in Table 6. The results displayed in Tables 5 and 6 suggest that the subjects can answer simple questions based on the summaries alone. They also seem to indicate that the subjects found the summaries quite helpful. It is interesting to note that even after reading 
complete stories the subjects are not always capable of answering the factual questions with perfect precision. 3.3 Putting Sentence Overlap and Human Judgment Together In order to check whether the two types of statistics measure the same or different qualities of the summaries, we explored whether the two are correlated. Table 7 displays the values of Spearman rank correlation coefficient between median values of answers for questions from Task 2 and measurements obtained by comparing computer-made summaries against the majority gold-standard summaries. All questions, except Q10 (relevance) and Q11 (completeness) are those asked and answered using the summary as the only source of information. Sentence overlap values (F-score, precision and recall) were discretized (banded) in order to be used in this test. These results are based on the values obtained for 20 stories in the test set ? a relatively small sample ? which prohibits drawing definite conclusions. However, in most cases the correlation coefficient between human opinions and sentence overlap measurements is below the cut-off 
14
value with 99% confidence, which is 0.57 (the exceptions are highlighted). This suggests that in our case the measurements using sentence overlap as their basis are not correlated with the opinions of subjects about the summaries. We also performed a one-way ANOVA test using human judgments as independent factors and sentence-overlap based measures as dependent variables. The results are in line with those obtained using Spearman coefficient. They are shown in Table 8. The F-values which are statistically significant with 99% confidence are highlighted (the cut-off value for questions Q4-Q12 is 4.89, for Q1 and Q2 ? 6.11 and for Q3 ? 8.29). 4 Conclusions and Future Work This paper presented an experimental way of evaluating automatically produced summaries of literary short stories. In the course of our experiments we have remarked a few issues pertinent to evaluating summaries of short fiction. Firstly, higher degree of redundancy of sentences in texts makes measures based on sentence overlap not very enlightening when evaluating extracted summaries. Secondly, at least in our corpus, the sentence overlap-based measures do not correlate well with those measuring opinions of humans about summaries. This work is exploratory, and as such raises more questions than it answers. In order to evaluate summaries of literary works in a meaningful and reliable way one needs to define criteria which make such summaries suitable or not suitable for a particular purpose. We will explore this issue in our future work. We also intend to apply the pyramid method of evaluating summaries to extracted summaries produced by the human annotators. 5 Acknowledgements The authors would like to express their gratitude to Connexor Oy and especially to Atro Voutilainen for their kind permission to use Connexor Machinese Syntax parser free of charge for research purposes. 
References J. Bland and D. Altman. 1986. Statistical methods for assessing agreement between two methods of clinical measurement. Lancet. 1986; 1(8476):307-310. J. Cohen, 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement. 1960; 20:37-46.. R. Huddleston and G. Pullum. 2002. The Cambridge Grammar of the English Language Usage, 123-124. Cambridge University Press. A. Kazantseva. 2006. Proc Student Research Workshop at EACL 2006, 55-63. I. Mani. 2001. Automatic Summarization. John Benjamins B.V. A. Nenkova and R. Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. Proc Human Language Technology Conference and NAACL. J. Quinlan. 1992. C4.5: Programs for Machine Learning, Morgan Kaufmann Pub., San Mateo, CA. P. Shrout and J. Fleiss. Intraclass correlations: uses in assessing rater reliability. Psychological Bulletin 1979; 86:420?428 J. Sim and C. Wright. 2005. The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements. Physical Therapy, 2005(85-3): 257-268. P. Tapanainen and T. J?rvinen. 1997. A non-projective dependency parser. Proc 5th Conference on Applied Natural Language Processing, 64-71. H. Van Halteren and S. Teufel. 2003. Examining the consensus between human summaries: initial experiments with factoid analysis. HLT/NAACL-2003 Workshop on Automatic Summarization, 57-64. 
15
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 37?47, Dublin, Ireland, August 23-29 2014.
Hierarchical Topical Segmentation with Affinity Propagation
Anna Kazantseva & Stan Szpakowicz
School of Electrical Engineering and Computer Science
University of Ottawa
Ottawa, Ontario, Canada
{ankazant,szpak}@eecs.uottawa.ca
Abstract
We present a hierarchical topical segmenter for free text. Hierarchical Affinity Propagation for
Segmentation (HAPS) is derived from a clustering algorithm Affinity Propagation. Given a doc-
ument, HAPS builds a topical tree. The nodes at the top level correspond to the most prominent
shifts of topic in the document. Nodes at lower levels correspond to finer topical fluctuations.
For each segment in the tree, HAPS identifies a segment centre ? a sentence or a paragraph which
best describes its contents. We evaluate the segmenter on a subset of a novel manually segmented
by several annotators, and on a dataset of Wikipedia articles. The results suggest that hierarchical
segmentations produced by HAPS are better than those obtained by iteratively running several
one-level segmenters. An additional advantage of HAPS is that it does not require the ?gold
standard? number of segments in advance.
1 Introduction
When an NLP application works with a document, it may benefit from knowing something about this
document?s high-level structure. Text summarization (Haghighi and Vanderwende, 2009), question an-
swering (Oh et al., 2007) and information retrieval (Ponte and Croft, 1998) are some of the examples
of such applications. Topical segmentation is a lightweight form of such structural analysis: given a
sequence of sentences or paragraphs, split it into a sequence of topical segments, each characterized by
a certain degree of topical unity. This is particularly useful for texts with little structure imposed by the
author, such as speech transcripts, meeting notes or literature.
The past decade has witnessed significant progress in the area of text segmentation. Most of the topical
segmenters (Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008; Kazantseva and Szpakowicz,
2011; Misra et al., 2011; Du et al., 2013) can only produce single-level segmentation, a worthy endeavour
in and of itself. Yet, to view the structure of a document linearly, as a sequence of segments, is in certain
discord with most theories of discourse structure, where it is more customary to consider documents as
trees (Mann and Thompson, 1988; Marcu, 2000; Hernault et al., 2010; Feng and Hirst, 2012) or graphs
(Wolf and Gibson, 2006). Regardless of the theory, we hypothesize that it may be useful to have an idea
about fluctuations of topic in documents beyond the coarsest level. It is the contribution of this work that
we develop such a hierarchical segmenter, implement it and do our best to evaluate it.
The segmenter described here is HAPS ? Hierarchical Affinity Propagation for Segmentation. It is
closely based on a graphical model for hierarchical clustering called Hierarchical Affinity Propagation
(Givoni et al., 2011). It is a similarity-based segmenter. It takes as input a matrix of similarities between
atomic units of text in the sequence to be segmented (sentences or paragraphs), the desired number of
levels in the topical tree and a preference value for each data point and each level. This value captures
a priori belief about how likely it is that this data point is a segment centre at that level. The preference
values also control the granularity of segmentation: how many segments are to be identified at each level.
The output is a topical tree. For each segment at every level, HAPS also finds a segment centre, a data
point which best describes the segment.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
37
The objective function maximized by the segmenter is net similarity ? the sum of similarities between
all segment centres and their children for all levels of the tree. This function is similar to the objective
function of the well-known k-means algorithm, except that here it is computed hierarchically.
It is not easy to evaluate HAPS. We are not aware of comparable hierarchical segmenters other than
that in (Eisenstein, 2009) which, unfortunately, is no longer publicly available. Therefore we compared
the trees built by HAPS to the results of running iteratively two state-of-the-art flat segmenters. The
results are compared on two datasets. A set of Wikipedia articles was automatically compiled by Carroll
(2010). The other set, created to evaluate HAPS, consists of nine chapters from the novel Moonstone by
Wilkie Collins. Each chapter was annotated for hierarchical structure by 3-6 people.
The evaluation is based on two metrics, windowDiff (Pevzner and Hearst, 2002) and evalHDS (Car-
roll, 2010). Both metrics are less then ideal. They do not give a complete picture of the quality of
topical segmentations, but the preliminary results suggest that running a global model for hierarchical
segmentation produces better results then iteratively running flat segmenters. Compared to the baseline
segmenters, HAPS has an important practical advantage. It does not require the number of segments as
an input; this requirement is customary for most flat segmenters.
We also made a rough attempt to evaluate the quality of the segment centres identified by HAPS. Using
20 chapters from several novels of Jane Austen, we compared the centres identified for each chapter
against summaries produces by a recent automatic summarizer CohSum (Smith et al., 2012). The basis
of comparison was the ROUGE metric (Lin, 2004). While far from conclusive, the results suggest that
segment centres identified by HAPS are rather comparable with the summaries produced by an automatic
summarizer.
A Java implementation of HAPS and the corpus of hierarchical segmentations for nine chapters of
Moonstone are publicly available. We consider these to be the main contributions of this research.
2 Related work
Most work on topical text segmentation has been done for single-level segmentation. Contemporary
approaches usually rely on the idea that topic shifts can be identified by finding shifts in the vocabulary
(Youmans, 1991). We can distinguish between local and global models for topical text segmentation.
Local algorithms have a limited view of the document. For example, TextTiling (Hearst, 1997) operates
by sliding a window through the input sequence and computing similarity between adjacent units. By
identifying ?valleys? in similarities, TextTiling identifies topic shifts. More recently, Marathe (2010)
used lexical chains and Blei and Moreno (2001) used Hidden Markov Models. Such methods are usually
very fast, but can be thrown off by small digressions in the text.
Among global algorithms, we can distinguish generative probabilistic models and similarity-based
models. Eisenstein and Barzilay (2008) model a document as a sequence of segments generated by latent
topic variables. Misra et al. (2011) and Du et al. (2013) have similar models. Malioutov and Barzilay
(2006) and (Kazantseva and Szpakowicz, 2011) use similarity-based representations. Both algorithms
take as input a matrix of similarities between sentences of the input document; the former uses graph
cuts to find cohesive segments, while the latter modifies a clustering algorithm to perform segmentation.
Research on hierarchical segmentation has been more scarce. Yaari (1997) produced hierarchical
segmentation by agglomerative clustering. Eisenstein (2009) used a Bayesian model to create topical
trees, but the system is regrettably no longer publicly available. Song et al. (2011) develop an algorithm
for hierarchical segmentation which iteratively splits a document in two at a place where cohesion links
are the weakest. A second pass transforms a deep binary tree into a shallow and broad structure.
Any flat segmenter can certainly be used iteratively to create trees of segments by subdividing each
segment, but this may be problematic. Topical segmenters are not perfect, so running them iteratively is
likely to compound the error. Most segmenters also require the number of segments as an input. This
estimate is feasible for flat segmentation. To know in advance the number of segments and sub-segments
at each level is not a realistic requirement when building a tree.
This work describes a hierarchical model of text segmentation. It takes a global view of the document
and of the topical hierarchy. Each iteration attempts to find the best assignment of segments for the
38
whole tree. It does not need to know the exact number of segments. Instead, it takes a more abstract
parameter, preference values, to specify the granularity of segmentation at each level. For each segment
it also outputs a segment centre, a unit of text which best captures the contents of the segment.
3 Creating a corpus of hierarchical segmentations
Before embarking on the task of building a hierarchical segmenter, we wanted to study how people
perform such a task. We also needed a benchmark corpus which could be used to evaluate the quality of
segmentations produced by HAPS.
To this end, we annotated nine chapters of the novel Moonstone for hierarchical structure. We settled
on these data because it is a subset of a publicly available dataset for flat segmentation (Kazantseva
and Szpakowicz, 2012). In our study, each chapter was annotated by 3-6 people (4.8 on average). The
annotators, undergraduate students of English, were paid $50 dollars each.
Procedure. The instructions asked the annotator to read the chapter and split it into top-level segments
according to where there is a perceptible shift of topic. She had to provide a one-sentence description of
what the segment is about. The procedure had to be repeated for each segment all the way down to the
level of individual paragraphs. Effectively, the annotators were building a detailed hierarchical outline
for each chapter.
Metrics. Two different metrics helped estimate the quality of our hierarchical dataset: windowDiff
(Pevzner and Hearst, 2002) and S (Fournier and Inkpen, 2012).
windowDiff is computed by sliding a window across the input sequence and checking, for each window
position, whether the number of reference breaks is the same as the number of breaks in the hypothetical
segmentation. The number of erroneous windows is then normalized by the total number of windows. In
Equation 1, N is the length of the input sequence and k is the size of the sliding window.
windowDiff =
1
N ? k
N?k
?
i=1
(|ref ? hyp| 6= 0) (1)
windowDiff is designed to compare sequences of segments, not trees. That is why we compute it for
each level between each pair of annotators who worked on the same chapter. It should be noted that
windowDiff is a penalty metric: higher values indicate less agreement (windowDiff= 0 corresponds to
two identical segmentations).
The S metric allows us to compare trees and take into account situations when the segmenter places a
boundary at a correct position but at a wrong level. S is an edit-distance metric. It computes the number
of operations necessary to turn one segmentation into another. There are three types of editing operations:
add/delete, transpose and substitute (change the level in the tree). The sum is normalized by the number
of possible boundaries in the sequence. S has an unfortunate downside of being too optimistic, but it
allows the breakdown of error types and it explicitly compares trees.
Unlike windowDiff, S is a similarity metric: higher values correspond to more similar segmentations.
The value of S between two identical segmentations is 1.
S(bs
a
, bs
b
, n) =
1? |boundary distance(bs
a
, bs
b
, n)|
pb(D)
(2)
Here boundary distance(bs
a
, bs
b
, n) is the total number of edit operations needed to turn a segmen-
tation bs
a
into bs
b
, n is the threshold defining the maximum distance of transpositions. pb(D) is the
maximum possible number of edits. Segmentations bs
a
and bs
a
are represented as strings of sets of
boundary positions. For example bs
a
= ({2}, {1,2}, {1,2}) corresponds to a hierarchical segmentation of
a three-unit sequence in the following manner: a segment boundary at level 1 after the first unit, segment
boundaries at levels 1 and 2 after the second unit and the third unit.
Corpus Analysis. On average, the annotators took 3.5 hours to complete the task (? = 1.6). The
average depth of the tree is 3.00 levels (? = 0.65), suggesting that the annotators prefer shallow but broad
structures. Table 1 reports the average breadth of the tree at different levels. In the Table and further
39
in this paper we refer to the bottom level of the tree (i.e., the leaves of the tree or the most fine-grained
level of segmentation) as level 1. In Table 1, level 4 refers to the top level of the tree (the coarsest
segmentations). The values were computed using only the breaks explicitly specified by the annotators
(i.e., we did not assume that a break at a coarse level implies a break at a more detailed level).
The average breadth of the trees at the bottom (level 1) is lower than that at level 2, indicating that only
a small percentage of the entire tree was annotated more than three levels deep. The table also shows the
average values of windowDiff computed for each possible pair of annotators. The values worsen toward
the bottom of the tree, suggesting that the annotators agree more about top-level segments and less and
less about finer fluctuations of topic.
We hypothesize that these shallow broad structures are due to the fact that it is difficult for people to
create deep recursive structures in their mental representations. We do not, however, have any hard data
to support this hypothesis. Many of the annotators specifically commented on the difficulty of the task. 9
out of 23 people included comments ranging from notes about specific places to general comments about
their lack of confidence. 4 annotators found several (specific) passages they had trouble with.
The average value of pairwise S is 0.79. We have noted earlier that the S metric tends to be optimistic
(that is due to its normalization factor) but it provides a breakdown of disagreements between the anno-
tators. According to S, 46.14% of disagreements are errors of omission (some of the annotators did not
include segment breaks where others did), 47.56% are disagreements about the level of segmentation
(the annotators placed boundaries in the same place but at different levels) and only 6.31% are errors
of transposition (the annotators do not agree about the exact placement but place boundaries within 1
position of each other). This distribution is more interesting than the overall value of S. Among other
things, it shows why it is so important to take into account adjacent levels when evaluating topical trees.
4 The HAPS algorithm
1
4.1 Factor graphs
The HAPS segmenter is based on factor graphs, a unifying formalism for such graphical models as
Markov or Bayesian networks. A factor graph is a bi-partite graph with two types of nodes, factor or
function nodes and variable nodes. Each factor node is connected to those variable nodes which are
its arguments. Running the well-known Max-Sum algorithm (Bishop, 2006) on a factor graph finds a
configuration of variables which maximizes the sum of all component functions. This is a message-
passing algorithm. All variable nodes send messages to their factor neighbours (functions in which those
nodes are variables) and all factor nodes send messages to their variable neighbours (their arguments).
A message ?
x?f
sent from a variable node x to a function node f is computed as a sum of all incoming
messages to x, except the message from the recipient function f :
?
x?f
=
?
f
?
?N(x)\f
?
f
?
?x
(3)
N(x) is the set of all function nodes which are x?s neighbours. Intuitively, the message reflects evi-
dence about the distribution of x from all functions which have x as an argument, except the function
corresponding to the receiving node f . A message ?
f(x,...)?x
sent from the factor node f(x, ...) to the
1
The derivation of the HAPS algorithm, quite involved, is unlikely to interest many readers. We only present the bare
minimum of facts about the algorithm, the framework of factor graphs and the derivation of HAPS from the underlying model
of Affinity Propagation. A detailed account appears in (Kazantseva, 2014).
Table 1: Average breadth of manually created topical trees and windowDiff value across different levels
Level Average breadth windowDiff
4 (top) 6.53 0.35
3 17.55 0.46
2 17.63 0.47
1 (bottom) 8.80 0.50
40
Cl?1
1
C
l?1
i
C
l?1
N
e
l?1
1
e
l?1
i
e
l?1
N
E
l?1
1
E
i
l ? 1
E
l?1
N
I
l?1
1
I
l?1
i
I
l?1
N
c
l?1
11
c
l?1
1i
c
l?1
1N
c
l?1
i1
c
l?1
ii
c
l?1
iN
c
l?1
N1
c
l?1
Ni
c
l?1
NN
S
11
l?1
S
l?1
1i
S
1N
l?1
S
l?1
i1
S
l?1
ii
S
l?1
iN
S
l?1
N1
S
l?1
Ni
S
l?1
NN
Level l - 1
C
l
1
C
l
j
C
l
N
e
l
1
e
l
j
e
l
N
E
l
1
E
l
j
E
l
N
I
l
1
I
l
i
I
l
N
c
l
11
c
l
1j
c
l
1N
c
l
i1
c
l
ij
c
l
iN
c
l
N1
c
l
Nj
c
l
NN
S
11
l
S
l
1j
S
1N
l
S
l
i1
S
l
ij
S
l
iN
S
l
N1
S
l
Nj
S
l
NN
Level l
(a) Fragment of the factor graph for levels l ? 1 and l
(b) Types of messages
sent in the HAPS model
Figure 1: Factor graph for HAPS ? Hierarchical Affinity Propagation for Segmentation
variable node x is computed as a maximum of the value of f(x) plus all messages incoming to f(x, ...)
other than the message from the recipient node x:
?
f?x
= max
N(f)\x
(f(x
1
, . . . , x
m
) +
?
x
?
?N(f)\x
?
x
?
?f
) (4)
N(f) is the set of all variable nodes which are f ?s neighbours. The message reflects the evidence about
the distribution of x from function f and its neighbours other than x.
4.2 Hierarchical Affinity Propagation for Segmentation
This work aims to build trees of topical segments. Each segment is characterized by a centre which best
describes its content. The objective function is net similarity, the sum of similarities between all centres
and the data points which they exemplify. The complete sequence of data points is to be segmented at
each level of the tree, subject to the following constraint: centres at each level l, l > 1, must be a subset
of the centres from the previous level l ? 1. Figure 1a shows a fragment of the factor graph describing
HAPS corresponding to levels l and l?1. The tree has L levels, from the root (l = L) down to the leaves
(l = 1). The superscripts of factor and variable nodes denote the level.
At each level, there areN
2
variable nodes c
l
ij
andN variable nodes e
l
j
(N is the number of data points
in the sequence to segment). A variable?s value is 0 or 1: c
l
ij
= 1? the data point i at level l belongs to
the segment centred around data point j; e
l
j
= 1? there is a segment centred around j at level l.
Four types of factor nodes in Figure 1a are I , E, C and S. The I factors ensure that each data point
is assigned to exactly one segment and that segment centres at level l are a subset of those from level
l ? 1. The E nodes ensure that segments are centred around the segment centres in solid blocks (rather
than unordered clusters). The values of I and E are 0 for valid configurations and -? otherwise. The S
factors capture similarities between data points. S
l
ij
= sim(i, j) if c
l
ij
= 1; S
l
ij
= 0 if c
l
ij
= 0.
2
The C
factors handle preferences in an analogous manner. Running the Max-Sum algorithm on the factor graph
in Figure 1a maximizes the net similarity between all segment centres and their children at all levels:
max
{c
l
ij
},{e
l
j
}
S({c
l
ij
}, {e
l
j
}) =
?
i,j,l
S
l
i,j
(c
l
ij
) +
?
i,l
I
l
i
(c
l
i1
, . . . , c
l
iN
, e
l?1
i
) +
?
j,l
E
l
j
(c
l
1j
, . . . , c
l
Nj
, e
l
j
) +
?
j,l
C
l
j
(e
l
j
) (5)
2
The value sim(i, j) is specified in the input matrix.
41
Figure 1b shows a close-up view of the messages that must be sent to find the optimizing configuration
of variables. Messages ?, ?, ?? do not need to be sent explicitly: their values are subsumed by other types
of messages. We only need to compute explicitly and send four types of messages: ?, ?, ? and ? .
Algorithm 1 shows the pseudo-code for the HAPS algorithm.
3
Intuitively, different parts of the update
messages in Algorithm 1 correspond to likelihood ratios between two hypotheses: whether a data point i
is or is not part of a segment centred around another data point j at a given level l. For example, here is
the availability (?) message sent from a potential segment centre j to itself at level l:
?
l
ij
= p
l
j
+ ?
l
j
+
j
max
s=1
(
j?1
?
k=s
?
l
kj
) +
N
max
e=j
(
e
?
k=j+1
?
l
kj
) (6)
Here p
l
j
incorporates the information about the preference value for the data point j at the level l. ?
l
j
brings in the information from the coarser level of the tree. The summand max
j
s=1
(
?
j?1
k=s
?
l
kj
) encodes
the likelihood that there is a segment starting before j given the values of responsibility messages for all
data points i such that i < j ? hence the information from a more detailed level of the tree as well as
the similarities between all data points i (i < j) and j. The summand max
N
e=j
(
?
e
k=j+1
?
l
kj
) does the
same for the tail-end of the segment (all data points i such that i > j).
Complexity analysis. The HAPS model contains N
2
c
l
ij
nodes at each level. In practice, however, the
matrix of similarities SIM does not need to be fully specified. It is customary to compute this matrix
with a large sliding window; the size should be at least twice the anticipated average length. On each
iteration, we need to send L*M*N messages ? and ?, resulting in the complexity O(L*M*N). Here L is
the number of levels, N is the number of data points in the sequence and M (M ? N ) is the size of the
sliding window used for computing similarities. The computation of ? and ? messages is independent
for each row and column respectively, so the algorithm would be easy to parallelize.
Parameter settings. An important advantage of HAPS is that it does not require the number of
segments in advance. Instead, the user needs to set the preference values for each level. However, HAPS
is fairly resistant to changes in preferences and this generic parameter is a convenient knob for fine-tuning
the desired granularity of segmentation, as opposed to specifying the exact number of segments at each
level of the tree. In this work we set preferences uniformly, but it is possible to incorporate additional
knowledge through more discriminative settings.
In all our experiments, preference values are set uniformly for each level of the tree, so effectively
all data points are equally likely to be chosen as segment centres at each level. As a starting point,
the preference value for the most detailed level of the tree should be about approximately equal to the
median similarity value (as specified in the input matrix). A near-zero preference value tends to result in
a medium number of segments and is thus suitable to the middle levels of the tree. A negative preference
value results in a small number of segments and is appropriate for identifying the most pronounced
segment breaks.
5 Experimental evaluation
In order to evaluate the quality of topical trees produced by HAPS, we ran the system on two datasets.
We compared the results obtained by HAPS against topical trees obtained by iteratively running two
high-performance single-level segmenters.
Datasets. We used the Moonstone corpus described in Section 2, and the Wikipedia dataset com-
piled by Carroll (2010). Created automatically from metadata on Web pages, the dataset consists of 66
Wikipedia entries on various topics; the annotations and the results concern sentences. In the Moonstone
corpus we work with paragraphs. To simplify evaluation and interpretation, we produced three-tier trees.
This is in line with the average depths of manual annotations in the Moonstone data.
3
It is not possible to include a detailed derivation of the new update messages in the space allowed here. The interested reader
can find these details in (Kazantseva, 2014). The derivation follows the same logic as (Givoni et al., 2011) and (Kazantseva and
Szpakowicz, 2011).
42
Algorithm 1 Hierarchical Affinity Propagation for Segmentation
1: input: 1) L pairwise similarity matrices {SIM
l
(i, j)}
(i,j)?{1,...,N}
2 ; 2) L preferences p
l
(one per
level l) indicating a priori likelihood of point i being a segment centre at level l
2: initialization: ?i, j : ?
ij
= 0 (set all availabilities to 0)
3: repeat
4: iteratively update ?, ?, ? and ? messages
5:
?i, l : ?
l?1
i
= max[0, ?
ii
?max
k 6=i
(s
l
ik
+ ?
l
ik
)]
6:
?i, j, l : ?
l
ij
=
?
?
?
?
?
min(0, ?
l
i
)?max
k 6=i
(s
l
ik
+ ?
l
ik
) if i = j
s
l
ij
+ min[max(0,??
l
i
)? ?
l
ii
,?max
k*i,j
(s
l
ik
+ ?
l
ik
)] if i 6= j
7:
?i, j, l : ?
l
ij
=
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
p
l
j
+ ?
l
j
+
j
max
s=1
(
j?1
?
k=s
?
l
kj
) +
N
max
e=j
(
e
?
k=j+1
?
l
kj
) if i = j
?
l
ij,i<j
= min[(
i
max
s=1
i?1
?
k=s
?
l
kj
+
j
?
k=i+1
?
l
kj
+
N
max
e=j
e
?
k=j+1
?
l
kj
) + p
l
j
+ ?
l
j
,
i
max
s=1
i?1
?
k=s
?
l
kj
+
j
min
s=i+1
s?1
?
k=i+1
?
l
kj
] if i < j
min[(
j
max
s=1
j?1
?
k=s
?
l
kj
+
i?1
?
k=j
?
l
kj
+
N
max
e=i
e
?
k=i+1
?
l
kj
) + p
l
j
+ ?
l
j
,
i?1
min
e=j
i?1
?
k=e+1
?
l
kj
+
N
max
e=i
e
?
k=i+1
?
l
kj
]
8:
?j, l : ?
l+1
j
= p
l
(j) + ?
l
jj
+
j
max
s=1
(
j?1
?
k=s
?
l
kj
) +
N
max
e=j
(
e
?
k=j+1
?
l
kj
)
9: until convergence
10: compute optimal configuration: ?i, j i is in the segment centred around j iff ?
ij
+ ?
ij
> 0
11: output: segment centres and segment boundaries
Baselines. Regrettably, we are not aware of another publicly available hierarchical segmenter. That is
why we used as baselines two recent flat segmenters: MCSeg (Malioutov and Barzilay, 2006) and BSeg
(Eisenstein and Barzilay, 2008). Both were first run to produce top-level segmentations. Each segment
thus computed was a new input document for segmentation. We repeated the procedure twice to obtain
three-tiered trees. MCSeg cannot be run without knowing the number of segments in advance. Therefore,
on each iteration, we had to specify the correct number of segments in the reference segmentation. BSeg
does not need the exact number of segments, so we had two settings: with and without knowing the
number of segments.
Evaluation metrics. We did our best to obtain a realistic picture of the results, but each metric has
its shortcomings. We compared topical trees using windowDiff and evalHDS (Carroll, 2010). Both
metrics are penalties: the higher the values, the worse the hypothetical segmentation. evalHDS computes
windowDiff for each level of the tree in isolation and weighs the errors according to their prominence in
43
the tree. We computed evalHDS using the publicly available Python implementation (Carroll, 2010).
4
When computing windowDiff, we treated each level of the tree as a separate segmentation and com-
pared each hypothetical level against a corresponding level in the reference segmentation.
To ensure that evaluations are well-defined at all levels, we propagated the more pronounced reference
breaks to lower levels (in both annotations and in the results). In effect, the whole sequence is segmented
at each level ? otherwise windowDiff would not be not well-defined. Conceptually this means that if
there is a topical shift of noticeable magnitude (e.g., at the top level), there must be at least a shift of less
pronounced magnitude (e.g., at an intermediate level).
The Moonstone dataset has on average 4.8 annotations per chapter. It is not obvious how to combine
these multiple annotations. We evaluated separately each hypothetical segmentation against each avail-
able gold standard. We report the averages across all annotators ? for both evalHDS and windowDiff ?
per level.
Preprocessing. The representations used by HAPS and the MCSeg are very similar. Both systems
compute a matrix of similarities between atomic units of the document (sentences or paragraphs). Each
unit was represented as a bag of words. The vectors were further weighted by the tf.idf value of the term
and also smoothed in the same manner as in (Malioutov and Barzilay, 2006). We computed cosine simi-
larity between vectors corresponding to each sentence or paragraph. We used tenfold cross-validation on
the Wikipedia dataset and fourfold cross-validation on the smaller Moonstone data.
The quality of the segment centres. In addition to finding topical shifts, HAPS identifies segment
centres ? sentences or paragraphs which best capture what each segment is about. In order to get a rough
estimate of the quality of the centres, we extracted paragraphs identified as segment centres at the second
(middle) level of HAPS trees. These pseudo-summaries were then compared to summaries created by
an automatic summarizer CohSum. We used ROUGE-1 and ROUGE-L metrics (Lin, 2004) as a basis
for comparison. CohSum identifies the most salient sentences in a document by running a variant of the
TextRank algorithm (Mihalcea and Tarau, 2004) on the entire document. In addition to using lexical
similarity, the summarizer takes into account coreference links between sentences. We ran CohSum at
10% compression rate.
The summarization experiment was performed on the Moonstone corpus. We also collected 20 chap-
ters from several other XIX century novels and used it in a separate experiment. The ROUGE package
requires manually written summaries to compare with the automatically created ones. We obtained the
summaries from the SparkNotes website.
5
6 Results and discussion
Table 2 shows the results of comparing HAPS with two baseline segmenters using windowDiff and
evalHDS. HAPS was run without knowing the number of segments. MCSeg required that the exact
number be specified. BSeg was tested with and without that parameter. Therefore, rows 3 and 4 in
Table 2 correspond to baselines considerably more informed than HAPS. This is especially true of the
bottom levels where sometimes knowing the exact number of segments unambiguously determines the
only possible segmentation.
The results suggest that HAPS performs well on the Moonstone data even when compared to more
informed baselines. This applies to both metrics, windowDiff and evalHDS. BSeg performs slightly
better at the bottom levels of the tree when it has the information about the exact number of segments.
We hypothesize that the advantage may be due to this additional information, especially when segmenting
already small segments at level 1 into a predefined number of segments. Another explanation may be
that when using windowDiff as the evaluation metric, HAPS was fine-tuned so as to maximize the value
of windowDiff at the top level, effectively disregarding lower levels of segmentation.
4
When working with the Moonstone dataset, we realized that the software produces very low values, almost too good to be
true. That is because the bottommost annotations are very fine-grained. Sometimes each paragraph corresponds to a separate
segment. This causes problems for the software. So, when we report evalHDS values for the Moonstone dataset, we only
consider two top levels of the tree, disregarding the leaves. We also remove the ?too good to be true? outliers, though the ?bad?
tail is left intact. We applied the same procedure to all three segmenters, only for the Moonstone dataset.
5
http://www.sparknotes.com/
44
Level Moonstone Wikipedia Moonstone Wikipedia
windowDiff windowDiff evalHDS evalHDS
HAPS
3 (top) 0.337 (? 0.060) 0.421 (? 0.060) 0.353 0.450
2 (middle) 0.422 (? 0.060) 0.447 (? 0.070) (? 0.072) (? 0.015)
1 (bottom) 0.556 (? 0.070) 0.617 (? 0.080)
MinCutSeg-iter.
3 (top) 0.375 0.440 (? 0.075) 0.377 0.444
2 (middle) 0.541 0.424 (? 0.064) (? 0.002) (? 0.002)
segm. known 1 (bottom) 0.601 0.471 (? 0.057)
BayesSeg-iter.
3 (top) 0.353 (? 0.071) 0.391 (? 0.070) 0.367 0.370
2 (middle) 0.406 (? 0.053) 0.344 (? 0.033) (? 0.089) (? 0.019)
segm. known 1 (bottom) 0.504 (? 0.064) 0.354 (? 0.033)
BayesSeg-iter.
3 (top) 0.600 (? 0.071) 0.637 (? 0.070) 0.453 0.437
2 (middle) 0.447 (? 0.053) 0.877 (? 0.033) (? 0.089) (? 0.022)
segm. unknown 1 (bottom) 0.545 (? 0.064) 0.952 (? 0.033)
Table 2: Evaluation of HAPS and iterative versions of APS, MCSeg and BSeg using windowDiff per level
(mean windowDiff and standard deviation for cross-validation)
Moonstone corpus Austen corpus
ROUGE-1 ROUGE-L ROUGE-1 ROUGE-L
Segment centres 0.341 0.321 0.291 0.301
(0.312, 0.370) (0.298, 0.346) (0.272, 0.311) (0.293, 0.330)
CohSum 0.294 0.269 0.305 0.307
summaries (0.243, 0.334) (0.226, 0.306) (0.290, 0.320) (0.287, 0.327)
Table 3: HAPS segment centres compared to CohSum summaries: ROUGE scores and 95% confidence
intervals
All segmenters perform worse on the Wikipedia dataset. Using that scale, informed BSeg performs the
best, but it is interesting to note a significant drop in performance when the number of segments is not
specified.
Overall, HAPS appears to perform better than, or comparably to, the more informed baselines, and
much better than the baseline not given information about the number of segments.
We also made a preliminary attempt to evaluate the quality of segment centres by comparing them to
the summaries created by the CohSum summarizer. In addition to working with the Moonstone corpus,
we collected a corpus of 20 chapters from various novels by Jane Austen.
Table 3 shows the results. They are not conclusive because there is no evidence that ROUGE scores
correlate with the quality of automatically created summaries for literature. According to the scores in
Table 3, however, the summaries created by CohSum cannot be distinguished from simple summaries
composed of segment centres identified by HAPS. We interpret this as a sign that the centres identified
by HAPS are approximately as informative as those created by an automatic summarizer.
7 A brief conclusion
This paper presented HAPS, a hierarchical segmenter for free text. Given an input document, HAPS
creates a topical tree and identifies a segment centre for each segment. One of the advantages of HAPS
is that it does not require the exact number of segments in advance. Instead, it estimates the number
of segments given information on generic preferences with regard to segmentation granularity. We also
created a corpus of hierarchical segmentations which has been annotated by 3-6 people per chapter.
A Java implementation of HAPS and the Moonstone corpus are publicly available.
6
Acknowledgements
We thank Chris Fournier (for computing S values using a beta version of SegEval software for hierar-
chical datasets), Lucien Carrol (for help and discussion of the evalHDS software and representation) and
Christian Smith (for allowing us to use his implementation of CohSum).
6
http://www.eecs.uottawa.ca/
?
ankazant/
45
References
Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.
David Blei and Pedro Moreno. 2001. Topic segmentation with an aspect hidden Markov Model. In Proceedings of
the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 343?348.
Lucien Carroll. 2010. Evaluating Hierarchical Discourse Segmentation. In Human Language Technologies: The
2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,
pages 993?1001.
Lan Du, Wray Buntine, and Mark Johnson. 2013. Topic Segmentation with a Structured Topic Model. In
Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 190?200, Atlanta, Georgia.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian Unsupervised Topic Segmentation. In Proceedings of the
2008 Conference on Empirical Methods in Natural Language Processing, pages 334?343, Honolulu, Hawaii.
Jacob Eisenstein. 2009. Hierarchical Text Segmentation from Multi-Scale Lexical Cohesion. In Proceedings of
the 2009 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 353?361. The Association for Computational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level Discourse Parsing with Rich Linguistic Features. In
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 60?68, Jeju Island, Korea, July. Association for Computational Linguistics.
Chris Fournier and Diana Inkpen. 2012. Segmentation Similarity and Agreement. In Proceedings of the 2012
Conference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 152?161, Montr?eal, Canada.
Inmar E. Givoni, Clement Chung, and Brendan J. Frey. 2011. Hierarchical Affinity Propagation. In Uncertainty
in AI, Proceedings of the Twenty-Seventh Conference (2011), pages 238?246.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring Content Models for Multi-Document Summarization. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 362?370, Boulder, Colorado, June.
Marti A. Hearst. 1997. TextTiling: segmenting text into multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Hugo Hernault, Helmut Prendinger, David A. duVerlea, and Mitsuru Ishizuka. 2010. HILDA: A Discourse Parser
Using Support Vector Machine Classification. Dialogue and Discourse, 3:1?33.
Anna Kazantseva and Stan Szpakowicz. 2011. Linear Text Segmentation Using Affinity Propagation. In Proceed-
ings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 284?293, Edinburgh,
Scotland.
Anna Kazantseva and Stan Szpakowicz. 2012. Topical Segmentation: a Study of Human Performance and a New
Measure of Quality. In Proceedings of the 2012 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 211?220, Montr?eal, Canada.
Anna Kazantseva. 2014. Topical Structure in Long Informal Documents. Ph.D. thesis, University of Ottawa.
?http://www.eecs.uottawa.ca/
?
ankazant/?.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of summaries. In Text Summarization
Branches Out, Proceedings of the ACL Workshop, pages 74?81, Barcelona, Spain.
Igor Malioutov and Regina Barzilay. 2006. Minimum Cut Model for Spoken Lecture Segmentation. In Pro-
ceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 25?32, Sydney, Australia.
William C. Mann and Sandra A. Thompson. 1988. Rhetorical Structure Theory: Toward a functional theory of
text organization. Text, 8(3):243?281.
Meghana Marathe. 2010. Lexical Chains Using Distributional Measures of Concept Distance. Master?s thesis,
University of Toronto.
46
Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. MIT Press, Cambridge,
Mass.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Dekang Lin and Dekai Wu, editors,
Proceedings of the Conference on Empirical Methods in Natural Language Processing 2004, pages 404?411,
Barcelona, Spain.
Hemant Misra, Franc?ois Yvon, Olivier Capp?e, and Joemon M. Jose. 2011. Text segmentation: A topic modeling
perspective. Information Processing and Management, 47(4):528?544.
Hyo-Jung Oh, Sung Hyon Myaeng, and Myung-Gil Jang. 2007. Semantic passage segmentation based on sentence
topics for question answering. Information Sciences, an International Journal, 177:3696?3717.
Lev Pevzner and Marti A. Hearst. 2002. A Critique and Improvement of an Evaluation Metric for Text Segmenta-
tion. Computational Linguistics, 28(1):19?36.
Jay M. Ponte and W. Bruce Croft. 1998. A Language Modeling Approach to Information Retrieval. In SIGIR
?98: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, pages 275?281, Melbourne, Australia.
Christian Smith, Henrik Danielsson, and Arne Jnsson. 2012. A more cohesive summarizer. In 24th International
Conference on Computational Linguistics, Proceedings of COLING 2012: Posters, pages 1161?1170, Mumbai,
India.
Fei Song, William M. Darling, Adnan Duric, and Fred W. Kroon. 2011. An iterative approach to text segmentation.
In Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR?11, pages 629?
640, Berlin, Heidelberg. Springer-Verlag.
Florian Wolf and Edward Gibson. 2006. Coherence in Natural Language: Data Structures and Applications. MIT
Press, Cambridge, MA.
Yaakov Yaari. 1997. Segmentation of Expository Texts by Hierarchical Agglomerative Clustering. In Proceedings
of International Conference on Recent Advances in Natural Language Processing RANLP97, pages 59?65,
Tzigov Chark, Bulgaria.
Gilbert Youmans. 1991. A new tool for discourse analysis: The vocabulary-management profile. Language,
67(4):763?789.
47
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 476?485, Dublin, Ireland, August 23-29 2014.
Measuring Lexical Cohesion: Beyond Word Repetition
Anna Kazantseva & Stan Szpakowicz
School of Electrical Engineering and Computer Science
University of Ottawa
Ottawa, Ontario, Canada
{ankazant,szpak}@eecs.uottawa.ca
Abstract
This paper considers the problem of finding topical shifts in documents and in particular at what
information can be leveraged to identify them. Recent research on topical segmentation usually
assumes that topical shifts in discourse are signalled by changes in vocabulary. This information,
however, is not always a sufficient indicator of a topical shift, especially for certain genres. This
paper explores an additional source of information. Our hypothesis is that the type of a referring
expression is an indicator of how accessible its antecedent is. The shorter and less informative
the expression (e.g., a personal pronoun versus a lengthy post-modified noun phrase), the more
accessible the antecedent is likely to be and the more likely it is that the topic under discussion has
remained constant between the two mentions. We explore how this information can be used to
augment a lexically-based topical segmenter. We test our hypothesis on two types of data, literary
narratives and lecture notes. The results suggest that our similarity metric is useful: depending on
the settings it either slightly improves the performance or leaves it unchanged. They also suggest
that certain types of referring expressions are more useful than others.
1 Introduction
In the past 10 years, research on topical segmentation has mostly centred on using surface vocabulary to
identify topical shifts. The intuition is that if the vocabulary changes perceptibly, so does the topic under
discussion. One popular way to model this assumption is by probabilistic graphical models. A document
may be modelled as a sequence of strings (e.g., sentences) generated by a latent topic variable, where
the topic variables correspond to distributions over a finite vocabulary. Similarity-based methods are
an alternative methodology. The segmenter explicitly measures the amount of lexical similarity between
sentences. Places where similarity is low are likely to indicate shifts of topic. The common thread among
these approaches is that they rely almost exclusively on the explicitly mentioned words.
The idea that vocabulary shifts indicate topical shifts dates back to Youmans (1991). Indeed, by and
large, introducing new concepts almost necessarily requires that the concepts be named and described.
How densely the concepts are explicitly mentioned and how often the mentions are repeated depends
to a large degree on the genre and on the cognitive complexity of the document. In scientific papers or
legal documents clarity is paramount, so the author will endeavour to state things explicitly and avoid
ambiguity. The less complicated the document, however, the less it is necessary to explicitly repeat
terminology. In literature, for example, word repetition is not only uncommon, but it is usually a sign
of bad writing. In casual conversations, the topic can easily be never mentioned explicitly. How can we
identify topical shifts in a document whose author does not ?hold the reader?s hand??
It turns out that lexical cohesion (or, put simply, word repetition) is only one of several devices of
cohesion (Halliday and Hasan, 1976, p. 29) Other possibilities are reference, substitution, ellipsis and
conjunction. In this paper we mainly explore referential cohesion.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
476
Figure 1: An example dialogue from the Moonstone corpus
?What?s wrong now?? I said once more.
?Rosanna?s late again for dinner,? says Nancy. ?And I?m sent to fetch her in. All the hard work falls on my shoulders in
this house. Let me alone, Mr. Betteredge!?
The person here mentioned as Rosanna was our second housemaid. ?Where is she?? I inquired. [. . . ]
?At the sands, of course!? says Nancy, with a toss of her head. ?She had another of her fainting fits this morning, and she
asked to go out and get a breath of fresh air. I have no patience with her!?
?Go back to your dinner, my girl,? I said. ?I have patience with her, and I?ll fetch her in.?
Figure 1 shows a snippet of a dialogue from the publicly available Moonstone corpus (Kazantseva and
Szpakowicz, 2012). The two speakers discuss a specific person, Rosanna, yet her name is mentioned
explicitly only twice. In the remainder of the dialogue the author uses pronouns to refer to this person,
whose identity is evident from the context. Running an automatic segmenter on such a document would
likely be challenging since focal concepts ? characters ? are often referred to by pronouns or definite
noun phrases (NPs) instead of explicit repetition.
The focal entity Rosanna is introduced once and then it is referred to by nominal and pronominal
anaphora, not by explicit repetition. Simplifying things somewhat, we can say that merely by the virtue
of encountering a referring expression (e.g., she or the person), we know that it refers to something that
must be clear from the context. The type of the referring expression also contains information about the
availability of the antecedent. A she implies that the ?she? in question is rather obvious, that is to say,
the antecedent is nearby and, more important for our purposes, the topical thread continues. A more
verbose referring expression (e.g., the woman in red) is more likely in situations where the antecedent is
less obvious and the reader needs additional information to disambiguate the expression.
The idea that the type of referring expression tells a lot about the accessibility of its antecedent dates
back to Giv?on (1981). He postulated that the more informative the referring expression is, the less acces-
sible the antecedent will be. Figure 2 shows the list of expressions from the least to the most informative.
Projecting this information onto our task, we can say that the more informative the expression is, the less
continuity there will be in the topic.
The main contribution of this work is to show how such information can be used to improve the
quality of text segmentation. We extract NPs and classify them by informativeness. This is achieved
with the help of a syntactic parser, but a lighter form of processing might do, perhaps even if it captured
personal pronouns. Using this information, we augment and correct a matrix of lexical similarities
between sentences, a structure frequently used as an input to a topical segmenter.
The results of using coreferential similarity are evaluated on a dataset of manually segmented chapters
from a novel (Kazantseva and Szpakowicz, 2012) and on transcripts of lectures in Artificial Intelligence
(Malioutov and Barzilay, 2006). We try the new similarity matrix on two publicly available similarity-
based segmenters APS (Kazantseva and Szpakowicz, 2011) and MinCutSeg (Malioutov and Barzilay,
2006). The results suggest that the new matrix never hurts, and in several case improves, the performance
of the segmenter, especially for the novel. We also check whether this metric would still be useful if
instead of the traditionally used lexical similarity we used a similarity metric which took synonymy into
account. In this case, the margin of improvement is lower, but still the coreferential similarity metric
never hurts the performance and often improves it.
Section 2 of the paper gives an overview of related work. Section 3 describes our similarity metric and
how we compute it. Section 4 shows the details of the experiments, while Section 5 discusses the results.
We conclude in Section 6 with a discussion of how our metric can be improved and simplified.
2 Background and related work
Much of research on topical segmentation of text is based on the idea that changes of topic are usually
accompanied by vocabulary changes. Introduced by Youmans (1991), it has since formed the backbone
477
of research on topical segmentation. We now briefly review recent work on text segmentation. Since
the focus of this research is on what information is useful for text segmentation, this review emphasizes
representations rather than algorithms.
Perhaps the simplest way of estimating topical similarity between sentences is to measure cosine sim-
ilarity between corresponding feature vectors. It has been used extensively in text segmentation. Hearst
(1994; 1997) describes TextTiling, an algorithm which identifies topical shifts by sliding a window
through the document and measures cosine similarity between adjacent windows. The drops in simi-
larity signal shifts of topic. More recently, Malioutov and Barzilay (2006) as well as Kazantseva and
Szpakowicz (2011) use graph cuts and factor graph clustering for text segmentation. Both systems rely
on cosine similarity between bag-of-word vectors as an underlying representation.
While cosine similarity between vectors is easy to compute, it is hardly a reliable metric of topical
similarity. Several researchers have used lexical chains ? first introduced by Halliday and Hasan (1976)
? to improve the performance of topical segmenters.
1
The intuition behind using lexical chains for text
segmentation is that the beginning and end of a chain tend to correspond to the beginning and end of
a topically cohesive segment. One version of TextTiling (Hearst, 1997) uses lexical chains manually
constructed using Roget?s Thesaurus. Okumura and Honda (1994) apply automatically created lexical
chains to segment a small set of documents in Japanese. More recently, Marathe (2010) tried to build
lexical chains using distributional semantics and apply the method to text segmentation.
Other proposals to move beyond word repetition in topical segmentation include the use of bigram
overlap in (Reynar, 1999), information about collocations in (Jobbins and Evett, 1998), LSA (Landauer
and Dumais, 1997) in (Choi et al., 2001; Olney and Cai, 2005) and WordNet in (Scaiano et al., 2010).
It should be noted that much of the recent work on topical segmentation revolves around generative
models. For example Blei and Moreno (2001) use HMM,while Eisenstein and Barzilay (2008), Misra
et al. (2011) and Du et al. (2013) use higher-order models. We do not review this work in detail here
because it centers on algorithms for text segmentation and not on the information supplied to those algo-
rithms, which is the focus of this research. Fundamentally, the text is modelled as a sequence of tokens
generated by latent topic variables. Although probabilistic segmenters can be extended to use addi-
tional information (e.g., Eisenstein and Barzilay (2008) augment their segmenter with information about
discourse markers), it is not trivial to change these models to include information such as synonymy,
co-reference and so on. That is why we do not review them in detail here.
As this brief review shows, a number of approaches have been proposed to measure cohesion between
sentences, that is to say, to describe to what extent a pair of sentences is ?about the same thing?. Most
of them have a common denominator: they use explicit lexical information, sometimes augmented by
semantic relations from thesauri or ontologies.
Lexical resources, such as ontologies and knowledge-bases, may help improve the quality of segmen-
tations, but such resources are not always available. They also may cause problems with precision. More
important, however, they do not solve a more fundamental problem: a text may be highly cohesive and
coherent without being tightly bound by either lexical cohesion or synonymy.
The main ideas developed in this work originate in (Giv?on, 1981). The author looks at the functional
domain of topical accessibility. A number of coding devices affect this property. They are listed in
Figure 2, ordered from the devices used to mark the most continuous topics to those which mark the least
continuous topics. The order in Figure 2 is governed by a simple principle: the more accessible the topic
is, the less information is used to code it. The author argues that the continuum is applicable in many
languages. He also mentions that while the exact values of the phenomenon in question are difficult to
predict or even estimate, their relative order can be predicted with certainty, even if some devices are
unavailable in some languages.
In a similar spirit, Ariel (2014) groups non-initial NPs into expressions with low accessibility (definite
NPs and proper names), those with intermediate accessibility (personal and demonstrative pronouns) and
those with high accessibility (pronouns).
In this work, we propose to leverage the presence and type of co-referential relations to improve
1
Very simply put, a lexical chain is a sequence of related words in a text.
478
the results of two recent similarity-based segmenters. Instead of resolving anaphoric references, we
assume that their mere presence often indicates topic continuity. With this augmented model, we segment
fiction and spoken lecture transcripts, the two types of data where low rates of lexical cohesion preclude
achieving segmentation of good quality using only surface information about token types.
3 Estimating coreferential similarity
In order to see whether knowledge about types of referential expressions is useful for measuring topi-
cal similarity, we incorporate this information into two publicly available similarity-based topical seg-
menters, MCSeg (Malioutov and Barzilay, 2006) and APS (Kazantseva and Szpakowicz, 2011). Nor-
mally, both MCSeg and APS measure similarity between sentences by computing cosine similarity be-
tween the vectors corresponding to bag-of-words representation for each sentence:
sim(s
1
, s
2
) =
s
1
? s
2
||s
1
|| ? ||s
2
||
(1)
Each atomic unit of text is represented as a vector of features corresponding the occurrences of each
token type. The vectors are weighted using tf.idf values for each token type. Next, a segmenter measures
cosine similarity between vectors according to Equation 1. That is the fundamental representation in both
MCSeg and APS. MCSeg identifies segment boundaries by creating a weighted cyclic graph and cutting
it so as to maximize the sum of edges within segments and to minimize the sum of severed edges. APS
segments the sequence by finding segment centres ? points which best capture the content of a segment
? and assigning data points to best segment centres so as to maximize net similarity.
The proposed similarity metric relies on the following idea: in order to measure how many concepts
two sentences share, we do not need to resolve anaphoric expressions in full, but only to map them onto
sentences which contain their most recent antecedent (without actually naming the antecedents). We do
that by parsing the documents with the Connexor parser (Tapanainen and J?arvinen, 1997) and extracting
all NPs with their constituents. Next, we attempt to classify the NPs into categories which would roughly
correspond to those listed in Figure 2 and to those in (Ariel, 2014).
A manual study by Brown (1983) suggests that the average referential distance for animate and inan-
imate entities differs widely within the same document.
2
That is why it makes sense to distinguish
between these two types. In the end, then, we classify each identified NP into one of the categories listed
in Figure 3. The list is not exhaustive and in some cases an NP may belong to more than one type. In
practice, however, an NP is always assigned a single type dictated by the implementation.
2
Brown (1983, pp. 323-324) compares referring expressions which denote human and non-human entities. She uses three
measurements: average distance to the nearest antecedent, average ambiguity and persistence. On all three counts, human and
non-human entities appear to have different distributions.
Figure 2: Linguistic coding devices which signal topic accessibility (Giv?on, 1981)
Most continuous (least surprising)
1. zero anaphora
2. unstressed pronouns (e.g., He was speaking loudly.)
3. right-dislocated definite noun phrases (NPs) (e.g., It is no good, that book.)
4. neutral-ordered definite NPs (e.g., That book is no good.)
5. left-dislocated definite NPs (e.g., That book, it is no good.)
6. Y-moved NP?s (e.g., The book they read in turns.)
7. cleft/focus constructions (e.g., It was that book, that was on her mind for weeks.)
8. referential indefinite NPs (e.g., He picked up a book and left.)
Least continuous (most surprising)
479
Figure 3: Categories of noun phrases taken into account when computing coreferential similarity
1. animate personal pronouns (he, she, they)
2. inanimate pronouns (it)
3. demonstrative pronouns (that, those)
4. animate proper names (John Hernecastle)
5. inanimate proper names (London)
6. animate definite noun phrases (the man)
7. inanimate definite noun phrases (the jewel)
8. animate indefinite noun phrases (a man)
9. inanimate indefinite noun phrases (a jewel)
Finally, coreferential similarity between sentences S
i
and S
j
is measured as follows:
coref sim(S
i
, S
j
) = (
?
|T |
t=0
count
S
j
t
? weight
t
|S
1
| ? |S
2
|
)
(j?i?1)?decayFactor
(2)
T is the set of of all types of referring expressions which we consider ? those given in Figure 3.
count
S
j
t
is the number of times when an expression of type t appears in the most recent sentence, S
j
.
Note that we only consider the referring expressions in the most recent sentence, because a referring
expression, by its nature, must refer to something previously mentioned. The ?tightness? of the link is
controlled by setting weight
t
for each expression type t. weight
t
effectively specifies how likely it is
that the antecedent for an expression of a type t appears in sentence s
i
. The values of the weights are
set experimentally on the holdout data. They can almost certainly be further fine-tuned. Intuitively, the
settings of the weights reflect the logic behind Giv?on?s theory. Consider an example vector of weights
for expressions, where a higher weight corresponds to a more accessible antecedent (for animate and
inanimate entities respectively).
<personal pronouns anim: 4, demonstr pronouns anim: 2, proper names anim: 1,
def np anim: 0.5, indef np anim: 0, pronouns inanim: 2, demonstr pronouns inanim: 2,
proper names inanim: 0, def np inanim: 0, indef np inanim: 0>
The denominator of Equation 2 normalizes the value by the product of the lengths of sentences S
1
and
S
2
. The exponent (j ? i ? 1) ? decayFactor is responsible for decreasing similarity as the distance
between sentence S
i
and S
j
increases. The decay factor, 0 < decayFactor < 1, is set experimentally,
and j ? i is the distance between sentences S
i
and S
j
, i < j.
Figure 4 contains a walk-through example of computing referential similarity between two sentences.
The coreferential similarity as defined by Equation 2 is rather limited. The first limitation is the range:
it can only measure similarity between nearby sentences or paragraphs, because it only makes sense
between the closest occurrences of an antecedent and a subsequent referring expression. For example,
it does not make sense to measure coreferential similarity between sentences that are several paragraphs
apart. Even if they indeed talk about the same entities, the topic has most likely been re-introduced
several times in between. That is why we only compute coreferential similarity for sentences no more
than decayWindow sentences apart. The value of decayWindow is usually between 2 and 6 and it is
set experimentally on the holdout set for each corpus.
The values of coref
s
im are usually quite small and the information used is rather one-sided. We
use it, therefore, in addition to, not instead of, lexical similarity. In our experiments, we first compute
lexical similarity between sentences (or paragraphs) and then modify the lexical matrix by adding to it
the matrix of coreferential similarity.
480
Figure 4: An example of computing coreferential similarity
coref sim(S
i
, S
j
) = (
?
|T |
t=0
count
S
j
t
? weight
t
|S
1
| ? |S
2
|
)
(j?i?1)?decayFactor
S1: ?At the sands, of course!? says Nancy, with a toss of her head.
S2: ?She had another of her fainting fits this morning, and she asked to go out and get a breath of fresh air.?
Expression counts:
personal pronouns anim: 2 (she, she)
demonstr pronouns anim: 0
proper names anim: 1
def np anim: 0
indef np anim: 0
pronouns inanim: 0
demonstr pronouns inanim: 1
proper names inanim: 0
def np inanim: 2 (this morning, fainting fits)
indef np inanim: 1 (a breath)
Weights:
4
2
1
0.5
0
2
2
0
0
0
coref sim(S
2
, S
1
) =
2? 4 + 1? 1 + 1? 2
21? 22
(2?1?1)?0.5
= 0.0234
4 Experimental results
The effectiveness of coreferential similarity metric has been tested in practice. A set of experiments
compared how much the metric improves the quality of topical segmentations. To this end, we ran
APS and MCSeg with and without adding coreferential similarity to lexical similarity, and compared
the results. We chose these segmenters for comparison because coreferential similarity can only be
naturally incorporated into a similarity-based segmenter.
Data. In our experiments we used two publicly available datasets. The first one is a set of lectures on
Artificial Intelligence (Malioutov and Barzilay, 2006). The dataset contains 22 documents which were
manually annotated for the presence of topical shifts. The second dataset is the Moonstone dataset de-
scribed in (Kazantseva and Szpakowicz, 2012). It contains 20 chapters from Wilkie Collins?s novel, each
annotated by 4-6 people. To reconcile these multiple reference annotations, we create a majority gold
standard. It only contains segment breaks which were marked by at least 30% of the annotators. Both
segmenters are compared against this gold standard. There is a fair amount of disagreement between the
annotators of this dataset. The average inter-annotator windowDiff is 0.38 (Kazantseva and Szpakow-
icz, 2012, pp. 215-216), but if one takes into account near-hits, then at least 50% of the boundaries are
marked by more than two annotators.
Both datasets are quite challenging. The lecture dataset contain a lot of rather informal speech and
there is not as much lexical repetition as would be in a more formal text. The Moonstone dataset is an
example of literary language, full of small digressions, dialogue and so on.
The first dataset is annotated at the level of individual sentences. The second dataset is annotated at the
level of paragraphs. We segment both datasets at the level of the gold-standard annotations (sentences
for lectures, paragraphs for the novel).
When working with paragraphs, coref sim is computed slightly differently:
coref sim(p
i
, p
j
) = (
?
|T |
t=0
count
p
j
t
? weight
t
|p
1
| ? |p
2
|
)
(j?i?1)?decayFactor
(3)
In this case, count
p
j
t
refers to the number of occurrences of expression of type t in the first
paragraphCutOff sentences of the paragraph p
j
, instead of the whole paragraph. The rationale behind
this heuristic is that the referring expressions in the opening sentences of the paragraph are likely to refer
481
to entities from the previous paragraph, while expressions in the middle or the end of the paragraph are
likely to refer to entities introduced inside the paragraph.
Segmenters and baselines. We use two publicly available topical segmenters in our experiments:
MCSeg and APS. The default version of each segmenter computes a similarity matrix between sentence
in the input document. The values in the matrix correspond to cosine similarity (Equation 1) computed
after the removal of stop words and weighting the bag-of-word vectors by tf.idf . The results obtained
using these default matrices are our first baseline.
In our experiments, we modify this matrix by adding to it the matrix of coreferential similarities. The
values of coreferential similarities are rather small and most modifications are localized. That is because
the value of decayWindow is set between 2 and 6 (see Section 3).
In addition to the matrices based on cosine similarity, we wanted to see if using a more intelligent
measure of topical similarity improves the results. We built one more flavour of similarity matrices using
the DKPro Similarity framework (B?ar et al., 2013). The framework contains a model of textual similarity
which has been used by the winning system at the SemEval Textual Similarity 2012 shared evaluation.
We use this model (further STS-2012) as a more competitive baseline for computing topical similarity.
The STS-2012 baseline consists of a log-linear regression model trained on the SemEval 2012 train-
ing data. It combines an assortment of measures of textual similarity to come up with its judgments.
The metrics include n-gram overlap, semantic similarity measures (based on both corpora and lexical
resources) and several measures of stylistic similarity. We chose to use this relatively complicated met-
ric because of its competitive performance at SemEval 2012. The system, however, was not designed
to measure topical similarity per se, especially between many sentences coming from the same source
document. By default, the STS-2012 baseline outputs values between 1 and 5. These were normalized to
be between 0 and 1.
Similarly to the experimental design with cosine similarity matrices, we try running the segmenters
using STS-2012 with and without adding coreferential similarity matrix to it.
On both datasets we set the weights for various types of referential expressions using hold-out sets
of two files. When setting the weights, we were guided by the principle captured in Figure 2: personal
pronouns suggest the tightest link, followed by demonstrative pronouns, proper names, and so on.
It should be noted that because we had to modify the native representation of both segmenters by sup-
plying a matrix computed using non-native code, we could not use the proper training scripts which come
with the segmenters. In effect, the results are likely to be lower than they could have been. Even so, this is
acceptable for our purposes because we are interested in the improvement gained by using coreferential
similarity, not in obtaining the best possible segmentation via the setting of the best parameters.
Processing. We computed the underlying lexical similarity matrices using the same procedure as
described in (Malioutov and Barzilay, 2006; Kazantseva and Szpakowicz, 2011), but using our own
code. In other words, we built a matrix of cosine similarities after removing stop words and weighting
the underlying vectors by tf.idf values.
In order to compute coreferential similarity, all documents were parsed using the Connexor parser
(Tapanainen and J?arvinen, 1997). The parser was chosen because it produces high-quality partial parses
of long sentences often encountered in the Moonstone dataset. We also tagged named entities and labelled
NPs as animate or inanimate using the Stanford Core NLP suite.
3
Metrics. We compare topical segmentations using the windowDiff metric:
winDiff =
1
N ? k
N?k
?
i=1
(|ref ? hyp| 6= 0) (4)
windowDiff slides a window of size k through the input sequence of length N . At every position of the
window, the metric compares the number of boundaries in the reference sequence and in the hypothetical
sequence. The number of erroneous windows is normalized by the total number of windows to obtain
the final value. windowDiff is a penalty metric: lower values correspond to better segmentations.
3
http://nlp.stanford.edu/software/corenlp.shtml
482
AI Lectures Moonstone
APS 0.420 (? 0.014) 0.441 (? 0.075)
APS-coref sim 0.411 (? 0.025) 0.391 (? 0.060)
APS-STS 0.428 (? 0.049) 0.479 (? 0.041)
APS-STS -coref sim 0.429 (? 0.020) 0.478 (? 0.035)
MCSeg 0.431 (? 0.045) 0.470 (? 0.095)
MCSeg-coref
s
im 0.410 (? 0.060) 0.413 (? 0.030)
MCSeg-STS 0.451 (? 0.023) 0.441 (? 0.051)
MCSeg-STS-coref sim 0.433 (? 0.070) 0.430 (? 0.025)
Table 1: Results of comparing APS and MCSeg using four different matrix types (windowDiff values and
standard deviation)
5 Evaluation
Table 1 presents the results of running APS and MCSeg using four different input matrices each. The
first column shows the combination of the name of the segmenter and the specific input matrix. APS and
MCSeg refer to the cases where both segmenters were run using simple cosine similarity matrices. STS
refers to matrices computed using STS-2012 from the DKPro Similarity framework. coref sim refers to
cosine similarity matrices modified by adding a matrix with coreferential similarities. STS?coref sim
are matrices computed using STS-2012 which had coreferential similarity added to them.
In all experiments, we set the weights for different types of referring expressions on two hold-out files.
The remainder of the data is divided into five folds. Standard deviation reported in the tables is computed
across folds.
Coreferential similarity improves the results of the cosine matrix for both segmenters, but the improve-
ment on the AI dataset is rather small (1% for APS and 2% for MCSeg).
It is interesting to see that in most cases using STS matrices slightly hurts the performance of the
segmenters compared to using simple cosine similarity matrices. The only exception is running MCSeg
on the Moonstone dataset which improves the performance by 3%.
Adding a matrix of coreferential similarities to STS matrices slightly improves the performance on
the Moonstone dataset and leaves it practically unchanged on the dataset of AI lectures.
It is somewhat surprising that using STS-2012 for similarity computation does not improve, and occa-
sionally worsens, the results compared to using simple cosine similarity. Coreferential similarity, on the
other hand, produces a small but consistent improvement.
We have examined the vectors of weights used in these experiments (set using hold-out data). On
the Moonstone dataset, the results are the best when personal animate pronouns get the highest weight,
followed by demonstrative animate pronouns, as well as inanimate pronouns, both regular and demon-
strative. Other expression types are assigned either a very small weight or the value 0, effectively making
them inconsequential. We hypothesize that this is due to the fact that the novel discusses people, their
relations and interactions, making animate entities central for estimating topical links.
The vectors used on the AI lecture dataset are similar, except that here the highest weights are given
to demonstrative and regular inanimate pronouns. These are followed by demonstrative and then regular
animate pronouns. This distribution is likely due to the fact that the lecture dataset discusses abstract
concepts, while people are likely to be noted more tangentially. We are not sure how to explain the fact
that in this dataset demonstrative pronouns have a slightly higher weight than the regular ones.
Identifying and categorizing noun phrases requires either high-quality NP-tagging or parsing. On the
other hand, most pronouns can be captured very easily, perhaps even using a list of words. It is interesting
to note that the most gain is due to these ?cheap? types of referring expressions. In the future, we plan to
implement a lighter version of the coreferential similarity metric which only considers pronouns.
6 Conclusions and future work
This paper has presented a method for improving the quality of topical segmentations by using infor-
mation about referential expressions in nearby sentences. The method slightly improves the quality of
segmentations and, what is even more important, seems never to worsen the results.
483
The necessity to perform complete parsing of the input document is a drawback of the current ap-
proach. We note in Section 5, however, that the only types of referential expressions which improve
performance are personal and demonstrative pronouns. Those can be easily captured without parsing. In
the near future we plan to investigate such a light-weight version of coref sim metric.
Another way to improve our current implementation would be a more objective method of setting the
weights for different types of referring expressions. At present, the expressions are set by hand on a
small hold-out set of documents. This is far from ideal. We plan to investigate if using logistic regression
or expectation maximization would make the system more robust.
References
Mira Ariel. 2014. Accessing Noun-Phrase Antecedents. Routledge, London and New York.
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2013. DKPro Similarity: An Open Source Framework for Text
Similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System
Demonstrations, pages 121?126, Sofia, Bulgaria.
David Blei and Pedro Moreno. 2001. Topic segmentation with an aspect hidden Markov Model. In Proceedings of
the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 343?348.
Elizabeth Brown. 1983. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993?1022.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Johanna Moore. 2001. Latent Semantic Analysis for Text Seg-
mentation. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,
pages 109?117, Pittsburgh, Pennsylvania.
Lan Du, Wray Buntine, and Mark Johnson. 2013. Topic Segmentation with a Structured Topic Model. In
Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 190?200, Atlanta, Georgia.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian Unsupervised Topic Segmentation. In Proceedings of the
2008 Conference on Empirical Methods in Natural Language Processing, pages 334?343, Honolulu, Hawaii.
Talmy Giv?on. 1981. Typology and Functional Domains. Studies in Language, 5(2):163?193.
M.A.K Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman, London and New York.
Marti A. Hearst. 1994. Multi-paragraph Segmentation of Expository Text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguistics, ACL ?94, pages 9?16, Las Cruces, New Mexico.
Marti A. Hearst. 1997. TextTiling: segmenting text into multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Amanda C. Jobbins and Lindsay J. Evett. 1998. Text Segmentation Using Reiteration and Collocation. In Pro-
ceedings of the 17th International Conference on Computational Linguistics - Volume 1, COLING ?98, pages
614?618, Montr?eal, Qu?ebec.
Anna Kazantseva and Stan Szpakowicz. 2011. Linear Text Segmentation Using Affinity Propagation. In Proceed-
ings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 284?293, Edinburgh,
Scotland.
Anna Kazantseva and Stan Szpakowicz. 2012. Topical Segmentation: a Study of Human Performance and a New
Measure of Quality. In Proceedings of the 2012 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 211?220, Montr?eal, Canada.
Thomas K Landauer and Susan T. Dumais. 1997. A solution to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of knowledge. Psychological Review, pages 211?240.
Igor Malioutov and Regina Barzilay. 2006. Minimum Cut Model for Spoken Lecture Segmentation. In Pro-
ceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 25?32, Sydney, Australia.
Meghana Marathe. 2010. Lexical Chains Using Distributional Measures of Concept Distance. Master?s thesis,
University of Toronto.
484
Hemant Misra, Franc?ois Yvon, Olivier Capp?e, and Joemon M. Jose. 2011. Text segmentation: A topic modeling
perspective. Information Processing and Management, 47(4):528?544.
Manabu Okumura and Takeo Honda. 1994. Word Sense Disambiguation and Text Segmentation Based On Lexical
Cohesion. In COLING 1994 Volume 2: The 15th International Conference on Computational Linguistics, pages
775?761, Kyoto, Japan.
Andrew Olney and Zhiqiang Cai. 2005. An Orthonormal Basis for Topic Segmentation in Tutorial Dialogue. In
Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language
Processing ?HLT ?05, pages 971?978, Vancouver, Canada.
Jeffrey C. Reynar. 1999. Statistical Models of Text Segmentation. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, pages 357?364.
Martin Scaiano, Diana Inkpen, Robert Lagani`ere, and Adele Reinhartz. 2010. Automatic Text Segmentation for
Movie Subtitles. In Advances in Artificial Intelligence, volume 6085 of Lecture Notes in Computer Science,
pages 295?298. Springer.
Pasi Tapanainen and Timo J?arvinen. 1997. A non-projective dependency parser. Proceedings of the 5th Confer-
ence on Applied Natural Language Processing, pages 64?71.
Gilbert Youmans. 1991. A new tool for discourse analysis: The vocabulary-management profile. Language,
67(4):763?789.
485
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 284?293,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Linear Text Segmentation Using Affinity Propagation
Anna Kazantseva
School of Electrical Engineering
and Computer Science,
University of Ottawa
ankazant@site.uottawa.ca
Stan Szpakowicz
School of Electrical Engineering
and Computer Science,
University of Ottawa &
Institute of Computer Science,
Polish Academy of Sciences
szpak@site.uottawa.ca
Abstract
This paper presents a new algorithm for lin-
ear text segmentation. It is an adaptation of
Affinity Propagation, a state-of-the-art clus-
tering algorithm in the framework of factor
graphs. Affinity Propagation for Segmenta-
tion, or APS, receives a set of pairwise simi-
larities between data points and produces seg-
ment boundaries and segment centres ? data
points which best describe all other data points
within the segment. APS iteratively passes
messages in a cyclic factor graph, until conver-
gence. Each iteration works with information
on all available similarities, resulting in high-
quality results. APS scales linearly for realistic
segmentation tasks. We derive the algorithm
from the original Affinity Propagation formu-
lation, and evaluate its performance on topi-
cal text segmentation in comparison with two
state-of-the art segmenters. The results sug-
gest that APS performs on par with or outper-
forms these two very competitive baselines.
1 Introduction
In complex narratives, it is typical for the topic to
shift continually. Some shifts are gradual, others ?
more abrupt. Topical text segmentation identifies the
more noticeable topic shifts. A topical segmenter?s
output is a very simple picture of the document?s
structure. Segmentation is a useful intermediate step
in such applications as subjectivity analysis (Stoy-
anov and Cardie, 2008), automatic summarization
(Haghighi and Vanderwende, 2009), question an-
swering (Oh, Myaeng, and Jang, 2007) and others.
That is why improved quality of text segmentation
can benefit other language-processing tasks.
We present Affinity Propagation for Segmenta-
tion (APS), an adaptation of a state-of-the-art clus-
tering algorithm, Affinity Propagation (Frey and
Dueck, 2007; Givoni and Frey, 2009).1 The origi-
nal AP algorithm considerably improved exemplar-
based clustering both in terms of speed and the qual-
ity of solutions. That is why we chose to adapt it to
segmentation. At its core, APS is suitable for seg-
menting any sequences of data, but we present it in
the context of segmenting documents. APS takes as
input a matrix of pairwise similarities between sen-
tences and, for each sentence, a preference value
which indicates an a priori belief in how likely
a sentence is to be chosen as a segment centre.
APS outputs segment assignments and segment cen-
tres ? data points which best explain all other points
in a segment. The algorithm attempts to maximize
net similarity ? the sum of similarities between all
data points and their respective segment centres.
APS operates by iteratively passing messages in
a factor graph (Kschischang, Frey, and Loeliger,
2001) until a good set of segments emerges. Each
iteration considers all similarities ? takes into ac-
count all available information. An iteration in-
cludes sending at most O(N2) messages. For the
majority of realistic segmentation tasks, however,
the upper bound is O(MN) messages, where M
is a constant. This is more computationally ex-
pensive than the requirements of locally informed
segmentation algorithms such as those based on
HMM or CRF (see Section 2), but for a globally-
informed algorithm the requirements are very rea-
sonable. APS is an instance of loopy-belief propaga-
tion (belief propagation on cyclic graphs) which has
1An implementation of APS in Java, and the data sets, can be
downloaded at ?www.site.uottawa.ca/?ankazant?.
284
been used to achieved state-of-the-art performance
in error-correcting decoding, image processing and
data compression. Theoretically, such algorithms
are not guaranteed to converge or to maximize the
objective function. Yet in practice they often achieve
competitive results.
APS works on an already pre-compiled similaritiy
matrix, so it offers flexibility in the choice of simi-
larity metrics. The desired number of segments can
be set by adjusting preferences.
We evaluate the performance of APS on three
tasks: finding topical boundaries in transcripts of
course lectures (Malioutov and Barzilay, 2006),
identifying sections in medical textbooks (Eisen-
stein and Barzilay, 2008) and identifying chapter
breaks in novels. We compare APS with two recent
systems: the Minimum Cut segmenter (Malioutov
and Barzilay, 2006) and the Bayesian segmenter
(Eisenstein and Barzilay, 2008). The comparison
is based on the WindowDiff metric (Pevzner and
Hearst, 2002). APS matches or outperforms these
very competitive baselines.
Section 2 of the paper outlines relevant research
on topical text segmentation. Section 3 briefly cov-
ers the framework of factor graphs and outlines the
original Affinity Propagation algorithm for cluster-
ing. Section 4 contains the derivation of the new
update messages for APSeg. Section 5 describes the
experimental setting, Section 6 reports the results,
Section 7 discusses conclusions and future work.
2 Related Work
This sections discusses selected text segmentation
methods and positions the proposed APS algorithm
in that context.
Most research on automatic text segmentation re-
volves around a simple idea: when the topic shifts,
so does the vocabulary (Youmans, 1991). We can
roughly subdivide existing approaches into two cat-
egories: locally informed and globally informed.
Locally informed segmenters attempt to identify
topic shifts by considering only a small portion of
complete document. A classical approach is Text-
Tiling (Hearst, 1997). It consists of sliding two ad-
jacent windows through text and measuring lexical
similarity between them. Drops in similarity corre-
spond to topic shifts. Other examples include text
segmentation using Hidden Markov Models (Blei
and Moreno, 2001) or Conditional Random Fields
(Lafferty, McCallum, and Pereira, 2001). Locally
informed methods are often very efficient because
of lean memory and CPU time requirements. Due to
a limited view of the document, however, they can
easily be thrown off by short inconsequential digres-
sions in narration.
Globally informed methods consider ?the big pic-
ture? when determining the most likely location of
segment boundaries. Choi (2000) applies divisive
clustering to segmentation. Malioutov and Barzilay
(2006) show that the knowledge about long-range
similarities between sentences improves segmenta-
tion quality. They cast segmentation as a graph-
cutting problem. The document is represented as a
graph: nodes are sentences and edges are weighted
using a measure of lexical similarity. The graph is
cut in a way which maximizes the net edge weight
within each segment and minimizes the net weight
of severed edges. Such Minimum Cut segmentation
resembles APS the most among others mentioned in
this paper. The main difference between the two is
in different objective functions.
Another notable direction in text segmentation
uses generative models to find segment boundaries.
Eisenstein and Barzilay (2008) treat words in a sen-
tence as draws from a multinomial language model.
Segment boundaries are assigned so as to maximize
the likelihood of observing the complete sequence.
Misra et al (2009) use a Latent Dirichlet aloca-
tion topic model (Blei, Ng, and Jordan, 2003) to find
coherent segment boundaries. Such methods output
segment boundaries and suggest lexical distribution
associated with each segment. Generative models
tend to perform well, but are less flexible than the
similarity-based models when it comes to incorpo-
rating new kinds of information.
Globally informed models generally perform bet-
ter, especially on more challenging datasets such as
speech recordings, but they have ? unsurprisingly ?
higher memory and CPU time requirements.
The APS algorithm described in this paper com-
bines several desirable properties. It is unsupervised
and, unlike most other segmenters, does not require
specifying the desired number of segments as an in-
put parameter. On each iteration it takes into account
the information about a large portion of the docu-
285
ment (or all of it). Because APS operates on a pre-
compiled matrix of pair-wise sentence similarities, it
is easy to incorporate new kinds of information, such
as synonymy or adjacency. It also provides some in-
formation as to what the segment is about, because
each segment is associated with a segment centre.
3 Factor graphs and affinity propagation
for clustering
3.1 Factor graphs and the max-sum algorithm
The APS algorithm is an instance of belief propa-
gation on a cyclic factor graph. In order to explain
the derivation of the algorithm, we will first briefly
introduce factor graphs as a framework.
Many computational problems can be reduced to
maximizing the value of a multi-variate function
F (x1, . . . , xn) which can be approximated by a sum
of simpler functions. In Equation 1, H is a set of
discrete indices and fh is a local function with argu-
ments Xh ? {x1, . . . , xn}:
F (x1, . . . , xn) =
?
h?H
fh(Xh) (1)
Factor graphs offer a concise graphical represen-
tation for such problems. A global function F which
can be decomposed into a sum of M local function
fh can be represented as a bi-partite graph with M
function nodes and N variable nodes (M = |H|).
Figure 1 shows an example of a factor graph for
F (x1, x2, x3, x4) = f1(x1, x2, x3)+ f2(x2, x3, x4).
The factor (or function) nodes are dark squares, the
variable nodes are light circles.
The well-known max-sum algorithm (Bishop,
2006) seeks a configuration of variables which max-
imizes the objective function. It finds the maximum
in acyclic factor graphs, but in graphs with cycles
neither convergence nor optimality are guaranteed
(Pearl, 1982). Yet in practice good approximations
can be achieved. The max-sum algorithm amounts
to propagating messages from function nodes to
variable nodes and from variable nodes to function
nodes. A message sent from a variable node x to a
function node f is computed as a sum of the incom-
ing messages from all neighbours of x other than f
(the sum is computed for each possible value of x):
?x?f =
?
f ??N(x)\f
?f ??x (2)
Figure 1: Factor graph for F (x1, x2, x3, x4)
= f1(x1, x2, x3) + f2(x2, x3, x4).
f1 f2
x1 x2 x3 x4
N(x) is the set of all function nodes which are x?s
neighbours. The message reflects the evidence about
the distribution of x from all functions which have x
as an argument, except for the function correspond-
ing to the receiving node f .
A message ?f?x from function f to variable x is
computed as follows:
?f?x = max
N(f)\x
(f(x1, . . . , xm) +
?
x??N(f)\x
?x??f )
(3)
N(f) is the set of all variable nodes which are f ?s
neighbours. The message reflects the evidence about
the distribution of x from function f and its neigh-
bours other than x.
A common message-passing schedule on cyclic
factor graphs is flooding: iteratively passing all
variable-to-function messages, then all function-to-
variable messages. Upon convergence, the summary
message reflecting final beliefs about the maximiz-
ing configuration of variables is computed as a sum
of all incoming function-to-variable messages.
3.2 Affinity Propagation
The APS algorithm described in this paper is a mod-
ification of the original Affinity Propagation algo-
rithm intended for exemplar-based clustering (Frey
and Dueck, 2007; Givoni and Frey, 2009). This sec-
tion describes the binary variable formulation pro-
posed by Givoni and Frey, and lays the groundwork
for deriving the new update messages (Section 4).
Affinity Propagation for exemplar-based cluster-
ing is formulated as follows: to cluster N data
points, one must specify a matrix of pairwise sim-
ilarities {SIM(i, j)}i,j?{1,...,N},i 6=j and a set of
self-similarities (so-called preferences) SIM(j, j)
which reflect a priori beliefs in how likely each data
point is to be selected as an exemplar. Preference
values occupy the diagonal of the similarity matrix.
The algorithm then assigns each data point to an ex-
emplar so as to maximize net similarity ? the sum of
286
Figure 2: Factor graph for affinity propagation.
E1 Ej EN
I1
Ii
IN
c11 c1j c1N
ci1 cij ciN
cN1 cNj cNN
S11 S1j S1N
Si1 Sij SiN
SN1 SNj SNN
similarities between all points and their respective
exemplars; this is expressed by Equation 7. Figure 2
shows a schematic factor graph for this problem,
with N2 binary variables. cij = 1 iff point j is an
exemplar for point i. Function nodes Ej enforce a
coherence constraint: a data point cannot exemplify
another point unless it is an exemplar for itself:
Ej(c1j , . . . , cNj) =
?
??
??
?? if cjj = 0 ? cij = 1
for some i 6= j
0 otherwise
(4)
An I node encodes a single-cluster constraint: each
data point must belong to exactly one exemplar ?
and therefore to one cluster:
Ii(ci1, . . . , ciN ) =
{
?? if ?j cij 6= 1
0 otherwise (5)
An S node encodes user-defined similarities
between data-points and candidate exemplars
(SIM(i, j) is the similarity between points i and
j):
Sij(cij) =
{
SIM(i, j) if cij = 1
0 otherwise (6)
Equation 7 shows the objective function which we
want to maximize: a sum of similarities between
data points and their exemplars, subject to the two
constraints (coherence and single-cluster per point).
S(c11, . . . , cNN ) =
?
i,j
Si,j(cij) +
?
i
Ii(ci1, . . . , ciN )
(7)
+
?
j
Ej(c1j , . . . , cNj)
According to Equation 3, the computation of a sin-
gle factor-to-variable message involves maximizing
over 2n configurations. E and I , however, are bi-
nary constraints and evaluate to ?? for most con-
figurations. This drastically reduces the number of
configurations which can maximize the message val-
ues. Given this simple fact, Givoni and Frey (2009)
show how to reduce the necessary update messages
to only two types of scalar ones: availabilities (?)
and responsibilities (?).2
A responsibility message ?ij , sent from a variable
node cij to function node Ej , reflects the evidence
of how likely j is to be an exemplar for i given all
other potential exemplars:
?ij = SIM(i, j)?maxk 6=j (SIM(i, k) + ?ik) (8)
An availability message ?ij , sent from a function
node Ej to a variable node cij , reflects how likely
point j is to be an exemplar for i given the evidence
from all other data points:
?ij =
?
????
????
?
k 6=j
max[?kj , 0] if i = j
min[0, ?jj +
?
k/?{i,j}
max[?kj , 0]] if i 6= j
(9)
Let ?ij(l) be the message value corresponding to set-
ting variable cij to l, l ? {0, 1}. Instead of sending
two-valued messages (corresponding to the two pos-
sible values of the binary variables), we can send
the difference for the two possible configurations:
?ij = ?ij(1)? ?ij(0) ? effectively, a log-likelihood
ratio.
2Normally, each iteration of the algorithm sends five types
of two-valued messages: to and from functions E and I and
a message from functions S. Fortunately, the messages sent
to and from E factors to the variable nodes subsume the three
other message types and it is not necessary to compute them
explicitly. See (Givoni and Frey, 2009, p.195) for details.
287
Figure 3: Examples of valid configuration of hidden
variables {cij} for clustering and segmentation.
(a) Clustering (b) Segmentation
The algorithm converges when the set of points
labelled as exemplars remains unchanged for a pre-
determined number of iterations. When the al-
gorithm terminates, messages to each variable are
added together. A positive final message indicates
that the most likely value of a variable cij is 1 (point
j is an exemplar for i), a negative message indicates
that it is 0 (j is not an exemplar for i).
4 Affinity Propagation for Segmentation
This section explains how we adapt the Affinity
Propagation clustering algorithm to segmentation.
In this setting, sentences are data points and we
refer to exemplars as segment centres. Given a doc-
ument, we want to assign each sentence to a segment
centre so as to maximize net similarity.
The new formulation relies on the same underly-
ing factor graph (Figure 2). A binary variable node
cij is set to 1 iff sentence j is the segment centre for
sentence i. When clustering is the objective, a clus-
ter may consist of points coming from anywhere in
the data sequence. When segmentation is the ob-
jective, a segment must consist of a solid block of
points around the segment centre. Figure 3 shows,
for a toy problem with 5 data points, possible valid
configurations of variables {cij} for clustering (3a)
and for segmentation (3b).
To formalize this new linearity requirement, we
elaborate Equation 4 into Equation 10. Ej evaluates
to ?? in three cases. Case 1 is the original coher-
ence constraint. Case 2 states that no point k may
be in the segment with a centre is j, if k lies before
the start of the segment (the sequence c(s?1)j = 0,
csj = 1 necessarily corresponds to the start of the
segment). Case 3 handles analogously the end of
the segment.
Ej =
?
??????????
??????????
?? 1. if cjj = 0 ? cij = 1 for some i 6= j
2. if cjj = 1 ? csj = 1 ? c(s?1)j = 0
? ckj = 1 for some s < j, k < s? 1
3. if cjj = 1 ? cej = 1 ? c(e+1)j = 0
? ckj = 1 for some e > j, k > e+ 1
0 otherwise
(10)
The E function nodes are the only changed part of
the factor graph, so we only must re-derive ? mes-
sages (availabilities) sent from factors E to variable
nodes. A function-to-variable message is computed
as shown in Equation 11 (elaborated Equation 3),
and the only incoming messages to E nodes are re-
sponsibilities (? messages):
?f?x = max
N(f)\x
(f(x1, . . . , xm) +
?
x??N(f)\x
?x??f ) =
(11)
max
cij , i 6=j
((Ej(c1j , . . . , cNj) +
?
cij , i 6=j
?ij(cij)))
We need to compute the message values for the
two possible settings of binary variables ? denoted
as ?ij(1) and ?ij(0) ? and propagate the difference
?ij = ?ij(1) - ?ij(0).
Consider the case of factor Ej sending an ? mes-
sage to the variable node cjj (i.e., i = j). If cjj = 0
then point j is not its own segment centre and the
only valid configuration is to set al other cij to 0:
?jj(0) = maxcij ,i 6=j
(Ej(c1j , . . . , cNj) +
?
cij ,i 6=j
?ij(cij))
(12)
=
?
i 6=j
?ij(0)
To compute ?ij(1) (point j is its own segment
centre), we only must maximize over configurations
which will not correspond to cases 2 and 3 in Equa-
tion 10 (other assignments are trivially non-optimal
because they would evaluate Ej to ??). Let the
start of a segment be s, 1 ? s < j and the end of
the segment be e, j + 1 < e ? N . We only need to
consider configurations such that all points between
s and e are in the segment while all others are not.
288
The following picture shows a valid configuration.3
1 s j e N
To compute the message ?ij(1), i = j, we have:
?jj(1) =
jmax
s=1
[
s?1?
k=1
?kj(0) +
j?1?
k=s
?kj(1)]+ (13)
Nmax
e=j
[
e?
k=j+1
?kj(1) +
N?
k=e+1
?kj(0)]
Subtracting Equation 12 from Equation 13, we get:
?jj = ?jj(1)? ?jj(0) = (14)
jmax
s=1
(
j?1?
k=s
?kj) +
Nmax
e=j
(
e?
k=j+1
?kj)
Now, consider the case of factor Ej sending an ?
message to a variable node cij other than segment
exemplar j (i.e., i 6= j). Two subcases are possible:
point i may lie before the segment centre j (i < j),
or it may lie after the segment centre (i > j).
The configurations which may maximize ?ij(1)
(the message value for setting the hidden variable
to 1) necessarily conform to two conditions: point
j is labelled as a segment centre (cjj = 1) and all
points lying between i and j are in the segment.
This corresponds to Equation 15 for i < j and to
Equation 16 for i > j. Pictorial examples of corre-
sponding valid configurations precede the equations.
1 s i j e N
?ij, i<j(1) =
imax
s=1
[
s?1?
k=1
?kj(0) +
i?1?
k=s
?kj(1)]+
(15)
j?
k=i+1
?kj(1) +
Nmax
e=j
[
e?
k=j+1
?kj(1) +
N?
k=e+1
?kj(0)]
3Variables cij set to 1 are shown as shaded circles, to 0 ? as
white circles. Normally, variables form a column in the factor
graph; we transpose them to save space.
1 s j i e N
?ij, i>j(1) =
jmax
s=1
[
s?1?
k=1
?kj(0) +
j?1?
k=s
?kj(1)]+
(16)
i?1?
k=j
?kj(1) +
Nmax
e=i
[
e?
k=i+1
?kj(1) +
N?
k=e+1
?kj(0)]
To compute the message value for setting the
hidden variable cij to 0, we again distinguish
between i < j and i > j and consider whether cjj
= 1 or cjj = 0 (point j is / is not a segment centre).
For cjj = 0 the only optimal configuration is cij = 0
for all i 6= j. For cjj = 1 the set of possible optimal
configurations is determined by the position of point
i with respect to point j. Following the same logic
as in the previous cases we get Equation 17 for
i < j and Equation 18 for i > j.
1 i s j e N
?ij(0) = max(
?
k/?i,j
?kj(0), (17)
i?1?
k=1
?kj(0) +
jmax
s=i+1
[
s?1?
k=i+1
?kj(0) +
j?1?
k=s
?kj(1)]+
?jj(1) +
Nmax
e=j
[
e?
k=j+1
?kj(1) +
N?
k=e+1
?kj(0)])
1 s j e i N
?ij(0) = max(
?
k/?i,j
?kj(0), (18)
jmax
s=1
[
s?1?
k=1
?kj(0) +
j?1?
k=s
?kj(1)]+
?jj(1) +
i?1max
e=j
[
e?
k=j+1
?kj(1) +
i?1?
k=e+1
?kj(0)]
N?
k=i+1
?kj(0))
Due to space constraints, we will omit the details
of subtracting Equation 17 from 15 and Equation 18
from 16. The final update rules for both i < j and
289
Algorithm 1 Affinity Propagation for Segmentation
1: input: 1) a set of pairwise similarities {SIM(i, j)}(i,j)?{1,...,N}2 , SIM(i, j) ? R; 2) a set of prefer-
ences (self-similarities) {SIM(i, i)}i?{1,...,N} indicating a priori likelihood of point i being a segment
centre
2: initialization: ?i, j : ?ij = 0 (set al availabilities to 0)
3: repeat
4: iteratively update responsibilities (?) and availabilities (?)
5:
?i, j : ?ij = SIM(i, j) + maxk 6=j (SIM(i, k)? ?ik)
6:
?i, j : ?ij =
?
?????????????????????????
?????????????????????????
jmax
s=1
(
j?1?
k=s
?kj) +
Nmax
e=j
(
e?
k=j+1
?kj) if i = j
min[ imax
s=1
i?1?
k=s
?kj +
j?
k=i+1
?kj +
Nmax
e=j
e?
k=j+1
?kj ,
imax
s=1
i?1?
k=s
?kj +
j
min
s=i+1
s?1?
k=i+1
?kj ] if i < j
min[ jmax
s=1
j?1?
k=s
?kj +
i?1?
k=j
?kj +
Nmax
e=i
e?
k=i+1
?kj ,
i?1
min
e=j
i?1?
k=e+1
?kj +
Nmax
e=i
e?
k=i+1
?kj ] if i > j
7: until convergence
8: compute the final configuration of variables: ?i, j j is the exemplar for i iff ?ij + ?ij > 0
9: output: exemplar assignments
i > j appear in Algorithm 1, where we summarize
the whole process.
The equations look cumbersome but they are triv-
ial to compute. Every summand corresponds to find-
ing the most likely start or end of the segment, tak-
ing into account fixed information. When computing
messages for any given sender node, we can remem-
ber the maximizing values for neighbouring recipi-
ent nodes. For example, after computing the avail-
ability message from factor Ej to cij , we must only
consider one more responsibility value when com-
puting the message from Ej to variable c(i+1)j . The
cost of computing a message is thus negligible.
When the matrix is fully specified, each iteration
requires passing 2N2 messages, so the algorithm
runs in O(N2) time and requires O(N2) memory
(to store the similarities, the availabilities and the
responsibilities). When performing segmentation,
however, the user generally has some idea about
the average or maximum segment length. In such
more realistic cases, the input matrix of similarities
is sparse ? it is constructed by sliding a window of
size M . M usually needs to be at least twice the
maximum segment length or thrice the average seg-
ment length. Each iteration, then, involves sending
2MN messages and the storage requirements are
also O(MN).
As is common in loopy belief propagation algo-
rithms, both availability and responsibility messages
are dampened to avoid overshooting and oscillating.
The dampening factor is ? where 0.5 ? ? < 1.
newMsg = ? ? oldMsg+ (1? ?)newMsg (19)
The APS algorithm is unsupervised. It only benefits
290
from a small development set to fine-tune a few pa-
rameters: preference values and the dampening fac-
tor. APS does not require (nor allow) specifying the
number of segments beforehand. The granularity of
segmentation is adjusted through preference values;
this reflect how likely each sentence is to be selected
as a segment centre. (This translates into the cost of
adding a segment.)
Because each message only requires the knowl-
edge about one column or row of the matrix, the al-
gorithm can be easily parallelized.
5 Experimental Setting
Datasets. We evaluate the performance of the
APS algorithm on three datasets. The first, com-
piled by Malioutov and Barzilay (2006), consists
of manually transcribed and segmented lectures on
Artificial Intelligence, 3 development files and 19
test files. The second dataset consists of 227 chap-
ters from medical textbooks (Eisenstein and Barzi-
lay, 2008), 5 of which we use for development. In
this dataset the gold standard segment boundaries
correspond to section breaks specified by the au-
thors. The third dataset consists of 85 works of fic-
tion downloaded from Project Gutenberg, 3 of which
are used for development. The segment boundaries
correspond to chapter breaks or to breaks between
individual stories. They were inserted automatically
using HTML markup in the downloaded files.
The datasets exhibit different characteristics. The
lecture dataset and the fiction dataset are challeng-
ing because they are less cohesive than medical text-
books. The textbooks are cognitively more difficult
to process and the authors rely on repetition of ter-
minology to facilitate comprehension. Since lexical
repetition is the main source of information for text
segmentation, we expect a higher performance on
this dataset. Transcribed speech, on the other hand,
is considerably less cohesive. The lecturer makes an
effort to speak in ?plain language? and to be com-
prehensible, relying less on terminology. The use of
pronouns is very common, as is the use of examples.
Repeated use of the same words is also uncom-
mon in fiction. In addition, the dataset was compiled
automatically using HTML markup. The markup
is not always reliable and occasionally the e-book
proofreaders skip it altogether, which potentially
adds noise to the dataset.
Baselines. We compare the performance of
APS with that of two state-of-the-art segmenters: the
Minimum Cut segmenter (Malioutov and Barzilay,
2006) and the Bayesian segmenter (Eisenstein and
Barzilay, 2008). The authors have made Java imple-
mentations publicly available. For the Minimum Cut
segmenter, we select the best parameters using the
script included with that distribution. The Bayesian
segmenter automatically estimates all necessary pa-
rameters from the data.
Preprocessing and the choice of similarity met-
ric. As described in Section 4, the APS algorithm
takes as inputs a matrix of pairwise similarities be-
tween sentences in the document and also, for each
sentence, a preference value.
This paper focuses on comparing globally in-
formed segmentation algorithms, and leaves for fu-
ture work the exploration of best similarity metrics.
To allow fair comparison, then, we use the same
metric as the Minimum Cut segmenter, cosine sim-
ilarity. Each sentence is represented as a vector of
token-type frequencies. Following (Malioutov and
Barzilay, 2006), the frequency vectors are smoothed
by adding counts of words from the adjacent sen-
tences and then weighted using a tf.idf metric (for
details, see ibid.) The similarity between sentence
vectors s1 and s2 is computed as follows:
cos(s1, s2) =
s1 ? s2
||s1|| ? ||s2||
(20)
The representation used by the Bayesian segmenter
is too different to be incorporated into our model di-
rectly, but ultimately it is based on the distribution
of unigrams in documents. This is close enough to
our representation to allow fair comparison.
The fiction dataset consists of books: novels or
collections of short stories. Fiction is known to ex-
hibit less lexical cohesion. That is why ? when
working on this dataset ? we work at the paragraph
level: the similarity is measured not between sen-
tences but between paragraphs. We use this repre-
sentation with all three segmenters.
All parameters have been fine-tuned on the devel-
opment portions of the datasets. For APS algorithm
per se we needed to set three parameters: the size of
the sliding window for similarity computations, the
dampening factor ? and the preference values. The
291
BayesSeg MinCutSeg APS
AI 0.443 0.437 0.404
Clinical 0.353 0.382 0.371
Fiction 0.377 0.381 0.350
Table 1: Results of segmenting the three datasets us-
ing the Bayesian segmenter, the Minimum Cut seg-
menter and APS.
parameters for the similarity metric (best variation
of tf.idf, the window size and the decay factor for
smoothing) were set using the script provided in the
Minimum Cut segmenter?s distribution.
Evaluation metric. We have measured the per-
formance of the segmenters with the WindowDiff
metric (Pevzner and Hearst, 2002). It is computed
by sliding a window through reference and through
segmentation output and, at each window position,
comparing the number of reference breaks to the
number of breaks inserted by the segmenter (hypo-
thetical breaks). It is a penalty measure which re-
ports the number of windows where the reference
and hypothetical breaks do not match, normalized
by the total number of windows. In Equation 21,
ref and hyp denote the number of reference and hy-
pothetical segment breaks within a window.
winDiff = 1N ? k
N?k?
i=1
(|ref ? hyp| 6= 0) (21)
6 Experimental Results and Discussion
Table 1 compares the performance of the three seg-
menters using WindowDiff values. On the lecture
and fiction datasets, the APS segmenter outperforms
the others by a small margin, around 8% over the
better of the two. It is second-best on the clinical
textbook dataset. According to a one-tailed paired
t-test with 95% confidence cut-off, the improvement
is statistically significant only on the fiction dataset.
All datasets are challenging and the baselines are
very competitive, so drawing definitive conclusions
is difficult. Still, we can be fairly confident that
APS performs at least as well as the other two seg-
menters. It also has certain advantages.
One important difference between APS and the
other segmenters is that APS does not require the
number of segments as an input parameter. This is
very helpful, because such information is generally
unavailable in any realistic deployment setting. The
parameters are fine-tuned to maximize WindowDiff
values, so this results in high-precision, low-recall
segment assignments; that is because WindowDiff
favours missing boundaries over near-hits.
APS also outputs segment centres, thus providing
some information about a segment?s topic. We have
not evaluated how descriptive the segment centres
are; this is left for future work.
APS performs slightly better than the other seg-
menters but not by much. We hypothesize that one
of the reasons is that APS relies on the presence of
descriptive segment centres which are not necessar-
ily present for large, coarse-grained segments such
as chapters in novels. It is possible for APS to have
an advantage performing fine-grained segmentation.
7 Conclusions and Future Work
In this paper we have presented APS ? a new algo-
rithm for linear text segmentation. APS takes into
account the global structure of the document and
outputs segment boundaries and segment centres. It
scales linearly in the number of input sentences, per-
forms competitively with the state-of-the-art and is
easy to implement. We also provide a Java imple-
mentation of the APS segmenter.
We consider two main directions for future work:
using more informative similarity metrics and mak-
ing the process of segmentation hierarchical. We
chose to use cosine similarity primarily to allow fair
comparison and to judge the algorithm itself, in iso-
lation from the information it uses. Cosine similarity
is a very simple metric which cannot provide an ad-
equate picture of topic fluctuations in documents. It
is likely that dictionary-based or corpus-based simi-
larity measures would yield a major improvement in
performance.
Reliance on descriptive segment centres may
handicap APS?s performance when looking for
coarse-grained segments. One possible remedy is to
look for shorter segments first and then merge them.
One can also modify the algorithm to perform hier-
archical segmentation: consider net similarity with
low-level segment centres as well as with high-level
ones. We plan to explore both possibilities.
292
Acknowledgements
We thank Inmar Givoni for explaining the details
of binary Affinity Propagation and for comment-
ing on our early ideas in this project. Many thanks
to Yongyi Mao for a helpful discussion on the use
Affinity Propagation for text segmentation.
References
Bishop, Christopher M. 2006. Pattern Recognition and
Machine Learning. Springer.
Blei, David and Pedro Moreno. 2001. Topic Segmenta-
tion with an Aspect Hidden Markov Model. In Pro-
ceedings of the 24th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 343?348. ACM Press.
Blei, David M., Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Choi, Freddy Y. Y. 2000. Advances in Domain Inde-
pendent Linear Text Segmentation. In Proceedings of
NAACL, pages 26?33.
Eisenstein, Jacob and Regina Barzilay. 2008. Bayesian
Unsupervised Topic Segmentation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 334?343, Honolulu,
Hawaii, October.
Frey, Brendan J. and Delbert Dueck. 2007. Clustering
by Passing Messages Between Data Points. Science,
315:972?976.
Givoni, Inmar E. and Brendan J. Frey. 2009. A Binary
Variable Model for Affinity Propagation. Neural Com-
putation, 21:1589?1600.
Haghighi, Aria and Lucy Vanderwende. 2009. Explor-
ing Content Models for Multi-Document Summariza-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 362?370, Boulder, Colorado, June.
Hearst, Marti A. 1997. TextTiling: segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23:33?64, March.
Kschischang, Frank R., Brendan J. Frey, and Hans-A
Loeliger. 2001. Factor graphs and the sum-product
algorithm. In IEEE Transactions on Information The-
ory, Vol 47, No 2, pages 498?519, February.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of ICML-01, pages 282?289.
Malioutov, Igor and Regina Barzilay. 2006. Minimum
Cut Model for Spoken Lecture Segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 25?
32, Sydney, Australia, July.
Misra, Hemant, Franc?ois Yvon, Joemon M. Jose, and
Olivier Cappe?. 2009. Text segmentation via topic
modeling: an analytical study. In 18th ACM Con-
ference on Information and Knowledge Management,
pages 1553?1556.
Oh, Hyo-Jung, Sung Hyon Myaeng, and Myung-Gil
Jang. 2007. Semantic passage segmentation based on
sentence topics for question answering. Information
Sciences, an International Journal, 177:3696?3717,
September.
Pearl, Judea. 1982. Reverend Bayes on inference en-
gines: A distributed hierarchical approach. In Pro-
ceedings of the American Association of Artificial In-
telligence National Conference on AI, pages 133?136,
Pittsburgh, PA.
Pevzner, Lev and Marti A. Hearst. 2002. A Critique and
Improvement of an Evaluation Metric for Text Seg-
mentation. Computational Linguistics, 28(1):19?36.
Stoyanov, Veselin and Claire Cardie. 2008. Topic identi-
fication for fine-grained opinion analysis. In COLING
?08 Proceedings of the 22nd International Conference
on Computational Linguistics - Volume 1, pages 817?
824.
Youmans, Gilbert. 1991. A new tool for discourse anal-
ysis: The vocabulary-management profile. Language,
67(4):763?789.
293
Summarizing Short Stories
Anna Kazantseva?
University of Ottawa
Stan Szpakowicz??
University of Ottawa
Polish Academy of Sciences
We present an approach to the automatic creation of extractive summaries of literary short
stories. The summaries are produced with a specific objective in mind: to help a reader decide
whether she would be interested in reading the complete story. To this end, the summaries give
the user relevant information about the setting of the story without revealing its plot. The system
relies on assorted surface indicators about clauses in the short story, the most important of which
are those related to the aspectual type of a clause and to the main entities in a story. Fifteen judges
evaluated the summaries on a number of extrinsic and intrinsic measures. The outcome of this
evaluation suggests that the summaries are helpful in achieving the original objective.
1. Introduction
In the last decade, automatic text summarization has become a popular research topic
with a curiously restricted scope of applications. A few innovative research directions
have emerged, including headline generation (Soricut and Marcu 2007), summarization
of books (Mihalcea and Ceylan 2007), personalized summarization (D??az and Gerva?s
2007), generation of tables-of-contents (Branavan, Deshpande, and Barzilay 2007), sum-
marization of speech (Fuentes et al 2005), dialogues (Zechner 2002), evaluative text
(Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks
2007). In addition, more researchers have been venturing past purely extractive sum-
marization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006). By
and large, however, most research in text summarization still revolves around texts
characterized by rigid structure. The better explored among such texts are news articles
(Barzilay and McKeown 2005), medical documents (Elhadad et al 2005), legal docu-
ments (Moens 2007), and papers in the area of computer science (Teufel and Moens
2002; Mei and Zhai 2008). Although summarizing these genres is a formidable challenge
in itself, it excludes a continually increasing number of informal documents available
electronically. Such documents, ranging from novels to personal Web pages, offer a
wealth of information that merits the attention of the text summarization community.
? School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave.,
Ottawa, Ontario K1N 6N5, Canada. E-mail: ankazant@site.uottawa.ca.
?? School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave.,
Ottawa, Ontario K1N 6N5, Canada. E-mail: szpak@site.uottawa.ca.
Submission received: 3 April 2007; revised submission received: 20 January 2009; accepted for publication:
29 July 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
We attempt to make a step in this direction by devising an approach to summarizing a
relatively unexplored genre: literary short stories.
Well-structured documents, such as news articles, exhibit a number of character-
istics that help identify some of the important passages without performing in-depth
semantic analysis. These characteristics include predictable location of typical items in
a document and in its well-delineated parts, cue words, and template-like structure
that often characterizes a genre (e.g., scientific papers). This is not the case in literature.
Quite the contrary?to write fiction in accordance with a template is a sure way to write
poor prose. One also cannot expect to find portions of text that summarize the main
idea behind a story, and even less so to find them in the same location. In addition, the
variety of literary devices (the widespread use of metaphor and figurative language,
leaving things unsaid and relying on the reader?s skill of reading between the lines,
frequent use of dialogue, etc.) makes summarizing fiction a very distinct task. It is
a contribution of this work to demonstrate that summarizing short fiction is feasible
using state-of-the-art tools in natural language technology. In the case of our corpus,
this is also done without deep semantic resources or knowledge bases, although such
resources would be of great help. We leverage syntactic information and shallow se-
mantics (provided by a gazetteer) to produce indicative summaries of short stories that
people find helpful and that outperform naive baselines and two state-of-the-art generic
summarizers.
We have restricted the scope of this potentially vast project in several ways. In
the course of this work we concentrate on producing summaries of short stories suit-
able for a particular purpose: to help a reader form adequate expectations about the
complete story and decide whether she would be interested in reading it. To this end,
the summary includes important elements of the setting of a story, such as the place
and the main characters, presented as excerpts from the complete story. The assump-
tion behind this definition is this: If a reader knows when and where the story takes
place and who its main characters are, she should be able to make informed decisions
about it.
With such a definition of short story summaries, re-telling the plot in the summary
is not among the objectives of this work; in fact, doing so is undesirable. We have
introduced this limitation for two reasons. There is an ?ideological? side of the decision:
Not many people want to know what happens in a story before reading it, even if this
may help them decide that the story is worth reading. There also is a practical side,
namely the complexity of the problem: Summarizing the plot would be considerably
more difficult (see Section 2 for a review of related work). We hope to tackle this
issue in the future. For now, creating indicative summaries of short stories is challenge
enough.
The summaries in Figures 1?3 illustrate our approach in the context of a naive
lead baseline and a ceiling. Figure 1 shows an example of an automatically produced
summary that meets the aforementioned criteria. A reader can see that the story is set in
a restaurant where the customers are tended to by two waitresses: the fair Aileen who
?wins hearts? and ?the-bag-o?-meal? plain-faced Tildy. If the reader chooses to pursue
the story, she will find the description of an accident of paramount importance to Tildy:
One day she is kissed by a customer in public! The event is more than flattering to
usually under-appreciated Tildy. It causes a complete change in how she views herself.
The story then unfolds to reveal that the customer was drunk on the day in question
and that he returned to apologize several days later. This apology is a severe blow to
Tildy and an abrupt end of many a dream that the incident had spurred in her head. The
story ends with Aileen trying to comfort her crying friend by saying ?He ain?t anything
72
Kazantseva and Szpakowicz Summarizing Short Stories
Figure 1
Example of a summary produced by the system.
of a gentleman or he wouldn?t ever of apologized.? Yet, the summary in Figure 1 does
not reveal these facts. For comparison, Figure 2 shows a summary obtained by taking
the same number of sentences from the beginning of the story. As the reader can see,
such a trivial approach is not sufficient to create a useful summary. Figure 3 shows a
manually created ?ideal? summary.
We experimented with a corpus of 47 stories from the 19th and early 20th century
written by renowned writers, including O. Henry, Jerome K. Jerome, Anton Chekhov,
and Guy de Maupassant. The stories, with the exception of a few fairy tales, are clas-
sical examples of short social fiction. The corpus was collected from Project Gutenberg
(www.gutenberg.org) and only contains stories in English. The average length of a story
is 3,333 tokens and the target compression rate expressed in the number of sentences
is 94%.
In order to create summaries of short stories that satisfy our stated criteria (hence-
forth indicative summaries), the system searches each story for sentences that focus
on important entities and relate the background of the story (as opposed to events).
Correspondingly, processing has two stages. Initially, the summarizer identifies two
types of important entities: main characters and locations. This is achieved using a
gazetteer, resolving anaphoric expressions and then identifying frequently mentioned
Figure 2
Example of a lead baseline summary.
73
Computational Linguistics Volume 36, Number 1
Figure 3
Example of a manual summary.
entities. Next, the system selects sentences that set out the background of the story
and focus on one of the important entities. In order to separate the background of a
story from the plot (i.e., events), we rely on the notion of aspect.1 We approximate the
aspectual type of a clause using either machine learning or manually produced rules.
This is achieved by relying on an array of verb-related features, such as tense, lexical
aspect of the main verb, presence of temporal expressions, and so on. Finally, the system
composes a summary out of the selected sentences.
Our task remains a significant challenge despite its limited scope. To produce such
indicative summaries successfully, one needs to consider many facets of the problem.
An informative data representation, computational complexity, and usability of the final
product are only some of them. Because the project is at the stage of an advanced
feasibility study, it has not been possible to do justice to all aspects of the problem.
Therefore, we concentrated on several specific issues and left many more to future work
and to fellow researchers.
Firstly, we sought to identify characteristics of short stories that could be helpful in
creating summaries. We devised an informative and practical data representation that
could be reproduced without too much cost or effort. Secondly, we restricted ourselves
to identifying the most informative portions of the stories and paid much less attention
to readability and coherence of the resulting summaries. Even though readability is an
important property, we hypothesized that informativeness is yet more important. Once
the task of identifying informative passages has been accomplished, one can work on
achieving coherence and readability. In the end, the emphasis was on the creation of
extractive summaries using established tools and methods and on the identification of
genre-specific properties that can help summarization.
The novelty of the task and the absence of agreed-upon measures for evaluating
summaries of literary prose call for a thorough evaluation using a variety of metrics.
That is why we conduct three distinct evaluation experiments. The summaries are
1 The term aspect is defined and explained in detail in Section 4. For now it suffices to say that by aspect
we mean a characteristic of a clause that gives readers an idea about the temporal flow of an event or a
state described in it. For example, the aspectual type of the sentence He likes to run is a state. The aspectual
type of He has run a marathon is an event.
74
Kazantseva and Szpakowicz Summarizing Short Stories
evaluated both extrinsically and intrinsically. They are also compared with two naive
baselines (lead and random) and two state-of-the-art summarization systems designed
for summarizing newswire (henceforth baseline summarizers).2
In the first experiment, 15 people read a mix of machine-made, random, and manual
summaries, and answer questions about them. Some questions are factual in nature
(e.g., list the main characters of the story), and others are subjective (e.g., rate the readability
of the summary). The results show that the machine-made summaries are significantly
better than the random baseline but they fall far short of the quality of the manual
summaries.
During the second evaluative experiment, the machine-made summaries are com-
pared against extracts created by people using sentence co-selection measures (pre-
cision, recall, and F-score). By sentence co-selection we mean measuring how many
sentences found in manually created extracts are selected for inclusion in automatically
produced summaries. The results suggest that our system outperforms all baselines,
including state-of-the-art summarizers.
The third part of the evaluation uses two ROUGE metrics (Lin 2004) to compare
the machine-made and the baseline summaries with the model abstracts. The results
suggest that these measures are not well suited for evaluating extractive indicative
summaries of short stories.
This paper is organized in the following manner. Section 2 gives a brief overview of
the body of research in automatic story comprehension. Section 3 describes the process
of identifying important entities in short stories. Section 4 introduces the notion of
aspect, gives an overview of the system?s design, and discusses the linguistic motivation
behind it. Section 5 describes the classification procedures (the use of machine learning
and manual rule creation) that distinguish between the descriptive elements of a story
and the passages that describe events. Section 6 reports on the evaluation of summaries
which our system produces. Section 7 draws conclusions and outlines directions for
future work.
2. Related Work
Summarization of literary prose is a relatively unexplored topic. There exists, however,
a substantial body of research tackling the problem of story comprehension. During
the 1970s and 1980s, a number of researchers in artificial intelligence built story-
understanding systems that relied in one way or another on contemporary research
in psychology and discourse processing.
Much of that line of research relied on an assumption that stories exhibit global
cognitive structure (known as macrostructure [van Dijk 1980] or schema [Bartlett 1932])
and that they can be decomposed into a finite number of cognitive units. According
to this view, diversity in stories is not due to an infinite number of plots, but to an
infinite number of combinations of a (relatively) small number of cognitive units. This
direction was pioneered in 1928 by Vladimir Propp (1968) with his detailed analysis of
100 Russian folk tales. After a period of oblivion, these ideas regained popularity: van
Dijk and Kintsch (1978) demonstrated the existence of macrostructures and their role
in story comprehension and recall; Rumelhart (1975), Thorndyke (1975), and Mandler
2 These systems are GISTexter (Harabagiu, Hickl, and Lacatusu 2007) and CLASSY (Schlesinger, O?Leary,
and Conroy 2008; Conroy, Schlesinger, and O?Leary 2007). See Section 6 for details.
75
Computational Linguistics Volume 36, Number 1
(1987) developed sets of cognitive schemas (story grammars) that could be applied to
certain types of stories; and Lehnert (1982) proposed to represent action-based stories
in terms of a finite number of plot units?configurations of affect (or emotional) states
of its characters.
The developments in psychology and discourse processing had gone hand in hand
with those in artificial intelligence and resulted in a score of story-understanding
and question-answering systems. Many of these relied on a set of manually encoded
schemas and chose the most appropriate one for a given story (Cullingford 1978; Dyer
1983; Leake 1989). For example, a system called BORIS (Dyer 1983) processed stories
word-by-word to create a very rich semantic representation of them using Memory
Organization Packets (MOPs) and Thematic Affect Units (TAUs). These knowledge
structures were activated by means of a very detailed lexicon where each lexeme was
associated with MOPs and TAUs it could invoke.
Systems such as BORIS could not process stories that did not conform to schemas
already at their disposal. Charniak and Goldman (1988) and Norvig (1989) at-
tempted to circumvent this problem by learning to recognize more general structures.
FAUSTUS (Norvig 1989) recognized six general classes of inferences by finding patterns
of connectivity in a semantic network. It could be adapted to new kinds of documents by
extending its knowledge base and not the underlying algorithm or patterns. Research in
automatic story comprehension offered a number of important solutions for subsequent
developments in artificial intelligence. No less important, it pointed out a number of
challenges. All these systems required a formidable amount of semantic knowledge and
a robust and efficient way of building a semantic representation of texts. In addition,
systems such as BORIS or SAM (Cullingford 1978) also needed a set of schemas or
schema-like scenarios. With such labor intensity, these requirements prohibit using
schema-based approaches for real-life stories (e.g., fiction) and only allow the processing
of artificially created examples.
In this historical context, our current approach to summarization of short fiction
appears rather modest: Our system does not ?understand? stories, nor does it retell
their plot. Instead, it offers the reader hints?important information about the story?s
setting?which should help her guess what type of story is to come. This assumption
appears reasonable because it has been shown that comprehension and recall of dis-
course are strongly influenced by the reader?s familiarity with the type of schema (van
Dijk and Kintsch 1978). Because our system is tailored to work with classics of the genre,
it was our expectation that the gist of the story?s setting offered to the reader in the
wording of the original would give her an idea about the story?s themes and likely plot
developments. The results of our experiments appear to back this assumption.
In addition, given the original objective, it seems reasonable that elements other
than the plot would have a strong influence on the reader?s decision to read or not to
read the story. The setting of the story, its characters, and style are some of the important
factors. Many outstanding literary works differ not so much in plot as in their setting or
moral.3 Our system attempts to capture important elements of the setting explicitly and
we expect that some elements of style may be captured implicitly due to the extractive
nature of the summaries.
3 Consider, for example, Goethe?s Faust and Bulgakov?s Master and Margarita. Although they both revolve
around the protagonist entering into a pact with the devil?albeit for different reasons?the latter takes
place in Moscow around 1930s and the two works are dramatically different.
76
Kazantseva and Szpakowicz Summarizing Short Stories
3. Identifying Important Entities
During the first stage of summary production the system identifies important entities
in stories. Initially, we planned to identify three types of entities: people, locations, and
time stamps. During a preliminary exploration of the corpus, we analyzed 14 stories for
the presence of surface indicators of characters, locations, and temporal anchors.4 We
employed the GATE Gazetteer (Cunningham et al 2002), and only considered entities
it recognized automatically.
The experiment revealed that the stories in the corpus contained multiple mentions
of characters (on average, 64 mentions per story, excluding pronouns). On the other
hand, the 14 stories contained only 22 location markers, mostly street names. Four
stories had no identifiable location markers. Finally, merely four temporal anchors
were identified in all 14 stories: two absolute (such as year) and two relative (e.g.,
Christmas). These findings support the intuitive idea that short stories revolve around
their characters, even if the ultimate goal is to show a larger social phenomenon. They
also suggest that looking for time stamps in short stories is unlikely to prove productive,
because such information is not included in these texts explicitly. That is why our system
does not attempt to identify them.
Because characters appear to be central to short stories, we designed our system
to maximize the amount of information available about them. It contains an anaphora
resolution module that resolves pronominal and noun phrase anaphoric references to
animate entities. The term anaphora, as used in this work, can be explained as a way
of mentioning a previously encountered entity without naming it explicitly. Consider
Examples 1a, 1b, and 1c from ?A Matter of Mean Elevation? by O. Henry. The noun
phrase Mlle. Giraud from Example 1a is an antecedent and the pronouns her and she from
Example 1c are anaphoric expressions or referents. Example 1c illustrates pronominal
anaphora, and Example 1b illustrates noun phrase anaphora. Here the noun phrase
the woman is the anaphoric expression which refers to the antecedent Mlle. Giraud from
Example 1a.
(1a) John Armstrongent1 and Mlle. Giraudent2 rode among the Andean peaks, enveloped
in their greatness and sublimity.
(1b) To Armstrongent1 the womanent2 seemed almost a holy thing.
(1c) Never yet since herent2 rescue had sheent2 smiled.
The anaphora resolution module only handles first and third person singular per-
sonal pronouns (I, me, my, he, his ...) and singular definite noun phrases that denote
animate entities (e.g., the man, but not men). It is implemented in Java, within the GATE
framework, using the Connexor Machinese Syntax parser (Tapanainen and Ja?rvinen
1997).
The system resolves anaphoric expressions in the following manner. Initially, the
documents are parsed with the Connexor Machinese Syntax parser. The parsed data are
then forwarded to the Gazetteer in GATE, which recognizes nouns denoting locations
and persons. The original version of the Gazetteer only recognizes named entities and
professions, but we extended it to include 137 common animate nouns such as man,
woman, soldier, or baby. During the next stage, pronominal anaphoric expressions are
4 The stories used in this exploration were later included in the training part of the data. They were never
used for testing.
77
Computational Linguistics Volume 36, Number 1
Table 1
Results of anaphora resolution.
Type of anaphora All Correct Incorrect Error rate, %
Pronominal 597 507 90 15.07
Nominal 152 96 56 36.84
Both 749 603 146 19.49
resolved using an implementation of the algorithm proposed by Lappin and Leass
(1994).5 Subsequently, anaphoric noun phrases are identified using the rules outlined by
Vieira and Poesio (2000). Finally, anaphoric noun phrases are resolved using a modified
version of the Lappin and Leass algorithm, adjusted to finding antecedents of nouns.
The implementation is described in detail in Kazantseva (2006).
A thorough evaluation of the anaphora resolution module would be prohibitively
labor-intensive. We estimated the performance of the module by manually verifying
the results it achieved on two short stories of the training set (Table 1). The error rates
for pronominal anaphora resolution are significantly lower than those for noun phrase
anaphora resolution (15.07% vs. 36.84%). This is not unexpected because resolving noun
phrase anaphora is known to be a very challenging task (Vieira and Poesio 2000). The
results also reveal that referring to characters by pronouns is much more frequent
than by noun phrases?in our case, the ratio of pronominal to nominal expressions is
almost 4:1. This suggests that resolving pronominal anaphoric expressions is crucial to
summarizing short stories.
The GATE Gazetteer, part of this module, also annotates the stories for the pres-
ence of expressions denoting locations. After resolving anaphoric expressions, char-
acters central to each story are selected based on normalized frequency counts taking
anaphoric expressions into account. The output of this module consists of short stories
annotated for the presence of location markers and main character mentions.
4. Selecting Descriptive Sentences Using Aspectual Information
4.1 Linguistic Definition of Aspect
We rely on aspect to select salient sentences that set out the background of a story. In
this paper, the term aspect denotes the same concept as what Huddleston and Pullum
(2002, page 118) call the situation type. The term refers to ?different ways of viewing
the internal temporal consistency of a situation? (Comrie 1976, page 3). Informally, the
aspect of a clause suggests the temporal flow of an event or a state and the speaker?s
position with respect to it.
A general aspectual classification based on Huddleston and Pullum (2002) appears
in Figure 4, with examples for each type.
5 Lappin and Leass (1994) present a rule-based algorithm for resolving pronominal anaphora. The
algorithm suggests the most likely antecedent after taking into account the candidates? syntactic function,
recency, and absence or presence of parallelism and cataphora with the referent. It also enforces
agreement between referent?antecedent pairs.
78
Kazantseva and Szpakowicz Summarizing Short Stories
Figure 4
Aspectual hierarchy after Huddleston and Pullum (2002).
The first distinction is between states and events. Events are processes that go on in
time and consist of successive phases (Vendler 1967, page 99). For instance, an event of
writing an essay consists of writing separate words, correcting, pausing between words,
and so on. A state of understanding each other, on the other hand, does not imply such
compositionality: It remains unchanged throughout the whole period when it is true.
In other words, the meaning of events exhibits a dynamic component, whereas that of
states does not.
Events are further categorized by whether a particular situation lasts for some time
or occurs momentarily. The latter type of events are referred to as achievements, and
events that imply duration are known as processes. For example, the nature of events
such as dropping, stumbling, or recognizing is that they occur instantaneously and,
therefore, are achievements. On the other hand, events such as playing golf or writing
an essay last for some time, so they are processes.
Processes are classified into accomplishments and activities depending on whether
a situation implies an ending (Vendler 1967, page 100). This property is known as
telicity. Reading a book in the context of Example 2a implies that the person finished
reading it: the overall situation is telic. We cannot say that she has read the book in the
first 15 minutes of doing so because the implied ending was not achieved (i.e., the book
has not been read). Such situations are referred to as accomplishments. On the other
hand, playing golf or talking on the phone does not imply that the process must end
with a specific conclusion and the situation is atelic. Such situations are called activities.
In addition, the aspectual type of a clause may be altered by multiplicity, for exam-
ple, repetitions. Consider Examples 2a and 2b.
(2a) She read a book.
(2b) She read a book a day.
Example 2b is referred to as a serial situation (Huddleston and Pullum 2002, page 123).
It is considered to be a state, even though a single act of reading a book would constitute
an event.
79
Computational Linguistics Volume 36, Number 1
Intuitively, stative?and especially serial?situations are more likely to be associ-
ated with descriptions, that is to say, with things that are, or things that were happening
for an extended period (consider He was a tall man vs. He opened the window). The
remainder of Section 4 describes how we identify single and serial stative clauses and
use them to construct summaries.
4.2 Overall System Design
Several system components are responsible for selecting salient background sentences.
A story, annotated for the presence of important entities (as outlined in Section 3), is
parsed with the Connexor Machinese Syntax parser. The sentences are then recursively
split into clauses based on the results of parsing. For the purposes of this project, a
clause is defined as a main verb as identified by the parser (whether finite or non-finite)
with all its complements, including subject, modifiers, and their constituents.
Next, each clause is represented as a vector of features describing its characteristics.
The system offers a choice: a fine-grained or coarse-grained representation. The main
difference between the two is in the level of detail at which each clause is represented.
For instance, a fine-grained feature vector has three different features with seven possi-
ble values to carry tense-related information: tense, is progressive, and is perfect, whereas
a coarse-grained vector carries only one binary feature, is simple past or present.6
Finally, the system selects salient descriptive sentences. Regardless of the granular-
ity of the representation, one may choose between two different procedures for sentence
selection. The first procedure employs machine learning techniques, namely the C5.0
decision tree induction (Quinlan 1992). The second procedure applies a set of manually
created rules that guide the classification process. Section 4.3 gives a motivation for fea-
tures used in each data set. Sections 5.1?5.3 describe the experimental setting. Section 6
presents the results.
The part of the system that selects descriptive sentences is implemented in Python.
4.3 Feature Selection: Description and Motivation
There are two criteria for the selection of features for both representations:
(Criterion 1) a clause should ?talk? about important things, such as characters or
locations
(Criterion 2) a clause should contain background descriptions rather then events
We hypothesize that sentences which satisfy both criteria are good candidates for inclu-
sion in indicative summaries. In other words, a summary that consists of such sentences
would familiarize the reader with important elements of the setting of the story, but
would not reveal the plot.
The features that contribute towards Criterion 1 can be divided into character-
related and location-related. We have designed character-related features to help iden-
tify sentences that focus on characters, not just mention them in passing. These features
are modeled so as to help identify sentences that contain at least one mention of an
important character with a salient grammatical function (e.g., subject). Location-related
6 Furthermore, in this article we refer to a data set annotated with the fine-grained features as the
fine-grained data set, and to the one annotated with the coarse-grained features as the coarse-grained
data set.
80
Kazantseva and Szpakowicz Summarizing Short Stories
Table 2
Description of the features in both data sets.
Fine-grained data set Coarse-grained data set
Type of Number of Number of Number of Number of
features features values features values
Character-related 10 18 4 6
Aspect-related 14 48 6 15
Location-related 2 4 2 4
Other 3 7 3 4
All 29 77 15 29
features are intended to help identify sentences where named entities tagged as loca-
tions by the Gazetteer indeed refer to location names.
Criterion 2 has been introduced to ensure that the selected sentences are back-
ground sentences (as opposed to those relating events) and are therefore suitable for
inclusion in indicative summaries. To this end, the features that contribute towards
Criterion 2 are designed to identify stative clauses and clauses that describe serial situa-
tions. A single unambiguous indicator of aspectual type does not exist, but a number of
verb-related characteristics of the clause may signal or limit its possible aspectual type.
These characteristics include the lexical aspect of the main verb, tense, the presence of
temporal expressions, voice, and certain properties of the direct object. The verb-related
features capture this information in our representation.7
The remainder of this section contains a detailed description of the various types of
features and motivates their inclusion. Table 2 shows how many features contribute
to each criterion, and how many discrete values they have. Appendix A contains a
complete list of features used in both representations, explains how they are computed,
and shows the cardinality of the sets of possible values.
Character-related features.Character-related features help ensure that selected sen-
tences are about one of the important characters in the story. So, this group of features
describes whether a clause contains a character mention and what its grammatical
function is (subject, object, indirect object, or other). Mentions of characters early in the
text tend to contain more salient background information. That is why character-related
features reflect the position of a parent sentence8 relative to the sentence where the
character is introduced. In addition, these features capture the presence of a character
mention that is premodified by a noun phrase. The interest in such mentions is inspired
by the fact that these constructions?appositions?often introduce new entities into the
discourse (Vieira and Poesio 2000). For the same reasons, the system also establishes
whether a character mention is nominal or pronominal (e.g., Jack vs. he), whether it is
used in the genitive case (e.g., Jack?s) and, for common nouns, whether the mention is
accompanied by an indefinite article.
7 It must be mentioned that several researchers have looked into automatically determining various
semantic properties of verbs (Siegel 1998b; Merlo et al 2002). These approaches, however, attempt to
determine properties of verbs viewed in isolation and do not deal with particular usages in the context of
concrete sentences. That is why we cannot directly re-apply that research in determining the aspectual
type of clauses.
8 By parent sentence we mean the sentence from which the clause is taken.
81
Computational Linguistics Volume 36, Number 1
Table 3
Privative featural identification of aspectual classes after Dorr and Olsen (1997).
Aspectual class Telic Dynamic Durative Examples
State + know, believe
Activity + + paint, walk
Accomplishment + + + destroy
Achievement + + notice, win
Location-related features. In discourse such as fiction, not all tokens that the
Gazetteer recognizes as markers of location denote locations. Location-related features
help identify mentions of locations in each clause and verify that these mentions indeed
denote a place. These features describe whether a clause contains a mention of a location
and whether it is embedded in a prepositional phrase. The rationale for these features is
that true location mentions are more likely to occur inside prepositional phrases, such
as from Chicago or to China.
Verb-related features. Verb-related features model the characteristics of a clause
that help determine its aspectual type.
Lexical aspect of a verb. Lexical aspect refers to a property of a verb when viewed
in isolation, without regard to the context provided by a particular clause. Just as for
clauses, a verb may be a state (or stative) verb (e.g., believe), or an event verb (e.g., run).
Event verbs are further subdivided into verbs of activity (e.g., read), accomplishment
(e.g., take a test), and achievement (e.g., drop).
The relation between the lexical aspect of a verb and the aspect of a clause has been
discussed by Vendler (1967), Dorr and Olsen (1997), and Huddleston and Pullum (2002,
pages 118?123). Dorr and Olsen have proposed a privative model of this relation?see
Table 3. The model states that verbs are categorized into aspectual classes based on
whether they exhibit one or more of the following properties: dynamicity, durativity,
and telicity. Dorr and Olsen speculate that, depending on the context of usage, verbs
may form clauses that have more of these properties than the main verb viewed in
isolation, but that it is impossible for a verb to ?shed? one of its properties. We illustrate
this in Examples 3a and 3b. In Example 3a the state verb know participates in an
accomplishment clause; the clause is telic, although the verb by itself is not. On the other
hand, an attempt to deprive the accomplishment verb destroy of its telic meaning when
constructing a clause of type activity fails to create an acceptable clause (Example 3b).
(3a) He knew it that very moment. (accomplishment)
(3b) *He was destroying it for an hour. (activity) 9
The lexical aspect of a verb influences the aspect of a clause. Several features in our
system capture this information. The fine-grained data set contains three features with
six possible values that show whether the main verb of a clause is durative, dynamic, or
telic. The coarse-grained data set contains a single feature with four possible values (the
lexical aspect of a verb according to the model in Table 3). We derive this information
from a manually compiled database of Lexical Conceptual Structures (Dorr and Olsen
1997), which contains these properties for 4,432 English verbs.
9 Throughout this paper, the asterisk (*) denotes incorrect or marginally correct usage.
82
Kazantseva and Szpakowicz Summarizing Short Stories
Grammatical tense. The grammatical tense used in a particular clause places a
number of constraints on its aspectual type. For instance, simple tenses are more
likely to be used in stative or habitual situations than progressive or perfect tenses.
It is also commonly accepted (Dowty 1979; Huddleston and Pullum 2002, page 119)
that stative clauses cannot be realized using progressive tenses (see Examples 4a and
4b). Huddleston and Pullum (2002, page 121) stipulate that it is also the case with
achievement clauses (see Example 4c).
(4a) John is running. (event, activity)
(4b) *John is knowing the answer. (state)
(4c) *John was recognizing her. (event, accomplishment)
Among the constraints that grammatical tense imposes there is the special relation
between simple present tense and event clauses. As a rule, clauses realized in simple
present tense cannot denote events, but only states (Huddleston and Pullum 2002,
page 119). The matter is illustrated in Examples 5a through 7b.
(5a) She knew history well. (state)
(5b) She knows history well. (state)
(6a) She fell off a chair. (event)
(6b) *She falls off a chair. (event)
(7a) She danced (last night). (event)
(7b) She dances. (state)
In the fine-grained data set the information related to tense is expressed using three
features with seven possible values (whether a clause is in present, past, or future tense;
whether it is progressive; and whether it is perfective). In the coarse-grained data set,
this information is expressed using one binary feature: whether a clause is in simple,
past, or present tense.
Temporal expressions. Temporal markers (often referred to as temporal adverbials),
such as usually, never, suddenly, at that moment, and many others, are widely employed
to mark the aspectual type of a sentence (Dowty 1979; Harkness 1987; By 2002). Such
markers provide a wealth of information and often unambiguously signal aspectual
type. For example:
(8a) She read a lot tonight.
(8b) She always read a lot. (or She used to read a lot.)
Such expressions are not easy to capture automatically, however. In order to use the
information expressed in temporal adverbials, we analyzed the training part of the
corpus for the presence of such expressions. There were 295 occurrences in 10 stories.
It turns out that this set can be reduced to 95 templates. For example, the expressions
this year, next year, that long year can all be reduced to a template ?some expression year?.
Possible values of ?time expression? are further restricted to allow only valid modifiers
(e.g., last, next, but not yellow). The system captures temporal expressions using a cas-
cade of regular expression. It first identifies the least ambiguous unit (in this example
83
Computational Linguistics Volume 36, Number 1
year) and then attempts to find the boundaries of the expression. The complete list of
regular expressions used appears in Kazantseva (2006).
Three features characterize each template: the type of the temporal expression
(location, duration, frequency, or enactment) (Harkness 1987); magnitude (year, day,
etc.); and plurality (year vs. years). The fine-grained data set contains three such fea-
tures with 14 possible values (type of expression, its magnitude, and plurality). The
coarse-grained data set contains one binary feature (whether a clause contains an ex-
pression that denotes a long period of time).
Voice. Usually, clauses in passive voice only occur with events (Siegel 1998b,
page 51). Both data sets contain one binary feature that describes this information.
Properties of the direct object. For some verbs, properties of the direct object help
determine whether a given clause is stative or dynamic.
(9a) She wrote a book. (event)
(9b) She wrote books. (state)
It is of particular interest whether the direct object follows a definite or indefinite
determiner and whether it is used in a singular or plural form. Two binary features
that describe this information are included in the fine-grained data set.
Several additional features in both data sets describe the overall characteristics of
a clause and its parent sentence, such as whether these were affirmative statements,
exclamations, or questions; their index in the text; and a few others. The fine-grained
data set contains three such features with seven possible values. The coarse-grained
data set contains three features with four values.
4.4 Handling Clauses with the Verb Have
The preceding section notes that the same verb may form clauses of different aspectual
types depending on its context. A verb with a particularly ambiguous aspect is the
verb have (when used as the main verb and not an auxiliary). Its meaning is strongly
influenced by what kind of direct object it takes. That is why determining its aspectual
type is a very challenging task. This issue is illustrated in Examples 10a?10c.
(10a) She had lunch. (event).
(10b) She had a friend. (state).
(10c) She had an accident. (event).
Due to the high degree of ambiguity, our system handles clauses with have as the
main verb in a manner different from all other clauses. This machinery remains the
same regardless of what options are used for the granularity of representation and for
sentence selection procedures.
In order to handle have-clauses, our system contains an implementation of an ap-
proach proposed by Siegel (1998a). The solution relies on WordNet (Fellbaum 1998) and
contains a set of rules that determine the aspectual type of a have-clause based on the
top WordNet category of the direct object. For instance, the direct object lunch from
Example 11a belongs to the category food and, according to rules from Siegel (1998a),
the aspectual type of a clause is event. The direct object friend from Example 11b belongs
to the category person, so the aspectual type of the clause is state. Siegel (1998a) used
WordNet 1.6, whereas we work with a newer version, WordNet 2.0. The structure of
84
Kazantseva and Szpakowicz Summarizing Short Stories
Figure 5
Pseudo-code for determining the type of have-clauses based on the WordNet category of direct
object (Siegel 1998b).
this newer ontology is different from that of version 1.6. For this reason, we consider
all parent categories in the hypernym tree, not only the top category. For the sake of
completeness, Figure 5 shows the pseudo-code for this procedure. The system judges a
have-clause to be summary-worthy if two conditions are fulfilled: the clause contains a
mention of one or more important characters and it is a state clause.
5. Experiments
5.1 Experimental Setting
The final version of the summarizer proceeds as follows. First of all, the stories are
parsed with the Connexor parser and named entities are recognized using the GATE
Gazetteer. Then the system resolves anaphoric references and identifies important char-
acters and locations. During the next stage, the summarizer splits all source sentences
into clauses and creates coarse- and fine-grained representations for each clause. A
clause is modeled as a vector of character-, location- and verb-related features. Finally,
the system employs two alternative procedures to select summary-worthy sentences:
manually designed rules and machine learning.
We performed a number of experiments to find out how successful our system is
in creating summaries of short stories. The experimental corpus consisted of 47 short
stories split into a training set of 27 stories and a test set of 20 stories. The average length
of a story in the corpus was 3,333 tokens, 244 sentences, or approximately 4.5 U.S.-letter-
sized pages. The corpus contains stories written by 17 different authors. It was split
manually so that its training and test portions contained approximately an equal num-
ber of stories by the same writer. The first author of this paper annotated each clause of
every story for summary-worthiness and achieved the compression rate of 6%, counted
in sentences. This rate was the target compression rate in all further experiments.
The training data set consisted of 10,525 clauses, 506 of which were annotated as
summary-worthy and all others as not summary-worthy. The test data set contained
7,890 clauses, 406 of them summary-worthy.
We fine-tuned the system and used the training portion of the data set to iden-
tify the best settings. Then we ran two sets of experiments on the test portion. In the
first set of experiments, we applied a manually designed set of rules that select sen-
tences for possible inclusion in summaries. These experiments are described in Sec-
tion 5.2. The second set of experiments relied on using machine-learning techniques
to create summaries. It is described in Section 5.3. After the completion of the experi-
ments, the summaries were evaluated by six judges. They were also compared against
85
Computational Linguistics Volume 36, Number 1
Figure 6
Examples of manually designed rules.
extractive summaries produced by three people. Section 6 discusses the evaluation
procedures in detail and reports the results.
5.2 Experiments with Manually Designed Rules
The first classification procedure applies manually designed rules to a clause-level
representation of the original stories to produce descriptive summaries. The rules are
designed using the same features as those used for machine learning and described in
Section 4.3 and in Appendix A.
The first author created two sets of rules to guide the sentence classification process:
one for the coarse-grained and another for the fine-grained representation. The rules
operate at clause level. If a clause is deemed summary-worthy, the complete parent
sentence is included in the summary. Figure 6 displays a few examples of rules for the
fine-grained data set (a clause is considered to be summary-worthy if a rule returns
True). The first rule attempts to select clauses that talk about one of the main characters
and contain temporal expressions of type enactment. The rationale for this rule is that
such clauses are likely to describe habitual activities of protagonists (e.g., He always
smoked.) The second rule follows the same rationale but the stativity of the situation
is signaled by the main stative verb. The third rule rejects clauses in progressive tense
because such clauses are unlikely to contain background information.
The set of rules for the fine-grained representation has a tree-like structure. It
processes the features of a clause and outputs a binary prediction. The rules for the
coarse-grained representation function differently. Each clause is assigned a score based
on the values of its features. The system then selects 6% of sentences that contain clauses
with the highest scores. The scores attributed to the particular feature values were
assigned and fine-tuned manually using linguistic knowledge described in Section 4.3.
The reasons why the procedures for the two data sets differ are as follows. Assigning
and fine-tuning the scores is a more flexible process and it is easier to perform manu-
ally. Ideally, we would apply score-based rules to both representations, but assigning
and fine-tuning the scores manually for the fine-grained data set is excessively labor-
intensive: there are too many features with too many values. For instance, one may
want to reward clauses in simple past or present tenses, reflecting the fact that such
clauses are more likely to be descriptive than those in perfect or progressive tenses.
This information is expressed in the coarse-grained data set using one binary feature
simple past present and fine-tuning the score is trivial. On the other hand, the same
86
Kazantseva and Szpakowicz Summarizing Short Stories
Figure 7
High-level overview of the rules for the fine-grained data set.
information in the fine-grained data set is distributed over three features with a total
of seven values: is perf (yes, no), is progressive (yes, no), and tense (past, present. future).
Distributing the ?reward? among three independent features is far less obvious.
The rules in both data sets, as well as the set of weights used for the coarse-grained
representation, were selected and fine-tuned empirically using the training portion of
the corpus as a guide. Once the parameters had been adjusted, the system produced
two sets of summaries for the test portion of the corpus (one for each representation).
The detailed algorithms for both data sets are too long for inclusion in this article.
Figures 7 and 8 show the rationale for the algorithms. The interested reader is referred
to Kazantseva (2006) for pseudo-code.
5.3 Experiments with Machine Learning
As an alternative to rule construction, in the second set of experiments we performed
decision tree induction with C5.0 (Quinlan 1992) to select salient descriptive sentences.
C5.0 was our choice mainly because of the readability of its output.
The training and test data sets exhibited an almost 1:17 class imbalance (i.e., only 6%
of all annotated clauses belonged to the positive class). Because the corpus was rather
small, we applied a number of techniques to correct class imbalance in the training data
set. These techniques included classification costs, undersampling (randomly removing
instances of the majority class), oversampling (randomly duplicating instances of the
minority class), and synthetic example generation (Chawlar et al 2002). Using tenfold
cross-validation on the training data set and original annotations by the first author,
Figure 8
High-level overview of the rules for the coarse-grained data set.
87
Computational Linguistics Volume 36, Number 1
we selected the best class-imbalance correction techniques for each representation and
also fine-tuned the learning parameters available in C5.0. These experiments brought
the best results when using classification costs for the coarse-grained data set and
undersampling for the fine-grained data set.
In order to see what features were most informative in each data set, we conducted
a small experiment. We removed one feature at a time from the training set and used the
decrease in F-score as a measure of informativeness. The experiment showed that in the
coarse-grained data set the following features were the most informative: the presence
of a character in a clause, the difference between the index of the current sentence and
the sentence where the character was first mentioned, syntactic function of a character
mention, index of the sentence, and tense. In the fine-grained data set the findings are
similar: the index of the sentence, whether a character mention is a subject, the presence
of a character mention in the clause, and whether the character mention is a pronoun
are more important than the other features.
After selecting the best parameters on the training data set using tenfold cross-
validation, the system produced two sets of summaries for the test data set.
6. Evaluation
6.1 Overview
We are not aware of any agreed-upon metrics for evaluating summaries of short fiction.
In fact, it is not wholly clear what makes one summary better than another even for
manual ones. That is why we evaluate our summaries using a variety of metrics and
baselines, hoping to obtain a stereoscopic view of their quality.
The first evaluation experiment aims to measure the informativeness and the use-
fulness of the summaries. It is designed so as to compare the machine-made summaries
with a random baseline and also with a ?ceiling?? manual abstracts (henceforth model
summaries). To achieve this, we engaged 15 evaluators to read the summaries and
the stories of the test set and to answer two types of questions about them: factual
(e.g., list main characters of the story) and subjective (e.g., rank readability of the summary).
Such experimental design allowed us to evaluate extrinsically the informativeness of
the summaries and intrinsically their usefulness. Both types of questions were asked
first after reading the summary alone and then after reading the complete story. The
summaries are an anonymous mix of random, machine-made, and model ones (i.e., the
evaluators did not know whether the summaries were produced by programs or by
people). Section 6.2 describes the experiment in detail.
The second round of evaluation aimed to evaluate the summaries by measuring
sentence co-selection with the manually created extracts. It was designed to allow
the comparison of machine-made summaries with two naive baselines and with two
state-of-the-art generic summarizers (baseline summarizers). Section 6.3 contains the
description of this experiment.
The third evaluation experiment compared the machine-made and the baseline
summaries with the manually created abstracts using ROUGE (Lin 2004)?a package
for automatically evaluating summaries. This experiment is described in Section 6.4.
6.2 Evaluating Informativeness and Usefulness of the Summaries
We define the objectives of this experiment as measuring the informativeness, the
usefulness and, to some extent, the linguistic quality of the summaries that our system
88
Kazantseva and Szpakowicz Summarizing Short Stories
produces. The informativeness is measured indirectly?by asking people factual
questions about the story. The linguistic quality and the usefulness are evaluated
intrinsically?by asking people to rank specific characteristics of the summaries.
Fifteen unbiased evaluators answer both types of questions twice, first after reading
the summary alone and then again after reading the complete story?repeating the
procedure for all 20 test stories. Asking the questions after reading the summary
alone measures the informativeness and the usefulness of the summaries in a realistic
situation: To an evaluator, the summary is the only source of information about the
original story. Repeating the procedure after reading the complete story evaluates the
summaries in a situation where the evaluator has the complete information about
the source. Each evaluator works with a mix of machine-made, random, and model
summaries with six or seven summaries of each kind. This allows comparing the
performance of our summarizer with a baseline and a ceiling.
Our summarizer produces four different flavors of summaries.10 The labor intensity
of the process prohibits asking the subjects to evaluate all four summary types. That is
also why it is not possible to use more than one baseline or the summaries created
by the baseline systems.11 Restricted to evaluating only one type of the machine-made
summaries, we opt for the coarse-grained rule-based ones, mainly because the coarse-
grained representation and the rules make it easier to trace why the system selects
specific sentences.
Conditions to be tested. We evaluate three factual and four subjective characteristics
of the summaries.
Factual
How well the reader can name
 the main characters
 the location
 the time
when the summary is the only source of information about the story.
Subjective
 How readable the summaries are.
 How much irrelevant information they contain.
 How complete they are.
 How useful they are for deciding whether to read the complete story.
Evaluating these facets of the summaries reveals whether we achieve the objective of
producing informative summaries. The focus of the system was not on readability. Still,
we evaluate how readable the summaries are, because severe lack of coherence may
prevent people from correctly interpreting the available information. We have provided
10 The options are as follows: either the coarse-grained or the fine-grained representation, and selecting
sentences using either rules or machine learning.
11 Each additional summary type would have required approximately 67 extra man-hours.
89
Computational Linguistics Volume 36, Number 1
no definitions or criteria to the subjects apart from the questions shown in Tables 4
and 5.
Baselines. We compare the machine-made summaries with a baseline and a ceiling.
The baseline consists of randomly selected sentences. Both the machine-made and the
random summaries contain the same number of sentences. The ceiling consists of the
summaries written by two human subjects. The summary writers were instructed to
write 20 summaries of short stories in a way that does not reveal all of the plot. They
received one example summary and were allowed to reuse sentences from the stories,
to employ metaphor and any other literary devices they found useful.
Metrics. The evaluators answered the factual questions about the main characters
and the location of the story in their own words. The first author rated the answers
on the scale of ?1 to 3. A score of 3 means that the answer is complete and correct,
2 = slightly incomplete, 1 = very incomplete, 0 = the subject cannot find the answer
in the text, and ?1 = the answer is incorrect. The question asking to identify the time
frame of the story is a multiple-choice one: select the century when the story takes place.
The answers to this question are rated on a binary scale (1 if the answer is correct,
0 if it is not or if the subject cannot infer time from the text). We calculate the mean
answers for each question and compare them across summary types using the Kruskal?
Wallis test and the Mann?Whitney test (also known as the Wilcoxon Rank?Sum test).
The tests are appropriate when the response variable is ordinal and the dependent
variable is categorical. Both tests are based on assigning ranks to the available data
points.
The Kruskal?Wallis test is a nonparametric test used to determine whether several
samples come from the same population. It is based on calculating the K statistic which
follows ?2 distribution for sample sizes of five or larger. Given i samples containing
ti data points each with Ri being the sum of ranks of all data points in sample ti, K is
calculated as follows (Leach 1979, page 150):
K = 12
n(n+ 1)
?
ti(
Ri
ti
? n+ 1
2
)2 (1)
In order to make pairwise comparisons between samples and to establish the locus
of the difference, we rely on the Mann?Whitney test. The test is based on calculating the
S statistic. For large sample sizes the distribution of S can be approximated using the
normal distribution.
S = 2R? t2(n+ 1) (2)
where t2 is the size of the smaller sample, n is the size of both samples together, and R
is the sum of ranks in the smaller sample. We use the Kruskal?Wallis test with 0.01 con-
fidence level. In order to avoid increasing the chance of Type I error when performing
pairwise comparisons, we set per-comparison confidence level for the Mann?Whitney
test at ? = ?/c where ? is the desired per-experiment confidence level and c is the
number of comparisons (Leach 1979, page 161). In our case ? = 0.0033.
All subjective questions are multiple-choice questions. An evaluator has to select a
score of 1 to 6, with 1 indicating a strong negative property and 6 indicating a strong
positive property. We opt for a scale with an even number of available values so as
to avoid the evaluators? giving excessive preference to the middle rank. We measure
the mean ranks for each question and compare them across summary types using the
Kruskal?Wallis and Mann?Whitney tests. The inter-annotator agreement is computed
90
Kazantseva and Szpakowicz Summarizing Short Stories
using Krippendorff?s ? (Krippendorff 2004, pp. 221?236) (henceforth ?). ? measures
disagreement between annotators corrected for chance disagreement.
? = 1 ?
Disagreementobserved
Disagreementexpected
= 1 ?
Average metric ?2 within all categories
Average metric ?2 within all items
(3)
Unlike other coefficients of inter-coder agreement, ? allows taking into account the
magnitude of disagreement by specifying a distance metric ?2. This property is crucial
in our case: a situation when raters disagree whether to give a summary a rank of 1 or 6
should be penalized more heavily than a situation when they do not agree between
the ranks of 5 and 6. When computing ?, we use the distance metric suggested by
Krippendorff for ordinal data (Krippendorff 2004, page 223):
ordinal ?2 = (
nc
2
+
g<k
?
g>c
ng +
nk
2
)2 (4)
where c and k, c < k, are the two ranks.
For all questions, the computation of ? is based on the following parameters: N =
300, n = 15, and c = 6, where N is the total number of items (i.e., summary?story pairs
ranked), n is the number of raters, and c is the number of available categories.
Subjects. The participants for the experiment were recruited by the means of adver-
tising at the Department of Linguistics at the University of Ottawa. Most of them are
third- and fourth-year undergraduate students of linguistics. The only requirement for
participation was to be a native speaker of English. We hired two people to create model
summaries for the 20 stories of the test set. The summary writers worked approximately
15?20 hours each. Fifteen people were hired to evaluate the summaries (i.e., to read
the summary?story pairs and answer the questions). The task of evaluating a sum-
mary required approximately 12?15 hours of labor per person. All participants were
paid. The instructions for summary writers are available at www.site.uottawa.ca/
?ankazant/instructions-writers.zip. The instructions for evaluators can be found
at www.site.uottawa.ca/?ankazant/instructions eval.zip.
Material. Each evaluator received 20 summary?story pairs. Because some questions
sought to measure the informativeness of the summary, every evaluator worked on 20
distinct stories of the test set and no one worked with the same story more than once.
The summaries were a randomly selected mix of random, machine-made, and model
summaries.
Procedure. The experiment was conducted remotely. The summary writers received
the test set of stories and the instructions and had seven working days to submit their
abstracts. A week later, we sent randomly generated packages of summary?story pairs
to the evaluators. The packages contained between six and seven summaries of each
kind (random, machine-made, and model). Each evaluator worked with exactly one
summary for each story, reading a total of 20 pairs. Every summary was evaluated by
five subjects. The evaluators had seven working days to complete the task.
Results. Informativeness. Table 4 shows the results of comparing the mean an-
swers between the machine-made, the baseline, and the model summaries using the
Kruskal?Wallis and Mann?Whitney tests. The column Groups shows homogeneous
groups, identified using the Mann?Whitney test with 99.67% confidence (recall that
per-comparison confidence level ? = 0.0033). The groups are denoted using distinct
literals (e.g., A, B, C).
91
Computational Linguistics Volume 36, Number 1
Table 4
Answers to factual questions.
Summary type After reading the summaries only After reading the stories
Mean rank Groups Std. Dev Mean rank Groups Std. Dev
Question: Name three main characters in the order of importance.
Model 2.24 A 0.73 2.73 A 0.49
Machine 2.21 A 0.69 2.71 A 0.56
Random 1.42 B 1.04 2.67 A 0.62
Question: Name the location of the story.
Model 2.1 A 1.25 2.62 A 0.93
Machine 1.39 B 1.33 2.79 A 0.62
Random 0.71 C 1.18 2.43 A 0.98
Question: Select the century when the story takes place.
Model 0.5 A 0.5 0.69 A 0.46
Machine 0.29 B 0.46 0.76 A 0.43
Random 0.19 B 0.39 0.7 A 0.54
The differences between the machine-made summaries and the random ones
are significant for the questions about characters and the location of the story. This
shows that in these respects the machine-made summaries are?rather predictably?
consistently more informative than the random ones. The difference between the
machine-made and the random summaries is not statistically significant for the question
asking to name the time of the story. Keeping in mind how rare absolute temporal
anchors are in short stories, this is not surprising. The manual summaries, however,
are ranked higher with statistical significance. This may suggest that the machine-made
summaries are not as coherent as the model ones, which prevents the reader from
finding implicit cues about timeframe available in the summaries.
The differences between the machine-made and the model summaries are signifi-
cant for the questions about the time and the place of the story, but not for the questions
about the main characters. This suggests that the machine-made summaries are almost
as informative as the model ones when it comes to informing the reader whom the story
is about. They cannot, however, give the reader as good an idea about the time and the
place of the story as the model summaries can.
All summary types are less informative than the complete story; that is, the differ-
ences between answers obtained after reading the summary alone and after reading the
complete story are significant in all cases.
Usefulness and linguistic quality. Table 5 shows mean ranks for the three summary
types, along with the homogeneous groups identified using the Mann?Whitney test,
with 99.67% confidence. The request to rank readability was made only once?after
reading the summary; the request to evaluate the completeness was made only after
reading the complete story. (The corresponding cells in Table 5 are empty.)
The results reveal that the evaluators consistently rank the model summaries as
best, the machine-made summaries as second-best, and the random ones as worst. The
differences between summary types are significant in all cases.
The readability of the machine-made summaries is ranked as slightly better than
average (3.28 on the scale of 1 to 6). For all other questions (relevance, completeness, and
92
Kazantseva and Szpakowicz Summarizing Short Stories
Table 5
Subjective rankings.
Summary type After reading the summaries only After reading the stories
Mean rank Groups Alpha Mean rank Groups Alpha
Question: How readable do you find the summary?
(Is it coherent and easy to read, or confusing and does not make sense?) (scale: 1 to 6)
Model 5.44 A 0.74
Machine 3.28 B
Random 1.89 C
Question: How much irrelevant information does the summary contain?
(useless, confusing information, fragments that do not make sense) (scale: 1 to 6)
Model 5.10 A 0.61 5.24 A 0.62
Machine 2.83 B 2.82 B
Random 1.93 C 1.85 C
Having read the story, do you find that a lot of important information is missing?
Rate how complete you find the summary. (scale: 1 to 6)
Model 5.18 A 0.69
Machine 2.81 B
Random 1.65 C
Imagine that this summary should help you decide whether you would like to read the complete
story. How helpful was the summary for this purpose? (scale: 1 to 6)
Model 5.22 A 0.60 5.11 A 0.63
Machine 2.81 B 2.81 B
Random 1.88 C 1.65 C
usefulness), the machine-made summaries are ranked as slightly worse than average
(around 2.81). This shows that even though the summaries are somewhat useful and
consistently outperform the random baseline, they fall short of the quality of the manual
abstracts. This is hardly surprising given the inherent difficulty of summarizing fiction
and the exploratory nature of this work. It is worth remarking that even the model
summaries do not appear to be perfect: The evaluators ranked them around 5.20, even
though they had significantly worse summaries to compare against. This may suggest
that the task is not easy even for people, let alne for a computer program.
The column labelled Alpha in Table 5 shows the results of measuring the extent to
which the evaluators agree when answering the subjective questions.12 The agreement
is measured using Krippendorff?s ?. The results show substantial agreement but fall
short of the reliability cut-off point of 0.8 suggested by Krippendorff. The failure to
reach such high agreement is hardly surprising: the task of ranking the quality of
the summaries is highly subjective. Instead of asking the subjects to bin items into
a predefined number of categories, the task calls for discretizing a concept which is
12 We have not measured the agreement for factual questions because those questions were answered in the
evaluators? own words and the answers were scored by the first author. To give the reader an idea of
variability of the answers, Table 4 reports standard deviation from the mean.
93
Computational Linguistics Volume 36, Number 1
continuous in nature: the quality of a summary. That is why we interpret the level of
agreement as sufficient for the purpose of evaluating the quality of the summaries.
6.3 Comparing the Machine-Made Summaries and the Manually Created Extracts
Measuring sentence co-selection between extractive summaries created by humans and
those created by automatic summarizers has a long tradition in the text summarization
community (Lin and Hovy 2000; Marcu 2000), but this family of measures has a number
of well-known shortcomings. As many have remarked on previous occasions (Mani
2001; Radev et al 2003), co-selection measures do not provide a complete assessment of
the quality of a summary. First of all, when a summary in question contains sentences
that do not appear in any of the model extracts, one may not be sure that those sentences
are uninformative or inappropriate for inclusion in a summary. In addition, documents
have internal discourse structure and sentences are often inter-dependent. Therefore,
even if a summary contains sentences found in one or more reference summaries, it
does not always mean that it is advisable to include those sentences in the summary in
question.
Sentence co-selection does not explicitly measure the quality of a summary. It does,
however, measure a quality that is objective and easy to pin down: how many sentences
that humans judge summary-worthy are included in the machine-made summary.
Such a metric is a useful complement to the results reported in Section 6.2. It has the
advantage of being easy to interpret and comprehend. It also has a long tradition of
usage which allows us to compare our summarizer?on a familiar scale?with other
summarization systems. That is why we chose co-selection as the basis for comparing
the summaries that our system produces with manually created extracts.13
Overview. The experiment involves six annotators divided into two groups of three.
Each annotator is asked to read 10 short stories and to select 6% of sentences that, in their
opinion, constitute a good indicative summary. In this manner three people annotate
each story of the test set for summary-worthy sentences. We used their annotations as a
gold standard and compared the machine-made summaries against them. In addition,
we used the same gold standard and metrics to evaluate the quality of two baseline
summaries and of two summaries produced by state-of-the art summarization systems.
Conditions to be tested. The purpose of the experiment is defined as measuring how
many sentences found in our system?s summaries and the baseline summaries occur in
the extractive summaries created by the human annotators. We are also interested in
finding out whether our summarizer outperforms the trivial baseline algorithms and
the existing state-of-the-art summarizers fine-tuned to summarizing newswire.
Baselines. In order to evaluate our summarizer comparatively, we defined two naive
baselines and a ceiling. Intuitively, when a person wishes to decide whether to read
a book, she opens it and flips through several pages at the beginning. Imitating this
process, we computed a simple lead baseline consisting of the first 6% of the sentences in
a story. The second baseline consists of 6% of sentences of the story selected at random.
The ceiling consists of all sentences deemed summary-worthy by one of the human
annotators.
13 We decided against using deeper approaches, such as the Pyramid method (Nenkova and Passonneau
2004), factoids (van Halteren and Teufel 2003), and relative utility (Radev and Tam 2003). The reason is
practical: These approaches have an unfortunate disadvantage of being considerably more
labor-intensive than the measures based on sentence co-selection.
94
Kazantseva and Szpakowicz Summarizing Short Stories
It is also necessary to see whether our genre-specific approach shows any improve-
ments over the existing generic state-of-the-art systems put to work on fiction. To this
end, we compared our summarizer with two systems that were top performers in the
Document Understanding Conference (henceforth DUC) 2007, the annual ?competi-
tion? for automatic summarizers. In DUC competitions the summarization systems
are evaluated on a variety of metrics: manually assigned scores (ranking readability,
grammaticality, non-redundancy, referential clarity, focus, and coherence), the pyramid
method (Nenkova and Passonneau 2004), and ROUGE scores (Lin 2004). There is no
unified ranking of the systems? performance, and selecting the best summarizer is
not straightforward. We chose two systems among the top performers in DUC 2007?
GISTexter (Harabagiu, Hickl, and Lacatusu 2007) and CLASSY (Schlesinger, O?Leary,
and Conroy 2008; Conroy, Schlesinger, and O?Leary 2007). GISTexter appears to be
the best summarizer according to the scores assigned by the human judges. Apart
from baselines, it is consistently ranked as the best or the second-best system on most
characteristics evaluated by the judges (the only exception is non-redundancy where
GISTexter is ranked eighth). CLASSY, on the other hand, is one of the four top systems
according to ROUGE scores. The scores it received from the human judges are also
quite good.
The main task in DUC 2007 called for creating 250-word summaries from a collec-
tion of newswire articles on a specific topic. Each input collection was accompanied by
a topic statement that briefly explained what the summaries should cover. Therefore,
both CLASSY and GISTexter are geared towards multi-document query-based sum-
marization of newswire?a task dramatically different from that of summarizing short
fiction.14 No adjustments were made to either system to make them more suitable for
summarizing stories. Therefore, the comparison is not wholly fair, but?in the absence
of systems similar to ours?it was the only possibility to compare our summarizer with
the state-of-the-art in the community and to verify whether genre-specific methods are
useful in summarizing fiction.
Evaluation metrics. By combining summaries created by the annotators in several
ways we create three distinct gold-standard summaries. The majority gold-standard
summary contains all sentences selected by at least two judges. It is best suited to give
an overall picture of how similar the machine-made summaries are to the man-made
ones. The union gold standard is obtained by considering all sentences that are judged
summary-worthy by at least one judge. Union summaries provide a more relaxed
measurement. Precision computed on the basis of the union gold standard gives an idea
of how many irrelevant sentences a given summary contains (sentences not selected by
any of the three judges are more likely to prove irrelevant). The intersection summaries
are obtained by combining sentences that all three judges deemed to be important.
Recall measured on the basis of the intersection gold standard says how many of the
most important sentences are included in summaries produced by the system (sentences
selected by all three judges are likely to be the most important ones). All summaries
are compared against each gold-standard summary using precision (P), recall (R), and
equally-weighted F-score (F).
Recall = TP
TP+ FN
(5)
14 GISTexter in particular relies on extensive syntactic and semantic query decomposition and, thus, is at a
disadvantage when no informative query is provided.
95
Computational Linguistics Volume 36, Number 1
Precision = TP
TP+ FP
(6)
F = 2TP
2TP+ FP+ FN
(7)
TP denotes true positives, FP = false positives, TN = true negatives, and FN = false
negatives.
The statistical significance of the differences between various types of summaries
is established using the one-way Analysis Of Variance test (ANOVA) and the Tukey
Honestly Significant Differences test (henceforth Tukey HSD).
ANOVA tests whether the differences between sample means are significant by
comparing variance between samples with total variance within samples:
f =
S 2between/(p? 1)
S 2within/(n? 1)
(8)
where p is the number of samples, n is the total number of observations, Sbetween is the
sum of squared deviations between sample means and the total mean, and Swithin is
the total sum of squared deviations within samples. The f statistic is distributed as F
when the null hypothesis is true (i.e., the differences between sample means are not
significant).
The power of ANOVA extends as far as verifying whether the differences between
sample means are significant overall, but the test does not say anything about differ-
ences between particular pairs of sample means. Tukey HSD is a test that does just that.
It measures q, the studentized range statistic:
q =
Ml ?Ms
SE
(9)
where Ml is the larger of the sample means, Ms is the smaller one, and SE is the standard
error of the data in question. In accordance with our interpretation of the three types of
gold standards, we use the most meaningful measurement for each standard: F-score
for the majority, precision for the union, and recall for the intersection. We also use the
same measurement to set the ceiling for each standard (i.e., by choosing the manually
created extract that compares best on that scale).
Before combining the extracts created by the annotators into gold-standard sum-
maries, we measure how well these people agree among themselves. We estimate
the agreement using Scott?s ? (Scott 1955).15 This coefficient measures the observed
agreement between judges corrected for chance agreement.
? =
Agreementobserved ? Agreementexpected
1 ? Agreementexpected
(10)
15 The coefficient is also known as Siegel and Castellan?s ? (Siegel and Castellan 1988).
96
Kazantseva and Szpakowicz Summarizing Short Stories
Table 6
Inter-annotator agreement on selecting summary-worthy sentences.
Statistic Group 1 Group 2 Average
?(4) 0.52 0.34 0.43
?(3) 0.50 0.34 0.42
Agreementobserved =
1
ic(c? 1)
?
i?I
?
k?K
nik(nik ? 1) (11)
Agreementexpected =
1
(ic)2
?
k?K
n2k (12)
where i is the number of items to be classified in set I, k is the number of available
categories in set K, c is the number of coders, nik is the number of coders who assign
item i to category k, and nk is the total number of items assigned to category k by all
annotators (Artstein and Poesio 2008, pp. 562?563).
Subjects. Six human subjects participated in annotating the test set of stories for the
presence of summary-worthy sentences. These people are colleagues and acquaintances
of the first author. At the time of the experiment none of them was familiar with the
design of the system. Four annotators are native speakers of English and the remaining
two have a very good command of the language.
Materials. The material for the experiment consisted of the 20 stories of the test set.
Three annotators created extractive summaries for each story. In addition, there were
eight distinct automatically produced summaries per story: four summaries produced
by our system, two baseline summaries, and two summaries created by the baseline
systems from DUC.
Procedure. The experiment was conducted by e-mail. The annotators received the
stories and had two weeks to annotate them. The participants reported having taken
10?20 hours to complete the task.
Results. Agreement. Table 6 reports the agreement between the judges within each
group and with the first author of this article. The agreement with the first author is
reported because she created the initial training and test data for experiments. The num-
bers 3 and 4 state whether the statistic is computed only for three subjects participating
in the evaluation or for four subjects (including the first author). As can be seen from
Table 6, the agreement statistics are computed for each group separately. This is because
the sets of stories that they annotated are disjoint. The ?Average? column shows an
average of these figures, to give a better overall idea.
The agreement values in Table 6 are rather low. They fall well below the 0.8 cut-off
point specified by Krippendorff (2004). On a less demanding scale, Landis and Koch
(1977) interpret values in the range of 0.21?0.4 as fair agreement and in the range of
0.41?0.6 as moderate agreement.16 Weak agreement is not surprising: Many researchers
16 Krippendorff?s suggestion refers to ?, rather than Scott?s ?, and Landis and Koch?s scale was created for
Cohen?s ? (Cohen 1960). In our setting, however, the values of ?,? and ? are almost the same.
97
Computational Linguistics Volume 36, Number 1
report that people do not agree well on what sentences constitute a good summary of
a document (Rath, Resnick, and Savage 1961; Salton et al 1997; Lin and Hovy 2003). In
most cases the agreement corresponding to ? of 0.42 would not be sufficient for creating
a resource, but we interpret this level of agreement as acceptable for evaluating a single
facet of the summaries that are also evaluated in other ways.
Co-selection. Tables 7?9 show the results of comparing eight different versions of the
computer-made summaries against the gold-standard summaries produced by people.
In each table, the entry HUMAN corresponds to the summaries created by the annotator
who achieves the highest scores for the corresponding standard. The ?Groups (metric)?
column reports homogeneous groups identified using Tukey HSD with 95% confidence
for the specified metric.
Our system outperforms both baseline algorithms and the baseline summarizers,
but it always falls short of the performance of the best human summary. The improve-
ment margins between the random and the baseline systems? summaries and those
produced by our system are rather wide. The weaker performance of the baseline
summarizers strongly suggests the need for genre-specific methods when summarizing
short fiction.
The differences between the lead summaries and the system-made ones are also
statistically significant, yet they are much narrower. We interpret this as an indication
that the lead baseline is more demanding than the random one when creating indicative
summaries of short fiction.
Table 7
Sentence co-selection between computer- and human-made summaries. Majority gold standard.
Data set Precision Recall F Groups (F)
HUMAN 64.95 84.18 72.41 A
Rules, fine-grained 39.20 53.35 44.47 B
Machine-learning, fine-grained 36.36 49.44 41.26 B
Rules, coarse-grained 35.42 44.39 38.94 B
Machine learning, coarse-grained 35.31 41.81 36.90 B
LEAD 25.50 31.18 27.57 C
CLASSY 7.14 9.08 7.70 D
GISTexter 9.67 6.30 6.91 D
RANDOM 4.10 5.40 4.57 D
Table 8
Sentence co-selection between computer- and human-made summaries. Union gold standard.
Data set Precision Recall F Groups (P)
HUMAN 1 55.61 71.05 A
Rules, fine-grained 56.10 32.11 40.56 BC
Rules, coarse-grained 53.98 29.77 38.04 BC
Machine-learning, fine-grained 52.39 30.66 38.35 C
Machine learning, coarse-grained 49.62 25.06 32.78 C
LEAD 36.76 18.53 24.50 DEF
GISTexter 22.70 6.18 9.28 EFG
CLASSY 19.74 10.23 13.05 FG
RANDOM 12.41 6.40 8.40 G
98
Kazantseva and Szpakowicz Summarizing Short Stories
Table 9
Sentence co-selection between computer- and human-made summaries. Intersection gold
standard.
Data set Precision Recall F Groups (R)
HUMAN 31.32 1.00 45.21 A
Rules, fine-grained 23.11 79.25 34.02 AB
Rules, coarse-grained 21.70 65.42 30.55 BC
Machine learning, coarse-grained 19.66 57.29 26.80 BC
Machine-learning, fine-grained 16.41 56.19 24.00 BC
LEAD 12.37 38.83 17.84 DE
CLASSY 3.58 8.04 4.89 F
GISTexter 3.20 6.06 3.83 F
RANDOM 0.79 1.67 1.06 F
The results also suggest that automatically produced summaries bear some resem-
blance to manual ones. There is no straightforward way to interpret these results as
good or bad in the context of other summarization systems. Firstly, the task is new and
no comparable results exist. Secondly, even though sentence co-selection metrics have
been widely used for evaluating summaries of other genres, different compression rates,
different gold standards, and availability of naturally occurring competitive baselines
(e.g., lead baseline in newswire summarization) make fair comparison difficult. For
example, Marcu (2000, page 214) reports achieving F-score of 76.04 when creating sum-
maries of newswire articles at 10% of their original length. The lead baseline achieves
F-score of 71.89. When summarizing dialogues, Zechner (2002, page 479) reports
weighted accuracy of 0.614 compared to the lead baseline?s performance of 0.438 (the
numbers are averages over five different summary sizes of 5%, 10%, 15%, 20%, and
25%). In this context we interpret the results in Tables 7?9 as suggesting that our genre-
specific system outperforms the naive baselines and two generic summarizers.
6.4 Evaluating Summaries using Lexical Overlap
ROUGE (Lin 2004) is a package for automatically evaluating summaries. Given one
or more gold-standard summaries (usually written by people), ROUGE offers several
metrics for evaluating the summary in question. The metrics reward lexical overlap
between the model summaries and the candidate one. Depending on the metric, the
lexical units taken into consideration are n-grams, word sequences, and word pairs.
Since 2004, ROUGE scores have been among the measures used for evaluating au-
tomatic summarizers at DUC. Following this tradition, we ran ROUGE to evaluate our
summaries and to compare them to the baselines (including CLASSY and GISTexter).
Conditions to be tested. The objective of the experiment was to establish how
much lexical overlap exists between the machine-made and the model summaries. We
achieved this by computing ROUGE-2 and ROUGE-SU4 scores.17
Baselines and material. We evaluated eight types of summaries: four types created
by our summarizer, the lead and the random baselines, and the summaries created by
17 ROUGE-2 and ROUGE-SU4 are two measures that were used at DUC 2007.
99
Computational Linguistics Volume 36, Number 1
GISTexter and CLASSY. In addition, we included a ceiling by computing ROUGE scores
for the model summaries.
Metrics. ROUGE-2 score measures the bigram recall between the reference summary
and the candidate one. It is computed according to the following formula:
ROUGE-2 =
?
s?S
?
b?s Countmatch(b)
?
s?S
?
b?s Count(b)
(13)
where S is the set of reference summaries, b is a bigram in the reference summary s,
Countmatch(b) is the number of bigrams that both summaries share, and Count(b) is the
total number of bigrams in the reference summary s.
ROUGE-S measures the similarity of a pair of summaries based on how many
skip-bigrams they have in common. A skip-bigram is any pair of words in a sentence,
allowing for arbitrary gaps.
ROUGE-S =
SKIP2(X,Y)
C(m, 2)
(14)
where X is the reference summary of length m, Y is the candidate summary, SKIP2(X,Y)
is the number of skip-bigram matches between X and Y, and C is the combination
function.
ROUGE-SU4 is an extension of ROUGE-S that also rewards matching unigrams.
The maximum gap allowed by skip-bigrams is 4 (hence SU4).
In order to compare the automatically produced summaries with those created by
humans we implemented the following leave-one-out procedure. At first, we computed
ROUGE scores by comparing all automatically produced summaries (i.e., those created
by our system and the baseline ones) and one of the model summaries against the
second available model summary. Next, the procedure was repeated but the model
summaries were switched. The significance of the differences was tested using ANOVA
and Tukey HSD for 95% confidence level. When calculating ANOVA and Tukey HSD,
we used the scores obtained from both runs.
Results. Tables 10 and 11 show ROUGE-2 and ROUGE-SU4 scores for all automati-
cally produced and model summaries. The results are inconclusive.
When using ROUGE-2 as a guide, the only summaries consistently different from
the rest with 95% confidence are the randomly generated ones. The scores of all other
summaries are too close to reject the hypothesis that the differences are due to chance.
This is the case even with the differences between the model and the automatically
produced summaries. A possible interpretation could be that all summaries are of
very high quality that is indistinguishable from that of the model summaries. This
hypothesis, however, can be easily dismissed: The results reported in Sections 6.2 and
6.3 clearly show that the quality of the summaries produced by our system is well below
the ceiling.
The situation is similar with ROUGE-SU4 scores, if not so dramatic. There are three
distinct groups of summaries. Group A includes the rule-based fine-grained summaries
and those produced by CLASSY. The second group includes the lead baseline, three
types of summaries created by our summarizer, the model summaries, and those cre-
ated by GISTexter. The last group contains the random and the lead baselines. Even
though ROUGE-SU4 measurement seems to have more discriminative power, it is at
least puzzling that it cannot distinguish between the model and the automatically
100
Kazantseva and Szpakowicz Summarizing Short Stories
Table 10
ROUGE-2 recall scores.
System ROUGE-2 Groups
HUMAN-1 0.0874 A
HUMAN-2 0.0807 A
Rules, fine-grained 0.0981 A
Machine learning, fine-grained 0.0905 A
GISTexter 0.0829 A
CLASSY 0.0826 A
Rules, coarse-grained 0.0816 A
Machine learning, coarse-grained 0.0808 A
LEAD 0.0572 AB
RANDOM 0.038 B
Table 11
ROUGE-SU4 recall scores.
System ROUGE-2 Groups
Rules, fine-grained 0.1684 A
CLASSY 0.1654 A
GISTexter 0.1607 AB
Rules, coarse-grained 0.1564 AB
HUMAN-1 0.1540 AB
Machine learning, coarse-grained 0.1468 AB
HUMAN-2 0.1426 AB
Machine learning, fine-grained 0.1584 AB
LEAD 0.127 BC
RANDOM 0.0956 C
produced summaries. In particular, placing the rule-based coarse-grained summaries
and the model ones in the same group directly contradicts the results reported in
Section 6.2?that people find the model summaries far superior to this particular type
of summary produced by our summarizer.
We interpret these results as suggesting that the ROUGE-2 and ROUGE-SU4 scores
are not well suited for evaluating indicative summaries of short stories. An explana-
tion could be that when people summarize fiction?rather than newswire or scientific
papers?they seem to use fewer sentences and clauses verbatim and, by and large, in-
troduce more generalization and abstraction. (We have made this informal observation
when processing the model summaries used in this experiment.) This results in little
lexical overlap with the source text and hence with extractive summaries of any flavor.
This hypothesis, however, is only preliminary and requires further investigation.18
18 It is possible that significantly increasing the number of model summaries would alleviate the problem.
Unfortunately, obtaining so many model summaries was prohibitively expensive in our case. To move in
this direction, we ran ROUGE without jackknifing to enable the use of two model summaries for
comparison. The results were similar to those reported in Tables 10 and 11: Only the random summaries
are consistently significantly worse than the rest.
101
Computational Linguistics Volume 36, Number 1
7. Conclusions
We presented an approach to summarizing literary short stories. The text sum-
marization community has not yet seriously explored this genre, except for early sem-
inal work on story understanding. In contrast with the story-understanding systems
proposed in the 1970s and 1980s, our system does not require labor-intensive semantic
resources?knowledge-bases and schemas?and it works on real-life stories, namely,
short fiction.
The summaries that the system produces, limited in scope, are intended to help
readers form adequate expectations about the original story. We have demonstrated
that such summaries can be produced without deep semantic resources, only relying
on syntax and the information about important entities in the story. According to the
judges who evaluated the summaries, our summaries are somewhat useful for their
original purpose, even if their quality falls far short of the quality of manual abstracts.
Our summaries appear better than the naive baselines and than two state-of-the-art
summarizers fine-tuned for working with newswire.
In the course of this work we have made a number of observations about automatic
summarization of short stories. First of all, we confirmed informally that characters
tend to be a central element of short fiction. Character mentions provide a wealth
of information that can be leveraged in automatic summarization. This finding was
also reflected in the approach proposed by Lehnert (1982). In addition, it appears that
position in text is important, as can be seen from the analysis of the usefulness of
features in Section 5.3. Besides, relatively high performance of the lead baselines also
suggests that position in text is a good indicator of salience, even though it plays a
lesser role than in more structured documents.
We view this work as a small step towards creating tools for searching, summa-
rizing, and otherwise processing fiction available electronically. The current system
accomplishes with some success a limited task of producing indicative summaries or
short stories, but much more work is needed to create high-quality flexible summaries of
literary works suitable for more than one purpose. Perhaps the most obvious extension
to the current system would be summarizing the plot of short stories. Although this is
not useful given our original criterion (forming adequate expectations about the story,
without ?spoilers?), the ability to handle plot would allow the creation of different
types of summaries. We also hope to explore the possibility of establishing structure
within stories: Knowing that certain portions of a story lay out the setting while
others describe events or the culmination would be a significant step towards better
summarization.
Evaluation of summaries of literary work is yet another dimension of the task that
needs to be considered. We have concentrated thus far on summary production rather
than on establishing the criteria that define the quality of the summary. Evaluation
of summaries remains an issue even where well-structured factual documents are
concerned. In fiction, it is far less clear what contributes towards the quality of the
summary: The facts, for instance, are likely to be less important than in scientific papers
or news items. Other candidate qualities may include closeness to the language or the
tone of the original story, the information about the author, the time period, or ideology
behind a certain work of fiction. This remains an open question, the answer to which
may well lie outside the field of computational linguistics.
102
Kazantseva and Szpakowicz Summarizing Short Stories
Appendix A: Features Used in the Coarse- and the Fine-Grained
Clause Representations
The appendix lists features computed to represent a clause in the fine-grained data set
(Table 12) and in the coarse-grained data set (Table 13). Prior to constructing feature
vectors, the stories are parsed with the Connexor Machinese Parser. All syntactic infor-
mation is computed on the basis of the parser output. The ?Category? column shows
whether a feature is character-related (C), location-related (L), aspect-related (A ), or
other (O). LCS refers to the database of Lexical Conceptual Structures (Dorr and Olsen
1997).
Features representing a clause in the fine-grained data set.
Table 12
Name Category Possible Description Default
values value
char if ind obj C yes, no yes if the clause contains a mention
of a character and its grammatical
function is indirect object
no
char if obj C yes, no yes if the clause contains a mention
of a character and its grammatical
function is direct object
no
char if subj C yes, no yes if the clause contains a mention
of a character and its grammatical
function is subject
no
char in sent C yes, no yes if the parent sentence contains a
mention of a character
no
char indef C def, indef def if the clause contains a mention
of a character and a) it is a proper
name or b) it is modified by a
definite determiner or a pronoun;
indef if the mention is modified by
an indefinite determiner
n/a
char is attr C yes, no yes if the mention of a character is in
the genitive case
n/a
char mention C yes, no yes if the clause contains a mention
of a character
no
char modified C yes, no yes if the mention of a character is
modified by a noun phrase
n/a
char pronoun C 1st, 3rd 1st if the clause contains a
pronominal mention of a character
and it is in 1st person (e.g., I); 3rd if
the pronominal mention is in 3rd
person (e.g., he)
n/a
nbr after
first mention
C continuous an integer that reflects the difference
between the index of the current
sentence and the sentence where the
character is first mentioned (it is
only defined for clauses containing
mentions of characters)
-1
loc in prep L yes, no yes if the clause contains a mention
of a location and is embedded in a
prepositional clause
no
103
Computational Linguistics Volume 36, Number 1
Table 12
(continued)
Name Category Possible Description Default
values value
loc present L yes, no yes if the clause contains a mention
of a location
no
durative A yes, no yes if the main verb of the clause is
durative; this information is
computed using LCS
no
dynamic A yes, no yes if the main verb of the clause is
dynamic; this information is
computed using LCS
no
modal A can, could,
shall, should,
would, must,
may, might,
dare, need, will,
ought, canst
a modal verb from the list, if it
appears in the clause
n/a
neg A yes, no yes if the main verb of the clause is
negated
no
obj def A yes, no no if the direct object of the main
verb is modified by an indefinite
determiner; yes in all other cases
where a direct object is present
n/a
obj plur A yes, no yes if the direct object of the verb is
in plural; no in all other cases where
a direct object is present
n/a
passive A yes, no yes if the clause is realized in passive
voice
no
perf A yes, no yes if the clause is realized in a
perfect tense
no
progr A yes, no yes if the clause is realized in a
progressive tense
no
telic A yes, no yes if the main verb of the clause is
telic; this information is computed
using LCS
no
tense A past, present,
future
the tense used in the clause n/a
tmp magn A min, hour, day,
week, month,
year, year plus
the magnitude of the core temporal
unit in the expression (defined for
clauses containing temporal
expressions and assigned using a set
of manually designed templates):
min if the core unit denotes a period
of no more than a minute (e.g., in a
few seconds, that moment); hour if it
denotes a period of no more than an
hour (e.g., during those hours, at
10 am); the values day through year
are assigned analogously, and
year plus denotes periods longer
than a year (e.g., for decades)
n/a
tmp plur A yes, no yes if the core temporal unit in the
expression is in plural (e.g., during
those years), no if it is singular
(e.g., that day); defined for clauses
containing temporal expressions
n/a
104
Kazantseva and Szpakowicz Summarizing Short Stories
Table 12
(continued)
Name Category Possible Description Default
values value
tmp type A location,
duration,
frequency,
enactment,
temporal
manner
the type of the expression (defined
for clauses containing temporal
expressions, and assigned using a
set of manually designed templates):
all values except temporal manner are
assigned according to the
classification of temporal
expressions available in the
linguistic literature (Harkness 1987),
for example today (location), during
those hours (duration), every day
(frequency), never (enactment);
temporal manner is a separate
pseudo-category defined to include
expressions such as immediately,
instantly etc.
n/a
clause type O assertive,
imperative,
infinitive,
subjunctive
the form of the main verb in the
clause as output by the parser:
imperative for clauses realized in the
imperative mood, subjunctive for
those realized in subjunctive,
infinitive for infinitival clauses (e.g.,
He decided to go), assertive otherwise
assertive
nbr of sent O continuous the index of the parent sentence in
text
-1
sent type O exclaim,
question,
assert
exclaim for clauses that are
exclamations, question for those
that are questions, and assert for all
others
assert
Features representing a clause in the coarse-grained data set.
Table 13
Name Category Possible Description Default
values value
char in clause C yes, no yes if the clause contains a mention
of a character
no
is subj obj C yes, no yes if the clause contains a mention
of a character and its grammatical
function is subject or direct object
no
modified by np C yes, no yes if the mention of a character is
present in the clause and it is
modified by a noun phrase
n/a
nbr after
first mention
C continuous an integer that reflects the difference
between the index of the current
sentence and the sentence where the
character is first mentioned (only
defined for clauses containing
mentions of characters)
-1
105
Computational Linguistics Volume 36, Number 1
Table 13
(continued)
Name Category Possible Description Default
values value
loc in prep L yes, no yes if the clause contains a mention
of a location embedded in a
prepositional clause
no
loc present L yes, no yes if the clause contains a mention
of a location
no
default aspect A state, activity,
accomp, achieve
default lexical aspect of the main
verb in the clause; computed
according to the privative model
defined in (Dorr and Olsen 1997)
n/a
has modal A yes, no yes if the clause contains a modal
verb
no
past perfect A yes, no yes if the clause is realized in past
perfect tense
no
politeness
with be
A yes, no yes if the clause contains one of the
following expressions: to be sorry, to
be delighted, to be glad, to be sad; the
feature is designed to help capture
politeness expressions (e.g., I am glad
to see you)
no
simple past
present
A yes, no yes if the clause is realized in simple
present or past tense
no
tmp exp
long duration
A no, long, short long if the clause contains a temporal
expression denoting a long period of
time, short if it contains an
expression denoting a short period
of time and no otherwise
no
is assertive
clause
O yes, no no if the clause is not an assertion yes
is assertive
sent
O yes, no no if the parent sentence is not
an assertion
yes
nbr of sent O continuous the index of the parent sentence
in text
-1
Acknowledgments
We are grateful to Connexor Oy and
especially to Atro Voutilainen for permission
to use the Connexor Machinese Syntax
parser free of charge for research purposes.
We thank John Conroy and Judith
Schlesinger for running CLASSY on our test
set, and Andrew Hickl for doing it with
GISTexter. Ana Arregui helped us recruit
students for the evaluation. Many thanks to
the annotators, summary writers, and raters,
who helped evaluate our summarizer. A
special thank-you goes to the anonymous
reviewers for Computational Linguistics for all
their incisive, insightful, and immensely
helpful comments. Support for this work
comes from the Natural Sciences and
Engineering Research Council of Canada.
References
Artstein, Ron and Massimo Poesio. 2008.
Inter-coder agreement for computational
linguistics (survey article). Computational
Linguistics, 34(4):555?596.
Bartlett, Frederic C. 1932. Remembering: A
Study in Experimental and Social Psychology.
Cambridge University Press, London.
Barzilay, Regina and Kathleen R. McKeown.
2005. Sentence fusion for multidocument
news summarization. Computational
Linguistics, 31(3):297?239.
Branavan, S. R. K., Pawan Deshpande, and
Regina Barzilay. 2007. Generating a
table-of-contents. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, pages 544?551,
Prague.
106
Kazantseva and Szpakowicz Summarizing Short Stories
By, Thomas. 2002. Tears in the Rain. Ph.D.
thesis, University of Sheffield.
Carenini, Giuseppe, Raymond Ng, and
Adam Pauls. 2006. Multi-document
summarization of evaluative text. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 305?313,
Trento.
Charniak, Eugene and Robert Goldman.
1988. A logic for semantic interpretation.
In Proceedings of the 26th Annual Meeting of
the Association for Computational Linguistics,
pages 87?94, State University of New York
at Buffalo, Buffalo, NY.
Chawlar, Nitesh V., Kevin W. Bowyer,
Lawrence O. Hall, and W. Philip
Kegelmeyer. 2002. SMOTE: synthetic
minority over-sampling techniques.
Journal of Artificial Intelligence Research,
16:321?357.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20:37?46.
Comrie, Bernard. 1976. Aspect. Cambridge
University Press, London.
Conroy, John M., Judith D. Schlesinger, and
Diane O. O?Leary. 2007. CLASSY 2007 at
DUC 2007. In Proceedings of the Document
Understanding Conference 2007, New York.
Available at http://duc.nist.gov/pubs/
2007papers/ida-umd.final.pdf.
Cullingford, R. E. 1978. Script Application:
Computer Understanding of Newspaper
Stories. Ph.D. thesis, Department of
Computer Science, Yale University.
Cunningham, Hamish, Diana Maynard,
Kalina Bontcheva, and Valentin Tablan.
2002. GATE: an Architecture for
Development of Robust HLT
applications. In Proceedings of the 40th
Anniversary Meeting of the Association for
Computational Linguistics, pages 168?175,
Philadelphia, PA.
D??az, Alberto and Pablo Gerva?s. 2007.
User-model based personalized
summarization. Information Processing and
Management, 43(6):1715?1734.
Dorr, Bonnie J. and Mari Broman Olsen.
1997. Deriving verbal and compositonal
lexical aspect for NLP applications. In
Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics and
8th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 151?158, Madrid.
Dowty, David. 1979. Word Meaning and
Montague Grammar. D. Reidel Publishing
Company, Dordrecht.
Dyer, Michael G. 1983. In-Depth
Understanding: A Computer Model of
Integrated Processing for Narrative
Comprehension. MIT Press,
Cambridge, MA.
Elhadad, Noemie, Min-Yen Kan, Judith
Klavans, and Kathleen McKeown. 2005.
Customization in a unified framework for
summarizing medical literature. Journal of
Artificial Intelligence in Medicine,
33(2):179?198.
Fellbaum, Christiane. 1998. WordNet: An
Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fuentes, Maria, Edgar Gonza`lez, Horacio
Rodr??gue, Jordi Turmo, and Laura Alonso
i Alemany. 2005. Summarizing
spontaneous speech using general text
properties. In Proceedings of International
Workshop Crossing Barriers in Text
Summarization Research, at Recent Advances
in Natural Language Processing 2005,
pages 10?18, Borovetz.
Harabagiu, Sandra, Andrew Hickl, and
Finley Lacatusu. 2007. Satisfying
information needs with multi-document
summaries. Information Processing and
Management, 43(6):1619?1642.
Harkness, Janet. 1987. Time adverbials in
English and reference time. In Alfred
Schopf, editor, Essays on Tensing in English,
Vol. I: Reference Time, Tense and Adverbs.
Max Niemeyer, Tu?bingen, pages 71?110.
Huddleston, Rodney D. and Geoffrey K.
Pullum. 2002. The Cambridge Grammar of
the English Language. Cambridge
University Press, New York.
Kazantseva, Anna. 2006. Automatic
summarization of short stories. Master?s
thesis, University of Ottawa. Available at
www.site.uottawa.ca/?ankazant/pubs/
thesis.tar.gz.
Krahmer, Emiel, Erwin Marsi, and Paul van
Pelt. 2008. Query-based sentence fusion is
better defined and leads to more preferred
results than generic sentence fusion. In
Proceedings of ACL-08: HLT, Short Papers,
pages 193?196, Columbus, OH.
Krippendorff, Klaus. 2004. Content Analysis.
An Introduction to Its Methodology. Sage
Publications, Thousand Oaks, CA.
Landis, J. Richards and Garry G. Koch. 1977.
The measurement of observer agreement
for categorical data. Biometrics,
33(1):159?174.
Lappin, Shalom and Herbert Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics,
20(4):535?561.
107
Computational Linguistics Volume 36, Number 1
Leach, Chris. 1979. Introduction to Statistics.
A Nonparametric Approach for the Social
Sciences. John Wiley and Sons, New York.
Leake, David. 1989. Anomaly detection
strategies for schema-based story
understanding. In Proceedings of the
Eleventh Annual Conference of the
Cognitive Science Society, pages 490?497,
Ann Arbor, MI.
Lehnert, Wendy G. 1982. Plot units: A
narrative summarization strategy. In
Wendy G. Lehnert and Martin H. Ringle,
editors, Strategies for Natural Language
Processing. Erlbaum, Hillsdale, NJ,
pages 375?414.
Lin, Chin-Yew. 2004. ROUGE: A package
for automatic evaluation of summaries.
In Marie-Francine Moens and Stan
Szpakowicz, editors, Text Summarization
Branches Out: Proceedings of the ACL-04
Workshop, pages 74?81, Barcelona.
Lin, Chin-Yew and Eduard Hovy. 2000. The
automated acquisition of topic signatures
for text summarization. In Proceedings of
the 18th Conference on Computational
Linguistics, pages 495?501, Morristown, NJ.
Lin, Chin-Yew and Eduard Hovy. 2003.
Automatic evaluation of summaries using
n-gram co-occurrence statistics. In Marti
Hearst and Mari Ostendorf, editors,
HLT-NAACL 2003: Main Proceedings,
pages 150?157, Edmonton.
Mandler, George. 1987. Determinants of
recognition. In E. van Der Meer and
J. Hoffman, editors, Knowledge-Aided
Information Processing. North Holland,
Amsterdam.
Mani, Indejeet. 2001. Automatic
Summarization. John Benjamins B.V.,
Amsterdam.
Marcu, Daniel. 2000. The Theory and Practice
of Discourse Parsing and Summarization.
The MIT Press, Cambridge, MA.
McDonald, Ryan. 2006. Discriminative
sentence compression with soft syntactic
evidence. In Proceedings of the 11th
Conference of the European Chapter of the
Association for Computational Linguistics,
pages 297?305, Trento.
Mei, Qiaozhu and ChengXiang Zhai. 2008.
Generating impact-based summaries
for scientific literature. In Proceedings
of ACL-08: HLT, pages 816?824,
Columbus, OH.
Merlo, Paola, Suzanne Stevenson, Vivian
Tsang, and Gianluca Allaria. 2002. A
multilingual paradigm for automatic
verb classification. In Proceedings
of the 40th Annual Meeting of the
Association for Computational Linguistics,
pages 207?214, University of
Pennsylvania, Philadelphia, PA.
Mihalcea, Rada and Hakan Ceylan. 2007.
Explorations in automatic book
summarization. In Proceedings of the
2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP-CoNLL), pages 380?389,
Prague.
Moens, Marie-Francine. 2007. Summarizing
court decisions. Information Processing and
Management, 43(6):1748?1764.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The Pyramid Method.
In Proceedings of the Human Language
Technology Conference and North American
Chapter of the Association for Computational
Linguistics Annual Meeting, pages 145?152,
Boston, MA.
Nomoto, Tadashi. 2007. Discriminative
sentence compression with random
conditional fields. Information Processing
and Management, 43(6):1571?1587.
Norvig, Peter. 1989. Marker passing as a
weak method for text inferencing.
Cognitive Science, 13(4):569?620.
Propp, Vladimir. 1968. Morphology of the
Folk Tale. Indiana University Press,
Bloomington, IN, 2nd edition.
Quinlan, J. Ross. 1992. C4. 5: Programs for
Machine Learning. Morgan Kaufmann,
San Mateo, CA.
Radev, Dragomir and Daniel Tam. 2003.
Summarization evaluation using relative
utility. In Proceedings of the 12th
International Conference on Information and
Knowledge Management, pages 508?511,
New York, NY.
Radev, Dragomir R., Simone Teufel, Horacio
Saggion, Wai Lam, John Blitzer, Hong Qi,
Arda C?elebi, Danyu Liu, and Elliott
Drabek. 2003. Evaluation challenges in
large-scale document summarization. In
Erhard Hinrichs and Dan Roth, editors,
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 375?382, Sapporo.
Rath, G. J., A. Resnick, and T. R. Savage.
1961. The formation of abstracts by the
selection of sentences. American
Documentation, 2(12):139?143.
Reeve, Lawrence H., Hyoil Han, and Ari D.
Brooks. 2007. The use of domain-specific
concepts in biomedical text
summarization. Information Processing and
Management, 43(6):1765?1776.
108
Kazantseva and Szpakowicz Summarizing Short Stories
Rumelhart, David E. 1975. Notes on a schema
for stories. In Daniel G. Bobrow and Allan
Collins, editors, Representation and
Understanding. Studies in Cognitive Science,
pages 221?237. Academic Press, New York.
Salton, Gerard, Amit Singhal, Mandar Mitra,
and Chris Buckley. 1997. Automatic text
structuring and summarization.
Information Processing and Management,
33(2):193?207.
Schlesinger, Judith D., Dianne P. O?Leary,
and John M. Conroy. 2008. Arabic/English
multi-document summarization with
CLASSY - The past and the future. In
Computational Linguistics and Intelligent Text
Processing, 9th International Conference,
CICLing 2008, pages 568?581, Haifa.
Scott, William. 1955. Reliability of content
analysis: The case of nominal scale coding.
Public Opinion Quarterly, 19(3):321?325.
Siegel, Eric V. 1998a. Disambiguating verbs
with the WordNet category of the direct
object. In Usage of WordNet in Natural
Language Processing Systems Workshop,
pages 9?15, Universite? de Montre?al.
Siegel, Eric V. 1998b. Linguistic Indicators for
Language Understanding: Using Machine
Learning Methods to Combine Corpus-Based
Indicators for Aspectual Classification of
Clauses. Ph.D. thesis, Columbia University,
New York.
Siegel, Sidney and John. N. Castellan, Jr.
1988. Nonparametric Statistics for the
Behavioral Sciences. McGraw Hill,
Boston, MA.
Soricut, Radu and Daniel Marcu. 2007.
Abstractive headline generation using
wild-expressions. Information Processing
and Management, 43(6):1536?1548.
Tapanainen, Pasi and Timo Ja?rvinen. 1997.
A non-projective dependency parser.
Proceedings of the 5th Conference on Applied
Natural Language Processing, pages 64?71,
Washington, DC.
Teufel, Simone and Marc Moens. 2002.
Summarizing scientific articles:
Experiments with relevance and rhetorical
status. Computational Linguistics,
28(4):409?445.
Thorndyke, Perry W. 1975. Cognitive
Structures in Human Story Comprehension
and Memory. Ph.D. thesis, Stanford
University.
van Dijk, Teun A. 1980. Macrostructures. An
Interdisciplinary Study of Global Structures
in Discourse, Interaction, and Cognition.
Laurence Erlbaum Associates,
Hillsdale, NJ.
van Dijk, Teun A. and Walter Kintsch. 1978.
Cognitive psychology and discourse:
Recalling and summarizing stories. In
Wolfgang U. Dressler, editor, Current
Trends in Textlinguistics. Walter de Gruyter,
New York, pages 61?79.
van Halteren, Hans and Simone Teufel.
2003. Examining the consensus between
human summaries: initial experiments
with factoid analysis. In Dragomir
Radev and Simone Teufel, editors,
HLT?NAACL 2003 Workshop: Text
Summarization (DUC03), pages 57?64,
Edmonton.
Vendler, Zeno. 1967. Linguistics in Philosophy.
Cornell University Press, Ithaca, NY.
Vieira, Renata and Massimo Poesio. 2000. An
empirically based system for processing
definite descriptions. Computational
Linguistics, 26(4):539?593.
Zechner, Klaus. 2002. Automatic
summarization of open-domain
multiparty dialogues in diverse genres.
Computational Linguistics, 28(4):447?485.
109

2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 211?220,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Topical Segmentation: a Study of Human Performance
and a New Measure of Quality
Anna Kazantseva
School of Electrical Engineering
and Computer Science,
University of Ottawa
ankazant@eecs.uottawa.ca
Stan Szpakowicz
School of Electrical Engineering
and Computer Science,
University of Ottawa &
Institute of Computer Science,
Polish Academy of Sciences
szpak@eecs.uottawa.ca
Abstract
In a large-scale study of how people find top-
ical shifts in written text, 27 annotators were
asked to mark topically continuous segments
in 20 chapters of a novel. We analyze the re-
sulting corpus for inter-annotator agreement
and examine disagreement patterns. The re-
sults suggest that, while the overall agree-
ment is relatively low, the annotators show
high agreement on a subset of topical breaks
? places where most prominent topic shifts
occur. We recommend taking into account
the prominence of topical shifts when evalu-
ating topical segmentation, effectively penal-
izing more severely the errors on more impor-
tant breaks. We propose to account for this in a
simple modification of the windowDiff metric.
We discuss the experimental results of evaluat-
ing several topical segmenters with and with-
out considering the importance of the individ-
ual breaks, and emphasize the more insightful
nature of the latter analysis.
1 Introduction
Topical segmentation is a useful intermediate step
in many high-level NLP applications such as in-
formation retrieval, automatic summarization and
question answering. It is often necessary to split a
long document into topically continuous segments.
Segmentation may be particularly beneficial when
working with documents without overt structure:
speech transcripts (Malioutov and Barzilay, 2006),
newswire (Misra et al, 2011) or novels (Kazantseva
and Szpakowicz, 2011). The customary approach
is to cast text segmentation as a binary problem: is
there a shift of topic between any two adjacent tex-
tual units (e.g., sentences or paragraphs)? While
necessary, this simplification is quite crude. Topic in
discourse usually changes continually; some shifts
are subtle, others ? more prominent.
The evaluation of text segmentation remains an
open research problem. It is a tradition to compile a
gold-standard segmentation reference using one or
more annotations created by humans. If an auto-
matic segmenter agrees with the reference, it is re-
warded, otherwise it is penalized (see Section 4 for
details). The nature of the task, however, is such that
creating and applying a reference segmentation is far
from trivial. The identification of topical shifts re-
quires discretization of a continuous concept ? how
much the topic changes between two adjacent units.
That is why annotators often operate at different lev-
els of granularity. Some people mark only the most
prominent topic fluctuations, while others also in-
clude finer changes. The task is also necessarily
under-defined. In addition to topic changes per se,
annotators effectively must classify some rhetorical
and pragmatic phenomena ? exactly how much it is
depends on the document genre. For simplicity we
do not directly address the latter problem here; we
concentrate on the former.
To study how people identify topical shifts in
written text, we asked 27 annotators to segment into
episodes 20 chapters of the novel The Moonstone
by Wilkie Collins. Each chapter was annotated by
4-6 people. An episode roughly corresponds to a
topically continuous segment ? the term is defined
211
in Section 3. The analysis of the resulting corpus
reveals that while the overall inter-annotator agree-
ment is quite low and is not uniform throughout each
chapter. Some topical shifts are marked by most or
all annotators, others ? by one or by a minority. In
fact, only about 50% of all annotated topical shifts
are supported by at least 50% of annotators (includ-
ing near-hits), while the other half is only marked by
a minority. In this work we take the agreement about
a certain topical shift as a measure of its prominence,
and show how this measure can be simply utilized
for the purpose of evaluation.
The main claim of this paper is perhaps the fol-
lowing: when evaluating the performance of auto-
matic segmenters, it is important to consider not
only the overall similarity between human and ma-
chine segmentations, but also to examine the regions
of disagreement. When a program misses or mis-
places a prominent topic shift ? a segment bound-
ary marked by all annotators ? it should be penal-
ized more than if it was mistaken about a boundary
marked by one person. Similarly, a false positive
in the region where none of the annotators found a
change in topic is worse than a boundary inserted in
a place where at least one person perceived a topic
change. We suggest that it is important to use all
available reference segmentations instead of com-
piling them into a single gold standard. We show
how a small modification to the popular windowD-
iff (Pevzner and Hearst, 2002) metric can allow con-
sidering multiple annotations at once.
To demonstrate the increased interpretive power
of such evaluation we run and evaluate several state-
of-the art segmenters on the corpus described in this
work. We evaluate their performance first in a con-
ventional manner ? by combining all available ref-
erences into one ? and then by using the proposed
modification. Comparing the results suggests that
the information provided by this method differs from
what existing methods provide.
Section 2 gives a brief background on text seg-
mentation. Section 3 describes the corpus and how
it was collected. Section 4 contain quantitative and
qualitative analysis of the corpus and its interpreta-
tions. Section 5 proposes a modified version of win-
dowDiff and motivates it. Section 6 compares eval-
uation of three segmenters in several different ways.
Section 7 contains the conclusions and outlines di-
rections for future work.
2 Background and Related Work
The goal of topical text segmentation is to identify
segments within which the topic under discussion
remains relatively constant. A flip-side of this def-
inition is identifying topic shifts ? places where the
topic shifts significantly or abruptly. In the context
of this paper we allow ourselves to use these two def-
initions interchangeably, sometimes talking about
identifying topic shifts, at other times ? about identi-
fying topically continuous segments. While the the-
oretical correctness of such usage remains question-
able, it is sufficient for the purpose of our discussion,
and it is in line with the literature on the topic.
There is a number of corpora annotated for the
presence of topical shifts by one or more annotators.
Passonneau and Litman (1997) describe an experi-
ment where seven untrained annotators were asked
to find discourse segments in a corpus of transcribed
narratives about a movie. While the authors show
that the agreement is significant, they also note that
people include segment boundaries at different rates.
Gruenstein, Niekrasz, and Purver (2005) describe
the process of annotating parts of two corpora of
meeting transcripts: ICSI (Janin et al, 2003) and
ISL (Burger, MacLaren, and Yu, 2002). Two peo-
ple annotated the texts at two levels: major and mi-
nor, corresponding to the more and less important
topic shifts. Topical shifts were to be annotated so
as to allow an outsider to glance at the transcript
and get the gist of what she missed. Not unlike
our work, the authors report rather low overall inter-
annotator agreement. Galley et al (2003) also com-
piled a layer of annotation for topical shifts for part
of the ICSI corpus, using a somewhat different pro-
cedure with three annotators. Malioutov and Barzi-
lay (2006) created a corpus of course lectures seg-
mented by four annotators, noting that the annota-
tors operated at different levels of granularity. In
these three projects, manual annotations were com-
piled into a single gold standard reference for use in
evaluating and fine-tuning automatic segmenters.
The work described in this paper is different in
several ways. To the best of our knowledge, this is
212
the first attempt to annotate literary texts for topical
shifts. Because we collected relatively many anno-
tations for each chapter (four to six), we can make
some generalizations as to the nature of the process.
In addition to compiling and describing the corpus,
we analyze disagreement patterns between annota-
tors. We claim that even though the annotators may
not agree on granularity, they do agree at some level,
at least with respect to most prominent breaks. We
propose that instead of compiling a single reference
from multiple annotations it may be more useful to
evaluate automatic segmenters against several anno-
tations at once. We will show how to do that.
3 The Overview of the Corpus
Our current work on text segmentation is part of a
larger project on automatic summarization of fic-
tion, which is why we chose a XIX century novel,
The Moonstone by Wilkie Collins, as the text to
be annotated. We used two chapters for a pilot
study and then another 20 for the large-scale experi-
ment. The annotators worked with individual chap-
ters and were required to align segment boundaries
with paragraph breaks.
Objectives. The main question behind this study
was this: ?How do people identify topical shifts in
literature?? This vague question can be mapped to
several more specific objectives. First, we sought
to verify that topical segmentation of literature was
a sensible task from the viewpoint of an untrained
annotator. Next, it was important to examine inter-
annotator agreement to make sure that the annota-
tors in fact worked on the same phenomena and that
the resulting corpus is a reasonable approximation of
how people segment literature in general. Third, in
addition to analyzing the overall agreement we also
took a close look at the type of common disagree-
ments, in search of patterns and insights to evaluate
automatic segmenters.
Subjects. The participants were undergraduate
students of an English department at the University
of Ottawa, recruited by email. They received $50
each for their participation. Everyone had to anno-
tate four chapters from The Moonstone, not neces-
sarily consecutive ones. The chapters were divided
so as to ensure an approximately equal workload.
We had planned six independent annotations for
each chapter of the novel.1 The annotators were di-
vided into five groups, each group asked to read and
annotate four distinct chapters. In the end we had
three groups with six people, one group with five
and one group with four.
Procedure. The experiment was conducted re-
motely. The students received email packages with
detailed instructions and an example of a segmented
chapter from a different novel. They had two weeks
to annotate the first two chapters and then two more
weeks to annotate another two chapters.
The annotators were instructed to read each chap-
ter and split it into episodes ? topically continuous
spans of text demarcated by the most perceptible
shifts of topic in the chapter. We asked the anno-
tators to provide a brief one-sentence description of
each episode, effectively creating a chapter outline.
The students were also asked to record places they
found challenging and to note the time it takes to
complete the task.
Because even short chapters of most traditional
novels are rather lengthy, we chose to use paragraphs
as the basic unit of annotation (sentences are more
common in text segmentation literature).
4 Corpus Analysis
Time. On average, an annotator required 137.9 min-
utes to complete both tasks. The standard devia-
tion was ? = 98.32 minutes appropriately reflecting
the fact that some students are very fast readers and
besides have already read the novel in one of their
classes, while others are quite slow.
The average chapter has 53.85 paragraphs (? =
29.31), the average segment length across all anno-
tators is 9.25 paragraphs (? = 9.77). On average the
annotators identified 5.80 episodes (? = 2.45) per
chapter. Figure 1 shows the distribution of the num-
ber of segments identified in each chapter. An indi-
vidual box plot is compiled using all available anno-
tations for that chapter ? six for most, four or five
for several. The data are plotted for individual chap-
ters, so the only source of variance is the disagree-
ment between annotators as to what is the appropri-
ate level of detail for the task. Figure 1 confirms
1We hired 30 students. Three did not complete the task.
213
Figure 1: Distribution of segment counts across chapters.
other researchers? findings: people find topical shifts
at different levels of granularity (Malioutov and
Barzilay, 2006; Gruenstein, Niekrasz, and Purver,
2005). We take this investigation further and explore
whether there are patterns to this disagreement and
how they can be interpreted and leveraged.
4.1 Inter-annotator Agreement
In order to make sure that our guidelines are suffi-
ciently clear and the annotators in fact annotate the
same phenomenon, it is important to measure inter-
annotator agreement (Artstein and Poesio, 2008).
This is particularly important given the fact that the
resulting corpus is intended as a benchmark dataset
for evaluation of automatic segmenters.
When looking at inter-annotator agreement inde-
pendently of the domain, the most commonly used
metrics are coefficients of agreement ? ? (Krippen-
dorff, 2004), ? (Cohen, 1960; Shrout and Fleiss,
1979), pi (Scott, 1955) and several others. In this
work we use a multi-annotator version of pi, also
known in the CL community as Fleiss?s ? (Shrout
and Fleiss, 1979; Siegel and Castellan, 1988) .
Fleiss?s ? is computed as follows:
? =
Agreementobserved ? Agreementexpected
1? Agreementexpected
(1)
Agreementobserved =
1
ic(c? 1)
X
i?I
X
k?K
nik(nik ? 1) (2)
Agreementexpected =
1
(ic)2
X
k?K
n2k (3)
where i is the number of items to be classified in set
I, k is the number of available categories in set K, c is
the number of annotators, nik is the number of anno-
tators who assign item i to category k, nk is the total
number of items assigned to category k by all anno-
tators (Artstein and Poesio, 2008, pp. 562-563). Ef-
fectively ?measures how much the annotators agree
above what can be expected by chance. The value
of ? is 0 where there is no agreement above chance
and 1 where the annotators agree completely.
While we report ? values for our dataset, it is
important to note that ? is ill-suited to measuring
agreement in segmentation. The main problem is its
insensitivity to near-hits. When asked to segment
a document, the annotators often disagree about the
exact placement of the boundary but agree that there
is a boundary somewhere in the region (e.g., con-
sider paragraphs 9-11 in segmentations in Figure 2).
It is desirable to give partial credit to such near-hits
instead of dismissing them as utter disagreement.
This cannot be achieved with ?. The second prob-
lem is the independence assumption: the label for
each item must be independent from the labels of all
other items. In our case, this would amount to claim-
ing, highly unrealistically, that the probability of a
topical shift between two sentences is independent
of the topical landscape of the rest of the document.
Two other commonly used agreement metrics are
Pk (Beeferman, Berger, and Lafferty, 1999) and win-
dowDiff (Pevzner and Hearst, 2002), both designed
to compare a hypothetical segmentation to a refer-
ence, not to measure agreement per se. A com-
mon feature of both metrics is that they award partial
credit to near-hits by sliding a fixed-length window
through the sequence and comparing the reference
segmentation and hypothetical segmentation at each
window position. The window size is generally set
at half the average segment length.
Pk (Equation 4) measures the probability that two
units randomly drawn from a document are correctly
classified as belonging to the same topical segment.
Pk has been criticized for penalizing false negatives
less than false positives and for being altogether in-
sensitive to certain types of error; see (Pevzner and
214
Hearst, 2002, pp. 22-26) for details. Despite its
shortcomings, Pk is widely used. We report it for
comparison with other corpora.
Pk(ref, hyp) =
X
1?i?j?n
D(i, j)(?ref (i, j) XNOR ?hyp(i, j))
(4)
Functions ?hyp and ?ref indicate whether the two
segment endpoints i and j belong to the same seg-
ment in the hypothetical segmentation and reference
segmentation respectively.
windowDiff was designed to remedy some of Pk?s
shortcomings. It counts erroneous windows in the
hypothetical sequence normalized by the total num-
ber of windows. A window is judged erroneous if
the boundary counts in the reference segmentation
and hypothetical segmentation differ; that is (|ref -
hyp| 6= 0) in Equation 5).
winDiff =
1
N ? k
N?kX
i=1
(|ref ? hyp| 6= 0) (5)
Both Pk and windowDiff produce penalty scores be-
tween 0 and 1, with 1 corresponding to all windows
being in error, and 0 ? to a perfect segmentation.
Table 1 reports Pk, windowDiff and ? values for
our corpus. Pk and windowDiff are computed pair-
wise for all annotators within one group and then
averaged. We set the window size to half the aver-
age segment length as measured across all annota-
tors who worked on a given chapter. The values are
computed for each group separately; Table 1 shows
the averages across five groups.
Even by most relaxed standards, e.g., (Landis and
Koch, 1977), the ? value of 0.38 corresponds to low
agreement. This is not surprising, since it only in-
cludes the cases when the annotators agree exactly
where the boundary should be. For the purpose of
our task, such a definition is too strict.
The values of windowDiff and Pk are more rea-
sonable; windowDiff = 0.34 means that on aver-
age a pair of annotators disagrees on 34% of win-
dows. windowDiff was originally designed to com-
pare only two segmentations. Our strategy of com-
puting its values pairwise is perhaps not optimal but
in the absence of another metric allowing to account
for near-hits we are practically forced to use it as a
primary means of inter-annotator agreement.
Table 1: Overview of inter-annotator agreement.
Mean Std. dev.
? 0.29 0.15
Pk 0.33 0.17
windowDiff 0.38 0.09
Figure 2: Example segmentation for Chapter 1.
4.2 Patterns of Disagreement
Figure 2 shows the segmentation of the shortest
chapter in the dataset. The overall agreement is
quite low (windowDiff=0.38, ? = 0.28). This is not
surprising, since annotators 1 and 3 found two seg-
ments, annotator 3 ? five segments, and annotator 4
? four. Yet al annotators agree on certain things: ev-
eryone found that there was a significant change of
topic between paragraphs 9 and 11 (though they dis-
agree on its exact placement). It is therefore likely
that the topical shift between paragraphs 9 and 11 is
quite prominent. Annotators 2 and 4 chose to place
a segment boundary after paragraph 2, while anno-
tators 1 and 3 did not place one there. It is likely that
the topical shift occurring there is less prominent, al-
though perceptible. According to these annotations,
the least perceptible topic shifts in the chapter oc-
cur after paragraph 4 (marked only by annotator 2)
and possibly after paragraph 11 (marked only by an-
notator 1). Overall, glancing at these segmentations
suggests that there is a prominent topical shift be-
tween paragraphs 9-11, three significant ones (after
2, 10 and 12) and several minor fluctuations (after 3
and possibly after 10 and 11).
Looking at the segmentations in Figure 2 it seems
likely that the disagreements between annotators 2
and 4 are due to granularity, while the annotators
1 and three disagree more fundamentally on where
the topic changes. When measuring agreement, we
would like to be able to distinguish between dis-
215
Figure 3: Quality of segment boundaries.
agreements due to granularity and disagreements
due to true lack of agreement (annotator 1 and 3).
We would also like to leverage this information for
the evaluation of automatic segmenters.
Distinguishing between true disagreement and
different granularity while taking into account near-
hits is not trivial, especially since we are working
with multiple annotations simultaneously and there
is no one correct segmentation.
In order to estimate the quality of individual
boundaries and look inside the segmented sequence,
we approximate the quality of each suggested seg-
ment boundary by the percentage of annotators who
marked it. Since the annotators may disagree on the
exact placement of the boundaries, our measurement
must be relaxed to allow for near-hits.
Figure 3 shows the distribution of segment bound-
aries using three different standards of quality. We
consider all segment boundaries introduced by at
least one annotator. Then, for each suggested bound-
ary we compute how much support there is from
peer annotators: what percentage of annotators in-
cluded this boundary in their segmentation. The left-
most box plot in Figure 3 corresponds to the most
strict standard. When computing support we only
consider perfect matches: segment boundaries spec-
ified in exactly the same location (window size =
0). The middle box plot is more relaxed: we con-
sider boundaries found within half of a windowD-
iff window size of the boundary under inspection.
The rightmost box plot corresponds to the inclusion
of boundaries found within a full windowDiff win-
dow size of the boundary under inspection.
Looking at exact matches (the leftmost box plot),
we observe that at least a half of segment bound-
aries were specified by less than 25% of annotators
(which corresponds to one person). It explains why
? values in Table 1 are so low: this is the only sort
of agreement ? captures. Also one can notice that
at most 25% of the boundaries have the support of
more than 50% of the annotators.
The picture changes if we consider all boundaries
within a tight window around the candidate bound-
ary (the middle box plot). This standard is twice
as strict as the regular windowDiff evaluation. Here
50% of all boundaries are marked by at least 35% at
and most 80% of annotators. Only 25% of bound-
aries are marked by less than 30% of the annotators.
The rightmost plot looks even better. If we con-
sider the support found within a window size of any
candidate boundary, then 50% of all boundaries are
supported by over 70% of annotators. However, we
find this way of measuring support too optimistic.
The reason is, again, the difference in the granu-
larity of segmentations. The window size used for
these measurements is based on the average segment
length across all annotations. For example, the aver-
age segment length for segmentation shown in Fig-
ure 2 is 4, making the window size 2. This size is
too relaxed for annotators 2 and 3, who were very
detailed. Due to the excessively large window there
will almost always be a boundary where fine-grained
annotations are concerned, but those boundaries will
not correspond to the same phenomena. That is why
we think that a stricter standard is generally more
appropriate. This is especially the case since we
work with paragraphs, not sentences. A distance of
2-3 sentences is quite tolerable, but a distance of 2-3
paragraphs is considerable, and it is far more likely
that a stricter notion of near-hits must be considered.
5 Proposed Modification to windowDiff
WindowDiff compares two segmentations by taking
into account near-hits ? penalizing them proportion-
ally to how far a hypothetical segment boundary is
216
from a reference boundary. Section 4.2 argued that
some boundaries are more prominent. We aim to
modify windowDiff so the prominence of the bound-
aries matters in evaluating automatic segmenters.
Recall that to compute windowDiff we slide a
window through the reference and the hypotheti-
cal segmentation and check whether the number of
boundaries is equal at each window position. The
number of erroneous windows is then normalized:
winDiff =
1
N ? k
N?kX
i=1
(|refi ? hypi| 6= 0) (6)
refi and hypoi are the counts of boundaries in a
given window in the reference and the hypothetical
sequence, N is the length of the complete sequence,
k is the window size (so there are N - k windows).
The prominence of a boundary can be approxi-
mated by how many annotators specified it in their
segmentations. One simple way to take prominence
into account is to slide a window through all avail-
able segmentations, not just one. A straighforward
modification to equation (6) achieves that:
winDiff ? =
1
h(N ?m)
hX
a=1
N?mX
i=1
(|refai ? hypi| 6= 0) (7)
A is the set of all available annotations and h is
their total number. Effectively, for each position of
the window the hypothetical output is penalized as
many times as there are reference annotations with
which it disagrees. Note that the window size m is
different from that used for pair-wise comparisons.
Following the convention, we recommend setting it
to half of the size of an average segment length (av-
eraged over all available references). The size of
the window effectively specifies a tolerance thresh-
old for what is an acceptable near-hit (as opposed to
a plain miss), and can be modified accordingly.
windowDiff and Pk range from 0 to 1, with 0
corresponding to an ideal segmentation. The upper
and lower bounds for Equation 7 are different and
depend on how much the reference segmentations
agree between themselves.2
2We find that the upper bound corresponds to the worst-case,
and the lower bound to the best-case scenario. To avoid confu-
sion, we talk of the best-case bound and the worst-case bound.
Let us refer to the most popular opinion for a
given position of the window as the majority opin-
ion. Then, for each window, the smallest possible
penalty is assigned if the hypothetical segmentation
correctly ?guesses? the majority opinion (the win-
dow then receives a penalty equal to the number of
annotators disagreeing with the majority opinion):
best case =
1
N ?m
N?mX
i=1
(h?majority support) (8)
Here majority support is the number of annota-
tors who support the most frequent opinion.
Conversely, to merit the highest penalty, a hypo-
thetical segmentation must ?guess? the least popu-
lar opinion (possibly an opinion not supported by
any annotators) at each window position. In Equa-
tion 9, unpopular support is the number of anno-
tators who agree with the least popular opinion.
worst case =
1
N ?m
N?mX
i=1
(h? unpopular support) (9)
In order to have a multi-annotator version of win-
dowDiff interpretable within the familiar [0, 1] in-
terval, we normalize Equation 7:
multWinDiff =
(
Ph
a=1
PN?m
i=1 (|refa ? hyp| 6= 0))? best case
h(N ?m)(worst case? best case)
(10)
The best and the worst-case bounds serve as indi-
cators of how much agreement there can be between
reference segmentations and so as indicators of how
difficult to segment a given document is.
The multWinDiff metric in Equation 10 has the
same desirable properties as the original metric,
namely it takes into account near hits and penal-
izes according to how far the reference and hypo-
thetical boundaries are. Additionally, for each win-
dow position it takes into account how much a hy-
pothetical segmentation is similar to all available an-
notations, thus penalizing mistakes according to the
prominence of boundaries (or to the certainty that
there are no boundaries).3
3Java code to compute multWinDiff is available as a part of
the APS segmenter. The corpus and the software can be down-
loaded at ?www.eecs.uottawa.ca/?ankazant?.
217
6 Experiments
In order to illustrate why using a single gold-
standard reference segmentation can be problem-
atic, we evaluate three publicly available seg-
menters, MinCutSeg (Malioutov and Barzilay,
2006), BayesSeg (Eisenstein and Barzilay, 2008)
and APS (Kazantseva and Szpakowicz, 2011), us-
ing several different gold standards and then using
all available annotations. The corpus used for eval-
uation is The Moonstone corpus described in Sec-
tions 3-4. We withheld the first four chapters for de-
velopment and used the remaining 16 for testing. We
also compared the segmenters to a random baseline
which consisted of randomly selecting a number of
boundaries equal to the average number of segments
across all available annotations.
None of the segmenters requires training in the
conventional sense, but APS and MinCutSeg seg-
meters come with scripts allowing to fine-tune sev-
eral parameters. We selected the best parameters for
these two segmenters using the first four chapters of
the corpus. BayesSeg segmeter, a probabilistic seg-
menter, does not require setting any parameters.
Table 2 sums up the results. Each row corre-
sponds to one reference segmentation and metric ?
regular windowDiff in the first six rows. We com-
piled several flavours of consensus reference seg-
mentations: 1) all boundaries marked by ? 50% of
the annotators (windowDiff ? 50%), 2) all boundaries
marked by ? 30% of the annotators (windowDiff ?
30%), 3) all boundaries marked by at least one an-
notator (windowDiff union). To illustrate why com-
paring against a single annotation is unreliable, we
report comparisons against three single-person an-
notations (windowDiff annotator 1, 4, 2). multWinDiff
is the proposed multi-annotator version from Equa-
tion 10. The best-case bound for multWinDiff is 0.21
and the worst-case bound is 1.0.
Each segmenter produced just one segmentation,
so the numbers in the Table 2 differ only depending
on the mode of evaluation. The cells are coloured.
The lightest shade correspond to the best perfor-
mance, darker shades ? to poorer performance. The
actual values for the first six rows are rather low, but
what is more bothersome is the lack of consistency
in the ranking of segmenters. Only the random base-
APS Bayes MinCut Rand.
windowDiff
?50%
0.60. 0.66 0.73 0.73
windowDiff
?30%
0.61 0.52 0.69 0.61
windowDiff
union
0.6 0.53 0.63 0.65
windowDiff
annotator 1
0.66 0.57 0.74 0.76
windowDiff
annotator 4
0.62 0.7 0.69 0.74
windowDiff
annotator 2
0.61 0.6 0.66 0.69
multWinDiff 0.23 0.28 0.31 0.41
Table 2: The three segmenters and a random baseline
compared using different references for computing win-
dowDiff. windowDiff ?50%: the gold standard consists
of all boundaries specified by at least 50% of the anno-
tators; windowDiff ?30%: all boundaries specified by
at least 30% of the annotators; windowDiff union: all
boundaries specified by at least one person; windowD-
iff annotator a: comparisons against individual annota-
tors. multWinDiff is multi-annotator windowDiff from
equation (10).
line remains the worst in most cases. The APS and
BayesSeg segmenters tend to appear better than the
MinCutSeg but it is not always the case and the rank-
ings among the three are not consistent.
The last row reports multi-annotator windowD-
iff which takes into account all available references
and also the best-case and the worst-case bounds. In
principle, there is no way to prove that the metric is
better than using windowDiff and a single reference
annotation. It does, however, take into account all
available information and provides a different, if not
unambiguously more true, picture of the compara-
tive performance of automatic segmenters.
7 Conclusions and Future Work
We have described a new corpus which can be used
in research on topical segmentation. The corpus is
compiled for fiction, a genre for which no such cor-
pus exists. It contains a reasonable number of anno-
tations per chapter to allow an in-depth analysis of
topical segmentation as performed by humans.
218
Our analysis of the corpus confirms the hypothe-
sis that when asked to find topical segments, people
operate at different levels of granularity. We show
that only a small percentage of segment boundaries
is agreed upon by all or almost all annotators. If,
however, near-hits are considered, suggested seg-
ment boundaries can be ranked by their prominence
using the information about how many people in-
clude each boundary in their annotation.
We propose a simple modification to windowD-
iff which allows for taking into account more than
one reference segmentation, and thus rewards or pe-
nalizes the output of automatic segmenters by con-
sidering the severity of their mistakes. The proposed
metric is not trouble-free. It is a window-based met-
ric so its value depends on the choice of the window
size. While it has become a convention to set the
window size to half of the average segment length in
the reference segmentation, it is not obvious that the
same logic applies in case of multi-annotator win-
dowDiff. The metric also hides whether false posi-
tives or false negatives are the main source of error.
All these shortcomings notwithstanding, the met-
ric offers an advantage of allowing the evaluation of
hypothetical segmentations with more subtlety than
those using a single gold standard reference. When
using regular windowDiff and a single reference seg-
mentation, one is restricted to an evaluation based
on binary comparisons: whether a given hypothet-
ical boundary is similar to the gold standard seg-
mentation (e.g., the majority opinion). Divergent
segmentations are penalized even if they are simi-
lar to minority opinions (and thus feasible, though
maybe less likely) or if they are completely different
from anything created by humans (and thus proba-
bly genuinely erroneous). Our version of windowD-
iff, however, takes into account multiple annotations
and gives partial reward to segmentations based on
how similar there are to any human segmentation,
not just the majority opinion (while giving prefer-
ence to high agreement with the majority opinion).
To evaluate the output of topical segmenters is
hard. There is disagreement between the annota-
tors about the appropriate level of granularity and
about the exact placement of segment boundaries.
The task itself is also a little vague. Just as it is
the case in automatic text summarization, generation
and other advanced NLP tasks, there is no single cor-
rect answer and the goal of a good evaluation met-
ric is to reward plausible hypotheses and to penalize
improbable ones. It is quite possible that a better
metric than the one proposed here can be devised;
see, for example, (Fournier and Inkpen, 2012)(Sca-
iano and Inkpen, 2012). We feel, however, that any
reliable metric for evaluating segmentations must ?
in one manner or another ? take into account more
than one annotation and the prominence of segment
breaks.
Acknowledgments
We thank William Klement and Chris Fournier for
commenting on an early draft of this paper. The first
author also thanks Chris Fournier and Martin Sca-
iano for insightful discussions about the evaluation
of topical segmenters. This work is partially funded
by the National Sciences and Engineering Research
Council of Canada and by the Ontario Graduate
Scholarship program.
References
Artstein, Ron and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4):555?596.
Beeferman, Doug, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34:177?210, February.
Burger, Susanne, Victoria MacLaren, and Hua Yu. 2002.
The ISL meeting corpus: the impact of meeting type
on speech style. In INTERSPEECH?02.
Cohen, Jacob. 1960. A coefficient of agreement for
nominal scales . Educational and Psychological Mea-
surement, 20:37?46.
Eisenstein, Jacob and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 334?343, Honolulu,
Hawaii, October.
Fournier, Chris and Diana Inkpen. 2012. Segmentation
similarity and agreement. In Proceedings of NAACL-
HLT 2012 (this volume), Montre?al, Canada, June.
Galley, Michel, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation
of multi-party conversation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 562?
219
569, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Gruenstein, Alexander, John Niekrasz, and Matthew
Purver. 2005. Meeting Structure Annotation: Data
and Tools. In In Proceedings of the SIGdial Workshop
on Discourse and Dialogue, pages 117?127.
Janin, Adam, Don Baron, Jane Edwards, D. Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin, Thilo
Pfau, Elizabeth Shriberg, A.ndreas Stolcke, and Chuck
Wooters. 2003. The ICSI Meeting Corpus. In
Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP-
03), volume 1, pages 364?367, April.
Kazantseva, Anna and Stan Szpakowicz. 2011. Lin-
ear Text Segmentation Using Affinity Propagation.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 284?
293, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Krippendorff, Klaus. 2004. Content Analysis. An Intro-
duction to Its Methodology. Sage Publications.
Landis, J. Richards and Garry G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159?174.
Malioutov, Igor and Regina Barzilay. 2006. Minimum
Cut Model for Spoken Lecture Segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 25?
32, Sydney, Australia, July.
Misra, Hemant, Franc?ois Yvon, Olivier Cappe?, and Joe-
mon M. Jose. 2011. Text segmentation: A topic mod-
eling perspective. Information Processing and Man-
agement, 47(4):528?544.
Passonneau, Rebecca J. and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Computational Linguistics, 23(1):103?139, March.
Pevzner, Lev and Marti A. Hearst. 2002. A Critique and
Improvement of an Evaluation Metric for Text Seg-
mentation. Computational Linguistics, 28(1):19?36.
Scaiano, Martin and Diana Inkpen. 2012. Getting
more from segmentation evaluation. In Proceedings
of NAACL-HLT 2012 (this volume), Montre?al, Canada,
June.
Scott, William. 1955. Reliability of content analysis:
The case of nominal scale coding. Public Opinion
Quaterly, 19(3):321?325.
Shrout, Patrick E. and Joseph L. Fleiss. 1979. Intraclass
correlations: uses in assessing rater reliability. Psy-
chological Bulletin, 86(2):420?428.
Siegel, Sidney and John. N. Jr. Castellan. 1988. Non-
parametric statistics for the behavioral sciences. Mc-
Graw Hill, Boston, MA.
220
