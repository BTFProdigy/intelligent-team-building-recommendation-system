Tagging of very large corpora: Tol)ic-Focus Articulation 
Eva  Burf i f iovf i  and Eva  Ha j i~ovf i  and Pet r  Sga l l  
\]nsl;il;ul; of Formal  and App l ied  Linguist ics,  
Facu l ty  (If' Math( ' .matics an(t Phys ics  
Char les  University,  P rague,  Cze(;h \]2,etml)li(: 
Abst rac t  
A\['ter a. bri(;f chara(:teriz:~tion f the th(;ory of the 
tot)i('-fo('us a rticulatioi~ (if the s('.nt('al(:(,. ('I'FA), 
rules 3A'c formulated that (letermin(; I:he a,~sign- 
menI; of al)t)rol)riate values (If |;hi'. "J'\]'\~\ attril)u?;(~ 
ill I;he l)ro(:(',qs of synl;a(:i;i(:o-s('manti(: tagging of 
i~ very large ('orlms lit' Cz(;(:h. 
1 I n t roduct ion :  The  Prague 
Dependency  Treebank  (PDT)  
PDT is a corpus (a part fl'om the Czech Na.tional 
Cortms), tagged on th(; following h'x('.l>: 
1. mort)hemi(: (POS and a illlOl;al;ions using a, 
v(n'y large nmnl)('.r of i;ag G :is r/'quired 1)y 
the language, with rich intl(~(:ti(/n; (:1". (\]lajiC: 
and llladk(~, \] 997)); 
2. 'mmlyl;ic' (del)en(lelwy syntax, with node,q 
for all word o(:cm'r(,.n(:(>, also for p l ln ( ; t l l a -  
tion ma,rks etc., aim wit\]~ the tags for roo f  
t)hemic units and for 1)asic kin(t,q of surfa(:e 
syntactic rch~tion.q (Slfl).je(:t, O1).j('.(:t, Adver- 
t)ial, A(ljun('t), (:f. (Ila.ji~,) 
3. tcctogrammatical  (und('.l'lying) syntax, 
with a iIluch lllOr(; detailed classifit:ation 
of synl;actic relal;ions and with nodes tbr 
aul;o,q0.manl;ic lexical oc(-urren('es only 
(ra.|;her tha.n flln(:l;ion words), with indices 
corresponding to the syntactic relations, 
such as Actor, Addressee, Object ive (Pa- 
tient), Locative, Mmmer, Means, etc., and 
to mort)hologieal values sudl  as Preterite 
(Anterior), Conditional, Plural, etc., and 
also as the prototyl)ical values of 'in', 'into', 
%n', ~from', etc.; ('ol'r(!lates of flmctional 
words (a.nd morph(;m('~s) on this leve, l ha v('~ 
the form of indices of lexi('al nod(', labels.l 
1An except ion  concerns  coord inat ing  con junct ions ,  
which,  in PDT,  are. t reated  as head nodes  of the  (:o- 
2 Represent ing  Top ic -Focus  
Ar t i cu la t ion  (TFA)  in TGTSs  
2.1 A I )r ief  character i za t ion  of TFA 
'l'h(; te(:togranunatical tr(,.e struct;ures (TGTSs)  
should (:alIi;Ul'('. nol, only the syntactic ((l(,.1)(;n- 
/Mmy) relations, lint also the. TFA of the ut- 
t(;ran(:es in the corpus, sin(:('. TFA is cx1)resscd 
l/y grammal;i(:al me,ms and is releva.nt for the 
meaning of (;he sentenc(; (even for its trut\]t (:on- 
ditions), i.e. it; constitutes one of the basic as- 
1)e(:ts of un(l('rlying structures. Tlm scmanli(: 
reh',van/:c. (hi' TFA can be illustra.t('d 1)y (~xaml)lcs 
such as (1), wlfi(:h is a translal:i(m of the Czech 
(.'x. (1') (the capitals (l('amt(*. the. 1)la(:(;m(mt of 
th/'. int()naCion /:c.ntr(', i.e. I;tm focus t)rol)er): 2
(1) 0,) 1;.,..d.i.4,. i.,..~.vo/..c.,, i.,. t/,.,; S t l J ;7 'LANI )S .  
(b) i',, l.h,e ,%cl, hm, ds, lz,'NGLI,2H is ,~'pol,:e',,. 
(~') (,~) A,..d.id..:,j .~, .,,,.l.,,..,,~ ,,.,. Shctl,.',,.a.~t,::,j4,. 
0,~ Tll, 0 VI~ CH. 
ordinl;('d groul)S. Th is  makes  it; l)ossibl(, to ret)resent 
l;he I;(}(;I;og~rantlllai;i(:al st;rll(:l;llres of all s('dlt('.ilt;es a.q I;lee.q 
(rath(,., than using more-dimensional net:works); in this 
point, PDT ditlers fl:om the theoretical assumt)tions of 
th('. l)ragnian lqmctional Gen('xativ('. Descril)t, ion (now 
discussed in (Haji~':ov(~ ? al., 1998)). 
~In the  1)rol, otyt)i(:at case the in tonat ion  (:e.ntre is char-  
acter ized 1)y fal l ing (or r is ing-fal l ing) stress, but  there  are 
also cases in which (similarly as in questions, to a cer- 
tain degree) the centre has a rising stress. This concerns 
utterances displaying a featm'e of hesitation or incom- 
pleteness, of. (M.,); ofte.n also with greet, ings (such as 
Czech Dobrd j ihv \[Good morning\]) a difference of this 
kind marks the 'starting' token, connected with the ex- 
pectation of an answering token, which exhibits a riffling 
sl;ress. Although in it S(~ll|;(*dlCC containing occurrences of 
l)oth a rising aild & falling sLress the former exl)resses a
contrastive (part; of) topic, we l)retier to analyze it its the 
fOCIlS ill ~ SC'II|;CI\].C( '. wiLhoul, all ()c(;urrellCe of the  lal;l;er; 
in such a l)osit ion, the  r is ing stress regular ly  is carr ied  
1)3' an i tem referr ing to 'new'  in format ion .  In wr i t ten  
t;ext;s, some occurrences  of |;he r is ing stress are marked  
1) 3, a semico lon or by ' . . .  '. 
139 
(b) Na Shetlandsk~jch ostvovech se mluv( 
ANGLICKY.  
The conmmnicative function of the sentence 
can basically be rendered by understanding its 
topic (T) as 'what is the sentence about', and 
its focus (F) as the information that is asserted 
about the topic, i.e., schematically, the interpre- 
tation of the sentence S can be understood as 
s - F(T) 
Thus, (1)(a) asserts, on its preferred reading 
(with just the locative modification constituting 
its focus) about where English is spoken that 
it is in the Shetlandt, which hardly can be ac- 
cepted as true w.r.t, what we know of the actual 
world, if no specific context is present. (1)(b) is 
understood as true, stating about E. that it is 
spoken in the S. 
In the TGTSs the order of nodes is such that 
all parts of T precede all parts of F. Moreover, 
the order of nodes corresponds to the scale of 
communicative dynamism (CD, see Section 3 
below); a less dynamic node prototypically has 
the broader scope than a more dynamic one (if 
the nodes correspond to operators). F proper is 
then the most dynamic (the rightmost) node. 
TFA is relevant also tbr the semantics of nega- 
tion: 
(2) John 
(a) 
(b) 
didn't come because he was ILL. 
The reason for Jolm's not-coming was 
his illness. 
The reason for John's coming (e.g. to 
the doctor) was not his illness but 
something else (e.g. he wanted to in- 
vite the doctor for a party). 
With the paraphrase (a), the negated verb 
'come' is included in T, i.e. the fact that John's 
being ill is the cause of an event is asserted about 
the event that he did not come. With (b), the 
main verb 'come' alto belongs to T, but what it 
negated, is the relation between T and F: John 
came, but what is asserted about his coming is 
that the cause of this event was not his illness 
(he might have been ill, though). 
Every node in a TGTS is either contextually 
bound (CB) or non-bound (NB); this opposi- 
tion is a linguistic couterpart of the cognitive 
dichotomy of 'given' vs. hmw', where also an 
item, if corresponding to a 'given' referent pre- 
sented as occupying a newly characterized spe- 
cific position (often in relation to one or more 
'given' items), has the feature NB, cf.: 
(3) Give th, is to YOUR mother. (My parents 
don't like s~tch gifts.) 
kno',,,s  oth ete," ,lane.) Ho,. 
ever, th.is time she only invited IIER. 
The indexical pronoun 'your' in (3) and the 
anaphoric pronoun 'her' in (4) can only rethr to 
items that in a sense are 'known' in the given 
situation. However, in these examples, both of 
them occur as NB; their stress indicates their 
flmction as F proper of the respective sentence. 
Prototypically, an NB node belongs to F and a 
CB node is in T; however, a node not dependent 
immediately on a finite verb (esp. an adjunct) 
need not meet this condition. Thus, in (5), 'my' 
as a shifter, directly determined by the condi- 
tions of the discourse, is CB, although belong- 
ing to F, since it; depends on a part of F (see 
(HajiSovi~ et al, 1998) fbr a definition of T and 
F on the basis of contextual boundness and of 
syntactic dependency, aswell as for other details 
of the given descriptive frmnework). 
2.2 The  a t t r ibute  TFA in PDT 
Three values of the attribute TFA are distin- 
guished with every node in a TGTS: 
1. T a non-contrastive CB node, which always 
has a lower degree of CD than its governor, 
if any; 
2. F an NB node (if different from the main 
verb, then following after its head word in 
the TGTS) 
3. C a contrastive CB node 
Examples: 
(5) (VoIby v Izracli.) Po volbdeh.(T) si 
IzvaeIci(T) zvykaji(F) na novdho(F) pre- 
mid,'a(F). 
(Headline in tile newspapers: Elections ill 
Israel.) After the elections(T), the It- 
raelis(T) get used(F) to a new(F) Prime 
Minister(F). 
140 
(c,) &o,.~,o,,,(,(:(c) o,,,(T).#(P) (lol,,.:,;(F), (,,~(., 
.j(,ko politik(C) v.evynikd(F). 
(As a) St)ortsman(C ) he(T) is(F) g(,od(F), 
but as a politician(C) he does not ex(:el(l?). 
The instructions for the assigmnent of the 
values of TFA can be briefly sl)e('itied as fol- 
lows, if the surface word order and the 1)osi- 
tion of the intonation center (IC, see fl)otnote 
2 above) is taken illtO account, as well as /;he 
%ysi:(;mie' (canonical) ordering of the kinds of 
dependents (wtfich, in fact, (:ml difl'er with dif- 
fer(mr hc.ad words; SO is Sl)e(:itie(t either in the 
valen(:y f lames i1: the in(livi(hml lexi(:al entries, 
or, if i)ossibh.', fl)r whole lexical (:lasses and sub- 
(:lasses): 
1. 
2. 
3. 
4. 
( " ? ,. ,, : the bearer )\] \] C ~ I i' t;vt)i(:allv the right- 
most del)endent of the verl) 
if the IC is placed (m a nod(~ other than 
the rightmost one, th(', (:Oml)lem(',ntai;ions 
1)laced after IC ~> T 
a left side (lepend(mt; of the verl) ~ T o1' C, 
except for cases in which it (:learly ('arri(> 
1C 
th(: verb and lhose of its d(:l)endents tlmt 
stan(l \])el;weell l;he ver\]) all(l the F-llotl(: (se(: 
1) and thai; re'e. or(h'.red (without all inter- 
v(ufing sisW.r node) a(:(:or(lil~g to SO ~ F; 
among sisi;(:r nodes, all those carrying ~.\[" %l- 
low afl, er all those with C, and all those ('at- 
tying F follow after all those with T; there 
a.rc two sets of (':(:eptions: 
(a,) a. fo(;llS sensitive i)m'tiele can (:arry 
F even when i)l"e(:edillg its governing 
node that carries C, of. Se(:ti(m :3.2 be- 
\ ]O \V  
(b) ~ node M ca,rrying T or C can tbl- 
h)w after its nlol;her node if a node 
with F is 1)resent alnong the nodes 
subordilmte to M, })ut is M)sent both 
mnong the sisters of M mM among 
its superot'dinate nodes (here the re- 
h~tion of 'superor(linate' and %ubor(li- 
nat(;' is the tra nsil:ive (:losm(: of 'gov- 
erning' mid Mq)(;ndent'); (:f. the :lO- 
tion of 'l)roxy fo(:us', (:hara(:terized in 
(ilaji~ovit el; al., 1998), and extort- 
ples such as (Kierdh, o u(7-it, ele j.si tam 
vidS.lQ l/idS1 j scm tam u~.ite.Ic ch.emie 
\[lit. (Which t;eacher.A(:cus have-you 
the,e ,~ee.?) I s~w the,'e (the) te.cl~e," 
of-chemisl;ry\], with which the Patieltt 
'ltrTitch', follows after the verl) in the ui:- 
derlying tle(}, although it carries 3.' 
Note: For Cze('h, the SO of the main tyl)eS 
of dependency has 1)een found (on the 1)asis 
of eml)irical mmlysis of texts and of exper- 
iments with groups of speakers, see (Sgall 
eL al., 1995)) to h~vc (with most verbs and 
other heads) the tbllowing form, as for the 
main kinds of dependents: 
A(:tor- i rl'(~mt)oral ,:: Lo(:atiolt -:; 
lnstrmnent ,: Addressee-;  1)aticnt 
1,2Ithet a 
5. eml)(~(hh',d a.t;tril)utes =~> F (unless they are 
on\]y re, l)eat(',(l or restored) 
6. il:dexic, al expre, ssiolm (jd lIl, l,v \[youl, l,(,d' 
I,:,)wl, t(.z:,j Ihei'e.l, we~,k for,.s of p~'o- 
nouns, pronomina.1 expressions with a gen- 
e,.',~l .,(;~:.i,,g (,.;Z~do I,~o,nebodyl, :i~d,~o',~ 
\[once upon a timel...) ~ T (except ill 
cases of (:ontrast or as bearers of IC) 
7. si;rong forms of pronouns - -> F (after 
t)rel)osil:ions an(l in coordinated (:OllStru(:- 
t;ions: l;he, assignment of T or F in @zc(:h is 
gui(lcd by (;it(', g(mcral rules l through 4) 
8. restored lmdes, deleted in the surf:we forms 
of s(~,ll{,(~llces ~ T; we devote Section 2.3 
below to l;he 1)lacelllOllt of the, restored 
nodes Note: There are special cases of (:o- 
ordination, both in Cech and in English, 
which do not mee, t this eolMition: e.g. in 
"l 'hey drank white a.nd red win('? the firsl; 
occurr(m('e of %vine', which m~y be NB, is 
delet;ed in the surface (and restored in the 
TGTS).  
9. a node N dei)endent o the left in a way 
not meeting the conditiol: of 1)rojectivity: 
C (this node is then placed lllore to 
the right, to meet that condition; these and 
;~Let us not(: that Dirc(:tional.3 ('where to') tbllows 
aft, er Patient in Czc(:h as well as in Fmglish and also in 
Gc, rman, a(:cording to the Cml)irical research discussed in 
(M.,); t:lms i( is not exact o characot;riz(; the canonical 
order of German as a "mirror image" of that of English. 
141 
other movements are discussed in Section 
2.4: below) 
10. the nodes subor(linate to such an N move 
together with it and get T or F (according 
to the rules above) 
Note: The resulting TGTSs are projective, 
i.e. t br every pair of nodes x, y in a TGTS it 
holds that if x depends on y and x follows (pre- 
cedes) 37, then every node z following (preceding) 
y and preceding (following) x is subordinate to 
y. Thus, 'not to meet the condition of projec- 
tivity' concerns tim 'analytic' trees; this means, 
in other words, that this condition would not be 
met if the positions of x and y in the left-to-right 
order of the nodes in the TOTS (in the 'under- 
lying word order') always corresponded to their 
positions in the surface (morphemic and %na- 
lytic') word order. 
Example (with a very simplified linearized no- 
tation of the TGTS, in which every dependent 
is closed in its pair of parentheses): 
(7) K jdsotu(C) neni(F) nejmen.~:f(F) 
For triumphing is-not the-least 
&~vod(F). 
reasoll 
(r') (neg.F) bTjt.F ((jdsot.C) d,fi, vod.F 
(neg.F) be.F ((triumlflfing.C) reason.F 
(least.F)) 
A sentence with a non-prototypic~fi placement 
of the IC: 
(8) (Vdtgina m, inistr'gt St@aginovy novd vlddy 
patti k v&'n~m dr,uh,~irn cjzndmgj,~'\[h,o 
ruskdho intrikdna Berezovskdh, o. ) 
(The majority of the miifisters of St6pa~i- 
ney's new government belongs to faithfifl 
fi'iends of the best known Russian intriguer 
Berezovskij.) 
I(F) AKSJONENKO(F) u(h'2ujc(T) 
Even(F) AKSJONENKO(F) keeps(T) 
s Bcrezovsk~rn(T) blfzkd(F) 
with Bere ovskij(T) dose(F) 
styky(T). 
contacts(T). 
2.3 The  pos i t ion  of  a restored node  
The degree of CD of a node that is being re- 
stored (i.e. supposed to have been deleted in 
the surface form of the sentence), and thus also 
its position in the underlying word order, is de- 
termined on the basis of its relationship to its 
governing node. Since such a node ahnost al- 
ways is contextually bound (with the exception 
of the specific case of coordinated structures, see 
the Note after point 8 in Section 2.2 above), it 
is placed to the left of its governing word; more 
specifically: 
(a) if the restored node RN depends on a verb, 
then: 
(b) 
(c) 
(aa) if RN is not the single item depending 
on the given verb token, then RN is 
to be added in the 'Wackernagel posi- 
tion'; 
(ab) if RN has no sister nodes, then it is 
placed at the beginning of the clause; 
if RN is restored as depending on a noun 
(or adjective), I{N is placed as the least dy- 
namic dependent of this governing word; 
if more than one node are inserted as de- 
pending on one and the same item, then 
their order should confornl to tile systemic 
(%anonical') ordering of the valency slots 
(see the remark on SO in Section 2.2 above, 
point 4). 
Point (a) appears to be substantiated by the 
fact that e.g. the subject )ronolln appears ill the 
zero form in Czech under similar conditions as 
the weak, clitic pronouns, for which the position 
imlnediately to the left of the verb is typical, cf. 
sentences such as VSera (on) p~'igel pozd5 \]Yes- 
terday (he) canto here late\], Janu (oni) nevidSli 
\[lit.: .Jane-Accus they have-not-seen\], o1"(On) 
spal \[He was-sleeping\]. This concerns also such 
deletable items as e.g. the Directional with pfi- 
jet \[arrive\], cf. Jan dnes (sere/tam) ncp~'~;jcl \[ it. 
.John to-day (he,'e/there) has-not-arrived\]. 
The appropriateness of these preliminary 
rules is being checked uring the tagging proce- 
dure, the results of which will be of importance 
for a more exact (and more complete) formula- 
tion of the relevant parts of the description of 
the sentence structure of Czech. This aspect 
142 
of the useflflness of the corpus tagging concerns 
also many ol;h(;r 1)oinl;s of grammar. 
2.4 Under ly ing  and  sur face word  order  
Within the tagging procedure, tim differences 
between the two levels of the left-to-right order 
can be described 1)y movelnent rules, a prelimi- 
nary tbrm of which can be brietly characterized 
as follows: 
1. if a node 1111 carries C and a node M2 de- 
l)ending on M1 is 1)laced to the right of a 
node M3 superordinate o M1 in the surtkce 
word order, then M1 is placed immediately 
to the left of M2 in the resulting tree; cf. 
e.g. &o,'tov~',c (M1) o,, .# (M3) dob,",'j (M2) 
Ilit. (As a) sportsman he is goodl, see ex. 
(6) in Section 2.2 
2. if the 1)ositions of the nodes MI, M2 and 
M3 differ front l)oint \] only in t;hat M1 (h> 
pends on M2, then again M1 is placed im- 
mediately to the left of M2 ill the resul/:ing 
tree; of. exanll)le (7) ill Se(:i;ion 2.2 a\])ove, 
in which jdsot  occut)ies the position of M\], 
d,ivod that of M2, and nen i  that of M3, or: 
(9) ,lirku (M1)j.sme pld 'novd i (M3)  
po,~'la, l, (M2) do F'r~n(:i('. 
IliL George.Ac(:us (M1) we-1)la\],ned 
(M3) 1;o-send (M2)to \]Clan(:e\] 
3. ~ compar~tive of an ~Mje(:tive thai; \])rece(les 
its governing 1).OUll in t;he surface is moved 
to the right of this noun in (,,xamt)les such as 
vdt.?i re&to nc~ 13o,s'to',, \[a. hn'ger town than 
Boston\]; I his surface order probably should 
be limited (by a rule of grammar) to cases 
in which the two nouns 1)elong to a single 
semantic sul)class. 
4. in sentences exhibiting a secondary place- 
ment of IC, the bearer of IC occupies the 
rightmost 1)osition in the resulting l:ree; cf. 
example (\])(b) in Section 2.1 al)ove, in 
whi('h 'English' is tile t bt:us prol)er; the as- 
suinl)gion underlying the. t)lacemenl; of IC 
in a written text is that g~ written form of ~ 
sentence may correspond to dit\[erent (silo- 
ken) sentences, according to the differences 
of the 1)lacement of IC in the al)l)ropriate 
way of 1)renouncing t;he sentence. 
3 The  spec ia l  case of  focus  sens i t i ve  
par t ic les  
Since the focus sensitive particles are idengified 
(1)y the flmctor value RHEM for 'rhematizer' or 
'focalizer'), it is possitfle to use PDT also for 
a sl)ecitication of their occurrences in different 
positions 1)oth in the det)endency structure of 
the sentence and in its TFA. Tile starting hy- 
l)otheses, which might be checked on the basis of 
PDT, are. as tbllows (of. (Hajieov5 ctal., 1998)): 
3.1 Focus sensit ive particles in 
i )rototypical pos i t ions 
The 1)rotol;yl)ical syntactic position of a foc, al- 
izer ca.ll t)e understood as that of a dependent 
of a verb node; thus, in examples like (10) or 
(11), it is 1)ossible to specit:y lhe scope of the 
foealizer as the whole subtree subordinated to 
lhe verl) (where "sul)ordilml;ed' is undersl;ood as 
t\]le transitive closure of klel)en(lent' in the re- 
flexive s('.nsc, so I:hat the, verl) itself is in('luded); 
the st'Ol)e is divided into 1)a(:kground and focus 
of the fl)calizer (ti:'), as will 1)e specified in 3.2. 
Thus, in the interl)retation of (10) on the read- 
ing ret)rcsented (with many siml)lifications) by 
(10') it is included that (according to what P. 
knows) among l;hose whom % saw there was 
noone else t;han M (i.e. while 'T. saw' consti- 
l;lll;es l;he 1)ackground of 'only', its fl" is 'Mary'). 
Similarly, if in (11) the negation (all;hough ex- 
\]n'css('d l)y ~t prefix in Czech) is handled as a 
det)cn(lelfl: of the \,er\]), its bad{ground is the 
subject and tt' includes 1)oth the vcrl) an(l t;he 
oh.iect. 
(10) Pavcl v'\[, ~'. Tomd.# 
'Paul knows that Thomas 
vidH .je'n MAIUL  
saw only MAI~Y.' 
(Paul) knows ((Tholnas) sa.w (only) 
(Mary)) 
Mart in  ne(~te NOVINK 
~Marl;in nol;-rea(ts NEWSPAPERS.' 
(10') 
(11) 
\]n (12) only the adjective constitutes the ff of 
'only', its background consisting of 'car' (among 
all cars, P. only wants a blue one); thus, the fo- 
calizer can best })e described here as dq)ending 
el l  ~car'. 
(12) Pct, r ch, ce .jc.n MODIU2 auto. 
'Petr wants o1:1.5, (a) 13\],UE car.' 
143 
3.2 Focus sensitive particles in the 
hierarchy of comnmnicat ive 
dynamism 
The primary position of a focalizer ill a TR is at 
the boundary between tile topic and the focus 
of the verb clause and the tbcus of tile clause is 
then identical to tile focus of tile focalizer. If a 
fbcalizer is included in the topic, then its focus 
contains those items which in the TR are placed 
between this focalizer mid the next item ularked 
as C to tile right and are nlore dynamic than the 
tbcalizer). 
It should be noted that CD is understood here 
as a partial ordering defined so that: 
(i) in every set of a head and its daughter 
nodes, every daughter node placed to the 
right of its head is more dynamic than ev- 
cry daughter node placed to the left of its 
head; 
(ii) the relation 'more dynanfic' is deternlined 
by the irrettexive trausil;ive closure of (i). 
~i'hus, e.g. in the TI{ (10'), 'knows' is more dy- 
nalnic than 'Paul' and less dynmnic than 'saw' 
according to tile point (i), and both 'only' and 
'Mary', being more dynanlic titan 'saw', are 
more dynmnie than ~knows' according to the 
point (ii); however, ~Thomas' is neither more 
nor less dynamic than 'knows'. If (10) is cut- 
bedded into a more conlplex sentence as (a part 
of) its topic, titan 'Mary' is more dynanfic thml 
%nly' and has the f~atm'e C; thus, e.g. with 
'Since Paul knows that Thomas saw only Mary, 
he is not afraid', 'Mary' constitutes the whole fl 
of 'only', similarly as in (10'). 
Tile underlying word order W (a linear order- 
ing) is then defined on the basis of CD, with (iii) 
and (iv) holding tbr every two nodes x and y in 
a tree: 
(iii) if node x is nlore dynamic than node y, then 
x tbllows y under W; 
(iv) if node x follows node y under W, node u is 
subordinated to x and node z is subordinate 
to y, then u tbllows both y and z, and x 
follows z under W. 
Among tile non-prototyt)ical , secondary posi- 
tions of tbcalizers, there are also the cases of 
their clustering (e.g. 'not only'), as well as the 
sentences in which a focalizer itself constitutes 
the whole locus of tile sentence ('He DID realize 
this'). 
4 Summary  
After a brief characterization f the Prague De- 
pendency Treebank and of tile Praguian theory 
of Topic-Focus Articulation we have presented 
a proposal how the main aspects of tile intbr- 
nlation structure of the sentence (i.e. of its 
topic-focus articulation) cml be integrated into 
the tagging system that captures the underly- 
ing structures. The present form of the system 
nmkes it possible to check our hypotheses on a 
large text corpus, and thus perhaps to achieve 
a higher degree of automation (and reliability) 
of the proposed procedure. The last section ex- 
emplifies how the t)roposed approach makes it 
possible to analyze structures with the so-called 
focus sensitive operators. 
References  
.Jan Ha.ji(?. Building a syntactically anno- 
tated corpus: Tile prague dependency tree- 
bank. In E. Hajif:ov{~, editor, Ls's'ues of Va- 
lency and Meaning, Studies in Honour of 
Jarnlila Panevov5, pages 106 132. Karolinum, 
Prague. 
Jml Haji5 mid Barbora Hladkfi. 1997. Proba- 
bilistic and rule-based tagger of all inflective 
language - a comparison. In P'~vceedinfls of
the F@h Uo'n:/'ercrzce on Applied Natural Lan- 
.quagc Processing, pages 111-118, Washing- 
ton, D.C. 
Eva Ha.iiSovi~, B. Partee, and Petr Sgall. 1998. 
Topic-focus articulation, tripartite structures, 
and semantic ontent. Kluwer, Amsterdam. 
Steedlnml M. hlformation structure and 
the syntax-phonology interface, unpublished 
ntanuscript. 
Petr Sgall, O. Pfeiffer, W. U. Dressier, and 
M. Pfieek. 1995. Experimental research on 
systemic ordering. Theoretical Linguistics. 
144 
Deletions and their reconstruction in tectogrammatical syntactic 
tagging of very large corpora 
Eva ItAJICOVA 
UFAL 
Charles University 
Malostransk6 nfim. 25 
118 00 Prague, Czech Republic 
hajicova @ufal.mff.cuni.cz 
MarkEta CEPLOVA 
0FAL 
Charles University 
Malostransk6 n&n. 25 
118 00 Prague, Czech Republic 
ceplovam@yahoo.com 
Abstract 
The procedure of reconstruction of the 
underlying structure of sentences (in the 
process of tagging a very large corpus of 
Czech) is described, with a special attention 
paid to the conditions under which the 
reconstruction of ellipted nodes is carried 
out. 
1. The tagging scenarios with different 
(degrees and types of) theoretical 
backgrounds have undergone a rather rapid 
development flom morphologically based 
part-of-speech (POS) tagging through 
treebanks capturing the surface syntactic 
structures of sentences to semantically 
oriented tagging models, taking into account 
the underlying structure of sentences and/or 
certain issues of the 'inner' semantics of 
lexical units and their collocations. 
One of the critical aspects of the tagging 
scenario capturing the underlying structure 
of the sentences i  the 'depth' of the resulting 
tree structures; in other words, how far these 
structures differ from the surface structures. 
If we take for granted (as is the case in most 
of the syntactic treebanks) that every word 
of the (surface) sentence should have a node 
of its own in the surface tree structure, then 
this issue can in part be reformulated in 
terms of two subquestions: 
(i) which surface nodes are superfluous and 
should be 'pruned away', 
(ii) which nodes should be assumed to be 
deleted in the surface and should be 
'restored' in the underlying structure (e.g. in 
forms of different kinds of dummy symbols, 
see Fillmore 1999). 
In our paper, we are concerned with the 
point (ii). 
2. In the TG and post-TG writings, it is 
common to distinguish between two types of 
deletions: (a) ellipsis proper and (b) 
gapping. For both of them, it is crucial that 
the elliptical construction and its antecedent 
should be parallel and 'identical' at least in 
some features. The two types of ellipsis can 
be illustrated by examples (1) and (2), 
respectively. 
(1) Psal jenom flkoly, kterd chtal. 
lit. 'He-wrote only homework's which he- 
wanted' 
(2) Honza dal Marii rfiki a Petr Ida tulipfin. 
lit. 'John gave Mary rose and Peter Ida tulip' 
For both types, a reconstruction i  some 
way or another is necessary, if the tree 
structure is to capture the underlying 
structure of the sentences. 
3. The examples quoted in the previous 
section cover what Quirk et al (1973, pp. 
536-620) call 'ellipsis in the strict sense'; 
they view ellipsis as a purely surface 
278 
phenomenon: the recoverability of the 
ellipted words is always unique and 'fits' 
into the surface structure. They difl'erentiate 
ellipsis fiom 'semantic implication' which 
would cover e.g. such cases as (3) and (4): 
(3) John wants to read. 
(4) Thanks. 
If (3) is 'reconstructed' as 'John wants John 
to read', then the two occurrences of 'John' 
are referentially different, which is not true 
about the interpretation of (3). With (4), it 
cannot be uniquely determined whether the 
full corresponding structure should be '1 owe 
you thanks' or 'I give you thanks' etc. 
4. For tagging a corpus on the underlying 
level, it is clear that we cannot limit 
ourselves to the cases of ellipsis in tile strict 
sense but we have to broaden lhe notion of 
'reconstruction' to cover both 
(i) deletions licensed by the grammatical 
i)roperties of sentence lements or sentence 
structure, and 
(it) deletions licensed only by the preceding 
context (be it co-text or context of situation). 
4.1. In our analysis of a sample of Czech 
National Corpus, two situations may occur 
within the group (i): 
(a) Only the position itself that should be 
"filled" in the sentence structure is 
predetemfined (i.e. a sentence element is 
subcategorized for this position), but its 
lexical setting is Tree'. 
This is e.g. the case of the so-called pro- 
drop character of Czech, where the position 
of the subject of a verb is 'given', but it may 
be filled in dependence on the context. 
(5) Pi:edseda vlfidy i:ekl, ~e pf'edlo~i nfivrh na 
zmenu volebniho systdmu. 
'The Prime-minister said that (0) will submit 
a proposal on tile change of the electoral 
system.' 
The 'dropped' subject of the verb pi:edlo2i 
'will submit' may refer to the Prime- 
minister, to the Govermnent, or to 
somebody else identifiable on the basis of 
the context. 
Here also belong cases of the semantically 
obligatory but deletable complementations 
of verbs: the Czech verb l)i:(/et 'to arrive' has 
as its obligatory complementation a Actor 
and a Directional "where-to" (the 
obligatoriness of the Directional 
complementation can be tested by a question 
test, see Panevovfi 1974; Sgall et al 1986), 
which can be deleted on the surface; its 
reference is determined by the context. 
(6) Vlak pi~ijede v poledne. 
'Tile train will arrive at noon.' 
The utterer of (6) deletes the Direction 
'where-to' because s/he assumes that the 
hearer knows the referent. 
(b) Both the position and its 'filler' are 
predetermined. 
This is the case of e.g. the subject of the 
infinitival complement of the so-called verbs 
of control as in (7). 
(7) Pi"edseda vVldy slibil pi~edlo~it nfivrh na 
zm6nu volebniho systdmu. 
'The Prime-minister promised to submit a 
proposal on the change of the electoral 
system.' 
The identification of the underlying subject 
of the infinitive is 'controlled' by the Actor 
of the main verb, in our example it is 'the 
Prime-minister'. 
Another example of this class of deletions 
are the so-called General Participants (close 
to the English one or German man): General 
Actor in (8), General Patient in (9), or 
General Addressee in (10). 
(8) Ta kniha byla u~ vydfina dvakrfit. 
'The book has already been published twice.' 
(9) V nedeli obvykle pe~,u. 
'On Sundays (I) usually bake.' 
(10) D6dei?ek 6asto vypravuie pohfidky. 
'Grandfather often tells fairy-tales.' 
279 
4.2 Within the group (ii), there belong cases 
of the so-called 'occasional ellipsis' 
conditioned by the context alone. 
We are aware that not everything in any 
position that is identifiable on the basis of 
the context can be deleted in Czech (as 
might be in an extreme way concluded from 
examples (11) through (14)). However, the 
conditions restricting the possibility of 
ellipsis in Czech seem to be less strict than 
e.g. in English, as illustrated by (15): 
(11) Milujeme a ctime svdho u~itele. 
'We love and honour our teacher.' 
(12) Marii j sem vid~l a slygel zpivat. 
lit. 'Mary-Acc. Aux-be saw and heard to- 
sing' 
'! saw and heard Mary singing.' 
(13) Jirka se v~era v hospod6 opil do 
n6moty a Honza dneska. 
lit. 'Jirka himself yesterday in pub drunk to 
death and Honza today.' 
'In the pub, Jirka drunk himself to death 
yesterday and Honza today.' 
(14) Petr f-ikal Pavlovi, aby ~el ven, a 
Martin, aby zflstal doma. 
'Peter told Pavel to go outside and Martin 
(told Pavel) to stay at home.' 
(15) (Potkaljsi veera Toma?) Potkal. 
'(Did you meet Tom yesterday?) Met'. 
4.3 in addition to setting principles of which 
nodes need to be restored it is also important 
to say in which cases no restoration is 
desirable. Nodes are not restored in cases of: 
(a) accidental omission (due to emotion, 
excitement or insufficient command of 
language, see e.g. Hlavsa 1990); 
(b) unfinished sentences, which usually lack 
focus (unlike ellipsis where the 'missing' 
elements belong to topic); 
(c) sentences without a finite verb that can 
be captured by a structure with a noun in its 
root (in these cases there are no empty 
positions, nothing can be really added). 
All these cases have no clear-cut boundaries, 
rather it is more appropriate to expect 
continual transitions. 
5.1 The Prague Dependency Tree Bank 
(PDT in the sequel), which has been 
inspired by the build-up of the Penn 
Treebank (Marcus, Santorini & 
Marcinkiewicz 1993; Marcus, Kim, 
Marcinkiewicz et al 1994), is aimed at a 
complex annotation of (a part of) the Czech 
National Corpus (CNC in the sequel), the 
creation of which is under progress at the 
Department of Czech National Corpus at the 
Faculty of Philosophy, Charles University 
(the corpus currently comprises about 100 
million tokens of word forms). PDT 
comprises three layers of annotations: (i) the 
morphemic layer with about 3000 
morphemic tag values; a tag is assigned to 
each word form of a sentence in the corpus 
and the process of tagging is based on 
stochastic procedures described by Haii6 
and Hladkfi (1997); (ii) analytic tree 
structures (ATSs) with every word form and 
punctuation mark explicitly represented asa 
node of a rooted tree, with no additional 
nodes added (except for the root of the tree 
of every sentence) and with the edges of the 
tree corresponding to (surface) dependency 
relations; (iii) tectogrammatical tree 
structures (TGTSs) corresponding to the 
underlying sentence representations, again 
dependency-based. 
At present the PDT contains 100000 
sentences (i.e. ATSs) tagged on the first two 
layers. As for the third layer, the input for 
the tagging procedure are the ATSs; this 
procedure is in its starting phase and is 
divided into (i) automatic preprocessing (see 
BOhmov~ and Sgall 2000) and (ii) the 
manual phase. The restoration of the 
syntactic information absent in the surface 
(morphemic) shape of the sentence (i.e. for 
which there are no nodes on the analytic 
level) is mostly (but not exclusively) done - 
280 
at least for the time being - in the manual 
phase of the transduction procedure. In this 
phase, the tagging of the topic-focus 
articulation is also performed (see 
Burfifiovfi, Hajieovfi and Sgall 2000). 
5.2 The reconstruction of deletions in 
TGTSs is guided by the following general 
principles: 
(i) All 'restored' nodes standing for elements 
deleted in the surface structure of the 
sentence but present in its underlying 
structure get marked by one of the following 
values in the attribute DEL: 
ELID: the 'restored' element stauds alone; 
e.g. the linearized TGTS (disregarding other 
than structural relations) for (16) is (16'). 
(Note: Every dependent item is enclosed in a 
pair of parenthesis. The capitalized 
abbreviations stand for dependency relations 
and are self-explaining; in our examples we 
use English lexical units to make the 
representations more transparent.) 
(16) Sbfral houby. 
'Collected-he mushrooms.' 
(16') (he.ACT.ELID) collected 
(mushrooms.PAT) 
ELEX: if the antecedent is an expanded 
head node and not all the deleted nodes 
belong to the obligatory colnplementations 
of the given node and as such not all are 
reconstructed, cf. e.g. the simplified TGTS 
for (13) in (13'). 
(13') ((Jirka.ACT) (yesterday.TWHEN) 
(pub.LOC) drunk-himself (to- 
death.MANN)) and (drunk-himself.ELEX 
(Honza.ACT) (today.TWHEN)) 
EXPN: if the given node itself was not 
ellipted but some of its complementations 
were and are not restored (see the principle 
(iii)(b) below), cf. e.g. the simplified TGTS 
in (15') for (15) above, with non- 
reconstructed telnporal modification: 
(15') (I.ACT.ELID) met.EXPN 
(Tom.PAT.ELID) 
(ii) The restored nodes are added 
immediately to the left of their governor. 
(iii) The following cases are prototypical 
examples of restorations (for an easier 
reference to the above discussion of the 
types of deletions, the primed numbers of 
tile TGTSs refer to the example sentences in
Section 4): 
(a) Restoration of nodes for 
complementations for which the head nodes 
(governors) are subcategorized. The 
assignment of the lexical labels is governed 
by the following principles: in pro-drop 
cases (5') (comparable to Fillmore's 1999 
CNI - constructionally-licensed null 
instantiation) and with an obligatory but 
deletable complementation (6') (cf. 
Filhnore's definite null instantiation, DNI) 
the lexieal value corresponds to the 
respective pronoun; with grammatical 
coreference (control), the lexical value is 
Cor (7'); in both these cases, the lexical 
wdue of the antecedent is put into a special 
attribute of Coreference; in cases of general 
participants (cf. Filllnore's indefinite null 
instantiation - INI) the lexical value is Gen 
(10'): 
(5') (prime-minister.ACT) said 
((he.ACT.ELID; COREF: prime-minister) 
will-submit.PAT (proposal.PAT 
(change.PAT (system.PAT 
(electoral.RSTR))))) 
(6') (train.ACT) will-arrive (noon.TWHEN) 
(here/there.ELID.DIR3) 
(7') (prime-lninister.ACT) promised 
((Cor.ACT.ELID; COREF: prilne-nainister) 
submit.PAT (proposal.PAT (change.PAT 
(system.PAT (electoral.RSTR))))) 
(10') (grandfather.ACT) (often.TWHEN) 
(Gen.ADDR.ELID) tells (fairy-tales.PAT) 
(b) Elipted optional complelnentations are 
not restored (see (13') above) unless they are 
governors of adjuncts. 
(c) For coordinated structures, the guiding 
principle says: whenever possible, give 
281 
precedence to a "constituent" coordination 
before a "sentential" one (more generally: 
"be as economical as possible"), thus 
examples like (t7) are not treated as 
sentential coordination (i.e. they are not 
transformed into structures corresponding to
(17')). 
(17) Karel pfinesl Jan6 kv6tiny a knihu. 
'Katel brought Jane flowers and a book.' 
(17') Karel pfinesl Jan6 kv6tiny a Katel 
pfinesl Jan6 knihu. 
'Karel brought Jane flowers and Karel 
brought Jane a book.' 
A special symbol CO is introduced in the 
complex labels for the coordinated nodes to 
mark which nodes stand in the coordination 
relation and which modify the coordination 
as a whole (see (11')); the lexical value of 
the restored elements is copied from the 
antecedents (see (13') above): 
(11') ((we.ACT) (love.CO) and (honour.CO) 
(our teacher.PAT)) 
The analysis of (11') is to be preferred to 
sentential coordination with deletion also for 
its correspondence with the fact, that in 
Czech object can stand after coordinated 
verbs only if the semantic relation between 
the verbs allows for a unifying 
interpretation, as shown by cases, where the 
object must be repeated with each verb 
(compare the contrast between (18) and 
(19)). 
(I 8) Potkal jsem Petra, ale nepoznal jsem 
ho. 
'I met Peter, but I didn't recognize him.' 
(19) ??Potkal, ale nepoznal jsem Petra. 
'I met but didn't recognize Peter.' 
However, there are cases where the 
coordination has to be taken as sentential or 
at least at a higher level. As modal verbs are 
represented asgramatemes of the main verb, 
sentences as (20) have to be analysed as in 
(20'): 
(20) Petr musel i cht61 pfijit. 
'Peter had to and wanted to come.' 
(20') (Peter.ACT) (had-to-come.CO) and 
(wanted-to-come.ELID. CO) 
Another case of a less strict adherence to the 
economy principle are sentences with 
double reading. Such a treatnaent hen 
allows for a distinction to be made between 
the two readings, e.g. in (21), namely 
between (a) 'villagers who are (both) old and 
sick' and (b) 'villagers who are sick (but not 
necessarily old) and villagers who ate old 
(but not necessarily sick)': 
(21) Jim zachrfinil star6 a nemocnd 
vesnieany. 
'Jim saved old and sick villagers.' 
(2 l'a) (Jim.ACT) saved (villagers.PAT 
((old.CO.RSTR) and (sick.CO.RSTR))) 
(2 l'b) (Jim.ACT) saved 
((villagers.CO.PAT.ELID (old.RSTR)) and 
(villagers.CO.PAT (sick.RSTR))) 
5.3 The research reported oll in this 
contribution is work in progress: the 
principles are set, but precisions are 
achieved as the annotators progress. There 
are many issues left for further 
investigation; let us mention just one of 
them, as an illustration. Both in (22) and in 
(23), the scope of 'mfilokdo' (few) is (at least 
on the preferential readings) wide ('there are 
few people such that...'); however, (24) is 
ambiguous: (i) there were few people such 
that gave P. a book and M. flowers, (ii) few 
people gave P. a book and few people gave 
M. flowers (not necessarily the same 
people). A similar ambiguity is exhibited by 
(25): (i) there was no such (single) person 
that would give P. a book and M. flowers, 
(ii) P. did not get a book and M. did not get 
flowers. However, there is no such 
ambiguity in (26). 
(22) Mfilokdo jf jablka a nejf ban,~ny. 
lit. 'Few eat apples and do-not-eat bananas' 
'Few people eat apples and do not eat 
bananas.' 
282 
(23) Mfilokdo dal Petrovi knihu a Marii 
kvetiny ne. 
lil. 'Few gave Peter book and Mary flowers 
not' 
'l~ew people gave Peter a book and did not 
give Mary flowcrs.' 
(24) Mfilokdo dal Petrovi knihu a Marii 
kv6tiny. 
lit. 'Few gave Peter book and Mary flowers' 
'Few people gave Peter a book and Mary 
flowers.' 
(25) Nikdo nedal Petrovi knihu a Marii 
kv6tiny. 
lit. 'Nobody did-not-give Peter book and 
Mary flowers' 
'Nobody gave Peter a book and Mary 
flowers.' 
(26) Petrovi nikdo nodal knihu a Marii 
kv6tiny. 
lit. 'Peter nobody did-not-give book and 
Mary flowers' 
'To Peter, nobody gave a book and to Mary, 
flowers.' 
An explanation of this behaviour offers 
itself in terms of the interplay el! contrast in 
polarity and of topic-focus articulation: an 
element standing at Ille beginning of the 
sentence with a contras\[ in polarity carries a 
wide scopc ('few' in (22) and (23)); with 
sentences without such a contrast both wide 
scope and narrow scope interpretations are 
possible ('few' and 'nobody' in (24) and (25), 
respectively); (25) differs from (26) in that 
in the latter sentence, the element in 
contrastive topic is 'Peter' in the first 
conjunct and 'Mary' in the second, rather 
than 'nobody', and there is no contrast in 
polarity involved. 
The tagging scheme sketched in the 
previous ections offers only a single TGTS 
for the ambiguous structures instead of two, 
which is an undesirable result. However, if 
lhe explanation offered above is confl'onted 
with a larger amount of data and confirmed, 
lhe difference between the two 
interpretations could be captured either by 
means of a combination of tags for the 
restored nodes and for the topic-focus 
articulation or by diflerent structures for 
coordination: while example (22) supports 
the economical treatment of coordinate 
structures (the ACT modifying the 
coordination as whole), examples (24) 
through (26) seem to suggest hat there may 
be cases where the other approach 
(sentential coordination with ellipsis) is 
more appropriate to capture the differences 
in meaning. 
Acknowledgement. The research reported 
on in this paper has bccn predominantly 
carried out within a project suplgorted by the 
Czech Grant Agency 405-96-K214 and in 
part by the Czech Ministry o17 Education VS 
96-151. 
References 
l~;6hmc, v{l A. and Sgall F'. (2000) Automat ic  
procedures in tectogrammatical t gging. In these 
lhocecdings. 
Burfifiovfi E., ltaji~ovfi E. and Sgall P. (2000) 
Tagging oJ' ve O' large coq~ora: T<)pic-Foctts 
Articuhltion. In this volume. 
Fillmore C. J. (1999) Silem amtFhora." Corlms, 
FrallleNel, all(\[ missin,~ colslplelllellts. Paper 
presented atthe TELI~.I workshop: P, ratislava. 
llaji,2 J. and ltladkfi 13. (1997) Probabilistic crag 
rule-based tagger of an iqflective hmguage - a 
eomparisols. In "Proceedings of the Fifth 
Conference on Applied Natural Language 
Processing", Washington, D.C., pp. 111 -118. 
llaii6ovfi E., Panevovfi J. and Sgall P. (1998) 
Language Resources Need Amtotatimls 7b Make 
Them Really Reusable: The Prague Dependency 
7)'eebank. in "Proceedings ofthe First International 
Conference on Language Resources & 
Evaluation", Granada, Spain, pp. 713-718. 
llaji~ovfi E., Partee B. and Sgall P. (1998) 7bpic- 
focus articulation, trOmrtite structures, and 
semantic ontent. Kluwcr, l)ordrccht. 
Hlavsa Z. (1990) Some Notes on Ellipsis in Czech 
Language cmd Linguistics. Studi italiani di 
linguistica leorica cd applicata 19, pp. 377-387. 
Marcus M. P., Kim G., Marcinkicwicz M. A. et al 
(1994) 7'he Penn Treebank: Amzotating l'redicate 
283 
Argunwnt Structure. Proceedings of tile ARPA 
Human Language Technology Workshop. Morgan 
Kaufmann, San Francisco. 
Marcus M. P., Santorini B. and Marcinkiewicz M. A. 
(1993) Building a Large Annotated Ccnpus of 
English: the Penn Treebank. Computational 
Linguislics, 19(2), pp. 313-330. 
Panevovfi J. (1974) On verbal frames in Functional 
Generative Description. Prague Bulletin of 
Mathematical Linguistics 22, pp. 3-40; 23(1975), 
pp. 17-52. 
Quirk R., Greenbaum S., Leech G. and Svartvik J. 
(1973) A glztmmar of contemporao, English. 2nd 
Ed. Longman, London. 
Sgall P., Haji~ovfi E. and Panevovfi J. (1986) The 
Meaning of the Sentence in lts Semalttic and 
Pragmatic Aspects, ed. by J. L. Mey, Dordrecht, 
Reidel - Prague, Academia. 
284 
 
	ACL Lifetime Achievement Award
Old Linguists Never Die, They Only Get
Obligatorily Deleted?
Eva Hajic?ova???
Charles University
1. Introduction
Martin Kay, in his speech delivered in 2005 on receipt of his ACL Lifetime Achievement
Award, specified computational linguistics as follows: ?Computational linguistics is
trying to do what linguists do in a computational manner? (Kay 2005, page 429). I
believe it is a legitimate question for a computational linguist to ask what linguists do.
Coming from Prague, it is then quite a natural question for me to look back and to
recollect what the ?old? linguists (who never die but get obligatorily deleted on the
visible surface) with the background of the world-famous Prague Linguistic School
(PLS) contributed to linguistic studies and perhaps to suggest what aspects of their
heritage are even today fruitful for computational linguistics.
First, to place the PLS in the course of the development of linguistic studies, it
should be recalled that the Prague Linguistic Circle belongs to the first bodies that took
part in the transition of the older diachronic paradigm of linguistics to a synchronic
theory of language. Soon after its first session (taking place in 1926 in the office of
the chairman of the Circle till his death in 1945), the Circle entered the international
scene first of all with its systematically elaborated phonological theory. Starting with
the Hague Linguistic Congress (see Actes 1928), Praguian phonology became the pilot
discipline of structural linguistics. This approach was far from unified, but the strength
of the Circle was in its spirit of dialogue, which kept the Circle receptive to new ideas,
rather than in any set of postulates commonly professed. In my talk I will concentrate
on three fundamental Prague School tenets, which I believe to have their validity also
in the modern context of linguistic theory and computational linguistics. What I have in
mind here is the Circle?s structural and functional orientation, as well as the attention it
has paid to the opposition of the center and the periphery of language structure, based
on the concept of markedness.
2. The Structural Point of View of PLS
The PLS is generally (and truly) characterized by two attributes: structural and func-
tional. Let us first turn to the structural point of view, namely, the School?s endeavor
to view language as a system of systems rather than to study individual phenomena as
ad hoc, non-systematic issues. The Circle shared de Saussure?s understanding of lan-
guage as a system of (bilateral) signs, in which only oppositions rather than fixed
? Logo on the Indiana University Linguistic Club tee-shirt, 1984.
?? Matematicko-fyzika?ln?? fakulta, Univerzita Karlova, Malostranske? na?mest?? 25, CZ-11800 Praha,
Czech Republic. This paper is the text of the talk given on receipt of the ACL?s Lifetime
Achievement Award in 2006.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 4
entities play a role. As mentioned above, this attitude was most apparently reflected
in the study of phonology as a system displaying distinctive features and employing
the notion of binary oppositions. Jakobson (1929) presented the phonological repertory
(both in synchrony and in diachrony) as a system of oppositions (mainly binary, priva-
tive), based on acoustic distinctive features and understood as the clue to the sound and
meaning relationship.
Along with phonemes and morphemes, the sentence was also recognized as one of
the fundamental fields of systematic oppositions, that is, as an ingredient of la langue.
Mathesius (1928, 1936) formulated a concept of functional syntax; a structural view of
syntax, based on the dependency relation, was elaborated by Tesnie`re (1934), a French
member of the Circle, who was a professor of the Ljubljana University; his monograph
was published only posthumously (Tesnie`re 1959), but his papers were known in
Prague, and his approach to syntax was applied to Czech by S?milauer (1947), who
combined dependency syntax with a constituent-based view of the relation between
predicate and subject.
Dependency-based approaches, which understand the verb as the center of the
sentence structure and describe this structure on the basis of binary relations between
heads and their modifiers, have been for a long time a matter of Continental syntactic
theories rather than of the mainstream syntactic approaches on the other side of the
Atlantic. However, the notion of head can be found also in Bloomfield (1933) when
referring to the names of the main constituents of the sentence, that is, NP (noun phrase,
with N as its head) and VP (verb phrase, with V as its head). In the framework of
the Chomskyan approach, originally based exclusively on the concept of immediate
constituents, the notion of head becomes the basic notion of X-bar theory. Originally,
four categories were singled out as possible heads of their respective maximal pro-
jections, namely, N, V, Adj, and P(rep); as remarked by James McCawley (personal
communication, around 1990), such a theory may be interesting unless the specification
of the set of basic categories grows beyond some reasonable limit. McCawley?s critical
remark reflected the gradual development of X-bar theory, which allowed practically
any constituent (or, more generally speaking, any arbitrary symbol for a grammati-
cal value) to act as the head, dependent on the needs of the analysis of this or that
construction.1
The very name head-driven phrase structure grammar, an influential theory combining
an immediate constituent approach with elements of a dependency-based approach,
as proposed by Pollard and Sag (1994), explicitly points out that the theory takes ac-
count of the main element within a constituent. Although their approach is constituent
based (working with a lexically based X-bar syntactic theory; [Pollard and Sag 1994,
page 362]), the authors are aware that the notion of constituent structure is widespread
but that it is not based on sufficiently convincing direct evidence. The authors refer to
Hudson?s (1984) approach and claim that it belongs to exceptions that do not overesti-
mate the constituent structuring of sentence elements.
It is sometimes doubted if the direction of the dependency relation, namely, the
determination of the governor and the dependent in each pair (syntagm) can be reliably
stated. We believe that in the prototypical case, the main criterion for this distinction
1 To be fair, I should add that one of the rare attempts at a more explicit characterization of the notion
of head can be found in Adger?s monograph on minimalism (Adger 2003, page 75), which mentions
two criteria: one based on the distribution of the whole constituent and the other taking into account
which constituent determines the reference of the whole constituent.
458
Hajic?ova? Old Linguists Never Die
can be based on the possibility that, in the endocentric constructions, the dependent
can be absent (not just deleted on the surface). Thus, for example, in Yesterday, my father
worked for the whole day in the garden it is possible to leave out the dependents yesterday,
my, for the whole day, and in the garden without the sentence losing its grammaticality.
However, there are exocentric pairs such as (to) find something, where neither of the two
members of the pair can be deleted and for which, therefore, the mentioned method
by itself cannot help to find out which element is the governor and which is the
dependent. What helps here is the principle of analogy on the level of parts of speech:
On the basis of the existence of verbs without objects it can be concluded that in such
pairs the verb is the governor (in our particular example, (to) find). In the same vein,
subject (Actor) can be understood as dependent on the verb, because there are verbs
also without a subject: In It is raining (Latin Pluit), It is just a surface ?filler,? absent
in the sentence structure proper. This view is also supported by the observation based
on the annotators? agreement when assigning the dependency structures (trees) in the
Prague Dependency Treebank: The annotators did not have too many troubles with the
determination of the direction of dependency; if there was a disagreement, it concerned
their assignment of the type (value) of the dependency relation (see Hajic?ova?, Pajas, and
Vesela? 2002).
There is one linguistic phenomenon?present more or less in every language?all
syntactic theories have to bother about, namely, the relation between syntactic struc-
ture and word order (the discontinuity of constituents, for which Gazdar [1981] intro-
duced the term unbounded dependencies, used, for example, also by Pollard and Sag [1994,
pages 157ff.]; see also the term long-distance dependencies used by some other authors) or,
in terms of formal dependency descriptions, the non-projectivity of syntactic construc-
tions. Informally speaking, the strongly restrictive condition of projectivity says that if a
node a depends on b and there is a node c between a and b in the linear ordering, c is
subordinated to b (where subordinated means an irreflexive transitive closure of depends).
See Figure 1 for an example of non-projective parts of a tree; the vertical line leading
from a intersects the dependency edge leading from a to c.2
The more restricted the formal syntactic description is, the more valuable are the
observations based on it; in this sense, the condition of projectivity might well serve
its purpose. However, there are seemingly many non-projective constructions in the
surface shapes of the sentences. The task then is to attempt to classify the constructions
in which the condition of projectivity is not met in the surface shape of the sentence
and to attempt at a description not only meeting the condition as far as the core of the
language system (see Section 4) is concerned, but also accounting by some simple, well-
defined means for the cases of superficial non-projectivity (a preliminary formulation
of movement rules specified as a transition from projective underlying trees to strings
of morphemes in which the condition of projectivity cannot be applied can be found in
Sgall [1997] and Hajic?ova? and Sgall [2003]). This is, of course, a rather strong hypothesis
that has to be verified and made more precise on the basis of systematic empirical
research. It should be mentioned in this connection that it is in line with the Praguian
approach that function words are distinguished from autosemantic words and that only
the latter constitute nodes of their own in the underlying trees; from this it follows that
in the numerous cases in which the ?non-projectivity? of surface word order concerns
2 Projectivity as a property of word order important for a formal description of language was already
stated by Hays (1960, 1964) and Lecerf (1960) in formal grammar; the condition of projectivity (in
different forms that have been proved as equivalent) was defined by Marcus (1965) and used in many
writings working with dependency descriptions.
459
Computational Linguistics Volume 32, Number 4
Figure 1
Examples of non-projective parts of a dependency tree.
auxiliary verbs or conjunctions, and so forth, the projectivity of underlying syntactic
structure is not at stake.
The introduction of the notion of a head brings into the foreground the connection of
grammar and lexicon; the necessity of such a relationship was already quite apparent in
the earlier works of Fillmore (1966, 1968) introducing the so-called case grammar, which
explicitly follows up Tesnie`re?s notion of valency. The term ?case? does not directly refer
to case as a morphological category but to the meaning (function) of a (morphological)
case: for example, Addressee is a prototypical meaning (function) of Dative, Agentive
is a prototypical meaning of Nominative, and so on. The concept of valency is crucial
in that it reflects the fundamental aspect of the presence of grammatical information
in the lexicon: The valency frame is a part of the lexical entry, in which the obligatory
and optional syntactic kinds of dependents of the given (head) word are registered.
It is also well known that Fillmore?s theory (as well as the thematic roles of Gruber
[1967]) played a substantial role in the introduction of theta roles (and theta grids) in
the Chomskyan model of government and binding (later called, more appropriately, the
Principles and Parameters model).
Fillmore himself explicitly mentioned that, when proposing his case grammar,
he did not primarily consider which formal description his approach would fit into.
However, he presents an example of how his approach can be formulated in terms of a
phrase structure model: The sentence S can be decomposed into two parts, Modality and
Proposition; the Proposition in turn can be articulated into the verb and a set of noun
phrases, which are characterized by one of the case markers, that is, K1NP, K2NP, . . . ,
KnNP. Each of these noun phrases can then be decomposed into the noun phrase proper
and the given case marker ki (Agentive, Addressee, Objective, etc.). The analysis of
Robinson (1969, 1970) devoted to the relation between Fillmore?s approach and that
of transformational grammar (of that time) throws an interesting light on the possiblity
of a smooth transition from a phrase-based approach to a dependency-based one, which
is more transparent and economical. In Fillmore?s proposal, the case relations, that is,
the relations of the noun phrase to the verb, are actually captured twice, once as the
marker ki and once as the characteristics of the given phrase (KiNP). It is then possible
to work with a pure dependency tree structure, where the root of the tree is the verb,
and the nouns (or, as the case may be, other word categories) depend on the root as
dependents with a certain type of relation.
It goes without saying that Fillmore?s case grammar and its follow-up frame nets
conception has influenced in a substantial way many of the contemporary approaches
not only to treebank annotation and computational lexicology (cf., e.g., Fillmore et al
2003) but also the work on underlying sentence structure in general.
Two ?historically? motivated and seemingly contradictory observations are in place
here: It can be documented by references to the development of linguistic theory in the
460
Hajic?ova? Old Linguists Never Die
past 50 years that the deeper the analysis goes, the more the need of an introduction of
the distinction between the notions of ?head? and ?modifier? (predicate, argument) is
felt. Let us only recall here such approaches as:
(i) the lexicosemantic analysis by Katz and Postal (1964), who work with the
notions of head and modifier when specifying selection restrictions;
(ii) the distinction between surface-oriented constituent structure and the
(underlying) functional structure in lexical functional grammar by Bresnan
(1978) and Kaplan and Bresnan (1982);
(iii) the above-mentioned case grammar by Fillmore, motivated by the
conviction that Chomskyan deep structure (with its specification of ?deep?
subject as a constituent of S regardless of the (different) semantic relations
of the given NP to the verb) is not deep enough to capture the real
underlying structure of the sentence; and
(iv) the consecutive introduction of theta roles into the government and
binding theory.3
On the other hand, dependency-based considerations have gradually and evasively
penetrated to the ?data?-oriented statistical methods and treebank annotations. As the
freshest example, let us only refer to the recent EACL 2006 conference in Trento and the
HLT-NAACL 2006 conference in New York with its CoNLL-X Shared Task on Multilin-
gual Dependency Parsing (working with treebanks of 12 languages, of different sizes).
In other words, the seemingly surface oriented analysis is prevailingly dependency
based.4
A possible explanation for this apparent contradiction may be looked for in the
economy and transparency of the dependency-based trees: In their applications, the
data-oriented systems also aim at a representation of the meaning of the surface shapes
of sentences (whatever one can understand by ?meaning?), so that their attention is
focused on a most transparent and economic way (avoiding ?extra? nodes for phrases
such as NPs, VPs, . . . , etc.) from the surface to the depth. Dependency analysis offers
such a way.5
3 We have restricted our attention here only to systems staying in principle within the development
of the Chomskyan paradigm or originating as a reaction to it. However, when discussing the relation
between or combination of constituent-based and dependency-based grammars, special attention
should be paid to the lexicalized tree-adjoining grammars (LTAG) continuing the original conception
of tree-adjoining grammar (TAG) as proposed by A. K. Joshi (see, e.g., Joshi 1985), which has served
as a basis for many studies on formal grammar as well as from the NLP domain. The similarity
between LTAG and a dependency-based description in relation to the model using the so-called
supertags (which encode syntactic information in terms of dependency) is analyzed by Joshi and
Srinivas (1994).
4 It should be recalled in this connection that within machine translation dependency-based systems
(sometimes in combination with phrase structure) were already at play in the early times of MT;
see, for example, the works of B. Vauquois, one of the founders of computational linguistics (Vauquois
1975; Vauquois and Chappuy 1985) and the systems developed in Japan under the influence and
guidance (direct or indirect) of M. Nagao (see the survey in Nagao [1989]).
5 In a similar vein, Steedman (2005) argues that the use of statistical language models is the only
way to create a computer program that automatically analyzes sentences on the basis of broadly
conceived grammars (with due regard to ambiguities) such as dependency-based grammars or
grammars specifying heads (governors); according to Steedman, these grammars work well because
they reflect a mixture of semantic information and information based on knowledge of the world.
461
Computational Linguistics Volume 32, Number 4
Among urgent questions to be asked with regard to the approaches to sentence
structure, there are then the following issues:
(i) Is it more appropriate to analyze a sentence such as In this garden, she was
reading a book on the history of Spain yesterday as having the complex verb
form was reading as its head, with she, (a) book, and garden as its dependents,
or to see the basic characteristics of its structure in distinguishing whether
in this garden or yesterday is more immediately connected with its verb?
(ii) Do we have clearer criteria for answering the former or the latter of these
two questions?
3. Prague School Functionalism
The other attribute of Prague structuralism is functional. Mathesius (1928, 1936), in-
spired especially by the philosophy of language of Marty (1908), presented his theory of
functional grammar, based on the concept of function as related to universal intentional
acts and treated as a dichotomy of functional onomatology and functional syntax.
Mathesius combined this universal dichotomy with the language-specific opposition
of function and form. As Sgall (1987) pointed out, the core of the system of language
was conceived of as consisting of levels, the units of which have their functions in that
they represent or express units of the adjacent higher levels, up to the non-linguistic
layer of cognitive content. The units of the system were understood as constituting
hierarchies in which some of them function as certain parts of the others. Thus, for
example, phonemes were defined and delimited one against the other on a functional
basis (two different phonemes can distinguish two morphemes), and the established
repertory of distinctive features gave a firm foundation to the description of the system
of phonemes as a structured whole. Strings of phonemes (morphs, in more modern
terminology) are understood as expressing morphemes, and sequences of morphemes
as expressing sentence structure.
Another important aspect of the functional approach is to view language as a
functioning system, adapted to its communicative role, diversified in more or less
different social and local varieties, and to describe the sentence structure as adapted
to its functioning in discourse.
This leads me to pay attention to the information structure (in our terms, topic?
focus articulation) of the sentence. Let me first look again at the history of the issue. It
was the Prague scholar Mathesius (1929, 1938) who introduced the study of information
structure into structural linguistics, preferring the terms Thema and Rhema (used before
in German linguistics by H. Ammann) to the older psychologisches Subjekt and Pra?dikat
(used by G. von der Gabelentz, H. Paul, and others), and understanding the former
(the topic) as one of the functions of the subject in English. He distinguished topic
proper, comment (focus) proper, and the accompanying elements of either of these
two parts. Later on, one of Mathesius? followers, Jan Firbas, extended the hierarchical
understanding of the information structure of the sentence by postulating a scale of
communicative dynamism. The Praguian concepts met a favorable response within
continental linguistics (one should mention in this connection especially the works by
British linguists M. A. K. Halliday and H. W. Kirkwood, several German linguists such
as J. Esser, R. Bartsch, and J. Jacobs, the French linguist J.-M. Zemb, and others). How-
ever, only the syntactic or word order consequences (or, as the case may be, conditions)
462
Hajic?ova? Old Linguists Never Die
of different sentence articulations into topic and focus were mostly taken into account,
and its relevance for and effects on the coherence of discourse.
A new impetus into the study of information structure was given by Petr Sgall,
who was the first to come up with examples testifying to the semantic effects
of this issue and claiming that two utterance tokens differing in their topic?focus
articulation are tokens of two different sentences, that is, that topic and focus belong
to the language system rather than only to the use of language in communication (Sgall
1967, page 205ff.). As a matter of fact, the split of transformational grammar into the
generative and the interpretative semantics wings coming out at the same time operated
with arguments based on sentences that in Praguian terms differ only in their topic?
focus structure (this fact, of course, not being recognized by the authors): See Lakoff?s
(1969) examples: Many men read few books against Few books are read by many men, John
talked about many problems to few girls versus John talked to few girls about many problems.
To be fair to the other side of the dispute, Chomsky (1965, page 224) noticed the semantic
difference between the sentences Everybody in the room knows at least two languages and
At least two languages are known by everybody in the room and was in doubt as to whether
this difference should be ascribed to the difference between active and passive; he
remarks that such a distinction might be described in terms of topic (as Lakoff [1969]
notes, in this consideration, the influence of Halliday [1967?1968] played its role). In
his reaction to the generative semanticists? criticism of the ?shallowness? of his deep
structure, Chomsky (1968) was even more inclined to use notions related to what in
present-day terms would be called the information structure of the sentence; he claims
that in the semantic interpretation of the sentence, one should take into account the
distinction between what he calls presupposition and focus, and a related notion of
the range of permissible focus. There are two interesting points in his approach: First,
Chomsky connects these syntactic issues with the placement of the intonation center
in the spoken form of the sentence, and second, he connects the possible operational
criterion for the determination of the choice of focus from the range of permissible focus
with the scope of negation. In particular, to decide what is the focus of the answer to Was
it an ex-convict with a red SHIRT that he was warned to look out for?, one should consider
possible different negative continuations such as No, he was warned to look out for an
AUTOMOBILE SALESMAN, or . . . for an ex-convict wearing DUNGARIES, . . . for an ex-
convict with a CARNATION, . . . for an ex-convict with a red TIE.
One could argue that it is the presence of structures with quantification rather than
the topic?focus articulation of the quoted examples that is responsible for the indicated
semantic distinction. However, the Praguian writings from the sixties convincingly
demonstrate that it is not difficult to find sentences without quantification that exhibit
the same phenomenon (for reasons I will mention in a minute, in the examples, the
capitals indicate the intonation center): Russian is spoken in SIBERIA versus In Siberia,
RUSSIAN is spoken, or John works on his dissertation on WEEKENDS versus On weekends,
John works on his DISSERTATION. In Russian linguistics, such examples have been
discussed as Kurit? ZDES? versus Zdes? KURIT?. The sentences quoted also document
that the difference cannot be ascribed to the active/passive distinction; neither can it
be claimed that the word order always plays a decisive role: Consider Halliday?s (1970)
famous example from a London underground station: Dogs must be CARRIED. With the
same word order, but with a change in the placement of the intonation center one gets
a certainly unwanted interpretation: DOGS must be carried would imply that everybody
stepping on the escalator has to carry a dog (in a similar vein as Carry DOGS.). A
plausible explanation of the semantic difference covering all these cases is to describe
them in terms of difference in their information structure.
463
Computational Linguistics Volume 32, Number 4
This had not been recognized or at least commonly accepted for some time on an
international scale until the appearance of Mats Rooth?s Ph.D. dissertation in 1984.6 Al-
though restricted to prosodic focus (pointing out that the difference in truth conditions
between such sentences as Mary only introduced BILL to Sue and Mary only introduced
Bill to SUE is only in the location of focus, denoted here by capitals), Rooth?s work
was an impetus for an increasing interest in the related issues, first in the domain of
formal semantics (here the influential role of Barbara H. Partee should be emphasized),
but soon literally everywhere. Let us mention in this context that semantic consid-
erations apparently stood behind the conception of combinatory categorial grammar
first proposed by Steedman (1996, 2000); his introduction of floating constituents, the
division line between which is given by the articulation of the sentence with regard to its
information structure rather than fixed, determined once for all. Steedman, in contrast to
many other researchers presently working in the domain of information structure, pays
a due respect to the close relation between information structure, syntactic sentence
structure, and prosody; in this respect, also the work on corpus annotation led by him
is a pioneering enterprise (Calhoun et al 2005).7
Due respect paid to the description of the information structure of the sentence is
also crucial for the study of discourse structure and coherence. It might be interesting
to note in this connection that the first systematic study indicating such a relation?
although in terms influenced by the then prevailing psychological view of language?
is Weil?s (1844) study on the order of words. The author introduces the notions of
progression of ?ideas,? distinguishing ?progression? and marche paralle`le: In the former
sequence (segment), the given sentence B is connected to the preceding sentence A by
starting with a reference to the idea that was at the end of A, whereas in the latter, the
sentences ?march in parallel,? that is, they begin with a reference to the same idea. It
is not difficult to see an analogy between this view and a more modern and explicit
treatment of the so-called centering theory (Grosz, Joshi, and Weinstein 1995) and its
shifts of centers.
I am dwelling at such length on the issues of information structure not just because
it is my favorite child (and, indeed, it is), but because I am fully convinced about the
importance of this issue for an adequate description of the sentence structure, both
in formal description of language as well as in natural language processing. Let me
illustrate by a personal recollection that I am not beating a straw man. Some time
ago (if I am not mistaken, it was in 1989) I was invited for an IBM-organized MT
conference in Garmisch-Partenkirchen to deliver a talk on the Praguian approach to
MT. Naturally enough, I devoted most of my time to illustrate examples of translations
from and to several European languages that topic?focus articulation as an important
aspect the translation (be it human or automatic) has to take into account. The group of
6 From a different perspective, the term focus was used by Grosz (1977). The author adopted a
psychological point of view of focus of attention and considered the sentence focus to be that item that is
in such a focus, that is, in our terms the topic of the sentence (what the sentence is about). Grosz? approach
has found its continuation in the centering theory mentioned below.
7 Let us note in this connection that the difficulties of a syntactic description based on phrase structure for
an adequate capturing of the topic?focus articulation were pointed out already in Sgall, Hajic?ova?, and
Benes?ova? (1973, page 163ff.) and in Hajic?ova? and Sgall (1975) and illustrated on examples such as This
year we will spend two weeks on Mallorca used in the context of How will you spend your holidays this year?, i.e.
with the focus part of the sentence being two weeks on Mallorca. Working with phrase structure, it would
be very difficult to characterize the two groups as a single phrase; in a similar vein, to determine the
topic of the sentence as a single phrase is also difficult: if the question were Where do you spend two holiday
weeks this year?, the focus of the answer would be on Mallorca, with the topic being this year we will spend
two weeks.
464
Hajic?ova? Old Linguists Never Die
people attending the meeting was extremely nice and friendly, and it was no wonder
that the program chairs, Margaret King and Jonathan Slocum, could dare to make the
concluding session a sort of fun ascribing to each of the papers some characteristic
evaluative attribute: It was quite symptomatic of those times that the issues discussed
in my paper were characterized as ?least important for MT? (it may be of interest to
recall that Mercer?s paper on the IBM statistical approach to MT delivered there was
evaluated as ?crazy science fiction?).
The question should then be discussed in which way is it possible to describe the
interplay of the dependency (or constituency)-based patterning of the sentence and the
topic?focus articulation (or information strcuture) of the sentence as two basic aspects
of syntax (now cf. Hajic?ova? and Sgall, in press). Is it true that a dependency-based view
of the underlying structure as the core of the language system (in which there are no
nodes corresponding to function words and the left-to-right order of lexical items meets
the condition of projectivity; cf. Section 2 above) might be useful in this respect?
I am happy to see that much has changed in this particular domain of studies since
those times; I cannot say I welcome all the changes, but it is encouraging to see that the
two Praguian tenets I have discussed so far?namely, the dependency approach and the
due regard to the information structure?have found an undisputable appraisal within
our field.
4. The Core of Language and the Periphery
The third Praguian notion I would like to mention is the distinction made between the
center (core) of language and the periphery. This distinction is closely connected with
the notion of markedness; markedness, characterizing the intrinsic asymmetry of binary
(and other) oppositions (not only in phonology, but also in morphology, in semiotics,
and in many other domains), was first systematically presented by R. Jakobson. It
was properly understood and used as an organizing principle of sign systems, also
in connection with language universals and language acquisition. As Battistella (1995)
notes, this notion belongs to those aspects of the Prague linguistic theory that in some
form have been taken over by Chomsky, who applied it, albeit in a different shape, in
his Principles and Parameters theory, as proposed in the early 1980s.
Although the relationships between the two oppositions of marked versus un-
marked phenomena and the core versus the periphery of the system of language are
far from straightforward (see Sgall 2002, 2004), it can be claimed that because language
is more stable in its core, regularities in language should be searched for first in this core;
only then is it possible to penetrate into the subtleties and irregularities of the periphery.
The relatively simple pattern of the core of language (in Sgall?s view, not far from the
transparent pattern of propositional calculus) makes it possible for children to learn
the regularities of their mother tongue on the basis of shared human mental capacities,
instantiated also by systems such as those of elementary arithmetic or Aristotelian logic.
The freedom of linguistic behavior, limited only by the speakers? desire to be understood
by their audience, offers space for the flexibility of the large and complex periphery (i.e.,
not only of individual exceptions, but also by most different sets of marked phenomena
determined by contextual conditions and lists).
The question to be asked then is which of the two possible approaches to how
to project this view to a formal description of language is to be preferred: to attempt
to describe all phenomena ?at once,? that is, to consider language as a whole and to
describe all phenomena ?at a single layer,? or to proceed from the core of the system to
its periphery.
465
Computational Linguistics Volume 32, Number 4
5. Corpus Annotation as a Test of Linguistic Theories
At the beginning of my talk, I promised to suggest which aspects of Praguian heritage
(and in a more general view, of linguistics as such) I believe to have been fruitful
for computational linguistics. When talking about the three particular aspects I have
chosen, I have tried to make some suggestions as to urgent questions to be asked. Let me
finish my talk by an illustration taken from the presently flourishing field of language
resources, corpus annotation, and evaluation.
It has been already commonly accepted in computational and corpus linguistics that
grammatical (or lexical semantic, etc.) annotation does not ?spoil? a corpus, because the
annotation is done ?in addition? to the raw corpus. Thus, on the contrary, annotation
may and should bring an additional value to the corpus. However, there are some
necessary conditions for an annotation to fulfil this aim: Its scenario should be carefully
(i.e., systematically and consistently) designed, and it should be based on a sound
linguistic theory. This view is corroborated by the existence of annotated corpora of
various languages (even if their creation is mostly done manually but supported by
annotator-friendly software tools or semiautomatic procedures): the Penn Treebank, its
successors as PropBank or the Penn Discourse Treebank, Tiger, the Prague Dependency
Treebank, and several others. These conditions being met, corpus annotation serves,
among other things, as an invaluable test for the linguistic theories standing behind
the annotation schemes, and as such represents an irreplaceable resource of linguis-
tic information for the construction and enrichment of grammars, both formal and
theoretical.
This claim can be documented by the case of the multilayered annotation of the
Prague Dependency Treebank (PDT; see, e.g., Hajic? 1998), which is based on the frame-
work of the Functional Generative Description (see, e.g., Sgall, Hajic?ova?, and Panevova?
1986). It is important to note that the PDT annotation concerns not only the surface
and morphemic shape of sentences, but also (and first of all) the underlying sentence
structure (tectogrammatical layer), which elucidates phenomena hidden on the sur-
face although unavoidable for the representation of the meaning and functioning of
the sentence, for modeling its comprehension and studying its semantico-pragmatic
interpretation, for the work on lexical semantics, and for dictionary buildup and many
other aims.
We have tried to demonstrate on certain selected grammatical and discourse phe-
nomena (in Hajic?ova?, Sgall 2006) that the process of the annotation during the last
decade and its results have allowed for an enrichment of this framework in several
regards. In particular, our examples were taken from the domain of the condition of
projectivity, classification of dependency relations, topic?focus articulation (the biparti-
tion of the sentence into topic and focus and the cannonical underlying word order in
the focus of the sentence), and some aspects of discourse structure.
6. Final Remarks
I have always been an optimist, and therefore let me go back to the Indiana Univer-
sity Linguistic Club logo from 1984 quoted in the title of my talk: I strongly believe
that old linguists never die, they only get obligatorily deleted. Deletions concern the
surface rather than the underlying structure so that we may hope that while the old
linguists? bodies may lie a-moldering in their graves, the best of their ideas will be
marching on.
466
Hajic?ova? Old Linguists Never Die
Acknowledgments
I would like to express my most sincere
thanks to the ACL for the Award and thus
for having given me the opportunity to pay
tribute in this talk to my Praguian teachers. I
would also like to express my deep gratitude
to my mentor and colleague, Petr Sgall, the
founder of Czech computational linguistics,
whose original ideas as well as broad scope
of knowledge and vision have made it
possible for the Prague School linguistic
ideas to cross over the boundaries of time, of
geographic zones, and of linguistic trends
and orientation. Last but not least, I would
like to pay credit to my younger colleagues
and students, the energy, commitment, and
friendliness of whom makes me not to think
of age and to enjoy my professional life.
References
Actes du Premier Congre`s international des
linguistes a` la Haye. 1928. A. W. Sijthoff,
Leiden.
Adger, David. 2003. Core Syntax. A Minimalist
Approach. Oxford University Press, Oxford.
Battistella, Edwin. 1995. Jakobson and
Chomsky on markedness. In E. Hajic?ova?,
M. C?ervenka, O. Les?ka, and P. Sgall,
editors, Prague Linguistic Circle Papers 1.
John Benjamins, Amsterdam, pages 55?72.
Bloomfield, Leonard. 1933. Language. Holt,
Rinehart and Winston, New York.
Bresnan, Joan. 1978. A realistic
transformational grammar. In M. Halle
et al, editor, Linguistic Theory and
Psychological Reality. MIT Press,
Cambridge, MA, pages 1?59.
Calhoun, Sasha, Malvina Nissim, Mark
Steedman, and Jason Brenier. 2005. A
framework for annotating information
structure in discourse. In A. Meyers,
editor, Pie in the Sky. Proceedings of the
ACL Workshop 2005. Ann Arbor, MI,
pages 45?52.
Chomsky, Noam. 1965. Aspects of the Theory of
Syntax. MIT, Cambridge, MA.
Chomsky, Noam. 1968. Deep structure,
surface structure and semantic
interpretation. In D. D. Steinberg and
L. A. Jakobovits, editors, Semantics: An
Interdisciplinary Reader in Philosophy,
Linguistics and Psychology. Cambridge
University Press, Cambridge,
pages 183?216.
Fillmore, Charles J. 1966. Toward a modern
theory of case. In D. A. Reibel and S. A.
Schane, editors, Modern Studies in English.
Prentice-Hall, Englewood Cliffs, NJ,
pages 361?375.
Fillmore, Charles J. 1968. The case for case. In
E. Bach and R. Harms, editors, Universals
in Linguistic Theory. Holt, Rinehart, and
Winston, New York, pages 1?90.
Gazdar, Gerald. 1981. Unbounded
dependencies and coordinate structure.
Linguistic Inquiry, 12:155?184.
Grosz, Barbara J. 1977. The Representation and
Use of Focus in Dialogue Understanding.
Ph.D. thesis, University of California,
Berkeley, CA.
Grosz, Barbara J., Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21(2):203?225.
Gruber, Jeffrey S. 1967. Functions of the
lexicon in formal descriptive grammar.
Technical Report (TM)-3770/00, Systems
Development Corporation, Santa Monica.
Hajic?, Jan. 1998. Building a syntactically
annotated corpus: The Prague
Dependency Treebank. In E. Hajic?ova?,
editor, Issues of Valency and Meaning.
Studies in Honour of Jarmila Panevova?.
Karolinum, Prague, pages 106?132.
Hajic?ova?, Eva, Petr Pajas, and Kater?ina
Vesela?. 2002. Corpus annotation on the
tectogrammatical layer: Summarizing of
the first stages of evaluation. The Prague
Bulletin of Mathematical Linguistics,
77:5?18.
Hajic?ova?, Eva and Petr Sgall. 1975. Topic and
focus in transformational grammar. Papers
in Linguistics, 8(1?2):13?58.
Hajic?ova?, Eva and Petr Sgall. 2003.
Dependency syntax in functional
generative description. In Vilmos Agel
et al, editors, Dependenz und Valenz, Vol. 1.
Walter de Gruyter, Berlin, pages 570?592.
Hajic?ova?, Eva and Petr Sgall. 2006. Corpus
annotation as a test of a linguistic theory.
In Proceedings of LREC 2006, Genoa.
Hajic?ova?, Eva and Petr Sgall. Forthcoming.
The fundamental significance of
information structure. In C. Caffi and
H. Haberland et al, editors, Future
Prospects of Pragmatics.
Halliday, Michael A. K. 1967?1968. Notes
on transitivity and theme in English.
Journal of Linguistics, 3:37?81, 199?244;
4:179?215.
Halliday, Michael A. K. 1970. A Course in
Spoken English: Intonation. Oxford
University Press, Oxford.
Hays, David G. 1960. Grouping and
dependency theories. In Proceedings of the
467
Computational Linguistics Volume 32, Number 4
National Symposium on Machine Translation,
pages 258?266, Englewood Cliffs, NJ.
Hays, David G. 1964. Dependency theory:
A formalism and some observations.
Language, 40:511?525.
Hudson, Richard. 1984. Word Grammar.
Blackwell, Oxford.
Jakobson, Roman. 1929. Remarques sur
l?e?volution phonologique du russe
compare?e a? celle des autres langues slaves.
Travaux du Cercle Linguistique de Prague,
volume 2.
Joshi, Aravind. 1985. Tree-adjoining
grammars: How much context-sensitivity
is required to provide reasonable
structural descriptions? In D. Dowty,
editor, Natural Language Processing.
Cambridge University Press, Cambridge,
pages 206?250.
Joshi, Aravind and Bangalore Srinivas.
1994. Disambiguation of super parts of
speech (or supertags): Almost parsing.
In Proceedings of the 15th International
Conference on Computational Linguistics
(COLING 1994), pages 154?160,
Kyoto, Japan.
Kaplan, Ronald and Joan Bresnan. 1982.
Lexical-Functional Grammar: A formal
system for grammatical representation.
In Joan Bresnan, editor, The Mental
Representation of Grammatical
Relations. MIT Press, Cambridge, MA,
pages 173?281.
Katz, Jerrold J. and Paul M. Postal. 1964. An
Integrated Theory of Linguistic Descriptions.
MIT Press, Cambridge, MA.
Kay, Martin. 2005. A life of language.
Computational Linguistics, 31(4):425?438.
Lakoff, George. 1969. On generative
semantics. In D. D. Steinberg and L. A.
Jakobovits, editors, Semantics: An
Interdisciplinary Reader in Philosophy,
Linguistics and Pyschology. Cambridge
University Press, Cambridge,
pages 232?296.
Lecerf, Yves. 1960. Programme des conflits,
mode`le des conflits. Traduction
Automatique, 1(4):11?18; 1(5):17?36.
Marcus, Solomon. 1965. Sur la notion de
projectivite?. Zeitschrift fu?r mathematische
Logik und Grundlagen der Mathematik,
11:181?192.
Marty, Anton. 1908. Untersuchungen zur
Grundlegung der allgemeinen Grammatik und
Sprachphilosophie 1. Halle/S.
Mathesius, Vile?m. 1928. On linguistic
characterology. Actes, pages 56?63.
Mathesius, Vile?m. 1929. Zur satzperspektive
im modernen Englisch. Archiv fu?r das
Studium der neueren Sprachen und
Literaturen, 155:202?210.
Mathesius, Vile?m. 1936. On some problems
of the systematic analysis of grammar.
Travaux du Cercle Linguistique de Prague,
volume 6, pages 95?107.
Nagao, Makoto. 1989. Machine Translation:
How Far Can It Go? Oxford University
Press, Oxford.
Pollard, Carl and Ivan A. Sag. 1994.
Head-driven Phrase Structure Grammar.
University of Chicago Press, Chicago and
London.
Robinson, Jane J. 1969. Case, category and
configuration. Journal of Linguistics,
6:57?80.
Robinson, Jane J. 1970. Dependency
structures and transformational rules.
Language, 46:259?285.
Sgall, Petr. 1967. Functional sentence
perspective in a generative description.
Prague Studies in Mathematical Linguistics,
2:203?225.
Sgall, Petr. 1987. Prague functionalism
and topic vs. focus. In Rene? Dirven and
Vile?m Fried, editors, Functionalism in
Linguistics. John Benjamins Publishing
Company, Amsterdam/Philadelphia,
pages 169?189.
Sgall, Petr. 1997. On the usefulness of
movement rules. In B. Caron, editor, Actes
du 16e Congre`s International des Linguistes
(Paris 20-25 juillet 1997). Elsevier Science,
Oxford.
Sgall, Petr. 2002. Freedom of language: Its
nature, its sources and its consequences.
Prague Linguistic Circle Papers, 4:309?329.
Sgall, Petr. 2004. Types of languages and the
simple pattern of the core of language. In
P. Sterkenburg, editor, Linguistics
Today?Facing a Greater Challenge (Plenary
lectures from CIL 17). John Benjamins,
Amsterdam/Philadelphia, pages 243?265.
Sgall, Petr, Eva Hajic?ova?, and Eva Benes?ova?.
1973. Topic, Focus and Generative Semantics.
Scriptor, Kronberg/Taunus.
Sgall, Petr, Eva Hajic?ova?, and Jarmila
Panevova?. 1986. The Meaning of the Sentence
in Its Semantic and Pragmatic Aspects.
Reidel, Dordrecht; Academia, Prague.
S?milauer, Vladim??r. 1947. Novoc?eska? skladba
[The syntax of Modern Czech]. Mikuta,
Prague.
Steedman, Mark. 1996. Surface Structure and
Interpetation. The MIT Press, Cambridge,
MA.
Steedman, Mark. 2000. Information structure
and the syntax?phonology interface.
Linguistic Inquiry, 31:649?689.
468
Hajic?ova? Old Linguists Never Die
Steedman, Mark. 2005. Grammar acquisition
by child and machine. Invited Talk at the
17th European Summer School of Language,
Logic and Information, Edinburgh.
Tesnie`re, Lucien. 1934. Comment construire
une syntaxe. Bulletin de la Faculte? des lettres
de Strasbourg, 12(7):219?229.
Tesnie`re, Lucien. 1959. Ele?ments de Syntaxe
Structurale. Klinksieck, Paris.
Vauquois, Bernard. 1975. La traduction
automatique a` grenoble. Documents de
linguistique quantitative, 24.
Vauquois, Bernard and Sylviane Chappuy.
1985. Static grammars: A formalism for the
description of linguistic models. In
Proceedings of the Conference on Theoretical
and Methodological Issues in Machine
Translation, pages 298?322, Colgate
University, Hamilton, New York.
Weil, Henri. 1844. De l?ordre des mots dans les
langues anciennes compare?es aux langues
modernes (The Order of Words in the Ancient
Languages Compared with That of Modern
Languages), Paris; Amsterdam [1978].
469

Topic-focus and salience*
Eva Haji?ov?
Faculty of Mathematics and Physics
Charles University
Malostransk? n?m. 25
118 00 Praha, Czech Republic
hajicova@ufal.mff.cuni.cz
Petr Sgall
Faculty of Mathematics and Physics
Charles University
Malostransk? n?m. 25
118 00 Praha, Czech Republic
sgall@ufal.mff.cuni.cz
                                                          
*Acknowledgement: The work reported on in this paper has been carried out under the projects GACR 405/96/K214 and
MSMT LN00A063.
1 Objectives and Motivation
Most of the current work on corpus annotation is
concentrated on morphemics, lexical semantics
and sentence structure. However, it becomes
more and more obvious that attention should and
can be also paid to phenomena that reflect the
links between a sentence and its context, i.e. the
discourse anchoring of utterances. If conceived
in this way, an annotated corpus can be used as a
resource for linguistic research not only within
the limits of the sentence, but also with regard to
discourse patterns. Thus, the applications of the
research to issues of information retrieval and
extraction may be made more effective; also
applications in new domains become feasible, be
it to serve for inner linguistic (and literary) aims,
such as text segmentation, specification of topics
of parts of a discourse, or for other disciplines.
These considerations have been a motivation
for the tectogrammatical (i.e. underlying, see
below) tagging done within the Prague
Dependency Treebank (PDT) to contain also
attributes concerning certain contextual features,
i.e. the contextual anchoring of word tokens and
their relationships to their coreferential
antecedents.
Along with this enrichment in the
intersentential aspect, we do not neglect to pay
attention to intrasentential issues, i.e. to sentence
structure, which displays its own features
oriented towards the contextual potential of the
sentence, namely its topic-focus articulation
(TFA).
In the present paper, we give first an outline
of the annotation scenario of the PDT (Section
2), concentrating then on the use of one of the
PDT attributes for the specification of the Topic
and the Focus (the 'information structure') of the
sentence (Section 3). In Section 4. we present
certain heuristics that partly are based on TFA
and that allow for the specification of the
degrees of salience in a discourse. The
application of these heuristics is illustrated in
Section 5.
2 Outline of the Prague Dependency
Treebank
The Prague Dependency Treebank (PDT) is
being built on the basis of the Czech National
Corpus (CNC), which grows rapidly in the range
of hundreds of millions of word occurrences in
journalistic and fiction texts. The PDT scenario
comprises three layers of annotation:
(i) the morphemic (POS) layer with about
2000 tags for the highly inflectional Czech
language; the whole CNC has been tagged by a
stochastic tagger (Haji? and Hladk? 1997;1998,
B?hmov? and Haji?ov? 1999, Hladk? 2000)
with a success rate of 95%; the tagger is based
on a fully automatic morphemic analysis of
Czech (Haji? in press);
(ii) a layer of 'analytic' ("surface") syntax
(see Haji? 1998): cca 100 000 Czech sentences,
i.e. samples of texts (each randomly chosen
sample consisting of 50 sentences of a coherent
text), taken from CNC, have been assigned
dependency tree structures; every word (as well
as every punctuation mark) has a node of its
own, the label of which specifies its analytic
function, i.e. Subj, Pred, Obj, Adv, different
kinds of function words, etc. (total of 40 values);
no nodes are added that are not in the surface
shape of the sentence (except for the root of the
tree, carrying the identification number of the
sentence); the sentences from CNC are
preprocessed by a dependency-based
modification of Collins et al's (1999) automatic
parser (with a success rate of about 80%),
followed by a manual tagging procedure that is
supported by a special user-friendly software
tool that enables the annotators to work with
(i.e. modify) the automatically derived graphic
representations of the trees;
(iii) the tectogrammatical (underlying)
syntactic layer: tectogrammatical tree structures
(TGTSs) are being assigned to a subset of the set
tagged according to (ii); by now, the
experimental phase has resulted in 20 samples of
50 sentences each; the TGTSs, based on
dependency syntax, are much simpler than
structural trees based on constituency
(minimalist or other), displaying a much lower
number of nodes and a more perspicuous
patterning; their basic characteristics are as
follows (a more detailed characterization of
tectogrammatics and motivating discussion,
which cannot be reproduced here, can be found
in Sgall et al 1986; Haji?ov? et al 1998):
(a) only autosemantic (lexical) words have
nodes of their own; function words, as far as
semantically relevant, are reflected by parts of
complex node labels (with the exception of
coordinating conjunctions);
(b) nodes are added in case of deletions on
the surface level;
(c) the condition of projectivity is met (i.e. no
crossing of edges is allowed);
(d) tectogrammatical functions ('functors')
such as Actor/Bearer, Patient, Addressee,
Origin, Effect, different kinds of Circumstantials
are assigned;
(e) basic features of TFA are introduced;
(f) elementary coreference links (both
grammatical and textual) are indicated.
Thus, a TGTS node label consists of the
lexical value of the word, of its '(morphological)
grammatemes' (i.e. the values of morphological
categories), its 'functors' (with a more subtle
differentiation of syntactic relations by means of
'syntactic grammatemes' (e.g. 'in', 'at', 'on',
'under'), of the attribute of Contextual
Boundness (see below), and of values
concerning intersentential links (see below).
3 From Contextual Boundness to the
Topic and the Focus of the Sentence
The dependency based TGTSs in PDT allow for
a highly perspicuous notation of sentence
structure, including an economical
representation of TFA, understood as one of the
main aspects of (underlying) sentence structure
along with all other kinds of semantically
relevant information expressed by grammatical
means. TFA is accounted for by one of the
following three values of a specific TFA
attribute assigned to every lexical
(autosemantic) occurrence: t for 'contextually
bound' (prototypically in Topic), c for
'contrastive (part of) Topic', or f (?non-bound?,
typically in Focus). The opposition of contextual
boundness is understood as the linguistically
structured counterpart of the distinction between
"given" and "new" information, rather than in a
straightforward etymological way (see Sgall,
Haji?ov? and Panevov? 1986, Ch. 3). Our
approach to TFA, which uses such operational
criteria of empirical adequateness as the
question test (with the item corresponding to a
question word prototypically constituting the
focus of the answer), represents an elaboration
of older ideas, discussed especially in Czech
linguistics since V. Mathesius and J. Firbas, in
the sense of an explicit treatment meeting the
methodological requirements of formal syntax.
The following rules determine the
appurtenance of a lexical occurrence to the
Topic (T) or to the Focus (F) of the sentence:
(a) the main verb (V) and any of its direct
dependents belong to F iff they carry index f;
(b) every item i that does not depend directly
on V and is subordinated to an element of F
different from V, belongs to F (where
"subordinated to" is defined as the irreflexive
transitive closure of "depend on");
(c) iff V and all items kj directly depending on it
carry index t, then those items kj to which some
items lm carrying f are subordinated are called
'proxy foci' and the items lm together with all
items subordinated to one of them belong to F,
where 1 ? j,m;
(d) every item not belonging to F according
to (a) - (c) belongs to T.
To illustrate how this approach makes it
possible to analyze also complex sentences as
for their TFA patterns, with neither T nor F
corresponding to a single constitutent, let us
present the following example, in which (1') is a
highly simplified linearized TGTS of (1); every
dependent item is enclosed in a pair of
parentheses; for the sake of transparency,
syntactic subscripts of the parentheses are left
out here, as well as subscripts indicating
morphological values, with the exception of the
two which correspond to function words, i.e.
Temp and Necess(ity); Fig. 1. presents the
respective tree structure, in which three parts of
each node label are specified, namely the lexical
value, the syntactic function (with ACT for
Actor/Bearer, RSTR for Restrictive,  MANN for
Manner, and OBJ for Objective), and the TFA
value:
(1) ?esk? radiokomunikace mus? v tomto
roce rychle splatit dluh televizn?m div?k?m.
This year, Czech Radiocommunications have
quickly to pay their debt to the TV viewers.
(1') ((?esk?.f) radiokomunikace.t)    ((tomto.t)
         Czech    Radiocommunications      this
roce.Temp.t) splatit.Necess.f  (rychle.f)
in-year          must-pay             quickly
(dluh.f ((televizn?m.f) div?k?m.f))
 debt TV                  viewers
Figure 1.
4 Degrees of Salience in a Discourse
During the development of a discourse, in the
prototypical case, a new discourse referent
emerges as corresponding to a lexical
occurrence that carries the index f; its further
occurrences in the discourse carry t and are
primarily guided by the scale of their degrees of
salience. This scale, which was discussed by
Haji?ov? and Vrbov? (1982), has to be reflected
in a description of the semantico-pragmatic
layer of the discourse. In this sense our approach
can be viewed as pointing to a useful enrichment
of the existing theories of discourse
representation (cf. also Kruijffov? 1998,
Krahmer 1998; Krahmer and Theune 1999).
In the annotation system of PDT, not only
values of attributes concerning sentence
structure are assigned, but also values of
attributes for coreferential links in the discourse,
which capture certain features typical for the
linking of sentences to each other and to the
context of situation and allow for a tentative
characterization of the discourse pattern in what
concerns the development of salience degrees
during the discourse.
The following attributes of this kind are
applied within a selected part of PDT, called
'model collection' (for the time being, essentially
only pronouns such as 'on' (he), including its
zero form, or 'ten' (this) are handled in this way):
COREF: the lexical value of the antecedent,
CORNUM: the serial number of the antecedent,
CORSNT: if the antecedent in the same
sentence: NIL, if not: PREVi for the i-th
preceding sentence.
An additional attribute, ANTEC, with its
value equal to the functor of the antecedent, is
used with the so-called grammatical coreference
(relative clauses, pronouns such as 'se' (-self),
the relation of control).
On the basis of these attributes (and of further
judgments, concerning especially associative
links between word occurrences), it is possible
to study the referential identity of different word
tokens in the flow of the discourse, and thus also
the development of salience degrees.
The following basic rules determining the
degrees of salience (in a preliminary
formulation) have been designed, with x(r)
indicating that the referent r has the salience
degree x, and 1 ? m,n:
(i) if r is expressed by a weak pronoun (or
zero) in a sentence, it retains its salience degree
after this sentence is uttered: n(r) --> n(r);
(ii) if r is expressed by a noun (group)
carrying f, then n(r) --> 0(r);
(iii) if r is expressed by a noun (group)
carrying t or c, then n(r) --> 1(r);
(iv) if n(r) --> m(r) in sentence S, then
m+2(q) obtains for every referent q that is not
itself referred to in S, but is immediately
associated with the item r present here1;
(v) if r neither is included in S, nor refers to
an associated object, then n(r) --> n+2(r).
These rules, which have been checked with
several pieces of English and Czech texts,
capture such points as e.g. the fact that in the
third utterance of Jim met Martin. He
immediately started to speak of the old school in
Sussex. Jim invited him for lunch the weak
pronoun in object can only refer to Martin,
whose image has become the most salient
referent by being mentioned in the second
utterance; on the other hand, the use of such a
pronoun also in the subject (in He invited him
for lunch) would make the reference unclear.
Since the only fixed point is that of maximal
salience, our rules technically determine the
degree of salience reduction (indicating 0 as the
maximal salience). Whenever an entity has a
salience distinctly higher than all competing
entities which can be referred to by the given
expression, this expression may be used as
giving the addressee a sufficiently clear
indication of the reference specification.2
5 Illustrations
The development of salience degrees during a
discourse, as far as determined by these rules,
may be illustrated on the basis of five sentence
tokens (utterances) from PDT, starting from (1),
which constitute a segment of a newspaper text
(we indicate the numerical values of salience
reduction for every noun token that is a referring
expression). We present here - similarly as with
(1') in Section 3 above - highly simplified
representations of these sentences, with
parentheses for every dependent member and
the symbols t, c, and f for contextual boundness;
                                                          
1 Only immediate associative links are taken into account
for the time being, such as those between (Czech) crown
and money,  or between TV or (its) signal and (its) viewer.
2 These tentative rules, which have been presented at
several occasions (starting with Haji?ov?  and Vrbov?
1982) for the aims of a further discussion, still wait for a
systematic testing and evaluation, as well as for
enrichments and more precise formulations. These issues
may find new opportunities now, when e.g. a comparison
with the centering theory gets possible and when a large set
of annotated examples from continuous texts in PDT is
available. An automatic derivation of such features can
only be looked for after the lexical units included get a very
complex and subtle semantic classification.
numbers of the degrees of salience (more
precisely, of salince reduction) for every
referring expression are inserted in the sentences
themselves. This example should enable the
reader to check (at least in certain aspects) the
general function of the procedure we use, as
well as the degree of its empirical adequacy in
the points it covers, and also our consistence in
assigning the indices. We are aware of the
preliminary character of our analysis, which
may and should be enriched in several respects
(not to cover only noun groups, to account for
possible episodic text segments, for oral speech
with the sentence prosody, for cases of
deictically, rather than anaphorically
conditioned salience, etc.).
We do not reflect several peripheral points,
such as the differences between surface word
order and the scale of CD (underlying WO),
mainly caused by the fact that a dependent often
precedes its head word on the surface (in
morphemics), although if the dependent has f
(as e.g. rychle (quickly) has in (1)), then it
follows its head under CD (with the exceptions
of focus sensitive particles, cf. Haji?ov?, Partee
and Sgall 1998); our translations are literal.
(1) ?esk? radiokomunikace.1 mus? v tomto
roce.1 rychle splatit dluh.0 televizn?m
div?k?m.0
In this year, Czech Radiocommunications
have quickly to pay their debt to the TV viewers.
(1') ((?esk?.f) radiokomunikace.t)   ((tomto.t)
        Czech    Radiocommunications      this
roce.Temp.t) splatit.Necess.f  (rychle.f)
in-year          must-pay             quickly
(dluh.f ((televizn?m.f) div?k?m.f))
 debt TV                viewers
(2) Jejich.1 vys?la?e.1 dosud pokr?vaj?
sign?lem.0 programu.0 ?T.1 2.0 m?n? ne?-
polovinu.0 ?zem?.0 republiky.0.
Their transmitters hitherto cover by-signal
of-the-program ?T2 less than a-half of-the-
territory of-the-Republic.
(2') ((jejich.t) vys?la?e.t) (dosud.t) pokr?vaj?.f
(sign?lem.f (programu.f (?T.t (2.f)))) ((m?n?.f
(ne?-polovinu.f)) ?zem?.f (republiky.t))
(3) Na moravsko-slovensk?m pomez?.1 je
?ada m?st.0, kde nezachyt? ani prvn? program.0
?esk? televize.1.
On the-Moravian-Slovakian borderline
there-is a-number of-places where (they) do-not-
get even the-first program of-Czech Television.
(3') ((na-moravsko-slovensk?m.t) pomez?.t)
je.f (?ada.f (m?st.f ((kde.t) (oni.t) (ne.f) zachyt?.f
((ani.f) (prvn?.f) program.t ((?esk?.t)
televize.t)))))
(4) Do rozd?len?.1 federace.1 toti? sign?l.1
zaji??ovaly vys?la?e.0 v SR.0.
Until the-division of-the-federation as-a-
matter-of-fact the-signal.Accusative provided
transmitters.Nominative in S(lovac)R(epublic).
(4') (do-rozd?len?.t (federace.t)) (toti?.t)
(sign?l.t) zaji??ovaly.t (vys?la?e.f (v-SR.f)).
(5) ?esk? televize ??d? urychlenou v?stavbu
nov?ch vys?la??.
Czech Television requires quick construction
of-new transmitters.
(5') ((?esk?.t) televize.t) ??d?.f
((urychlenou.f) v?stavbu.f ((nov?ch.f)
vys?la??.t))
The development of salience reduction of the
referents most frequently mentioned in (1) - (5)
is characterized in Tab. 1, which includes
numbers of salience reduction degrees and of
those rules from Section 3 that are the main
sources of the degrees. Two further remarks
may be added, concerning details of our analysis
that have not been discussed above and may not
be directly found in the previous publications we
refer to: (a) a noun group consisting of a head
with t or c and of one or more adjuncts with f
constitutes a referring expression as a whole, in
the prototypical case, and gets degree 0, if it
occurs in F; this concerns e.g. the group vys?la?e
v SR  (?transmitters in  the Slovac Republic?) in
sentence (4), or ?T 2 (CTV 2) in (2); here 2 is
treated as an adjunct of CT; (b) the difference
between the degrees 0 and 1 is not sufficient for
a safe choice of reference, so that, e.g., the
reference of the pronoun jejich (their) after (1)
by itself is indistinct, and only inferencing helps
to establish that ?esk? radiokomunikace (Czech
Radiocommunications) are referred to (viewers
normally do not have transmitters at their
diposal).
after (1) (2) (3) (4) (5)
CRC 1 1 3 5 7
(iii) (iii) (iv) (v) (v)
CTV 3 1 1 2 1
(iv) (iii) (iii) (iv) (iii)
CTV1 2 2 0 2 3
(iv) (iv) (ii) (iv) (iv)
CTV2 2 0 2 2 3
(iv) (ii) (iv) (iv) (iv)
viewer 0 2 2 3 3
(ii) (iv) (i) (iv) (iv)
sig. 3 0 2 1 3
(iv) (ii) (iv) (iii) (iv)
CR 3 1 3 3 3
(iv) (iii) (iv) (iv) (iv)
CSF - - 3 1 3
(iv) (iii) (v)
terr. 3 0 2 2 4
(iv) (ii) (iv) (iv) (v)
tr. - 1 2 0 0
(iii) (iv) (ii) (ii)
Table 1.
Abbreviations:
CRC - Czech Radio(tele)communications
CTV - Czech TV
CR - Czech Republic
CSF - (CS) Federation
CTV1(2) - 1st (2nd) program of CTV
tr. - transmitter
terr. - territory of CR
sig. - signal of CTV
Even with this short piece of discourse, its
segmentation is reflected, if its first subsegment,
discussed up to now (sentences (1) - (5)), is
compared with its continuation, i.e. sentences
(6) - (9), given below. While the first segment
deals primarily with CTV and its signal (cf. the
relatively high salience of CTV, CTV1, CTV2,
RC, signal and viewer  in most parts of the
segment), sentences (6) ? (9) are devoted to
financial issues, as can be seen from the
following facts: (a) money gets degree 0 after
(6), in which it functions as its focus proper (the
most dynamic item), (b) Czech crown gets
degree 1 after (7), in which it is an embedded
part of the focus, and (c) the group financial
coverage gets degree 1 in sentence (8).
The continuation is presented here without
the TGTSs:
(6) Na?e spole?nost m??e ?kol splnit, ale
chyb?j? n?m pen?ze.
Our company can the-task.Accusative fulfil, but
is-lacking us.Dative the-money.Nominative.
(7) Letos by v?stavba technick?ho za??zen? v
sedmi lokalit?ch st?la 120 mili?n? korun, ale
m??eme uvolnit jen 80 mili?n?.
This-year, would the-construction of-technical
equipment in seven localities cost 120 million
crowns, but we-can spend only 80 million.
 (8) Proto o finan?n?m zabezpe?en? jedn?me
s ?eskou televiz?, uv?d? ekonomick? ?editel
?esk?ch radiotelekomunikac? Miroslav Cu??n.
Therefore about (its) financial coverage we-
discuss with Czech Television, states the-
economic director of-Czech
Radiotelcommunications M. C.
(9) Dal??ch 62 mili?n? korun si vy??d?
v?stavba vys?la?? a p?evad??? sign?lu v
pohrani??.
Further 62 million crowns.Accusative Refl. will-
require the-construction.Nominative of-
transmitters and transferrers of-the-signal in the-
border-area.
6 Conclusions
We are aware that, along with the rules
characterized above, there are other factors that
have to be investigated, which are important for
different kinds of discourses. This concerns
various aspects of the discourse situation, of
domain knowledge, of specific textual patterns
(with episodes, poetic effects, and so on).
Factors of these and further kinds can be studied
on the basis of the salience degrees, which are
typical for basic discourse situations.
In any case, we may conclude that it is useful
for a theory of discourse semantics to reflect the
degrees of salience. This makes it possible to
distinguish the reference potential of referring
expressions and thus the connectedness of the
discourse. Discourse analysis of this kind may
also be useful for application domains such as
text segmentation (in accordance with topics of
individual segments), or data mining (specifying
texts in which a given topic is actually treated,
rather than being just occasionally mentioned).
References
B?hmov? A. and E. Haji?ov? (1999). The Prague
Dependency Tree Bank I: How much of the
underlying syntactic structure can be tagged
automatically? The Prague Bulletin of
Mathematical Linguistics 71, 5-12.
Collins M., Haji? J., Brill E., Ramshaw L. and C.
Tillmann (1999). A statistical parser for Czech. In:
Proceedings of 37th Annual Meeting of ACL,
Cambridge, Mass.: M.I.T. Press, 505-512.
Haji? J. (1998). Building a syntactically annotated
corpus: The Prague Dependency Treebank. In:
Issues of Valency and Meaning. Studies in Honour
of Jarmila Panevov?, ed. by E. Haji?ov?, 106-132.
Prague: Karolinum.
Haji? J. (in press). Disambiguation of rich inflection
(Computational morphology of Czech).
Prague:Karolinum.
Haji? J. and Hladk? B. (1997). Probabilistic and rule-
based tagger of an inflective language - a
comparison. In Proceedings of the Fifth
Conference on Applied Natural Language
Processing, Washington, D.C., 111-118.
Haji? J. and Hladk? B. (1998). Czech language
processing - POS tagging. In: Proceedings of the
First International Conference on Language
Resources & Evaluation, Granada.
Haji?ov? E., Partee B. and P. Sgall (1998): Topic-
focus articulation, tripartite structures, and
semantic content. Amsterdam:Kluwer
Haji?ov? E. and J. Vrbov? (1982). On the role of the
hierarchy of activation in the process of natural
language understanding. In: COLING 82. Ed. by J.
Horeck?. Amsterdam: North Holland, 107-113.
Krahmer E. (1998), Presupposition and anaphora.
CSLI Lecture Notes 89. CSLI, Stanford, CA.
Krahmer E. and M. Theune (1999), Efficient
generation of descriptions in context. In: R. Kibble
and K. van Deemter (eds.), Proceedings of the
workshop The Generation of Nominal Expression,
associated with the 11th European Summer School
in Logic, Language and Information.
Kruijff-Korbayov? I. (1998): The dynamic potential
of topic and focus: A Praguian approach to
Discourse Representation Theory. Prague: Charles
University, Faculty of Mathematics and Physics,
Ph.D. dissertation.
Sgall P., Haji?ov? E. and J. Panevov? (1986): The
Meaning of the Sentence in Its Semantic and
Pragmatic Aspects, ed. by J. L. Mey,
Dordrecht:Reidel - Prague: Academia.
Deep Syntactic Annotation: Tectogrammatical Representation and Beyond
Petr Sgall
Center for
Computational Linguistics
sgall@ufal.mff.cuni.cz
Jarmila Panevova?
Institute of Formal
and Applied Linguistics
panevova@ufal.mff.cuni.cz
Eva Hajic?ova?
Center for
Computational Linguistics
hajicova@ufal.mff.cuni.cz
Abstract
The requirements of the depth and precision
of annotation vary for different intended uses
of the corpus but it has been commonly ac-
cepted nowadays that the standard annotations
of surface structure are only the first steps in
a more ambitious research program, aiming at
a creation of advanced resources for most dif-
ferent systems of natural language processing
and for testing and further enrichment of lin-
guistic and computational theories. Among
the several possible directions in which we be-
lieve the standard annotation systems should
go (and in some cases already attempt to go)
beyond the POS tagging or shallow syntactic
annotations, the following four are character-
ized in the present contribution: (i) predicate-
argument representation of the underlying syn-
tactic relations as basically corresponding to a
rooted tree that can be univocally linearized,
(ii) the inclusion of the information structure
using very simple means (the left-to-right or-
der of the nodes and three attribute values),
(iii) relating this underlying structure (render-
ing the ?linguistic meaning,? i.e. the semanti-
cally relevant counterparts of the grammatical
means of expression) to certain central aspects
of referential semantics (reference assignment
and coreferential relations), and (iv) handling
of word sense disambiguation. The first three
issues are documented in the present paper on
the basis of our experience with the devel-
opment of the structure and scenario of the
Prague Dependency Treebank which provides
for syntactico-semantic annotation of large text
segments from the Czech National Corpus and
which is based on a solid theoretical frame-
work.
1 Introduction1
It has been commonly accepted within the computational
linguistics community involved in corpus annotation that
part-of-speech tagging and shallow syntactic annotation
though very progressive, important and useful tasks at
their time, are only the first steps in a more ambitious
research program, aiming at a creation of advanced re-
sources for most different systems of natural language
processing and for testing and further enrichment of lin-
guistic and computational theories. On the basis of our
experience with the development and implementation of
the annotation scheme of the Prague Dependency Tree-
bank (PDT, (Hajic? et al, 2001a), (Bo?hmova?, 2004)), we
would like to indicate four directions, in which we be-
lieve the standard annotation systems should be (and in
some cases already attempt to be) extended to fulfill the
present expectations. We believe that we can offer useful
insights in three of these, namely
(i) an adequate and perspicuous way to represent the
underlying syntactic relations as basically corre-
sponding to a rooted tree that can be equivocally lin-
earized,
(ii) with the inclusion of the information structure us-
ing very simple means (the left-to-right order of the
nodes and two indexes), and
(iii) in relating this underlying structure (rendering the
?linguistic meaning,? i.e. the semantically relevant
counterparts of the grammatical means of expres-
sion) to certain central issues of referential seman-
tics (reference assignment and coreferential rela-
tions).
1The research reported on in this paper has been supported
by the project of the Czech Ministry of Education LN00A063,
and by the grants of the Grant Agency of the Czech Republic
No. 405/03/0913 and No. 405/03/0377.
These insights have been elaborated into annotation
guidelines which now are being used (and checked) on
the basis of PDT, i.e. of syntactico-semantic annotations
of large text segments from the Czech National Corpus,
which allows for a reliable confirmation of the adequacy
of the chosen theoretical framework and for its enrich-
ment in individual details. The fourth dimension we have
in mind is that of handling word-sense disambiguation,
for which the material of our annotated texts, the PDT,
serves as a starting point.
2 The Extensions
2.1 The Structure: Deep Dependency and Valency
The development of formal theories of grammar has doc-
umented that when going beyond the shallow grammat-
ical structure toward some kind of functional or seman-
tic structure, two notions become of fundamental impor-
tance: the notion of the head of the structure and the no-
tion of valency (i.e. of the requirements that the heads
impose on the structure they are heads of). To adduce
just some reflections of this tendency from American
linguistic scene, Fillmore?s ?case grammar? (with verb
frames) and his FrameNet ((Fillmore et al, 2003)), Bres-
nan?s and Kaplans?s lexical functional grammar (with
the distinction between the constituent and the functional
structure and with an interesting classification of func-
tions) and Starosta?s ?lexicase grammar? can serve as
the earliest examples. To put it in terms of formal syn-
tactic frameworks, the phrase structure models take on
at least some traits of the dependency models of lan-
guage; Robinson has shown that even though Fillmore
leaves the issue of formal representation open, the phrase-
structure based sentence structure he proposes can be eas-
ily and adequately transposed in terms of dependency
grammar. Dependency account of sentence structure is
deeply rooted in European linguistic tradition and it is no
wonder then that formal descriptions originating in Eu-
rope are dependency-based (see Sgall, Kunze, Hellwig,
Hudson, Mel?chuk). We understand it as crucial to use
sentence representations ?deep? enough to be adequate as
an input to a procedure of semantic(-pragmatic) interpre-
tation (i.e. representing function words and endings by
indexes of node labels, restoring items which are deleted
in the morphemic or phonemic forms of sentences and
distinguishing tens of kinds of syntactic relations), rather
than to be satisfied with some kind of ?surface? syntax.
The above-mentioned development of formal frame-
works toward an inclusion of valency in some way or an-
other has found its reflection in the annotation scenarios
that aimed at going beyond the shallow structure of sen-
tences. An important support for annotation conceived in
this way can be found in schemes that are based on an
investigation of the subcategorization of lexical units that
function as heads of complex structures, see. Fillmore?s
FRAMENET, the PropBank as a further stage of the de-
velopment of the Penn Treebank (Palmer et al, 2001)
and Levin?s verb classes (Levin, 1993) on which the LCS
Database (Dorr, 2001) is based. There are other systems
working with some kind of ?deep syntactic? annotation,
e.g. the broadly conceived Italian project carried out in
Pisa (N. Calzolari, A. Zampolli) or the Taiwanese project
MARVS; another related framework is presented by the
German project NEGRA, basically surface oriented, with
which the newly produced subcorpus TIGER contains
more information on lexical semantics. Most work that
has already been carried out concerns subcategorization
frames (valency) of verbs but this restriction is not nec-
essary: not only verbs but also nouns or adjectives and
adverbs may have their ?frames? or ?grids?.
One of the first complex projects aimed at a deep (un-
derlying) syntactic annotation of a large corpus is the al-
ready mentioned Prague Dependency Treebank (Hajic?,
1998); it is designed as a complex annotation of Czech
texts (taken from the Czech National Corpus); the under-
lying syntactic dependency relations (called functors) are
captured in the tectogrammatical tree structures (TGTS);
see (Hajic?ova?, 2000). The set of functors comprises
53 valency types subclassified into (inner) participants
(arguments) and (free) modifications (adjuncts). Some
of the free modifications are further subcategorized into
more subtle classes (constituting mainly the underlying
counterparts, or meanings, of prepositions).
Each verb entry in the lexicon is assigned a valency
frame specifying which type of participant or modifi-
cation can be associated with the given verb; the va-
lency frame also specifies which participant/modification
is obligatory and which is optional with the given verb
entry (in the underlying representations of sentences),
which of them is deletable on the surface, which may or
must function as a controller, and so on. Also nouns and
adjectives have their valency frames.
The shape of TGTSs as well as the repertory and clas-
sification of the types of modifications of the verbs is
based on the theoretical framework of the Functional
Generative Description, developed by the Prague re-
search team of theoretical and computational linguis-
tics as an alternative to Chomskyan transformational
grammar (Sgall et al, 1986). The first two arguments,
though labeled by ?semantically descriptive? tags ACT
and PAT (Actor and Patient, respectively) correspond
to the first and the second argument of a verb (cf.
Tesnie`re?s (Tesnie`re, 1959) first and second actant), the
other three arguments of the verb being then differen-
tiated (in accordance with semantic considerations) as
ADDR(essee), ORIG(in) or EFF(ect); these five func-
tors belong to the set of participants (arguments) and
are distinguished from (free) modifications (adjuncts)
such as LOC(ative), several types of directional and tem-
poral (e.g. TWHEN) modifications, APP(urtenance),
R(e)STR(ictive attribute), DIFF(erence), PREC(eding
cotext referred to), etc. on the basis of two basic oper-
ational criteria (Panevova?, 1974), (Panevova?, 1994):
(i) can the given type of modification modify in princi-
ple every verb?
(ii) can the given type of modification occur in the
clause more than once?
If the answers to (i) and (ii) are yes, then the modifica-
tion is an adjunct, if not, then we face an argument.
We assume that the cognitive roles can be determined
on the basis of combinations of the functors with the lex-
ical meanings of individual verbs (or other words), e.g.
the Actor of buy is the buyer, that of sell is the seller, the
Addressee and the Patient of tell are the experiencer and
the object of the message, respectively.The valency dic-
tionary created for and used during the annotation of the
Prague Dependency Treebank, called PDT-VALLEX, is
described in (Hajic? et al, 2003). The relation between
function and (morphological) form as used in the valency
lexicon is described in (Hajic? and Ures?ova?, 2003).
An illustration of this framework is presented in Fig. 1.
#48
SENT
&Gen;
ADDR
jen?e
PREC
tuzemsk?
RSTR
v?robce
ACT
dodat
PRED
hlava
PAT
 
ty i
RSTR
den
DIFF
pozd
TWHEN
Figure 1: A simplified TGTS of the Czech sentence Jenz?e
tuzemsky? vy?robce dostal hlavy o c?tyr?i dny pozde?ji. ?How-
ever, the domestic producer got the heads four days later.?
Let us adduce further two examples in which the func-
tors are written in capitals in the otherwise strongly sim-
plified representations, where most of the adjuncts are un-
derstood as depending on nouns, whereas the other func-
tors concern the syntactic relations to the verb. Let us
note that with the verb arrive the above mentioned test
determines the Directional as a (semantically) obligatory
item that can be specified by the hearer according to the
given context (basically, as here or there):
(1) Jane changed her house from a shabby cottage into
a comfortable home.
(1?) Jane.ACT changed her.APP house.PAT
from-a-shabby.RSTR cottage.ORIG
into-a-comfortable.RSTR home.EFF.
(2) Yesterday Jim arrived by car.
(2?) Yesterday.TWHEN Jim.ACT arrived here.DIR3 by-
car.MEANS.
A formulation of an annotation scenario based on well-
specified subcategorization criteria helps to compare dif-
ferent schemes and to draw some conclusions from such
a comparison. In (Hajic?ova? and Kuc?erova?, 2002) the au-
thors attempt to investigate how different frameworks an-
notating some kind of deep (underlying) syntactic level
(the LCS Data, PropBank and PDT) compare with each
other (having in mind also a more practical applica-
tion, namely a machine translation project the modules
of which would be ?machine-learned?, using a procedure
based on syntactically annotated parallel corpora). We
are convinced that such a pilot study may also contribute
to the discussions on a possibility/impossibility of for-
mulating a ?theory neutral? syntactic annotation scheme.
The idea of a theory neutral annotation scenario seems
to be an unrealistic goal: it is hardly possible to imag-
ine a classification of such a complex subsystem of lan-
guage as the syntactic relations are, without a well mo-
tivated theoretical background; moreover, the languages
of the annotated texts are of different types, and the the-
oretical frameworks the authors of the schemes are used
to work with differ in the ?depth? or abstractness of the
classification of the syntactic relations. However, the dif-
ferent annotation schemes seem to be translatable if the
distinctions made in them are stated as explicitly as pos-
sible, with the use of operational criteria, and supported
by larger sentential contexts. The third condition is made
realistic by very large text corpora being available elec-
tronically; making the first two conditions a realistic goal
is fully in the hands of the designers of the schemes.
2.2 Topic/Focus Articulation
Another aspect of the sentence structure that has to be
taken into account when going beyond the shallow struc-
ture of sentences is the communicative function of the
sentence, reflected in its information structure. As has
been convincingly argued for during decades of linguistic
discussions (see studies by Rooth, Steedman, and several
others, and esp. the argumentation in (Hajic?ova? et al,
1998)), the information structure of the sentence (topic-
focus articulation, TFA in the sequel) is semantically rel-
evant and as such belongs to the semantic structure of the
sentence. A typical declarative sentence expresses that
its focus holds about its topic, and this articulation has
its consequences for the truth conditions, especially for
the differences between meaning proper, presuppositiona
and allegations (see (Hajic?ova?, 1993); (Hajic?ova? et al,
1998)).
TFA often is understood to constitute a level of its own,
but this is not necessary, and it would not be simple to de-
termine the relationships between this level and the other
layers of language structure. In the Functional Generative
Description (Sgall et al, 1986), TFA is captured as one of
the basic aspects of the underlying structure, namely as
the left-to-right dimension of the dependency tree, work-
ing with the basic opposition of contextual boundness;
the contextually bound (CB) nodes stand to the left of the
non-bound (NB) nodes, with the verb as the root of the
tree being either contextually bound or non-bound.
It should be noted that the opposition of NB/CB is the
linguistically patterned counterpart of the cognitive (and
pre-systemic) opposition of ?given? and ?new? informa-
tion. Thus, e.g. in (3) the pronoun him (being NB), in
fact constitutes the focus of the sentence.
(3) (We met a young pair.) My older companion recog-
nized only HIM.
In the prototypical case, NB items belong to the focus
of the sentence, and CB ones constitute its topic; sec-
ondary cases concern items which are embedded more
deeply than to depend on the main verb of the sentence,
cf. the position of older in (3), which may be understood
as NB, although it belongs to the topic (being an adjunct
of the CB noun companion).
In the tectogrammatical structures of the PDT anno-
tation scenario, we work with three values of the TFA
attribute, namely t (contextually bound node), c (contex-
tually bound contrastive node) and f (contextually non-
bound node). 20,000 sentences of the PDT have al-
ready been annotated in this way, and the consistency and
agreement of the annotators is being evaluated. It seems
to be a doable task to annotate and check the whole set of
TGTSs (i.e. 55,000 sentences) by the end of 2004. This
means that by that time the whole set of 55,000 sentences
will be annotated (and checked for consistency) on both
aspects of deep syntactic structure. An algorithm the in-
put of which are the TGTSs with their TFA values and
the output of which is the division of the whole sentence
structure into the (global) topic and the (global) focus is
being formulated.
2.3 Coreference
The inclusion into the annotation scheme of the two as-
pects mentioned above in Sect. 2.1 and 2.2, namely the
deep syntactic relations and topic-focus articulation, con-
siderably extends the scenario in a desirable way, toward
a more complex representation of the meaning of the sen-
tence. The third aspect, the account of coreferential rela-
tions, goes beyond linguistic meaning proper toward what
can be called the sense of the utterance (Sgall, 1994).
Two kinds of coreferential relations have to be distin-
guished: grammatical coreference (i.e. with verbs of con-
trol, with reflexive pronouns, with verbal complements
and with relative pronouns) and textual (which may cross
sentence boundaries), both endophoric and exophoric.
Several annotation schemes have been reported at re-
cent conferences (ACL, LREC) that attempt at a rep-
resentation of coreference relations in continuous texts.
As an example of an attempt to integrate the treatment
of anaphora into a complex deep syntactic scenario, we
would like to present here a brief sketch of the scheme
realized in the Prague Dependency Treebank. For the
time being, we are concerned with coreference relations
in their narrower sense, i.e. not covering the so-called
bridging anaphora (for a possibility to cover also the lat-
ter phenomenon, see (Bo?hmova?, 2004)).
In the Prague Dependency Treebank, coreference is
understood as an asymmetrical binary relation between
nodes of a TGTS (not necessarily the same TGTS), or,
as the case may be, as a relation between a node and
an entity that has no corresponding counterpart in the
TGTS(s). The node from which the coreferential link
leads, is called an anaphor, and the node, to which the
link leads, is called an antecedent.
The present scenario of the PDT provides three coref-
erential attributes: coref, cortype and corlemma. The at-
tribute coref contains the identifier of the antecedent; if
there are more than one antecedents of one anaphor, the
attribute coref includes a sequence of identifiers of the
relevant antecedents; since every node of a TGTS has an
identifier of its own it is a simple programming task to
select the specific information on the antecedent. The at-
tribute cortype includes the information on the type of
coreference (the possible values are gram for grammat-
ical and text for textual coreference), or a sequence of
the types of coreference, where each element of cortype
corresponds to an element of coref. The attribute cor-
lemma is used for cases of a coreference between a node
and an entity that has no corresponding counterpart in the
TGTS(s): for the time being, there are two possible val-
ues of this attribute, namely segm in the case of a coref-
erential link to a whole segment of the preceding text (not
just a sentence), and exoph in the case of an exophoric
relation. Cases of reference difficult to be identified even
if the situation is taken into account are marked by the
assignment of unsp as the lemma of the anaphor. This
does not mean that a decision is to be made between two
or more referents but that the reference cannot be fully
specified even within a broader context.
In order to facilitate the task of the annotators and
to make the resulting structures more transparent and
telling, the coreference relations are captured by arrows
leading from the anaphor to the antecedent and the types
of coreference are distinguished by different colors of the
arrows. There are certain notational devices used in cases
when the antecedent is not within the co-text (exophoric
coreference) or when the link should lead to a whole seg-
ment rather than to a particular node. If the anaphor
corefers to more than a single node or to a subtree, the
link leads to the closest preceding coreferring node (sub-
tree). If there is a possibility to choose between a link to
an antecedent or to a postcedent, the link always leads to
the antecedent.
#51
SENT
&Gen;
ACT
n jak?
RSTR
zem
ACT
usn?st_se
COND
?stavn?
RSTR
z?kon
PAT
pak
TWHEN
ten
PAT
t ?ko
MANN
m nit
PRED
Figure 2: A TGTS of the sentence Pokud se ne?jaka? zeme?
usnese na u?stavn??m za?konu, pak se to te?z?ko me?n??. ?If a
country accepts a constitution law, then this is difficult to
change.?
The manual annotation is made user-friendly by a spe-
cial module within the TRED editor (Hajic? et al, 2001b)
which is being used for all three subareas of annotation.
In the case of coreference, an automatic pre-selection of
nodes relevant for annotation is used, making the process
faster.
Until now, about 30,000 sentences have been annotated
as for the above types of coreference relations. One of the
advantages of a corpus-based study of a language phe-
nomenon is that the researchers become aware of sub-
tleties and nuances that are not apparent. For those who
attempt at a corpus annotation, of course, it is necessary
to collect a list of open questions which have a temporary
solution but which should be studied more intensively
and to a greater detail in the future.
Another issue the study of which is significant and
can be facilitated by an availability of a semantically an-
notated corpus, is the question of a (finite) mechanism
the listener (reader) can use to identify the referents. If
the backbone of such a mechanism is seen in the hierar-
chy (partial ordering) of salience, then it can be under-
stood that this hierarchy typically is modified by the flow
of discourse in a way that was specified and illustrated
by (Hajic?ova?, 1993), (Hajic?ova? et al, in prep). In the
flow of a discourse, prototypically, a new discourse ref-
erent emerges as corresponding to a lexical occurrence
that carries f; further occurrences carry t or c, their ref-
erents being primarily determined by their degrees of
salience, although the difference between the lowest de-
grees of salience reduction, is not decisive. It appears to
be possible to capture at least certain aspects of this hi-
erarchy by some (still tentative) heuristic rules, which tie
up the increase/decrease of salience with the position of
the given item in the topic or in the focus of the given ut-
terance. It should also be remarked that there are certain
permanently salient referents, which may be referred to
by items in topic (as ?given? information) without having
a referentially identical antecedent in the discourse. We
denote them as carrying t or c, but perhaps it would be
more adequate to consider them as being always able to
be accommodated
(i) by the utterance itself, as especially the indexicals
(I, you, here, now, yesterday,. . . ),
(ii) by the given culture (democracy, Paris, Shake-
speare, don Quijote,. . . ), by universal human expe-
rience (sun, sky), or
(iii) by the general domain concerned (history, biol-
ogy,...).
Since every node in the PDT carries one of the TFA
values (t, c or f) from which the appurtenance of the
given item to the topic or focus of the whole sentence
can be determined, it will be possible to use the PDT
data and the above heuristics to start experiments with
an automatic assignment of coreferential relations and
check them against the data with the manual annotation
of coreference.
2.4 Lexical Semantics
The design of the tectogrammatical representation is such
that the nodes in the tectogrammatical tree structure rep-
resent (almost) only the autosemantic words found in the
written or spoken utterance they represent. We believe
that it is thus natural to start distinguishing word senses
only at this level (and not on a lower level, such as surface
syntax or linearized text).
Moreover, there is a close relation between valency
and word senses. We hypothesize that with a suitable
set of dependency relations (both inner participants and
free modifications, see Sect. 2.1), there is only one va-
lency frame per word sense (even though synonyms or
near synonyms might have different valency frames). The
opposite is not true: there can be several word senses with
an identical valency frame.
Although in the detailed valency lexicon VALLEX
(Lopatkova?, 2003), (Lopatkova? et al, 2003) an attempt
has originally been made to link the valency frames to
(Czech) EuroWordNet (Pala and ?Sevec?ek, 1999) senses
to prove this point, this has been abandoned for the time
being because of the idiosyncrasies in WordNet design,
which does not allow to do so properly.
We thus proceed independently with word sense an-
notation based on the Czech version of WordNet. Cur-
rently, we have annotated 10,000 sentences with word
senses, both nouns and verbs. We are assessing now fur-
ther directions in annotation; due to low inter-annotator
agreement, we will probably tend to annotate only over a
preselected subset of the WordNet synsets. An approach
to building semantic lexicons that is more related to our
concept of meaning representation is being prepared in
the meantime (Holub and Stran?a?k, 2003).
3 Conclusions
Up to now, the framework has been checked on a large
amount of running text segments from the Czech Na-
tional Corpus (as for the valency classification 55,000 ut-
terances, as for the topic-focus structure 20,000 ones). In
several cases, it was found that a more detailed classifi-
cation is needed (e.g. with the differentiation of the Gen-
eral Actor vs. Unspecified, cf. the difference between
One can cook well with this oven and At this pub they
cook well). However, it has been confirmed that good
results can be achieved with the chosen classification of
about 40 valency types and of 15 other grammatical at-
tribute types (such as (Semantic) Number, Tense, Modal-
ities, etc., but also different values of Location, such as
those corresponding to the preferred functions of in, at,
on, under, over, etc., or of Benefactive (positive vs. neg-
ative), and so on). It can be supposed that the core of
language corresponds to underlying sentence structures
and to their unmarked morphemic and phonemic coun-
terparts. The marked layers have to be described by spe-
cific sets of rules, most of which concern irregularities
of morphemics, including differences between the under-
lying order of nodes and the surface (morphemic) word
order, especially in cases in which the latter does not di-
rectly meet the condition of projectivity (with no cross-
ing of edges, cf. the discontinuous constituents of other
frameworks).
The prototypical varieties of sentence structure can
thus be characterized by projective rooted trees, which
points to the possibility to describe the core of language
structure on the basis of a maximally perspicuous pat-
tern that comes close to patterns present in other domains
(primitive logic, arithmetics, and so on) which are nor-
mally mastered by children. Structures of this kind are
not only appropriate for computer implementation, but
they also help understand the relative easiness of mas-
tering the mother tongue, without a necessity to assume
complex innate mechanisms specific for the language fac-
ulty.
References
Alena Bo?hmova?. 2004. Automatized Procedures in
the Process of Annotation of PDT. Ph.D. the-
sis, Charles University, Faculty of Mathemartics and
Physics, Prague.
Bonnie Dorr. 2001. The LCS Database.
http://www.umiacs.umd.edu/ ?bonnie/
LCS Database Documentation.html.
Charles J. Fillmore, Christopher R. Robinson, and
Miriam R. L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Eva Hajic?ova? and Ivona Kuc?erova?. 2002. Argu-
ment/valency structure in PropBank, LCS database and
Prague Dependency Treebank: A comparative study.
In Proceedings of LREC.
Jan Hajic? and Zden?ka Ures?ova?. 2003. Linguistic Anno-
tation: from Links to Cross-Layer Lexicons. In Pro-
ceedings of The Second Workshop on Treebanks and
Linguistic Theories, volume 9 of Mathematical Mod-
eling in Physics, Engineering and Cognitive Sciences,
pages 69?80. Va?xjo? University Press, November 14?
15, 2003.
Jan Hajic?, Eva Hajic?ova?, Petr Pajas, Jarmila Panevova?,
Petr Sgall, and Barbora Vidova?-Hladka?. 2001a.
Prague Dependency Treebank 1.0 (Final Production
Label). CDROM CAT: LDC2001T10, ISBN 1-58563-
212-0.
Jan Hajic?, Petr Pajas, and Barbora Hladka?. 2001b.
The Prague Dependency Treebank: Annotation Struc-
ture and Support. In IRCS Workshop on Linguistic
Databases, pages 105?114, Philadelphia, PA, Dec. 11?
13.
Jan Hajic?, Alevtina Be?mova?, Petr Pajas, Jarmila
Panevova?, Veronika ?Rezn??c?kova?, and Zden?ka Ures?ova?.
2003. PDT-VALLEX: Creating a Large-coverage Va-
lency Lexicon for Treebank Annotation. In Joakim
Nivre and Erhard Hinrichs, editors, 2nd International
Workshop on Treebanks and Linguistic Theories, vol-
ume 9 of Mathematical Modeling in Physics, Engi-
neering and Cognitive Sciences, pages 57?68. Va?xjo?
University Press, Va?xjo?, Sweden, Nov. 14?15, 2003.
Jan Hajic?. 1998. Building a Syntactically Anno-
tated Corpus: The Prague Dependency Treebank. In
Eva Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?19.
Prague Karolinum, Charles University Press.
Eva Hajic?ova?, Barbara Partee, and Petr Sgall. 1998.
Topic-focus articulation, tripartite structures, and se-
mantic content. Kluwer Academic Publishers, Ams-
terdam, Netherlands.
Eva Hajic?ova?, Jir??? Havelka, and Petr Sgall. in prep.
Topic and Focus, Anaphoric Relations and Degrees of
Salience. In Prague Linguistic Circle Papers 5. Ams-
terdam/Philadelphia: John Benjamins.
Eva Hajic?ova?. 1993. Issues of Sentence Structure and
Discourse. Charles University, Prague, Czech Repub-
lic.
Eva Hajic?ova?. 2000. Dependency-Based Underlying-
Structure Tagging of a Very Large Czech Corpus.
In Special issue of TAL journal, Grammaires de
De?pendence / Dependency Grammars (ed. Sylvian Ka-
hane), pages 57?78. Hermes.
Martin Holub and Pavel Stran?a?k. 2003. Approaches to
building semantic lexicons. In WDS?03 Proceedings
of Contributed Papers, Part I, pages 173?178, Prague.
MATFYZPRESS, Charles University.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press.
Marke?ta Lopatkova?, Zdene?k ?Zabokrtsky?, Karol??na
Skwarska, and Va?clava Benes?ova?. 2003. VALLEX
1.0. http://ckl.mff.cuni.cz/zabokrtsky/
vallex/1.0.
Marke?ta Lopatkova?. 2003. Valency in the Prague De-
pendency Treebank: Building the Valency Lexicon.
Prague Bulletin of Mathematical Linguistics, 79?80:in
print.
Karel Pala and Pavel ?Sevec?ek. 1999. Czech
wordnet. http://www.fi.muni.cz/nlp/grants/
ewn cz.ps.en.
Martha Palmer, J Rosenzweig, and S Cotton. 2001.
Automatic Predicate Argument Analysis of the Penn
TreeBank. In J Allan, editor, Processdings of HLT
2001, First Int. Conference on Human Technology Re-
search. Morgan Kaufmann, San Francisco.
Jarmila Panevova?. 1974. On verbal Frames in Functional
Generative Description. Prague Bulletin of Mathemat-
ical Linguistics, 22:3?40.
Jarmila Panevova?. 1994. Valency Frames and the Mean-
ing of the Sentence. In Philip Luelsdorff, editor, The
Prague School of Structural and Functional Linguis-
tics, pages 223?243. John Benjamins, Amsterdam-
Philadephia.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The meaning of the sentence and its semantic and
pragmatic aspects. Reidel, Dordrecht.
Petr Sgall. 1994. Meaning, Reference and Discourse Pat-
terns. In Philip Luelsdorff, editor, The Prague School
of Structural and Functional Linguistics, pages 277?
309. John Benjamins, Amsterdam-Philadephia.
Lucien Tesnie`re. 1959. ?Elements de Syntaxe Structurale.
Klincksieck, Paris.
Proceedings of the Linguistic Annotation Workshop, pages 191?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Panel Session: Discourse Annotation
Manfred Stede
Dept. of Linguistics
University of Potsdam
stede@ling.uni-potsdam.de
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Eva Hajic?ova?
Faculty of Math. and Physics
Charles University
hajicova@ufal.ms.mff.cuni.cz
Brian Reese
Dept. of Linguistics
Univ. of Texas at Austin
bjreese@mail.utexas.edu
Simone Teufel
Computer Laboratory
Univ. of Cambridge
sht25@cl.cam.uk
Bonnie Webber
School of Informatics
Univ. of Edinburgh
bonnie@inf.ed.ac.uk
Theresa Wilson
Dept. of Comp. Science
Univ. of Pittsburgh
twilson@cs.pitt.edu
1 Introduction
The classical ?success story? of corpus annotation
are the various syntax treebanks that provide struc-
tural analyses of sentences and have enabled re-
searchers to develop a range of new and highly suc-
cessful data-oriented approaches to sentence pars-
ing. In recent years, however, a number of corpora
have been constructed that provide annotations on
the discourse level, i.e. information that reaches be-
yond the sentence boundaries. Phenomena that have
been annotated include coreference links, the scope
of connectives, and coherence relations. Many of
these are phenomena on whose handling there is
not a general agreement in the research community,
and therefore the question of ?recycling? corpora by
other people and for other purposes is often diffi-
cult. (To some extent, this is due to the fact that dis-
course annotation deals ?only? with surface reflec-
tions of underlying, abstract objects.) At the same
time, the efforts needed for building high-quality
discourse corpora are considerable, and thus one
should be careful in deciding how to invest those ef-
forts. One aspect of providing added-value with an-
notation projects is that of shared corpora: If a vari-
ety of annotation efforts is executed on the same pri-
mary data, the series of annotation levels can yield
insights that the creators of the individual levels had
not explicitly planned for. A clear case is the rela-
tionship between coherence relations and connective
use: When both levels are marked individually and
with independent annotation guidelines, then after-
wards the correlations between coherence relations,
cue usage (and possibly other factors, if annotated)
can be studied systematically. This conception of
multi-level annotation presupposes, of course, that
the technical problems of setting annotation levels
in correspondence to one another be resolved.
The panel on discourse annotation is organized
by Manfred Stede and Janyce Wiebe. It aims at
surveying the scene of discourse corpora, exploring
chances for synergy, and identifying desiderata for
future corpus creation projects. In preparation for
the panel, the participants have provided the follow-
ing short descriptions of the various copora in whose
construction they have been involved.
2 Prague Dependency Treebank
(Eva Hajic?ova?, Prague)
One of the maxims of the work on the Prague De-
pendency Treebank is that one should not overlook,
disregard and thus lose what the sentence structure
offers when one attempts to analyze the structure of
discourse, thus moving from ?the trees? to ?the for-
est?. Therefore, we emphasize that discourse anno-
tation should make use of every possible detail the
annotation of the component parts of the discourse,
namely the sentences, puts at our disposal. This
is, of course, not only true for the surface shape of
the sentence (i.e., the surface means of expression),
but (and most importantly) for the underlying repre-
sentation of sentences. The panel contribution will
introduce the (multilayered) annotation scenario of
the Prague Dependency Treebank and illustrate the
point using some of the particular features of the un-
derlying structure of sentences that can be made use
of in planning the scenario of discourse ?treebanks?.
191
3 SDRT in Newspaper Text
(Brian Reese, Austin)
We are currently working under the auspices of
an NSF grant to build and train a discourse parser
and codependent anaphora resolution program to
test discourse theories empirically. The training re-
quires the construction of a corpus annotated with
discourse structure and coreference information. So
far, we have annotated the MUC61 corpus for dis-
course structure and are in the process of annotating
the ACE22 corpus; both corpora are already anno-
tated for coreference. One of the goals of the project
is to investigate whether using the right frontier con-
straint improves the system?s performance in resolv-
ing anaphors. Here we detail some experiences we
have had with the discourse annotation process.
An implementation of the extant SDRT (Asher and
Lascarides, 2003) glue logic for building discourse
structures is insufficient to deal with open domain
text, and we cannot envision an extended version
at the present time able to deal with the problem.
Thus, we have opted for a machine learning based
approach to discourse parsing based on superficial
features, like BNL. To build an implementation to
test these ideas, we have had to devise a corpus of
texts annotated for discourse structure in SDRT.
Each of the 60 texts in the MUC6 corpus, and now
18 of the news stories in ACE2, were annotated by
two people familiar with SDRT. The annotators then
conferred and agreed upon a gold standard. Our
annotation effort took the hierarchical structure of
SDRT seriously and built graphs in which the nodes
are discourse units and the arcs represent discourse
relations between the units. The units could either be
simple (elementary discourse units: EDUs) or they
could be complex. We assumed that in principle the
units were recursively generated and could have an
arbitrary though finite degree of complexity.
4 Potsdam Commentary Corpus
(Manfred Stede, Potsdam)
Construction of the Potsdam Commentary Corpus
(PCC) began in 2003 and is still ongoing. It is a
1The Message Understanding Conference, www-nlpir.
nist.gov/related projects/muc/.
2The Automated Content Extraction program,
www.nist.gov/speech/tests/ace/.
genre-specific corpus of German newspaper com-
mentaries, taken from the daily papers Ma?rkische
Allgemeine Zeitung and Tagesspiegel. One central
aim is to provide a tool for studying mechanisms
of argumentation and how they are reflected on the
linguistic surface. The corpus on the one hand is a
collection of ?raw? data, which is used for genre-
oriented statistical explorations. On the other hand,
we have identified two sub-corpora that are subject
to a rich multi-level annotation (MLA).
The PCC176 (Stede, 2004) is a sub-corpus that
is available upon request for research purposes. It
consists of 176 relatively short commentaries (12-
15 sentences), with 33.000 tokens in total. The
sentences have been PoS-tagged automatically (and
manually checked); sentence syntax was anno-
tated semi-automatically using the TIGER scheme
(Brants et al, 2002) and Annotate3 tool. In addition,
we annotated coreference (PoCos (Krasavina and
Chiarcos, 2007)) and rhetorical structure according
to RST (Mann and Thompson, 1988). Our anno-
tation software architecture consists of a variety of
standard, external tools that can be used effectively
for the different annotation types. Their XML output
is then automatically converted to a generic format
(PAULA, (Dipper, 2005)), which is read into the lin-
guistic database ANNIS (Dipper et al, 2004), where
the annotations are aligned, so that the data can be
viewed and queried across annotation levels.
The PCC10 is a sub-corpus of 10 commentaries
that serves as ?testbed? for further developing the
annotation levels. On the one hand, we are apply-
ing recent guidelines on annotation of information
structure (Go?tze et al, 2007). On the other hand,
based on experiences with the RST annotation, we
are replacing the rhetorical trees with a set of dis-
tinct, simpler annotation layers: thematic structure,
conjunctive relations (Martin, 1992), and argumen-
tation structure (Freeman, 1991); these are comple-
mented by the other levels mentioned above for the
PCC176. The primary motivation for this step is the
high degree of arbitrariness that annotators reported
when producing the RST trees (see (Stede, 2007)).
By separating the thematic from the intentional in-
formation, and accounting for the surface-oriented
3www.coli.uni-saarland.de/projects/
sfb378/negra-corpus/annotate.html
192
conjunctive relations (which are similar to what is
annotated in the PDTB, see Section 6), we hope to
? make annotation easier: handling several ?sim-
ple? levels individually should be more effec-
tive than a single, very complex annotation
step;
? end up with less ambiguity in the annotations,
since the reasons for specific decisions can be
made explicit (by annotations on ?simpler? lev-
els);
? be more explicit than a single tree can be: if a
discourse fulfills, for example, a function both
for thematic development and for the writer?s
intention, they can both be accounted for;
? provide the central information that a ?tradi-
tional? rhetorical tree conveys, without loosing
essential information.
5 AZ Corpus
(Simone Teufel, Cambridge)
The Argumentative Zoning (AZ) annotation scheme
(Teufel, 2000; Teufel and Moens, 2002) is con-
cerned with marking argumentation steps in scien-
tific articles. One example for an argumentation step
is the description of the research goal, another an
overt comparison of the authors? work with rival ap-
proaches. In our scheme, these argumentation steps
have to be associated with text spans (sentences or
sequences of sentences). AZ?Annotation is the la-
belling of each sentence in the text with one of these
labels (7 in the original scheme in (Teufel, 2000)).
The AZ labels are seen as relations holding between
the meanings of these spans, and the rhetorical act
of the entire paper. (Teufel et al, 1999) reports on
interannotator agreement studies with this scheme.
There is a strong interrelationship between the ar-
gumentation in a paper, and the citations writers use
to support their argument. Therefore, a part of the
computational linguistics corpus has a second layer
of annotation, called CFC (Teufel et al, 2006) or
Citation Function Classification. CFC? annotation
records for each citation which rhetorical function it
plays in the argument. This is following the spirit of
research in citation content analysis (e.g., (Moravc-
sik and Murugesan, 1975)). An example for a ci-
tation function would be ?motivate that the method
used is sound?. The annotation scheme contains
12 functions, clustered into ?superiority?, ?neutral
comparison/contrast?, ?praise or usage? and ?neu-
tral?.
One type of research we hope to do in the future
is to study the relationship between these rhetori-
cal phonemena with more traditional discourse phe-
nomena, e.g. anaphoric expressions.
The CmpLg/ACL Anthology corpora consist of
320/9000 papers in computational linguistics. They
are partially annotated with AZ and CFC markup. A
subcorpus of 80 parallelly annotated papers (AZ and
CFF) can be obtained from us for research (12000
sentences, 1756 citations). We are currently port-
ing both schemes to chemistry in the framework
of the EPSRC-sponsored project SciBorg. In the
course of this work a larger, more general AZ an-
notation scheme was developed. The SciBorg effort
will result in an AZ/CFC?annotated chemistry cor-
pus available to the community in 2009.
In terms of challenges, the most time-consuming
aspects of creating this annotated corpus were for-
mat conversions on the corpora, and cyclic adapta-
tions of scheme and guidelines. Another problem is
the simplification of annotating only full sentences;
sometimes, annotators would rather mark a clause
or sometimes even just an NP. However, we found
these cases to be relatively rare.
6 Penn Discourse Treebank
(Bonnie Webber, Edinburgh)
The Penn Discourse TreeBank (Miltsakaki et al,
2004; Prasad et al, 2004; Webber, 2005) anno-
tates discourse relations over the Wall Street Jour-
nal corpus (Marcus et al, 1993), in terms of dis-
course connectives and their arguments. Following
the approach towards discourse structure in (Webber
et al, 2003), the PDTB takes a lexicalized approach,
treating discourse connectives as the anchors of the
relations and thus as discourse-level predicates tak-
ing two Abstract Objects as their arguments. An-
notated are the text spans that give rise to these ar-
guments. There are primarily two types of connec-
tives in the PDTB: explicit and implicit, the latter
being inserted between adjacent paragraph-internal
sentence pairs not related by an explicit connective.
193
Also annotated in the PDTB is the attribution of
each discourse relation and of its arguments (Dinesh
et al, 2005; Prasad et al, 2007). (Attribution itself
is not considered a discourse relation.) A prelimi-
nary version of the PDTB was released in April 2006
(PDTB-Group, 2006), and is available for download
at http://www.seas.upenn.edu/?pdtb. This release only has
implicit connectives annotated in three sections of
the corpus. The annotation of all implicit connec-
tives, along with a hierarchical semantic classifica-
tion of all connectives (Miltsakaki et al, 2005), will
appear in the final release of the PDTB in August
2007.
Here I want to mention three of the challenges we
have faced in developing the PDTB:
(I) Words and phrases that can function as con-
nectives can also serve other roles. (Eg, when can be
a relative pronoun, as well as a subordinating con-
junction.) It has been difficult to identify all and
only those cases where a token functions as a dis-
course connective, and in many cases, the syntactic
analysis in the Penn TreeBank (Marcus et al, 1993)
provides no help. For example, is as though always a
subordinating conjunction (and hence a connective)
or do some tokens simply head a manner adverbial
(eg, seems as though . . . versus seems more rushed
as though . . . )? Is also sometimes a discourse con-
nective relating two abstract objects and other times,
an adverb that presupposes that a particular property
holds of some other entity? If so, when one and
when the other? In the PDTB, annotation has erred
on the side of false positives.
(II) In annotating implicit connectives, we discov-
ered systematic non-lexical indicators of discourse
relations. In English, these include cases of marked
syntax (eg, Had I known the Queen would be here,
I would have dressed better.) and cases of sentence-
initial PPs and adjuncts with anaphoric or deictic
NPs such as at the other end of the spectrum, adding
to that speculation. These cases labelled ALTLEX,
for ?alternative lexicalisation? have not been anno-
tated as connectives in the PDTB because they are
fully productive (ie, not members of a more eas-
ily annotated closed set of tokens). They comprise
about 1% of the cases the annotators have consid-
ered. Future discourse annotation will benefit from
further specifying the types of these cases.
(III) The way in which spans are annotated as ar-
guments to connectives also raises a challenge. First,
because the PDTB annotates both structural and
anaphoric connectives (Webber et al, 2003), a span
can serve as argument to >1 connective. Secondly,
unlike in the RST corpus (Carlson et al, 2003) or the
Discourse GraphBank (Wolf and Gibson, 2005), dis-
course segments are not separately annotated, with
annotators then identifying what discourse relations
hold between them. Instead, in annotating argu-
ments, PDTB annotators have selected the minimal
clausal text span needed to interpret the relation.
This could comprise an embedded, subordinate or
coordinate clause, an entire sentence, or a (possi-
bly disjoint) sequence of sentences. As a result,
there are fairly complex patterns of spans within and
across sentences that serve as arguments to differ-
ent connectives, and there are parts of sentences that
don?t appear within the span of any connective, ex-
plicit or implicit. The result is that the PDTB pro-
vides only a partial but complexly-patterned cover
of the corpus. Understanding what?s going on and
what it implies for discourse structure (and possibly
syntactic structure as well) is a challenge we?re cur-
rently trying to address (Lee et al, 2006).
7 MPQA Opinion Corpus
(Theresa Wilson, Pittsburgh)
Our opinion annotation scheme (Wiebe et al, 2005)
is centered on the notion of private state, a gen-
eral term that covers opinions, beliefs, thoughts, sen-
timents, emotions, intentions and evaluations. As
Quirk et al (1985) define it, a private state is a state
that is not open to objective observation or verifica-
tion. We can further view private states in terms of
their functional components ? as states of experi-
encers holding attitudes, optionally toward targets.
For example, for the private state expressed in the
sentence John hates Mary, the experiencer is John,
the attitude is hate, and the target is Mary.
We create private state frames for three main types
of private state expressions (subjective expressions)
in text:
? explicit mentions of private states, such as
?fears? in ?The U.S. fears a spill-over?
? speech events expressing private states, such as
?said? in ?The report is full of absurdities,?
194
Xirao-Nima said.
? expressive subjective elements, such as ?full of
absurdities? in the sentence just above.
Frames include the source (experiencer) of the
private state, the target, and various properties such
as polarity (positive, negative, or neutral) and inten-
sity (high, medium, or low). Sources are nested. For
example, for the sentence ?China criticized the U.S.
report?s criticism of China?s human rights record?,
the source is ?writer, China, U.S. report?, reflecting
the facts that the writer wrote the sentence and the
U.S. report?s criticism is the target of China?s criti-
cism. It is common for multiple frames to be created
for a single clause, reflecting various levels of nest-
ing and the type of subjective expression.
The annotation scheme has been applied to a
corpus, called the ?Multi-Perspective Question An-
swering (MPQA) Corpus,? reflecting its origins in
the 2002 NRRC Workshop on Multi-Perspective
Question Answering (MPQA) (Wiebe et al, 2003)
sponsored by ARDA AQUAINT (it is also called
?OpinionBank?). It contains 535 documents and a
total of 11,114 sentences. The articles in the cor-
pus are from 187 different foreign and U.S. news
sources, dating from June 2001 to May 2002. Please
see (Wiebe et al, 2005) and Theresa Wilson?s forth-
coming PhD dissertation for further information, in-
cluding the results of inter-coder agreement studies.
References
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation. Cambridge University Press, Cam-
bridge.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In J. van
Kuppevelt & R. Smith, editor, Current Directions in
Discourse and Dialogue. Kluwer, New York.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005. At-
tribution and the (non-)alignment of syntactic and dis-
course arguments of connectives. In ACL Workshop
on Frontiers in Corpus Annotation, Ann Arbor MI.
Stefanie Dipper, Michael Go?tze, Manfred Stede, and Till-
mann Wegst. 2004. Annis: A linguistic database for
exploring information structure. In Interdisciplinary
Studies on Information Structure, ISIS Working papers
of the SFB 632 (1), pages 245?279.
Stefanie Dipper. 2005. XML-based stand-off represen-
tation and exploitation of multi-level linguistic annota-
tion. In Rainer Eckstein and Robert Tolksdorf, editors,
Proceedings of Berliner XML Tage, pages 39?50.
James B. Freeman. 1991. Dialectics and the
Macrostructure of Argument. Foris, Berlin.
Michael Go?tze, Cornelia Endriss, Stefan Hinterwimmer,
Ines Fiedler, Svetlana Petrova, Anne Schwarz, Stavros
Skopeteas, Ruben Stoel, and Thomas Weskott. 2007.
Information structure. In Information structure in
cross-linguistic corpora: annotation guidelines for
morphology, syntax, semantics, and information struc-
ture, volume 7 of ISIS Working papers of the SFB 632,
pages 145?187.
Olga Krasavina and Christian Chiarcos. 2007. Potsdam
Coreference Scheme. In this volume.
Alan Lee, Rashmi Prasad, Aravind Joshi, Nikhil Dinesh,
and Bonnie Webber. 2006. Complexity of dependen-
cies in discourse. In Proc. 5th Workshop on Treebanks
and Linguistic Theory (TLT?06), Prague.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. TEXT, 8:243?281.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large scale anno-
tated corpus of English: The Penn TreeBank. Compu-
tational Linguistics, 19:313?330.
James R. Martin. 1992. English text: system and struc-
ture. John Benjamins, Philadelphia/Amsterdam.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating discourse connec-
tives and their arguments. In NAACL/HLT Workshop
on Frontiers in Corpus Annotation, Boston.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Experiments
on sense annotation and sense disambiguation of dis-
course connectives. In 4t Workshop on Treebanks and
Linguistic Theory (TLT?05), Barcelona, Spain.
Michael J. Moravcsik and Poovanalingan Murugesan.
1975. Some results on the function and quality of ci-
tations. Soc. Stud. Sci., 5:88?91.
The PDTB-Group. 2006. The Penn Discourse TreeBank
1.0 annotation manual. Technical Report IRCS 06-01,
University of Pennsylvania.
195
Rashmi Prasad, Eleni Miltsakaki, Aravind Joshi, and
Bonnie Webber. 2004. Annotation and data mining
of the Penn Discourse TreeBank. In ACL Workshop
on Discourse Annotation, Barcelona, Spain, July.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind Joshi,
and Bonnie Webber. 2007. Attribution and its annota-
tion in the Penn Discourse TreeBank. TAL (Traitement
Automatique des Langues.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech, and
Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, New York.
Manfred Stede. 2004. The Potsdam commentary corpus.
In Proceedings of the ACL Workshop on Discourse An-
notation, pages 96?102, Barcelona.
Manfred Stede. 2007. RST revisited: disentangling nu-
clearity. In Cathrine Fabricius-Hansen and Wiebke
Ramm, editors, ?Subordination? versus ?coordination?
in sentence and text ? from a cross-linguistic perspec-
tive. John Benjamins, Amsterdam. (to appear).
Simone Teufel and Marc Moens. 2002. Summaris-
ing scientific articles ? experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?446.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumenta-
tion in research articles. In Proceedings of the 9th Eu-
ropean Conference of the ACL (EACL-99), pages 110?
117, Bergen, Norway.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. An annotation scheme for citation function. In
Proceedings of SIGDIAL-06, Sydney, Australia.
Simone Teufel. 2000. Argumentative Zoning: Infor-
mation Extraction from Scientific Text. Ph.D. thesis,
School of Cognitive Science, University of Edinburgh,
Edinburgh, UK.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29:545?587.
Bonnie Webber. 2005. A short introduction to the Penn
Discourse TreeBank. In Copenhagen Working Papers
in Language and Speech Processing.
Janyce Wiebe, Eric Breck, Chris Buckley, Claire Cardie,
Paul Davis, Bruce Fraser, Diane Litman, David Pierce,
Ellen Riloff, Theresa Wilson, David Day, and Mark
Maybury. 2003. Recognizing and organizing opinions
expressed in the world press. In Working Notes of the
AAAI Spring Symposium in New Directions in Ques-
tion Answering, pages 12?19, Palo Alto, California.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31:249?287.
196
