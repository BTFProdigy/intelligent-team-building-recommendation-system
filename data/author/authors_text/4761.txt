An Indexing Method Based on Sentences* 
Li Li 1, Chunfa Yuan1 , K.F. Wong2, and Wenjie Li3 
1State Key Laboratory of Intelligent Technology and System 
1Dept. of Computer Science & Technology, Tsinghua University, Beijing 100084 
 Email: lili97@mails.tsinghua.edu.cn; cfyuan@tsinghua.edu.cn 
2D e p t. o f  S y s te m  E n g in e e r in g  &  E n g in e e r in g  M a n a g e m e n t, T h e  C h in e se  U n iv e rs ity  o f H o n g  K o n g , H o n g  K o n g . 
Email: kfwong@se.cuhk.edu.hk 
3
 Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong. 
Email: cswjli@comp.polyu.edu.hk  
 
                                                          
*Supported by Natural Science Foundation of China(Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 177?182,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improving Chinese Word Segmentation on Micro-blog Using Rich
Punctuations
Longkai Zhang Li Li Zhengyan He Houfeng Wang? Ni Sun
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
zhlongk@qq.com, li.l@pku.edu.cn, hezhengyan.hit@gmail.com,
wanghf@pku.edu.cn,sunny.forwork@gmail.com
Abstract
Micro-blog is a new kind of medium
which is short and informal. While no
segmented corpus of micro-blogs is avail-
able to train Chinese word segmentation
model, existing Chinese word segmenta-
tion tools cannot perform equally well
as in ordinary news texts. In this pa-
per we present an effective yet simple ap-
proach to Chinese word segmentation of
micro-blog. In our approach, we incor-
porate punctuation information of unla-
beled micro-blog data by introducing char-
acters behind or ahead of punctuations,
for they indicate the beginning or end of
words. Meanwhile a self-training frame-
work to incorporate confident instances is
also used, which prove to be helpful. Ex-
periments on micro-blog data show that
our approach improves performance, espe-
cially in OOV-recall.
1 INTRODUCTION
Micro-blog (also known as tweets in English) is
a new kind of broadcast medium in the form of
blogging. A micro-blog differs from a traditional
blog in that it is typically smaller in size. Further-
more, texts in micro-blogs tend to be informal and
new words occur more frequently. These new fea-
tures of micro-blogs make the Chinese Word Seg-
mentation (CWS) models trained on the source do-
main, such as news corpus, fail to perform equally
well when transferred to texts from micro-blogs.
For example, the most widely used Chinese seg-
menter ?ICTCLAS? yields 0.95 f-score in news
corpus, only gets 0.82 f-score on micro-blog data.
The poor segmentation results will hurt subse-
quent analysis on micro-blog text.
?Corresponding author
Manually labeling the texts of micro-blog is
time consuming. Luckily, punctuations provide
useful information because they are used as indi-
cators of the end of previous sentence and the be-
ginning of the next one, which also indicate the
start and the end of a word. These ?natural bound-
aries? appear so frequently in micro-blog texts that
we can easily make good use of them. TABLE 1
shows some statistics of the news corpus vs. the
micro-blogs. Besides, English letters and digits
are also more than those in news corpus. They
all are natural delimiters of Chinese characters and
we treat them just the same as punctuations.
We propose a method to enlarge the training
corpus by using punctuation information. We
build a semi-supervised learning (SSL) framework
which can iteratively incorporate newly labeled in-
stances from unlabeled micro-blog data during the
training process. We test our method on micro-
blog texts and experiments show good results.
This paper is organized as follows. In section 1
we introduce the problem. Section 2 gives detailed
description of our approach. We show the experi-
ment and analyze the results in section 3. Section
4 gives the related works and in section 5 we con-
clude the whole work.
2 Our method
2.1 Punctuations
Chinese word segmentation problem might be
treated as a character labeling problem which
gives each character a label indicating its position
in one word. To be simple, one can use label ?B?
to indicate a character is the beginning of a word,
and use ?N? to indicate a character is not the be-
ginning of a word. We also use the 2-tag in our
work. Other tag sets like the ?BIES? tag set are not
suiteable because the puctuation information can-
not decide whether a character after punctuation
should be labeled as ?B? or ?S?(word with Single
177
Chinese English Number Punctuation
News 85.7% 0.6% 0.7% 13.0%
micro-blog 66.3% 11.8% 2.6% 19.3%
Table 1: Percentage of Chinese, English, number, punctuation in the news corpus vs. the micro-blogs.
character).
Punctuations can serve as implicit labels for the
characters before and after them. The character
right after punctuations must be the first character
of a word, meanwhile the character right before
punctuations must be the last character of a word.
An example is given in TABLE 2.
2.2 Algorithm
Our algorithm ?ADD-N? is shown in TABLE 3.
The initially selected character instances are those
right after punctuations. By definition they are all
labeled with ?B?. In this case, the number of train-
ing instances with label ?B? is increased while the
number with label ?N? remains unchanged. Be-
cause of this, the model trained on this unbal-
anced corpus tends to be biased. This problem can
become even worse when there is inexhaustible
supply of texts from the target domain. We as-
sume that labeled corpus of the source domain can
be treated as a balanced reflection of different la-
bels. Therefore we choose to estimate the bal-
anced point by counting characters labeling ?B?
and ?N? and calculate the ratio which we denote
as ?. We assume the enlarged corpus is also bal-
anced if and only if the ratio of ?B? to ?N? is just
the same to? of the source domain.
Our algorithm uses data from source domain to
make the labels balanced. When enlarging corpus
using characters behind punctuations from texts
in target domain, only characters labeling ?B? are
added. We randomly reuse some characters label-
ing ?N? from labeled data until ratio? is reached.
We do not use characters ahead of punctuations,
because the single-character words ahead of punc-
tuations take the label of ?B? instead of ?N?. In
summary our algorithm tackles the problem by du-
plicating labeled data in source domain. We de-
note our algorithm as ?ADD-N?.
We also use baseline feature templates include
the features described in previous works (Sun and
Xu, 2011; Sun et al, 2012). Our algorithm is not
necessarily limited to a specific tagger. For sim-
plicity and reliability, we use a simple Maximum-
Entropy tagger.
3 Experiment
3.1 Data set
We evaluate our method using the data from
weibo.com, which is the biggest micro-blog ser-
vice in China. We use the API provided by
weibo.com1 to crawl 500,000 micro-blog texts of
weibo.com, which contains 24,243,772 charac-
ters. To keep the experiment tractable, we first ran-
domly choose 50,000 of all the texts as unlabeled
data, which contain 2,420,037 characters. We
manually segment 2038 randomly selected micro-
blogs.We follow the segmentation standard as the
PKU corpus.
In micro-blog texts, the user names and URLs
have fixed format. User names start with ?@?, fol-
lowed by Chinese characters, English letters, num-
bers and ? ?, and terminated when meeting punc-
tuations or blanks. URLs also match fixed pat-
terns, which are shortened using ?http://t.
cn/? plus six random English letters or numbers.
Thus user names and URLs can be pre-processed
separately. We follow this principle in following
experiments.
We use the benchmark datasets provided by the
second International Chinese Word Segmentation
Bakeoff2 as the labeled data. We choose the PKU
data in our experiment because our baseline meth-
ods use the same segmentation standard.
We compare our method with three baseline
methods. The first two are both famous Chinese
word segmentation tools: ICTCLAS3 and Stan-
ford Chinese word segmenter4, which are widely
used in NLP related to word segmentation. Stan-
ford Chinese word segmenter is a CRF-based seg-
mentation tool and its segmentation standard is
chosen as the PKU standard, which is the same
to ours. ICTCLAS, on the other hand, is a HMM-
based Chinese word segmenter. Another baseline
is Li and Sun (2009), which also uses punctua-
tion in their semi-supervised framework. F-score
1http://open.weibo.com/wiki
2http://www.sighan.org/bakeoff2005/
3http://ictclas.org/
4http://nlp.stanford.edu/projects/
chinese-nlp.shtml\#cws
178
? ? ? ? ? ? ? ? ? ? ? ?
B - - - - - B - - - - -
B N B B N B B N B B N B
Table 2: The first line represents the original text. The second line indicates whether each character is
the Beginning of sentence. The third line is the tag sequence using ?BN? tag set.
ADD-N algorithm
Input: labeled data {(xi, yi)li?1}, unlabeled data {xj}l+uj=l+1.
1. Initially, let L = {(xi, yi)li?1} and U = {xj}l+uj=l+1.2. Label instances behind punctuations in U as ?B? and add them into
L.
3. Calculate ?B?, ?N? ratio ? in labeled data.
4. Randomly duplicate characters whose labels are ?N? in L to make
?B?/?N?= ?
5. Repeat:
5.1 Train a classifier f from L using supervised learning.
5.2 Apply f to tag the unlabeled instances in U .
5.3 Add confident instances from U to L.
Table 3: ADD-N algorithm.
is used as the accuracy measure. The recall of
out-of-vocabulary is also taken into consideration,
which measures the ability of the model to cor-
rectly segment out of vocabulary words.
3.2 Main results
Method P R F OOV-R
Stanford 0.861 0.853 0.857 0.639
ICTCLAS 0.812 0.861 0.836 0.602
Li-Sun 0.707 0.820 0.760 0.734
Maxent 0.868 0.844 0.856 0.760
No-punc 0.865 0.829 0.846 0.760
No-balance 0.869 0.877 0.873 0.757
Our method 0.875 0.875 0.875 0.773
Table 4: Segmentation performance with different
methods on the development data.
TABLE 4 summarizes the segmentation results.
In TABLE 4, Li-Sun is the method in Li and
Sun (2009). Maxent only uses the PKU data for
training, with neither punctuation information nor
self-training framework incorporated. The next 4
methods all require a 100 iteration of self-training.
No-punc is the method that only uses self-training
while no punctuation information is added. No-
balance is similar to ADD N. The only difference
between No-balance and ADD-N is that the for-
mer does not balance label ?B? and label ?N?.
The comparison of Maxent and No-punctuation
shows that naively adding confident unlabeled in-
stances does not guarantee to improve perfor-
mance. The writing style and word formation of
the source domain is different from target domain.
When segmenting texts of the target domain using
models trained on source domain, the performance
will be hurt with more false segmented instances
added into the training set.
The comparison of Maxent, No-balance and
ADD-N shows that considering punctuation as
well as self-training does improve performance.
Both the f-score and OOV-recall increase. By
comparing No-balance and ADD-N alone we can
find that we achieve relatively high f-score if we
ignore tag balance issue, while slightly hurt the
OOV-Recall. However, considering it will im-
prove OOV-Recall by about +1.6% and the f-
score +0.2%.
We also experimented on different size of un-
labeled data to evaluate the performance when
adding unlabeled target domain data. TABLE 5
shows different f-scores and OOV-Recalls on dif-
ferent unlabeled data set.
We note that when the number of texts changes
from 0 to 50,000, the f-score and OOV both are
improved. However, when unlabeled data changes
to 200,000, the performance is a bit decreased,
while still better than not using unlabeled data.
This result comes from the fact that the method
?ADD-N? only uses characters behind punctua-
179
Size P R F OOV-R
0 0.864 0.846 0.855 0.754
10000 0.872 0.869 0.871 0.765
50000 0.875 0.875 0.875 0.773
100000 0.874 0.879 0.876 0.772
200000 0.865 0.865 0.865 0.759
Table 5: Segmentation performance with different
size of unlabeled data
tions from target domain. Taking more texts into
consideration means selecting more characters la-
beling ?N? from source domain to simulate those
in target domain. If too many ?N?s are introduced,
the training data will be biased against the true dis-
tribution of target domain.
3.3 Characters ahead of punctuations
In the ?BN? tagging method mentioned above,
we incorporate characters after punctuations from
texts in micro-blog to enlarge training set.We also
try an opposite approach, ?EN? tag, which uses
?E? to represent ?End of word?, and ?N? to rep-
resent ?Not the end of word?. In this contrasting
method, we only use characters just ahead of punc-
tuations. We find that the two methods show sim-
ilar results. Experiment results with ADD-N are
shown in TABLE 6 .
Unlabeled ?BN? tag ?EN? tag
Data size F OOV-R F OOV-R
50000 0.875 0.773 0.870 0.763
Table 6: Comparison of BN and EN.
4 Related Work
Recent studies show that character sequence la-
beling is an effective formulation of Chinese
word segmentation (Low et al, 2005; Zhao et al,
2006a,b; Chen et al, 2006; Xue, 2003). These
supervised methods show good results, however,
are unable to incorporate information from new
domain, where OOV problem is a big challenge
for the research community. On the other hand
unsupervised word segmentation Peng and Schu-
urmans (2001); Goldwater et al (2006); Jin and
Tanaka-Ishii (2006); Feng et al (2004); Maosong
et al (1998) takes advantage of the huge amount
of raw text to solve Chinese word segmentation
problems. However, they usually are less accurate
and more complicated than supervised ones.
Meanwhile semi-supervised methods have been
applied into NLP applications. Bickel et al (2007)
learns a scaling factor from data of source domain
and use the distribution to resemble target do-
main distribution. Wu et al (2009) uses a Domain
adaptive bootstrapping (DAB) framework, which
shows good results on Named Entity Recognition.
Similar semi-supervised applications include Shen
et al (2004); Daume? III and Marcu (2006); Jiang
and Zhai (2007); Weinberger et al (2006). Be-
sides, Sun and Xu (2011) uses a sequence labeling
framework, while unsupervised statistics are used
as discrete features in their model, which prove to
be effective in Chinese word segmentation.
There are previous works using punctuations as
implicit annotations. Riley (1989) uses it in sen-
tence boundary detection. Li and Sun (2009) pro-
posed a compromising solution to by using a clas-
sifier to select the most confident characters. We
do not follow this approach because the initial er-
rors will dramatically harm the performance. In-
stead, we only add the characters after punctua-
tions which are sure to be the beginning of words
(which means labeling ?B?) into our training set.
Sun and Xu (2011) uses punctuation information
as discrete feature in a sequence labeling frame-
work, which shows improvement compared to the
pure sequence labeling approach. Our method
is different from theirs. We use characters after
punctuations directly.
5 Conclusion
In this paper we have presented an effective yet
simple approach to Chinese word segmentation on
micro-blog texts. In our approach, punctuation in-
formation of unlabeled micro-blog data is used,
as well as a self-training framework to incorpo-
rate confident instances. Experiments show that
our approach improves performance, especially in
OOV-recall. Both the punctuation information and
the self-training phase contribute to this improve-
ment.
Acknowledgments
This research was partly supported by Na-
tional High Technology Research and Devel-
opment Program of China (863 Program) (No.
2012AA011101), National Natural Science Foun-
dation of China (No.91024009) and Major
National Social Science Fund of China(No.
12&ZD227).
180
References
Bickel, S., Bru?ckner, M., and Scheffer, T. (2007).
Discriminative learning for differing training
and test distributions. In Proceedings of the 24th
international conference on Machine learning,
pages 81?88. ACM.
Chen, W., Zhang, Y., and Isahara, H. (2006). Chi-
nese named entity recognition with conditional
random fields. In 5th SIGHAN Workshop on
Chinese Language Processing, Australia.
Daume? III, H. and Marcu, D. (2006). Domain
adaptation for statistical classifiers. Journal of
Artificial Intelligence Research, 26(1):101?126.
Feng, H., Chen, K., Deng, X., and Zheng, W.
(2004). Accessor variety criteria for chinese
word extraction. Computational Linguistics,
30(1):75?93.
Goldwater, S., Griffiths, T., and Johnson, M.
(2006). Contextual dependencies in unsuper-
vised word segmentation. In Proceedings of
the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting
of the Association for Computational Linguis-
tics, pages 673?680. Association for Computa-
tional Linguistics.
Jiang, J. and Zhai, C. (2007). Instance weight-
ing for domain adaptation in nlp. In Annual
Meeting-Association For Computational Lin-
guistics, volume 45, page 264.
Jin, Z. and Tanaka-Ishii, K. (2006). Unsuper-
vised segmentation of chinese text by use of
branching entropy. In Proceedings of the COL-
ING/ACL on Main conference poster sessions,
pages 428?435. Association for Computational
Linguistics.
Li, Z. and Sun, M. (2009). Punctuation as im-
plicit annotations for chinese word segmenta-
tion. Computational Linguistics, 35(4):505?
512.
Low, J., Ng, H., and Guo, W. (2005). A maximum
entropy approach to chinese word segmenta-
tion. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing,
volume 164. Jeju Island, Korea.
Maosong, S., Dayang, S., and Tsou, B. (1998).
Chinese word segmentation without using lex-
icon and hand-crafted training data. In Pro-
ceedings of the 17th international confer-
ence on Computational linguistics-Volume 2,
pages 1265?1271. Association for Computa-
tional Linguistics.
Pan, S. and Yang, Q. (2010). A survey on trans-
fer learning. Knowledge and Data Engineering,
IEEE Transactions on, 22(10):1345?1359.
Peng, F. and Schuurmans, D. (2001). Self-
supervised chinese word segmentation. Ad-
vances in Intelligent Data Analysis, pages 238?
247.
Riley, M. (1989). Some applications of tree-based
modelling to speech and language. In Pro-
ceedings of the workshop on Speech and Nat-
ural Language, pages 339?352. Association for
Computational Linguistics.
Shen, D., Zhang, J., Su, J., Zhou, G., and Tan,
C. (2004). Multi-criteria-based active learning
for named entity recognition. In Proceedings
of the 42nd Annual Meeting on Association for
Computational Linguistics, page 589. Associa-
tion for Computational Linguistics.
Sun, W. and Xu, J. (2011). Enhancing chi-
nese word segmentation using unlabeled data.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pages 970?979. Association for Computational
Linguistics.
Sun, X., Wang, H., and Li, W. (2012). Fast on-
line training with frequency-adaptive learning
rates for chinese word segmentation and new
word detection. In Proceedings of the 50th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 253?262, Jeju Island, Korea. Association
for Computational Linguistics.
Weinberger, K., Blitzer, J., and Saul, L. (2006).
Distance metric learning for large margin near-
est neighbor classification. In In NIPS. Citeseer.
Wu, D., Lee, W., Ye, N., and Chieu, H. (2009).
Domain adaptive bootstrapping for named en-
tity recognition. In Proceedings of the 2009
Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3-Volume
3, pages 1523?1532. Association for Computa-
tional Linguistics.
Xue, N. (2003). Chinese word segmentation as
character tagging. Computational Linguistics
and Chinese Language Processing, 8(1):29?48.
Zhao, H., Huang, C., and Li, M. (2006a). An im-
proved chinese word segmentation system with
181
conditional random field. In Proceedings of the
Fifth SIGHAN Workshop on Chinese Language
Processing, volume 117. Sydney: July.
Zhao, H., Huang, C., Li, M., and Lu, B. (2006b).
Effective tag set selection in chinese word seg-
mentation via conditional random field model-
ing. In Proceedings of PACLIC, volume 20,
pages 87?94.
182
Person Name Disambiguation based on Topic Model
Jiashen Sun, Tianmin Wang and Li Li
Center of Intelligence Science and Technology
Beijing University of Posts and Telecommunications
b.bigart911@gmail.com,
tianmin180@sina.com, wbg111@126.com
Xing Wu
School of Computer
Beijing University of Posts and 
Telecommunications
wuxing-6@163.com
Abstract
In this paper we describe our 
participation in the SIGHAN 2010 Task-
3 (Person Name Disambiguation) and 
detail our approaches. Person Name 
Disambiguation is typically viewed as an 
unsupervised clustering problem where 
the aim is to partition a name?s contexts 
into different clusters, each representing 
a real world people. The key point of 
Clustering is the similarity measure of 
context, which depends upon the features 
selection and representation. Two 
clustering algorithms, HAC and 
DBSCAN, are investigated in our system. 
The experiments show that the topic 
features learned by LDA outperforms
token features and more robust.
1 Introduction
Most current web searches relate to person
names. A study of the query log of the 
AllTheWeb and Altavista search sites gives an 
idea of the relevance of the people search task: 
11-17% of the queries were composed of a 
person name with additional terms and 4% were 
identified simply as person names (Spink et al,
2004).
However, there is a high level of ambiguity 
where multiple individuals share the same name 
and thus the harvesting and the retrieval of 
relevant information becomes more difficult. 
This ambiguity has recently become an active 
research topic and, simultaneously, a relevant 
application domain for Web search services. 
Zoominfo.com, Spock.com and 123people.com 
are examples of sites which perform web people 
search, although with limited disambiguation 
capabilities (Artiles et al, 2009).
This issue directed current researchers 
towards the definition of a new task called Web 
People Search (WePS) or Personal Name 
Disambiguation (PND). The key assumption 
underlying the task is that the context 
surrounding an ambiguous person name is 
indicative of its ascription. The goal of the 
clustering task was to group web pages
containing the target person's name, so that 
pages referring to the same individual are 
assigned to the same cluster. For this purpose a 
large dataset was collected and manually 
annotated.
Moreover, because of the ambiguity in word 
segmentation in Chinese, person name detection
is necessary, which is subtask of Named Entity 
Recognition (NER). NER is one of difficulties 
of the study of natural language processing, of 
which the main task is to identify person names, 
place names, organization names, number, time 
words, money and other entities. The main 
difficulties of Chinese person name entity 
recognition are embodied in the following points: 
1) the diversity of names form; 2) the Chinese 
character within names form words with each; 3) 
names and their context form words; 4) 
translation of foreign names require special 
considerations. 
In this paper we describe our system and
approach in the SIGHAN 2010 task-3 (Person 
Name Disambiguation). A novel Bayesian 
approach is adopt in our system, which
formalizes the disambiguation problem in a 
generative model. For each ambiguous name we 
first draw a distribution over person, and then 
generate context words according to this 
distribution. It is thus assumed that different 
persons will correspond to distinct lexical 
distributions. In this framework, Person Name 
Disambiguation postulates that the observed data 
(contexts) are explicitly intended to 
communicate a latent topic distribution 
corresponding to real world people.
The remainder of this paper is structured as 
follows. We first present an overview of related 
work (Section 2) and then describe our system 
which consists of NER and clustering in more 
details (Sections 3 and 4). Section 5 describes 
the resources and evaluation results in our 
experiments. We discuss our results and 
conclude our work in Section 6.
2 Related Work
The most commonly used feature is the bag of 
words in local or global context of the 
ambiguous name (Ikeda et al, 2009; Romano et 
al., 2009). Because the given corpus is often not 
large enough to learn the realistic probabilities 
or weights for those features, traditional 
algorithm such as vector-based techniques used 
in large-scale text will lead to data sparseness.
In recent years, more and more important 
studies have attempted to overcome the problem
to get a better (semantic) similarity measures. A
lot of features such as syntactic chunks, named 
entities, dependency parses, semantic role labels,
etc., were employed. However, these features 
need many NLP preprocessing (Chen, 2009). 
Many studies show that they can achieve state-
of-the-art performances only with lightweight 
features. Pedersen et al (2005) present
SenseClusters which represents the instances to 
be clustered using second order co?occurrence 
vectors. Kozareva (2008) focuses on the 
resolution of the web people search problem 
through the integration of domain information,
which can represent relationship between 
contexts and is learned from WordNet. PoBOC 
clustering (Cleuziou et al, 2004) is used which 
builds a weighted graph with weights being the
similarity among the objects.
Another way is to utilize universal data
repositories as external knowledge sources (Rao 
et al, 2007; Kalmar and Blume, 2007; Pedersen 
and Kulkarni; 2007) in order to give more
realistic frequency for a proper name or measure 
whether a bigram is a collocation.
Phan et al (2008) presents a general 
framework for building classifiers that deal with 
short and sparse text and Web segments by
making the most of hidden topics discovered 
from large-scale data collections. Samuel Brody 
et al (2009) adopt a novel Bayesian approach 
and formalize the word sense induction problem 
in a generative model.
Previous work using the WePS1 (Artiles et al,
2007) or WePS2 data set (Artiles et al, 2009) 
shows that standard document clustering 
methods can deliver excellent performance if 
similarity measure is enough good to represent 
relationship of context.
The study in Chinese PND is still in its 
infancy. Person Name detection is often 
necessary in Chinese. At present, the main 
technology of person name recognition is used 
statistical models, and the hybrid approach. Liu
et al (2000) designed a Chinese person name 
recognition system based on statistical methods, 
using samples of names from the text corpus and 
the real amount of statistical data to improve the 
system performance, while the shortcoming is 
that samples of name database are too small, 
resulting in low recall. Li et al (2006) use the 
combination of the boundary templates and local 
statistics to recognize Chinese person name, the 
recognition process is to use the boundary with 
the frequency of template to identify potential 
names, and to recognize the results spread to the 
entire article in order to recall missing names 
caused by sparse data.
3 Person Name Recognition
In this section, we focus on Conditional Random 
Fields (CRFs) algorithm to establish the 
appropriate language model. Given of the input 
text, we may detect the potential person names 
in the text fragments, and then take various 
features into account to recognize of Chinese 
person names.
Conditional Random Fields as a sequence 
learning method has been successfully applied in 
many NLP tasks. More details of the its 
principle can be referred in (Lafferty, McCallum, 
and Pereira, 2001; Wallach, 2004). We here will 
focus on how to apply CRFs in our person name 
recognition task.
3.1 CRFs-based name recognition
CRFs is used to get potential names as the first 
stage name recognition outcome. To avoid the 
interference that caused by word segmentation 
errors, we use single Chinese character 
information rather than word as discriminative 
features for CRFs learning model. 
We use BIEO label strategy to transfer the name 
recognition as a sequence learning task. The 
label set includes: B-Nr (Begin, the initial 
character of name), I-Nr (In, the middle 
character of name), E-Nr(End, the end character 
of name) and O (Other, other characters that 
aren?t name).
3.2 Rule-based Correction
After labeling the potential names by CRFs 
model, we apply a set of rules to boost 
recognition result, which has been proved to be 
the key to improve Chinese name recognition.
The error of the potential names outcome by 
CRFs model is mainly divided into the following 
categories: the initial character of name is not 
recognized, the middle character of name is not 
recognized, the end character of name is not 
recognized, and their combinations of those 
three errors. The other two extreme errors, 
including non-name recognition for the anchor 
name, and the name is not recognized as 
potential names.
In the stage of rule-based correction, we first 
conduct word segmentation for the text. The 
segmentation process is also realized with the 
method of CRFs, without using dictionaries and 
other external knowledge. The detailed 
description is beyond this paper, which can be 
accessible in the paper (Lafferty, McCallum, and 
Pereira, 2001). The only thing we should note is 
that part of the error in such segmentation result 
obtained in this way can be corrected through 
the introduction of an external dictionary.
For each potential name, and we examine it 
from the following two aspects:
1) It is reasonable to use the word in a person 
name, including checking the surname and the 
character used in names;
2) The left and right borders are correct. 
Check the left and right sides of the cutting unit 
can be added to the names, including the words
used before names, the words used behind 
names and the surname and character used in 
names.
4 Clustering
4.1 Features
The clustering features we used can be divided 
into two types, one is token features, including 
word (after stop-word removal), uni-character
and bi-character, the other is topic features, 
which is topic-based distribution of global or 
window context learned by LDA (Latent 
Dirichlet Allocation) model.
4.1.1 Token-based Features
Simple token-based features are used in almost 
every disambiguation system. Here, we extract 
three kinds of tokens: words, uni-char and bi-
char occurring in a given document.
Then, each token in each feature vector is 
weighed by using a tf-idf weighting and entropy 
weighting schemes defined as follows.
tf-idf weighting:
log( )ik ik
i
N
a f
n
 <
entropy weighting:
1
1
log( 1.0) 1 log( )
log( )
N
ij ij
ik ik
j i i
f f
a f
N n n 
? ?? ?
  ? ?? ?? ?? ?? ?
?<
where is the frequency of term i in 
document k, N is the number of document in 
corpus, is the frequency of term i in corpus. 
So,
1
1
log( )
log( )
N
ij ij
j i i
f f
N n n 
? ?
? ?
? ?
?
is the average uncertainty or entropy of term i.
Entropy weighting is based on information 
theoretic ideas and is the most sophisticated
weighting scheme.
4.1.2 Features Selection
In this Section, we give a brief introduction on 
two effective unsupervised feature selection 
methods, DF and global tf-idf.
DF (Document frequency) is the number of 
documents in which a term occurs in a dataset. It 
is the simplest criterion for term selection and 
easily scales to a large dataset with linear
computation complexity. It is a simple but 
effective feature selection method for text 
categorization (Yang & Pedersen, 1997).
We introduce a new feature selection method 
called ?global tf-idf? that takes the term weight 
into account. Because DF assumes that each 
term is of same importance in different 
documents, it is easily biased by those common 
terms which have high document frequency but 
uniform distribution over different classes.
Global tf-idf is proposed to deal with this 
problem:
1
N
i ik
k
g tfidf
 
 ?
4.1.3 Latent Dirichlet Allocation (LDA)
Our work is related to Latent Dirichlet 
Allocation (LDA, Blei et al 2003), a 
probabilistic generative model of text generation. 
LDA models each document using a mixture 
over K topics, which are in turn characterized as 
distributions over words. The main motivation is 
that the task, fail to achieve high accuracy due to 
the data sparseness.
LDA is a generative graphical model as 
shown in Figure 1. It can be used to model and 
discover underlying topic structures of any kind 
of discrete data in which text is a typical 
example. LDA was developed based on an 
assumption of document generation process 
depicted in both Figure 1 and Table 1.
Figure 1 Generation Process for LDA
4.1.4 LDA Estimation with Gibbs Sampling
Estimating parameters for LDA by directly and 
exactly maximizing the likelihood of the whole 
data collection is intractable. The solution to this 
is to use approximate estimation methods like 
Gibbs Sampling (Griffiths and Steyvers, 2004).
Here, we only show the most important 
formula that is used for topic sampling for words.
$IWHUILQLVKLQJ*LEEV6DPSOLQJWZRPDWULFHV?
DQG?DUH computed as follows.
where ? is the latent topic distribution 
corresponding to real world people.
4.1.5 Topic-based Features
Through the observation for the given corpus, 
many key information, like occupation,
affiliation, mentor, location, and so on, in many 
cases, around the target name. So, both local and 
global context are choose to doing topic analysis.
Finally, the latent topic distributions are topic-
based representation of context.
4.2 Clustering
Our system trusts the result of Person Name
detection absolutely, so contexts need to do 
clustering only if they refer to persons with the 
same name. We experimented with two different 
classical clustering methods: HAC and 
DBSCAN.
4.2.1 HAC
At the heart of hierarchical clustering lies the 
definition of similarity between clusters, which 
based on similarity between individual 
documents. In my system, a linear combination 
of similarity based on both local and global 
context is employed:
(1 )global localsim sim simD D ?  
where, the general similarity between two 
features-vector of documents di and dj is 
defined as the cosine similarity:
( , ) i ji j
i j
d d
sim d d
d d
 
<
We will now refine this algorithm for the 
different similarity measures of single-link,
complete-link, group-average and centroid 
clustering when clustering two smaller clusters 
together. In our approach we used an overall
similarity stopping threshold.
4.2.2 DBSCAN
In this section, we present the algorithm 
DBSCAN (Density Based Spatial Clustering of 
Applications with Noise) (Ester et al, 1996) 
(Table 2) which is designed to discover the 
clusters and the noise in a spatial database.
Table 2 Algorithm of DBSCAN
Arbitrary select a point p
Retrieve all points density-reachable from p wrt
Eps and MinPts.
If p is a core point, a cluster is formed.
If p is a border point, no points are density-
reachable from p and DBSCAN visits the 
next point of the database.
Continue the process until all of the points
5 Experiments and Results Analysis
We run all experiments on SIGHAN 2010 
training and test corpus. 
5.1 Preprocessing and Person Name 
Recognition
Firstly, a word segmentation tool based on CRF 
is used in each document. Then, person name 
recognition is processing. The training data for
word segmentation and PNR is People's Daily in 
January, 1998 and the whole 2000, respectively.
5.2 Feature Space
Our experiments used five types of feature (uni-
char, bi-char, word and topic in local and global), 
two feature weighting methods (tf-idf and 
entropy) and two feature selection methods (DF 
and global tf-idf).
5.3 Model Selection in LDA
Our model is conditioned on the Dirichlet 
hyperparameters D andE , the number of topic
K and iterations. The value for the D was set to 
0.2, which was optimized in tuning experiment 
used training datasets. The E was set to 0.1,
which is often considered optimal in LDA-
related models (Griffiths and Steyvers, 2004). 
The K was set to 200. The Gibbs sampler was 
run for 1,000 iterations.
5.4 Clustering Results and Analysis
Since the parameter setting for the clustering 
system is very important, we focus only on the 
B-cubed scoring (Artiles et al, 2009), and 
acquire an overall optimal fixed stop-threshold 
from the training data, and then use it in test data. 
In this section, we report our results evaluated 
by the clustering scoring provided by SIGNAN 
2010 evaluation, which includes both the B-
cubed scoring and the purity-based scoring.
Table 3 and 4 demonstrate the performance (F 
scores) of our system in different features 
representation and clustering for the training 
data of the SIGNAN 2010. In Table 3, the 
numbers in parentheses are MinPts and Eps 
respectively, and stop-threshold in Table 4. As 
shown in Table 3, DBSCAN isn?t suitable for 
this task, and the results are very sensitive to
parameters. So we didn?t submit DBSCAN-
based results.
Table 4 shows that the best averaged F-scores
for PND are based on topic model, which meet 
our initial assumptions, and result based on 
merging local and global information is a bit 
better than both local and global information
independently. Also, the results based on topic 
model are the most robust because the F-score of 
variation is slightly with stop-threshold changing.
Conversely, the results based on token are not 
like this. As the performance of segmentation is 
not very satisfactory, results based on word are 
worst, even worse than uni-char-based. In 
addition, it is found that global tf-idf is better 
than DF, which is the simplest unsupervised 
feature selection method. Entropy weighting is
more effective than tf-idf weighting.
Table 5 shows that the evaluation results in 
test data on SIGHAN 2010, and the last two 
lines are results in diagnosis test. We are in fifth 
place. The evaluation results (F-score) of Person 
Name Recognition in training data is 0.965.
Features FS Weighting B-Cubed P-IP
precision recall F P IP F
word (0.19)
DF
tf-idf
79.05 79.68 76.49 83.25 85.84 82.72
word (0.2) 80.99 75.72 75.54 84.67 83.08 82.2
word (0.3) entropy 78.8 80.71 77.42 83.13 86.62 83.58
word (0.25)
global tf-idf tf-idf
80.79 83.1 80.53 84.88 88.32 85.79
word (0.23) 79.45 84.49 79.66 83.76 89.25 85.08
uni-char (0.43)
DF tf-idf
76.47 85.46 78.77 81.7 90.05 84.45
uni-char (0.5) 82.34 75.97 77 86.11 83.54 83.78
uni-char (0.48) 80.42 79.44 78.01 84.53 86.17 84.26
bi-char (0.35) 88.3 67.75 75.34 89.96 77.38 82.44
bi-char (0.315) 81.84 81.58 80.54 85.72 87.17 85.8
local topic (0.6) 78.76 86.8 80.63 83.27 91.16 85.88
global topic (0.4) 77.92 88.72 81.04 82.67 92.64 86.26
global topic (0.7) 80.54 88.43 83.55 84.76 92.55 88.02
merged topic (0.63) 81.39 87.82 83.88 85.42 91.94 88.21
Table 3    Performance of HAC
B-Cubed P-IP
MinPts 
and Eps
precision recall F P IP F
2  0.9 64.15 95.84 74.19 71.95 97.36 80.97
2  0.4 71.34 62.25 63.95 76.56 71.94 72.59
3  0.9 64.15 95.88 74.2 71.95 97.37 80.97
6  0.95 64.12 96.55 74.44 71.92 97.79 81.12
B-Cubed P-IP
precision recall F P IP F
80.33 94.52 85.79 85.1 96.46 89.77
80.56 92.56 85.29 85.34 95.19 89.5
80.43 95.41 86.18 85.07 97.06 89.96
80.82 93.41 85.77 85.62 95.76 89.91
Table 5   Evaluation Results in test data
Table 4    Performance of DBSCAN
6 Discussion and Future Work
In this paper, we present implementation of our 
systems for SIGHAN-2010 PND bekeoff,.The 
experiments show that the topic features learned 
by LDA outperform token features and exhibit 
good robustness.
However, in our system, only given data is 
exploited. We are going to collect a very large 
external data as universal dataset to train topic 
model, and then do clustering on both a small set 
of training data and a rich set of hidden topics 
discovered from universal dataset. The universal 
dataset can be snippets returned by search 
engine or Wikipedia queried by target name and 
some keywords, and so on.
We built our PDN system on the result of 
person name recognition. However, it is not 
appropriate to toally trust the result of Person 
Name detection. So an algorithm that can correct 
NER mistakes should be investigated in future 
work..
Moreover, Cluster Ensemble system can 
ensure the result to be more robust and accurate 
accordingly, which is another direction of future 
work..
Acknowledgments
This research has been partially supported by the 
National Science Foundation of China (NO. 
NSFC90920006). We also thank Xiaojie Wang,
Caixia Yuan and Huixing Jiang for useful 
discussion of this work.
References
Spink, B. Jansen, and J. Pedersen. 2004.
Searching for people on web search engines. 
Journal of Documentation, 60:266 -278.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 
2009. Weps 2 evaluation campaign: overview 
of the web people search clustering task. In 
WePS 2 Evaluation Workshop. WWW 
Conference.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 
2007. The semeval-2007 weps evaluation: 
Establishing a benchmark for the web people 
search task. In Proceedings of the Fourth 
International Workshop on Semantic 
Evaluations (SemEval-2007).ACL.
M. Ikeda, S. Ono, I. Sato, M. Yoshida, and H. 
Nakagawa. 2009. Person name 
disambiguation on the web by twostage 
clustering. In 2nd Web People Search 
Evaluation Workshop (WePS 2009), 18th 
WWW Conference.
L. Romano, K. Buza, C. Giuliano, and L. 
Schmidt-Thieme. 2009. Person name 
disambiguation on the web by twostage 
clustering. In 2nd Web People Search 
Evaluation Workshop (WePS 2009), 18th 
WWW Conference.
Y. Chen, S. Y. M. Lee, and C.-R. Huang. 2009.
Polyuhk: A robust information extraction 
system for web personal names. In 2nd Web 
People Search Evaluation Workshop (WePS 
2009), 18th WWW Conference.
Z. Kozareva, R. Moraliyski, and G. Dias. 2008.
Web people search with domain ranking. In 
TSD '08: Proceedings of the 11th 
international conference on Text, Speech and
Dialogue, 133-140, Berlin, Heidelberg.
Pedersen, Ted, Amruta Purandare, and Anagha 
Kulkarni. 2005. Name Discrimination by 
Clustering Similar Contexts. In Proceedings 
of the Sixth International Conference on
Intelligent Text Processing and 
Computational Linguistics, Mexico City, 
Mexico.
G. Cleuziou, L. Martin, and C. Vrain. 2004.
Poboc: an overlapping clustering algorithm.
application to rule-based classification and 
textual data, 440-444.
Kalmar, Paul and Matthias Blume. 2007. FICO: 
Web Person Disambiguation Via Weighted 
Similarity of Entity Contexts. In Proceedings 
of the Fourth International Workshop on 
Semantic Evaluations (SemEval-2007).ACL.
Rao, Delip, Nikesh Garera and David Yarowsky. 
2007. JHU1 : An Unsupervised Approach to 
Person Name Disambiguation using Web 
Snippets. In Proceedings of the Fourth 
International Workshop on Semantic 
Evaluations (SemEval-2007).ACL.
Pedersen, Ted and Anagha Kulkarni. 2007.
Unsupervised Discrimination of Person 
Names in Web Contexts. In Proceedings of 
the Eighth International Conference on
Intelligent Text Processing and 
Computational Linguistics, Mexico City, 
Mexico.
Phan, X., Nguyen, L. and Horiguchi. 2008.
Learning to Classify Short and Sparse Test & 
Web with Hidden Topics from large-scale 
Data collection. In Proceedings of 17th
International World Wide Web Conference. 
(Beijing, China, April 21-25, 2008). ACM 
Press, New York, NY, 91-100.
Samuel Brody and Mirella Lapata. 2009.
Bayesian word sense induction In 
Proceedings of the 12th Conference of the 
European Chapter of the Association for 
Computational Linguistics, 103-111.
Sun et al 1995. Identifying Chinese Names in 
Unrestricted Texts (in Chinese). In Journal of 
Chinese Information Processing, 9(2):16-27.
Liu et al 2000. Statistical Chinese Person 
Names Identification (in Chinese). In Journal 
of Chinese Information Processing, 14(3):16-
24.
Huang et al 2001. Identification of Chinese 
Names Based on Statistics (in Chinese). In
Journal of Chinese Information Processing,
15(2):31-37.
Li et al 2006. Chinese Name Recognition Based 
on Boundary Templates and Local Frequency
(in Chinese). In Journal of Chinese 
Information Processing, 20(5):44-50.
Mao et al 2007. Recognizing Chinese Person 
Names Based on Hybrid Models (in Chinese).
In Journal of Chinese Information Processing,
21(2):22-27.
J. Lafferty, A. McCallum, and F. Pereira. 2001. 
Conditional random fields: Probabilistic 
models for segmenting and labeling sequence
data. In Proc. ICML-01, 282-289, 2001.
Wallach, Hanna. 2004. Conditional random 
fields: An introduction. Technical report,
University of Pennsylvania, Department of 
Computer and Information Science.
Yang, Y. and Pedersen, J. O. 1997. A 
comparative study on feature selection in text
categorization. In Proceedings of ICML-97, 
14th International Conference on Machine
Learning (Nashville, US, 1997), 412?420.
Martin Ester, Hans-Peter Kriegel, J?rg Sander, 
Xiaowei Xu. 1996. A density-based algorithm 
for discovering clusters in large spatial 
databases with noise. In Proceedings of the 
Second International Conference on 
Knowledge Discovery and Data Mining 
(KDD-96). AAAI Press. 226-231.
Blei, David M., Andrew Y. Ng, and Michael I. 
Jordan. 2003. Latent dirichlet alocation. In J. 
Machine Learn. Res. 3, 993-1022.
T. Griffiths and M. Steyvers. 2004. Finding 
scientific topics. In The National Academy of 
Sciences, 101:5228-5235.
