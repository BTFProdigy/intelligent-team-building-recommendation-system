Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20?29,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Minimum Translation Modeling with Recurrent Neural Networks
Yuening Hu
Department of Computer Science
University of Maryland, College Park
ynhu@cs.umd.edu
Michael Auli, Qin Gao, Jianfeng Gao
Microsoft Research
Redmond, WA, USA
{michael.auli,qigao,jfgao}@microsoft.com
Abstract
We introduce recurrent neural network-
based Minimum Translation Unit (MTU)
models which make predictions based on
an unbounded history of previous bilin-
gual contexts. Traditional back-off n-gram
models suffer under the sparse nature of
MTUs which makes estimation of high-
order sequence models challenging. We
tackle the sparsity problem by modeling
MTUs both as bags-of-words and as a
sequence of individual source and target
words. Our best results improve the out-
put of a phrase-based statistical machine
translation system trained on WMT 2012
French-English data by up to 1.5 BLEU,
and we outperform the traditional n-gram
based MTU approach by up to 0.8 BLEU.
1 Introduction
Classical phrase-based translation models rely
heavily on the language model and the re-
ordering model to capture dependencies between
phrases. Sequence models over Minimum Trans-
lation Units (MTUs) have been shown to com-
plement both syntax-based (Quirk and Menezes,
2006) as well as phrase-based (Zhang et al., 2013)
models by explicitly modeling relationships be-
tween phrases. MTU models have been tradi-
tionally estimated using standard back-off n-gram
techniques (Quirk and Menezes, 2006; Crego and
Yvon, 2010; Zhang et al., 2013), similar to word-
based language models (?2).
However, the estimation of higher-order n-gram
models becomes increasingly difficult due to data
sparsity issues associated with large n-grams, even
when training on over one hundred billion words
(Heafield et al., 2013); bilingual units are much
sparser than words and are therefore even harder
to estimate. Another drawback of n-gram mod-
els is that future predictions are based on a limited
amount of previous context that is often not suf-
ficient to capture important aspects of human lan-
guage (Rastrow et al., 2012).
Recently, several feed-forward neural network-
based models have achieved impressive improve-
ments over traditional back-off n-gram models in
language modeling (Bengio et al., 2003; Schwenk
et al., 2007; Schwenk et al., 2012; Vaswani et al.,
2013), as well as translation modeling (Allauzen et
al., 2011; Le et al., 2012; Gao et al., 2013). These
models tackle the data sparsity problem by rep-
resenting words in continuous space rather than
as discrete units. Similar words are grouped in
the same sub-space rather than being treated as
separate entities. Neural network models can be
seen as functions over continuous representations
exploiting the similarity between words, thereby
making the estimation of probabilities over higher-
order n-grams easier.
However, feed-forward networks do not directly
address the limited context issue either, since pre-
dictions are based on a fixed-size context, similar
to back-off n-gram models. We therefore focus
in this paper on recurrent neural network architec-
tures, which address the limited context issue by
basing predictions on an unbounded history of pre-
vious events which allows to capture long-span de-
pendencies. Recurrent architectures have recently
advanced the state of the art in language model-
ing (Mikolov et al., 2010; Mikolov et al., 2011a;
Mikolov, 2012) outperforming multi-layer feed-
forward based networks in perplexity and word er-
ror rate for speech recognition (Arisoy et al., 2012;
Sundermeyer et al., 2013). Recent work has also
shown successful applications to machine transla-
tion (Mikolov, 2012; Auli et al., 2013; Kalchbren-
ner and Blunsom, 2013). We extend this work by
modeling Minimum Translation Units with recur-
rent neural networks.
Specifically, we introduce two recurrent neu-
ral network-based MTU models to address the is-
20
M1 M2 M3 M4 M5
Yu        ZuoTian JuXing Le HuiTan
held
=> null
=> Yesterday
=> held
=> the
=> meeting
? ?? ?? ? ??
Yu
ZuoTian
JuXing_Le
null
HuiTan
null        
the meeting null yesterday
M1: 
M2: 
M3: 
M4: 
M5: 
Figure 1: Example Minimum Translation Unit
partitioning based on Zhang et al. (2013).
sues regarding data sparsity and limited context
sizes by leveraging continuous representations and
the unbounded history of the recurrent architec-
ture. Our first approach frames the problem as a
sequence modeling task over minimal units (?3).
The second model improves over the first by mod-
eling an MTU as a bag-of-words, thereby allow-
ing us to learn representations over sub-structures
of minimal units that are shared across MTUs
(?4). Our models significantly outperform the tra-
ditional back-off n-gram based approach and we
show that they act complementary to a very strong
recurrent neural network-based language model
based solely on target words (?5).
2 Minimum Translation Units
Banchs et al. (2005) introduced the idea of framing
translation as a sequence modeling problem where
a sentence pair is generated in left-to-right order as
a sequence of bilingual n-grams. Minimum Trans-
lation Units (Quirk and Menezes, 2006; Zhang
et al., 2013) are an extension which additionally
permit tuples with empty source or target sides,
thereby allowing insertion or deletion phrase pairs.
The two basic requirements for MTUs are that
there are no overlapping word alignment links be-
tween phrase pairs and it should not be possible to
extract smaller phrase pairs without violating the
word alignment constraints. Informally, we can
think of MTUs as small phrase pairs that cannot
be broken down any further without violating the
two requirements.
Minimum Translation Units partition a sentence
pair into a set of minimal bilingual units or tu-
Words MTUs
Tokens 34,769,416 14,853,062
Types 143,524 1,315,512
Singleton types 34.9% 80.1%
Table 1: Token and type counts for both source
and target words as well as MTUs based on the
WMT 2006 German to English data set (cf. ?5).
ples obtained by an algorithm similar to phrase-
extraction (Koehn et al., 2003). Figure 1 illus-
trates such a partitioning. Modeling minimal units
has two advantages over considering larger phrase
pairs that are effectively composed of MTUs:
First, minimal units result in a unique partition-
ing of a sentence pair. This has the advantage that
we avoid modeling spurious derivations, that is,
multiple derivations generating the same sentence
pair. Second, minimal units result in smaller mod-
els with a smoother distribution than models based
on composed units (Zhang et al., 2013).
Sentence pairs can be generated in multiple or-
ders, such as left-to-right or right-to-left, either in
source or target order. For example, the source
left-to-right order of the sentence pair in Figure 1
is simply M1, M2, M3, M4, M5, while the tar-
get left-to-right order is M3, M4, M5, M1, M2.
We deal with inserted or deleted words similar to
Zhang et al. (2013): The source side null token of
an inserted target phrase is placed next to the last
source word aligned to the closest preceding non-
null aligned target phrase; a similar rule is applied
to null tokens on the target side. For example, in
Figure 1 we place M4 straight after M3 because
?the?, the aligned target phrase, is after ?held?, the
previous non-null aligned target phrase.
We can straightforwardly estimate an n-gram
model over MTUs to estimate the probability
of a sentence pair using standard back-off tech-
niques commonly employed in language mod-
eling. For example, a trigram model in tar-
get left-to-right order factors the sentence pair in
Figure 1 as p(M3) p(M4|M3) p(M5|M3,M4)
p(M1|M4,M5)p(M2|M5,M1).
If we would like to model larger contexts, then
we quickly run into data sparsity issues. To illus-
trate this point, consider the parameter growth of
an n-gram model which is driven by the vocabu-
lary size |V | and the n-gram order n: O(|V |
n
).
Clearly, the exact estimation of higher-order n-
21
gram probabilities becomes more difficult with
large n, leading to the estimation of events with
increasingly sparse statistics, or having to rely
on statistics from lower-order events with back-
off models, which is less desirable. Even word-
based language models rarely ventured so far
much beyond 5-gram statistics as demonstrated
by Heafield et al. (2013) who trained a, by to-
day?s standards, very large 5-gram model on 130B
words. Data sparsity is therefore an even more sig-
nificant issue for MTU models relying on much
larger vocabularies. In our setting, the MTU vo-
cabulary is an order of magnitude larger than a
word vocabulary obtained from the same data (Ta-
ble 1). Furthermore, most MTUs are observed
only once making the reliable estimation of prob-
abilities very challenging.
Neural network-based sequence models tackle
the data sparsity problem by learning continuous
word representations, that group similar words to-
gether in continuous space. For example, the
distributional representations induced by recurrent
neural networks have been found to have interest-
ing syntactic and semantic regularities (Mikolov
et al., 2013). Furthermore, these representations
can be exploited to estimate more reliable statis-
tics over higher-order n-grams than with discrete
word units. Recurrent neural networks go beyond
fixed-size contexts and allow the model to keep
track of long-span dependencies that are important
for future predictions. In the next sections we will
present Minimum Translation Unit models based
on recurrent architectures.
3 Atomic MTU RNN Model
The first model we introduce is based on the recur-
rent neural network language model of Mikolov
et al. (2010). We frame the problem as a tradi-
tional sequence modeling task which treats MTUs
as atomic units, similar to the approach taken by
the traditional back-off n-gram models.
The model is factored into an input layer, a hid-
den layer with recurrent connections, and an out-
put layer (Figure 2). The input layer encodes the
MTU at time t as a 1-of-N vector m
t
with all val-
ues being zero except for the entry representing
the MTU. The output layer y
t
represents a proba-
bility distribution over possible next MTUs; both
the input and output layers are of size |V |, the size
of the MTU vocabulary. The hidden layer state h
t
encodes the history of all MTUs observed in the
mt
ht-1
ht
yt
V
W
U
0
0
1
0
0
0
0
0
Figure 2: Structure of the atomic recurrent neu-
ral network MTU model following the word-based
RNN model of Mikolov (2012).
sequence up to time step t.
The state of the hidden layer is determined by
the input layer and the hidden layer configuration
of the previous time step h
t?1
. The weights of the
connections between the layers are summarized in
a number of matrices: U represents weights from
the input layer to the hidden layer, and W repre-
sents connections from the previous hidden layer
to the current hidden layer. Matrix V contains
weights between the current hidden layer and the
output layer.
The hidden and output layers are computed
via a series of matrix-vector products and non-
linearities:
h
t
= s(Um
t
+Wh
t?1
)
y
t
= g(Vh
t
)
where
s(z) =
1
1 + exp {?z}
, g(z
m
) =
exp {z
m
}
?
k
exp {z
k
}
are sigmoid and softmax functions, respectively.
Additionally, the network is interpolated with a
maximum entropy model of sparse n-gram fea-
tures over input MTUs (Mikolov et al., 2011a).
The maximum entropy weights D are added to
the output activations before applying the softmax
function and are estimated jointly with all other
parameters (Figure 3).
1
1
While these features depend on multiple input MTUs, we
22
mt
ht-1
ht
yt
V
W
U
0
0
1
0
0
0
0
0
T
D
c t
Figure 3: Structure of atomic recurrent neural net-
work MTU model with classing layer c
t
and direct
connections D between the input and output lay-
ers (cf. Figure 2).
The model is optimized via a maximum likeli-
hood objective function using stochastic gradient
descent. Training is based on the truncated back
propagation through time algorithm, which unrolls
the network and then computes error gradients
over multiple time steps (Rumelhart et al., 1986);
we use a cross entropy criterion to obtain the error
vector with respect to the output activations and
the desired prediction. After training, the output
layer represents posteriors p(m
t+1
|m
t
t?n+1
,h
t
),
the probability of the next MTU given the previ-
ous n input MTUs m
t
t?n+1
= m
t
, . . . ,m
t?n+1
and the current hidden layer configuration h
t
.
Na??ve computation of the probability distribu-
tion over the next MTU is very expensive for large
vocabularies, such as commonly encountered for
MTU models (Table 1). A well established ef-
ficiency trick assigns each possible output to a
unique class and then uses a two-step process to
find the probability of an MTU, instead of comput-
ing the probability of all possible outputs (Good-
man, 2001; Emami and Jelinek, 2005; Mikolov et
al., 2011b). Under this scheme we compute the
probability of an MTU by multiplying the prob-
ability of its class c
i
t
with the probability of the
depicted them for simplicity as a connection between the
current input vectorm
t
and the output layer.
minimal unit conditioned on the class:
p(m
t+1
|m
t
t?n+1
,h
t
) =
p(c
i
t
|m
t
t?n+1
,h
t
) p(m
t+1
|c
i
t
,m
t
t?n+1
,h
t
)
This factorization reduces the complexity of com-
puting the output probabilities from O(|V |) to
O(|C| + max
i
|c
i
|) where |C| is the number of
classes and |c
i
| is the number of minimal units
in class c
i
. The best case complexity O(
?
|V |)
requires the number of classes and MTUs to be
evenly balanced, i.e., each class contains exactly
as many minimal units as there are classes.
Figure 3 illustrates how classing changes the
structure of the network by adding an additional
output layer for the class probabilities.
4 Bag-of-words MTU RNN Model
The previous model treats MTUs as atomic sym-
bols which leads to large vocabularies requir-
ing large parameter sets and expensive inference.
However, similar MTUs may share the same
words, or words which are related in continuous
space. The atomic MTU model does not exploit
this since it cannot access the internal structure of
a minimal unit.
The approach we pursue next is to break MTUs
into individual source and target words (Le et al.,
2012) in order to exploit structural similarities be-
tween infrequently observed minimal units. Sin-
gletons represent the vast majority of our MTU
vocabulary (Table 1). This resembles the word-
hashing trick of Huang et al. (2013) who repre-
sented individual words as a bag-of-character n-
grams to reduce the vocabulary size of a neural
network-based model in an information retrieval
setting.
2
We first describe a theoretically appealing but
computationally expensive model and then discuss
a more practical variation. The input layer of this
model accepts the current minimal unit as a K-of-
N vector representing K source and target words
as opposed to the 1-of-N encoding of entire MTUs
in the previous model (Figure 4). Larger MTUs
may contain the same word more than once and we
simply adjust their count to one.
3
Different to the
2
Applying the same technique would likely result in too many
collisions since we are dealing with multi-word units instead
of single words.
3
We found no effect on accuracy when using the unmodified
count in initial experiments.
23
x t
ht-1
ht
w t
V
W
U
1
0
1
1
0
1
0
0
D yt
C
. . .
. . .
. . .
sr c
tgt
MT U
Figure 4: Structure of MTU bag-of-words recur-
rent neural network model. The input layer rep-
resents a minimal unit as a bag-of-words and the
output layer y
t
is a probability distribution over
possible next MTUs depending on the activations
of the word layer w
t
representing source and tar-
get words of minimal units.
previous model, the input vector has now multiple
active entries whose signals are absorbed into the
new hidden layer configuration.
This bag-of-words encoding of minimal units
dramatically reduces the vocabulary size but it in-
evitably maps different MTUs to the same encod-
ing. On our data set, we observe less than 0.2% of
minimal units that are involved in collisions, a rate
that is similar to Huang et al. (2013). In practice
collisions are unlikely to affect accuracy in our set-
ting because MTUs that are mapped to the same
encoding usually do not differ much in semantic
meaning as illustrated by the following examples:
erfolg haben ? succeed collides with haben er-
folg? succeed, or damit ,? to and , damit? to;
in both examples either the auxiliary verb haben or
the comma changes position, neither of which sig-
nificantly changes the meaning for this particular
pair of MTUs.
The structure of the bag-of-words MTU RNN
models is shown in Figure 4. Similar to the atomic
MTU RNN model (?3), the hidden layer combines
the signal from the input layer and the previous
hidden layer configuration. The hidden layer acti-
vations feed into a word layer w
t
representing the
source and target words that part of all possible
MTUs; it is of the same size as the input layer. The
word layer is connected to a convolutional out-
put layer y
t
by weights summarized in the sparse
matrix C. The output layer represents all possi-
ble next minimal units, where each MTU entry is
only connected to neurons in the word layer repre-
senting its source and target words. The word and
MTU layers are then computed as follows:
w
t
= s(Vh
t
)
y
t
= g(Cw
t
)
However, there are a number of computational
issues with this model: First, we cannot efficiently
factor the word layer w
t
into classes such as for
the atomic MTU RNN model because we require
all its activations to compute the MTU output
layer y
t
. This reduces the best case complex-
ity of computing the word layer from O(
?
|V |)
back to linear in the number of source and tar-
get words |V |. In practice this results in between
200-1000 more activations that need to be com-
puted, depending on the word vocabulary size.
Second, turning the MTU output layer into a con-
volutional layer is not enough to sufficiently re-
duce the computational effort to compute the out-
put activations since the number of connections
between the word and MTU layers is very imbal-
anced. This is because frequent words, such as
function words, are part of many MTUs and there-
fore have a very high out-degree, e.g., the neuron
representing ?the? has over 82K outgoing edges.
On the other hand, infrequent words, have a very
low out-degree. This imbalance makes it hard
to efficiently compute activations and error gradi-
ents, even on a GPU, since some neurons require
substantially more work than others.
4
For these reasons we decided to design a sim-
pler, more tractable version of this model (Fig-
ure 5). The simplified model still represents an
input MTU as a bag-of-words but minimal units
are generated word-by-word, first emitting source
words and then target words. This is in contrast
to the original model which predicted an MTU as
a single unit. Decomposing the next MTU into
individual words dramatically reduces the size of
the output layer, thereby resulting in faster com-
putation of the outputs and making normalization
4
In initial experiments we found this model to be over twenty
times slower than the atomic MTU RNN model with esti-
mated training times of over 6 weeks. This was despite us-
ing a vastly smaller vocabulary and by computing the word
layer on a, by current standards, high-end GPU (NVIDIA
Tesla K20c) using sparse matrix optimizations (cuSPARSE)
for the convolutional layer.
24
mt
ht-1
ht
yt
V
W
U
1
0
1
1
0
1
0
0
T
D
c t
mt+ 1
sr c
tgt
MT U
Figure 5: Simplified MTU bag-of-words recurrent
neural network model (cf. Figure 4). An MTU is
input as bag-of-words and the next MTU is pre-
dicted as a sequence of both source and target
words.
into probabilities easier. Furthermore, the output
layer can be factorized into classes requiring only
a fraction of the neurons to be computed, a much
more efficient solution compared to the original
model which required calculation of the entire out-
put layer.
The simplified model computes the probability
of the next MTU m
t+1
as a product of individual
word probabilities:
p(m
t+1
|m
t
t?n+1
,h
t
) = (1)
?
a
1
,...,a
u
?m
t+1
p(c
k
|m
t
t?n+1
,h
t
)
p(a
k
|c
k
,m
t
t?n+1
,h
t
)
where we predict a sequence of source and target
words a
1
, . . . , a
u
? m
t+1
with a class-structured
output layer, similar to the atomic model (?3).
Training still uses a cross entropy criterion and
back propagation through time, however, error
vectors are computed on a per-word basis, instead
of a per-MTU basis. Direct connections between
the input and output layers are based on source and
target words which is less sparse than basing direct
features on entire MTUs such as for the original
bag-of-words model.
Overall, the simplified model retains the bag-of-
words input representation of the original model,
while permitting the efficient factorization of the
word-output layer into classes.
5 Experiments
We evaluate the effectiveness of both the atomic
MTU RNN model (?3) and the simplified bag-of-
words MTU RNN model (?4) in an n-best rescor-
ing setting, comparing against a trigram back-off
MTU model as well as the phrasal decoder 1-best
output which we denote as the baseline.
5.1 Experimental Setup
Baselines. We experiment with an in-house
phrase-based system similar to Moses (Koehn et
al., 2007), scoring translations by a set of common
features including maximum likelihood estimates
of source given target mappings p
MLE
(e|f) and
vice versa p
MLE
(f |e), as well as lexical weight-
ing estimates p
LW
(e|f) and p
LW
(f |e), word and
phrase-penalties, a linear distortion feature and
a lexicalized reordering feature. The baseline
includes a standard modified Kneser-Ney word-
based language model trained on the target-side of
the parallel corpora described below. Log-linear
weights are estimated with minimum error rate
training (MERT; Och, 2003).
The 1-best output by the phrase-based decoder
is the baseline accuracy. As a second baseline we
experiment with a trigram back-off MTU model
trained on all extracted MTUs, denoted as n-gram
MTU. The trigram MTU model is estimated with
the same modified Kneser-Ney framework as the
target side language model. All MTU models are
trained in target left-to-right MTU order which
performed well in initial experiments.
Evaluation. We test our approach on two differ-
ent data sets. First, we train a German to English
system based on the data of the WMT 2006 shared
task (Koehn and Monz, 2006). The parallel corpus
includes about 35M words of parliamentary pro-
ceedings for training, a development set and two
test sets with 2000 sentences each.
Second, we experiment with a French to En-
glish system based on 102M words of training data
from the WMT 2012 campaign. The majority of
the training data set is parliamentary proceedings
except for about 5m words which are newswire; all
MTU models are trained on the newswire subset
since we found similar accuracy to using all data in
initial experiments. We evaluate on four newswire
domain test sets from 2008, 2010 and 2011 as well
as the 2010 system combination test set contain-
ing between 2034 to 3003 sentences. Log-linear
weights are estimated on the 2009 data set com-
25
prising 2525 sentences. We evaluate all systems
in a single reference BLEU setting.
Rescoring Setup. We rescore the 1000-best out-
put of the baseline phrase-based decoder by ei-
ther the trigram back-off MTU model or the
RNN models. The baseline accuracy is obtained
by choosing the 1-best decoder output. We re-
estimate the log-linear weights for rescoring by
running a further iteration of MERT with the ad-
ditional feature values; we initialize the rescoring
feature weight to zero and try 20 random restarts.
At test time we use the new set of log-linear
weights to rescore the test set n-best list.
Neural Network Setup. We trained the recur-
rent neural network models on between 88% and
93% of each data set and used the remainder as
validation data. The vocabulary of the atomic
MTU RNN model is comprised of all MTU types
which were observed more than once in the train-
ing data.
5
Similarly, we modeled all non-singleton
words for the bag-of-words MTU RNN model.
We obtain classes for words or MTUs using a
version of Brown-Clustering with an additional
regularization term to optimize the runtime of
the language model (Brown et al., 1992; Zweig
and Makarychev, 2013). Direct connections use
features over unigrams, bigrams and trigrams of
words or MTUs, depending on the model. Fea-
tures are hashed to a table with at most 500 million
values following Mikolov et al. (2011a). We use
the standard settings for the model with the default
learning rate ? = 0.1 that decays exponentially if
the validation set entropy does not decrease. Back
propagation through time computes error gradi-
ents over the past twenty time steps. Training
is stopped after 20 epochs or when the valida-
tion entropy does not decrease over two epochs.
Throughout, we use a hidden layer size of 100
which provided a good trade-off between time and
accuracy in initial experiments.
5.2 Results
We first report the decoder 1-best output as the
first baseline and then rescore our two data sets
(Table 2 and Table 3) with the n-gram back-off
MTU model to establish a second baseline (n-
gram MTU). The n-gram model improves by 0.4
BLEU over the decoder 1-best on all test sets for
German to English. On French-English accuracy
5
We tried modeling all MTUs which did not contain a single-
ton word but observed no significant effect on accuracy.
dev test1 test2
Baseline 25.8 26.0 26.0
n-gram MTU 26.3 26.6 26.4
atomic MTU RNN 26.5 26.8 26.5
BoW MTU RNN 26.5 27.0 26.9
word RNNLM 26.5 27.1 26.8
Combined 26.8 27.3 27.1
Table 2: German to English BLEU results for
the decoder 1-best output (Baseline) compared to
rescoring with a target left-to-right trigram MTU
model (n-gram MTU), our two recurrent neural
network-based MTU models, a word-based RNN-
based language model (word RNNLM), as well
as a combination of the three RNN-based models
(Combined).
improves on three out of five sets by up to 0.7
BLEU.
Next, we evaluate the accuracy of the MTU
RNN models. The atomic MTU RNN model im-
proves over the n-gram MTU model on all test sets
for German to English, however, for French to En-
glish the back-off model performs better on two
out of four test sets.
The next question we answer is if breaking
MTUs into individual units to leverage similarities
in the internal structure can help accuracy. The re-
sults (Table 2 and Table 3) for the bag-of-words
model (BoW MTU RNN) clearly show that this is
the case for both language pairs. We significantly
improve over the n-gram MTU model as well as
the atomic RNN model on all test sets. We observe
gains of up to 0.5 BLEU over the n-gram MTU
model for German to English as well as French to
English; improvements over the decoder baseline
are up to 1.2 BLEU for French to English.
How do our models compare to other neural net-
work approaches that rely only on target side in-
formation? To answer this question we compare
to the strong language model of Mikolov (2012;
RNNLM) which has recently improved the state-
of-the-art in language modeling perplexity. The
results (Table 2 and Table 3) show that RNNLM
performs competitively. However, our approaches
model translation since we use both source and tar-
get information as opposed to scoring only the flu-
ency of the target side, such as done by RNNLM.
Can our models act complementary to a strong
RNN language model? Our final experiment com-
bines the atomic MTU RNN model, the BoW
26
dev news2008 news2010 news2011 newssyscomb2010
Baseline 24.3 20.5 24.4 25.1 24.3
n-gram MTU 24.6 20.8 24.4 25.8 24.3
atomic MTU RNN 24.6 20.7 24.4 25.5 24.3
BoW MTU RNN 25.2 21.2 24.8 26.3 24.6
word RNNLM 25.1 21.4 25.1 26.4 24.9
Combined 25.4 21.4 25.1 26.6 24.9
Table 3: French to English BLEU results for the decoder 1-best output (Baseline) compared to various
MTU models (cf. Table 2).
MTU RNN model, and the RNNLM (Combined).
The results (Table 2 and Table 3) confirm that this
is the case. For German to English translation
accuracy improves by 0.2 to 0.3 BLEU over the
RNNLM alone, with gains of up to 1.3 BLEU over
the baseline and up to 0.7 BLEU over the n-gram
MTU model. Improvements for French to English
are lower but we can see some gains on news2011
and on the dev set. Overall, we improve accuracy
on the French to English task by up to 1.5 BLEU
over the decoder 1-best, and by up to 0.8 BLEU
over the n-gram MTU model.
6 Related Work
Our approach of modeling Minimum Translation
Units is very much in line with recent work on n-
gram-based translation models (Crego and Yvon,
2010), and more recently, continuous space-based
translation models (Le et al., 2012). The mod-
els presented in this paper differ in a number of
key aspects: We use a recurrent architecture repre-
senting an unbounded history of MTUs rather than
a feed-forward style network. Feed-forward net-
works as well as back-off n-gram models rely on a
finite history which results in predictions indepen-
dent of anything but a short context of words. A
recent side-by-side comparison between recurrent
and feed-forward style neural networks (Sunder-
meyer et al., 2013) has shown that recurrent ar-
chitectures outperform feed-forward networks in
a language modeling task, a similar problem to
modeling sequences over Minimum Translation
Units.
Furthermore, the input of our best model is a
bag-of-words representation of an MTU, unlike
the ordered source and target word n-grams used
by Crego and Yvon (2010) as well as Le et al.
(2012). Finally, we model both source and target
words in a single recurrent neural network. The
approach of Le et al. (2012) factorizes the joint
probability over an MTU sequence in a way that
suggests the use of separate neural network mod-
els for the source and the target sides, where each
model generates words on the respective side only.
Other work on applying recurrent neural net-
works to machine translation (Mikolov, 2012; Auli
et al., 2013; Kalchbrenner and Blunsom, 2013)
concentrated on word-based language and transla-
tion models, whereas we model Minimum Trans-
lation Units.
7 Conclusion and Future Work
Minimum Translation Unit models based on recur-
rent neural networks lead to substantial gains over
their classical n-gram back-off models. We intro-
duced two models of which the best improves ac-
curacy by up to 1.5 BLEU over the 1-best decoder
output, and by 0.8 BLEU over a trigram MTU
model in an n-best rescoring setting.
Our experiments have shown that representing
MTUs as bags-of-words leads to better accuracy
since this exploits similarities in the internal struc-
ture of Minimum Translation Units, which is not
possible when modeling them as atomic symbols.
We have also shown that our models are comple-
mentary to a very strong RNN language model
(Mikolov, 2012).
In future work, we would like to make the initial
version of the bag-of-words model computation-
ally more tractable using a better GPU implemen-
tation. This model combines the efficient bag-of-
words input representation with the ability to pre-
dict MTUs as single units while explicitly model-
ing the constituent words in an intermediate layer.
8 Acknowledgements
We would like to thank Kristina Toutanova for
providing a dataset and for helpful discussions re-
lated to this work. We also thank the four anony-
mous reviewers for their comments.
27
References
Alexandre Allauzen, H?el`ene Bonneau-Maynard, Hai-
Son Le, Aur?elien Max, Guillaume Wisniewski,
Franc?ois Yvon, Gilles Adda, Josep Maria Crego,
Adrien Lardilleux, Thomas Lavergne, and Artem
Sokolov. 2011. LIMSI @ WMT11. In Proc. of
WMT, pages 309?315, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep Neural Net-
work Language Models. In NAACL-HLT Work-
shop on the Future of Language Modeling for HLT,
pages 20?28, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Proc.
of EMNLP, October.
Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert,
Patrik Lambert, and Jos?e B. Mari?no. 2005. Statis-
tical Machine Translation of Euparl Data by Using
bilingual n-grams. In Proc. of ACL Workshop on
Building and Using Parallel Texts, pages 133?136,
Jun.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479,
Dec.
Josep Crego and Franc?ois Yvon. 2010. Factored bilin-
gual n-gram language models for statistical machine
translation. Machine Translation, 24(2):159?175.
Ahmad Emami and Frederick Jelinek. 2005. A Neu-
ral Syntactic Language Model. Machine Learning,
60(1-3):195?227, September.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning Semantic Representations
for the Phrase Translation Model. Technical Report
MSR-TR-2013-88, Microsoft Research, September.
Joshua Goodman. 2001. Classes for Fast Maximum
Entropy Training. In Proc. of ICASSP.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Proc.
of ACL, August.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning Deep
Structured Semantic Models for Web Search using
Clickthrough Data. In Proc. of CIKM, October.
Nal Kalchbrenner and Phil Blunsom. 2013. Re-
current Continuous Translation Models. In Proc.
of EMNLP, pages 1700?1709, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In Proc. of NAACL Workshop
on Statistical Machine Translation, pages 102?121.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proc. of HLT-NAACL, pages 127?133, Edmonton,
Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, Jun.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Proc. of HLT-NAACL, pages
39?48, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Tom?a?s Mikolov, Karafi?at Martin, Luk?a?s Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proc. of INTERSPEECH, pages 1045?1048.
Tom?a?s Mikolov, Anoop Deoras, Daniel Povey, Luk?a?s
Burget, and Jan
?
Cernock?y. 2011a. Strategies
for Training Large Scale Neural Network Language
Models. In Proc. of ASRU, pages 196?201.
Tom?a?s Mikolov, Stefan Kombrink, Luk?a?s Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2011b. Ex-
tensions of Recurrent Neural Network Language
Model. In Proc. of ICASSP, pages 5528?5531.
Tom?a?s Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space-
Word Representations. In Proc. of NAACL, pages
746?751, Stroudsburg, PA, USA, June. Association
for Computational Linguistics.
Tom?a?s Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
Statistical Machine Translation. In Proc. of NAACL,
pages 8?16, New York, Jun.
28
Ariya Rastrow, Sanjeev Khudanpur, and Mark Dredze.
2012. Revisiting the Case for Explicit Syntactic
Information in Language Models. In NAACL-HLT
Workshop on the Future of Language Modeling for
HLT, pages 50?58. Association for Computational
Linguistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In Symposium on Parallel and
Distributed Processing.
Holger Schwenk, Marta R. Costa-juss`a, and Jos?e A. R.
Fonollosa. 2007. Smooth Bilingual N -Gram Trans-
lation. In Proc. of EMNLP, pages 430?438, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space
Language Models on a GPU for Statistical Machine
Translation. In NAACL-HLT Workshop on the Fu-
ture of Language Modeling for HLT, pages 11?19.
Association for Computational Linguistics.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl?uter, and Hermann Ney.
2013. Comparison of Feedforward and Recurrent
Neural Network Language Models. In IEEE In-
ternational Conference on Acoustics, Speech, and
Signal Processing, pages 8430?8434, Vancouver,
Canada, May.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-scale
Neural Language Models improves Translation. In
Proc. of EMNLP. Association for Computational
Linguistics, October.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond left-to-right: Multiple de-
composition structures for smt. In Proc. of NAACL,
pages 12?21, Atlanta, Georgia, June. Association
for Computational Linguistics.
Geoff Zweig and Konstantin Makarychev. 2013.
Speed Regularization and Optimality in Word Class-
ing. In Proc. of ICASSP.
29
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 36?39,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
Argviz: Interactive Visualization of Topic Dynamics in Multi-party
Conversations
Viet-An Nguyen
Dept. of Comp. Science
and UMIACS
University of Maryland
College Park, MD
vietan@cs.umd.edu
Yuening Hu
Dept. of Comp. Science
and UMIACS
University of Maryland
College Park, MD
ynhu@cs.umd.edu
Jordan Boyd-Graber
iSchool and
UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Department of Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
We introduce an efficient, interactive
framework?Argviz?for experts to analyze
the dynamic topical structure of multi-party
conversations. Users inject their needs,
expertise, and insights into models via iterative
topic refinement. The refined topics feed into a
segmentation model, whose outputs are shown
to users via multiple coordinated views.
1 Introduction
Uncovering the structure of conversations often re-
quires close reading by a human expert to be effective.
Political debates are an interesting example: political
scientists carefully analyze what gets said in debates
to explore how candidates shape the debate?s agenda
and frame issues or how answers subtly (or not so
subtly) shift the conversation by dodging the question
that was asked (Rogers and Norton, 2011).
Computational methods can contribute to the
analysis of topical dynamics, for example through
topic segmentation, dividing a conversation into
smaller, topically coherent segments (Purver, 2011);
or through identifying and summarizing the topics
under discussion (Blei et al, 2003; Blei, 2012). How-
ever, the topics uncovered by such methods can be
difficult for people to interpret (Chang et al, 2009),
and previous visualization frameworks for topic
models?e.g., ParallelTopics (Dou et al, 2011), Top-
icViz (Eisenstein et al, 2012), the Topical Guide,1 or
topic model visualization (Chaney and Blei, 2012)?
are not particularly well suited for linearly structured
conversations.
This paper describes Argviz, an integrated, inter-
active system for analyzing the topical dynamics of
1http://tg.byu.edu/
multi-party conversations. We bring together previ-
ous work on Interactive Topic Modeling (ITM) (Hu
et al, 2011), which allows users to efficiently inject
their needs, expertise, and insights into model build-
ing via iterative topic refinement, with Speaker Iden-
tity for Topic Segmentation (SITS) (Nguyen et al,
2012), a state-of-the-art model for topic segmenta-
tion and discovery of topic shifts in conversations.
Argviz?s interface allows users to quickly grasp the
topical flow of the conversation, discern when the
topic changes and by whom, and interactively visual-
ize the conversation?s details on demand.
2 System Overview
Our overall system consists of three steps: (1) data
preprocessing, (2) interactive topic modeling, and (3)
conversational topic segmentation and visualization.
Data preprocessing Preprocessing creates bags of
words that can be used by models. First, stopwords
and low frequency terms are removed from tokenized
text. This is then used as the data for topic modeling.
Interactive topic modeling The topic model-
ing process then discovers?through posterior
inference?the topics that best explain the conver-
sational turns. Each of the topics is a multinomial
distribution over words, which can be displayed to
users along with the association of turns (documents)
to these topics.
The result of topic modeling may be imperfect;
we give users an opportunity to refine and curate the
topics using Interactive Topic Modeling (ITM) (Hu
et al, 2011). The feedback from users is encoded
in the form of correlations: word types that should
co-occur in a topic or which should not. As these
correlations are incorporated into the model, the top-
ics learned by the model change and are presented
36
again to the user. The process repeats over multiple
iterations until the user is satisfied.
In addition, a simple but important part of the
interactive user experience is the ability for users to
label topics, i.e., to identify a ?congress? topic that
includes ?bill?, ?vote?, ?representative?, etc.
ITM is a web-based application with a HTML and
jQuery2 front end, connected via Ajax and JSON.
Topic segmentation After the user has built inter-
pretable topics, we use SITS?a hierarchical topic
model (Nguyen et al, 2012)?to jointly discover the
set of topics discussed in a given set of conversations
and how these topics change during each conversa-
tion. We use the output of ITM to initialize SITS3
with a high quality user-specific set of topics. The
outputs of SITS consist of (1) a set of topics, (2) a
distribution over topics for each turn, and (3) a proba-
bility associated with each turn indicating how likely
the topic of that turn has been shifted.
The outputs of SITS are displayed using Argviz
(Figure 2). Argviz is a web-based application, built
using Google Web Toolkit (GWT),4 which allows
users to visualize and manipulate SITS?s outputs en-
tirely in their browser after a single server request.
3 Argviz: Coordinated Conversational
Views
Given the limited screen of a web browser, Argviz
follows the multiple coordinated views approach
(Wang Baldonado et al, 2000; North and Shneider-
man, 2000) successfully used in Spotfire (Ahlberg,
1996), Improvise (Weaver, 2004), and SocialAc-
tion (Perer and Shneiderman, 2006). Argviz supports
three main coordinated views: transcript, overview
and topic.
Transcript occupies the prime real estate for a
close reading. It has a transcript panel and a speaker
panel. The transcript panel displays the original tran-
script. Each conversational turn is numbered and
color-coded by speaker. The color associated with
each speaker can be customized using the speaker
panel, which lists all the speakers.
2 http://jquery.com/
3Through per-word topic assignments
4 https://developers.google.com/web-toolkit/
Overview shows how topics gain and lose promi-
nence during the conversation. SITS?s outputs in-
clude a topic distribution and a topic shift probability
for each turn in the conversation. In Argviz, these are
represented using a heatmap and topic shift column.
In the heatmap, each turn-specific topic distribu-
tion is displayed by a heatmap row (Sopan et al,
2013). There is a cell for each topic, and the color
intensity of each cell is proportional to the probability
of the corresponding topic of a particular turn. Thus,
users can see the topical flow of the conversation
through the vertical change in cells? color intensities
as the conversation progresses. In addition, the topic
shift column shows the topic shift probability (in-
ferred by SITS) using color-coded bar charts, helping
users discern large topic changes in the conversation.
Each row is associated with a turn in the conversation;
clicking on one shifts the transcript view.
Topic displays the set of topics learned by SITS
(primed by ITM), with font-size proportional to the
words? topic probabilities. The selected topic panel
goes into more detail, with bar charts showing the
topic-word distribution. For example, in Figure 2, the
Foreign Affairs topic in panel E has high probability
words ?iraq?, ?afghanistan?, ?war?, etc. in panel F.
4 Demo: Detecting 2008 Debate Dodges
Visitors will have the opportunity to experiment with
the process of analyzing the topical dynamics of dif-
ferent multi-party conversations. Multiple datasets
will be preprocessed and set up for users to choose
and analyze. Examples of datasets that will be avail-
able include conversation transcripts from CNN?s
Crossfire program and debates from the 2008 and
2012 U.S. presidential campaigns. For this section,
we focus on examples from the 2008 campaign.
Interactive topic refinement After selecting a
dataset and a number of topics, the first thing a user
can do is to label topics. This will be used later in
Argviz and helps users build a mental model of what
the topics are. For instance, the user may rename the
second topic ?Foreign Policy?.
After inspecting the ?Foreign Policy? topic, the
user may notice the omission of Iran from the most
probable words in the topic. A user can remedy that
by adding the words ?Iran? and ?Iranians? into the
37
Figure 1: ITM user interface for refining a topic. Users can iteratively put words into different ?bins?, label topics, and
add new words to the topic. Users can also click on the provided links to show related turns for each topic in context.
Figure 2: The Argviz user interface consists of speaker panel (A), transcript panel (B), heatmap (C), topic shift column
(D), topic cloud panel (E), selected topic panel (F).
38
important words bin (Figure 1). Other bins include
ignored words for words that should be removed (e.g.,
?thing? and ?work? from this topic) from the topic
and trash (e.g., ?don?, which is a stop word).
The user can commit these changes by pressing the
Save changes button. The back end relearns given
the user?s feedback. Once users are satisfied with
the topic quality, they can click on the Finish button
to stop updating topics and start running the SITS
model, initialized using the final set of refined topics.
Visual analytic of conversations After SITS fin-
ishes (which takes just a few moments), users see the
dataset?s conversations in the Argviz interface. Fig-
ure 2 shows Argviz displaying the 2008 vice presiden-
tial debate between Senator Joe Biden and Governor
Sarah Palin, moderated by Gwen Ifill.
Users can start exploring the interface from any
of the views described in Section 3 to gain insight
about the conversation. For example, a user may
be interested in seeing how the ?Economy? is dis-
cussed in the debates. Clicking on a topic in the topic
cloud panel highlights that column in the heatmap.
The user can now see where the ?Economy? topic
is discussed in the debate. Next to the heatmap, the
topic shift column when debate participants changed
the topic. The red bar in turn 48 shows an interac-
tion where Governor Palin dodged a question on the
?bankruptcy bill? to discuss her ?record on energy?.
Clicking on this turn shows the interaction in the
transcript view, allowing a closer reading.
Users might also want to contrast the topics that
were discussed before and after the shift. This can
be easily done with the coordination between the
heatmap and the topic cloud panel. Clicking on a
cell in the heatmap will select the corresponding
topic to display in the selected topic panel. In our
example, the topic of the conversation was shifted
from ?Economy? to ?Energy? at turn 48.
5 Conclusion
Argviz is an efficient, interactive framework that al-
lows experts to analyze the dynamic topical structure
of multi-party conversations. We are engaged in col-
laborations with domain experts in political science
exploring the application of this framework to politi-
cal debates, and collaborators in social psychology
exploring the analysis of intra- and inter-cultural ne-
gotiation dialogues.
References
[Ahlberg, 1996] Ahlberg, C. (1996). Spotfire: an informa-
tion exploration environment. SIGMOD, 25(4):25?29.
[Blei, 2012] Blei, D. M. (2012). Probabilistic topic mod-
els. Communications of the ACM, 55(4):77?84.
[Blei et al, 2003] Blei, D. M., Ng, A., and Jordan, M.
(2003). Latent Dirichlet alocation. JMLR, 3.
[Chaney and Blei, 2012] Chaney, A. J.-B. and Blei, D. M.
(2012). Visualizing topic models. In ICWSM.
[Chang et al, 2009] Chang, J., Boyd-Graber, J., Wang, C.,
Gerrish, S., and Blei, D. M. (2009). Reading tea leaves:
How humans interpret topic models. In NIPS.
[Dou et al, 2011] Dou, W., Wang, X., Chang, R., and Rib-
arsky, W. (2011). ParallelTopics: A probabilistic ap-
proach to exploring document collections. In VAST.
[Eisenstein et al, 2012] Eisenstein, J., Chau, D. H., Kittur,
A., and Xing, E. (2012). TopicViz: interactive topic
exploration in document collections. In CHI.
[Hu et al, 2011] Hu, Y., Boyd-Graber, J., and Satinoff, B.
(2011). Interactive topic modeling. In ACL.
[Nguyen et al, 2012] Nguyen, V.-A., Boyd-Graber, J., and
Resnik, P. (2012). SITS: A hierarchical nonparametric
model using speaker identity for topic segmentation in
multiparty conversations. In ACL.
[North and Shneiderman, 2000] North, C. and Shneider-
man, B. (2000). Snap-together visualization: a user
interface for coordinating visualizations via relational
schemata. In AVI, pages 128?135.
[Perer and Shneiderman, 2006] Perer, A. and Shneider-
man, B. (2006). Balancing systematic and flexible
exploration of social networks. IEEE Transactions on
Visualization and Computer Graphics, 12(5):693?700.
[Purver, 2011] Purver, M. (2011). Topic segmentation. In
Spoken Language Understanding: Systems for Extract-
ing Semantic Information from Speech.
[Rogers and Norton, 2011] Rogers, T. and Norton, M. I.
(2011). The artful dodger: Answering the wrong ques-
tion the right way. Journal of Experimental Psychology:
Applied, 17(2):139?147.
[Sopan et al, 2013] Sopan, A., Freier, M., Taieb-Maimon,
M., Plaisant, C., Golbeck, J., and Shneiderman, B.
(2013). Exploring data distributions: Visual design
and evaluation. JHCI, 29(2):77?95.
[Wang Baldonado et al, 2000] Wang Baldonado, M. Q.,
Woodruff, A., and Kuchinsky, A. (2000). Guidelines
for using multiple views in information visualization.
In AVI, pages 110?119.
[Weaver, 2004] Weaver, C. (2004). Building highly-
coordinated visualizations in Improvise. In INFOVIS.
39
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 248?257,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Interactive Topic Modeling
Yuening Hu
Department of Computer Science
University of Maryland
ynhu@cs.umd.edu
Jordan Boyd-Graber
iSchool
University of Maryland
jbg@umiacs.umd.edu
Brianna Satinoff
Department of Computer Science
University of Maryland
bsonrisa@cs.umd.edu
Abstract
Topic models have been used extensively as a
tool for corpus exploration, and a cottage in-
dustry has developed to tweak topic models
to better encode human intuitions or to better
model data. However, creating such extensions
requires expertise in machine learning unavail-
able to potential end-users of topic modeling
software. In this work, we develop a frame-
work for allowing users to iteratively refine
the topics discovered by models such as la-
tent Dirichlet alocation (LDA) by adding con-
straints that enforce that sets of words must ap-
pear together in the same topic. We incorporate
these constraints interactively by selectively
removing elements in the state of a Markov
Chain used for inference; we investigate a va-
riety of methods for incorporating this infor-
mation and demonstrate that these interactively
added constraints improve topic usefulness for
simulated and actual user sessions.
1 Introduction
Probabilistic topic models, as exemplified by prob-
abilistic latent semantic indexing (Hofmann, 1999)
and latent Dirichlet alocation (LDA) (Blei et al,
2003) are unsupervised statistical techniques to dis-
cover the thematic topics that permeate a large cor-
pus of text documents. Topic models have had con-
siderable application beyond natural language pro-
cessing in computer vision (Rob et al, 2005), bi-
ology (Shringarpure and Xing, 2008), and psychol-
ogy (Landauer et al, 2006) in addition to their canon-
ical application to text.
For text, one of the few real-world applications
of topic models is corpus exploration. Unannotated,
noisy, and ever-growing corpora are the norm rather
than the exception, and topic models offer a way to
quickly get the gist a large corpus.1
1For examples, see Rexa http://rexa.info/, JSTOR
Contrary to the impression given by the tables
shown in topic modeling papers, topics discovered
by topic modeling don?t always make sense to os-
tensible end users. Part of the problem is that the
objective function of topic models doesn?t always cor-
relate with human judgements (Chang et al, 2009).
Another issue is that topic models ? with their bag-
of-words vision of the world ? simply lack the nec-
essary information to create the topics as end-users
expect.
There has been a thriving cottage industry adding
more and more information to topic models to cor-
rect these shortcomings; either by modeling perspec-
tive (Paul and Girju, 2010; Lin et al, 2006), syn-
tax (Wallach, 2006; Gruber et al, 2007), or author-
ship (Rosen-Zvi et al, 2004; Dietz et al, 2007). Sim-
ilarly, there has been an effort to inject human knowl-
edge into topic models (Boyd-Graber et al, 2007;
Andrzejewski et al, 2009; Petterson et al, 2010).
However, these are a priori fixes. They don?t help
a frustrated consumer of topic models staring at a
collection of topics that don?t make sense. In this
paper, we propose interactive topic modeling (ITM),
an in situ method for incorporating human knowl-
edge into topic models. In Section 2, we review prior
work on creating probabilistic models that incorpo-
rate human knowledge, which we extend in Section 3
to apply to ITM sessions. Section 4 discusses the
implementation of this process during the inference
process. Via a motivating example in Section 5, simu-
lated ITM sessions in Section 6, and a real interactive
test in Section 7, we demonstrate that our approach is
able to focus a user?s desires in a topic model, better
capture the key properties of a corpus, and capture
diverse interests from users on the web.
http://showcase.jstor.org/blei/, and the NIH
https://app.nihmaps.org/nih/.
248
2 Putting Knowledge in Topic Models
At a high level, topic models such as LDA take as
input a number of topics K and a corpus. As output,
a topic model discovers K distributions over words
? the namesake topics ? and associations between
documents and topics. In LDA both of these out-
puts are multinomial distributions; typically they are
presented to users in summary form by listing the
elements with highest probability. For an example
of topics discovered from a 20-topic model of New
York Times editorials, see Table 1.
When presented with poor topics learned from
data, users can offer a number of complaints:2
these documents should have similar topics but
don?t (Daume? III, 2009); this topic should have syn-
tactic coherence (Gruber et al, 2007; Boyd-Graber
and Blei, 2008); this topic doesn?t make any sense
at all (Newman et al, 2010); this topic shouldn?t be
associated with this document but is (Ramage et al,
2009); these words shouldn?t be the in same topic
but are (Andrzejewski et al, 2009); or these words
should be in the same topic but aren?t (Andrzejewski
et al, 2009).
Many of these complaints can be addressed by
using ?must-link? constraints on topics, retaining An-
drzejewski et als (2009) terminology borrowed from
the database literature. A ?must-link? constraint is a
group of words whose probability must be correlated
in the topic. For example, Figure 1 shows an example
constraint: {plant, factory}. After this constraint is
added, the probabilities of ?plant? and ?factory? in
each topic are likely to both be high or both be low.
It?s unlikely for ?plant? to have high probability in a
topic and ?factory? to have a low probability. In the
next section, we demonstrate how such constraints
can be built into a model and how they can even be
added while inference is underway.
In this paper, we view constraints as transitive; if
?plant? is in a constraint with ?factory? and ?factory?
is in a constraint with ?production,? then ?plant? is
in a constraint with ?production.? Making this as-
sumption can simplify inference slightly, which we
take advantage of in Section 3.1, but the real reason
for this assumption is because not doing so would
2Citations in this litany of complaints are offline solutions for
addressing the problem; the papers also give motivation why
such complaints might arise.
Constraints Prior Structure
{}
dogbark tree plant factory leash
?
?
?
?
?
?
{plant, factory}
dogbark tree
plant
factory
leash
?
? ?
?
2?
?
?
{plant, factory}
{dog, bark, leash}
dogbark
tree
plant factoryleash
?
?
?
?
2?
?
?
3?
Figure 1: How adding constraints (left) creates new topic
priors (right). The trees represent correlated distributions
(assuming ? >> ?). After the {plant, factory} constraint
is added, it is now highly unlikely for a topic drawn from
the distribution to have a high probability for ?plant? and
a low probability for ?factory? or vice versa. The bottom
panel adds an additional constraint, so now dog-related
words are also correlated. Notice that the two constraints
themselves are uncorrelated. It?s possible for both, either,
or none of ?bark? and ?plant? (for instance) to have high
probability in a topic.
introduce ambiguity over the path associated with an
observed token in the generative process. As long as
a word is either in a single constraint or in the general
vocabulary, there is only a single path. The details of
this issue are further discussed in Section 4.
3 Constraints Shape Topics
As discussed above, LDA views topics as distribu-
tions over words, and each document expresses an
admixture of these topics. For ?vanilla? LDA (no con-
straints), these are symmetric Dirichlet distributions.
A document is composed of a number of observed
words, which we call tokens to distinguish specific
observations from the more abstract word (type) as-
sociated with each token. Because LDA assumes
a document?s tokens are interchangeable, it treats
the document as a bag-of-words, ignoring potential
relations between words.
This problem with vanilla LDA can be solved by
encoding constraints, which will ?guide? different
words into the same topic. Constraints can be added
to vanilla LDA by replacing the multinomial distri-
bution over words for each topic with a collection of
249
tree-structured multinomial distributions drawn from
a prior as depicted in Figure 1. By encoding word
distributions as a tree, we can preserve conjugacy
and relatively simple inference while encouraging
correlations between related concepts (Boyd-Graber
et al, 2007; Andrzejewski et al, 2009; Boyd-Graber
and Resnik, 2010). Each topic has a top-level dis-
tribution over words and constraints, and each con-
straint in each topic has second-level distribution
over the words in the constraint. Critically, the per-
constraint distribution over words is engineered to be
non-sparse and close to uniform. The top level distri-
bution encodes which constraints (and unconstrained
words) to include; the lower-level distribution forces
the probabilities to be correlated for each of the con-
straints.
In LDA, a document?s token is produced in the
generative process by choosing a topic z and sam-
pling a word from the multinomial distribution ?z of
topic z. For a constrained topic, the process now can
take two steps. First, a first-level node in the tree is
selected from ?z . If that is an unconstrained word,
the word is emitted and the generative process for
that token is done. Otherwise, if the first level node
is constraint l, then choose a word to emit from the
constraint?s distribution over words piz,l.
More concretely, suppose for a corpus with M
documents we have a set of constraints ?. The prior
structure has B branches (one branch for each word
not in a constraint and one for each constraint). Then
the generative process for constrained LDA is:
1. For each topic i ? {1, . . .K}:
(a) draw a distribution over the B branches (words and
constraints) ?i ? Dir(~?), and
(b) for each constraint ?j ? ?, draw a distribution over
the words in the constraint pii,j ? Dir(?), where
pii,j is a distribution over the words in ?j
2. Then for each document d ? {1, . . .M}:
(a) first draw a distribution over topics ?d ? Dir(?),
(b) then for each token n ? {1, . . . Nd}:
i. choose a topic assignment zd,n ? Mult(?d),
and then
ii. choose either a constraint or word from
Mult(?zd,n):
A. if we chose a word, emit that word wd,n
B. otherwise if we chose a constraint index ld,n,
emit a word wd,n from the constraint?s dis-
tribution over words in topic zd,n: wd,n ?
Mult(pizd,n,ld,n).
In this model, ?, ?, and ? are Dirichlet hyperpa-
rameters set by the user; their role is explained below.
3.1 Gibbs Sampling for Topic Models
In topic modeling, collapsed Gibbs sampling (Grif-
fiths and Steyvers, 2004) is a standard procedure for
obtaining a Markov chain over the latent variables
in the model. Given certain technical conditions,
the stationary distribution of the Markov chain is
the posterior (Neal, 1993). Given M documents the
state of a Gibbs sampler for LDA consists of topic
assignments for each token in the corpus and is rep-
resented as Z = {z1,1 . . . z1,N1 , z2,1, . . . zM,NM }. In
each iteration, every token?s topic assignment zd,n
is resampled based on topic assignments for all the
tokens except for zd,n. (This subset of the state is
denoted Z?(d,n)). The sampling equation for zd,n is
p(zd,n = k|Z?(d,n), ?, ?) ?
Td,k + ?
Td,? +K?
Pk,wd,n + ?
Pk,? + V ?
(1)
where Td,k is the number of times topic k is used in
document d, Pk,wd,n is the number of times the type
wd,n is assigned to topic k, and ?, ? are the hyperpa-
rameters of the two Dirichlet distributions, and B is
the number of top-level branches (this is the vocab-
ulary size for vanilla LDA). When a dot replaces a
subscript of a count, it represents the marginal sum
over all possible topics or words, e.g. Td,? =
?
k Td,k.
The count statistics P and T provide summaries of
the state. Typically, these only change based on as-
signments of latent variables in the sampler; in Sec-
tion 4 we describe how changes in the model?s struc-
ture (in addition to the latent state) can be reflected
in these count statistics.
Contrasting with the above inference is the infer-
ence for a constrained model. (For a derivation, see
Boyd-Graber, Blei, and Zhu (2007) for the general
case or Andrzejewski, Zhu, and Craven (2009) for
the specific case of constraints.) In this case the
sampling equation for zd,n is changed to p(zd,n =
k|Z?(d,n), ?, ?, ?)
?
?
??
??
Td,k+?
Td,?+K?
Pk,wd,n+?
Pk,?+V ?
if ?l, wd,n 6? ?l
Td,k+?
Td,?+K?
Pk,l+Cl?
Pk,?+V ?
Wk,l,wd,n+?
Wk,l,?+Cl?
wd,n ? ?l
, (2)
where Pk,wd,n is the number of times the uncon-
strained word wd,n appears in topic k; Pk,l is the
250
number of times any word of constraint ?l appears in
topic k; Wk,l,wd,n is the number of times word wd,n
appears in constraint ?l in topic k; V is the vocabu-
lary size; Cl is the number of words in constraint ?l.
Note the differences between these two samplers for
constrained words; however, for unconstrained LDA
and for unconstrained words in constrained LDA, the
conditional probability is the same.
In order to make the constraints effective, we set
the constraint word-distribution hyperparameter ?
to be much larger than the hyperparameter for the
distribution over constraints and vocabulary ?. This
gives the constraints higher weight. Normally, esti-
mating hyperparameters is important for topic mod-
eling (Wallach et al, 2009). However, in ITM, sam-
pling hyperparameters often (but not always) undoes
the constraints (by making ? comparable to ?), so we
keep the hyperparameters fixed.
4 Interactively adding constraints
For a static model, inference in ITM is the same as
in previous models (Andrzejewski et al, 2009). In
this section, we detail how interactively changing
constraints can be accommodated in ITM, smoothly
transitioning from unconstrained LDA (n.b. Equa-
tion 1) to constrained LDA (n.b. Equation 2) with one
constraint, to constrained LDA with two constraints,
etc.
A central tool that we will use is the strategic unas-
signment of states, which we call ablation (distinct
from feature ablation in supervised learning). As
described in the previous section, a sampler stores
the topic assignment of each token. In the implemen-
tation of a Gibbs sampler, unassignment is done by
setting a token?s topic assignment to an invalid topic
(e.g. -1, as we use here) and decrementing any counts
associated with that word.
The constraints created by users implicitly signal
that words in constraints don?t belong in a given
topic. In other models, this input is sometimes used
to ?fix,? i.e. deterministically hold constant topic as-
signments (Ramage et al, 2009). Instead, we change
the underlying model, using the current topic assign-
ments as a starting position for a new Markov chain
with some states strategically unassigned. How much
of the existing topic assignments we use leads to four
different options, which are illustrated in Figure 2.
Previous New
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:-1, dog:-1, leash:-1 dog:-1]
[bark:-1, bark:-1, plant:-1, tree:-1]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:3]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:-1, dog:-1, leash:3 dog:-1]
[bark:-1, bark:-1, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:-1, dog:-1, leash:-1 dog:-1]
[bark:-1, bark:-1, plant:-1, tree:-1]
[tree:-1,play:-1,forest:-1,leash:-1]
None
Term
Doc
All
Figure 2: Four different strategies for state ablation after
the words ?dog? and ?bark? are added to the constraint
{?leash,? ?puppy?} to make the constraint {?dog,? ?bark,?
?leash,? ?puppy?}. The state is represented by showing the
current topic assignment after each word (e.g. ?leash? in
the first document has topic 3, while ?forest? in the third
document has topic 1). On the left are the assignments
before words were added to constraints, and on the right
are the ablated assignments. Unassigned words are given
the new topic assignment -1 and are highlighted in red.
All We could revoke all state assignments, essen-
tially starting the sampler from scratch. This does
not allow interactive refinement, as there is nothing
to enforce that the new topics will be in any way
consistent with the existing topics. Once the topic
assignments of all states are revoked, the counts for
T , P and W (as described in Section 3.1) will be
zero, retaining no information about the state the user
observed.
Doc Because topic models treat the document con-
text as exchangeable, a document is a natural context
for partial state ablation. Thus if a user adds a set of
words S to constraints, then we have reason to sus-
pect that all documents containing any one of S may
have incorrect topic assignments. This is reflected
in the state of the sampler by performing the UNAS-
SIGN (Algorithm 1) operation for each word in any
document containing a word added to a constraint.
Algorithm 1 UNASSIGN(d, n, wd,n, zd,n = k)
1: T : Td,k ? Td,k ? 1
2: If wd,n /? ?old,
P : Pk,wd,n ? Pk,wd,n ? 1
3: Else: suppose wd,n ? ?oldm ,
P : Pk,m ? Pk,m ? 1
W : Wk,m,wd,n ?Wk,m,wd,n ? 1
251
This is equivalent to the Gibbs2 sampler of Yao
et al (2009) for incorporating new documents in
a streaming context. Viewed in this light, a user
is using words to select documents that should be
treated as ?new? for this refined model.
Term Another option is to perform ablation only
on the topic assignments of tokens whose words have
added to a constraint. This applies the unassignment
operation (Algorithm 1) only to tokens whose corre-
sponding word appears in added constraints (i.e. a
subset of the Doc strategy). This makes it less likely
that other tokens in similar contexts will follow the
words explicitly included in the constraints to new
topic assignments.
None The final option is to move words into con-
straints but keep the topic assignments fixed. Thus,
P and W change, but not T , as described in Algo-
rithm 2.3 This is arguably the simplest option, and
in principle is sufficient, as the Markov chain should
find a stationary distribution regardless of the starting
position. In practice, however, this strategy is less
interactive, as users don?t feel that their constraints
are actually incorporated in the model, and inertia
can keep the chain from reflecting the constraints.
Algorithm 2 MOVE(d, n, wd,n, zd,n = k,?l)
1: If wd,n /? ?old,
P : Pk,wd,n ? Pk,wd,n ? 1, Pk,l ? Pk,l + 1
W : Wk,l,wd,n ?Wk,l,wd,n + 1
2: Else, suppose wd,n ? ?oldm ,
P : Pk,m ? Pk,m ? 1, Pk,l ? Pk,l + 1
W : Wk,m,wd,n ?Wk,m,wd,n ? 1
Wk,l,wd,n ?Wk,l,wd,n + 1
Regardless of what ablation scheme is used, after
the state of the Markov chain is altered, the next
step is to actually run inference forward, sampling
assignments for the unassigned tokens for the ?first?
time and changing the topic assignment of previously
assigned tokens. How many additional iterations are
3This assumes that there is only one possible path in the con-
straint tree that can generate a word; in other words, this as-
sumes that constraints are transitive, as discussed at the end of
Section 2. In the more general case, when words lack a unique
path in the constraint tree, an additional latent variable specifies
which possible paths in the constraint tree produced the word;
this would have to be sampled. All other updating strategies
are immune to this complication, as the assignments are left
unassigned.
required after adding constraints is a delicate tradeoff
between interactivity and effectiveness, which we
investigate further in the next sections.
5 Motivating Example
To examine the viability of ITM, we begin with a
qualitative demonstration that shows the potential
usefulness of ITM. For this task, we used a corpus
of about 2000 New York Times editorials from the
years 1987 to 1996. We started by finding 20 initial
topics with no constraints, as shown in Table 1 (left).
Notice that topics 1 and 20 both deal with Russia.
Topic 20 seems to be about the Soviet Union, with
topic 1 about the post-Soviet years. We wanted to
combine the two into a single topic, so we created a
constraint with all of the clearly Russian or Soviet
words (boris, communist, gorbachev, mikhail, russia,
russian, soviet, union, yeltsin ). Running inference
forward 100 iterations with the Doc ablation strat-
egy yields the topics in Table 1 (right). The two
Russia topics were combined into Topic 20. This
combination also pulled in other relevant words that
not near the top of either topic before: ?moscow?
and ?relations.? Topic 1 is now more about elections
in countries other than Russia. The other 18 topics
changed little.
While we combined the Russian topics, other re-
searchers analyzing large corpora might preserve the
Soviet vs. post-Soviet distinction but combine topics
about American government. ITM allows tuning for
specific tasks.
6 Simulation Experiment
Next, we consider a process for evaluating our ITM
using automatically derived constraints. These con-
straints are meant to simulate a user with a predefined
list of categories (e.g. reviewers for journal submis-
sions, e-mail folders, etc.). The categories grow more
and more specific during the session as the simulated
users add more constraint words.
To test the ability of ITM to discover relevant
subdivisions in a corpus, we use a dataset with pre-
defined, intrinsic labels and assess how well the dis-
covered latent topic structure can reproduce the cor-
pus?s inherent structure. Specifically, for a corpus
with M classes, we use the per-document topic dis-
tribution as a feature vector in a supervised classi-
252
Topic Words
1
election, yeltsin, russian, political, party, democratic, russia, presi-
dent, democracy, boris, country, south, years, month, government, vote,
since, leader, presidential, military
2
new, york, city, state, mayor, budget, giuliani, council, cuomo, gov,
plan, year, rudolph, dinkins, lead, need, governor, legislature, pataki,
david
3
nuclear, arms, weapon, defense, treaty, missile, world, unite, yet, soviet,
lead, secretary, would, control, korea, intelligence, test, nation, country,
testing
4
president, bush, administration, clinton, american, force, reagan, war,
unite, lead, economic, iraq, congress, america, iraqi, policy, aid, inter-
national, military, see
...
20
soviet, lead, gorbachev, union, west, mikhail, reform, change, europe,
leaders, poland, communist, know, old, right, human, washington,
western, bring, party
Topic Words
1
election, democratic, south, country, president, party, africa, lead, even,
democracy, leader, presidential, week, politics, minister, percent, voter,
last, month, years
2
new, york, city, state, mayor, budget, council, giuliani, gov, cuomo,
year, rudolph, dinkins, legislature, plan, david, governor, pataki, need,
cut
3 nuclear, arms, weapon, treaty, defense, war, missile, may, come, test,american, world, would, need, lead, get, join, yet, clinton, nation
4
president, administration, bush, clinton, war, unite, force, reagan, amer-
ican, america, make, nation, military, iraq, iraqi, troops, international,
country, yesterday, plan
...
20
soviet, union, economic, reform, yeltsin, russian, lead, russia, gor-
bachev, leaders, west, president, boris, moscow, europe, poland,
mikhail, communist, power, relations
Table 1: Five topics from a 20 topic topic model on the editorials from the New York times before adding a constraint
(left) and after (right). After the constraint was added, which encouraged Russian and Soviet terms to be in the same
topic, non-Russian terms gained increased prominence in Topic 1, and ?Moscow? (which was not part of the constraint)
appeared in Topic 20.
fier (Hall et al, 2009). The lower the classification
error rate, the better the model has captured the struc-
ture of the corpus.4
6.1 Generating automatic constraints
We used the 20 Newsgroups corpus, which contains
18846 documents divided into 20 constituent news-
groups. We use these newsgroups as ground-truth
labels.5
We simulate a user?s constraints by ranking words
in the training split by their information gain (IG).6
After ranking the top 200 words for each class
by IG, we delete words associated with multiple
labels to prevent constraints for different labels
from merging. The smallest class had 21 words
remaining after removing duplicates (due to high
4Our goal is to understand the phenomena of ITM, not classifica-
tion, so these classification results are well below state of the
art. However, adding interactively selected topics to the state
of the art features (tf-idf unigrams) gives a relative error reduc-
tion of 5.1%, while just adding topics from vanilla LDA gives
a relative error reduction of 1.1%. Both measurements were
obtained without tuning or weighting features, so presumably
better results are possible.
5http://people.csail.mit.edu/jrennie/20Newsgroups/
In preprocessing, we deleted short documents, leaving 15160
documents, including 9131 training documents and 6029 test
documents (default split). Tokenization, lemmatization, and
stopword removal was performed using the Natural Language
Toolkit (Loper and Bird, 2002). Topic modeling was performed
using the most frequent 5000 lemmas as the vocabulary.
6IG is computed by the Rainbow toolbox
http://www.cs.umass.edu/ mccallum/bow/rainbow/
overlaps of 125 words between ?talk.religion.misc?
and ?soc.religion.christian,? and 110 words between
?talk.religion.misc? and ?alt.atheism?), so the top 21
words for each class were the ingredients for our
simulated constraints. For example, for the class
?soc.religion.christian,? the 21 constraint words in-
clude ?catholic, scripture, resurrection, pope, sab-
bath, spiritual, pray, divine, doctrine, orthodox.? We
simulate a user?s ITM session by adding a word to
each of the 20 constraints until each of the constraints
has 21 words.
6.2 Simulation scheme
Starting with 100 base iterations, we perform suc-
cessive rounds of refinement. In each round a new
constraint is added corresponding to the newsgroup
labels. Next, we perform one of the strategies for
state ablation, add additional iterations of Gibbs sam-
pling, use the newly obtained topic distribution of
each document as the feature vector, and perform
classification on the test / train split. We do this for
21 rounds until each label has 21 constraint words.
The number of LDA topics is set to 20 to match the
number of newsgroups. The hyperparameters for all
experiments are ? = 0.1, ? = 0.01, and ? = 100.
At 100 iterations, the chain is clearly not con-
verged. However, we chose this number of iterations
because it more closely matches the likely use case as
users do not wait for convergence. Moreover, while
investigations showed that the patterns shown in Fig-
253
ure 4 were broadly consistent with larger numbers
of iterations, such configurations sometimes had too
much inertia to escape from local extrema. More iter-
ations make it harder for the constraints to influence
the topic assignment.
6.3 Investigating Ablation Strategies
First, we investigate which ablation strategy best al-
lows constraints to be incorporated. Figure 3 shows
the classification error of six different ablation strate-
gies based on the number of words in each constraint,
ranging from 0 to 21. Each is averaged over five dif-
ferent chains using 10 additional iterations of Gibbs
sampling per round (other numbers of iterations are
discussed in Section 6.4). The model runs forward 10
iterations after the first round, another 10 iterations
after the second round, etc. In general, as the number
of words per constraint increases, the error decreases
as models gain more information about the classes.
Strategy Null is the non-interactive baseline that
contains no constraints (vanilla LDA), but runs infer-
ence for a comparable number of rounds. All Initial
and All Full are non-interactive baselines with all
constraints known a priori. All Initial runs the model
for the only the initial number of iterations (100 it-
erations in this experiment), while All Full runs the
model for the total number of iterations added for the
interactive version. (That is, if there were 21 rounds
and each round of interactive modeling added 10 iter-
ations, All Full would have 210 iterations more than
All Initial).
While Null sees no constraints, it serves as an
upper baseline for the error rate (lower error being
better) but shows the effect of additional inference.
All Full is a lower baseline for the error rate since
it both sees the constraints at the beginning and also
runs for the maximum number of total iterations. All
Initial sees the constraints before the other ablation
techniques but it has fewer total iterations.
The Null strategy does not perform as well as
the interactive versions, especially with larger con-
straints. Both All Initial and All Full, however, show
a larger variance (as denoted by error bands around
the average trends) than the interactive schemes. This
can be viewed as akin to simulated annealing, as the
interactive search has more freedom to explore in
early rounds. As more constraint words are added
each round, the model is less free to explore.
Words per constraint
Error
0.380.40
0.420.44
0.460.48
0.50
0 5 10 15 20
StrategyAll FullAll InitialDocNoneNullTerm
Figure 3: Error rate (y-axis, lower is better) using different
ablation strategies as additional constraints are added (x-
axis). Null represents standard LDA, as the unconstrained
baseline. All Initial and All Full are non-interactive, con-
strained baselines. The results of None, Term, Doc are
more stable (as denoted by the error bars), and the error
rate is reduced gradually as more constraint words are
added.
The error rate of each interactive ablation strategy
is (as expected) between the lower and upper base-
lines. Generally, the constraints will influence not
only the topics of the constraint words, but also the
topics of the constraint words? context in the same
document. Doc ablation gives more freedom for the
constraints to overcome the inertia of the old topic
distribution and move towards a new one influenced
by the constraints.
6.4 How many iterations do users have to wait?
Figure 4 shows the effect of using different numbers
of Gibbs sampling iterations after changing a con-
straint. For each of the ablation strategies, we run
{10, 20, 30, 50, 100} additional Gibbs sampling iter-
ations. As expected, more iterations reduce error,
although improvements diminish beyond 100 itera-
tions. With more constraints, the impact of additional
iterations is lessened, as the model has more a priori
knowledge to draw upon.
For all numbers of additional iterations, while the
Null serves as the upper baseline on the error rate
in all cases, the Doc ablation clearly outperforms
the other ablation schemes, consistently yielding a
lower error rate. Thus, there is a benefit when the
model has a chance to relearn the document context
when constraints are added. The difference is even
larger with more iterations, suggesting Doc needs
more iterations to ?recover? from unassignment.
The luxury of having hundreds or thousands of
additional iterations for each constraint would be im-
254
Words per constraint
Err
or
0.40
0.42
0.44
0.46
0.48
0.50
 10
0 5 10 15 20
 20
0 5 10 15 20
 30
0 5 10 15 20
 50
0 5 10 15 20
100
0 5 10 15 20
Strategy
Doc
None
Null
Term
Figure 4: Classification accuracy by strategy and number of additional iterations. The Doc ablation strategy performs
best, suggesting that the document context is important for ablation constraints. While more iterations are better, there
is a tradeoff with interactivity.
practical. For even moderately sized datasets, even
one iteration per second can tax the patience of in-
dividuals who want to use the system interactively.
Based on these results and an ad hoc qualitative ex-
amination of the resulting topics, we found that 30
additional iterations of inference was acceptable; this
is used in later experiments.
7 Getting Humans in the Loop
To move beyond using simulated users adding the
same words regardless of what topics were discov-
ered by the model, we needed to expose the model
to human users. We solicited approximately 200
judgments from Mechanical Turk, a popular crowd-
sourcing platform that has been used to gather lin-
guistic annotations (Snow et al, 2008), measure topic
quality (Chang et al, 2009), and supplement tradi-
tional inference techniques for topic models (Chang,
2010). After presenting our interface for collecting
judgments, we examine the results from these ITM
sessions both quantitatively and qualitatively.
7.1 Interface for soliciting refinements
Figure 5 shows the interface used in the Mechanical
Turk tests. The left side of the screen shows the
current topics in a scrollable list, with the top 30
words displayed for each topic.
Users create constraints by clicking on words from
the topic word lists. The word lists use a color-coding
scheme to help the users keep track of which words
they are currently grouping into constraints. The right
side of the screen displays the existing constraints.
Users can click on icons to edit or delete each one.
The constraint currently being built is also shown.
Figure 5: Interface for Mechanical Turk experiments.
Users see the topics discovered by the model and select
words (by clicking on them) to build constraints to be
added to the model.
Clicking on a word will remove that word from the
current constraint.
As in Section 6, we can compute the classification
error for these users as they add words to constraints.
The best users, who seemed to understand the task
well, were able to decrease classification error. (Fig-
ure 6). The median user, however, had an error re-
duction indistinguishable from zero. Despite this, we
can examine the users? behavior to better understand
their goals and how they interact with the system.
7.2 Untrained users and ITM
Most of the large (10+ word) user-created constraints
corresponded to the themes of the individual news-
groups, which users were able to infer from the
discovered topics. Common constraint themes that
255
Round
Re
lat
ive
 E
rro
r
0.94
0.96
0.98
1.00
0 1 2 3 4
Best Session
 10 Topics 
 20 Topics 
 50 Topics 
 75 Topics 
Figure 6: The relative error rate (using round 0 as a base-
line) of the best Mechanical Turk user session for each of
the four numbers of topics. While the 10-topic model does
not provide enough flexibility to create good constraints,
the best users could clearly improve classification with
more topics.
matched specific newsgroups included religion, space
exploration, graphics, and encryption. Other com-
mon themes were broader than individual news-
groups (e.g. sports, government and computers). Oth-
ers matched sub-topics of a single newsgroup, such
as homosexuality, Israel or computer programming.
Some users created inscrutable constraints, like
(?better, people, right, take, things?) and (?fbi, let,
says?). They may have just clicked random words to
finish the task quickly. While subsequent users could
delete poor constraints, most chose not to. Because
we wanted to understand broader behavior we made
no effort to squelch such responses.
The two-word constraints illustrate an interesting
contrast. Some pairs are linked together in the corpus,
like (?jesus, christ?) and (?solar, sun?). With others,
like (?even, number?) and (?book, list?), the users
seem to be encouraging collocations to be in the
same topic. However, the collocations may not be in
any document in this corpus. Another user created a
constraint consisting of male first names. A topic did
emerge with these words, but the rest of the words
in that topic seemed random, as male first names are
not likely to co-occur in the same document.
Not all sensible constraints led to successful topic
changes. Many users grouped ?mac? and ?windows?
together, but they were almost never placed in the
same topic. The corpus includes separate newsgroups
for Macintosh and Windows hardware, and divergent
contexts of ?mac? and ?windows? overpowered the
prior distribution.
The constraint size ranged from one word to over
40. In general, the more words in the constraint,
the more likely it was to noticeably affect the topic
distribution. This observation makes sense given
our ablation method. A constraint with more words
will cause the topic assignments to be reset for more
documents.
8 Discussion
In this work, we introduced a means for end-users
to refine and improve the topics discovered by topic
models. ITM offers a paradigm for non-specialist
consumers of machine learning algorithms to refine
models to better reflect their interests and needs. We
demonstrated that even novice users are able to under-
stand and build constraints using a simple interface
and that their constraints can improve the model?s
ability to capture the latent structure of a corpus.
As presented here, the technique for incorporating
constraints is closely tied to inference with Gibbs
sampling. However, most inference techniques are
essentially optimization problems. As long as it is
possible to define a transition on the state space that
moves from one less-constrained model to another
more-constrained model, other inference procedures
can also be used.
We hope to engage these algorithms with more
sophisticated users than those on Mechanical Turk
to measure how these models can help them better
explore and understand large, uncurated data sets. As
we learn their needs, we can add more avenues for
interacting with topic models.
Acknowledgements
We would like to thank the anonymous reviewers, Ed-
mund Talley, Jonathan Chang, and Philip Resnik for
their helpful comments on drafts of this paper. This
work was supported by NSF grant #0705832. Jordan
Boyd-Graber is also supported by the Army Research
Laboratory through ARL Cooperative Agreement
W911NF-09-2-0072 and by NSF grant #1018625.
Any opinions, findings, conclusions, or recommenda-
tions expressed are the authors? and do not necessar-
ily reflect those of the sponsors.
256
References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic mod-
eling via Dirichlet forest priors. In Proceedings of
International Conference of Machine Learning.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine Learn-
ing Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2008. Syntactic
topic models. In Proceedings of Advances in Neural
Information Processing Systems.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual su-
pervised latent Dirichlet alocation. In Proceedings of
Emperical Methods in Natural Language Processing.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean
Gerrish, and David M. Blei. 2009. Reading tea leaves:
How humans interpret topic models. In Neural Infor-
mation Processing Systems.
Jonathan Chang. 2010. Not-so-latent Dirichlet alocation:
Collapsed Gibbs sampling using human judgments. In
NAACL Workshop: Creating Speech and Language
Data With Amazon?ss Mechanical Turk.
Hal Daume? III. 2009. Markov random topic fields. In
Proceedings of Artificial Intelligence and Statistics.
Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In Pro-
ceedings of International Conference of Machine Learn-
ing.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. Proceedings of the National Academy
of Sciences, 101(Suppl 1):5228?5235.
Amit Gruber, Michael Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic Markov models. In Artificial Intelligence
and Statistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1):10?18.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence.
Thomas K. Landauer, Danielle S. McNamara, Dennis S.
Marynick, and Walter Kintsch, editors. 2006. Proba-
bilistic Topic Models. Laurence Erlbaum.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexan-
der Hauptmann. 2006. Which side are you on? identi-
fying perspectives at the document and sentence levels.
In Proceedings of the Conference on Natural Language
Learning (CoNLL).
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Tools and methodologies for
teaching.
Radford M. Neal. 1993. Probabilistic inference using
Markov chain Monte Carlo methods. Technical Report
CRG-TR-93-1, University of Toronto.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Conference of the North American Chapter of
the Association for Computational Linguistics.
Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In Association for the Advancement of
Artificial Intelligence.
James Petterson, Smola Alex, Tiberio Caetano, Wray Bun-
tine, and Narayanamurthy Shravan. 2010. Word fea-
tures for latent Dirichlet alocation. In Neural Informa-
tion Processing Systems.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of Emperical Methods
in Natural Language Processing.
Fergus Rob, Li Fei-Fei, Perona Pietro, and Zisserman An-
drew. 2005. Learning object categories from Google?s
image search. In International Conference on Com-
puter Vision.
Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model for
authors and documents. In Proceedings of Uncertainty
in Artificial Intelligence.
Suyash Shringarpure and Eric P. Xing. 2008. mStruct:
a new admixture model for inference of population
structure in light of both genetic admixing and allele
mutations. In Proceedings of International Conference
of Machine Learning.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast?but is it good? Evalu-
ating non-expert annotations for natural language tasks.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Hanna Wallach, David Mimno, and Andrew McCallum.
2009. Rethinking LDA: Why priors matter. In Pro-
ceedings of Advances in Neural Information Processing
Systems.
Hanna M. Wallach. 2006. Topic modeling: Beyond bag-
of-words. In Proceedings of International Conference
of Machine Learning.
Limin Yao, David Mimno, and Andrew McCallum. 2009.
Efficient methods for topic model inference on stream-
ing document collections. In Knowledge Discovery and
Data Mining.
257
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 275?279,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Efficient Tree-Based Topic Modeling
Yuening Hu
Department of Computer Science
University of Maryland, College Park
ynhu@cs.umd.edu
Jordan Boyd-Graber
iSchool and UMIACS
University of Maryland, College Park
jbg@umiacs.umd.edu
Abstract
Topic modeling with a tree-based prior has
been used for a variety of applications be-
cause it can encode correlations between words
that traditional topic modeling cannot. How-
ever, its expressive power comes at the cost
of more complicated inference. We extend
the SPARSELDA (Yao et al, 2009) inference
scheme for latent Dirichlet alocation (LDA)
to tree-based topic models. This sampling
scheme computes the exact conditional distri-
bution for Gibbs sampling much more quickly
than enumerating all possible latent variable
assignments. We further improve performance
by iteratively refining the sampling distribution
only when needed. Experiments show that the
proposed techniques dramatically improve the
computation time.
1 Introduction
Topic models, exemplified by latent Dirichlet aloca-
tion (LDA) (Blei et al, 2003), discover latent themes
present in text collections. ?Topics? discovered by
topic models are multinomial probability distribu-
tions over words that evince thematic coherence.
Topic models are used in computational biology, com-
puter vision, music, and, of course, text analysis.
One of LDA?s virtues is that it is a simple model
that assumes a symmetric Dirichlet prior over its
word distributions. Recent work argues for structured
distributions that constrain clusters (Andrzejewski et
al., 2009), span languages (Jagarlamudi and Daume?
III, 2010), or incorporate human feedback (Hu et al,
2011) to improve the quality and flexibility of topic
modeling. These models all use different tree-based
prior distributions (Section 2).
These approaches are appealing because they
preserve conjugacy, making inference using Gibbs
sampling (Heinrich, 2004) straightforward. While
straightforward, inference isn?t cheap. Particularly
for interactive settings (Hu et al, 2011), efficient
inference would improve perceived latency.
SPARSELDA (Yao et al, 2009) is an efficient
Gibbs sampling algorithm for LDA based on a refac-
torization of the conditional topic distribution (re-
viewed in Section 3). However, it is not directly
applicable to tree-based priors. In Section 4, we pro-
vide a factorization for tree-based models within a
broadly applicable inference framework that empiri-
cally improves the efficiency of inference (Section 5).
2 Topic Modeling with Tree-Based Priors
Trees are intuitive methods for encoding human
knowledge. Abney and Light (1999) used tree-
structured multinomials to model selectional restric-
tions, which was later put into a Bayesian context
for topic modeling (Boyd-Graber et al, 2007). In
both cases, the tree came from WordNet (Miller,
1990), but the tree could also come from domain
experts (Andrzejewski et al, 2009).
Organizing words in this way induces correlations
that are mathematically impossible to represent with
a symmetric Dirichlet prior. To see how correlations
can occur, consider the generative process. Start with
a rooted tree structure that contains internal nodes
and leaf nodes. This skeleton is a prior that generates
K topics. Like vanilla LDA, these topics are distribu-
tions over words. Unlike vanilla LDA, their structure
correlates words. Internal nodes have a distribution
pik,i over children, where pik,i comes from per-node
Dirichlet parameterized by ?i.1 Each leaf node is
associated with a word, and each word must appear
in at least (possibly more than) one leaf node.
To generate a word from topic k, start at the root.
Select a child x0 ? Mult(pik,ROOT), and traverse
the tree until reaching a leaf node. Then emit the
leaf?s associated word. This walk replaces the draw
from a topic?s multinomial distribution over words.
1Choosing these Dirichlet priors specifies the direction (i.e.,
positive or negative) and strength of correlations that appear.
275
The rest of the generative process for LDA remains
the same, with ?, the per-document topic multinomial,
and z, the topic assignment.
This tree structure encodes correlations. The closer
types are in the tree, the more correlated they are.
Because types can appear in multiple leaf nodes, this
encodes polysemy. The path that generates a token is
an additional latent variable we must sample.
Gibbs sampling is straightforward because the tree-
based prior maintains conjugacy (Andrzejewski et
al., 2009). We integrate the per-document topic dis-
tributions ? and the transition distributions pi. The
remaining latent variables are the topic assignment z
and path l, which we sample jointly:2
p(z = k, l = ?|Z?, L?, w) (1)
? (?k + nk|d)
?
(i?j)??
?i?j + ni?j|k
?
j? (?i?j? + ni?j?|k)
where nk|d is topic k?s count in the document d;
?k is topic k?s prior; Z? and L? are topic and path
assignments excluding wd,n; ?i?j is the prior for
edge i ? j, ni?j|t is the count of edge i ? j in
topic k; and j? denotes other children of node i.
The complexity of computing the sampling distri-
bution is O(KLS) for models with K topics, paths
at most L nodes long, and at most S paths per word
type. In contrast, for vanilla LDA the analogous
conditional sampling distribution requires O(K).
3 Efficient LDA
The SPARSELDA (Yao et al, 2009) scheme for
speeding inference begins by rearranging LDA?s sam-
pling equation into three terms:3
p(z = k|Z?, w) ? (?k + nk|d)
? + nw|k
?V + n?|k
(2)
?
?k?
?V + n?|k
? ?? ?
sLDA
+
nk|d?
?V + n?|k
? ?? ?
rLDA
+
(?k + nk|d)nw|k
?V + n?|k
? ?? ?
qLDA
Following their lead, we call these three terms
?buckets?. A bucket is the total probability mass
marginalizing over latent variable assignments (i.e.,
sLDA ?
?
k
?k?
?V+n?|k
, similarly for the other buck-
ets). The three buckets are a smoothing only bucket
2For clarity, we omit indicators that ensure ? ends at wd,n.
3To ease notation we drop the d,n subscript for z and w in
this and future equations.
sLDA, document topic bucket rLDA, and topic word
bucket qLDA (we use the ?LDA? subscript to contrast
with our method, for which we use the same bucket
names without subscripts).
Caching the buckets? total mass speeds the compu-
tation of the sampling distribution. Bucket sLDA is
shared by all tokens, and bucket rLDA is shared by a
document?s tokens. Both have simple constant time
updates. Bucket qLDA has to be computed specifi-
cally for each token, but only for the (typically) few
types with non-zero counts in a topic.
To sample from the conditional distribution, first
sample which bucket you need and then (and only
then) select a topic within that bucket. Because the
topic-term bucket qLDA often has the largest mass
and has few non-zero terms, this speeds inference.
4 Efficient Inference in Tree-Based Models
In this section, we extend the sampling techniques
for SPARSELDA to tree-based topic modeling. We
first factor Equation 1:
p(z = k, l = ?|Z?, L?, w) (3)
? (?k + nk|d)N
?1
k,?[S? +Ok,?].
Henceforth we call Nk,? the normalizer for path ?
in topic k, S? the smoothing factor for path ?, and
Ok,? the observation for path ? in topic k, which are
Nk,? =
?
(i?j)??
?
j?
(?i?j? + ni?j?|k)
S? =
?
(i?j)??
?i?j (4)
Ok,? =
?
(i?j)??
(?i?j + ni?j|k)?
?
(i?j)??
?i?j .
Equation 3 can be rearranged in the same way
as Equation 5, yielding buckets analogous to
SPARSELDA?s,
p(z = k,l = ?|Z?, L?, w) (5)
?
?kS?
Nk,?
? ?? ?
s
+
nk|dS?
Nk,?
? ?? ?
r
+
(?k + nk|d)Ok,?
Nk,?
? ?? ?
q
.
Buckets sum both topics and paths. The sampling
process is much the same as for SPARSELDA: select
which bucket and then select a topic / path combina-
tion within the bucket (for a slightly more complex
example, see Algorithm 1).
276
Recall that one of the benefits of SPARSELDA was
that s was shared across tokens. This is no longer
possible, as Nk,? is distinct for each path in tree-
based LDA. Moreover, Nk,? is coupled; changing
ni?j|k in one path changes the normalizers of all
cousin paths (paths that share some node i).
This negates the benefit of caching s, but we re-
cover some of the benefits by splitting the normalizer
to two parts: the ?root? normalizer from the root node
(shared by all paths) and the ?downstream? normal-
izer. We precompute which paths share downstream
normalizers; all paths are partitioned into cousin sets,
defined as sets for which changing the count of one
member of the set changes the downstream normal-
izer of other paths in the set. Thus, when updating
the counts for path l, we only recompute Nk,l? for all
l? in the cousin set.
SPARSELDA?s computation of q, the topic-word
bucket, benefits from topics with unobserved (i.e.,
zero count) types. In our case, any non-zero path, a
path with any non-zero edge, contributes.4 To quickly
determine whether a path contributes, we introduce
an edge-masked count (EMC) for each path. Higher
order bits encode whether edges have been observed
and lower order bits encode the number of times the
path has been observed. For example, if a path of
length three only has its first two edges observed, its
EMC is 11000000. If the same path were observed
seven times, its EMC is 11100111. With this formu-
lation we can ignore any paths with a zero EMC.
Efficient sampling with refined bucket While
caching the sampling equation as described in the
previous section improved the efficiency, the smooth-
ing only bucket s is small, but computing the asso-
ciated mass is costly because it requires us to con-
sider all topics and paths. This is not a problem
for SparseLDA because s is shared across all tokens.
However, we can achieve computational gains with
an upper bound on s,
s =
?
k,?
?k
?
(i?j)?? ?i?j
?
(i?j)??
?
j? (?i?j? + ni?j?|k)
?
?
k,?
?k
?
(i?j)?? ?i?j
?
(i?j)??
?
j? ?i?j?
= s?. (6)
A sampling algorithm can take advantage of this
by not explicitly calculating s. Instead, we use s?
4C.f. observed paths, where all edges are non-zero.
as proxy, and only compute the exact s if we hit the
bucket s? (Algorithm 1). Removing s? and always
computing s yields the first algorithm in Section 4.
Algorithm 1 SAMPLING WITH REFINED BUCKET
1: for word w in this document do
2: sample = rand() ?(s? + r + q)
3: if sample < s? then
4: compute s
5: sample = sample ?(s+ r + q)/(s? + r + q)
6: if sample < s then
7: return topic k and path ? sampled from s
8: sample ? = s
9: else
10: sample ? = s?
11: if sample < r then
12: return topic k and path ? sampled from r
13: sample ? = r
14: return topic k and path ? sampled from q
Sorting Thus far, we described techniques for ef-
ficiently computing buckets, but quickly sampling
assignments within a bucket is also important. Here
we propose two techniques to consider latent vari-
able assignments in decreasing order of probability
mass. By considering fewer possible assignments,
we can speed sampling at the cost of the overhead
of maintaining sorted data structures. We sort top-
ics? prominence within a document (SD) and sort the
topics and paths of a word (SW).
Sorting topics? prominence within a document
(SD) can improve sampling from r and q; when we
need to sample within a bucket, we consider paths in
decreasing order of nk|d.
Sorting path prominence for a word (SW) can im-
prove our ability to sample from q. The edge-masked
count (EMC), as described above, serves as a proxy
for the probability of a path and topic. If, when sam-
pling a topic and path from q, we sample based on
the decreasing EMC, which roughly correlates with
path probability.
5 Experiments
In this section, we compare the running time5 of our
sampling algorithm (FAST) and our algorithm with
the refined bucket (RB) against the unfactored Gibbs
sampler (NAI?VE) and examine the effect of sorting.
Our corpus has editorials from New York Times
5Mean of five chains on a 6-Core 2.8-GHz CPU, 16GB RAM
277
Number of Topics
T50 T100 T200 T500
NAIVE 5.700 12.655 29.200 71.223
FAST 4.935 9.222 17.559 40.691
FAST-RB 2.937 4.037 5.880 8.551
FAST-RB-SD 2.675 3.795 5.400 8.363
FAST-RB-SW 2.449 3.363 4.894 7.404
FAST-RB-SDW 2.225 3.241 4.672 7.424
Vocabulary Size
V5000 V10000 V20000 V30000
NAI?VE 4.815 12.351 28.783 51.088
FAST 2.897 9.063 20.460 38.119
FAST-RB 1.012 3.900 9.777 20.040
FAST-RB-SD 0.972 3.684 9.287 18.685
FAST-RB-SW 0.889 3.376 8.406 16.640
FAST-RB-SDW 0.828 3.113 7.777 15.397
Number of Correlations
C50 C100 C200 C500
NAI?VE 11.166 12.586 13.000 15.377
FAST 8.889 9.165 9.177 8.079
FAST-RB 3.995 4.078 3.858 3.156
FAST-RB-SD 3.660 3.795 3.593 3.065
FAST-RB-SW 3.272 3.363 3.308 2.787
FAST-RB-SDW 3.026 3.241 3.091 2.627
Table 1: The average running time per iteration (S) over
100 iterations, averaged over 5 seeds. Experiments begin
with 100 topics, 100 correlations, vocab size 10000 and
then vary one dimension: number of topics (top), vocabu-
lary size (middle), and number of correlations (bottom).
from 1987 to 1996.6 Since we are interested in vary-
ing vocabulary size, we rank types by average tf-idf
and choose the top V . WordNet 3.0 generates the cor-
relations between types. For each synset in WordNet,
we generate a subtree with all types in the synset?
that are also in our vocabulary?as leaves connected
to a common parent. This subtree?s common parent
is then attached to the root node.
We compared the FAST and FAST-RB against
NAI?VE (Table 1) on different numbers of topics, var-
ious vocabulary sizes and different numbers of cor-
relations. FAST is consistently faster than NAI?VE
and FAST-RB is consistently faster than FAST. Their
benefits are clearer as distributions become sparse
(e.g., the first iteration for FAST is slower than later
iterations). Gains accumulate as the topic number
increases, but decrease a little with the vocabulary
size. While both sorting strategies reduce time, sort-
ing topics and paths for a word (SW) helps more than
sorting topics in a document (SD), and combining the
613284 documents, 41554 types, and 2714634 tokens.
1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.42
4
6
8
10
12
14
16
Average number of senses per constraint word
Aver
age 
runn
ing ti
me p
er ite
ratio
n (S)
 
 NaiveFastFast?RB
Fast?RB?sDFast?RB?sWFast?RB?sDW
Figure 1: The average running time per iteration against
the average number of senses per correlated words.
two is (with one exception) better than either alone.
As more correlations are added, NAI?VE?s time in-
creases while that of FAST-RB decreases. This is be-
cause the number of non-zero paths for uncorrelated
words decreases as more correlations are added to the
model. Since our techniques save computation for
every zero path, the overall computation decreases
as correlations push uncorrelated words to a limited
number of topics (Figure 1). Qualitatively, when the
synset with ?king? and ?baron? is added to a model,
it is associated with ?drug, inmate, colombia, water-
front, baron? in a topic; when ?king? is correlated
with ?queen?, the associated topic has ?king, parade,
museum, queen, jackson? as its most probable words.
These represent reasonable disambiguations. In con-
trast to previous approaches, inference speeds up as
topics become more semantically coherent (Boyd-
Graber et al, 2007).
6 Conclusion
We demonstrated efficient inference techniques for
topic models with tree-based priors. These methods
scale well, allowing for faster exploration of models
that use semantics to encode correlations without sac-
rificing accuracy. Improved scalability for such algo-
rithms, especially in distributed environments (Smola
and Narayanamurthy, 2010), could improve applica-
tions such as cross-language information retrieval,
unsupervised word sense disambiguation, and knowl-
edge discovery via interactive topic modeling.
278
Acknowledgments
We would like to thank David Mimno and the anony-
mous reviewers for their helpful comments. This
work was supported by the Army Research Labora-
tory through ARL Cooperative Agreement W911NF-
09-2-0072. Any opinions or conclusions expressed
are the authors? and do not necessarily reflect those
of the sponsors.
References
Steven Abney and Marc Light. 1999. Hiding a seman-
tic hierarchy in a Markov model. In Proceedings of
the Workshop on Unsupervised Learning in Natural
Language Processing.
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic mod-
eling via Dirichlet forest priors. In Proceedings of
International Conference of Machine Learning.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine Learn-
ing Research, 3:993?1022.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Gregor Heinrich. 2004. Parameter estima-
tion for text analysis. Technical report.
http://www.arbylon.net/publications/text-est.pdf.
Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff.
2011. Interactive topic modeling. In Association for
Computational Linguistics.
Jagadeesh Jagarlamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned corpora. In
Proceedings of the European Conference on Informa-
tion Retrieval (ECIR).
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264.
Alexander J. Smola and Shravan Narayanamurthy. 2010.
An architecture for parallel topic models. International
Conference on Very Large Databases, 3.
Limin Yao, David Mimno, and Andrew McCallum. 2009.
Efficient methods for topic model inference on stream-
ing document collections. In Knowledge Discovery and
Data Mining.
279
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 359?369,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Anchors Regularized: Adding Robustness and Extensibility
to Scalable Topic-Modeling Algorithms
Thang Nguyen
iSchool and UMIACS,
University of Maryland
and National Library of Medicine,
National Institutes of Health
daithang@umiacs.umd.edu
Yuening Hu
Computer Science
University of Maryland
ynhu@cs.umd.edu
Jordan Boyd-Graber
iSchool and UMIACS
University of Maryland
jbg@umiacs.umd.edu
Abstract
Spectral methods offer scalable alternatives
to Markov chain Monte Carlo and expec-
tation maximization. However, these new
methods lack the rich priors associated with
probabilistic models. We examine Arora et
al.?s anchor words algorithm for topic mod-
eling and develop new, regularized algo-
rithms that not only mathematically resem-
ble Gaussian and Dirichlet priors but also
improve the interpretability of topic models.
Our new regularization approaches make
these efficient algorithms more flexible; we
also show that these methods can be com-
bined with informed priors.
1 Introduction
Topic models are of practical and theoretical inter-
est. Practically, they have been used to understand
political perspective (Paul and Girju, 2010), im-
prove machine translation (Eidelman et al, 2012),
reveal literary trends (Jockers, 2013), and under-
stand scientific discourse (Hall et al, 2008). The-
oretically, their latent variable formulation has
served as a foundation for more robust models
of other linguistic phenomena (Brody and Lapata,
2009).
Modern topic models are formulated as a la-
tent variable model. Like hidden Markov mod-
els (Rabiner, 1989, HMM), each token comes from
one of K unknown distributions. Unlike a HMM,
topic models assume that each document is an ad-
mixture of these hidden components called topics.
Posterior inference discovers the hidden variables
that best explain a dataset. Typical solutions use
MCMC (Griffiths and Steyvers, 2004) or variational
EM (Blei et al, 2003), which can be viewed as local
optimization: searching for the latent variables that
maximize the data likelihood.
An exciting vein of new research provides
provable polynomial-time alternatives. These ap-
proaches provide solutions to hidden Markov mod-
els (Anandkumar et al, 2012), mixture mod-
els (Kannan et al, 2005), and latent variable gram-
mars (Cohen et al, 2013). The key insight is not to
directly optimize observation likelihood but to in-
stead discover latent variables that can reconstruct
statistics of the assumed generative model. Unlike
search-based methods, which can be caught in lo-
cal minima, these techniques are often guaranteed
to find global optima.
These general techniques can be improved by
making reasonable assumptions about the models.
For example, Arora et al (2012b)?s approach for in-
ference in topic models assume that each topic has
a unique ?anchor? word (thus, we call this approach
anchor). This approach is fast and effective; be-
cause it only uses word co-occurrence information,
it can scale to much larger datasets than MCMC or
EM alternatives. We review the anchor method in
Section 2.
Despite their advantages, these techniques are
not a panacea. They do not accommodate the
rich priors that modelers have come to expect.
Priors can improve performance (Wallach et al,
2009), provide domain adaptation (Daum?e III,
2007; Finkel and Manning, 2009), and guide mod-
els to reflect users? needs (Hu et al, 2013). In
Section 3, we regularize the anchor method to
trade-off the reconstruction fidelity with the penalty
terms that mimic Gaussian and Dirichlet priors.
Another shortcoming is that these models have
not been scrutinized using standard NLP evalua-
tions. Because these approaches emerged from
the theory community, anchor?s evaluations, when
present, typically use training reconstruction. In
Section 4, we show that our regularized models can
generalize to previously unseen data?as measured
by held-out likelihood (Blei et al, 2003)?and are
more interpretable (Chang et al, 2009; Newman
et al, 2010). We also show that our extension to
the anchor method enables new applications: for
359
K number of topics
V vocabulary size
M document frequency: minimum documents an an-
chor word candidate must appear in
Q word co-occurrence matrix
Q
i,j
= p(w
1
= i, w
2
= j)
?Q conditional distribution of Q
?
Q
i,j
= p(w
1
= j |w
2
= i)
?Q
i,?
row i of
?
Q
A topic matrix, of size V ?K
A
j,k
= p(w = j | z = k)
C anchor coefficient of size K ? V
C
j,k
= p(z = k |w = j)
S set of anchor word indexes {s
1
, . . . s
K
}
? regularization weight
Table 1: Notation used. Matrices are in bold
(Q,C), sets are in script S
example, using an informed priors to discover con-
cepts of interest.
Having shown that regularization does improve
performance, in Section 5 we explore why. We
discuss the trade-off of training data reconstruction
with sparsity and why regularized topics are more
interpretable.
2 Anchor Words: Scalable Topic Models
In this section, we briefly review the anchor
method and place it in the context of topic model
inference. Once we have established the anchor
objective function, in the next section we regularize
the objective function.
Rethinking Data: Word Co-occurrence Infer-
ence in topic models can be viewed as a black box:
given a set of documents, discover the topics that
best explain the data. The difference between an-
chor and conventional inference is that while con-
ventional methods take a collection of documents
as input, anchor takes word co-occurrence statis-
tics. Given a vocabulary of size V , we represent
this joint distribution asQ
i,j
= p(w
1
= i, w
2
= j),
each cell represents the probability of words appear-
ing together in a document.
Like other topic modeling algorithms, the output
of the anchor method is the topic word distribu-
tions A with size V ? K, where K is the total
number of topics desired, a parameter of the al-
gorithm. The k
th
column of A will be the topic
distribution over all words for topic k, and A
w,k
is
the probability of observing type w given topic k.
Anchors: Topic Representatives The anchor
method (Arora et al, 2012a) is based on the sepa-
rability assumption (Donoho and Stodden, 2003),
which assumes that each topic contains at least one
namesake ?anchor word? that has non-zero proba-
bility only in that topic. Intuitively, this means that
each topic has unique, specific word that, when
used, identifies that topic. For example, while
?run?, ?base?, ?fly?, and ?shortstop? are associated
with a topic about baseball, only ?shortstop? is un-
ambiguous, so it could serve as this topic?s anchor
word.
Let?s assume that we knew what the anchor
words were: a set S that indexes rows in Q. Now
consider the conditional distribution of word i,
the probability of the rest of the vocabulary given
an observation of word i; we represent this as
?
Q
i,?
,
as we can construct this by normalizing the rows of
Q. For an anchor word s
a
? S, this will look like
a topic;
?
Q
?shortstop?,?
will have high probability
for words associated with baseball.
The key insight of the anchor algorithm is that
the conditional distribution of polysemous non-
anchor words can be reconstructed as a linear com-
bination of the conditional distributions of anchor
words. For example,
?
Q
?fly?,?
could be recon-
structed by combining the anchor words ?insecta?,
?boeing?, and ?shortshop?. We represent the coeffi-
cients of this reconstruction as a matrix C, where
C
i,k
= p(z = k |w = i). Thus, for any word i,
?
Q
i,?
?
?
s
k
?S
C
i,k
?
Q
s
k
,?
. (1)
The coefficient matrix is not the usual output of a
topic modeling algorithm. The usual output is the
probability of a word given a topic. The coefficient
matrix C is the probability of a topic given a word.
We use Bayes rule to recover the topic distribution
p(w = i|z = k) ?
A
i,k
? p(z = k|w = i)p(w = i)
= C
i,k
?
j
?
Q
i,j
(2)
where p(w) is the normalizer of Q to obtain
?
Q
w,?
.
The geometric argument for finding the anchor
words is one of the key contributions of Arora et
al. (2012a) and is beyond the scope of this paper.
The algorithms in Section 3 use the anchor selec-
tion subroutine unchanged. The difference in our
approach is in how we discover the anchor coeffi-
cients C.
From Anchors to Topics After we have the an-
chor words, we need to find the coefficients that
360
best reconstruct the data
?
Q (Equation 1). Arora
et al (2012a) chose the C that minimizes the KL
divergence between
?
Q
i,?
and the reconstruction
based on the anchor word?s conditional word vec-
tors
?
s
k
?S
C
i,k
?
Q
s
k
,?
,
C
i,?
= argminC
i,?
D
KL
?
?
?
Q
i,?
||
?
s
k
?S
C
i,k
?
Q
s
k
,?
?
?
.
(3)
The anchor method is fast, as it only de-
pends on the size of the vocabulary once the co-
occurrence statistics Q are obtained. However, it
does not support rich priors for topic models, while
MCMC (Griffiths and Steyvers, 2004) and varia-
tional EM (Blei et al, 2003) methods can. This
prevents models from using priors to guide the
models to discover particular themes (Zhai et al,
2012), or to encourage sparsity in the models (Yao
et al, 2009). In the rest of this paper, we correct
this lacuna by adding regularization inspired by
Bayesian priors to the anchor algorithm.
3 Adding Regularization
In this section, we add regularizers to the anchor
objective (Equation 3). In this section, we briefly
review regularizers and then add two regularizers,
inspired by Gaussian (L
2
, Section 3.1) and Dirich-
let priors (Beta, Section 3.2), to the anchor objec-
tive function (Equation 3).
Regularization terms are ubiquitous. They typ-
ically appear as an additional term in an opti-
mization problem. Instead of optimizing a func-
tion just of the data x and parameters ?, f(x, ?),
one optimizes an objective function that includes
a regularizer that is only a function of parame-
ters: f(w, ?) + r(?). Regularizers are critical in
staid methods like linear regression (Ng, 2004),
in workhorse methods such as maximum entropy
modeling (Dud??k et al, 2004), and also in emerging
fields such as deep learning (Wager et al, 2013).
In addition to being useful, regularization terms
are appealing theoretically because they often corre-
spond to probabilistic interpretations of parameters.
For example, if we are seeking the MLE of a proba-
bilistic model parameterized by ?, p(x|?), adding
a regularization term r(?) =
?
L
i=1
?
2
i
corresponds
to adding a Gaussian prior
f(?
i
) =
1
?
2pi?
2
exp
{
?
?
2
i
2?
2
}
(4)
Corpus Train Dev Test Vocab
NIPS 1231 247 262 12182
20NEWS 11243 3760 3726 81604
NYT 9255 2012 1959 34940
Table 2: The number of documents in the train,
development, and test folds in our three datasets.
and maximizing log probability of the posterior
(ignoring constant terms) (Rennie, 2003).
3.1 L
2
Regularization
The simplest form of regularization we can add is
L
2
regularization. This is similar to assuming that
probability of a word given a topic comes from a
Gaussian distribution. While the distribution over
topics is typically Dirichlet, Dirichlet distributions
have been replaced by logistic normals in topic
modeling applications (Blei and Lafferty, 2005)
and for probabilistic grammars of language (Cohen
and Smith, 2009).
Augmenting the anchor objective with an L
2
penalty yields
C
i,?
=argmin
C
i,?
D
KL
?
?
?
Q
i,?
||
?
s
k
?S
C
i,k
?
Q
s
k
,?
?
?
+ ??C
i,?
? ?
i,?
?
2
2
, (5)
where regularization weight ? balances the impor-
tance of a high-fidelity reconstruction against the
regularization, which encourages the anchor coeffi-
cients to be close to the vector ?. When the mean
vector ? is zero, this encourages the topic coeffi-
cients to be zero. In Section 4.3, we use a non-zero
mean ? to encode an informed prior to encourage
topics to discover specific concepts.
3.2 Beta Regularization
The more common prior for topic models is a
Dirichlet prior (Minka, 2000). However, we cannot
apply this directly because the optimization is done
on a row-by-row basis of the anchor coefficient
matrix C, optimizing C for a fixed word w for and
all topics. If we want to model the probability of
a word, it must be the probability of word w in a
topic versus all other words.
Modeling this dichotomy (one versus all others
in a topic) is possible. The constructive definition
of the Dirichlet distribution (Sethuraman, 1994)
states that if one has a V -dimensional multinomial
? ? Dir(?
1
. . . ?
V
), then the marginal distribution
361
of ?
w
follows ?
w
? Beta(?
w
,
?
i 6=w
?
i
). This is
the tool we need to consider the distribution of a
single word?s probability.
This requires including the topic matrix as part
of the objective function. The topic matrix is a lin-
ear transformation of the coefficient matrix (Equa-
tion 2). The objective for beta regularization be-
comes
C
i,?
=argmin
C
i,?
D
KL
?
?
?
Q
i,?
||
?
s
k
?S
C
i,k
?
Q
s
k
,?
?
?
? ?
?
s
k
?S
log (Beta(A
i,k
; a, b)), (6)
where ? again balances reconstruction against the
regularization. To ensure the tractability of this
algorithm, we enforce a convex regularization func-
tion, which requires that a > 1 and b > 1. If we
enforce a uniform prior?E
Beta(a,b)
[A
i,k
] =
1
V
?
and that the mode of the distribution is also
1
V
,
1
this gives us the following parametric form for a
and b:
a =
x
V
+ 1, and b =
(V ? 1)x
V
+ 1 (7)
for real x greater than zero.
3.3 Initialization and Convergence
Equation 5 and Equation 6 are optimized using L-
BFGS gradient optimization (Galassi et al, 2003).
We initialize C randomly from Dir(?) with ? =
60
V
(Wallach et al, 2009). We update C after opti-
mizing all V rows. The newly updated C replaces
the old topic coefficients. We track how much
the topic coefficients C change between two con-
secutive iterations i and i + 1 and represent it as
?C ? ?C
i+1
?C
i
?
2
. We stop optimization when
?C ? ?. When ? = 0.1, the L
2
and unregularized
anchor algorithm converges after a single iteration,
while beta regularization typically converges after
fewer than ten iterations (Figure 4).
4 Regularization Improves Topic Models
In this section, we measure the performance of
our proposed regularized anchor word algorithms.
We will refer to specific algorithms in bold. For
example, the original anchor algorithm is an-
chor. Our L
2
regularized variant is anchor-L
2
,
1
For a, b < 1, the expected value is still the uniform
distribution but the mode lies at the boundaries of the simplex.
This corresponds to a sparse Dirichlet distribution, which our
optimization cannot at present model.
and our beta regularized variant is anchor-beta.
To provide conventional baselines, we also com-
pare our methods against topic models from varia-
tional inference (Blei et al, 2003, variational) and
MCMC (Griffiths and Steyvers, 2004; McCallum,
2002, MCMC).
We apply these inference strategies on three di-
verse corpora: scientific articles from the Neural
Information Processing Society (NIPS),
2
Internet
newsgroups postings (20NEWS),
3
and New York
Times editorials (Sandhaus, 2008, NYT). Statistics
for the datasets are summarized in Table 2. We split
each dataset into a training fold (70%), develop-
ment fold (15%), and a test fold (15%): the training
data are used to fit models; the development set are
used to select parameters (anchor thresholdM , doc-
ument prior ?, regularization weight ?); and final
results are reported on the test fold.
We use two evaluation measures, held-out likeli-
hood (Blei et al, 2003, HL) and topic interpretabil-
ity (Chang et al, 2009; Newman et al, 2010, TI).
Held-out likelihood measures how well the model
can reconstruct held-out documents that the model
has never seen before. This is the typical evaluation
for probabilistic models. Topic interpretability is a
more recent metric to capture how useful the topics
can be to human users attempting to make sense of
a large datasets.
Held-out likelihood cannot be computed with
existing anchor algorithms, so we use the topic
distributions learned from anchor as input to a ref-
erence variational inference implementation (Blei
et al, 2003) to compute HL. This requires an ad-
ditional parameter, the Dirichlet prior ? for the
per-document distribution over topics. We select ?
using grid search on the development set.
To compute TI and evaluate topic coherence,
we use normalized pairwise mutual informa-
tion (NPMI) (Lau et al, 2014) over topics? twenty
most probable words. Topic coherence is com-
puted against the NPMI of a reference corpus. For
coherence evaluations, we use both intrinsic and
extrinsic text collections to compute NPMI. Intrin-
sic coherence (TI-i) is computed on training and
development data at development time and on train-
ing and test data at test time. Extrinsic coherence
(TI-e) is computed from English Wikipedia articles,
with disjoint halves (1.1 million pages each) for
distinct development and testing TI-e evaluation.
2
http://cs.nyu.edu/
?
roweis/data.html
3
http://qwone.com/
?
jason/20Newsgroups/
362
ll l l l
l
l
l
l
l
l
l
l
l l
l
l
l
l l l
l l l l
?392?390
?388
?4720?4710
?4700?4690
?4680
?890.0?887.5
?885.0?882.5
20NEWS
NIPS
NYT
100 300 500 700 900Document Frequency M
HL 
Sco
re l l
l l l
l
l
l
l
l
l l l
l
l
l
l
l
l
l l
l l l l
0.020.03
0.040.05
0.060.07
0.0550.060
0.065
0.060.07
0.080.09
0.10
20NEWS
NIPS
NYT
100 300 500 700 900Document Frequency M
TI?
i Sc
ore
Figure 1: Grid search for document frequency M for our datasets with 20 topics (other configurations not
shown) on development data. The performance on both HL and TI score indicate that the unregularized
anchor algorithm is very sensitive to M . The M selected here is applied to subsequent models.
Topics l 20 40 60 80
Beta L2
l l l l l llllll l l l l lllll
l l l l l llllll l l l l lllll
l l l l l llllll
l l l l lllll
l l l l l llllll l l l l lllll
l l l l l llllll l l l l lllll
l l l l l llllll l l l l lllll
?410?405
?400?395
?390
?4800?4750
?4700?4650
?920?910
?900?890
?880
20NEWS
NIPS
NYT
00.01 0.1 0.5 1 00.01 0.1 0.5 1Regularization Weight ?
HL 
Sco
re
Topics l 20 40 60 80
Beta L2
l
l l l
l llllll l l
l l lll
l
l
l l
l l l llllll l l
l
l l
l
lll
l l
l l l llllll
l l l l lllll
l l l l l llllll l l l l lllll
l l l l l llllll
l l l l lllll
l l l l l llllll l l l l lllll
0.020.04
0.060.08
0.10
0.020.04
0.060.08
0.060.09
0.120.15
20NEWS
NIPS
NYT
0 0.01 0.1 0.5 1 0 0.01 0.1 0.5 1Regularization Weight ?
TI?
i Sc
ore
Figure 2: Selection of ? based on HL and TI scores on the development set. The value of ? = 0 is
equivalent to the original anchor algorithm; regularized versions find better solutions as the regularization
weight ? becomes non-zero.
4.1 Grid Search for Parameters on
Development Set
Anchor Threshold A good anchor word must
have a unique, specific context but also explain
other words well. A word that appears only once
will have a very specific cooccurence pattern but
will explain other words? coocurrence poorly be-
cause the observations are so sparse. As discussed
in Section 2, the anchor method uses document
frequency M as a threshold to only consider words
with robust counts.
Because all regularizations benefit equally
from higher-quality anchor words, we use cross-
validation to select the document frequency cut-
off M using the unregularized anchor algorithm.
Figure 1 shows the performance of anchor with
different M on our three datasets with 20 topics for
our two measures HL and TI-i.
Regularization Weight Once we select a cutoff
M for each combination of dataset, number of top-
ics K and a evaluation measure, we select a reg-
ularization weight ? on the development set. Fig-
ure 2 shows that beta regularization framework im-
proves topic interpretability TI-i on all datasets and
improved the held-out likelihood HL on 20NEWS.
The L
2
regularization also improves held-out like-
lihood HL for the 20NEWS corpus (Figure 2).
In the interests of space, we do not show the
figures for selecting M and ? using TI-e, which is
similar to TI-i: anchor-beta improves TI-e score on
all datasets, anchor-L
2
improves TI-e on 20NEWS
and NIPS with 20 topics and NYT with 40 topics.
4.2 Evaluating Regularization
With document frequency M and regularization
weight ? selected from the development set, we
363
compare the performance of those models on the
test set. We also compare with standard implemen-
tations of Latent Dirichlet Allocation: Blei?s LDAC
(variational) and Mallet (mcmc). We run 100 iter-
ations for LDAC and 5000 iterations for Mallet.
Each result is averaged over three random runs
and appears in Figure 3. The highly-tuned, widely-
used implementations uniformly have better held-
out likelihood than anchor-based methods, but the
much faster anchor methods are often comparable.
Within anchor-based methods, L
2
-regularization
offers comparable held-out likelihood as unregular-
ized anchor, while anchor-beta often has better
interpretability. Because of the mismatch between
the specialized vocabulary of NIPS and the general-
purpose language of Wikipedia, TI-e has a high
variance.
4.3 Informed Regularization
A frequent use of priors is to add information to a
model. This is not possible with the existing an-
chor method. An informed prior for topic models
seeds a topic with words that describe a topic of in-
terest. In a topic model, these seeds will serve as a
?magnet?, attracting similar words to the topic (Zhai
et al, 2012).
We can achieve a similar goal with anchor-L
2
.
Instead of encouraging anchor coefficients to be
zero in Equation 5, we can instead encourage word
probabilities to close to an arbitrary mean ?
i,k
.
This vector can reflect expert knowledge.
One example of a source of expert knowledge
is Linguistic Inquiry and Word Count (Pennebaker
and Francis, 1999, LIWC), a dictionary of key-
words related to sixty-eight psychological concepts
such as positive emotions, negative emotions, and
death. For example, it associates ?excessive, estate,
money, cheap, expensive, living, profit, live, rich,
income, poor, etc.? for the concept materialism.
We associate each anchor word with its closest
LIWC category based on the cooccurrence matrix
Q. This is computed by greedily finding the an-
chor word that has the highest cooccurrence score
for any LIWC category: we define the score of a
category to anchor word w
s
k
as
?
i
Q
s
k
,i
, where i
ranges over words in this category; we compute the
scores of all categories to all anchor words; then
we find the highest score and assign the category to
that anchor word; we greedily repeat this process
until all anchor words have a category.
Given these associations, we create a goal mean
?
i,k
. If there are L
i
anchor words associated with
LIWC word i, ?
i,k
=
1
L
i
if this keyword i is associ-
ated with anchor word w
s
k
and zero otherwise.
We apply anchor-L
2
with informed priors on
NYT with twenty topics and compared the topics
against the original topics from anchor. Table 3
shows that the topic with anchor word ?soviet?,
when combined with LIWC, draws in the new words
?bush? and ?nuclear?; reflecting the threats of force
during the cold war. For the topic with topic word
?arms?, when associated with the LIWC category
with the terms ?agree? and ?agreement?, draws
in ?clinton?, who represented a more conciliatory
foreign policy compared to his republican prede-
cessors.
5 Discussion
Having shown that regularization can improve the
anchor topic modeling algorithm, in this section
we discuss why these regularizations can improve
the model and the implications for practitioners.
Efficiency Efficiency is a function of the number
of iterations and the cost of each iteration. Both
anchor and anchor-L
2
require a single iteration,
although the latter?s iteration is slightly more ex-
pensive. For beta, as described in Section 3.2,
we update anchor coefficients C row by row, and
then repeat the process over several iterations until
it converges. However, it often converges within
ten iterations (Figure 4) on all three datasets: this
requires much fewer iterations than MCMC or vari-
ational inference, and the iterations are less expen-
sive. In addition, since we optimize each row C
i,?
independently, the algorithm can be easily paral-
lelized.
Sensitivity to Document Frequency While the
original anchor is sensitive to the document fre-
quency M (Figure 1), adding regularization makes
this less critical. Both anchor-L
2
and anchor-beta
are less sensitive to M than anchor.
To highlight this, we compare the topics of an-
chor and anchor-beta whenM = 100. As Table 4
shows, the words ?article?, ?write?, ?don? and
?doe? appear in most of anchor?s topics. While
anchor-L
2
also has some bad topics, it still can find
reasonable topics, demonstrating anchor-beta?s
greater robustness to suboptimal M .
L
2
(Sometimes) Improves Generalization As
Figure 2 shows, anchor-L
2
sometimes improves
held-out development likelihood for the smaller
364
Algorithm l anchor anchor?beta anchor?L2 MCMC variational
20NEWS
l l l l
l l l l
l
l l l
?410
?405
?400
?395
?390
0.03
0.04
0.05
0.06
0.07
0.06
0.08
0.10
HL
TI?e
TI?i
20 40 60 80
topic number
NIPS
l
l
l l
l
l l
l
l l l l
?4580
?4560
?4540
?4520
?4500
?4480
?4460
0.08
0.09
0.10
0.11
0.06
0.07
0.08
0.09
HL
TI?e
TI?i
20 40 60 80
topic number
NYT
l l
l l
l
l l l
l l l l
?880
?870
?860
0.07
0.08
0.09
0.08
0.10
0.12
0.14
HL
TI?e
TI?i
20 40 60 80
topic number
Figure 3: Comparing anchor-beta and anchor-L
2
against the original anchor and the traditional vari-
ational and MCMC on HL score and TI score. variational and mcmc provide the best held-out gener-
alization. anchor-beta sometimes gives the best TI score and is consistently better than anchor. The
specialized vocabulary of NIPS causes high variance for the extrinsic interpretability evaluation (TI-e).
Topic Shared Words Original (Top, green) vs. Informed L
2
(Bottom, orange)
soviet
american make president soviet union
war years
gorbachev moscow russian force economic world europe politi-
cal communist lead reform germany country
military state service washington bush army unite chief troops
officer nuclear time week
district
assembly board city county district
member state york
representative manhattan brooklyn queens election bronx council
island local incumbent housing municipal
people party group social republican year make years friend
vote compromise million
peace
american force government israel peace
political president state unite
washington
war military country minister leaders nation world palestinian
israeli election
offer justice aid deserve make bush years fair clinton hand
arms
arms bush congress force iraq make north
nuclear president state washington weapon
administration treaty missile defense war military korea
reagan
agree agreement american accept unite share clinton
years
trade
administration america american country
economic government make president state
trade unite washington
world market japan foreign china policy price political
business economy congress year years clinton bush
buy
Table 3: Examples of topic comparison between anchor and informed anchor-L
2
. A topic is labeled
with the anchor word for that topic. The bold words are the informed prior from LIWC. With an informed
prior, relevant words appear in the top words of a topic; this also draws in other related terms (red).
20NEWS corpus. However, the ? selected on devel-
opment data does not always improve test set per-
formance. This, in Figure 3, anchor-beta closely
tracks anchor. Thus, L
2
regularization does not
hurt generalization while imparting expressiveness
and robustness to parameter settings.
Beta Improves Interpretability Figure 3 shows
that anchor-beta improves topic interpretability
(TI) compared to unregularized anchor methods. In
this section, we try to understand why.
We first compare the topics from the original
anchor against anchor-beta to analyze the topics
qualitatively. Table 5 shows that beta regulariza-
tion promotes rarer words within a topic and de-
motes common words. For example, in the topic
about hockey with the anchor word game, ?run?
and ?good??ambiguous, polysemous words?in
the unregularized topic are replaced by ?playoff?
365
Topic anchor anchor-beta
frequently
article write don doe make time people good
file question
article write don doe make people time good
email file
debate
write article people make don doe god key gov-
ernment time
people make god article write don doe key
point government
wings game team write wings article win red play
hockey year
game team wings win red hockey play season
player fan
stats player team write game article stats year good
play doe
stats player season league baseball fan team in-
dividual playoff nhl
compile program file write email doe windows call prob-
lem run don
compile program code file ftp advance package
error windows sun
Table 4: Topics from anchor and anchor-beta with M = 100 on 20NEWS with 20 topics. Each topic is
identified with its associated anchor word. When M = 100, the topics of anchor suffer: the four colored
words appear in almost every topic. anchor-beta, in contrast, is less sensitive to suboptimal M .
l
l
l
l l l l l l l l l l l l l l l l l l0
10
20
30
40
0 5 10 15 20Iteration
?C
Dataset l 20NEWS NIPS NYT
Figure 4: Convergence of anchor coefficient C for
anchor-beta. ?C is the difference of current C
from theC at the previous iteration. C is converged
within ten iterations for all three datasets.
and ?trade? in the regularized topic. These words
are less ambiguous and more likely to make sense
to a consumer of topic models.
Figure 5 shows why this happens. Compared
to the unregularized topics from anchor, the beta
regularized topic steals from the rich and creates a
more uniform distribution. Thus, highly frequent
words do not as easily climb to the top of the distri-
bution, and the topics reflect topical, relevant words
rather than globally frequent terms.
6 Conclusion
A topic model is a popular tool for quickly get-
ting the gist of large corpora. However, running
such an analysis on these large corpora entail a
substantial computational cost. While techniques
such as anchor algorithms offer faster solutions, it
comes at the cost of the expressive priors common
in Bayesian formulations.
This paper introduces two different regulariza-
tions that offer users more interpretable models
and the ability to inject prior knowledge without
sacrificing the speed and generalizability of the
underlying approach. However, one sacrifice that
this approach does make is the beautiful theoretical
guarantees of previous work. An important piece
of future work is a theoretical understanding of
generalizability in extensible, regularized models.
Incorporating other regularizations could further
improve performance or unlock new applications.
Our regularizations do not explicitly encourage
sparsity; applying other regularizations such as L
1
could encourage true sparsity (Tibshirani, 1994),
and structured priors (Andrzejewski et al, 2009)
could efficiently incorporate constraints on topic
models.
These regularizations could improve spectral al-
gorithms for latent variables models, improving the
performance for other NLP tasks such as latent vari-
able PCFGs (Cohen et al, 2013) and HMMs (Anand-
kumar et al, 2012), combining the flexibility and
robustness offered by priors with the speed and
accuracy of new, scalable algorithms.
Acknowledgments
We would like to thank the anonymous reviewers,
Hal Daum?e III, Ke Wu, and Ke Zhai for their help-
ful comments. This work was supported by NSF
Grant IIS-1320538. Boyd-Graber is also supported
by NSF Grant CCF-1018625. Any opinions, find-
ings, conclusions, or recommendations expressed
here are those of the authors and do not necessarily
reflect the view of the sponsor.
366
computer drive game god power
?20
?15
?10
?5
0
?20
?15
?10
?5
0
anchor
anchor?beta
Rank of word in topic (topic shown by anchor word)
log
 p(
wo
rd
 | to
pic
)
Figure 5: How beta regularization influences the topic distribution. Each topic is identified with its
associated anchor word. Compared to the unregularized anchor method, anchor-beta steals probability
mass from the ?rich? and prefers a smoother distribution of probability mass. These words often tend to
be unimportant, polysemous words common across topics.
Topic Shared Words anchor (Top, green) vs. anchor-beta (Bottom, orange)
computer computer means science screen
system phone university problem doe work windows internet
software chip mac set fax technology information data
quote mhz pro processor ship remote print devices complex cpu
electrical transfer ray engineering serial reduce
power
power play period supply
ground light battery engine
car good make high problem work back turn control current
small time
circuit oil wire unit water heat hot ranger input total joe plug
god
god jesus christian bible faith church life christ belief
religion hell word lord truth love
people make things true doe
sin christianity atheist peace heaven
game
game team player play win fan hockey season baseball
red wings score division league goal leaf cup toronto
run good
playoff trade
drive
drive disk hard scsi controller card floppy ide mac bus
speed monitor switch apple cable internal port meg
problem work
ram pin
Table 5: Comparing topics?labeled by their anchor word?from anchor and anchor-beta. With beta
regularization, relevant words are promoted, while more general words are suppressed, improving topic
coherence.
References
Animashree Anandkumar, Daniel Hsu, and Sham M.
Kakade. 2012. A method of moments for mixture
models and hidden markov models. In Proceedings
of Conference on Learning Theory.
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. In Proceedings
of the International Conference of Machine Learn-
ing.
Sanjeev Arora, Rong Ge, Yoni Halpern, David M.
Mimno, Ankur Moitra, David Sontag, Yichen Wu,
and Michael Zhu. 2012a. A practical algorithm
for topic modeling with provable guarantees. CoRR,
abs/1212.4777.
Sanjeev Arora, Rong Ge, and Ankur Moitra. 2012b.
Learning topic models - going beyond svd. CoRR,
abs/1204.1956.
David M. Blei and John D. Lafferty. 2005. Correlated
topic models. In Proceedings of Advances in Neural
Information Processing Systems.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine
Learning Research, 3.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the Euro-
pean Chapter of the Association for Computational
Linguistics, Athens, Greece.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Proceedings of Advances in Neural Information Pro-
cessing Systems.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying
in unsupervised grammar induction. In Conference
of the North American Chapter of the Association
for Computational Linguistics.
367
Shay Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2013. Experiments with
spectral learning of latent-variable PCFGs. In Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the Association for Com-
putational Linguistics.
David Donoho and Victoria Stodden. 2003. When
does non-negative matrix factorization give correct
decomposition into parts? page 2004. MIT Press.
Miroslav Dud??k, Steven J. Phillips, and Robert E.
Schapire. 2004. Performance guarantees for reg-
ularized maximum entropy density estimation. In
Proceedings of Conference on Learning Theory.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the Association
for Computational Linguistics.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Morristown, NJ,
USA.
Mark Galassi, Jim Davies, James Theiler, Brian Gough,
Gerard Jungman, Michael Booth, and Fabrice Rossi.
2003. Gnu Scientific Library: Reference Manual.
Network Theory Ltd.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl 1):5228?5235.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In Proceedings of Emperical Methods
in Natural Language Processing.
Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff,
and Alison Smith. 2013. Interactive topic modeling.
Machine Learning Journal.
Matt L. Jockers. 2013. Macroanalysis: Digital Meth-
ods and Literary History. Topics in the Digital Hu-
manities. University of Illinois Press.
Ravindran Kannan, Hadi Salmasian, and Santosh Vem-
pala. 2005. The spectral method for general mixture
models. In Proceedings of Conference on Learning
Theory.
Ken Lang. 2007. 20 newsgroups data set.
Jey Han Lau, David Newman, and Timothy Baldwin.
2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality.
In Proceedings of the European Chapter of the Asso-
ciation for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Thomas P. Minka. 2000. Estimating a
dirichlet distribution. Technical report, Mi-
crosoft. http://research.microsoft.com/en-
us/um/people/minka/papers/dirichlet/.
David Newman, Jey Han Lau, Karl Grieser, and Timo-
thy Baldwin. 2010. Automatic evaluation of topic
coherence. In Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regu-
larization, and rotational invariance. In Proceedings
of the International Conference of Machine Learn-
ing.
Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering
multi-faceted topics. In Association for the Advance-
ment of Artificial Intelligence.
James W. Pennebaker and Martha E. Francis. 1999.
Linguistic Inquiry and Word Count. Lawrence Erl-
baum, 1 edition, August.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
286.
Jason Rennie. 2003. On l2-norm regularization and
the Gaussian prior.
Sam Roweis. 2002. NIPS 1-12 Dataset.
Evan Sandhaus. 2008. The New
York Times annotated corpus.
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2008T19.
Jayaram Sethuraman. 1994. A constructive definition
of Dirichlet priors. Statistica Sinica, 4:639?650.
Robert Tibshirani. 1994. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society, Series B, 58:267?288.
Stefan Wager, Sida Wang, and Percy Liang. 2013.
Dropout training as adaptive regularization. In Pro-
ceedings of Advances in Neural Information Pro-
cessing Systems, pages 351?359.
Hanna Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking LDA: Why priors matter.
In Proceedings of Advances in Neural Information
Processing Systems.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. In Knowledge
Discovery and Data Mining.
368
Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mo-
hamad Alkhouja. 2012. Mr. LDA: A flexible large
scale topic modeling package using variational infer-
ence in mapreduce. In Proceedings of World Wide
Web Conference.
369
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1166?1176,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Polylingual Tree-Based Topic Models for Translation Domain Adaptation
Yuening Hu
?
Computer Science
University of Maryland
ynhu@cs.umd.edu
Ke Zhai
?
Computer Science
University of Maryland
zhaike@cs.umd.edu
Vladimir Eidelman
FiscalNote Inc.
Washington DC
vlad@fiscalnote.com
Jordan Boyd-Graber
iSchool and UMIACS
University of Maryland
jbg@umiacs.umd.edu
Abstract
Topic models, an unsupervised technique
for inferring translation domains improve
machine translation quality. However, pre-
vious work uses only the source language
and completely ignores the target language,
which can disambiguate domains. We pro-
pose new polylingual tree-based topic mod-
els to extract domain knowledge that con-
siders both source and target languages and
derive three different inference schemes.
We evaluate our model on a Chinese to En-
glish translation task and obtain up to 1.2
BLEU improvement over strong baselines.
1 Introduction
Probabilistic topic models (Blei and Lafferty,
2009), exemplified by latent Dirichlet aloca-
tion (Blei et al, 2003, LDA), are one of the most
popular statistical frameworks for navigating large
unannotated document collections. Topic models
discover?without any supervision?the primary
themes presented in a dataset: the namesake topics.
Topic models have two primary applications: to
aid human exploration of corpora (Chang et al,
2009) or serve as a low-dimensional representa-
tion for downstream applications. We focus on
the second application, which has been fruitful for
computer vision (Li Fei-Fei and Perona, 2005),
computational biology (Perina et al, 2010), and
information retrieval (Kataria et al, 2011).
In particular, we use topic models to aid statisti-
cal machine translation (Koehn, 2009, SMT). Mod-
ern machine translation systems use millions of
examples of translations to learn translation rules.
These systems work best when the training corpus
has consistent genre, register, and topic. Systems
that are robust to systematic variation in the train-
ing set are said to exhibit domain adaptation.
? indicates equal contributions.
As we review in Section 2, topic models are
a promising solution for automatically discover-
ing domains in machine translation corpora. How-
ever, past work either relies solely on monolingual
source-side models (Eidelman et al, 2012; Hasler
et al, 2012; Su et al, 2012), or limited modeling
of the target side (Xiao et al, 2012). In contrast,
machine translation uses inherently multilingual
data: an SMT system must translate a phrase or sen-
tence from a source language to a different target
language, so existing applications of topic mod-
els (Eidelman et al, 2012) are wilfully ignoring
available information on the target side that could
aid domain discovery.
This is not for a lack of multilingual topic mod-
els. Topic models bridge the chasm between lan-
guages using document connections (Mimno et
al., 2009), dictionaries (Boyd-Graber and Resnik,
2010), and word alignments (Zhao and Xing, 2006).
In Section 2, we review these models for discover-
ing topics in multilingual datasets and discuss how
they can improve SMT.
However, no models combine multiple bridges
between languages. In Section 3, we create a
model?the polylingual tree-based topic models
(ptLDA)?that uses information from both external
dictionaries and document alignments simultane-
ously. In Section 4, we derive both MCMC and
variational inference for this new topic model.
In Section 5, we evaluate our model on the task
of SMT using aligned datasets. We show that ptLDA
offers better domain adaptation than other topic
models for machine translation. Finally, in Sec-
tion 6, we show how these topic models improve
SMT with detailed examples.
2 Topic Models for Machine Translation
Before considering past approaches using topic
models to improve SMT, we briefly review lexical
weighting and domain adaptation for SMT.
1166
2.1 Statistical Machine Translation
Statistical machine translation casts machine trans-
lation as a probabilistic process (Koehn, 2009). For
a parallel corpus of aligned source and target sen-
tences (F , E), a phrase
?
f ? F is translated to a
phrase e? ? E according to a distribution p
w
(e?|
?
f).
One popular method to estimate the probability
p
w
(e?|
?
f) is via lexical weighting features.
Lexical Weighting In phrase-based SMT, lexi-
cal weighting features estimate the phrase pair
quality by combining lexical translation probabil-
ities of words in a phrase (Koehn et al, 2003).
Lexical conditional probabilities p(e|f) are maxi-
mum likelihood estimates from relative lexical fre-
quencies c(f, e)/
?
e
c(f, e) , where c(f, e) is the
count of observing lexical pair (f, e) in the train-
ing dataset. The phrase pair probabilities p
w
(e?|
?
f)
are the normalized product of lexical probabili-
ties of the aligned word pairs within that phrase
pair (Koehn et al, 2003). In Section 2.2, we create
topic-specific lexical weighting features.
Cross-Domain SMT A SMT system is usu-
ally trained on documents with the same genre
(e.g., sports, business) from a similar style (e.g.,
newswire, blog-posts). These are called domains.
Translations within one domain are better than
translations across domains since they vary dra-
matically in their word choices and style. A correct
translation in one domain may be inappropriate in
another domain. For example, ???? in a newspa-
per usually means ?underwater diving?. On social
media, it means a non-contributing ?lurker?.
Domain Adaptation for SMT Training a SMT
system using diverse data requires domain adap-
tation. Early efforts focus on building separate
models (Foster and Kuhn, 2007) and adding fea-
tures (Matsoukas et al, 2009) to model domain
information. Chiang et al (2011) combine these
approaches by directly optimizing genre and col-
lection features by computing separate translation
tables for each domain.
However, these approaches treat domains as
hand-labeled, constant, and known a priori. This
setup is at best expensive and at worst infeasible for
large data. Topic models provide a solution where
domains can be automatically induced from raw
data: treat each topic as a domain.
1
1
Henceforth we will use the term ?topic? and ?domain?
interchangeably: ?topic? to refer to the concept in topic models
and ?domain? to refer to SMT corpora.
2.2 Inducing Domains with Topic Models
Topic models take the number of topics K and a
collection of documents as input, where each docu-
ment is a bag of words. They output two distribu-
tions: a distribution over topics for each document
d; and a distribution over words for each topic. If
each topic defines a SMT domain, the document?s
topic distribution is a soft domain assignment for
that document.
Given the soft domain assignments, Eidelman et
al. (2012) extract lexical weighting features condi-
tioned on the topics, optimizing feature weights us-
ing the Margin Infused Relaxed Algorithm (Cram-
mer et al, 2006, MIRA). The topics come from
source documents only and create topic-specific
lexical weights from the per-document topic distri-
bution p(k | d). The lexical probability conditioned
on the topic is expected count e
k
(e, f) of a word
translation pair under topic k,
c?
k
(e, f) =
?
d
p(k|d)c
d
(e, f), (1)
where c
d
(?) is the number of occurrences of the
word pair in document d. The lexical probability
conditioned on topic k is the unsmoothed probabil-
ity estimate of those expected counts
p
w
(e|f ; k) =
c?
k
(e,f)?
e
c?
k
(e,f)
, (2)
from which we can compute the phrase pair proba-
bilities p
w
(e?|
?
f ; k) by multiplying the lexical prob-
abilities and normalizing as in Koehn et al (2003).
For a test document d, the document topic dis-
tribution p(k | d) is inferred based on the topics
learned from training data. The feature value of a
phrase pair (e?,
?
f) is
f
k
(e?|
?
f) = ? log
{
p
w
(e?|
?
f ; k) ? p(k|d)
}
, (3)
a combination of the topic dependent lexical weight
and the topic distribution of the document, from
which we extract the phrase. Eidelman et al (2012)
compute the resulting model score by combining
these features in a linear model with other standard
SMT features and optimizing the weights.
Conceptually, this approach is just reweighting
examples. The probability of a topic given a docu-
ment is never zero. Every translation observed in
the training set will contribute to p
k
(e|f); many of
the expected counts, however, will be less than one.
This obviates the explicit smoothing used in other
domain adaptation systems (Chiang et al, 2011).
1167
We adopt this framework in its entirety. Our
contribution are topics that capture multilingual
information and thus better capture the domains in
the parallel corpus.
2.3 Beyond Vanilla Topic Models
Eidelman et al (2012) ignore a wealth of infor-
mation that could improve topic models and help
machine translation. Namely, they only use mono-
lingual data from the source language, ignoring all
target-language data and available lexical semantic
resources between source and target languages.
Different complement each other to reduce ambi-
guity. For example, ???? in a Chinese document
can be either ?hobbyhorse? in a children?s topic,
or ?Trojan virus? in a technology topic. A short
Chinese context obscures the true topic. However,
these terms are unambiguous in English, revealing
the true topic.
While vanilla topic models (LDA) can only be
applied to monolingual data, there are a number
of topic models for parallel corpora: Zhao and
Xing (2006) assume aligned word pairs share same
topics; Mimno et al (2009) connect different lan-
guages through comparable documents. These
models take advantage of word or document align-
ment information and infer more robust topics from
the aligned dataset.
On the other hand, lexical information can in-
duce topics from multilingual corpora. For in-
stance, orthographic similarity connects words with
the same meaning in related languages (Boyd-
Graber and Blei, 2009), and dictionaries are a
more general source of information on which words
share meaning (Boyd-Graber and Resnik, 2010).
These two approaches are not mutually exclu-
sive, however; they reveal different connections
across languages. In the next section, we combine
these two approaches into a polylingual tree-based
topic model.
3 Polylingual Tree-based Topic Models
In this section, we bring existing tree-based topic
models (Boyd-Graber et al, 2007, tLDA) and
polylingual topic models (Mimno et al, 2009,
pLDA) together and create the polylingual tree-
based topic model (ptLDA) that incorporates both
word-level correlations and document-level align-
ment information.
Word-level Correlations Tree-based topic mod-
els incorporate the correlations between words by
encouraging words that appear together in a con-
cept to have similar probabilities given a topic.
These concepts can come from WordNet (Boyd-
Graber and Resnik, 2010), domain experts (An-
drzejewski et al, 2009), or user constrains (Hu et
al., 2013). When we gather concepts from bilin-
gual resources, these concepts can connect different
languages. For example, if a bilingual dictionary
defines ???? as ?computer?, we combine these
words in a concept.
We organize the vocabulary in a tree structure
based on these concepts (Figure 1): words in the
same concept share a common parent node, and
then that concept becomes one of many children of
the root node. Words that are not in any concept?
uncorrelated words?are directly connected to
the root node. We call this structure the tree prior.
When this tree serves as a prior for topic models,
words in the same concept are correlated in topics.
For example, if ???? has high probability in a
topic, so will ?computer?, since they share the same
parent node. With the tree priors, each topic is no
longer a distribution over word types, instead, it is a
distribution over paths, and each path is associated
with a word type. The same word could appear in
multiple paths, and each path represents a unique
sense of this word.
Document-level Alignments Lexical resources
connect languages and help guide the topics. How-
ever, these resources are sometimes brittle and may
not cover the whole vocabulary. Aligned document
pairs provide a more corpus-specific, flexible asso-
ciation across languages.
Polylingual topic models (Mimno et al, 2009)
assume that the aligned documents in different lan-
guages share the same topic distribution and each
language has a unique topic distribution over its
word types. This level of connection between lan-
guages is flexible: instead of requiring the exact
matching on words and sentences, only a coarse
document alignment is necessary, as long as the
documents discuss the same topics.
Combine Words and Documents We propose
polylingual tree-based topic models (ptLDA),
which connect information across different lan-
guages by incorporating both word correlation (as
in tLDA) and document alignment information (as
in pLDA). We initially assume a given tree struc-
ture, deferring the tree?s provenance to the end of
this section.
1168
Generative Process As in LDA, each word to-
ken is associated with a topic. However, tree-based
topic models introduce an additional step of select-
ing a concept in a topic responsible for generating
each word token. This is represented by a path y
d,n
through the topic?s tree.
The probability of a path in a topic depends on
the transition probabilities in a topic. Each concept
i in topic k has a distribution over its children nodes
is governed by a Dirichlet prior: pi
k,i
? Dir(?
i
).
Each path ends in a word (i.e., a leaf node) and
the probability of a path is the product of all of
the transitions between topics it traverses. Topics
have correlations over words because the Dirichlet
parameters can encode positive or negative correla-
tions (Andrzejewski et al, 2009).
With these correlated in topics in hand, the gen-
eration of documents are very similar to LDA. For
every document d, we first sample a distribution
over topics ?
d
from a Dirichlet prior Dir(?). For
every token in the documents, we first sample a
topic z
dn
from the multinomial distribution ?
d
, and
then sample a path y
dn
along the tree according to
the transition distributions specified by topic z
dn
.
Because every path y
dn
leads to a word w
dn
in lan-
guage l
dn
, we append the sampled word w
dn
to
document d
l
dn
. Aligned documents have words in
both languages; monolingual documents only have
words in a single language.
The full generative process is:
1: for topic k ? 1, ? ? ? ,K do
2: for each internal node n
i
do
3: draw a distribution pi
ki
? Dir(?
i
)
4: for document set d ? 1, ? ? ? , D do
5: draw a distribution ?
d
? Dir(?)
6: for each word in documents d do
7: choose a topic z
dn
? Mult(?
d
)
8: sample a path y
dn
with probability
?
(i,j)?y
dn
pi
z
dn
,i,j
9: y
dn
leads to word w
dn
in language l
dn
10: append token w
dn
to document d
l
dn
If we use a flat symmetric Dirichlet prior instead
of the tree prior, we recover pLDA; and if all docu-
ments are monolingual (i.e., with distinct distribu-
tions over topics ?), we recover tLDA. ptLDA con-
nects different languages on both the word level (us-
ing the word correlations) and the document level
(using the document alignments). We compare
these models? machine translation performance in
Section 5.
computer, 
market, ?
government, ??
science, ??
Dictionary: Vocabulary: English (0), Chinese (1)
computer

market ?
government
??
science
??
??scientific policy
0    scientific
0    policy
1    
1    ?
0    computer  
0    market
0    government
0    science
1    ??
1    ??
1    ??
Prior Tree:
 0  1
Figure 1: An example of constructing a prior tree
from a bilingual dictionary: word pairs with the
same meaning but in different languages are con-
cepts; we create a common parent node to group
words in a concept, and then connect to the root; un-
correlated words are connected to the root directly.
Each topic uses this tree structure as a prior.
Build Prior Tree Structures One remaining
question is the source of the word-level connections
across languages for the tree prior. We consider
two resources to build trees that correlate words
across languages. The first are a multilingual dic-
tionaries (dict), which match words with the same
meaning in different languages together. These re-
lations between words are used as the concepts in
the prior tree (Figure 1).
In addition, we extract the word alignments from
aligned sentences in a parallel corpus. The word
pairs define concepts for the prior tree (align). We
use both resources for our models (denoted as
ptLDA-dict and ptLDA-align) in our experiments
(Section 5) and show that they yield comparable
performance in SMT.
4 Inference
Inference of probabilistic models discovers the pos-
terior distribution over latent variables. For a col-
lection of D documents, each of which contains
N
d
number of words, the latent variables of ptLDA
are: transition distributions pi
ki
for every topic k
and internal node i in the prior tree structure; multi-
nomial distributions over topics ?
d
for every docu-
ment d; topic assignments z
dn
and path y
dn
for the
n
th
word w
dn
in document d. The joint distribution
of polylingual tree-based topic models is
p(w, z,y,?,pi;?, ?) =
?
k
?
i
p(pi
ki
|?
i
) (4)
?
?
d
p(?
d
|?) ?
?
d
?
n
p(z
dn
|?
d
)
?
?
d
?
n
(
p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)
)
.
Exact inference is intractable, so we turn to ap-
1169
proximate posterior inference to discover the latent
variables that best explain our data. Two widely
used approximation approaches are Markov chain
Monte Carlo (Neal, 2000, MCMC) and variational
Bayesian inference (Blei et al, 2003, VB). Both
frameworks produce good approximations of the
posterior mode (Asuncion et al, 2009). In addition,
Mimno et al (2012) propose hybrid inference that
takes advantage of parallelizable variational infer-
ence for global variables (Wolfe et al, 2008) while
enjoying the sparse, efficient updates for local vari-
ables (Neal, 1993). In the rest of this section, we
discuss all three methods in turn.
We explore multiple inference schemes because
while all of these methods optimize likelihood be-
cause they might give different results on the trans-
lation task.
4.1 Markov Chain Monte Carlo Inference
We use a collapsed Gibbs sampler for tree-based
topic models to sample the path y
dn
and topic as-
signment z
dn
for word w
dn
,
p(z
dn
= k, y
dn
= s|?z
dn
,?y
dn
,w;?,?)
? I [?(s) = w
dn
] ?
N
k|d
+?
?
k
?
(N
k
?
|d
+?)
?
?
i?j?s
N
i?j|k
+?
i?j?
j
?
(N
i?j
?
|k
+?
i?j
?
)
,
where ?(s) represents the word that path s leads
to, N
k|d
is the number of tokens assigned to topic k
in document d and N
i?j|k
is the number of times
edge i? j in the tree assigned to topic k, exclud-
ing the topic assignment z
dn
and its path y
dn
of
current token w
dn
. In practice, we sample the la-
tent variables using efficient sparse updates (Yao et
al., 2009; Hu and Boyd-Graber, 2012).
4.2 Variational Bayesian Inference
Variational Bayesian inference approximates the
posterior distribution with a simplified variational
distribution q over the latent variables: document
topic proportions ?, transition probabilities pi, topic
assignments z, and path assignments y.
Variational distributions typically assume a
mean-field distribution over these latent variables,
removing all dependencies between the latent vari-
ables. We follow this assumption for the transi-
tion probabilities q(pi |?) and the document topic
proportions q(? |?); both are variational Dirichlet
distributions. However, due to the tight coupling
between the path and topic variables, we must
model this joint distribution as one multinomial,
q(z,y |?). If word token w
dn
has K topics and
S paths, it has a K ? S length variational multino-
mial ?
dnks
, which represents the probability that
the word takes path s in topic k. The complete
variational distribution is
q(?,pi, z,y|?,?,?) =
?
d
q(?
d
|?
d
)? (5)
?
k
?
i
q(pi
ki
|?
ki
) ?
?
d
?
n
q(z
dn
, y
dn
|?
dn
).
Our goal is to find the variational distribution q
that is closest to the true posterior, as measured by
the Kullback-Leibler (KL) divergence between the
true posterior p and variational distribution q. This
induces an ?evidence lower bound? (ELBO, L) as a
function of a variational distribution q: L =
E
q
[log p(w, z,y,?,pi)]? E
q
[log q(?,pi, z,y)]
=
?
k
?
i
E
q
[log p(pi
ki
|?
i
)]
+
?
d
E
q
[log p(?
d
|?)]
+
?
d
?
n
E
q
[log p(z
dn
, y
dn
|?
d
,pi)p(w
dn
|y
dn
)]
+ H[q(?)] + H[q(pi)] + H[q(z,y)], (6)
where H[?] represents the entropy of a distribution.
Optimizing L using coordinate descent provides
the following updates:
?
dnkt
? exp{?(?
dk
)??(
?
k
?
dk
) (7)
+
?
i?j?s
(
?(?
k,i?j
)??(
?
j
?
?
k,i?j
?
)
)
};
?
dk
= ?
k
+
?
n
?
s??
?1
(w
dn
)
?
dnkt
; (8)
?
k,i?j
= ?
i?j
(9)
+
?
d
?
n
?
s??
?
(w
dn
)
?
dnkt
I [i? j ? s] ;
where ?
?
(w
dn
) is the set of all paths that lead to
wordw
dn
in the tree, and t represents one particular
path in this set. I [i? j ? s] is the indicator of
whether path s contains an edge from node i to j.
4.3 Hybrid Stochastic Inference
Given the complementary strengths of MCMC and
VB, and following hybrid inference proposed by
Mimno et al (2012), we also derive hybrid infer-
ence for ptLDA.
The transition distributions pi are treated identi-
cally as in variational inference. We posit a varia-
tional Dirichlet distribution ? and choose the one
that minimizes the KL divergence between the true
posterior and the variational distribution.
For topic z and path y, instead of variational
updates, we use a Gibbs sampler within a document.
We sample z
dn
and y
dn
conditioned on the topic
1170
and path assignments of all other document tokens,
based on the variational expectation of pi,
q(z
dn
= k, y
dn
= s|?z
dn
,?y
dn
;w) ? (10)
(?+
?
m 6=n
I [z
dm
= k])
? exp{E
q
[log p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)]}.
This equation embodies how this is a hybrid algo-
rithm: the first term resembles the Gibbs sampling
term encoding how much a document prefers a
topic, while the second term encodes the expecta-
tion under the variational distribution of how much
a path is preferred by this topic,
E
q
[log p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)] = I
[?(y
dn
)=w
dn
]
?
?
i?j?y
dn
E
q
[log ?
z
dn
,i?j
].
For every document, we sweep over all its to-
kens and resample their topic z
dn
and path y
dn
conditioned on all the other tokens? topic and path
assignments ?z
dn
and ?y
dn
. To avoid bias, we
discard the first B burn-in sweeps and take the
following M samples. We then use the empirical
average of these samples update the global varia-
tional parameter q(pi|?) based on how many times
we sampled these paths
?
k,i?j
=
1
M
?
d
?
n
?
s??
?1
(w
dn
)
(
I [i? j ? s]
? I [z
dn
= k, y
dn
= s]
)
+ ?
i?j
. (11)
For our experiments, we use the recommended set-
tingsB = 5 andM = 5 from Mimno et al (2012).
5 Experiments
We evaluate our new topic model, ptLDA, and exist-
ing topic models?LDA, pLDA, and tLDA?on their
ability to induce domains for machine translation
and the resulting performance of the translations
on standard machine translation metrics.
Dataset and SMT Pipeline We use the NIST MT
Chinese-English parallel corpus (NIST), excluding
non-UN and non-HK Hansards portions as our train-
ing dataset. It contains 1.6M sentence pairs, with
40.4M Chinese tokens and 44.4M English tokens.
We replicate the SMT pipeline of Eidelman et al
(2012): word segmentation (Tseng et al, 2005),
align (Och and Ney, 2003), and symmetrize (Koehn
et al, 2003) the data. We train a modified Kneser-
Ney trigram language model on English (Chen and
Goodman, 1996). We use CDEC (Dyer et al, 2010)
for decoding, and MIRA (Crammer et al, 2006)
for parameter training. To optimize SMT system,
we tune the parameters on NIST MT06, and report
results on three test sets: MT02, MT03 and MT05.
2
Topic Models Configuration We compare our
polylingual tree-based topic model (ptLDA) against
tree-based topic models (tLDA), polylingual topic
models (pLDA) and vanilla topic models (LDA).
3
We also examine different inference algorithms?
Gibbs sampling (gibbs), variational inference
(variational) and hybrid approach (variational-
hybrid)?on the effects of SMT performance. In
all experiments, we set the per-document Dirichlet
parameter ? = 0.01 and the number of topics to
10, as used in Eidelman et al (2012).
Resources for Prior Tree To build the tree for
tLDA and ptLDA, we extract the word correla-
tions from a Chinese-English bilingual dictio-
nary (Denisowski, 1997).
4
We filter the dictionary
using the NIST vocabulary, and keep entries map-
ping single Chinese and single English words. The
prior tree has about 1000 word pairs (dict).
We also extract the bidirectional word align-
ments between Chinese and English using
GIZA++ (Och and Ney, 2003). We then remove
the word pairs appearing more than 50K times or
fewer than 500 times and construct a second prior
tree with about 2500 word pairs (align).
We apply both trees to tLDA and ptLDA, denoted
as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDA-
align. However, tLDA-align and ptLDA-align do
worse than tLDA-dict and ptLDA-dict, so we omit
tLDA-align in the results.
Domain Adaptation using Topic Models We
examine the effectiveness of using topic models
for domain adaptation on standard SMT evalua-
tion metrics?BLEU (Papineni et al, 2002) and
TER (Snover et al, 2006). We report the results
on three different test sets (Figure 2), and all SMT
results are averaged over five runs.
We refer to the SMT model without domain adap-
tation as baseline.
5
LDA marginally improves ma-
chine translation (less than half a BLEU point).
2
The NIST datasets contain 878, 919, 1082 and 1664 sen-
tences for MT02, MT03, MT05 and MT06 respectively.
3
For Gibbs sampling, we use implementations available in
Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum,
2002) for LDA and pLDA.
4
This is a two-level tree structure. However, one could
build a more sophisticated tree prior with a hierarchical dictio-
nary such as multilingual WordNet.
5
Our replication of Eidelman et al (2012) yields slightly
higher baseline performance, but the trend is consistent.
1171
gibbs variational variational?hybrid
34.8 +0.3 +0.6 +0.4
+1.2 +0.5
35.1 +0.1 +0.3 +0.2 +0.7 +0.4
31.4 +0.4 +0.7 +0.4 +1 +0.4
34.8 +0.4 +0.5 +0.4 +0.8 +0.5
35.1
?0.1 +0.2 ?0.1 +0.2 +0.2
31.4 +0.3 +0.5 +0.3 +0.8 +0.4
34.8 +0.2 +0.4 +0.2 +0.7 +0.4
35.1
?0.1 ?0.1 ?0.1 +0.2 +0.2
31.4 +0.3 +0.3 +0.1 +0.6 +0.3
3132
3334
3536
37
3132
3334
3536
37
3132
3334
3536
37
mt02
mt03
mt05
BLE
U S
cor
e
model baseline LDA pLDA ptLDA?align ptLDA?dict tLDA?dict
gibbs variational variational?hybrid
61.9 ?0.1
?1 ?1.2
?2.5 ?1.1
60.1
?0.3
?0.9 ?0.8
?1.9 ?0.9
63.3
?0.9
?1.3 ?1.2
?2.6 ?1.1
61.9
?0.4
?1 ?0.6
?1.6 ?1.3
60.1
?0.2
?0.5 ?0.1
?1 ?0.7
63.3
?0.5
?1 ?0.4
?1.5 ?1.2
61.9
?0.3
?0.7 ?0.1
?1.6 ?0.9
60.1 0 ?0.2 +0.2
?1.1 ?0.5
63.3
?0.4
?0.7 ?0.1
?1.6 ?0.8
5658
6062
6466
5658
6062
6466
5658
6062
6466
mt02
mt03
mt05
TE
R S
cor
e
model baseline LDA pLDA ptLDA?align ptLDA?dict tLDA?dict
Figure 2: Machine translation performance for different models and inference algorithms against the
baseline, on BLEU (top, higher the better) and TER (bottom, lower the better) scores. Our proposed ptLDA
performs best. Results are averaged over 5 random runs. For model ptLDA-dict with different inference
schemes, the BLEU improvement on three test sets is mostly significant with p = 0.01, except the results
on MT03 using variational and variational-hybrid inferences.
Polylingual topic models pLDA and tree-based
topic models tLDA-dict are consistently better than
LDA, suggesting that incorporating additional bilin-
gual knowledge improves topic models. These im-
provements are not redundant: our new ptLDA-dict
model, which has aspects of both models yields the
best performance among these approaches?up to a
1.2 BLEU point gain (higher is better), and -2.6 TER
improvement (lower is better). The BLEU improve-
ment is significant (Koehn, 2004) at p = 0.01,
6
except on MT03 with variational and variational-
hybrid inference.
While ptLDA-align performs better than base-
line SMT and LDA, it is worse than ptLDA-dict,
possibly because of errors in the word alignments,
making the tree priors less effective.
Scalability While gibbs has better translation
scores than variational and variational-hybrid, it
is less scalable to larger datasets. With 1.6M NIST
6
Because we have multiple runs of each topic model (and
thus different translation models), we select the run closest to
the average BLEU for the translation significance test.
training sentences, gibbs takes nearly a week to
run 1000 iterations. In contrast, the parallelized
variational and variational-hybrid approaches,
which we implement in MapReduce (Dean and
Ghemawat, 2004; Wolfe et al, 2008; Zhai et al,
2012), take less than a day to converge.
6 Discussion
In this section, we qualitatively analyze the trans-
lation results and investigate how ptLDA and its
cousins improve SMT. We also discuss other ap-
proaches to improve unsupervised domain adapta-
tion for SMT.
6.1 How do Topic Models Help SMT?
We present two examples of how topic models can
improve SMT. The first example shows both LDA
and ptLDA improve the baseline. The second exam-
ple shows how LDA introduce biases that mislead
SMT and how ptLDA?s bilingual constraints correct
these mistakes.
Figure 3 shows a sentence about a company
1172
source ???????????
 , ????
reference
sony has already sold about 570,000 units of narrowband connection 
kits in north america at the price of about 39 us dollars and some 20 
compatible games .
baseline
LDA
ptLDA
? internet links set ...
? internet links kit ? 
? internet links kit ?  
? with about 20 of the game .
? , there are about 20 compatible games .
? , there are about 20 compatible games .
source ?  ... ? ???

LDA-Topic 0 (business)
ptLDA-Topic 0 (business)
reference
? connection kits ... ? some 20 compatible games .

	, ???

??(company), ??(China), ?(service), ?
(market), ?(technology), ?(industry), ??
(provide), (develop), ?(year), 
(product), 
?, ??(coorporate), ?, ??(manage), ?
(invest), (economy), ?(international), ?
(system), (bank)
??(company), ?(service), ?(market), ?
(technology), china, ?(industry), 

(product), market, company, technology, services, 
?(system), year, industry, products, business, 
(economy), information, ??(manage), ?
(invest), percent, ?	(internet), companies, world, 
system, ??(information), ?(increase), 
(device), service, (service)
Figure 3: Better SMT result using topic models for domain adaptation. Top row: the source sentence and
its reference translation. Middle row: the highlighted translations from different approaches. Bottom row:
the change of relevant translation probabilities after incorporating the domain knowledge from LDA and
ptLDA. Right: most-probable words of the topic the source sentence is assigned to under LDA (top) and
ptLDA (bottom). The Chinese translations are in parenthesis.
introducing new technology gadgets where both
LDA and ptLDA improve translations. The base-
line translates ???? to ?set? (red), and ???? to
?with? (blue), which do not capture the reference
meaning of a add-on device that works with com-
patible games. Both LDA and ptLDA assign this
sentence to a business domain, which makes the
translations probabilities shift toward correct trans-
lations: the probability of translating ???? to
?compatible? and the probability of translating ??
?? to ?kit? in the business domain are both signif-
icantly larger than without the domain knowledge;
and the probabilities of translating ???? to ?with?
and the probability of translating ?set? to ????
in the business domain decrease.
The second example (Figure 4) illustrates how
ptLDA offers further improvements over LDA. The
source sentence discusses foreign affairs. The
baseline correctly translates the word ???? to
?affect?. However, LDA?which only takes mono-
lingual information from the source language?
assigns this sentence to economic development.
This misleads SMT to lower the probability for
the correct translation ?affect?; it chooses ?impact?
instead. In contrast, ptLDA?which incorporates
bilingual constraints?successfully labels this sen-
tence as foreign affairs and produces a softer, more
nuanced translation that better matches the refer-
ence. The translation of ???? is very similar,
except in this case, both the baseline and LDA
produce the incorrect translation ?the commitment
of?. This is possible because the probabilities of
translating ???? to ?promised to? and translat-
ing ?promised to? to ???? (the correct transla-
tion, in both directions) increase when conditioned
on ptLDA?s correct topic but decrease when condi-
tioned on LDA?s incorrect topic.
6.2 Other Approaches
Other approaches have used topic models for ma-
chine translation. Xiao et al (2012) present a topic
similarity model based on LDA that produces a fea-
ture that weights grammar rules based on topic
compatibility. They also model the source and tar-
get side of rules and compare the target similarity
during decoding by projecting the target distribu-
tion into the source space. Hasler et al (2012)
use the source-side topic assignments from hidden
topic Markov models (Gruber et al, 2007, HTMM)
which models documents as a Markov chain and
assign one topic to the whole sentence, instead of
a mixture of topics. Su et al (2012) also apply
HTMM to monolingual data and apply the results to
machine translation. To our knowledge, however,
this is the first work to use multilingual topic mod-
els for domain adaptation in machine translation.
6.3 Improving Language Models
Topic models capture document-level properties
of language, but a critical component of machine
translation systems is the language model, which
provides local constraints and preferences. Do-
main adaptation for language models (Bellegarda,
2004; Wood and Teh, 2009) is an important avenue
for improving machine translation. Models that si-
multaneously discover global document themes as
well as local, contextual domain-specific informa-
1173
source
????, ?????????, ???????????Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 79?82,
Baltimore, Maryland, USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Concurrent Visualization of Relationships between Words and Topics in
Topic Models
Alison Smith
?
, Jason Chuang
?
, Yuening Hu
?
, Jordan Boyd-Graber
?
, Leah Findlater
?
?
University of Maryland, College Park, MD
?
University of Washington, Seattle, WA
amsmit@cs.umd.edu, jcchuang@cs.washington.edu, ynhu@cs.umd.edu, jbg@umiacs.umd.edu, leahkf@umd.edu
Abstract
Analysis tools based on topic models are
often used as a means to explore large
amounts of unstructured data. Users of-
ten reason about the correctness of a model
using relationships between words within
the topics or topics within the model. We
compute this useful contextual informa-
tion as term co-occurrence and topic co-
variance and overlay it on top of stan-
dard topic model output via an intuitive
interactive visualization. This is a work
in progress with the end goal to combine
the visual representation with interactions
and online learning, so the users can di-
rectly explore (a) why a model may not
align with their intuition and (b) modify
the model as needed.
1 Introduction
Topic modeling is a popular technique for analyz-
ing large text corpora. A user is unlikely to have
the time required to understand and exploit the raw
results of topic modeling for analysis of a corpus.
Therefore, an interesting and intuitive visualiza-
tion is required for a topic model to provide added
value. A common topic modeling technique is La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003),
which is an unsupervised algorithm for perform-
ing statistical topic modeling that uses a ?bag of
words? approach. The resulting topic model repre-
sents the corpus as an unrelated set of topics where
each topic is a probability distribution over words.
Experienced users who have worked with a text
corpus for an extended period of time often think
of the thematic relationships in the corpus in terms
of higher-level statistics such as (a) inter-topic cor-
relations or (b) word correlations. However, stan-
dard topic models do not explicitly provide such
contextual information to the users.
Existing tools based on topic models, such
as Topical Guide (Gardner et al., 2010), Top-
icViz (Eisenstein et al., 2012), and the topic vi-
sualization of (Chaney and Blei, 2012) support
topic-based corpus browsing and understanding.
Visualizations of this type typically represent stan-
dard topic models as a sea of word clouds; the in-
dividual topics within the model are presented as
an unordered set of word clouds ? or something
similar ? of the top words for the topic
1
where
word size is proportional to the probability of the
word for the topic. A primary issue with word
clouds is that they can hinder understanding (Har-
ris, 2011) due to the fact that they lack information
about the relationships between words. Addition-
ally, topic model visualizations that display topics
in a random layout can lead to a huge, inefficiently
organized search space, which is not always help-
ful in providing a quick corpus overview or assist-
ing the user to diagnose possible problems with
the model.
The authors of Correlated Topic Models (CTM)
(Lafferty and Blei, 2006) recognize the limitation
of existing topic models to directly model the cor-
relation between topics, and present an alterna-
tive algorithm, CTM, which models the correla-
tion between topics discovered for a corpus by us-
ing a more flexible distribution for the topic pro-
portions in the model. Topical n-gram models
(TNG) (Wang et al., 2007) discover phrases in
addition to topics. TNG is a probabilistic model
which assigns words and n-grams based on sur-
rounding context, instead of for all references in
the corpus. These models independently account
for the two limitations of statistical topic modeling
discussed in this paper by modifying the underly-
ing topic modeling algorithm. Our work aims to
provide a low-cost method for incorporating this
1
This varies, but typically is either the top 10 to 20 words
or the number of words which hold a specific portion of the
distribution weight.
79
information as well as visualizing it in an effec-
tive way. We compute summary statistics, term
co-occurrence and topic covariance, which can be
overlaid on top of any traditional topic model. As
a number of application-specific LDA implemen-
tations exist, we propose a meta-technique which
can be applied to any underlying algorithm.
We present a relationship-enriched visualiza-
tion to help users explore topic models through
word and topic correlations. We propose inter-
actions to support user understanding, validation,
and refinement of the models.
2 Group-in-a-box Layout for Visualizing
a Relationship-Enriched Topic Model
Existing topic model visualizations do not eas-
ily support displaying the relationships between
words in the topics and topics in the model. In-
stead, this requires a layout that supports intuitive
visualization of nested network graphs. A group-
in-a-box (GIB) layout (Rodrigues et al., 2011) is a
network graph visualization that is ideal for our
scenario as it is typically used for representing
clusters with emphasis on the edges within and
between clusters. The GIB layout visualizes sub-
graphs within a graph using a Treemap (Shneider-
man, 1998) space filling technique and layout al-
gorithms for optimizing the layout of sub-graphs
within the space, such that related sub-graphs are
placed together spatially. Figure 1 shows a sample
group-in-a-box visualization.
We use the GIB layout to visually separate top-
ics of the model as groups. We implement each
topic as a force-directed network graph (Fruchter-
man and Reingold, 1991) where the nodes of the
graph are the top words of the topic. An edge ex-
ists between two words in the network graph if
the value of the term co-occurrence for the word
pair is above a certain threshold,
2
and the edge is
weighted by this value. Similarly, the edges be-
tween the topic clusters represent the topic covari-
ance metric. Finally, the GIB layout optimizes the
visualization such that related topic clusters are
placed together spatially. The result is a topic visu-
alization where related words are clustered within
the topics and related topics are clustered within
the overall layout.
2
There are a variety of techniques for setting this thresh-
old; currently, we aim to display fewer, stronger relationships
to balance informativeness and complexity of the visualiza-
tion
Figure 1: A sample GIB layout from (Rodrigues
et al., 2011). The layout visualizes clusters dis-
tributed in a treemap structure where the partitions
are based on the size of the clusters.
3 Relationship Metrics
We compute the term and topic relationship in-
formation required by the GIB layout as term
co-occurrence and topic covariance, respectively.
Term co-occurrence is a corpus-level statistic that
can be computed independently from the LDA al-
gorithm. The results of the LDA algorithm are re-
quired to compute the topic covariance.
3.1 Corpus-Level Term Co-Occurrence
Prior work has shown that Pointwise Mutual
Information (PMI) is the most consistent scor-
ing method for evaluating topic model coher-
ence (Newman et al., 2010). PMI is a statistical
technique for measuring the association between
two observations. For our purposes, PMI is used
to measure the correlation between each term pair
within each topic on the document level
3
. The
PMI is calculated for every possible term pair in
the ingested data set using Equation 1. The visu-
alization uses only the PMI for the term pairs for
the top terms for each topic, which is a small sub-
set of the calculated PMI values. Computing the
PMI is trivial compared to the LDA calculation,
and computing the values for all pairs allows the
job to be run in parallel, as opposed to waiting for
the results of the LDA job to determine the top
term pairs.
PMI(x, y) = log
p(x, y)
p(x)p(y)
(1)
The PMI measure represents the probability of
observing x given y and vice-versa. PMI can be
3
We use document here, but the PMI can be computed at
various levels of granularity as required by the analyst intent.
80
positive or negative, where 0 represents indepen-
dence, and PMI is at its maximum when x and y
are perfectly associated.
3.2 Topic Covariance
To quantify the relationship between topics in the
model, we calculate the topic covariance metric
for each pair of topics. To do this, we use the
theta vector from the LDA output. The theta vec-
tor describes which topics are used for which doc-
uments in the model, where theta(d,i) represents
how much the ith topic is expressed in document
d. The equations for calculation the topic covari-
ance are shown below.
?
d
i
=
?
d
i
?
j
(?
d
j
)
(2)
?
i
=
1
D
?
d
(?
d
i
) (3)
?(i, j) =
1
D
?
d
(?
d
i
? ?
i
)(?
d
j
? ?
j
)) (4)
4 Visualization
The visualization represents the individual topics
as network graphs where nodes represent terms
and edges represent frequent term co-occurrence,
and the layout of the topics represents topic co-
variance. The most connected topic is placed in
the center of the layout, and the least connected
topics are placed at the corners. Figure 2 shows
the visualization for a topic model generated for
a 1,000 document NSF dataset. As demonstrated
in Figure 3, a user can hover over a topic to see
the related topics
4
. In this example, the user has
hovered over the {visualization, visual, interac-
tive} topic, which is related to {user, interfaces},
{human, computer, interaction}, {design, tools},
and {digital, data, web} among others. Unlike
other topical similarity measures, such as cosine
similarity or a count of shared words, the topic co-
variance represents topics which are typically dis-
cussed together in the same documents, helping
the user to discover semantically similar topics.
On the topic level, the size of the node in the
topic network graph represents the probability of
the word given the topic. By mapping word proba-
bility to the area of the nodes instead of the height
4
we consider topics related if the topic co-occurrence is
above a certain pre-defined threshold.
Figure 2: The visualization utilizes a group-in-a-
box-inspired layout to represent the topic model as
a nested network graph.
of words, the resulting visual encoding is not af-
fected by the length of the words, a well-known
issue with word cloud presentations that can visu-
ally bias longer terms. Furthermore, circles can
overlap without affecting a user?s ability to visu-
ally separate them, and lead to more compact and
less cluttered visual layout. Hovering over a word
node highlights the same word in other topics as
shown in Figure 4.
This visualization is an alternative interface
for Interactive Topic Modeling (ITM) (Hu et al.,
2013). ITM presents users with topics that can be
modified as appropriate. Our preliminary results
show that topics containing highly-weighted sub-
clusters may be candidates for splitting, whereas
positively correlated topics are likely to be good
topics, which do not need to be modified. In fu-
ture work, we intend to perform an evaluation to
show that this visualization enhances quality and
efficiency of the ITM process.
To support user interactions required by the
ITM algorithm, the visualization has an edit mode,
which is shown in Figure 5. Ongoing work in-
cludes developing appropriate visual operations to
support the following model-editing operations:
1. Adding words to a topic
2. Removing words from a topic
3. Requiring two words to be linked within a
topic (must link)
4. Requiring two words to be forced into sepa-
rate topics (cannot link)
5 Conclusion and Future Work
The visualization presented here provides a novel
way to explore topic models with incorporated
81
Figure 3: The user has hovered over the most-
central topic in the layout, which is the most con-
nected topic. The hovered topic is outlined, and
the topic name is highlighted in turquoise. The
topic names of the related topics are also high-
lighted.
Figure 4: The visualization where the user has
hovered over a word of interest. The same word
is highlighted turquoise in other topics.
Figure 5: The edit mode for the visualization.
From this mode, the user can add words, remove
words, or rename the topic.
term and topic correlation information. This is a
work in progress with the end goal to combine the
visual representation with interactive topic mod-
eling to allow users to explore (a) why a model
may not align with their intuition and (b) modify
the model as needed. We plan to deploy the tool
on real-world domain users to iteratively refine the
visualization and evaluate it in ecologically valid
settings.
References
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet
allocation. Machine Learning Journal, 3:993?1022.
Allison June-Barlow Chaney and David M Blei. 2012. Visualizing topic mod-
els. In ICWSM.
Jacob Eisenstein, Duen Horng Chau, Aniket Kittur, and Eric Xing. 2012. Top-
icviz: interactive topic exploration in document collections. In CHI?12
Extended Abstracts, pages 2177?2182. ACM.
Thomas MJ Fruchterman and Edward M Reingold. 1991. Graph draw-
ing by force-directed placement. Software: Practice and experience,
21(11):1129?1164.
Matthew J Gardner, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric
Ringger, and Kevin Seppi. 2010. The topic browser: An interactive tool
for browsing topic models. In NIPS Workshop on Challenges of Data Vi-
sualization.
Jacon Harris. 2011. Word clouds considered harm-
ful. http://www.niemanlab.org/2011/10/
word-clouds-considered-harmful/.
Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, and Alison Smith. 2013.
Interactive topic modeling. Machine Learning, pages 1?47.
JD Lafferty and MD Blei. 2006. Correlated topic models. In NIPS, Proceed-
ings of the 2005 conference, pages 147?155. Citeseer.
David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Au-
tomatic evaluation of topic coherence. In HLT, pages 100?108. ACL.
Eduarda Mendes Rodrigues, Natasa Milic-Frayling, Marc Smith, Ben Shnei-
derman, and Derek Hansen. 2011. Group-in-a-box layout for multi-
faceted analysis of communities. In ICSM, pages 354?361. IEEE.
Ben Shneiderman. 1998. Treemaps for space-constrained visualization of hi-
erarchies.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams:
Phrase and topic discovery, with an application to information retrieval. In
ICDM, pages 697?702. IEEE.
82
