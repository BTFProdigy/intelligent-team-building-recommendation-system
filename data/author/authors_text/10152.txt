Proceedings of the EACL 2009 Workshop on Cognitive Aspects of Computational Language Acquisition, pages 34?41,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Categorizing Local Contexts as a Step in Grammatical Category
Induction
Markus Dickinson
Indiana University
Bloomington, IN USA
md7@indiana.edu
Charles Jochim
Indiana University
Bloomington, IN USA
cajochim@indiana.edu
Abstract
Building on the use of local contexts, or
frames, for human category acquisition,
we explore the treatment of contexts as
categories. This allows us to examine and
evaluate the categorical properties that lo-
cal unsupervised methods can distinguish
and their relationship to corpus POS tags.
From there, we use lexical information
to combine contexts in a way which pre-
serves the intended category, providing a
platform for grammatical category induc-
tion.
1 Introduction and Motivation
In human category acquisition, the immediate lo-
cal context of a word has proven to be a reliable
indicator of its grammatical category, or part of
speech (e.g., Mintz, 2002, 2003; Redington et al,
1998). Likewise, category induction techniques
cluster word types together (e.g., Clark, 2003;
Schu?tze, 1995), using similar information, i.e.,
distributions of local context information. These
methods are successful and useful (e.g. Koo et al,
2008), but in both cases it is not always clear
whether errors in lexical classification are due to a
problem in the induction algorithm or in what con-
texts count as identifying the same category (cf.
Dickinson, 2008). The question we ask, then, is:
what role does the context on its own play in defin-
ing a grammatical category? Specifically, when do
two contexts identify the same category?
Many category induction experiments start by
trying to categorize words, and Parisien et al
(2008) categorize word usages, a combination of
a word and its context. But to isolate the effect the
context has on the word, we take the approach of
categorizing contexts as a first step towards clus-
tering words. By separating out contexts for word
clustering, we can begin to speak of better dis-
ambiguation models as a foundation for induc-
tion. We aim in this paper to thoroughly investi-
gate what category properties contexts can or can-
not distinguish by themselves.
With this approach, we are able to more thor-
oughly examine the categories used for evaluation.
Evaluation of induction methods is difficult, due to
the variety of corpora and tagsets in existence (see
discussion in Clark, 2003) and the variety of po-
tential purposes for induced categories (e.g., Koo
et al, 2008; Miller et al, 2004). Yet improving the
evaluation of category induction is vital, as eval-
uation does not match up well with grammar in-
duction evaluation (Headden III et al, 2008). For
many evaluations, POS tags have been mapped
to a smaller tagset (e.g., Goldwater and Griffiths,
2007; Toutanova and Johnson, 2008), but there
have been few criteria for evaluating the quality
of these mappings. By isolating contexts, we can
investigate how each mapping affects the accuracy
of a method and the lexicon.
Using corpus annotation also allows us to ex-
plore the relation between induced categories
and computationally or theoretically-relevant cat-
egories (e.g., Elworthy, 1995). While human cate-
gory acquisition results successfully divide a lexi-
con into categories, these categories are not neces-
sarily ones which are appropriate for many com-
putational purposes or match theoretical syntactic
analysis. This work can also serve as a platform to
help drive the design of new tagsets, or refinement
of old ones, by outlining which types of categories
are or are not applicable for category induction.
After discussing some preliminary issues in sec-
tion 2, in section 3 we examine to what extent con-
texts by themselves can distinguish different cat-
egory properties and how this affects evaluation.
Namely, we propose that corpus tagsets should
be clear about identifying syntactic/distributional
properties and about how tagset mappings for
evaluation should outline how much information
34
is lost by mapping. In section 4, in more prelimi-
nary work, we add lexical information to contexts,
in order to merge them together and see which still
identify the same category.
2 Preliminaries
2.1 Background
Research on language acquisition has addressed
how humans learn categories of words, and we use
this as a starting point. Mintz (2002) shows that
local context, in the form of a frame of two words
surrounding a target word, leads to the target?s
categorization in adults, and Mintz (2003) shows
that frequent frames supply category information
in child language corpora. A frame is not decom-
posed into its left and right sides (cf., e.g., Reding-
ton et al, 1998; Clark, 2003; Schu?tze, 1995), but
is taken as their joint occurrence (Mintz, 2003).1
For category acquisition, frequent frames are
used, those with a frequency above a certain
threshold. These predict category membership, as
the set of words appearing in a given frame should
represent a single category. The frequent frame
you it, for example, largely identifies verbs, as
shown in (1), taken from child-directed speech in
the CHILDES database (MacWhinney, 2000). For
frequent frames in six subcorpora of CHILDES,
Mintz (2003) obtains both high type and token ac-
curacy in categorizing words.
(1) a. you put it
b. you see it
The categories do not reflect fine-grained lin-
guistic distinctions, though, nor do they fully ac-
count for ambiguous words. Indeed, accuracies
slightly degrade when moving from ?Standard La-
beling?2 to the more fine-grained ?Expanded La-
beling,?3 from .98 to .91 in token accuracy and
from .93 to .91 in type accuracy. In scaling the
method beyond child-directed speech, it would
be beneficial to use annotated data, which allows
for ambiguity and distinguishes a word?s cate-
gory across corpus instances. Furthermore, even
though many frames identify the same category,
1This use of frame is different than that used for subcate-
gorization frames, which are also used to induce word classes
(e.g., Korhonen et al, 2003).
2Categories = noun, verb, adjective, preposition, adverb,
determiner, wh-word, not, conjunction, and interjection.
3Nouns split into nouns and pronouns; verbs split into
verbs, auxiliaries, and copula
the method does not thoroughly specify how to re-
late them.
It has been recognized for some time that wider
contexts result in better induction models (e.g.,
Parisien et al, 2008; Redington et al, 1998), but
many linguistic distinctions rely on lexical infor-
mation that cannot be inferred from additional
context (Dickinson, 2008), so focusing on short
contexts can provide many insights. The use of
frames allows for frequent recurrent contexts and
a way to investigate corpus categories, or POS tags
(cf., e.g., Dickinson and Jochim, 2008). An added
benefit of starting with this method is that it can be
converted to a model of online acquisition (Wang
and Mintz, 2007). For this paper, however, we
only investigate the type of information input into
the model.
2.2 Some definitions
Frequency The core idea of using frames is that
words used in the same context are associated with
each other, and the more often these contexts oc-
cur, the more confidence we have that the frame in-
dicates a category. Setting a threshold to obtain the
45 most frequent frames in each subcorpus (about
80,000 words on average), (Mintz, 2003) allows a
frame to occur often enough to be meaningful and
have a variety of target words in the frame.
To determine what category properties frames
pinpoint (section 3), we use two thresholds to de-
fine frequent. Singly occurring frames cannot pro-
vide any information about groupings of words,
so we first consider frames that occur more than
once. This gives a large number of frames, cover-
ing much of the corpus (about 970,000 tokens), but
frames with few instances have very little informa-
tion. For the other threshold, frequent frames are
those which have a frequency of 200, about 0.03%
of the total number of frames in the corpus. One
could explore more thresholds, but for compar-
ing tagset mappings, these provide a good picture.
The higher threshold is appropriate for combining
contexts (section 4), as we need more information
to tell whether two frames behave similarly.
Accuracy To evaluate, we need a measure of the
accuracy of each frame. Mintz (2003) and Red-
ington et al (1998) calculate accuracy by counting
all pairs of words (types or tokens) that are from
the same category, divided by all possible pairs of
words in a grouping. This captures the idea that
each word should have the same category as every
35
other word in its category set.
Viewing the task as disambiguating contexts
(see section 3), however, this measurement does
not seem to adequately represent cases with a ma-
jority label. For example, if three words have
the tag X and one Y , pairwise comparison re-
sults in an accuracy of 50%, even though X is
dominant. To account for this, we measure the
precision of the most frequent category instances
among all instances, e.g., 75% for the above ex-
ample (cf. the notion of purity in Manning et al,
2008). Additionally, we only use measurements
of token precision. Token precision naturally han-
dles ambiguous words and is easy to calculate in a
POS-annotated corpus.
3 Categories in local contexts
In automatic category induction, a category is of-
ten treated as a set, or cluster, of words (Clark,
2003; Schu?tze, 1995), and category ambiguity is
represented by the fact that words can appear in
more than one set. Relatedly, one can cluster word
usages, a combination of a word and its context
(Parisien et al, 2008). An erroneous classification
occurs when a word is in an incorrect set, and one
source of error is when the contexts being treated
as indicative of the same category are actually am-
biguous. For example, in a bigram model, the con-
text be identifies nouns, adjectives, and verbs,
among others.
Viewed in this way, it is important to gauge
the precision of contexts for distinguishing a cat-
egory (cf. also Dickinson, 2008). In other words,
how often does the same context identify the same
category? And how fine-grained is the category
that the context distinguishes? To test whether
a frame defines a single category in non-child-
directed speech, we focus on which categorical
properties frames define, and for this we use a
POS-annotated corpus. Due to its popularity for
unsupervised POS induction research (e.g., Gold-
berg et al, 2008; Goldwater and Griffiths, 2007;
Toutanova and Johnson, 2008) and its often-used
tagset, for our initial research, we use the Wall
Street Journal (WSJ) portion of the Penn Treebank
(Marcus et al, 1993), with 36 tags (plus 9 punc-
tuation tags), and we use sections 00-18, leaving
held-out data for future experiments.4
Defining frequent frames as those occurring at
4Even if we wanted child-directed speech, the CHILDES
database (MacWhinney, 2000) uses coarse POS tags.
least 200 times, we find 79.5% token precision.
Additionally, we have 99 frames, identifying 14
types of categories as the majority tag (common
noun (NN) being the most prevalent (37 frames)).
For a threshold of 2, we have 77.3% precision for
67,721 frames and 35 categories.5 With precision
below 80%, we observe that frames are not fully
able to disambiguate these corpus categories.
3.1 Frame-defined categories
These corpus categories, however, are composed
of a variety of morphological and syntactic fea-
tures, the exact nature of which varies from tagset
to tagset. By merging different tags, we can factor
out different types of morphological and syntac-
tic properties to determine which ones are more or
less easily identified by frames. Accuracy will of
course improve by merging tags; what is important
is for which mappings it improves.
We start with basic categories, akin to those
in Mintz (2003). Despite the differences among
tagsets, these basic categories are common, and
merging POS tags into basic categories can show
that differences in accuracy have more to do with
stricter category labels than language type. We
merged tags to create basic categories, as in table 1
(adapted from Hepple and van Genabith (2000);
see appendix A for descriptions).6
Category Corpus tags
Determiner DT, PDT, PRP$
Adjective JJ, JJR, JJS
Noun NN, NNS, PRP, NNP, NNPS
Adverb RB, RBR, RBS
Verb MD, VB, VBD, VBG, VBN,
VBP, VBZ
Wh-Det. WDT, WP$
Table 1: Tag mappings into basic categories
These broader categories result in the accuracies
in table 2, and we also record accuracies for the
similar PTB-17 tagset used in a variety of unsu-
pervised tagging experiments (Smith and Eisner,
2005), which mainly differs by treating VBG and
VBN uniquely. With token precision around 90%,
it seems that frame-based disambiguation is gener-
ally identifying basic categories, though with less
5LS (List item marker) is not identified; UH (interjection)
appears in one repeating frame, and SYM (symbol) in two.
6The 13 other linguistic tags were not merged, i.e., CC,
CD, EX, FW, IN, LS, POS, RP, SYM, TO, UH, WP, WRB.
36
accuracy than in Mintz (2003).
? 2 ? 200
Orig. 77.3% 79.5%
Merged 85.9% 91.0%
PTB-17 85.1% 89.7%
Table 2: Effect of mappings on precision
But which properties of the tagset do the
frame contexts accurately capture and which do
they not? To get at this question, we ex-
plore linguistically-motivated mappings between
the original tagset and the fully-merged tagset in
table 1. Given the predominance of verbs and
nouns, we focus on distinguishing linguistic prop-
erties within these categories. For example, sim-
ply by merging nouns and leaving all other orig-
inal tags unchanged, we move from 79.5% token
precision to 88.4% (for the threshold of 200).
Leaving all other mappings as in table 1, we
merge nouns and verbs along two dimensions:
their common syntactic properties or their com-
mon morphological properties. Ideally, we pre-
fer frames to pick out syntactic properties, since
morphological properties can assumedly be deter-
mined from word-internal properties (see Clark,
2003; Christiansen and Monaghan, 2006).
Specifically, we can merge nouns by noun
type (PRP [pronoun], NN/NNS [common noun],
NNP/NNPS [proper noun]) or by noun form, in
this case based on grammatical number (PRP
[pronoun], NN/NNP [singular noun], NNS/NNPS
[plural noun]). We can merge verbs by finite-
ness (MD [modal], VBP/VBZ/VBD [finite verb],
VB/VBG/VBN [nonfinite verb]) or by verb form
(MD [modal], VB/VBP [base], VBD/VBN [-ed],
VBG [-ing], VBZ [-s]). In the latter case, verbs
with consistently similar forms are grouped?e.g.,
see can be a baseform (VB) or a present tense verb
(VBP).
The results are given in tables 3 and 4. We
find that merging verbs by finiteness and nouns by
noun type results in higher precision. This con-
firms that contexts can better distinguish syntactic,
but not necessarily morphological, properties. As
we will see in the next section, this mapping also
maintains distinctions in the lexicon. Such use of
local contexts, along with tag merging, can be used
to evaluate tagsets which claim to be distributional
(see, e.g., Dickinson and Jochim, 2008).
It should be noted that we have only explored
Noun type Noun form
Finiteness 82.9% 81.2%
Verb form 81.2% 79.5%
Table 3: Mapping precision (freq. ? 2)
Noun type Noun form
Finiteness 86.4% 85.3%
Verb form 84.5% 83.4%
Table 4: Mapping precision (freq. ? 200)
category mappings which merge tags, ignoring
possible splits. While splitting a tag like TO (to)
into prepositional and infinitival uses would be
ideal, we do not have the information automati-
cally available. We are thus limited in our eval-
uation by what the tagset offers. Some tag splits
can be automatically recovered (e.g., splitting PRP
based on properties such as person), but if it is au-
tomatically recoverable from the lexicon, we do
not necessarily need context to identify it, an idea
we turn to in the next section.
3.2 Evaluating tagset mappings
Some of the category distinctions made by frames
are more or less important for the context to make.
For example, it is detrimental if we conflate VB
and VBP because this is a prominent ambiguity for
many words (e.g., see). On the other hand, there
are no words which can be both VBP (e.g., see)
and VBZ (e.g., sees). Ideally, induction methods
would be able to distinguish all these cases?just
as they often make distinctions beyond what is in a
tagset?but there are differences in how problem-
atic the mappings are. If we group VB and VBP
into one tag, there is no way to recover that distinc-
tion; for VBP and VBZ, there are at least different
words which inherently take the different tags.
Thus, a mapping is preferred which does not
conflate tags that vary for individual words. To
calculate this, we compare the original lexicon
with a mapped lexicon and count the number of
words which lose a distinction. Consider the
words accept and accepts: accept varies between
VB and VBP; accepts is only VBZ. When we map
tags based on verb form, we count 1 for accept,
as VB and VBP are now one tag (Verb). When
we map verbs based on finiteness, we count 0 for
these two words, as accept still has two tags (V-
nonfin, V-fin) and accepts has one tag (V-fin).
37
We evaluate our mappings in table 5 by enumer-
ating the number of word types whose distinctions
are lost by a particular mapping (out of 44,520
word types); we also repeat the token precision
values for comparison. Perhaps unsurprisingly,
grouping words based on form results in high con-
fusability (cf. the discussion of see in section 3.1).
On the other hand, merging nouns by type and
verbs by finiteness results in something of a bal-
ance between precision and non-confusability. It
is thus these types of categorizations which we can
reasonably expect induction models to capture.
Lost Precision
Mapping tags ? 2 ? 200
All mappings 3003 85.9% 91.0%
PTB-17 2038 85.1% 89.7%
N. form/V. form 2699 79.5% 83.4%
N. type/V. form 2148 81.2% 84.5%
N. form/Finite 951 81.2% 85.3%
N. type/Finite 399 82.9% 86.4%
No mappings 0 77.3% 79.5%
Table 5: Confusable word types
For induction evaluation, in addition to an ac-
curacy metric, a metric such as the one we have
just proposed is important to gauge how much cor-
pus annotation information is lost when perform-
ing tagset mappings. For example, the PTB-17
mapping (Smith and Eisner, 2005) is commonly
used for evaluating category induction (Goldwa-
ter and Griffiths, 2007; Toutanova and Johnson,
2008), yet it loses distinctions for 2038 words.
We could also define mappings which lose no
distinctions in the lexicon. Initial experiments
show that this allows no merging of nouns, and
that the resulting precision is only minimally bet-
ter than no mapping at all. We should also note
that the number of confusable words may be too
high, given errors in the lexicon (cf. Dickinson,
2008). For example, removing tags occurring less
than 10% of the time for a word results in only 305
confusable words for the Noun type/Finiteness
(NF) mapping and 1575 for PTB-17.
4 Combining contexts
We have narrowly focused on identical contexts,
or frames, for identifying categories, but this could
leave us with as many categories as frames (67,721
for ? 2, 99 for ? 200, instead of 35 and 30). We
need to reduce the number of categories without
inappropriately merging them (cf. the notion of
?completeness? in Mintz, 2003; Christiansen and
Monaghan, 2006). Thus far, we have not utilized
a frame?s target words; we turn to these now, in
order to better gauge the effectiveness of frames
for identifying categories. Although the work is
somewhat preliminary, our goal is to continue to
investigate when contexts identify the same cate-
gory. This merging of contexts is different than
clustering words (e.g., Clark, 2000; Brown et al,
1992), but is applicable, as word clustering relies
on knowing which contexts identify the same cat-
egory.
4.1 Word-based combination
On their own, frames at best distinguish only very
broad categorical properties. This is perhaps un-
surprising, as the finer-grained distinctions in cor-
pora seem to be based on lexical properties more
than on additional context (see, e.g., Dickinson,
2008). If we want to combine contexts in a way
which maps to corpus tagsets, then, we need to
examine the target words. It is likely that two sets
share the same tag if they contain the same words
(cf. overlap in Mintz, 2003). In fact, the more a
frame?s word set overlaps with another?s word set,
the more likely it is unambiguous in the first place,
as the other set provides corroborating evidence.
Therefore, we use overlap of frames? word sets as
a criterion to combine them.
This allows us to combine frames which do not
share context words. For example, in (2) we find
frames identifying baseform verbs (VB) (2a) and
frames identifying cardinal numbers (CD) (2b),
despite having a variety of context words. Their
target word sets, however, are sufficiently similar.
(2) a. will to, will the, to the, to up,
would the, to their, n?t the,
to a, to its, to that, to to
b. or cents, $ million, rose %,
a %, about %, to %, $ a,
$ billion
By viewing frames as categories, in the fu-
ture we could also investigate splitting cate-
gories, based on subsets of words, morpho-
logical/phonological cues (e.g., Christiansen and
Monaghan, 2006), or on additional context words,
better handling frames that are ambiguous.
Calculating overlap We merge frames whose
word sets overlap, using a simple weighted fre-
38
quency distance metric. We define sufficient over-
lap as the case where a given percent of the words
in one frame?s word set are found in the other?s
word set. We define this test in either direction,
as smaller sets can be a subset of a larger set. For
example, the frames the on (224 tokens) and the
of (4304 tokens) have an overlap of 78 tokens;
overlap here is 34.8% (78/224). While we could
use a more sophisticated form of clustering (see,
e.g., Manning et al, 2008), this will help deter-
mine the viability of this general approach.
Of course, two sets may share a category with
relatively few shared words, and so we transitively
combine sets of contexts. If the overlap of frames
A andB meet our overlap criterion and the overlap
of frames A and C also meet the criterion, then all
three sets are merged, even if B and C have only
a small amount of overlap.7
Using the threshold of 200, we test criteria of
30%, 40%, and 50% overlap and consider the
frames? overlap calculated as a percentage of word
types or as a percentage of word tokens. For exam-
ple, if a word type occurs 10 times in one word set
and 20 in the other, the overlap of types is 1, and
the overlap of tokens is 10. Token overlap better
captures similarities in distributions of words.
4.2 Evaluation
Table 6 shows the number of categories for the
30%, 40%, and 50% type-based (TyB) and token-
based (ToB) overlap criteria for merging. As we
can see, the overlap based on tokens in word sets
results in more categories, i.e., fewer merges.
% TyB ToB
50% 59 75
40% 42 64
30% 27 50
Table 6: Number of categories by condition
The precision of each of these criteria is given
in table 7, evaluating on both the original tagset
and the noun type/finiteness (NF) mapping. We
can see that the token-based overlap is consistently
more accurate than type-based overlap, and there
is virtually no drop in precision for any of the
token-based conditions.8 Thus, for the rest of the
evaluation, we use only the token-based overlap.
7We currently do not consider overlap of already merged
sets, e.g., between A+B and C.
8Experiments at 20% show a noticeable drop in precision.
% Tags Frames TyB ToB
50% Orig. 79.5% 76.4% 79.5%
NF 86.4% 82.8% 86.4%
40% Orig. 79.5% 75.7% 79.3%
NF 86.4% 81.8% 86.1%
30% Orig. 79.5% 74.7% 79.1%
NF 86.4% 81.7% 86.1%
Table 7: Precision of merged frames
We mentioned that if frame word sets overlap,
the less ambiguous their category should be. We
check this by looking at the difference between
merged and unmerged frames, as shown in table 8.
The number of categories are also given in paren-
theses; for example, for 30% overlap, 41 frames
are unmerged, and the remaining 58 make up 9
categories. These results confirm for this data that
frames which are merged have a higher precision.
Merged Unmerged Overall
50% 93.4% (7) 79.9% (68) 86.4% (75)
40% 89.7% (10) 81.1% (54) 86.1% (64)
30% 89.7% (9) 77.4% (41) 86.1% (50)
Table 8: Precision of merged & unmerged frames
for NF mapping (with number of categories)
But are we only merging a select, small set of
words? To gauge this, we measure how much
of the corpus is categorized by the 99 most fre-
quent frames. Namely, 46,874 tokens occur as tar-
gets in our threshold of 99 frequent frames out of
663,608 target tokens in the entire corpus,9 a re-
call of 7.1%. Table 9 shows some recall figures for
the frequent frames. There are 9621 word types in
the set of target words for the 99 frequent frames,
which is 27.2% of the target lexicon. Crucially,
though, these 9621 are realized as 523,662 target
tokens in the corpus, or 78.9%. The words cate-
gorized by the frequent frames extend to a large
portion of the corpus (cf. also Mintz, 2003).
Tokens Types Coverage
Merged (30%) 5.0% 20.0% 61.5%
Unmerged (30%) 2.0% 11.5% 65.9%
Total Overlap 7.1% 27.2% 78.9%
Table 9: Recall of frames
9Because we remove frames which contain punctuation,
the set of target tokens is a subset of all words in the corpus.
39
4.2.1 Qualitative analysis
To better analyze what is happening for future
work, we look more closely at 30% overlap. Of
the 58 frames merged into 9 categories, 54 of them
have the samemajority tag after merging. The four
frames which get merged into a different category
are worth investigating, to see the method?s limi-
tations and potential for improvement.
Of the four frames which lose their majority
tag after merging, two can be ignored when map-
ping to the NF tags. The frame it the with ma-
jority tag VBZ becomes VBD when merged, but
both are V-fin. Likewise, n?t to changes from
VB to VBN, both cases of V-nonfin. The third
case reveals an evaluation problem with the orig-
inal tagset: the frames million $ (IN) and %
$ (TO) are merged into a category labeled TO.
The tag TO is for the word to and is not split into
prepositional and infinitival uses. Corpus cate-
gories such as these, which overlap in their def-
initions yet cannot be merged (due to their non-
overlapping uses), are particularly problematic for
evaluation.
The final case which does not properly merge is
the most serious. The frame is the (37% of to-
kens as preposition (IN)) merges with is a (41%
of tokens as VBG); the merged VBG category has
an precision of 34%. The distribution of tags is rel-
atively similar, the highest percentages being for
IN and VBG in both. This highlights the point
made earlier, that more information is needed, to
split the word sets.
4.2.2 TIGER Corpus
To better evaluate frequent frames for determin-
ing categories, we also test them on the German
TIGER corpus (Brants et al, 2002), version 2,
to see how the method handles data with freer
word order and more morphological complexity.
We use the training data, with the data split as
in Dubey (2004). The frequency threshold for
the WSJ (0.03% of all frames) leaves us with
only 60 frames in the TIGER corpus, and 51 of
these frames have a majority tag of NN.10 Thus,
we adjusted the threshold to 0.02% (102 mini-
mum occurrences), thereby obtaining 119 frequent
frames, with a precision of 82.0%. For the 30%
token-based overlap (the best result for English),
frames merged into 81 classes, with 79.1% pre-
cision. These precision figures are on a par with
10We use no tagset mappings for our TIGER experiments.
English (cf. table 7).11 Part of this might be due
to the fact that NN is still a large majority (76% of
the frames). Additionally, we find that, although
the frame tokens make up only 5.2% of the corpus
and the types make up 15.9% of the target lexi-
con, those types correspond to 67.2% of the target
corpus tokens.
5 Summary and Outlook
Building on the use of frames for human category
acquisition, we have explored the benefits of treat-
ing contexts?in this case, frames?as categories
and analyzed the consequences. This allowed us
to examine a way to evaluate tagset mappings and
provide feedback on distributional tagset design.
From there, we explored using lexical information
to combine contexts in a way which generally pre-
serves the intended category.
We evaluated this on English and German, but,
to fully verify our findings, a high priority is to
perform similar experiments on more corpora, em-
ploying different tagsets, for different languages.
Additionally, we need to expand the definition of
a context to more accurately categorize contexts,
while at the same time not lowering recall.
Acknowledgements
We wish to thank the Indiana University Compu-
tational Linguistics discussion group for feedback,
as well as the three anonymous reviewers.
A Some Penn Treebank POS tags
DT Determiner
JJ Adjective
JJR Adjective, comparative
JJS Adjective, superlative
MD Modal
NN Noun, singular or mass
NNS Noun, plural
NNP Proper noun, singular
NNPS Proper noun, plural
PDT Predeterminer
PRP Personal pronoun
PRP$ Possessive pronoun
RB Adverb
RBR Adverb, comparative
RBS Adverb, superlative
VB Verb, base form
VBD Verb, past tense
VBG Verb, gerund or present participle
VBN Verb, past participle
VBP Verb, non-3rd person singular present
VBZ Verb, 3rd person singular present
WDT Wh-determiner
WP$ Possessive wh-pronoun
11Interestingly, thresholds of 20% and 10% result in simi-
larly high precision.
40
References
Brants, Sabine, Stefanie Dipper, Silvia Hansen,
Wolfgang Lezius and George Smith (2002). The
TIGER Treebank. In Proceedings of TLT-02.
Sozopol, Bulgaria.
Brown, Peter F., Peter V. deSouza, Robert L. Mer-
cer, T. J. Watson, Vincent J. Della Pietra and
Jenifer C. Lai (1992). Class-Based n-gram
Models of Natural Language. Computational
Linguistics 18(4), 467?479.
Christiansen, Morten H. and Padraic Monaghan
(2006). Discovering verbs through multiple-cue
integration. In Action Meets Word: How Chil-
dren Learn Verbs, Oxford: OUP.
Clark, Alexander (2000). Inducing Syntactic Cat-
egories by Context Distribution Clustering. In
Proceedings of CoNLL-00. Lisbon, Portugal.
Clark, Alexander (2003). Combining Distribu-
tional and Morphological Information for Part
of Speech Induction. In Proceedings of EACL-
03. Budapest.
Dickinson, Markus (2008). Representations for
category disambiguation. In Proceedings of
Coling 2008. Manchester.
Dickinson, Markus and Charles Jochim (2008). A
Simple Method for Tagset Comparison. In Pro-
ceedings of LREC 2008. Marrakech, Morocco.
Dubey, Amit (2004). Statistical Parsing for Ger-
man: Modeling syntactic properties and anno-
tation differences. Ph.D. thesis, Saarland Uni-
versity, Germany.
Elworthy, David (1995). Tagset Design and In-
flected Languages. In Proceedings of the ACL-
SIGDAT Workshop. Dublin.
Goldberg, Yoav, Meni Adler and Michael Elhadad
(2008). EM Can Find Pretty Good HMM POS-
Taggers (When Given a Good Start). In Pro-
ceedings of ACL-08. Columbus, OH.
Goldwater, Sharon and Tom Griffiths (2007). A
fully Bayesian approach to unsupervised part-
of-speech tagging. In Proceedings of ACL-07.
Prague.
Headden III, William P., David McClosky and
Eugene Charniak (2008). Evaluating Unsu-
pervised Part-of-Speech Tagging for Grammar
Induction. In Proceedings of Coling 2008.
Manchester.
Hepple, Mark and Josef van Genabith (2000).
Experiments in Structure-Preserving Grammar
Compaction. In 1st Meeting on Speech Tech-
nology Transfer. Seville, Spain.
Koo, Terry, Xavier Carreras and Michael Collins
(2008). Simple Semi-supervised Dependency
Parsing. In Proceedings of ACL-08. Columbus,
OH.
Korhonen, Anna, Yuval Krymolowski and Zvika
Marx (2003). Clustering Polysemic Subcatego-
rization Frame Distributions Semantically. In
Proceedings of ACL-03. Sapporo.
MacWhinney, Brian (2000). The CHILDES
project: Tools for analyzing talk. Mahwah, NJ:
Lawrence Erlbaum Associates, third edn.
Manning, Christopher D., Prabhakar Raghavan
and Hinrich Schu?tze (2008). Introduction to In-
formation Retrieval. CUP.
Marcus, M., Beatrice Santorini and M. A.
Marcinkiewicz (1993). Building a large anno-
tated corpus of English: The Penn Treebank.
Computational Linguistics 19(2), 313?330.
Miller, Scott, Jethran Guinness and Alex Zama-
nian (2004). Name Tagging with Word Clusters
and Discriminative Training. In Proceedings of
HLT-NAACL 2004. Boston, MA.
Mintz, Toben H. (2002). Category induction
from distributional cues in an artificial lan-
guage. Memory & Cognition 30, 678?686.
Mintz, Toben H. (2003). Frequent frames as a
cue for grammatical categories in child directed
speech. Cognition 90, 91?117.
Parisien, Christopher, Afsaneh Fazly and Suzanne
Stevenson (2008). An Incremental Bayesian
Model for Learning Syntactic Categories. In
Proceedings of CoNLL-08. Manchester.
Redington, Martin, Nick Chater and Steven Finch
(1998). Distributional Information: A Powerful
Cue for Acquiring Syntactic Categories. Cogni-
tive Science 22(4), 425?469.
Schu?tze, Hinrich (1995). Distributional Part-of-
Speech Tagging. In Proceedings of EACL-95.
Dublin, Ireland.
Smith, Noah A. and Jason Eisner (2005). Con-
trastive Estimation: Training Log-Linear Mod-
els on Unlabeled Data. In Proceedings of
ACL?05. Ann Arbor, MI.
Toutanova, Kristina and Mark Johnson (2008).
A Bayesian LDA-based Model for Semi-
Supervised Part-of-speech Tagging. In Pro-
ceedings of NIPS 2008. Vancouver.
Wang, Hao and Toben H. Mintz (2007). A Dy-
namic Learning Model for Categorizing Words
Using Frames. In Proceedings of BUCLD 32.
pp. 525?536.
41
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 42?48,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Improving Citation Polarity Classification with Product Reviews
Charles Jochim
?
IBM Research ? Ireland
charlesj@ie.ibm.com
Hinrich Sch?utze
Center for Information & Language Processing
University of Munich
Abstract
Recent work classifying citations in scien-
tific literature has shown that it is possi-
ble to improve classification results with
extensive feature engineering. While this
result confirms that citation classification
is feasible, there are two drawbacks to
this approach: (i) it requires a large anno-
tated corpus for supervised classification,
which in the case of scientific literature
is quite expensive; and (ii) feature engi-
neering that is too specific to one area of
scientific literature may not be portable to
other domains, even within scientific liter-
ature. In this paper we address these two
drawbacks. First, we frame citation clas-
sification as a domain adaptation task and
leverage the abundant labeled data avail-
able in other domains. Then, to avoid
over-engineering specific citation features
for a particular scientific domain, we ex-
plore a deep learning neural network ap-
proach that has shown to generalize well
across domains using unigram and bigram
features. We achieve better citation clas-
sification results with this cross-domain
approach than using in-domain classifica-
tion.
1 Introduction
Citations have been categorized and studied for
a half-century (Garfield, 1955) to better under-
stand when and how citations are used, and
to record and measure how information is ex-
changed (e.g., networks of co-cited papers or au-
thors (Small and Griffith, 1974)). Recently, the
value of this information has been shown in practi-
cal applications such as information retrieval (IR)
?
This work was primarily conducted at the IMS ? Uni-
versity of Stuttgart.
(Ritchie et al, 2008), summarization (Qazvinian
and Radev, 2008), and even identifying scientific
breakthroughs (Small and Klavans, 2011). We ex-
pect that by identifying and labeling the function
of citations we can improve the effectiveness of
these applications.
There has been no consensus on what aspects
or functions of a citation should be annotated and
how. Early citation classification focused more on
citation motivation (Garfield, 1964), while later
classification considered more the citation func-
tion (Chubin and Moitra, 1975). Recent stud-
ies using automatic classification have continued
this tradition of introducing a new classification
scheme with each new investigation into the use
of citations (Nanba and Okumura, 1999; Teufel
et al, 2006a; Dong and Sch?afer, 2011; Abu-Jbara
et al, 2013). One distinction that has been more
consistently annotated across recent citation clas-
sification studies is between positive and negative
citations (Athar, 2011; Athar and Teufel, 2012;
Abu-Jbara et al, 2013).
1
The popularity of this
distinction likely owes to the prominence of sen-
timent analysis in NLP (Liu, 2010). We follow
much of the recent work on citation classification
and concentrate on citation polarity.
2 Domain Adaptation
By concentrating on citation polarity we are able
to compare our classification to previous citation
polarity work. This choice also allows us to access
the wealth of existing data containing polarity an-
notation and then frame the task as a domain adap-
tation problem. Of course the risk in approaching
the problem as domain adaptation is that the do-
mains are so different that the representation of
a positive instance of a movie or product review,
for example, will not coincide with that of a posi-
1
Dong and Sch?afer (2011) also annotate polarity, which
can be found in their dataset (described later), but this is not
discussed in their paper.
42
tive scientific citation. On the other hand, because
there is a limited amount of annotated citation data
available, by leveraging large amounts of anno-
tated polarity data we could potentially even im-
prove citation classification.
We treat citation polarity classification as a sen-
timent analysis domain adaptation task and there-
fore must be careful not to define features that are
too domain specific. Previous work in citation po-
larity classification focuses on finding new cita-
tion features to improve classification, borrowing
a few from text classification in general (e.g., n-
grams), and perhaps others from sentiment analy-
sis problems (e.g., the polarity lexicon from Wil-
son et al (2005)). We would like to do as little
feature engineering as possible to ensure that the
features we use are meaningful across domains.
However, we do still want features that somehow
capture the inherent positivity or negativity of our
labeled instances, i.e., citations or Amazon prod-
uct reviews. Currently a popular approach for ac-
complishing this is to use deep learning neural net-
works (Bengio, 2009), which have been shown
to perform well on a variety of NLP tasks us-
ing only bag-of-word features (Collobert et al,
2011). More specifically related to our work, deep
learning neural networks have been successfully
employed for sentiment analysis (Socher et al,
2011) and for sentiment domain adaptation (Glo-
rot et al, 2011). In this paper we examine one
of these approaches, marginalized stacked denois-
ing autoencoders (mSDA) from Chen et al (2012),
which has been successful in classifying the po-
larity of Amazon product reviews across product
domains. Since mSDA achieved state-of-the-art
performance in Amazon product domain adapta-
tion, we are hopeful it will also be effective when
switching to a more distant domain like scientific
citations.
3 Experimental Setup
3.1 Corpora
We are interested in domain adaptation for citation
classification and therefore need a target dataset of
citations and a non-citation source dataset. There
are two corpora available that contain citation
function annotation, the DFKI Citation Corpus
(Dong and Sch?afer, 2011) and the IMS Citation
Corpus (Jochim and Sch?utze, 2012). Both corpora
have only about 2000 instances; unfortunately,
there are no larger corpora available with citation
annotation and this task would benefit from more
annotated data. Due to the infrequent use of neg-
ative citations, a substantial annotation effort (an-
notating over 5 times more data) would be nec-
essary to reach 1000 negative citation instances,
which is the number of negative instances in a sin-
gle domain in the multi-domain corpus described
below.
The DFKI Citation Corpus
2
has been used for
classifying citation function (Dong and Sch?afer,
2011), but the dataset alo includes polarity an-
notation. The dataset has 1768 citation sentences
with polarity annotation: 190 are labeled as pos-
itive, 57 as negative, and the vast majority, 1521,
are left neutral. The second citation corpus, the
IMS Citation Corpus
3
contains 2008 annotated ci-
tations: 1836 are labeled positive and 172 are la-
beled negative. Jochim and Sch?utze (2012) use
annotation labels from Moravcsik and Murugesan
(1975) where positive instances are labeled confir-
mative, negative instances are labeled negational,
and there is no neutral class. Because each of
the citation corpora is of modest size we combine
them to form one citation dataset, which we will
refer to as CITD. The two citation corpora com-
prising CITD both come from the ACL Anthol-
ogy (Bird et al, 2008): the IMS corpus uses the
ACL proceedings from 2004 and the DFKI corpus
uses parts of the proceedings from 2007 and 2008.
Since mSDA also makes use of large amounts of
unlabeled data, we extend our CITD corpus with
citations from the proceedings of the remaining
years of the ACL, 1979?2003, 2005?2006, and
2009.
There are a number of non-citation corpora
available that contain polarity annotation. For
these experiments we use the Multi-Domain Senti-
ment Dataset
4
(henceforth MDSD), introduced by
Blitzer et al (2007). We use the version of the
MDSD that includes positive and negative labels
for product reviews taken from Amazon.com in
the following domains: books, dvd, electronics,
and kitchen. For each domain there are 1000 pos-
itive reviews and 1000 negative reviews that com-
prise the ?labeled? data, and then roughly 4000
more reviews in the ?unlabeled?
5
data. Reviews
2
https://aclbib.opendfki.de/repos/
trunk/citation_classification_dataset/
3
http://www.ims.uni-stuttgart.de/
?
jochimcs/citation-classification/
4
http://www.cs.jhu.edu/
?
mdredze/
datasets/sentiment/
5
It is usually treated as unlabeled data even though it ac-
43
Corpus Instances Pos. Neg. Neut.
DFKI 1768 190 57 1521
IMS 2008 1836 172 ?
MDSD 27,677 13,882 13,795 ?
Table 1: Polarity corpora.
were preprocessed so that for each review you find
a list of unigrams and bigrams with their frequency
within the review. Unigrams from a stop list of 55
stop words are removed, but stop words in bigrams
remain.
Table 1 shows the distribution of polarity labels
in the corpora we use for our experiments. We
combine the DFKI and IMS corpora into the CITD
corpus. We omit the citations labeled neutral from
the DFKI corpus because the IMS corpus does not
contain neutral annotation nor does the MDSD. It
is the case in many sentiment analysis corpora that
only positive and negative instances are included,
e.g., (Pang et al, 2002).
The citation corpora presented above are both
unbalanced and both have a highly skewed distri-
bution. The MDSD on the other hand is evenly
balanced and an effort was even made to keep
the data treated as ?unlabeled? rather balanced.
For this reason, in line with previous work us-
ing MDSD, we balance the labeled portion of the
CITD corpus. This is done by taking 179 unique
negative sentences in the DFKI and IMS corpora
and randomly selecting an equal number of posi-
tive sentences. The IMS corpus can have multiple
labeled citations per sentence: there are 122 sen-
tences containing the 172 negative citations from
Table 1. The final CITD corpus comprises this
balanced corpus of 358 labeled citation sentences
plus another 22,093 unlabeled citation sentences.
3.2 Features
In our experiments, we restrict our features to un-
igrams and bigrams from the product review or
citation context (i.e., the sentence containing the
citation). This follows previous studies in do-
main adaptation (Blitzer et al, 2007; Glorot et al,
2011). Chen et al (2012) achieve state-of-the-art
results on MDSD by testing the 5000 and 30,000
most frequent unigram and bigram features.
Previous work in citation classification has
largely focused on identifying new features for
tually contains positive and negative labels, which have been
used, e.g., in (Chen et al, 2012).
improving classification accuracy. A significant
amount of effort goes into engineering new fea-
tures, in particular for identifying cue phrases,
e.g., (Teufel et al, 2006b; Dong and Sch?afer,
2011). However, there seems to be little consen-
sus on which features help most for this task. For
example, Abu-Jbara et al (2013) and Jochim and
Sch?utze (2012) find the list of polar words from
Wilson et al (2005) to be useful, and neither study
lists dependency relations as significant features.
Athar (2011) on the other hand reported significant
improvement using dependency relation features
and found that the same list of polar words slightly
hurt classification accuracy. The classifiers and
implementation of features varies between these
studies, but the problem remains that there seems
to be no clear set of features for citation polarity
classification.
The lack of consensus on the most useful cita-
tion polarity features coupled with the recent suc-
cess of deep learning neural networks (Collobert et
al., 2011) further motivate our choice to limit our
features to the n-grams available in the product re-
view or citation context and not rely on external
resources or tools for additional features.
3.3 Classification with mSDA
For classification we use marginalized stacked de-
noising autoencoders (mSDA) from Chen et al
(2012)
6
plus a linear SVM. mSDA takes the con-
cept of denoising ? introducing noise to make the
autoencoder more robust ? from Vincent et al
(2008), but does the optimization in closed form,
thereby avoiding iterating over the input vector to
stochastically introduce noise. The result of this
is faster run times and currently state-of-the-art
performance on MDSD, which makes it a good
choice for our domain adaptation task. The mSDA
implementation comes with LIBSVM, which we
replace with LIBLINEAR (Fan et al, 2008) for
faster run times with no decrease in accuracy. LIB-
LINEAR, with default settings, also serves as our
baseline.
3.4 Outline of Experiments
Our initial experiments simply extend those of
Chen et al (2012) (and others who have used
MDSD) by adding another domain, citations. We
train on each of the domains from the MDSD ?
6
We use their MATLAB implementation available at
http://www.cse.wustl.edu/
?
mchen/code/
mSDA.tar.
44
books dvd electronics kitchen
0.0
0.1
0.2
0.3
0.4
0.5
0.6
SVMmSDAIn?domain F1
Figure 1: Cross domain macro-F
1
results train-
ing on Multi-Domain Sentiment Dataset and test-
ing on citation dataset (CITD). The horizontal line
indicates macro-F
1
for in-domain citation classifi-
cation.
books, dvd, electronics, and kitchen ? and test on
the citation data. We split the labeled data 80/20
following Blitzer et al (2007) (cf. Chen et al
(2012) train on all ?labeled? data and test on the
?unlabeled? data). These experiments should help
answer two questions: does a larger amount of
training data, even if out of domain, improve ci-
tation classification; and how well do the differ-
ent product domains generalize to citations (i.e.,
which domains are most similar to citations)?
In contrast to previous work using MDSD, a lot
of the work in domain adaptation also leverages a
small amount of labeled target data. In our second
set of experiments, we follow the domain adap-
tation approaches described in (Daum?e III, 2007)
and train on product review and citation data be-
fore testing on citations.
4 Results and Discussion
4.1 Citation mSDA
Our initial results show that using mSDA for do-
main adaptation to citations actually outperforms
in-domain classification. In Figure 1 we com-
pare citation classification with mSDA to the SVM
baseline. Each pair of vertical bars represents
training on a domain from MDSD (e.g., books)
and testing on CITD. The dark gray bar indicates
the F
1
scores for the SVM baseline using the
30,000 features and the lighter gray bar shows the
mSDA results. The black horizontal line indicates
the F
1
score for in-domain citation classification,
which sometimes represents the goal for domain
adaptation. We can see that using a larger dataset,
even if out of domain, does improve citation clas-
sification. For books, dvd, and electronics, even
the SVM baseline improves on in-domain classifi-
cation. mSDA does better than the baseline for all
domains except dvd. Using a larger training set,
along with mSDA, which makes use of the un-
labeled data, leads to the best results for citation
classification.
In domain adaptation we would expect the do-
mains most similar to the target to lead to the
highest results. Like Dai et al (2007), we mea-
sure the Kullback-Leibler divergence between the
source and target domains? distributions. Accord-
ing to this measure, citations are most similar to
the books domain. Therefore, it is not surprising
that training on books performs well on citations,
and intuitively, among the domains in the Amazon
dataset, a book review is most similar to a scien-
tific citation. This makes the good mSDA results
for electronics a bit more surprising.
4.2 Easy Domain Adaptation
The results in Section 4.1 are for semi-supervised
domain adaptation: the case where we have some
large annotated corpus (Amazon product reviews)
and a large unannotated corpus (citations). There
have been a number of other successful attempts at
fully supervised domain adaptation, where it is as-
sumed that some small amount of data is annotated
in the target domain (Chelba and Acero, 2004;
Daum?e III, 2007; Jiang and Zhai, 2007). To see
how mSDA compares to supervised domain adap-
tation we take the various approaches presented by
Daum?e III (2007). The results of this comparison
can be seen in Table 2. Briefly, ?All? trains on
source and target data; ?Weight? is the same as
?All? except that instances may be weighted dif-
ferently based on their domain (weights are chosen
on a development set); ?Pred? trains on the source
data, makes predictions on the target data, and
then trains on the target data with the predictions;
?LinInt? linearly interpolates predictions using the
source-only and target-only models (the interpola-
tion parameter is chosen on a development set);
?Augment? uses a larger feature set with source-
specific and target-specific copies of features; see
45
Domain Baseline All Weight Pred LinInt Augment mSDA
books 54.5 54.8 52.0 51.9 53.4 53.4 57.1
dvd 53.2 50.9 56.0 53.4 51.9 47.5 51.6
electronics 53.4 49.0 50.5 53.4 54.8 51.9 59.2
kitchen 47.9 48.8 50.7 53.4 52.6 49.2 50.1
citations 51.9 ? ? ? ? ? 54.9
Table 2: Macro-F
1
results on CITD using different domain adaptation approaches.
(Daum?e III, 2007) for further details.
We are only interested in citations as the tar-
get domain. Daum?e?s source-only baseline cor-
responds to the ?Baseline? column for domains:
books, dvd, electronics, and kitchen; while his
target-only baseline can be seen for citations in the
last row of the ?Baseline? column in Table 2.
The semi-supervised mSDA performs quite
well with respect to the fully supervised ap-
proaches, obtaining the best results for books and
electronics, which are also the highest scores over-
all. Weight and Pred have the highest F
1
scores for
dvd and kitchen respectively. Daum?e III (2007)
noted that the ?Augment? algorithm performed
best when the target-only results were better than
the source-only results. When this was not the
case in his experiments, i.e., for the treebank
chunking task, both Weight and Pred were among
the best approaches. In our experiments, training
on source-only outperforms target-only, with the
exception of the kitchen domain.
We have included the line for citations to see the
results training only on the target data (F
1
= 51.9)
and to see the improvement when using all of the
unlabeled data with mSDA (F
1
= 54.9).
4.3 Discussion
These results are very promising. Although they
are not quite as high as other published results
for citation polarity (Abu-Jbara et al, 2013)
7
, we
have shown that you can improve citation polarity
classification by leveraging large amounts of an-
notated data from other domains and using a sim-
ple set of features.
mSDA and fully supervised approaches can also
be straightforwardly combined. We do not present
those results here due to space constraints. The
7
Their work included a CRF model to identify the citation
context that gave them an increase of 9.2 percent F
1
over a
single sentence citation context. Our approach achieves sim-
ilar macro-F
1
on only the citation sentence, but using a dif-
ferent corpus.
combination led to mixed results: adding mSDA
to the supervised approaches tended to improve F
1
over those approaches but results never exceeded
the top mSDA numbers in Table 2.
5 Related Work
Teufel et al (2006b) introduced automatic citation
function classification, with classes that could be
grouped as positive, negative, and neutral. They
relied in part on a manually compiled list of cue
phrases that cannot easily be transferred to other
classification schemes or other scientific domains.
Athar (2011) followed this and was the first to
specifically target polarity classification on scien-
tific citations. He found that dependency tuples
contributed the most significant improvement in
results. Abu-Jbara et al (2013) also looks at both
citation function and citation polarity. A big con-
tribution of this work is that they also train a CRF
sequence tagger to find the citation context, which
significantly improves results over using only the
citing sentence. Their feature analysis indicates
that lexicons for negation, speculation, and po-
larity were most important for improving polarity
classification.
6 Conclusion
Robust citation classification has been hindered by
the relative lack of annotated data. In this pa-
per we successfully use a large, out-of-domain,
annotated corpus to improve the citation polarity
classification. Our approach uses a deep learning
neural network for domain adaptation with labeled
out-of-domain data and unlabeled in-domain data.
This semi-supervised domain adaptation approach
outperforms the in-domain citation polarity classi-
fication and other fully supervised domain adapta-
tion approaches.
Acknowledgments. We thank the DFG for
funding this work (SPP 1335 Scalable Visual An-
alytics).
46
References
Amjad Abu-Jbara, Jefferson Ezra, and Dragomir
Radev. 2013. Purpose and polarity of citation: To-
wards NLP-based bibliometrics. In Proceedings of
NAACL-HLT, pages 596?606.
Awais Athar and Simone Teufel. 2012. Context-
enhanced citation sentiment detection. In Proceed-
ings of NAACL-HLT, pages 597?601.
Awais Athar. 2011. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceed-
ings of ACL Student Session, pages 81?87.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and Trends in Machine Learning,
2(1):1?127.
Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,
Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir Radev, and Yee Fan Tan. 2008.
The ACL anthology reference corpus: A reference
dataset for bibliographic research in computational
linguistics. In Proceedings of LREC, pages 1755?
1759.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL, pages 440?447.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Proceedings of EMNLP, pages 285?292.
Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Wein-
berger, and Fei Sha. 2012. Marginalized denoising
autoencoders for domain adaptation. In Proceedings
of ICML, pages 767?774.
Daryl E. Chubin and Soumyo D. Moitra. 1975. Con-
tent analysis of references: Adjunct or alternative to
citation counting? Social Studies of Science, 5:423?
441.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive bayes classifiers for
text classification. In AAAI, pages 540?545.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL, pages 256?263.
Cailing Dong and Ulrich Sch?afer. 2011. Ensemble-
style self-training on citation classification. In Pro-
ceedings of IJCNLP, pages 623?631.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Eugene Garfield. 1955. Citation indexes to science:
A new dimension in documentation through associ-
ation of ideas. Science, 122:108?111.
Eugene Garfield. 1964. Can citation indexing be au-
tomated? In Statistical Association Methods for
Mechanized Documentation, Symposium Proceed-
ings, pages 189?192.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of ICML, pages 513?520.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL,
pages 264?271.
Charles Jochim and Hinrich Sch?utze. 2012. Towards
a generic and flexible citation classifier based on
a faceted classification scheme. In Proceedings of
COLING, pages 1343?1358.
Bing Liu. 2010. Sentiment analysis and subjectivity.
In Nitin Indurkhya and Fred J. Damerau, editors,
Handbook of Natural Language Processing, Second
Edition. CRC Press, Taylor and Francis Group.
Michael J. Moravcsik and Poovanalingam Murugesan.
1975. Some results on the function and quality of
citations. Social Studies of Science, 5:86?92.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In Proceedings of IJCAI, pages 926?
931.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proceedings of COLING, pages 689?
696.
Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for information
retrieval. In Proceedings of CIKM, pages 213?222.
Henry G. Small and Belver C. Griffith. 1974. The
structure of scientific literatures I: Identifying and
graphing specialties. Science Studies, 4(1):17?40.
Henry Small and Richard Klavans. 2011. Identifying
scientific breakthroughs by combining co-citation
analysis and citation context. In Proceedings of In-
ternational Society for Scientometrics and Informet-
rics.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151?161.
47
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006a. An annotation scheme for citation function.
In Proceedings of SIGdial Workshop on Discourse
and Dialogue, pages 80?87.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006b. Automatic classification of citation function.
In Proceedings of EMNLP, pages 103?110.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of ICML, pages 1096?1103.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP, pages 347?354.
48
