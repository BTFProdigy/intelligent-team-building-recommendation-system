247
248
249
250
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 169?172,
Prague, June 2007. c?2007 Association for Computational Linguistics
An OWL Ontology for HPSG
Graham Wilcock
University of Helsinki
PO Box 9
00014 Helsinki, Finland
graham.wilcock@helsinki.fi
Abstract
The paper presents an OWL ontology for
HPSG. The HPSG ontology is integrated
with an existing OWL ontology, GOLD, as a
community of practice extension. The basic
ideas are illustrated by visualizations of type
hierarchies for parts of speech.
1 Introduction
The paper presents an OWL ontology for HPSG
(Head-driven Phrase Structure Grammar) (Sag et al,
2003). OWL is the W3C Web Ontology Language
(http://www.w3.org/2004/OWL). An existing ontol-
ogy is used as a starting point: GOLD (Section 2)
is a general ontology for linguistic description. As
HPSG is a more specific linguistic theory, the HPSG
ontology (Section 3) is integrated inside GOLD as
a sub-ontology known as a community of practice
extension (Section 4).
2 GOLD: A General Ontology for
Linguistic Description
GOLD, a General Ontology for Linguistic Descrip-
tion (http://www.linguistics-ontology.org/) (Farrar
and Langendoen, 2003) is an OWL ontology that
aims to capture ?the general knowledge of the field
that is usually possessed by a well trained linguist.
This includes knowledge that potentially forms the
basis of any theoretical framework. In particular,
GOLD captures the fundamentals of descriptive lin-
guistics. Examples of such knowledge are ?a verb
is a part of speech?, ?gender can be semantically
grounded?, or ?linguistic expressions realize mor-
phemes?.? (Farrar and Lewis, 2005).
As far as possible GOLD uses language-neutral
and theory-neutral terminology. For instance, parts
of speech are subclasses of gold:GrammaticalUnit
as shown in Figure 1. As GOLD is language-neutral,
a wide range of parts of speech are included. For
example, both Preposition and Postposition are in-
cluded as subclasses of Adposition. The classes in
the OWLViz graphical visualization (on the right in
Figure 1) have been selected from the complete list
in the Asserted Hierarchy (on the left).
Originally GOLD was intended to be neutral
where linguistic theories had divergent views, but
a recent development is the idea of supporting dif-
ferent sub-communities as communities of practice
(Farrar and Lewis, 2005) within the GOLD frame-
work. A community of practice may focus on de-
veloping a consensus in a specific area, for example
in phonology or in Bantu languages. On the other
hand, communities of practice may focus on com-
peting theories, where each sub-community has its
own distinctive terminology and divergent concep-
tualization. In this case, the aim is to capture ex-
plicitly the relationship between the sub-community
view and the overall framework, in the form of a
Community Of Practice Extension (COPE) (Farrar
and Lewis, 2005). A COPE is a sub-ontology that
inherits from, and extends, the overall GOLD on-
tology. Sub-ontology classes are distinguished from
each other by different namespace prefixes, for ex-
ample gold:Noun and hpsg:noun.
3 An OWL Ontology for HPSG
HPSG OWL is an OWL ontology for HPSG that is
currently under development. As the aims of the first
version of the ontology are clarity and acceptability,
169
Figure 1: Parts of speech in GOLD
it carefully follows the standard textbook version of
HPSG by Sag et al (2003). This also means that the
first version is English-specific, as the core gram-
mars presented in the textbook are English-specific.
In HPSG OWL, parts of speech are subclasses of
hpsg:pos, as shown in Figure 2. As this version is
English-specific, it has prepositions (hpsg:prep) but
not postpositions. Parts of speech that have agree-
ment features (in English) form a distinct subclass
hpsg:agr-pos including hpsg:det (determiner) and
hpsg:verb. Within hpsg:agr-pos, hpsg:comp (com-
plementizer) and hpsg:noun form a further subclass
hpsg:nominal. This particular conceptualization of
the type hierarchy is specific to (Sag et al, 2003).
The Prote?ge?-OWL (http://protege.stanford.edu)
ontology editor supports both visual construction
and visual editing of the hierarchy. For example, if
hpsg:adj had agreement features, it could be moved
under hpsg:agr-pos by a simple drag-and-drop (in
the Asserted Hierarchy pane on the left). Both the
visualization (in the OWLViz pane on the right) and
the underlying OWL statements (not shown) are au-
tomatically generated. The grammar writer does not
edit OWL statements directly.
This is a significant advantage of the new technol-
ogy over current grammar development tools. For
example, LKB (Copestake, 2002) can produce a vi-
sualization of the type hierarchy from the underlying
Type Definition Language (TDL) statements, but the
hierarchy can only be modified by textually editing
the TDL statements.
4 A Community of Practice Extension
HPSG COPE is a community of practice extension
that integrates the HPSG ontology within GOLD.
The COPE is an OWL ontology that imports both
the GOLD and the HPSG ontologies. Apart from
the import statements, the COPE consists entirely of
170
Figure 2: Parts of speech in HPSG
rdfs:subClassOf and rdfs:subPropertyOf statements.
HPSG COPE defines HPSG classes as subclasses of
GOLD classes and HPSG properties as subproper-
ties of GOLD properties.
In the COPE, parts of speech in HPSG are sub-
sumed by appropriate parts of speech in GOLD,
as shown in Figure 3. In some cases this is
straightforward, for example hpsg:adj is mapped to
gold:Adjective. In other cases, the HPSG theory-
specific terminology differs significantly from the
theory-neutral terminology in GOLD. Some of
the mappings are based on definitions of the
HPSG terms given in a glossary in (Sag et al,
2003), for example the mapping of hpsg:conj
(conjunction) to gold:CoordinatingConnective and
the mapping of hpsg:comp (complementizer) to
gold:SubordinatingConnective.
Properties in HPSG OWL are defined by HPSG
COPE as subproperties of GOLD properties. For ex-
ample, the HPSG OWL class hpsg:sign (Sag et al,
2003) (p. 475) properties:
PHON type: list (a sequence of word forms)
SYN type: gram-cat (a grammatical category)
SEM type: sem-struc (a semantic structure)
are mapped to the GOLD class gold:LinguisticSign
properties:
hasForm Range: PhonologicalUnit
hasGrammar Range: GrammaticalUnit
hasMeaning Range: SemanticUnit
by the HPSG COPE rdfs:subPropertyOf definitions:
hpsg:PHON subproperty of gold:hasForm
hpsg:SYN subproperty of gold:hasGrammar
hpsg:SEM subproperty of gold:hasMeaning
5 Conclusion
The paper has described an initial version of an
OWL ontology for HPSG, together with an approach
to integrating it with GOLD as a community of prac-
171
Figure 3: Parts of speech in the Community of Practice Extension
tice extension. Perhaps a rigorous foundation of
typed feature structures and a clear type hierarchy
makes HPSG more amenable to expression as an on-
tology than other linguistic theories.
Prote?ge?-OWL supports visual development and
visual editing of the ontology. This is a significant
practical advantage over existing grammar develop-
ment tools. OWLViz provides graphical visualiza-
tions of any part of the ontology.
OWL DL (Description Logic) reasoners can be
run inside Prote?ge? to check consistency and to do
cross-classification. One current research topic is
how to exploit reasoners to perform automatically
the kind of cross-classification that is widely used in
HPSG linguistic analyses.
Another current topic is how to implement HPSG
lexical rules and grammar rules in the ontology. An
interesting possibility is to use the W3C Semantic
Web Rule Language, SWRL (Wilcock, 2006).
References
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stanford,
CA.
Scott Farrar and D. Terence Langendoen. 2003. A lin-
guistic ontology for the semantic web. GLOT Interna-
tional, 7.3:97?100.
Scott Farrar and William D. Lewis. 2005. The GOLD
Community of Practice: An infrastructure for linguis-
tic data on the web. http://www.u.arizona.edu/?farrar/.
Ivan A. Sag, Thomas Wasow, and Emily Bender. 2003.
Syntactic Theory: A Formal Introduction. CSLI Pub-
lications, Stanford, CA.
Graham Wilcock. 2006. Natural language parsing with
GOLD and SWRL. In RuleML-2006, Rules and Rule
Markup Languages for the Semantic Web (Online Pro-
ceedings), Athens, GA.
172
Condence-based Adaptivity in Response Generation
for a Spoken Dialogue System
Kristiina Jokinen
University of Art and Design Helsinki
00560 Helsinki, Finland
kjokinen@uiah.fi
Graham Wilcock
University of Helsinki
00014 Helsinki, Finland
Graham.Wilcock@helsinki.fi
Abstract
The paper addresses the issue of how to increase
adaptivity in response generation for a spoken
dialogue system. Realization strategies for dia-
logue responses depend on communicative con-
dence levels and interaction management goals.
We rst describe a Java/XML-based generator
which produces dierent realizations of system
responses based on agendas specied by the di-
alogue manager. We then discuss how greater
adaptivity can be achieved by using a set of dis-
tinct generator agents, each of which is special-
ized in its realization strategy (e.g. highly ellip-
tical or highly explicit). This allows a simpler
design of each generator agent, while increas-
ing the overall system adaptivity to meet the
requirements for exible cooperation in incre-
mental and immediate interactive situations.
1 Introduction
When describing the desired characteristics of
human-computer interaction, the common key-
words are cooperation and naturalness. Coop-
eration is used to refer to the participants' abil-
ity to collaborate with each other on a given
task and to provide informative and helpful con-
tributions in a given task context. Naturalness,
however, describes the participants' reaction
which is appropriate in the current communica-
tive context, and usually presupposes reasoning
through which the participants can adapt them-
selves to the requirements of the situation and
to the knowledge level of their partner.
Naturalness in spoken interaction can be
characterised by features such as incrementality
and immediacy (Jokinen et al, 1998). Speak-
ers exchange information and present the new
information in a stepwise manner, constructing
a common ground by providing new pieces of
relevant information to complete the task that
the interaction was initiated for. They mon-
itor their own presentations and react to the
partner's contributions immediately, often si-
multaneously, to prevent potential misunder-
standings growing and causing problems to the
interaction. Spoken dialogue systems that aim
at natural interaction with the user should thus
have capabilities for incremental and immedi-
ate management of interaction. In other words,
they should be able to produce responses that
take into account the requirements of an incre-
mental and immediate interactive situation. In
this paper we discuss how various types of sys-
tem responses can be generated taking into ac-
count the special requirements of incremental
and immediate interaction situations.
The paper is structured as follows. In Sec-
tion 2 we discuss concrete examples from a spo-
ken dialogue system, in which dierent forms
of surface realization are required in order to
achieve interaction management goals. This
may involve, for example, deliberate repetition
of old information to get implicit conrmation
of speech recognition accuracy. When there is
a high level of condence in the eectiveness of
the communication channel, however, the pre-
ferred form of response realization is to include
only the new information in the response.
Section 3 describes an implementation of dia-
logue response generation based on Java, XML
and XSL transformations. The implementation
can generate the dierent realizations discussed
in Section 2. The way in which the genera-
tor chooses between the dierent realizations
is based on detailed specications of the infor-
mation status of dierent concepts, given in an
agenda by the dialogue manager component.
In Section 4 we then discuss an alternative
approach to the choice of realization, in which
a set of distinct generation agents are used, each
agent being specialized in its realization strat-
egy (e.g. highly elliptical or highly explicit).
The design of the individual generation agents
is therefore simpler, but the system as a whole
increases in adaptivity by choosing which agent
to use according to the wider dialogue context.
We describe a Java and XML-based framework
which supports this architecture, and discuss a
strategy for adaptivity based on communicative
condence.
2 Interaction Management
In this section we discuss concrete examples
from a spoken dialogue system. The domain
is public transportation in Helsinki.
2.1 Agenda, NewInfo and Topic
To enable incremental presentation and imme-
diate reaction, Jokinen et al (1998) structure
the context with the help of NewInfo and Topic,
so that the generator can distinguish the new in-
formation that is meant to be put across in the
current dialogue situation, and the background
information that the new information is linked
to. The pool of contextual information includes
an agenda, a set of concepts marked with the
help of 'topic' and 'newinfo' tags, the tags be-
ing determined by the dialogue manager which
decides how the system is to react next.
The generator can freely use the tagged pieces
of information in order to realise the system's
intention as a surface string, but it is not forced
to include in the response all the concepts that
the dialogue manager has designated as rele-
vant in the agenda. Thus the dialogue manager
and the generator communicate via the specif-
ically marked conceptual items in the shared
knowledge-base, but they both make their own
decisions on the basis of their own reasoning and
task management. The dialogue manager need
not know about particular rules of surface real-
isation while the generator need not know how
to decide the information status of the concepts
in the current dialogue situation.
While the notions of NewInfo and Topic are
often used to illustrate the characteristics of
word-order variation, their importance in spo-
ken dialogue systems can be mostly shown in
the planning process that lies at the border
of dialogue processing and tactical generation.
Consider rst the following questions by the
user:
(1) Which bus goes to Malmi?
(2) How do I get to Malmi?
The dialogue manager has analysed them as
timetable requests related to the user's going to
Malmi. It has also recognized that NewInfo in
(1) is the information concerning bus numbers
while NewInfo in (2) concerns means of trans-
portation.
In the case of (1), the information that the
dialogue manager gets from the task manager
which consults the timetable database, is that
there is bus 74 that goes to Malmi. The dialogue
manager decides to put the following concepts
into the agenda in the shared knowledge pool
(using XML as discussed in Section 3):
<agenda>
<concept info="Topic">
<type>means-of-transportation</type>
<value>bus</value>
</concept>
<concept info="Topic">
<type>destination</type>
<value>malmi</value>
</concept>
<concept info="Topic">
<type>bus</type>
<value>exists</value>
</concept>
<concept info="NewInfo">
<type>busnumber</type>
<value>74</value>
</concept>
</agenda>
The dialogue manager also tags the concepts
as NewInfo or Topic, reecting its knowledge of
how the concepts relate to the current dialogue
situation. The concept 'busnumber' is tagged
as NewInfo, and the other three as Topic, since
this is a match with the new information asked
in the previous utterance, and the user will be
likely to link the response correctly.
The generator can then select the concepts
and decide the surface realisation as described
in Section 4. The simplest realisation is:
(1a) Number 74.
2.2 Indirect Requests
For example (2), however, the situation is some-
what more complicated since the user question
can be understood either as a literal question
about public transportation to Malmi, or as an
indirect request for buses that go to Malmi. In-
formation about the previous dialogue situation
must thus be used, and we relate the dierence
to the Topic of the conversation: in the 'literal'
case, the Topic is Malmi, the destination where
the speaker wants to nd out transportation
for, whereas in the indirect request, the Topic is
the bus as a means of transportation to Malmi.
Consequently, in the 'literal' case, the dialogue
manager consults the task manager by request-
ing a value for the concept 'means of trans-
portation', while in the indirect request, the di-
alogue manager requests task manager to give
a value for the busnumber. In both cases, how-
ever, the information that the dialogue man-
ager gets from the task manager is the same:
that there exists a means of transportation to
Malmi, namely a bus and the busnumber is 74.
1
When deciding on the response to the 'lit-
eral' case, the dialogue manager regards the
means of transportation as NewInfo and the
destination to Malmi as Topic, continuing the
information structure of the previous question.
The two other concepts, 'bus' and 'busnumber',
are also tagged as new but since the NewInfo
that matches the dialogue situation concerns
the public transportation to Malmi, the piece of
information of the bus number is extra informa-
tion that can be seen as a sign of cooperation on
the system's side, rather than a necessary new
information to be told to the user, i.e. they
can be added in the response if the time con-
straints allow this and the level of cooperation
so requests.
The agenda in the shared knowledge pool in
case (2a) is as follows:
<agenda>
<concept info="NewInfo">
<type>means-of-transportation</type>
<value>bus</value>
</concept>
<concept info="Topic">
1
The dialogue manager and task manager commu-
nicate with each other via a particular task-form which
has as its parameters the concepts important for the task
manager to fetch information from the database. If the
form is lled in so that a database query can be per-
formed, the task manager returns the form with all the
appropriate parameters lled in, and thus lets the dia-
logue manager decide on the status of the parameters
and their values with respect to the dialogue situation.
<type>destination</type>
<value>malmi</value>
</concept>
<concept info="NewInfo">
<type>bus</type>
<value>exists</value>
</concept>
<concept info="NewInfo">
<type>busnumber</type>
<value>74</value>
</concept>
</agenda>
In the case of an indirect request, the dia-
logue manager again relies on the dialogue con-
text when tagging the concepts for the agenda.
The Topic is the means of transportation to
Malmi, whereas the NewInfo concerns the bus-
number, and so only the 'bus' and 'busnumber'
are tagged as new. For (2b), the shared knowl-
edge is thus as follows:
<agenda>
<concept info="Topic">
<type>means-of-transportation</type>
<value>bus</value>
</concept>
<concept info="Topic">
<type>destination</type>
<value>malmi</value>
</concept>
<concept info="NewInfo">
<type>bus</type>
<value>exists</value>
</concept>
<concept info="NewInfo">
<type>busnumber</type>
<value>74</value>
</concept>
</agenda>
The dierence in the system responses is re-
ected in the alternatives (2a) and (2b):
(2a) By bus - number 74.
(2b) Bus 74 goes there.
2.3 Condence Levels
The next example is related to a dierent as-
pect of spoken dialogue systems: condence in
speech recognition results. The dialogue man-
ager gets the recognized words together with
their recognition scores, and decides on the ap-
propriate action depending on the condence
levels.
(3) When will the next bus leave for Malmi?
(a) 2.20pm
(b) It will leave at 2.20pm
(c) The next bus to Malmi leaves at
2.20pm
As is common, we assume that if recognition
is above a certain condence level, the system
will use the simplest and most straightforward
answer, while if the recognition error becomes
bigger, a conrmation strategy has to be used.
Thus response (3a) is used when the system has
condence that the user has indeed asked about
new information concerning the next bus leav-
ing for Malmi (cf. 1a). Response (3b) is also
used in the similar situation where the system is
condent about its recognition, but the dialogue
situation diers from the one in (3a) in that
now the system assumes that the user expects a
polite full response, instead of an elliptical an-
swer as in (3a) where the user has talked about
the buses to Malmi and just wants to check the
next one leaving. The alternative (3c) is used
when the system explicitly wants to conrm the
Topic (= next bus to Malmi), so as not to allow
user to draw false implicatures about which bus
timetable the answer concerns.
The agendas for the alternatives (3a) and (3b)
are similar, and the dierence in the surface re-
alizations is due to the dierent interaction his-
tory: in the former case the Topic continues and
the dialogue history contains the concepts des-
tination and bus as previous Topics, whereas in
the latter case, the previous Topics may be dif-
ferent concepts or there may me no previous
Topic at all (beginning of the dialogue).
(3a,b)
<agenda>
<concept info="Topic">
<type>means-of-transportation</type>
<value>bus</value>
</concept>
<concept info="Topic">
<type>destination</type>
<value>malmi</value>
</concept>
<concept info="Topic">
<type>bus</type>
<value>exists</value>
</concept>
<concept info="NewInfo">
<type>bustime</type>
<value>2.20pm</value>
</concept>
</agenda>
Also the agenda for the alternative (3c) looks
the same, except for the feature <confidence>
which characterizes the system's own evaluation
of how condent it is of the correctness of the
recognized concepts. The value 1 marks cer-
tainty as in the case of bustime whose value is
retrieved from the database. This feature has
been left out in the other representations for
the sake of clarity: if the condence is above
the threshold, the concept is treated according
to its information status in the shared pool.
(3c)
<agenda>
<concept info="Topic">
<type>means-of-transportation</type>
<value>bus</value>
<confidence>0.6</confidence>
</concept>
<concept info="Topic">
<type>destination</type>
<value>malmi</value>
<confidence>0.2</confidence>
</concept>
<concept info="Topic">
<type>bus</type>
<value>exists</value>
<confidence>0.6</confidence>
</concept>
<concept info="NewInfo">
<type>bustime</type>
<value>2.20pm</value>
<confidence>1.0</confidence>
</concept>
</agenda>
3 Dialogue Response Generation
The system's competence in dialogue manage-
ment is shown in the two tasks that the sys-
tem must perform: evaluating the user goal,
and response generation. The former results
in strategic decision about operationally appro-
priate goals, while the latter concerns how the
same goal can be realised in dierent ways in
dierent contexts.
We now describe the framework which per-
forms the dialogue response generation. The
content determination has been done by the di-
alogue manager, which has selected the relevant
concepts to put on the agenda. The discourse
planning is based closely on the specication of
Topic and NewInfo by the dialogue manager,
but also includes specic decisions by the gener-
ator described in Section 3.2. The response gen-
eration continues with an aggregation stage, de-
scribed in Section 3.3, followed by a stage which
combines lexicalization and generation of refer-
ring expressions, described in Section 3.4. Mor-
phological generation, which is very important
for one of the languages generated (Finnish), is
done in a separate stage.
The framework is based on Java, XML and
XSL transformations. The implemented system
can generate the responses which we have dis-
cussed. In the next section, we will describe an
extension to this framework, suitable for adap-
tive and exible response generation.
3.1 A Pipeline Architecture
The generator starts from an agenda of con-
cepts specied in XML, set up by the dialogue
manager as shown in the examples in Section 2.
The generator produces linguistic output which
is also specied in XML, to be passed to the
speech synthesizer. We are therefore generating
XML from XML. The simplest way to do this
is to apply a set of XML transformations spec-
ied in XSL (XML Stylesheet Language). We
do this using the Xalan XSL Processor (Apache
XML Project, 2001) which is open-source soft-
ware written entirely in Java.
With the Xalan XSL Processor it is easy to
set up a sequence of transformations, in which
the output of one transformation becomes the
input to the next transformation. This kind of
\piping" is a natural way to implement the stan-
dard pipeline architecture regularly used in nat-
ural language generation systems (Reiter and
Dale, 2000).
The ease of setting up a pipeline architecture
with XSL raises the general question of whether
XSL transformations are suitable for wider use
in NLG systems. This is discussed by Cawsey
(2000), who concludes that relatively simple
XSL transformations can be used for generation
when the input is fairly constrained, but XSL is
not suitable for less constrained input, when we
need to turn to general purpose programming
languages or NLG tools.
However, XSL can be combined with gen-
eral purpose programming languages by embed-
ding extension functions in the XSL templates.
These functions can be written in Java (Apache
XML Project, 2001). This means that even
where general purpose programming languages
are required for specic purposes, such as com-
plex morphology, a pipeline of XSL transforma-
tions can still be used as a general framework.
3.2 Focus-based Generation
The model of dialogue response generation
which we use is based on generation from the
new information focus, as proposed by Jokinen
et al (1998). In this model, response planning
starts from the new information focus, called
NewInfo. One of the tasks of the generator is
to decide how to present the NewInfo to the
user: whether it should be presented by itself
or whether it should be wrapped in appropriate
linking information.
The wrapping of the NewInfo depends on
the pragmatic requirements of the dynamic dia-
logue context. When the context permits a u-
ent exchange of contributions, wrapping will be
avoided and the response will be based on new
information only. When the context requires
more clarity and explicitness, the new informa-
tion will be wrapped by Topic information in
order to avoid ambiguity and misunderstand-
ing. When the communication channel is work-
ing well, wrapping will be reduced, and when
there are uncertainties about what was actually
said, wrapping will be increased to provide im-
plicit conrmation.
Typically, XSL transformations are used
to convert information content represented in
XML into a desired presentation format, for ex-
ample in HTML. There is usually no need for
complex re-ordering of the content. Here how-
ever, the generator must convert the unordered
bag of concepts in the agenda into a syntacti-
cally correct ordered sequence of words to be
passed to the speech synthesizer. Also, the new
information focus tends to come last in surface
order, so the linking information (if any) will
generally precede the new information in the
surface realization.
Simple reordering can be performed in XSL,
for example by using XSL modes. We have ex-
perimented with applying XSL templates rst
with a Topic mode (if required), followed by a
NewInfo mode. The usefulness of XSL modes
for such purposes is noted by Cawsey (2000).
However, as we must also handle detailed syn-
tactic ordering, we use aggregation templates as
described in Section 3.3.
If the real-time requirements of the system al-
low su?cient time, the generator can decide on
the optimum way to wrap the new information,
but if there is extreme urgency to produce a re-
sponse, the generator can simply give the new
information without wrapping it. If this leads
to misunderstanding, the system can attempt
to repair this in subsequent turns. In this sense,
the model oers an any-time algorithm, impor-
tant for providing incremental and immediate
responses for spoken interactive situations.
3.3 Aggregation
The aggregation stage selects those concepts
marked as NewInfo as the basis for generation,
and also decides whether NewInfo will be the
only output, or whether it will be preceded by
the Topic linking concepts.
In order to implement detailed syntactic or-
dering, we use aggregation templates as a form
of sentence plan specication. The aggregation
templates are implemented by means of XSL
named templates, as in the following simplied
example:
<xsl:template name="NUM-DEST-TIME">
<aggregation>
<tree><node>S</node>
<tree><node>NP</node>
<xsl:copy-of select=".
/concept[type='busnumber']"/>
</tree>
<tree><node>V</node>
<xsl:copy-of select=".
/concept[type='bus']"/>
</tree>
<tree><node>PP</node>
<xsl:copy-of select=".
/concept[type='destination']"/>
</tree>
<tree><node>PP</node>
<xsl:copy-of select=".
/concept[type='bustime']"/>
</tree>
</tree>
</aggregation>
</xsl:template>
The selected aggregation template creates a
new XML document instance, with root node
<aggregation>. Its child nodes are one or more
<tree> nodes, containing syntactic categories
and other features. The trees contain variable
slots, which will be lled in later by the lexical-
ization and referring expression stages. In the
aggregation stage, the concepts from the agenda
are copied directly into the appropriate slots by
means of <xsl:copy-of> statements.
Our aggregation templates are quite similar
to the syntactic templates described by Theune
(2000). As argued by van Deemter et al (1999),
this kind of syntactic template-based approach,
which rather resembles TAG-based generation,
is fundamentally well-founded.
The selection of an appropriate aggregation
template is based on which concept types are in
the agenda and on their information status as
Topic or NewInfo. The logic is implemented by
means of nested <xsl:choose> statements, as
in the following example:
<!-- CHOOSE TEMPLATE BASED ON AGENDA -->
<xsl:template match="agenda">
<xsl:choose>
<xsl:when test="concept[@info='NewInfo']
/type='means-of-transportation'">
<xsl:call-template name="BY-TRANSPORT"/>
</xsl:when>
<xsl:when test="concept[@info='NewInfo']
/type='bus'">
<xsl:choose>
<xsl:when test="concept[@info='NewInfo']
/type='busnumber'">
<xsl:call-template name="NUM-DEST-TIME"/>
</xsl:when>
...
</xsl:choose>
</xsl:when>
<xsl:when test="concept[@info='NewInfo']
/type='busnumber'">
<xsl:call-template name="NUMBER-ONLY"/>
</xsl:when>
...
</xsl:choose>
</xsl:template>
Here, if means-of-transportation is NewInfo
as in (2a), the template named BY-TRANSPORT
is selected. If means-of-transportation is not
NewInfo, but bus and busnumber are NewInfo
as in (2b), template NUM-DEST-TIME is selected.
If only busnumber is NewInfo, as in (1a), the
template NUMBER-ONLY is selected.
3.4 Referring Expressions
In the lexicalization and referring expression
stages of the response generation pipeline, the
concepts in the aggregation templates are re-
placed by lexical items and referring expres-
sions. In general, concepts which are marked
as Topic are realized as pronouns, as shown by
the following simplied examples:
<!-- REFERRING EXPRESSIONS: PRONOUNS -->
<xsl:template
match="concept[@info='Topic']"
mode="referring-expression">
<xsl:choose>
<xsl:when test="type='busnumber'">
<xsl:text> it </xsl:text>
</xsl:when>
<xsl:when test="type='destination'">
<xsl:text> there </xsl:text>
</xsl:when>
<xsl:when test="type='bustime'">
<xsl:text> then </xsl:text>
</xsl:when>
</xsl:choose>
</xsl:template>
Here, a destination concept marked as Topic
is pronominalized as there, as in (2b). By con-
trast, concepts which are marked as NewInfo
are realized as full descriptions. In the following
template, the destination concept is realized by
a prepositional phrase, to followed by the text
value of the destination placename, obtained by
the <xsl:value-of> statement.
<!-- REFERRING EXPRESSIONS: DESCRIPTIONS -->
<xsl:template
match="concept[@info='NewInfo']"
mode="referring-expression">
<xsl:choose>
<xsl:when test="type='busnumber'">
<xsl:text> number </xsl:text>
<xsl:value-of select="value/text()"/>
</xsl:when>
<xsl:when test="type='destination'">
<xsl:text> to </xsl:text>
<xsl:value-of select="value/text()"/>
</xsl:when>
<xsl:when test="type='bustime'">
<xsl:text> at </xsl:text>
<xsl:value-of select="value/text()"/>
</xsl:when>
</xsl:choose>
</xsl:template>
The above examples are simplied to show
simple text output. The nal stages of response
generation actually perform syntactic and mor-
phological realization producing XML output
(SABLE or VoiceXML) which is passed to the
speech synthesizer.
4 Condence-based Adaptivity
In general, when condence in speech recog-
nition accuracy goes down, the dialogue sys-
tem needs to adapt by increasing the repetition
of old information to check that it is correct.
When condence in speech recognition accuracy
is high, the system should adapt by reducing the
repetition of old information, given the dialogue
context itself does not require this. Normally,
with high speech recognition condence, a u-
ent dialogue will be made up of responses with
only new information.
4.1 A Development Framework
In order to allow this kind of variation in the re-
sponses produced, the framework in which the
dialogue management is embedded must itself
be designed specically to support adaptivity.
One such system is the Jaspis adaptive speech
application framework (Turunen and Hakuli-
nen, 2000). Jaspis is a general agent-based de-
velopment architecture, and on the most general
level it contains managers which handle general
coordination between the system components
and functional modules (such as the Input Man-
ager, the Dialogue Manager and the Presenta-
tion Manager). Within each manager there are
several agents which handle various interaction
situations, as well as a set of evaluators which
try to choose the best possible agent to handle
each situation. The architecture also exploits
a shared knowledge-base called the Information
Storage, where the information about the cur-
rent state of the system is kept and which each
of the agents can read and update.
The adaptivity-oriented architecture of our
dialogue system is shown in Figure 1 (the In-
put Manager is left out). The Dialogue Man-
ager consists of a number of dialogue agents that
are experts on one specic aspect of dialogue
management and whose activities are controlled
and coordinated by a particualr dialogue con-
troller (which thus currently acts as the central
evaluator for all the dialogue agents). The Di-
alogue Manager decides what to say next on
Figure 1: Part of the system architecture
the basis of the dialogue context recorded in
the Information Storage by the various agents,
and by consulting the Task Manager, whenever
the requested information requires factual in-
formation about the task itself. The output of
this reasoning is expressed in terms of concepts
marked as NewInfo and Topic, and stored in the
shared Information Storage in the XML format
as shown in the examples in Section 2.
The response generation takes place in the
presentation management module, which con-
tains several generator agents, each of which
specialized in one particular type of generation.
The agents may, for example, generate in dier-
ent languages (we are developing generators for
English and Finnish). In the current implemen-
tation, we mainly consider agents for pronomi-
nalization, explicitness and politeness.
We are developing generation agents at three
distinct explicitness levels. The rst agent gen-
erates NewInfo only, and is suitable for quick in-
formal interactions with high speech recognition
condence like example (3a). The second agent
generates NewInfo wrapped by Topic, where
Topic is normally realized as a minimal refer-
ring expression such as a pronoun. This agent
is suitable for more polite interactions with good
speech recognition, as in example (3b). The
third agent generates a fully explicit Topic, and
is suitable for situations where speech recogni-
tion condence is low, and conrmation of the
topic is required, as in example (3c). One ad-
vantage of this approach is that the design of
the individual generators is simplied, as they
can follow a xed realization strategy, but the
overall adaptivity of the system is increased.
The selection of the appropriate generator
agent is the task of the component called the
Adapter in Figure 1. The Adapter is a particu-
lar kind of evaluator based on a neural network
implementation. Input consists of a number of
features which are encoded as binary features,
and output consists of categories that represent
the dierent generators. The features are ex-
tracted from the shared information storage and
concern e.g. the content of the planned utter-
ance (Topic, NewInfo), recognition condence
of the previous user utterance, and general re-
quirements for cooperative, natural responses.
4.2 Adaptivity
Adaptivity is one of the desirable properties for
learning dialogue systems (Jokinen, 2000). It
is linked to the system's cooperation with the
user, i.e. its capability to provide informative
and helpful responses but also its capability to
tailor responses according to various situations
the users nd themselves in.
In the above framework, one approach to pro-
viding the desired adaptivity is to have gen-
erator agents with dierent levels of explicit-
ness. Changing levels of condence in speech
recognition accuracy can then lead to selecting
generator agents with more or less explicitness.
The detailed mechanisms for switching between
these dierent agents by means of the evalua-
tors in the Jaspis framework, including a soft
computing approach based on neural networks,
are being evaluated.
Related research has studied adaptivity with
respect to system strategies, and identied con-
rmation as one of the helpful strategies that
spoken dialogue systems can use in order to
show cooperation and allow the user to cor-
rect misrecognized words (Danieli and Gerbino,
1995). The use of system initiative also helps re-
duce misrecognition errors and thus contributes
to user satisfaction (Walker et al, 1998). How-
ever, a xed dialogue strategy may not suit all
users, whose knowledge of the system's capabil-
ities may dier. Adaptivity can thus be related
to the system's ability to change from a user ini-
tiative strategy to a system initiative one, or to
use varied conrmation strategies, in response
to circumstances and the user model. Empiri-
cal evaluation of one such system shows that an
adaptable system outperforms a non-adaptable
one (Litman and Pan, 2000).
We have widened the notion of adaptivity
to concern also the system's generation strate-
gies in maintaining natural interaction with the
user. The dialogue manager can be said to select
among dialogue strategies, such as conrmation
or initiative, and the choice is implicitly shown
in the selection of the concepts in the agenda.
The presentation manager then selects a gener-
ator agent to realise the agenda, and can be said
to further extend the system's adaptivity as an
aspect of the realization possibilities available
for the system. The same agenda can be re-
alised dierently (as in examples 3a and 3b) by
dierent generator agents, and thus the system
can adapt on dierent levels. The selection of
conrmation or non-conrmation strategy can
also depend on the system's other capabilities.
4.3 Condence
The aim of the adaptivity-oriented architecture
is to enable the spoken dialogue system to adapt
its responses to the changing condence lev-
els concerning the system's knowledge of the
current dialogue situation. At the start of a
new dialogue, when there is no previous history
on which to establish any general communica-
tive condence, the system should start with a
highly explicit response generator, and gradu-
ally, if some level of condence is established,
switch to a less explicit generator.
The high level of explicitness in the system
responses has several aspects. Simply by pro-
viding a quantity of words to be recognised and
acknowledged, the system can verify its under-
standing of the relevant concepts. Explicitness
thus enables speech recognition condence to be
established, and is related to the system's con-
rmation strategy as in example (3c).
It is also associated with politeness: a high
level of explicitness is more polite, and a high
level of elliptical expression is more friendly.
Since politeness is expected with strangers,
more explicitness is therefore appropriate at the
start of a dialogue and less appropriate as the
dialogue proceeds: it is thus inversely related
to the condence of the partner which gets es-
tablished in the shared situation. This pattern
of gradual change from a more formal initial
register to a more informal register as the di-
alogue progresses is well known, at least in cul-
tures in which register is not dictated strictly
by social hierarchy. Dierences between En-
glish dialogues (dynamic register adaptivity)
and Japanese dialogues (xed register through-
out) have been studied (Kume et al, 1989).
Generation of referring expressions is mainly
concerned with enabling successful discrimina-
tion of the correct referent. However, referring
expressions in dialogue systems are also strongly
aected by the level of condence. When there
is some doubt, it is safer to use highly explicit
referring expressions. When there is a high level
of condence, it is normal to take certain risks
for the sake of uent interaction. In fact the
dierence between denite descriptions and pro-
nouns is based on condence: if an entity has
not been mentioned previously, it has no history
on which any condence can be established, so
a high level of explicitness is required.
4.4 Communicative Obligations
As argued by Allwood (1976), communication
creates normative social obligations which con-
cern the speaker's rational coordinated interac-
tion. Obligations are connected to a particular
activity and a role in the activity, varying also
according to the speakers' familiarity and rela-
tive status with each other.
In dialogue systems, communicative obliga-
tions are usually part of the system's control
structure. The system can take the initiative,
give helpful information in anticipation of the
user's questions or to resolve problematic sit-
uations (misheard words, ambiguous referents,
etc.), or simply react to the user input as best
as it can. Obligations are thus used as a basic
motivation for action (Traum and Allen, 1994).
In our framework communicative obligations
are dispersed among the agents and evaluator
control. This allows us to make the obligations
overt, since they can be implemented as simple
dialogue agents. However, as their application
order is not xed, the overall architecture sup-
ports exible interaction where the basic com-
municative ability of the system is shown in the
functioning of the system itself. The systems's
cooperation is not only a pre-assigned disposi-
tion to act in a helpful way, but involves reason-
ing about the appropriate act in the context.
5 Conclusion
We have described our work on adaptivity in re-
sponse generation for a spoken dialogue system,
and have argued in favour of a system architec-
ture using highly specialized agents. The sys-
tem adapts its responses to dialogue situations
by means of the detailed agendas specied by
the dialogue manager and the selection of the
generator agent by the presentation manager.
Further evaluation of a larger demonstrator sys-
tem is planned.
References
Jens Allwood. 1976. Linguistic Communication
as Action and Cooperation. Department of
Linguistics, University of Goteborg. Gothen-
burg Monographs in Linguistics 2.
Apache XML Project. 2001. Xalan-Java
version 2.1.0. http://xml.apache.org/xalan-
j/index.html.
Alison Cawsey. 2000. Presenting tailored re-
source descriptions: Will XSLT do the job?
In 9th International Conference on the World
Wide Web.
Morena Danieli and Elisabetta Gerbino. 1995.
Metrics for evaluating dialogue strategies in
a spoken language system. In Proceedings
of the AAAI Spring Symposium on Empiri-
cal Methods in Discourse Interpretation and
Generation, pages 34{39.
Kristiina Jokinen, Hideki Tanaka, and Akio
Yokoo. 1998. Planning dialogue contribu-
tions with new information. In Proceedings
of the Ninth International Workshop on Nat-
ural Language Generation, pages 158{167,
Niagara-on-the-Lake, Ontario.
Kristiina Jokinen. 2000. Learning dialogue sys-
tems. In L. Dybkjaer, editor, LREC 2000
Workshop: From Spoken Dialogue to Full
Natural Interactive Dialogue - Theory, Em-
pirical Analysis and Evaluation, pages 13{17,
Athens.
Masako Kume, Gayle K. Sato, and Kei Yoshi-
moto. 1989. A descriptive framework for
translating speaker's meaning: Towards a di-
alogue translation system between Japanese
and English. In Fourth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 264{271,
Manchester.
Diane Litman and Shimei Pan. 2000. Predict-
ing and adapting to poor speech recognition
in a spoken dialogue system. In Proceed-
ings of the Seventeenth National Conference
on Articial Intelligence (AAAI-2000), pages
722{728, Austin, TX.
Ehud Reiter and Robert Dale. 2000. Building
Natural Language Generation Systems. Cam-
bridge University Press.
Mariet Theune. 2000. From Data to Speech:
Language Generation in Context. Ph.D. the-
sis, Eindhoven University of Technology.
David Traum and James F. Allen. 1994. Dis-
course obligations in dialogue processing. In
32nd Annual Meeting of the Association for
Computational Linguistics, pages 1{8, Las
Cruces.
Markku Turunen and Jaakko Hakulinen. 2000.
Jaspis - a framework for multilingual adaptive
speech applications. In Proceedings of 6th In-
ternational Conference on Spoken Language
Processing, Beijing.
Kees van Deemter, Emiel Krahmer, and Mariet
Theune. 1999. Plan-based vs. template-
based NLG: A false opposition? In Proceed-
ings of the KI'99 Workshop: May I Speak
Freely?, pages 1{5, Saarbrucken.
Marilyn Walker, Diane Litman, Candace
Kamm, and Alicia Abella. 1998. Evaluat-
ing spoken dialogue agents with PARADISE:
Two case studies. Computer Speech and Lan-
guage, 12-3.
Adaptive Dialogue Systems - Interaction with Interact
Kristiina Jokinen and Antti Kerminen and Mauri Kaipainen
Media Lab, University of Art and Design Helsinki
Ha?meentie 135 C, FIN-00560 Helsinki, Finland
{kjokinen|akermine|mkaipain}@uiah.fi
Tommi Jauhiainen and Graham Wilcock
Department of General Linguistics, University of Helsinki
FIN-00014 University of Helsinki, Finland
{tsjauhia|gwilcock}@ling.helsinki.fi
Markku Turunen and Jaakko Hakulinen
TAUCHI Unit, University of Tampere
FIN-33014 University of Tampere, Finland
{mturunen|jh}@cs.uta.fi
Jukka Kuusisto and Krista Lagus
Neural Networks Research Centre, Helsinki University of Technology
P.O.9800 FIN-02015 HUT, Finland
{krista|jkuusist}@james.hut.fi
Abstract
Technological development has made
computer interaction more common
and also commercially feasible, and
the number of interactive systems has
grown rapidly. At the same time, the
systems should be able to adapt to var-
ious situations and various users, so as
to provide the most efficient and help-
ful mode of interaction. The aim of
the Interact project is to explore nat-
ural human-computer interaction and
to develop dialogue models which will
allow users to interact with the com-
puter in a natural and robust way. The
paper describes the innovative goals of
the project and presents ways that the
Interact system supports adaptivity on
different system design and interaction
management levels.
1 Introduction
The need for flexible interaction is apparent not
only in everyday computer use, but also in vari-
ous situations and services where interactive sys-
tems can diminish routine work on the part of
the service provider, and also cater for the users
with fast and tailored access to digital infor-
mation (call centers, help systems, interactive
banking and booking facilities, routing systems,
information retrieval, etc.).
The innovative goal of the Finnish Interact
project is to enable natural language interac-
tion in a wider range of situations than has been
possible so far, and in situations where its use
has not been functional or robust enough. This
means that the systems should support rich in-
teraction and also be able to learn and adapt
their functionality to the changing situation. It
also implies that the needs of special groups will
be taken into account when designing more nat-
ural interactive systems. Within the current sys-
tem, such scenarios can e.g. include an intelli-
gent bus-stop which allows spoken and text in-
teraction concerning city transportation, with a
sign language help facility.
The project addresses especially the problem
of adaptivity: the users are situated in mo-
bile environments in which their needs, activities
and abilities vary. To allow the users to express
their wishes in a way characteristic to them and
       Philadelphia, July 2002, pp. 64-73.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
the situation, interaction with the system should
take place in a robust and efficient manner, en-
abling rich and flexible communication. Natu-
ral language is thus the preferred mode of in-
teraction, compared to graphical interfaces for
example. Adaptivity also appears in the tech-
niques and methods used in the modelling of
the interaction and the system?s processing ca-
pabilities. An important aspect in this respect
is to combine machine learning techniques with
rule-based natural language processing, to in-
vestigate limitations and advantages of the two
approaches for language technology.
In this paper we focus on adaptivity which
manifests itself in various system properties:
? agent-based architecture
? natural language capability
? self-organising topic recognition
? conversational ability.
The paper is organized as follows. We first
introduce the dialogue system architecture. We
then explain how the modules function and
address the specific design decisions that con-
tribute to the system?s adaptivity. We conclude
by discussing the system?s capabilities and pro-
viding pointers for future work.
2 Agent-based architecture
To allow system development with reusable
modules, flexible application building and easy
combination of different techniques, the frame-
work must itself be designed specifically to sup-
port adaptivity. We argue in favour of a sys-
tem architecture using highly specialized agents,
and use the Jaspis adaptive speech application
framework (Turunen and Hakulinen, 2000; Tu-
runen and Hakulinen, 2001a). Compared to e.g.
Galaxy (Seneff et al, 1998), the system supports
more flexible component communication. The
system is depicted in Figure 1.
2.1 Information Storage
The Jaspis architecture contains several features
which support adaptive applications. First of
all, the information about the system state is
kept in a shared knowledge base called Informa-
tion Storage. This blackboard-type information
storage can be accessed by each system compo-
nent via the Information Manager, which allows
them to utilize all the information that the sys-
tem contains, such as dialogue history and user
profiles, directly. Since the important informa-
tion is kept in a shared place, system compo-
nents can be stateless, and the system can switch
between them dynamically. Information Stor-
age thus facilitates the system?s adaptation to
different internal situations, and it also enables
the most suitable component to be chosen to
handle each situation.
2.2 Flexible Component Management
The system is organized into modules which
contain three kinds of components: managers,
agents and evaluators. Each module contains
one manager which co-ordinates component in-
teraction inside the module. The present archi-
tecture implements e.g. the Input/Output Man-
ager, the Dialogue Manager and the Presenta-
tion Manager, and they have different priorities
which allow them to react to the interaction flow
differently. The basic principle is that whenever
a manager stops processing, all managers can
react to the situation, and based on their prior-
ities, one of them is selected. There is also the
Interaction Manager which coordinates applica-
tions on the most general level.
The number and type of modules that can be
connected to the system is not limited. The In-
teraction Manager handles all the connections
between modules and the system can be dis-
tributed for multiple computers. In Interact
we have built a demonstration application on
bus-timetable information which runs on several
platforms using different operating systems and
programming languages. This makes the system
highly modular and allows experiments with dif-
ferent approaches from multiple disciplines.
2.3 Interaction Agents and Evaluators
Inside the modules, there are several agents
which handle various interaction situations such
as speech output presentations and dialogue de-
cisions. These interaction agents can be very
Figure 1: The system architecture.
specialized, e.g. they deal only with speech
recognition errors or outputs related to greet-
ings. They can also be used to model differ-
ent interaction strategies for the same task, e.g.
different dialogue agents can implement alterna-
tive dialogue strategies and control techniques.
Using specialized agents it is possible to con-
struct modular, reusable and extendable inter-
action components that are easy to implement
and maintain. For example, different error han-
dling methods can be included to the system by
constructing new agents which handle errors us-
ing alternative approaches. Similarly, we can
support multilingual outputs by constructing
presentation agents that incorporate language
specific features for each language, while imple-
menting general interaction techniques, such as
error correction methods, to take care of error
situations in speech applications in general (Tu-
runen and Hakulinen, 2001b).
The agents have different capabilities and the
appropriate agent to handle a particular situa-
tion at hand is selected dynamically based on
the context. The choice is done using evalua-
tors which determine applicability of the agents
to various interaction situations. Each evaluator
gives a score for every agent, using a scale be-
tween [0,1]. Zero means that an agent is not
suitable for the situation, one means that an
agent is perfectly suitable for the situation, val-
ues between zero and one indicate the level of
suitability. Scaling functions can be used to em-
phasize certain evaluators over the others The
scores are then multiplied, and the final score, a
suitability factor, is given for every agent. Since
scores are multiplied, an agent which receives
zero from one evaluator is useless for that situ-
ation. It is possible to use different approaches
in the evaluation of the agents, and for instance,
the dialogue evaluators are based on reinforce-
ment learning.
Simple examples of evaluators are for instance
presentation evaluators that select presentation
agents to generate suitable implicit or explicit
confirmations based on the dialogue history and
the system?s knowledge of the user. Another ex-
ample concerns dialogue strategies: the evalua-
tors may give better scores for system-initiative
agents if the dialogue is not proceeding well with
the user-initiative dialogue style, or the evalua-
tors may prefer presentation agents which give
more detailed and helpful information, if the
users seem to have problems in communicating
with the application.
Different evaluators evaluate different aspects
of interaction, and this makes the evaluation
process highly adaptive itself: there is no single
evaluator which makes the final decision. In-
stead, the choice of the appropriate interaction
agent is a combination of different evaluations.
Evaluators have access to all information in the
Information Storage, for example dialogue his-
tory and other contextual information, and it is
also possible to use different approaches in the
evaluation of the agents (such as rule-based and
statistical approaches). Evaluators are the key
concept when considering the whole system and
its adaptation to various interaction situations.
2.4 Distributed Input and Output
The input/output subsystem is also distributed
which makes it possible to use several input and
output devices for the same purposes. For ex-
ample, we can use several speech recognition
engines, each of which with different capabili-
ties, to adapt the system to the user?s way of
talking. The system architecture contains vir-
tual devices which abstract the actual devices,
such as speech recognizers and speech synthesiz-
ers. From the application developers viewpoint
this makes it easy to experiment with different
modalities, since special agents are used to add
and interpret modality specific features. It is
also used for multilingual inputs and outputs,
although the Interact project focuses on Finnish
speech applications.
3 Natural Language Capabilities
The use of Finnish as an interaction language
brings special problems for the system?s nat-
ural language understanding component. The
extreme multiplicity of word forms prevents the
use of all-including dictionaries. For instance,
a Finnish noun can theoretically have around
2200, and a verb around 12000 different forms
(Karlsson, 1983). In spoken language these
numbers are further increased as all the differ-
ent ways to pronounce any given word come into
consideration (Jauhiainen, 2001). Our dialogue
system is designed to understand both written
and spoken input.
3.1 Written and spoken input
The different word forms are analyzed using
Fintwol, the two-level morphological analyzer
for Finnish (Koskenniemi, 1983). The forms are
currently input to the syntactic parser CPARSE
(Carlson, 2001). However, the flexible sys-
tem architecture also allows us to experiment
with different morphosyntactic analyzers, such
as TextMorfo (Kielikone Oy 1999) and Conexor
FDG (Conexor Oy 1997-2000), and we plan
to run them in parallel as separate competing
agents to test and compare their applicability
as well as the Jaspis architecture in the given
task.
We use the Lingsoft Speech Recognizer for the
spoken language input. The current state of the
Finnish speech recognizer forces us to limit the
user?s spoken input to rather restricted vocab-
ulary and utterance structure, compared to the
unlimited written input. The system uses full
word lists which include all the morphological
forms that are to be recognized, and a strict
context-free grammar which dictates all the pos-
sible utterance structures. We are currently ex-
ploring possibilities for a HMM-based language
model, with the conditional probabilities deter-
mined by a trigram backoff model.
3.2 Language analysis
The task of the parsing component is to map
the speaker utterances into task-relevant do-
main concepts which are to be processed by
the dialogue manager. The number of domain
concepts concerning the demonstration system?s
application domain, bus-timetables, is rather
small and contains e.g. bus, departure-time
and arrival-location. However, semantically
equivalent utterances can of course vary in the
lexical elements they contain, and in written and
especially in spoken Finnish the word order in
almost any given sentence can also be changed
without major changes on the semantic level un-
derstood by the system (the difference lies in the
information structure of the utterance). For in-
stance, the request How does one get to Malmi?
can be realised as given in Table 1.
There are two ways to approach the problem:
on one hand we can concentrate on finding the
keywords and their relevant word forms, on the
other hand we can use more specialized syntac-
tic analyzers. At the moment we use CPARSE
as the syntactic analyzer for text-based input.
The grammar has been adjusted for the demon-
Kuinka pa?a?see bussilla Malmille?
Miten pa?a?see Malmille bussilla?
Kuinka Malmille pa?a?see bussilla?
Malmille miten pa?a?see bussilla?
Milla? bussilla pa?a?se Malmille?
Malmille olisin bussia kysellyt.
Pa?a?seeko? bussilla Malmille?
Table 1: Some alternative utterances for Kuinka
pa?a?see Malmille bussilla? ?How does-one-get to-
Malmi by bus?
stration system so that it especially looks for
phrases relevant to the task at hand. For in-
stance, if we can correctly identify the inflected
word form Malmille from the input string, we
can be quite certain of the user wishing to know
something about getting to Malmi.
The current speech input does not go through
any special morpho-syntactic analysis because
of the strict context-free grammar used by the
speech recognizer. The dictionary used by the
recognizer is tagged with the needed morpholog-
ical information and the context-free rules are
tagged with the needed syntactic information.
3.3 Language generation
The language generation function is located in
the system?s Presentation Manager module. Un-
like language analysis, for which different ex-
isting Finnish morphosyntactic analyzers can
be used, there are no readily available general-
purpose Finnish language generators. We are
therefore developing specific generation compo-
nents for this project. The flexible system ar-
chitecture allows us to experiment with different
generators.
Unfortunately the existing Finnish syntactic
analyzers have been designed from the outset as
?parsing grammars?, which are difficult or im-
possible to use for generation. However, the two-
level morphology model (Koskenniemi, 1983) is
in principle bi-directional, and we are work-
ing towards its use in morphological generation.
Fortunately there is also an existing Finnish
speech synthesis project (Vainio, 2001), which
we can use together with the language genera-
tors.
Some of our language generation components
use the XML-based generation framework de-
scribed by Wilcock (2001), which has the ad-
vantage of integrating well with the XML-based
system architecture. The generator starts from
an agenda which is created by the dialogue man-
ager, and is available in the system?s Informa-
tion Storage in XML format. The agenda con-
tains a list of semantic concepts which the dia-
logue manager has tagged as Topic or NewInfo.
From the agenda the generator creates a re-
sponse plan, which passes through the genera-
tion pipeline stages for lexicalization, aggrega-
tion, referring expressions, syntactic and mor-
phological realization. At all stages the response
specification is XML-based, including the final
speech markup language which is passed to the
speech synthesizer.
The system architecture allows multiple gen-
erators to be used. In addition to the XML-
based pipeline components we have some pre-
generated outputs, such as greetings at the start
and end of the dialogue or meta-acts such as
wait-requests and thanking. We are also ex-
ploiting the agent-based architecture to increase
the system?s adaptivity in response generation,
using the level of communicative confidence as
described by Jokinen and Wilcock (2001).
4 Recognition of Discussion Topic
One of the important aspects of the system?s
adaptivity is that it can recognize the correct
topic that the user wants to talk about. By
?topic? we refer to the general subject matter
that a dialogue is about, such as ?bus timetables?
and ?bus tickets?, realized by particular words in
the utterances. In this sense, individual doc-
uments or short conversations may be seen to
have one or a small number of topics, one at a
time.
4.1 Topically ordered semantic space
Collections of short documents, such as news-
paper articles, scientific abstracts and the like,
can be automatically organized onto document
maps utilizing the Self-Organizing Map algo-
rithm (Kohonen, 1995). The document map
methodology has been developed in the WEB-
SOM project (Kohonen et al, 2000), where the
largest map organized consisted of nearly 7 mil-
lion patent abstracts.
We have applied the method to dialogue topic
recognition by carring out experiments on 57
Finnish dialogues, recorded from the customer
service phone line of Helsinki City Transport
and transcribed manually into text. The dia-
logues are first split into topically coherent seg-
ments (utterances or longer segments), and then
organized on a document map. On the ordered
map, each dialogue segment is found in a spe-
cific map location, and topically similar dialogue
segments are found near it. The document map
thus forms a kind of topically ordered semantic
space. A new dialogue segment, either an utter-
ance or a longer history, can likewise be auto-
matically positioned on the map. The coordi-
nates of the best-matching map unit may then
be considered as a latent topical representation
for the dialogue segment.
Furthermore, the map units can be labeled us-
ing named topic classes such as ?timetables? and
?tickets?. One can then estimate the probability
of a named topic class for a new dialogue seg-
ment by construing a probability model defined
on top of the map. A detailed description of the
experiments as well as results can be found in
(Lagus and Kuusisto, 2002).
4.2 Topic recognition module
The topical semantic representation, i.e. the
map coordinates, can be used as input for the
dialogue manager, as one of the values of the
current dialogue state. The system architecture
thus integrates a special topic recognition mod-
ule that outputs the utterance topic in the In-
formation Storage. For a given text segment,
say, the recognition result from the speech rec-
ognizer, the module returns the coordinates of
the best-matching dialogue map unit as well as
the most probable prior topic category (if prior
categorization was used in labeling the map).
5 Dialogue Management
The main task of the dialogue manager com-
ponent is to decide on the appropriate way to
react to the user input. The reasoning includes
recognition of communicative intentions behind
the user?s utterances as well as planning of the
system?s next action, whether this is information
retrieval from a database or a question to clarify
an insufficiently specified request. Natural inter-
action with the user also means that the system
should not produce relevant responses only in
terms of correct database facts but also in terms
of rational and cooperative reactions. The sys-
tem could learn suitable interaction strategies
from its interaction with the user, showing adap-
tation to various user habits and situations.
5.1 Constructive Dialogue Model
A uniform basis for dialogue management can
be found in the communicative principles re-
lated to human rational and coordinated inter-
action (Allwood et al, 2000; Jokinen, 1996).
The speakers are engaged in a particular activ-
ity, they have a certain role in that activity, and
their actions are constrained by communicative
obligations. They act by exchanging new in-
formation and constructing a shared context in
which to resolve the underlying task satisfacto-
rily.
The model consists of a set of dialogue states,
defined with the help of dialogue acts, obser-
vations of the context, and reinforcement val-
ues. Each action results in a new dialogue
state. The dialogue act, Dact, describes the act
that the speaker performs by a particular utter-
ance, while the topic Top and new information
NewInfo denote the semantic content of the ut-
terance and are related to the task domain. To-
gether these three create a useful first approx-
imation of the utterance meaning by abstract-
ing over possible linguistic realisations. Unfilled
task goals TGoals keep track of the activity re-
lated information still necessary to fulfil the un-
derlying task (a kind of plan), and the speaker
information is needed to link the state to pos-
sible speaker characteristics. The expectations,
Expect are related to communicative obligations,
and used to constrain possible interpretations of
the next act. Consequently, the system?s inter-
nal states can be reduced to a combination of
these categories, all of which form an indepen-
dent source of information for the system to de-
cide on the next move.
5.2 Dialogue agents and evaluators
A dialogue state and all agents that contribute
to a dialogue state are shown in Figure 2. The
Dialogue Model is used to classify the current
utterance into one of the dialogue act categories
(Jokinen et al, 2001), and to predict the next
dialogue acts (Expect). The Topic Model rec-
ognizes the domain, or discussion topic, of the
user input as described above.
Figure 2: Dialogue states for user?s utter-
ance and system action, together with dialogue
agents involved in producing various informa-
tion.
All domains out of the system?s capabili-
ties are handled with the help of a special
OutOfDomain-agent which informs the user of
the relevant tasks and possible topics directly.
This allows the system to deal with error sit-
uations, such as irrelevant user utterances, ef-
ficiently and flexibly without invoking the Dia-
logue Manager to evaluate appropriate dialogue
strategies. The information about error situ-
ations and the selected system action is still
available for dialogue and task goal management
through the shared Information Storage.
The utterance Topic and New Information
(Topic, NewInfo) of the relevant user utter-
ances are given by the parsing unit, and sup-
plemented with discourse knowledge by ellipsis
and anaphora resolution agents (which are In-
put Agents). Task related goals are produced by
Task Agents, located in a separate Task Man-
ager module. They also access the backend
database, the public transportation timetables
of Helsinki.
The Dialogue Manager (DM) consists of
agents corresponding to possible system actions
(Figure 3). There are also some agents for inter-
nal system interaction, illustrated in the figure
with a stack of agents labeled with Agent1. One
agent is selected at a time, and the architecture
permits us to experiment with various compet-
ing agents for the same subtask: the evaluators
are responsible for choosing the one that best
fits in the particular situation.
Figure 3: The Dialogue Manager component.
Two types of evaluators are responsible for
choosing the agent in DM, and thus implement-
ing the dialogue strategy. The QEstimate eval-
uator chooses the agent that has proven to be
most rewarding so far, according to a Q-learning
(Watkins and Dayan, 1992) algorithm with on-
line -greedy policy (Sutton and Barto, 1998).
That agent is used in the normal case and the
decision is based on the dialogue state presented
in Figure 2. The underlying structure of the
QEstimate evaluator is illustrated in Figure 4.
The evaluator is based on a table of real val-
ues, indexed by dialogue states, and updated af-
ter each dialogue. The agent with the highest
Figure 4: The QEstimate evaluator.
value for the current dialogue state gets selected.
Adaptivity of the dialogue management comes
from the reinforcement learning algorithm of
this evaluator.
On the other hand, if one of the error evalu-
ators (labeled with Error1..N) detects that an
error has occurred, the QEstimate evaluator is
overridden and a predetermined agent is selected
to handle the error situation (Figure 5). In these
cases, only the the correct agent is given a non-
zero value, forcing the dialogue manager to se-
lect that agent. Examples of such errors include
situations when the user utterance is not recog-
nized by the speech recognizer, its topic is irrele-
vant to the current domain, or its interpretation
is inconsistent with the dialogue context.
Figure 5: An error evaluator.
Because all possible system actions are
reusable agents, we can easily implement a dif-
ferent dialogue management strategy by adding
evaluators, or replacing the current QEstimate
evaluator. We are developing another strategy
based on recurrent self-organizing maps, that
learns to map dialogue states to correct actions
by fuzzy clustering, minimizing the amount of
human labor in designing the dialogue strategy.
6 Demo System and Future Work
The project deals with both speech and text
communication, interfacing with the user by
telephone and web-applications. An initial in-
tegrated system with basic functionality and in-
teraction capabilities concerning bus timetable
inquiries has been built. Sample interactions are
given in Figures 6 and 7. Various other appli-
cation types such as intelligent question-answer
systems, automatic call-centers, and other ser-
vices that would benefit from flexible natural
interaction can also be envisaged.
The current system is the first prototype and
intended to provide useful basis for further ex-
perimentation and system development. The
planned activities include further investigations
of the limitations and advantages of various ap-
proaches for language technology, as well as us-
ability studies concerning real users and users
with special needs.
(1) UT: mmilla?s bussilla ma? pa?a?sen malamille
?with which bus do I get to Malami?
UR: milla? bussilla pa?a?sen malmille
?with which bus do I get to Malmi?
SU: mista? olittekaan la?hdo?ssa? malmille
?where do you want to leave for Malmi?
UT: no hakaniemesta?
?well from Hakaniemi?
UR: hakaniemesta?
?from Hakaniemi?
SU: bussi 70 la?htee sielta? malmille
?bus 70 leaves from there to Malmi?
UT: selva? kiitos hei
?ok thank you bye?
UR: bussi kiitos hei
?bus thank you bye?
SU: kiitos hei
?thank you bye?
Figure 6: Sample dialogue with low recognition
score for the destination Malmille ?to Malmi?
(UT = Utterance as actually made by the user,
UR = Utterance as recognized by the system,
SU = System utterance).
(2) UT: koskas mennee seuraava pussi arapi-
asta malmille
?when will the next bus go from Arapia to
Malmi?
UR: koska menee seuraava bussi arabiasta
malmille
?when will the next bus go from Arabia to
Malmi?
SU: kello 14 30
?at 14:30?
UT: minka? numeroinen se on
?which number is it?
UR: mina? nelonen selva?
?I four okay?
SU: olen pahoillani osaan vastata vain
bussiaikatauluja koskeviin kysymyksiin
?sorry I can reply only to questions con-
cerning bus timetables?
UT: aha no joo jaa kiitos hei
?I see ok thank you bye?
UR: onko aijaa kiitos hei
?is I see thank you bye?
SU: kiitos hei
?thank you bye?
Figure 7: Sample dialogue with misrecognition
of user utterance minka? numeroinen se on
(UT = Utterance as actually made by the user,
UR = Utterance as recognized by the system,
SU = System utterance).
7 Acknowledgements
The research is carried out in the context of the
USIX Interact project, a collaboration project
between four Finnish universities, and funded
by the National Technology Agency, the leading
IT companies ICL Invia oyj, Sonera oyj, Ling-
soft oy, and Gurusoft oy, as well as the Finnish
Association for the Deaf and the Arla Institute.
References
J. Allwood, D. Traum, and K. Jokinen. 2000. Coop-
eration, dialogue and ethics. International Jour-
nal of Human-Computer Studies, 53:871?914.
L. Carlson. 2001. CPARSE manual. http://www
.ling.helsinki.fi/ lcarlson/cparse09en.html.
T. Jauhiainen. 2001. Using existing written lan-
guage analyzers in understanding natural spoken
Finnish. In Proceedings of Nodalida ?01, Uppsala.
K. Jokinen and G. Wilcock. 2001. Confidence-based
adaptivity in response generation for a spoken di-
alogue system. In Proceedings of the 2nd SIGdial
Workshop on Discourse and Dialogue, pages 80?
89, Aarhus.
K. Jokinen, T. Hurtig, K. Hynna?, K. Kanto,
M. Kaipainen, and A. Kerminen. 2001. Self-
organizing dialogue management. In Proceedings
of the 2nd Workshop on Natural Language Pro-
cessing and Neural Networks, pages 77?84, Tokyo.
K. Jokinen. 1996. Goal formulation based on com-
municative principles. In Proceedings of the 16th
COLING, pages 598?603.
F. Karlsson. 1983. Suomen kielen a?a?nne- ja muoto-
rakenne. WSOY, Juva.
T. Kohonen, S. Kaski, K. Lagus, J. Saloja?rvi,
V. Paatero, and A. Saarela. 2000. Organization
of a massive document collection. IEEE Transac-
tions on Neural Networks, Special Issue on Neural
Networks for Data Mining and Knowledge Discov-
ery, 11(3):574?585, May.
T. Kohonen. 1995. Self-Organizing Maps. Springer,
Berlin.
K. Koskenniemi. 1983. Two-level morphology: a
general computational model for word-form recog-
nition and production. University of Helsinki,
Helsinki.
K. Lagus and J. Kuusisto. 2002. Topic identifica-
tion in natural language dialogues using neural
networks. In Proceedings of the 3rd SIGdial Work-
shop on Discourse and Dialogue, Philadelphia.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. Galaxy-II: A reference architecture
for conversational system development. In Pro-
ceedings of ICSLP-98, Sydney.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing: An Introduction. MIT Press, Cambridge,
Massachusetts.
M. Turunen and J. Hakulinen. 2000. Jaspis - a
framework for multilingual adaptive speech appli-
cations. In Proceedings of the 6th International
Conference on Spoken Language Processing, Bei-
jing.
M. Turunen and J. Hakulinen. 2001a. Agent-based
adaptive interaction and dialogue management ar-
chitecture for speech applications. In Text, Speech
and Dialogue. Proceedings of the Fourth Interna-
tional Conference (TSD-2001), pages 357?364.
M. Turunen and J. Hakulinen. 2001b. Agent-based
error handling in spoken dialogue systems. In Pro-
ceedings of Eurospeech 2001, pages 2189?2192.
M. Vainio. 2001. Artificial Neural Network Based
Prosody Models for Finnish Text-to-Speech Syn-
thesis. Ph.D. thesis, University of Helsinki.
C. Watkins and P. Dayan. 1992. Technical note:
Q-learning. Machine Learning, 8:279?292.
G. Wilcock. 2001. Pipelines, templates and transfor-
mations: XML for natural language generation. In
Proceedings of the 1st NLP and XML Workshop,
pages 1?8, Tokyo.
Proceedings of the SIGDIAL 2013 Conference, pages 360?362,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Open-Domain Information Access with Talking Robots
Kristiina Jokinen and Graham Wilcock
University of Tartu, Estonia and University of Helsinki, Finland
kjokinen@ut.ee, graham.wilcock@helsinki.fi
Abstract
The demo shows Wikipedia-based open-
domain information access dialogues with
a talking humanoid robot. The robot uses
face-tracking, nodding and gesturing to
support interaction management and the
presentation of information to the partner.
1 Introduction
The demo shows open-domain information access
dialogues with the WikiTalk system on a Nao
humanoid robot (Jokinen and Wilcock, 2012b).
An annotated video of the demo can be seen
at https://docs.google.com/file/d/
0B-D1kVqPMlKdOEcyS25nMWpjUG8.
The WikiTalk system can be viewed from two
complementary perspectives: as a spoken dialogue
system or as a question-answering (QA) system.
Viewed as a spoken dialogue system, WikiTalk
supports constructive interaction for talking about
interesting topics (Jokinen and Wilcock, 2012a).
However, using Wikipedia as its knowledge source
instead of a finite database means that WikiTalk
is completely open-domain. This is a significant
breakthrough compared with traditional closed-
domain spoken dialogue systems.
Viewed as a QA system, WikiTalk provides
Wikipedia-based open-domain knowledge access
(Wilcock, 2012). However, by using sentences
and paragraphs from Wikipedia, the system is able
to talk about the topic in a conversational manner,
thus differing from a traditional QA system.
The Nao robot prototype version of WikiTalk
was implemented by Csapo et al (2012) during
eNTERFACE 2012, the 8th International Summer
Workshop on Multimodal Interfaces at Supe?lec in
Metz (Figure 1). The humanoid robot uses face-
tracking, nodding and gesturing to support interac-
tion management and the presentation of new in-
formation to the partner (Han et al, 2012; Meena
et al, 2012).
Figure 1: Working with the Nao humanoid robot.
2 Outline of the system
At the heart of the system (Figure 2) is a conver-
sation manager based on a finite state machine.
However, the states are not based on the domain-
specific tasks and utterences for a fixed domain.
In WikiTalk, the states function at a more abstract
dialogue management level dealing for example
with topic initiation, topic continuation, and topic
switching. Further details of this approach are
given by Wilcock (2012).
The finite state machine also has extensions that
store various parameters of past interactions and
influence the functionality of the state machine.
The conversation manager communicates with a
Wikipedia manager to obtain information from
Wikipedia, and a Nao manager to map its states
onto the actions of the robot.
To enable the robot to react to various events
while getting information from Wikipedia, the
Nao manager registers events and alerts the appro-
priate components of the system when anything of
interest occurs either on the inside or the outside
of the system. Figure 2 shows three examples of
360
Figure 2: The system architecture, from (Csapo et al, 2012).
event handling within the Nao Talk module which
drives the robot?s speech functionality. The func-
tions isSaying(), startOfParagraph(),
and endOfSentence() are called periodically
by the Nao manager, and return True whenever the
robot is talking, reaches the start of a paragraph, or
finishes a sentence, respectively. Whenever such
events occur, the Nao manager can trigger appro-
priate reactions, for example, through the Gestures
module which drives the robot?s nodding and ges-
turing functionalities.
The history of the user?s interactions is stored in
a statistics dictionary in the conversation manager.
Using a set of simple heuristics, it is possible to
create more interesting dialogues by ensuring that
the robot does not give the same instructions to the
user in the same way over and over again, and by
varying the level of sophistication in terms of the
functionalities that are introduced to the user by
the robot. For example, at first the robot gives sim-
ple instructions, allowing the user to practice and
understand the basic functionalities of the system.
For more advanced users, the system suggests new
kinds of use cases which may not have previously
been known to the user.
A corpus of videos of user trials with the system
(Figure 3) was collected at the eNTERFACE 2012
workshop. The user trials and user questionnaires
were used for system evaluation, which is reported
by Anastasiou et al (2013).
3 Outline of the demo
The demo is deliberately live, unscripted, and im-
provised. However, it typically starts with the
robot in a sitting position. The robot stands up and
greets the user, then asks what topic the user wants
to hear about. The robot suggests some of its own
favourite topics.
When the user selects a topic, the system gets
information about the topic from Wikipedia and
divides it into chunks suitable for spoken dialogue
contributions. The system then manages the spo-
ken presentation of the chunks according to the
user?s reactions. If the user asks for more, or oth-
erwise shows interest in the topic, the system con-
tinues with the next chunk.
Crucially, the system makes smooth topic shifts
by following the hyperlinks in Wikipedia when-
ever the user repeats the name of one of the
links. For example, if the system is talking about
Shakespeare and says ?Shakespeare was born in
Stratford-upon-Avon?, the user can say ?Stratford-
upon-Avon?? and the system smoothly switches
topics and starts talking about Stratford-upon-
Avon using the Wikipedia information about this
new topic.
The user can ask for any chunk to be repeated,
or go back to the previous chunk. The user can
also interrupt the current chunk and ask to skip to
another chunk on the same topic.
361
Figure 3: Testing spoken interaction with Nao.
The user can interrupt the robot at any time by
touching the top of the robot?s head. The robot
stops talking and explicitly acknowledges the in-
terruption by saying ?Oh sorry!? and waiting for
the user?s input. The user can then tell it to con-
tinue, to go back, to skip to another chunk, or to
switch to a new topic.
The dialogue is open-domain and typically
wanders freely from topic to topic by smooth topic
shifts following the links in Wikipedia. However,
if the user wants to jump to an entirely unrelated
topic, an awkward topic shift can be made by say-
ing the command ?Alphabet!? and spelling the
first few letters of the new topic using a spelling
alphabet (Alpha, Bravo, Charlie, etc.).
As well as talking about topics selected by the
user, the robot can take the initiative by suggesting
potentially interesting new topics. One way to do
this is by using the ?Did you know ...?? sections
from Wikipedia that are new every day.
The demo ends when the user tells the robot to
stop. The robot thanks the user and sits down.
4 Previous demos
The system was first demonstrated in July 2012 at
the 8th International Summer Workshop on Multi-
modal Interfaces (eNTERFACE 2012) in Metz.
An annotated video of this demo can be seen
at https://docs.google.com/file/d/
0B-D1kVqPMlKdOEcyS25nMWpjUG8.
The system was also demonstrated at the 3rd
IEEE International Conference on Cognitive Info-
communications (CogInfoCom 2012).
Acknowledgements
We thank Adam Csapo, Emer Gilmartin, Jonathan
Grizou, Frank Han, Raveesh Meena and Dimitra
Anastasiou for their collaboration, both on the Nao
WikiTalk implementation and on the user evalua-
tions conducted at eNTERFACE 2012.
We also thank Supe?lec and especially Professor
Olivier Pietquin for providing the Nao robots both
for the eNTERFACE 2012 workshop and for the
SIGDIAL-2013 demo.
References
Dimitra Anastasiou, Kristiina Jokinen, and Graham
Wilcock. 2013. Evaluation of WikiTalk - user stud-
ies of human-robot interaction. In Proceedings of
15th International Conference on Human-Computer
Interaction (HCII 2013), Las Vegas, USA.
Adam Csapo, Emer Gilmartin, Jonathan Grizou, Jing-
Guang Han, Raveesh Meena, Dimitra Anastasiou,
Kristiina Jokinen, and Graham Wilcock. 2012.
Multimodal conversational interaction with a hu-
manoid robot. In Proceedings of 3rd IEEE Interna-
tional Conference on Cognitive Infocommunications
(CogInfoCom 2012), pages 667?672, Kosice.
JingGuang Han, Nick Campbell, Kristiina Jokinen, and
Graham Wilcock. 2012. Investigating the use of
non-verbal cues in human-robot interaction with a
Nao robot. In Proceedings of 3rd IEEE Interna-
tional Conference on Cognitive Infocommunications
(CogInfoCom 2012), pages 679?683, Kosice.
Kristiina Jokinen and Graham Wilcock. 2012a. Con-
structive interaction for talking about interesting
topics. In Proceedings of Eighth International
Conference on Language Resources and Evaluation
(LREC 2012), Istanbul.
Kristiina Jokinen and Graham Wilcock. 2012b. Multi-
modal open-domain conversations with the Nao
robot. In Fourth International Workshop on Spoken
Dialogue Systems (IWSDS 2012), Paris.
Raveesh Meena, Kristiina Jokinen, and Graham
Wilcock. 2012. Integration of gestures and speech
in human-robot interaction. In Proceedings of 3rd
IEEE International Conference on Cognitive Info-
communications (CogInfoCom 2012), pages 673?
678, Kosice.
Graham Wilcock. 2012. WikiTalk: A spoken
Wikipedia-based open-domain knowledge access
system. In Proceedings of the COLING 2012 Work-
shop on Question Answering for Complex Domains,
pages 57?69, Mumbai.
362
Proceedings of the 25th International Conference on Computational Linguistics, pages 124?125,
Dublin, Ireland, August 23-29 2014.
Towards automatic annotation of communicative gesturing
Kristiina Jokinen
University of Tartu
Estonia
kristiina.jokinen@ut.ee
Graham Wilcock
University of Helsinki
Finland
graham.wilcock@helsinki.fi
Abstract
We report on-going work on automatic annotation of head and hand gestures in videos of conversational inter-
action. The Anvil annotation tool was extended by two plugins for automatic face and hand tracking. The results
of automatic annotation are compared with the human annotations on the same data.
1 Introduction
Hand and head movements are important in human communication as they not only accompany
speech to emphasize the message, but also coordinate and control the interaction. However, video
analysis of human behaviour is a slow and resource-consuming procedure even by trained annotators
using tools such as Anvil (Kipp 2001). There is an urgent need for more advanced tools to speed up
the process by performing higher-level annotation functions automatically.
We use two Anvil plugins, a face tracker (Jongejan 2012) and a hand tracker (Saatmann 2014), that
automatically create annotations for head and hand movements. Objects are recognized based on visu-
al  features  such  as  colour  and  texture,  and  Haar-liked  digital  image  features,  using  OpenCV frame-
work. Motion trajectories are estimated by calculating the mean velocity and acceleration during the
time span of a set of frames (we experimented with 7 frames as more than 10 makes the algorithm in-
sensitive for quick, short movements). Movement annotations with respect to velocity and acceleration
are marked on the appropriate Anvil track, to indicate the movement and its start and stop. The inter-
face has controls for minimum saturation threshold and for how many frames to skip (Figure 1).
Figure 1 Anvil interface of the new hand tracker plugin.
2 Comparison of human and automatic annotations
Compared with human annotation the trackers are good at detecting some movements but prone to
mis-detecting other movements. Problems occurred e.g. when the hue of the hands was similar to the
background colour, or if the direction of the movement is reversed quickly, so that the time span is not
long enough to detect velocity up to the thresholds (short head movements). Acceleration annotation
did not recognize movements if they start and stop slowly. Changing the detection threshold can im-
prove results, but is a trade-off as it prevents small movements being detected. However, the plugins
will be of great help in multimodal analysis. Using the plugins reduces the time spent on annotating
these movements, which in turn results annotations in increased productivity.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
124
Here we present a more detailed analysis of the human and automatic annotations with reference to
face tracking. The annotated hand and head movements are listed in Table 1. From the collected data
we used four sample videos, each about six minutes long, altogether 45 303 frames. Table 2 shows the
number of elements automatically recognized using velocity and acceleration, with precision scores,
i.e. manually annotated gestures correctly recognized by the automatic annotation.
Head movements Hand movements
Nod down Backward Waggle Both
Nod up Forward Shake Single
Turn sideways Tilt Other Complex
Other
Table 1. Annotation features for head and hand movements.
Gesture Manual annotation Velocity  Acceleration
NodDown 149 110  (74%) 108  (72%)
NodUp 42 15   (36%) 27   (64%)
TurnSide 40 29   (73%) 27   (68%)
HeadBackward 27 18   (67%) 14   (52%)
HeadForward 21 17   (81%) 18   (86%)
Tilt 57 35   (61%) 29   (51%)
Waggle 12 11   (92%) 8   (67%)
HeadOther 3 2   (67%) 1   (33%)
Total 351 237  (73%) 232  (66%)
Table 2. Manual and automatic head movement annotations for 4 videos.
Precision: Velocity 73%, Acceleration 66%
Figure 2 shows two examples of the annotation results on the Anvil annotation board, one where the
face tracker recognized head movements appropriately, and one where the face tracker ?invented?
movements which the human annotator does not recognize as communicative gestures.
Figure 2. Face tracker detecting manual annotation categories (left) and inventing face movements (right).
3 Future work
Following the work outlined in Jokinen and Scherer (2012), we will compare the top-down linguistic-
pragmatic analysis of movements with the bottom-up signal-level observations. We will also use a
machine-learning approach to analyse if there are any systematics with the problematic cases. We may
also explore if a recognized movement can be automatically interpreted with respect to communicative
intentions. In human-robot interaction, the automatic gesture recognition model can be used to study
the robot?s understanding of the situation and of human control gestures, cf. Han et al. (2012).
References
Han, J., Campbell, N., Jokinen, K. and Wilcock, G. (2012). Investigating the use of non-verbal cues in human-
robot interaction with a Nao robot, in Proceedings of 3rd IEEE International Conference on Cognitive Info-
communications (CogInfoCom 2012), Kosice, 679-683.
Jokinen, K. and Scherer S. (2012). Embodied Communicative Activity in Cooperative Conversational Interac-
tions - studies in Visual Interaction Management. Acta Polytechnica Hungarica. 9(1), pp. 19-40.
Jongejan, B. (2012) Automatic annotation of face velocity and acceleration in Anvil. Proceedings of the Lan-
guage Resources and Evaluation Conference (LREC-2012). Istanbul, Turkey.
Kipp, M. (2001). Anvil ? A generic annotation tool for multimodal dialogue. Proceedings of the Seventh Euro-
pean Conference on Speech Communication and Technology, pp. 1367-1370.
Saatmann, P. (2014). Experiments With Hand-tracking Algorithm in Video Conversations. Proceedings of the
5th Nordic Symposium on Multimodal Communication.
125
