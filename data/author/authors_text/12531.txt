CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 65?72
Manchester, August 2008
Improving Word Segmentation by Simultaneously Learning Phonotactics
Daniel Blanchard
Computer & Information Sciences
University of Delaware
dsblanch@udel.edu
Jeffrey Heinz
Linguistics & Cognitive Science
University of Delaware
heinz@udel.edu
Abstract
The most accurate unsupervised word seg-
mentation systems that are currently avail-
able (Brent, 1999; Venkataraman, 2001;
Goldwater, 2007) use a simple unigram
model of phonotactics. While this sim-
plifies some of the calculations, it over-
looks cues that infant language acquisition
researchers have shown to be useful for
segmentation (Mattys et al, 1999; Mattys
and Jusczyk, 2001). Here we explore the
utility of using bigram and trigram phono-
tactic models by enhancing Brent?s (1999)
MBDP-1 algorithm. The results show
the improved MBDP-Phon model outper-
forms other unsupervised word segmenta-
tion systems (e.g., Brent, 1999; Venkatara-
man, 2001; Goldwater, 2007).
1 Introduction
How do infants come to identify words in the
speech stream? As adults, we break up speech
into words with such ease that we often think
that there are audible pauses between words in the
same sentence. However, unlike some written lan-
guages, speech does not have any completely reli-
able markers for the breaks between words (Cole
and Jakimik, 1980). In fact, languages vary on how
they signal the ends of words (Cutler and Carter,
1987), which makes the task even more daunting.
Adults at least have a lexicon they can use to rec-
ognize familiar words, but when an infant is first
born, they do not have a pre-existing lexicon to
consult. In spite of these challenges, by the age of
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
six months infants can begin to segment words out
of speech (Bortfeld et al, 2005). Here we present
an efficient word segmentation system aimed to
model how infants accomplish the task.
While an algorithm that could reliably extract
orthographic representations of both novel and fa-
miliar words from acoustic data is something we
would like to see developed, following earlier re-
searchers, we simplify the problem by using a text
that does not contain any word boundary markers.
Hereafter, we use the phrase ?word segmentation?
to mean some process which adds word boundaries
to a text that does not contain them.
This paper?s focus is on unsupervised, incre-
mental word segmentation algorithms; i.e., those
that do not rely on preexisting knowledge of a par-
ticular language, and those that segment the cor-
pus one utterance at a time. This is in contrast
to supervised word segmentation algorithms (e.g.,
Teahan et al, 2000), which are typically used for
segmenting text in documents written in languages
that do not put spaces between their words like
Chinese. (Of course, unsupervised word segmen-
tation algorithms also have this application.) This
also differs from batch segmentation algorithms
(Goldwater, 2007; Johnson, 2008b; Fleck, 2008),
which process the entire corpus at least once be-
fore outputting a segmentation of the corpus. Un-
supervised incremental algorithms are of interest
to some psycholinguists and acquisitionists inter-
ested in the problem of language learning, as well
as theoretical computer scientists who are inter-
ested in what unsupervised, incremental models
are capable of achieving.
Phonotactic patterns are the rules that deter-
mine what sequences of phonemes or allophones
are allowable within words. Learning the phono-
tactic patterns of a language is usually modeled
65
separately from word segmentation; e.g., current
phonotactic learners such as Coleman and Pierre-
humbert (1997), Heinz (2007), or Hayes and Wil-
son (2008) are given word-sized units as input.
However, infants appear to simultaneously learn
which phoneme combinations are allowable within
words and how to extract words from the input. It
is reasonable that the two processes feed into one
another, and when infants acquire a critical mass of
phonotactic knowledge, they use it to make judge-
ments about what phoneme sequences can occur
within versus across word boundaries (Mattys and
Jusczyk, 2001). We use this insight, also suggested
by Venkataraman (2001) and recently utilized by
Fleck (2008) in a different manner, to enhance
Brent?s (1999) model MBDP-1, and significantly
increase segmentation accuracy. We call this mod-
ified segmentation model MBDP-Phon.
2 Related Work
2.1 Word Segmentation
The problem of unsupervised word segmentation
has attracted many earlier researchers over the
past fifty years (e.g., Harris, 1954; Olivier, 1968;
de Marcken, 1995; Brent, 1999). In this section,
we describe the base model MBDP-1, along with
two other segmentation approaches, Venkataraman
(2001) and Goldwater (2007). In ?4, we compare
MBDP-Phon to these models in more detail. For
a thorough review of word segmentation literature,
see Brent (1999) or Goldwater (2007).
2.1.1 MBDP-1
Brent?s (1999) MBDP-1 (Model Based Dy-
namic Programming) algorithm is an implemen-
tation of the INCDROP framework (Brent, 1997)
that uses a Bayesian model of how to generate an
unsegmented text to insert word boundaries. The
generative model consists of five steps:
1. Choose a number of word types, n.
2. Pick n distinct strings from ?
+
#, which will
make up the lexicon, L. Entries in L are la-
beled W
1
. . .W
n
. W
0
= $, where $ is the
utterance boundary marker.
3. Pick a function, f , which maps word types to
their frequency in the text.
4. Choose a function, s, to map positions in the
text to word types.
5. Concatenate the words in the order specified
by s, and remove the word delimiters (#).
It is important to note that this model treats the
generation of the text as a single event in the prob-
ability space, which allows Brent to make a num-
ber of simplifying assumptions. As the values for
n,L, f, and s completely determine the segmenta-
tion, the probability of a particular segmentation,
w
m
, can be calculated as:
P (w
m
) = P (n,L, f, s) (1)
To allow the model to operate on one utterance at
a time, Brent states the probability of each word in
the text as a recursive function, R(w
k
), where w
k
is the text up to and including the word at position
k, w
k
. Furthermore, there are two specific cases
for R: familiar words and novel words. If w
k
is
familiar, the model already has the word in its lex-
icon, and its score is calculated as in Equation 2.
R(w
k
) =
f(w
k
)
k
?
(
f(w
k
)? 1
f(w
k
)
)
2
(2)
Otherwise, the word is novel, and its score is cal-
culated using Equation 3
1
(Brent and Tao, 2001),
R(w
k
) =
6
pi
2
?
n
k
?
P
?
(a
1
)...P
?
(a
q
)
1?P
?
(#)
?
(
n?1
n
)
2
(3)
where P
?
is the probability of a particular
phoneme occurring in the text. The third term of
the equation for novel words is where the model?s
unigram phonotactic model comes into play. We
detail how to plug a more sophisticated phonotac-
tic learning model into this equation in ?3. With
the generative model established, MBDP-1 uses a
Viterbi-style search algorithm to find the segmen-
tation for each utterance that maximizes the R val-
ues for each word in the segmentation.
Venkataraman (2001) notes that considering the
generation of the text as a single event is un-
likely to be how infants approach the segmenta-
tion problem. However, MBDP-1 uses an incre-
mental search algorithm to segment one utterance
at a time, which is more plausible as a model of
infants? word segmentation.
1
Brent (1999) originally described the novel word score
as R(w
k
) =
6
pi
2
?
n
k
k
?
P
?
(W
n
k
)
1?
n
k
?1
n
k
?
?
n
k
j=1
P
?
(W
j
)
?
(
n
k
?1
n
k
)
2
,
where P
?
is the probability of all the phonemes in the word
occurring together, but the denominator of the third term was
dropped in Brent and Tao (2001). This change drastically
speeds up the model, and only reduces segmentation accuracy
by ? 0.5%.
66
2.1.2 Venkataraman (2001)
MBDP-1 is not the only incremental unsuper-
vised segmentation model that achieves promis-
ing results. Venkataraman?s (2001) model tracks
MBDP-1?s performance so closely that Batchelder
(2002) posits that the models are performing the
same operations, even though the authors describe
them differently.
Venkataraman?s model uses a more traditional,
smoothed n-gram model to describe the distribu-
tion of words in an unsegmented text.
2
The most
probable segmentation is retrieved via a dynamic
programming algorithm, much like Brent (1999).
We use MBDP-1 rather than Venkataraman?s
approach as the basis for our model only because it
was more transparent how to plug in a phonotactic
learning module at the time this project began.
2.1.3 Goldwater (2007)
We also compare our results to a segmenter put
forward by Goldwater (2007). Goldwater?s seg-
menter uses an underlying generative model, much
like MBDP-1 does, only her language model is
described as a Dirichlet process (see also John-
son, 2008b). While this model uses a unigram
model of phoneme distribution, as did MBDP-1, it
implements a bigram word model like Venkatara-
man (2001). A bigram word model is useful in
that it prevents the segmenter from assuming that
frequent word bigrams are not simply one word,
which Goldwater observes happen with a unigram
version of her model.
Goldwater uses a Gibbs sampler augmented
with simulated annealing to sample from the pos-
terior distribution of segmentations and deter-
mine the most likely segmentation of each utter-
ance.
3
This approach requires non-incremental
learning.
4
We include comparison with Goldwa-
ter?s segmenter because it outperforms MBDP-1
and Venkataraman (2001) in both precision and
recall, and we are interested in whether an incre-
mental algorithm supplemented with phonotactic
learning can match its performance.
2.2 Phonotactic Learning
Phonotactic acquisition models have seen a surge
in popularity recently (e.g., Coleman and Pierre-
2
We refer the reader to Venkataraman (2001) for the de-
tails of this approach.
3
We direct the reader to Goldwater (2007) for details.
4
In our experiments and those in Goldwater (2007), the
segmenter runs through the corpus 1000 times before out-
putting the final segmentation.
humbert, 1997; Heinz, 2007; Hayes and Wilson,
2008). While Hayes and Wilson present a more
complex Maximum Entropy phonotactic model in
their paper than the one we add to MBDP-1, they
also evaluate a simple n-gram phonotactic learner
operating over phonemes. The input to the mod-
els is a list of English onsets and their frequency
in the lexicon, and the basic trigram learner simply
keeps track of the trigrams it has seen in the cor-
pus. They test the model on novel words with ac-
ceptable rhymes?some well-formed (e.g., [kIp]),
and some less well-formed (e.g., [stwIk])?so any
ill-formedness is attributable to onsets. This ba-
sic trigram model explains 87.7% of the variance
in the scores that Scholes (1966) reports his 7th
grade students gave when subjected to the same
test. When Hayes and Wilson run their Maximum
Entropy phonotactic learning model with n-grams
over phonological features, the r-score increases
substantially to 95.6%.
Given the success and simplicity of the basic n-
gram phonotactic model, we choose to integrate
this with MBDP-1.
3 Extending MBDP-1 with Phonotactics
The main contribution of our work is adding
a phonotactic learning component to MBDP-1
(Brent, 1999). As we mention in ?2.1.1, the third
term of Equation 3 is where MBDP-1?s unigram
phonotactic assumption surfaces. The original
model simply multiplies the probabilities of all the
phonemes in the word together and divides by one
minus the probability of a particular phoneme be-
ing the word boundary to come up with probabil-
ity of the phoneme combination. The order of the
phonemes in the word has no effect on its score.
The only change we make to MBDP-1 is to the
third term of Equation 3. In MBDP-Phon this be-
comes
q
?
i=0
P
MLE
(a
i
. . . a
j
) (4)
where a
i
. . . a
j
is an n-gram inside a proposed
word, and a
0
and a
q
are both the word boundary
symbol, #
5
.
It is important to note that probabilities calcu-
lated in Equation 4 are maximum likelihood esti-
mates of the joint probability of each n-gram in the
word. The maximum likelihood estimate (MLE)
5
The model treats word boundary markers like a phoneme
for the purposes of storing n-grams (i.e., a word boundary
marker may occur anywhere within the n-grams).
67
for a particular n-gram inside a word is calculated
by dividing the total number of occurrences of that
n-gram (including in the word we are currently ex-
amining) by the total number of n-grams (includ-
ing those in the current word). The numbers of
n-grams are computed with respect to the obtained
lexicon, not the corpus, and thus the frequency of
lexical items in the corpus does not affect the n-
gram counts, just like Brent?s unigram phonotactic
model and other phonotactic learning models (e.g.,
Hayes and Wilson, 2008).
We use the joint probability instead of the con-
ditional probability which is often used in compu-
tational linguistics (Manning and Sch?utze, 1999;
Jurafsky and Martin, 2000), because of our intu-
ition that the joint probability is truer to the idea
that a phonotactically well-formed word is made
up of n-grams that occur frequently in the lexicon.
On the other hand, the conditional probability is
used when one tries to predict the next phoneme
that will occur in a word, rather than judging the
well-formedness of the word as a whole.
6
We are able to drop the denominator that was
originally in Equation 3, because P
?
(#) is zero
for an n-gram model when n > 1. This sim-
ple modification allows the model to learn what
phonemes are more likely to occur at the begin-
nings and ends of words, and what combinations
of phonemes rarely occur within words.
What is especially interesting about this mod-
ification is that the phonotactic learning compo-
nent estimates the probabilities of the n-grams by
using their relative frequencies in the words the
segmenter has extracted. The phonotactic learner
is guaranteed to see at least two valid patterns in
every utterance, as the n-grams that occur at the
beginnings and ends of utterances are definitely
at the beginnings and ends of words. This al-
lows the learner to provide useful information to
the segmenter even early on, and as the segmenter
correctly identifies more words, the phonotactic
learner has more correct data to learn from. Not
only is this mutually beneficial process supported
by evidence from language acquisitionists (Mat-
tys et al, 1999; Mattys and Jusczyk, 2001), it also
resembles co-training (Blum and Mitchell, 1998).
We refer to the extended version of Brent?s model
6
This intuition is backed up by preliminary results sug-
gesting MBDP-Phon performs better when usingMLEs of the
joint probability as opposed to conditional probability. There
is an interesting question here, which is beyond the scope of
this paper, so we leave it for future investigation.
described above as MBDP-Phon.
4 Evaluation
4.1 The Corpus
We run all of our experiments on the Bernstein-
Ratner (1987) infant-directed speech corpus from
the CHILDES database (MacWhinney and Snow,
1985). This is the same corpus that Brent (1999),
Goldwater (2007), and Venkataraman (2001) eval-
uate their models on, and it has become the de
facto standard for segmentation testing, as unlike
other corpora in CHILDES, it was phonetically
transcribed.
We examine the transcription system Brent
(1999) uses and conclude some unorthodox
choices were made when transcribing the corpus.
Specifically, some phonemes that are normally
considered distinct are combined into one symbol,
which we call a bi-phone symbol. These phonemes
combinations include diphthongs and vowels fol-
lowed by /?/. Another seemingly arbitrary deci-
sion is the distinction between stressed and un-
stressed syllabic /?/ sound (i.e., there are differ-
ent symbols for the /?/ in ?butter? and the /?/ in
?bird?) since stress is not marked elsewhere in the
corpus. To see the effect of these decisions, we
modified the corpus so that the bi-phone symbols
were split into two
7
and the syllabic /?/ symbols
were collapsed into one.
4.2 Accuracy
We ran MBDP-1 on the original corpus, and the
modified version of the corpus. As illustrated by
Figures 1 and 2, MBDP-1 performs worse on the
modified corpus with respect to both precision and
recall. As MBDP-1 and MBDP-Phon are both iter-
ative learners, we calculate segmentation precision
and recall values over 500-utterance blocks. Per
Brent (1999) and Goldwater (2007), precision and
recall scores reflect correctly segmented words,
not correctly identified boundaries.
We also test to see how the addition of an n-gram
phonotactic model affects the segmentation accu-
racy of MBDP-Phon by comparing it to MBDP-
1 on our modified corpus.
8
As seen in Figure 3,
MBDP-Phon using bigrams (henceforth MBDP-
Phon-Bigrams) is consistently more precise in its
7
We only split diphthongs whose first phoneme can occur
in isolation in English, so the vowels in ?bay? and ?boat? were
not split.
8
We also compare MBDP-Phon to MBDP-1 on the origi-
nal corpus. The results are given in Tables 1 and 2.
68
0.45
0.50
0.55
0.60
0.65
0.70
0.75
500 1500 2500 3500 4500 5500 6500 7500 8500 9500
P
r
e
c
i
s
i
o
n
Utterances Processed
Modified Original
Figure 1: Precision of MBDP-1 on both corpora.
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
500 1500 2500 3500 4500 5500 6500 7500 8500 9500
R
e
c
a
l
l
Utterances Processed
Modified Original
Figure 2: Recall of MBDP-1 on both corpora.
segmentation thanMBDP-1, and bests it by? 18%
in the last block. Furthermore, MBDP-Phon-
Bigrams significantly outpaces MBDP-1 with re-
spect to recall only after seeing 1000 utterances,
and finishes the corpus ? 10% ahead of MBDP-
1 (see Figure 4). MBDP-Phon-Trigrams does not
fair as well in our tests, falling behind MBDP-1
and MBDP-Phon-Bigrams in recall, and MBDP-
Phon-Bigrams in precision. We attribute this poor
performance to the fact that we are not currently
smoothing the n-gram models in any way, which
leads to data sparsity issues when using trigrams.
We discuss a potential solution to this problem in
?5.
Having established that MBDP-Phon-Bigrams
significantly outperforms MBDP-1, we compare
its segmentation accuracy to those of Goldwater
(2007) and Venkataraman (2001).
9
As before, we
9
We only examine Venkataraman?s unigram model, as his
bigram and trigram models perform better on precision, but
worse on recall.
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
500 1500 2500 3500 4500 5500 6500 7500 8500 9500
P
r
e
c
i
s
i
o
n
Utterances Processed
MBDP-1 MBDP-Bigrams MBDP-Trigrams
Figure 3: Precision of MBDP-1 and MBDP-Phon
on modified corpus.
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
500 1500 2500 3500 4500 5500 6500 7500 8500 9500
R
e
c
a
l
l
Utterances Processed
MBDP-1 MBDP-Bigrams MBDP-Trigrams
Figure 4: Recall of MBDP-1 and MBDP-Phon on
modified corpus.
run the models on the entire corpus, and then mea-
sure their performance over 500-utterance blocks.
MBDP-Phon-Bigrams edges out Goldwater?s
model in precision on our modified corpus, with
an average precision of 72.79% vs. Goldwa-
ter?s 70.73% (Table 1). If we drop the first 500-
utterance block for MBDP-Phon-Bigrams because
the model is still in the early learning stages,
whereas Goldwater?s has seen the entire corpus, its
average precision increases to 73.21% (Table 1).
When considering the recall scores in Table 2,
it becomes clear that MBDP-Phon-Bigrams has a
clear advantage over the other models. Its aver-
age recall is higher than or nearly equal to both
of the other models? maximum scores. Since
Venkataraman?s (2001) model performs similarly
to MBDP-1, it is no surprise that MBDP-Phon-
Bigrams achieves higher precision and recall.
69
MBDP-
Phon-
Bigrams
Venkataraman Goldwater
Original: Utterances 0 to 9790
Avg. 72.84% 67.46% 67.87%
Max. 79.91% 71.79% 71.98%
Min. 63.97% 61.77% 61.87%
Modified: Utterances 0 to 9790
Avg. 72.79% 59.64% 70.73%
Max. 80.60% 66.84% 74.61%
Min. 64.78% 52.54% 65.29%
Modified: Utterances 500 to 9790
Avg. 73.21% 59.54% 70.59%
Max. 80.60% 66.84% 74.61%
Min. 67.40% 52.54% 65.29%
Table 1: Precision statistics for MBDP-Phon-
Bigrams, Goldwater, and Venkataraman on both
corpora over 500-utterance blocks.
The only metric by which MBDP-Phon-
Bigrams does not outperform the other algorithms
is lexical precision, as shown in Table 3. Lexi-
cal precision is the ratio of the number of correctly
identified words in the lexicon to the total number
of words in the lexicon (Brent, 1999; Venkatara-
man, 2001).
10
The relatively poor performance
of MBDP-Phon-Bigrams is due to the incremental
nature of the MBDP algorithm. Initially, it makes
numerous incorrect guesses that are added to the
lexicon, and there is no point at which the lexi-
con is purged of earlier erroneous guesses (c.f. the
improved lexical precision when omitting the first
block in Table 3). On the other hand, Goldwater?s
algorithm runs over the corpus multiple times, and
only produces output when it settles on a final seg-
mentation.
In sum, MBDP-Phon-Bigrams significantly im-
proves the accuracy of MBDP-1, and achieves
better performance than the models described in
Venkataraman (2001) and Goldwater (2007).
5 Future Work
There are many ways to implement phonotactic
learning. One idea is to to use n-grams over phono-
logical features, as per Hayes and Wilson (2008).
Preliminary results have shown that we need to add
smoothing to our n-grammodel, and we plan to use
10
See Brent (1999) for a discussion of the meaning of this
statistic.
MBDP-
Phon-
Bigrams
Venkataraman Goldwater
Original: Utterances 0 to 9790
Avg. 72.03% 70.02% 71.02%
Max. 79.31% 75.59% 76.79%
Min. 44.71% 42.57% 64.32%
Modified: Utterances 0 to 9790
Avg. 74.63% 66.24% 70.48%
Max. 82.45% 70.47% 74.79%
Min. 47.63% 44.71% 63.74%
Modified: Utterances 500 to 9790
Avg. 76.05% 67.37% 70.28%
Max. 82.45% 70.47% 74.79%
Min. 71.92% 63.86% 63.74%
Table 2: Recall statistics for MBDP-Phon-
Bigrams, Goldwater, and Venkataraman on both
corpora over 500-utterance blocks.
Modified Kneser-Ney smoothing (Chen and Good-
man, 1998).
Another approach would be to develop a
syllable-based phonotactic model (Coleman and
Pierrehumbert, 1997). Johnson (2008b) achieves
impressive segmentation results by adding a sylla-
ble level with Adaptor grammars.
Some languages (e.g., Finnish, and Navajo)
contain long-distance phonotactic constraints that
cannot be learned by n-gram learners (Heinz,
2007). Heinz (2007) shows that precedence-based
learners?which work like a bigram model, but
without the restriction that the elements in the bi-
gram be adjacent?can handle many long-distance
agreement patterns (e.g., vowel and consonantal
harmony) in the world?s languages. We posit that
adding such a learner to MBDP-Phon would allow
it to handle a greater variety of languages.
Since none of these approaches to phonotactic
learning depend on MBDP-1, it is also of interest
to integrate phonotactic learners with other word
segmentation strategies.
In addition to evaluating segmentation models
integrated with phonotactic learning on their seg-
mentation performance, it would be interesting to
evaluate the quality of the phonotactic grammars
obtained. A good point of comparison for English
are the constraints obtained by Hayes and Wilson
(2008), since the data with which they tested their
phonotactic learner is publicly available.
Finally, we are looking forward to investigat-
70
MBDP-
Phon-
Bigrams
Venkataraman Goldwater
Original: Utterances 0 to 9790
Avg. 47.69% 49.78% 56.50%
Max. 49.71% 52.95% 63.09%
Min. 46.30% 41.83% 55.33%
Modified: Utterances 0 to 9790
Avg. 48.31% 45.98% 58.03%
Max. 50.42% 48.90% 65.58%
Min. 41.74% 36.57% 56.43%
Modified: Utterances 500 to 9790
Avg. 54.34% 53.06% 57.95%
Max. 63.76% 54.35% 62.30%
Min. 51.31% 51.95% 56.52%
Table 3: Lexical precision statistics for MBDP-
Phon-Bigrams, Goldwater, and Venkataraman on
both corpora over 500-utterance blocks.
ing the abilities of these segmenters on corpora
of different languages. Fleck (2008) tests her seg-
menter on a number of corpora, including Arabic
and Spanish, and Johnson (2008a) applies his seg-
menter to a corpus of Sesotho.
6 Conclusion
From the results established in ?4, we can con-
clude that MBDP-Phon using a bigram phonotac-
tic model is more accurate than the models de-
scribed in Brent (1999), Venkataraman (2001), and
Goldwater (2007). The n-gram phonotactic model
improves overall performance, and is especially
useful for corpora that do not encode diphthongs
with bi-phone symbols. The main reason there
is such a marked improvement with MBDP-Phon
vs. MBDP-1 when the bi-phone symbols were re-
moved from the original corpus is that these bi-
phone symbols effectively allow MBDP-1 to have
a select few bigrams in the cases where it would
otherwise over-segment.
The success of MBDP-Phon is not clear evi-
dence that the INCDROP framework (Brent, 1997)
is superior to Venkataraman or Goldwater?s mod-
els. We imagine that adding a phonotactic learning
component to either of their models would also im-
prove their performance.
We also tentatively conclude that phonotactic
patterns can be learned from unsegmented text.
However, the phonotactic patterns learned by our
model ought to be studied in detail to see how well
they match the phonotactic patterns of English.
MBDP-Phon?s performance reinforces the the-
ory put forward by language acquisition re-
searchers that phonotactic knowledge is a cue for
word segmentation (Mattys et al, 1999; Mattys
and Jusczyk, 2001). Furthermore, our results in-
dicate that learning phonotactic patterns can oc-
cur simultaneously with word segmentation. Fi-
nally, further investigation of the simultaneous ac-
quisition of phonotactics and word segmentation
appears fruitful for theoretical and computational
linguists, as well as acquisitionists.
Acknoledgements
We are grateful to Roberta Golinkoff who inspired
this project. We also thank Vijay Shanker for
valuable discussion, Michael Brent for the corpus,
and Sharon Goldwater for the latest version of her
code.
References
Batchelder, Eleanor Olds. 2002. Bootstrapping the
lexicon: a computational model of infant speech
segmentation. Cognition, 83(2):167?206.
Bernstein-Ratner, Nan. 1987. The phonology of
parent child speech, volume 6. Erlbaum, Hills-
dale, NJ.
Blum, Avrim and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Workshop on Computational Learning Theory,
pages 92?100.
Bortfeld, Heather, James Morgan, Roberta
Golinkoff, and Karen Rathbun. 2005. Mommy
and me: Familiar names help launch babies into
speech-stream segmentation. Psychological
Science, 16(4):298?304.
Brent, Michael R. 1997. Towards a unified model
of lexical acquisition and lexical access. Journal
of Psycholinguistic Research, 26(3):363?375.
Brent, Michael R. 1999. An efficient, probabilis-
tically sound algorithm for segmentation and
word discovery. Machine Learning, 34:71?105.
Brent, Michael R and Xiaopeng Tao. 2001. Chi-
nese text segmentation with mbdp-1: Making
the most of training corpora. In 39th Annual
Meeting of the ACL, pages 82?89.
Chen, Stanley F and Joshua Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98,
71
Center for Research in Computing Technology,
Harvard University.
Cole, Ronald and Jola Jakimik. 1980. A model of
speech perception, pages 136?163. Lawrence
Erlbaum Associates, Hillsdale, NJ.
Coleman, John and Janet Pierrehumbert. 1997.
Stochastic phonological grammars and accept-
ability. In Third Meeting of the ACL SIGPHON,
pages 49?56. ACL, Somerset, NJ.
Cutler, Anne and David Carter. 1987. The predom-
inance of strong initial syllables in the english
vocabulary. Computer Speech and Language,
2(3-4):133?142.
de Marcken, Carl. 1995. Acquiring a lexicon from
unsegmented speech. In 33rd Annual Meeting
of the ACL, pages 311?313.
Fleck, Margaret M. 2008. Lexicalized phonotactic
word segmentation. In 46th Annual Meeting of
the ACL, pages 130?138. ACL, Morristown, NJ.
Goldwater, Sharon. 2007. Nonparametric
Bayesian Models of Lexical Acquisition. Ph.D.
thesis, Brown University, Department of Cogni-
tive and Linguistic Sciences.
Harris, Zellig. 1954. Distributional structure.
Word, 10(2/3):146?62.
Hayes, Bruce and Colin Wilson. 2008. A maxi-
mum entropy model of phonotactics and phono-
tactic learning. Linguistic Inquiry.
Heinz, Jeffrey. 2007. Inductive Learning of Phono-
tactic Patterns. Ph.D. thesis, University of Cali-
fornia, Los Angeles, Department of Linguistics.
Johnson, Mark. 2008a. Unsupervised word seg-
mentation for sesotho using adaptor grammars.
In Tenth Meeting of ACL SIGMORPHON, pages
20?27. ACL, Morristown, NJ.
Johnson, Mark. 2008b. Using adaptor grammars
to identify synergies in the unsupervised acqui-
sition of linguistic structure. In 46th Annual
Meeting of the ACL, pages 398?406. ACL, Mor-
ristown, NJ.
Jurafsky, Daniel and James Martin. 2000. Speech
and Language Processing. Prentice-Hall.
MacWhinney, Brian and Catherine Snow. 1985.
The child language data exchange system. Jour-
nal of child language, 12(2):271?95.
Manning, Christopher and Hinrich Sch?utze. 1999.
Foundations of Statistical Natural Language
Processing. MIT Press.
Mattys, Sven and Peter Jusczyk. 2001. Phonotac-
tic cues for segmentation of fluent speech by in-
fants. Cognition, 78:91?121.
Mattys, Sven, Peter Jusczyk, Paul Luce, and James
Morgan. 1999. Phonotactic and prosodic effects
on word segmentation in infants. Cognitive Psy-
chology, 38:465?494.
Olivier, Donald. 1968. Stochastic Grammars and
Language Acquisition Mechanisms. Ph.D. the-
sis, Harvard Univerity.
Scholes, Robert. 1966. Phonotactic Grammatical-
ity. Mouton, The Hague.
Teahan, W. J., Rodger McNab, Yingying Wen, and
Ian H. Witten. 2000. A compression-based al-
gorithm for chinese word segmentation. Com-
putational Linguistics, 26(3):375?393.
Venkataraman, Anand. 2001. A statistical model
for word discovery in transcribed speech. Com-
putational Linguistics, 27(3):352?372.
72
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 48?57,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
A Report on the First Native Language Identification Shared Task
Joel Tetreault?, Daniel Blanchard? and Aoife Cahill?
? Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USA
Joel.Tetreault@nuance.com
? Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA
{dblanchard, acahill}@ets.org
Abstract
Native Language Identification, or NLI, is the
task of automatically classifying the L1 of a
writer based solely on his or her essay writ-
ten in another language. This problem area
has seen a spike in interest in recent years
as it can have an impact on educational ap-
plications tailored towards non-native speak-
ers of a language, as well as authorship pro-
filing. While there has been a growing body
of work in NLI, it has been difficult to com-
pare methodologies because of the different
approaches to pre-processing the data, differ-
ent sets of languages identified, and different
splits of the data used. In this shared task, the
first ever for Native Language Identification,
we sought to address the above issues by pro-
viding a large corpus designed specifically for
NLI, in addition to providing an environment
for systems to be directly compared. In this
paper, we report the results of the shared task.
A total of 29 teams from around the world
competed across three different sub-tasks.
1 Introduction
One quickly growing subfield in NLP is the task
of identifying the native language (L1) of a writer
based solely on a sample of their writing in an-
other language. The task is framed as a classifica-
tion problem where the set of L1s is known a priori.
Most work has focused on identifying the native lan-
guage of writers learning English as a second lan-
guage. To date this topic has motivated several pa-
pers and research projects.
Native Language Identification (NLI) can be use-
ful for a number of applications. NLI can be used in
educational settings to provide more targeted feed-
back to language learners about their errors. It
is well known that speakers of different languages
make different kinds of errors when learning a lan-
guage (Swan and Smith, 2001). A writing tutor
system which can detect the native language of the
learner will be able to tailor the feedback about the
error and contrast it with common properties of the
learner?s language. In addition, native language is
often used as a feature that goes into authorship pro-
filing (Estival et al, 2007), which is frequently used
in forensic linguistics.
Despite the growing interest in this field, devel-
opment has been encumbered by two issues. First
is the issue of data. Evaluating an NLI system re-
quires a corpus containing texts in a language other
than the native language of the writer. Because of
a scarcity of such corpora, most work has used the
International Corpus of Learner English (ICLEv2)
(Granger et al, 2009) for training and evaluation
since it contains several hundred essays written by
college-level English language learners. However,
this corpus is quite small for training and testing
statistical systems which makes it difficult to tell
whether the systems that are developed can scale
well to larger data sets or to different domains.
Since the ICLE corpus was not designed with the
task of NLI in mind, the usability of the corpus for
this task is further compromised by idiosyncrasies
in the data such as topic bias (as shown by Brooke
and Hirst (2011)) and the occurrence of characters
which only appear in essays written by speakers of
certain languages (Tetreault et al, 2012). As a result,
it is hard to draw conclusions about which features
48
actually perform best. The second issue is that there
has been little consistency in the field in the use of
cross-validation, the number of L1s, and which L1s
are used. As a result, comparing one approach to
another has been extremely difficult.
The first Shared Task in Native Language Identifi-
cation is intended to better unify this community and
help the field progress. The Shared Task addresses
the two deficiencies above by first using a new cor-
pus (TOEF11, discussed in Section 3) that is larger
than the ICLE and designed specifically for the task
of NLI and second, by providing a common set of
L1s and evaluation standards that everyone will use
for this competition, thus facilitating direct compar-
ison of approaches. In this report we describe the
methods most participants used, the data they eval-
uated their systems on, the three sub-tasks involved,
the results achieved by the different teams, and some
suggestions and ideas about what we can do for the
next iteration of the NLI shared task.
In the following section, we provide a summary
of the prior work in Native Language Identification.
Next, in Section 3 we describe the TOEFL11 cor-
pus used for training, development and testing in this
shared task. Section 4 describes the three sub-tasks
of the NLI Shared Task as well as a review of the
timeline. Section 5 lists the 29 teams that partici-
pated in the shared task, and introduce abbreviations
that will be used throughout this paper. Sections 6
and 7 describe the results of the shared task and a
separate post shared task evaluation where we asked
teams to evaluate their system using cross-validation
on a combination of the training and development
data. In Section 8 we provide a high-level view of
the common features and machine learning methods
teams tended to use. Finally, we offer conclusions
and ideas for future instantiations of the shared task
in Section 9.
2 Related Work
In this section, we provide an overview of some of
the common approaches used for NLI prior to this
shared task. While a comprehensive review is out-
side the scope of this paper, we have compiled a
bibliography of related work in the field. It can be
downloaded from the NLI Shared Task website.1
To date, nearly all approaches have treated the
task of NLI as a supervised classification problem
where statistical models are trained on data from the
different L1s. The work of Koppel et al (2005) was
the first in the field and they explored a multitude
of features, many of which are employed in several
of the systems in the shared tasks. These features
included character and POS n-grams, content and
function words, as well as spelling and grammati-
cal errors (since language learners have tendencies
to make certain errors based on their L1 (Swan and
Smith, 2001)). An SVM model was trained on these
features extracted from a subsection of the ICLE
corpus consisting of 5 L1s.
N-gram features (word, character and POS) have
figured prominently in prior work. Not only are they
easy to compute, but they can be quite predictive.
However, there are many variations on the features.
Past reseach efforts have explored different n-gram
windows (though most tend to focus on unigrams
and bigrams), different thresholds for how many n-
grams to include as well as whether to encode the
feature as binary (presence or absence of the partic-
ular n-gram) or as a normalized count.
The inclusion of syntactic features has been a fo-
cus in recent work. Wong and Dras (2011) explored
the use of production rules from two parsers and
Swanson and Charniak (2012) explored the use of
Tree Substitution Grammars (TSGs). Tetreault et
al. (2012) also investigated the use of TSGs as well
as dependency features extracted from the Stanford
parser.
Other approaches to NLI have included the use of
Latent Dirichlet Analysis to cluster features (Wong
et al, 2011), adaptor grammars (Wong et al, 2012),
and language models (Tetreault et al, 2012). Ad-
ditionally, there has been research into the effects of
training and testing on different corpora (Brooke and
Hirst, 2011).
Much of the aforementioned work takes the per-
spective of optimizing for the task of Native Lan-
guage Identification, that is, what is the best way of
modeling the problem to get the highest system ac-
curacy? The problem of Native Language Identifica-
1http://nlisharedtask2013.org/bibliography-of-related-
work-in-nli
49
tion is also of interest to researchers in Second Lan-
guage Acquisition where they seek to explain syn-
tactic transfer in learner language (Jarvis and Cross-
ley, 2012).
3 Data
The dataset for the task was the new TOEFL11
corpus (Blanchard et al, 2013). TOEFL11 con-
sists of essays written during a high-stakes college-
entrance test, the Test of English as a Foreign Lan-
guage (TOEFL R?). The corpus contains 1,100 es-
says per language sampled as evenly as possible
from 8 prompts (i.e., topics) along with score lev-
els (low/medium/high) for each essay. The 11 na-
tive languages covered by our corpus are: Ara-
bic (ARA), Chinese (CHI), French (FRE), German
(GER), Hindi (HIN), Italian (ITA), Japanese (JAP),
Korean (KOR), Spanish (SPA), Telugu (TEL), and
Turkish (TUR).
The TOEFL11 corpus was designed specifically
to support the task of native language identifica-
tion. Because all of the essays were collected
through ETS?s operational test delivery system for
the TOEFL R? test, the encoding and storage of all
texts in the corpus is consistent. Furthermore, the
sampling of essays was designed to ensure approx-
imately equal representation of native languages
across topics, insofar as this was possible.
For the shared task, the corpus was split into
three sets: training (TOEFL11-TRAIN), development
(TOEFL11-DEV), and test (TOEFL11-TEST). The
train corpus consisted of 900 essays per L1, the de-
velopment set consisted of 100 essays per L1, and
the test set consisted of another 100 essays per L1.
Although the overall TOEFL11 corpus was sampled
as evenly as possible with regard to language and
prompts, the distribution for each language is not ex-
actly the same in the training, development and test
sets (see Tables 1a, 1b, and 1c). In fact, the distri-
bution is much closer between the training and test
sets, as there are several languages for which there
are no essays for a given prompt in the development
set, whereas there are none in the training set, and
only one, Italian, for the test set.
It should be noted that in the first instantiation of
the corpus, presented in Tetreault et al (2012), we
used TOEFL11 to denote the body of data consisting
of TOEFL11-TRAIN and TOEFL11-DEV. However,
in this shared task, we added 1,100 sentences for a
test set and thus use the term TOEFL11 to now de-
note the corpus consisting of the TRAIN, DEV and
TEST sets. We expect the corpus to be released
through the the Linguistic Data Consortium in 2013.
4 NLI Shared Task Description
The shared task consisted of three sub-tasks. For
each task, the test set was TOEFL11-TEST and only
the type of training data varied from task to task.
? Closed-Training: The first and main task
was the 11-way classification task using only
the TOEFL11-TRAIN and optionally TOEFL11-
DEV for training.
? Open-Training-1: The second task allowed
the use of any amount or type of training data
(as is done by Brooke and Hirst (2011)) exclud-
ing any data from the TOEFL11, but still evalu-
ated on TOEFL11-TEST.
? Open-Training-2: The third task allowed the
use of TOEFL11-TRAIN and TOEFL11-DEV
combined with any other additional data. This
most closely reflects a real-world scenario.
Additionally, each team could submit up to 5 dif-
ferent systems per task. This allowed a team to ex-
periment with different variations of their core sys-
tem.
The training data was released on January 14,
with the development data and evaluation script re-
leased almost one month later on February 12. The
train and dev data contained an index file with the L1
for each essay in those sets. The previously unseen
and unlabeled test data was released on March 11
and teams had 8 days to submit their system predic-
tions. The predictions for each system were encoded
in a CSV file, where each line contained the file ID
of a file in TOEFL11-TEST and the corresponding
L1 prediction made by the system. Each CSV file
was emailed to the NLI organizers and then evalu-
ated against the gold standard.
5 Teams
In total, 29 teams competed in the shared task com-
petition, with 24 teams electing to write papers de-
scribing their system(s). The list of participating
50
Lang. P1 P2 P3 P4 P5 P6 P7 P8
ARA 113 113 113 112 112 113 112 112
CHI 113 113 113 112 112 113 112 112
FRE 128 128 76 127 127 60 127 127
GER 125 125 125 125 125 26 125 124
HIN 132 132 132 71 132 38 132 131
ITA 142 70 122 141 141 12 141 131
JAP 108 114 113 113 113 113 113 113
KOR 113 113 113 112 112 113 112 112
SPA 124 120 38 124 123 124 124 123
TEL 139 139 139 41 139 26 139 138
TUR 132 132 72 132 132 37 132 131
Total 1369 1299 1156 1210 1368 775 1369 1354
(a) Training Set
Lang. P1 P2 P3 P4 P5 P6 P7 P8
ARA 12 13 13 13 14 7 14 14
CHI 14 14 0 15 15 14 13 15
FRE 17 18 0 14 19 0 13 19
GER 15 15 16 10 13 0 15 16
HIN 16 17 17 0 17 0 16 17
ITA 18 0 0 30 31 0 21 0
JAP 0 14 15 14 15 14 14 14
KOR 15 8 15 2 13 15 16 16
SPA 7 0 0 21 7 21 21 23
TEL 16 17 17 0 17 0 16 17
TUR 22 4 0 22 7 0 22 23
Total 152 120 93 141 168 71 181 174
(b) Dev Set
Lang. P1 P2 P3 P4 P5 P6 P7 P8
ARA 13 11 12 14 10 13 12 15
CHI 13 14 13 13 7 14 14 12
FRE 13 14 11 15 14 8 11 14
GER 15 14 16 16 12 2 12 13
HIN 13 13 14 15 7 15 10 13
ITA 13 19 16 16 15 0 11 10
JAP 8 14 12 11 10 15 14 16
KOR 12 12 8 14 12 14 13 15
SPA 10 13 16 14 4 12 15 16
TEL 10 10 11 14 13 15 11 16
TUR 15 9 18 16 8 6 13 15
Total 135 143 147 158 112 114 136 155
(c) Test Set
Table 1: Number of essays per language per prompt in each data set
teams, along with their abbreviations, can be found
in Table 2.
6 Shared Task Results
This section summarizes the results of the shared
task. For each sub-task, we have tables listing the
51
Team Name Abbreviation
Bobicev BOB
Chonger CHO
CMU-Haifa HAI
Cologne-Nijmegen CN
CoRAL Lab @ UAB COR
CUNI (Charles University) CUN
cywu CYW
dartmouth DAR
eurac EUR
HAUTCS HAU
ItaliaNLP ITA
Jarvis JAR
kyle, crossley, dai, mcnamara KYL
LIMSI LIM
LTRC IIIT Hyderabad HYD
Michigan MIC
MITRE ?Carnie? CAR
MQ MQ
NAIST NAI
NRC NRC
Oslo NLI OSL
Toronto TOR
Tuebingen TUE
Ualberta UAB
UKP UKP
Unibuc BUC
UNT UNT
UTD UTD
VTEX VTX
Table 2: Participating Teams and Team Abbrevia-
tions
top submission for each team and its performance
by overall accuracy and by L1.2
Table 3 shows results for the Closed sub-task
where teams developed systems that were trained
solely on TOEFL11-TRAIN and TOEFL11-DEV. This
was the most popular sub-task with 29 teams com-
peting and 116 submissions in total for the sub-task.
Most teams opted to submit 4 or 5 runs.
The Open sub-tasks had far fewer submissions.
Table 4 shows results for the Open-1 sub-task where
teams could train systems using any training data ex-
cluding TOEFL11-TRAIN and TOEFL11-DEV. Three
teams competed in this sub-task for a total of 13 sub-
2For those interested in the results of all submissions, please
contact the authors.
missions. Table 5 shows the results for the third sub-
task ?Open-2?. Four teams competed in this task for
a total of 15 submissions.
The challenge for those competing in the Open
tasks was finding enough non-TOEFL11 data for
each L1 to train a classifier. External corpora com-
monly used in the competition included the:
? ICLE: which covered all L1s except for Ara-
bic, Hindi and Telugu;
? FCE: First Certificate in English Corpus
(Yannakoudakis et al, 2011): a collection of
essay written for an English assessment exam,
which covered all L1s except for Arabic, Hindi
and Telugu
? ICNALE: International Corpus Network of
Asian Learners of English (Ishikawa, 2011):
a collection of essays written by Chinese,
Japanese and Korean learners of English along
with 7 other L1s with Asian backgrounds.
? Lang8: http://www.lang8.com: a social net-
working service where users write in the lan-
guage they are learning, and get corrections
from users who are native speakers of that lan-
guage. Shared Task participants such as NAI
and TOR scraped the website for all writng
samples from English language learners. All
of the L1s in the shared task are represented on
the site, though the Asian L1s dominate.
The most challenging L1s to find data for seemed
to be Hindi and Telugu. TUE used essays written
by Pakastani students in the ICNALE corpus to sub-
stitute for Hindi. For Telugu, they scraped mate-
rial from bilingual blogs (English-Telugu) as well
as other material for the web. TOR created cor-
pora for Telugu and Hindi by scraping news articles,
tweets which were geolocated in the Hindi and Tel-
ugu speaking areas, and translations of Hindi and
Telugu blogs using Google Translate.
We caution directly comparing the results of the
Closed sub-task to the Open ones. In the Open-1
sub-task most teams had smaller training sets than
used in the Closed competition which automatically
puts them at a disadvantage, and in some cases there
52
L1 F-Score
Team
Name
Run Overall
Acc.
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
JAR 2 0.836 0.785 0.856 0.860 0.893 0.775 0.905 0.854 0.813 0.798 0.802 0.854
OSL 2 0.834 0.816 0.850 0.874 0.912 0.792 0.873 0.828 0.806 0.783 0.792 0.840
BUC 5 0.827 0.840 0.866 0.853 0.931 0.736 0.873 0.851 0.812 0.779 0.760 0.796
CAR 2 0.826 0.859 0.847 0.810 0.921 0.762 0.877 0.825 0.827 0.768 0.802 0.790
TUE 1 0.822 0.810 0.853 0.806 0.897 0.768 0.883 0.842 0.776 0.772 0.824 0.812
NRC 4 0.818 0.804 0.845 0.848 0.916 0.745 0.903 0.818 0.790 0.788 0.755 0.790
HAI 1 0.815 0.804 0.842 0.835 0.903 0.759 0.845 0.825 0.806 0.776 0.789 0.784
CN 2 0.814 0.778 0.845 0.848 0.882 0.744 0.857 0.812 0.779 0.787 0.784 0.827
NAI 1 0.811 0.814 0.829 0.828 0.876 0.755 0.864 0.806 0.789 0.757 0.793 0.802
UTD 2 0.809 0.778 0.846 0.832 0.892 0.731 0.866 0.846 0.819 0.715 0.784 0.784
UAB 3 0.803 0.820 0.804 0.822 0.905 0.724 0.850 0.811 0.736 0.777 0.792 0.786
TOR 1 0.802 0.754 0.827 0.827 0.878 0.722 0.850 0.820 0.808 0.747 0.784 0.798
MQ 4 0.801 0.800 0.828 0.789 0.885 0.738 0.863 0.826 0.780 0.703 0.782 0.802
CYW 1 0.797 0.769 0.839 0.782 0.833 0.755 0.842 0.815 0.770 0.741 0.828 0.788
DAR 2 0.781 0.761 0.806 0.812 0.870 0.706 0.846 0.788 0.776 0.730 0.723 0.767
ITA 1 0.779 0.738 0.775 0.832 0.873 0.711 0.860 0.788 0.742 0.708 0.762 0.780
CHO 1 0.775 0.764 0.835 0.798 0.888 0.721 0.816 0.783 0.670 0.688 0.786 0.758
HAU 1 0.773 0.731 0.820 0.806 0.897 0.686 0.830 0.832 0.763 0.703 0.702 0.736
LIM 4 0.756 0.737 0.760 0.788 0.886 0.654 0.808 0.775 0.756 0.712 0.701 0.745
COR 5 0.748 0.704 0.806 0.783 0.898 0.670 0.738 0.794 0.739 0.616 0.730 0.741
HYD 1 0.744 0.680 0.778 0.748 0.839 0.693 0.788 0.781 0.735 0.613 0.770 0.754
CUN 1 0.725 0.696 0.743 0.737 0.830 0.714 0.838 0.676 0.670 0.680 0.697 0.684
UNT 3 0.645 0.667 0.682 0.635 0.746 0.558 0.687 0.676 0.620 0.539 0.667 0.609
BOB 4 0.625 0.513 0.684 0.638 0.751 0.612 0.706 0.647 0.549 0.495 0.621 0.608
KYL 1 0.590 0.589 0.603 0.643 0.634 0.554 0.663 0.627 0.569 0.450 0.649 0.507
UKP 2 0.583 0.592 0.560 0.624 0.653 0.558 0.616 0.631 0.565 0.456 0.656 0.489
MIC 3 0.430 0.419 0.386 0.411 0.519 0.407 0.488 0.422 0.384 0.400 0.500 0.396
EUR 1 0.386 0.500 0.390 0.277 0.379 0.487 0.522 0.441 0.352 0.281 0.438 0.261
VTX 5 0.319 0.367 0.298 0.179 0.297 0.159 0.435 0.340 0.370 0.201 0.410 0.230
Table 3: Results for closed task
L1 F-Score
Team
Name
Run Overall
Acc.
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
TOR 5 0.565 0.410 0.776 0.692 0.754 0.277 0.680 0.660 0.650 0.653 0.190 0.468
TUE 2 0.385 0.114 0.502 0.420 0.430 0.167 0.611 0.485 0.348 0.385 0.236 0.314
NAI 2 0.356 0.329 0.450 0.331 0.423 0.066 0.511 0.426 0.481 0.314 0.000 0.207
Table 4: Results for open-1 task
L1 F-Score
Team
Name
Run Overall
Acc.
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
TUE 1 0.835 0.798 0.876 0.844 0.883 0.777 0.883 0.836 0.794 0.846 0.826 0.818
TOR 4 0.816 0.770 0.861 0.840 0.900 0.704 0.860 0.834 0.800 0.816 0.804 0.790
HYD 1 0.741 0.677 0.782 0.755 0.829 0.693 0.784 0.777 0.728 0.613 0.766 0.744
NAI 3 0.703 0.676 0.695 0.708 0.846 0.618 0.830 0.677 0.610 0.663 0.726 0.688
Table 5: Results for open-2 task
53
was a mismatch in the genre of corpora (for exam-
ple, tweets by Telugu speakers are different in com-
position than essays written by Telugu speakers).
TUE and TOR were the only two teams to partic-
ipate in all three sub-tasks, and their Open-2 sys-
tems outperformed their respective best systems in
the Closed and Open-1 sub-tasks. This suggests, un-
surprisingly, that adding more data can benefit NLI,
though quality and genre of data are also important
factors.
7 Cross Validation Results
Upon completion of the competition, we asked the
participants to perform 10-fold cross-validation on a
data set consisting of the union of TOEFL11-TRAIN
and TOEFL11-DEV. This was the same set of data
used in the first work to use any of the TOEFL11
data (Tetreault et al, 2012), and would allow another
point of comparison for future NLI work. For direct
comparison with Tetreault et al (2012), we provided
the exact folds used in that work.
The results of the 10-fold cross-validation are
shown in Table 6. Two teams had systems that per-
formed at 84.5 or better, which is just slightly higher
than the best team performance on the TOEFL11-
TEST data. In general, systems that performed well
in the main competition also performed similarly
(in terms of performance and ranking) in the cross-
validation experiment. Please note that we report
results as they are reported in the respective papers,
rounding to just one decimal place where possible.
8 Discussion of Approaches
With so many teams competing in the shared task
competition, we investigated whether there were any
commonalities in learning methods or features be-
tween the teams. In this section, we provide a coarse
grained summary of the common machine learning
methods teams employed as well as some of the
common features. Our summary is based on the in-
formation provided in the 24 team reports.
While there are many machine learning algo-
rithms to choose from, the overwhelming majority
of teams used Support Vector Machines. This may
not be surprising given that most prior work has also
used SVMs. Tetreault et al (2012) showed that one
could achieve even higher performance on the NLI
Team Accuracy
CN 84.6
JAR 84.5
OSL 83.9
BUC 82.6
MQ 82.5
TUE 82.4
CAR 82.2
NAI 82.1
Tetreault et al (2012) 80.9
HAU 79.9
LIM 75.9
CUN 74.2
UNT 63.8
MIC 63
Table 6: Results for 10-fold cross-validation on
TOEFL11-TRAIN + TOEFL11-DEV
task using ensemble methods for combining classi-
fiers. Four teams also experimented with different
ways of using ensemble methods. Three teams used
Maximum Entropy methods for their modeling. Fi-
nally, there were a few other teams that tried differ-
ent methods such as Discriminant Function Analysis
and K-Nearest Neighbors. Possibly the most distinct
method employed was that of string kernels by the
BUC team (who placed third in the closed compe-
tition). This method only used character level fea-
tures. A summary of the machine learning methods
is shown in Table 7.
A summary of the common features used across
teams is shown in Table 8. It should be noted that
the table does not detail the nuanced differences in
how the features are realized. For example, in the
case of n-grams, some teams used only the top k
most frequently n-grams while others used all of the
n-grams available. If interested in more information
about the particulars of a system and its feature, we
recommend reading the team?s summary report.
The most common features were word, character
and POS n-gram features. Most teams used n-grams
ranging from unigrams to trigrams, in line with prior
literature. However several teams used higher-order
n-grams. In fact, four of the top five teams (JAR,
OSL, CAR, TUE) generally used at least 4-grams,
54
Machine Learning Teams
SVM CN, UNT, MQ, JAR, TOR, ITA, CUN, TUE, COR, NRC, HAU, MIC, CAR
MaxEnt / logistic regression LIM, HAI, CAR
Ensemble MQ, ITA, NRC, CAR
Discriminant Function Analysis KYL
String Kernels / LRD BUC
PPM BOB
k-NN VTX
Table 7: Machine Learning algorithms used in Shared Task
and some, such as OSL and JAR, went as high 7 and
9 respectively in terms of character n-grams.
Syntactic features, which were first evaluated in
Wong and Dras (2011) and Swanson and Char-
niak (2012) were used by six teams in the competi-
tion, with most using dependency parses in different
ways. Interestingly, while Wong and Dras (2011)
showed some of the highest performance scores on
the ICLE corpus using parse features, only two of
the six teams which used them placed in the top ten
in the Closed sub-task.
Spelling features were championed by Koppel et
al. (2005) and in subsequent NLI work, however
only three teams in the competition used them.
There were several novel features that teams tried.
For example, several teams tried skip n-grams, as
well as length of words, sentences and documents;
LIM experimented with machine translation; CUN
had different features based on the relative frequen-
cies of the POS and lemma of a word; HAI tried
several new features based on passives and context
function; and the TUE team tried a battery of syn-
tactic features as well as text complexity measures.
9 Summary
We consider the first edition of the shared task a
success as we had 29 teams competing, which we
consider a large number for any shared task. Also
of note is that the task brought together researchers
not only from the Computational Linguistics com-
munity, but also those from other linguistics fields
such as Second Language Acquisition.
We were also delighted to see many teams build
on prior work but also try novel approaches. It is
our hope that finally having an evaluation on a com-
mon data set will allow researchers to learn from
each other on what works well and what does not,
and thus the field can progress more rapidly. The
evaluation scripts are publicly available and we ex-
pect that the data will become available through the
Linguistic Data Consortium in 2013.
For future editions of the NLI shared task, we
think it would be interesting to expand the scope of
NLI from identifying the L1 of student essays to be
able to identify the L1 of any piece of writing. The
ICLE and TOEFL11 corpora are both collections of
academic writing and thus it may be the case that
certain features or methodologies generalize better
to other writing genres and domains. For those in-
terested in robust NLI approaches, please refer to the
TOR team shared task report as well as Brooke and
Hirst (2012).
In addition, since the TOEFL11 data contains pro-
ficiency level one could include an evaluation by
proficiency level as language learners make differ-
ent types of errors and may even have stylistic differ-
ences in their writing as their proficiency progresses.
Finally, while this may be in the periphery of the
scope of an NLI shared task, one interesting evalua-
tion is to see how well human raters can fare on this
task. This would of course involve knowledgeable
language instructors who have years of experience
in teaching students from different L1s. Our think-
ing is that NLI might be one task where computers
would outperform human annotators.
Acknowledgments
We would like to thank Derrick Higgins and mem-
bers of Educational Testing Service for assisting us
in making the TOEFL11 essays available for this
shared task. We would also like to thank Patrick
Houghton for assisting the shared task organizers.
55
Feature Type Teams
Word N-Grams 1 CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, UAB,
CYW, NAI, NRC, MIC, CAR
2 CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, COR,
UAB, CYW, NAI, NRC, HAU, MIC, CAR
3 UNT, MQ, JAR, KYL, CUN, COR, HAU, MIC, CAR
4 JAR, KYL, CAR
5 CAR
POS N-grams 1 CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, HAI, CAR
2 CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, COR, HAI,
NAI, NRC, MIC, CAR
3 CN, UNT, JAR, TOR, LIM, CUN, TUE, COR, HAI, NAI, NRC,
CAR
4 CN, JAR, TUE, HAI, NRC, CAR
5 TUE, CAR
Character N-Grams 1 CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, HAI, CAR
2 CN, UNT, MQ, JAR, TOR, ITA, LIM, BOB, OSL, COR, HAI, NAI,
HAU, MIC, CAR
3 CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, VTX, COR, HAI,
NAI, NRC, HAU, MIC, CAR
4 CN, JAR, LIM, BOB, OSL, HAI, HAU, MIC, CAR
5 CN, JAR, BOB, OSL, HAU, CAR
6 CN, JAR, OSL,
7 JAR, OSL
8-9 JAR
Function N-Grams MQ, UAB
Syntactic Features Dependencies MQ, TOR, ITA, TUE, NAI, NRC
TSG MQ, TOR, NAI,
CF Productions TOR,
Adaptor Grammars MQ
Spelling Features LIM,CN, HAI
Table 8: Common Features used in Shared Task
In addition, thanks goes to the BEA8 Organizers
(Joel Tetreault, Jill Burstein and Claudia Leacock)
for hosting the shared task with their workshop. Fi-
nally, we would like to thank all the teams for partic-
ipating in this first shared task and making it a suc-
cess. Their feedback, patience and enthusiasm made
organizing this shared task a great experience.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391?408, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 263?272, Melbourne, Australia.
Sylviane Granger, Estelle Dagneaux, and Fanny Meunier.
2009. The International Corpus of Learner English:
Handbook and CD-ROM, version 2. Presses Universi-
taires de Louvain, Louvain-la-Neuve, Belgium.
Shin?ichiro Ishikawa. 2011. A New Horizon in Learner
Corpus Studies: The Aim of the ICNALE Projects. In
G. Weir, S. Ishikawa, and K. Poonpon, editors, Cor-
56
pora and Language Technologies in Teaching, Learn-
ing and Research. University of Strathclyde Publish-
ing.
Scott Jarvis and Scott Crossley, editors. 2012. Approach-
ing Language Transfer Through Text Classification:
Explorations in the Detection-based Approach, vol-
ume 64. Multilingual Matters Limited, Bristol, UK.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Michael Swan and Bernard Smith, editors. 2001.
Learner English: A teacher?s guide to interference and
other problems. Cambridge University Press, 2 edi-
tion.
Benjamin Swanson and Eugene Charniak. 2012. Na-
tive Language Detection with Tree Substitution Gram-
mars. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 193?197, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic Modeling for Native Language Identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124, Canberra, Australia, December.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 180?189, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
57
