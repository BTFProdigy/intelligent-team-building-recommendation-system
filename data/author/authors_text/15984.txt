2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 377?381,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Intra-Speaker Topic Modeling for Improved Multi-Party Meeting
Summarization with Integrated Random Walk
Yun-Nung Chen and Florian Metze
School of Computer Science, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213-3891, USA
{yvchen,fmetze}@cs.cmu.edu
Abstract
This paper proposes an improved approach to extrac-
tive summarization of spoken multi-party interac-
tion, in which integrated random walk is performed
on a graph constructed on topical/ lexical relations.
Each utterance is represented as a node of the graph,
and the edges? weights are computed from the topi-
cal similarity between the utterances, evaluated us-
ing probabilistic latent semantic analysis (PLSA),
and from word overlap. We model intra-speaker
topics by partially sharing the topics from the same
speaker in the graph. In this paper, we perform ex-
periments on automatically and manually generated
transcripts. For automatic transcripts, our results
show that intra-speaker topic sharing and integrating
topical/ lexical relations can help include the impor-
tant utterances.
1 Introduction
Speech summarization is an active and important topic of
research (Lee and Chen, 2005), because multimedia/ spo-
ken documents are more difficult to browse than text or
image content. While earlier work was focused primarily
on broadcast news content, recent effort has been increas-
ingly directed to new domains such as lectures (Glass
et al, 2007; Chen et al, 2011) and multi-party interac-
tion (Banerjee and Rudnicky, 2008; Liu and Liu, 2010).
We describe experiments on multi-party interaction found
in meeting recordings, performing extractive summariza-
tion (Liu et al, 2010) on transcripts generated by auto-
matic speech recognition (ASR) and human annotators.
Graph-based methods for computing lexical centrality
as importance to extract summaries (Erkan and Radev,
2004) have been investigated in the context of text sum-
marization. Some works focus on maximizing cover-
age of summaries using the objective function (Gillick,
2011). Speech summarization carries intrinsic difficul-
ties due to the presence of recognition errors, sponta-
neous speech effect, and lack of segmentation. A gen-
eral approach has been found very successful (Furui et
al., 2004), in which each utterance in the document d,
U = t1t2...ti...tn, represented as a sequence of terms ti,
is given an importance score:
I(U, d) =
1
n
n?
i=1
[?1s(ti, d) + ?2l(ti) (1)
+ ?3c(ti) + ?4g(ti)] + ?5b(U),
where s(ti, d), l(ti), c(ti), g(ti) are respectively some
statistical measure (such as TF-IDF), linguistic measure
(e.g., different part-of-speech tags are given different
weights), confidence score and N-gram score for the term
ti, and b(U) is calculated from the grammatical structure
of the utterance U , and ?1, ?2, ?3, ?4 and ?5 are weight-
ing parameters. For each document, the utterances to be
used in the summary are then selected based on this score.
In recent work, Chen (2011) proposed a graphical
structure to rescore I(U, d), which can model the topical
coherence between utterances using random walk within
documents. Similarly, we now use a graph-based ap-
proach to consider the importance of terms and the simi-
larity between utterances, where topical and lexical simi-
larity are integrated in the graph, so that utterances topi-
cally or lexically similar to more important utterances are
given higher scores. Using topical similarity can com-
pensate the negative effects of recognition errors on sim-
ilarity evaluated on word overlap to some extent. In addi-
tion, this paper proposes an approach of modeling intra-
speaker topics in the graph to improve meeting summa-
rization (Garg et al, 2009) using information from multi-
party interaction, which is not available in lectures or
broadcast news.
2 Proposed Approach
We apply word stemming and noise utterance filtering for
utterances in all meetings. Then we construct a graph to
compute the importance of all utterances.
377
U1 
U2 
U3 U4 
U5 
U6 
pt(4, 3) 
pt(3, 4) 
A4t = {U3, U5, U6} 
B4t= {U1, U3, U6} 
Figure 1: A simplified example of the graph considered.
We formulate the utterance selection problem as ran-
dom walk on a directed graph, in which each utterance
is a node and the edges between them are weighted by
topical and lexical similarity. The basic idea is that an
utterance similar to more important utterances should be
more important (Chen et al, 2011). We formulate two
types of directed edge, topical edges and lexical edges,
which are weighted by topical and lexical similarity re-
spectively. We then keep only the top N outgoing edges
with the highest weights from each node, while consider
incoming edges to each node for importance propagation
in the graph. A simplified example for such a graph with
topical edges is in Figure 1, in which Ati and B
t
i are the
sets of neighbors of the node Ui connected respectively
by outgoing and incoming topical edges.
2.1 Parameters from PLSA
Probabilistic latent semantic analysis (PLSA) (Hofmann,
1999) has been widely used to analyze the semantics
of documents based on a set of latent topics. Given
a set of documents {dj , j = 1, 2, ..., J} and all terms
{ti, i = 1, 2, ...,M} they include, PLSA uses a set of
latent topic variables, {Tk, k = 1, 2, ...,K}, to charac-
terize the ?term-document? co-occurrence relationships.
The PLSA model can be optimized with EM algorithm
by maximizing a likelihood function. We utilize two pa-
rameters from PLSA, latent topic significance (LTS) and
latent topic entropy (LTE) (Kong and Lee, 2011) in the
paper.
Latent Topic Significance (LTS) for a given term ti
with respect to a topic Tk can be defined as
LTSti(Tk) =
?
dj?D
n(ti, dj)P (Tk | dj)
?
dj?D
n(ti, dj)[1? P (Tk | dj)]
, (2)
where n(ti, dj) is the occurrence count of term ti in a
document dj . Thus, a higher LTSti(Tk) indicates the
term ti is more significant for the latent topic Tk.
Latent Topic Entropy (LTE), for a given term ti can be
calculated from the topic distribution P (Tk | ti):
LTE(ti) = ?
K?
k=1
P (Tk | ti) logP (Tk | ti), (3)
where the topic distribution P (Tk | ti) can be estimated
from PLSA. LTE(ti) is a measure of how the term ti is
focused on a few topics, so a lower latent topic entropy
implies the term carries more topical information.
2.2 Statistical Measures of a Term
The statistical measure of a term ti, s(ti, d) in (1) can be
defined in terms of LTE(ti) in (3) as
s(ti, d) =
? ? n(ti, d)
LTE(ti)
, (4)
where ? is a scaling factor such that 0 ? s(ti, d) ? 1; the
score s(ti, d) is inversely proportion to the latent topic
entropy LTE(ti). Some works (Kong and Lee, 2011)
showed that the use in (1) of s(ti, d) as defined in (4) out-
performed the very successful ?significance score? (Furui
et al, 2004) in speech summarization; then, we use it as
the baseline.
2.3 Similarity between Utterances
Within a document d, we can first compute the probabil-
ity that the topic Tk is addressed by an utterance Ui:
P (Tk | Ui) =
?
t?Ui
n(t, Ui)P (Tk | t)
?
t?Ui
n(t, Ui)
. (5)
Then an asymmetric topical similarity TopicSim(Ui, Uj)
for utterances Ui to Uj (with direction Ui ? Uj) can
be defined by accumulating LTSt(Tk) in (2) weighted by
P (Tk | Ui) for all terms t in Uj over all latent topics:
TopicSim(Ui, Uj) =
?
t?Uj
K?
k=1
LTSt(Tk)P (Tk | Ui),
(6)
where the idea is very similar to the generative probability
in IR. We call it generative significance of Ui given Uj .
Within a document d, the lexical similarity is the mea-
sure of word overlap between the utterance Ui and Uj .
We compute LexSim(Ui, Uj) as the cosine similarity be-
tween two TF-IDF vectors from Ui and Uj like well-
known LexRank (Erkan and Radev, 2004). Note that
LexSim(Ui, Uj) = LexSim(Uj , Ui)
2.4 Intra-Speaker Topic Modeling
We assume a single speaker usually focuses on similar
topics, so if an utterance is important, the scores of the
utterances from the same speaker should be increased.
Then we increase the similarity between the utterances
from the same speaker to share the topics:
TopicSim?k(Ui, Uj) =
?
???
???
TopicSim(Ui, Uj)1+w
, if Ui ? Sk and Uj ? Sk
TopicSim(Ui, Uj)1?w
, otherwise
(7)
378
where Sk is the set including all utterances from speaker
k, and w is a weighting parameter for modeling the
speaker relation, which means the level of coherence of
topics within a single speaker. Here the topics from the
same speaker can partially shared.
2.5 Integrated Random Walk
We modify random walk (Hsu and Kennedy, 2007; Chen
et al, 2011) to integrate two types of similarity over the
graph obtained above. v(i) is the new score for node Ui,
which is the interpolation of three scores, the normalized
initial importance r(i) for node Ui and the score con-
tributed by all neighboring nodes Uj of node Ui weighted
by pt(j, i) and pl(j, i),
v(i) = (1? ?? ?)r(i) (8)
+ ?
?
Uj?Bti
pt(j, i)v(j) + ?
?
Uj?Bli
pl(j, i)v(j),
where ? and ? are the interpolation weights, Bti is the set
of neighbors connected to node Ui via topical incoming
edges,Bli is the set of neighbors connected to node Ui via
lexical incoming edges, and
r(i) =
I(Ui, d)
?
Uj
I(Uj , d)
(9)
is normalized importance scores of utterance Ui, I(Ui, d)
in (1). We normalize topical similarity by the total sim-
ilarity summed over the set of outgoing edges, to pro-
duce the weight pt(j, i) for the edge from Uj to Ui on the
graph. Similarly, pl(j, i) is normalized in lexical edges.
(8) can be iteratively solved with the approach very
similar to that for the PageRank problem (Page et al,
1998). Let v = [v(i), i = 1, 2, ..., L]T and r = [r(i), i =
1, 2, ..., L]T be the column vectors for v(i) and r(i) for all
utterances in the document, where L is the total number
of utterances in the document d and T represents trans-
pose. (8) then has a vector form below,
v = (1? ?? ?)r+ ?Ptv + ?Plv (10)
=
(
(1? ?? ?)reT + ?Pt + ?Pl
)
v = P?v,
where Pt and Pl areL?Lmatrices of pt(j, i) and pl(j, i)
respectively, and e = [1, 1, ..., 1]T. It has been shown
that the solution v of (10) is the dominant eigenvector
of P? (Langville and Meyer, 2006), or the eigenvector
corresponding to the largest absolute eigenvalue of P?.
The solution v(i) can then be obtained.
3 Experiments
3.1 Corpus
The corpus used in this research consists of a sequence of
naturally occuring meetings, which featured largely over-
lapping participant sets and topics of discussion. For each
meeting, SmartNotes (Banerjee and Rudnicky, 2008) was
used to record both the audio from each participant as
well as his notes. The meetings were transcribed both
manually and using a speech recognizer; the word error
rate is around 44%. In this paper we use 10 meetings held
from April to June of 2006. On average each meeting had
about 28 minutes of speech. Across these 10 meetings
there were 6 unique participants; each meeting featured
between 2 and 4 of these participants (average: 3.7). The
total number of utterances is 9837 across 10 meetings. In
this paper, we separate dev set (2 meetings) and test set
(8 meetings). Dev set is used to tune the parameters such
as ?, ?, w.
The reference summaries are given by the set of ?note-
worthy utterances?: two annotators manually labelled the
degree (three levels) of ?noteworthiness? for each utter-
ance, and we extract the utterances with the top level of
?noteworthiness? to form the summary of each meeting.
In the following experiments, for each meeting, we ex-
tract the top 30% number of terms as the summary.
3.2 Evaluation Metrics
Automated evaluation utilizes the standard DUC eval-
uation metric ROUGE (Lin, 2004) which represents
recall over various n-grams statistics from a system-
generated summary against a set of human generated peer
summaries. F-measures for ROUGE-1 (unigram) and
ROUGE-L (longest common subsequence) can be eval-
uated in exactly the same way, which are used in the fol-
lowing results.
3.3 Results
Table 1 shows the performance achieved by all proposed
approaches. In these experiments, the damping factor,
(1 ? ? ? ?) in (8), is empirically set to 0.1. Row (a)
is the baseline, which use LTE-based statistical measure
to compute the importance of utterances I(U, d). Row
(b) is the result only considering lexical similarity; row
(c) only uses topical similarity. Row (d) are the re-
sults additionally including speaker information such as
TopicSim?(Ui, Uj). Row (e) is the result performed by
integrated random walk (with ? 6= 0 and ? 6= 0) using
parameters that have been optimized on the dev set.
3.3.1 Graph-Based Approach
We can see the performance after graph-based re-
computation, shown in rows (b) and (c), is significantly
better than the baseline, shown in row (a), for both ASR
and manual transcripts. For ASR transcripts, topical sim-
ilarity and lexical similarity give similar results. For man-
ual transcripts, topical similarity performs slightly worse
than lexical similarity, because manual transcripts don?t
contain the recognition errors, and therefore word overlap
can accurately measure the similarity between two utter-
379
F-measure
ASR Transcripts Manual Transcripts
ROUGE-1 ROUGE-L ROUGE-1 ROUGE-L
(a) Baseline: LTE 46.816 46.256 44.987 44.162
(b) LexSim (? = 0, ? = 0.9) 48.940 48.504 46.540 45.858
(c) TopicSim (? = 0.9, ? = 0) 49.058 48.436 46.199 45.392
(d) Intra-Speaker TopicSim 49.212 48.351 47.104 46.299
(e) Integrated Random Walk 49.792 49.156 46.714 46.064
MAX RI +6.357 +6.269 +4.706 +4.839
Table 1: Maximum relative improvement (RI) with respect to the baseline for all proposed approaches (%).
48
48.5
49
49.5
50
0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
ROUGE-1
ROUGE-L
? ? 
F-measure 
Figure 2: The performance from integrated random walk with
different combination weights, ? and ? (? + ? = 0.9 in all
cases) for ASR transcripts.
ances. However, for ASR transcripts, although topical
similarity is not as accurate as lexical similarity, it can
compensate for recognition errors, so that the approaches
have similar performance. Thus, graph-based approaches
can significantly improve the baseline results.
3.3.2 Effectiveness of Intra-Speaker Modeling
We find that modeling intra-speaker topics can improve
the performance (row (c) and row (d)), which means
speaker information is useful to model the topical simi-
larity. The experiment shows intra-speaker modeling can
help us include the important utterances for both ASR
and manual transcripts.
3.3.3 Integration of Topical and Lexical Similarity
Row (e) shows the result of the proposed approach,
which integrates topical and lexical similarity into a sin-
gle graph, considering two types of relations together.
For ASR transcripts, row (e) is better than row (b) and
row (d), which means topical similarity and lexical sim-
ilarity can model different types of relations, because of
recognition errors. Figure 2 shows the sensitivity of the
combination weights for integrated random walk. We can
see topical similarity and lexical similarity are additive,
i.e. they can compensate each other, improving the per-
formance by integrating two types of edges in a single
graph. Note that the exact values of ? and ? do not mat-
ter so much for the performance.
For manual transcripts, row (e) cannot perform better
by combing two types of similarity, which means topical
similarity can dominate lexical similarity, since without
recognition errors topical similarity can model the rela-
tions accurately and additionally modeling intra-speaker
topics can effectively improve the performance.
In addition, Banerjee and Rudnicky (2008) used su-
pervised learning to detect noteworthy utterances on the
same corpus, and achieved ROGURE-1 scores of around
43% for ASR, and 47% for manual transcriptions. Our
unsupervised approach performs better, especially for
ASR transcripts.
Note that the performance on ASR is better than on
manual transcripts. Because a higher percentage of
recognition errors occurs on ?unimportant? words, these
words tend to receive lower scores; we can then exclude
the utterances with more errors, and achieve better sum-
marization results. Other recent work has also demon-
strated better performance for ASR than manual tran-
scripts (Chen et al, 2011; Kong and Lee, 2011).
4 Conclusion and Future Work
Extensive experiments and evaluation with ROUGE met-
rics showed that intra-speaker topics can be modeled
in topical similarity and that integrated random walk
can combine the advantages from two types of edges
for imperfect ASR transcripts, where we achieved more
than 6% relative improvement. We plan to model inter-
speaker topics in the graph-based approach in the future.
Acknowledgements
The first author was supported by the Institute of Edu-
cation Science, U.S. Department of Education, through
Grants R305A080628 to Carnegie Mellon University.
Any opinions, findings, and conclusions or recommen-
dations expressed in this publication are those of the au-
thors and do not necessarily reflect the views or official
policies, either expressed or implied of the Institute or
the U.S. Department of Education.
380
References
Banerjee, S. and Rudnicky, A. I. 2008. An extractive-
summarizaion baseline for the automatic detection of note-
worthy utterances in multi-party human-human dialog. Proc.
of SLT.
Chen, Y.-N., Huang, Y., Yeh, C.-F., and Lee, L.-S. 2011. Spo-
ken lecture summarization by random walk over a graph con-
structed with automatically extracted key terms. Proc. of In-
terSpeech.
Erkan, G. and D. R. Radev., D. R. 2004. LexRank: Graph-
based lexical centrality as salience in text summarization.
Journal of Artificial Intelligence Research, Vol. 22.
Furui, S., Kikuchi, T., Shinnaka, Y., and Hori, C. 2004.
Speech-to-text and speech-to-speech summarization of spon-
taneous speech. IEEE Trans. on Speech and Audio Process-
ing.
Garg, N., Favre, B., Reidhammer, K., and Hakkani-Tu?r 2009.
ClusterRank: A graph based method for meeting summariza-
tion. Proc. of InterSpeech.
Gillick, D. J. 2011. The elements of automatic summarization.
PhD thesis, EECS, UC Berkeley.
Glass J., Hazen, T. J., Cyphers, S., Malioutov, I., Huynh, D.,
and Barzilay, R. 2007. Recent progress in the MIT spoken
lecture processing project. Proc. of InterSpeech.
Hofmann, T. 1999. Probabilistic latent semantic indexing.
Proc. of SIGIR.
Hsu, W. and Kennedy, L. 2007. Video search reranking through
random walk over document-level context graph. Proc. of
MM.
Kong, S.-Y. and Lee, L.-S. 2011. Semantic analysis and orga-
nization of spoken documents based on parameters derived
from latent topics. IEEE Trans. on Audio, Speech and Lan-
guage Processing, 19(7): 1875-1889.
Langville, A. and Meyer, C. 2005. A survey of eigenvector
methods for web information retrieval. SIAM Review.
Lee, L.-S. and Chen, B. 2005. Spoken document understanding
and organization. IEEE Signal Processing Magazine.
Lin, C. 2004. Rouge: A package for automatic evaluation
of summaries. Proc. of Workshop on Text Summarization
Branches Out.
Liu, F. and Liu, Y. 2010. Using spoken utterance compression
for meeting summarization: A pilot study. Proc. of SLT.
Liu Y., Xie, S., and Liu, F. 2010. Using N-best recognition
output for extractive summarization and keyword extraction
in meeting speech. Proc. of ICASSP.
Page, L., Brin, S., Motwani, R., Winograd, T. 1998. The pager-
ank citation ranking: bringing order to the web. Technical
Report, Stanford Digital Library Technologies Project.
381
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 382?385,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Towards Using EEG to Improve ASR Accuracy
Yun-Nung Chen, Kai-Min Chang, and Jack Mostow
Project LISTEN (http://www.cs.cmu.edu/?listen)
School of Computer Science, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213-3891, USA
{yvchen,kkchang,mostow}@cs.cmu.edu
Abstract
We report on a pilot experiment to improve the per-
formance of an automatic speech recognizer (ASR)
by using a single-channel EEG signal to classify the
speaker?s mental state as reading easy or hard text.
We use a previously published method (Mostow et
al., 2011) to train the EEG classifier. We use its prob-
abilistic output to control weighted interpolation of
separate language models for easy and difficult read-
ing. The EEG-adapted ASR achieves higher accu-
racy than two baselines. We analyze how its perfor-
mance depends on EEG classification accuracy. This
pilot result is a step towards improving ASR more
generally by using EEG to distinguish mental states.
1 Introduction
Humans use speech to communicate what?s on their
mind. However, until now, automatic speech recogniz-
ers (ASR) and dialogue systems have had no direct way
to take into account what is going on in a speaker?s
mind. Some work has attempted to infer cognitive
states from volume and speaking rate to adapt language
modeling (Ward and Vega, 2009) or from query click
logs (Hakkani-Tu?r et al, 2011) to detect domains. A
new way to address this limitation is to infer mental states
from electroencephalogram (EEG) signals.
EEG is a voltage signal that can be measured on the
surface of the scalp, arising from large areas of coordi-
nated neural activity. This neural activity varies as a func-
tion of development, mental state, and cognitive activity,
and EEG can measurably detect such variation.
Recently, a few companies have scaled back medical
grade EEG technology to create portable EEG headsets
that are commercially available and simple to use. The
NeuroSky MindSetTM (2009), for example, is an audio
headset equipped with a single-channel EEG sensor. It
measures the voltage between an electrode that rests on
the forehead and electrodes in contact with the ear. Un-
like the multi-channel electrode nets worn in labs, the
sensor requires no gel or saline for recording, and re-
quires no expertise to wear. Even with the limitations
of recording from only a single sensor and working with
untrained users, Furthermore, Mostow et al(2011) used
its output signal to distinguish easy from difficult reading,
achieving above-chance accuracy. Here we build on that
work by using the output of such classifiers to adapt lan-
guage models for ASR and thereby improve recognition
accuracy.
The most similar work is Jou and Schultz?s (2008) use
of electromyographic (EMG) signals generated by human
articulatory muscles in producing speech. They showed
that augmenting acoustic features with these EMG fea-
tures can achieve rudimentary silent speech detection.
Pasley et al (2012) used electrocorticographic (ECoG)
recordings from nonprimary auditory cortex in the human
superior temporal gyrus to reconstruct acoustic informa-
tion in speech sounds. Our work differs from these efforts
in that we use a consumer-grade single-channel EEG sen-
sor measuring frontal lobe activities, and that we use the
detected mental state just to help improve ASR perfor-
mance rather than to dictate or reconstruct speech, which
are much harder tasks.
Section 2 describes how to use machine learning to dis-
tinguish mental states associated with easy and difficult
readings. Section 3 describes how we use EEG classifier
output to adapt ASR language models. Section 4 uses an
oracle simulation to show how increasing EEG classifier
accuracy will affect ASR accuracy. Section 5 concludes.
2 Mental State Classification Using EEG
We use training and testing data from Mostow et al?s
(2011) experiment, which presented text passages, one
sentence at a time, to 10 adults and 11 nine- to ten-year-
olds wearing a Neurosky MindsetTM (2009). They read
three easy and three difficult texts aloud, in alternating
382
order. The ?easy? passages were from texts classified by
the Common Core Standards1 at the K-1 level. The ?diffi-
cult? passages were from practice materials for the Grad-
uate Record Exam2 and the ACE GED test3. Across the
reading conditions, passages ranged from 62 to 83 words
long. Although instructed to read the text aloud, the read-
ers (especially children) did not always read correctly or
follow the displayed sentences.
Following Mostow et al (2011), we trained binary lo-
gistic regression classifiers to estimate the probability that
an EEG signal is associated with reading an easy (or diffi-
cult) sentence. As features for logistic regression we used
the streams of values logged by the MindSet:
1. The raw EEG signal, sampled at 512 Hz
2. A filtered version of the raw signal, also sampled at
512 Hz, which is raw signal smoothed over a win-
dow of 2 seconds
3. Proprietary ?attention? and ?meditation? measures,
reported at 1 Hz
4. A power spectrum of 1Hz bands from 1-256 Hz, re-
ported at 8 Hz
5. An indicator of signal quality, reported at 1 Hz
Head movement or system instability led to missing or
poor-quality EEG data for some utterances, which we ex-
cluded in order to focus on utterances with clear acous-
tic and EEG signals. The features for each utterance
consisted of measures 1-4, averaged over the utterance,
excluding the 15% of observations where measure 5 re-
ported poor signals. After filtering, the data includes 269
utterances from adults and 243 utterances from children,
where 327 utterances are for the easy passages and 185
utterances are for the difficult passages. To balance the
classes, we used the undersampling method for training.
We trained a reader-specific classifier on each reader?s
data from all but one text passage, tested it on each
sentence in the held-out passage, performed this proce-
dure for each passage, and averaged the results to cross-
validate accuracy within readers. We computed classifi-
cation accuracy as the percentage of utterances classified
correctly. Classification accuracy for adults?, children?s,
and total oral reading was 71.49%, 58.74%, and 65.45%
respectively. A one-tailed t-test, with classification accu-
racy on an utterance as the random variable, showed that
EEG classification was significantly better than chance.
3 Language Model Adaptation for ASR
Traditional ASR decodes a word sequence W ? from the
acoustic model and language model as below:
1http://www.corestandards.org
2http://majortests.com/gre/reading comprehension.php
3http://college.cengage.com:80/devenglish/resources/reading
ace/students
W ? = argmaxW P (W | A) (1)
= argmaxW
P (A | W ) ? P (W )
P (A)
To incorporate EEG, we include mental state N as an ad-
ditional observation in the decoding procedure:
W ? = argmaxW P (W | A,N) (2)
= argmaxW
P (A | W ) ? P (W | N)
P (A)
The six passages use a vocabulary of 430 distinct
words. To evaluate the impact on ASR accuracy of us-
ing EEG to adapt language models, we needed acoustic
models appropriate for the speakers. For adult speech, we
used the US English HUB4 Acoustic Model from CMU
Sphinx. For children?s speech, we used Project LISTEN?s
acoustic models trained on children?s oral reading.
We used separate trigram language models (with bi-
gram and unigram backoff) for easy and difficult text ?
EasyLM, trained on the three easy passages, and Diffi-
cultLM, trained on the three difficult passages. Both lan-
guage models used the same lexicon, consisting of the
430 words in all six target passages. All experiments used
the same ASR parameter values.
As a gold standard, all utterances were manually tran-
scribed by a native English speaker. To measure ASR per-
formance, we computed Word Accuracy (WACC) as the
number of words recognized correctly minus insertions
divided by number of words in the reference transcripts
for each reader, and averaged them.
Then we can adapt the language model to estimate
P (W | N) using mental state information. Using the
EEG classifier described in Section 2, we adapted the lan-
guage model separately for each utterance, using three
types of language model adaptation: hard selection, soft
selection, and combination with ASR output.
3.1 Hard Selection of Language Models
Given the probabilistic estimate that a given utterance
was easy or difficult (SEasy(N) and SDifficult(N)), hard se-
lection simply picks EasyLM if the utterance was likelier
to be easy, or DifficultLM otherwise:
PHard(W | N) = IC(N) ? PEasy(W ) (3)
+ (1 ? IC(N)) ? PDiff(W ).
Here IC(N) = 1 if SEasy(N) > SDifficult(N), and
PEasy(W ) and PDiff(W ) are the probability of word W in
EasyLM and DifficultLM, respectively. For comparison,
the Random Pick baseline randomly picks either EasyLM
or DifficultLM:
383
WACC
Adult Child
Easy Difficult All Easy Difficult All
(a) Baseline 1: Random Pick 54.5 51.2 53.8 32.8 14.7 30.6
(b) EEG-based: Hard Selection 57.6 49.4 52.7 36.4 17.0 32.8
(c) Baseline 2: Equal Weight 63.2 59.9 56.5 37.3 19.5 33.4
(d) EEG-based: Soft Selection w/o smoothing 57.2 48.8 52.4 35.8 17.2 32.5
(e) EEG-based: Soft Selection w/ smoothing 66.0 62.3 64.2 39.8 22.7 36.2
(f) Baseline 3: Weight from ASR (? = 0) 63.8 60.6 61.5 39.2 20.0 35.0
(g) Weight from ASR and EEG (? = 0.5) 64.5 63.4 63.5 39.2 21.9 36.0
Table 1: ASR performance of proposed approaches using EEG-based classification of mental states.
PRandom(W ) = IR ? PEasy(W ) (4)
+ (1 ? IRandom) ? PDiff(W ).
Here IR is randomly set to 0 or 1.
3.2 Soft Selection of Language Models
Mental state classification based on EEG is imperfect,
and using only the corresponding language model (Ea-
syLM or DifficultLM) to decode the target utterance is li-
able to perform worse when the classifier is wrong. Thus,
we use the classifier?s probabilistic estimate that the ut-
terance is easy (or difficult) as interpolation weights to
linearly combine EasyLM and DifficultLM:
PSoft(W | N) = wEasy(N) ? PEasy(W ) (5)
+ wDiff(N) ? PDiff(W ).
Here wEasy(N) and wDiff(N) are from classifier?s output.
wEasy(N) = SEasy(N), wDiff(N) = SDiff(N) (6)
Additionally, we can adjust the range of weights by
smoothing the probability outputted by the EEG classi-
fier:
wEasy(N) =
? + SEasy(N)
2? + 1
, (7)
wDiff(N) =
? + SDiff(N)
2? + 1
Here SEasy(N) (or SDiff(N)) is the classifier?s probabilis-
tic estimate that the sentence is easy (or difficult) and
? is the smoothing weight, which we set to 0.5. Af-
ter smoothing the probabilities, wEasy(N) and wDiff(N)
each lie within the interval [0.25, 0.75], and wEasy(N) +
wDiff(N) = 1. That is, Soft Selection with smoothing in-
terpolates the two language models, but assigns a weight
of at least 0.25 to each one to reduce the impact of EEG
classifier errors. Notice that ? = 0 is equivalent to EEG
Soft Selection without smoothing.
For comparison, the Equal Weight baseline interpo-
lates EasyLM and DifficultLM with equal weights:
PEqual(W ) = 0.5 ? PEasy(W ) + 0.5 ? PDiff(W ) (8)
3.3 Combination with ASR Output
Given the ASR results from the Equal Weight baseline,
we can derive S?Easy(N) as:
S?Easy(N) = ? ? SEasy(N) (9)
+ (1 ? ?) ?
PEasy(W0)
PEasy(W0) + PDiff(W0)
Here we can estimate S?Easy(N) based on the classifier?s
output and the probability of the recognized words W0 in
EasyLM. We can derive S?Diff(N) in the same way. Then
we can use (5) and (7) to re-decode the utterances by us-
ing S?Easy(N) and S
?
Diff(N). Here ? is a linear interpola-
tion weight, where we set to 0.5 to give equal weights to
ASR output and EEG. For comparison, the ASR baseline
uses weights from only the ASR results, where ? = 0.
Notice that the case of ? = 1 is equivalent to EEG Soft
Selection with smoothing.
3.4 Results of Proposed Approaches
Table 1 shows the performance of our proposed ap-
proaches and the corresponding baselines as measured by
WACC. According to one-tailed t-tests with word accu-
racy of an utterance as the random variable, the results
in boldface are significantly better tgan their respective
baselines (p ? 0.05).
Hard Selection (row b) outperforms the Random Pick
baseline (row a). Soft Selection without smoothing (row
d) has similar performance as Hard Selection because the
classifier often outputs probability estimates that are ei-
ther 1 or 0. However, Soft Selection with smoothing (row
e) outperforms the Equal Weight baseline (row c). The
Weight from ASR baseline (row f) is better than the other
baselines. Weight from ASR and EEG (row g) can fur-
ther improve performance, but it?s not better than Soft
Selection with smoothing (row e) - evidence that EEG
gives good estimation for choosing language models. In
short, Table 1 shows that using EEG to choose between
EasyLM and DifficultLM achieves higher ASR accuracy
than the baselines that do not use EEG.
Comparing the first two baselines, the Equal Weight
baseline (row c) outperforms the Random Pick baseline
384
0 
10 
20 
30 
40 
50 
60 
70 
0 10 20 30 40 50 60 70 80 90 100 
Easy utt. 
Difficult utt. 
All utt. 
Predicted WACC (%) 
Simulated Accuracy of Classification (%) 
(a) Adult 
0 
10 
20 
30 
40 
50 
60 
70 
0 10 20 30 40 50 60 70 80 90 100 
Easy utt. 
Difficult utt. 
All utt. 
Predicted WACC (%) 
Simulated Accuracy of Classification (%) 
(b) Child  
Figure 1: The simulated accuracy graphs plot the predicted ASR word accuracy against the level of EEG classification accuracy
simulated by an oracle.
(row a) in every column, because the loss in ASR accu-
racy from picking the wrong language model outweighs
the improvement from picking the right one. Similarly,
EEG-based Soft Selection with smoothing (row e) out-
performs EEG-based Hard Selection (row b) in every col-
umn because the interpolated language model is more
robust to EEG classification error. The third base-line,
Weight from ASR (row f) depends solely on ASR results
to estimate weights; it performs better than other base-
lines, but not as well as EEG-based Soft Selection with
smoothing (row e). That is, using EEG alone can weight
the two language models better than ASR alone.
4 Oracle Simulation
To explore the relationship between EEG classifier ac-
curacy and the effect of EEG-based adaptation on ASR
accuracy, we simulate different classification accuracies
and used Hard Selection to predict the resulting ASR ac-
curacy by selecting between the ASR output from Ea-
syLM and DifficultLM according to the simulated clas-
sifier accuracy. We use the resulting Word Accuracy to
predict ASR performance at that level of EEG classifier
accuracy.
Figure 1 plots predicted ASR WACC against simulated
EEG classification accuracy. As expected, the predicted
ASR accuracy increases as EEG classification accuracy
increases, for both groups (adults and children) and both
levels of difficulty (easy and difficult). However, Figure
1a and 1b shows that WACC was much lower for children
than for adults, especially on difficult utterances, where
even 100% simulated EEG classifier accuracy achieves
barely 20% WACC. One explanation is that on difficult
sentences, children produced reading mistakes and/ or
off-task speech. In contrast, adults read better and stayed
on task. Not only is predicted ASR accuracy higher on
adults? reading, it improves substantially as simulated
EEG classifier accuracy increases.
5 Conclusion
This paper shows that classifying EEG signals from an in-
expensive single-channel device can help adapt language
models to significantly improve ASR performance. An
interpolated language model smoothed to compensate for
classification errors yielded the best performance. ASR
performance depended on the accuracy of mental state
classification. Future work includes improving EEG clas-
sification accuracy, detecting other relevant mental states,
such as emotion, and improving ASR by using word-level
EEG classification. A neurologically-informed ASR may
better capture what people intend to communicate, and
augment acoustic input with non-verbal cues to ASR or
dialogue systems.
Acknowledgements
This work was supported by the Institute of Education
Sciences, U.S. Department of Education, through Grant
R305A080628 to Carnegie Mellon University. Any opin-
ions, findings, and conclusions or recommendations ex-
pressed in this publication are those of the authors and do
not necessarily reflect the views or official policies, either
expressed or implied of the Institute or the U.S. Depart-
ment of Education. We thank the students, educators, and
LISTENers who helped create our data, and the reviewers
for their helpful comments.
References
Hakkani-Tn?r, D., Tur, G., Heck, L., and Shriberg, E. 2011.
Bootstrapping domain detection using query click logs for
new domains Proceedings of InterSpeech, 709-712.
Jou, S.-C. S. and Schultz, T.. 2008. Ears: Electromyograpical
Automatic Recognition of Speech. Proceedings of Biosig-
nals, 3-12.
Mostow, J., Chang, K.-M., and Nelson, J. 2011. Toward Ex-
ploiting EEG Input in a Reading Tutor. Proceedings of the
15th International Conference on Artificial Intelligence in
Education, 230-237.
NeuroSky 2009. NeuroSky?s SenseTM Meters and Detection of
Mental State: Neurisky, Inc.
Pasley, B. N. and et al 2012. Reconstructing speech from au-
ditory cortex. PLos Biology, 10(1), 1-13.
Ward, N. G. and Vega, A. 2009. Towards the use of cognitive
states in language modeling. Proceedings of ASRU, 323-326.
385
Proceedings of the 8th International Natural Language Generation Conference, pages 99?102,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Two-Stage Stochastic Email Synthesizer
Yun-Nung Chen and Alexander I. Rudnicky
School of Computer Science, Carnegie Mellon University
5000 Forbes Ave., Pittsburgh, PA 15213-3891, USA
{yvchen, air}@cs.cmu.edu
Abstract
This paper presents the design and im-
plementation details of an email synthe-
sizer using two-stage stochastic natural
language generation, where the first stage
structures the emails according to sender
style and topic structure, and the second
stage synthesizes text content based on the
particulars of an email structure element
and the goals of a given communication
for surface realization. The synthesized
emails reflect sender style and the intent of
communication, which can be further used
as synthetic evidence for developing other
applications.
1 Introduction
This paper focuses on synthesizing emails that re-
flect sender style and the intent of the communica-
tion. Such a process might be used for the gener-
ation of common messages (for example a request
for a meeting without direct intervention from the
sender). It can also be used in situations where nat-
uralistic emails are needed for other applications.
For instance, our email synthesizer was developed
to provide emails to be used as part of synthetic
evidence of insider threats for purposes of train-
ing, prototyping, and evaluating anomaly detec-
tors (Hershkop et al., 2011).
Oh and Rudnicky (2002) showed that stochas-
tic generation benefits from two factors: 1) it
takes advantage of the practical language of a do-
main expert instead of the developer and 2) it re-
states the problem in terms of classification and
labeling, where expertise is not required for de-
veloping a rule-based generation system. In the
present work we investigate the use of stochastic
techniques for generation of a different class of
communications and whether global structures can
be convincingly created. Specifically we inves-
tigate whether stochastic techniques can be used
to acceptably model longer texts and individual
sender characteristics in the email domain, both of
which may require higher cohesion to be accept-
able (Chen and Rudnicky, 2014).
Our proposed system involves two-stage
stochastic generation, shown in Figure 1, in which
the first stage models email structures according
to sender style and topic structure (high-level
generation), and the second stage synthesizes
text content based on the particulars of a given
communication (surface-level generation).
2 The Proposed System
The whole architecture of the proposed system is
shown in left part of Figure 1, which is composed
of preprocessing, first-stage generation for email
organization, and second-stage generation for sur-
face realization.
In preprocessing, we perform sentence segmen-
tation for each email, and then manually anno-
tate each sentence with a structure element, which
is used to create a structural label sequence for
each email and then to model sender style and
topic structure for email organization (1st stage in
the figure). The defined structural labels include
greeting, inform, request, suggestion, question,
answer, regard, acknowledgement, sorry, and sig-
nature. We also annotate content slots, including
general classes automatically created by named
entity recognition (NER) (Finkel et al., 2005) and
hand-crafted topic classes, to model text content
for surface realization (2nd stage in the figure).
The content slots include person, organization, lo-
cation, time, money, percent, and date (general
classes), and meeting, issue, and discussion (topic
classes).
2.1 Modeling Sender Style and Topic
Structure for Email Organization
In the first stage, given the sender and the fo-
cused topic from the input, we generate the email
structures by predicted sender-topic-specific mix-
ture models, where the detailed is illustrated as be-
99
Predicting 
Mixture 
Models 
Email Archive 
Building 
Structure 
LM 
Structural Label Annotation 
Structural Label Sequences 
Generating 
Email 
Structures 
Generated Structural 
Label Sequences 
<greeting> 
<inform> 
? 
Slot-Value Pairs 
Slot Annotation 
Emails w/ Slots 
Building 
Content 
LM 
Generating 
Text 
Content 
Scoring 
Email 
Candidates 
Filling 
Slots 
Synthesized Emails 
Hi Peter 
Today?s ... 
1st-Stage Generation 
2nd-Stage Generation 
Sender 
Model 
Topic 
Model 
Sender Topic 
System Input Preprocessing 
Synthesized Emails 
with Slots 
Hi [person] 
Today?s ... 
Request is generated at 
the narrative level. 
Form filling: 
? Topic Model 
? Sender Model 
? Slot fillers 
Figure 1: The system architecture (left) and the demo synthesizer (right).
low.
2.1.1 Building Structure Language Models
Based on the annotation of structural labels, each
email can be transformed into a structural label
sequence. Then we train a sender-specific struc-
ture model using the emails from each sender and
a topic-specific model using the emails related to
each topic. Here the structure models are tri-
gram models with Good-Turing smoothing (Good,
1953).
2.1.2 Predicting Mixture Models
With sender-specific and topic-specific structure
models, we predict the sender-topic-specific mix-
ture models by interpolating the probabilities of
two models.
2.1.3 Generating Email Structures
We generate structural label sequences randomly
according to the distribution from sender-topic-
specific models. Smoothed trigram models may
generate any unseen trigrams based on back-off
methods, resulting in more randomness. In ad-
dition, we exclude unreasonable emails that don?t
follow two simple rules.
1. The structural label ?greeting? only occurs at
the beginning of the email.
2. The structural label ?signature? only occurs
at the end of the email.
2.2 Surface Realization
In the second stage, our surface realizer consists
of four aspects: building content language models,
generating text content, scoring email candidates,
and filling slots.
2.2.1 Building Content Language Models
After replacing the tokens with the slots, for each
structural label, we train an unsmoothed 5-gram
language model using all sentences belonging to
the structural label. Here we assume that the usage
of within-sentence language is independent across
senders and topics, so generating the text content
only considers the structural labels. Unsmoothed
5-gram language models introduce some variabil-
ity in the output sentences while preventing non-
sense sentences.
2.2.2 Generating Text Content
The input to surface realization is the generated
structural label sequences. We use the correspond-
ing content language model for the given struc-
tural label to generate word sequences randomly
according to distribution from the language model.
Using unsmoothed 5-grams will not generate
any unseen 5-grams (or smaller n-grams at the
beginning and end of a sentence), avoiding gen-
eration of nonsense sentences within the 5-word
window. With a structural label sequence, we can
generate multiple sentences to form a synthesized
email.
100
2.3 Scoring Email Candidates
The input to the system contains the required in-
formation that should be included in the synthe-
sized result. For each synthesized email, we penal-
ize it if the email 1) contains slots for which there
is no provided valid value, or 2) does not have
the required slots. The content generation engine
stochastically generates a candidate email, scores
it, and outputs it when the synthesized email with
a zero penalty score.
2.4 Filling Slots
The last step is to fill slots with the appropriate
values. For example, the sentence ?Tomorrow?s
[meeting] is at [location].? becomes ?Tomorrow?s
speech seminar is at Gates building.? The right
part of Figure 1 shows the process of the demo sys-
tem,where based on a specific topic, a sender, and
an interpolation weight, the system synthesizes an
email with structural labels first and then fills slots
with given slot fillers.
3 Experiments
We conduct a preliminary experiment to evaluate
the proposed system. The corpus used for our ex-
periments is the Enron Email Dataset1, which con-
tains a total of about 0.5M messages. We selected
the data related to daily business for our use. This
includes data from about 150 users, and we ran-
domly picked 3 senders, ones who wrote many
emails, and define additional 3 topic classes (meet-
ing, discussion, issue) as topic-specific entities
for the task. Each sender-specific model (across
topics) or topic-specific model (across senders) is
trained on 30 emails.
3.1 Evaluation of Sender Style Modeling
To evaluate the performance of sender style, 7 sub-
jects were given 5 real emails from each sender
and then 9 synthesized emails. They were asked
to rate each synthesized email for each sender on
a scale between 1 to 5.
With higher weight for sender-specific model
when predicting mixture models, average normal-
ized scores the corresponding senders receives ac-
count for 45%, which is above chance (33%). This
suggests that sender style can be noticed by sub-
jects. In a follow-up questionnaire, subjects indi-
cated that their ratings were based on greeting us-
age, politeness, the length of email and other char-
acteristics.
1https://www.cs.cmu.edu/?enron/
3.2 Evaluation of Surface Realization
We conduct a comparative evaluation of two
different generation algorithms, template-based
generation and stochastic generation, on the
same email structures. Given a structural label,
template-based generation consisted of randomly
selecting an intact whole sentence with the target
structural label. This could be termed sentence-
level NLG, while stochastic generation is word-
level NLG.
We presented 30 pairs of (sentence-, word-)
synthesized emails, and 7 subjects were asked to
compare the overall coherence of an email, its
sentence fluency and naturalness; then select their
preference. The experiments showed that word-
based stochastic generation outperforms or per-
forms as well as the template-based algorithm
for all criteria (coherence, fluency, naturalness,
and preference). Some subjects noted that nei-
ther email seemed human-written, perhaps an ar-
tifact of our experimental design. Nevertheless,
we believe that this stochastic approach would re-
quire less effort compared to most rule-based or
template-based systems in terms of knowledge en-
gineering.
In the future, we plan to develop an automatic
email structural label annotator in order to build
better language models (structure language mod-
els and content language models) by increasing
training data, and then improve the naturalness of
synthesized emails.
4 Conclusion
This paper illustrates a design and implementation
of an email synthesizer with two-stage stochastic
NLG: first a structure is generated, and then text is
generated for each structure element. Here sender
style and topic structure can be modeled. We be-
lieve that this system can be applied to create re-
alistic emails and could be carried out using mix-
tures containing additional models based on other
characteristics. The proposed system shows that
emails can be synthesized using a small corpus of
labeled data, and the performance seems accept-
able; however these models could be used to boot-
strap the labeling of a larger corpus which in turn
could be used to create more robust models.
Acknowledgments
The authors wish to thank Brian Lindauer and Kurt
Wallnau from the Software Engineering Institute
of Carnegie Mellon University for their guidance,
advice, and help.
101
References
Yun-Nung Chen and Alexander I. Rudnicky. 2014.
Two-stage stochastic natural language generation for
email synthesis by modeling sender style and topic
structure. In Proceedings of the 8th International
Natural Language Generation Conference.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370. Association for Computational Lin-
guistics.
Irving J Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3-4):237?264.
Shlomo Hershkop, Salvatore J Stolfo, Angelos D
Keromytis, and Hugh Thompson. 2011. Anomaly
detection at multiple scales (ADAMS).
Alice H Oh and Alexander I Rudnicky. 2002.
Stochastic natural language generation for spoken
dialog systems. Computer Speech & Language,
16(3):387?407.
102
Proceedings of the 8th International Natural Language Generation Conference, pages 152?156,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Two-Stage Stochastic Natural Language Generation for Email Synthesisby Modeling Sender Style and Topic Structure
Yun-Nung Chen and Alexander I. Rudnicky
School of Computer Science, Carnegie Mellon University
5000 Forbes Ave., Pittsburgh, PA 15213-3891, USA
{yvchen, air}@cs.cmu.edu
Abstract
This paper describes a two-stage pro-
cess for stochastic generation of email, in
which the first stage structures the emails
according to sender style and topic struc-
ture (high-level generation), and the sec-
ond stage synthesizes text content based
on the particulars of an email element
and the goals of a given communication
(surface-level realization). Synthesized
emails were rated in a preliminary experi-
ment. The results indicate that sender style
can be detected. In addition we found
that stochastic generation performs better
if applied at the word level than at an
original-sentence level (?template-based?)
in terms of email coherence, sentence flu-
ency, naturalness, and preference.
1 Introduction
This paper focuses on generating language for the
email domain, with the goal of producing mails
that reflect sender style and the intent of the com-
munication. Such a process might be used for the
generation of common messages (for example a
request for a meeting without direct intervention
from the sender). It can also be used in situations
where naturalistic email is needed for other ap-
plications. For instance, our email generator was
developed to provide emails to be used as part of
synthetic evidence of insider threats for purposes
of training, prototyping, and evaluating anomaly
detectors (Hershkop et al., 2011).
There are two approaches to natural language
generation (NLG), one focuses on generating text
using templates or rules (linguistic) methods, the
another uses corpus-based statistical techniques.
Oh and Rudnicky (2002) showed that stochastic
generation benefits from two factors: 1) it takes
advantage of the practical language of a domain
expert instead of the developer and 2) it restates
the problem in terms of classification and label-
ing, where expertise is not required for developing
a rule-based generation system. They found that
naive listeners found such utterances as accept-
able as human-generated utterances. Belz (2005)
also proposed a probabilistic NLG approach to
make systems more robust and components more
reusable, reducing manual corpus analysis.
However, most work usually focused on well-
structured documents such as news and Wikipedia,
while email messages differ from them, which
reflect senders? style and are more spontaneous.
Lampert et al. (2009) segmented email messages
into zones, including sender zones, quoted con-
versation zones, and boilerplate zones. This paper
only models the text in the sender zone, new con-
tent from the current sender. In the present work,
we investigate the use of stochastic techniques for
generation of a different class of communications
and whether global structures can be convincingly
created in the email domain.
A lot of NLG systems are applied in dialogue
systems, some of which focus on topic model-
ing (Sauper and Barzilay, 2009; Barzilay and Lap-
ata, 2008; Barzilay and Lee, 2004), proposing al-
gorithms to balance local fit of information and
global coherence. However, they seldom con-
sider to model the speaker?s characteristics. Gill
et al. (2012) considered sentiment such as open-
ness and neuroticism to specify characters for di-
alogue generation. In stead of modeling authors?
attitudes, this paper proposes the first approach of
synthesizing emails by modeling their writing pat-
terns. Specifically we investigate whether stochas-
tic techniques can be used to acceptably model
longer texts and individual speaker characteristics
in the emails, both of which may require higher
cohesion to be acceptable.
2 Overview of Framework
Our proposed NLG approach has three steps: pre-
processing training data, modeling sender style
and topic structure for email organization, fol-
lowed by surface realization, shown in Figure 1.
In preprocessing, we segment sentences for
each email, and label email structural elements.
This is used to create a structural label sequence
for each email, and then used to model sender
style and topic structure for email organization
(1st stage in the figure). Content slots are also
annotated for surface realization (2nd stage in the
figure). Details are in Section 3.
From the annotated corpus, we build sender-
specific and topic-specific structure language
models based on structural label sequences, and
use a mixture sender-topic-specific model to
stochastically generate email structure in the first
stage. The process is detailed in Section 4.
152
Predicting 
Mixture 
Models 
Email 
Document 
Archive 
Building 
Structure 
LM Structural Label 
Annotation Structural 
Label 
Sequences 
Generating 
Email 
Structures 
Generated Structural 
Label Sequences 
<greeting> 
<inform> 
? 
Slot-Value Pairs 
Slot 
Annotation Emails w/ 
Slots 
Building 
Content 
LM 
Generating 
Text Content 
Scoring 
Email 
Candidates 
Email Candidates 
Filling 
Slots 
Synthesized Emails 
Hi Peter 
Today?s ... 
1st Stage: Modeling Sender Style and Topic Structure for Email Organization 
2nd Stage: Surface Realization 
Hi [Person] 
Today?s ... 
Sender-Specific 
Model 
Topic-Specific 
Model 
Sender Topic 
Input to NLG 
Training Data Preprocessing 
Figure 1: The proposed framework of two-stage NLG component.
In the second stage, we build a content lan-
guage model for each structural element and then
stochastically generate sentences using the se-
quence generated in the first stage. To ensure that
required slot-value pairs occur in the text, candi-
dates emails are filtered to retain only those texts
that contain the desired content slots. These slots
are then filled to produce the final result. Section 5
explains the process.
3 Training Data Preprocessing
To model sender style and topic structure, we an-
notate the data with defined structural labels in
Section 3.1, and data with slots to model text con-
tent of language in Section 3.2.
3.1 Structural Label Annotation
Based on examination of the corpus, we defined
10 email structure elements:
1. greeting: a friendly expression or respectful
phrase, typically at the start of an email.
2. inform: to give or impart knowledge of a fact
or circumstance.
3. request: the act of asking for something to be
given or done, especially as a favor or cour-
tesy.
4. suggestion: to mention or introduce (an idea,
proposition, plan, etc.) for consideration or
possible action.
5. question: an interrogative sentence in an
form, requesting information in reply.
6. answer: a reply or response to a question, etc.
7. regard: to have or show respect or concern
for, usually at the end of an email.
8. acknowledgement: to show or express appre-
ciation or gratitude.
9. sorry: express regret, compunction, sympa-
thy, pity, etc.
10. signature: a sender?s name usually at the end
of the email.
We perform sentence segmentation using punc-
tuation and line-breaks and then manually tag each
sentence with a structure label. We exclude the
header of emails for labeling. Figure 2 shows an
example email with structural labels.
From:  Kitchen, Louise Sent: Thursday, April 05, 2001 11:15 AM To: Beck, Sally Cc: Piper, Greg; Jafry, Rahil Subject: Re: Costs   Shukaly resigned and left. But I assume the invitation will be extended to all of their groups so that whoever they want can attend. 
 I would actually prefer that the presentation is actually circulated to the groups on Friday rather than presented as we will wait forever on getting an offsite together. How about circulating the presentation and then letting them refer all questions to Rahil - see how much interest you get. One on ones are much better and I think this is how Rahil should proceed. 
 We need to get in front of customers in the next couple of weeks. Let's aim to get a least three customers this quarter.  Louise 
suggestion 
inform 
request 
signature 
header 
content 
Figure 2: The email with structural labels.
3.2 Slot Annotation
The input to NLG may contain the information
that needs to be included in the synthesized emails.
Tokens in the corpus text corresponding to slots
are replaced by slot (or concept) tokens prior to
building content language models. Slots are clas-
sified into general class and topic class below.
3.2.1 General Class
We use existing named entity recognition (NER)
tools for identifying general classes. Finkel et al.
(2005) used CRF to label sequences of words in
text that are names of things, such as person, or-
ganization, etc. There are three models trained on
different data, which are a 4-class model trained
for CoNLL1, a 7-class model trained for MUC,
and a 3-class model trained on both data sets for
the intersection of those class sets below.
? 4-class: location, person, organization, misc
? 7-class: location, person, organization, time,
money, percent, date
Considering that 3-class model performs higher
accuracy and 7-class model provides better cover-
age, we take the union of outputs produced by 3-
class and 7-class models and use the labels output
by 3-class model if the two models give different
results, since the 3-class model is trained on both
data sets and provides better accuracy.
1http://www.cnts.ua.ac.be/conll2003/
ner/
153
sender-specific model 
topic-specific model 
mixture model 
Figure 3: The visualization of the mixture model.
3.2.2 Topic Class
Many named entities cannot be recognized by a
general NER, because they are topic-specific in-
formation. Accordingly we define additional enti-
ties that are part of the email domain.
4 Modeling Sender Style and TopicStructure for Email Organization
Given a target sender and topic focus specified in
system input, email structures can be generated by
predicted sender-topic-specific mixture models.
4.1 Building Structure Language Models
Based on the annotation of structural labels, each
email can be expressed as a structural label se-
quence. Then we can train a sender-specific and
a topic-specific structure model using the emails
from each sender and the emails related to each
topic respectively. Here the structure models are
n-gram models with Good-Turing smoothing (n =
3) (Good, 1953).
4.2 Predicting Mixture Models
Using sender-specific and topic-specific structure
models, we predict sender-topic-specific mixture
models by interpolation:
Pi,j(l) = ?P si (l) + (1? ?)P tj (l), (1)
where Pi,j(l) is the estimated probability that thestructural label l occurs from the sender i and for
the topic j, P si (l) is the probability of the struc-tural label l from the sender i (regardless of top-
ics), P tj (l) is the probability of the structural label
l related to the topic j (regardless of senders), and
? is the interpolation weight, balancing between
sender style and topic focus. Figure 3 illustrates
the mixture models combined by sender-specific
and topic-specific models.
4.3 Generating Email Structure
We generate structural label sequences randomly
according to the distribution from sender-topic-
specific models. To generate the structural label
sequences from the sender i and related to the
topic j, the probability of the structural label lkusing n-gram language model is
Pi,j(lk) = Pi,j(lk | lk?1, lk?2, ..., lk?(n?1)). (2)
Since we use smoothed trigrams, we may gen-
erate unseen trigrams based on back-off methods,
resulting in some undesirable randomness. We
therefore exclude unreasonable emails that don?t
follow two simple rules.
1. The structural label ?greeting? only occurs at
the beginning of the email.
2. The structural label ?signature? only occurs
at the end of the email.
5 Surface Realization
Our surface realizer has four elements: building
language models, generating text content, scoring
email candidates, and filling slots.
5.1 Building Content Language Models
After replacing the tokens with slots, for each
structural label, we train an unsmoothed n-gram
language model using all sentences with that struc-
tural label. We make a simplifying assumption
that the usage of within-sentence language can be
treated as independent across senders; generating
the text content only considers the structural la-
bels. We use 5-gram to balance variability in gen-
erated sentences while minimizing nonsense sen-
tences.
Given a structural label, we use the content lan-
guage model probability directly to predict the
next word. The most likely sentence is W ? =
argmaxP (W | l), where W is a word sequence
and l is a structural label. However, in order to
introduce more variation, we do not look for the
most likely sentence but generate each word ran-
domly according to the distribution similar to Sec-
tion 4.3 and illustrated below.
5.2 Generating Text Content
The input to surface realization is the generated
structural label sequence. We use the correspond-
ing content language model trained for the given
structural label to generate word sequences ran-
domly according to the distribution from the lan-
guage model. The probability of a word wi usingthe n-gram language model is
P (wi) = P (wi | wi?1, wi?2, ..., wi?(n?1), l),
(3)
where l is the input structural label. Since we build
separate models for different structural labels, (3)
can be written as
P (wi) = P (wi | wi?1, wi?2, ..., wi?(n?1)) (4)
using the model for l.
Using unsmoothed 5-grams will not generate
any unseen 5-grams (or smaller n-grams at the be-
ginning and end of a sentence). This precludes
generation of nonsense sentences within the 5-
word window. Given a generated structural label
sequence, we can generate multiple sentences to
create a synthesized email.
154
5.3 Scoring Email Candidates
The input to NLG contains the required informa-
tion that needs to be in the output email, as de-
scribed in Section 3.2. For each synthesized email,
we penalize it if the email 1) contains slots for
which there is no provided valid value, or 2) does
not have the required slots.
The content generation engine stochastically
generates an email candidate and scores it. If the
email has a zero penalty it is passed on.
5.4 Filling Slots
The last step is to fill slots with the appropriate
values. For example, the sentence ?Tomorrow?s
[meeting] is at [location].? could become ?Tomor-
row?s speech seminar is at Gates building.?
6 Experiments
6.1 Setup
The corpus used for our experiments is the Enron
Email Dataset2, which contains a total of about
0.5M messages. We selected the data related to
daily business for our use, including data from
about 150 users. We randomly picked 3 senders,
ones who wrote many emails, and defined addi-
tional 3 topic classes (meeting, discussion, issue)
as topic-specific entities for the task. Each sender-
specific model (across topics) or topic-specific
model (across senders) is trained on 30 emails.
6.2 Evaluation of Sender Style Modeling
To evaluate the performance of sender style, 7 sub-
jects were given 5 real emails from each sender
and then 9 synthesized emails. They were asked
to rate each synthesized email for each sender on
a scale of 1 (highly confident that the email is not
from the sender) to 5 (highly confident that the
email is from that sender).
With ? = 0.75 in (1) for predicting mix-
ture models (higher weight for sender-specific
model), average normalized scores the corre-
sponding senders receives account for 45%; this
is above chance (which would be 33%). This sug-
gests that sender style can be noticed by subjects,
although the effect is weak, and we are in the pro-
cess of designing a larger evaluation. In a follow-
up questionnaire, subjects indicated that their rat-
ings were based on greeting usage, politeness, the
length of email and other characteristics.
6.3 Evaluation of Surface Realization
We conduct a comparative evaluation of two dif-
ferent generation algorithms, template-based gen-
eration and stochastic generation, on the same
email structures. The average number of sen-
tences in synthesized emails is 3.8, because our
data is about daily business and has relatively short
emails. Given a structural label, template-based
2https://www.cs.cmu.edu/?enron/
generation consisted of randomly selecting an in-
tact whole sentence with the target structural label.
This could be termed sentence-level NLG, while
stochastic generation is word-level NLG.
We presented 30 pairs of (sentence-, word-)
synthesized emails, and 7 subjects were asked to
compare the overall coherence of an email, its sen-
tence fluency and naturalness; then select their
preference. Table 1 shows subjects? preference
according to the rating criteria. The word-based
stochastic generation outperforms or performs as
well as the template-based algorithm for all cri-
teria, where a t-test on an email as a random vari-
able shows no significant improvement but p-value
is close to 0.05 (p = 0.051). Subjects indicated
that emails from word-based stochastic genera-
tion are more natural; word-level generation is less
likely to produce an unusual sentences from the
real data; word-level generation produces more
conventional sentences. Some subjects noted that
neither email seemed human-written, perhaps an
artifact of our experimental design. Nevertheless,
we believe that this stochastic approach would re-
quire less effort compared to most rule-based or
template-based systems in terms of knowledge en-
gineering.
Template Stochastic No Diff.
Coherence 36.19 38.57 25.24
Fluency 28.10 40.48 31.43
Naturalness 35.71 45.71 18.57
Preference 36.67 42.86 20.48
Overall 34.17 41.90 23.93
Table 1: Generation algorithm comparison (%).
7 Conclusion
This paper presents a two-stage stochastic NLG
for synthesizing emails: first a structure is gener-
ated, and then text is generated for each structure
element, where sender style and topic structure
can be modeled. Subjects appear to notice sender
style and can also tell the difference between tem-
plates using original sentences and stochastically
generated sentences. We believe that this tech-
nique can be used to create realistic emails and that
email generation could be carried out using mix-
tures containing additional models based on other
characteristics. The current study shows that email
can be synthesized using a small corpus of labeled
data; however these models could be used to boot-
strap the labeling of a larger corpus which in turn
could be used to create more robust models.
Acknowledgments
The authors wish to thank Brian Lindauer and Kurt
Wallnau from the Software Engineering Institute
of Carnegie Mellon University for their guidance,
advice, and help.
155
References
Regina Barzilay and Mirella Lapata. 2008. Modeling lo-
cal coherence: An entity-based approach. Computational
Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the drift:
Probabilistic content models, with applications to genera-
tion and summarization. In HLT-NAACL, pages 113?120.
Anja Belz. 2005. Corpus-driven generation of weather fore-
casts. In Proc. 3rd Corpus Linguistics Conference.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into in-
formation extraction systems by gibbs sampling. In Proc.
of ACL, pages 363?370.
Alastair J Gill, Carsten Brockmann, and Jon Oberlander.
2012. Perceptions of alignment and personality in gener-
ated dialogue. In Proceedings of the Seventh International
Natural Language Generation Conference, pages 40?48.
Association for Computational Linguistics.
Irving J Good. 1953. The population frequencies of species
and the estimation of population parameters. Biometrika,
40(3-4):237?264.
Shlomo Hershkop, Salvatore J Stolfo, Angelos D Keromytis,
and Hugh Thompson. 2011. Anomaly detection at multi-
ple scales (ADAMS).
Andrew Lampert, Robert Dale, and Ce?cile Paris. 2009. Seg-
menting email message text into zones. In Proceedings
of the 2009 Conference on Empirical Methods in Natural
Language Processing, volume 2, pages 919?928. Associ-
ation for Computational Linguistics.
Alice H Oh and Alexander I Rudnicky. 2002. Stochastic nat-
ural language generation for spoken dialog systems. Com-
puter Speech & Language, 16(3):387?407.
Christina Sauper and Regina Barzilay. 2009. Automati-
cally generating wikipedia articles: A structure-aware ap-
proach. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the
AFNLP: Volume 1-Volume 1, pages 208?216. Association
for Computational Linguistics.
156
