Two levels of evaluation in a complex NL system
Jean-Baptiste Berthelin
LIMSI-CNRS
B?t.508 Universit? Paris XI
91403 Orsay, France
jbb@limsi.fr
Brigitte Grau
LIMSI-CNRS
B?t.508 Universit? Paris XI
91403 Orsay, France
bg@limsi.fr
Martine Hurault-Plantet
LIMSI-CNRS
B?t.508 Universit? Paris XI
91403 Orsay, France
mhp@limsi.fr
Abstract
The QALC question-answering system,
developed at LIMSI, has been a
participant for two years in the QA
track of the TREC conference. In this
paper, we present a quantitative
evaluation of various modules in our
system, based on two criteria: first, the
numbers of documents containing the
correct answer and selected by the
system; secondly, the number of
answers found. The first criterion is
used for evaluating locally the modules
in the system, which contribute in
selecting documents that are likely to
contain the answer. The second one
provides a global evaluation of the
system. As such, it also serves for an
indirect evaluation of various modules.
1 Introduction
For two years, the TREC Evaluation
Conference, (Text REtrieval Conference) has
been featuring a Question Answering track, in
addition to those already existing. This track
involves searching for answers to a list of
questions, within a collection of documents
provided by NIST, the conference organizer.
Questions are factual or encyclopaedic, while
documents are newspaper articles. The TREC9-
QA track, for instance, proposed 700 questions
whose answers should be retrieved in a corpus
of about one million documents.
In addition to the evaluation, by human
judges, of their systems? results (Voorhees and
Tice, 2000), TREC participants are also
provided with an automated evaluation tool,
along with a database. These data consist of a
list of judgements of all results sent in by all
participants. The evaluation tool automatically
delivers a score to a set of answers given by a
system to a set of questions. This score is
derived from the mean reciprocal rank of the
first five answers. For each question, the first
correct answers get a mark in reverse proportion
to their rank. Those evaluation tool and data are
quite useful, since it gives us a way of
appreciating what happens when modifying our
system to improve it.
We have been taking part to TREC for two
years, with the QALC question-answering
system (Ferret et al 2000), currently developed
at LIMSI. This system has following
architecture: parsing of the question to find the
expected type of the answer, selection of a
subset of documents among the approximately
one million TREC-provided items, tagging of
named entities within the documents, and,
finally, search for possible answers. Some of the
components serve to enrich both questions and
documents, by adding system-readable data into
them. Such is the case for the modules that parse
questions and tag documents. Other components
operate a selection among documents, using
added data. One example of such modules are
those which select relevant documents, another
is the one which extracts the answer from the
documents.
A global evaluation of the system is based on
judgement about its answers. This criterion
provides only indirect evaluation of each
component, via the evolution of the final score
when this component is modified. To get a
closer evaluation of our modules, we need other
criteria. In particular, concerning the evaluation
of components for document selection, we
adopted an additional criterion about selected
relevant documents, that is, those that yield the
correct answer.
This paper describes a quantitative
evaluation of various modules in our system,
based on two criteria: first, the number of
selected relevant documents, and secondly, the
number of found answers. The first criterion is
used for evaluating locally the modules in the
system, which contribute in selecting documents
that are likely to contain the answer. The second
one provides a global evaluation of the system.
It also serves for an indirect evaluation of
various modules.
2 System architecture
Figure 1 shows the architecture of the QALC
system, made of five separate modules:
Question analysis, Search engine, Re-indexing
and selection of documents, Named entity
recognition, and Question/sentence pairing.
Tagged Questions:
Named entity tags
Vocabulary &
  frequencies
Named entity
 recognition
Candidate
terms
Retrieved
documents
Tagged sentences: named entity
    tags and term indexation
Ordered sequences of 250 and
           50 characters
Question analysis Search engine
Questions
Subset of ranked documents
Corpus
Re-indexing and selection of
      documents (FASTR)
Question/Sentence pairing
Figure 1. The QALC system
2.1 Question analysis
Question analysis is performed in order to assign
features to questions and use these features for
the matching measurement between a question
and potential answer sentences. It relies on a
shallow parser which spots discriminating
patterns and assigns categories to a question.
The categories correspond to the types of named
entities that are likely to constitute the answer to
this question. Named entities receive one of the
following types: person, organisation, location
(city or place), number (a time expression or a
number expression). For example the pattern
how far yields to the answer type length:
Question: How far away is the moon?
Answer type: LENGTH
Answer  within the document :
With a <b_numex_TYPE="NUMBER"> 28
<e_numex> -power telescope you can see it on
the  moon <b_numex_TYPE="LENGTH">
250,000 miles <e_numex> away.
2.2 Selection of relevant documents
The second module is a classic search
engine, giving, for each question, a ranked list of
documents, each of which could contain the
answer.
This set of documents is then processed by a
third module, made of FASTR (Jacquemin,
1999), a shallow transformational natural
language analyser and of a ranker. This module
can select, among documents found by the
search engine, a subset that satisfies more
refined criteria. FASTR improves things
because it indexes documents with a set of
terms, including not only the (simple or
compound) words of the initial question, but
also their morphological, syntactic and semantic
variants. Each index is given a weight all the
higher as it is close to the original word in the
question, or as it is significant. For instance,
original terms are considered more reliable than
semantic variants, and proper names are
considered more significant than nouns. Then,
documents are ordered according to the number
and the quality of the terms they contain. An
analysis of the weight graph of the indexed
documents enables the system to select a
relevant subpart of those documents, whose size
varies along the questions. Thus, when the curve
presents a high negative slope, the system only
select documents before the fall, otherwise a
fixed threshold is used.
2.3 Named entity recognition
The fourth module tags named entities in
documents selected by the third one. Named
entities are recognized through a combination of
lexico-syntactic patterns and significantly large
lexical data. The three lists used for lexical
lookup are CELEX (1998), a lexicon of 160,595
inflected words with associated lemma and
syntactic category, a list of 8,070 first names
(6,763 of which are from the CLR (1998)
archive) and a list of 211,587 family names also
from the CLR archive.
2.4 Question-sentence pairing
The fifth module evaluates each sentence in
the ranker-selected documents, using a
similarity measure between, on one side, terms
and named entities in the sentence, and on the
other side, words in the questions and expected
answer type. To do so, it uses the results of the
question parser, and the named entity tagger,
along with a frequency-weighted vocabulary of
the TREC corpus.
The QALC system proposes long and short
answers. Concerning the short ones, the system
focuses on parts of sentences that contain the
expected named entity tags, when they are
available, or on the larger subpart without any
terms.
3 Search engine evaluation
The second module of the QALC system deals
with the selection, through a search engine, of
documents that may contain an answer to a
given question from the whole TREC corpus
(whose size is about 3 gigabytes).
We tested three search engines with the 200
questions that were proposed at the TREC8 QA
track. The first one is Zprise, a vectorial search
engine developed by NIST. The second is
Indexal (de Loupy et al1998), a pseudo-boolean
search engine developed by Bertin
Technologies1. The third search engine is ATT
whose results to the TREC questions are
provided by NIST in the form of ranked lists of
the top 1000 documents retrieved for each
question. We based our search engine tests on
                                                           
1 We are grateful to Bertin Technologies for providing us
with the outputs of Indexal on the TREC collection for the
TREC8-QA and TREC9-QA question set.
the list of relevant documents extracted from the
list of correct answers provided by TREC
organizers.
Since a search engine produces a large
ranked list of relevant documents, we had to
define the number of documents to retain for
further processing. Indeed, having too many
documents leads to a question processing time
that is too long, but conversely, having too few
documents reduces the possibility of obtaining
the correct answer. The other goal of the tests
obviously was to determine the best search
engine, that is to say the one that gives the
highest number of relevant documents.
3.1 Document selection threshold
In order to determine the best selection
threshold, we carried out four different tests
with the Zprise search engine. We ran Zprise for
the 200 questions and then compared the
number of relevant documents respectively in
the top 50, 100, 200, and 500 retrieved
documents. Table 1 shows the test results.
Selection
Threshold
Questions with
relevant
documents
Questions with
no relevant
documents
50 181 19
100 184 16
200 193 7
500 194 6
Table 1. Number of questions with and
without relevant documents retrieved for
different thresholds
According to Table 1, the improvement of
the search engine results tends to decrease
beyond the threshold of 200 documents. The top
200 ranked documents thus seem to offer the
best trade-off between the number of documents
in which the answer may be found and the
question processing time.
3.2 Evaluation
We compared the results given by the three
search engines for a threshold of 200
documents. Table 2 gives the tests results.
Search Engine Indexal Zprise ATT
Number of questions
with relevant
documents retrieved
182 193 194
Number of questions
without relevant
documents retrieved
18 7 6
Total number of
relevant documents
that were retrieved
814 931 1021
Table 2. Compared performances of the
Indexal, Zprise and ATT search engines
All three search engines perform quite well.
Nevertheless, the ATT search engine revealed
itself the most efficient according to the
following two criteria: the lowest number of
questions for which no relevant document was
retrieved, and the most relevant documents
retrieved for all the 200 questions. Both criteria
are important. First, it is most essential to obtain
relevant documents for as many questions as
possible. But the number of relevant documents
for each question also counts, since having more
sentences containing the answer implies a
greater probability to actually find it.
4 Document ranking evaluation
As the processing of 200 documents by the
following Natural Language Processing (NLP)
modules still was too time-consuming, we
needed an additional stronger selection. The
selection of relevant documents performed by
the re-indexing and selection module relies on
an NLP-based indexing composed of both
single-word and phrase indices, and linguistic
links between the occurrences and the original
terms. The original terms are extracted from the
questions. The tool used for extracting text
sequences that correspond to occurrences or
variants of these terms is FASTR (Jacquemin,
1999). The ranking of the documents relies on a
weighted combination of the terms and variants
extracted from the documents. The use of multi-
words and variants for document weighting
makes a finer ranking possible.
The principle of the selection is the
following: when there is a sharp drop of the
documents weight curve after a given rank, we
keep only those documents which occur before
the drop. Otherwise, we arbitrarily keep the first
100.
In order to evaluate the efficiency of the
ranking process, we proceeded to several
measures. First, we apply our system on the
material given for the TREC8 evaluation, one
time with the ranking process, and another time
without this process. 200 documents were
retained for each of the 200 questions. The
system was scored by 0.463 in the first case, and
by 0.452 in the second case. These results show
that document selection slightly improves the
final score while much reducing the amount of
text to process.
However, a second measurement gave us
more details about how things are improved.
Indeed, when we compare the list of relevant
documents selected by the search engine with
the list of ranker-selected ones, we find that the
ranker loses relevant documents. For thirteen
questions among the 200 in the test, the ranker
did not consider relevant documents selected by
the search engine. What happens is: the global
score improves, because found answers rank
higher, but the number of found answers
remains the same.
The interest to perform such a selection is
also illustrated by the results given Table 3,
computed on the TREC9 results.
Number of documents
selected by ranking
100 <<100
Distribution among the
questions
342
(50%)
340
(50%)
Number of correct
answers
175
(51%)
200
(59%)
Number of correct answer
at rank 1
88
(50%)
128
(64%)
Table 3. Evaluation of the ranking process
We see that the selection process discards
documents for 50% of the questions: 340
questions are processed from less than 100
documents. For those 340 questions, the average
number of selected documents is 37. The
document set retrieved for those questions has a
weight curve with a sharp drop. QALC finds
more often the correct answer and in a better
position for these 340 questions than for the 342
remaining ones. These results are very
interesting when applying such time-consuming
processes as named-entities recognition and
question/sentence matching. Document selection
will also enable us to apply further sentence
syntactic analysis.
5 Question-sentence pairing evaluation
We sent to TREC9 two runs which gave
answers of 250 characters length, and one run
which gave answers of 50 characters length. The
first and the last runs used ATT as search
engine, and the second one, Indexal. Results are
consistent with our previous analysis (see
Section 3.2). Indeed, the run with ATT search
engine gives slightly better results (0.407 strict)2
than those obtained with the Indexal search
engine (0.375 strict).  Table 4 sums up the
number of answers found by our two runs.
Rank of the correct
answer retrieved
Run using
ATT
Run using
Indexal
1 216 187
2 to 5 159 185
Total of correct
answers retrieved
375 372
No correct answer
retrieved
307 310
Table 4. Number of correct answers retrieved,
by rank, for the two runs at 250 characters
The score of the run with answers of 50
characters length was not encouraging,
amounting only 0.178, with 183 correct answers
retrieved3.
5.1 Long answers
From results of the evaluation concerning
document ranking, we see that the performance
level of the question-sentence matcher depends
partly on the set of sentences it has parsed, and
not only on the presence, or absence, of the
answer within these sentences. In other words,
we do not find the answer each time it is in the
set of selected sentences, but we find it easily if
there are few documents (and then few
sentences) selected. That is because similarity
                                                           
2 With this score, the QALC system was ranked 6th among
25 participants at TREC9 QA task for answers with 250
characters length.
3 With this score, the QALC system was ranked 19th
among 24 participants at TREC9 QA task for answers with
50 characters length.
assessment relies upon a small number of
criteria, which are found to be insufficiently
discriminant. Therefore, several sentences
obtain the same mark, in which case, the rank of
the correct answer depends on the order in
which sentences are encountered.
This is something we cannot yet manage, so
we evaluated the matcher?s performance,
without any regard to the side effect induced by
document processing order. As remarked in 3.2,
search engines perform well. In particular, ATT
retains relevant documents, namely, those that
yield good answers, for 97 percent of the
questions. The ranker, while improving the final
score, loses some questions. After it stepped in,
the system retains relevant documents for 90%
of the questions. The matcher finds a relevant
document in the first five answers for 74% of
the questions, but answers only 62% of them
correctly.  Finding the right document is but one
step, knowing where to look inside it is no
obvious task.
5.2 Short answers
A short answer is selectively extracted from
a long one. We submitted this short answer
selector (under 50 characters) to evaluation
looking for the impact of the expected answer
type. Among TREC questions, some expect an
answer consisting of a named entity: for instance
a date, a personal or business name. In such
cases, assigning a type to the answer is rather
simple, although it implies the need of a good
named entity recognizer. Answers to other
questions (why questions for instance, or some
sort of what questions), however, will consist of
a noun or sentence. Finding its type is more
complex, and is not done very often.
Some systems, like FALCON (Harabagiu et
al 2000) use Wordnet word class hierarchies to
assign types to answers. Among 682 answers in
TREC9, 57.5% were analysed by our system as
named-entity questions, while others received
no type assignment. Among answers from our
best 250-character run, 62.7% were about
named entities. However, our run for shorter
answers, yielding a more modest score, gives
84% of named-entities answers. In our system
answer type assignment is of surprisingly small
import, where longer answers are concerned.
However, it does modify the selecting process,
when the answer is extracted from a longer
sentence.
Such evaluations help us to see more clearly
where our next efforts should be directed.
Having more criteria in the similarity
measurement would, in particular, be a source of
improvement.
6 Discussion
We presented quantitative evaluations. But since
we feel that evaluations should contribute to
improvements of the system, more qualitative
and local ones also appear interesting.
TREC organizers send us, along with run
results, statistics about how many runs found the
correct answer, and at which rank. Such
statistics are useful in many ways. Particularly,
they provide a characterisation of a posteriori
difficult questions. Knowing that a question is a
difficult one is certainly relevant when trying to
answer it. Concerning this problem, de Loupy
and Bellot (2000) proposed an interesting set of
criteria to recognize a priori difficult questions.
They use word frequency, multi-words,
polysemy (a source of noise) and synonymy (a
source of silence). They argue that an
?intelligent? system could even insist that a
question be rephrased when it is too difficult.
While their approach is indeed quite promising,
we consider that their notion of a priori
difficulty should be complemented by the notion
of a posteriori difficulty we mentioned: the two
upcoming examples of queries show that a
question may seem harmless at first sight, even
using de Loupy and Bellot?s criteria, and still
create problems for most systems.
From these statistics, we also found
disparities between our system and others for
certain questions. At times, it finds a good
answer where most others fail and obviously the
reverse also happens. This is the case in the two
following examples. The first one concerns an
interesting issue in a QA system that is the
determination of which terms from the question
are to be selected for the question-answer
pairing. This is particularly important when the
question has few words. For instance, to the
question  How far away is the moon?, our term
extractor kept not only moon?(NN), but also
away?(RB) . Moreover, our question parser
knows that how far is an interrogative phrase
yielding a LENGTH type for the answer. This
leads our system to retrieve the correct answer:
With a 28-power telescope, you can see it on the
moon 250,000 miles away4.
The second example concerns the relative
weight of the terms within the question. When a
proper noun is present, it must be found in the
answer, hence an important weight for it. Look
at the question Who manufactures the software,
? ?PhotoShop??? . The term extractor kept
s o f t w a r e ( N N ) , PhotoShop(NP),  a n d
manufacture(VBZ) as terms to be matched, but
the matcher assigns equal weights to them, so
we could not find the answer5. Later, we
modified these weights, and the problem was
solved.
Indeed, evaluation corpus seems to be
difficult to build. Apart from the problem of the
question difficulty level, question type
distribution may also vary from a corpus to
another. For instance, we note that TREC8
proposed much more questions with named
entity answer type (about 80%) than TREC9
(about 60%). Thus, some participants who trains
their systems on the TREC8 corpus were
somehow disapointed by their results at TREC9
with regards with their training results (Scott
and Gaizauskas, 2000).
However, it is generally hard to predict what
will happen if we modify the system. A local
improvement can result in a loss of performance
for other contexts. Although the system?s
complexity cannot be reduced to just two levels
(a local one and a global one), this can be an
efficient step in the design of improvements to
the whole system via local adjustments. But this
is a very frequent situation in engineering tasks.
7 Conclusion and perspectives
Each evaluation reflects a viewpoint, underlying
the criterion we use. In our case, the choice of
criteria was guided by the existence of two main
stages in the QA process, namely the selection
of relevant documents and the selection of the
answer among the selected documents
sentences. Sometimes, such criteria concur in
                                                           
4 Among the 42 runs using 250 byte limit, submitted at
TREC9-QA, only seven found the correct answer at rank 1,
and 27 do not found it.
5 22 runs, out of 42 found the right answer at rank 1. Only
9 were unable to find it.
revealing the same positive or negative feature
of the system. They can also yield a more
precise assessment of the reasons behind these
features, as was the case in our evaluation of the
ranker. Moreover, when a system consists of
several modules, their specific evaluations
should imply different criteria.
This is particularly true in dialogue systems,
where different kinds of processes are co-
operating. Since information retrieval is an
interactive task, it seems natural to associate a
dialogue component to it. Indeed, users tend to
ask a question, evaluate the answer, and
reformulate their question to make it more
specific (or, contrariwise, more general, or quite
different). A QA system is, therefore, a good
applicative setting for a dialogue module.
Quantitative assessment of the QA system
would be useful in assessing the dialogue system
in this particular context. Such a global
assessment would provide an objective
judgement about whether the task (finding the
answer) was achieved, or not. Successfulness in
a task is a necessary component of the
evaluation, nevertheless it is just a part of it.
Obviously, dialogue evaluation is also a matter
of cost (time, number of exchanges) and of user-
friendliness (cognitive ergonomy).
However, objectivity is almost impossible to
attain in these domains. In a recent debate
(LREC 2000), serious objections about natural
language tools evaluation and validation were
developed e.g. by Sabah (2000). The main issue
he raises is about the great complexity of such
systems. However, we consider that by going as
far as possible in the experimental search for
evaluation criteria, we also make a meaningful
contribution to this debate. While it is true that
complexity should never be ignored, we
consider that, by successive approximate
modelisation and evaluation cycles, we can
capture some of it at each step of our system?s
developement.
References
CELEX. 1998.
http://www.ldc.upenn.edu/readme_files/celex.read
me.html. Consortium for Lexical Resources,
UPenns, Eds.
CLR. 1998. http://crl.nmsu.edu/cgi-
bin/Tools/CLR/clrcat#D3. Consortium for Lexical
Resources, NMSUs, Eds., New Mexico.
Fabre C., Jacquemin C, 2000. Boosting variant
recognition with light semantics. Proceedings
COLING?2000, pp. 264-270, Luxemburg.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA, MIT Press.
Ferret O., Grau B., Hurault-Plantet M., Illouz G.,
Jacquemin C. (2000), QALC ? the Question-
Answering system of LIMSI-CNRS. Pre-
proceedings of TREC9, NIST, Gaithersburg, CA.
Harabagiu S., Pasca M., Maiorano J. 2000.
Experiments with Open-Domain Textual Question
Answering. Proceedings of Coling'2000,
Saarbrucken, Germany.
Jacquemin C. 1999. Syntagmatic and paradigmatic
representations of term variation. Proceedings of
ACL'99. 341-348.
de Loupy C., Bellot P., El-B?ze M., Marteau P.-F..
Query Expansion and Classification of Retrieved
Documents, TREC7 (1998), 382-389.
de Loupy C., Bellot P. 2000. Evaluation of
Document Retrieval Systems and Query
Difficulty. Proceedings of the Second
International Conference on Language Resources
and Evaluation (LREC 2000) Workshop, Athens,
Greece. 32-39.
Sabah G. 2000 To validate or not to validate? Some
theoretical difficulties for a scientific evaluation of
natural language processing systems. Proceedings
of the Second International Conference on
Language Resources and Evaluation (LREC 2000)
Workshop, Athens, Greece. 58-61.
Scott S., Gaizauskas R. 2000. University of Sheffield
TREC-9 Q & A System. Pre-proceedings of
TREC9, NIST, Gaithersburg, CA. 548-557.
Voorhees E., Tice D. 2000. Implementing a Question
Answering Evaluation. Proceedings of the Second
International Conference on Language Resources
and Evaluation (LREC 2000). Athens, Greece. 40-
45.
A cross-comparison of two clustering methods
Olivier Ferret
CEA Saclay
DTI/SITI
91191 Gif-sur-Yvette Cedex
ferret@sphinx.cea.fr
Brigitte grau and Miche`le Jardino
LIMSI CNRS
BP133
91403 Orsay, France
bg,jardino@limsi.fr
Abstract
Many Natural Language Processing ap-
plications require semantic knowledge
about topics in order to be possible or to
be efficient. So we developed a system,
SEGAPSITH, that acquires it automat-
ically from text segments by using an
unsupervised and incremental cluster-
ing method. In such an approach, an
important problem consists of the vali-
dation of the learned classes. To do that,
we applied another clustering method,
that only needs to know the number
of classes to build, on the same sub-
set of text segments and we reformu-
late our evaluation problem in compar-
ing the two classifications. So, we es-
tablished different criteria to compare
them, based either on the words as class
descriptors or on the thematic units.
Our first results lead to show a great
correlation between the two classifica-
tions.
1 Introduction
Among all the applications in Natural Language
Processing (NLP), many require semantic knowl-
edge about topics in order to be possible or to
be efficient. These applications are, for exam-
ple, topic segmentation and identification or text
classification. As this kind of knowledge is not
easy to build manually, we developed a system,
SEGAPSITH (Ferret and Grau, 1998a), (Fer-
ret and Grau, 1998b), to acquire it automatically.
In this field, there are two classes of approaches.
Supervised learning that requires to know a pri-
ori which topics have to be learned and to pos-
sess a tagged corpus as a learning set. It is the
approach generally adopted by the different sys-
tems, as those participating to TREC or TDT.
However, we wanted to design a system allow-
ing us to work in open domain, without any re-
striction about the subjects to be represented and,
thus, to be recognized in texts. SEGAPSITH
is grounded on an unsupervised and incremental
learning based on a conceptual clustering method.
After a thematic segmentation of the texts that
divides a text in segments made of lemmatized
words, i. e. thematic units, the system aggregates
similar enough thematic units. Aggregation con-
sists of regrouping all the words of the different
similar units and associating to them a weight ac-
cording to their occurrence number. This weight
represents the importance of a word relative to the
described topic. The incremental aspect allows us
to augment topic knowledge by treating succes-
sive corpora without reconsidering the knowledge
already existing.
In such an approach, an important problem
consists of the validation of the learned classes.
As we do not possess an existing classifica-
tion that agrees with the granularity level of our
classes, we decided to accomplish this evaluation
by using a second classification method on the
same data and by comparing their results. This
second method is an entropy-based method, and
requires to know the number of classes to form.
So, if both results are similar enough, although
the methods applied are different, we could con-
clude that the learned classes are quite relevant
and that the two methods are efficient.
After applying the second method, we pos-
sess two sets composed by the same number of
classes. Each class regroups thematic units and
is described by a set of words. So, we established
different criteria to compare them, based either on
the words as class descriptors or on the thematic
units they gather. After the presentation of the
two methods, we shall present our tests and the
first results we obtained.
2 Semantic domain learning
This description aims at showing the data used for
learning, and the specificity of the learned classes.
2.1 The thematic segmentation:
SEGCOHLEX
Studied texts are newspaper articles coming from
two corpora: ?Le Monde? and ?AFP?. Some of
these texts have been used to build a lexical net-
work where links between two words represent an
evaluation of their mutual information to capture
semantic and pragmatic relations between them,
computed from their co-occurrence count. In or-
der to build class of words linked to a same topic,
we first realize a topic segmentation of the texts
in thematic units (TU) whose words refer to the
same topic, and learning is applied on these the-
matic units.
Text segmentation is based on the use of the
collocation network. A topic is detected by com-
puting a cohesion value for each word resulting
from the relations found in the network between
these words and their neighbors in a text. As in
Kozima?s work (Kozima, 1993), this computa-
tion operates on words belonging to a focus win-
dow that is moved all over the text.The cohe-
sion values lead to build a graph and by succes-
sive transformations applied to it, texts are auto-
matically divided in discourse segments. Such a
method leads to delimit small segments, whose
size is equivalent to a paragraph, i. e. capa-
ble of retrieving topic variations in short texts,
as newswires for example. Table 1 shows an ex-
tract of the words belonging to a cohesive seg-
ment about a dedication of a book.
2.2 Semantic Domain learning in
SEGAPSITH
Learning a semantic domain consists of aggre-
gating all the most cohesive thematic units, TUs,
that are related to a same subject, i. e. a same
kind of situation. We only retain segments whose
cohesion value is higher than a threshold, in or-
der to ground our learning on the more reliable
units. Similarity between a thematic unit and
a semantic domain is evaluated from their com-
mon words. When the similarity value exceeds a
threshold, the thematic unit is aggregated to the
semantic domain, otherwise a new domain is cre-
ated. Each aggregation of a new thematic unit in-
creases the system?s knowledge about one topic
by reinforcing recurrent words and adding new
ones. Weights on words represent their impor-
tance relative to the topic and are computed from
the number of occurrences of these words in the
TUs.
Units related to a same topic are found in dif-
ferent texts and often develop different points of
view of a same type of subject. To ensure a better
similarity between them, SEGAPSITH enriches
a particular description given by a text segment
by adding to these units those words of the collo-
cation network that are particularly linked to the
words found in the segment. Table 2 gives an ex-
tract of the words added to the segment of Table
1.
This method leads SEGAPSITH to learn spe-
cific topic representations (see Table 3) as op-
posed to (Lin, 1997) for example, whose method
builds general topic descriptions as for economy,
sport, etc. Moreover, it does not depend on any a
priori classification of the texts.
We applied the learning module of SEGAP-
SITH on one month (May 1994) of AFP
newswires, corresponding to 7823 TUs. In our
experiments (Ferret and Grau, 1998a), (Ferret
and Grau, 1998b), we showed that domains reach
a stability at 15 to 20 aggregations, and that words
having a weight below 0.1 are rarely related to the
domain. Thus, we only selected domains result-
ing from at least 15 aggregations for our cross-
comparison, i.e. 71 domains regrouping 4935
TUs and 4380 words having a weight upon 0.1.
A lot of domains share common words, and are
close enough to be considered as different repre-
sentations of specific points of view of a general
topic, as economy, sport, etc.
3 Entropy-based clustering
The second clustering method gives an optimal
partition of the 4935 thematic units in 71 non-
words weight words weight
strider 0.683 entourer (to surround) 0.368
toward 0.683 signature (signature) 0.366
de?dicacer (to dedicate) 0.522 exemplaire (exemplar) 0.357
apposer (to append) 0.467 page (page) 0.332
pointu (sharp-pointed) 0.454 train (train) 0.331
relater (to relate) 0.445 centaine (hundred) 0.330
boycottage (boycotting) 0.436 sentir (to feel) 0.328
autobus (bus) 0.435 livre (book) 0.289
enfoncer (to drive in) 0.410 personne (person) 0.267
Table 1: Extract of a segment about a dedication
inferred words weight inferred words weight
paraphe (paraph) 0.522 imprimerie (press) 0.418
presse parisien (parisian-press) 0.480 e?diter (to publish) 0.407
best seller (best seller) 0.477 biographie (biography) 0.406
maison d?e?dition (publishing house) 0.450 librairie (bookshop) 0.405
libraire (bookseller) 0.447 poche (pocket) 0.389
tome (tome) 0.445 e?diteur (publisher) 0.363
Grasset (a publisher) 0.440 lecteur (reader) 0.355
re?e?diter (to republish) 0.428 israe?lien (Israeli) 0.337
parution (appearance) 0.427 e?dition (publishing) 0.333
Table 2: Extract of words selected in the collocation network for the segment of Table 1
words occurrences weight
juge d?instruction (examining judge) 58 0.501
garde a` vue (police custody) 50 0.442
bien social (public property) 46 0.428
inculpation (charging) 49 0.421
e?crouer (to imprison) 45 0.417
chambre d?accusation (court of criminal appeal) 47 0.412
recel (receiving stolen goods) 42 0.397
pre?sumer (to presume) 45 0.382
police judiciaire (criminal investigation department) 42 0.381
escroquerie (fraud) 42 0.381
Table 3: The most representative words of a domain about justice
overlapping clusters according to the word dis-
tributions in the units. It is realized with an al-
gorithm which looks like K-means (here K=71).
Each cluster is the merge of several thematic units
and is represented by its centroid. We search
for the partition which minimizes the Kullback-
Leibleir divergence (Cover and Thomas, 1991)
between the word distributions of the thematic
units and those of of their centroids. This entropy-
based measure is convex (Jardino, 2000), this pro-
priety permits to get an optimal partition whatever
the initial conditions.
3.1 Entropy
We assume that each thematic unit is represented
by one quantitative vector whose components are
the relative occurrences of a selection of words re-
lated to the unit. The advantages of this normal-
ization is that the representation of the thematic
units does not depend on the length of the units
and can be modelized in the frame of the infor-
mation theory (Cover and Thomas, 1991).
Assuming that O
w;tu
is the occurrence of the
word labelled w in the thematic unit labelled tu
and that O
tu
is the occurrence of all the words in
the thematic unit tu, such that O
tu
=
P
w
O
w;tu
,
each thematic unit vector component , p(w=tu),
is :
p(w=tu) =
O
w;tu
O
tu
(1)
When the thematic units are unclassed, their
entropy is given by (Cover and Thomas, 1991):
H
TU
=  
X
w;tu
p(w; tu)  ln[p(wjtu)] (2)
with p(w; tu) = Ow;tuP
w;tu
O
w;tu
When the thematic units are gathered in K clus-
ters, labelled k, the cluster entropy is, H
K
:
H
K
=  
X
w;k
p(w; k)  ln[p(wjk)] (3)
where p(wjk) is defined as :
p(wjtu 2 k) = p(wjk) =
O
w;k
O
k
(4)
with O
w;k
=
P
tu2k
O
w;tu
, O
k
=
P
tu2k
O
tu
and
p(w; k) =
O
w;k
P
w;k
O
w;k
The cluster entropy is always higher than or
equal to the unit entropy (log-sum rule (Cover and
Thomas, 1991)), so that the Kullback-Leibleir di-
vergence defined as:
D
KL
= H
K
 H
TU
(5)
is always higher than or equal to 0.
3.2 Clustering algorithm
Minimizing the Kullback-Leibler divergence
amounts to minimize the entropy H
K
because
H
TU
does not depend on the clusters.
The number of possible partitions is huge,
roughly 493571. We have observed that a random
search is faster than a systematic one (Jardino,
2000), and we have used this paradigm to build
the algorithm described below:
1- Define a priori, K, the cluster number, here
K=71.
2- Initialize: put all the thematic units in one clus-
ter, calculate the entropy H
K
(equation 3). The
remaining K-1 clusters are empty.
3- Do the random selection of one thematic unit
and of another cluster for this unit.
4- Move the unit from its former cluster to the new
randomly selected one, calculate the new entropy.
5- If the new entropy is lower, leave the unit in its
new cluster, otherwise move it back to its initial
cluster.
6- Repeat 3 to 5 until there is no more change.
The optimal clustering of the 4935 thematic
units in 71 clusters is performed on a workstation
(SGI Indy) within twenty minutes.
4 Comparing two classifications
We established different criteria for comparing
the two classifications, based on the elements
used to describe the classes. First, each class is a
set of words with an occurrence number for each
of them; second it is also a set of thematic units.
So, the comparison can be done along these two
points of view.
In order to evaluate the overlapping of the
classes of words, we applied each classifica-
tion method on the two classification results:
the classes of words resulting from the second
method are classified relative to the semantic do-
mains. For comparing the classes of TUs, we ap-
plied the entropy measure on one hand to measure
the overlapping of the classes, and  and Mantel
tests on the other hand to evaluate the differences
in the repartition of all the TUs.
4.1 The word point of view
4.1.1 Classification by similarity
The classification of the clusters relative to the
semantic domains exploits the same similarity
measure than the one used for the learning phase.
In a first step, some domains are selected accord-
ing to the value of the activ function:
activ(d) =
X
i
W
d;i
W
c;i
(6)
where W
d;i
is the weight of the word i in the
semantic domain d and W
c;i
is the weight of the
same word in the cluster c. This first step was
used in the learning phase because the number of
semantic domains was increasing rapidly and this
measure leads to a first fast selection of interesting
domains before evaluating an in-depth similarity.
We kept this step, even if it was not necessary,
in order to apply exactly the same method in the
evaluation phase. Afterwards, each selected do-
main can be compared to the cluster by using the
similarity measure given below. If one of these
similarity values is greater than a given threshold,
fixed to 0.25 in our tests, the cluster is linked to
the domain that is the most similar to it. The sim-
ilarity measure is:
sim(d; c) =
4
r
P
w
W
d;w
P
t
W
d;t
P
w
O
d;w
P
t
O
d;t
P
w
W
c;w
P
t
W
c;t
P
w
O
c;w
P
t
O
cl;t
(7)
where the w index is used for indicating com-
mon words between the cluster c and the seman-
tic domain d and the t index, for indicating all
the words of the cluster or the domain. W is the
weight of a word and O its occurrence number.
The similarity measure is only based on the
common words. As learning is unsupervised and
incremental, differences at time t might disappear
at time t+1. The comparison is based on the pro-
portion of common words relative to the total of
words of each entity to be compared. The evalu-
ation of the common words in each entity is done
according to their occurrence number and their
weight. So, we avoid to obtain a high similarity
value between two entities that only share very
few words having a high weight. We combine
these criteria in a geometrical mean for evaluating
each entity and for computing the global similar-
ity from the evaluation of the two entities in order
to smooth the effect of few recurrent words when
the domains are in their formation phase, words
that would act as attractors otherwise.
4.1.2 Entropy-based classification
For each of the 71 clusters, we have searched
for the nearest domain obtained with the same
kind of entropy-based measure defined above. We
assume that we have a probabilistic model which
gives the predictions of the words according to the
domains. In order to avoid the null value, non-
learned events are infered using the Witten-Bell
interpolation (Witten and Bell, 1991). The inter-
polated value of the prediction of a word w, know-
ing the domain d is p0(wjd) such that:
p
0
(wjd) =
O(w; d) + n
sw
(d)=V
O(d) + n
sw
(d)
(8)
where n
sw
(d) is the number of words seen in each
domain and V the size of the vocabulary. Each
cluster is also defined by a set of words and we
compare the distribution of the words in the clus-
ter with the distributions of the words in the do-
mains (equation 8) with the Kullback-Leibler di-
vergence. Each cluster is associated with the near-
est domain.
4.1.3 Comparison
The results of the two classifications described
above are given in Table 4. For the similarity-
based classification, only 3 clusters do not match
with any domain and 47 different domains are se-
lected for the 68 remaining clusters with 34 links
that are one cluster-one domain. For the entropy-
based classification, 44 clusters have been associ-
ated to the 71 domains.
Several clusters are linked to the same domain.
This can been explained by the closeness of some
of the domains. This is shown when they are hi-
erarchically classified; we obtain then 34 general
domains that regroup 1 to 5 domains each. We
also observe that most clusters are only linked to
one domain. The two methods give almost the
same results and show that the two classifications
are similar.
domain)cluster links links
link (similarity) (entropy)
no link 3
1!1 34 29
1!2 8 8
1!3 3 4
1!4 1 2
1!5 1
1!6 1
Table 4: Number of links between one domain
and the clusters
4.2 The TU point of view
4.2.1 A simple comparison
One domain and one cluster are associated to
each thematic unit. It is then possible to calcu-
late the number of domains and clusters which
partially or fully overlap. Table 5 represents the
intersection between the two partitions. For each
domain we chose the cluster which has the highest
intersection with the domain. Then we calculated
the percentage of thematic units of this domain
which are both in the domain and in this chosen
cluster.
coverage number of clusters
cov=100% 8
80%cov?100% 16
60%cov?80% 19
40%cov?60% 19
20%cov?40% 8
cov<20% 1
Table 5: Coverage rates of UT which are common
to each domain and those of the associated clus-
ters which correspond to the highest intersection
Height domains are identical to height clusters.
The lowest coverage (18%) is obtained for one
domain. The other seventy domains cover more
than 20% of the clusters.
4.2.2 Comparison with the  coefficient
In order to compare more precisely our two
classifications, we used the  coefficient as it was
done by Dietterich in (Dietterich, 2000) and as it
is often done in the field of remote sensing for ex-
ample. The  coefficient measures the degree of
agreement among several judgements and is ex-
pressed as follows:
k =

1
  
2
1  
2
(9)
where 
1
is the proportion of times that the
judgments agree and 
2
is the proportion of times
that we could expect the judgments to agree by
chance. As we are in a case of unsupervised clas-
sification whereas Dietterich?s work was about
supervised classification (building of decision
trees), we have first set a one-to-one mapping
between the semantic domains and the clusters.
This was done by a very simple procedure: we
computed the size of the intersection between
each cluster and each domain; then we iteratively
mapped the cluster and the domain that had the
largest intersection until each cluster was mapped
with a domain. Of course, this is not an optimal
procedure in order to ensure that the intersection
of each couple of classes is the largest one but it
can be considered as a baseline.
Then, the evaluation of the  coefficient was
done by building a matrix K  K, with K, the
number of classes (clusters or domains), such that
each element k
i;j
is equal to the number of TUs
assigned to the class i by SEGAPSITH and to the
class j by the entropy-based clustering. 
1
, which
estimates the probability that the two classifica-
tions agree, is defined by:

1
=
P
K
i=1
k
i;i
N
(10)
where N is the total number of TUs. It eval-
uates the proportion of TUs that were put in the
same classes by the two clustering algorithms.

2
, which estimates the probability that the two
algorithms agree by chance, is given by:

2
=
K
X
i=1
(
k
i+
N

k
+i
N
) (11)
where k+i
N
and ki+
N
are the marginal distribu-
tions.
The  coefficient that results from the evalu-
ation of 
1
and 
2
is equal to 0 when the two
clustering algorithms agree only by chance and
to 1 when they really agree for each TU. Negative
values occur when there is a systematic disagree-
ment.
For the 71 classes of our test set, we computed
the  coefficient in two cases. First, with a ran-
dom mapping of the clusters and domains. We
got K = -0.013, which is very close to the agree-
ment by chance. Second, we applied the above
mapping procedure and got K = 0.484, which in-
dicates a significant correlation between the two
classifications. We think that with a more com-
plex mapping procedure, the  would be higher.
4.2.3 Application of the Mantel Test
In this paradigm, each classified thematic unit,
tu, is described according to its position in the
classification in relation to all the classified el-
ements. This position is characterized by a dis-
tance between tu and each other element. In the
work we present here, we choose a simple dis-
tance: dist(tu
1
; tu
2
) = 0 if tu
1
and tu
2
are part
of the same class; otherwise, dist(tu
1
; tu
2
) = 1.
However more complex distances may be used
when the classifications are hierarchical ones for
example. After this first step, each tu
i
of the two
classifications to compare is characterized by a
vector, each element of which, d
ij
, is equal to
the distance between tu
i
and tu
j
. Hence, each
classification is characterized by a distance ma-
trix, which is a square symmetric matrix of size
N
2
= 4935
2
. Comparing the two classifica-
tions amounts to compare their distance matrices.
In the ecology field, such kind of comparison is
achieved by a statistical test, called the Mantel
test (Mantel, 1967). In (Legendre, 2000), Legen-
dre defines the Mantel test as ? a procedure to test
the hypothesis that the distances among objects
in a matrix A are linearly independent of the dis-
tances among the same objects in another matrix
B. The result of this test may be used as support
for or against the hypothesis that the process that
generated the first set of distances is independent
of the process that generated the second set. The
unique feature of the Mantel test is the use of a
linear statistic to assess the relationship between
two distance matrices?. The basic statistic used in
the Mantel test is the Z statistic:
ZS =
i=N
X
i=1
j=N
X
j=1
x
ij
y
ij
As the elements of a distance matrix are not in-
dependent, the significance of ZS, the Z statistic
that is computed for the two distance matrices to
compare, is evaluated by comparing this value to
the Z statistic that is computed for matrices whose
rows and columns are randomly permuted. A dis-
tribution of random values is obtained by comput-
ing the statistic for many permuted matrices and
if ZS is significantly above this distribution, the
hypothesis that the two matrices are independent
is rejected 1.
1The Z statistic is maximal when the two distance matri-
ces are identical: the x
ij
y
ij
term is not equal to zero only if
x
ij
and y
ij
are equal to 1. Hence, each difference that could
be introduced between the two matrices, decreases its value.
As an exploratory step, we applied the Man-
tel test in order to compare the results of the two
classification methods we presented in this arti-
cle. We used the software developed by Adam
Liedloff (Liedloff, 1999). As the number of
TUs is too large in comparison with the capabil-
ities of this software, we experimented the Man-
tel test only on a subset of 1000 TUs. With the
distance matrix computed from the results of the
two classification methods, we got a Z statistic
(ZS) equal to 940; 894. The maximum value of
ZS is 978; 460 for the domains and 948; 608 for
the clusters. The random distribution was built
from 99 permuted matrices and its ZS value is
937; 708  232. As the proportion of the val-
ues from the random distribution that are above
ZS is equal to zero, we can reject the hypothe-
sis that the two matrices are independent and as a
consequence, we can think that the two compared
classifications are globally similar. However, as
the results of the Mantel test are not easy to inter-
pret, further tests must be performed to see what
are the relations between these results and those
of the other comparing methods and to determine
if this test is actually suited for comparing such
kind of classifications.
5 Conclusion
We presented in this paper an approach for eval-
uating the results of an unsupervised learning
method, when no human evaluation is possible or
when no classification exists as a reference. As
a result, this method builds classes of weighted
words that regroup thematic units. We defined
in a previous work a stability threshold of these
classes, thus we aim at evaluating this subset of
classes. To do that, we applied another clustering
method that only needs to know the number of
classes to build on the same subset of TUs and we
reformulate our evaluation problem in comparing
the two classifications. Our first results lead to
show a great correlation between the two results.
We now have to develop other tests, for exam-
ple on a different number of classes, to verify our
first results. A second step will be to evaluate the
methods on the same task, as a classification task
for example, whose protocol has to be defined.
References
T. Cover and J. Thomas. 1991. Elements of Informa-
tion Theory. Wiley & sons, New York.
T. G. Dietterich. 2000. An experimental comparison
of three methods for constructing ensembles of de-
cision trees: Bagging, boosting, and randomization.
Machine Learning, 40:139?158.
O. Ferret and B. Grau. 1998a. A thematic segmen-
tation procedure for extracting semantic domains
from texts. In ECAI, Brighton, UK.
O. Ferret and B. Grau. 1998b. Structuration d?un
re?seau de cooccurrences lexicales en domaines se?-
mantiques par analyse de textes. In NLPIA, Monc-
ton, Canada.
M. Jardino. 2000. Unsupervised non-hierarchical
entropy-based clustering. In H.A.L.Kiers, J.-
Rasson, P.J.F.Groenen, and M.Schader, editors,
Data Analysis, Classification, and Related Meth-
ods. Springer.
H. Kozima. 1993. Text segmentation based on sim-
ilarity between words. In ACL (Student Session),
Ohio, USA.
P. Legendre. 2000. Comparison of permutation meth-
ods for the partial correlation and partial correla-
tion and partial mantel tests. Statistical Computa-
tion and Simulation, 67:37?73.
Liedloff. 1999. Mantel nonparametric test calculator.
http://www.sci.qut.edu.au/nrs/mantel.htm.
C.-Y. Lin. 1997. Robust Automated Topic Identifica-
tion. Ph.D. thesis, University of Southern Califor-
nia.
N. Mantel. 1967. The detection of disease cluster-
ing and a generalized regression approach. Cancer
Res., 27:209?220.
I.T. Witten and T.C. Bell. 1991. The zero-frequency
problem: estimating the probabilities of novel
events in adaptative text compression. IEEE Trans-
actions on Information Theory, 37(3):1085?1094.
Terminological variants for document selection and
question/answer matching
Olivier Ferret Brigitte Grau Martine Hurault-Plantet
Gabriel Illouz Christian Jacquemin
LIMSI-CNRS
Bat.508 Universit? ParisXI
91403 Orsay, France
{ferret, grau, mhp, gabrieli, jacquemin}@limsi.fr
Abstract
Answering precise questions requires
applying Natural Language techniques
in order to locate the answers inside
retrieved documents. The QALC
system, presented in this paper,
participated to the Question Answering
track of the TREC8 and TREC9
evaluations. QALC exploits an analysis
of documents based on the search for
multi-word terms and their variations.
These indexes are used to select a
minimal number of documents to be
processed and to give indices when
comparing question and sentence
representations. This comparison also
takes advantage of a question analysis
module and recognition of numeric and
named entities in the documents.
1 Introduction
The Question Answering (QA) track at TREC8
and TREC9 is due to the recent need for more
sophisticated paradigms in Information
Retrieval (IR). Question answering generally
refers to encyclopedic or factual questions that
require concise answers. But current IR
techniques do not yet enable a system to give
precise answers to precise questions. Question
answering is thus an area of IR that calls for
Natural Language Processing (NLP) techniques
that can provide rich linguistic features as
output. Such NLP modules should be deeply
integrated in search and matching components
so that answer selection can be performed on
such linguistic features and take advantage of
them. In addition, IR and NLP techniques have
to collaborate in the resulting system in order to
cope with large-scale and broad coverage text
databases while deriving benefit from added
knowledge.
We developed a system for question
answering, QALC, evaluated in the framework
of the QA tracks at TREC8 and TREC9. The
QALC system comprises NLP modules for
multi-word term and named entity extraction
with a specific concern for term conflation
through variant recognition. Since named entity
recognition has already been described
extensively in other publications (Baluja 1999),
we present the contribution of terminological
variants to adding knowledge to our system.
The two main activities involving
terminology in NLP are term acquisition and
term recognition. Basically, terms can be viewed
as a particular type of lexical data. Term
variation may involve structural, morphological,
and semantic transformations of single or multi-
words terms (Fabre and Jacquemin, 2000).
In this paper, we describe how QALC uses
high level indexes, made of terms and variants,
to select among documents the most relevant
ones with regard to a question, and then to
match candidate answers with this question. In
the selection process, the documents first
retrieved by a search engine, are then
postfiltered and ranked through a weighting
scheme based on high level indexes, in order to
retain the top ranked ones. Similarly, all systems
that participated in TREC9 have a search engine
component that firstly selects a subset of the
provided database of about one million
documents. Since a search engine produces a
ranked list of relevant documents, systems then
have to define the highest number of documents
to retain. Indeed, having too many documents
leads to a question processing time that is too
long, but conversely, having too few documents
reduces the possibility of obtaining the correct
answer. For reducing the amount of text to
process, one approach consists of keeping one or
more relevant text paragraphs from each
document retrieved. Kwok et al(2000), for
instance use an IR engine that retrieves the top
300 sub-documents of about 300-550 words and,
on the other hand, the FALCON system
(Harabagiu et al 2000) performs a paragraph
retrieval stage after the application of a boolean
retrieval engine. These systems work on the
whole database and apply a bag-of-words
technique to select passages whereas QALC first
retains a large subset of documents, among
which it then selects relevant documents by
applying richer criteria based on the use of the
linguistic structures of the words.
QALC indexes, used for document selection,
are made of single and multi-word terms
retrieved by a 2-step procedure: (1)?automatic
term extraction from questions through part-of-
speech tagging and pattern matching and
(2)?automatic document indexing through term
recognition and variant conflation. As a result,
linguistic variation is explicitly addressed
through the exploitation of word paradigms,
contrarily to other approaches like the one taken
in COPSY (Schwarz 1988) where an
approximate matching technique between the
query and the documents implicitly takes it into
account. Finally, terms acquired at step?(1) and
indexes from step?(2) are also used by the
matching procedure between a question and the
relevant document sentences.
In the next section, we describe the
architecture of the QALC system. Then, we
present the question processing for term
extraction. We continue with the description of
FASTR, a transformational shallow parser that
recognizes and marks the extracted terms as well
as their linguistic variants within the documents.
The two following sections present the modules
of the QALC system where terms and variants
are used, namely the document selection and
question/answer matching modules. Finally, we
present the results obtained by the QALC
system as well as an evaluation of the
contribution of this NLP technique to the QA
task through the use of the reference collections
for the QA track. In conclusion, suggestions for
more ambitious, but still realistic, developments
using NLP are outlined.
2 System Overview
Natural Language Processing components in the
QALC system (see Figure 1) enrich the selected
documents with terminological indexes in order
to go beyond reasoning about single words. Rich
linguistic features are also used to deduce what a
question is about.
Tagged Questions:
Named entity tags
Vocabulary &
  frequencies
Named entity
 recognition
Candidate
terms
Retrieved
documents
Tagged sentences: named entity
    tags and term indexation
Ordered sequences of 250 and
           50 characters
Question analysis Search engine
Questions
Subset of ranked documents
Corpus
Re-indexing and selection of
      documents (FASTR)
Question/Sentence pairing
Figure 1. The QALC system
The analysis of a question relies on a shallow
parser which spots discriminating patterns and
assigns categories to the question. The
categories correspond to the types of entities that
are likely to constitute the answer to the
question.
In order to select the best documents from
the results given by the search engine and to
locate the answers inside them, we work with
terms and their variants, i.e. morphologic,
syntactic and semantic equivalent expressions.
A term extractor has been developed, based on
syntactic patterns which describe complex
nominal phrases and their subparts. These terms
are used by FASTR (Jacquemin 1999), a
shallow transformational natural language
analyzer that recognizes their occurrences and
their variants. Each occurrence or variant
constitutes an index that is subsequently used in
the processes of document ranking and
question/document matching.
Documents are ordered according to a weight
computed thanks to the number and the quality
of the terms and variants they contain. For
example, original terms with proper names are
considered more reliable than semantic variants.
An analysis of the weight graph enables the
system to select a relevant subpart of the
documents, whose size varies along the
questions. This selection takes all its importance
when applying the last processes which consist
of recognizing named-entities and analyzing
each sentence to decide whether it is a possible
answer or not. As such processes are time
consuming we attempt to limit their application
to a minimal number of documents.
Named entities are recognized in the
documents and used to measure the similarity
between the document sentences and a question.
Named entities receive one of the following
types: person, organization, location (city or
place), number (a time expression or a number
expression). They are defined in a way similar to
the MUC task and recognized through a
combination of lexico-syntactic patterns and
significantly large lexical data.
Finally, the question/answer matching
module uses all the data extracted from the
questions and the documents by the preceding
modules. We developed a similarity measure
that attributes weights to each characteristic, i.e.
named entity tags and terms and variants, and
makes a combination of them. The QALC
system proposes long and short answers.
Concerning the short ones, the system focuses
on parts of sentences that contain the expected
named entity tags, when they are known, or on
the largest subpart without any terms of the
question.
3 Terms and Variants
3.1 Term extraction
For automatic acquisition of terms from
questions, we use a simple technique of filtering
through patterns of part-of-speech categories.
No statistical ranking is possible because of the
small size of the questions from which terms are
extracted. First, questions are tagged with the
help of the TreeTagger (Schmid 1999). Patterns
of syntactic categories are then used to extract
terms from the tagged questions. They are very
close to those described by Justeson and
Katz?(1995), but we do not include post-posed
prepositional phrases. The pattern used for
extracting terms is:
(((((JJ | NN | NP | VBG)) ? (JJ | NN | NP | VBG) (NP
| NN))) | (VBD) | (NN) | (NP) | (CD))
where NN are common nouns, NP proper nouns,
JJ adjectives, VBG gerunds, VBD past
participles and CD numeral determiners.
The longest string is acquired first and
substrings can only be acquired if they do not
begin at the same word as the superstring. For
instance, from the sequence nameNN ofIN theDT
USNP helicopterNN pilotNN shotVBD downRP,
the following four terms are acquired: U S
helicopter pilot, helicopter pilot, pilot, and
shoot.
The mode of acquisition chosen for terms
amounts to considering only the substructures
that correspond to an attachment of modifiers to
the leftmost constituents (the closest one). For
instance, the decomposition of US helicopter
pilot into helicopter pilot and pilot is equivalent
to extracting the subconstituents of the structure
[US [helicopter [pilot]]].
3.2 Variant recognition through FASTR
The automatic indexing of documents is
performed by FASTR (Jacquemin 1999), a
transformational shallow parser for the
recognition of term occurrences and variants.
Terms are transformed into grammar rules and
the single words building these terms are
extracted and linked to their morphological and
semantic families.
The morphological family of a single word w
is the set M(w) of terms in the CELEX database
(CELEX 1998) which have the same root
morpheme as w. For instance, the morphological
family of the noun maker is made of the nouns
maker, make and remake, and the verbs to make
and to remake.
The semantic family of a single word w is the
union S (w ) of the synsets  of WordNet1.6
(Fellbaum 1998) to which w belongs. A synset is
a set of words that are synonymous for at least
one of their meanings. Thus, the semantic family
of a word w is the set of the words w' such that
w' is considered as a synonym of one of the
meanings of w. The semantic family of maker,
obtained from WordNet1.6, is composed of
three nouns: maker, manufacturer, shaper and
the semantic family of c a r is car, auto,
automobile, machine, motorcar.
Variant patterns that rely on morphological
and semantic families are generated through
metarules. They are used to extract terms and
variants from the document sentences in the
TREC corpus. For instance, the following
pattern, named NtoSemArg, extracts the
occurrence making many automobiles as a
variant of the term car maker:
VM('maker') RP? PREP? (ART (NN|NP)? PREP)?
ART? (JJ?|?NN?|?NP |?VBD?|?VBG)[0-3] NS('car')
where RP are particles, PREP prepositions, ART
articles, and VBD, VBG verbs. VM('maker') is
any verb in the morphological family of the
noun maker and NS('car') is any noun in the
semantic family of car.
Relying on the above morphological and
semantic families, auto maker, auto parts
maker , car manufacturer, make autos, and
making many automobiles are extracted as
correct variants of the original term car maker
through the set of metarules used for the QA
track experiment. Unfortunately, some incorrect
variants are extracted as well, such as make
those cuts in auto produced by the preceding
metarule.
3.3 Document selection
The output of NLP-based indexing is a list of
term occurrences composed of a document
identifier d, a term identifier?a pair t(q,i)
composed of a question number q and a unique
index i?, a text sequence, and a variation
identifier v (a metarule). For instance, the
following index :
LA092690-0038 t(131,1)
making many automobiles NtoVSemArg
means that the occurrence making many
automobiles from document d=LA092690-0038
is obtained as a variant of term i=1 in question
q=131 (car maker) through the variation
NtoVSemArg given in Section 3.2.
Each document d selected for a question q is
associated with a weight. The weighting scheme
relies on a measure of quality of the different
families of variations described by
Jacquemin?(1999): non-variant occurrences are
weighted 3.0, morphological and morpho-
syntactic variants are weighted 2.0, and
semantic and morpho-syntactico-semantic
variants are weighted 1.0.
Since proper names are more reliable indices
than common names, each term t(q,i) receives a
weight P(t(q , i )) between 0 and 1.0
corresponding to its proportion of proper names.
For instance, President Cleveland's wife is
weighted 2/3=0.66. Since another factor of
reliability is the length of terms, a factor |t(q,i)|
in the weighting formula denotes the number of
words in term t(q,i). The weight Wq(d) of a
query q  in a document d  is given by the
following formula (1). The products of the
weightings of each term extracted by the indexer
are summed over the indices I(d) extracted from
document d and normalized according to the
number of terms |T(q)| in query q.
  
W (d)
( ) ( ( ( , ))) ( , )
( )
q
( ( , ), ) ( )
=
? + ?
?
? w v P t q i t q i
T q
t q i v I d
1 2
         (1)
Mainly two types of weighting curves are
observed for the retrieved documents: curves
with a plateau and a sharp slope at a given
threshold (Figure 2.a) and curves with a slightly
decreasing weight (Figure 2.b).
The edge of a plateau is detected by examining
simultaneously the relative decrease of the slope
with respect to the preceding one, and the
relative decrease of the value with respect to the
preceding one. When a threshold is detected, we
only select documents before this threshold,
otherwise a fixed cutoff threshold is used. In our
experiments, for each query q, the 200 best
ranked documents retrieved by the search
engine1 were subsequently processed by the re-
indexing module. Our studies (Ferret et al 2000)
show that 200 is a minimum number such as
almost all the relevant documents are kept.
When no threshold was detected, we fixed the
value of the threshold to 100.
0
0
10
10
20
20
30
30
40
40
50
50
60
60
70
70
80
80
90
90
100
100
0
0
1
1
2
2
3
3
4
4
5
5
6
6
7
8
9
10
rank of the document
w
ei
gh
t
Question #87
rank of the document
Truncation of the ranked list
Question #86
w
ei
gh
t
(a)
(b)
Figure 2. Two types of weighting curve.
Through this method, the cutoff threshold is
8 for question #87 (Who followed Willy Brandt
as chancellor of the Federal Republic of
Germany?, Figure 2(a))2 and 100 for question
#86 (Who won two gold medals in skiing in the
Olympic Games in Calgary?, Figure 2(b)). As
indicated by Figure??2(a), there is an important
difference of weight between documents #8 and
#9. The weight of document #8 is 9.57 while the
                                                           
1 We used in particular Indexal (Loupy et al1998), a search
engine provided by Bertin Technologie.
2 Questions come from the TREC8 data.
weight of document #9 is 7.29 because the term
Federal Republic only exists in document #8.
This term has a high weight because it is
composed of two proper names.
4 Question-Answer Matching
4.1 Question type categorization
Question type categorization is performed in
order to assign features to questions and use
these features for the similarity measurement
between a question and potential answer
sentences. Basically, question categorization
allows the prediction of the kind(s) of answer,
called target (for instance, NUMBER).
Sentences inside the retrieved documents are
labeled with the same tags as questions. During
the similarity measurement, the more the
question and a sentence share the same tags, the
more they are considered as involved in a
question-answer relation. For example:
Question:
How many people live in the Falklands?
?> target = NUMBER
Answer:
F a l k l a n d s  p o p u l a t i o n  o f  <bnumex
TYPE=NUMBER> 2,100 <enumex> is
concentrated.
We established 17 types of answer. Some
systems define more categories. For instance
Prager et al (2000) identify about 50 types of
answer.
4.2 Answer Selection
In the QALC system, we have taken the
sentence as a basic unit because it is large
enough to contain the answer to questions about
simple facts and to give a context that permits
the user to judge if the suggested answer is
actually correct. The module associates each
question with the Na most similar sentences (Na
is equal to 5 for the QA task at TREC).
The overall principle of the selection process
is the following: each sentence from the
documents selected for a question is compared
with this question. To perform this comparison,
sentences and questions are turned into vectors
that contain three kinds of elements: content
words, term identifiers and named entity tags. A
specific weight (between 0 and 1.0) is associated
with each of these elements in order to express
their relative importance.
The content words are the lemmatized forms
of mainly adjectives, verbs and nouns such as
they are given by the TreeTagger. Each content
word in a vector is weighted according to its
degree of specificity in relation to the corpus in
which answers are searched through the tf.idf
weighting scheme. For questions, the term
identifiers refer to the terms extracted by the
term extractor described in Section?3.1 and
receive a fixed weight. In sentence vectors, term
identifiers are associated with the normalized
score from the ranking module (see Section 3.3).
The named entity tags correspond to the possible
types of answers, provided by the question
analysis module. In each sentence these tags
delimit the named entities that were recognized
by the corresponding module of the QALC
system and specify their type. Unlike term
identifiers, named entity tags are given the same
fixed weight in both sentence and question
vectors because the matching module uses the
types of the named entities and not their values.
In our experiments, the linguistic features
(terms and named entities) are used to favor
appropriate sentences when they have not
enough content words in common with the
question or when the question only contains a
few content words. Thus, the weights of term
identifiers or named entity tags are reduced by
applying a coefficient in order to be globally
lower than the weights of the content words.
Finally, the comparison between a sentence
vector Vd and a question vector Vq is achieved
by computing the following similarity measure:
?
?
=
j j
i i
dq
wq
wd
VVsim ),( (2)
where wqj is the weight of an element in the
question vector and wdi is the weight of an
element in a sentence vector that is also in the
question vector. This measure evaluates the
proportion and the importance of the elements in
the question vector that are found in the
sentence vector with regards to all the elements
of the question vector. Moreover, when the
similarity value is nearly the same for two
sentences, we favor the one in which the content
words of the question are the least scattered.
The next part gives an example of the
matching operations for the TREC8 question
Q16 What two US biochemists won the Nobel
Prize in medicine in 1992? This question is
turned into the following vector:
two (1.0) US (1.0) biochemist (0.9)
nobel (1.0) prize (0,6) medicine (0,5)
win (0,3) 1992 (1.0) <PERSON> (0.5)
16.01 (0.5) 16.04 (0.5)
where <PERSON> is the expected type of the
answer, 16.01 is the identifier of the U S
biochemist term and 16.04 is the identifier of the
Nobel Prize term.
The same kind of vector is built for the
sentence <NUMBER> Two </NUMBER> US
biochemists, <PERSON> Edwin Krebs
</PERSON> and <CITY> Edmond </CITY>
Fischer, jointly won the <NUMBER> 1992
</NUMBER> Nobel Medicine Prize for work
that could advance the search for an anti-cancer
drug, coming from the document FT924-14045
that was selected for the question Q163 :
two (1.0) US (1.0) biochemist (0.9)
nobel (1.0) prize (0,6) medicine (0,5)
win (0,3) 1992 (1.0) Edwin (0.0)
Krebs (0.0) Edmond (0.0) Fischer (0.0)
work (0.0) advance (0.0) search (0.0)
anti-cancer (0.0) jointly (0.0) drug (0.0)
<PERSON> (0.5) <NUMBER> (0.0) <CITY>(0.0)
16.01 (0.5) 16.04 (0.3)
where the weight 0.0 is given to the elements
that are not part of the question vector. The term
US biochemist is found with no variation and
Nobel Prize appears as a syntactic variant.
Finally, according to (2), the similarity measure
between theses two vectors is equal to 0.974.
5 Results and Evaluation
We sent to TREC9 three runs whose variations
concern the searched engine used and the length
of the answer (250 or 50 characters). Among
those runs, the best one obtained a score of
0.407 with 375 correct answers among 682
questions, for answers of 250 characters length.
The score computed by NIST is the reciprocal
mean of the rank, from 1 to 5, of the correct
                                                           
3 This sentence is taken from the output of the named entity
recognizer.
answer. With this score, the QALC system was
ranked 6th among 25 participants at TREC 9
QA task.
Document selection relies on a quantitative
measure, i.e. the document weight, whose
computation is based on syntactic and semantic
indices, i.e. the terms and the terminological
variants. Those indices allow the system to take
into account words as well as group of words
and their internal relations within the
documents. Following examples, that we have
got from selected documents for TREC9 QA
task, show what kind of indices are added to the
question words.
For the question 252 When was the first flush
toilet invented? , one multi-word extracted term
is flush toilet. This term is marked by FASTR
when recognized in a document, but it is also
marked when a variant is found, as for instance
low-flush toilet in the following document
sentence where low-flush is recognized as
equivalent to flush:
Santa Barbara , Calif. , is giving $ 80 to
anyone who converts to a low-flush toilet.
252.01   flush toilet[JJ][NN]
             low-flush[flush][JJ] toilet[toilet][NN]
             1.00
In the given examples, after the identification
number of the term, appears the reference term,
made of the lemmatized form of the words and
their syntactic category, followed by the variant
found in the sentence, with each word, its
lemmatized form and its category, and finally its
weight.
In the example above, the term found in the
sentence is equivalent to the reference term, and
thus its weight is 1.00.
The second example shows a semantic
variant. Salary and average salary are terms
extracted from the question 337, What's the
average salary of a professional baseball player
?. The semantic variant pay, got from WordNet,
was recognized in the following sentence?:
Did the NBA union opt for the courtroom
because its members, whose average pay tops
$500000 a year, wouldn't stand still for a
strike over free agency ?
337.01    salary[NN] pay[pay][NN] 0.25
337.00    average [JJ]salary[NN]
               average[average][JJ] pay[pay][NN]
               0.40
In order to evaluate the efficiency of the
selection process, we proceeded to several
measures. We apply our system on the material
given for the TREC8 evaluation, one time with
the selection process, and another time without
this process. At each time, 200 documents were
returned by the search engine for each of the 200
questions. When selection was applied, at most
100 documents were selected and subsequently
processed by the matching module. Otherwise,
the 200 documents were processed. The system
was scored by 0.463 in the first case, and by
0.452 in the second case. These results show
that the score increases when processing less
documents above all because it is just the
relevant documents that are selected.
The benefit from performing such a selection
is also illustrated by the results given in Table 1,
computed on the TREC9 results.
Number of documents selected
by ranking
100 <<100
Distribution among the
questions
342
(50%)
340
(50%)
Number of correct answers 175
(51%)
200
(59%)
Number of correct answer at
rank 1
88
(50%)
128
(64%)
Table 1. Evaluation of the ranking process
We see that the selection process discards a
lot of documents for 50% of the questions (340
questions are processed from less than 100
documents). The document set retrieved for
those questions had a weighting curve with a
sharp slope and a plateau as in Figure 2(a).
QALC finds more often the correct answer and
in a better position for these 340 questions than
for the 342 remaining ones. The average number
of documents selected, when there are less than
100, is 37. These results are very interesting
when applying such time-consuming processes
as  named ent i ty  recogni t ion and
question/sentence matching. Document selection
will also enable us to apply later on syntactic
and semantic sentence analysis.
6 Conclusion
The goal of a question-answering system is to
find an answer to a precise question, with a
response time short enough to satisfy the user.
As the answer is searched within a great amount
of documents, it seems relevant to apply mainly
numerical methods because they are fast. But, as
we said in the introduction, precise answers
cannot be obtained without adding NLP tools to
IR techniques. In this paper, we proposed a
question answering system which uses
terminological variants first to reduce the
number of documents to process while
increasing the system performance, and then to
improve the matching between a question and its
potential answers. Furthermore, reducing the
amount of text to process will afterwards allow
us to apply more complex methods such as
semantic analysis. Indeed, TREC organizers
foresee a number of possible improvements for
the future?: real-time answering, evaluation and
justification of the answer, completeness of the
answer which could result from answers
distributed along multiple documents, and
finally interactive question answering so that the
user could specify her/his intention. All those
improvements require more data sources as well
as advanced reasoning about pragmatic and
semantic knowledge.
Thus, the improvements that we now want to
bring to our system will essentially pertain to a
semantic and pragmatic approach. For instance,
WordNet that we already use to get the semantic
variants of a word, will be exploited to refine
our set of question types. We also plan to use a
shallow syntactico-semantic parser in order to
construct a semantic representation of both the
potential answer and the question. This
representation will allow QALC to select the
answer not only from the terms and variants but
also from the syntactic and semantic links that
terms share with each other.
References
Baluja, S., Vibhu O. M., Sukthankar, R. 1999
Applying machine learning for high performance
named-entity extraction. P r o c e e d i n g s
PACLING'99 Waterloo, CA. 365-378.
CELEX. 1998.
http://www.ldc.upenn.edu/readme_files/celex.read
me.html. Consortium for Lexical Resources,
UPenns, Eds.
Fabre C., Jacquemin C, 2000. Boosting variant
recognition with light semantics. Proceedings
COLING?2000, pp. 264-270, Luxemburg.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA, MIT Press.
Ferret O., Grau B., Hurault-Plantet M., Illouz G.,
Jacquemin C. (2000), QALC ? the Question-
Answering system of LIMSI-CNRS, pre-
proceedings of TREC9, NIST, Gaithersburg, CA.
Harabagiu S., Pasca M., Maiorano J. 2000.
Experiments with Open-Domain Textual Question
Answering. Proceedings of  Coling'2000,
Saarbrucken, Germany.
Jacquemin C. 1999. Syntagmatic and paradigmatic
representations of term variation. Proceedings of
ACL'99. 341-348.
Justeson J., Katz S. 1995. Technical terminology:
some linguistic properties and an algorithm for
identification in texte. Natural Language
Engineering. 1: 9-27.
Kwok K.L., Grunfeld L., Dinstl N., Chan M. 2000.
TREC9 Cross Language, Web and Question-
Answering Track experiments using PIRCS. Pre-
proceedings of TREC9, Gaithersburg, MD, NIST
Eds. 26-35.
Loupy C. , Bellot P., El-B?ze M., Marteau P.-F..
Query Expansion and Classification of Retrieved
Documents, TREC (1998), 382-389.
Prager J., Brown, E., Radev, D., Czuba, K. (2000),
One Search Engine or two for Question-
Answering, NISTs, Eds., Proceedings of TREC9,
Gaithersburg, MD. 250-254.
Schmid H. 1999. Improvments in Part-of-Speech
Tagging with an Application To German.
Natural?Language Processing Using Very Large
Corpora, Dordrecht, S. Armstrong, K. W. Chuch,
P. Isabelle, E. Tzoukermann,  D. Yarowski, Eds.,
Kluwer Academic Publisher.
Schwarz C. 1988. The TINA Project: text content
analysis at the Corporate Research Laboratories at
Siemens. Proceedings of Intelligent Multimedia
Information Retrieval Systems and Management
(RIAO?88) Cambridge, MA. 361-368.
Evaluation and Improvement
of Cross-Lingual Question Answering Strategies
Anne-Laure Ligozat and Brigitte Grau and Isabelle Robba and Anne Vilnat
LIMSI-CNRS
91403 Orsay Cedex, France
firstname.lastname@limsi.fr
Abstract
This article presents a bilingual question
answering system, which is able to process
questions and documents both in French
and in English. Two cross-lingual strate-
gies are described and evaluated. First, we
study the contribution of biterms trans-
lation, and the influence of the comple-
tion of the translation dictionaries. Then,
we propose a strategy for transferring the
question analysis from one language to the
other, and we study its influence on the
performance of our system.
1 Introduction
When a question is asked in a certain language
on the Web, it can be interesting to look for the
answer to the question in documents written in
other languages in order to increase the number of
documents returned. The CLEF evaluation cam-
paign for cross-language question answering sys-
tems addresses this issue by encouraging the deve-
lopment of such systems.
The objective of question answering systems
is to return precise answers to natural-language
questions, instead of the list of documents usually
returned by a search engine. The opening to mul-
tilingualism of question answering systems raises
issues both for the Information Retrieval and the
Information Extraction points of view.
This article presents a cross-language question
answering system able to treat questions and docu-
ments either in French or in English. Two different
strategies for shifting language are evaluated, and
several possibilities of evolution are presented.
2 Presentation of our question answering
system
Our bilingual question answering system has
participated in the CLEF 2005 evaluation cam-
paign 1. The CLEF QA task aims at evaluating dif-
ferent question answering systems on a given set
of questions, and a given corpus of documents, the
questions and the documents being either in the
same language (except English) or in two diffe-
rents languages. Last year, our system participated
in the French to English task, for which the ques-
tions are in French and the documents to search in
English.
This system is composed of several modules
that are presented Figure 1. The first module ana-
lyses the questions, and tries to detect a few of
their characteristics, that will enable us to find the
answers in the documents. Then the collection is
processed thanks to MG search engine 2. The do-
cuments returned are reindexed according to the
presence of the question terms, and more preci-
sely to the number and type of these terms ; next,
a module recognizes the named entities, and the
sentences from the documents are weighted accor-
ding to the information on the question. Finally,
different processes are applied depending on the
expected answer type, in order to extract answers
from the sentences.
3 Cross-language strategies for question
answering systems
Two main approaches are possible to deal with
multilingualism in question answering systems :
1Multilingual Question Answering task at the Cross Lan-
guage Evaluation Forum, http ://clef-qa.itc.it/
2MG for Managing Gigabytes
http ://www.cs.mu.oz.au/mg/
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
23
English
terms
Fusion
English
answers
English
questions
   
Collection
French
questions
   Selection
   Named entity tagging
Answer extraction
   Reindexing and ranking
   Sentence weighting
Document processing
   Answer extraction
English
   Focus 
   Answer type
   Semantically linked words
   Main verb
   Terms
   Syntactic relations
Question analysis
translation
answers
2 lists of ranked
                  (a)
                 (b)
Search
engine
FIG. 1 ? Architecture of our cross-language question answering system
question translation and term-by-term translation.
These approaches have been implemented and
evaluated by many systems in the CLEF evalua-
tions, which gives a wide state-of-the-art of this
domain and of the possible cross-language strate-
gies.
The first approach consists in translating the
whole question into the target language, and then
processing the question analysis in this target lan-
guage. This approach is the most widely used, and
has for example been chosen by the following sys-
tems : (Perret, 2004), (Jijkoun et al, 2004), (Neu-
mann and Sacaleanu, 2005), (de Pablo-Sa?nchez et
al., 2005), (Tanev et al, 2005). Among these sys-
tems, several have measured the performance loss
between their monolingual and their bilingual sys-
tems. Thus, the English-French version of (Perret,
2004) has a 11 % performance loss (in terms of ab-
solute loss), dropping from 24.5% to 13.5% of cor-
rect answers. The English-Dutch version of (Jij-
koun et al, 2004)?s system has an approximative
10% performance loss of correct answers : the per-
centage of correct answers drops from 45.5% to
35%. As for (de Pablo-Sa?nchez et al, 2005), they
lose 6% of correct answers between their Spanish
monolingual system and their English-Spanish bi-
lingual system. (Hartrumpf, 2005) also conducted
an experiment by translating the questions from
English to German, and reports a drop from about
50% of performance.
For their cross-language system, (Neumann and
Sacaleanu, 2004) chose to use several machine
translation tools, and to gather the different trans-
lations into a ?bag of words? that is used to ex-
pand queries. Synonyms are also added to the
?bag of words? and EuroWordNet 3 is used to
3Multilingual database with wordnets for several Euro-
disambiguate. They lose quite few correct ans-
wers between their German monolingual system
and their German-English bilingual system, with
which they obtain respectively 25 and 23.5% of
correct answers.
Translating the question raises two main pro-
blems : syntactically incorrect questions may be
produced, and the resolution of translation am-
biguities may be wrong. Moreover, the unknown
words such as some proper names are not or in-
correctly translated. We will describe later several
possibilities to deal with these problems, as well
as our own solution.
Other systems such as (Sutcliffe et al, 2005) or
(Tanev et al, 2004) use a term-by-term translation.
In this approach, the question is analyzed in the
source language and then the information retur-
ned by the question analysis is translated into the
target language. (Tanev et al, 2004), who partici-
pated in the Bulgarian-English and Italian-English
tasks in 2004, translate the question keywords by
using bilingual dictionaries and MultiWordNet 4.
In order to limit the noise stemming from the dif-
ferent translations and to have a better cohesion,
they validate the translations in two large cor-
pora, AQUAINT and TIPSTER. This system got a
score of 22.5% of correct answers in the bilingual
task, and 28% in the monolingual task in 2004.
(Sutcliffe et al, 2005) combine two translation
tools and a dictionary to translate phrases. Even-
tually, (Laurent et al, 2005) also translate words
or idioms, by using English as a pivot language.
The performance of this system is of 64% of cor-
rect answers for the French monolingual task, and
pean languages, http ://www.illc.uva.nl/EuroWordNet/
4Multilingual lexical database in which the Italian Word-
Net is strictly aligned with Princeton WordNet, http ://multi-
wordnet.itc.it
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
24
39.5% for the English-French bilingual task.
4 Adopted approach
In order to deal with the conversion from French
to English in our system, two strategies are ap-
plied in parallel. They differ on what is translated
to treat the question asked in French. The first sub-
system called MUSQAT proceeds to the question
analysis in French, and then translates the ques-
tion terms extracted by this question analysis mo-
dule, following the - - - arrows in Figure 1. The
second sub-system makes use of a machine trans-
lation tool (Reverso 5) to obtain translations of the
questions and then our English monolingual sys-
tem called QALC is applied, following the ..-.. ar-
rows in Figure 1 . These strategies will be detailed
later in the article.
If they represent the most common strategies for
this kind of task, an original feature of our system
is the implementation of both strategies, which en-
ables us to merge the results obtained by following
these strategies, in order to improve the global per-
formance of our system.
In Table 1, we present an analysis of the results
we obtained for the CLEF evaluation campaign.
We evaluate the results obtained at two different
points of the question-answering process, i.e. af-
ter the sentence selection (point (a) in Figure 1),
and after the answer extraction (point (b) in Fi-
gure 1). At point (a), we count how many ques-
tions (among the global evaluation set of 200 ques-
tions) have an appropriate answer in the first five
sentences. At point (b), we distinguish the answers
the analysis process labels as named entities (NE),
from the others, since the corresponding answe-
ring processes are different. We also detail how
many answers are ranked first, or in the first five
ranks, as we take into account the first five ans-
wers.
As illustrated in Table 1, the two strategies for
dealing with multilingualism give quite different
results, which can be explained by each strategy
characteristics.
MUSQAT proceeds to the question analysis
with French questions correctly expressed, and
which analysis is therefore more reliable. Yet, the
terms translations are then obtained from every
possible translation of each term, and thus without
taking account any context ; moreover, they de-
pend on the quality of the dictionaries used, and
5http ://www.reverso.net/
MUSQAT Reverso
+QALC
% %
(a) : Sentences first 5 41 46
with an answer ranks
(b) : Correct rank 1 18 14
NE answers
first 5 26 17
ranks
(b) : Correct rank 1 16 13
other answers
first 5 23 20
ranks
(b) : Total rank 1 17 13
(NE + non NE)
first 5 24 19
ranks
Final result 19
(fusion of both strategies)
TAB. 1 ? Performance of our system in CLEF
2005
introduce noise because of the erroneous transla-
tions.
In MUSQAT, we do not only translate mono-
terms (i.e. terms composed of single word) : the
biterms (composed of two words) of the French
questions are also extracted by the question analy-
sis. Every sequence of two terms which are tagged
as adjective/common noun or proper noun/proper
noun... constitutes a biterm. Each word of the bi-
term is translated, and then the existence of the
corresponding biterm built in English is checked
in the corpus. The biterms thus obtained are then
used by the further modules of the system. Taking
biterms into account is useful since they provide
a minimal context to the words forming them, as
well for the translation as for the re-indexing and
re-ranking of the documents (see Figure 1), as ex-
plained in (Ferret et al, 2002). Moreover, the pre-
sence of the biterm translations in the corpus is a
kind of validation of the monoterms translations.
As for translating the question, which is imple-
mented by Reverso+QALC, it presents the advan-
tage of giving a unique translation of the question
terms, which is quite reliable. But the grammati-
cality or realism of the question are not assured,
and thus the question analysis, based on regular
expression patterns, can be disturbed.
In this work, we tried to evaluate each strategy
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
25
and to bypass their drawbacks : on the one hand
(Section 5), by examining how the biterm transla-
tion in MUSQAT could be more reliable, and on
the other hand (Section 6) by improving the ques-
tion analysis, by relying on the French questions,
for QALC.
5 Biterm translation
The translation of terms and biterms present in
the question is achieved using two dictionaries.
The first of them, which was used last year for
our participation to CLEF is Magic-Dic 6. It is a
dictionary under GPL licence, which was retained
for its capacity to evolve. Indeed users can sub-
mit new translations which are controlled before
being integrated. Yet, it is quite incomplete. This
year we used FreeDict as well (FreeDict is also un-
der GPL licence), to fill in the gaps of Magic-Dic.
FreeDict added 424 translations to the 690 terms
already obtained. By mixing both sets of transla-
tions we obtained 463 additional biterms, making
a total of 777 biterms.
Nevertheless, whatever the quality and the size
of the dictionaries are, the problem of biterm trans-
lation remains the same : since biterms are not in
the dictionaries, the only way for us to get their
translation is to combine all the different term
translations. The main drawback of this approach
is the generated noise, for none of the terms consti-
tuting the biterm is disambiguated. For example,
three different translations are found for the bi-
term Conseil de de?fense : defense council, defense
advice and defense counsel ; but only the first of
those should be finally retained by our system.
To reduce this noise, an interesting possibility is
to validate the obtained biterms by searching them
or their variants in the complete collection of do-
cuments. (Grefenstette, 1999) reports a quite simi-
lar experiment in the context of a machine trans-
lation task : he uses the Web in order to order the
possible translations of noun phrases, and in par-
ticular noun biterms. Fastr (Jacquemin, 1996) is
a parser which takes as input a corpus and a list
of terms (multi or monoterms) and outputs the in-
dexed corpus in which terms and their variants are
recognized. Hence, Fastr is quite adequate for bi-
terms validation : it tags all the biterms present in
the collection, whether in their original form or in
a variant that can be semantic or syntactic.
In order to validate the biterms, the complete
6http ://magic-dic.homeunix.net
collection of the CLEF campaign (500 Mbyte) was
first tagged using the TreeTagger, then Fastr was
applied. The results are presented Table 2 : 39.5%
of the 777 biterms were found in the collection, in
a total of 63,404 occurrences. Thus there is an ave-
rage of 206 occurrences for each biterm. If we do
not take into account the biterm which is the most
represented (last year with 30,981 occurrences),
this average falls to 105. The 52 biterms which are
found in their original form only are most of the
time names of persons. Lastly, biterms that are ne-
ver found in their original form, are often consti-
tuted of one term badly translated, for example the
biterm oil importation is not present in the collec-
tion but its variant import of oil is found 28 times.
Then, it may be interesting to replace these biterms
by the most represented of their variants.
Whenever a biterm is thus validated (found in
the collection beyond a chosen threshold), the
translation of its terms is itself validated, other
translations being discarded. Thus, biterm valida-
tion enables us to validate monoterm translations.
Then, the following step will be to evaluate how
this new set of terms and biterms improves the re-
sults of MUSQAT.
After CLEF 2005 evaluation, we had at our dis-
posal the set of questions in their English original
version (this set was provided by the organizers).
We had also the English translation (far less cor-
rect) provided by the automatic translator Reverso.
As we can see it Table 3, for each set of ques-
tions the number of terms and biterms is nearly
the same. In the set of translations given by Re-
verso, we manually examined how many biterms
were false and found that here again the figures
were close to those of the original version. There
are two main reasons for which a biterm may be
false :
? in two thirds of cases, the association itself is
false : the two terms should not have been as-
sociated ; it is the case for example of many
country from the question How many coun-
tries joined the international coalition to res-
tore the democratic government in Haiti ? 7
? in one third of cases, one of the terms is
not translated or translated with an erroneous
term, like movement zapatiste coming from
the question What carry the courtiers of the
movement zapatiste in Mexico ? 8
7This sentence is an example of very good translation gi-
ven by Reverso
8This sentence is an example of bad translation given by
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
26
Total Number of biterms 777
Number of biterms found in the collection 307 - 39.5%
Number of biterms found in their original form only 52 - 17%
Number of biterms found with semantic variations only 150 - 54%
TAB. 2 ? Magic-Dic and FreeDict biterms validated by Fastr
Questions Questions Questions
in French translated in English in English
by Reverso (original version)
Terms 1180 1122 1163
Biterms 272 204 261
False Biterms 33 38 27
Common Biterms - 106
TAB. 3 ? Biterms in the different sets of questions
However, we calculated that among the 204 bi-
terms given by Reverso, 106 are also present in the
original set of questions in English. Among the 98
remaining biterms, 38 are false (for the reasons gi-
ven above). Then, there are 60 biterms which are
neither erroneous nor present in the original ver-
sion. Some of them contain a term which has been
translated using a different word, but that is never-
theless correct ; yet, most of these 60 biterms have
a different syntax from those constructed from the
original version, which is due to the syntax of the
questions translated by Reverso.
This leads us to conclude that even if Reverso
produces syntactically erroneous questions, the
vocabulary it chooses is most of the time adequate.
Yet, it is still interesting to use also the biterms
constructed from the dictionaries since they are
much more numerous and provide variants of the
biterms returned by Reverso.
6 Multilingual question analysis
We have developed for the evaluations a ques-
tion analysis in both languages. It is based on the
morpho-syntactic tagging and the syntactic analy-
sis of the questions. Then different elements are
detected from both analyses : recognition of the
expected answer type, of the question category, of
the temporal context...
There are of course lexicons and patterns which
are specific to each language, but the core of the
module is independent from the language. This
Reverso, which should have produced What do supporters of
the Zapatistas in Mexico wear ?
module was evaluated on corpora of similar ques-
tions in French and in English, and its results on
both languages are quite close (around 90% of re-
call and precision for the expected answer type
for example ; for more details, see (Ligozat et al,
2006)).
As presented above, our system relies on two
distinct strategies to answer to a cross-language
question :
? Either the question is analyzed in the ori-
ginal language, and next translated term-by-
term. The question analysis is then more re-
liable since it processes a grammatically cor-
rect question ; yet, the translation of terms has
no context to rely on.
? Or the question is first translated into the
target language before being analyzed. Al-
though this strategy improves the translation,
its main inconvenient is that each translation
error has strong consequences on the ques-
tion analysis. We will now try to evaluate to
which extent the translation errors actually
influence our question analysis and to find so-
lutions to avoid minimize this influence in the
Reverso+QALC system.
An error in the question translation can lead to
wrong terms or an incorrect English construction.
Thus, the translation of the question ?Combien y
a-t-il d?habitants en France ?? (?How many inhabi-
tants are there in France ??) is ?How much is there
of inhabitants in France ??.
In order to evaluate our second strategy, Re-
verso+QALC, using question translation and then
a monolingual system, it is interesting to estimate
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
27
the influence of a such a coarse translation on the
results of our system.
In order to avoid these translating problems, it
is possible to adapt either the input or the out-
put of the translating module. (Ahn et al, 2004)
present an example of a system processing pre-
and post-corrections thanks to surface reformu-
lation rules. However, this type of correction is
highly dependent on the kind of questions to pro-
cess, as well as on the errors of the translation tool
that is used.
We suggest to use another kind of processing,
which makes the most of the cross-lingual charac-
ter of the task, in order to improve the analysis of
the translated questions and to take into account
the possibilities of errors in these questions.
Our present system already takes into account
some of the most frequent translation errors, by
allowing the question analysis module to loosen
some of its rules in case the question be transla-
ted. Thus, a definition question such as ?Qu?est-
ce que l?UNITA ??, translated ?What UNITA ??
by our translating tool, instead of ?What is the
UNITA ??, will nevertheless be correctly analyzed
by our rules : indeed, the pattern WhatGN will be
considered as corresponding to a definition ques-
tion, while on a non-translated question, only the
pattern WhatBeGN will be allowed.
In order to try and improve our processing of
approximations in the translated questions, the so-
lution we suggest here consists in making the
question analysis in both the source and the target
languages, and in reporting the information (or at
least part of it) returned by the source analysis into
the target analysis. This is possible first because
our system treats both the languages in a parallel
way, and second, some of the information retur-
ned by the question analysis module use the same
terms in English and in French, like for example
the question category or the expected Named En-
tity type.
More precisely, we propose, in the task with
French questions and English documents, to ana-
lyse the French questions, and their English trans-
lations, and then to report the question category
and the expected answer type of the French ques-
tions into the English question analysis. The in-
formation found in the source language should be
more reliable since obtained on a real question.
For example, for the question ?Combien de
communaute?s Di Mambro a-t-il cre?e ?? (?How
many communities has Di Mambro created ??),
Reverso?s translation is ?How many Di Mambro
communities has he create ?? which prevents the
question analysis module to analyze it correctly.
The French analysis is thus used, which provides
the question category combien (how many) and the
expected named entity type NUMBER. This infor-
mation is reported in the English analysis file.
These characteristics of the question are used at
two different steps of the question answering pro-
cess : when selecting the candidate sentences and
when extracting the answers. Improving their re-
liability should then enable us to increase the num-
ber of correct answers after these two steps.
In order to test this strategy, we conducted an
experiment based on the CLEF 2005 FR-EN task,
and the 200 corresponding French questions. We
launched the question answering system on three
question files :
? The first question file (here called English
file) contained the original English questions
(provided by the CLEF organizers). This file
will be considered as a test file, since the re-
sults of our system on this file represent those
that would be reached without translation er-
rors.
? The second file (called Translated file) contai-
ned the translated questions analysis.
? The last file (called Improved file) contained
the same analysis, but for which the question
category and the expected answer type were
replaced by those of the French analysis.
Then we searched for the number of correct ans-
wers for each input question file after the sentence
selection and after the answer extraction. The re-
sults obtained by our system on each file are pre-
sented on Figure 2, Figure 3 and Figure 4. These
figures present the number of questions expecting
a named entity answer, expecting another kind of
answer, and the total number of questions, as well
as the results of our system on each type of ques-
tion : the number of correct questions are given at
the first five ranks, and at the first rank, first for the
sentences (?long answers?) and then for the short
answers.
These results show that the information trans-
fer from the source language to the target lan-
guage significantly improves the system?s results ;
the number of correct answers increases in every
case. It increases from 34 on the translated ques-
tions file to 36 on the improved file, and from 52
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
28
FIG. 2 ? QALC?s results (i.e. number of correct
answers) on the 200 questions
FIG. 3 ? Results on the named entities questions
FIG. 4 ? Results on the non named entities ques-
tions
to 55 for the first 5 ranks. These results are closer
to those of the monolingual system, which returns
41 correct answers at the first rank, and 59 on the
first 5 ranks.
It is interesting to see that the difference bet-
ween the monolingual and the bilingual systems
is less noticeable after the sentence selection step
than after the answer extraction step, which tends
to prove that the last step of our process is more
sensitive to translation errors. Moreover, this expe-
riment shows that this step can be improved thanks
to an information transfer between the source and
the target languages. In order to extend this stra-
tegy, we could also match each French question
term to its English equivalent, in order to trans-
late all the information given by the French analy-
sis into English. Thus, the question analysis errors
would be minimized.
7 Conclusion
The originality of our cross-language question
answering system is to use in parallel the two
most widely used strategies for shifting language,
which enables us to benefit from the advantages
of each strategy. Yet, each method presents draw-
backs, that we tried to evaluate in this article, and
to bypass.
For the term-by-term translation, we make the
most of the question biterms in order to restrict the
possible translation ambiguities. By validating the
biterms in the document collection, we have im-
proved the quality of both the biterms and the mo-
noterms translations. We hope this improvement
will lead to a better selection of the candidate sen-
tences from the documents.
For the question translation, we use the infor-
mation deduced from the source language to avoid
the problems coming from a bad or approximative
translation. This strategy enables us to solve some
of the problems coming from non-grammatical
translations ; matching each term of the French
question with its English equivalent would enable
us to transfer all the information of the French ana-
lysis. But the disambiguation errors of the transla-
tion remain.
References
Kisuh Ahn, Beatrix Alex, Johan Bos, Tiphaine Del-
mas, Jochen L. Leidner, and Matthew B. Smillie.
2004. Cross-lingual question answering with QED.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
29
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, pages 335?342, Bath, UK.
Ce?sar de Pablo-Sa?nchez, Ana Gonza?lez-Ledesma,
Jose? Luis Mart??nez-Ferna?ndez, Jose? Maria Guirao,
Paloma Martinez, and Antonio Moreno. 2005.
MIRACLE?s 2005 approach to cross-lingual ques-
tion answering. In Working Notes, CLEF Cross-
Language Evaluation Forum, Vienna, Austria.
Olivier Ferret, Brigitte Grau, Martine Hurault-Plantet,
Gabriel Illouz, Christian Jacquemin, Laura Mon-
ceaux, Isabelle Robba, and Anne Vilnat. 2002. How
NLP can improve question answering. Knowledge
Organization, 29(3-4).
Gregory Grefenstette. 1999. The world wide web as
a resource for example-based machine translation
tasks. In ASLIB Conference on Translating and the
Computer, volume 21, London, UK.
Sven Hartrumpf. 2005. University of Hagen at
QA@CLEF 2005 : Extending knowledge and dee-
pening linguistic processing for question answering.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, Vienna, Austria.
Christian Jacquemin. 1996. A symbolic and surgical
acquisition of terms through variation. Connectio-
nist, Statistical and Symbolic Approaches to Lear-
ning for Natural Language Processing, pages 425?
438.
Valentin Jijkoun, Gilad Mishne, Maarten de Rijke, Ste-
fan Schlobach, David Ahn, and Karin Muller. 2004.
The University of Amsterdam at QA@CLEF2004.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, pages 321?325, Bath, UK.
Dominique Laurent, Patrick Se?gue?la, and Sophie
Ne`gre. 2005. Cross lingual question answering
using QRISTAL for CLEF 2005. In Working Notes,
CLEF Cross-Language Evaluation Forum, Vienna,
Austria.
Anne-Laure Ligozat, Brigitte Grau, Isabelle Robba,
and Anne Vilnat. 2006. L?extraction des re?ponses
dans un syste`me de question-re?ponse. In Traitement
Automatique des Langues Naturelles (TALN 2006),
Leuven, Belgium.
Gu?nter Neumann and Bogdan Sacaleanu. 2004.
Experiments on robust NL question interpretation
and multi-layered doument annotation for a cross-
language question / answering system. In Working
Notes, CLEF Cross-Language Evaluation Forum,
pages 311?320, Bath, UK.
Gu?nter Neumann and Bogdan Sacaleanu. 2005. DF-
KI?s LT-lab at the CLEF 2005 multiple language
question answering track. In Working Notes, CLEF
Cross-Language Evaluation Forum, Vienna, Aus-
tria.
Laura Perret. 2004. Question answering system for the
French language. In Working Notes, CLEF Cross-
Language Evaluation Forum, pages 295?305, Bath,
UK.
Richard F.E. Sutcliffe, Michael Mulcahy, Igal Gabbay,
Aoife O?Gorman, Kieran White, and Darina Slat-
tery. 2005. Cross-language French-English ques-
tion answering using the DLT system at CLEF 2005.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, Vienna, Austria.
Hristo Tanev, Matteo Negri, Bernardo Magnini, and
Milen Kouylekov. 2004. The DIOGENE ques-
tion answering system at CLEF-2004. In Working
Notes, CLEF Cross-Language Evaluation Forum,
pages 325?333, Bath UK.
Hristo Tanev, Milen Kouylekov, Bernardo Magnini,
Matteo Negri, and Kiril Simov. 2005. Exploiting
linguistic indices and syntactic structures for multi-
lingual question answering : ITC-irst at CLEF 2005.
In Working Notes, CLEF Cross-Language Evalua-
tion Forum, Vienna, Austria.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
30
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852?1857,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Event Role Extraction using Domain-Relevant Word Representations
Emanuela Boros?
??
Romaric Besanc?on
?
Olivier Ferret
?
Brigitte Grau
??
?
CEA, LIST, Vision and Content Engineering Laboratory, F-91191, Gif-sur-Yvette, France
?
LIMSI, rue John von Neumann, Campus Universitaire d?Orsay, F-91405 Orsay cedex
?
ENSIIE, 1 square de la r?esistance F-91025
?
Evry cedex
firstname.lastname@cea.fr firstname.lastname@limsi.fr
Abstract
The efficiency of Information Extraction
systems is known to be heavily influenced
by domain-specific knowledge but the cost
of developing such systems is consider-
ably high. In this article, we consider the
problem of event extraction and show that
learning word representations from unla-
beled domain-specific data and using them
for representing event roles enable to out-
perform previous state-of-the-art event ex-
traction models on the MUC-4 data set.
1 Introduction
In the Information Extraction (IE) field, event ex-
traction constitutes a challenging task. An event
is described by a set of participants (i.e. at-
tributes or roles) whose values are text excerpts.
The event extraction task is related to several sub-
tasks: event mention detection, candidate role-
filler extraction, relation extraction and event tem-
plate filling. The problem we address here is the
detection of role-filler candidates and their associ-
ation with specific roles in event templates. For
this task, IE systems adopt various ways of ex-
tracting patterns or generating rules based on the
surrounding context, local context and global con-
text (Patwardhan and Riloff, 2009). Current ap-
proaches for learning such patterns include boot-
strapping techniques (Huang and Riloff, 2012a;
Yangarber et al., 2000), weakly supervised learn-
ing algorithms (Huang and Riloff, 2011; Sudo et
al., 2003; Surdeanu et al., 2006), fully supervised
learning approaches (Chieu et al., 2003; Freitag,
1998; Bunescu and Mooney, 2004; Patwardhan
and Riloff, 2009) and other variations. All these
methods rely on substantial amounts of manually
annotated corpora and use a large body of lin-
guistic knowledge. The performance of these ap-
proaches is related to the amount of knowledge
engineering deployed and a good choice of fea-
tures and classifiers. Furthermore, the efficiency
of the system relies on the a priori knowledge of
the applicative domain (the nature of the events)
and it is generally difficult to apply a system on
a different domain with less annotated data with-
out reconsidering the design of the features used.
An important step forwards is TIER
light
(Huang
and Riloff, 2012a) that targeted the minimization
of human supervision with a bootstrapping tech-
nique for event roles detection. Also, PIPER (Pat-
wardhan and Riloff, 2007; Patwardhan, 2010) dis-
tinguishes between relevant and irrelevant regions
and learns domain-relevant extraction patterns us-
ing a semantic affinity measure. Another possi-
ble approach for dealing with this problem is to
combine the use a restricted set of manually anno-
tated data with a much larger set of data extracted
in an unsupervised way from a corpus. This ap-
proach was experimented for relations in the con-
text of Open Information Extraction (Soderland et
al., 2010) but not for extracting events and their
participants to our knowledge.
In this paper, we propose to approach the task
of labeling text spans with event roles by auto-
matically learning relevant features that requires
limited prior knowledge, using a neural model to
induce semantic word representations (commonly
referred as word embeddings) in an unsupervised
fashion, as in (Bengio et al., 2006; Collobert and
Weston, 2008). We exploit these word embed-
dings as features for a supervised event role (mul-
ticlass) classifier. This type of approach has been
proved efficient for numerous tasks in natural lan-
guage processing, including named entity recog-
nition (Turian et al., 2010), semantic role label-
ing (Collobert et al., 2011), machine translation
(Schwenk and Koehn, 2008; Lambert et al., 2012),
word sense disambiguation (Bordes et al., 2012) or
sentiment analysis (Glorot et al., 2011; Socher et
al., 2011) but has never been used, to our knowl-
1852
edge, for an event extraction task. Our goal is two-
fold: (1) to prove that using as only features word
vector representations makes the approach com-
petitive in the event extraction task; (2) to show
that these word representations are scalable and
robust when varying the size of the training data.
Focusing on the data provided in MUC-4 (Lehnert
et al., 1992), we prove the relevance of our ap-
proach by outperforming state-of-the-art methods,
in the same evaluation environment as in previous
works.
2 Approach
In this work, we approach the event extraction task
by learning word representations from a domain-
specific data set and by using these representa-
tions to identify the event roles. This idea relies
on the assumption that the different words used
for a given event role in the text share some se-
mantic properties, related to their context of use
and that these similarities can be captured by spe-
cific representations that can be automatically in-
duced from the text, in an unsupervised way. We
then propose to rely only on these word repre-
sentations to detect the event roles whereas, in
most works (Riloff, 1996; Patwardhan and Riloff,
2007; Huang and Riloff, 2012a; Huang and Riloff,
2012b), the role fillers are represented by a set
of different features (raw words, their parts-of-
speech, syntactic or semantic roles in the sen-
tence).
Furthermore, we propose two additional contri-
butions to the construction of the word representa-
tions. The first one is to exploit limited knowledge
about the event types (seed words) to improve the
learning procedure by better selecting the dictio-
nary. The second one is to use a max operation
1
on
the word vector representations in order to build
noun phrase representations (since slot fillers are
generally noun phrases), which represents a better
way of aggregating the semantic information born
by the word representations.
2.1 Inducing Domain-Relevant Word
Representations
In order to induce the domain-specific word rep-
resentations, we project the words into a 50-
dimensional word space. We chose a single
1
This max operation consists in taking, for each compo-
nent of the vector, the max value of this component for each
word vector representation.
layer neural network (NN) architecture that avoids
strongly engineered features, assumes little prior
knowledge about the task, but is powerful enough
to capture relevant domain information. Follow-
ing (Collobert et al., 2011), we use an NN which
learns to predict whether a given text sequence
(short word window) exists naturally in the consid-
ered domain. We represent an input sequence of n
words as ?w
i
? = ?w
i?(n/2)
. . . , w
i
, . . . w
i+(n/2)
?.
The main idea is that each sequence of words in
the training set should receive a higher score than
a sequence in which one word is replaced with
a random one. We call the sequence with a ran-
dom word corrupted (
?
?w
i
?) and denote as correct
(?w
i
?) all the sequences of words from the data
set. The goal of the training step is then to min-
imize the following loss function for a word w
i
in the dictionary D: C
w
i
=
?
w
i
?D
max(0, 1 ?
g(?w
i
?)+g(
?
?w
i
?)), where g(?) is the scoring func-
tion given by the neural network. Further details
and evaluations of these embeddings can be found
in (Bengio et al., 2003; Bengio et al., 2006; Col-
lobert and Weston, 2008; Turian et al., 2010). For
efficiency, words are fed to our architecture as in-
dices taken from a finite dictionary. Obviously,
a simple index does not carry much useful infor-
mation about the word. So, the first layer of our
network maps each of these word indices into a
feature vector, by a lookup table operation. Our
first contribution intervenes in the process of the
choosing the proper dictionary. (Bengio, 2009)
has shown that the order of the words in the dic-
tionary of the neural network is not indifferent to
the quality of the achieved representations: he pro-
posed to order the dictionary by frequency and se-
lect the words for the corrupted sequence accord-
ing to this order. In our case, the most frequent
words are not always the most relevant for the task
of event role detection. Since we want to have a
training more focused to the domain specific task,
we chose to order the dictionary by word relevance
to the domain. We accomplish this by considering
a limited number of seed words for each event type
that needs to be discovered in text (e.g. attack,
bombing, kidnapping, arson). We then rate with
higher values the words that are more similar to the
event types words, according to a given semantic
similarity, and we rank them accordingly. We use
the ?Leacock Chodorow? similarity from Word-
net 3.0 (Leacock and Chodorow, 1998). Initial ex-
perimental results proved that using this domain-
1853
oriented order leads to better performance for the
task than the order by frequency.
2.2 Using Word Representations to Identify
Event Roles
After having generated for each word their vec-
tor representation, we use them as features for the
annotated data to classify event roles. However,
event role fillers are not generally single words but
noun phrases that can be, in some cases, identi-
fied as named entities. For identifying the event
roles, we therefore apply a two-step strategy. First,
we extract the noun chunks using SENNA
2
parser
(Collobert et al., 2011; Collobert, 2011) and we
build a representation for these chunks defined as
the maximum, per column, of the vector represen-
tations of the words it contains. Second, we use
a statistical classifier to recognize the slot fillers,
using this representation as features. We chose
the extra-trees ensemble classifier (Geurts et al.,
2006), which is a meta estimator that fits a num-
ber of randomized decision trees (extra-trees) on
various sub-samples of the data set and use averag-
ing to improve the predictive accuracy and control
over-fitting.
3 Experiments and Results
3.1 Task Description
We conducted the experiments on the official
MUC-4 training corpus that consists of 1,700 doc-
uments and instantiated templates for each doc-
ument. The task consists in extracting informa-
tion about terrorist events in Latin America from
news articles. We classically considered the fol-
lowing 4 types of events: attack, bombing, kid-
napping and arson. These are represented by tem-
plates containing various slots for each piece of
information that should be extracted from the doc-
ument (perpetrators, human targets, physical tar-
gets, etc). Following previous works (Huang and
Riloff, 2011; Huang and Riloff, 2012a), we only
consider the ?String Slots? in this work (other slots
need different treatments) and we group certain
slots to finally consider the five slot types PerpInd
(individual perpetrator), PerpOrg (organizational
perpetrator), Target (physical target), Victim (hu-
man target name or description) and Weapon (in-
strument id or type). We used 1,300 documents
(DEV) for training, 200 documents (TST1+TST2)
2
Code and resources can be found at http://ml.
nec-labs.com/senna/
for tuning, and 200 documents (TST3+TST4) as
the blind test set. To compare with similar works,
we do not evaluate the template construction and
only focus on the identification of the slot fillers:
for each answer key in a reference template, we
check if we find it correctly with our extraction
method, using head noun matching (e.g., the vic-
tim her mother Martha Lopez Orozco de Lopez is
considered to match Matha Lopez), and merging
duplicate extractions (so that different extracted
slot fillers sharing the same head noun are counted
only once). We also took into account the answer
keys with multiple values in the reference, deal-
ing with conjunctions (when several victims are
named, we need to find all of them) and disjunc-
tions (when several names for the same organiza-
tion are possible, we need to find any of them).
Our results are reported as Precision/Recall/F1-
score for each event role separately and averaged
on all roles.
3.2 Experiments
In all the experiments involving our model, we es-
tablished the following stable choices of parame-
ters: 50-dimensional vectors obtained by training
on sequences of 5 words, which is consistent with
previous studies (Turian et al., 2010; Collobert
and Weston, 2008). All the hyper-parameters of
our model (e.g. learning rate, size of the hidden
layer, size of the word vectors) have been chosen
by finetuning our event extraction system on the
TST1+TST2 data set. For DRVR-50 and W2V-50,
the embeddings were built from the whole training
corpus (1,300 documents) and the dictionary was
made of all the words of this corpus under their
inflected form.
We used the extra-trees ensemble classifier im-
plemented in (Pedregosa et al., 2011), with hyper-
parameters optimized on the validation data: for-
est of 500 trees and the maximum number of
features to consider when looking for the best
split is
?
number features. We present a 3-
fold evaluation: first, we compare our system with
state-of-the-art systems on the same task, then we
compare our domain-relevant vector representa-
tions (DRVR-50) to more generic word embed-
dings (C&W50, HLBL-50)
3
and finally to another
3
C&W-50 are described in (Collobert and Weston,
2008), HLBL-50 are the Hierarchical log-bilinear embed-
dings (Mnih and Hinton, 2007), provided by (Turian et
al., 2010), available at http://metaoptimize.com/
projects/wordreprs induced from the Reuters-RCV1
1854
State-of-the-art systems
PerpInd PerpOrg Target Victim Weapon Average
(Riloff, 1996) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46
(Patwardhan and Riloff, 2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/36/40
(Patwardhan and Riloff, 2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52
(Huang and Riloff, 2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56
(Huang and Riloff, 2012a) 47/51/47 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50
(Huang and Riloff, 2012b) 54/57/56 55/49/51 55/68/61 63/59/61 62/64/63 58/60/59
Models based on word embeddings
C&W-50 80/55/65 64/65/64 76/72/74 53/63/57 85/64/73 68/63/65
HLBL-50 81/53/64 63/67/65 78/72/75 53/63/58 93/64/75 69/62/66
W2V-50 79/57/66 88/71/79 74/72/73 69/75/71 97/65/78 77/68/72
DRVR-50 79/57/66 91/74/81 79/57/66 77/75/76 92/58/81 80/67/73
Table 1: Accuracy of ?String Slots? on the TST3 + TST4 test set P/R/F1 (Precision/Recall/F1-Score)
word representation construction on the domain-
specific data (W2V-50)
4
.
Figure 1: F1-score results for event role labeling
on MUC-4 data, for different size of training data,
of ?String Slots? on the TST3+TST4 with differ-
ent parameters, compared to the learning curve of
TIER (Huang and Riloff, 2012a). The grey points
represent the performances of other IE systems.
Figure 1 presents the average F1-score results,
computed over the slots PerpInd, PerpOrg, Tar-
get, Victim and Weapon. We observe that mod-
els relying on word embeddings globally outper-
form the state-of-the-art results, which demon-
strates that the word embeddings capture enough
semantic information to perform the task of event
newswire corpus
4
W2V-50 are the embeddings induced from the MUC4
data set using the negative sampling training algorithm
(Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et
al., 2013c), available at https://code.google.com/
p/word2vec/
role labeling on ?String Slots? without using any
additional hand-engineered features. Moreover,
our representations (DRVR-50) clearly surpass the
models based on generic embeddings (C&W-50
and HLBL-50) and obtain better results than W2V-
50, based the competitive model of (Mikolov et
al., 2013a), even if the difference is small. We
can also note that the performance of our model
is good even with a small amount of training data,
which makes it a good candidate to easily develop
an event extraction system on a new domain.
Table 1 provides a more detailed analysis of the
comparative results. We can see in this table that
our results surpass those of previous systems (0.73
vs. 0.59) with, particularly, a consistently higher
precision on all roles, whereas recall is smaller for
certain roles (Target and Weapon). To further ex-
plore the impact of these representations, we com-
pared our word embeddings with other word em-
beddings (C&W-50, HLBL-50) and report the re-
sults in Figure 1 and Table 1. The results show
that our model also outperforms the models using
others word embeddings (F1-score of 0.73 against
0.65, 0.66). This proves that a model learned
on a domain-specific data set does indeed pro-
vide better results, even if its size is much smaller
(whereas it is usually considered that neural mod-
els require often important training data). Finally,
we also achieve slightly better results than W2V-50
with other word representations built on the same
corpus, which shows that the choices made for the
word representation construction, such as the use
of domain information for word ordering, tend to
have a positive impact.
1855
4 Conclusions and Perspectives
We presented in this paper a new approach for
event extraction by reducing the features to only
use unsupervised word representations and a small
set of seed words. The word embeddings induced
from a domain-specific corpus bring improvement
over state-of-art models on the standard MUC-
4 corpus and demonstrate a good scalability on
different sizes of training data sets. Therefore,
our proposal offers a promising path towards eas-
ier and faster domain adaptation. We also prove
that using a domain-specific corpus leads to bet-
ter word vector representations for this task than
using other publicly-available word embeddings
(even if they are induced from a larger corpus).
As future work, we will reconsider the archi-
tecture of the neural network and we will refo-
cus on creating a deep learning model while tak-
ing advantage of a larger set of types of infor-
mation such as syntactic information, following
(Levy and Goldberg, 2014), or semantic informa-
tion, following (Yu and Dredze, 2014).
References
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastian
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
DawnE. Holmes and LakhmiC. Jain, editors, Inno-
vations in Machine Learning, volume 194 of Studies
in Fuzziness and Soft Computing, pages 138?186.
Springer Berlin Heidelberg.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and trends in Machine Learning,
2(1).
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text seman-
tic parsing. In Fifteenth International Conference
on Artificial Intelligence and Statistics (AISTATS
2012), pages 127?135.
Razvan Bunescu and Raymond J Mooney. 2004.
Collective information extraction with relational
markov networks. In 42nd Annual Meeting on As-
sociation for Computational Linguistics (ACL-04),
pages 438?445.
Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In 41st international Annual Meeting on Association
for Computational Linguistics (ACL-2003), pages
216?223.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In 25th In-
ternational Conference of Machine learning (ICML-
08), pages 160?167. ACM.
Ronan Collobert, Jason Weston, L?eon Battou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In 14th International Con-
ference on Artificial Intelligence and Statistics (AIS-
TATS 2011).
Dayne Freitag. 1998. Information extraction from
HTML: Application of a general machine learning
approach. In AAAI?98, pages 517?523.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale senti-
ment classification: A deep learning approach. In
28th International Conference on Machine Learning
(ICML-11), pages 513?520.
Ruihong Huang and Ellen Riloff. 2011. Peeling back
the layers: Detecting event role fillers in secondary
contexts. In ACL 2011, pages 1137?1147.
Ruihong Huang and Ellen Riloff. 2012a. Bootstrapped
training of event extraction classifiers. In 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL 2012), pages
286?295.
Ruihong Huang and Ellen Riloff. 2012b. Modeling
textual cohesion for event extraction. In 26th Con-
ference on Artificial Intelligence (AAAI 2012).
Patrik Lambert, Holger Schwenk, and Fr?ed?eric Blain.
2012. Automatic translation of scientific documents
in the hal archive. In LREC 2012, pages 3933?3936.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and Wordnet similarity for word
sense identification. In Christiane Fellbaum, edi-
tor, WordNet: An electronic lexical database., pages
265?283. MIT Press.
Wendy Lehnert, Claire Cardie, David Fisher, John Mc-
Carthy, Ellen Riloff, and Stephen Soderland. 1992.
University of Massachusetts: MUC-4 test results
and analysis. In 4th Conference on Message under-
standing, pages 151?158.
1856
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2014), Short Papers, pages 302?308, Bal-
timore, Maryland, June.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In International Conference
on Learning Representations (ICLR 20013), work-
shop track.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26 (NIPS 2013), pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In NAACL-HLT 2013, pages
746?751.
Andriy Mnih and Geoffrey Hinton. 2007. Three
new graphical models for statistical modelling. In
24th International Conference of Machine learning
(ICML 2007), pages 641?648. ACM.
Siddharth Patwardhan and Ellen Riloff. 2007. Ef-
fective information extraction with semantic affinity
patterns and relevant regions. In 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2007), pages 717?727.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In 2009 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2009), pages 151?160.
Siddharth Patwardhan. 2010. Widening the field of
view of information extraction through sentential
event recognition. Ph.D. thesis, University of Utah.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In AAAI?96, pages
1044?1049.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In IJCNLP 2008, pages 661?666.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and nat-
ural language with recursive neural networks. In
28th International Conference on Machine Learning
(ICML-11), pages 129?136.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open
information extraction to domain-specific relations.
AI Magazine, 31(3):93?102.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic ie pattern acquisition. In
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL-03), pages 224?231.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno.
2006. A hybrid approach for the acquisition of
information extraction patterns. In EACL-2006
Workshop on Adaptive Text Extraction and Mining
(ATEM 2006), pages 48?55.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In 48th international
Annual Meeting on Association for Computational
Linguistics (ACL 2010), pages 384?394.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
18th Internation Conference on Computational Lin-
guistics (COLING 2000), pages 940?946.
Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2014), Short Papers, pages 545?550,
Baltimore, Maryland, June.
1857
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 598?602, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
LIMSIILES: Basic English Substitution for Student Answer Assessment at
SemEval 2013
Martin Gleize
LIMSI-CNRS & ENS
B.P. 133 91403 ORSAY CEDEX, France
gleize@limsi.fr
Brigitte Grau
LIMSI-CNRS & ENSIIE
B.P. 133 91403 ORSAY CEDEX, France
bg@limsi.fr
Abstract
In this paper, we describe a method for as-
sessing student answers, modeled as a para-
phrase identification problem, based on sub-
stitution by Basic English variants. Basic En-
glish paraphrases are acquired from the Sim-
ple English Wiktionary. Substitutions are ap-
plied both on reference answers and student
answers in order to reduce the diversity of
their vocabulary and map them to a common
vocabulary. The evaluation of our approach
on the SemEval 2013 Joint Student Response
Analysis and 8th Recognizing Textual Entail-
ment Challenge data shows promising results,
and this work is a first step toward an open-
domain system able to exhibit deep text un-
derstanding capabilities.
1 Introduction
Automatically assessing student answers is a chal-
lenging natural language processing task (NLP). It
is a way to make test grading easier and improve
adaptive tutoring (Dzikovska et al, 2010), and is the
goal of the SemEval 2013?s task 7, titled Joint Stu-
dent Response Analysis. More specifically, given a
question, a known correct ?reference answer? and a
1- or 2-sentence student answer, the goal is to deter-
mine the student?s answer accuracy (Dzikovska et
al., 2013). This can be seen as a paraphrase identi-
fication problem between student answers and refer-
ence answers.
Paraphrase identification searches whether two
sentences have essentially the same meaning (Culi-
cover, 1968). Automatically generating or extract-
ing semantic equivalences for the various units of
language ? words, phrases, and sentences ? is an im-
portant problem in NLP and is being increasingly
employed to improve the performance of several
NLP applications (Madnani and Dorr, 2010), like
question-answering and machine translation.
Paraphrase identification would benefit from
a precise and broad-coverage semantic language
model. This is unfortunately difficult to obtain to its
full extent for any natural language, due to the size
of a typical lexicon and the complexity of grammat-
ical constructions. Our hypothesis is that the sim-
pler the language lexicon is, the easier it will be to
access and compare meaning of sentences. This as-
sumption is justified by the multiple attempts at con-
trolled natural languages (Schwitter, 2010) and es-
pecially simplified forms of English. One of them,
Basic English (Ogden, 1930), has been adopted by
the Wikipedia Project as the preferred language of
the Simple English Wikipedia1 and its sister project
the Simple English Wiktionary2.
Our method starts with acquiring paraphrases
from the Simple English Wiktionary?s definitions.
Using those, we generate variants of both sentences
whose meanings are to be compared. Finally, we
compute traditional lexical and semantic similarity
measures on those two sets of variants to produce
features to train a classifier on the SemEval 2013
datasets in order to take the final decision.
2 Acquiring simplifying paraphrases
Simple Wiktionary word definitions are different
from usual dictionary definitions. Aside from the
1http://simple.wikipedia.org
2http://simple.wiktionary.org
598
simplified language, they often prefer to give a
complete sentence where the word ? e.g. a verb ? is
used in context, along with an explanation of what it
means. To define the verb link, Simple Wiktionary
states that If you link two or more things, you make a
connection between them (1), whereas the standard
Wiktionary uses the shorter and more cryptic To
connect two or more things.
We notice in this example that the definition
from Simple Wiktionary consists of two clauses,
linked by a subordination relation. It?s actually the
case for a lot of verb definitions: a quick statistical
study shows that 70% of these definitions are
composed of two clauses, an independent clause,
and a subordinate clause (often an adverbial clause).
One clause illustrates how the verb is used, the
other gives the explanation and the actual dictionary
definition, as in example (1). These definitions are
the basis of our method for acquiring paraphrases.
2.1 Pre-processing
We use the Stanford Parser to parse the definitions
and get a dependency graph (De Marneffe and Man-
ning, 2008). Using a few hand-written rules, we then
retrieve both parts of the definition, which we call
the word part and the defining part (see table 1 page
3 for examples). We can do this for definitions of
verbs, but also for nouns, like the giraffe is the tallest
land animal in the world to define giraffe, or adjec-
tives, like if something is bright it gives out or fills
with much light to define bright. We only provide
the details of our method for processing verb defini-
tions, as they correspond to the most complex cases,
but we proceed similarly for noun, adjective and ad-
verb definitions.
2.2 Argument matching
Word and defining parts alone are not paraphrases,
but we can obtain phrasal paraphrases from them. If
we see word part and defining part as two semanti-
cally equivalent predications, we have to identify the
two predicates with their arguments, then match ar-
guments with corresponding meaning, i.e. match ar-
guments which designate the same entity or assume
the same semantic function in both parts, as showed
in Table 2.
For verb definitions, we identify the predicates as
you ? you
link ? make
? ? a connection
? ? between
two or more things ? them
Table 2: Complete matching for the definition of verb link
the main verbs in both clauses (hence link matching
with make in table 2) and their arguments as a POS-
filtered list of their syntactic descendants. Then,
our assumption is that every argument of the word
part predicate is present in the defining part, and
the defining part predicate can have extra arguments
(like a connection).
We define s(A,B), the score of the pair of argu-
ments (A,B), with argument A in the word part and
argument B in the defining part. We then define a
matching M as a set of such pairs, such that ev-
ery element of every possible pair of arguments is
found at most one time in M . A complete match-
ing is a matching M that matches every argument
in the word part, i.e., for each word part argument
A, there exists a pair of arguments in M which con-
tains A. Finally, we compute the matching score of
M , S(M), as the sum of scores of all pairs of M .
The score function s(A,B) is a hand-crafted lin-
ear combination of several features computed on a
pair of arguments (A,B) including:
? Raw string similarity. Sometimes the same
word is reused in the defining part.
? Having an equal/compatible dependency rela-
tion with their respective main verb.
? Relative position in clause.
? Relative depth in parsing tree. These last 3 fea-
tures assess if the two arguments play the same
syntactic role.
? Same gender and number. If different, it?s
unlikely that the two arguments designate the
same entity.
? If (A,B) is a pair (noun phrase, pronoun). We
hope to capture an anaphoric expression and its
antecedent.
599
Word (POS-tag) Word part Defining part
link (V) you link two or more things you make a connection between them
giraffe (N) the giraffe the tallest land animal in the world
bright (Adj) something is bright it gives out or fills with much light
Table 1: Word part and defining part of some Simple Wiktionary definitions
? WordNet similarity (Pedersen et al, 2004). If
words belong to close synsets, they?re more
likely to identify the same entity.
2.3 Phrasal paraphrases
We compute the complete matching M which maxi-
mizes the matching score S(M). Although it is pos-
sible to enumerate all matchings, it is intractable;
therefore when predicates have more than 4 argu-
ments, we prefer constructing a best matching with a
beam search algorithm. After replacing each pair of
arguments with linked variables, and attaching un-
matched arguments to the predicates, we finally ob-
tain phrasal paraphrases of this form:
? X link Y , X make a connection between Y ?
3 Paraphrasing exercise answers
3.1 Paraphrase generation and pre-ranking
Given a sentence, and our Simple Wiktionary para-
phrases (about 20,650 extracted paraphrases), we
can generate sentential paraphrases by simple syn-
tactic pattern matching ?and do so recursively by
taking previous outputs as input?, with the intent
that these new sentences use increasingly more Ba-
sic English. We generate as many variants starting
from both reference answers and student answers as
we can in a fixed amount of time, as an anytime al-
gorithm would do. We prioritize substituting verbs
and adjectives over nouns, and non Basic English
words over Basic English words.
Given a student answer and reference answers, we
then use a simple Jaccard distance (on lowercased
lemmatized non-stopwords) to score the closeness
of student answer variants to reference answer vari-
ants: we measure how close the vocabulary used in
the two statements has become. More specifically,
for each reference answer A, we compute the n clos-
est variants of the student answer to A?s variant set.
In our experiments, n = 10. We finally rank the
reference answers according to the average distance
from their n closest variants to A?s variant set and
keep the top-ranked one for our classification exper-
iment. Figure 1 illustrates the whole process.
RA1
RA2
...
SA
0
1
2
3
4
5
RA2
RA1
1. 1
A B
C 1. 5
2. 3
2. 3
...
...
Figure 1: Variants are generated from all reference an-
swers (RA) and the student answer (SA). For each ref-
erence answer RA, student answer variants are ranked
based on their lexical distance from the variants of RA.
The reference with the n closer variants to the student
variants is kept (here: RA1).
3.2 Classifying student answers
SemEval 2013 task 7 offers 3 problems: a 5-way
task, with 5 different answer judgements, and 3-way
and 2-way tasks, conflating more judgement cate-
gories each time. Two different corpora, Beetle and
SciEntsBank, were labeled with the 5 following la-
bels: Correct, Partially correct incomplete, Contra-
dictory, Irrelevant and Non Domain, as described in
(Dzikovska et al, 2012). We see the n-way task as a
n-way classification problem. The instances of this
problem are the pairs (student answer, reference an-
swer).
We compute for each instance the following fea-
tures: For each of the n closest variants of the stu-
dent answer to some variant of the reference answer
computed in the pre-ranking phase:
? Jaccard similarity coefficient on non-
stopwords.
? A boolean representing if the two statements
have the same polarity or not, where polarity
600
is defined as the number of neg dependencies
in the Stanford Parser dependency graph.
? Number of ?paraphrasing steps? necessary to
obtain the variant from a raw student answer.
? Highest WordNet similarity of their respective
nouns.
? WordNet similarity of the main verbs.
General features:
? Answer count (how many students typed this
answer), provided in the datasets.
? Length ratio between the student answer and
the closest reference answer.
? Number of (non-stop)words which appear nei-
ther in the question nor the reference answers.
We train an SVM classifier (with a one-against-one
approach to multiclass classification) on both Beetle
and SciEntsBank, for each n-way task.
3.3 Evaluation
Table 3 presents our system?s overall accuracy on the
5-way task, along with the top scores at SemEval
2013, mean scores, and baselines ?majority class
and lexical overlap? described in (Dzikovska et al,
2012).
System
Beetle
unseen answers
SciEntsBank
unseen questions
Majority 0.4010 0.4110
Lexical
overlap
0.5190 0.4130
Mean 0.5326 0.4078
ETS-run-1 0.5740 0.5320
ETS-run-2 0.7150 0.4010
Simple
Wiktio
0.5330 0.4820
Table 3: SemEval 2013 evaluation results.
Our system performs slightly better in overall ac-
curacy on Beetle unseen answers and SciEntsBank
unseen questions than both baselines and the mean
scores. While results are clearly below the best sys-
tem trained on the Beetle corpus questions, we hold
the third best score for the 5-way task on SciEnts-
Bank unseen questions, while not fine-tuning our
system specifically for this corpus. This is rather
encouraging as to how suitable Simple Wiktionary
is as a resource to extract open-domain knowledge
from.
4 Discussion
The system we present in this paper is the first
step towards an open-domain machine reading sys-
tem capable of understanding and reasoning. Di-
rect modeling of the semantics of a full natural lan-
guage appears too difficult. We therefore decide to
first project the English language onto a simpler En-
glish, so that it is easier to model and draw infer-
ences from.
One complementary approach to a minimalistic
language model, is to accept that texts are replete
with gaps: missing information that cannot be in-
ferred by reasoning on the text alone, but require
a certain amount of background knowledge. Penas
and Hovy (2010) show that these gaps can be filled
by maintaining a background knowledge base built
from a large corpus.
Although Simple Wiktionary is not a large corpus
by any means, it can serve our purpose of acquiring
basic knowledge for assessing exercise answers, and
has the advantage to be in constant evolution and ex-
pansion, as well as interfacing very easily with the
richer Wiktionary and Wikipedia.
Our future work will be focused on enriching and
improving the robustness of our knowledge acqui-
sition step from Simple Wiktionary, as well as in-
troducing a true normalization of English to Basic
English.
Acknowledgments
We acknowledge the Wikimedia Foundation for
their willingness to provide easily usable versions
of their online collaborative resources.
References
P.W. Culicover. 1968. Paraphrase generation and
information retrieval from stored text. In Mechanical
Translation and Computational Linguistics, 11(12),
7888.
601
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.
Myroslava O. Dzikovska, Diana Bental, Johanna D.
Moore, Natalie Steinhauser, Gwendolyn Campbell,
Elaine Farrow, and Charles B. Callaway. 2010.
Intelligent tutoring with natural language support
in the BEETLE II system. In Proceedings of Fifth
European Conference on Technology Enhanced
Learning (EC-TEL 2010), Barcelona.
Myroslava O. Dzikovska, Rodney D. Nielsen and Chris
Brew. 2012. Towards Effective Tutorial Feedback
for Explanation Questions: A Dataset and Baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2012), Montreal.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan and Hoa Trang Dang.
2013. SemEval-2013 Task 7: The Joint Student
Response Analysis and 8th Recognizing Textual
Entailment Challenge. In Proceedings of the 7th
International Workshop on Semantic Evaluation
(SemEval 2013), in conjunction with the Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM 2013). Atlanta, Georgia, USA. 13-14 June.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of
data-driven methods. In Computational Linguistics
36 (3), 341-387.
Charles Kay Ogden. 1930. Basic English: A General
Introduction with Rules and Grammar. Paul Treber,
London.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. 2004. WordNet::similarity?measuring
the relatedness of concepts. In Proceedings of
the Nineteenth National Conference on Artificial
Intelligence(AAAI-04), pages 10241025.
Anselmo Penas and Eduard H. Hovy. 2010. Filling
Knowledge Gaps in Text for Machine Reading. COL-
ING (Posters) 2010: 979-987, Beijing.
Rolf Schwitter. 2010. Controlled Natural Languages for
Knowledge Representation. COLING (Posters) 2010,
Beijing.
602
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 87?96,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Methods Combination and ML-based Re-ranking of Multiple
Hypothesis for Question-Answering Systems
Arnaud Grappy
LIMSI-CNRS
arnaud.grappy@limsi.fr
Brigitte Grau
LIMSI-CNRS
ENSIIE
brigitte.grau@limsi.fr
Sophie Rosset
LIMSI-CNRS
sophie.rosset@limsi.fr
Abstract
Question answering systems answer cor-
rectly to different questions because they
are based on different strategies. In order
to increase the number of questions which
can be answered by a single process, we
propose solutions to combine two question
answering systems, QAVAL and RITEL.
QAVAL proceeds by selecting short pas-
sages, annotates them by question terms,
and then extracts from them answers which
are ordered by a machine learning valida-
tion process. RITEL develops a multi-level
analysis of questions and documents. An-
swers are extracted and ordered according
to two strategies: by exploiting the redun-
dancy of candidates and a Bayesian model.
In order to merge the system results, we de-
veloped different methods either by merg-
ing passages before answer ordering, or by
merging end-results. The fusion of end-
results is realized by voting, merging, and
by a machine learning process on answer
characteristics, which lead to an improve-
ment of the best system results of 19 %.
1 Introduction
Question-answering systems aim at giving short
and precise answers to natural language ques-
tions. These systems are quite complex, and
include many different components. Question-
Answering systems are generally organized
within a pipeline which includes at a high level
at least three components: questions processing,
snippets selection and answers extraction. But
each module of these systems is quite different.
They are based on different knowledge sources
and processing. Even if the global performance of
these systems are similar, they show great dispar-
ity when examining local results. Moreover there
is no question-answering system able to answer
correctly to all possible questions. Considering all
QA evaluation campaigns in French like CLEF,
EQUER or Qu?ro, or for other languages like
TREC, no system obtained 100% correct answers
at first rank. A new direction of research was built
upon these observations: how can we combine
correct answers provided by different systems?
This work deals with this issue1 . In this paper
we describe different experiments concerning the
combination of QA systems. We used two differ-
ent available systems, QAVAL and RITEL, while
RITEL includes two different answer extraction
strategies. We propose to merge the results of
these systems at different levels. First, at an in-
termediary step (for example, between snippet se-
lection and answer extraction). This approach al-
lows to evaluate a fusion process based on the in-
tegration of different strategies. Another way to
proceed is to execute the fusion at the end of each
system. The aim is then to choose between all the
candidate answers the best one for each question.
Such an approach has been successfully applied
in the information retrieval field, with the defini-
tion of different functions for combining results
of search engines (Shaw and Fox, 1994). How-
ever, in QA, the problem is different as answers to
questions are not made of a list of answers, but are
made of excerpts of texts, which may be different
in their writing, but which correspond to a unique
and same answer. Thus, we propose fusion meth-
ods that rely on the information generally com-
puted by QA systems, such as score, rank, an-
1This work was partially financed by OSEO under the
Quro program
87
swer redundancy, etc. We defined new voting and
scoring functions, and a machine learning system
to combine these features. Most of the strategies
presented here allow a clear improvement (up to
19 %) on the first ranked correct answers.
In the following, related work is presented in
the section 2. We then describe the different sys-
tems used in this work (Section 3.1 and 3.2). The
proposed approach are presented (Section 4 and
5). The methods and the different systems are
then evaluated on the same corpus.
2 Related work
QA system hybridization often consists in merg-
ing end-results. The first studies presented here
aim at merging the results of different strate-
gies for finding answers in the same set of doc-
uments. (Jijkoun and Rijke, 2004) developed sev-
eral strategies for answering questions, based on
different paradigms for extracting answers. They
search for answers in a knowledge base or by ap-
plying extraction patterns or by selecting the n-
grams the closest to the question words. They de-
fined different methods for recognizing the simi-
larity of two answers: equality, inclusion and an
edit distance. The merging of answers is realized
by summing the confidence scores of similar an-
swers and leads to improve the number of right
answers at first rank of 31 %.
(Tellez-Valero et al, 2010) combine the out-
put of QA systems, whose strategy is not known.
They only dispose of the provided answers asso-
ciated with a supporting snippet. Merging is done
by a machine learning approach, which combines
different criteria such as the question category, the
expected answer type, the compatibility between
the provided answer and the question, the system
which was applied and the rate of question terms
in the snippet. When applying this module on the
CLEF QA systems which were run on the Span-
ish data, they obtain a better MRR2 value than the
best system from 0.62 up to 0.73.
In place of diversifying the answering strate-
gies, another possibility is to apply a same strat-
egy on different collections. (Aceves-Pe?rez et al,
2008) apply classical merging strategies to mul-
tilingual QA systems, by merging answers ac-
cording to their rank or by combining their con-
fidence scores, normalized or not. They show that
2Mean Reciprocal Rank
the combination of normalized scores obtains re-
sults which are better than a monolingual system
(MRR from 0.64 up to 0.75). They also tested
hybridization at the passage level by extracting
answers from the overall set of passages which
proved to be less relevant than answer merging.
(Chalendar et al, ) combine results obtained by
searching the Web in parallel to a given collec-
tion. The combination which consists in boosting
answers if they are found by the two systems is
very effective, as it is less probable to find same
incorrect answers on different documents.
The hybridization we are interested in concerns
the merging of different strategies and different
system capabilities in order to improve the final
result. We tested different hybridization levels,
and different merging methods. One is closed
to (Tellez-Valero et al, 2010) as it is based on
a validation module. Other are voting and scor-
ing methods which have been defined according
to our task, and are compared to classical merg-
ing scheme which have been proposed in infor-
mation retrieval (Shaw and Fox, 1994), ComSum
and CombMNZ.
3 The Question-Answering systems
3.1 The QAVAL system
3.1.1 General overview
QAVAL(Grappy et al, 2011) is made of se-
quential modules, corresponding to five main
steps (see Fig. 1). The question analysis provides
main characteristics for retrieving passages and
for guiding the validation process. Short passages
of about 300-character long are obtained directly
from the search engine Lucene and are annotated
with question terms and their weighted variants.
They are then parsed by a syntactic parser and en-
riched with the question characteristics, which al-
lows QAVAL to compute the different features for
validating or discarding candidate answers.
A specificity of QAVAL relies on its validation
module. Candidate answers are extracted accord-
ing to the expected answer type, i.e. a named en-
tity or not. In case of a named entity, all the named
entities corresponding to the expected type are
extracted while, in the second case, QAVAL ex-
tracts all the noun phrases which are not question
phrases. As many candidate answers can be ex-
tracted, a first step consists in recognizing obvious
false answers. Answers from a passage that does
88
Que
stio
n 
an
aly
sis
Pas
sa
ge 
se
lec
tion
An
sw
er
va
lida
tion
 
an
d 
ran
kin
g
Can
did
ate
 
an
sw
er
ex
tra
ctio
n
An
no
tat
ion
 
an
d 
syn
tac
tic
an
aly
sis
of p
as
sa
ges
Que
stio
ns
an
sw
ers
an
sw
ers
an
sw
ers
Do
cu
m
en
ts
An
sw
er
ran
kin
g
An
sw
er
fus
ion
QA
VA
L
RIT
EL
 
Sta
nda
rd
RIT
EL
 
Pro
bab
ilis
tic
5 a
ns
we
rs
 

Que
stio
n 
an
aly
sis
An
no
tat
ion
 
an
d 
syn
tac
tic
an
aly
sis
of p
as
sa
ges
Pas
sa
ge 
se
lec
tion
Can
did
ate
 
an
sw
er
ex
tra
ctio
n
An
sw
er
ran
kin
g
Hy
brid
iza
tion
poi
nt
Figure 1: The QAVAL and RITEL systems and their
possible hybridizations
not contain all the named entities of the question
are discarded. The remaining answers are then
ranked based on a learning method which com-
bines features characterizing the passage and the
candidate answer it provides. The QAVAL sys-
tem has been evaluated on factual questions and
obtains good results.
3.1.2 Answer ranking by validation
A machine based learning validation module
provides scores to each candidate answer. Fea-
tures relative to passages aim at evaluating in
which part a passage conveys the same meaning
as the question. They are based on lexical fea-
tures, as the rate of question words in the passage,
their POS tag, the main terms of the question, etc.
Features relative to the answer represent the
property that an answer has to be of an expected
type, if explicitly required, and to be related to
the question terms. Another kind of criterion con-
cerns the answer redundancy: the most frequent
an answer is, the most relevant it is. Answer type
verification is applied for questions which give an
explicit type for the answer, as in ?Which presi-
dent succeeded Georges W. Bush?? that expects
as answer the name of a president, more specific
than the named entity type PERSON. This mod-
ule (Grappy and Grau, 2010) combines results
given by different kinds of verifications, based
on named entity recognizers and searches in cor-
pora. To evaluate the relation degree of an answer
with the question terms, QAVAL computes i) the
longest chain of consecutive common words be-
tween the question plus the answer and the pas-
sage; ii) the average distance between the answer
and each of the question words in the passage.
Other criteria are the passage rank given by us-
ing results of the passage analysis, the question
category, i.e. definition, characterization of an en-
tity, verb modifier or verb complement, etc.
3.2 The RITEL systems
3.3 General overview
The RITEL system (see Figure 1) which we used
in these experiments is fully described in (Bernard
et al, 2009). This system has been devel-
oped within the framework of the Ritel project
which aimed at building a human-machine dia-
logue system for question-answering in open do-
main (Toney et al, 2008).
The same multilevel analysis is carried out on
both queries and documents. The objective of this
analysis is to find the bits of information that may
be of use for search and extraction, called perti-
nent information chunks. These can be of dif-
ferent categories: named entities, linguistic enti-
ties (e.g., verbs, prepositions), or specific entities
(e.g., scores). All words that do not fall into such
chunks are automatically grouped into chunks via
a longest-match strategy. The analysis is hierar-
chical, resulting in a set of trees. Both answers
and important elements of the questions are sup-
posed to be annotated as one of these entities.
The first step of the QA system itself is to build
a search descriptor (SD) that contains the impor-
tant elements of the question, and the possible
answer types with associated weights. Answer
types are predicted through rules based on com-
binations of elements of the question. On all sec-
ondary and mandatory chunks, the possible trans-
formations (synonym, morphological derivation,
etc.) are indicated and weighted in the SD. Docu-
ments are selected using this SD. Each element of
the document is scored with the geometric mean
of the number of occurrences of all the SD ele-
ments that appear in it, and sorted by score, keep-
ing the n-best. Snippets are extracted from the
document using fixed-size windows and scored
using the geometrical mean of the number of oc-
89
currences of all the SD elements that appear in the
snippet, smoothed by the document score.
3.3.1 Answer selection and ranking
Two different strategies are implemented in RI-
TEL. The first one is based on distance between
question words and candidate answer, named RI-
TEL Standard. The second one is based on a
Bayesian model, named RITEL Probabilistic.
Distance-based answer scoring The snippets
are sorted by score and examined one by one in-
dependently. Every element in a snippet with a
type found in the list of expected answer types of
the SD is considered an answer candidate. RITEL
associates to each candidate answer a score which
is the sum of the distances between itself and the
elements of the SD. That score is smoothed with
the snippet score through a ?-ponderated geomet-
ric mean. All the scores for the different instances
of the same element are added together. The enti-
ties with the best scores then win. The scores for
identical (type,value) pairs are added together and
give the final scoring to the candidate answers.
Answer scoring through Bayesian modeling
This method of answer scoring is built upon a
Bayesian modeling of the process of estimating
the quality of an answer candidate. This approach
relies on multiple elementary models including
element co-occurrence probabilities, question el-
ement appearance probability in the context of a
correct answer and out of context answer proba-
bility. The model parameters are either estimated
on the documents or are set empirically. This sys-
tem has not better result than the distance-based
one but is interesting because it allows to obtain
different correct answers.
3.4 Systems combination
The systems we used in these experiments are
very different especially with respect to the pas-
sage selection and the answer extraction and scor-
ing methods. The QAVAL system proceeds to
the passage selection before any analysis while
the two RITEL systems do a complete and multi-
level analysis on the documents before the pas-
sage selection. Concerning the answer extraction
and scoring, the QAVAL system uses an answer
validation process based on machine learning ap-
proach while the answer extraction of the RITEL-
S system uses a distance-based scoring and the
RITEL-P Bayesian models. It seems then inter-
esting to combine these various approaches in a
in-system way (see Section 4): (1) the passages
selected by the QAVAL system are provided as
document collection to the RITEL systems; (2)
the candidate answers provided by the RITEL
systems are given to the answer validation mod-
ule of the QAVAL system.
We also worked, in a more classical way, on
interleaving results of answer selection methods
(see Section 5 and 6). These methods make use of
the various information provided by the different
systems along with all candidate answers.
4 Internal combination
4.1 QAVAL snippets used by RITEL
The RITEL system proceeds to a complete analy-
sis of the document which is used during the doc-
ument and selection extraction procedure and ob-
tains 80.3% of the questions having a correct an-
swer in at least one passage. The QAVAL system
extracts short passages (150) using Lucene and
obtains a score of 88%. We hypothesized that the
RITEL?s fine-grained analysis could better work
on small collection than on the overall document
collection (combination 1 Fig. 1). We consider
the passages extracted by the QAVAL system be-
ing a new collection for the RITEL system. First,
the analysis is done on this new collection and
the analysis result is indexed. Then the gen-
eral question-answering procedures are applied:
question analysis, SD construction, document and
snippet extraction and then answer selection and
ranking. The two answer extraction methods have
been applied and the results are presented in the
Table 1. This simple approach does not allow any
All documents QAVAL? snippets
Ritel-S Ritel-P Ritel-S Ritel-P
top-1 34.0% 22.4% 29.9% 22.4%
MRR 0.41 0.29 0.38 0.32
top-20 61.2% 48.7% 54.4% 49.7%
Table 1: Results of Ritel systems (Ritel-S used
the distance-based answer scoring, Ritel-P used the
Bayesian modeling) working on the QAVAL? snippets.
improvement. Actually all the results are worsen-
ing, except maybe for the Ritel-P systems (which
is actually not the best one). One of our hypoth-
esis is that the QAVAL snippets are too short and
90
do not fit the criteria used by the RITEL system.
4.2 Answer validation
In QAVAL, answer ranking is done by an an-
swer validation module (fully described in sec-
tion 3.1). The candidate answers ranked by this
module are associated to a confidence score. The
objective of this answer validation module is to
decide whether the candidate answer is correct or
not given an associated snippet. The objective is
to use this answer validation module on the candi-
date answers and the snippets provided by all the
systems (combination 2 Fig. 1). Unfortunately,
this method did not obtain better results than the
best system. We assume that this module being
learnt on the QAVAL data only is not robust to
different data and more specifically to the passage
length which is larger in RITEL than in QAVAL.
A possible improvement could be to add answers
found by the RITEL system in the training base.
5 Voting methods and scores
combination
These methods are based on a comparison be-
tween the candidate answers: are they identical ?
An observation that can be made concerning the
use of a strict equality between answers is that in
some cases, 2 different answers can be more or
less identical. For example if one system returns
?Sarkozy? and another one ?Nicolas Sarkozy? we
may want to consider these two answers as iden-
tical. We based the comparison of answers on the
notion of extended equality. For that, we used
morpho-syntactic information such as the lemmas
and the part of speech of each words of the an-
swers. The TreeTagger tool3 has been used. An
answer R1 is then considered as included in an
answer R2 if all non-empty words of R1 are in-
cluded in R2. Two words having the same lemma
are considered as identical. For example ?chanta?
and ?chanterons? are identical because they share
the same lemma ?chanter?. Adjectives, proper
names and substantives are considered as non-
empty words. Following this definition, two an-
swers R1 and R2 are considered identical if R1 is
included in R2 and R2 in R1.
3www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
5.1 Merge based on candidate answer rank
The first information we used takes into account
the rank of the candidate answers. The hypothesis
beyond this is that the systems often provide the
correct answer at first position, if they found it.
5.1.1 Simple interleaving
The first method, and probably the simplest, is
to merge the candidate answers provided by all
the systems: the first candidate answer of the first
system is ranked in the first position; the first an-
swer of the second system is ranked in the sec-
ond position; the second answer of the first sys-
tem is ranked in the third position, and so on. If
one answer was already merged (because ranked
at a higher rank by another system), it is not used.
We choose to base the systems order given their
individual score. The first system is QAVAL, the
second RITEL-S and the third RITEL-P. Follow-
ing that method, the accuracy (percentage of cor-
rect answers at first rank) is the one obtained by
the best system. But we assume that the MRR at
the top-n (with n > 1) would be improved.
5.1.2 Sum of the inverse of the rank
The simple interleaving method does not take
into account the answer rank provided by the dif-
ferent systems. However, this information may
be relevant and was used in order to merge can-
didate answer extracted from different document
collection, Web articles and news paper (Chalen-
dar et al, ). In our case, answers are extracted
from the same document collection by the dif-
ferent systems. Then it is possible that the same
wrong answers will be extracted by the different
systems.
A first possible method to take into account
the rank provided by the systems is to weight the
candidate answer using this information. For a
same answer provided by the different systems,
the weight is the sum of the inverse of the rank
given by the systems. To compare the answers the
strict equality is applied. If a system ranks an an-
swer at the first position and another system ranks
the same answer at the second position, the weight
is 1.5 (1 + 12 ). The following equation express in
a more formalized way this method.
weight =
? 1
rank
Comparing to the previous method, that one
should allow to place more correct answers at the
first rank.
91
5.2 Using confidence scores
In order to rank all their candidate answers, the
systems used a confidence score associated to
each candidate answer. We then wanted to use
these confidence scores in order to re-rank all the
candidate answers provided by all the systems.
But this is only possible if all systems produce
comparable scores. This is not the case. QAVAL
produces scores ranging from -1 to +1. RITEL-
P, being probabilistic, produces a score between 0
and +1. And RITEL-S does not use strict interval
and the scores are potentially ranged from ?? to
+?. The following normalization (a linear re-
gression) has been applied to the RITEL-S and
RITEL-P scores in order to place it in the range
-1 to 1.
valuenormalized =
2 ? valueorigin
valMin ? valMax
? 1
5.2.1 Sum of confidence scores
In order to compare our methods with classi-
cal approaches, we used two methods presented
in (Shaw and Fox, 1994):
? CombSum which adds the different confi-
dence scores of an answer given by the dif-
ferent systems;
? CombMNZ which adds the confidence
scores of the different systems and multiply
the obtained value by the number of systems
having found the considered answer.
5.2.2 Hybrid method
An hybrid method combining the rank and the
confidence score has been defined. The weight is
the sum of two elements: the higher confidence
score and a value taking into account the rank
given by the different systems. This value is de-
pendent on the number of answers, the type of the
equality (the answers are included or equal) which
results in the form of a bonus, and the rank of the
different considered answers. The weight of an
answer a to a question q is then:
w(a) = s(a) +
?
be ? (|a(q)| ?
?
r(a)) (1)
with be the equality bonus, w the weight, s, the
score and r the rank.
The equality bonus, found empirically, is given
for each systems pair. The value is 3 if the two
answers are equal, 2 if an answer is included in
the other and 1 otherwise. When an answer is
found by two or more systems, the higher con-
fidence score is kept. The result of this method is
that the answers extracted by more than one sys-
tem are favored. An answer found by only one
system, even with a very high confidence score,
may be downgraded.
6 Machine-learning-based method for
answer re-ranking
To solve a re-ranking problem, machine learn-
ing approaches can be used (for example (Mos-
chitti et al, 2007)). But in most of the cases,
the objective is to re-rank answers provided by
one system, that means to re-rank multiple hy-
potheses from one system. In our case, we want
to re-rank multiple answers from different sys-
tems. We decided to use an SVM-based approach,
namely SVMrank (Joachims, 2006), which is well
adapted to our problem. An important aspect is
then to choose the pertinent features for such a
task. Our objective is to consider robust enough
features to deal with different systems? answers
without introducing biases. Two classes of char-
acteristic should be able to give a useful represen-
tation of the answers: those related to the answer
itself and those related to the question.
6.1 Answer characteristics
First of all, we should use the rank and the score
as we did in the preceding merging methods. The
problem may appear here because not all candi-
date answers are found by the different systems.
In that case, the score and the rank given to these
systems is then -2. It guarantees us that the fea-
tures are out of the considered range [?1,+1].
Considering that, it may be useful to know which
system provided the considered answer. For each
answer all systems having found that answer are
indicated. Moreover this information may help
to distinguish answers coming from for example
QAVAL and RITEL-S or RITEL-P from answers
coming from RITEL-S and RITEL-P. The two RI-
TEL systems share most of the modules and their
answers may have the same problems. Concern-
ing the answer, another aspect may be of interest:
how many time this answer has been found? The
question is not, how many times the answer ap-
pears in the documents but how many times the
answer appears in a context allowing this answer
92
to be considered as a candidate answer. We used
the number of different snippets selected by the
systems in which that answer was found.
6.2 Question characteristics
When observing the results obtained by the sys-
tems on different questions, we observed that the
?kind? of the question has an impact on the sys-
tems? performance. More specifically, it is largely
accepted in the community that at least two crite-
ria are of importance: the length of the question,
and the type of the expected answer (EAT).
Question length We may consider that the length
of the questions is more or less a good indicator
for the complexity level of the question. The num-
ber of non-empty words of the question can then
be a interesting feature.
Expected answer type One of the task of the
question processing, in a classical Question-
Answering system, is to decide of which type will
be the answer. For example, for a question like
Who is the president of France? the type of the
expected answer will be a named entity of the
class person and for a question like what wine to
drink with seafood? that the EAT is not a named
entity. (Grappy, 2009) observed that the QAVAL
system is better when the EAT is of a named entity
class. It is possible that adding this information
will, during the learning phase, positively weight
an answer coming from RITEL when the EAT is
not a named entity.
The value of this feature indicates the compat-
ibility of the answer and the EAT. We used the
method presented in (Grappy and Grau, 2010) and
already used for the answer validation module of
the QAVAL system. This method is based on a
ML-based combination of different methods us-
ing named entity dictionaries, wikipedia knowl-
edge, etc. This system gives a confidence score,
ranging from -1 to +1 which indicates the con-
fidence the system has in compatibility between
the answer and the EAT. In some cases, the ques-
tion processing module may indicate if the EAT
is of a more fine-grained entity. For example, the
question Who is the president of France? is not
only waiting for a person but more precisely for a
person having the function of a president. A new
feature is then added. If the EAT is a fine-grained
named entity, then the value is 1 and -1 otherwise.
7 Experiments and results
7.1 Data and observations
For the training of the SVM model, we used
the answers to 104 questions provided by the
2009 Quaero evaluation campaign (Quintard et
al., 2010). Only 104 questions have been used be-
cause we need to have at least one correct answer
provided by at least one system in the training
base for each question. Models have been trained
using 5, 10, 15 and 20 answers for each system.
For the evaluation, we used 147 factoid ques-
tions used in the 2010 Quaero4 evaluation cam-
paign. The document collection is made of
500,000 Web pages5. We used the Mean Re-
ciprocal Rank (MRR) as it is a usual metric in
Question-Answering on the first five candidate
answers. The MRR is the average of the recip-
rocal ranks of all considered answers. We also
used the top-1 metric which indicates the number
of correct answers ranked at the first position.
The baseline results, provided by each of the
three systems, are presented in Table 2. QAVAL
and RITEL-S have quite similar results which are
higher than those obtained by the RITEL-P sys-
tem. We can observe that, within the 20 top ranks,
38% of the questions have an answer given by
all the systems, 76 % by at least 2 systems and
21% receive no correct answers. The best possi-
ble result that could be obtained by a perfect fu-
sion method is also indicated in this table (0.79 of
MRR and 79% for top-1). Such a method would
lead to rank first each correct answer found by at
least a system. Figure 2 presents the answer repar-
System MRR % top-1 (#)
QAVAL 0.45 36 (53)
RITEL-S 0.41 32 (47)
RITEL-P 0.26 18 (27)
Perfect fusion 0.79 79 (115)
Table 2: Baseline results
tition between ranks 2 and 20 (the numbers of cor-
rect answers in first rank are given in Table 2).
This figure shows that the systems ranked the cor-
rect answer mostly in the first positions. That
means that these systems are relatively effective
for re-ranking their own candidate answers. Very
4http://www.quaero.org
5crawled by Exalead http://www.exalead.com/
93
few correct answers are ranked after the tenth po-
sition. Following these observations, the evalua-
tions are done on the first 10 candidate answers.
2 3 4 5 6 7 8 9 10 200
2
4
6
8
10
12
14
16
18
20
22
QAVALRITEL-S RITTEL-PSVM
3 4 5 6 7 8 9 1
Figure 2: Answer repartition
7.2 Results and analysis
Table 3 presents the results obtained with the dif-
ferent merging methods: simple interleaving (In-
ter.), Sum of the inverse of the rank, CombSum,
CombMNZ, hybrid method (Hyb. Meth.) and
SVM model. In order to evaluate the impact of the
RITEL-P (which achieved less good results), the
results are given using two (QAVAL and RITEL-
S) or three systems.
Method MRR % Top-1 (#)
(2 sys. / 3 sys.) (2 sys. / 3 sys.)
Inter. 0.47 / 0.45 36 (53) / 36 (53)
? 1
rang 0.48 / 0.46 38 (56)/ 36 (53)
CombSum 0.46 / 0.44 38 (56) / 34 (50)
CombMNZ 0.46/ 0.44 38 (56) / 35 (51)
Hyb. meth. 0.49 /0.44 40 (58) / 34 (50)
SVM 0.48 / 0.51 39 (57) / 42 (62)
QAVAL 0.44 36 (53)
Table 3: General results.
As shown in Table 3, the different methods
improve the results and the best method is the
SVM-based model which allows an improvement
of 19% of correct answer at first rank. This re-
sult is significantly better than the baseline result
and this method can be considered as very effec-
tive. Figure 2 shows the results of this model. In
order to validate our choice of using the SVM-
Rank model, we also tested the use of a com-
bination of decision trees, as QAVAL obtained
# candidate answers % Top-1 (#)
20 39 (58)
15 39 (58)
10 43 (63)
5 37 (55)
Table 4: Impact of the number of candidate answers
normalization MRR # Top-1
without 0.49 58 (39%)
with 0.51 63 (43%)
Table 5: Impact of the normalization
good results with this classifier in the validation
module. We obtained a MRR of 0.44 which is
obviously lower than the result obtained by the
SVM method. Generally speaking, the methods
taking into account the answer rank allow better
results than the methods using the answer confi-
dence score. Another interesting observation is
that the interleaving methods obtained better re-
sults when not using the RITEL-P system while
the SVM one obtained better results when using
the three systems. We assume that these two sys-
tems, RITEL-S and RITEL-P are too similar to
provide strict useful information, but that a ML-
based approach is able to generalize such infor-
mation.
In order to validate our choice of using only
the first ten candidate answers, we did some more
tests using 5, 10, 15 and 20 candidate answers.
Table 4 shows the results obtained with the SVM
model. We can see that is is better to consider
10 candidate answers. Beyond the first 10 can-
didate answers it is difficult to re-rank the cor-
rect answer without adding unsustainable noise.
Moreover most of the correct answers are in the
first ten candidates.
In order to validate the confidence score nor-
malization, we did experiments with and without
this normalization. Table 5 presents results which
validate our choice.
To better understand how the fusion is made,
we observed the repartition of the correct answers
at the first rank and at the top five ranks according
to the number of systems which extracted them
(figure 3 and figure 4). We do this for the three
best fusion approaches: the ML method with 3
systems, the hybrid method and the sum of the in-
verse of the ranks with two systems. As we can
94
Feuille1
Page 1
SVM Hybrid sum 1/rank
%
5%
10%
15%
20%
25%
30%
35%
40%
45%
50%
27%
12%
36% 33%
4%
3% 4%
1 system
2 systems
3 systems
Figure 3: First rank Feuille1
Page 1
SVM Hybrid sum 1/rank
%
15%
05%
25%
35%
45%
75%
65%
26%
14%
40% 43%
10% 15% 10%
1 system
0 systems
2 systems
Figure 4: Top five ranks
see, in most of the cases, the three approaches of-
ten rank the correct answers found by all the sys-
tems. The best approach is the SVM-based one.
It ranks 98 % of the correct answers given by the
3 systems in top 5 ranks. It also ranks better cor-
rect answers given by 2 systems (60% are ranked
in the top 5 ranks versus about 48 % with the two
other methods).
The rank-based method is globally reliable for
selecting correct answers in the top 5 ranks. This
behavior is consistent with the fact that our QA
systems, when they found a correct answer, gen-
erally rank it in first positions.
Some correct answers given by only one sys-
tem remain in the first position, and about 10%
of them remain in the top 5 ranks and are not su-
perseded by common wrong answers. However
the major part of these correct single-system an-
swers are discarded after the 5 first ranks (39% of
them by the SVM method, 45% by the rank-based
method and 53% by the hybrid method). In that
case, a ML method is a better solution for decid-
ing, however an improvement would be possible
only if other features could be found for a better
characterization of a correct answer, or maybe by
enlarging the training base.
According to these results, we also can expect
that with more QA systems, a fusion approach
would be more effective.
8 Conclusion
Improving QA systems is a very difficult task,
given the variability of the pairs (question / an-
swering passages), the complexity of the pro-
cesses and the variability of they performances.
Thus, an improvement can be searched by the hy-
bridization of different QA systems. We studied
hybridization at different levels, internal combi-
nation of processes and merging of end-results.
The first combination type did not proved to be
useful, maybe because each system has its global
coherence leading their modules to be more in-
terdependent than expected. Thus it appears
that combining different strategies is better re-
alized with the combination of their end-results,
specially when these strategies obtain good re-
sults. We proposed different combination meth-
ods, based on the confidence scores, the answer
rank, that are adapted to the QA context, and
a ML-method which considers more features for
characterizing the answers. This last method ob-
tains the better results, even if the simpler ones
also show good results. The proposed methods
can be applied to other QA systems, as the fea-
tures used are generally provided by the systems.
References
R.M. Aceves-Pe?rez, M. Montes-y Go?mez, L. Vil-
lasen?or-Pineda, and L.A. Uren?a-Lo?pez. 2008. Two
approaches for multilingual question answering:
Merging passages vs. merging answers. Interna-
tional Journal of Computational Linguistics & Chi-
nese Language Processing, 13(1):27?40.
G. Bernard, S. Rosset, O. Galibert, E. Bilinski, and
G. Adda. 2009. The LIMSI participation to the
QAst 2009 track. In Working Notes of CLEF 2009
Workshop, Corfu, Greece, October.
G. De Chalendar, T. Dalmas, F. Elkateb-gara, O. Fer-
ret, B. Grau, M. Hurault-plantet, G. Illouz, L. Mon-
ceaux, I. Robba, and A. Vilnat. The question an-
swering system QALC at LIMSI: experiments in
using Web and WordNet.
Arnaud Grappy and Brigitte Grau. 2010. Answer type
validation in question answering systems. In Adap-
95
tivity, Personalization and Fusion of Heterogeneous
Information, RIAO ?10, pages 9?15.
Arnaud Grappy, Brigitte Grau, Mathieu-Henri Falco,
Anne-Laure Ligozat, Isabelle Robba, and Anne Vil-
nat. 2011. Selecting answers to questions from web
documents by a robust validation process. In The
2011 IEEE/WIC/ACM International Conference on
Web Intelligence.
Arnaud Grappy. 2009. Validation de rponses dans un
systme de questions rponses. Ph.D. thesis, Universit
Paris Sud, Orsay.
Valentin Jijkoun and Maarten De Rijke. 2004. Answer
Selection in a Multi-Stream Open Domain Question
Answering System. In Proceedings 26th European
Conference on Information Retrieval (ECIR?04),
volume 2997 of LNCS, pages 99?111. Springer.
Thorsten Joachims. 2006. Training linear SVMs
in linear time. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge
discovery and data mining, KDD ?06, pages 217?
226, New York, NY, USA. ACM.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
Syntactic and Shallow Semantic Kernels for Ques-
tion Answer Classification. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 776?783, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Ludovic Quintard, Olivier Galibert, Gilles Adda,
Brigitte Grau, Dominique Laurent, Veronique
Moriceau, Sophie Rosset, Xavier Tannier, and Anne
Vilnat. 2010. Question Answering on Web Data:
The QA Evaluation in Quaero. In LREC?10, Val-
letta, Malta, May.
Joseph A. Shaw and Edward A. Fox. 1994. Combina-
tion of multiple searches. In TREC-2. NIST SPE-
CIAL PUBLICATION SP.
Alberto Tellez-Valero, Manuel Montes Gomez,
Luis Villasenor Pineda, and Anselmo Penas. 2010.
Towards multi-stream question answering using
answer validation. Informatica, 34(1):45?54.
Dave Toney, Sophie Rosset, Aurlien Max, Olivier Gal-
ibert, and Eric Bilinski. 2008. An Evaluation of
Spoken and Textual Interaction in the RITEL Inter-
active Question Answering System. In European
Language Resources Association (ELRA), editor,
Proceedings of the Sixth International Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco, May.
96
