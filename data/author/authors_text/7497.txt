TSUBAKI: An Open Search Engine Infrastructure for
Developing New Information Access Methodology
Keiji Shinzato?, Tomohide Shibata?, Daisuke Kawahara?,
Chikara Hashimoto?? and Sadao Kurohashi?
?Graduate School of Informatics, Kyoto University
?National Institute of Information and Communications Technology
??Department of Informatics, Yamagata University
{shinzato, shibata, kuro}@nlp.kuee.kyoto-u.ac.jp
dk@nict.go.jp ch@yz.yamagata-u.ac.jp
Abstract
As the amount of information created by
human beings is explosively grown in the
last decade, it is getting extremely harder
to obtain necessary information by conven-
tional information access methods. Hence,
creation of drastically new technology is
needed. For developing such new technol-
ogy, search engine infrastructures are re-
quired. Although the existing search engine
APIs can be regarded as such infrastructures,
these APIs have several restrictions such as a
limit on the number of API calls. To help the
development of new technology, we are run-
ning an open search engine infrastructure,
TSUBAKI, on a high-performance comput-
ing environment. In this paper, we describe
TSUBAKI infrastructure.
1 Introduction
As the amount of information created by human be-
ings is explosively grown in the last decade (Uni-
versity of California, 2003), it is getting extremely
harder to obtain necessary information by con-
ventional information access methods, i.e., Web
search engines. This is obvious from the fact that
knowledge workers now spend about 30% of their
day on only searching for information (The Del-
phi Group White Paper, 2001). Hence, creation of
drastically new technology is needed by integrating
several disciplines such as natural language process-
ing (NLP), information retrieval (IR) and others.
Conventional search engines such as Google and
Yahoo! are insufficient to search necessary informa-
tion from the current Web. The problems of the con-
ventional search engines are summarized as follows:
Cannot accept queries by natural language sen-
tences: Search engine users have to represent their
needs by a list of words. This means that search
engine users cannot obtain necessary information if
they fail to represent their needs into a proper word
list. This is a serious problem for users who do not
utilize a search engine frequently.
Cannot provide organized search results: A
search result is a simple list consisting of URLs,
titles and snippets of web pages. This type of re-
sult presentation is obviously insufficient consider-
ing explosive growth and diversity of web pages.
Cannot handle synonymous expressions: Exist-
ing search engines ignore a synonymous expression
problem. Especially, since Japanese uses three kinds
of alphabets, Hiragana, Katakana and Kanji, this
problem is more serious. For instance, although both
Japanese words ????? and ???? mean child,
the search engines provide quite different search re-
sults for each word.
We believe that new IR systems that overcome the
above problems give us more flexible and com-
fortable information access and that development
of such systems is an important and interesting re-
search topic.
To develop such IR systems, a search engine in-
frastructure that plays a low-level layer role (i.e., re-
trieving web pages according to a user?s query from
a huge web page collection) is required. The Appli-
cation Programming Interfaces (APIs) provided by
189
commercial search engines can be regarded as such
search engine infrastructures. The APIs, however,
have the following problems:
1. The number of API calls a day and the num-
ber of web pages included in a search result are
limited.
2. The API users cannot know how the acquired
web pages are ranked because the ranking mea-
sure of web pages has not been made public.
3. It is difficult to reproduce previously-obtained
search results via the APIs because search en-
gine?s indices are updated frequently.
These problems are an obstacle to develop new IR
systems using existing search engine APIs.
The research project ?Cyber Infrastructure for the
Information-explosion Era1? gives researchers sev-
eral kinds of shared platforms and sophisticated
tools, such as an open search engine infrastructure,
considerable computational environment and a grid
shell software (Kaneda et al, 2002), for creation of
drastically new IR technology. In this paper, we de-
scribe an open search engine infrastructure TSUB-
AKI, which is one of the shared platforms devel-
oped in the Cyber Infrastructure for the Information-
explosion Era project. The overview of TSUBAKI is
depicted in Figure 1. TSUBAKI is built on a high-
performance computing environment consisting of
128 CPU cores and 100 tera-byte storages, and it
can provide users with search results retrieved from
approximately 100 million Japanese web pages.
The mission of TSUBAKI is to help the develop-
ment of new information access methodology which
solves the problems of conventional information ac-
cess methods. This is achieved by the following
TSUBAKI?s characteristics:
API without any restriction: TSUBAKI pro-
vides its API without any restrictions such as the
limited number of API calls a day and the number
of results returned from an API per query, which are
the typical restrictions of the existing search engine
APIs. Consequently, TSUBAKI API users can de-
velop systems that handle a large number of web
pages. This feature is important for dealing with the
Web that has the long tail aspect.
1http://i-explosion.ex.nii.ac.jp/i-explosion/ctr.php/m/Inde-
xEng/a/Index/
	







 	
Extracting Hyponyms of Prespecified Hypernyms
from Itemizations and Headings in Web Documents
Keiji Shinzato Kentaro Torisawa
Japan Advanced Institute of Science and Technology,
1-1 Asahidai, Tatsunokuchi-machi, Nomi-gun, Ishikawa, 923-1292 JAPAN
{skeiji,torisawa}@jaist.ac.jp
Abstract
This paper describes a method to acquire hy-
ponyms for given hypernyms from HTML doc-
uments on the WWW. We assume that a head-
ing (or explanation) of an itemization (or list-
ing) in an HTML document is likely to contain
a hypernym of the items in the itemization, and
we try to acquire hyponymy relations based on
this assumption. Our method is obtained by ex-
tending Shinzato?s method (Shinzato and Tori-
sawa, 2004) where a common hypernym for ex-
pressions in itemizations in HTML documents is
obtained by using statistical measures. By us-
ing Japanese HTML documents, we empirically
show that our proposed method can obtain a sig-
nificant number of hyponymy relations which
would otherwise be missed by alternative meth-
ods.
1 Introduction
Hyponymy relations can play a crucial role in var-
ious NLP systems, and there have been many at-
tempts to develop automatic methods to acquire hy-
ponymy relations from text corpora (Hearst, 1992;
Caraballo, 1999; Imasumi, 2001; Fleischman et al,
2003; Morin and Jacquemin, 2003; Ando et al,
2003). Most of these techniques have relied on par-
ticular linguistic patterns, such as ?NP such as NP.?
The frequencies of use for such linguistic patterns
are relatively low, though, and there can be many ex-
pressions that do not appear in such patterns even
if we look at large corpora. The effort of searching
for other clues indicating hyponymy relations is thus
significant.
Our aim is to extract hyponyms of prespecified hy-
pernyms from the WWW. We use itemizations (or
lists) in HTML documents, such as the one in Fig-
ure 1(A), and their headings (?Car Company List? in
the figure). In a similar attempt, Shinzato and Tori-
sawa proposed an automatic method to obtain a com-
mon hypernym of expressions in the same itemiza-
tions in HTML documents (Shinzato and Torisawa,
2004) by using statistical measures such as document
frequencies and inverse document frequencies. In
the following, we call this method the Algorithm for
Hyponymy Relation Acquisition from Itemizations
(AHRAI). On the other hand, the method we propose
in this paper is called Hyponym Extraction Algorithm
? Car Company List
? Toyota
? Honda
? Nissan
? Car List
? Toyota
? Honda
? Nissan
(A) (B)
Figure 1: Examples of itemization
from Itemizations and Headings (HEAIH).
The difference between AHRAI and HEAIH is
that HEAIH uses the headings attached to item-
izations, while AHRAI obtains hypernyms without
looking at the headings. This difference has a sig-
nificant consequence in the acquisition of hyponymy
relations. A hyponym tends to have more than one
hypernym. For instance, ?Toyota? can have at least
two hypernyms, ?company? and ?car.? AHRAI may
be able to obtain ?company,? for instance, from the
itemizations presented in Figure 1(A), but it can-
not simultaneously obtain ?car.? Consider the item-
ization in Figure 1(B). Although the heading of the
itemization says that the items in the itemizations are
cars, AHRAI will provide ?company? as a hypernym
of the itemizations. This is because AHRAI does not
use the headings as clues for finding hypernyms and
the itemizations in (A) and (B) are actually identi-
cal. Of course, the method could produce the hy-
pernym ?car? from different itemizations; this is un-
likely, though, because the itemizations suggesting
that ?Toyota? is a ?car? are likely to again include
the names of other car manufactures such as ?Nis-
san? and ?Honda,? so the itemization must be more
or less similar to the ones in the figure. In such situa-
tions, the procedure is likely to consistently produce
?company? instead of ?car.?
On the other hand, HEAIH can simultaneously
recognize ?Toyota? as a hyponym of the two hy-
pernyms by using the headings. Given a set of hy-
pernyms, for which we?d like to acquire their hy-
ponyms, HEAIH finds the headings (or, more pre-
cisely, candidates of headings) that include the given
hypernyms, and extracts the itemizations which are
located near the headings. The procedure then pro-
duces hyponymy relations under the assumption that
the expressions in the itemizations are hyponyms of
the given hypernym. For example, given ?car? and
?car company? as hypernyms, the procedure finds
documents including headings such as ?Car Com-
pany List? and ?Car List.? If it is lucky enough, it
finds documents such as those in Figure 1, and ex-
tracts the expressions ?Toyota? ?Honda,? and ?Nis-
san? from the itemizations near the headings. It will
then obtain that ?Toyota? is a hyponym of ?car com-
pany? from document (A) in the figure, while it finds
that ?Toyota? is a hyponym of ?car? from (B).
However, the task is not that simple. A problem is
that we do not know how to identify the correspon-
dence between itemizations and their headings pre-
cisely. One may think that, for instance, she can use
the distance between an itemization and (candidates
of) its heading in the HTML file as a clue for finding
the correspondence. However, we empirically show
that this is not the case. To solve this problem, we
used a modified version of AHRAI. This method can
produce a ranked list of hypernym candidates from
the itemizations only. We assume that if the top n el-
ements of a ranked list produced by AHRAI include
many substrings of a given hypernym, the heading
including the hypernym is attached to the itemiza-
tion.
Note that the original AHRAI produced the top
element of the ranked list as a hypernym, while
HEAIH recognizes a string as a hypernym if its sub-
strings are included in the top n elements in the
ranked list. This helps the HEAIH to acquire hy-
ponymy relations that the AHRAI cannot. Consider
the itemizations in Figure 1. AHRAI may produce
?company? as the top element of a ranked list for
both (A) and (B). But if ?car? is in the top n ele-
ments in the list as well, HEAIH can recognize ?car?
as a hypernym for (B).
This paper is organized as follows. Section 2 de-
scribes AHRAI. Our proposed method, HEAIH, is
presented in Section 3. The experimental results ob-
tained by using Japanese HTML documents are pre-
sented in Section 4, where we compared our method
and alternative methods.
2 Previous Work: AHRAI
The Algorithm for Hyponymy Relation Acquisition
from Itemization (AHRAI) acquires hyponymy rela-
tions from HTML documents according to three as-
sumptions.
Assumption A Expressions included in the same
itemization or listing in an HTML document are
likely to have a common hypernym.
Assumption B Given a set of hyponyms that have
a common hypernym, the hypernym appears in
many documents that include the hyponyms.
Assumption C Hyponyms and their hypernyms are
semantically similar.
We call expressions in an itemization hyponym
candidates. A set of the hyponym candidates ex-
tracted from a single itemization or list is called a
hyponym candidate set (HCS). For the itemization
in Figure 1 (A), we would treat Toyota, Honda, and
Nissan as hyponym candidates, and regard them as
members of the same HCS.
The procedure consists of the following four steps.
Note that Steps 1-3 correspond to Assumptions A-C.
Step 1 Extraction of hyponym candidates from
itemized expressions in HTML documents.
Step 2 Selection of a hypernym candidate by us-
ing document frequencies and inverse docu-
ment frequencies.
Step 3 Ranking of hypernym candidates and HCSs
based on semantic similarities between hyper-
nym and hyponym candidates.
Step 4 Application of a few additional heuristics to
elaborate computed hypernym candidates and
hyponym candidates.
Step 1 is performed by using a rather simple al-
gorithm operating on HTML tags. See Shinzato and
Torisawa, 2004, for more details. The other steps are
described in the following.
2.1 Step 2
In Step 2, the procedure selects a common hyper-
nym candidate for an HCS. First, two sets of doc-
uments are prepared. The first set of documents is
a large number of HTML documents that are ran-
domly selected and downloaded. This set of doc-
uments is called a global document set, and is as-
sumed to indicate the general tendencies of word fre-
quencies. Then the procedure downloads the docu-
ments including each hyponym candidate in a given
HCS by using an existing search engine 1. This doc-
ument set is called a local document set, and is used
to determine the strength of the association of nouns
with the hyponym candidates.
Let us denote a given HCS as C, a local document
set obtained from all the items in C as LD(C), and
a global document set as G. N is a set of the nouns
that can be hypernym candidates2 A hypernym can-
didate, denoted as h(C), for C is obtained through
the following formula.
h(C) = argmaxn?N{hS(n,C)}
hS(n,C) = df(n,LD(C)) ? idf(n,G)
df(n,D) is a document frequency, which is actually
the number of documents including a noun n in a
document set D. idf(n,G) is an inverse document
frequency, which is defined as log (|G|/df(n,G)).
1As in Shinzato and Torisawa, 2004, we used the search en-
gine ?goo.? (http://www.goo.ne.jp). Note that we enclosed the
strings to be searched by ?? so that the engine does not split them
to words automatically.
2We simply used the most frequent nouns observed in a large
corpora as N .
The score hS has a large value for a noun that ap-
pears in a large number of documents in the local
document set and is found in a relatively small num-
ber of documents in the global document set. This
reflects Assumption B given above.
2.2 Step 3
By Step 2, the procedure can produce pairs of a hy-
pernym candidate and an HCS, which are denoted by
{?h(Ci), Ci?}mi=1. Here, Ci is an HCS, and h(Ci) is
a common hypernym candidate for hyponym candi-
dates in an HCS Ci.
In Step 3, the similarity between hypernym candi-
dates and hyponym candidates is considered to ex-
clude non-hypernyms that are strongly associated
with hyponym candidates from the hypernym can-
didates obtained by h(C), according to Assumption
C. For instance, non-hypernym ?price? may be a
value of h({Toyota,Honda}) because it is strongly
associated with the words Toyota and Honda in
HTML documents. Such non-hypernyms are ex-
cluded based on the assumption that non-hypernyms
have relatively low semantic similarities to the hy-
ponym candidates, while the behavior of true hy-
pernyms should be semantically similar to the hy-
ponyms. In the ?price? example, the similarity be-
tween ?price? and ?Toyota? is relatively low, and we
can expect that ?price? is excluded from the output.
The semantic similarities between hyponym can-
didates in an HCS C and a hypernym candidate n
are computed using a cosine measure between co-
occurrence vectors:
sim(n,C) = (ho(C) ? hy(n))/(|ho(C)||hy(n)|).
Here, ho(C) denotes a co-occurrence vector of hy-
ponym candidates, while hy(n) is the co-occurrence
vector of a hypernym candidate n. Assume that
all possible argument positions are denoted as
{p1, ? ? ? , pl} and {v1, ? ? ? , vo} denotes a set of verbs.
Then, the above vectors are defined as follows.
ho(C) = ?fh(C, p1, v1), ? ? ? , fh(C, pl, vo)?
hy(n) = ?f(n, p1, v1), ? ? ? , f(n, pl, vo)?
Here, fh(C, p, v) denotes the frequency of the hy-
ponym candidates in an HCS C occupying an argu-
ment position p of a verb v in a local document set
and f(n, p, v) is the frequency of a noun n occupying
an argument position p of a verb v in a large docu-
ment set.
The procedure sorts the hypernym-HCS pairs
{?h(Ci), Ci?}mi=1 using the value
sim(h(Ci), Ci) ? hS(h(Ci), Ci).
Then, the top elements of the sorted pairs are likely
to contain a hypernym candidate and an HCS that
are semantically similar to each other. The final out-
put of AHRAI is the top k pairs in this ranking af-
ter some heuristic rules are applied to it in Step 4.
Rule 1 If the number of documents that include a hypernym
candidate is less than the sum of the numbers of the documents
that include an item in the HCS, then discard both the hypernym
candidate and the HCS from the output.
Rule 2 If a hypernym candidate appears as substrings of an
item in its HCS and it is not a suffix of the item, then discard
both the hypernym candidate and the HCS from the output. If
a hypernym candidate is a suffix of its hyponym candidate, then
half of the members of an HCS must have the hypernym can-
didate as their suffixes. Otherwise, discard both the hypernym
candidate and its HCS from the output.
Rule 3 If a hypernym candidate is an expression belonging
to the category of place names, then replace it by ?place name.?
Recognition of place names was done by an existing morpho-
logical analyzer.
Figure 2: Heuristic rules of AHRAI
In other words, the procedure discards the remain-
ing m ? k pairs in the ranking because they tend to
include erroneous hypernyms.
2.3 Step 4
The steps described up to now can produce a hy-
pernym for hyponym candidates with a certain pre-
cision. However, Shinzato et al reported that the
rules shown in Figure 2 can contribute to higher ac-
curacy. In general, we can expect that a hypernym is
used in a wider range of contexts than those of its hy-
ponyms, and that the number of documents includ-
ing the hypernym candidate should be larger than the
number of web documents including hyponym can-
didates. This justifies Rule 1. Rule 2 is effective
since Japanese is a head final language, and seman-
tic head of a complex noun phrase is the last noun.
Rule 3 was justified by the observation that when a
set of place names is given as an HCS, the procedure
tends to produce the name of the region that includes
all the places designated by the hyponym candidates.
(See Shinzato and Torisawa, 2004 for more details.)
Recall that in Step 3, the ranked pairs of an HCS
and its common hypernym are obtained. By apply-
ing the above rules to these, some pairs are removed
from the ranked pairs, or are modified. For some
given integer k, the top k pairs of the obtained ranked
pairs become the final output of our procedure, as
mentioned before.
3 Proposed Method: HEAIH
Our proposed method, Hyponym Extraction Algo-
rithm from Itemizations and Headings (HEAIH),
is obtained by using some steps of AHRAI. The
HEAIH procedure is given a set of l hypernyms, de-
noted by X = {xi}li=1, where xi is a hypernym,
and finds hyponyms for the hypernyms. The basic
behavior of the HEAIH is summarized as follows.
First, it downloads the documents which are likely to
contain itemizations consisting of hyponyms of the
given hypernyms. This is done by generating possi-
ble headings or explanations of the itemizations by
using prespecified linguistic patterns and by search-
?X(?)??? (table of X) ?X(?)???? (guide to X)
?X(?)????? (category of X) ?X(?)???? (list of X)
?X(?)??? (vote to X) ?X(?)????? (menu of X)
?X(?)?????? (ranking of X)
X is a place holder that a given hypernym fills in.
Figure 3: Patterns for generating headings
ing the documents including the expressions with an
existing search engine. Second, the procedure ap-
plies Steps 1 and 2 of AHRAI and computes a ranked
list of hypernym candidates for each HCS extracted
from the itemizations in the downloaded documents.
The list is ranked in descending order of the hS score
values. Note that the ranked list is generated inde-
pendently from a given hypernym.
We assume that a given hypernym is likely to be
a true hypernym if the top elements of the ranked
list of hypernym candidates contain many substrings
of the hypernym. The procedure computes a score
value, which is designed so that it has a large value
when many substrings of the given hypernym are
included in the list. Then, the pairs of a given hy-
pernym and a corresponding HCS are sorted by the
score value, and only the top k pairs are provided as
the output of the whole procedure.
More precisely, HEAIH consists of Steps A-E,
each of which are described below.
Step A For each of the given hypernyms, denoted
by xi, generate a set of strings which are typically
used in headings, such as ?List of xi,? by using the
prespecified patterns listed in Figure 3. The set of
generated strings for a hypernym xi is denoted by
Hd(xi). Give each string in Hd(xi) to an existing
search engine and pick up a string that has the maxi-
mum hit count in Hd(xi). Then, download the doc-
uments in the ranking produced by the engine for the
picked up string. In our experiments, we downloaded
the top 25 documents for each hypernym if the rank-
ing contained more than 25 documents. Otherwise,
all the documents were downloaded.
Step B Identify the itemizations in the downloaded
documents and extract the expressions in them by us-
ing Step 1 of AHRAI. The results obtained in this
step are denoted by B(X) = {?x?h, Ch?}mh=1, where
x?h is one of the given hypernyms and Ch is an HCS
extracted from a document downloaded for x?h.
Step C Apply Step 2 of AHRAI to each HCS
Ch such that ?x?h, Ch? ? B(X), and then obtain a
ranked list that contains the top p words according to
the hS values and is ranked by the values. We denote
the list as HCList(Ch).
Step D Sort the set B(X) = {?x?h, Ch?}mh=1 in de-
scending order of the hSC value, which is given be-
low.
hSC(x?h, Ch) = sim(x?h, Ch)
?
?p
j=1{sub(x?h, jth(HCList(Ch), j))?
hS(jth(HCList(Ch), j), Ch)}
jth(list, j) denotes the j-th element in list and
sub(x, y) =
{
1 if y is a substring of x
0 otherwise.
In short, the score hSC is the sum of the score values
hS for the substrings of a given hypernym that was
contained in the top p elements of the ranked list pro-
duced by Step 2 of AHRAI. In our experiments, we
assumed p = 10. In addition, the score is weighted
by the similarity measure sim(x,C)3.
Step E Apply Rules 1 and 2 of AHRAI to each el-
ement of the sorted list obtained in Step D, and pro-
duce the top k pairs that survived the check by the
rules as the final output. In our experiments, we as-
sumed k = 200, while we obtained B(X) consisting
of 2,034 pairs.
Note that the weighting factor sim(x,C) in hSC
contributed to high accuracy in our experiments us-
ing a development set.
4 Experimental Results
To evaluate our procedure, we had to provide a set of
proper hypernyms for which HEAIH would find hy-
ponyms. This was a rather difficult task. There are
many nouns that cannot be hypernyms. We assumed
that the Japanese noun sequences or nouns that occu-
pied the position of X in the patterns ?X??? (table
of X) ?X???? (guide to X) ???? X? (succes-
sive (or chronological list of) X) and ???X? (well-
known X) in corpora were appropriate as hypernyms.
(Despite this filtering, there were some inappropri-
ate hypernyms in the set of hypernyms subjected to
the procedures in our experiments. These inappro-
priate hypernyms included expressions whose hy-
ponyms change drastically according to the situa-
tion in which the expressions are used. Examples
are ?recommended products.? One cannot determine
the possible hyponyms without knowing who is rec-
ommending. We judged any hyponymy relations in-
cluding such hypernyms as being unacceptable. )
We downloaded 1.00? 106 Japanese HTML doc-
uments (1.26 GB without tags), applied the above
patterns and found 8,752 expressions. Then, we ran-
domly picked out 100 hypernym candidates from
869 expressions that occurred with the above pat-
terns more than three times, and 100 hypernym can-
didates from the remaining 7,883 expressions. These
200 hypernym candidates became the input for our
procedure. As mentioned, we downloaded a maxi-
mum of 25 pages for each hypernym, and extracted
3In HEAIH, the hypernym x may not be included in the set
of nouns for which we obtained a co-occurrence vector since x is
simply given to the procedure from outside, and the procedure
may not be able to compute the sim values. In that case, we
replace x with the longest suffix of x that is contained in the set
of nouns for which co-occurrence vectors were obtained. The
head final characteristic of the Japanese language justifies this
replacement.
??? (laboratories, 34)*, ???? (health food/beverage, 18)*, ?
??? (welfare facilities, 13)*, ?? (functionalities, 12), ????
(parks in cities, 10)*,? (stores/shops, 10)*,?? (emperors, 7)*,??
(districts, 6)*, ?? (businesses, 6), ?? (legacies, 6)*, ??????
(offered products, 5),???? (participant companies, 5),?? (works
of art, 5)*, ??? (parts of machines, 5), ?????? (Japan?s top
three something, 4), ?? (novels, 4)*, ??? (club activities, 3)*, ?
???? (fortune telling websites, 3)*,???? (rules of business, 3),
??????? (time attack, 3), ???? (commands, 3), ????
(recommended products, 2), ??? (producers,2), ? (poems, 2)*, ?
(cities or markets, 2)*, ???? (alpine plants, 2)*, ???? (names
of teams, 2)*, ??????? (side businesses, 2), ??? (jobs, 2),
?? (things, 1), ???? (Japanese versions, 1), ?? (animals, 1)*,
?? (specialties, 1), ?? (introductions, 1), ??? (novelists, 1)*,
?? (questions, 1), ?? (data, 1), ?????? (working at home,
1), ????? (students? clubs, 1)*, ?? (venues, 1) ?? (names
of railway stations, 1)*, ???????? (dept. of multimedia), ?
?????? (?Power Stone? amulet, 1)*, ??? (bands/groups of
musicians, 1)*,??? (chefs, 1) *,?????? (game programs, 1)*,
??? (characters in games/movies/stories, 1)*, ???? (idols, 1)*,
Figure 4: List of hypernyms in the HEAIH output
3,211 itemizations from them. (We restricted the
itemizations to the ones containing less than or equal
to 30 items.) Then, we picked out 2,034 itemiza-
tions and used them in our evaluation. The choice
was made in the following manner. First, for each
hypernym candidate, the itemizations were sorted in
ascending order of the distance between the occur-
rence of the hypernym candidate and the itemization
in the downloaded page. Then, the itemizations in
the top 65% were chosen for each hypernym.4. This
selection was made to eliminate the itemizations lo-
cated extremely far from the given hypernyms and
to keep the number of itemizations close to 2,000,
which was the number of itemizations used in Shin-
zato and Torisawa, 2004.
Recall that HEAIH (and AHRAI) require two dif-
ferent types of document sets: global document sets
and local document sets. As a global document
set, we used the downloaded 1.00 ? 106 HTML
documents used to obtain hypernyms given to the
HEAIH. As a local document set for each hyponym
candidate, we downloaded the top 100 documents in
the ranking produced by a search engine. In addition,
we used 5.72?106 Japanese HTML documents (6.27
GB without tags) to obtain co-occurrence vectors to
calculate the semantic similarities between expres-
sions. To derive co-occurrence vectors, we parsed
the documents by using a downgraded version of an
existing parser (Kanayama et al, 2000) and collected
co-occurrences from the parsing results.
As mentioned, we obtained 200 pairs of a hyper-
nym and an HCS as the final HEAIH output. All
the hypernyms appearing in the output are listed in
Figure 4 along with their English translations and
4Particularly, when only one itemization was obtained for a
hypernym, it was selected.
hypernym HCS
?? *??, *??, *??, *??, *??, *??
(emperor) (These are Chinese Emperors.)
???? *?????????,
(welfare *?????????,
facilities) *???????????
(These are welfare facilities)
???? *???????, *?????,
(health food/ *????, *?????, *????
beverage) (These are teas which are good for health.)
????? *?????, *??????
(fortune telling *???????, *???????
websites) (These are fortune telling websites.)
??? ???,?????,???????
(novelist) (These are novels.)
Figure 5: Examples of the acquired hyponymy rela-
tions
the number of HCSs that the procedure produced
with the hypernym. In the 200 pairs, 48 hypernyms
appeared. The HCSs were taken from 119 distinct
websites, and the maximum number of the HCSs
taken from a single site was 7. The resulting pairs
of hypernym candidates and hyponym candidates
were checked by the authors according to the
definition of the hypernym given in Miller et al,
1990; i.e., we checked if the expression ?a hyponym
candidate is a kind of a hypernym candidate.? is
acceptable. Figure 5 shows some examples of the
hypernym-HCS pairs that were obtained by HEAIH.
A hyponym candidates in the HCSs is marked by
?*? if it is a proper hyponym of the hypernym in
the pair. We then computed the precision, which
was the ratio of correct hypernym-hyponym pairs
against all the pairs obtained from the top 200 pairs
of an HCS and its hypernym candidate. The graph
in Figure 6 plots the precision obtained by HEAIH,
along with the precisions of the alternative methods
as we explain later. The x-axis of the graph indicates
the number of hypernym-hyponym pairs obtained
from the top j pairs of an HCS and its hypernym
candidate, while the y-axis indicates the precision.
More precisely, the curve plots the points denoted by
?
?j
h=1 |Ci|, (
?j
h=1 correct(Ch, x?h))/(
?j
h=1 |Ch|)?,
where the output of the HEAIH is denoted by
{?x?h, Ch?}200h=1 and 1 ? j ? 200. correct(Ch, x?h)
indicates the number of hyponym candidates in Ch
that are true hyponyms of the hypernym x?h.
We compared the performances of the following
five alternative methods with that of HEAIH.
Alternative 1 Produce pairs consisting of a given
hypernym and a hyponym candidate in an HCS if the
given hypernym is a suffix of the hyponym candi-
date. Note that Japanese is a head final language and
that suffixes of hyponym candidates are good candi-
dates to be hypernyms.
Alternative 2 Extract hyponymy relations by ap-
plying lexicosyntactic patterns to the documents in
the local document sets for our method. We used
 0
 20
 40
 60
 80
 100
 0  500  1000  1500  2000
pre
cisi
on 
[%]
# of hypernym hyponym pairs
HEAIHAlternative 1Alternative 2Alternative 3
 Alternative 4Alternative 5
Figure 6: Precision of hyponymy relations
hypernym?hyponym?,hyponym, .*??? .* hypernym,
hyponym .*???? .* hypernym,
hyponym .*??? .* hypernym,
hyponym .*?? (?|?)? hypernym,
hyponym .*????? .* hypernym,
hyponym .*? (? |?)? .* hypernym,
hyponym .* (? |??) .* hypernym
The hypernym and hyponym may be bracketed by??or ??.
Figure 7: Lexicosyntactic patterns
patterns proposed in previous work (Imasumi, 2001;
Ando et al, 2003) (Figure 7). Note that these are reg-
ular expressions and may overgenerate hyponymy
relations; however, they do not miss the relations ac-
quired through more sophisticated methods such as
those with parsers.
Alternative 3 Extract hyponymy relations by look-
ing for lexicosyntactic patterns with an existing
search engine. The patterns used were basically
the same as those used in Alternative 2. However,
the expression ?.*? was eliminated from the pat-
terns and the disjunctions ?|? were expanded to sim-
ple strings since the engine would not accept regu-
lar expressions. In addition, the pattern ?hypernym
?hyponym?? was not used because the brackets ?
??? were not treated properly by the engine.
Alternative 4 Original AHRAI.
Alternative 5 Produce hypernym-hyponym pairs
according to only the distance between the headings
including the hypernym and the itemizations includ-
ing HCSs. Recall that Hd(x) is the set of strings
likely to be headings of itemizations for a given hy-
pernym x. This alternative method computes the dis-
tance in bytes between the position of a member of
Hd(x) in a downloaded document and the position
of the itemization including an HCS. The pairs of an
itemization and a given hypernym are then sorted ac-
cording to this distance to produce the 200 pairs with
the smallest distance as pairs of hypernyms and the
corresponding HCSs. Note that we assumed a head-
ing must appear before an HCS.
 0
 20
 40
 60
 80
 100
 0  500  1000  1500  2000  2500
pre
cisi
on 
[%]
# of hypernym hyponym pairs
HEAIHAHRAI (full)
 HEAIH (restricted)Alternative 4
Figure 8: Comparison between HEAIH and AHRAI
We checked if the above alternatives can acquire
the correct pairs of a hypernym and a hyponym ob-
tained by HEAIH. In other words, we counted how
many correct pairs produced by HEAIH were also
acquired by the alternatives when using the same
document set. Note that all the alternative methods
except for Alternative 5 were applied only to the 200
pairs of a hypernym and an HCS that were the final
HEAIH output. The results are presented in Figure 6.
The curves indicate the ratios of correct hyponymy
relations that are acquired by an alternative against
all the relations produced by HEAIH. As for Alterna-
tives 1-4, we plotted the graph assuming the pairs of
hypernym candidates and hyponym candidates were
sorted in the same order as the order obtained by our
procedure. In the case of Alternative 5, the 2,034
pairs of a hypernym candidates and an HCS, which
were the results of Step B in HEAIH, were sorted ac-
cording to the distance between headings and item-
izations, and only the top 200 pairs were produced as
the final output. The results suggest that our method
can acquire a significant number of hyponymy rela-
tions that the alternatives miss.
We then conducted a fairer comparison between
HEAIH and Alternative 4 (or AHRAI). There are
some hypernyms that can never be produced by
AHRAI since these hypernyms are not considered in
AHRAI. Recall that we computed the score hS for
the nouns in a set N , which contained the 155,345
nouns most frequently observed in the downloaded
5.72? 106 documents in our experiments. If a given
hypernym was not included in N , AHRAI could not
produce that hypernym. In addition, some of the
given hypernyms are actually noun sequences (or
complex nouns) and cannot be members of N . On
the other hand, HEAIH can acquire a hypernym not
included in N if the hypernym contains substrings
included in N . Thus, we also compared the per-
formance under the assumption that only the hyper-
nyms included in N could be true hypernyms. The
results are presented in Figure 8. ?Alternative 4?
refers to the performance of AHRAI, while ?HEAIH
(restricted)? indicates the performance of HEAIH
 0
 20
 40
 60
 80
 100
 0  200  400  600  800
pre
cisi
on 
[%]
# of hypernym hyponym pairs
HEAIHAlternative 1Alternative 2Alternative 3Alternative 4Alternative 5
Figure 9: Comparison with balanced data
when the produced hypernyms were restricted to the
members of N . They show that HEAIH still out-
performed AHRAI. In addition, the curve ?AHRAI
(full)? shows the performance of AHRAI when we
accept the hypernyms that were not given to the
HEAIH and all the 2,034 pairs of a hypernym candi-
date and an HCS were sorted according to the origi-
nal score for AHRAI to produce the top 200 pairs. In
this case, AHRAI outperformed HEAIH, though the
difference is small.
In the next set of experiments, we compared
HEAIH and Alternatives 1-5 in a slightly different
setting. Recall that Figure 4 gave the list of hy-
pernyms in the HEAIH output and the number of
HCSs that the procedure produced with each hyper-
nym. The data was not balanced very evenly. While
the procedure found 34 HCSs for laboratories, it pro-
vided only one HCS for animals. We tried to reeval-
uate these methods by using more balanced data.
From the data, we eliminated the pairs of a hyper-
nym and an HCS that were not included in the top
five for each hypernym in the ranking of the HEAIH
output. In other words, each hypernym could have a
maximum of only five HCSs in the evaluation data.
This reduced the influence by dominant hypernyms.
In addition, we removed problematic hypernyms
from the evaluation data. The preserved hypernyms
are marked by ?*? in Figure 4. We preserved only the
hypernyms that could have proper nouns, names of
species, or trade names as their hyponyms.5 In addi-
tion, there are inappropriate hypernyms such as those
for which we could not determine their hyponyms
without knowing the situation in which the hyper-
nyms are used, as mentioned before. We eliminated
5Evidently, this condition was more restrictive than we ex-
pected with regard to hypernyms, and some intuitively accept-
able hypernyms were not preserved. Examples are ?jobs? and
?business? (For their Japanese translation, we could not find hy-
ponyms which were either proper nouns, names of species, or
trade names). We made this restriction simply to keep the condi-
tion simple and to reduce borderline cases of proper hypernyms.
Note that some of the eliminated hypernyms, such as ?jobs? and
?business?, were treated as proper hypernyms in the first com-
parison in Figure 6.
such hypernyms too. We also removed ?things? be-
cause it was too general. As a result of these changes,
the evaluation data contained 73 pairs of a hyper-
nym and an HCS. The comparison using this data
is shown in Figure 9. HEAIH still acquired a large
number of correct hyponymy relations that the alter-
native methods miss.
5 Conclusions
We have presented a new method for acquiring hy-
ponyms for prespecified hypernyms by using item-
izations and their headings (or explanations.) This
method was developed by modifying Shinzato?s al-
gorithm to find hypernyms from itemizations in
HTML documents. The method could find a large
number of hyponymy relations that alternative meth-
ods, including the original Shinzato algorithm, could
not.
References
Maya Ando, Satoshi Sekine, and Shun Ishizaki.
2003. Automatic extraction of hyponyms from
newspaper using lexicosyntactic patterns. In IPSJ
SIG Technical Report 2003-NL-157, pages 77?82.
in Japanese.
Sharon A. Caraballo. 1999. Automatic construc-
tion of a hypernym-labeled noun hierarchy from
text. In Proceedings of 37th Annual Meeting of the
Association for Computational Linguistics, pages
120?126.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online
question answering: Answering questions before
they are asked. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 1?7.
Marti A. Hearst. 1992. Automatic acquistition of
hyponyms from large text corpora. In Proceed-
ings of the 14th International Conference on Com-
putational Linguistics, pages 539?545.
Kyosuke Imasumi. 2001. Automatic acquisition
of hyponymy relations from coordinated noun
phrases and appositions. Master?s thesis, Kyushu
Institute of Technology.
Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mit-
suishi, and Jun?ichi Tsujii. 2000. A hybrid
Japanese parser with hand-crafted grammar and
statistics. In Proceedings of COLING 2000, pages
411?417.
Emmanuel Morin and Christian Jacquemin. 2003.
Automatic acquisition and expansion of hyper-
nym links. In Computer and the Humanities 2003.
forthcoming.
Keiji Shinzato and Kentaro Torisawa. 2004. Acquir-
ing hyponymy relations from web documents. In
Proceedings of HLT-NAACL 2004, pages 73?80.
Acquiring Hyponymy Relations from Web Documents
Keiji Shinzato Kentaro Torisawa
School of Information Science,
Japan Advanced Institute of Science and Technology (JAIST)
1-1 Asahidai, Tatsunokuchi, Nomi-gun, Ishikawa, 923-1292 JAPAN
{skeiji,torisawa}@jaist.ac.jp
Abstract
This paper describes an automatic method for
acquiring hyponymy relations from HTML
documents on the WWW. Hyponymy relations
can play a crucial role in various natural lan-
guage processing systems. Most existing ac-
quisition methods for hyponymy relations rely
on particular linguistic patterns, such as ?NP
such as NP?. Our method, however, does not
use such linguistic patterns, and we expect
that our procedure can be applied to a wide
range of expressions for which existing meth-
ods cannot be used. Our acquisition algo-
rithm uses clues such as itemization or listing
in HTML documents and statistical measures
such as document frequencies and verb-noun
co-occurrences.
1 Introduction
The goal of this work is to become able to automatically
acquire hyponymy relations for a wide range of words
or phrases from HTML documents on the WWW. We do
not use particular lexicosyntactic patterns, as previous at-
tempts have (Hearst, 1992; Caraballo, 1999; Imasumi,
2001; Fleischman et al, 2003; Morin and Jacquemin,
2003; Ando et al, 2003). The frequencies of use for such
lexicosyntactic patterns are relatively low, and there can
be many words or phrases that do not appear in such pat-
terns even if we look at a large number of texts. The effort
of searching for other clues indicating hyponymy rela-
tions is thus significant. We try to acquire hyponymy re-
lations by combining three different types of clue obtain-
able from a wide range of words or phrases. The first type
of clue is inclusion in itemizations or lists found in typi-
cal HTML documents on the WWW. The second consists
of statistical measures such as the document frequency
(df) and the inverse document frequency (idf), which are
? Car Specification
? Toyota
? Honda
? Nissan
Figure 1: An example of itemization
popular in the IR literature. The third is verb-noun co-
occurrence in normal corpora.
In our acquisition, we made the following assumptions.
Assumption A Expressions included in the same item-
ization or listing in an HTML document are likely
to have a common hypernym.
Assumption B Given a set of hyponyms that have a
common hypernym, the hypernym appears in many
documents that include the hyponyms.
Assumption C Hyponyms and their hypernyms are se-
mantically similar.
Our acquisition process computes a common hyper-
nym for expressions in the same itemizations. It pro-
ceeds as follows. First, we download a large number
of HTML documents from the WWW and extract a set
of natural language expressions that are listed in the
same itemized region of documents. Consider the item-
ization in Fig. 1. We extract the set of expressions,
{Toyota,Honda,Nissan} from it. From Assumption A,
we can treat these expressions as candidates of hyponyms
that have a common hypernym such as ?company?. We
call such expressions in the same itemization hyponym
candidates. Particularly, a set of the hyponym candi-
dates extracted from a single itemization or list is called
a hyponym candidate set (HCS). For the example docu-
ment, we would treat Toyota, Honda, and Nissan as hy-
ponym candidates, and regard them as members of the
same HCS.
We then download documents that include at least one
hyponym candidate by using an existing search engine,
and pick up a noun that appears in the documents and
that has the largest score. The score was designed so
that words appearing in many downloaded documents are
highly ranked, according to Assumption B. We call the
selected noun a hypernym candidate for the given hy-
ponym candidates.
Note that if we download documents including ?Toy-
ota? or ?Honda?, many will include the word ?company?,
which is a true hypernym of Toyota. However, words
which are not hypernyms, but which are closely associ-
ated with Toyota or Honda (e.g., ?price?) will also be in-
cluded in many of the downloaded documents. The next
step of our procedure is designed to exclude such non-
hypernyms according to Assumption C. We compute the
similarity between hypernym candidates and hyponym
candidates in an HCS, and eliminate the HCS and its hy-
pernym candidate from the output if they are not seman-
tically similar. For instance, if the previous step of our
procedure produces ?price? as a hypernym candidate for
Toyota and Honda, then the hypernym candidate and the
hyponym candidates are removed from the output. We
empirically show that this helps to improve overall preci-
sion.
Finally, we further elaborate computed hypernym can-
didates by using additional heuristic rules. Though we
admit that these rules are rather ad hoc, they worked well
in our experiments.
We have tested the effectiveness of our methods
through a series of experiments in which we used HTML
documents downloaded from actual web sites. We ob-
served that our method can find a significant number of
hypernyms that (at least some of) alternative hypernym
acquisition procedures cannot acquire, at least, when only
a rather small amount of texts are available.
In this paper, Section 2 describes our acquisition al-
gorithm. Section 3 gives our experimental results which
we obtained using Japanese HTML documents, and Sec-
tion 4 discusses the benefit obtained through our method
based on a comparison with alternative methods.
2 Acquisition Algorithm
Our acquisition algorithm consists of four steps, as ex-
plained in this section.
Step 1 Extraction of hyponym candidates from itemized
expressions in HTML documents.
Step 2 Selection of a hypernym candidate with respect
to df and idf.
Step 3 Ranking of hypernym candidates and HCSs
based on semantic similarities between hypernym
and hyponym candidates.
<UL>
<LI>Car Specification</LI>
<UL>
<LI>Toyota</LI>
<LI>Honda</LI>
<LI>Nissan</LI>
</UL>
</UL>
Figure 2: An example of HTML documents
Step 4 Application of a few additional heuristics to elab-
orate computed hypernym candidates and hyponym
candidates.
2.1 Step 1: Extraction of hyponym candidates
The objective of Step 1 is to extract an HCS, which is a set
of hyponym candidates that may have a common hyper-
nym, from the itemizations or lists in HTML documents.
Many methods can be used to do this. Our approach is
a simple one. Each expression in an HTML document
can be associated with a path, which specifies both the
HTML tags that enclose the expression and the order of
the tags. Consider the HTML document in Figure 2. The
expression ?Car Specification? is enclosed by the tags
<LI>,</LI> and <UL>,</UL>. If we sort these tags
according to their nesting order, we obtain a path (UL, LI)
and this path specifies the information regarding the place
of the expression. We write ?(UL, LI),Car Specification?
if (UL, LI) is a path for the expression ?Car Specifica-
tion?. We can then obtain the following paths for the ex-
pressions from the document.
?(UL, LI),Car Specification?,
?(UL, UL, LI),Toyota?,
?(UL, UL, LI),Honda?,
?(UL, UL, LI),Nissan?
Basically, our method extracts the set of expressions
associated with the same path as an HCS 1. In the above
example, we can obtain the HCS {Toyota,Honda, ? ? ?}.
We extract an itemization only when its size is n and
3 < n < 20. This is because the processing of large
itemizations (particularly the downloading of the related
documents) is time-consuming, and small itemizations
are often used to obtain a proper layout in HTML doc-
uments2.
1We actually need to distinguish different occurrences of the
tags in some cases to prevent distinct itemizations from being
recognized as a single itemization.
2We found some words that are often inserted into an item-
ization but do not have common semantic properties with other
items in the same itemization, during the experiments using a
development set. ???? (links)? and ???? (help)? are ex-
amples of such words. We prepared a list of such words con-
sisting of 70 items, and removed them from the HCSs obtained
in Step 1.
2.2 Step 2: Selection of a hypernym candidate by df
and idf
In Step 1, we can obtain a set of hyponym candidates,
an HCS, that may have a common hypernym. In Step
2, we select a common hypernym candidate for an HCS.
First, we prepare two sets of documents. We randomly
select a large number of HTML documents and download
them. We call this set of documents a global document
set. We assume this document set indicates the general
tendencies of word frequencies. Then we download the
documents including each hyponym candidate in a given
HCS. This document set is called a local document set,
and we use it to know the strength of the association of
nouns with the hyponym candidates.
Let us denote a given HCS as C, a local document
set obtained from all the items in C as LD(C), and a
global document set as G. We also assume that N is
a set of words, which can be candidates of hypernym3.
A hypernym candidate, denoted as h(C), for C is ob-
tained through the following formula, where df (n,D) is
the number of documents that include a noun n in a doc-
ument set D.
h(C) = argmaxn?N{df (n,LD(C)) ? idf (n,G)}
idf (n,G) = log |G|df (n,G)
The score has a large value for a noun that appears in a
large number of documents in the local document set and
is found in a relatively small number of documents in the
global document set.
In general, nouns strongly associated with many items
in a given HCS tend to be selected through the above for-
mula. Since hyponym candidates tend to share a common
semantic property, and their hypernym is one of the words
strongly associated with the common property, the hyper-
nym is likely to be picked up through the above formula.
Note that a process of generalization is performed auto-
matically by treating all the hyponym candidates in an
HCS simultaneously. That is, words strongly connected
with only one hyponym candidate (for instance, ?Lexus?
for Toyota) have relatively low score values since we ob-
tain statistical measures from all the local document sets
for all the hyponym candidates in an HCS.
Nevertheless, this scoring method is a weak method in
one sense. There could be many non-hypernyms that are
3In our experiments, N is a set consisting of 37,639 words,
each of which appeared more than 500 times in 33 years of
Japanese newspaper articles (Yomiuri newspaper 1987-2001,
Mainichi newspaper 1991-1999 and Nikkei newspaper 1983-
1990; 3.01 GB in total). We excluded 116 nouns that we ob-
served never be hypernyms from N . An example of such noun
is ?? (I)?. We found them in the experiments using a develop-
ment set.
strongly associated with many of the hyponym candidates
(for instance, ?price? for Toyota and Honda). Such non-
hypernyms are dealt with in the next step.
An evident alternative to this method is to use
tf (n,LD(C)), which is the frequency of a noun n in the
local document set, instead of df (n,LD(C)). We tried
using this method in our experiments, but it produced less
accurate results, as we show in Section 3.
2.3 Step 3: Ranking of hypernym candidates and
HCSs by semantic similarity
Thus, our procedure can produce pairs consisting of a
hypernym candidate and an HCS, which are denoted by
{?h(C1), C1?, ?h(C2), C2?, ? ? ? , ?h(Cm), Cm?}.
Here, C1, ? ? ? , Cm are HCSs, and h(Ci) is a common hy-
pernym candidate for hyponym candidates in an HCS Ci.
In Step 3, our procedure ranks these pairs by using the
semantic similarity between h(Ci) and the items in Ci.
The final output of our procedure is the top k pairs in this
ranking after some heuristic rules are applied to it in Step
4. In other words, the procedure discards the remaining
m ? k pairs in the ranking because they tend to include
erroneous hypernyms.
As mentioned, we cannot exclude non-hypernyms that
are strongly associated with hyponym candidates from
the hypernym candidate obtained by h(C). For exam-
ple, the value of h(C) may be a non-hypernym ?price?,
rather than ?company?, when C = {Toyota,Honda}.
The objective of Step 3 is to exclude such non-hypernyms
from the output of our procedure. We expect such non-
hypernyms to have relatively low semantic similarities to
the hyponym candidates, while the behavior of true hy-
pernyms should be semantically similar to the hyponyms.
If we rank the pairs of hypernym candidates and HCSs
according to their semantic similarities, the low ranked
pairs are likely to have an erroneous hypernym candidate.
We can then obtain relatively precise hypernyms by dis-
carding the low ranked pairs.
The similarities are computed through the following
steps. First, we parse all the texts in the local document
set, and check the argument positions of verbs where hy-
ponym candidates appear. (To parse texts, we use a down-
graded version of an existing parser (Kanayama et al,
2000) throughout this work.) Let us denote the frequency
of the hyponym candidates in an HCS C occupying an
argument position p of a verb v as fhypo(C, p, v). As-
sume that all possible argument positions are denoted as
{p1, ? ? ? , pl} and all the verbs as {v1, ? ? ? , vm}. We then
define the co-occurrence vector of hyponym candidates
as follows.
hypov(C) = ?fhypo(C, p1, v1), fhypo(C, p2, v1), ? ? ? ,
fhypo(C, pl?1, vm), fhypo(C, pl, vm)?
In the same way, we can define the co-occurrence vec-
tor of a hypernym candidate n.
hyperv(n) = ?f(n, p1, v1), ? ? ? , f(n, pl, vm)?
Here, f(n, p, v) is the frequency of a noun n occupying
an argument position p of a verb v obtained from the pars-
ing results of a large number of documents - 33 years of
Japanese newspaper articles (Yomiuri newspaper 1987-
2001, Mainichi newspaper 1991-1999, and Nikkei news-
paper 1990-1998; 3.01 GB in total) - in our experimental
setting.
The semantic similarities between hyponym candi-
dates in C and a hypernym candidate n are then computed
by a cosine measure between the vectors:
sim(n,C) = hypov(C) ? hyperv(n)|hypov(C)||hyperv(n)|
Our procedure sorts the hypernym-HCS pairs
{?h(Ci), Ci?}mi=1 using the value
sim(h(Ci), Ci) ? df (h(Ci),LD(Ci)) ? idf (h(Ci), G)
Note that we consider not only the similarity but also the
df ? idf score used in Step 2 in the sorting.
An evident alternative to the above method is the al-
gorithm that re-ranks the top j hypernym candidates ob-
tained by df ? idf for a given HCS by using the same
score. However, we found no significant improvement
when this alternative was used in our experiments, as we
later explain.
2.4 Step 4: Application of other heuristic rules
The procedure described up to now can produce a hyper-
nym for hyponym candidates with a certain precision. We
found, though, that we can improve accuracy by using a
few more heuristic rules, which are listed below.
Rule 1 If the number of documents that include a hyper-
nym candidate is less than the sum of the numbers of
the documents that include an item in the HCS, then
discard both the hypernym candidate and the HCS
from the output.
Rule 2 If a hypernym candidate appears as substrings of
an item in its HCS and it is not a suffix of the item,
then discard both the hypernym candidate and the
HCS from the output. If a hypernym candidate is
a suffix of its hyponym candidate, then half of the
members of an HCS must have the hypernym can-
didate as their suffixes. Otherwise, discard both the
hypernym candidate and its HCS from the output.
Rule 3 If a hypernym candidate is an expression belong-
ing to the category of place names, then replace it by
?place name?.
In general, we can expect that a hypernym is used in
a wider range of contexts than those of its hyponyms,
and that the number of documents including the hyper-
nym candidate should be larger than the number of web
documents including hyponym candidates. This justifies
Rule 1. We use the hit counts given by an existing search
engine as the number of documents including an expres-
sion.
As for Rule 2, note that Japanese is a head
final language, and a semantic head of a com-
plex noun phrase is the last noun. Consider
the following two Japanese complex nouns.
amerika-eiga / nihon-eiga
(American) (movie) / (Japanese) (movie)
Apparently an American movie is a kind of movie as is
a Japanese movie. There are many multi-word expres-
sions whose hypernyms are their suffixes, and if some
expressions share a common suffix, it is likely to be their
hypernym. However, if a hypernym candidate appears in
a position other than as a suffix of a hyponym candidate,
the hypernym candidate is likely to be an erroneous one.
In addition, if a hypernym candidate is a common suffix
of only a small portion of an HCS, then the HCS tends
not to have semantic uniformity, and such a hypernym
candidate should be eliminated from the output. (We em-
pirically determined ?one-half? as a threshold in our ex-
periments on the development set.)
As for Rule 3, in our experiments on a development set,
we found that our procedure could not provide precise hy-
pernyms for place names such as ?Kyoto? and ?Tokyo?.
In the case of Kyoto and Tokyo, our procedure produced
?Japan? as a hypernym candidate. Although ?Japan? is
consistent with most of our assumptions regarding hy-
pernyms, it is a holonym of Kyoto and Tokyo, but their
hypernym. In general, when a set of place names is given
as an HCS, the procedure tends to produce the name of
the region or area that includes all the places designated
by the hyponym candidates. We then added the rule to re-
place such place names by the expression ?place name,?
which is a true hypernym in many of such cases 4.
Recall that we obtained the ranked pairs of an HCS and
its common hypernym in Step 3. By applying the above
rules, some pairs are removed from the ranked pairs, or
are modified. For some given integer k, the top k pairs of
the obtained ranked pairs become the final output of our
procedure, as mentioned before.
3 Experimental Results
We downloaded about 8.71 ? 105 HTML documents
(10.4 GB with HTML tags), and extracted 9.02 ? 104
HCSs through the method described in Section 2.1. We
4To judge if a hypernym candidate is a place name, we
used the output of a morphological analyzer (Matsumoto et al,
1993).
 0
 20
 40
 60
 80
 100
 0  500  1000  1500  2000  2500
ac
cu
rac
y [%
]
# of hypernym hyponym pairs
Proposed Method (Step 4)Step 3Step 2Step 2 (tf)
Figure 3: Contribution of each step
 0
 20
 40
 60
 80
 100
 0  500  1000  1500  2000  2500
ac
cu
rac
y [%
]
# of hypernym hyponym pairs
Proposed Method
-Step 3
-Step 4
-Rule 1
-Rule 2
-Rule 3
Figure 4: Contribution of each step and rule
randomly picked 2,000 HCSs from among the extracted
HCS as our test set. The test set contained 13,790 hy-
ponym candidates. (Besides these HCSs, we used a de-
velopment set consisting of about 4,000 HCSs to develop
our algorithm.) For each single hyponym candidate, we
downloaded the top 100 documents in the ranking pro-
duced by a search engine5 as a local document set if the
engine found more than 100 documents. Otherwise, all
the documents were downloaded. (Note that a local doc-
ument set for an HCS may contain more than 100 doc-
uments.) As a global document set, we used the down-
loaded 1.00 ? 106 HTML documents (1.26 GB without
HTML tags).
Fig. 3 shows the accuracy of hypernyms
obtained after Steps 2, 3, and 4. We as-
sumed each step produced the sorted pairs of
an HCS and a hypernym, which are denoted by
{?h(C1), C1?, ?h(C2), C2?, ? ? ? , ?h(Cm), Cm?}. The
sorting was done by the score sim(h(Ci), Ci) ?
df (h(Ci),LD(Ci)) ? idf (h(Ci), G) after Steps 3 and
4, as described before, while the output of Step 2 was
sorted by the df ? idf score. In addition, we assumed
5The search engine ?goo?. (http://www.goo.ne.jp)
 0
 20
 40
 60
 80
 100
 0  500  1000  1500  2000  2500
ac
cu
rac
y [%
]
# of hypernym hyponym pairs
Proposed MethodRe-ranking of top 2 hypernym candidatesRe-ranking of top 3 hypernym candidatesRe-ranking of top 4 hypernym candidatesRe-ranking of top 5 hypernym candidates
Figure 5: Contribution of re-ranking
each step produced only the top 200 pairs in the sorted
pairs. (Since the output of Step 4 is the final output, this
means that we also assumed that only the top 200 pairs
of a hypernym and an HCS would be produced as final
output with our procedure. In other words, the remaining
1,800 (=2,000-200) pairs were discarded. )
The resulting hypernyms were checked by the authors
according to the definition of the hypernym given in
Miller et al, 1990, i.e., we checked if the expression ?a
hyponym candidate is a kind of a hypernym candidate.?
is acceptable. Then, we computed the precision, which is
the ratio of the correct hypernym-hyponym pairs against
all the pairs obtained from the top n pairs of an HCS and
its hypernym candidate. The x-axis of the graph indicates
the number of hypernym-hyponym pairs obtained from
the top n pairs of an HCS and its hypernym candidate,
while the y-axis indicates the precision.
More precisely, the curve for Step i plots the following
points, where 1 ? j ? 200.
?
j?
k=1
|Ck|,
?j
k=1 correct(Ck, h(Ck))?j
k=1 |Ck|
?
correct(Ck, h(Ck)) indicates the number of hyponym
candidates in Ck that are true hyponyms of h(Ck). Note
that after Step 4, the precision reached about 75% for 701
hyponym candidates, which was slightly more than 5%
of all the given hyponym candidates. For 1398 hyponym
candidates (about 10% of all the candidates), the preci-
sion was about 61%.
Another important point is that ?Step 2 (tf)? in the
graph refers to an alternative to our Step 2 procedure; i.e.,
the Step 2 procedure in which df (h(C),LD(C)) was re-
placed by tf (h(C),LD(C)). One can see the Step 2 pro-
cedure with df works better than that with tf .
Table 1 shows some examples of the acquired HCSs
and their common hypernyms. Recall that a common suf-
fix of an HCS is a good candidate to be a hypernym. The
examples were taken from cases where a common suffix
hypernym?hyponym?,hyponym .*??? .* hypernym,
hyponym .*?? (?|?)? hypernym,
hyponym .*???? .* hypernym,
hyponym .*??? .* hypernym,
hyponym .*? (? |?)? .* hypernym,
hyponym .*????? .* hypernym,
hyponym .* (? |??) .* hypernym
The hypernym and hyponym may be bracketed by??or ??.
Figure 6: lexicosyntactic patterns
of an HCS was not produced as a hypernym. This list
is actually the output of Step 3, and shows which HCSs
and their hypernym candidates were eliminated/modified
from the output in Step 4 and which rule was fired to
eliminate/modify them.
Next, we eliminated some steps from the whole pro-
cedure. Figure 4 shows the accuracy when one of the
steps was eliminated from the procedure. ?-Step X? or
?-Rule X? refers to the accuracies obtained through the
procedure from which step X or rule X were eliminated.
Note that both graphs indicate that every step and rule
contributed to the improvement of the precision.
Figure 5 compares our method and an alternative
method, which was the algorithm that re-ranks the top j
hypernym candidates for a given HCS by using the score
sim(h,C) ? df (h,LD(C)) ? idf (h,G), where h is a hy-
pernym candidate, in Step 3. (Recall that our algorithm
uses the score only for sorting pairs of HCSs and their hy-
pernym. In other words, we do not re-rank the hypernym
candidates for a single HCS.) We found no significant im-
provement when the alternative was used.
4 Comparison with alternative methods
We have shown that our assumptions are effective for ac-
quiring hypernyms. However, there are other alternative
methods applicable under our settings. We evaluated the
followings methods and compared the results with those
of our procedure.
Alternative 1 Compute the non-null suffixes that are
shared by the maximum number of hyponym can-
didates, and regard the longest as a hypernym can-
didate.
Alternative 2 Extract hypernyms for hyponym candi-
dates by looking at the captions or titles of the item-
izations from which hyponym candidates are ex-
tracted.
Alternative 3 Extract hypernyms by using lexicosyntac-
tic patterns.
Alternative 4 Combinations of Alternative 1-3.
The evaluation method for Alternative 1 and Alterna-
tive 2 is the same as the one for our method. We simply
judged if the produced hypernyms are acceptable or not.
But we used different evaluation method for the other
alternatives. We checked if the correct hypernyms pro-
duced by our method can be found by these alternatives.
This is simply for the sake of easiness of the evaluation.
Note that we evaluated Alternative 1 and Alternative 2 in
the second evaluation scheme when they are combined
and are used as a part of Alternative 4.
More detailed explanations on the alternative methods
are given below.
Alternative 1 Recall that Japanese is a head final lan-
guage, and we have explained that common suffixes of
hyponym candidates are good candidates to be common
hyponyms. Alternative 1 computes a hypernym candidate
according to this principle.
Alternative 2 This method uses the captions of the
itemizations, which are likely to contain a hypernym of
the items in the itemization. We manually found cap-
tions or titles that are in the position such that they can
explain the content of the itemization, and picked up the
caption closest to the itemization and the second closest
to it. Then, we checked if the picked-up captions included
the proper hypernyms. Note that the precision obtained
by this method is just an upper bound of real performance
because we do not have a method to extract hypernyms
from captions at least at the current stage of our research.
Alternative 3 We prepared the lexicosyntactic patterns
in Fig. 6, which are similar to the ones used in the pre-
vious studies of hypernym acquisition in Japanese (Ima-
sumi, 2001; Ando et al, 2003). One difference from the
previous studies was that we used a regular expression
instead of a parser. This may have caused some errors,
but our patterns were more generous than those used in
the previous studies, and did not miss the expressions
matched to the patterns from the previous studies. In
other words, the accuracy obtained with our patterns was
an upper bound on the performance obtained by the previ-
ous proposal. Another difference was that the procedure
was given correct pairs of a hypernym and a hyponym
computed beforehand using our proposed method, and it
only checked whether given pairs could be found by us-
ing the lexicosyntactic patterns from given texts. In other
words, this alternative method checked if the lexicosyn-
tactic patterns could find the hypernym-hyponym pairs
successfully obtained by our procedure. The texts used
were local document sets from which our procedure com-
puted a hypernym candidate. If our procedure has better
figures than this method, this means that our procedure
can produce hypernyms that cannot be acquired by pat-
terns, at least, from a rather small number of texts (i.e., a
maximum of 100 documents per hyponym candidate).
 0
 20
 40
 60
 80
 100
 0  500  1000  1500  2000
ac
cu
rac
y [%
]
# of hypernym hyponym pairs
Proposed MethodAlternative 1Alternative 2Alternative 3Alternative 4
Figure 7: Comparison with alternative methods
Alternative 4 We also compared our procedure with
the combination of all the above methods: Alternative
4. Again, we checked whether the combination could
find the correct hypernym-hyponym pairs provided by
our method. The difference between the precision of our
method and that of Alternative 4 reflects the number of
hypernym-hyponym pairs that our method could acquire
and that Alternative 4 could not. We assumed that for a
given HCS a hypernym was successfully acquired if one
of the above methods could find the correct hypernym. In
other words, the performance of Alternative 4 would be
achieved only when there were a technique to combine
the output of the above methods in an optimal way.
Figure 7 shows the comparison between our procedure
and the alternative methods. We plotted the graph as-
suming the pairs of hypernym candidates and hyponym
candidates were sorted in the same order as the order ob-
tained by our procedure6. The results suggest that our
method can acquire a significant number of hypernyms
that the alternative methods cannot obtain, when we gave
rather small amount of texts, a maximum of 100 docu-
ments per hyponym candidate, as in our current experi-
mental settings. There is possibility that the difference,
particularly the difference from the peformance of Alter-
native 3, becomes smaller when we give more texts to the
alternative methods. But the comparison in such settings
is actually a difficult task because of the time required for
downloading. It is our possible future work.
5 Concluding Remarks and Future Work
We have proposed a method for acquiring hyponymy re-
lations from Web documents, and have shown its effec-
tiveness through experimental results. We also showed
6More precisely, we sorted only the hyponym candidates in
the order used by our procedure for sorting, and attached the
hypernym candidates produced by each alternative to the hy-
ponym candidates.
that our method could find a significant number of hy-
ponymy relations that alternative methods could not, at
least when the amount of documents used was rather
small.
The first goal of our future work is to further improve
the precision of our method. One possible approach will
be to combine our methods with alternative techniques,
which were actually examined in our experiments. Our
second goal is to extend our method so that it can han-
dle multi-word hypernyms. Currently, our method pro-
duces just ?company? as a hypernym of ?Toyota?. If we
can obtain a multi-word hypernym such as ?automobile
manufacturer,? it can provide more useful information to
various types of natural language processing systems.
References
Maya Ando, Satoshi Sekine, and Shun Ishizaki. 2003.
Automatic extraction of hyponyms from newspaper us-
ing lexicosyntactic patterns. In IPSJ SIG Technical Re-
port 2003-NL-157, pages 77?82. in Japanese.
Sharon A. Caraballo. 1999. Automatic construction of
a hypernym-labeled noun hierarchy from text. In Pro-
ceedings of 37th Annual Meeting of the Association for
Computational Linguistics, pages 120?126.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online ques-
tion answering: Answering questions before they are
asked. In Proceedings of the 41st Annural Meeting of
the Association for Computational Linguistics, pages
1?7.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th International Conference on Computational Lin-
guistics, pages 539?545.
Kyosuke Imasumi. 2001. Automatic acquisition of hy-
ponymy relations from coordinated noun phrases and
appositions. Master?s thesis, Kyushu Institute of Tech-
nology.
Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsuishi,
and Jun?ichi Tsujii. 2000. A hybrid Japanese parser
with hand-crafted grammar and statistics. In Proceed-
ings of COLING 2000, pages 411?417.
Yuji Matsumoto, Sadao Kurohashi, Takehito Utsuro, Hi-
roshi Taeki, and Makoto Nagao. 1993. Japanese
Morphological Analyzer JUMAN user?s manual. in
Japanese.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to wordnet: An on-line lexical database.
Journal of Lexicography, 3(4):235?244.
Emmanuel Morin and Christian Jacquemin. 2003. Auto-
matic acquisition and expansion of hypernym links. In
Computer and the Humanities 2003. forthcoming.
Table 1: Examples of the acquired pairs of a hypernym candidate and HCS.
Rank Hypernyms Rank Fired Hypernyms
by Hyponym candidate sets obtained in by Rules obtained in
Step4 Step3 Step3 1 2 3 Step4
?? (murder)*,?? (arson)*,?? (rape)*,??? (burglary)*,
?? ??29 ???? (burgle robbery)*,???? (theft without breaking-in)*, (crime) 68 ? ? ? (crime)????? (robbery without breaking-in)*
???? (Moscow)*,??? (Kiev)*,????? (Tashkend)*,
??
69
???? (Minsk)*,???? (Tbilisi)*,?????? (Dushanbe)*, ???
169 ? ? + (place????? (Bishkek)*,???? (Astana)*,????? (Kishinev)*, (Russia)
name)???? (Erevan)*,??? (Baku)*,?????? (Ashkhabad)*
????? (Seguignol)*,???? (Yasuo Fujii)*,
???? (Yuji Goshima)*,???? (Tomotaka Tamaki)*,
?? ??78 ???? (Hiroki Fukutome)*,???? (Keiichi Hirano)*, (player) 196 ? ? ? (player)????? (Sheldon)*,???? (Kazuhiko Shiotani)*,
(These are baseball players.)
???????? (wireless card),?????????(security),
?? ??81 ????? (radio),??????? (a kind of instrument), (wireless) 200 ? ? ? (wireless)???????? (a kind of department)
???? (Diapensia lapponica)*,????? (Sasa kurilensis),
? ?116 ???????? (Rhododendron aureum)*, (flower) 280 ? ? ? (flower)???????? (Polygonatum lasianthum)*
???? (shiitake mushroom)*,??????? (Hericium ramosum)*,
????? (Pseudocolus schellenbergiae)*, ??? ???
127 ???????? (Rhodophyllus murraii)*, (mash- 306 ? ? ? (mash-
?????? (Amanita virgineoides Bas)*, room) room)
????? (Pseudocolus schellenbergiae)*
139
?? (music),?? (movie),??? (cartoon), ???
324 ? ? ?
???
??? (encounter),??? (artiste) (web site) (web site)
????? (Ryunosuke Akutagawa),???? (Tsugi Takano),
???? (Bokusui Wakayama),????? (Motojiro Kajii),
???? (Roka Tokutomi),????? (Yuriko Miyamoto),
150
???? (Soseki Natume),????? (Kantaro Tanaka), ??
343 ? ? ?
??
????? (Doppo Kunikida),???? (Kyusaku Yumeno), (work) (work)
????????? (William Blake),??? (Kan Kikuchi),
???????? (parse error)
(These are novelists.)
???? (May Day),????? (Christmas Day),????? (Easter),
?? (the New Year),??? (All Saints? Day),??? (Epifania),
??
??
172 ????? (Emancipation Day),????? (Immacolata concezione), (Japan) 391 ? ? + (place???????? (Stefano?s Day),????? (Ferragosto) name)
(These are national holidays in Italy.)
????? (mother)*,?? (warm current)*,?? (cloud drift)*,
?? ??184 ??? (blue sky girl)*,?????? (beauty has guilt)* (movie) 416 ? ? ? (movie)(These are Japanese movies.)
??? (group of galaxies),?????? (member),
??
? ???????? (Andromeda Galaxy)*,??? (The Galaxy)*, (galaxy) 10 ? + ? ?????? (local group of galaxies)
???? (Brazil),????? (Philippine),?? (Korea),
?
??? (India),???? (U.S.A.),?? (Thailand), ??
80 + ? + ??? (China),??? (Peru),??????? (Australia), (Japan)
?????? (Argentina),???? (Spain)
?*? indicates a hyponym candidate that is a true hyponym of the provided hypernym candidate.
?+? in the ?Fired Rules? column indicates a firing rule, while ??? specifies the rule that doesn?t fire.
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 2?11,
Beijing, August 2010
Exploiting Term Importance Categories and
Dependency Relations for Natural Language Search
Keiji Shinzato
Graduate School of Informatics,
Kyoto University
shinzato@i.kyoto-u.ac.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
kuro@i.kyoto-u.ac.jp
Abstract
In this paper, we propose a method that
clearly separates terms (words and de-
pendency relations) in a natural language
query into important and other terms, and
differently handles the terms according to
their importance. The proposed method
uses three types of term importance: nec-
essary, optional, and unnecessary. The
importance are detected using linguistic
clues. We evaluated the proposed method
using a test collection for Japanese infor-
mation retrieval. Performance was resul-
tantly improved by differently handling
terms according to their importance.
1 Introduction
Currently, search engines that receive a couple of
keywords reflecting users? information needs pre-
dominate. These keyword-based searches have
been focused on evaluation conferences for infor-
mation retrieval (IR) such as TREC and NTCIR.
Search engines based on keywords, however, have
a crucial problem that it is difficult for their users
to represent complex needs, such as ?I want to
know what Steve Jobs said about the iPod.? A
natural language sentence can more adeptly ac-
commodate such information needs than a couple
of keywords because users can straightforwardly
present their needs. We call a query represented
by a sentence a natural language query (NLQ).
The other advantage of NLQs is that search
engines can leverage dependency relations be-
tween words in a given query. Dependency rela-
tions allow search engines to retrieve documents
with a similar linguistic structure to that of the
query. Search performance improvement can be
expected through the use of dependency relations.
For handling an NLQ, we can consider a con-
junctive search (AND search) that retrieves docu-
ments that include all terms in the query, a simple
methodology similar to real-world Web searches.
This methodology, however, often leads to insuf-
ficient amounts of search results. In some in-
stances, no documents match the query. This
problem occurs because the amount of search re-
sults is inversely proportional to the number of
terms used in a search; and an NLQ includes many
terms. Hence, a conjunctive search simply using
all terms in an NLQ is problematic.
Apart from this, we can consider conventional
IR methodology. This approach performs a dis-
junctive search (OR search), and then ranks re-
trieved documents according to scores that are
computed by term weights derived from retrieval
models. The methodology attempts to use term
weights to distinguish important terms and other
items. However, a problem arises in that irrelevant
documents are more highly ranked than relevant
ones when giving NLQs. This is because an NLQ
tends to contain some important terms and many
noisy (redundant) terms and document relevancy
is calculated from the combinations of these term
weights.
Avoiding the above problems, we define three
discrete categories of term importance: necessary;
optional, and unnecessary, and propose a method
that classifies words and dependency relations in
an NLQ into term importance, and then, when per-
forming document retrieval, differently handles
the terms according to their importance. The nec-
essary type includes expressions in Named Enti-
2
ties (NEs) and compound nouns, the optional in-
cludes redundant verbs and the unnecessary in-
cludes expressions that express inquiries such as
?I want to find.? The process of IR consists of two
steps: document collecting and document scor-
ing. The proposed method uses only necessary
terms for document collecting and necessary and
optional terms for document scoring.
We evaluated the proposed method using
the test collections built at the NTCIR-3 and
NTCIR-4 conferences for evaluating Japanese IR.
Search performance was resultantly improved by
differently handling terms (words and dependency
relations) according to their importance.
This paper is organized as follows. Section 2
shows related work, and section 3 describes how
to leverage dependency relations in our retrieval
method. Section 4 presents term importance cate-
gories, and section 5 gives methodology for de-
tecting such categories. Experiment results are
shown in section 6.
2 Related Work
A large amount of the IR methodology that has
been proposed (Robertson et al, 1992; Ponte and
Croft, 1998) depends on retrieval models such as
probabilistic and language models. Bendersky
and Croft (Bendersky and Croft, 2008), for in-
stance, proposed a new language model in which
important noun phrases can be considered.
IR methodology based on important term detec-
tion has also been proposed (Callan et al, 1995;
Allan et al, 1997; Liu et al, 2004; Wei et al,
2007). These previous methods have commonly
focused on noun phrases because the methods as-
sumed that a document relates to a query if the
two have common noun phrases. Liu et al (Liu et
al., 2004) classified noun phrases into four types:
proper nouns, dictionary phrases (e.g., computer
monitor), simple phrases, and complex phrases,
and detected them from a keyword-based query
by using named entity taggers, part-of-speech pat-
terns, and dictionaries such as WordNet. The
detected phrases were assigned different window
sizes in a proximity operator according to their
types. Wei et al (Wei et al, 2007) extended Liu?s
work for precisely detecting noun phrases. Their
method used hit counts obtained from Google and
Wikipedia in addition to clues used in Liu?s work.
The differences between the proposed method and
these methods are (i) the proposed method fo-
cuses on an NLQ while the previous methods fo-
cus on a keyword-based query, (ii) the proposed
method needs no dictionaries, and (iii) while the
previous methods retrieve documents by proxim-
ity searches of words in phrases, the proposed
method retrieves them by dependency relations
in phrases. Therefore, the proposed method does
not need to adjust window size, and naturally per-
forms document retrieval based on noun phrases
by using dependency relations.
Linguistically motivated IR research pointed
out that dependency relations did not con-
tribute to significantly improving performance
due to low accuracy and robustness of syntac-
tic parsers (Jones, 1999). Current state-of-the-art
parsers, however, can perform high accuracy for
real-world sentences. Therefore, dependency re-
lations are remarked in IR (Miyao et al, 2006;
Shinzato et al, 2008b). For instance, Miyao et
al. (Miyao et al, 2006) proposed an IR system for
a biomedical domain that performs deep linguis-
tic analysis on a query and each document. Their
system represented relations between words by a
predicate-argument structure, and used ontologi-
cal databases for handling synonyms. Their ex-
periments using a small number of short queries
showed that their proposed system significantly
improved search performance versus a system not
performing deep linguistic analysis. Shinzato
et al (Shinzato et al, 2008b) proposed a Web
search system that handles not only words but
also dependency relations as terms; yet they did
not discuss the effectiveness of dependency rela-
tions. This paper reveals the effectiveness of de-
pendency relations through experiments using test
collections for Japanese Web searches.
3 Exploitation of Dependency Relation
One of the advantages of an NLQ is leveraging
dependency relations between words in the query.
We can expect that search performance improves
because the dependency relations allow systems
to retrieve documents that have similar linguistic
structure to that of the query. Therefore the pro-
posed method exploits dependency relations for
3
 



return to


	





spectacular




 


active







